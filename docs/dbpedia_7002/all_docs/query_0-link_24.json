{
    "id": "dbpedia_7002_0",
    "rank": 24,
    "data": {
        "url": "https://worldwidescience.org/topicpages/s/simulation-based%2Busability%2Btest.html",
        "read_more_link": "",
        "language": "en",
        "title": "based usability test: Topics by WorldWideScience.org",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://worldwidescience.org/sites/www.osti.gov/files/public/image-files/WWSlogo_wTag650px-min.png",
            "https://worldwidescience.org/sites/www.osti.gov/files/public/image-files/OSTIlogo.svg",
            "https://worldwidescience.org/sites/www.osti.gov/files/public/image-files/ICSTIlogo.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Empirical usability testing in a component-based environment : improving test efficiency with component-specific usability measures\n\nNARCIS (Netherlands)\n\nBrinkman, W.P.; Haakma, R.; Bouwhuis, D.G.; Bastide, R.; Palanque, P.; Roth, J.\n\n2005-01-01\n\nThis paper addresses the issue of usability testing in a component-based software engineering environment, specifically measuring the usability of different versions of a component in a more powerful manner than other, more holistic, usability methods. Three component-specific usability measures are\n\nUsability Testing For Android Based Application âJogja Smart Tourismâ\n\nScience.gov (United States)\n\nHarwati; Djati Widodo, Imam\n\n2017-06-01\n\nThe android based application âJogja Smart Tourism (JST)â is designed to help everyone who visited Yogyakarta to enjoy their travel. As new application, it is need to be tested for its usability before launched. Usability testing will show how easy user interfaces are to used. The objective of this research is to demonstrate the result of usability testing for application JST based on five characteristics: learnability, effectiveness, memorability, errors, and satisfaction. About 30 respondents were involved to test the usability of this application. Learnability and effectiveness is calculated from some task that should be finished by respondents, and the rest aspects are calculated from questionnaires that should be answered after simulation. There are 14 functions bound in this usability testing. The result shows total usability level is in 81.75%. Learnability testing shows that 98.8% of respondent could finish the task successfully with 87.5% in efficiency. The memorability level of respondents is good (84.5%) where their ability to fix the errors is 71.5%. And the last for satisfaction level of application interface is 66.25%. Low level of satisfaction occurred because most of respondent felt uncomfortable with landscape interface of application because they should turn their mobile phone while using JST application and also it happened because the lack of using picture and colour inside the application. Both of these becomes important note for the improvement of further applications where the interface in a portrait version is more comfort the use and also utilization of colour and the image will be the main focus to improve customer satisfaction.\n\nIntegrating usability testing and think-aloud protocol analysis with \"near-live\" clinical simulations in evaluating clinical decision support.\n\nScience.gov (United States)\n\nLi, Alice C; Kannry, Joseph L; Kushniruk, Andre; Chrimes, Dillon; McGinn, Thomas G; Edonyabo, Daniel; Mann, Devin M\n\n2012-11-01\n\nUsability evaluations can improve the usability and workflow integration of clinical decision support (CDS). Traditional usability testing using scripted scenarios with think-aloud protocol analysis provide a useful but incomplete assessment of how new CDS tools interact with users and clinical workflow. \"Near-live\" clinical simulations are a newer usability evaluation tool that more closely mimics clinical workflow and that allows for a complementary evaluation of CDS usability as well as impact on workflow. This study employed two phases of testing a new CDS tool that embedded clinical prediction rules (an evidence-based medicine tool) into primary care workflow within a commercial electronic health record. Phase I applied usability testing involving \"think-aloud\" protocol analysis of 8 primary care providers encountering several scripted clinical scenarios. Phase II used \"near-live\" clinical simulations of 8 providers interacting with video clips of standardized trained patient actors enacting the clinical scenario. In both phases, all sessions were audiotaped and had screen-capture software activated for onscreen recordings. Transcripts were coded using qualitative analysis methods. In Phase I, the impact of the CDS on navigation and workflow were associated with the largest volume of negative comments (accounting for over 90% of user raised issues) while the overall usability and the content of the CDS were associated with the most positive comments. However, usability had a positive-to-negative comment ratio of only 0.93 reflecting mixed perceptions about the usability of the CDS. In Phase II, the duration of encounters with simulated patients was approximately 12 min with 71% of the clinical prediction rules being activated after half of the visit had already elapsed. Upon activation, providers accepted the CDS tool pathway 82% of times offered and completed all of its elements in 53% of all simulation cases. Only 12.2% of encounter time was spent using the\n\nCommercial versus in-situ usability testing of healthcare information systems: towards \"public\" usability testing in healthcare organizations.\n\nScience.gov (United States)\n\nKushniruk, Andre W; Borycki, Elizabeth M; Kannry, Joseph\n\n2013-01-01\n\nThe need for improved usability in healthcare IT has been widely recognized. In addition, methods from usability engineering, including usability testing and usability inspection have received greater attention. Many vendors of healthcare software are now employing usability testing methods in the design and development of their products. However, despite this, the usability of healthcare IT is still considered to be problematic and many healthcare organizations that have purchased systems that have been tested at vendor testing sites are still reporting a range of usability and safety issues. In this paper we explore the distinction between commercial usability testing (conducted at centralized vendor usability laboratories and limited beta test sites) and usability testing that is carried out locally within healthcare organizations that have purchased vendor systems and products (i.e. public \"in-situ\" usability testing). In this paper it will be argued that both types of testing (i.e. commercial vendor-based testing) and in-situ testing are needed to ensure system usability and safety.\n\nFidelity considerations for simulation-based usability assessments of mobile ICT for hospitals\n\nDEFF Research Database (Denmark)\n\nDahl, Yngve; Alsos, Ole A; SvanÃ¦s, Dag\n\n2010-01-01\n\ntraining simulation fidelity theories. Based on a review of the training simulation literature, a set of fidelity dimensions through which training simulations are often adjusted to meet specific goals are identified. It is argued that the same mechanisms can be used in usability assessments of mobile ICT...... for hospitals. Our argument is substantiated by using the identified set of fidelity dimensions in a retrospective analysis of two usability assessments. The analysis explains how the configuration of fidelity dimensions, each reflecting various degrees of realism vis-Ã -vis the actual performance context...\n\nFrom Usability Testing to Clinical Simulations: Bringing Context into the Design and Evaluation of Usable and Safe Health Information Technologies. Contribution of the IMIA Human Factors Engineering for Healthcare Informatics Working Group.\n\nScience.gov (United States)\n\nKushniruk, A; Nohr, C; Jensen, S; Borycki, E M\n\n2013-01-01\n\nThe objective of this paper is to explore human factors approaches to understanding the use of health information technology (HIT) by extending usability engineering approaches to include analysis of the impact of clinical context through use of clinical simulations. Methods discussed are considered on a continuum from traditional laboratory-based usability testing to clinical simulations. Clinical simulations can be conducted in a simulation laboratory and they can also be conducted in real-world settings. The clinical simulation approach attempts to bring the dimension of clinical context into stronger focus. This involves testing of systems with representative users doing representative tasks, in representative settings/environments. Application of methods where realistic clinical scenarios are used to drive the study of users interacting with systems under realistic conditions and settings can lead to identification of problems and issues with systems that may not be detected using traditional usability engineering methods. In conducting such studies, careful consideration is needed in creating ecologically valid test scenarios. The evidence obtained from such evaluation can be used to improve both the usability and safety of HIT. In addition, recent work has shown that clinical simulations, in particular those conducted in-situ, can lead to considerable benefits when compared to the costs of running such studies. In order to bring context of use into the testing of HIT, clinical simulation, involving observing representative users carrying out tasks in representative settings, holds considerable promise.\n\nIntegration of MSFC Usability Lab with Usability Testing\n\nScience.gov (United States)\n\nCheng, Yiwei; Richardson, Sally\n\n2010-01-01\n\nAs part of the Stage Analysis Branch, human factors engineering plays an important role in relating humans to the systems of hardware and structure designs of the new launch vehicle. While many branches are involved in the technical aspects of creating a launch vehicle, human factors connects humans to the scientific systems with the goal of improving operational performance and safety while reducing operational error and damage to the hardware. Human factors engineers use physical and computerized models to visualize possible areas for improvements to ensure human accessibility to components requiring maintenance and that the necessary maintenance activities can be accomplished with minimal risks to human and hardware. Many methods of testing are used to fulfill this goal, such as physical mockups, computerized visualization, and usability testing. In this analysis, a usability test is conducted to test how usable a website is to users who are and are not familiar with it. The testing is performed using participants and Morae software to record and analyze the results. This analysis will be a preliminary test of the usability lab in preparation for use in new spacecraft programs, NASA Enterprise, or other NASA websites. The usability lab project is divided into two parts: integration of the usability lab and a preliminary test of the usability lab.\n\nUsing a fuzzy comprehensive evaluation method to determine product usability: A test case.\n\nScience.gov (United States)\n\nZhou, Ronggang; Chan, Alan H S\n\n2017-01-01\n\nIn order to take into account the inherent uncertainties during product usability evaluation, Zhou and Chan [1] proposed a comprehensive method of usability evaluation for products by combining the analytic hierarchy process (AHP) and fuzzy evaluation methods for synthesizing performance data and subjective response data. This method was designed to provide an integrated framework combining the inevitable vague judgments from the multiple stages of the product evaluation process. In order to illustrate the effectiveness of the model, this study used a summative usability test case to assess the application and strength of the general fuzzy usability framework. To test the proposed fuzzy usability evaluation framework [1], a standard summative usability test was conducted to benchmark the overall usability of a specific network management software. Based on the test data, the fuzzy method was applied to incorporate both the usability scores and uncertainties involved in the multiple components of the evaluation. Then, with Monte Carlo simulation procedures, confidence intervals were used to compare the reliabilities among the fuzzy approach and two typical conventional methods combining metrics based on percentages. This case study showed that the fuzzy evaluation technique can be applied successfully for combining summative usability testing data to achieve an overall usability quality for the network software evaluated. Greater differences of confidence interval widths between the method of averaging equally percentage and weighted evaluation method, including the method of weighted percentage averages, verified the strength of the fuzzy method.\n\nCross cultural usability testing\n\nDEFF Research Database (Denmark)\n\nClemmensen, Torkil; Goyal, Shivam\n\n2005-01-01\n\nIn this paper, we present the results of a pilot study in Denmark of cross cultural effects on Think Aloud usability testing. We provide an overview of previous research on cross cultural usability evaluation with a special focus on the relationship between the evaluator and the test user....... This relation was studied in an experiment with usability testing of a localized clipart application in which eight participants from Denmark and India formed pairs of evaluator-test user. The test users were asked to think aloud and the evaluators' role were to facilitate the test users thinking aloud...... and hereby identify usability problems with the clipart application. Data on the evaluators' and test users' behaviour were recorded and analyzed by coding and summarizing statistics on these behavioural events. The results show that Think Aloud Usability Test of a localized application is most effectively...\n\nUsability testing for dummies\n\nCERN Multimedia\n\nCERN. Geneva\n\n2017-01-01\n\nUsability testing seems complicated and time-consuming. Is it though? In fact, it is the best way to understand how real users experience your product. In this interactive session, we will do a live usability test and you will get advice on how to conduct your own usability tests.\n\nTemplates for Cross-Cultural and Culturally Specific Usability Testing\n\nDEFF Research Database (Denmark)\n\nClemmensen, Torkil\n\n2011-01-01\n\nThe cultural diversity of users of technology challenges our methods for usability testing. This article suggests templates for cross-culturally and culturally specific usability testing, based on studies of usability testing in companies in Mumbai, Beijing, and Copenhagen. Study 1 was a cross...... tests. The result was the construction of templates for usability testing. The culturally specific templates were in Mumbai âuser-centered evaluation,â Copenhagen âclient-centered evaluation,â and Beijing âevaluator-centered evaluation.â The findings are compared with related research...\n\nUsability testing a practical guide for librarians\n\nCERN Document Server\n\nBlakiston, Rebecca\n\n2014-01-01\n\nDo you want to improve the usability of your library website, but feel that it is too difficult, time-consuming, or expensive? Usability Testing: A Practical Guide for Librarians will teach you how to make the case for usability testing, define your audience and their goals, select a usability testing method appropriate for your particular context, plan for an in-house usability test, conduct an effective in-house usability test, analyze usability test results, and create and implement a plan for ongoing, systematic usability testing. Step-by-step instructions, along with a myriad of examples,\n\nAn investigation of the efficacy of collaborative virtual reality systems for moderated remote usability testing.\n\nScience.gov (United States)\n\nChalil Madathil, Kapil; Greenstein, Joel S\n\n2017-11-01\n\nCollaborative virtual reality-based systems have integrated high fidelity voice-based communication, immersive audio and screen-sharing tools into virtual environments. Such three-dimensional collaborative virtual environments can mirror the collaboration among usability test participants and facilitators when they are physically collocated, potentially enabling moderated usability tests to be conducted effectively when the facilitator and participant are located in different places. We developed a virtual collaborative three-dimensional remote moderated usability testing laboratory and employed it in a controlled study to evaluate the effectiveness of moderated usability testing in a collaborative virtual reality-based environment with two other moderated usability testing methods: the traditional lab approach and Cisco WebEx, a web-based conferencing and screen sharing approach. Using a mixed methods experimental design, 36 test participants and 12 test facilitators were asked to complete representative tasks on a simulated online shopping website. The dependent variables included the time taken to complete the tasks; the usability defects identified and their severity; and the subjective ratings on the workload index, presence and satisfaction questionnaires. Remote moderated usability testing methodology using a collaborative virtual reality system performed similarly in terms of the total number of defects identified, the number of high severity defects identified and the time taken to complete the tasks with the other two methodologies. The overall workload experienced by the test participants and facilitators was the least with the traditional lab condition. No significant differences were identified for the workload experienced with the virtual reality and the WebEx conditions. However, test participants experienced greater involvement and a more immersive experience in the virtual environment than in the WebEx condition. The ratings for the virtual\n\nUsability Methods for Ensuring Health Information Technology Safety: Evidence-Based Approaches. Contribution of the IMIA Working Group Health Informatics for Patient Safety.\n\nScience.gov (United States)\n\nBorycki, E; Kushniruk, A; Nohr, C; Takeda, H; Kuwata, S; Carvalho, C; Bainbridge, M; Kannry, J\n\n2013-01-01\n\nIssues related to lack of system usability and potential safety hazards continue to be reported in the health information technology (HIT) literature. Usability engineering methods are increasingly used to ensure improved system usability and they are also beginning to be applied more widely for ensuring the safety of HIT applications. These methods are being used in the design and implementation of many HIT systems. In this paper we describe evidence-based approaches to applying usability engineering methods. A multi-phased approach to ensuring system usability and safety in healthcare is described. Usability inspection methods are first described including the development of evidence-based safety heuristics for HIT. Laboratory-based usability testing is then conducted under artificial conditions to test if a system has any base level usability problems that need to be corrected. Usability problems that are detected are corrected and then a new phase is initiated where the system is tested under more realistic conditions using clinical simulations. This phase may involve testing the system with simulated patients. Finally, an additional phase may be conducted, involving a naturalistic study of system use under real-world clinical conditions. The methods described have been employed in the analysis of the usability and safety of a wide range of HIT applications, including electronic health record systems, decision support systems and consumer health applications. It has been found that at least usability inspection and usability testing should be applied prior to the widespread release of HIT. However, wherever possible, additional layers of testing involving clinical simulations and a naturalistic evaluation will likely detect usability and safety issues that may not otherwise be detected prior to widespread system release. The framework presented in the paper can be applied in order to develop more usable and safer HIT, based on multiple layers of evidence.\n\nMerits of usability testing for PACS selection\n\nNARCIS (Netherlands)\n\nJorritsma, Wiard; Cnossen, Fokie; van Ooijen, Peter M. A.\n\nObjectives: To compare the usability of different Picture Archiving and Communication System (PACS) workstations, determine whether a usability test has added value with respect to the traditional way of comparing PACSs based on functional requirements, and to evaluate the appropriateness of a\n\nUsability Assessment of E-CafÃ© Operational Management Simulation Game\n\nScience.gov (United States)\n\nChang, Chiung-sui; Huang, Ya-Ping\n\n2013-01-01\n\nTo ensure the quality of digital simulation game, we utilized the usability evaluation heuristic in the design and development processes of e-cafÃ© operational management game-based learning material for students. The application of usability evaluations during this study is described. Additionally, participant selection, data collection andâ¦\n\nFrom Usability Testing to Clinical Simulations: Bringing Context into the Design and Evaluation of Usable and Safe Health Information Technologies\n\nDEFF Research Database (Denmark)\n\nKushniruk, Andre; NÃ¸hr, Christian; Jensen, Sanne\n\n2013-01-01\n\nof clinical context into stronger focus. This involves testing of systems with representative users doing representative tasks, in representative settings/environments. Results: Application of methods where realistic clinical scenarios are used to drive the study of users interacting with systems under...... such evaluation can be used to improve both the usability and safety of HIT. In addition, recent work has shown that clinical simulations, in particular those conducted in-situ, can lead to considerable benefits when compared to the costs of running such studies. Conclusion: In order to bring context of use...\n\nUsability testing of ANSWER: a web-based methotrexate decision aid for patients with rheumatoid arthritis.\n\nScience.gov (United States)\n\nLi, Linda C; Adam, Paul M; Townsend, Anne F; Lacaille, Diane; Yousefi, Charlene; Stacey, Dawn; Gromala, Diane; Shaw, Chris D; Tugwell, Peter; Backman, Catherine L\n\n2013-12-01\n\nDecision aids are evidence-based tools designed to inform people of the potential benefit and harm of treatment options, clarify their preferences and provide a shared decision-making structure for discussion at a clinic visit. For patients with rheumatoid arthritis (RA) who are considering methotrexate, we have developed a web-based patient decision aid called the ANSWER (Animated, Self-serve, Web-based Research Tool). This study aimed to: 1) assess the usability of the ANSWER prototype; 2) identify strengths and limitations of the ANSWER from the patient's perspective. The ANSWER prototype consisted of: 1) six animated patient stories and narrated information on the evidence of methotrexate for RA; 2) interactive questionnaires to clarify patients' treatment preferences. Eligible participants for the usability test were patients with RA who had been prescribed methotrexate. They were asked to verbalize their thoughts (i.e., think aloud) while using the ANSWER, and to complete the System Usability Scale (SUS) to assess overall usability (rangeâ=â0-100; higherâ=âmore user friendly). Participants were audiotaped and observed, and field notes were taken. The testing continued until no new modifiable issues were found. We used descriptive statistics to summarize participant characteristics and the SUS scores. Content analysis was used to identified usability issues and navigation problems. 15 patients participated in the usability testing. The majority were aged 50 or over and were university/college graduates (nâ=â8, 53.4%). On average they took 56Â minutes (SDâ=â34.8) to complete the tool. The mean SUS score was 81.2 (SDâ=â13.5). Content analysis of audiotapes and field notes revealed four categories of modifiable usability issues: 1) information delivery (i.e., clarity of the information and presentation style); 2) navigation control (i.e., difficulties in recognizing and using the navigation control buttons); 3) layout (i.e., position of the\n\nUsability Testing Of Web Mapping Portals\n\nDirectory of Open Access Journals (Sweden)\n\nPetr VoldÃ¡n\n\n2011-05-01\n\nFull Text Available This study presents a usability testing as method, which can be used to improve controlling of web map sites. Study refers to the basic principles of this method and describes particular usability tests of mapping sites. In this paper are identified potential usability problems of web sites: Amapy.cz, Google maps and Mapy.cz. The usability testing was focused on problems related with user interfaces, addresses searching and route planning of the map sites.\n\nThe Evaluator Effect in Usability Tests\n\nDEFF Research Database (Denmark)\n\nJacobsen, Niels Ebbe; Hertzum, Morten; John, Bonnie E.\n\n1998-01-01\n\nUsability tests are applied in industry to evaluate systems and in research as a yardstick for other usability evaluation methods. However, one potential threat to the reliability of usability tests has been left unaddressed: the evaluator effect. In this study, four evaluators analyzed four vide...\n\nThe value of usability testing for Internet-based adolescent self-management interventions: \"Managing Hemophilia Online\".\n\nScience.gov (United States)\n\nBreakey, Vicky R; Warias, Ashley V; Ignas, Danial M; White, Meghan; Blanchette, Victor S; Stinson, Jennifer N\n\n2013-10-04\n\nAs adolescents with hemophilia approach adulthood, they are expected to assume responsibility for their disease management. A bilingual (English and French) Internet-based self-management program, \"Teens Taking Charge: Managing Hemophilia Online,\" was developed to support adolescents with hemophilia in this transition. This study explored the usability of the website and resulted in refinement of the prototype. A purposive sample (n=18; age 13-18; mean age 15.5 years) was recruited from two tertiary care centers to assess the usability of the program in English and French. Qualitative observations using a \"think aloud\" usability testing method and semi-structured interviews were conducted in four iterative cycles, with changes to the prototype made as necessary following each cycle. This study was approved by research ethics boards at each site. Teens responded positively to the content and appearance of the website and felt that it was easy to navigate and understand. The multimedia components (videos, animations, quizzes) were felt to enrich the experience. Changes to the presentation of content and the website user-interface were made after the first, second and third cycles of testing in English. Cycle four did not result in any further changes. Overall, teens found the website to be easy to use. Usability testing identified end-user concerns that informed improvements to the program. Usability testing is a crucial step in the development of Internet-based self-management programs to ensure information is delivered in a manner that is accessible and understood by users.\n\nThe effects of protoype medium on usability testing.\n\nScience.gov (United States)\n\nBoothe, Chase; Strawderman, Lesley; Hosea, Ethan\n\n2013-11-01\n\nInconsistencies among testing methods and results in previous research prompted this study that builds upon a systematic usability testing research framework to better understand how interface medium influences users' abilities to detect usability flaws in applications. Interface medium was tested to identify its effects on users' perceptions of usability and abilities to detect usability problems and severe usability problems. Results indicated that medium has no effect on users' abilities to detect usability problems or perceptions of usability. However, results did indicate an interaction between the medium and the tested application in which users were able to identify significantly more usability problems on a higher fidelity medium using a particular application. Results also indicated that as users' perceptions of an application's usability increases, the users are less able to detect usability problems in that application. Usability testing should begin early in the design process, even if low fidelity mediums will be used. Copyright Â© 2013 Elsevier Ltd and The Ergonomics Society. All rights reserved.\n\nVirtual gaming simulation of a mental health assessment: A usability study.\n\nScience.gov (United States)\n\nVerkuyl, Margaret; Romaniuk, Daria; Mastrilli, Paula\n\n2018-05-18\n\nProviding safe and realistic virtual simulations could be an effective way to facilitate the transition from the classroom to clinical practice. As nursing programs begin to include virtual simulations as a learning strategy; it is critical to first assess the technology for ease of use and usefulness. A virtual gaming simulation was developed, and a usability study was conducted to assess its ease of use and usefulness for students and faculty. The Technology Acceptance Model provided the framework for the study, which included expert review and testing by nursing faculty and nursing students. This study highlighted the importance of assessing ease of use and usefulness in a virtual game simulation and provided feedback for the development of an effective virtual gaming simulation. The study participants said the virtual gaming simulation was engaging, realistic and similar to a clinical experience. Participants found the game easy to use and useful. Testing provided the development team with ideas to improve the user interface. The usability methodology provided is a replicable approach to testing virtual experiences before a research study or before implementing virtual experiences into curriculum. Copyright Â© 2018 Elsevier Ltd. All rights reserved.\n\nUsability Testing: A Case Study.\n\nScience.gov (United States)\n\nChisman, Janet; Walbridge, Sharon; Diller, Karen\n\n1999-01-01\n\nDiscusses the development and results of usability testing of Washington State University's Web-based OPAC (online public access catalog); examines how easily users could navigate the catalog and whether they understood what they were seeing; and identifies problems and what action if any was taken. (LRW)\n\nWeb usability evaluation on BloobIS website by using hallway usability testing method and ISO 9241:11\n\nScience.gov (United States)\n\nDwi Susanto, Tony; Ingesti Prasetyo, Anisa; Astuti, Hanim Maria\n\n2018-03-01\n\nAt this moment, the need for web as an information media is highly important. Not only confined in the infotainment area, government, and education, but health as well uses the web media as a medium for providing information effectively. BloobIS is a web based application which integrates blood supply and distribution information at the Blood Transfusion Unit. Knowing how easy information is on BloobIS is marked by how convenient the website is used. Up until now, the BloobIS website is nearing completion but testing has not yet been performed to users and is on the testing and development phase in the Development Life Cycle software. Thus, an evaluation namely the quality control software which focuses on the perspective of BloobIs web usability is required. Hallway Usability Testing and ISO 9241:11 are the methods chosen to measure the BloobIS application usability. The expected outputs of the quality control software focusing on the usability evaluation are being able to rectify the usability deficiencies on the BloobIs web and provide recommendations to develop the web as a basic BloobIS web quality upgraed which sets a goal to amplify the satisfaction of web users based on usability factors in ISO 9241:11.\n\nComparative study of heuristic evaluation and usability testing methods.\n\nScience.gov (United States)\n\nThyvalikakath, Thankam Paul; Monaco, Valerie; Thambuganipalle, Himabindu; Schleyer, Titus\n\n2009-01-01\n\nUsability methods, such as heuristic evaluation, cognitive walk-throughs and user testing, are increasingly used to evaluate and improve the design of clinical software applications. There is still some uncertainty, however, as to how those methods can be used to support the development process and evaluation in the most meaningful manner. In this study, we compared the results of a heuristic evaluation with those of formal user tests in order to determine which usability problems were detected by both methods. We conducted heuristic evaluation and usability testing on four major commercial dental computer-based patient records (CPRs), which together cover 80% of the market for chairside computer systems among general dentists. Both methods yielded strong evidence that the dental CPRs have significant usability problems. An average of 50% of empirically-determined usability problems were identified by the preceding heuristic evaluation. Some statements of heuristic violations were specific enough to precisely identify the actual usability problem that study participants encountered. Other violations were less specific, but still manifested themselves in usability problems and poor task outcomes. In this study, heuristic evaluation identified a significant portion of problems found during usability testing. While we make no assumptions about the generalizability of the results to other domains and software systems, heuristic evaluation may, under certain circumstances, be a useful tool to determine design problems early in the development cycle.\n\nPrivacy Enhancing Keyboard: Design, Implementation, and Usability Testing\n\nDirectory of Open Access Journals (Sweden)\n\nZhen Ling\n\n2017-01-01\n\nFull Text Available To protect users from numerous password inference attacks, we invent a novel context aware privacy enhancing keyboard (PEK for Android touch-based devices. Usually PEK would show a QWERTY keyboard when users input text like an email or a message. Nevertheless, whenever users enter a password in the input box on his or her touch-enabled device, a keyboard will be shown to them with the positions of the characters shuffled at random. PEK has been released on the Google Play since 2014. However, the number of installations has not lived up to our expectation. For the purpose of usable security and privacy, we designed a two-stage usability test and performed two rounds of iterative usability testing in 2016 and 2017 summer with continuous improvements of PEK. The observations from the usability testing are educational: (1 convenience plays a critical role when users select an input method; (2 people think those attacks that PEK prevents are remote from them.\n\nThe impact of usability reports and user test observations on developers understanding of usability data\n\nDEFF Research Database (Denmark)\n\nHÃ¸egh, Rune Thaarup; Nielsen, Christian Monrad; Pedersen, Michael Bach\n\n2006-01-01\n\nof the system. This article presents results from an exploratory study of 2 ways of providing feedback from a usability evaluation: observation of user tests and reading usability reports. A case study and a field experiment were used to explore how observation and usability reports impact developers......' understanding of usability data. The results indicate that observation of user tests facilitated a rich understanding of usability problems and created empathy with the users and their work. The usability report had a strong impact on the developers' understanding of specific usability problems and supported...\n\nThe value of usability testing for Internet-based adolescent self-management interventions: âManaging Hemophilia Onlineâ\n\nScience.gov (United States)\n\n2013-01-01\n\nBackground As adolescents with hemophilia approach adulthood, they are expected to assume responsibility for their disease management. A bilingual (English and French) Internet-based self-management program, âTeens Taking Charge: Managing Hemophilia Online,â was developed to support adolescents with hemophilia in this transition. This study explored the usability of the website and resulted in refinement of the prototype. Methods A purposive sample (n=18; age 13â18; mean age 15.5 years) was recruited from two tertiary care centers to assess the usability of the program in English and French. Qualitative observations using a âthink aloudâ usability testing method and semi-structured interviews were conducted in four iterative cycles, with changes to the prototype made as necessary following each cycle. This study was approved by research ethics boards at each site. Results Teens responded positively to the content and appearance of the website and felt that it was easy to navigate and understand. The multimedia components (videos, animations, quizzes) were felt to enrich the experience. Changes to the presentation of content and the website user-interface were made after the first, second and third cycles of testing in English. Cycle four did not result in any further changes. Conclusions Overall, teens found the website to be easy to use. Usability testing identified end-user concerns that informed improvements to the program. Usability testing is a crucial step in the development of Internet-based self-management programs to ensure information is delivered in a manner that is accessible and understood by users. PMID:24094082\n\nUsability Testing of Two Ambulatory EHR Navigators.\n\nScience.gov (United States)\n\nHultman, Gretchen; Marquard, Jenna; Arsoniadis, Elliot; Mink, Pamela; Rizvi, Rubina; Ramer, Tim; Khairat, Saif; Fickau, Keri; Melton, Genevieve B\n\n2016-01-01\n\nDespite widespread electronic health record (EHR) adoption, poor EHR system usability continues to be a significant barrier to effective system use for end users. One key to addressing usability problems is to employ user testing and user-centered design. To understand if redesigning an EHR-based navigation tool with clinician input improved user performance and satisfaction. A usability evaluation was conducted to compare two versions of a redesigned ambulatory navigator. Participants completed tasks for five patient cases using the navigators, while employing a think-aloud protocol. The tasks were based on Meaningful Use (MU) requirements. The version of navigator did not affect perceived workload, and time to complete tasks was longer in the redesigned navigator. A relatively small portion of navigator content was used to complete the MU-related tasks, though navigation patterns were highly variable across participants for both navigators. Preferences for EHR navigation structures appeared to be individualized. This study demonstrates the importance of EHR usability assessments to evaluate group and individual performance of different interfaces and preferences for each design.\n\nToward more usable electronic voting: testing the usability of a smartphone voting system.\n\nScience.gov (United States)\n\nCampbell, Bryan A; Tossell, Chad C; Byrne, Michael D; Kortum, Philip\n\n2014-08-01\n\nThe goal of this research was to assess the usability of a voting system designed for smart-phones. Smartphones offer remote participation in elections through the use of pervasive technology. Voting on these devices could, among other benefits, increase voter participation while allowing voters to use familiar technology. However, the usability of these systems has not been assessed. A mobile voting system optimized for use on a smartphone was designed and tested against traditional voting platforms for usability. There were no reliable differences between the smartphone-based system and other voting methods in efficiency and perceived usability. More important, though, smartphone owners committed fewer errors on the mobile voting system than on the traditional voting systems. Even with the known limitations of small mobile platforms in both displays and controls, a carefully designed system can provide a usable voting method. Much of the concern about mobile voting is in the area of security; therefore, although these results are promising, security concerns and usability issues arising from mitigating them must be strongly considered. The results of this experiment may help to inform current and future election and public policy officials about the benefits of allowing voters to vote with familiar hardware.\n\nDevelopment and testing of new upper-limb prosthetic devices: research designs for usability testing.\n\nScience.gov (United States)\n\nResnik, Linda\n\n2011-01-01\n\nThe purposes of this article are to describe usability testing and introduce designs and methods of usability testing research as it relates to upper-limb prosthetics. This article defines usability, describes usability research, discusses research approaches to and designs for usability testing, and highlights a variety of methodological considerations, including sampling, sample size requirements, and usability metrics. Usability testing is compared with other types of study designs used in prosthetic research.\n\nA usability test is not an interview\n\nDEFF Research Database (Denmark)\n\nHertzum, Morten\n\n2016-01-01\n\nUsability tests are conducted to gauge usersâ experience with a system, preferably before it is released for real use, and thereby find any problems that prevent users from completing their tasks, slow them down, or otherwise degrade their user experience. Such tests are important to successful...... systems development, yet test procedures vary and the quality of test results is sometimes contested. While there is no single accepted procedure for usability specialists to follow when conducting usability tests, these tests normally involve users who think out loud while using a system and an evaluator...... who observes the usersâ behavior and listens in on their thoughts. This common core of usability tests is illustrated in Figure 1. The possible variations include, for example, whether the users work individually or in pairs, whether the evaluator is in the room with the user or in an adjoining room...\n\nSU-F-T-249: Application of Human Factors Methods: Usability Testing in the Radiation Oncology Environment\n\nInternational Nuclear Information System (INIS)\n\nWarkentin, H; Bubric, K; Giovannetti, H; Graham, G; Clay, C\n\n2016-01-01\n\nPurpose: As a quality improvement measure, we undertook this work to incorporate usability testing into the implementation procedures for new electronic documents and forms used by four affiliated radiation therapy centers. Methods: A human factors specialist provided training in usability testing for a team of medical physicists, radiation therapists, and radiation oncologists from four radiotherapy centers. A usability testing plan was then developed that included controlled scenarios and standardized forms for qualitative and quantitative feedback from participants, including patients. Usability tests were performed by end users using the same hardware and viewing conditions that are found in the clinical environment. A pilot test of a form used during radiotherapy CT simulation was performed in a single department; feedback informed adaptive improvements to the electronic form, hardware requirements, resource accessibility and the usability testing plan. Following refinements to the testing plan, usability testing was performed at three affiliated cancer centers with different vault layouts and hardware. Results: Feedback from the testing resulted in the detection of 6 critical errors (omissions and inability to complete task without assistance), 6 non-critical errors (recoverable), and multiple suggestions for improvement. Usability problems with room layout were detected at one center and problems with hardware were detected at one center. Upon amalgamation and summary of the results, three key recommendations were presented to the documentâs authors for incorporation into the electronic form. Documented inefficiencies and patient safety concerns related to the room layout and hardware were presented to administration along with a request for funding to purchase upgraded hardware and accessories to allow a more efficient workflow within the simulator vault. Conclusion: By including usability testing as part of the process when introducing any new document\n\nSU-F-T-249: Application of Human Factors Methods: Usability Testing in the Radiation Oncology Environment\n\nEnergy Technology Data Exchange (ETDEWEB)\n\nWarkentin, H [Cross Cancer Institute, Edmonton, AB (Canada); Bubric, K [Alberta Health Services, Calgary, AB (Canada); Giovannetti, H [Jack Ady Cancer Centre, Lethbridge, AB (Canada); Graham, G [Tom Baker Cancer Centre, Calgary, AB (Canada); Clay, C [Central Alberta Cancer Centre, Red Deer, AB (Canada)\n\n2016-06-15\n\nPurpose: As a quality improvement measure, we undertook this work to incorporate usability testing into the implementation procedures for new electronic documents and forms used by four affiliated radiation therapy centers. Methods: A human factors specialist provided training in usability testing for a team of medical physicists, radiation therapists, and radiation oncologists from four radiotherapy centers. A usability testing plan was then developed that included controlled scenarios and standardized forms for qualitative and quantitative feedback from participants, including patients. Usability tests were performed by end users using the same hardware and viewing conditions that are found in the clinical environment. A pilot test of a form used during radiotherapy CT simulation was performed in a single department; feedback informed adaptive improvements to the electronic form, hardware requirements, resource accessibility and the usability testing plan. Following refinements to the testing plan, usability testing was performed at three affiliated cancer centers with different vault layouts and hardware. Results: Feedback from the testing resulted in the detection of 6 critical errors (omissions and inability to complete task without assistance), 6 non-critical errors (recoverable), and multiple suggestions for improvement. Usability problems with room layout were detected at one center and problems with hardware were detected at one center. Upon amalgamation and summary of the results, three key recommendations were presented to the documentâs authors for incorporation into the electronic form. Documented inefficiencies and patient safety concerns related to the room layout and hardware were presented to administration along with a request for funding to purchase upgraded hardware and accessories to allow a more efficient workflow within the simulator vault. Conclusion: By including usability testing as part of the process when introducing any new document\n\nQualitative Marketing Research Through Usability Testing\n\nDirectory of Open Access Journals (Sweden)\n\nOrzan Mihai\n\n2008-04-01\n\nFull Text Available Usability is an attribute of any good product, just as its functionality. It refers mainly to the utility of a product for its indented users, as well as to its ease of use. And whilst a correct functionality is critical for the commercial success of any product, its value comes through the human needs that it fulfills, which is determined through various marketing research techniques. In parallel, the IT&C community has developed in the last two decades its own type of research, called usability testing, used mainly to evaluate interface ease of use and all usability problems associated with.software products. This article aims at finding the right place for usability testing and usability professionals in the marketing community, as well as drawing a wider picture, from a marketing research perspective, on one of the most popular topics in IT&C community for the benefit of marketing scholars and professionals.\n\nMethods of Usability Testing in Libraries Web Sites\n\nDirectory of Open Access Journals (Sweden)\n\nEman Fawzy\n\n2006-03-01\n\nFull Text Available A Study about libraries' web sites evaluation, that is the Usability, the study talking about methods of usability testing and define it, and its important in web sites evaluation, then details the methods of usability: questionnaire, core groups, testing experimental model, cards arrangement, and composed evaluation.\n\nWhy Citizen Science Without Usability Testing Will Underperform\n\nScience.gov (United States)\n\nRomano, C.; Gay, P.; Owens, R.; Burlea, G.\n\n2017-12-01\n\nCitizen science projects must undergo usability testing and optimization if they are to meet their stated goals. This presentation will include video of usability tests conducted upon citizen science websites. Usability testing is essential to the success of online interaction, however, citizen science projects have just begun to include this critical activity. Interaction standards in citizen science lag behind those of commercial interests, and published research on this topic is limited. Since online citizen science is by definition, an exchange of information, a clear understanding of how users experience an online project is essential to informed decision-making. Usability testing provides that insight. Usability testing collects data via direct observation of a person while she interacts with a digital product, such as a citizen science website. The test participant verbalizes her thoughts while using the website or application; the moderator follows the participant and captures quantitative measurement of the participant's confidence of success as she advances through the citizen science project. Over 15 years of usability testing, we have observed that users who do not report a consistent sense of progress are likely to abandon a website after as few as three unrewarding interactions. Since citizen science is also a voluntary activity, ensuring seamless interaction for users is mandatory. Usability studies conducted on citizen science websites demonstrate that project teams frequently underestimate a user's need for context and ease of use. Without usability testing, risks to online citizen science projects include high bounce rate (users leave the website without taking any action), abandonment (of the website, tutorials, registration), misunderstanding instructions (causing disorientation and erroneous conclusions), and ultimately, underperforming projects.\n\nUsability testing of interaction components: taking the message exchange as a measure of usability\n\nNARCIS (Netherlands)\n\nBrinkman, W.P.; Haakma, R.; Bouwhuis, D.G.; Jacob, R.J.K.; Limbourg, Q; Vanderdonckt, J.\n\n2004-01-01\n\nComponent-based Software Engineering (CBSE) is concerned with the development of systems from reusable parts (components), and the development and maintenance of these parts. This study addresses the issue of usability testing in a CBSE environment, and specifically automatically measuring the\n\nUsability Testing: Too Early? Too Much Talking? Too Many Problems?\n\nDEFF Research Database (Denmark)\n\nHertzum, Morten\n\n2016-01-01\n\nUsability testing has evolved in response to a search for tests that are cheap, early, easy, and fast. In addition, it accords with a situational definition of usability, such as the one propounded by ISO. By approaching usability from an organizational perspective, this author argues that usabil......Usability testing has evolved in response to a search for tests that are cheap, early, easy, and fast. In addition, it accords with a situational definition of usability, such as the one propounded by ISO. By approaching usability from an organizational perspective, this author argues...... that usability should (also) be evaluated late, that usability professionals should be wary of thinking aloud, and that they should focus more on effects achievement than problem detection....\n\nEthical Concerns in Usability Testing Involving Older Adults\n\nDEFF Research Database (Denmark)\n\nMÃ¸ller, Margrethe Hansen\n\nBased on experience from the research project âUser Manuals for Older Adultsâ, this paper discusses whether there are special ethical concerns with older adults as test persons in a usability test involving the think-aloud method. In this context, older adults are defined as individuals with normal...\n\nUsability testing of Avoiding Diabetes Thru Action Plan Targeting (ADAPT) decision support for integrating care-based counseling of pre-diabetes in an electronic health record.\n\nScience.gov (United States)\n\nChrimes, Dillon; Kitos, Nicole R; Kushniruk, Andre; Mann, Devin M\n\n2014-09-01\n\nUsability testing can be used to evaluate human-computer interaction (HCI) and communication in shared decision making (SDM) for patient-provider behavioral change and behavioral contracting. Traditional evaluations of usability using scripted or mock patient scenarios with think-aloud protocol analysis provide a way to identify HCI issues. In this paper we describe the application of these methods in the evaluation of the Avoiding Diabetes Thru Action Plan Targeting (ADAPT) tool, and test the usability of the tool to support the ADAPT framework for integrated care counseling of pre-diabetes. The think-aloud protocol analysis typically does not provide an assessment of how patient-provider interactions are effected in \"live\" clinical workflow or whether a tool is successful. Therefore, \"Near-live\" clinical simulations involving applied simulation methods were used to compliment the think-aloud results. This complementary usability technique was used to test the end-user HCI and tool performance by more closely mimicking the clinical workflow and capturing interaction sequences along with assessing the functionality of computer module prototypes on clinician workflow. We expected this method to further complement and provide different usability findings as compared to think-aloud analysis. Together, this mixed method evaluation provided comprehensive and realistic feedback for iterative refinement of the ADAPT system prior to implementation. The study employed two phases of testing of a new interactive ADAPT tool that embedded an evidence-based shared goal setting component into primary care workflow for dealing with pre-diabetes counseling within a commercial physician office electronic health record (EHR). Phase I applied usability testing that involved \"think-aloud\" protocol analysis of eight primary care providers interacting with several scripted clinical scenarios. Phase II used \"near-live\" clinical simulations of five providers interacting with standardized\n\nWeb usability testing with a Hispanic medically underserved population.\n\nScience.gov (United States)\n\nMoore, Mary; Bias, Randolph G; Prentice, Katherine; Fletcher, Robin; Vaughn, Terry\n\n2009-04-01\n\nSkilled website developers value usability testing to assure user needs are met. When the target audience differs substantially from the developers, it becomes essential to tailor both design and evaluation methods. In this study, researchers carried out a multifaceted usability evaluation of a website (Healthy Texas) designed for Hispanic audiences with lower computer literacy and lower health literacy. METHODS INCLUDED: (1) heuristic evaluation by a usability engineer, (2) remote end-user testing using WebEx software; and (3) face-to-face testing in a community center where use of the website was likely. Researchers found standard usability testing methods needed to be modified to provide interpreters, increased flexibility for time on task, presence of a trusted intermediary such as a librarian, and accommodation for family members who accompanied participants. Participants offered recommendations for website redesign, including simplified language, engaging and relevant graphics, culturally relevant examples, and clear navigation. User-centered design is especially important when website developers are not representative of the target audience. Failure to conduct appropriate usability testing with a representative audience can substantially reduce use and value of the website. This thorough course of usability testing identified improvements that benefit all users but become crucial when trying to reach an underserved audience.\n\nGetting a technology-based diabetes intervention ready for prime time: a review of usability testing studies.\n\nScience.gov (United States)\n\nLyles, Courtney R; Sarkar, Urmimala; Osborn, Chandra Y\n\n2014-10-01\n\nConsumer health technologies can educate patients about diabetes and support their self-management, yet usability evidence is rarely published even though it determines patient engagement, optimal benefit of any intervention, and an understanding of generalizability. Therefore, we conducted a narrative review of peer-reviewed articles published from 2009 to 2013 that tested the usability of a web- or mobile-delivered system/application designed to educate and support patients with diabetes. Overall, the 23 papers included in our review used mixed (nâ=â11), descriptive quantitative (nâ=â9), and qualitative methods (nâ=â3) to assess usability, such as documenting which features performed as intended and how patients rated their experiences. More sophisticated usability evaluations combined several complementary approaches to elucidate more aspects of functionality. Future work pertaining to the design and evaluation of technology-delivered diabetes education/support interventions should aim to standardize the usability testing processes and publish usability findings to inform interpretation of why an intervention succeeded or failed and for whom.\n\nUsability Testing of a Complex Clinical Decision Support Tool in the Emergency Department: Lessons Learned.\n\nScience.gov (United States)\n\nPress, Anne; McCullagh, Lauren; Khan, Sundas; Schachter, Andy; Pardo, Salvatore; McGinn, Thomas\n\n2015-09-10\n\nAs the electronic health record (EHR) becomes the preferred documentation tool across medical practices, health care organizations are pushing for clinical decision support systems (CDSS) to help bring clinical decision support (CDS) tools to the forefront of patient-physician interactions. A CDSS is integrated into the EHR and allows physicians to easily utilize CDS tools. However, often CDSS are integrated into the EHR without an initial phase of usability testing, resulting in poor adoption rates. Usability testing is important because it evaluates a CDSS by testing it on actual users. This paper outlines the usability phase of a study, which will test the impact of integration of the Wells CDSS for pulmonary embolism (PE) diagnosis into a large urban emergency department, where workflow is often chaotic and high stakes decisions are frequently made. We hypothesize that conducting usability testing prior to integration of the Wells score into an emergency room EHR will result in increased adoption rates by physicians. The objective of the study was to conduct usability testing for the integration of the Wells clinical prediction rule into a tertiary care center's emergency department EHR. We conducted usability testing of a CDS tool in the emergency department EHR. The CDS tool consisted of the Wells rule for PE in the form of a calculator and was triggered off computed tomography (CT) orders or patients' chief complaint. The study was conducted at a tertiary hospital in Queens, New York. There were seven residents that were recruited and participated in two phases of usability testing. The usability testing employed a \"think aloud\" method and \"near-live\" clinical simulation, where care providers interacted with standardized patients enacting a clinical scenario. Both phases were audiotaped, video-taped, and had screen-capture software activated for onscreen recordings. Phase I: Data from the \"think-aloud\" phase of the study showed an overall positive outlook on\n\nOn the assessment of usability testing methods for children\n\nNARCIS (Netherlands)\n\nMarkopoulos, P.; Bekker, M.M.\n\n2003-01-01\n\nThe paper motivates the need to acquire methodological knowledge for involving children as test users in usability testing. It introduces a methodological framework for delineating comparative assessments of usability testing methods for children participants. This framework consists in three\n\nDiscovering the User: A Practical Glance at Usability Testing.\n\nScience.gov (United States)\n\nCampbell, Nicole; Walbridge, Sharon; Chisman, Janet; Diller, Karen R.\n\n1999-01-01\n\nThis interview focuses on usability testing, one method for evaluating whether library Web sites are useful for users. Discussion includes: benefits and drawbacks of usability testing; why it is important; lessons learned from testing; advice for someone interested in doing a study; and recommendations. (AEF)\n\nTowards a Usability and Error \"Safety Net\": A Multi-Phased Multi-Method Approach to Ensuring System Usability and Safety.\n\nScience.gov (United States)\n\nKushniruk, Andre; Senathirajah, Yalini; Borycki, Elizabeth\n\n2017-01-01\n\nThe usability and safety of health information systems have become major issues in the design and implementation of useful healthcare IT. In this paper we describe a multi-phased multi-method approach to integrating usability engineering methods into system testing to ensure both usability and safety of healthcare IT upon widespread deployment. The approach involves usability testing followed by clinical simulation (conducted in-situ) and \"near-live\" recording of user interactions with systems. At key stages in this process, usability problems are identified and rectified forming a usability and technology-induced error \"safety net\" that catches different types of usability and safety problems prior to releasing systems widely in healthcare settings.\n\nUsability Testing for Developing Effective Interactive Multimedia Software: Concepts, Dimensions, and Procedures\n\nDirectory of Open Access Journals (Sweden)\n\nSung Heum Lee\n\n1999-04-01\n\nFull Text Available Usability testing is a dynamic process that can be used throughout the process of developing interactive multimedia software. The purpose of usability testing is to find problems and make recommendations to improve the utility of a product during its design and development. For developing effective interactive multimedia software, dimensions of usability testing were classified into the general categories of: learnability; performance effectiveness; flexibility; error tolerance and system integrity; and user satisfaction. In the process of usability testing, evaluation experts consider the nature of users and tasks, tradeoffs supported by the iterative design paradigm, and real world constraints to effectively evaluate and improve interactive multimedia software. Different methods address different purposes and involve a combination of user and usability testing, however, usability practitioners follow the seven general procedures of usability testing for effective multimedia development. As the knowledge about usability testing grows, evaluation experts will be able to choose more effective and efficient methods and techniques that are appropriate to their goals.\n\nBuilding a Remote Usability Testing Body of Knowledge (RUTBOK)\n\nDEFF Research Database (Denmark)\n\nLizano, Fulvio; Stage, Jan\n\n2012-01-01\n\nRemote Usability Testing (RUT) is an alternative method of usability evaluations. The main aim of RUT is the reduction of costs in the usability evaluation process. Since middle of the 90s RUT, has emerged as a technique which seems as is gaining maturity across the time. In general any discipline...\n\nTowards Evidence Based Usability in Health Informatics?\n\nNARCIS (Netherlands)\n\nMarcilly, Romaric; Peute, Linda W.; Beuscart-Zephir, Marie-Catherine; Jaspers, Monique W.\n\n2015-01-01\n\nIn a Health Information Technology (HIT) regulatory context in which the usability of this technology is more and more a critical issue, there is an increasing need for evidence based usability practice. However, a clear definition of evidence based usability practice and how to achieve it is still\n\nEstimating Sample Size for Usability Testing\n\nDirectory of Open Access Journals (Sweden)\n\nAlex CazaÃ±as\n\n2017-02-01\n\nFull Text Available One strategy used to assure that an interface meets user requirements is to conduct usability testing. When conducting such testing one of the unknowns is sample size. Since extensive testing is costly, minimizing the number of participants can contribute greatly to successful resource management of a project. Even though a significant number of models have been proposed to estimate sample size in usability testing, there is still not consensus on the optimal size. Several studies claim that 3 to 5 users suffice to uncover 80% of problems in a software interface. However, many other studies challenge this assertion. This study analyzed data collected from the user testing of a web application to verify the rule of thumb, commonly known as the âmagic number 5â. The outcomes of the analysis showed that the 5-user rule significantly underestimates the required sample size to achieve reasonable levels of problem detection.\n\nUsability Testing of an Academic Library Web Site: A Case Study.\n\nScience.gov (United States)\n\nBattleson, Brenda; Booth, Austin; Weintrop, Jane\n\n2001-01-01\n\nDiscusses usability testing as a tool for evaluating the effectiveness and ease of use of academic library Web sites; considers human-computer interaction; reviews major usability principles; and explores the application of formal usability testing to an existing site at the University at Buffalo (NY) libraries. (Author/LRW)\n\nSociotechnical Human Factors Involved in Remote Online Usability Testing of Two eHealth Interventions.\n\nScience.gov (United States)\n\nWozney, Lori M; Baxter, Pamela; Fast, Hilary; Cleghorn, Laura; Hundert, Amos S; Newton, Amanda S\n\n2016-02-03\n\n. Results highlight important human-computer interactions and human factor qualities that impact usability testing processes. Moderators need an advanced skill and knowledge set to address the social interaction aspects of Web-based usability testing and technical aspects of conferencing software during test sessions. Findings from moderator-focused studies can inform the design of remote testing platforms and real-time usability evaluation processes that place less cognitive burden on moderators and test users.\n\nUsability Testing in a Library Web Site Redesign Project.\n\nScience.gov (United States)\n\nMcMullen, Susan\n\n2001-01-01\n\nDiscusses the need for an intuitive library information gateway to meet users' information needs and describes the process involved in redesigning a library Web site based on experiences at Roger Williams University. Explains usability testing methods that were used to discover how users were interacting with the Web site interface. (Author/LRW)\n\nA Comparison of What Is Part of Usability Testing in Three Countries\n\nScience.gov (United States)\n\nClemmensen, Torkil\n\nThe cultural diversity of users of technology challenges our methods for usability evaluation. In this paper we report and compare three ethnographic interview studies of what is a part of a standard (typical) usability test in a company in Mumbai, Beijing and Copenhagen. At each of these three locations, we use structural and contrast questions do a taxonomic and paradigm analysis of a how a company performs a usability test. We find similar parts across the three locations. We also find different results for each location. In Mumbai, most parts of the usability test are not related to the interactive application that is tested, but to differences in user characteristics, test preparation, method, and location. In Copenhagen, considerations about the client's needs are part of a usability test. In Beijing, the only varying factor is the communication pattern and relation to the user. These results are then contrasted in a cross cultural matrix to identify cultural themes that can help interpret results from existing laboratory research in usability test methods.\n\n\"Think aloud\" and \"Near live\" usability testing of two complex clinical decision support tools.\n\nScience.gov (United States)\n\nRichardson, Safiya; Mishuris, Rebecca; O'Connell, Alexander; Feldstein, David; Hess, Rachel; Smith, Paul; McCullagh, Lauren; McGinn, Thomas; Mann, Devin\n\n2017-10-01\n\nLow provider adoption continues to be a significant barrier to realizing the potential of clinical decision support. \"Think Aloud\" and \"Near Live\" usability testing were conducted on two clinical decision support tools. Each was composed of an alert, a clinical prediction rule which estimated risk of either group A Streptococcus pharyngitis or pneumonia and an automatic order set based on risk. The objective of this study was to further understanding of the facilitators of usability and to evaluate the types of additional information gained from proceeding to \"Near Live\" testing after completing \"Think Aloud\". This was a qualitative observational study conducted at a large academic health care system with 12 primary care providers. During \"Think Aloud\" testing, participants were provided with written clinical scenarios and asked to verbalize their thought process while interacting with the tool. During \"Near Live\" testing participants interacted with a mock patient. Morae usability software was used to record full screen capture and audio during every session. Participant comments were placed into coding categories and analyzed for generalizable themes. Themes were compared across usability methods. \"Think Aloud\" and \"Near Live\" usability testing generated similar themes under the coding categories visibility, workflow, content, understand-ability and navigation. However, they generated significantly different themes under the coding categories usability, practical usefulness and medical usefulness. During both types of testing participants found the tool easier to use when important text was distinct in its appearance, alerts were passive and appropriately timed, content was up to date, language was clear and simple, and each component of the tool included obvious indicators of next steps. Participant comments reflected higher expectations for usability and usefulness during \"Near Live\" testing. For example, visit aids, such as automatically generated order sets\n\nLibrary Website Usability Test Project\n\nKAUST Repository\n\nRamli, Rindra M.; Bukhari, Duaa\n\n2013-01-01\n\nThis usability testing project was conducted to elicit an understanding of our community use of the library website. The researchers wanted to know how our users are interacting with the library website and the ease of obtaining relevant information from the website. The methodology deployed was computer user testing where participants are made to answer several questions and executing the actions on the library website. Their actions are recorded via Techsmith Camtasia software for later analysis by the researchers.\n\nLibrary Website Usability Test Project\n\nKAUST Repository\n\nRamli, Rindra M.\n\n2013-06-01\n\nThis usability testing project was conducted to elicit an understanding of our community use of the library website. The researchers wanted to know how our users are interacting with the library website and the ease of obtaining relevant information from the website. The methodology deployed was computer user testing where participants are made to answer several questions and executing the actions on the library website. Their actions are recorded via Techsmith Camtasia software for later analysis by the researchers.\n\nNew Options for Usability Testing Projects in Business Communication Courses\n\nScience.gov (United States)\n\nJameson, Daphne A.\n\n2013-01-01\n\nThe increasing availability of recording technologies makes it easier to include usability testing projects in business communication courses. Usability testing is a method of discovering whether people can navigate, read, and understand a print or electronic communication well enough to achieve a particular purpose in a reasonable time frame.â¦\n\nA comparison of usability methods for testing interactive health technologies: methodological aspects and empirical evidence.\n\nScience.gov (United States)\n\nJaspers, Monique W M\n\n2009-05-01\n\nUsability evaluation is now widely recognized as critical to the success of interactive health care applications. However, the broad range of usability inspection and testing methods available may make it difficult to decide on a usability assessment plan. To guide novices in the human-computer interaction field, we provide an overview of the methodological and empirical research available on the three usability inspection and testing methods most often used. We describe two 'expert-based' and one 'user-based' usability method: (1) the heuristic evaluation, (2) the cognitive walkthrough, and (3) the think aloud. All three usability evaluation methods are applied in laboratory settings. Heuristic evaluation is a relatively efficient usability evaluation method with a high benefit-cost ratio, but requires high skills and usability experience of the evaluators to produce reliable results. The cognitive walkthrough is a more structured approach than the heuristic evaluation with a stronger focus on the learnability of a computer application. Major drawbacks of the cognitive walkthrough are the required level of detail of task and user background descriptions for an adequate application of the latest version of the technique. The think aloud is a very direct method to gain deep insight in the problems end users encounter in interaction with a system but data analyses is extensive and requires a high level of expertise both in the cognitive ergonomics and in computer system application domain. Each of the three usability evaluation methods has shown its usefulness, has its own advantages and disadvantages; no single method has revealed any significant results indicating that it is singularly effective in all circumstances. A combination of different techniques that compliment one another should preferably be used as their collective application will be more powerful than applied in isolation. Innovative mobile and automated solutions to support end-user testing have\n\nAssessment of diet and physical activity of brazilian schoolchildren: usability testing of a web-based questionnaire.\n\nScience.gov (United States)\n\nda Costa, Filipe Ferreira; Schmoelz, Camilie Pacheco; Davies, Vanessa Fernandes; Di Pietro, PatrÃ­cia Faria; Kupek, Emil; de Assis, Maria Alice Altenburg\n\n2013-08-19\n\nInformation and communication technology (ICT) has been used with increasing frequency for the assessment of diet and physical activity in health surveys. A number of Web-based questionnaires have been developed for children and adolescents. However, their usability characteristics have scarcely been reported, despite their potential importance for improving the feasibility and validity of ICT-based methods. The objective of this study was to describe the usability evaluation of the Consumo Alimentar e Atividade FÃ­sica de Escolares (CAAFE) questionnaire (Food Consumption and Physical Activity Questionnaire for schoolchildren), a new Web-based survey tool for the self-assessment of diet and physical activity by schoolchildren. A total of 114 schoolchildren aged 6 to 12 years took part in questionnaire usability testing carried out in computer classrooms at five elementary schools in the city of Florianopolis, Brazil. Schoolchildren used a personal computer (PC) equipped with software for recording what is on the computer screen and the children's speech during usability testing. Quantitative and qualitative analyses took into account objective usability metrics such as error counts and time to complete a task. Data on the main difficulties in accomplishing the task and the level of satisfaction expressed by the children were assessed by the observers using a standardized form and interviews with the children. Descriptive statistics and content analysis were used to summarize both the quantitative and the qualitative aspects of the data obtained. The mean time for completing the questionnaire was 13.7 minutes (SD 3.68). Compared to the children in 2nd or 3rd grades, those in 4th or 5th grades spent less time completing the questionnaire (median 12.4 vs 13.3 minutes, P=.022), asked for help less frequently (median 0 vs 1.0 count, P=.005), had a lower error count (median 2.0 vs 8.0 count, Pperformance score (median 73.0 vs 68.0, P=.005). Children with a PC at home\n\nUsability Testing of a National Substance Use Screening Tool Embedded in Electronic Health Records.\n\nScience.gov (United States)\n\nPress, Anne; DeStio, Catherine; McCullagh, Lauren; Kapoor, Sandeep; Morley, Jeanne; Conigliaro, Joseph\n\n2016-07-08\n\nScreening, brief intervention, and referral to treatment (SBIRT) is currently being implemented into health systems nationally via paper and electronic methods. The purpose of this study was to evaluate the integration of an electronic SBIRT tool into an existing paper-based SBIRT clinical workflow in a patient-centered medical home. Usability testing was conducted in an academic ambulatory clinic. Two rounds of usability testing were done with medical office assistants (MOAs) using a paper and electronic version of the SBIRT tool, with two and four participants, respectively. Qualitative and quantitative data was analyzed to determine the impact of both tools on clinical workflow. A second round of usability testing was done with the revised electronic version and compared with the first version. Personal workflow barriers cited in the first round of testing were that the electronic health record (EHR) tool was disruptive to patient's visits. In Round 2 of testing, MOAs reported favoring the electronic version due to improved layout and the inclusion of an alert system embedded in the EHR. For example, using the system usability scale (SUS), MOAs reported a grade \"1\" for the statement, \"I would like to use this system frequently\" during the first round of testing but a \"5\" during the second round of analysis. The importance of testing usability of various mediums of tools used in health care screening is highlighted by the findings of this study. In the first round of testing, the electronic tool was reported as less user friendly, being difficult to navigate, and time consuming. Many issues faced in the first generation of the tool were improved in the second generation after usability was evaluated. This study demonstrates how usability testing of an electronic SBRIT tool can help to identify challenges that can impact clinical workflow. However, a limitation of this study was the small sample size of MOAs that participated. The results may have been biased to\n\nA Facebook-Based Obesity Prevention Program for Korean American Adolescents: Usability Evaluation.\n\nScience.gov (United States)\n\nPark, Bu Kyung; Nahm, Eun-Shim; Rogers, Valerie E; Choi, Mona; Friedmann, Erika; Wilson, Marisa; Koru, Gunes\n\nAdolescent obesity is one of the most serious global public health challenges. Social networking sites are currently popular among adolescents. Therefore, the obesity prevention program for Korean American adolescents was developed on the most popular social networking site, Facebook. The purpose of this study was to evaluate the usability of a culturally tailored Facebook-based obesity prevention program for Korean American adolescents (Healthy Teens). An explorative descriptive design of usability testing was used. Usability testing employing one-on-one observation, the think-aloud method, audio taping, screen activity capture, and surveys was performed. Twenty participants were recruited from two Korean language schools (mean age, 15.40Â Â±Â 1.50Â years). Recruitment and user testing was performed between February and April 2014. Content analysis, using the inductive coding approach, was performed by three coders to analyze transcriptions. Descriptive statistics were used to analyze quantitative data including demographic characteristics, perceived usability, eHealth literacy, and health behaviors. Testing revealed several usability issues in content, appearance, and navigation. Participants' comments regarding content were positive. Although the Facebook platform provided limited flexibility with respect to building the site, participants described the program's appearance as appropriate. Most participants did not experience difficulty in navigating the program. Our preliminary findings indicated that participants perceived the Healthy Teens program as usable and useful. This program could be used as a robust platform for the delivery of health education to adolescents. Further research is required to assess the effects of Facebook-based programs on adolescent obesity prevention. Copyright Â© 2016 National Association of Pediatric Nurse Practitioners. Published by Elsevier Inc. All rights reserved.\n\nInvolving the users remotely: an exploratory study using asynchronous usability testing\n\nDirectory of Open Access Journals (Sweden)\n\nBeth Filar Williams\n\n2015-02-01\n\nFull Text Available Open Educational Resources (OER are increasingly used in the higher education landscape as a solution for a variety of copyright, publishing and cost-prohibiting issues. While OERs are becoming more common, reports of usability tests that evaluate how well learners can use them to accomplish their learning tasks have lagged behind. Because both the researchers and the learners in this study use resources and tools remotely, asynchronous usability testing of a prototype OER and MOOC online guide was conducted with an exploratory group of users to determine the guideâs ease of use for two distinct groups of users: Educators and Learners. In this article, we share the background and context of this usability project, suggest best methods for asynchronous remote usability testing, and share challenges and insights of the process and results of the testing\n\nDevelopment and usability testing of a web-based decision support for users and health professionals in psychiatric services.\n\nScience.gov (United States)\n\nGrim, Katarina; Rosenberg, David; Svedberg, Petra; SchÃ¶n, Ulla-Karin\n\n2017-09-01\n\nShared decision making (SMD) related to treatment and rehabilitation is considered a central component in recovery-oriented practice. Although decision aids are regarded as an essential component for successfully implementing SDM, these aids are often lacking within psychiatric services. The aim of this study was to use a participatory design to facilitate the development of a user-generated, web-based decision aid for individuals receiving psychiatric services. The results of this effort as well as the lessons learned during the development and usability processes are reported. The participatory design included 4 iterative cycles of development. Various qualitative methods for data collection were used with potential end users participating as informants in focus group and individual interviews and as usability and pilot testers. Interviewing and testing identified usability problems that then led to refinements and making the subsequent prototypes increasingly user-friendly and relevant. In each phase of the process, feedback from potential end-users provided guidance in developing the formation of the web-based decision aid that strengthens the position of users by integrating access to information regarding alternative supports, interactivity between staff and users, and user preferences as a continual focus in the tool. This web-based decision aid has the potential to strengthen service users' experience of self-efficacy and control as well as provide staff access to user knowledge and preferences. Studies employing participatory models focusing on usability have potential to significantly contribute to the development and implementation of tools that reflect user perspectives. (PsycINFO Database Record (c) 2017 APA, all rights reserved).\n\nBrief Report: Web-based Management of Adolescent Chronic Pain: Development and Usability Testing of an Online Family Cognitive Behavioral Therapy Program\n\nScience.gov (United States)\n\nPalermo, Tonya M.\n\n2009-01-01\n\nObjectivesâThis study evaluates the usability and feasibility of a Web-based intervention (Web-MAP) to deliver cognitive behavioral therapy (CBT) to adolescents with chronic pain and their parents.âMethodsâThe Web site was evaluated in two stages. In stage one, recovered adolescents and parents (n = 5 dyads), who had completed office-based CBT through a pediatric pain management clinic, completed ratings of Web site content, usability, appearance, and theme. In stage two, treatment-seeking adolescents and their parents (n = 6 dyads) completed the full-length Web program. Program usage data were obtained to assess interaction with the Web site. ResultsâParticipants rated moderate to strong acceptability of the program. Usage data indicated that participants interacted with the site and used communication features.âConclusionsâFeedback from usability testing provided important information in the process of designing a feasible Web-based treatment for adolescents with chronic pain for use in a randomized controlled trial. PMID:18669578\n\nUsability testing of a monitoring and feedback tool to stimulate physical activity\n\nDirectory of Open Access Journals (Sweden)\n\nvan der Weegen S\n\n2014-03-01\n\n, implemented after connectivity problems in the pilot test. The mean score on the PSSUQ for the website improved from 5.6 (standard deviation [SD] 1.3 to 6.5 (SD 0.5, and for the app from 5.4 (SD 1.5 to 6.2 (SD 1.1. Satisfaction in the pilot was not very high according to the SUMI. Discussion: The use of laboratory versus real-life tests and expert-based versus user-based tests revealed a wide range of usability issues. The usability of the It's LiFe! tool improved considerably during the study. Keywords: accelerometry, chronic obstructive pulmonary disease, diabetes mellitus type 2, heuristic evaluation, telemonitoring, thinking aloud\n\nDevelopment of a reliable simulation-based test for diagnostic abdominal ultrasound with a pass/fail standard usable for mastery learning\n\nDEFF Research Database (Denmark)\n\nÃstergaard, Mia L; Nielsen, Kristina R; Albrecht-Beste, Elisabeth\n\n2018-01-01\n\ntraining can benefit from competency-based education based on reliable tests. â¢ This simulation-based test can differentiate between competency levels of ultrasound examiners. â¢ This test is suitable for competency-based education, e.g. mastery learning. â¢ We provide a pass/fail standard without false...... from The European Federation of Societies for Ultrasound in Medicine and Biology. Four groups of experience levels were constructed: Novices (medical students), trainees (first-year radiology residents), intermediates (third- to fourth-year radiology residents) and advanced (physicians with ultrasound...\n\nInvestigation into the usability of MSDL in South African C2 tactical simulations\n\nCSIR Research Space (South Africa)\n\nLe Roux, WH\n\n2007-06-01\n\nFull Text Available The applicability and usability of the Military Simulation Definition Language (MSDL) is investigated in context of the South African air Defence simulation community. A constructive, tactical command and control simulation environment used...\n\nComparison of CTA and Textual Feedback in Usability Testing for Malaysian Users\n\nDEFF Research Database (Denmark)\n\nSivaji, Ashok; Clemmensen, Torkil; Nielsen, SÃ¸ren Feodor\n\nUsability moderators found that the concurrent think-aloud (CTA) method has some cultural limitation that impacts usability testing with Malaysian users. This gives rise to proposing a new method called textual feedback. The research question is to determine whether there are any differences...... in usability defects from the concurrent think-aloud (CTA) method (Condition 2) and textual feedback method (Condition 1) within the same group of Malaysian users. A pair-wise t-test was used, whereby users were subjected to performing usability task using both methods. Results reveal that we can reject...\n\nResearch on the Application of GSR and ECG in the Usability Testing of an Aggregation Reading App\n\nDirectory of Open Access Journals (Sweden)\n\nSha Liu\n\n2016-09-01\n\nFull Text Available Usability testing is a very important step in improving App design and development. The traditional usability testing methods are based on users'expressions and behaviors, which hardly show users' emotional experience and cognitive load in real time. The introduction of an electrophysiological technique can make up for the deficiency of the traditional usability testing methods. In this study, a usability testing was carried out with the old and the new version of an App software. The behavior and the subjective evaluation of the participants were recorded, and their GSR and ECG signals were collected. Then, 14 physiological characteristics, such as GSR-Mean, LF, HF, LF/HF, etc., were extracted from the GSR and ECG signals. These characteristics were analyzed, and a significance test of difference of the two versions was made. This research indicated that there is a certain application value of GSR and HRV in usability testing and evaluation of an App product. But the meanings of the physiological characteristics must be explained in combination with the behavior and subjective evaluation of users. The result can prove that physiological characteristics have obvious advantages in real-time monitoring users' emotional changes, which can be helpful to find the usability problems of the product.\n\nHarnessing scientific literature reports for pharmacovigilance. Prototype software analytical tool development and usability testing.\n\nScience.gov (United States)\n\nSorbello, Alfred; Ripple, Anna; Tonning, Joseph; Munoz, Monica; Hasan, Rashedul; Ly, Thomas; Francis, Henry; Bodenreider, Olivier\n\n2017-03-22\n\nWe seek to develop a prototype software analytical tool to augment FDA regulatory reviewers' capacity to harness scientific literature reports in PubMed/MEDLINE for pharmacovigilance and adverse drug event (ADE) safety signal detection. We also aim to gather feedback through usability testing to assess design, performance, and user satisfaction with the tool. A prototype, open source, web-based, software analytical tool generated statistical disproportionality data mining signal scores and dynamic visual analytics for ADE safety signal detection and management. We leveraged Medical Subject Heading (MeSH) indexing terms assigned to published citations in PubMed/MEDLINE to generate candidate drug-adverse event pairs for quantitative data mining. Six FDA regulatory reviewers participated in usability testing by employing the tool as part of their ongoing real-life pharmacovigilance activities to provide subjective feedback on its practical impact, added value, and fitness for use. All usability test participants cited the tool's ease of learning, ease of use, and generation of quantitative ADE safety signals, some of which corresponded to known established adverse drug reactions. Potential concerns included the comparability of the tool's automated literature search relative to a manual 'all fields' PubMed search, missing drugs and adverse event terms, interpretation of signal scores, and integration with existing computer-based analytical tools. Usability testing demonstrated that this novel tool can automate the detection of ADE safety signals from published literature reports. Various mitigation strategies are described to foster improvements in design, productivity, and end user satisfaction.\n\nComputer-facilitated rapid HIV testing in emergency care settings: provider and patient usability and acceptability.\n\nScience.gov (United States)\n\nSpielberg, Freya; Kurth, Ann E; Severynen, Anneleen; Hsieh, Yu-Hsiang; Moring-Parris, Daniel; Mackenzie, Sara; Rothman, Richard\n\n2011-06-01\n\nProviders in emergency care settings (ECSs) often face barriers to expanded HIV testing. We undertook formative research to understand the potential utility of a computer tool, \"CARE,\" to facilitate rapid HIV testing in ECSs. Computer tool usability and acceptability were assessed among 35 adult patients, and provider focus groups were held, in two ECSs in Washington State and Maryland. The computer tool was usable by patients of varying computer literacy. Patients appreciated the tool's privacy and lack of judgment and their ability to reflect on HIV risks and create risk reduction plans. Staff voiced concerns regarding ECS-based HIV testing generally, including resources for follow-up of newly diagnosed people. Computer-delivered HIV testing support was acceptable and usable among low-literacy populations in two ECSs. Such tools may help circumvent some practical barriers associated with routine HIV testing in busy settings though linkages to care will still be needed.\n\nA framework for evaluating electronic health record vendor user-centered design and usability testing processes.\n\nScience.gov (United States)\n\nRatwani, Raj M; Zachary Hettinger, A; Kosydar, Allison; Fairbanks, Rollin J; Hodgkins, Michael L\n\n2017-04-01\n\nCurrently, there are few resources for electronic health record (EHR) purchasers and end users to understand the usability processes employed by EHR vendors during product design and development. We developed a framework, based on human factors literature and industry standards, to systematically evaluate the user-centered design processes and usability testing methods used by EHR vendors. We reviewed current usability certification requirements and the human factors literature to develop a 15-point framework for evaluating EHR products. The framework is based on 3 dimensions: user-centered design process, summative testing methodology, and summative testing results. Two vendor usability reports were retrieved from the Office of the National Coordinator's Certified Health IT Product List and were evaluated using the framework. One vendor scored low on the framework (5 pts) while the other vendor scored high on the framework (15 pts). The 2 scored vendor reports demonstrate the framework's ability to discriminate between the variabilities in vendor processes and to determine which vendors are meeting best practices. The framework provides a method to more easily comprehend EHR vendors' usability processes and serves to highlight where EHR vendors may be falling short in terms of best practices. The framework provides a greater level of transparency for both purchasers and end users of EHRs. The framework highlights the need for clearer certification requirements and suggests that the authorized certification bodies that examine vendor usability reports may need to be provided with clearer guidance. Â© The Author 2016. Published by Oxford University Press on behalf of the American Medical Informatics Association. All rights reserved. For Permissions, please email: journals.permissions@oup.com\n\nUsability testing for the rest of us: the application of discount usability principles in the development of an online communications assessment application.\n\nScience.gov (United States)\n\nBrock, Douglas; Kim, Sara; Palmer, Odawni; Gallagher, Thomas; Holmboe, Eric\n\n2013-01-01\n\nUsability evaluation provides developers and educators with the means to understand user needs, improve overall product utility, and increase user satisfaction. The application of \"discount usability\" principles developed to make usability testing more practical and useful may improve user experience at minimal cost and require little existing expertise to conduct. We describe an application of discount usability to a high-fidelity online communications assessment application developed by the University of Washington for the American Board of Internal Medicine. Eight internal medicine physicians completed a discount usability test. Sessions were recorded and the videos analyzed for significant usability concerns. Concerns were identified, summarized, discussed, and prioritized by the authors in collaboration with the software developers before implementing any changes to the interface. Thirty-eight significant usability issues were detected and four technical problems were identified. Each issue was responded to through modification of the software, by providing additional instruction, or delayed for a later version to be developed. Discount usability can be easily implemented in academic developmental activities. Our study resulted in the discovery and remediation of significant user problems, in addition to giving important insight into the novel methods built into the application.\n\nUserTesting.com: A Tool for Usability Testing of Online Resources\n\nScience.gov (United States)\n\nKoundinya, Vikram; Klink, Jenna; Widhalm, Melissa\n\n2017-01-01\n\nExtension educators are increasingly using online resources in their program design and delivery. Usability testing is essential for ensuring that these resources are relevant and useful to learners. On the basis of our experiences with iteratively developing products using a testing service called UserTesting, we promote the use of fee-basedâ¦\n\nUsability Evaluation of a Web-Based Learning System\n\nScience.gov (United States)\n\nNguyen, Thao\n\n2012-01-01\n\nThe paper proposes a contingent, learner-centred usability evaluation method and a prototype tool of such systems. This is a new usability evaluation method for web-based learning systems using a set of empirically-supported usability factors and can be done effectively with limited resources. During the evaluation process, the method allows forâ¦\n\nUsability Testing for Serious Games: Making Informed Design Decisions with User Data\n\nDirectory of Open Access Journals (Sweden)\n\nPablo Moreno-Ger\n\n2012-01-01\n\nFull Text Available Usability testing is a key step in the successful design of new technologies and tools, ensuring that heterogeneous populations will be able to interact easily with innovative applications. While usability testing methods of productivity tools (e.g., text editors, spreadsheets, or management tools are varied, widely available, and valuable, analyzing the usability of games, especially educational âseriousâ games, presents unique usability challenges. Because games are fundamentally different than general productivity tools, âtraditionalâ usability instruments valid for productivity applications may fall short when used for serious games. In this work we present a methodology especially designed to facilitate usability testing for serious games, taking into account the specific needs of such applications and resulting in a systematically produced list of suggested improvements from large amounts of recorded gameplay data. This methodology was applied to a case study for a medical educational game, MasterMed, intended to improve patientsâ medication knowledge. We present the results from this methodology applied to MasterMed and a summary of the central lessons learned that are likely useful for researchers who aim to tune and improve their own serious games before releasing them for the general public.\n\nSimulation-based Testing of Control Software\n\nEnergy Technology Data Exchange (ETDEWEB)\n\nOzmen, Ozgur [Oak Ridge National Lab. (ORNL), Oak Ridge, TN (United States); Nutaro, James J. [Oak Ridge National Lab. (ORNL), Oak Ridge, TN (United States); Sanyal, Jibonananda [Oak Ridge National Lab. (ORNL), Oak Ridge, TN (United States); Olama, Mohammed M. [Oak Ridge National Lab. (ORNL), Oak Ridge, TN (United States)\n\n2017-02-10\n\nIt is impossible to adequately test complex software by examining its operation in a physical prototype of the system monitored. Adequate test coverage can require millions of test cases, and the cost of equipment prototypes combined with the real-time constraints of testing with them makes it infeasible to sample more than a small number of these tests. Model based testing seeks to avoid this problem by allowing for large numbers of relatively inexpensive virtual prototypes that operate in simulation time at a speed limited only by the available computing resources. In this report, we describe how a computer system emulator can be used as part of a model based testing environment; specifically, we show that a complete software stack including operating system and application software - can be deployed within a simulated environment, and that these simulations can proceed as fast as possible. To illustrate this approach to model based testing, we describe how it is being used to test several building control systems that act to coordinate air conditioning l"
    }
}
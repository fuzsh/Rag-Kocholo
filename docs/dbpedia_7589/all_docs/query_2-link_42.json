{
    "id": "dbpedia_7589_2",
    "rank": 42,
    "data": {
        "url": "https://dokumen.pub/download/justification-logic-reasoning-with-reasons-1108661106-9781108661102.html",
        "read_more_link": "",
        "language": "en",
        "title": "Justification Logic: Reasoning with Reasons 1108661106, 9781108661102",
        "top_image": "https://dokumen.pub/img/justification-logic-reasoning-with-reasons-1108661106-9781108661102.jpg",
        "meta_img": "https://dokumen.pub/img/justification-logic-reasoning-with-reasons-1108661106-9781108661102.jpg",
        "images": [
            "https://dokumen.pub/dokumenpub/assets/img/dokumenpub_logo.png",
            "https://dokumen.pub/img/200x200/justification-logic-reasoning-with-reasons-cambridge-tracts-in-mathematics-1nbsped-1108424910-9781108424912.jpg",
            "https://dokumen.pub/img/200x200/reasons-justification-and-defeat-2021931162-9780198847205-9780192586490.jpg",
            "https://dokumen.pub/img/200x200/reasoning-and-the-logic-of-things-0674749669-0674749677.jpg",
            "https://dokumen.pub/img/200x200/logic-techniques-of-formal-reasoning-0-15-551180-7-9780155511804.jpg",
            "https://dokumen.pub/img/200x200/reasoning-with-law-9781472562463-9781841130705.jpg",
            "https://dokumen.pub/img/200x200/dialog-systems-a-perspective-from-language-logic-and-computation-logic-argumentation-amp-reasoning-22-3030614379-9783030614379.jpg",
            "https://dokumen.pub/img/200x200/computing-with-logic-logic-programming-with-prolog-0805366814-9780805366815.jpg",
            "https://dokumen.pub/img/200x200/thinking-with-women-philosophers-critical-essays-in-practical-contemporary-philosophy-logic-argumentation-amp-reasoning-30-3031126610-9783031126611.jpg",
            "https://dokumen.pub/img/200x200/introduction-to-logic-circuits-amp-logic-design-with-verilog-9783030136055.jpg",
            "https://dokumen.pub/img/200x200/introduction-to-logic-circuits-amp-logic-design-with-vhdl.jpg",
            "https://dokumen.pub/img/200x200/justification-logic-reasoning-with-reasons-1108661106-9781108661102.jpg",
            "https://dokumen.pub/dokumenpub/assets/img/dokumenpub_logo.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "Classical logic is concerned, loosely, with the behaviour of truths. Epistemic logic similarly is about the behaviour of...",
        "meta_lang": "en",
        "meta_favicon": "https://dokumen.pub/dokumenpub/assets/img/apple-icon-57x57.png",
        "meta_site_name": "dokumen.pub",
        "canonical_link": "https://dokumen.pub/justification-logic-reasoning-with-reasons-1108661106-9781108661102.html",
        "text": "Citation preview\n\nJustification Logic Classical logic is concerned, loosely, with the behavior of truths. Epistemic logic similarly is about the behavior of known or believed truths. Justification logic is a theory of reasoning that enables the tracking of evidence for statements and therefore provides a logical framework for the reliability of assertions. This book, the first in the area, is a systematic account of the subject, progressing from modal logic through to the establishment of an arithmetic interpretation of intuitionistic logic. The presentation is mathematically rigorous but in a style that will appeal to readers from a wide variety of areas to which the theory applies. These include mathematical logic, artificial intelligence, computer science, philosophical logic and epistemology, linguistics, and game theory.\n\ns e r g e i a rt e m ov is Distinguished Professor at the City University of New York. He is a specialist in mathematical logic, logic in computer science, control theory, epistemology, and game theory. He is credited with solving long-standing problems in constructive logic that had been left open by G¨odel and Kolmogorov since the 1930s. He has pioneered studies in the logic of proofs and justifications that render a new, evidence-based theory of knowledge and belief. The most recent focus of his interests is epistemic foundations of game theory. m e lv i n f i t t i n g is Professor Emeritus at the City University of New York. He has written or edited a dozen books and has worked in intensional logic, semantics for logic programming, theory of truth, and tableau systems for nonclassical logics. In 2012 he received the Herbrand Award from the Conference on Automated Deduction. He was on the faculty of the City University of New York from 1969 to his retirement in 2013, at Lehman College, and at the Graduate Center, where he was in the Departments of Mathematics, Computer Science, and Philosophy.\n\nCAMBRIDGE TRACTS IN MATHEMATICS\n\nGENERAL EDITORS ´ W. FULTON, F. KIRWAN, B. BOLLOBAS, P. SARNAK, B. SIMON, B. TOTARO A complete list of books in the series can be found at www.cambridge.org/mathematics. Recent titles include the following:\n\n181. 182. 183. 184. 185. 186. 187. 188. 189. 190. 191. 192. 193. 194. 195. 196. 197. 198. 199. 200. 201. 202. 203. 204. 205. 206. 207. 208. 209. 210. 211. 212. 213. 214. 215.\n\nTotally Positive Matrices. By A. Pinkus Nonlinear Markov Processes and Kinetic Equations. By V. N. Kolokoltsov Period Domains over Finite and p-adic Fields. By J.-F. Dat, S. Orlik, and M. Rapoport Algebraic Theories. By J. Ad´amek, J. Rosick´y, and E. M. Vitale Rigidity in Higher Rank Abelian Group Actions I: Introduction and Cocycle Problem. By A. Katok and V. Nit¸ ic˘a Dimensions, Embeddings, and Attractors. By J. C. Robinson Convexity: An Analytic Viewpoint. By B. Simon Modern Approaches to the Invariant Subspace Problem. By I. Chalendar and J. R. Partington Nonlinear Perron–Frobenius Theory. By B. Lemmens and R. Nussbaum Jordan Structures in Geometry and Analysis. By C.-H. Chu Malliavin Calculus for L´evy Processes and Infinite-Dimensional Brownian Motion. By H. Osswald Normal Approximations with Malliavin Calculus. By I. Nourdin and G. Peccati Distribution Modulo One and Diophantine Approximation. By Y. Bugeaud Mathematics of Two-Dimensional Turbulence. By S. Kuksin and A. Shirikyan A Universal Construction for Groups Acting Freely on Real Trees. By I. Chiswell and T. M¨uller The Theory of Hardy’s Z-Function. By A. Ivi´c Induced Representations of Locally Compact Groups. By E. Kaniuth and K. F. Taylor Topics in Critical Point Theory. By K. Perera and M. Schechter Combinatorics of Minuscule Representations. By R. M. Green Singularities of the Minimal Model Program. By J. Koll´ar Coherence in Three-Dimensional Category Theory. By N. Gurski Canonical Ramsey Theory on Polish Spaces. By V. Kanovei, M. Sabok, and J. Zapletal A Primer on the Dirichlet Space. By O. El-Fallah, K. Kellay, J. Mashreghi, and T. Ransford Group Cohomology and Algebraic Cycles. By B. Totaro Ridge Functions. By A. Pinkus Probability on Real Lie Algebras. By U. Franz and N. Privault Auxiliary Polynomials in Number Theory. By D. Masser Representations of Elementary Abelian p-Groups and Vector Bundles. By D. J. Benson Non-homogeneous Random Walks. By M. Menshikov, S. Popov, and A. Wade Fourier Integrals in Classical Analysis (Second Edition). By C. D. Sogge Eigenvalues, Multiplicities and Graphs. By C. R. Johnson and C. M. Saiago Applications of Diophantine Approximation to Integral Points and Transcendence. By P. Corvaja and U. Zannier Variations on a Theme of Borel. By S. Weinberger The Mathieu Groups. By A. A. Ivanov Slender Modules and Rings I: Foundations. By R. Dimitric\n\nJustification Logic Reasoning with Reasons S E R G E I A RT E M OV Graduate Center, City University of New York M E LV I N F I T T I N G Graduate Center, City University of New York\n\nUniversity Printing House, Cambridge CB2 8BS, United Kingdom One Liberty Plaza, 20th Floor, New York, NY 10006, USA 477 Williamstown Road, Port Melbourne, VIC 3207, Australia 314–321, 3rd Floor, Plot 3, Splendor Forum, Jasola District Centre, New Delhi – 110025, India 79 Anson Road, #06–04/06, Singapore 079906 Cambridge University Press is part of the University of Cambridge. It furthers the University’s mission by disseminating knowledge in the pursuit of education, learning, and research at the highest international levels of excellence. www.cambridge.org Information on this title: www.cambridge.org/9781108424912 DOI: 10.1017/9781108348034 © Sergei Artemov and Melvin Fitting 2019 This publication is in copyright. Subject to statutory exception and to the provisions of relevant collective licensing agreements, no reproduction of any part may take place without the written permission of Cambridge University Press. First published 2019 Printed and bound in Great Britain by Clays Ltd, Elcograf S.p.A. A catalogue record for this publication is available from the British Library. Library of Congress Cataloging-in-Publication Data Names: Artemov, S. N., author. | Fitting, Melvin, 1942- author. Title: Justification logic : reasoning with reasons / Sergei Artemov (Graduate Center, City University of New York), Melvin Fitting (Graduate Center, City University of New York). Description: Cambridge ; New York, NY : Cambridge University Press, 2019. | Series: Cambridge tracts in mathematics ; 216 | Includes bibliographical references and index. Identifiers: LCCN 2018058431 | ISBN 9781108424912 (hardback : alk. paper) Subjects: LCSH: Logic, Symbolic and mathematical. | Inquiry (Theory of knowledge) | Science–Theory reduction. | Reasoning. Classification: LCC QA9 .A78 2019 | DDC 511.3–dc23 LC record available at https://lccn.loc.gov/2018058431 ISBN 978-1-108-42491-2 Hardback Cambridge University Press has no responsibility for the persistence or accuracy of URLs for external or third-party internet websites referred to in this publication and does not guarantee that any content on such websites is, or will remain, accurate or appropriate.\n\nTo our wives, Lena and Roma.\n\nContents\n\nIntroduction 1 What Is This Book About? 2 What Is Not in This Book?\n\npage x xii xvii\n\n1\n\nWhy Justification Logic? 1.1 Epistemic Tradition 1.2 Mathematical Logic Tradition 1.3 Hyperintensionality 1.4 Awareness 1.5 Paraconsistency\n\n1 1 4 8 9 10\n\n2\n\nThe Basics of Justification Logic 2.1 Modal Logics 2.2 Beginning Justification Logics 2.3 J0 , the Simplest Justification Logic 2.4 Justification Logics in General 2.5 Fundamental Properties of Justification Logics 2.6 The First Justification Logics 2.7 A Handful of Less Common Justification Logics\n\n11 11 12 14 15 20 23 27\n\n3\n\nThe Ontology of Justifications 3.1 Generic Logical Semantics of Justifications 3.2 Models for J0 and J 3.3 Basic Models for Positive and Negative Introspection 3.4 Adding Factivity: Mkrtychev Models 3.5 Basic and Mkrtychev Models for the Logic of Proofs LP 3.6 The Inevitability of Possible Worlds: Modular Models 3.7 Connecting Justifications, Belief, and Knowledge 3.8 History and Commentary\n\n31 31 36 38 39 42 42 45 46\n\nvii\n\nviii\n\nContents\n\n4\n\nFitting Models 4.1 Modal Possible World Semantics 4.2 Fitting Models 4.3 Soundness Examples 4.4 Canonical Models and Completeness 4.5 Completeness Examples 4.6 Formulating Justification Logics\n\n48 48 49 52 60 65 72\n\n5\n\nSequents and Tableaus 5.1 Background 5.2 Classical Sequents 5.3 Sequents for S4 5.4 Sequent Soundness, Completeness, and More 5.5 Classical Semantic Tableaus 5.6 Modal Tableaus for K 5.7 Other Modal Tableau Systems 5.8 Tableaus and Annotated Formulas 5.9 Changing the Tableau Representation\n\n75 75 76 79 81 84 90 91 93 95\n\n6\n\nRealization – How It Began 6.1 The Logic LP 6.2 Realization for LP 6.3 Comments\n\n100 100 103 108\n\n7\n\nRealization – Generalized 7.1 What We Do Here 7.2 Counterparts 7.3 Realizations 7.4 Quasi-Realizations 7.5 Substitution 7.6 Quasi-Realizations to Realizations 7.7 Proving Realization Constructively 7.8 Tableau to Quasi-Realization Algorithm 7.9 Tableau to Quasi-Realization Algorithm Correctness 7.10 An Illustrative Example 7.11 Realizations, Nonconstructively 7.12 Putting Things Together 7.13 A Brief Realization History\n\n110 110 112 113 116 118 120 126 128 131 133 135 138 139\n\n8\n\nThe Range of Realization 8.1 Some Examples We Already Discussed 8.2 Geach Logics 8.3 Technical Results\n\n141 141 142 144\n\nContents 8.4 8.5 8.6 8.7 8.8\n\nGeach Justification Logics Axiomatically Geach Justification Logics Semantically Soundness, Completeness, and Realization A Concrete S4.2/JT4.2 Example Why Cut-Free Is Needed\n\nix 147 149 150 152 155\n\n9\n\nArithmetical Completeness and BHK Semantics 9.1 Arithmetical Semantics of the Logic of Proofs 9.2 A Constructive Canonical Model for the Logic of Proofs 9.3 Arithmetical Completeness of the Logic of Proofs 9.4 BHK Semantics 9.5 Self-Referentiality of Justifications\n\n158 158 161 165 174 179\n\n10\n\nQuantifiers in Justification Logic 10.1 Free Variables in Proofs 10.2 Realization of FOS4 in FOLP 10.3 Possible World Semantics for FOLP 10.4 Arithmetical Semantics for FOLP\n\n181 182 186 191 212\n\n11\n\nGoing Past Modal Logic 11.1 Modeling Awareness 11.2 Precise Models 11.3 Justification Awareness Models 11.4 The Russell Scenario as a JAM 11.5 Kripke Models and Master Justification 11.6 Conclusion References Index\n\n222 223 225 226 228 231 233 234 244\n\nIntroduction\n\nWhy is this thus? What is the reason of this thusness?1\n\nModal operators are commonly understood to qualify the truth status of a proposition: necessary truth, proved truth, known truth, believed truth, and so on. The ubiquitous possible world semantics for it characterizes things in universal terms: X is true in some state if X is true in all accessible states, where various conditions on accessibility are used to distinguish one modal logic from another. Then (X → Y) → (X → Y) is valid, no matter what conditions are imposed, by a simple and direct argument using universal quantification. Suppose both (X → Y) and X are true at an arbitrary state. Then both X and X → Y are true at all accessible states, whatever “accessible” may mean. By the usual understanding of →, Y is true at all accessible states too, and so Y is true at the arbitrary state we began with. Although arguments like these have a strictly formal nature and are studied as modal model theory, they also give us some insights into our informal, everyday use of modalities. Still, something is lacking. Suppose we think of as epistemic, and to emphasize this we use K instead of for the time being. For some particular X, if you assert the colloquial counterpart of KX, that is, if you say you know X, and I ask you why you know X, you would never tell me that it is because X is true in all states epistemically compatible with this one. You would, instead, give me some sort of explicit reason: “I have a mathematical proof of X,” or “I read X in the encyclopedia,” or “I observed that X is the case.” If I asked you why K(X → Y) → (KX → KY) is valid you would probably say something like “I could use my reason for X and combine it with my reason for X → Y, and infer Y.” This, in effect, would be your reason for Y, given that you had reasons for X and for X → Y. 1\n\nCharles Farrar Browne (1834–1867) was an American humorist who wrote under the pen name Artemus Ward. He was a favorite writer of Abraham Lincoln, who would read his articles to his Cabinet. This quote is from a piece called Moses the Sassy, Ward (1861).\n\nx\n\nIntroduction\n\nxi\n\nNotice that this neatly avoids the logical omniscience problem: that we know all the consequences of what we know. It replaces logical omniscience with the more acceptable claim that there are reasons for the consequences of what we know, based on the reasons for what we know, but reasons for consequences are more complicated things. In our example, the reason for Y has some structure to it. It combines reasons for X, reasons for X → Y, and inference as a kind of operation on reasons. We will see more examples of this sort; in fact, we have just seen a fundamental paradigm. In place of a modal operator, , justification logics have a family of justification terms, informally intended to represent reasons, or justifications. Instead of X we will see t:X, where t is a justification term and the formula is read “X is so for reason t,” or more briefly, “t justifies X.” At a minimum, justification terms are built up from justification variables, standing for arbitrary justifications. They are built up using a set of operations that, again at a minimum, contains a binary operation ·. For example, x · (y · x) is a justification term, where x and y are justification variables. The informal understanding of · is that t · u justifies Y provided t justifies an implication with Y as its consequent, and u justifies the antecedent. In justification logics the counterpart of (X → Y) → (X → Y) is t:(X → Y) → (u:X → [t · u]:Y) where, as we will often do, we have added square brackets to enhance readability. Note that this exactly embodies the informal explanation we gave in the previous paragraph for the validity of K(X → Y) → (KX → KY). That is, Y has a justification built from justifications for X and for X → Y using an inference that amounts to a modus ponens application—we can think of the · operation as an abstract representation of this inference. Other behaviors of modal operators, X → X for instance, will require operators in addition to ·, and appropriate postulated behavior, in order to produce justification logics that correspond to modal logics in which X → X is valid. Examples, general methods for doing this, and what it means to “correspond” all will be discussed during the course of this book. One more important point. Suppose X and Y are equivalent formulas, that is, we have X ↔ Y. Then in any normal modal logic we will also have X ↔ Y. Let us interpret the modal operator epistemically again, and write KX ↔ KY. In fact, KX ↔ KY, when read in the usual epistemic way, can sometimes be quite an absurd assertion. Consider some astronomically complicated tautology X of classical propositional logic. Because it is a tautology, it is equivalent\n\nxii\n\nIntroduction\n\nto P ∨ ¬P, which we may take for Y. Y is hardly astronomically complicated. However, because X ↔ Y, we will have KX ↔ KY. Clearly, we know Y essentially by inspection and hence KY holds, while KX on the other hand will involve an astronomical amount of work just to read it, let alone to verify it. Informally we see that, while both X and Y are tautologies, and so both are knowable in principle, any justification we might give for knowing one, combined with quite a lot of formula manipulation, can give us some justification for knowing the other. The two justifications may not, indeed will not, be the same. One is simple, the other very complex. Modal logic is about propositions. Propositions are, in a sense, the content of formulas. Propositions are not syntactical objects. “It’s good to be the king” and “Being the king is good” express the same proposition, but not in the same way. Justifications apply to formulas. Equivalent formulas determine the same proposition, but can be quite different as formulas. Syntax must play a fundamental role for us, and you will see that it does, even in our semantics. Consider one more very simple example. A → (A∧ A) is an obvious tautology. We might expect KA → K(A ∧ A). But we should not expect t:A → t:(A ∧ A). If t does, in fact, justify A, a justification of A ∧ A may involve t, but also should involve facts about the redundancy of repetition; t by itself cannot be expected to suffice. Modal logics can express, more or less accurately, how various modal operators behave. This behavior is captured axiomatically by proofs, or semantically using possible world reasoning. These sorts of justifications for modal operator behavior are not within a modal logic, but are outside constructs. Justification logics, on the other hand, can represent the whys and wherefores of modal behavior quite directly, and from within the formal language itself. We will see that most standard modal logics have justification counterparts that can be used to give a fine-grained, internal analysis of modal behavior. Perhaps, this will help make clear why we used the quotation we did at the beginning of this Introduction.\n\n1 What Is This Book About? How did justification logics originate? It is an interesting story, with revealing changes of direction along the way. Going back to the days when G¨odel was a young logician, there was a dream of finding a provability interpretation for intuitionistic logic. As part of his work on that project, in G¨odel (1933), G¨odel showed that one could abstract some of the key features of provability and make a propositional modal logic using them. Then, remarkably but\n\nIntroduction\n\nxiii\n\nnaturally, one could embed propositional intuitionistic logic into the resulting system. C. I. Lewis had pioneered the modern formal study of modal logics (Lewis, 1918; Lewis and Langford, 1932), and G¨odel observed that his system was equivalent to the Lewis system S4. All modern axiomatizations of modal logics follow the lines pioneered in G¨odel’s note, while Lewis’s original formulation is rarely seen today. G¨odel showed that propositional intuitionistic logic embedded into S4 using a mapping that inserted in front of every subformula. In effect, intuitionistic logic could be understood using classical logic plus an abstract notion of provability: a propositional formula X is an intuitionistic theorem if and only if the result of applying G¨odel’s mapping is a theorem of S4. (This story is somewhat simplified. There are several versions of the G¨odel translation—we have used the simplest one to describe. And G¨odel did not use the symbol but rather an operator Bew, short for beweisbar, or provability in the German language. None of this affects our main points.) Unfortunately, the story breaks off at this point because G¨odel also noted that S4 does not behave like formal provability (e.g., in arithmetic), by using the methods he had pioneered in his work on incompleteness. Specifically, S4 validates X → X, so in particular we have ⊥ → ⊥ (where ⊥ is falsehood). This is equivalent to ¬⊥, which is thus provable in S4. If we had an embedding of S4 into formal arithmetic under which corresponded to G¨odel’s arithmetic formula representing provability, we would be able to prove in arithmetic that falsehood was not provable. That is, we would be able to show provability of consistency, violating G¨odel’s second incompleteness theorem. So, work on an arithmetic semantics for propositional intuitionistic logic paused for a while. Although it did not solve the problem of a provability semantics for intuitionistic logic, an important modal/arithmetic connection was eventually worked out. One can define a modal logic by requiring that its validities are those that correspond to arithmetic validities when reading as G¨odel’s provability formula. It was shown in Solovay (1976) that this was a modal logic already known in the literature, though as noted earlier, it is not S4. Today, the logic is called GL, standing for G¨odel–L¨ob logic. GL is like S4 except that the T axiom X → X, an essential part of S4, is replaced by a modal formula abstractly representing L¨ob’s theorem: (X → X) → X. S4 and GL are quite different logics. By now the project for finding an arithmetic interpretation of intuitionistic logic had reached an impasse. Intuitionistic logic embedded into S4, but S4 did not embed into formal arithmetic. GL embedded into formal arithmetic, but the G¨odel translation does not embed intuitionistic logic into GL.\n\nxiv\n\nIntroduction\n\nIn his work on incompleteness for Peano arithmetic, G¨odel gave a formula Bew(x, y)\n\nthat represents the relation: x is the G¨odel number of a proof of a formula with G¨odel number y. Then, a formal version of provability is ∃xBew(x, y) which expresses that there is a proof of (the formula whose G¨odel number is) y. If this formula is what corresponds to in an embedding from a modal language to Peano arithmetic, we get the logic GL. But in a lecture in 1938 G¨odel pointed out that we might work with explicit proof representatives instead of with provability (G¨odel, 1938). That is, instead of using an embedding translating every occurrence of by ∃xBew(x, y), we might associate with each occurrence of some formal term t that somehow represents a particular proof, allowing different occurrences of to be associated with different terms t. Then in the modal embedding, we could make the occurrence of associated with t correspond to Bew(ptq, y), where ptq is a G¨odel number for t. For each occurrence of we would need to find some appropriate term t, and then each occurrence of would be translated into arithmetic differently. The existential quantifier in ∃xBew(x, y) has been replaced with a meta-existential quantifier, outside the formal language. We provide an explicit proof term, rather than just asserting that one exists. G¨odel believed that this approach should lead to a provability embedding of S4 into Peano arithmetic. G¨odel’s proposal was not published until 1995 when Volume 3 of his collected works appeared. By this time the idea of using a modal-like language with explicit representatives for proofs had been rediscovered independently by Sergei Artemov, see Artemov (1995, 2001). The logic that Artemov created was called LP, which stood for logic of proofs. It was the first example of a justification logic. What are now called justification terms were called proof terms in LP. Crucially, Artemov showed LP filled the gap between modal S4 and Peano arithmetic. The connection with S4 is primarily embodied in a Realization Theorem, which has since been shown to hold for a wide range of justification logic, modal logic pairs. It will be extensively examined in this book. The connection between LP and formal arithmetic is Artemov’s Arithmetic Completeness Theorem, which also will be examined in this book. Its range is primarily limited to the original justification logic, LP, and a few close relatives. This should not be surprising, though. G¨odel’s motivation for his formulation of S4 was that should embody properties of a formal arithmetic proof predicate. This connection with arithmetic provability is not present for almost all modal\n\nIntroduction\n\nxv\n\nlogics and is consequently also missing for corresponding justification logics, when they exist. Nonetheless, the venerable goal of finding a provability interpretation for propositional intuitionistic logic had been attained. The G¨odel translation embeds propositional intuitionistic logic into the modal logic S4. The Realization Theorem establishes an embedding of S4 into the justification logic LP. And the Arithmetic Completeness Theorem shows that LP embeds into formal arithmetic. It was recognized from the very beginning that the connection between S4 and LP could be weakened to sublogics of S4 and LP. Thus, there were justification logic counterparts for the standard modal logics, K, K4, T, and a few others. These justification logics had arithmetic connections because they were sublogics of LP. The use of proof term was replaced with justification term. Although the connection with arithmetic was weaker than it had been with LP, justification terms still had the role of supplying explicit justifications for epistemically necessary statements. One can consult Artemov (2008) and Artemov and Fitting (2012) for survey treatments, though the present book includes the material found there. Almost all of the early work on justification logics was proof-theoretically based. Realization theorems were shown constructively, making use of a sequent calculus. The existence of an algorithm to compute what are called realizers is important, but this proof-theoretic approach limits the field to those logics known to have sequent calculus proof systems. For a time it was hoped that various extensions of sequent and tableau calculi would be useful and, to some extent, this has been the case. The most optimistic version of this hope was expressed in Artemov (2001) quite directly, “Gabbay’s Labelled Deductive Systems, Gabbay (1994), may serve as a natural framework for LP.” Unfortunately this seems to have been too optimistic. While the formats had similarities, the goals were different, and the machinery did not interact well. A semantics for LP and its near relatives, not based on arithmetic provability, was introduced in Mkrtychev (1997) and is discussed in Chapter 3. (A constructive version of the canonical model for LP with a completeness theorem can be found already in Artemov (1995).) Mkrtychev’s semantics did not use possible worlds and had a strong syntactic flavor. Possible worlds were added to the mix in Fitting (2005), producing something that potentially applied much more broadly than the earlier semantics. This is the subject of Chapter 4. Using this possible world semantics, a nonconstructive, semantic-based, proof of realization was given. It was now possible to avoid the use of a sequent calculus, though the algorithmic nature of realization was lost. More recently, a semantics with a very simple structure was created, Artemov’s basic semantics (Artemov, 2012). It is presented in Chapter 3. Its machinery is almost minimal\n\nxvi\n\nIntroduction\n\nfor the purpose. In this book, we will use possible world semantics to establish very general realization results, but basic models will often be used when we simply want to show some formula fails to be a theorem. Though its significance was not properly realized at the time, in 2005 the subject broadened when a justification logic counterpart of S5 was introduced in Pacuit (2005) and Rubtsova (2006a, b), with a connecting realization theorem. There was no arithmetical interpretation for this justification logic. Also there is no sequent calculus for S5 of the standard kind, so the proof given for realization was nonconstructive, using a version of the semantics from Fitting (2005). The semantics needed some modification to what is called its evidence function, and this turned out to have a greater impact than was first realized. Eventually constructive proofs connecting S5 and its justification counterpart were found. These made use of cut-free proof systems that were not exactly standard sequent calculi. Still, the door to a larger room was beginning to open. Out of the early studies of the logics of proofs and its variants a general logical framework for reasoning about epistemic justification at large naturally emerged, and the name, Justification Logic, was introduced (cf. Artemov, 2008). Justification Logic is based on justification assertions, t:F, that are read t is a justification for F, with a broader understanding of the word justification going beyond just mathematical proofs. The notion of justification, which has been an essential component of epistemic studies since Plato, had been conspicuously absent in the mathematical models of knowledge within the epistemic logic framework. The Justification Logic framework fills in this void. In Fitting (2016a) the subject expanded abruptly. Using nonconstructive semantic methods it was shown that the family of modal logics having justification counterparts is infinite. The justification phenomenon is not the relatively narrow one it first seemed to be. While that work was nonconstructive, there are now cut-free proof systems of various kinds for a broader range of modal logics than was once the case, and these have been used successfully to create realization algorithms, in Kuznets and Goetschi (2012), for instance. It may be that the very general proof methodologies of Fitting (2015) and especially Negri (2005) and Negri and von Plato (2001) will extend the constructive range still further, perhaps even to the infinite family that nonconstructive methods are known to work for. This is active current work. Work on quantified justification logics exists, but the subject is considerably behind its propositional counterpart. An important feature of justification logics is that they can, in a very precise sense, internalize their own proofs. Doing this for axioms is generally simple. Rules of inference are more of a problem. Earlier we discussed a justification formula as a simple, representative exam-\n\nIntroduction\n\nxvii\n\nple: t:(X → Y) → (u:X → [t · u]:Y). This, in effect, internalizes the axiomatic modus ponens rule. The central problem in developing quantified justification logics was how to internalize the rule of universal generalization. It turned out that the key was the clear separation between two roles played by individual variables. On the one hand, they are formal symbols, and one can simply infer ∀xϕ(x) from a proof of ϕ(x). On the other hand, they can be thought of as open for substitution, that is, throughout a proof one can replace free occurrences of x with a term t to produce a new proof (subject to appropriate freeness of substitution conditions, of course). These two roles for variables are actually incompatible. It was the introduction of specific machinery to keep track of which role a variable occurrence had that made possible the internalization of proofs, and thus a quantified justification logic. An axiomatic version of first-order LP was introduced in Artemov and Yavorskaya (Sidon) (2011) and a possible world semantics for it in Fitting (2011a, 2014b). A connection with formal arithmetic was established. There is a constructive proof of a Realization Theorem, connecting first-order LP with firstorder S4. Unlike propositionally, no nonconstructive proof is currently known The possible world semantics includes the familiar monotonicity condition on world domains. It is likely that all this can be extended to a much broader range of quantified modal logics than just first-order S4, provided monotonicity is appropriate. A move to constant domain models, to quantified S5 in particular, has been made, and a semantics, but not yet a Realization Theorem, can be found in Fitting and Salvatore (2018). Much involving quantification is still uncharted territory. This book will cover the whole range of topics just described. It will not do so in the historical order that was followed in this Introduction, but will make use of the clearer understanding that has emerged from study of the subject thus far. We will finish with the current state of affairs, standing on the edge of unknown lands. We hope to prepare some of you for the journey, should you choose to explore further on your own.\n\n2 What Is Not in This Book? There are several historical works and pivotal developments in justification logic that will not be covered in the book due to natural limitations, and in this section we will mention them briefly. We are confident that other books and surveys will do justice to these works in more detail. Apart from G¨odel’s lecture, G¨odel (1938), which remained unpublished\n\nxviii\n\nIntroduction\n\nuntil 1995 and thus could not influence development in this area, the first results and publications on the logic of proofs are dated 1992: a technical report, Artemov and Straßen (1992), based on work done in January of 1992 in Bern, and a conference presentation of this work at CSL’92 published in Springer Lecture Notes in Computer Science as Artemov and Straßen (1993a). In this work, the basic logic of proofs was presented: it had proof variables, and the format t is a proof of F, but without operations on proofs. However, it already had the first installment of the fixed-point arithmetical completeness construction together with an observation that, unlike provability logic, the logic of proofs cannot be limited to one standard proof predicate “from the textbook” or to any single-conclusion proof predicate. This line was further developed in Artemov and Straßen (1993b), where the logic of single-conclusion proof predicates (without operations on proofs) was studied. This work introduced the unification axiom, which captures singleconclusioness by propositional tools. After the full-scale logic of proofs with operations had been discovered (Artemov, 1995), the logic of single-conclusion proofs with operations was axiomatized in V. Krupski (1997, 2001). A similar technique was used recently to characterize so-called sharp single-conclusion justification models in Krupski (2018). Another research direction pursued after the papers on the basic logic of proofs was to combine provability and explicit proofs. Such a combination, with new provability principles, was given in Artemov (1994). Despite its title, this paper did not introduce what is known now as The Logic of Proofs, but rather a fusion of the provability logic GL and the basic logic of proofs without operations, but with new arithmetical principles combining proofs and provability and an arithmetical completeness theorem. After the logic of proofs paper (Artemov, 1995), the full-scale logic of provability and proofs (with operations), LPP, was axiomatized and proved arithmetically complete in Sidon (1997) and Yavorskaya (Sidon) (2001). A leaner logic combining provability and explicit proofs, GLA, was introduced and proved arithmetically complete in Nogina (2006, 2014b). Unlike LPP, the logic GLA did not use additional operations on proofs other than those inherited from LP. Later, GLA was used to find a complete classification of reflection principles in arithmetic that involve provability and explicit proofs (Nogina, 2014a). The first publication of the full-scale logic of proofs with operations, LP, which became the first justification logic in the modern sense, was Artemov (1995). This paper contains all the results needed to complete G¨odel’s program of characterizing intuitionistic propositional logic IPC and its BHK semantics via proofs in classical arithmetic: internalization, the realization theorem for S4 in LP, arithmetical semantics for LP, and the arithmetical completeness the-\n\nIntroduction\n\nxix\n\norem. It took six years for the corresponding journal paper to appear: Artemov (2001). In Goris (2008), the completeness of LP for the semantics of proofs in Peano arithmetic was extended to the semantics of proofs in Buss’s bounded arithmetic S12 . In view of applications in epistemology, this result shows that explicit knowledge in the propositional framework can be made computationally feasible. Kuznets and Studer (2016) extend the arithmetical interpretation of LP from the original finite constant specifications to a wide class of constant specifications, including some infinite ones. In particular, this “weak” arithmetical interpretation captures the full logic of proofs LP with the total constant specification. Decidability of LP (with the total constant specification) was also established in Mkrtychev (1997), and this opened the door to decidability and complexity studies in justification logics using model-theoretic and other means. Among the milestones are complexity estimates in Kuznets (2000), Brezhnev and Kuznets (2006), Krupski (2006a), Milnikel (2007), Buss and Kuznets (2012), and Achilleos (2014a). The arithmetical provability semantics for the Logic of Proofs, LP, naturally generalizes to a first-order version with conventional quantifiers and to a version with quantifiers over proofs. In both cases, axiomatizability questions were answered negatively in Artemov and Yavorskaya (2001) and Yavorsky (2001). A natural and manageable first-order version of the logic of proofs, FOLP, has been studied in Artemov and Yavorskaya (Sidon) (2011), Fitting (2014a), and Fitting and Salvatore (2018) and will be covered in Chapter 10. Originally, the logic of proofs was formulated as a Hilbert-style axiomatic system, but this has gradually broadened. Early attempts were tableau based (which could equivalently be presented using sequent calculus machinery). Tableaus generally are analytic, meaning that everything entering into a proof is a subformula of what is being proved. This was problematic for attempts at LP tableaus because of the presence of the · operation, which represented an application of modus ponens, a rule that is decidedly not analytic. Successful tableau systems, though not analytic, for LP and closely related logics can be found in Fitting (2003, 2005) and Renne (2004, 2006). The analyticity problem was overcome in Ghari (2014, 2016a). Broader proof systems have been investigated: hypersequents in Kurokawa (2009, 2012), prefixed tableaus in Kurokawa (2013), and labeled deductive systems in Ghari (2017). Indeed some of this has led to new realization results (Artemov, 1995, 2001, 2002, 2006; Artemov and Bonelli, 2007; Ghari, 2012; Kurokawa, 2012). Finding a computational reading of justification logics has been a natural research goal. There were several attempts to use the ideas of LP for building a lambda-calculus with internalization, cf. Alt and Artemov (2001), Artemov\n\nxx\n\nIntroduction\n\n(2002), Artemov and Bonelli (2007), Pouliasis and Primiero (2014), and others. Corresponding combinatory logic systems with internalization were studied in Artemov (2004), Krupski (2006b), and Shamkanov (2011). These and other studies can serve as a ground for further applications in typed programming languages. A version of the logic of proofs with a built-in verification predicate was considered in Protopopescu (2016a, b). The aforementioned intuition that justification logic naturally avoids the logical omniscience problem has been formalized and studied in Artemov and Kuznets (2006, 2009, 2014). The key idea there was to view logical omniscience as a proof complexity problem: The logical omniscience defect occurs if an epistemic system assumes knowledge of propositions, which have no feasible proofs. Through this prism, standard modal logics are logically omniscient (modulo some common complexity assumptions), and justification logics are not logically omniscient. The ability of justification logic to track proof complexity via time bounds led to another formal definition of logical omniscience in Wang (2011a) with the same conclusion: Justification logic keeps logical omniscience under control. Shortly after the first paper on the logic of proofs, it became clear that the logical tools developed are capable of evidence tracking in a general setting and as such can be useful in epistemic logic. Perhaps, the first formal work in this direction was Artemov et al. (1999), in which modal logic S5 was equivalently modified and supplied with an LP-style explicit counterpart. Applications to epistemology have benefited greatly from Fitting semantics, which connected justification logics to mainstream epistemology via possible worlds models. In addition to applications discussed in this book, we would like to mention some other influential work. Game semantics of justification logic was studied in Renne (2008) and dynamic epistemic logic with justifications in Renne (2008) and Baltag et al. (2014). In Sedl´ar (2013), Fitting semantics for justification models was elaborated to a special case of the models of general awareness. Multiagent justification logic and common knowledge has been studied in Artemov (2006), Antonakos (2007), Yavorskaya (Sidon) (2008), Bucheli et al. (2010, 2011), Bucheli (2012), Antonakos (2013), and Achilleos (2014b, 2015a, b). In Dean and Kurokawa (2010), justification logic was used for the analysis of Knower and Knowability paradoxes. A fast-growing and promising area is probabilistic justification logic, cf. Milnikel (2014), Artemov (2016b), Kokkinis et al. (2016), Ghari (2016b), and Lurie (2018).\n\nIntroduction\n\nxxi\n\nWe are deeply indebted to all contributors to the exciting justification logic project, without whom there would not be this book. Very special thanks to our devoted readers for their sharp eyes and their useful comments: Vladimir Krupski, Vincent Alexis Peluce, and Tatiana Yavorskaya (Sidon).\n\nI think there is no sense in forming an opinion when there is no evidence to form it on. If you build a person without any bones in him he may look fair enough to the eye, but he will be limber and cannot stand up; and I consider that evidence is the bones of an opinion.2\n\n2\n\nMark Twain (1835–1910). The quote is from his last novel, Personal Recollections of Joan of Arc, Twain (1896).\n\n1 Why Justification Logic?\n\nThe formal details of justification logic will be presented starting with the next chapter, but first we give some background and motivation for why the subject was developed in the first place. We will see that it addresses, or at least partially addresses, many of the fundamental problems that have been found in epistemic logic over the years. We will also see in more detail how it relates to our understanding of intuitionistic logic. And finally, we will see how it can be used to mitigate some well-known issues that have arisen in philosophical investigations.\n\n1.1 Epistemic Tradition The properties of knowledge and belief have been a subject for formal logic at least since von Wright and Hintikka (Hintikka, 1962; von Wright, 1951). Knowledge and belief are both treated as modalities in a way that is now very familiar—Epistemic Logic. But of the celebrated three criteria for knowledge (usually attributed to Plato), justified, true, belief, Gettier (1963); Hendricks (2005), epistemic modal logic really works with only two of them. Possible worlds and indistinguishability model belief—one believes what is so under all circumstances thought possible. Factivity brings a trueness component into play—if something is not so in the actual world it cannot be known, only believed. But there is no representation for the justification condition. Nonetheless, the modal approach has been remarkably successful in permitting the development of rich mathematical theory and applications (Fagin et al., 1995; van Ditmarsch et al., 2007). Still, it is not the whole picture. The modal approach to the logic of knowledge is, in a sense, built around the universal quantifier: X is known in a situation if X is true in all situations indistinguishable from that one. Justifications, on the other hand, bring an ex1\n\n2\n\nWhy Justification Logic?\n\nistential quantifier into the picture: X is known in a situation if there exists a justification for X in that situation. This universal/existential dichotomy is a familiar one to logicians—in formal logics there exists a proof for a formula X if and only if X is true in all models for the logic. One thinks of models as inherently nonconstructive, and proofs as constructive things. One will not go far wrong in thinking of justifications in general as much like mathematical proofs. Indeed, the first justification logic was explicitly designed to capture mathematical proofs in arithmetic, something that will be discussed later. In justification logic, in addition to the category of formulas, there is a second category of justifications. Justifications are formal terms, built up from constants and variables using various operation symbols. Constants represent justifications for commonly accepted truths—axioms. Variables denote unspecified justifications. Different justification logics differ on which operations are allowed (and also in other ways too). If t is a justification term and X is a formula, t:X is a formula, and is intended to be read t is a justification for X. One operation, common to all justification logics, is application, written like multiplication. The idea is, if s is a justification for A → B and t is a justification for A, then [s · t] is a justification for B.1 That is, the validity of the following is generally assumed s:(A → B) → (t:A → [s · t]:B).\n\n(1.1)\n\nThis is the explicit version of the usual distributivity of knowledge operators, and modal operators generally, across implication K(A → B) → (KA → KB).\n\n(1.2)\n\nHow adequately does the traditional modal form (1.2) embody epistemic closure? We argue that it does so poorly! In the classical logic context, (1.2) only claims that it is impossible to have both K(A → B) and KA true, but KB false. However, because (1.2), unlike (1.1), does not specify dependencies between K(A → B), KA, and KB, the purely modal formulation leaves room for a counterexample. The distinction between (1.1) and (1.2) can be exploited in a discussion of the paradigmatic Red Barn Example of Goldman and Kripke; here is a simplified version of the story taken from Dretske (2005). 1\n\nFor better readability brackets will be used in terms, “[,]”, and parentheses in formulas, “(,).” Both will be avoided when it is safe.\n\n1.1 Epistemic Tradition\n\n3\n\nSuppose I am driving through a neighborhood in which, unbeknownst to me, papiermˆach´e barns are scattered, and I see that the object in front of me is a barn. Because I have barn-before-me percepts, I believe that the object in front of me is a barn. Our intuitions suggest that I fail to know barn. But now suppose that the neighborhood has no fake red barns, and I also notice that the object in front of me is red, so I know a red barn is there. This juxtaposition, being a red barn, which I know, entails there being a barn, which I do not, “is an embarrassment.”\n\nIn the first formalization of the Red Barn Example, logical derivation will be performed in a basic modal logic in which is interpreted as the “belief” modality. Then some of the occurrences of will be externally interpreted as a knowledge modality K according to the problem’s description. Let B be the sentence “the object in front of me is a barn,” and let R be the sentence “the object in front of me is red.” (1) B, “I believe that the object in front of me is a barn.” At the metalevel, by the problem description, this is not knowledge, and we cannot claim KB. (2) (B ∧ R), “I believe that the object in front of me is a red barn.” At the metalevel, this is actually knowledge, e.g., K(B ∧ R) holds. (3) (B∧R → B), a knowledge assertion of a logical axiom. This is obviously knowledge, i.e., K(B ∧ R → B). Within this formalization, it appears that epistemic closure in its modal form (1.2) is violated: K(B∧R), and K(B∧R → B) hold, whereas, by (1), we cannot claim KB. The modal language here does not seem to help resolving this issue. Next consider the Red Barn Example in justification logic where t:F is interpreted as “I believe F for reason t.” Let u be a specific individual justification for belief that B, and v for belief that B ∧ R. In addition, let a be a justification for the logical truth B ∧ R → B. Then the list of assumptions is (i) u:B, “u is a reason to believe that the object in front of me is a barn”; (ii) v:(B ∧ R), “v is a reason to believe that the object in front of me is a red barn”; (iii) a:(B ∧ R → B). On the metalevel, the problem description states that (ii) and (iii) are cases of knowledge, and not merely belief, whereas (i) is belief, which is not knowledge. Here is how the formal reasoning goes: (iv) a:(B ∧ R → B) → (v:(B ∧ R) → [a·v]:B), by principle (1.1); (v) v:(B ∧ R) → [a·v]:B, from 3 and 4, by propositional logic; (vi) [a·v]:B, from 2 and 5, by propositional logic.\n\n4\n\nWhy Justification Logic?\n\nNotice that conclusion (vi) is [a · v]:B, and not u:B; epistemic closure holds. By reasoning in justification logic it was concluded that [a·v]:B is a case of knowledge, i.e., “I know B for reason a · v.” The fact that u:B is not a case of knowledge does not spoil the closure principle because the latter claims knowledge specifically for [a·v]:B. Hence after observing a red fac¸ade, I indeed know B, but this knowledge has nothing to do with (i), which remains a case of belief rather than of knowledge. The justification logic formalization represents the situation fairly. Tracking justifications represents the structure of the Red Barn Example in a way that is not captured by traditional epistemic modal tools. The justification logic formalization models what seems to be happening in such a case; closure of knowledge under logical entailment is maintained even though “barn” is not perceptually known. One could devise a formalization of the Red Barn Example in a bimodal language with distinct modalities for knowledge and belief. However, it seems that such a resolution must involve reproducing justification tracking arguments in a way that obscures, rather than reveals, the truth. Such a bimodal formalization would distinguish u:B from [a · v]:B not because they have different reasons (which reflects the true epistemic structure of the problem), but rather because the former is labeled “belief” and the latter “knowledge.” But what if one needs to keep track of a larger number of different unrelated reasons? By introducing a multiplicity of distinct modalities and then imposing various assumptions governing the interrelationships between these modalities, one would essentially end up with a reformulation of the language of justification logic itself (with distinct terms replaced by distinct modalities). This suggests that there may not be a satisfactory “halfway point” between a modal language and the language of justification logic, at least inasmuch as one tries to capture the essential structure of examples involving the deductive nature of knowledge.\n\n1.2 Mathematical Logic Tradition According to Brouwer, truth in constructive (intuitionistic) mathematics means the existence of a proof, cf. Troelstra and van Dalen (1988). In 1931–34, Heyting and Kolmogorov gave an informal description of the intended proof-based semantics for intuitionistic logic (Kolmogoroff, 1932; Heyting, 1934), which is now referred to as the Brouwer–Heyting–Kolmogorov (BHK) semantics. According to the BHK conditions, a formula is “true” if it hasa proof. Further-\n\n1.2 Mathematical Logic Tradition\n\n5\n\nmore, a proof of a compound statement is connected to proofs of its components in the following way: • a proof of A ∧ B consists of a proof of proposition A and a proof of proposition B, • a proof of A∨B is given by presenting either a proof of A or a proof of B, • a proof of A → B is a construction transforming proofs of A into proofs of B, • falsehood ⊥ is a proposition, which has no proof; ¬A is shorthand for A → ⊥. This provides a remarkably useful informal way of understanding what is and what is not intuitionistically acceptable. For instance, consider the classical tautology (P ∨ Q) ↔ (P ∨ (Q ∧ ¬P)), where we understand ↔ as mutual implication. And we understand ¬P as P → ⊥, so that a proof of ¬P would amount to a construction converting any proof of P into a proof of ⊥. Because ⊥ has no proof, this amounts to a proof that P has no proof—a refutation of P. According to BHK semantics the implication from right to left in (P ∨ Q) ↔ (P ∨ (Q ∧ ¬P)) should be intuitionistically valid, by the following argument. Given a proof of P ∨ (Q ∧ ¬P) it must be that we are given a proof of one of the disjuncts. If it is P, we have a proof of one of P ∨ Q. If it is Q ∧ ¬P, we have proofs of both conjuncts, hence a proof of Q, and hence again a proof of one of P ∨ Q. Thus we may convert a proof of P ∨ (Q ∧ ¬P) into a proof of P ∨ Q. On the other hand, (P ∨ Q) → (P ∨ (Q ∧ ¬P)) is not intuitionistically valid according to the BHK ideas. Suppose we are given a proof of P ∨ Q. If we have a proof of the disjunct P, we have a proof of P ∨ Q. But if we have a proof of Q, there is no reason to suppose we have a refutation of P, and so we cannot conclude we have a proof of Q ∧ ¬P, and things stop here. Kolmogorov explicitly suggested that the proof-like objects in his interpretation (“problem solutions”) came from classical mathematics (Kolmogoroff, 1932). Indeed, from a foundational point of view this reflects Kolmogorov’s and G¨odel’s goal to define intuitionism within classical mathematics. From this standpoint, intuitionistic mathematics is not a substitute for classical mathematics, but helps to determine what is constructive in the latter. The fundamental value of the BHK semantics for the justification logic project is that informally but unambiguously BHK suggests treating justifications, here mathematical proofs, as objects with operations. In G¨odel (1933), G¨odel took the first step toward developing a rigorous proof-based semantics for intuitionism. G¨odel considered the classical modal logic S4 to be a calculus describing properties of provability:\n\n6\n\nWhy Justification Logic?\n\n(1) (2) (3) (4)\n\nAxioms and rules of classical propositional logic, (F → G) → (F → G), F → F, F → F, `F . (5) Rule of necessitation: ` F\n\nBased on Brouwer’s understanding of logical truth as provability, G¨odel defined a translation tr(F) of the propositional formula F in the intuitionistic language into the language of classical modal logic: tr(F) is obtained by prefixing every subformula of F with the provability modality . Informally speaking, when the usual procedure of determining classical truth of a formula is applied to tr(F), it will test the provability (not the truth) of each of F’s subformulas, in agreement with Brouwer’s ideas. From G¨odel’s results and the McKinseyTarski work on topological semantics for modal logic (McKinsey and Tarski, 1948), it follows that the translation tr(F) provides a proper embedding of the Intuitionistic Propositional Calculus, IPC, into S4, i.e., an embedding of intuitionistic logic into classical logic extended by the provability operator. IPC ` F\n\n⇔\n\nS4 ` tr(F).\n\n(1.3)\n\nConceptually, this defines IPC in S4. Still, G¨odel’s original goal of defining intuitionistic logic in terms of classical provability was not reached because the connection of S4 to the usual mathematical notion of provability was not established. Moreover, G¨odel noted that the straightforward idea of interpreting modality F as F is provable in a given formal system T contradicted his second incompleteness theorem. Indeed, (F → F) can be derived in S4 by the rule of necessitation from the axiom F → F. On the other hand, interpreting modality as the predicate of formal provability in theory T and F as contradiction converts this formula into a false statement that the consistency of T is internally provable in T . The situation after G¨odel (1933) can be described by the following figure where “X ,→ Y” should be read as “X is interpreted in Y”: IPC ,→ S4 ,→ ? ,→ CLASSICAL PROOFS.\n\nIn a public lecture in Vienna in 1938, G¨odel observed that using the format of explicit proofs t is a proof of F\n\n(1.4)\n\ncan help in interpreting his provability calculus S4 (G¨odel, 1938). Unfortunately, G¨odel (1938) remained unpublished until 1995, by which time the\n\n1.2 Mathematical Logic Tradition\n\n7\n\nG¨odelian logic of explicit proofs had already been rediscovered, axiomatized as the Logic of Proofs LP, and supplied with completeness theorems connecting it to both S4 and classical proofs (Artemov, 1995, 2001). The Logic of Proofs LP became the first in the justification logic family. Proof terms in LP are nothing but BHK terms understood as classical proofs. With LP, propositional intuitionistic logic received the desired rigorous BHK semantics: IPC ,→ S4 ,→ LP ,→ CLASSICAL PROOFS .\n\nSeveral well-known mathematical notions that appeared prior to justification logic have sometimes been perceived as related to the BHK idea: Kleene realizability (Troelstra, 1998), Curry–Howard isomorphism (Girard et al., 1989; Troelstra and Schwichtenberg, 1996), Kreisel–Goodman theory of constructions (Goodman, 1970; Kreisel, 1962, 1965), just to name a few. These interpretations have been very instrumental for understanding intuitionistic logic, though none of them qualifies as the BHK semantics. Kleene realizability revealed a fundamental computational content of formal intuitionistic derivations; however it is still quite different from the intended BHK semantics. Kleene realizers are computational programs rather than proofs. The predicate “r realizes F” is not decidable, which leads to some serious deviations from intuitionistic logic. Kleene realizability is not adequate for the intuitionistic propositional calculus IPC. There are realizable propositional formulas not derivable in IPC (Rose, 1953).2 The Curry–Howard isomorphism transliterates natural derivations in IPC to typed λ-terms, thus providing a generic functional reading for logical derivations. However, the foundational value of this interpretation is limited because, as proof objects, Curry–Howard λ-terms denote nothing but derivations in IPC itself and thus yield a circular provability semantics for the latter. An attempt to formalize the BHK semantics directly was made by Kreisel in his theory of constructions (Kreisel, 1962, 1965). The original variant of the theory was inconsistent; difficulties already occurred at the propositional level. In Goodman (1970) this was fixed by introducing a stratification of constructions into levels, which ruined the BHK character of this semantics. In particular, a proof of A → B was no longer a construction that could be applied to any proof of A. 2\n\nKleene himself denied any connection of his realizability with the BHK interpretation.\n\n8\n\nWhy Justification Logic?\n\n1.3 Hyperintensionality Justification logic offers a formal framework for hyperintensionality. The hyperintensional paradox was formulated in Cresswell (1975). It is well known that it seems possible to have a situation in which there are two propositions p and q which are logically equivalent and yet are such that a person may believe the one but not the other. If we regard a proposition as a set of possible worlds then two logically equivalent propositions will be identical, and so if “x believes that” is a genuine sentential functor, the situation described in the opening sentence could not arise. I call this the paradox of hyperintensional contexts. Hyperintensional contexts are simply contexts which do not respect logical equivalence.\n\nStarting with Cresswell himself, several ways of dealing with this have been proposed. Generally, these involve adding more layers to familiar possible world approaches so that some way of distinguishing between logically equivalent sentences is available. Cresswell suggested that the syntactic form of sentences be taken into account. Justification logic, in effect, does this through its mechanism for handling justifications for sentences. Thus justification logic addresses some of the central issues of hyperintensionality but, as a bonus, we automatically have an appropriate proof theory, model theory, complexity estimates, and a broad variety of applications. A good example of a hyperintensional context is the informal language used by mathematicians conversing with each other. Typically when a mathematician says he or she knows something, the understanding is that a proof is at hand, but this kind of knowledge is essentially hyperintensional. For instance Fermat’s Last Theorem, FLT, is logically equivalent to 0 = 0 because both are provable and hence denote the same proposition, as this is understood in modal logic. However, the context of proofs distinguishes them immediately because a proof of 0 = 0 is not necessarily a proof of FLT, and vice versa. To formalize mathematical speech, the justification logic LP is a natural choice because t:X was designed to have characteristics of “t is a proof of X.” The fact that propositions X and Y are equivalent in LP, that LP ` X ↔ Y, does not warrant the equivalence of the corresponding justification assertions, and typically t:X and t:Y are not equivalent, t:X 6↔ t:Y. Indeed, as we will see, this is the case for every justification logic. Going further LP, and justification logic in general, is not only sufficiently refined to distinguish justification assertions for logically equivalent sentences, but it also provides flexible machinery to connect justifications of equivalent sentences and hence to maintain constructive closure properties desirable for a logic system. For example, let X and Y be provably equivalent, i.e., there is a proof u of X ↔ Y, and so u:(X ↔ Y) is provable in LP. Suppose also\n\n1.4 Awareness\n\n9\n\nthat v is a proof of X, and so v:X. It has already been mentioned that this does not mean v is a proof of Y—this is a hyperintensional context. However within the framework of justification logic, building on the proofs of X and of X ↔ Y, we can construct a proof term f (u, v), which represents the proof of Y and so f (u, v):Y is provable. In this respect, justification logic goes beyond Cresswell’s expectations: Logically equivalent sentences display different but constructively controlled epistemic behavior.\n\n1.4 Awareness The logical omniscience problem is that in epistemic logics all tautologies are known and knowledge is closed under consequence, both of which are unreasonable. In Fagin and Halpern (1988) a simple mechanism for avoiding the problems was introduced. One adds to the usual Kripke model structure an awareness function A indicating for each world which formulas the agent is aware of at this world. Then a formula is taken to be known at a possible world u if (1) the formula is true at all worlds accessible from u (the Kripkean condition for knowledge) and (2) the agent is aware of the formula at u. The awareness function A can serve as a practical tool for blocking knowledge of an arbitrary set of formulas. However, as logical structures, awareness models exhibit abnormal behavior due to the lack of natural closure properties. For example, the agent can know A ∧ A but be unaware of A and hence not know it. Fitting models for justification logic, presented in Chapter 4, use a forcing definition reminiscent of the one from awareness models: For any given justification t, the justification assertion t:F holds at world u iff (1) F holds at all worlds v accessible from u and (2) t is an admissible evidence for F at u, u ∈ E(s, F), read as “u is a possible world at which s is relevant evidence for F.” The principal difference is that postulated operations on justifications relate to natural closure conditions on admissible evidence functions E in justification logic models. Indeed, this idea has been explored in Sedl´ar (2013), which works with the language of LP and thinks of it as a multiagent modal logic, and taking justification terms as agents (more properly, actions of agents). This shows that justification logic models absorb the usual epistemic themes of awareness, group agency, and dynamics in a natural way.\n\n10\n\nWhy Justification Logic?\n\n1.5 Paraconsistency Justification logic offers a well-principled approach to paraconsistency, which looks for noncollapsing logical ways of dealing with contradictory sets of assumptions, e.g., {A, ¬A}. The following obvious observation shows how to convert any set of assumptions Γ = {A1 , A2 , A3 , . . .} into a logically consistent set of sentences while maintaining all the intrinsic structure of Γ. Informally, instead of (perhaps inconsistently) assuming that Γ holds, we assume only that each sentence A from Γ has a justification, i.e., ~x : Γ = {x1:A1 , x2:A2 , x3:A3 , . . .}. It is easy to see that for each Γ, the set ~x:Γ is consistent in what will be our basic justification logic J. For example, for Γ = {A, ¬A}, ~x : Γ = {x1:A, x2:¬A}, states that x1 is a justification for A and x2 is a justification for ¬A. Within justification logic J in which no factivity (or even consistency) of justifications is assumed, the set of assumptions {x1:A, x2:¬A} is consistent, unlike the original set of assumptions {A, ¬A}. There is nothing paraconsistent, magical, or artificial in reasoning from ~x:Γ in justification logic J. In practical terms, this means we gain the ability to effectively reason about inconsistent data sets, keeping track of justifications and their dependencies, with the natural possibility to draw meaningful conclusions even when some assumed justifications from ~x:Γ become compromised and should be discharged.\n\n2 The Basics of Justification Logic\n\nIn this chapter we discuss matters of syntax and axiomatics. All material is propositional, and will be so until Chapter 10. Justification logics are closely related to modal logics, so we start briefly with them in order to fix the basic notation. And just as normal modal logics all extend a single simplest example, K, all justification logics extend a single simplest example, J0 . We will begin our discussion with modal logics, then we will discuss the justification logic J0 in detail, and finally we will extend things to the most common and bestknown justification logics. A much broader family of justification logics will be discussed in Chapter 8.\n\n2.1 Modal Logics All propositional formulas throughout this book are built up from a countable family of propositional variables. We use P, Q, . . . as propositional variables, with subscripts if necessary, and we follow the usual convention that these are all distinct. As our main propositional connective we have implication, →. We have negation, ¬, which we will take as primitive, or defined using the propositional constant ⊥ representing falsehood, as convenient and appropriate at the time. We also use conjunction, ∧, disjunction, ∨, and equivalence, ↔, and these too may be primitive or defined depending on circumstances. We omit outer parentheses in formulas when it will do no harm. We usually have a single modal necessity operator. It will generally be represented by though in epistemic contexts it may be represented by K. A dual operator representing possibility, ♦, is a defined operator and actually plays little role here. There is much work on epistemic logics with multiple agents, and there is some study of justification counterparts for them. When 11\n\n12\n\nThe Basics of Justification Logic\n\ndiscussing these and their connections with modal logics, we will subscript the modal operators just described. To date, no justification logic corresponding to a nonnormal modal logic has been introduced, so only normal modal logics will appear here. A normal modal logic is a set of modal formulas that contains all tautologies and all formulas of the form (X → Y) → (X → Y) and is closed under uniform substitution, modus ponens, and necessitation (if X is present, so is X). The smallest normal modal logic is K; it is a subset of all normal modal logics. The logic K has a standard axiom system. Axioms are all tautologies (or enough of them) and all formulas of the form (X → Y) → (X → Y). Rules are Modus Ponens X, X → Y ⇒ Y and Necessitation X ⇒ X. We are not actually interested in the vast collection of normal modal logics, but only in those for which a Hilbert system exists, having an axiomatization using a finite set of axiom schemes. In practice, this means adding axiom schemes to the axiomatization for K just given. We assume everybody knows axiom systems like T, K4, S4, and so on. We will refer to such logics as axiomatically formulated. Of course semantics plays a big role in modal logics, but we postpone a discussion for the time being.\n\n2.2 Beginning Justification Logics Justification logics, syntactically, are like modal logics except that justification terms take the place of . Justification terms are intended to represent reasons or justifications for formulas. They have structure that encodes reasoning that has gone into them. We begin our formal presentation here. Definition 2.1 (Justification Term) up as follows.\n\nThe set Tm of justification terms is built\n\n(1) There is a set of justification variables, x, y, . . . , x1 , y1 , . . . . Every justification variable is a justification term. (2) There is a set of justification constants, a, b, . . . , a1 , b1 , . . . . Every justification constant is a justification term. (3) There are binary operation symbols, + and ·. If u and v are justification terms, so are (u + v) and (u · v). (4) There may be additional function symbols, f , g, . . . , f1 , g1 , . . . , of various arities. Which ones are present depends on the logic in question. If f is an nplace justification function symbol of the logic, and t1 , . . . , tn are justification terms, f (t1 , . . . , tn ) is a justification term.\n\n2.2 Beginning Justification Logics\n\n13\n\nNeither + nor · is assumed to be commutative or associative, and there is no distributive law. We do, however, allow ourselves the notational convenience of omitting parentheses with multiple occurrences of ·, assuming associativity to the left. Thus, for instance, a · b · c · d is short for (((a · b) · c) · d). We make the same assumption concerning +, though it actually plays a much lesser role. Also we will generally assume that · binds more strongly than +, writing a·b+c instead of (a · b) + c for instance. Definition 2.2 (Justification Formula) The set of justification formulas, Fm, is built in the usual recursive way, as follows. (1) There is a set Var of propositional variables, P, Q, . . . , P1 , Q1 , . . . (these are also known as propositional letters). Every propositional variable is a justification formula. (2) ⊥ (falsehood) is a justification formula. (3) If X and Y are justification formulas, so is (X → Y). (4) If t is a justification term and X is a justification formula, then t:X is a justification formula. We will sometimes use other propositional connectives, ∧, ∨, ↔, which we can think of as defined connectives, or primitive as convenient. Outer parentheses may be omitted in formulas if no confusion will result. If justification term t has a complex structure we generally will write [t]:X, using square brackets as a visual aid. Square brackets have no theoretical significance. In a modal formula, is supposed to express that something is necessary, or known, or obligatory, or some such thing, but it does not say why. A justification term encodes this missing information; it provides the why absent from modal formulas. This is what their structure is for. Justification variables stand for arbitrary justification terms, and substitution for them is examined beginning with Definition 2.17. Justification constants stand for reasons that are not further analyzed—typically they are reasons for axioms. Their role is discussed in more detail once constant specifications are introduced, in Definition 10.32. The · operation corresponds to modus ponens. If X → Y is so for reason s and X is so for reason t, then Y is so for reason s · t. (Reasons are not unique—Y may be true for other reasons too.) The + operation is a kind of weakening. If X is so for either reason s or reason t, then s + t is also a reason for X. Other operations on justification terms, if present, correspond to features peculiar to particular modal logics and will be discussed as they come up.\n\n14\n\nThe Basics of Justification Logic\n\n2.3 J0 , the Simplest Justification Logic As we will see, there are some justification logics having a version of a necessitation rule; there are others that do not. Some justification logics are closed under substitution of formulas for propositional variables, others are not. Allowing such a range of behavior is essential to enable us to capture and study the interactions of important features of modal logics that are sometimes hidden from us. But one consequence is, there is no good justification analog of the family of normal modal logics. Still, all justification logics have a common core, which we call J0 , and it is a kind of analog of the weakest normal modal logic, K, even though there is nothing structural we can point to as determining a “normal” justification logic apart from giving an axiomatization. In this section we present J0 axiomatically; subsequently we discuss what must be added to get the general family of justification logics. Definition 2.3 (Justification logic J0 ) The language of J0 has no justification function symbols beyond the basic two binary ones + and ·. The axiom schemes are as follows.\n\nClassical: All tautologies (or enough of them) Application: All formulas of the form s:(X → Y) → (t:X → [s · t]:Y) Sum: All formulas of the forms s:X → [s + t]:X and t:X → [s + t]:X The only J0 rule of inference is Modus Ponens, X, X → Y ⇒ Y. J0 is a very weak justification logic. It is, for instance, incapable of proving that any formula has a justification, see Section 3.2. Reasoning in J0 is analogous to reasoning in the modal logic K without a necessitation rule! What we can do in J0 is derive interesting facts about justifications provided we make explicit what other formulas we would need to have justifications for. We give an example to illustrate this. To help bring out the points we want to make, if (X1 ∧ . . . ∧ Xn ) → Y is provable in J0 we may write X1 , . . . , Xn `J0 Y. Order of formulas and placement of parentheses in the conjunction of the Xi don’t matter because we have classical logic to work with. In modal K, a common first example of a theorem is (X ∧Y) → (X ∧Y). Here is the closest we can come to this in J0 . Our presentation is very much abbreviated.\n\n2.4 Justification Logics in General\n\n15\n\nExample 2.4 Assume u, v, and w are justification variables. 1. 2. 3. 4. 5.\n\nu:((X ∧ Y) → X) → (w:(X ∧ Y) → [u · w]:X) v:((X ∧ Y) → Y) → (w:(X ∧ Y) → [v · w]:Y) (u:((X ∧ Y) → X) ∧ v:((X ∧ Y) → y)) → (w:(X ∧ Y) → [u · w]:X) (u:((X ∧ Y) → X) ∧ v:((X ∧ Y) → y)) → (w:(X ∧ Y) → [v · w]:Y) (u:((X ∧ Y) → X) ∧ v:((X ∧ Y) → y)) → (w:(X ∧ Y) → ([u · w]:X ∧ [v · w]:Y))\n\nApplication Axiom Application Axiom Classical Logic on 1, 2 Classical Logic on 1, 2 Classical Logic on 3, 4\n\nSo we have shown that u:((X ∧ Y) → X), v:((X ∧ Y) → Y) `J0 (w:(X ∧ Y) → ([u · w]:X ∧ [v · w]:Y)) which we can read as an analog of (X ∧Y) → (X ∧Y) as follows. In J0 , for any w there are justification terms t1 and t2 such that w:(X ∧ Y) → (t1:X ∧ t2:Y), provided we have justifications for the tautologies (X ∧ Y) → X and (X ∧ Y) → Y. Note that t1 = u · w and t2 = v · w are different terms. But, making use of the Sum Axiom scheme, these can be brought together as t1 + t2 = u · w + v · w. It is important to understand that justifications, when they exist, are not unique.\n\n2.4 Justification Logics in General The core justification logic J0 is extended to form other justification logics using two quite different types of machinery. First, one can add new operations on justification terms, besides the basic + and ·, along with axiom schemes governing their use, similar to Sum and Application. This is directly analogous to the way axiom schemes are added to K to create other modal logics. Second, one can specify which truths of logic we assume we have justifications for. This is related to the roles u:((X ∧ Y) → X) and v:((X ∧ Y) → Y) play in Example 2.4. We devote most of this section to the second kind of extension. It is, in fact, the intended role for justification constants that, up to now, have not been used for anything special. For the time being let us assume we have a justification logic resulting from the addition of function symbols and axiom schemes to J0 . The details don’t matter for now, but it should be understood that our axioms may go beyond those for J0 . Axioms of justification logics, like axioms generally, are simply assumed and are not analyzed further. The role of justification constant symbols is to\n\n16\n\nThe Basics of Justification Logic\n\nserve as reasons or justifications for axioms. If A is an axiom, we can simply announce that constant symbol c plays the role of a justification for it. It may be that some axioms are assumed to have such justifications, but not necessarily all. Suppose we look at Example 2.4 again, and suppose we have decided that (X ∧ Y) → X is an axiom for which we have a specific justification, let us say the constant symbol c plays this role. Similarly let us say the constant symbol d represents a justification for (X ∧ Y) → Y. Examining the derivation given in Example 2.4, it is easy to see that if we replace the variable u throughout by c, and the variable v throughout by d we still have a derivation, but one of c:((X ∧ Y) → X), d:((X ∧ Y) → Y) `J0 (w:(X ∧ Y) → ([c · w]:X ∧ [d · w]:Y)). If we add c:((X ∧ Y) → X) and d:((X ∧ Y) → Y) to our axioms for J0 , we can simply prove the formula (w:(X ∧ Y) → ([c · w]:X ∧ [d · w]:Y)). Roughly speaking, a constant specification tells us what axioms we have justifications for and which constants justify these axioms. As we just saw, we can use a constant specification as a source of additional axioms. But there is an important complication. If A is an axiom and constant symbol c justifies it, c:A conceptually also acts like an axiom, and it too may have its own justification. Then a constant symbol, say d, could come in here too, as a justification for c:A, and thus we might want to assume d:c:A. This repeats further, of course. For many purposes exact details don’t matter much, so how constants are used, and for what purposes, is turned into a kind of parameter of our logics, called a constant specification. Definition 2.5 (Constant Specification) A constant specification CS for a given justification logic is a set of formulas meeting the following conditions. (1) Members of CS are of the form cn:cn−1: . . . c1:A where n > 0, A is an axiom of JL, and each ci is a constant symbol. (2) If cn:cn−1: . . . c1:A is in CS where n > 1, then cn−1: . . . c1:A is in CS too. Thus CS contains all intermediate specifications for whatever it contains. One reason why constant specifications are treated as parameters can be discovered through a close look at Definition 2.3. It does not really provide an axiomatization for J0 , but rather a scheme for axiomatizations. The axioms called Classical in that definition are not fully specified, and in common practice many classical logic axiomatizations are in use. Any set sufficient to derive all tautologies will do. Then many different axiomatizations for J0 would meet the required conditions, and similarly for any justification logic extending J0\n\n2.4 Justification Logics in General\n\n17\n\nas well. Because constants are supposed to be associated with axioms, a variety of constant specifications come up naturally. And because details like this often matter very little, treating constant specifications as a parameter is quite reasonable. Definition 2.6 (Logic of Justifications with a Constant Specification) Let JL be a justification logic, resulting from the addition of function symbols to the language of J0 and corresponding axiom schemes to those of J0 . Let CS be a constant specification for JL. Then JL(CS) is the logic JL with members of CS added as axioms (not axiom schemes), still with modus ponens as the only rule of inference. Constant specifications allow for great flexibility. A constant specification could associate many constants with a single axiom, or none at all. Allowing for many could be of use in tracking where particular pieces of reasoning come from. Allowing none might be appropriate in dealing with axioms that have some simple form, say X → X, but where the size of X is astronomical. Or again we might want to use the same constant for all instances of a particular axiom schema, or we might want to keep the instances clearly distinguishable. If details don’t matter at all for some particular purpose, we might want to associate a single constant symbol with every axiom, no matter what the form. Such a constant would simply be a record that a formula is an axiom, without going into particulars. Some conditions on constant specifications have shown themselves to be of special interest and have been given names. Here is a list of the most common. There are others. Definition 2.7 (Constant Specification Conditions) Let CS be a constant specification for a justification logic JL. The following requirements may be placed on CS. Empty: CS = ∅. This amounts to working with JL itself. Epistemically one can think of it as appropriate for the reasoning of a completely skeptical agent. Finite: CS is a finite set of formulas. This is fully representative because any specific derivation in a Justification Logic will be finite and so will involve only a finite set of constants. Schematic: If A and B are both instances of the same axiom scheme, c:A ∈ CS if and only if c:B ∈ CS, for every constant symbol c. Total: For each axiom A of JL and any constants c1 , c2 , . . . , cn we have cn:cn−1: . . . c1:A ∈ CS.\n\n18\n\nThe Basics of Justification Logic\n\nAxiomatically Appropriate: For every axiom A and for every n > 0 there are constant symbols ci so that cn:cn−1: . . . c1:A ∈ CS. The working of justification axiom systems is specified as follows. Definition 2.8 (Consequence) Suppose JL is a justification logic, CS is a constant specification for JL, S is an arbitrary set of formulas (not schemes), and X is a single formula. By S `JL(CS) X we mean there is a finite sequence of formulas, ending with X, in which each formula is either a instance of an axiom scheme of JL, a member of CS, a member of S , or follows from earlier formulas by modus ponens. If {Y1 , . . . , Yk } `JL(CS) X we will simplify notation and write Y1 , . . . , Yk `JL(CS) X. If ∅ `JL(CS) X we just write `JL(CS) X, or sometimes even JL(CS) ` X. When presenting examples of axiomatic derivations using a constant speciCS fication CS, we will write c + X as a suggestive way of saying that c:X ∈ CS, and we will say “c justifies X”. We conclude this section with some examples of theorems of justification logics. For these we work with JL(CS) where JL is any justification logic and CS is any constant specification for it that is axiomatically appropriate. We assume JL has been axiomatized taking all tautologies as axioms, though taking “enough” would give similar results once we have Theorem 2.14. Example 2.9 P → P is a theorem of any normal modal logic. It has more than one proof. We could simply note that it is an instance of a tautology, X → X. Or we could begin with P → P, a simpler instance of this tautology, apply necessitation getting (P → P), and then use the K axiom (P → P) → (P → P) and modus ponens to conclude P → P. While these are different modal derivations, the result is the same. But when we mimic the steps in JL(CS), they lead to different results. Let t be an arbitrary justification term. Then t:P → t:P is a theorem of JL(CS) because it is an instance of a tautology. But also P → P is an instance of a tautology and so, because JL(CS) is assumed axiomatically appropriate, the constant specification assigns some constant to it; say c:(P → P) ∈ CS. Because c:(P → P) → (t:P → [c · t]:P) is an axiom, t:P → [c · t]:P follows by modus ponens. In justification logic, instead of a single formula P → P with two proofs we have two different theorems that contain traces of their proofs. Both t:P → t:P and t:P → [c · t]:P say that if there is a reason for P, then there is a reason for P, but they give us different reasons. One of the first things everybody shows axiomatically when studying modal\n\n2.4 Justification Logics in General\n\n19\n\nlogic is that (P ∧ Q) ↔ (P ∧ Q) is provable in K, and thus is provable in every axiom system for a normal modal logic. But the argument from left to right is quite different from the argument from right to left. Because justification theorems contain traces of their proofs, we should not expect a single justification analog of this modal equivalence, but rather separate results for the left–right implication and for the right–left implication. Example 2.10 Here is a justification derivation corresponding to the usual modal argument for (P ∧ Q) → (P ∧ Q). 1. 2. 3. 4. 5. 6. 7. 8. 9.\n\n(P ∧ Q) → P c:((P ∧ Q) → P) c:((P ∧ Q) → P) → (t:(P ∧ Q) → [c · t]:P) t:(P ∧ Q) → [c · t]:P (P ∧ Q) → Q d:((P ∧ Q) → Q) d:((P ∧ Q) → Q) → (t:(P ∧ Q) → [d · t]:Q) t:(P ∧ Q) → [d · t]:Q t:(P ∧ Q) → ([c · t]:P ∧ [d · t]:Q)\n\ntautology cons spec Application Axiom mod pon on 2, 3 tautology cons spec Application Axiom mod pon on 6, 7 class log on 4, 8\n\nCS Then t:(P ∧ Q) → ([c · t]:P ∧ [d · t]:Q) is a theorem of JL(CS) where c + CS ((P ∧ Q) → P) and d + ((P ∧ Q) → Q).\n\nExample 2.11 A justification counterpart of the modal theorem (P∧Q) → (P ∧ Q) follows. 1. 2. 3. 4. 5. 6.\n\nP → (Q → (P ∧ Q)) c:(P → (Q → (P ∧ Q))) c:(P → (Q → (P ∧ Q))) → (t:P → [c · t]:(Q → (P ∧ Q))) t:P → [c · t]:(Q → (P ∧ Q)) [c · t]:(Q → (P ∧ Q)) → (u:Q → [c · t · u]:(P ∧ Q)) (t:P ∧ u:Q) → [c · t · u]:(P ∧ Q)\n\ntautology cons spec Application Axiom mod pon on 2, 3 Application Axiom class log on 4, 5\n\nCS So (t:P ∧ u:Q) → [c · t · u]:(P ∧ Q) is a theorem of JL(CS) where c + (P → (Q → (P ∧ Q))).\n\nOur final example illustrates the use of +, which has not come up so far. It is for handling situations where there is more than one explanation needed for something, as in a proof by cases. At first glance this seems rather minor, but + turns out to play a vital role when we come to realization results. Example 2.12 (X ∨ Y) → (X ∨ Y) is a theorem of K with an elementary\n\n20\n\nThe Basics of Justification Logic\n\nproof that we omit. Let us construct a counterpart in JL(CS), still assuming that CS is axiomatically appropriate and all tautologies are axioms. 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. 11. 12. 13.\n\nX → (X ∨ Y) c:(X → (X ∨ Y)) c:(X → (X ∨ Y)) → (t:X → [c · t]:(X ∨ Y)) t:X → [c · t]:(X ∨ Y) Y → (X ∨ Y) d:(Y → (X ∨ Y)) d:(Y → (X ∨ Y)) → (u:Y → [d · u]:(X ∨ Y)) u:Y → [d · u]:(X ∨ Y) [c · t]:(X ∨ Y) → [c · t + d · u]:(X ∨ Y) [d · u]:(X ∨ Y) → [c · t + d · u]:(X ∨ Y) t:X → [c · t + d · u]:(X ∨ Y) u:Y → [c · t + d · u]:(X ∨ Y) (t:X ∨ u:Y) → [c · t + d · u]:(X ∨ Y)\n\ntautology cons spec Application Axiom mod pon on 2, 3 tautology cons spec Application Axiom mod pon on 6, 7 Sum Axiom Sum Axiom clas log on 4, 9 clas log on 8, 10 class log on 11, 12\n\nThe consequents of 4 and 8 both provide reasons for X ∨ Y, but the reasons are different. We have used + to combine them, getting a justification analog of (X ∨ Y) → (X ∨ Y).\n\n2.5 Fundamental Properties of Justification Logics All justification logics have certain common and useful properties. Some features are identical with those of classical logic; others have twists that are special to justification logics. This section is devoted to ones we will use over and over. Throughout this section let JL be a justification logic and CS be a constant specification for it. Because the only rule of inference is modus ponens the classical proof of the deduction theorem applies. We thus have S , X `JL(CS) Y if and only if S `JL(CS) X → Y. Because formal proofs are finite we have compactness that, combined with the deduction theorem, tells us: S `JL(CS) X if and only if `JL(CS) Y1 → (Y2 → . . . → (Yn → X) . . .) for some Y1 , Y2 , . . . , Yn ∈ S . These are exactly like their classical counterparts. Furthermore, the following serves as a replacement for the modal Necessitation Rule. Definition 2.13 (Internalization) JL has the internalization property relative to constant specification CS provided, if `JL(CS) X then for some justification term t, `JL(CS) t:X. In addition we say that JL has the strong internalization\n\n2.5 Fundamental Properties of Justification Logics\n\n21\n\nproperty if t contains no justification variables and no justification operation or function symbols except ·. That is, t is built up from justification constants using only ·. Theorem 2.14 If CS is an axiomatically appropriate constant specification for JL then JL has the strong internalization property relative to CS. Proof By induction on proof length. Suppose `JL(CS) X and the result is known for formulas with shorter proofs. If X is an axiom of JL or a member of CS, there is a justification constant c such that c:X is in CS, and so c:X is provable. If X follows from earlier proof lines by modus ponens from Y → X and Y then, by the induction hypothesis, `JL(CS) s:(Y → X) and `JL(CS) t:Y for some s, t containing no justification variables, and with · as the only function symbol. Using the J0 Application Axiom s:(Y → X) → (t:Y → [s · t]:X) and modus ponens, we get `JL(CS) [s · t]:X. If X is provable using an axiomatically appropriate constant specification so is t:X, and the term t constructed in the preceding proof actually internalizes the steps of the axiomatic proof of X, hence the name internalization. Of course different proofs of X will produce different justification terms. Here is an extremely simple example, but one that is already sufficient to illustrate this point. Example 2.15 Assume JL is a justification logic, CS is an axiomatically appropriate constant specification for it, and all tautologies are axioms of JL. CS P → P is a tautology so c + (P → P) for some c. Then c:(P → P) is a theorem, and we have the justification term c internalizing a proof of P → P. Here is a more roundabout proof of P → P, giving us a more complicated internalizing term. Following the method in the proof of Theorem 2.14, we construct the internalization simultaneously. 1. 2. 3. 4. 5.\n\n(P → (P → P)) → ((P → P) → (P → P)) P → (P → P) (P → P) → (P → P) P→P P→P\n\ntautology tautology mod pon on 1, 2 tautology mod pon on 3, 4\n\nd (cons spec) e (cons spec) d·e c (cons spec) d·e·c\n\nThis time we get a justification term d · e · c, or more properly (d · e) · c, CS internalizing a proof of P → P, where c + ((P → (P → P)) → ((P → P) → CS CS (P → P))), e + (P → (P → P)), and c + (P → P). The problem of finding a “simplest” justification term is related to the problem of finding the “simplest” proof of a provable formula. It is not entirely clear what this actually means.\n\n22\n\nThe Basics of Justification Logic\n\nCorollary 2.16 (Lifting Lemma) Suppose JL is a justification logic that has the internalization property relative to CS (in particular, if CS is axiomatically appropriate). If X1 , . . . , Xn `JL(CS) Y then for any justification terms t1 , . . . , tn there is a justification term u so that t1:X1 , . . . , tn:Xn `JL(CS) u:Y. Proof The proof is by induction on n. If n = 0 this is simply the definition of Internalization. Suppose the result is known for n, and we have X1 , . . . , Xn , Xn+1 `JL(CS) Y. We show that for any t1 , . . . , tn , tn+1 there is some u so that t1:X1 , . . . , tn:Xn , tn+1: Xn+1 `JL(CS) u:Y. Using the deduction theorem, X1 , . . . , Xn `JL(CS) (Xn+1 → Y). By the induction hypothesis, for some v we have t1:X1 , . . . , tn:Xn `JL(CS) v:(Xn+1 → Y). Now v:(Xn+1 → Y) → (tn+1:Xn+1 → [v · tn+1 ]:Y) is an axiom hence t1:X1 , . . ., tn:Xn `JL(CS) (tn+1:Xn+1 → [v · tn+1 ]:Y). By modus ponens, t1:X1 , . . . , tn:Xn , tn+1:Xn+1 `JL(CS) [v · tn+1 ]:Y, so take u to be v · tn+1 . Next we move on to the role of justification variables. We said earlier, rather informally, that variables stood for arbitrary justification terms. In order to make this somewhat more precise, we need to introduce substitution. Definition 2.17 (Substitution) A substitution is a function σ mapping some set of justification variables to justification terms, with no variable in the domain of σ mapping to itself. We are only interested in substitutions with finite domain. If the domain of σ is {x1 , . . . , xn }, and each xi maps to justification term ti , it is standard to represent this substitution by (x1 /t1 , . . . , xn /tn ), or sometimes as (~x/~t). For a justification formula X the result of applying a substitution σ is denoted Xσ; likewise tσ is the result of applying substitution σ to justification term t. Substitutions map axioms of a justification logic into axioms (because axiomatization is by schemes), and they preserve modus ponens applications. But one must be careful because the role of constants changes with a substitution. Suppose CS is a constant specification, A is an axiom, and c:A is added to a proof where this addition is authorized by CS. Because axiomatization is by schemes Aσ is also an axiom, but if we add c:Aσ to a proof this may no longer meet constant specification CS. A new constant specification, call it (CS)σ, can be computed from the original one: put c:Aσ ∈ (CS)σ just in case c:A ∈ CS, for any c. If CS was axiomatically appropriate, CS ∪ (CS)σ will also be. So, if X is provable using an axiomatically appropriate constant specification CS, the same will be true for Xσ, not using the original constant specification but rather using CS ∪ (CS)σ. But this is more detail than we generally need to care about. The following suffices for much of our purposes.\n\n2.6 The First Justification Logics\n\n23\n\nTheorem 2.18 (Substitution Closure) Suppose JL is a justification logic and X is provable in JL using some (axiomatically appropriate) constant specification. Then for any substitution σ, Xσ is also provable in JL using some (axiomatically appropriate) constant specification. We introduce some special notation that suppresses details of constant specifications when we don’t need to care about these details. Definition 2.19 Let JL be a justification logic. We write `JL X as short for: there is some axiomatically appropriate constant specification CS so that `JL(CS) X. Theorem 2.20 Let JL be a justification logic. (1) If `JL X then `JL Xσ for any substitution σ. (2) If `JL X and `JL X → Y then `JL Y. Proof Item (1) is directly from Theorem 2.18. For item (2), suppose `JL X and `JL X → Y. Then there are axiomatically appropriate constant specifications CS1 and CS2 so that `JL(CS1 ) X and `JL(CS2 ) X → Y. Now CS1 ∪ CS2 will also be an axiomatically appropriate constant specification and `JL(CS1 ∪CS2 ) X and `JL(CS1 ∪CS2 ) X → Y, so `JL(CS1 ∪CS2 ) Y and hence `JL Y. In fact, it is easy to check that `JL X if and only if `JL(T) X, where T is the total constant specification. This gives an alternate, and easier, characterization.\n\n2.6 The First Justification Logics In this section and the next we present a number of specific examples of justification logics. We have tried to be systematic in naming these justification logics. Of course modal logic is not entirely consistent in this respect, and justification logic inherits some of its quirks, but we have tried to minimize anomalies. Naming Conventions: It is common to name modal logics by stringing axiom names after K; for instance KT, K4, and so on, with K itself as the simplest case. When we have justification logic counterparts for such modal logics, we will use the same name except with a substitution of J for K; for instance JT, J4, and so on. There is a problem here because a modal logic generally has more than one justification counterpart (if it has any). We will specify which one we have in mind. Formally, JT, J4, and so on result from the addition of axiom schemes, justification function symbols, and a constant specification to\n\n24\n\nThe Basics of Justification Logic\n\nJ0 . When details of a constant specification matter, we will write things like JT(CS), J4(CS), and so on, making the constant specification explicit. We will rarely refer to J0 again because its definition does not actually allow for a constant specification. From now on we will use J for J0 extended with some constant specification, and we will write J(CS) when explicitness is called for. Note that J0 can be thought of as J(∅), where ∅ is the empty constant\n\nspecification. The general subject of justification logics evolved from the aforementioned G¨odel–Artemov project, which embeds intuitionistic logic into the modal logic S4, which in turn embeds into the justification logic known as LP (for logic of proofs). It is with LP and its standard sublogics that we are concerned in this section. These are the best-known justification logics, just as K, T (or sometimes KT), S4 (or sometimes KT4), and a few others are the best-known modal logics. For the time being the notion of a justification logic being a counterpart of a modal logic will be an intuitive one. A proper definition will be given in Section 7.2. With two exceptions, the justification logics examined here arise by adding additional operations to the + and · common to all justification logics. The first exception involves factivity, with which we begin. Factivity for modal logics is represented by the axiom scheme X → X. If we think of the necessity operator epistemically, this would be written KX → X. It asserts that if X is known, then X is so. The justification counterpart is the following axiom scheme. Factivity t:X → X Factivity is a strong assumption: justifications cannot be wrong. Nonetheless, if the justification is a mathematical proof, factivity is something mathematicians are generally convinced of. If we think of knowledge as justified, true belief, factivity is built in. Philosophers generally understand justified, true belief to be inherent in knowledge, but not sufficient, see Gettier (1963). The modal axiom scheme X → X is called T. The weakest normal modal logic including all instances of this scheme is KT, sometimes abbreviated simply as T. We use JT for J plus Factivity and, as noted earlier, we use JT(CS) when a specific constant specification is needed. Note that the languages of JT and J are the same. There is one more such example, after which additional operation symbols must be brought in. Consistency is an important special case of Factivity. Modally it can be represented in several ways. One can assume the axiom scheme X → ♦X. In\n\n2.6 The First Justification Logics\n\n25\n\nany normal modal logic this turns out to coincide with assuming ¬⊥ (where ⊥ represents falsehood), or equivalently ⊥ → ⊥, which is a very special instance of X → X. If one thinks of as representing provability, ¬⊥ says falsehood is not provable—consistency. Suppose one thinks of deontically, so that X is read that X is obligatory, or perhaps that it is obligatory to bring about a state in which X. Then X → ♦X, or equivalently X → ¬¬X says that if X is obligatory, then ¬X isn’t—a plausible condition on obligations. It is because of this interesting deontic reading that any of the equivalent versions is commonly called D, standing for deontic. Any of these has a justification counterpart. We adopt the following version. Consistency t:⊥ → ⊥ JD is J plus Consistency. Note that JT extends JD.\n\nPositive Introspection is a common assumption about an agent’s knowledge: If an agent knows something, the agent knows that it is known; an agent can introspect about the knowledge he or she possesses. In logics of knowledge it is formulated as KX → KKX. If one understands as representing provability in formal arithmetic, it is possible to prove that a proof is correct: X → X. To formulate a justification logic counterpart, Artemov introduced a one-place function symbol on justification terms, denoted ! and written in prefix position. The intuitive idea is that if t is a justification of something, !t is a justification that t is, indeed, such a justification. Note that the basic language of justification logics has been extended, and this must be reflected in any constant speci"
    }
}
{
    "id": "dbpedia_2610_2",
    "rank": 78,
    "data": {
        "url": "https://worldwidescience.org/topicpages/h/high%2Bperformance%2Bcluster.html",
        "read_more_link": "",
        "language": "en",
        "title": "high performance cluster: Topics by WorldWideScience.org",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://worldwidescience.org/sites/www.osti.gov/files/public/image-files/WWSlogo_wTag650px-min.png",
            "https://worldwidescience.org/topicpages/h/images/arrow-up.gif",
            "https://worldwidescience.org/topicpages/h/images/arrow-down.gif",
            "https://worldwidescience.org/sites/www.osti.gov/files/public/image-files/OSTIlogo.svg",
            "https://worldwidescience.org/sites/www.osti.gov/files/public/image-files/ICSTIlogo.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Improving the Eco-Efficiency of High Performance Computing Clusters Using EECluster\n\nDirectory of Open Access Journals (Sweden)\n\nAlberto CocaÃ±a-FernÃ¡ndez\n\n2016-03-01\n\nFull Text Available As data and supercomputing centres increase their performance to improve service quality and target more ambitious challenges every day, their carbon footprint also continues to grow, and has already reached the magnitude of the aviation industry. Also, high power consumptions are building up to a remarkable bottleneck for the expansion of these infrastructures in economic terms due to the unavailability of sufficient energy sources. A substantial part of the problem is caused by current energy consumptions of High Performance Computing (HPC clusters. To alleviate this situation, we present in this work EECluster, a tool that integrates with multiple open-source Resource Management Systems to significantly reduce the carbon footprint of clusters by improving their energy efficiency. EECluster implements a dynamic power management mechanism based on Computational Intelligence techniques by learning a set of rules through multi-criteria evolutionary algorithms. This approach enables cluster operators to find the optimal balance between a reduction in the cluster energy consumptions, service quality, and number of reconfigurations. Experimental studies using both synthetic and actual workloads from a real world cluster support the adoption of this tool to reduce the carbon footprint of HPC clusters.\n\nFPGA cluster for high-performance AO real-time control system\n\nScience.gov (United States)\n\nGeng, Deli; Goodsell, Stephen J.; Basden, Alastair G.; Dipper, Nigel A.; Myers, Richard M.; Saunter, Chris D.\n\n2006-06-01\n\nWhilst the high throughput and low latency requirements for the next generation AO real-time control systems have posed a significant challenge to von Neumann architecture processor systems, the Field Programmable Gate Array (FPGA) has emerged as a long term solution with high performance on throughput and excellent predictability on latency. Moreover, FPGA devices have highly capable programmable interfacing, which lead to more highly integrated system. Nevertheless, a single FPGA is still not enough: multiple FPGA devices need to be clustered to perform the required subaperture processing and the reconstruction computation. In an AO real-time control system, the memory bandwidth is often the bottleneck of the system, simply because a vast amount of supporting data, e.g. pixel calibration maps and the reconstruction matrix, need to be accessed within a short period. The cluster, as a general computing architecture, has excellent scalability in processing throughput, memory bandwidth, memory capacity, and communication bandwidth. Problems, such as task distribution, node communication, system verification, are discussed.\n\nEnabling Diverse Software Stacks on Supercomputers using High Performance Virtual Clusters.\n\nEnergy Technology Data Exchange (ETDEWEB)\n\nYounge, Andrew J. [Sandia National Lab. (SNL-NM), Albuquerque, NM (United States); Pedretti, Kevin [Sandia National Lab. (SNL-NM), Albuquerque, NM (United States); Grant, Ryan [Sandia National Lab. (SNL-NM), Albuquerque, NM (United States); Brightwell, Ron [Sandia National Lab. (SNL-NM), Albuquerque, NM (United States)\n\n2017-05-01\n\nWhile large-scale simulations have been the hallmark of the High Performance Computing (HPC) community for decades, Large Scale Data Analytics (LSDA) workloads are gaining attention within the scientific community not only as a processing component to large HPC simulations, but also as standalone scientific tools for knowledge discovery. With the path towards Exascale, new HPC runtime systems are also emerging in a way that differs from classical distributed com- puting models. However, system software for such capabilities on the latest extreme-scale DOE supercomputing needs to be enhanced to more appropriately support these types of emerging soft- ware ecosystems. In this paper, we propose the use of Virtual Clusters on advanced supercomputing resources to enable systems to support not only HPC workloads, but also emerging big data stacks. Specifi- cally, we have deployed the KVM hypervisor within Cray's Compute Node Linux on a XC-series supercomputer testbed. We also use libvirt and QEMU to manage and provision VMs directly on compute nodes, leveraging Ethernet-over-Aries network emulation. To our knowledge, this is the first known use of KVM on a true MPP supercomputer. We investigate the overhead our solution using HPC benchmarks, both evaluating single-node performance as well as weak scaling of a 32-node virtual cluster. Overall, we find single node performance of our solution using KVM on a Cray is very efficient with near-native performance. However overhead increases by up to 20% as virtual cluster size increases, due to limitations of the Ethernet-over-Aries bridged network. Furthermore, we deploy Apache Spark with large data analysis workloads in a Virtual Cluster, ef- fectively demonstrating how diverse software ecosystems can be supported by High Performance Virtual Clusters.\n\nThe high performance cluster computing system for BES offline data analysis\n\nInternational Nuclear Information System (INIS)\n\nSun Yongzhao; Xu Dong; Zhang Shaoqiang; Yang Ting\n\n2004-01-01\n\nA high performance cluster computing system (EPCfarm) is introduced, which used for BES offline data analysis. The setup and the characteristics of the hardware and software of EPCfarm are described. The PBS, a queue management package, and the performance of EPCfarm is presented also. (authors)\n\nSpiking neural networks on high performance computer clusters\n\nScience.gov (United States)\n\nChen, Chong; Taha, Tarek M.\n\n2011-09-01\n\nIn this paper we examine the acceleration of two spiking neural network models on three clusters of multicore processors representing three categories of processors: x86, STI Cell, and NVIDIA GPGPUs. The x86 cluster utilized consists of 352 dualcore AMD Opterons, the Cell cluster consists of 320 Sony Playstation 3s, while the GPGPU cluster contains 32 NVIDIA Tesla S1070 systems. The results indicate that the GPGPU platform can dominate in performance compared to the Cell and x86 platforms examined. From a cost perspective, the GPGPU is more expensive in terms of neuron/s throughput. If the cost of GPGPUs go down in the future, this platform will become very cost effective for these models.\n\nHow to build a high-performance compute cluster for the Grid\n\nCERN Document Server\n\nReinefeld, A\n\n2001-01-01\n\nThe success of large-scale multi-national projects like the forthcoming analysis of the LHC particle collision data at CERN relies to a great extent on the ability to efficiently utilize computing and data-storage resources at geographically distributed sites. Currently, much effort is spent on the design of Grid management software (Datagrid, Globus, etc.), while the effective integration of computing nodes has been largely neglected up to now. This is the focus of our work. We present a framework for a high- performance cluster that can be used as a reliable computing node in the Grid. We outline the cluster architecture, the management of distributed data and the seamless integration of the cluster into the Grid environment. (11 refs).\n\nTrajectories of Symptom Clusters, Performance Status, and Quality of Life During Concurrent Chemoradiotherapy in Patients With High-Grade Brain Cancers.\n\nScience.gov (United States)\n\nKim, Sang-Hee; Byun, Youngsoon\n\nSymptom clusters must be identified in patients with high-grade brain cancers for effective symptom management during cancer-related therapy. The aims of this study were to identify symptom clusters in patients with high-grade brain cancers and to determine the relationship of each cluster with the performance status and quality of life (QOL) during concurrent chemoradiotherapy (CCRT). Symptoms were assessed using the Memorial Symptom Assessment Scale, and the performance status was evaluated using the Karnofsky Performance Scale. Quality of life was assessed using the Functional Assessment of Cancer Therapy-General. This prospective longitudinal survey was conducted before CCRT and at 2 to 3 weeks and 4 to 6 weeks after the initiation of CCRT. A total of 51 patients with newly diagnosed primary malignant brain cancer were included. Six symptom clusters were identified, and 2 symptom clusters were present at each time point (ie, \"negative emotion\" and \"neurocognitive\" clusters before CCRT, \"negative emotion and decreased vitality\" and \"gastrointestinal and decreased sensory\" clusters at 2-3 weeks, and \"body image and decreased vitality\" and \"gastrointestinal\" clusters at 4-6 weeks). The symptom clusters at each time point demonstrated a significant relationship with the performance status or QOL. Differences were observed in symptom clusters in patients with high-grade brain cancers during CCRT. In addition, the symptom clusters were correlated with the performance status and QOL of patients, and these effects could change during CCRT. The results of this study will provide suggestions for interventions to treat or prevent symptom clusters in patients with high-grade brain cancer during CCRT.\n\nPerformance of space charge simulations using High Performance Computing (HPC) cluster\n\nCERN Document Server\n\nBartosik, Hannes; CERN. Geneva. ATS Department\n\n2017-01-01\n\nIn 2016 a collaboration agreement between CERN and Istituto Nazionale di Fisica Nucleare (INFN) through its Centro Nazionale Analisi Fotogrammi (CNAF, Bologna) was signed [1], which foresaw the purchase and installation of a cluster of 20 nodes with 32 cores each, connected with InfiniBand, at CNAF for the use of CERN members to develop parallelized codes as well as conduct massive simulation campaigns with the already available parallelized tools. As outlined in [1], after the installation and the set up of the first 12 nodes, the green light to proceed with the procurement and installation of the next 8 nodes can be given only after successfully passing an acceptance test based on two specific benchmark runs. This condition is necessary to consider the first batch of the cluster operational and complying with the desired performance specifications. In this brief note, we report the results of the above mentioned acceptance test.\n\nA high performance image processing platform based on CPU-GPU heterogeneous cluster with parallel image reconstroctions for micro-CT\n\nInternational Nuclear Information System (INIS)\n\nDing Yu; Qi Yujin; Zhang Xuezhu; Zhao Cuilan\n\n2011-01-01\n\nIn this paper, we report the development of a high-performance image processing platform, which is based on CPU-GPU heterogeneous cluster. Currently, it consists of a Dell Precision T7500 and HP XW8600 workstations with parallel programming and runtime environment, using the message-passing interface (MPI) and CUDA (Compute Unified Device Architecture). We succeeded in developing parallel image processing techniques for 3D image reconstruction of X-ray micro-CT imaging. The results show that a GPU provides a computing efficiency of about 194 times faster than a single CPU, and the CPU-GPU clusters provides a computing efficiency of about 46 times faster than the CPU clusters. These meet the requirements of rapid 3D image reconstruction and real time image display. In conclusion, the use of CPU-GPU heterogeneous cluster is an effective way to build high-performance image processing platform. (authors)\n\nThe performance of a new Geant4 Bertini intra-nuclear cascade model in high throughput computing (HTC) cluster architecture\n\nEnergy Technology Data Exchange (ETDEWEB)\n\nAatos, Heikkinen; Andi, Hektor; Veikko, Karimaki; Tomas, Linden [Helsinki Univ., Institute of Physics (Finland)\n\n2003-07-01\n\nWe study the performance of a new Bertini intra-nuclear cascade model implemented in the general detector simulation tool-kit Geant4 with a High Throughput Computing (HTC) cluster architecture. A 60 node Pentium III open-Mosix cluster is used with the Mosix kernel performing automatic process load-balancing across several CPUs. The Mosix cluster consists of several computer classes equipped with Windows NT workstations that automatically boot, daily and become nodes of the Mosix cluster. The models included in our study are a Bertini intra-nuclear cascade model with excitons, consisting of a pre-equilibrium model, a nucleus explosion model, a fission model and an evaporation model. The speed and accuracy obtained for these models is presented. (authors)\n\nAcademic Performance and Lifestyle Behaviors in Australian School Children: A Cluster Analysis.\n\nScience.gov (United States)\n\nDumuid, Dorothea; Olds, Timothy; MartÃ­n-FernÃ¡ndez, Josep-Antoni; Lewis, Lucy K; Cassidy, Leah; Maher, Carol\n\n2017-12-01\n\nPoor academic performance has been linked with particular lifestyle behaviors, such as unhealthy diet, short sleep duration, high screen time, and low physical activity. However, little is known about how lifestyle behavior patterns (or combinations of behaviors) contribute to children's academic performance. We aimed to compare academic performance across clusters of children with common lifestyle behavior patterns. We clustered participants (Australian children aged 9-11 years, n = 284) into four mutually exclusive groups of distinct lifestyle behavior patterns, using the following lifestyle behaviors as cluster inputs: light, moderate, and vigorous physical activity; sedentary behavior and sleep, derived from 24-hour accelerometry; self-reported screen time and diet. Differences in academic performance (measured by a nationally administered standardized test) were detected across the clusters, with scores being lowest in the Junk Food Screenies cluster (unhealthy diet/high screen time) and highest in the Sitters cluster (high nonscreen sedentary behavior/low physical activity). These findings suggest that reduction in screen time and an improved diet may contribute positively to academic performance. While children with high nonscreen sedentary time performed better academically in this study, they also accumulated low levels of physical activity. This warrants further investigation, given the known physical and mental benefits of physical activity.\n\nSTEMsalabim: A high-performance computing cluster friendly code for scanning transmission electron microscopy image simulations of thin specimens\n\nInternational Nuclear Information System (INIS)\n\nOelerich, Jan Oliver; Duschek, Lennart; Belz, JÃ¼rgen; Beyer, Andreas; Baranovskii, Sergei D.; Volz, Kerstin\n\n2017-01-01\n\nHighlights: â¢ We present STEMsalabim, a modern implementation of the multislice algorithm for simulation of STEM images. â¢ Our package is highly parallelizable on high-performance computing clusters, combining shared and distributed memory architectures. â¢ With STEMsalabim, computationally and memory expensive STEM image simulations can be carried out within reasonable time. - Abstract: We present a new multislice code for the computer simulation of scanning transmission electron microscope (STEM) images based on the frozen lattice approximation. Unlike existing software packages, the code is optimized to perform well on highly parallelized computing clusters, combining distributed and shared memory architectures. This enables efficient calculation of large lateral scanning areas of the specimen within the frozen lattice approximation and fine-grained sweeps of parameter space.\n\nSTEMsalabim: A high-performance computing cluster friendly code for scanning transmission electron microscopy image simulations of thin specimens\n\nEnergy Technology Data Exchange (ETDEWEB)\n\nOelerich, Jan Oliver, E-mail: jan.oliver.oelerich@physik.uni-marburg.de; Duschek, Lennart; Belz, JÃ¼rgen; Beyer, Andreas; Baranovskii, Sergei D.; Volz, Kerstin\n\n2017-06-15\n\nHighlights: â¢ We present STEMsalabim, a modern implementation of the multislice algorithm for simulation of STEM images. â¢ Our package is highly parallelizable on high-performance computing clusters, combining shared and distributed memory architectures. â¢ With STEMsalabim, computationally and memory expensive STEM image simulations can be carried out within reasonable time. - Abstract: We present a new multislice code for the computer simulation of scanning transmission electron microscope (STEM) images based on the frozen lattice approximation. Unlike existing software packages, the code is optimized to perform well on highly parallelized computing clusters, combining distributed and shared memory architectures. This enables efficient calculation of large lateral scanning areas of the specimen within the frozen lattice approximation and fine-grained sweeps of parameter space.\n\nA High Performance Multi-Core FPGA Implementation for 2D Pixel Clustering for the ATLAS Fast TracKer (FTK) Processor\n\nCERN Document Server\n\nSotiropoulou, C-L; The ATLAS collaboration; Beretta, M; Gkaitatzis, S; Kordas, K; Nikolaidis, S; Petridou, C; Volpi, G\n\n2014-01-01\n\nThe high performance multi-core 2D pixel clustering FPGA implementation used for the input system of the ATLAS Fast TracKer (FTK) processor is presented. The input system for the FTK processor will receive data from the Pixel and micro-strip detectors read out drivers (RODs) at 760Gbps, the full rate of level 1 triggers. Clustering is required as a method to reduce the high rate of the received data before further processing, as well as to determine the cluster centroid for obtaining obtain the best spatial measurement. Our implementation targets the pixel detectors and uses a 2D-clustering algorithm that takes advantage of a moving window technique to minimize the logic required for cluster identification. The design is fully generic and the cluster detection window size can be adjusted for optimizing the cluster identification process. Î¤he implementation can be parallelized by instantiating multiple cores to identify different clusters independently thus exploiting more FPGA resources. This flexibility mak...\n\nHigh-Performance, Multi-Node File Copies and Checksums for Clustered File Systems\n\nScience.gov (United States)\n\nKolano, Paul Z.; Ciotti, Robert B.\n\n2012-01-01\n\nModern parallel file systems achieve high performance using a variety of techniques, such as striping files across multiple disks to increase aggregate I/O bandwidth and spreading disks across multiple servers to increase aggregate interconnect bandwidth. To achieve peak performance from such systems, it is typically necessary to utilize multiple concurrent readers/writers from multiple systems to overcome various singlesystem limitations, such as number of processors and network bandwidth. The standard cp and md5sum tools of GNU coreutils found on every modern Unix/Linux system, however, utilize a single execution thread on a single CPU core of a single system, and hence cannot take full advantage of the increased performance of clustered file systems. Mcp and msum are drop-in replacements for the standard cp and md5sum programs that utilize multiple types of parallelism and other optimizations to achieve maximum copy and checksum performance on clustered file systems. Multi-threading is used to ensure that nodes are kept as busy as possible. Read/write parallelism allows individual operations of a single copy to be overlapped using asynchronous I/O. Multinode cooperation allows different nodes to take part in the same copy/checksum. Split-file processing allows multiple threads to operate concurrently on the same file. Finally, hash trees allow inherently serial checksums to be performed in parallel. Mcp and msum provide significant performance improvements over standard cp and md5sum using multiple types of parallelism and other optimizations. The total speed-ups from all improvements are significant. Mcp improves cp performance over 27x, msum improves md5sum performance almost 19x, and the combination of mcp and msum improves verified copies via cp and md5sum by almost 22x. These improvements come in the form of drop-in replacements for cp and md5sum, so are easily used and are available for download as open source software at http://mutil.sourceforge.net.\n\nClustering at high redshifts\n\nInternational Nuclear Information System (INIS)\n\nShaver, P.A.\n\n1986-01-01\n\nEvidence for clustering of and with high-redshift QSOs is discussed. QSOs of different redshifts show no clustering, but QSOs of similar redshifts appear to be clustered on a scale comparable to that of galaxies at the present epoch. In addition, spectroscopic studies of close pairs of QSOs indicate that QSOs are surrounded by a relatively high density of absorbing matter, possibly clusters of galaxies\n\nHIGH PERFORMANCE PHOTOGRAMMETRIC PROCESSING ON COMPUTER CLUSTERS\n\nDirectory of Open Access Journals (Sweden)\n\nV. N. Adrov\n\n2012-07-01\n\nFull Text Available Most cpu consuming tasks in photogrammetric processing can be done in parallel. The algorithms take independent bits as input and produce independent bits as output. The independence of bits comes from the nature of such algorithms since images, stereopairs or small image blocks parts can be processed independently. Many photogrammetric algorithms are fully automatic and do not require human interference. Photogrammetric workstations can perform tie points measurements, DTM calculations, orthophoto construction, mosaicing and many other service operations in parallel using distributed calculations. Distributed calculations save time reducing several days calculations to several hours calculations. Modern trends in computer technology show the increase of cpu cores in workstations, speed increase in local networks, and as a result dropping the price of the supercomputers or computer clusters that can contain hundreds or even thousands of computing nodes. Common distributed processing in DPW is usually targeted for interactive work with a limited number of cpu cores and is not optimized for centralized administration. The bottleneck of common distributed computing in photogrammetry can be in the limited lan throughput and storage performance, since the processing of huge amounts of large raster images is needed.\n\nClustering high dimensional data\n\nDEFF Research Database (Denmark)\n\nAssent, Ira\n\n2012-01-01\n\nHigh-dimensional data, i.e., data described by a large number of attributes, pose specific challenges to clustering. The so-called âcurse of dimensionalityâ, coined originally to describe the general increase in complexity of various computational problems as dimensionality increases, is known...... to render traditional clustering algorithms ineffective. The curse of dimensionality, among other effects, means that with increasing number of dimensions, a loss of meaningful differentiation between similar and dissimilar objects is observed. As high-dimensional objects appear almost alike, new approaches...... for clustering are required. Consequently, recent research has focused on developing techniques and clustering algorithms specifically for high-dimensional data. Still, open research issues remain. Clustering is a data mining task devoted to the automatic grouping of data based on mutual similarity. Each cluster...\n\nPerformance Analysis of Cluster Formation in Wireless Sensor Networks.\n\nScience.gov (United States)\n\nMontiel, Edgar Romo; Rivero-Angeles, Mario E; Rubino, Gerardo; Molina-Lozano, Heron; Menchaca-Mendez, Rolando; Menchaca-Mendez, Ricardo\n\n2017-12-13\n\nClustered-based wireless sensor networks have been extensively used in the literature in order to achieve considerable energy consumption reductions. However, two aspects of such systems have been largely overlooked. Namely, the transmission probability used during the cluster formation phase and the way in which cluster heads are selected. Both of these issues have an important impact on the performance of the system. For the former, it is common to consider that sensor nodes in a clustered-based Wireless Sensor Network (WSN) use a fixed transmission probability to send control data in order to build the clusters. However, due to the highly variable conditions experienced by these networks, a fixed transmission probability may lead to extra energy consumption. In view of this, three different transmission probability strategies are studied: optimal, fixed and adaptive. In this context, we also investigate cluster head selection schemes, specifically, we consider two intelligent schemes based on the fuzzy C-means and k-medoids algorithms and a random selection with no intelligence. We show that the use of intelligent schemes greatly improves the performance of the system, but their use entails higher complexity and selection delay. The main performance metrics considered in this work are energy consumption, successful transmission probability and cluster formation latency. As an additional feature of this work, we study the effect of errors in the wireless channel and the impact on the performance of the system under the different transmission probability schemes.\n\nPerformance Analysis of Cluster Formation in Wireless Sensor Networks\n\nDirectory of Open Access Journals (Sweden)\n\nEdgar Romo Montiel\n\n2017-12-01\n\nFull Text Available Clustered-based wireless sensor networks have been extensively used in the literature in order to achieve considerable energy consumption reductions. However, two aspects of such systems have been largely overlooked. Namely, the transmission probability used during the cluster formation phase and the way in which cluster heads are selected. Both of these issues have an important impact on the performance of the system. For the former, it is common to consider that sensor nodes in a clustered-based Wireless Sensor Network (WSN use a fixed transmission probability to send control data in order to build the clusters. However, due to the highly variable conditions experienced by these networks, a fixed transmission probability may lead to extra energy consumption. In view of this, three different transmission probability strategies are studied: optimal, fixed and adaptive. In this context, we also investigate cluster head selection schemes, specifically, we consider two intelligent schemes based on the fuzzy C-means and k-medoids algorithms and a random selection with no intelligence. We show that the use of intelligent schemes greatly improves the performance of the system, but their use entails higher complexity and selection delay. The main performance metrics considered in this work are energy consumption, successful transmission probability and cluster formation latency. As an additional feature of this work, we study the effect of errors in the wireless channel and the impact on the performance of the system under the different transmission probability schemes.\n\nJMS: An Open Source Workflow Management System and Web-Based Cluster Front-End for High Performance Computing.\n\nScience.gov (United States)\n\nBrown, David K; Penkler, David L; Musyoka, Thommas M; Bishop, Ãzlem Tastan\n\n2015-01-01\n\nComplex computational pipelines are becoming a staple of modern scientific research. Often these pipelines are resource intensive and require days of computing time. In such cases, it makes sense to run them over high performance computing (HPC) clusters where they can take advantage of the aggregated resources of many powerful computers. In addition to this, researchers often want to integrate their workflows into their own web servers. In these cases, software is needed to manage the submission of jobs from the web interface to the cluster and then return the results once the job has finished executing. We have developed the Job Management System (JMS), a workflow management system and web interface for high performance computing (HPC). JMS provides users with a user-friendly web interface for creating complex workflows with multiple stages. It integrates this workflow functionality with the resource manager, a tool that is used to control and manage batch jobs on HPC clusters. As such, JMS combines workflow management functionality with cluster administration functionality. In addition, JMS provides developer tools including a code editor and the ability to version tools and scripts. JMS can be used by researchers from any field to build and run complex computational pipelines and provides functionality to include these pipelines in external interfaces. JMS is currently being used to house a number of bioinformatics pipelines at the Research Unit in Bioinformatics (RUBi) at Rhodes University. JMS is an open-source project and is freely available at https://github.com/RUBi-ZA/JMS.\n\nJMS: An Open Source Workflow Management System and Web-Based Cluster Front-End for High Performance Computing.\n\nDirectory of Open Access Journals (Sweden)\n\nDavid K Brown\n\nFull Text Available Complex computational pipelines are becoming a staple of modern scientific research. Often these pipelines are resource intensive and require days of computing time. In such cases, it makes sense to run them over high performance computing (HPC clusters where they can take advantage of the aggregated resources of many powerful computers. In addition to this, researchers often want to integrate their workflows into their own web servers. In these cases, software is needed to manage the submission of jobs from the web interface to the cluster and then return the results once the job has finished executing. We have developed the Job Management System (JMS, a workflow management system and web interface for high performance computing (HPC. JMS provides users with a user-friendly web interface for creating complex workflows with multiple stages. It integrates this workflow functionality with the resource manager, a tool that is used to control and manage batch jobs on HPC clusters. As such, JMS combines workflow management functionality with cluster administration functionality. In addition, JMS provides developer tools including a code editor and the ability to version tools and scripts. JMS can be used by researchers from any field to build and run complex computational pipelines and provides functionality to include these pipelines in external interfaces. JMS is currently being used to house a number of bioinformatics pipelines at the Research Unit in Bioinformatics (RUBi at Rhodes University. JMS is an open-source project and is freely available at https://github.com/RUBi-ZA/JMS.\n\nJMS: An Open Source Workflow Management System and Web-Based Cluster Front-End for High Performance Computing\n\nScience.gov (United States)\n\nBrown, David K.; Penkler, David L.; Musyoka, Thommas M.; Bishop, Ãzlem Tastan\n\n2015-01-01\n\nComplex computational pipelines are becoming a staple of modern scientific research. Often these pipelines are resource intensive and require days of computing time. In such cases, it makes sense to run them over high performance computing (HPC) clusters where they can take advantage of the aggregated resources of many powerful computers. In addition to this, researchers often want to integrate their workflows into their own web servers. In these cases, software is needed to manage the submission of jobs from the web interface to the cluster and then return the results once the job has finished executing. We have developed the Job Management System (JMS), a workflow management system and web interface for high performance computing (HPC). JMS provides users with a user-friendly web interface for creating complex workflows with multiple stages. It integrates this workflow functionality with the resource manager, a tool that is used to control and manage batch jobs on HPC clusters. As such, JMS combines workflow management functionality with cluster administration functionality. In addition, JMS provides developer tools including a code editor and the ability to version tools and scripts. JMS can be used by researchers from any field to build and run complex computational pipelines and provides functionality to include these pipelines in external interfaces. JMS is currently being used to house a number of bioinformatics pipelines at the Research Unit in Bioinformatics (RUBi) at Rhodes University. JMS is an open-source project and is freely available at https://github.com/RUBi-ZA/JMS. PMID:26280450\n\nHigh-performance dynamic quantum clustering on graphics processors\n\nEnergy Technology Data Exchange (ETDEWEB)\n\nWittek, Peter, E-mail: peterwittek@acm.org [Swedish School of Library and Information Science, University of Boras, Boras (Sweden)\n\n2013-01-15\n\nClustering methods in machine learning may benefit from borrowing metaphors from physics. Dynamic quantum clustering associates a Gaussian wave packet with the multidimensional data points and regards them as eigenfunctions of the Schroedinger equation. The clustering structure emerges by letting the system evolve and the visual nature of the algorithm has been shown to be useful in a range of applications. Furthermore, the method only uses matrix operations, which readily lend themselves to parallelization. In this paper, we develop an implementation on graphics hardware and investigate how this approach can accelerate the computations. We achieve a speedup of up to two magnitudes over a multicore CPU implementation, which proves that quantum-like methods and acceleration by graphics processing units have a great relevance to machine learning.\n\nPerformance Evaluation of Spectral Clustering Algorithm using Various Clustering Validity Indices\n\nOpenAIRE\n\nM. T. Somashekara; D. Manjunatha\n\n2014-01-01\n\nIn spite of the popularity of spectral clustering algorithm, the evaluation procedures are still in developmental stage. In this article, we have taken benchmarking IRIS dataset for performing comparative study of twelve indices for evaluating spectral clustering algorithm. The results of the spectral clustering technique were also compared with k-mean algorithm. The validity of the indices was also verified with accuracy and (Normalized Mutual Information) NMI score. Spectral clustering algo...\n\nFirst high energy hydrogen cluster beams\n\nInternational Nuclear Information System (INIS)\n\nGaillard, M.J.; Genre, R.; Hadinger, G.; Martin, J.\n\n1993-03-01\n\nThe hydrogen cluster accelerator of the Institut de Physique Nucleaire de Lyon (IPN Lyon) has been upgraded by adding a Variable Energy Post-accelerator of RFQ type (VERFQ). This operation has been performed in the frame of a collaboration between KfK Karlsruhe, IAP Frankfurt and IPN Lyon. The facility has been designed to deliver beams of mass selected Hn + clusters, n chosen between 3 and 49, in the energy range 65-100 keV/u. For the first time, hydrogen clusters have been accelerated at energies as high as 2 MeV. This facility opens new fields for experiments which will greatly benefit from a velocity range never available until now for such exotic projectiles. (author) 13 refs.; 1 fig\n\nHigh-performance dynamic quantum clustering on graphics processors\n\nInternational Nuclear Information System (INIS)\n\nWittek, Peter\n\n2013-01-01\n\nClustering methods in machine learning may benefit from borrowing metaphors from physics. Dynamic quantum clustering associates a Gaussian wave packet with the multidimensional data points and regards them as eigenfunctions of the SchrÃ¶dinger equation. The clustering structure emerges by letting the system evolve and the visual nature of the algorithm has been shown to be useful in a range of applications. Furthermore, the method only uses matrix operations, which readily lend themselves to parallelization. In this paper, we develop an implementation on graphics hardware and investigate how this approach can accelerate the computations. We achieve a speedup of up to two magnitudes over a multicore CPU implementation, which proves that quantum-like methods and acceleration by graphics processing units have a great relevance to machine learning.\n\nCTEx Beowulf cluster for MCNP performance\n\nInternational Nuclear Information System (INIS)\n\nGonzaga, Roberto N.; Amorim, Aneuri S. de; Balthar, Mario Cesar V.\n\n2011-01-01\n\nThis work is an introduction to the CTEx Nuclear Defense Department's Beowulf Cluster. Building a Beowulf Cluster is a complex learning process that greatly depends upon your hardware and software requirements. The feasibility and efficiency of performing MCNP5 calculations with a small, heterogeneous computing cluster built in Red Hat's Fedora Linux operating system personal computers (PC) are explored. The performance increases that may be expected with such clusters are estimated for cases that typify general radiation transport calculations. Our results show that the speed increase from additional slave PCs is nearly linear up to 10 processors. The pre compiled parallel binary version of MCNP uses the Message-Passing Interface (MPI) protocol. The use of this pre compiled parallel version of MCNP5 with the MPI protocol on a small, heterogeneous computing cluster built from Red Hat's Fedora Linux operating system PCs is the subject of this work. (author)\n\nComparing the performance of biomedical clustering methods\n\nDEFF Research Database (Denmark)\n\nWiwie, Christian; Baumbach, Jan; RÃ¶ttger, Richard\n\n2015-01-01\n\nexpression to protein domains. Performance was judged on the basis of 13 common cluster validity indices. We developed a clustering analysis platform, ClustEval (http://clusteval.mpi-inf.mpg.de), to promote streamlined evaluation, comparison and reproducibility of clustering results in the future......Identifying groups of similar objects is a popular first step in biomedical data analysis, but it is error-prone and impossible to perform manually. Many computational methods have been developed to tackle this problem. Here we assessed 13 well-known methods using 24 data sets ranging from gene....... This allowed us to objectively evaluate the performance of all tools on all data sets with up to 1,000 different parameter sets each, resulting in a total of more than 4 million calculated cluster validity indices. We observed that there was no universal best performer, but on the basis of this wide...\n\nCluster-based DBMS Management Tool with High-Availability\n\nDirectory of Open Access Journals (Sweden)\n\nJae-Woo Chang\n\n2005-02-01\n\nFull Text Available A management tool which is needed for monitoring and managing cluster-based DBMSs has been little studied. So, we design and implement a cluster-based DBMS management tool with high-availability that monitors the status of nodes in a cluster system as well as the status of DBMS instances in a node. The tool enables users to recognize a single virtual system image and provides them with the status of all the nodes and resources in the system by using a graphic user interface (GUI. By using a load balancer, our management tool can increase the performance of a cluster-based DBMS as well as can overcome the limitation of the existing parallel DBMSs.\n\nCluster Cooperation in Wireless-Powered Sensor Networks: Modeling and Performance Analysis.\n\nScience.gov (United States)\n\nZhang, Chao; Zhang, Pengcheng; Zhang, Weizhan\n\n2017-09-27\n\nA wireless-powered sensor network (WPSN) consisting of one hybrid access point (HAP), a near cluster and the corresponding far cluster is investigated in this paper. These sensors are wireless-powered and they transmit information by consuming the harvested energy from signal ejected by the HAP. Sensors are able to harvest energy as well as store the harvested energy. We propose that if sensors in near cluster do not have their own information to transmit, acting as relays, they can help the sensors in a far cluster to forward information to the HAP in an amplify-and-forward (AF) manner. We use a finite Markov chain to model the dynamic variation process of the relay battery, and give a general analyzing model for WPSN with cluster cooperation. Though the model, we deduce the closed-form expression for the outage probability as the metric of this network. Finally, simulation results validate the start point of designing this paper and correctness of theoretical analysis and show how parameters have an effect on system performance. Moreover, it is also known that the outage probability of sensors in far cluster can be drastically reduced without sacrificing the performance of sensors in near cluster if the transmit power of HAP is fairly high. Furthermore, in the aspect of outage performance of far cluster, the proposed scheme significantly outperforms the direct transmission scheme without cooperation.\n\nPerformance assessment of the SIMFAP parallel cluster at IFIN-HH Bucharest\n\nInternational Nuclear Information System (INIS)\n\nAdam, Gh.; Adam, S.; Ayriyan, A.; Dushanov, E.; Hayryan, E.; Korenkov, V.; Lutsenko, A.; Mitsyn, V.; Sapozhnikova, T.; Sapozhnikov, A; Streltsova, O.; Buzatu, F.; Dulea, M.; Vasile, I.; Sima, A.; Visan, C.; Busa, J.; Pokorny, I.\n\n2008-01-01\n\nPerformance assessment and case study outputs of the parallel SIMFAP cluster at IFIN-HH Bucharest point to its effective and reliable operation. A comparison with results on the supercomputing system in LIT-JINR Dubna adds insight on resource allocation for problem solving by parallel computing. The solution of models asking for very large numbers of knots in the discretization mesh needs the migration to high performance computing based on parallel cluster architectures. The acquisition of ready-to-use parallel computing facilities being beyond limited budgetary resources, the solution at IFIN-HH was to buy the hardware and the inter-processor network, and to implement by own efforts the open software concerning both the operating system and the parallel computing standard. The present paper provides a report demonstrating the successful solution of these tasks. The implementation of the well-known HPL (High Performance LINPACK) Benchmark points to the effective and reliable operation of the cluster. The comparison of HPL outputs obtained on parallel clusters of different magnitudes shows that there is an optimum range of the order N of the linear algebraic system over which a given parallel cluster provides optimum parallel solutions. For the SIMFAP cluster, this range can be inferred to correspond to about 1 to 2 x 10 4 linear algebraic equations. For an algorithm of polynomial complexity N Î± the task sharing among p processors within a parallel solution mainly follows an (N/p)Î± behaviour under peak performance achievement. Thus, while the problem complexity remains the same, a substantial decrease of the coefficient of the leading order of the polynomial complexity is achieved. (authors)\n\nHigh performance data transfer\n\nScience.gov (United States)\n\nCottrell, R.; Fang, C.; Hanushevsky, A.; Kreuger, W.; Yang, W.\n\n2017-10-01\n\nThe exponentially increasing need for high speed data transfer is driven by big data, and cloud computing together with the needs of data intensive science, High Performance Computing (HPC), defense, the oil and gas industry etc. We report on the Zettar ZX software. This has been developed since 2013 to meet these growing needs by providing high performance data transfer and encryption in a scalable, balanced, easy to deploy and use way while minimizing power and space utilization. In collaboration with several commercial vendors, Proofs of Concept (PoC) consisting of clusters have been put together using off-the- shelf components to test the ZX scalability and ability to balance services using multiple cores, and links. The PoCs are based on SSD flash storage that is managed by a parallel file system. Each cluster occupies 4 rack units. Using the PoCs, between clusters we have achieved almost 200Gbps memory to memory over two 100Gbps links, and 70Gbps parallel file to parallel file with encryption over a 5000 mile 100Gbps link.\n\nCluster Cooperation in Wireless-Powered Sensor Networks: Modeling and Performance Analysis\n\nDirectory of Open Access Journals (Sweden)\n\nChao Zhang\n\n2017-09-01\n\nFull Text Available A wireless-powered sensor network (WPSN consisting of one hybrid access point (HAP, a near cluster and the corresponding far cluster is investigated in this paper. These sensors are wireless-powered and they transmit information by consuming the harvested energy from signal ejected by the HAP. Sensors are able to harvest energy as well as store the harvested energy. We propose that if sensors in near cluster do not have their own information to transmit, acting as relays, they can help the sensors in a far cluster to forward information to the HAP in an amplify-and-forward (AF manner. We use a finite Markov chain to model the dynamic variation process of the relay battery, and give a general analyzing model for WPSN with cluster cooperation. Though the model, we deduce the closed-form expression for the outage probability as the metric of this network. Finally, simulation results validate the start point of designing this paper and correctness of theoretical analysis and show how parameters have an effect on system performance. Moreover, it is also known that the outage probability of sensors in far cluster can be drastically reduced without sacrificing the performance of sensors in near cluster if the transmit power of HAP is fairly high. Furthermore, in the aspect of outage performance of far cluster, the proposed scheme significantly outperforms the direct transmission scheme without cooperation.\n\nLatent Cluster Analysis of Instructional Practices Reported by High- and Low-performing Mathematics Teachers in Four Countries\n\nOpenAIRE\n\nCheng, Qiang; Hsu, Hsien-Yuan\n\n2017-01-01\n\nUsing Trends in International Mathematics and Science Study (TIMSS) 2011 eighth-grade international dataset, this study explored the profiles of instructional practices reported by high- and low-performing mathematics teachers across the US, Finland, Korea, and Russia. Concepts of conceptual teaching and procedural teaching were used to frame the design of the current study. Latent cluster analysis was applied in the investigation of the profiles of mathematics teachersâ instructional practic...\n\nLATENT CLUSTER ANALYSIS OF INSTRUCTIONAL PRACTICES REPORTED BY HIGH- AND LOW-PERFORMING MATHEMATICS TEACHERS IN FOUR COUNTRIES\n\nDirectory of Open Access Journals (Sweden)\n\nQiang Cheng\n\n2017-06-01\n\nFull Text Available Using Trends in International Mathematics and Science Study (TIMSS 2011 eighth-grade international dataset, this study explored the profiles of instructional practices reported by high- and low-performing mathematics teachers across the US, Finland, Korea, and Russia. Concepts of conceptual teaching and procedural teaching were used to frame the design of the current study. Latent cluster analysis was applied in the investigation of the profiles of mathematics teachersâ instructional practices across the four education systems. It was found that all mathematics teachers in the high- and low-performing groups used procedurally as well as conceptually oriented practices in their teaching. However, one group of high-performing mathematics teachers from the U.S. sample and all the high-performing teachers from Finland, Korea, and Russia showed more frequent use of conceptually oriented practices than their corresponding low-performing teachers. Another group of U.S. high-performing mathematics teachers showed a distinctive procedurally oriented pattern, which presented a rather different picture. Such results provide useful suggestions for practitioners and policy makers in their effort to improve mathematics teaching and learning in the US and in other countries as well.DOI: http://dx.doi.org/10.22342/jme.8.2.4066.115-132\n\nPerformance of the cluster-jet target for PANDA\n\nEnergy Technology Data Exchange (ETDEWEB)\n\nHergemoeller, Ann-Katrin; Bonaventura, Daniel; Grieser, Silke; Hetz, Benjamin; Koehler, Esperanza; Khoukaz, Alfons [Institut fuer Kernphysik, Westfaelische Wilhelms-Universitaet Muenster, 48149 Muenster (Germany)\n\n2016-07-01\n\nThe success of storage ring experiments strongly depends on the choice of the target. For this purpose, a very appropriate internal target for such an experiment is a cluster-jet target, which will be the first operated target at the PANDA experiment at FAIR. In this kind of target the cluster beam itself is formed due to the expansion of pre-cooled gases within a Laval nozzle and is prepared afterwards via two orifices, the skimmer and the collimator. The target prototype, operating successfully for years at the University of Muenster, provides routinely target thicknesses of more than 2 x 10{sup 15} (atoms)/(cm{sup 2}) in a distance of 2.1 m behind the nozzle. Based on the results of the performance of the cluster target prototype the final cluster-jet target source was designed and set into operation in Muenster as well. Besides the monitoring of the cluster beam itself and the thickness with two different monitoring systems at this target, investigations on the cluster mass via Mie scattering will be performed. In this presentation an overview of the cluster target design, its performance and the Mie scattering method are presented and discussed.\n\nHigh-dimensional cluster analysis with the Masked EM Algorithm\n\nScience.gov (United States)\n\nKadir, Shabnam N.; Goodman, Dan F. M.; Harris, Kenneth D.\n\n2014-01-01\n\nCluster analysis faces two problems in high dimensions: first, the âcurse of dimensionalityâ that can lead to overfitting and poor generalization performance; and second, the sheer time taken for conventional algorithms to process large amounts of high-dimensional data. We describe a solution to these problems, designed for the application of âspike sortingâ for next-generation high channel-count neural probes. In this problem, only a small subset of features provide information about the cluster member-ship of any one data vector, but this informative feature subset is not the same for all data points, rendering classical feature selection ineffective. We introduce a âMasked EMâ algorithm that allows accurate and time-efficient clustering of up to millions of points in thousands of dimensions. We demonstrate its applicability to synthetic data, and to real-world high-channel-count spike sorting data. PMID:25149694\n\nPerformance Evaluation of Incremental K-means Clustering Algorithm\n\nOpenAIRE\n\nChakraborty, Sanjay; Nagwani, N. K.\n\n2014-01-01\n\nThe incremental K-means clustering algorithm has already been proposed and analysed in paper [Chakraborty and Nagwani, 2011]. It is a very innovative approach which is applicable in periodically incremental environment and dealing with a bulk of updates. In this paper the performance evaluation is done for this incremental K-means clustering algorithm using air pollution database. This paper also describes the comparison on the performance evaluations between existing K-means clustering and i...\n\nConfined SnO2 quantum-dot clusters in graphene sheets as high-performance anodes for lithium-ion batteries.\n\nScience.gov (United States)\n\nZhu, Chengling; Zhu, Shenmin; Zhang, Kai; Hui, Zeyu; Pan, Hui; Chen, Zhixin; Li, Yao; Zhang, Di; Wang, Da-Wei\n\n2016-05-16\n\nConstruction of metal oxide nanoparticles as anodes is of special interest for next-generation lithium-ion batteries. The main challenge lies in their rapid capacity fading caused by the structural degradation and instability of solid-electrolyte interphase (SEI) layer during charge/discharge process. Herein, we address these problems by constructing a novel-structured SnO2-based anode. The novel structure consists of mesoporous clusters of SnO2 quantum dots (SnO2 QDs), which are wrapped with reduced graphene oxide (RGO) sheets. The mesopores inside the clusters provide enough room for the expansion and contraction of SnO2 QDs during charge/discharge process while the integral structure of the clusters can be maintained. The wrapping RGO sheets act as electrolyte barrier and conductive reinforcement. When used as an anode, the resultant composite (MQDC-SnO2/RGO) shows an extremely high reversible capacity of 924âmAh g(-1) after 200 cycles at 100âmA g(-1), superior capacity retention (96%), and outstanding rate performance (505âmAh g(-1) after 1000 cycles at 1000âmA g(-1)). Importantly, the materials can be easily scaled up under mild conditions. Our findings pave a new way for the development of metal oxide towards enhanced lithium storage performance.\n\nGALAXY CLUSTERS AT HIGH REDSHIFT AND EVOLUTION OF BRIGHTEST CLUSTER GALAXIES\n\nInternational Nuclear Information System (INIS)\n\nWen, Z. L.; Han, J. L.\n\n2011-01-01\n\nIdentification of high-redshift clusters is important for studies of cosmology and cluster evolution. Using photometric redshifts of galaxies, we identify 631 clusters from the Canada-France-Hawaii Telescope (CFHT) wide field, 202 clusters from the CFHT deep field, 187 clusters from the Cosmic Evolution Survey (COSMOS) field, and 737 clusters from the Spitzer Wide-area InfraRed Extragalactic Survey (SWIRE) field. The redshifts of these clusters are in the range 0.1 â¼ + - m 3.6 Î¼ m colors of the BCGs are consistent with a stellar population synthesis model in which the BCGs are formed at redshift z f â¥ 2 and evolved passively. The g' - z' and B - m 3.6Î¼m colors of the BCGs at redshifts z > 0.8 are systematically bluer than the passive evolution model for galaxies formed at z f â¼ 2, indicating star formation in high-redshift BCGs.\n\nCost/Performance Ratio Achieved by Using a Commodity-Based Cluster\n\nScience.gov (United States)\n\nLopez, Isaac\n\n2001-01-01\n\nResearchers at the NASA Glenn Research Center acquired a commodity cluster based on Intel Corporation processors to compare its performance with a traditional UNIX cluster in the execution of aeropropulsion applications. Since the cost differential of the clusters was significant, a cost/performance ratio was calculated. After executing a propulsion application on both clusters, the researchers demonstrated a 9.4 cost/performance ratio in favor of the Intel-based cluster. These researchers utilize the Aeroshark cluster as one of the primary testbeds for developing NPSS parallel application codes and system software. The Aero-shark cluster provides 64 Intel Pentium II 400-MHz processors, housed in 32 nodes. Recently, APNASA - a code developed by a Government/industry team for the design and analysis of turbomachinery systems was used for a simulation on Glenn's Aeroshark cluster.\n\nDesigning a High Performance Parallel Personal Cluster\n\nOpenAIRE\n\nKapanova, K. G.; Sellier, J. M.\n\n2016-01-01\n\nToday, many scientific and engineering areas require high performance computing to perform computationally intensive experiments. For example, many advances in transport phenomena, thermodynamics, material properties, computational chemistry and physics are possible only because of the availability of such large scale computing infrastructures. Yet many challenges are still open. The cost of energy consumption, cooling, competition for resources have been some of the reasons why the scientifi...\n\nDark matter phenomenology of high-speed galaxy cluster collisions\n\nInternational Nuclear Information System (INIS)\n\nMishchenko, Yuriy; Ji, Chueng-Ryong\n\n2017-01-01\n\nWe perform a general computational analysis of possible post-collision mass distributions in high-speed galaxy cluster collisions in the presence of self-interacting dark matter. Using this analysis, we show that astrophysically weakly self-interacting dark matter can impart subtle yet measurable features in the mass distributions of colliding galaxy clusters even without significant disruptions to the dark matter halos of the colliding galaxy clusters themselves. Most profound such evidence is found to reside in the tails of dark matter halos' distributions, in the space between the colliding galaxy clusters. Such features appear in our simulations as shells of scattered dark matter expanding in alignment with the outgoing original galaxy clusters, contributing significant densities to projected mass distributions at large distances from collision centers and large scattering angles of up to 90 \"c\"i\"r\"c\"l\"e. Our simulations indicate that as much as 20% of the total collision's mass may be deposited into such structures without noticeable disruptions to the main galaxy clusters. Such structures at large scattering angles are forbidden in purely gravitational high-speed galaxy cluster collisions. Convincing identification of such structures in real colliding galaxy clusters would be a clear indication of the self-interacting nature of dark matter. Our findings may offer an explanation for the ring-like dark matter feature recently identified in the long-range reconstructions of the mass distribution of the colliding galaxy cluster CL0024+017. (orig.)\n\nDark matter phenomenology of high-speed galaxy cluster collisions\n\nEnergy Technology Data Exchange (ETDEWEB)\n\nMishchenko, Yuriy [Izmir University of Economics, Faculty of Engineering, Izmir (Turkey); Ji, Chueng-Ryong [North Carolina State University, Department of Physics, Raleigh, NC (United States)\n\n2017-08-15\n\nWe perform a general computational analysis of possible post-collision mass distributions in high-speed galaxy cluster collisions in the presence of self-interacting dark matter. Using this analysis, we show that astrophysically weakly self-interacting dark matter can impart subtle yet measurable features in the mass distributions of colliding galaxy clusters even without significant disruptions to the dark matter halos of the colliding galaxy clusters themselves. Most profound such evidence is found to reside in the tails of dark matter halos' distributions, in the space between the colliding galaxy clusters. Such features appear in our simulations as shells of scattered dark matter expanding in alignment with the outgoing original galaxy clusters, contributing significant densities to projected mass distributions at large distances from collision centers and large scattering angles of up to 90 {sup circle}. Our simulations indicate that as much as 20% of the total collision's mass may be deposited into such structures without noticeable disruptions to the main galaxy clusters. Such structures at large scattering angles are forbidden in purely gravitational high-speed galaxy cluster collisions. Convincing identification of such structures in real colliding galaxy clusters would be a clear indication of the self-interacting nature of dark matter. Our findings may offer an explanation for the ring-like dark matter feature recently identified in the long-range reconstructions of the mass distribution of the colliding galaxy cluster CL0024+017. (orig.)\n\nFamily and academic performance: identifying high school student profiles\n\nDirectory of Open Access Journals (Sweden)\n\nAlicia Aleli Chaparro Caso LÃ³pez\n\n2016-01-01\n\nFull Text Available The objective of this study was to identify profiles of high school students, based on variables related to academic performance, socioeconomic status, cultural capital and family organization. A total of 21,724 high school students, from the five municipalities of the state of Baja California, took part. A K-means cluster analysis was performed to identify the profiles. The analyses identified two clearly-defined clusters: Cluster 1 grouped together students with high academic performance and who achieved higher scores for socioeconomic status, cultural capital and family involvement, whereas Cluster 2 brought together students with low academic achievement, and who also obtained lower scores for socioeconomic status and cultural capital, and had less family involvement. It is concluded that the family variables analyzed form student profiles that can be related to academic achievement.\n\nPerformance Analysis of Memory Transfers and GEMM Subroutines on NVIDIA Tesla GPU Cluster\n\nEnergy Technology Data Exchange (ETDEWEB)\n\nAllada, Veerendra, Benjegerdes, Troy; Bode, Brett\n\n2009-08-31\n\nCommodity clusters augmented with application accelerators are evolving as competitive high performance computing systems. The Graphical Processing Unit (GPU) with a very high arithmetic density and performance per price ratio is a good platform for the scientific application acceleration. In addition to the interconnect bottlenecks among the cluster compute nodes, the cost of memory copies between the host and the GPU device have to be carefully amortized to improve the overall efficiency of the application. Scientific applications also rely on efficient implementation of the BAsic Linear Algebra Subroutines (BLAS), among which the General Matrix Multiply (GEMM) is considered as the workhorse subroutine. In this paper, they study the performance of the memory copies and GEMM subroutines that are critical to port the computational chemistry algorithms to the GPU clusters. To that end, a benchmark based on the NetPIPE framework is developed to evaluate the latency and bandwidth of the memory copies between the host and the GPU device. The performance of the single and double precision GEMM subroutines from the NVIDIA CUBLAS 2.0 library are studied. The results have been compared with that of the BLAS routines from the Intel Math Kernel Library (MKL) to understand the computational trade-offs. The test bed is a Intel Xeon cluster equipped with NVIDIA Tesla GPUs.\n\nPerformance Analysis of Memory Transfers and GEMM Subroutines on NVIDIA Tesla GPU Cluster\n\nInternational Nuclear Information System (INIS)\n\nAllada, Veerendra; Benjegerdes, Troy; Bode, Brett\n\n2009-01-01\n\nCommodity clusters augmented with application accelerators are evolving as competitive high performance computing systems. The Graphical Processing Unit (GPU) with a very high arithmetic density and performance per price ratio is a good platform for the scientific application acceleration. In addition to the interconnect bottlenecks among the cluster compute nodes, the cost of memory copies between the host and the GPU device have to be carefully amortized to improve the overall efficiency of the application. Scientific applications also rely on efficient implementation of the BAsic Linear Algebra Subroutines (BLAS), among which the General Matrix Multiply (GEMM) is considered as the workhorse subroutine. In this paper, they study the performance of the memory copies and GEMM subroutines that are critical to port the computational chemistry algorithms to the GPU clusters. To that end, a benchmark based on the NetPIPE framework is developed to evaluate the latency and bandwidth of the memory copies between the host and the GPU device. The performance of the single and double precision GEMM subroutines from the NVIDIA CUBLAS 2.0 library are studied. The results have been compared with that of the BLAS routines from the Intel Math Kernel Library (MKL) to understand the computational trade-offs. The test bed is a Intel Xeon cluster equipped with NVIDIA Tesla GPUs.\n\nConfined SnO2 quantum-dot clusters in graphene sheets as high-performance anodes for lithium-ion batteries\n\nScience.gov (United States)\n\nZhu, Chengling; Zhu, Shenmin; Zhang, Kai; Hui, Zeyu; Pan, Hui; Chen, Zhixin; Li, Yao; Zhang, Di; Wang, Da-Wei\n\n2016-01-01\n\nConstruction of metal oxide nanoparticles as anodes is of special interest for next-generation lithium-ion batteries. The main challenge lies in their rapid capacity fading caused by the structural degradation and instability of solid-electrolyte interphase (SEI) layer during charge/discharge process. Herein, we address these problems by constructing a novel-structured SnO2-based anode. The novel structure consists of mesoporous clusters of SnO2 quantum dots (SnO2 QDs), which are wrapped with reduced graphene oxide (RGO) sheets. The mesopores inside the clusters provide enough room for the expansion and contraction of SnO2 QDs during charge/discharge process while the integral structure of the clusters can be maintained. The wrapping RGO sheets act as electrolyte barrier and conductive reinforcement. When used as an anode, the resultant composite (MQDC-SnO2/RGO) shows an extremely high reversible capacity of 924âmAh gâ1 after 200 cycles at 100âmA gâ1, superior capacity retention (96%), and outstanding rate performance (505âmAh gâ1 after 1000 cycles at 1000âmA gâ1). Importantly, the materials can be easily scaled up under mild conditions. Our findings pave a new way for the development of metal oxide towards enhanced lithium storage performance. PMID:27181691\n\nAnalysis of SCTP and TCP based communication in high-speed clusters\n\nInternational Nuclear Information System (INIS)\n\nKozlovszky, M.; Berceli, T.; Kutor, L.\n\n2006-01-01\n\nPerformance and financial constraints are pushing modern DAQs (Data Acquisition Systems) to use distributed cluster environments instead of monolith one-box systems. Inside clusters application communication layers should support outstanding high performance requirements. We are currently investigating different network protocols that could meet the requirements of high speed/low latency peer-to-peer communication within DAQ clusters. We have carried out various performance measurements with TCP and SCTP over Fast and Gigabit Ethernet. We are focusing on Ethernet Technologies, because this transport medium is broad deployed, cost efficient and it has much better cost/throughput ratio than other available communication alternatives (e.g.: Myrinet, Infiniband). During this study, a protocol performance measurement application with different peer transport components has been developed. In the first part of the paper, we give a short comparison of the two protocols (SCTP and TCP), and an introduction of the transport layer structure developed. Later on we discuss the performance results of single/multi-stream peer-to-peer communication, give overview about application code transition possibilities from application developer point of view between the two protocols, and draw conclusions about usability\n\nCentroid based clustering of high throughput sequencing reads based on n-mer counts.\n\nScience.gov (United States)\n\nSolovyov, Alexander; Lipkin, W Ian\n\n2013-09-08\n\nMany problems in computational biology require alignment-free sequence comparisons. One of the common tasks involving sequence comparison is sequence clustering. Here we apply methods of alignment-free comparison (in particular, comparison using sequence composition) to the challenge of sequence clustering. We study several centroid based algorithms for clustering sequences based on word counts. Study of their performance shows that using k-means algorithm with or without the data whitening is efficient from the computational point of view. A higher clustering accuracy can be achieved using the soft expectation maximization method, whereby each sequence is attributed to each cluster with a specific probability. We implement an open source tool for alignment-free clustering. It is publicly available from github: https://github.com/luscinius/afcluster. We show the utility of alignment-free sequence clustering for high throughput sequencing analysis despite its limitations. In particular, it allows one to perform assembly with reduced resources and a minimal loss of quality. The major factor affecting performance of alignment-free read clustering is the length of the read.\n\nOperational mesoscale atmospheric dispersion prediction using high performance parallel computing cluster for emergency response\n\nInternational Nuclear Information System (INIS)\n\nSrinivas, C.V.; Venkatesan, R.; Muralidharan, N.V.; Das, Someshwar; Dass, Hari; Eswara Kumar, P.\n\n2005-08-01\n\nAn operational atmospheric dispersion prediction system is implemented on a cluster super computer for 'Online Emergency Response' for Kalpakkam nuclear site. The numerical system constitutes a parallel version of a nested grid meso-scale meteorological model MM5 coupled to a random walk particle dispersion model FLEXPART. The system provides 48 hour forecast of the local weather and radioactive plume dispersion due to hypothetical air borne releases in a range of 100 km around the site. The parallel code was implemented on different cluster configurations like distributed and shared memory systems. Results of MM5 run time performance for 1-day prediction are reported on all the machines available for testing. A reduction of 5 times in runtime is achieved using 9 dual Xeon nodes (18 physical/36 logical processors) compared to a single node sequential run. Based on the above run time results a cluster computer facility with 9-node Dual Xeon is commissioned at IGCAR for model operation. The run time of a triple nested domain MM5 is about 4 h for 24 h forecast. The system has been operated continuously for a few months and results were ported on the IMSc home page. Initial and periodic boundary condition data for MM5 are provided by NCMRWF, New Delhi. An alternative source is found to be NCEP, USA. These two sources provide the input data to the operational models at different spatial and temporal resolutions and using different assimilation methods. A comparative study on the results of forecast is presented using these two data sources for present operational use. Slight improvement is noticed in rainfall, winds, geopotential heights and the vertical atmospheric structure while using NCEP data probably because of its high spatial and temporal resolution. (author)\n\nEnhanced high-order harmonic generation from Argon-clusters\n\nNARCIS (Netherlands)\n\nTao, Yin; Hagmeijer, Rob; Bastiaens, Hubertus M.J.; Goh, S.J.; van der Slot, P.J.M.; Biedron, S.; Milton, S.; Boller, Klaus J.\n\n2017-01-01\n\nHigh-order harmonic generation (HHG) in clusters is of high promise because clusters appear to offer an increased optical nonlinearity. We experimentally investigate HHG from Argon clusters in a supersonic gas jet that can generate monomer-cluster mixtures with varying atomic number density and\n\nHigh-performance scientific computing in the cloud\n\nScience.gov (United States)\n\nJorissen, Kevin; Vila, Fernando; Rehr, John\n\n2011-03-01\n\nCloud computing has the potential to open up high-performance computational science to a much broader class of researchers, owing to its ability to provide on-demand, virtualized computational resources. However, before such approaches can become commonplace, user-friendly tools must be developed that hide the unfamiliar cloud environment and streamline the management of cloud resources for many scientific applications. We have recently shown that high-performance cloud computing is feasible for parallelized x-ray spectroscopy calculations. We now present benchmark results for a wider selection of scientific applications focusing on electronic structure and spectroscopic simulation software in condensed matter physics. These applications are driven by an improved portable interface that can manage virtual clusters and run various applications in the cloud. We also describe a next generation of cluster tools, aimed at improved performance and a more robust cluster deployment. Supported by NSF grant OCI-1048052.\n\nOn the performance limiting behavior of defect clusters in commercial silicon solar cells\n\nEnergy Technology Data Exchange (ETDEWEB)\n\nSopori, B.L.; Chen, W.; Jones, K. [National Renewable Energy Lab., Golden, CO (United States); Gee, J. [Sandia National Labs., Albuquerque, NM (United States)\n\n1998-09-01\n\nThe authors report the observation of defect clusters in high-quality, commercial silicon solar cell substrates. The nature of the defect clusters, their mechanism of formation, and precipitation of metallic impurities at the defect clusters are discussed. This defect configuration influences the device performance in a unique way--by primarily degrading the voltage-related parameters. Network modeling is used to show that, in an N/P junction device, these regions act as shunts that dissipate power generated within the cell.\n\nCluster size matters: Size-driven performance of subnanometer clusters in catalysis, electrocatalysis and Li-air batteries\n\nScience.gov (United States)\n\nVajda, Stefan\n\n2015-03-01\n\nThis paper discusses the strongly size-dependent performance of subnanometer cluster based catalysts in 1) heterogeneous catalysis, 2) electrocatalysis and 3) Li-air batteries. The experimental studies are based on I. fabrication of ultrasmall clusters with atomic precision control of particle size and their deposition on oxide and carbon based supports; II. test of performance, III. in situand ex situ X-ray characterization of cluster size, shape and oxidation state; and IV.electron microscopies. Heterogeneous catalysis. The pronounced effect of cluster size and support on the performance of the catalyst (catalyst activity and the yield of Cn products) will be illustrated on the example of nickel and cobalt clusters in Fischer-Tropsch reaction. Electrocatalysis. The study of the oxygen evolution reaction (OER) on size-selected palladium clusters supported on ultrananocrystalline diamond show pronounced size effects. While Pd4 clusters show no reaction, Pd6 and Pd17 clusters are among the most active catalysts known (in in terms of turnover rate per Pd atom). The system (soft-landed Pd4, Pd6, or Pd17 clusters on an UNCD Si coated electrode) shows stable electrochemical potentials over several cycles, and the characterization of the electrodes show no evidence for evolution or dissolution of either the support Theoretical calculations suggest that this striking difference may be a demonstration that bridging Pd-Pd sites, which are only present in three-dimensional clusters, are active for the oxygen evolution reaction in Pd6O6. Li-air batteries. The studies show that sub-nm silver clusters have dramatic size-dependent effect on the lowering of the overpotential, charge capacity, morphology of the discharge products, as well as on the morphology of the nm size building blocks of the discharge products. The results suggest that by precise control of the active surface sites on the cathode, the performance of Li-air cells can be significantly improved\n\nHigh Intensity Femtosecond XUV Pulse Interactions with Atomic Clusters: Final Report\n\nEnergy Technology Data Exchange (ETDEWEB)\n\nDitmire, Todd [Univ. of Texas, Austin, TX (United States). Center for High Energy Density Science\n\n2016-10-12\n\nWe propose to expand our recent studies on the interactions of intense extreme ultraviolet (XUV) femtosecond pulses with atomic and molecular clusters. The work described follows directly from work performed under BES support for the past grant period. During this period weÂ upgradedÂ theÂ THORÂ laserÂ atÂ UTÂ AustinÂ byÂ replacing the regenerative amplifier with optical parametric amplification (OPA) using BBO crystals. This increased the contrast of the laser, the total laser energy to ~1.2 J , and decreased the pulse width to below 30 fs. We built a new all reflective XUV harmonic beam line into expanded lab space. This enabled an increase influence by a factor of 25 and an increase in the intensity by a factor of 50. The goal of the program proposed in this renewal is to extend this class of experiments to available higher XUV intensity and a greater range of wavelengths. In particular we plan to perform experiments to confirm our hypothesis about the origin of the high charge states in these exploding clusters, an effect which we ascribe to plasma continuum lowering (ionization potential depression) in a cluster nano-Â­plasma. To do this we will perform experiments in which XUV pulses of carefully chosen wavelength irradiate clusters composed of only low-Z atoms and clusters with a mixture of this low-Â­Z atom with higher Z atoms. The latter clusters will exhibit higher electron densities and will serve to lower the ionization potential further than in the clusters composed only of low Z atoms. This should have a significant effect on the charge states produced in the exploding cluster. We will also explore the transition of explosions in these XUV irradiated clusters from hydrodynamic expansion to Coulomb explosion. The work proposed here will explore clusters of a wider range of constituents, including clusters from solids. Experiments on clusters from solids will be enabled by development we performed during the past grant period in which we constructed and\n\nPerformance clustering and incentives in the UK pension fund industry\n\nOpenAIRE\n\nDavid Blake; Bruce N. Lehmann; Allan Timmermann\n\n2002-01-01\n\nDespite pension fund managers being largely unconstrained in their investment decisions, this paper reports evidence of clustering in the performance of a large cross-section of UK pension fund managers around the median fund manager. This finding is explained in terms of the predominance of a single investment style (balanced management), the fee structures and incentives operating in the UK pension fund industry to maximise relative rather than absolute performance, the high concentration i...\n\nTowards Urban High-technolgy Clusters: An International Comparison\n\nDirectory of Open Access Journals (Sweden)\n\nJÃºlia Bosch\n\n2012-01-01\n\nFull Text Available This paper presents the results of a comparative study of 23 urban or regional high-technology clusters (media, ICT, energy, biotechnology all over the world, analyzing how they were created, how they are managed and how they operate, and the strategies followed to improve and excel in their fields of action. Special attention is given to issues related to descriptive aspects, R&D, performance of the clusters, location factors and incentives to attract companies. The empirical analysis applied to the identified clusters was done through a questionnaire sent to the representatives of the clusterâs management. When analyzing the data, the study has combined quantitative and qualitative methods, depending on the information to be processed. The data collection was done through a selection of indicators chosen in order to cover the different elements that cluster literature coincide in consider essential to develop a competitive economic cluster in urban regions. The main obstacle we find with the information available to carry out this study has been its heterogeneity and different quality of the data. 22@Barcelona appears to be in a good position to compete with other excelling clusters, but it still needs to improve in areas such as financial supply for R&D and start-ups and coordination between the different actors involved in urban economic development. Our research also contributes to the discussion on the role of public institutions in the cluster development policies. In the clusters studied here, especially in Barcelona, we have seen that a capable and resourceful public administration can determine the success of the cluster initiative.\n\nCluster-assembled overlayers and high-temperature superconductors\n\nInternational Nuclear Information System (INIS)\n\nOhno, T.R.; Yang, Y.; Kroll, G.H.; Krause, K.; Schmidt, L.D.; Weaver, J.H.; Kimachi, Y.; Hidaka, Y.; Pan, S.H.; de Lozanne, A.L.\n\n1991-01-01\n\nX-ray photoemission results for interfaces prepared by cluster assembly with nanometer-size clusters deposited on high-T c superconductors (HTS's) show a reduction in reactivity because atom interactions with the surface are replaced by cluster interactions. Results for conventional atom deposition show the formation of overlayer oxides that are related to oxygen depletion and disruption of the near-surface region of the HTS's. For cluster assembly of Cr and Cu, there is a very thin reacted region on single-crystal Bi 2 Sr 2 CaCu 2 O 8 . Reduced reactivity is observed for Cr cluster deposition on single-crystal YBa 2 Cu 3 O 7 -based interfaces. There is no evidence of chemical modification of the surface for Ge and Au cluster assembly on Bi 2 Sr 2 CaCu 2 O 8 (100). The overlayer grown by Au cluster assembly on Bi 2 Sr 2 CaCu 2 O 8 covers the surface at low temperature but roughening occurs upon warming to 300 K. Scanning-tunneling-microscopy results for the Au(cluster)/Bi 2 Sr 2 CaCu 2 O 8 system warmed to 300 K shows individual clusters that have coalesced into large clusters. These results offer insight into the role of surface energies and cluster interactions in determining the overlayer morphology. Transmission-electron-microscopy results for Cu cluster assembly on silica show isolated irregularly shaped clusters that do not interact at low coverage. Sintering and labyrinth formation is observed at intermediate coverage and, ultimately, a continuous film is achieved at high coverage. Silica surface wetting by Cu clusters demonstrates that dispersive force are important for these small clusters\n\nImproved Ant Colony Clustering Algorithm and Its Performance Study\n\nScience.gov (United States)\n\nGao, Wei\n\n2016-01-01\n\nClustering analysis is used in many disciplines and applications; it is an important tool that descriptively identifies homogeneous groups of objects based on attribute values. The ant colony clustering algorithm is a swarm-intelligent method used for clustering problems that is inspired by the behavior of ant colonies that cluster their corpses and sort their larvae. A new abstraction ant colony clustering algorithm using a data combination mechanism is proposed to improve the computational efficiency and accuracy of the ant colony clustering algorithm. The abstraction ant colony clustering algorithm is used to cluster benchmark problems, and its performance is compared with the ant colony clustering algorithm and other methods used in existing literature. Based on similar computational difficulties and complexities, the results show that the abstraction ant colony clustering algorithm produces results that are not only more accurate but also more efficiently determined than the ant colony clustering algorithm and the other methods. Thus, the abstraction ant colony clustering algorithm can be used for efficient multivariate data clustering. PMID:26839533\n\nImplementation and performance of the ATLAS pixel clustering neural networks\n\nCERN Document Server\n\nGagnon, Louis-Guillaume; The ATLAS collaboration\n\n2018-01-01\n\nThe high particle densities produced by the Large Hadron Collider (LHC) mean that in the ATLAS pixel detector the clusters of deposited charge start to merge. A neural network-based approach is used to estimate the number of particles contributing to each cluster, and to accurately estimate the hit positions even in the presence of multiple particles. This talk thoroughly describes the algorithm and its implementation as well as present a set of benchmark performance measurements. The problem is most acute in the core of high-momentum jets where the average separation between particles becomes comparable to the detector granularity. This is further complicated by the high number of interactions per bunch crossing. Both these issues will become worse as the Run 3 and HL-LHC programme require analysis of higher and higher pT jets, while the interaction multiplicity rises. Future prospects in the context of LHC Run 3 and the upcoming ATLAS inner detector upgrade are also discussed.\n\nGraphic-Card Cluster for Astrophysics (GraCCA) -- Performance Tests\n\nOpenAIRE\n\nSchive, Hsi-Yu; Chien, Chia-Hung; Wong, Shing-Kwong; Tsai, Yu-Chih; Chiueh, Tzihong\n\n2007-01-01\n\nIn this paper, we describe the architecture and performance of the GraCCA system, a Graphic-Card Cluster for Astrophysics simulations. It consists of 16 nodes, with each node equipped with 2 modern graphic cards, the NVIDIA GeForce 8800 GTX. This computing cluster provides a theoretical performance of 16.2 TFLOPS. To demonstrate its performance in astrophysics computation, we have implemented a parallel direct N-body simulation program with shared time-step algorithm in this system. Our syste...\n\nFamily-based clusters of cognitive test performance in familial schizophrenia\n\nDirectory of Open Access Journals (Sweden)\n\nPartonen Timo\n\n2004-07-01\n\nFull Text Available Abstract Background Cognitive traits derived from neuropsychological test data are considered to be potential endophenotypes of schizophrenia. Previously, these traits have been found to form a valid basis for clustering samples of schizophrenia patients into homogeneous subgroups. We set out to identify such clusters, but apart from previous studies, we included both schizophrenia patients and family members into the cluster analysis. The aim of the study was to detect family clusters with similar cognitive test performance. Methods Test scores from 54 randomly selected families comprising at least two siblings with schizophrenia spectrum disorders, and at least two unaffected family members were included in a complete-linkage cluster analysis with interactive data visualization. Results A well-performing, an impaired, and an intermediate family cluster emerged from the analysis. While the neuropsychological test scores differed significantly between the clusters, only minor differences were observed in the clinical variables. Conclusions The visually aided clustering algorithm was successful in identifying family clusters comprising both schizophrenia patients and their relatives. The present classification method may serve as a basis for selecting phenotypically more homogeneous groups of families in subsequent genetic analyses.\n\nA Fast General-Purpose Clustering Algorithm Based on FPGAs for High-Throughput Data Processing\n\nCERN Document Server\n\nAnnovi, A; The ATLAS collaboration; Castegnaro, A; Gatta, M\n\n2012-01-01\n\nWe present a fast general-purpose algorithm for high-throughput clustering of data âwith a two dimensional organizationâ. The algorithm is designed to be implemented with FPGAs or custom electronics. The key feature is a processing time that scales linearly with the amount of data to be processed. This means that clustering can be performed in pipeline with the readout, without suffering from combinatorial delays due to looping multiple times through all the data. This feature makes this algorithm especially well suited for problems where the data has high density, e.g. in the case of tracking devices working under high-luminosity condition such as those of LHC or Super-LHC. The algorithm is organized in two steps: the first step (core) clusters the data; the second step analyzes each cluster of data to extract the desired information. The current algorithm is developed as a clustering device for modern high-energy physics pixel detectors. However, the algorithm has much broader field of applications. In ...\n\nCluster analysis of received constellations for optical performance monitoring\n\nNARCIS (Netherlands)\n\nvan Weerdenburg, J.J.A.; van Uden, R.; Sillekens, E.; de Waardt, H.; Koonen, A.M.J.; Okonkwo, C.\n\n2016-01-01\n\nPerformance monitoring based on centroid clustering to investigate constellation generation offsets. The tool allows flexibility in constellation generation tolerances by forwarding centroids to the demapper. The relation of fibre nonlinearities and singular value decomposition of intra-cluster\n\nClustering high dimensional data using RIA\n\nEnergy Technology Data Exchange (ETDEWEB)\n\nAziz, Nazrina [School of Quantitative Sciences, College of Arts and Sciences, Universiti Utara Malaysia, 06010 Sintok, Kedah (Malaysia)\n\n2015-05-15\n\nClustering may simply represent a convenient method for organizing a large data set so that it can easily be understood and information can efficiently be retrieved. However, identifying cluster in high dimensionality data sets is a difficult task because of the curse of dimensionality. Another challenge in clustering is some traditional functions cannot capture the pattern dissimilarity among objects. In this article, we used an alternative dissimilarity measurement called Robust Influence Angle (RIA) in the partitioning method. RIA is developed using eigenstructure of the covariance matrix and robust principal component score. We notice that, it can obtain cluster easily and hence avoid the curse of dimensionality. It is also manage to cluster large data sets with mixed numeric and categorical value.\n\nElectron acceleration via high contrast laser interacting with submicron clusters\n\nInternational Nuclear Information System (INIS)\n\nZhang Lu; Chen Liming; Wang Weiming; Yan Wenchao; Yuan Dawei; Mao Jingyi; Wang Zhaohua; Liu Cheng; Shen Zhongwei; Li Yutong; Dong Quanli; Lu Xin; Ma Jinglong; Wei Zhiyi; Faenov, Anatoly; Pikuz, Tatiana; Li Dazhang; Sheng Zhengming; Zhang Jie\n\n2012-01-01\n\nWe experimentally investigated electron acceleration from submicron size argon clusters-gas target irradiated by a 100 fs, 10 TW laser pulses having a high-contrast. Electron beams are observed in the longitudinal and transverse directions to the laser propagation. The measured energy of the longitudinal electron reaches 600 MeV and the charge of the electron beam in the transverse direction is more than 3 nC. A two-dimensional particle-in-cell simulation of the interaction has been performed and it shows an enhancement of electron charge by using the cluster-gas target.\n\nHigh-performance computing â an overview\n\nScience.gov (United States)\n\nMarksteiner, Peter\n\n1996-08-01\n\nAn overview of high-performance computing (HPC) is given. Different types of computer architectures used in HPC are discussed: vector supercomputers, high-performance RISC processors, various parallel computers like symmetric multiprocessors, workstation clusters, massively parallel processors. Software tools and programming techniques used in HPC are reviewed: vectorizing compilers, optimization and vector tuning, optimization for RISC processors; parallel programming techniques like shared-memory parallelism, message passing and data parallelism; and numerical libraries.\n\nHeterogeneous Gpu&Cpu Cluster For High Performance Computing In Cryptography\n\nDirectory of Open Access Journals (Sweden)\n\nMichaÅ Marks\n\n2012-01-01\n\nFull Text Available This paper addresses issues associated with distributed computing systems andthe application of mixed GPU&CPU technology to data encryption and decryptionalgorithms. We describe a heterogenous cluster HGCC formed by twotypes of nodes: Intel processor with NVIDIA graphics processing unit and AMDprocessor with AMD graphics processing unit (formerly ATI, and a novel softwareframework that hides the heterogeneity of our cluster and provides toolsfor solving complex scientific and engineering problems. Finally, we present theresults of numerical experiments. The considered case study is concerned withparallel implementations of selected cryptanalysis algorithms. The main goal ofthe paper is to show the wide applicability of the GPU&CPU technology tolarge scale computation and data processing.\n\nComparison of wind mill cluster performance: A multicriteria approach\n\nEnergy Technology Data Exchange (ETDEWEB)\n\nRajakumar, D.G.; Nagesha, N. [Visvesvaraya Technological Univ., Karnataka (India)\n\n2012-07-01\n\nEnergy is a crucial input for the economic and social development of any nation. Both renewable and non-renewable energy contribute in meeting the total requirement of the economy. As an affordable and clean energy source, wind energy is amongst the world's fastest growing renewable energy forms. Though there are several wind-mill clusters producing energy in different geographical locations, evaluating their performance is a complex task and not much of literature is available in this area. In this backdrop, an attempt is made in the current paper to estimate the performance of a wind-mill cluster through an index called Cluster Performance Index (CPI) adopting a multi-criteria approach. The proposed CPI comprises four criteria viz., Technical Performance Indicators (TePI), Economic Performance Indicators (EcPI), Environmental Performance Indicators (EnPI), and Sociological Performance Indicators (SoPI). Under each performance criterion a total of ten parameters are considered with five subjective and five objective oriented responses. The methodology is implemented by collecting empirical data from three wind-mill clusters located at Chitradurga, Davangere, and Gadag in the southern Indian State of Karnataka. Totally fifteen different stake holders are consulted through a set of structured researcher administered questionnaire to collect the relevant data in each wind farm. Stake holders involved engineers working in wind farms, wind farm developers, Government officials from energy department and a few selected residential people near the wind farms. The results of the study revealed that Chitradurga wind farm performed much better with a CPI of 45.267 as compared to Gadag (CPI of 28.362) and Davangere (CPI of 19.040) wind farms. (Author)\n\nCommunication Software Performance for Linux Clusters with Mesh Connections\n\nEnergy Technology Data Exchange (ETDEWEB)\n\nJie Chen; William Watson\n\n2003-09-01\n\nRecent progress in copper based commodity Gigabit Ethernet interconnects enables constructing clusters to achieve extremely high I/O bandwidth at low cost with mesh connections. However, the TCP/IP protocol stack cannot match the improved performance of Gigabit Ethernet networks especially in the case of multiple interconnects on a single host. In this paper, we evaluate and compare the performance characteristics of TCP/IP and M-VIA software that is an implementation of VIA.In particular, we focus on the performance of the software systems for a mesh communication architecture and demonstrate the feasibility of using multiple Gigabit Ethernet cards on one host to achieve aggregated bandwidth and latency that are not only better than what TCP provides but also compare favorably to some of the special purpose high-speed networks. In addition, implementation of a new M-VIA driver for one type of Gigabit Ethernet card will be discussed.\n\nComparative Performance Of Using PCA With K-Means And Fuzzy C Means Clustering For Customer Segmentation\n\nDirectory of Open Access Journals (Sweden)\n\nFahmida Afrin\n\n2015-08-01\n\nFull Text Available Abstract Data mining is the process of analyzing data and discovering useful information. Sometimes it is called knowledge Discovery. Clustering refers to groups whereas data are grouped in such a way that the data in one cluster are similar data in different clusters are dissimilar. Many data mining technologies are developed for customer segmentation. PCA is working as a preprocessor of Fuzzy C means and K- means for reducing the high dimensional and noisy data. There are many clustering method apply on customer segmentation. In this paper the performance of Fuzzy C means and K-means after implementing Principal Component Analysis is analyzed. We analyze the performance on a standard dataset for these algorithms. The results indicate that PCA based fuzzy clustering produces better results than PCA based K-means and is a more stable method for customer segmentation.\n\nRelevant Subspace Clustering\n\nDEFF Research Database (Denmark)\n\nMÃ¼ller, Emmanuel; Assent, Ira; GÃ¼nnemann, Stephan\n\n2009-01-01\n\nSubspace clustering aims at detecting clusters in any subspace projection of a high dimensional space. As the number of possible subspace projections is exponential in the number of dimensions, the result is often tremendously large. Recent approaches fail to reduce results to relevant subspace...... clusters. Their results are typically highly redundant, i.e. many clusters are detected multiple times in several projections. In this work, we propose a novel model for relevant subspace clustering (RESCU). We present a global optimization which detects the most interesting non-redundant subspace clusters...... achieves top clustering quality while competing approaches show greatly varying performance....\n\nPerformance comparison analysis library communication cluster system using merge sort\n\nScience.gov (United States)\n\nWulandari, D. A. R.; Ramadhan, M. E.\n\n2018-04-01\n\nBegins by using a single processor, to increase the speed of computing time, the use of multi-processor was introduced. The second paradigm is known as parallel computing, example cluster. The cluster must have the communication potocol for processing, one of it is message passing Interface (MPI). MPI have many library, both of them OPENMPI and MPICH2. Performance of the cluster machine depend on suitable between performance characters of library communication and characters of the problem so this study aims to analyze the comparative performances libraries in handling parallel computing process. The case study in this research are MPICH2 and OpenMPI. This case research execute sortingâs problem to know the performance of cluster system. The sorting problem use mergesort method. The research method is by implementing OpenMPI and MPICH2 on a Linux-based cluster by using five computer virtual then analyze the performance of the system by different scenario tests and three parameters for to know the performance of MPICH2 and OpenMPI. These performances are execution time, speedup and efficiency. The results of this study showed that the addition of each data size makes OpenMPI and MPICH2 have an average speed-up and efficiency tend to increase but at a large data size decreases. increased data size doesnât necessarily increased speed up and efficiency but only execution time example in 100000 data size. OpenMPI has a execution time greater than MPICH2 example in 1000 data size average execution time with MPICH2 is 0,009721 and OpenMPI is 0,003895 OpenMPI can customize communication needs.\n\nPerformance of a Real-time Multipurpose 2-Dimensional Clustering Algorithm Developed for the ATLAS Experiment\n\nCERN Document Server\n\nAUTHOR|(INSPIRE)INSPIRE-00372074; The ATLAS collaboration; Sotiropoulou, Calliope Louisa; Annovi, Alberto; Kordas, Kostantinos\n\n2016-01-01\n\nIn this paper the performance of the 2D pixel clustering algorithm developed for the Input Mezzanine card of the ATLAS Fast TracKer system is presented. Fast TracKer is an approved ATLAS upgrade that has the goal to provide a complete list of tracks to the ATLAS High Level Trigger for each level-1 accepted event, at up to 100 kHz event rate with a very small latency, in the order of 100Âµs. The Input Mezzanine card is the input stage of the Fast TracKer system. Its role is to receive data from the silicon detector and perform real time clustering, thus to reduce the amount of data propagated to the subsequent processing levels with minimal information loss. We focus on the most challenging component on the Input Mezzanine card, the 2D clustering algorithm executed on the pixel data. We compare two different implementations of the algorithm. The first is one called the ideal one which searches clusters of pixels in the whole silicon module at once and calculates the cluster centroids exploiting the whole avail...\n\nPerformance of a Real-time Multipurpose 2-Dimensional Clustering Algorithm Developed for the ATLAS Experiment\n\nCERN Document Server\n\nGkaitatzis, Stamatios; The ATLAS collaboration\n\n2016-01-01\n\nIn this paper the performance of the 2D pixel clustering algorithm developed for the Input Mezzanine card of the ATLAS Fast TracKer system is presented. Fast TracKer is an approved ATLAS upgrade that has the goal to provide a complete list of tracks to the ATLAS High Level Trigger for each level-1 accepted event, at up to 100 kHz event rate with a very small latency, in the order of 100 Âµs. The Input Mezzanine card is the input stage of the Fast TracKer system. Its role is to receive data from the silicon detector and perform real time clustering, thus to reduce the amount of data propagated to the subsequent processing levels with minimal information loss. We focus on the most challenging component on the Input Mezzanine card, the 2D clustering algorithm executed on the pixel data. We compare two different "
    }
}
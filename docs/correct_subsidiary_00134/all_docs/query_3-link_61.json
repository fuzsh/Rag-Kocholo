{
    "id": "correct_subsidiary_00134_3",
    "rank": 61,
    "data": {
        "url": "https://bucket4j.com/8.7.0/toc.html",
        "read_more_link": "",
        "language": "en",
        "title": "Bucket4j 8.7.0 Reference",
        "top_image": "",
        "meta_img": "",
        "images": [],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "3.1. JCache integration\n\nBucket4j supports any GRID solution which compatible with JCache API (JSR 107) specification.\n\nNote\n\nDo not forget to read Distributed usage checklist before using the Bucket4j over the JCache cluster.\n\nTo use the JCache extension you also need to add the following dependency:\n\n<dependency> <groupId>com.bucket4j</groupId> <artifactId>bucket4j-jcache</artifactId> <version>8.7.0</version> </dependency>\n\nNote\n\nsee Java compatibility matrix if you need for build that is compatible with Java 8\n\nJCache expects javax.cache.cache-api to be a provided dependency. Do not forget to add the following dependency:\n\n<dependency> <groupId>javax.cache</groupId> <artifactId>cache-api</artifactId> <version>${jcache.version}</version> </dependency>\n\n3.1.1. Example 1 - limiting access to HTTP server by IP address\n\nServletFilter would be the obvious place to check limits:\n\npublic class IpThrottlingFilter implements javax.servlet.Filter { private static final BucketConfiguration configuration = BucketConfiguration.builder() .addLimit(limit -> limit.capacity(30).refillGreedy(30, ofMinutes(1))) .build(); // cache for storing token buckets, where IP is key. @Inject private javax.cache.Cache<String, byte[]> cache; private ProxyManager<String> buckets; @Override public void init(FilterConfig filterConfig) throws ServletException { // init bucket registry buckets = new JCacheProxyManager<>(cache); } @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException { HttpServletRequest httpRequest = (HttpServletRequest) servletRequest; String ip = IpHelper.getIpFromRequest(httpRequest); // acquire cheap proxy to the bucket Bucket bucket = proxyManager.builder().build(key, configuration); // tryConsume returns false immediately if no tokens available with the bucket if (bucket.tryConsume(1)) { // the limit is not exceeded filterChain.doFilter(servletRequest, servletResponse); } else { // limit is exceeded HttpServletResponse httpResponse = (HttpServletResponse) servletResponse; httpResponse.setContentType(\"text/plain\"); httpResponse.setStatus(429); httpResponse.getWriter().append(\"Too many requests\"); } } }\n\n3.1.2. Example 2 - limiting access to service by contract agreements\n\npublic class IpThrottlingFilter implements javax.servlet.Filter { // service to provide per user limits @Inject private LimitProvider limitProvider; // cache for storing token buckets, where IP is key. @Inject private javax.cache.Cache<String, byte[]> cache; private ProxyManager<String> buckets; @Override public void init(FilterConfig filterConfig) throws ServletException { // init bucket registry buckets = new JCacheProxyManager<>(cache); } @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException { HttpServletRequest httpRequest = (HttpServletRequest) servletRequest; String userId = AutentificationHelper.getUserIdFromRequest(httpRequest); // prepare configuration supplier which will be called(on the first interaction with proxy) if the bucket was not saved yet previously. Supplier<BucketConfiguration> configurationLazySupplier = getConfigSupplierForUser(userId); // acquire cheap proxy to the bucket Bucket bucket = proxyManager.builder().build(key, configurationLazySupplier); // tryConsume returns false immediately if no tokens available with the bucket if (bucket.tryConsume(1)) { // the limit is not exceeded filterChain.doFilter(servletRequest, servletResponse); } else { // limit is exceeded HttpServletResponse httpResponse = (HttpServletResponse) servletResponse; httpResponse.setContentType(\"text/plain\"); httpResponse.setStatus(429); httpResponse.getWriter().append(\"Too many requests\"); } } private Supplier<BucketConfiguration> getConfigSupplierForUser(String userId) { return () -> { long translationsPerDay = limitProvider.readPerDayLimitFromAgreementsDatabase(userId); return BucketConfiguratiion.builder() .addLimit(limit -> limit.capacity(translationsPerDay).refillGreedy(1_000, ofDays(1))) .build(); }; } }\n\n3.1.3. Why JCache specification is not enough in modern stacks and since 3.0 were introduced the dedicated modules for Infinispan, Hazelcast, Coherence and Ignite?\n\nAsynchronous processing is very important for high-throughput applications, but JCache specification does not specify asynchronous API, because two early attempts to bring this kind of functionality at spec level 307, 312 were failed in absence of consensus.\n\nSad, but true, if you need for asynchronous API, then JCache extension is useless, and you need to choose from following extensions:\n\nbucket4j-ignite\n\nbucket4j-hazelcast\n\nbucket4j-infinispan\n\nbucket4j-coherence\n\nAlso, implementing the asynchronous support for any other JCache provider outside of the list above should be an easy exercise, so feel free to return back the pull request addressed to cover your favorite JCache provider.\n\n3.1.4. Verification of compatibility with a particular JCache provider is your responsibility\n\nImportant\n\nKeep in mind that there are many non-certified implementations of JCache specifications on the market. Many of them want to increase their popularity by declaring support for the JCache API, but often only the API is supported and the semantic of JCache is totally ignored. Usage Bucket4j with this kind of library should be completely avoided.\n\nBucket4j is only compatible with implementations that obey the JCache specification rules(especially related to EntryProcessor execution). Oracle Coherence, Apache Ignite, Hazelcast are good examples of safe implementations of JCache.\n\nImportant\n\nBecause it is impossible to test all possible JCache providers, you need to test your provider by yourself.\n\nJust run this code in order to be sure that your implementation of JCache provides good isolation for EntryProcessors\n\nimport javax.cache.Cache; import javax.cache.processor.EntryProcessor; import java.util.concurrent.CountDownLatch; import java.io.Serializable; public class CompatibilityTest { final Cache<String, Integer> cache; public CompatibilityTest(Cache<String, Integer> cache) { this.cache = cache; } public void test() throws InterruptedException { String key = \"42\"; int threads = 4; int iterations = 1000; cache.put(key, 0); CountDownLatch latch = new CountDownLatch(threads); for (int i = 0; i < threads; i++) { new Thread(() -> { try { for (int j = 0; j < iterations; j++) { EntryProcessor<String, Integer, Void> processor = (EntryProcessor<String, Integer, Void> & Serializable) (mutableEntry, objects) -> { int value = mutableEntry.getValue(); mutableEntry.setValue(value + 1); return null; }; cache.invoke(key, processor); } } finally { latch.countDown(); } }).start(); } latch.await(); int value = cache.get(key); if (value == threads * iterations) { System.out.println(\"Implementation which you use is compatible with Bucket4j\"); } else { String msg = \"Implementation which you use is not compatible with Bucket4j\"; msg += \", \" + (threads * iterations - value) + \" writes are missed\"; throw new IllegalStateException(msg); } } }\n\nThe check does 4000 increments of integer in parallel and verifies that no one update has been missed. If the check passed then your JCache provider is compatible with Bucket4j, the throttling will work fine in a distributed and concurrent environment. If the check is not passed, then reach out to the particular JCache provider team and consult why its implementation misses the writes.\n\n3.3. Apache Ignite integration\n\nBefore use bucket4j-ignite module please read [bucket4j-jcache documentation](jcache-usage.md), because bucket4j-ignite is just a follow-up of bucket4j-jcache.\n\nBucket4j supports Ignite Thin-Client as well as regular deployment scenarios.\n\nQuestion: Bucket4j already supports JCache since version 1.2. Why it was needed to introduce direct support for Apache Ignite? Answer: Because JCache API (JSR 107) does not specify asynchronous API, developing the dedicated module bucket4j-ignite was the only way to provide asynchrony for users who use Bucket4j and Apache Ignite together.\n\nQuestion: Should I migrate from bucket4j-jcache to bucketj-ignite If I do not need an asynchronous API? Answer: No, you should not migrate to bucketj-ignite in this case.\n\n3.3.1. Dependencies\n\nTo use bucket4j-ignite extension you need to add following dependency:\n\n<dependency> <groupId>com.bucket4j</groupId> <artifactId>bucket4j-ignite</artifactId> <version>8.7.0</version> </dependency>\n\nNote\n\nsee Java compatibility matrix if you need for build that is compatible with Java 8\n\n3.3.2. Example of Bucket instantiation via IgniteProxyManager\n\norg.apache.ignite.IgniteCache<K, byte[]> cache = ...; private static final IgniteProxyManager proxyManager = new IgniteProxyManager(cache); ... BucketConfiguration configuration = BucketConfiguration.builder() .addLimit(limit -> limit.capacity(1_000).refillGreedy(1_000, ofMinutes(1))) .build(); Bucket bucket = proxyManager.builder().build(key, configuration);\n\nImportant\n\nPay attention that IgniteProxyManager requires all nodes in the cluster to contain bucket4j Jars in classpath.\n\n3.3.3. Example of Bucket instantiation via Thin Client\n\norg.apache.ignite.client.ClientCache<K, byte[]> cache = ...; org.apache.ignite.client.ClientCompute clientCompute = ...; private static final IgniteThinClientProxyManager<K> proxyManager = new IgniteThinClientProxyManager(cache, clientCompute) ... BucketConfiguration configuration = BucketConfiguration.builder() .addLimit(limit -> limit.capacity(1_000).refillGreedy(1_000, ofMinutes(1))) .build(); Bucket bucket = proxyManager.builder().build(key, configuration);\n\nImportant\n\nPay attention that IgniteThinClientProxyManager requires all nodes in the cluster to contain bucket4j Jars in classpath.\n\n3.3.4. Example of Bucket instantiation of via Thin Client and IgniteThinClientCasBasedProxyManager\n\norg.apache.ignite.client.ClientCache<K, byte[]> cache = ...; private static final IgniteThinClientCasBasedProxyManager<K> proxyManager = new IgniteThinClientCasBasedProxyManager(cache) ... BucketConfiguration configuration = BucketConfiguration.builder() .addLimit(limit -> limit.capacity(1_000).refillGreedy(1_000, ofMinutes(1))) .build(); Bucket bucket = proxyManager.builder().build(key, configuration);\n\nImportant\n\nIgniteThinClientCasBasedProxyManager does not require all nodes in the cluster to contain bucket4j Jars in classpath, but it operates with more latency, so choose it over IgniteThinClientProxyManager if and only if you have no control over cluster classpath.\n\n3.8. Production checklist especially in the context of distributed systems\n\nBefore using Bucket4j in clustered scenario you need to understand, agree, and configure the following points:\n\nDo not forget about exception handling\n\nWhen working within a distributed system, it is inevitable that requests may cross the border of the current JVM, leading to communication on the network. The network being unreliable, it is impossible to avoid failures. Thus you should embrace this reality and be ready to get unchecked exceptions when interacting with a distributed bucket. It is your responsibility to handle(or ignore) such exceptions:\n\nYou probably do not want to fail business transactions if the grid responsible for throttling goes down. If this is the case you can simply log the exception and continue your business transaction without throttling\n\nIf you wish to fail your business transaction when the grid responsible for throttling goes down, simply rethrow or don’t catch the exception\n\nDo not forget to configure backups\n\nIf the state of any bucket should survive the restart/crash of the grid node that holds its state, you need to configure backups yourself, in a way specific to the particular grid vendor. For example, see how to configure backups for Apache Ignite.\n\nRetention tuning is your responsibility\n\nWhen dealing with multi-tenant scenarios like a bucket per user or a bucket per IP address, the number of buckets in the cache will continuously increase. This is because a new bucket will be created each time a new key is detected. To prevent exhausting the available memory of your cluster you need to configure the following aspects:\n\nExpiration since last access - in order to allow the grid to remove the keys which haven’t been used in a long time. For example, see how to configure expiration policy for Apache Ignite.\n\nMaximum cache size(in units of bytes) - Obviously it is preferable to lose bucket data than lose the whole cluster due to memory exception.\n\nHigh availability(HA) tuning and testing is your responsibility\n\nThere are no special settings for HA supported by Bucket4j because Bucket4j does nothing more than just invoking EntryProcessors on the cache. Instead, Bucket4j relies on you to configure the cache with proper parameters that control redundancy and high availability.\n\nYears of experience working with the distributed system has taught the author that High Availability does not come for free. You need to test and verify that your system remains available. This cannot be provided by this or any other library. Your system will most certainly go down if you do not plan for that.\n\n4.2. Implicit configuration replacement\n\nHow does explicit configuration replacement work for case of distributed buckets:\n\ndistributed bucket operates with configuration that was provided at the time of its creation. Providing the new configuration via RemoteBucketBuilder takes no effect if bucket already exists in the persistent storage, because configuration is stored together with state of bucket. There is only one way to replace configuration of bucket - is explicit calling of replaceConfiguration(or its async analog).\n\nExplicit config replacement can be awkward in the following cases:\n\nIt requires for library client to write the code for configuration replacement. It is unnecessary job, that is especially hard when bucket4j is used behind of high-level frameworks like bucket4j-spring-boot-starter, when end-clients are not mentally prepared to work directly with low-level API of Bucket4j.\n\nIt can confuse the user in the following scenario: user stores limits in the configuration for example in proerties or yaml file, user updates configuration files and restarts application and he becomes surprised because new limits are not applied for buckets that survive application restart in the storage, because as was mentioned above only one way to change the config for already persisted bucket is explicitly calling of replaceConfiguration for each persisted bucket.\n\nFor some persistent technologies like Redis it is costly to identify all buckets that are persisted in the storage, because lack of mechanisms for grouping like tables or caches leads to scan all keys, even keys that points to data that not related to rate-limiting.\n\nImplicit configuration replacement solution:\n\nImplicit configuration replacement feature is addressed to solve the awkwards described above. It works based on configuration version, when bucket detects that persisted configuration version is less that provided through builder API then persisted configuration is being replaced automatically without disturbing the client. Both RemoteBucketBuilder and RemoteAsyncBucketBuilder contains the API to configure desired configuration version.\n\nExample of usage\n\nBucketConfiguration config = ...; BucketProxy bucket = proxyManager.builder() .withImplicitConfigurationReplacement(1, TokensInheritanceStrategy.PROPORTIONALLY) .build(666L, config);\n\n4.3. Framework to implement custom work with your database\n\nThe Bucket4j library allows implementing work with any database. If you didn’t find in distributed realization your database (currently Bucket4j supports the next databases: Redis, Hazelcast, Apache Ignite, Infinispan, Oracle coherence, PostgreSQL, MySQL) you can implement your database as a distributed storage. All what you need to do, extends from io.github.bucket4j.distributed.proxy.generic.select_for_update.AbstractLockBasedProxyManager or AbstractSelectForUpdateBasedProxyManager<T> and override 3 methods and create your implementation which implements from io.github.bucket4j.distributed.proxy.generic.select_for_update.LockBasedTransaction.\n\nStep by step to take that.\n\nFirst of all we need to create our custom proxy manages which extends from AbstractLockBasedProxyManager<T> or AbstractSelectForUpdateBasedProxyManager<T> (as genetic classes takes a type of key table). To define in which class you should extend, need to understand the main idea of these classes:\n\nAbstractLockBasedProxyManager<T> - Uses to realize based on exclusive locks\n\nAbstractSelectForUpdateBasedProxyManager<T> - Uses to realize Select For Update concept\n\nAfter need to override works of allocation transaction, to do that, we should override method allocateTransaction. The main idea of allocateTransaction to just return class which implements LockBasedTransaction (for AbstractLockBasedProxyManager<T>) or SelectForUpdateBasedTransaction (for AbstractSelectForUpdateBasedProxyManager<T>) - we will implement it later And override removeProxy() for remove bucket from the table which store buckets.\n\nSecond of all\n\nNeed to implement LockBasedTransaction or SelectForUpdateBasedTransaction to realize custom work of database for transaction.\n\nTo do that, we need to create a custom class to implement from one of these classes\n\nLockBasedTransaction\n\n/** * Begins transaction if underlying storage requires transactions. * There is strong guarantee that {@link #commit()} or {@link #rollback()} will be called if {@link #begin()} returns successfully. */ void begin(); /** * Rollbacks transaction if underlying storage requires transactions */ void rollback(); /** * Commits transaction if underlying storage requires transactions */ void commit(); /** * Locks data by the key associated with this transaction and returns data that is associated with the key. * There is strong guarantee that {@link #unlock()} will be called if {@link #lockAndGet()} returns successfully. * * @return Returns the data by the key associated with this transaction, or null data associated with key does not exist */ byte[] lockAndGet(); /** * Unlocks data by the key associated with this transaction. */ void unlock(); /** * Creates the data by the key associated with this transaction. * * @param data bucket state to persists */ void create(byte[] data); /** * Updates the data by the key associated with this transaction. * * @param data bucket state to persists */ void update(byte[] data); /** * Frees resources associated with this transaction */ void release();\n\nAs an example, you can see to the PostgreSQL or MySQL realization which based on select for update concept.\n\nSelectForUpdateBasedTransaction\n\n/** * Begins transaction if underlying storage requires transactions. * There is strong guarantee that {@link #commit()} or {@link #rollback()} will be called if {@link #begin()} returns successfully. */ void begin(); /** * Rollbacks transaction if underlying storage requires transactions */ void rollback(); /** * Commits transaction if underlying storage requires transactions */ void commit(); /** * Locks data by the key associated with this transaction and returns data that is associated with the key. * * @return the data by the key associated with this transaction, or null data associated with key does not exist */ LockAndGetResult tryLockAndGet(); /** * Creates empty data by for the key associated with this transaction. * This operation is required to be able to lock data in the scope of next transaction. * * @return true if data has been inserted */ boolean tryInsertEmptyData(); /** * Updates the data by the key associated with this transaction. * * @param data bucket state to persists */ void update(byte[] data); /** * Frees resources associated with this transaction */ void release();"
    }
}
{
    "id": "correct_subsidiary_00134_3",
    "rank": 39,
    "data": {
        "url": "https://bucket4j.com/8.12.1/toc.html",
        "read_more_link": "",
        "language": "en",
        "title": "Bucket4j 8.12.1 Reference",
        "top_image": "",
        "meta_img": "",
        "images": [],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "3.1. Production checklist especially in the context of distributed systems\n\nBefore using Bucket4j in clustered scenario you need to understand, agree, and configure the following points:\n\nDo not forget about exception handling\n\nWhen working within a distributed system, it is inevitable that requests may cross the border of the current JVM, leading to communication on the network. The network being unreliable, it is impossible to avoid failures. Thus, you should embrace this reality and be ready to get unchecked exceptions when interacting with a distributed bucket. It is your responsibility to handle(or ignore) such exceptions:\n\nYou probably do not want to fail business transactions if the grid responsible for throttling goes down. If this is the case you can simply log the exception and continue your business transaction without throttling\n\nIf you wish to fail your business transaction when the grid responsible for throttling goes down, simply rethrow or don’t catch the exception\n\nDo not forget to configure backups\n\nIf the state of any bucket should survive the restart/crash of the grid node that holds its state, you need to configure backups yourself, in a way specific to the particular grid vendor. For example, see how to configure backups for Apache Ignite.\n\nRetention tuning is your responsibility\n\nWhen dealing with multi-tenant scenarios like a bucket per user or a bucket per IP address, the number of buckets in the cache will continuously increase. This is because a new bucket will be created each time a new key is detected.\n\nTo prevent exhausting the available memory of your cluster you need to configure the following aspects: * Maximum cache size(in units of bytes) - Obviously it is preferable to lose bucket data than lose the whole cluster due to memory exception. * Expiration policy Bucket4j provides way to configure flexible per-entry expiration for mostly integrations(excepting Apache Ignite). You need to read Bucket4j documentation for your particular backend in order tp find-out the way to configure expire policy.\n\nHigh availability(HA) tuning and testing is your responsibility\n\nThere are no special settings for HA supported by Bucket4j because Bucket4j does nothing more than just invoking EntryProcessors on the cache. Instead, Bucket4j relies on you to configure the cache with proper parameters that control redundancy and high availability.\n\nYears of experience working with the distributed system has taught the author that High Availability does not come for free. You need to test and verify that your system remains available. This cannot be provided by this or any other library. Your system will most certainly go down if you do not plan for that.\n\n4.3. Framework to implement custom work with your database\n\nThe Bucket4j library allows implementing work with any database. If you didn’t find in distributed realization your database (currently Bucket4j supports the next databases: Redis, Hazelcast, Apache Ignite, Infinispan, Oracle coherence, PostgreSQL, MySQL) you can implement your database as a distributed storage. All what you need to do, extends from io.github.bucket4j.distributed.proxy.generic.select_for_update.AbstractLockBasedProxyManager or AbstractSelectForUpdateBasedProxyManager<T> and override 3 methods and create your implementation which implements from io.github.bucket4j.distributed.proxy.generic.select_for_update.LockBasedTransaction.\n\nStep by step to take that.\n\nFirst of all we need to create our custom proxy manages which extends from AbstractLockBasedProxyManager<T> or AbstractSelectForUpdateBasedProxyManager<T> (as genetic classes takes a type of key table). To define in which class you should extend, need to understand the main idea of these classes:\n\nAbstractLockBasedProxyManager<T> - Uses to realize based on exclusive locks\n\nAbstractSelectForUpdateBasedProxyManager<T> - Uses to realize Select For Update concept\n\nAfter need to override works of allocation transaction, to do that, we should override method allocateTransaction. The main idea of allocateTransaction to just return class which implements LockBasedTransaction (for AbstractLockBasedProxyManager<T>) or SelectForUpdateBasedTransaction (for AbstractSelectForUpdateBasedProxyManager<T>) - we will implement it later And override removeProxy() for remove bucket from the table which store buckets.\n\nSecond of all\n\nNeed to implement LockBasedTransaction or SelectForUpdateBasedTransaction to realize custom work of database for transaction.\n\nTo do that, we need to create a custom class to implement from one of these classes\n\nLockBasedTransaction\n\n/** * Begins transaction if underlying storage requires transactions. * There is strong guarantee that {@link #commit()} or {@link #rollback()} will be called if {@link #begin()} returns successfully. */ void begin(); /** * Rollbacks transaction if underlying storage requires transactions */ void rollback(); /** * Commits transaction if underlying storage requires transactions */ void commit(); /** * Locks data by the key associated with this transaction and returns data that is associated with the key. * There is strong guarantee that {@link #unlock()} will be called if {@link #lockAndGet()} returns successfully. * * @return Returns the data by the key associated with this transaction, or null data associated with key does not exist */ byte[] lockAndGet(); /** * Unlocks data by the key associated with this transaction. */ void unlock(); /** * Creates the data by the key associated with this transaction. * * @param data bucket state to persists */ void create(byte[] data); /** * Updates the data by the key associated with this transaction. * * @param data bucket state to persists */ void update(byte[] data); /** * Frees resources associated with this transaction */ void release();\n\nAs an example, you can see to the PostgreSQL or MySQL realization which based on select for update concept.\n\nSelectForUpdateBasedTransaction\n\n/** * Begins transaction if underlying storage requires transactions. * There is strong guarantee that {@link #commit()} or {@link #rollback()} will be called if {@link #begin()} returns successfully. */ void begin(); /** * Rollbacks transaction if underlying storage requires transactions */ void rollback(); /** * Commits transaction if underlying storage requires transactions */ void commit(); /** * Locks data by the key associated with this transaction and returns data that is associated with the key. * * @return the data by the key associated with this transaction, or null data associated with key does not exist */ LockAndGetResult tryLockAndGet(); /** * Creates empty data by for the key associated with this transaction. * This operation is required to be able to lock data in the scope of next transaction. * * @return true if data has been inserted */ boolean tryInsertEmptyData(); /** * Updates the data by the key associated with this transaction. * * @param data bucket state to persists */ void update(byte[] data); /** * Frees resources associated with this transaction */ void release();"
    }
}
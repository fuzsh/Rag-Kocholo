{
    "id": "correct_subsidiary_00134_3",
    "rank": 22,
    "data": {
        "url": "https://ateam25.rssing.com/chan-30962332/all_p1.html",
        "read_more_link": "",
        "language": "en",
        "title": "Coherence – ATeam Chronicles",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://pixel.quantserve.com/pixel/p-KygWsHah2_7Qa.gif",
            "https://www.ateam-oracle.com/wp-content/uploads/2013/08/10000-csv2.png",
            "https://www.ateam-oracle.com/wp-content/uploads/2013/08/10000-csv3.png",
            "https://s1.wp.com/wp-includes/images/smilies/icon_wink.gif?m=1129645325g",
            "https://www.ateam-oracle.com/wp-content/uploads/2014/05/EM_Coh1.png",
            "https://www.ateam-oracle.com/wp-content/uploads/2014/05/EM_Coh2.png",
            "https://www.ateam-oracle.com/wp-content/uploads/2014/05/EM_Coh3.png",
            "https://www.ateam-oracle.com/wp-content/uploads/2014/05/EM_Coh4.png",
            "https://www.ateam-oracle.com/wp-content/uploads/2014/05/EM_Coh5.png",
            "https://www.ateam-oracle.com/wp-content/uploads/2014/05/EM_Coh6.png",
            "https://www.ateam-oracle.com/wp-content/uploads/2014/05/EM_Coh7.png",
            "https://www.ateam-oracle.com/wp-content/uploads/2014/05/EM_Coh8.png",
            "https://www.ateam-oracle.com/wp-content/uploads/2014/05/EM_Coh9.png",
            "https://www.ateam-oracle.com/wp-content/uploads/2014/05/managementbuilderinitial-1024x100.png",
            "https://www.ateam-oracle.com/wp-content/uploads/2014/05/coherencestartupparameters.png",
            "https://www.ateam-oracle.com/wp-content/uploads/2014/05/emcoherencembean1.png",
            "https://augustacrime.com/wp-content/uploads/2019/07/Jessica-Carpenter-37-Simple-battery-150x150.jpg",
            "https://www.trueshayari.in/wp-content/uploads/2018/12/Mahadev-Status-in-Hindi.jpg",
            "https://www.greytrix.com/blogs/sagex3/wp-content/uploads/2021/11/Fig02-Export-customization-screen-1024x500.png",
            "https://augustacrime.com/wp-content/uploads/2017/01/imageCRESTONCURRY.jpg",
            "https://thepost.s3.amazonaws.com/wp-content/uploads/2014/08/0CA0F25I-150x150.jpg",
            "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhYlXaAGT5qxGYlcbexesYx_-ff9dQu80qBf1mKyPQ-ZUENH5qAUXqSUAH58_1VU4qAOlrr3ph_2ZsNlwnh5BcJ8Nnco6UVvHhAg-Lhgh6r2HEGLpQiM6YrVYtCmR4h-r0STfhOgbeH1C40Ffk7CcrI4QpS31LK0AtQ_T_LWwiLRg3RWSVKHGUMcIzG1XT2/w320-h320/Very%20small%20kolam%20designs%20for%20flats%201a.png",
            "https://3.bp.blogspot.com/-NP1u31H_hVE/WIJo02ELdQI/AAAAAAAAADY/eCMDy6ZuNoESiAXT9rhyePg9W9gujKiKwCEw/s400/p.txt.jpg",
            "https://4.bp.blogspot.com/-D2ybkIpqREU/Wti4IoCYwLI/AAAAAAAABEg/UTVmhPWy1QcrZg6t-w8mJpp49Ho6MfwfQCLcBGAs/s640/Anasuya%2B3.png",
            "https://2.bp.blogspot.com/-7TLcIsTWCRM/WJdhZJ8T1vI/AAAAAAAANlA/7-XTO95WE1494iVWxC5MIpWXckxuscj3QCLcB/s640/meenakshi%2Bjoshi%2Banchor.jpg",
            "https://lh5.ggpht.com/-drHHw9Z5ZwY/Tmp1gKWmZ1I/AAAAAAAALFc/8dMBz_U8mJw/s400/dragon_ball_73.gif",
            "https://i.imgur.com/yTvNsOQl.jpg",
            "https://busyteacher.org/uploads/posts/2012-11/thumbs/1353086477_make-or-do-collocations-key-0.png",
            "https://1.bp.blogspot.com/-m_gxW9wUrow/YWpAej2ComI/AAAAAAAADwI/lPbemMAKTPcFQMerHxF4zzzDiJkzDqjeACLcBGAsYHQ/w300-h400/WhatsApp%2BImage%2B2021-10-16%2Bat%2B11.00.18%2BAM.jpeg",
            "https://augustacrime.com/wp-content/uploads/2016/09/imageADRIANHOSEY.jpg",
            "https://chrisukorg.files.wordpress.com/2015/02/perry.jpg?w=529&h=511",
            "https://img.over-blog.com/600x881/3/61/67/39/TRAVAIL/Lexique0004.jpg",
            "https://s3.amazonaws.com/nixle/uploads/pub_media/md/user24872-1464275450-media1_a5a7a6_240_180_PrsMe_.jpeg",
            "https://busyteacher.org/uploads/posts/2015-06/thumbs/1434962640_crossword-months-0.png",
            "https://cdn.comsol.com/wordpress/2013/08/The-operating-principle-of-the-drug-delivery-device.png",
            "https://2.bp.blogspot.com/-Ifb_WhTLXIM/WznDciL8lUI/AAAAAAAAM4k/AW5glKbYDLE4ABrSubU4RHAs__vbNnMmQCLcBGAs/s400/28%2Bi.PNG",
            "https://jpcdn.it/img/small/71850111dc29343c90e84c90a0de1a2e.jpg",
            "https://www.thesun.ie/wp-content/uploads/sites/3/2023/08/episode-generics-no-n-picture-584113140.jpg?strip=all&&w=620&&h=413&&crop=1",
            "https://community.cadence.com/resized-image/__size/1280x960/__key/communityserver-discussions-components-files/28/pastedimage1721900432893v1.png",
            "https://www.the-sun.com/wp-content/uploads/sites/6/2024/07/walmart-store-receipt-walmart-brand-912546864_4cf2a7.jpg?strip=all&w=960",
            "https://media.adverts.ie/eyJidWNrZXQiOiJtZWRpYS5hZHNpbWcuY29tIiwia2V5IjoiYzU0ZTQ1MjJhMDgyMGNlZjExYzNhNDcyODQ0YzQ0NmY5YjAwMGZiMzEwNGJlN2ZkNGZlYWU3YzE0ODkyMGVkMi5qcGciLCJvdXRwdXRGb3JtYXQiOiJqcGVnIiwiZWRpdHMiOnsicmVzaXplIjp7IndpZHRoIjoyMjcsImhlaWdodCI6MTg3fX19?signature=12517ddb0860daea228c84c6763d11c504f91785b6d74441e8272264aff45f8b",
            "https://i.etsystatic.com/5465916/r/il/bce7ff/1698716462/il_570xN.1698716462_dni8.jpg",
            "https://i.etsystatic.com/5376867/r/il/d97efc/6109916569/il_570xN.6109916569_aboq.jpg",
            "https://i.etsystatic.com/5262314/r/il/096f23/5151254922/il_570xN.5151254922_drr4.jpg",
            "https://i.etsystatic.com/6055453/r/il/d83579/3180156836/il_570xN.3180156836_khfw.jpg",
            "https://i.etsystatic.com/6571804/r/il/5ade57/1835833676/il_570xN.1835833676_31gp.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "//www.rssing.com/favicon.ico",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "A recent A-Team engagement required the development of a custom PartitionAssignmentStrategy (PAS). By way of background, a PAS is an implementation of a Java interface that controls how a Coherence partitioned cache service assigns partitions (primary and backup copies) across the available set of storage-enabled members. While seemingly straightforward, this is actually a very difficult problem to solve. Traditionally, Coherence used a distributed algorithm spread across the cache servers (and as of Coherence 3.7, this is still the default implementation). With the introduction of the PAS interface, the model of operation was changed so that the logic would run solely in the cache service senior member. Obviously, this makes the development of a custom PAS vastly less complex, and in practice does not introduce a significant single point of failure/bottleneck. Note that Coherence ships with a default PAS implementation but it is not used by default. Further, custom PAS implementations are uncommon (this engagement was the first custom implementation that we know of). The particular implementation mentioned above also faced challenges related to managing multiple backup copies but that won’t be discussed here.\n\nThere were a few challenges that arose during design and implementation:\n\nNaive algorithms had an unreasonable upper bound of computational cost.\n\nThere was significant complexity associated with configurations where the member count varied significantly between physical machines.\n\nMost of the complexity of a PAS is related to rebalancing, not initial assignment (which is usually fairly simple). A custom PAS may need to solve several problems simultaneously, such as:\n\nEnsuring that each member has a similar number of primary and backup partitions (e.g. each member has the same number of primary and backup partitions)\n\nEnsuring that each member carries similar responsibility (e.g. the most heavily loaded member has no more than one partition more than the least loaded).\n\nEnsuring that each partition is on the same member as a corresponding local resource (e.g. for applications that use partitioning across message queues, to ensure that each partition is collocated with its corresponding message queue).\n\nEnsuring that a given member holds no more than a given number of partitions (e.g. no member has more than 10 partitions)\n\nEnsuring that backups are placed far enough away from the primaries (e.g. on a different physical machine or a different blade enclosure)\n\nAchieving the above goals while ensuring that partition movement is minimized.\n\nThese objectives can be even more complicated when the topology of the cluster is irregular. For example, if multiple cluster members may exist on each physical machine, then clearly the possibility exists that at certain points (e.g. following a member failure), the number of members on each machine may vary, in certain cases significantly so. Consider the case where there are three physical machines, with 3, 3 and 9 members each (respectively). This introduces complexity since the backups for the 9 members on the the largest machine must be spread across the other 6 members (to ensure placement on different physical machines), preventing an even distribution. For any given problem like this, there are usually reasonable compromises available, but the key point is that objectives may conflict under extreme (but not at all unlikely) circumstances.\n\nThe most obvious general purpose partition assignment algorithm (possibly the only general purpose one) is to define a scoring function for a given mapping of partitions to members, and then apply that function to each possible permutation, selecting the most optimal permutation. This would result in N! (factorial) evaluations of the scoring function. This is clearly impractical for all but the smallest values of N (e.g. a partition count in the single digits). It’s difficult to prove that more efficient general purpose algorithms don’t exist, but the key take away from this is that algorithms will tend to either have exorbitant worst case performance or may fail to find optimal solutions (or both) — it is very important to be able to show that worst case performance is acceptable. This quickly leads to the conclusion that the problem must be further constrained, perhaps by limiting functionality or by using domain-specific optimizations. Unfortunately, it can be very difficult to design these more focused algorithms.\n\nIn the specific case mentioned, we constrained the solution space to very small clusters (in terms of machine count) with small partition counts and supported exactly two backup copies, and accepted the fact that partition movement could potentially be significant (preferring to solve that issue through brute force). We then used the out-of-the-box PAS implementation as a fallback, delegating to it for configurations that were not supported by our algorithm. Our experience was that the PAS interface is quite usable, but there are intrinsic challenges to designing PAS implementations that should be very carefully evaluated before committing to that approach.\n\nAll content listed on this page is the property of Oracle Corp. Redistribution not allowed without written permission\n\nWhen integrating Coherence into applications, each application has its own set of requirements with respect to data integrity guarantees. Developers often describe these requirements using expressions like “avoiding dirty reads” or “making sure that updates are transactional”, but we often find that even in a small group of people, there may be a wide range of opinions as to what these terms mean. This may simply be due to a lack of familiarity, but given that Coherence sits at an intersection of several (mostly) unrelated fields, it may be a matter of conflicting vocabularies (e.g. “consistency” is similar but different in transaction processing versus multi-threaded programming).\n\nSince almost all data read consistency issues are related to the concept of concurrency, it is helpful to start with a definition of that, or rather what it means for two operations to be concurrent. Rather than implying that they occur “at the same time”, concurrency is a slightly weaker statement — it simply means that it can’t be proven that one event precedes (or follows) the other. As an example, in a Coherence application, if two client members mutate two different cache entries sitting on two different cache servers at roughly the same time, it is likely that one update will precede the other by a significant amount of time (say 0.1ms). However, since there is no guarantee that all four members have their clocks perfectly synchronized, and there is no way to precisely measure the time it takes to send a given message between any two members (that have differing clocks), we consider these to be concurrent operations since we can not (easily) prove otherwise.\n\nSo this leads to a question that we hear quite frequently: “Are the contents of the near cache always synchronized with the underlying distributed cache?”. It’s easy to see that if an update on a cache server results in a message being sent to each near cache, and then that near cache being updated that there is a window where the contents are different. However, this is irrelevant, since even if the application reads directly from the distributed cache, another thread update the cache before the read is returned to the application.\n\nEven if no other member modifies a cache entry prior to the local near cache entry being updated (and subsequently read), the purpose of reading a cache entry is to do something with the result, usually either displaying for consumption by a human, or by updating the entry based on the current state of the entry. In the former case, it’s clear that if the data is updated faster than a human can perceive, then there is no problem (and in many cases this can be relaxed even further). For the latter case, the application must assume that the value might potentially be updated before it has a chance to update it. This almost aways the case with read-only caches, and the solution is the traditional optimistic transaction pattern, which requires the application to explicitly state what assumptions it made about the old value of the cache entry. If the application doesn’t want to bother stating those assumptions, it is free to lock the cache entry prior to reading it, ensuring that no other threads will mutate the entry, a pessimistic approach.\n\nThe optimistic approach relies on what is sometimes called a “fuzzy read”. In other words, the application assumes that the read should be correct, but it also acknowledges that it might not be. (I use the qualifier “sometimes” because in some writings, “fuzzy read” indicates the situation where the application actually sees an original value and then later sees an updated value within the same transaction — however, both definitions are roughly equivalent from an application design perspective). If the read is not correct it is called a “stale read”. Going back to the definition of concurrency, it may seem difficult to precisely define a stale read, but the practical way of detecting a stale read is that is will cause the encompassing transaction to roll back if it tries to update that value.\n\nThe pessimistic approach relies on a “coherent read”, a guarantee that the value returned is not only the same as the primary copy of that value, but also that it will remain that way. In most cases this can be used interchangeably with “repeatable read” (though that term has additional implications when used in the context of a database system).\n\nIn none of cases above is it possible for the application to perform a “dirty read”. A dirty read occurs when the application reads a piece of data that was never committed. In practice the only way this can occur is with multi-phase updates such as transactions, where a value may be temporarily update but then withdrawn when a transaction is rolled back. If another thread sees that value prior to the rollback, it is a dirty read. If an application uses optimistic transactions, dirty reads will merely result in a lack of forward progress (this is actually one of the main risks of dirty reads — they can be chained and potentially cause cascading rollbacks).\n\nThe concepts of dirty reads, fuzzy reads, stale reads and coherent reads are able to describe the vast majority of requirements that we see in the field. However, the important thing is to define the terms used to define requirements. A quick web search for each of the terms in this article will show multiple meanings, so I’ve selected what are generally the most common variations, but it never hurts to state each definition explicitly if they are critical to the success of a project (many applications have sufficiently loose requirements that precise terminology can be avoided).\n\nAll content listed on this page is the property of Oracle Corp. Redistribution not allowed without written permission\n\nDisabling hardware multicast (by configuring well-known addresses aka WKA) will place significant stress on the network. For messages that must be sent to multiple servers, rather than having a server send a single packet to the switch and having the switch broadcast that packet to the rest of the cluster, the server must send a packet to each of the other servers. While hardware varies significantly, consider that a server with a single gigabit connection can send at most ~70,000 packets per second. To continue with some concrete numbers, in a cluster with 500 members, that means that each server can send at most 140 cluster-wide messages per second. And if there are 10 cluster members on each physical machine, that number shrinks to 14 cluster-wide messages per second (or with only mild hyperbole, roughly zero). It is also important to keep in mind that network I/O is not only expensive in terms of the network itself, but also the consumption of CPU required to send (or receive) a message (due to things like copying the packet bytes, processing a interrupt, etc).\n\nFortunately, Coherence is designed to rely primarily on point-to-point messages, but there are some features that are inherently one-to-many:\n\nAnnouncing the arrival or departure of a member\n\nUpdating partition assignment maps across the cluster\n\nCreating or destroying a NamedCache\n\nInvalidating a cache entry from a large number of client-side near caches\n\nDistributing a filter-based request across the full set of cache servers (e.g. queries, aggregators and entry processors)\n\nInvoking clear() on a NamedCache\n\nThe first few of these are operations that are primarily routed through a single senior member, and also occur infrequently, so they usually are not a primary consideration. There are cases, however, where the load from introducing new members can be substantial (to the point of destabilizing the cluster). Consider the case where cluster in the first paragraph grows from 500 members to 1000 members (holding the number of physical machines constant). During this period, there will be 500 new member introductions, each of which may consist of several cluster-wide operations (for the cluster membership itself as well as the partitioned cache services, replicated cache services, invocation services, management services, etc). Note that all of these introductions will route through that one senior member, which is sharing its network bandwidth with several other members (which will be communicating to a lesser degree with other members throughout this process). While each service may have a distinct senior member, there’s a good chance during initial startup that a single member will be the senior for all services (if those services start on the senior before the second member joins the cluster). It’s obvious that this could cause CPU and/or network starvation. In the current release of Coherence (3.7.1.3 as of this writing), the pure unicast code path also has less sophisticated flow-control for cluster-wide messages (compared to the multicast-enabled code path), which may also result in significant heap consumption on the senior member’s JVM (from the message backlog). This is almost never a problem in practice, but with sufficient CPU or network starvation, it could become critical.\n\nFor the non-operational concerns (near caches, queries, etc), the application itself will determine how much load is placed on the cluster. Applications intended for deployment in a pure unicast environment should be careful to avoid excessive dependence on these features. Even in an environment with multicast support, these operations may scale poorly since even with a constant request rate, the underlying workload will increase at roughly the same rate as the underlying resources are added.\n\nUnless there is an infrastructural requirement to the contrary, multicast should be enabled. If it can’t be enabled, care should be taken to ensure the added overhead doesn’t lead to performance or stability issues. This is particularly crucial in large clusters.\n\nAll content listed on this page is the property of Oracle Corp. Redistribution not allowed without written permission\n\nWhile the costs of XA transactions are well known (e.g. increased data contention, higher latency, significant disk I/O for logging, availability challenges, etc.), in many cases they are the most attractive option for coordinating logical transactions across multiple resources.\n\nThere are a few common approaches when integrating Coherence into applications via the use of an application server’s transaction manager:\n\nUse of Coherence as a read-only cache, applying transactions to the underlying database (or any system of record) instead of the cache.\n\nUse of TransactionMap interface via the included resource adapter.\n\nUse of the new ACID transaction framework, introduced in Coherence 3.6.\n\nEach of these may have significant drawbacks for certain workloads.\n\nUsing Coherence as a read-only cache is the simplest option. In this approach, the application is responsible for managing both the database and the cache (either within the business logic or via application server hooks). This approach also tends to provide limited benefit for many workloads, particularly those workloads that either have queries (given the complexity of maintaining a fully cached data set in Coherence) or are not read-heavy (where the cost of managing the cache may outweigh the benefits of reading from it). All updates are made synchronously to the database, leaving it as both a source of latency as well as a potential bottleneck. This approach also prevents addressing “hot data” problems (when certain objects are updated by many concurrent transactions) since most database servers offer no facilities for explicitly controlling concurrent updates. Finally, this option tends to be a better fit for key-based access (rather than filter-based access such as queries) since this makes it easier to aggressively invalidate cache entries without worrying about when they will be reloaded. The advantage of this approach is that it allows strong data consistency as long as optimistic concurrency control is used to ensure that database updates are applied correctly regardless of whether the cache contains stale (or even dirty) data. Another benefit of this approach is that it avoids the limitations of Coherence’s write-through caching implementation.\n\nTransactionMap is generally used when Coherence acts as system of record. TransactionMap is not generally compatible with write-through caching, so it will usually be either used to manage a standalone cache or when the cache is backed by a database via write-behind caching. TransactionMap has some restrictions that may limit its utility, the most significant being:\n\nThe lock-based concurrency model is relatively inefficient and may introduce significant latency and contention. As an example, in a typical configuration, a transaction that updates 20 cache entries will require roughly 40ms just for lock management (assuming all locks are granted immediately, and excluding validation and writing which will require a similar amount of time). This may be partially mitigated by denormalizing (e.g. combining a parent object and its set of child objects into a single cache entry), at the cost of increasing false contention (e.g. transactions will conflict even when updating different child objects).\n\nIf the client (application server JVM) fails during the commit phase, locks will be released immediately, and the transaction may be partially committed. In practice, this is usually not as bad as it may sound since the commit phase is usually very short (all locks having been previously acquired). Note that this vulnerability does not exist when a single NamedCache is used and all updates are confined to a single partition (generally implying the use of partition affinity).\n\nThe unconventional TransactionMap API is cumbersome but manageable.\n\nOnly a few methods are transactional, primarily get(), put() and remove().\n\nThe ACID transactions framework (accessed via the Connection class) provides atomicity guarantees by implementing the NamedCache interface, maintaining its own cache data and transaction logs inside a set of private partitioned caches. This feature may be used as either a local transactional resource or as logging XA resource. However, a lack of database integration precludes the use of this functionality for most applications. A side effect of this is that this feature has not seen significant adoption, meaning that any use of this is subject to the usual headaches associated with being an early adopter (greater chance of bugs and greater risk of hitting an unoptimized code path). As a result, for the moment, we generally recommend against using this feature.\n\nIn summary, it is possible to use Coherence in XA-oriented applications, and several customers are doing this successfully, but it is not a core usage model for the product, so care should be taken before committing to this path. For most applications, the most robust solution is normally to use Coherence as a read-only cache of the underlying data resources, even if this prevents taking advantage of certain product features.\n\nAll content listed on this page is the property of Oracle Corp. Redistribution not allowed without written permission\n\nThe graph above was generated from the output of the Coherence Datagram Test utility. The Coherence Datagram Test is a tool that sends and receives UDP packets between two ore more machines to evaluate the health and performance of the network between those machines. The above test was run for 100 secs on two server-class machines with a 1 Gb Ethernet connection to the same switch. I think it’s pretty clear from the graph that there is significant packet loss between the two machines. Here’s what the graph looks like on a healthy network:\n\nThe first step is to actually run the Datagram Test to generate report data:\n\nserver1$ java -server -cp coherence.jar com.tangosol.net.DatagramTest -local 192.168.1.100 -log 192.168.1.100.log -txDurationMs 100000 -polite 192.168.1.101\n\nserver2$ java -server -cp coherence.jar com.tangosol.net.DatagramTest -local 192.168.1.101 -log 192.168.1.101.log -txDurationMs 100000 192.168.1.100\n\nThe above pair of commands will run a bi-directional test for 100 seconds, generating a tab-delimited report in the file specified by -log. As of Coherence 3.6, the tab-delimited report spits out aggregated lifetime (since the test began) metrics every 100,000 (by default) received packets. For analyzing packet loss, it makes more sense to look at the metrics accumulated between reporting intervals rather than since the beginning of the test, since lifetime metrics could mask spikes that occur later in the test. Luckily, the per interval metrics we need to look at can be derived from the lifetime metrics. The following awk script will calculate the additional columns of interest (as well as fix a bug in the test where the data columns don’t align with the header columns due to two missing delimiters):\n\n#!/usr/bin/awk -f BEGIN { FS = \"[\\t\\r\\n]\"; } # Header line /^publisher/ { if (FILENAME == \"\") { FILENAME = \"stdin\"; } else { print(\"Processing \" FILENAME); } gsub(/[\\r\\n]/, \"\", $0); header = sprintf(\"%s\\tinterval duration secs\\tinterval missing packets\\tinterval drop rate\\tinterval success rate\\tinterval throughput mb/sec\", $0); for (outfile in aOutfile) { close(aOutfile[outfile]); } delete aPrevSent; delete aPrevReceived; delete aPrevMissing; delete aPrevDurationMillis; delete aDurationOffset; delete aOutfile; next; } # Initialize prev values aPrevSent[$1] == \"\" { aPrevSent[$1] = 0; aPrevReceived[$1] = 0; aPrevMissing[$1] = 0; aPrevDurationMillis[$1] = 0; aDurationOffset[$1] = 0; aOutfile[$1] = FILENAME \".\" substr($1, 2, length($1)) \".csv\"; if (aOutfile[$1] ~ /^stdin/) { print(header); } else { print(header) > aOutfile[$1]; } } # Account for packet sequence restart $2 < aPrevDurationMillis[$1] { aPrevSent[$1] = 0; aPrevReceived[$1] = 0; aPrevMissing[$1] = 0; aDurationOffset[$1] += aPrevDurationMillis[$1]; } # Skip duplicate lines $2 == aPrevDurationMillis[$1] { next; } { split($11, aOoo, /^[0-9]/); sOoo = sprintf(\"%s\\t%s\", substr($11, 1, 1), aOoo[2]); split($13, aGapMillis, /^[0-9]/); sGapMillis = sprintf(\"%s\\t%s\", substr($13, 1, 1), aGapMillis[2]); cIntervalDurationMillis = $2 - aPrevDurationMillis[$1]; cIntervalSent = $6 - aPrevSent[$1]; cIntervalReceived = $7 - aPrevReceived[$1]; cIntervalMissing = $8 - aPrevMissing[$1]; dflIntervalDropRate = cIntervalMissing / cIntervalSent; dflIntervalSuccessRate = 1 - dflIntervalDropRate; dflIntervalThroughput = (($3 * cIntervalReceived) / (cIntervalDurationMillis / 1000)) / (1024 * 1024); aPrevDurationMillis[$1] = $2; aPrevSent[$1] = $6; aPrevReceived[$1] = $7; aPrevMissing[$1] = $8; if (aOutfile[$1] ~ /^stdin/) { printf(\"%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%.3f\\t%d\\t%f\\t%f\\t%d\\n\", $1, $2 + aDurationOffset[$1], $3, $4, $5, $6, $7, $8, $9, $10, sOoo, $12, sGapMillis, cIntervalDurationMillis / 1000, cIntervalMissing, dflIntervalDropRate, dflIntervalSuccessRate, dflIntervalThroughput); } else { printf(\"%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%.3f\\t%d\\t%f\\t%f\\t%d\\n\", $1, $2 + aDurationOffset[$1], $3, $4, $5, $6, $7, $8, $9, $10, sOoo, $12, sGapMillis, cIntervalDurationMillis / 1000, cIntervalMissing, dflIntervalDropRate, dflIntervalSuccessRate, dflIntervalThroughput) > aOutfile[$1]; } }\n\nThis script will take the output of the -log option and produce a new file. Assuming you save the contents of the above script to augment-datagram-test.awk and set the execute bit, you can use the script as follows:\n\nserver1$ ./augment-datagram-test.awk 192.168.1.101.log\n\nThe above command will generate a new file called 192.168.1.101.log.192.168.1.100:10000.csv which contains the additional columns “interval duration secs”, “interval missing packets”, “interval drop rate”, “interval success rate” and “interval throughput mb/sec”. The script will produce one csv file for each publisher present in the tab-delimited report. The script will also accept multiple tab-delimited files as input, processing each one independently, and can also accept input piped through stdin (with output going to stdout).\n\nTo actually generate the graphs, I use R. I encountered R earlier this year working with a customer, but didn’t have the chance to play around with it myself. Before I decided to use R, I was taking the output from my awk script and importing into a spreadsheet application and then generating graphs. This proved to be quite tedious and involved too many mouse clicks for my taste, so I turned to R to let me script the process and eliminate the need for a spreadsheet application altogether. R is also much more flexible when it comes to producing graphs, as you have complete control over the plot area. After a few days of playing around with R, I was able to come up with the following script to generate the graphs seen at the beginning of this post:\n\nargs <- commandArgs(TRUE) for (file in args) { outfile <- paste(file, \".png\", sep = \"\") cat(\"Plotting \", file, \" as \", outfile, \"\\n\", sep = \"\") # Read and process input file dgt <- read.table(file, header = TRUE, sep = \"\\t\") x <- dgt$duration.ms / 1000 y <- dgt$interval.drop.rate * 100 x.range <- c(0, max(x)) y.range <- c(0, max(y, 20)) nonzero <- which(y > 0) loss.intervals <- (length(nonzero) / length(y)) * 100 throughput.range <- c(0, max(dgt$interval.throughput.mb.sec, 120)) title <- sub(\"\\\\.log\\\\.\", \" <- \", file) title <- sub(\"\\\\.csv\", \"\", title) # Create plot as PNG png(filename = outfile, height = 400, width = 600, bg = \"white\") # Set margins to make room for right-side axis labels par(mar = c(7,5,4,5) + 0.1) # Plot packet loss line plot(x, y, type = \"l\", main = title, xlab = \"Time (secs)\", ylab = \"Loss (%)\", col = \"blue\", xlim = x.range, ylim = y.range, lwd = 2) # Circle points where packet loss > 0 points(x[nonzero], y[nonzero], cex=1.5) # Plot throughput line lines(x, dgt$interval.throughput.mb.sec * (y.range[2] / throughput.range[2]), col = \"green\", lwd = 2) # Create right-side axis labels and tick marks axis(4, at = y.range[2] * c(0:4) / 4, labels = (throughput.range[2] / 4) * c(0:4)) mtext(\"Throughput (MB/s)\", side = 4, line = 3) # Draw the background grid lines grid() # Report the number of intervals that experienced loss (as a %) mtext(sprintf(\"Intervals w/ Loss: %.2f%%\", loss.intervals), side = 1, line = 3, adj = 1) # Create the legend at the bottom legend(\"bottom\", inset = -0.4, c(\"loss\", \"throughput\"), col = c(\"blue\", \"green\"), lty = 1, lwd = 2, bty = \"n\", horiz = TRUE, xpd = TRUE) # Close the PNG dev.off() }\n\nAssuming you save the contents of the above script as plot-datagram.r, you can invoke the script as follows:\n\nserver1$ r -q --slave -f plot-datagram.r --args 192.168.1.101.log.192.168.1.100:10000.csv\n\nThe output from the above command will be a new file called 192.168.1.101.log.192.168.1.100:10000.csv.png which represents a graph of both packet loss and throughput over the duration of the test. The circles indicate intervals where packet loss occurred. This script can also accept multiple files as input, generating a graph for each in a separate file.\n\nWith both scripts in hand, generating graphs to visualize packet loss from the output of the Datagram Test can be done in a few seconds:\n\nserver1$ ./augment-datagram-test.awk *.log server1$ r -q --slave -f plot-datagram.r --args *.csv\n\nAll content listed on this page is the property of Oracle Corp. Redistribution not allowed without written permission\n\nRecently, I’ve worked on a couple of projects that required pre-loading data from an Oracle Database table into an Oracle Coherence cache. There are many ways to accomplish this task, but what I’ve found to work well is to distribute the load processing to the Coherence grid itself. To do this, I use the Coherence Invocation Service.\n\nTo get started, let’s look at the SQL query that will be used to retrieve all rows from the underlying table (all columns have type VARCHAR2):\n\nSELECT id, customer_id, zip5, zip4 FROM customer_zip\n\nThe class definition for the objects to be cached looks like:\n\npublic class CustomerZip implements ExternalizableLite, PortableObject { // ----- constructors ---------------------------------------------------- public CustomerZip() { } public CustomerZip(String sId, String sCustomerId, String sZip5, String sZip4) { m_sId = sId; m_sCustomerId = sCustomerId; m_sZip5 = sZip5; m_sZip4 = sZip4; } // ----- accessors/mutators ---------------------------------------------- /* removed for brevity */ // ----- ExternalizableLite interface ------------------------------------ /* removed for brevity */ // ----- PortableObject interface ---------------------------------------- /* removed for brevity */ // ----- data members ---------------------------------------------------- private String m_sZip5; private String m_sZip4; private String m_sCustomerId; private String m_sId; }\n\nThe first approach I tried involved pulling back all of the ids from the table, running them through PartitionedService#getKeyOwner and then submitting a task to each member with the set of ids for that member to load. This method leverages Coherence’s data partitioning to distribute the rows among the loading members. This worked fine in my testing with a small number of rows, but when I applied this to the full data set of over 13 million rows, I quickly ran out of memory trying to query and process all of the ids. In addition, querying and processing the ids takes time.\n\nThe second, and final, approach I tried involved pulling back only the row count. Dividing the rows up among the loading members was now simply a matter of establishing the first and last rows to load for each member. I can then use the Oracle pseudocolumn ROWNUM to execute the following query on each member:\n\nSELECT * FROM (SELECT a.*, ROWNUM r FROM (SELECT id, customer_id, zip5, zip4 FROM customer_zip ORDER BY id) a WHERE ROWNUM <= ?) WHERE r >= ?\n\nThis query allows each loading member to specify the last and first rows to load and allows the database to filter out all of the rows outside its range. In my testing, I found that range sizes beyond a certain threshold started performing exponentially slower (perhaps a DB tuning issue, but IANADBA ). You could easily run into this scenario with a large number of rows and a small number of loading members. To handle this situation, I further broke down each member’s range into smaller ranges and had each member execute multiple queries. Processing the results of these queries and performing bulk puts into the cache requires breaking up the results into batches as well. Here’s a look at the code that actually executes the query and inserts the entries into Coherence (to be executed on each loading member). This code is actually part of a CacheLoader implementation that is used for read-through as well. Having the read-through and pre-load logic co-located allows me to share database properties (connection information, SQL statements, etc…).\n\npublic void preload(NamedCache cache, int iFirstRow, int iLastRow, int cFetchSize, int cMaxQueryRange) { String sSqlQuery = ...; // see above String sCacheName = cache.getCacheName(); Connection con = null; PreparedStatement stmtPrep = null; ResultSet rs = null; try { con = getConnection(); stmtPrep = con.prepareStatement(sSqlQuery); stmtPrep.setFetchSize(cFetchSize); // break the query up into batches based on cMaxQueryRange int cRows = (iLastRow - iFirstRow) + 1; int cBatches = cRows / cMaxQueryRange; int cRemaining = cRows % cMaxQueryRange; // add additional batch to handle any remainder cBatches += cRemaining == 0 ? 0 : 1; Map mapBuffer = new HashMap(cFetchSize); int iBatchFirstRow; int iBatchLastRow = iFirstRow - 1; int cRowsLoadedTotal = 0; log(\"Executing preload query in \" + cBatches + \" batches\"); for (int i = 0; i < cBatches; ++i) { iBatchFirstRow = iBatchLastRow + 1; // last row for the batch or the entire range iBatchLastRow = Math.min(iLastRow, iBatchFirstRow + (cMaxQueryRange - 1)); stmtPrep.setInt(1, iBatchLastRow); stmtPrep.setInt(2, iBatchFirstRow); rs = stmtPrep.executeQuery(); // process cFetchSize rows at a time while (processResults(rs, mapBuffer, cFetchSize)) { cache.putAll(mapBuffer); mapBuffer.clear(); } rs.close(); } } catch (SQLException e) { log(e); throw new RuntimeException(e); } finally { close(con, stmtPrep, rs); } } protected boolean processResults(ResultSet rs, Map mapResults, int cFetchSize) throws SQLException { for (int i = 0; i < cFetchSize && rs.next(); ++i) { // create domain object from single row CustomerZip customerZip = createCustomerZip(rs); mapResults.put(customerZip.getId(), customerZip); } return mapResults.size() > 0; }\n\nThe final piece of required code is the one that generates the ranges for each member and issues each member a task to execute. As I mentioned earlier, I use the Coherence Invocation Service to asynchronously execute a task on each loading member. For my use case, the set of loading members is simply every member running the Invocation Service, except for the member issuing the tasks:\n\nprotected Map<Member, PreloadTask> generateTasks(Set<Member> setMembers, int cRows) { Map<Member, PreloadTask> mapTasks = new HashMap<Member, PreloadTask>(setMembers.size()); if (cRows <= m_cFetchSize) { // for small number of rows, just send the load to one member Member member = setMembers.iterator().next(); PreloadTask task = new PreloadTask(m_sCacheName, 1, cRows, m_cFetchSize, m_cMaxQueryRange); mapTasks.put(member, task); } else { int cMembers = setMembers.size(); int cMinRowsPerMember = cRows / cMembers; int cRemainingRows = cRows % cMembers; int iFirstRow; int iLastRow = 0; for (Member member : setMembers) { iFirstRow = iLastRow + 1; iLastRow = iFirstRow + cMinRowsPerMember + (cRemainingRows-- > 0 ? 1 : 0) - 1; PreloadTask task = new PreloadTask(m_sCacheName, iFirstRow, iLastRow, m_cFetchSize, m_cMaxQueryRange); mapTasks.put(member, task); } } return mapTasks; }\n\nThe final step is to asynchronously invoke each member’s task, and then wait for all of them to finish. I use a CountDownLatch and an InvocationObserver to track the completion of all tasks:\n\npublic void preloadCache() { final String sCacheName = \"CustomerZipCache\"; int cRows = getRowCount(); InvocationService serviceInv = (InvocationService) CacheFactory.getService(\"InvocationService\"); long ldtStart = System.currentTimeMillis(); Set<Member> setLoadingMembers = getLoadingMembers(serviceInv); Map<Member, PreloadTask> mapMemberTasks = generateTasks(setLoadingMembers, cRows); // prepare the invocation observer int cTasks = mapMemberTasks.size(); final CountDownLatch latch = new CountDownLatch(cTasks); InvocationObserver observer = new InvocationObserver() { public void memberCompleted(Member member, Object oResult) { latch.countDown(); log(String.format(\"%s: load finished on %s\", sCacheName, member.toString())); } public void memberFailed(Member member, Throwable eFailure) { // TODO: resubmit tasks due to transient failures latch.countDown(); log(String.format(\"%s: load failed on %s\", sCacheName, member.toString())); CacheFactory.log(eFailure); } public void memberLeft(Member member) { // TODO: resubmit to a member that is up latch.countDown(); log(String.format(\"%s: member left before load finished (%s)\", sCacheName, member.toString())); } public void invocationCompleted() { log(String.format(\"%s: invocation has completed\", sCacheName)); } }; // asynchronously execute each member's task for (Map.Entry<Member, PreloadTask> entry : mapMemberTasks.entrySet()) { Member member = entry.getKey(); Set setTaskMembers = Collections.singleton(member); PreloadTask task = entry.getValue(); serviceInv.execute(task, setTaskMembers, observer); log(String.format(\"%s: rows %d-%d sent to %s\", sCacheName, task.getFirstKey(), task.getLastKey(), member.toString())); } // wait for all tasks to finish try { latch.await(); } catch (InterruptedException e) { } long lDurationMillis = System.currentTimeMillis() - ldtStart; log(String.format(\"%s: pre-loaded %d rows in %.3f secs (%.3f rows/sec)\", sCacheName, cRows, lDurationMillis / 1000.0, cRows / (lDurationMillis / 1000.0))); NamedCache cache = CacheFactory.getCache(sCacheName); log(String.format(\"%s: final size is %d\", sCacheName, cache.size())); }\n\nIf you’re reading carefully, you’ll see that I am actually issuing database queries from two logical places: the grid client that generates the tasks and the grid members executing the load. I mentioned earlier that I’m sharing database parameters between read-through and pre-load by using a CacheLoader. I will have to save the details of how I achieve that sharing for another post.\n\nAll content listed on this page is the property of Oracle Corp. Redistribution not allowed without written permission\n\nCoherence monitoring can be challenging depending on the number of nodes and hosts. Users use Jconsole for getting a view of their Coherence applications. This is done by creating a JMX node that joins the cluster and thereby enabling monitoring. This is good to get current view. However if you are looking for metrics history as well as alerting capabilities, Jconsole does not provide it. Enterprise Manager provides this functionality. For best results, upgrade to the latest version of OEM (12cR2 at the time of writing this blog).\n\nSteps for getting Coherence cluster management from OEM:\n\n1) Install OEM. Refer to http://docs.oracle.com/cd/E24628_01/install.121/e22624/toc.htm\n\n2) Deploy OEM agent to the host where you plan to start the Coherence JMX node. Once the agent is deployed you then need to deploy Fusion Middleware Plugin.\n\nOnce the plugin is deployed you can locate the jars needed for coherence management\n\n/home/oracle/omsagent is the agent home in the above instance\n\n3) Setup JMX node\n\nFollowing JARs MUST be added to the classpath of the JMX node\n\n<OEM_Agent_Home>/plugins/oracle.sysman.emas.agent.plugin_12.1.0.3.0/archives/coherence/coherenceEMIntg.jar\n\n<OEM_Agent_Home>/plugins/oracle.sysman.emas.agent.plugin_12.1.0.3.0/archives/coherence/bulkoperationsmbean.jar\n\nUnlike the regular JMX coherence node you need to use oracle.sysman.integration.coherence.EMIntegrationServer class which is part of the plugin to start the JMX node for OEM.\n\nFollowing mandatory parameters must be added to Java system properties for the JMX node.\n\n-Dtangosol.coherence.management.remote=true (enables remote monitoring)\n\n-Dtangosol.coherence.management=all (enables monitoring for all nodes)\n\n-Dcom.sun.management.jmxremote.port=<TCP port> (required for remote connection)\n\n-Dtangosol.coherence.member=<Unique member name across the cluster> (required for target name)\n\n-Dtangosol.coherence.machine=<Host name> (required to correlate with the Host target, use the same string as shown on the respective Host target name)\n\n-Doracle.coherence.machine=<Host name> (required to correlate with the Host target, use the same string as shown on the respective Host target name)\n\n-Dtangosol.coherence.distributed.localstorage=false disables caching to make sure node is a dedicated monitoring node)\n\n-Xms2g -Xmx2g (the JMX node must have adequate resources, depending on your cluster size you may have to increase the size)\n\n-Dtangosol.coherence.management.refresh.expiry=1m (makes sure the monitoring data is refreshed every one minute. If this is not set then the refresh is done every 1 second which is too heavy on the JVM and monitoring is adversely affected)\n\nA process grep on the JMX node will look similar to the below\n\n/home/oracle/software/jdk1.6.0_37/bin/java -Xms2g -Xmx2g -Dtangosol.coherence.cacheconfig=coherence-cache-config.xml -Dtangosol.coherence.distributed.localstorage=false -Dtangosol.coherence.override=coefmw-tangosol-coherence-override.xml -Dtangosol.pof.config=pof-config.xml -Dtangosol.coherence.management=all -Dtangosol.coherence.management.remote=true -Dtangosol.coherence.management.refresh.expiry=1m -Dtangosol.coherence.management.remote.registryport=9192 -Dtangosol.coherence.management.remote.connectionport=9193 -Dtangosol.coherence.management.remote.host=HOSTNAME -Dtangosol.coherence.member=jmx1-HOSTNAME -Dtangosol.coherence.machine=HOSTNAME.us.oracle.com -Dtangosol.coherence.site=Linlithgow -Dtangosol.coherence.rack=OscOvm -Dtangosol.coherence.role=jmx -Dtangosol.coherence.listenaddr=HOSTNAME -Dtangosol.coherence.listenport=9190 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=9194 -Doracle.coherence.machine=HOSTNAME.us.oracle.com oracle.sysman.integration.coherence.EMIntegrationServer\n\n4. Modify the start scripts for all the nodes.\n\nAdd the following mandatory parameters to the Java system properties for the nodes OEM identifies node.\n\n-Dtangosol.coherence.management.remote=true\n\n-Dtangosol.coherence.member=<unique member name in the cluster>\n\n-Dtangosol.coherence.cluster=<cluster name>\n\n-Dtangosol.coherence.machine=<Host name> (required to correlate with the Host target, use the same string as shown on the respective Host target name)\n\n-Doracle.coherence.machine=<Host name> (required to correlate with the Host target, use the same string as shown on the respective Host target name)\n\nProcess grep on the process should look similar to the following\n\njava -Xms4g -Xmx4g -Dtangosol.coherence.cacheconfig=HOSTNAME-proxy-cache-config.xml -Dtangosol.coherence.distributed.localstorage=false -Dtangosol.coherence.override=HOSTNAME-tangosol-coherence-override.xml -Dtangosol.pof.config=pof-config.xml -Dtangosol.coherence.member=proxy1-HOSTNAME20 -Dtangosol.coherence.machine=HOSTNAME20.us.oracle.com -Dtangosol.coherence.site=Denver -Dtangosol.coherence.rack=Dev -Dtangosol.coherence.role=proxy -Dtangosol.coherence.listenaddr=HOSTNAME20 -Dtangosol.coherence.listenport=9170 -Dtangosol.coherence.proxyaddr=HOSTNAME20 -Dtangosol.coherence.proxyport=9180 -Doracle.coherence.jamjvmid=Data-Grid/proxy1-HOSTNAME20 -Doracle.coherence.machine=HOSTNAME20.us.oracle.com com.tangosol.net.DefaultCacheServer\n\nYou can do a quick validation by connecting through Jconsole\n\n5. Go to OEM console and do the discovery of the cluster\n\nComplete the registration. Give about a minute for the cluster to be discovered and status getting reported. Now navigate to the Coherence cluster and list all the members\n\nThis completes the discovery of Coherence targets on OEM.\n\nMore details on metrics, alerts, configuration etc.. will follow in part 2.\n\nAll content listed on this page is the property of Oracle Corp. Redistribution not allowed without written permission\n\nBecause Coherence cache servers are built on top of Java, they must deal with the heap size limitations that arise from garbage collection (GC). This is often at odds with the intended use case for Coherence, which is to manage large amounts of data in memory. In order to fully utilize the available memory on server machines while minimizing the number of JVMs required, we want to use the largest heap sizes that are possible without running into garbage collection problems (excessive pauses and/or CPU utilization). In practice, with modern JVMs (Java 1.6 or higher), heap sizes of 4-8GB are most common. 16GB heaps are much less common, and 32GB heaps are very rare. Thus, our focus is on basic tuning of JVMs with 4-8GB heaps. Similar considerations will also apply to cache clients that have large near caches (in fact, GC overhead for near caches will often be higher than for cache servers due to differences in how objects are stored in memory).\n\nThis article focuses on the HotSpot JVM as it is the most common JVM used for Coherence deployments. We’re not focused on getting optimal performance out of the JVM, but rather identifying a simple set of parameters that produce acceptable performance across a broad range of scenarios. Advanced JVM tuning (and especially version-specific tuning) is another area of expertise.\n\nThe most critical JVM options (highlighted in the Coherence production checklist) are the -server flag and making sure that the initial (-Xms) and maximum (-Xmx) heap sizes are set to the same values. But there are other options that can make a big difference.\n\n-XX:+UseConcMarkSweepGC or -XX:UseParallelGC\n\nThe CMS collector is a popular option for reducing “stop the world” GC pauses. This feature, by itself, will often suffice if the only objective is to eliminate GC pauses. However, it also tends to consume a significant amount of processor resources so it will likely reduce overall system throughput. On top of this, there are often complex relationships between latency (which would be worsened by GC pauses) and throughput, so some experimentation may be required.\n\nUsing the Parallel collector instead of CMS will reduce processor use at the cost of some moderate GC pauses. With modern hardware, these pauses are usually acceptably short and infrequent for user-facing applications with heaps of 4-8GB.\n\n-XX:NewRatio=8\n\nThe new ratio specifies how much of the heap is used for new objects versus tenured objects. Coherence tends to hold onto objects for considerably longer than would be typical in a Java environment, and as such the default ratio is typically fairly low for a Coherence workload. The optimal setting will require some testing (with fully populated Coherence caches), but values in the range of 4-10 will usually be a better fit for cache servers (a slightly lower range will be used for cache clients, depending on the size and usage patterns of the near caches).\n\n-XX:+UseParNewGC\n\nUse a multi-threaded collector for the young generation.\n\nThere are other options that we occasionally see used (e.g. -XX:+UseLargePages, -XX:SurvivorRatio=6, -XX:MaxPermSize=128m, -XX:+CMSParallelRemarkEnabled, -XX:ParallelGCThreads=4) but most of the time using the handful of options above will suffice. In general, tuning the memory management features of the JVM is very vendor-specific and version-specific, not to mention hardware-specific and OS-specific. And of course there are occasionally entire new collectors (such as G1, which is still fairly infrequently used in Coherence installations). There are also occasionally incompatibilities between these settings (e.g. NewRatio being ignored when used with CMS on certain JVM versions). It is generally best to stick to the defaults as much as possible to avoid surprises, and only specify non-default behavior when there is a clear benefit. This also reduces dependencies on a specific JVM offering or version, making ongoing maintenance simpler and less risky.\n\nAll content listed on this page is the property of Oracle Corp. Redistribution not allowed without written permission\n\n* Reposted with permission from Oracle Coherence Blog by Dave Felcey.\n\nOverview\n\nCoherence 12c (12.1.3) adds support for Memcached clients to directly store data a in Coherence cluster using the Binary Memcached protocol. This post outlines how to configure the Coherence Memcached Adaptor and includes a simple PHP example to show how Memecached clients can connect to a Coherence cluster.\n\nThe Memcached adaptor is configured as a proxy service that runs in the Coherence cluster. This is highlighted in the example cache configuration below:\n\n<?xml version=\"1.0\"?> <cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config http://xmlns.oracle.com/coherence/coherence-cache-config/1.2/coherence-cache-config.xsd\"> <defaults> <serializer>pof</serializer> </defaults> <caching-scheme-mapping> <cache-mapping> <cache-name>memcache</cache-name> <scheme-name>DistributedCacheScheme</scheme-name> </cache-mapping> </caching-scheme-mapping> <caching-schemes> <distributed-scheme> <scheme-name>DistributedCacheScheme</scheme-name> <service-name>DistributedCacheService</service-name> <backing-map-scheme> <local-scheme/> </backing-map-scheme> <autostart>true</autostart> </distributed-scheme> <proxy-scheme> <service-name>MemecachdTcpCacheService</service-name> <acceptor-config> <memcached-acceptor> <cache-name>memcache</cache-name> <memcached-auth-method>plain</memcached-auth-method> <address-provider>memcached-addr-provider</address-provider> </memcached-acceptor> </acceptor-config> <autostart>true</autostart> </proxy-scheme> </caching-schemes> </cache-config>\n\nNote a couple of things:\n\nThe <cache-name> element under <memcached-acceptor> defines the distributed cache for the Memcached acceptor. All Memcached clients connecting to this acceptor will access this cache.\n\nYou can optionally enable client authentication by setting the <memcached-auth-method>. Currently the only supported method is SASL (Simple Authentication and Security Layer) plain authentication.\n\nThe <address-provider> element defines the socket-address that the Memcached Adapter will listen on for connections. It is defined in the Coherence operational configuration file.\n\nBelow is an example operational configuration file that shows the setup of an address-provider:\n\n<?xml version='1.0'?> <coherence xmlns=\"http://xmlns.oracle.com/coherence/coherence-operational-config\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-operational-config http://xmlns.oracle.com/coherence/coherence-operational-config/1.2/coherence-operational-config.xsd\"> <cluster-config> <address-providers> <address-provider id=\"memcached-addr-provider\"> <socket-address> <address>127.0.0.1</address> <port>11211</port> </socket-address> </address-provider> </address-providers> </cluster-config> <logging-config> <destination system-property=\"tangosol.coherence.log\">stdout</destination> <severity-level system-property=\"tangosol.coherence.log.level\">9</severity-level> </logging-config> <security-config> <identity-asserter> <class-name>memcached.PasswordIdentityAsserter</class-name> </identity-asserter> </security-config> </coherence>\n\nAuthentication\n\nFor authentication, you can write a custom IdentityAsserter implementation and declare it in the operational configuration file. This is shown above but the relevant snippet is shown below:\n\n… <security-config> <identity-asserter> <class-name>memcached.PasswordIdentityAsserter</class-name> </identity-asserter> </security-config> …\n\nThe Memcached Adapter calls the IdentityAsserter implementation and passes thecom.tangosol.net.security.UsernameAndPassword object as a token.\n\nA sample IdentityAsserter is shown below:\n\npackage memcached; import com.tangosol.net.Service; import com.tangosol.net.security.IdentityAsserter; import com.tangosol.net.security.UsernameAndPassword; import javax.security.auth.Subject; public class PasswordIdentityAsserter implements IdentityAsserter { public Subject assertIdentity(Object oToken, Service service) throws SecurityException { if (oToken instanceof UsernameAndPassword) { UsernameAndPassword secToken = (UsernameAndPassword) oToken; String sPwd = new String(secToken.getPassword()); if (secToken.getUsername().equals(\"username\") && sPwd.equals(\"password\")) { return new Subject(); } } throw new SecurityException(\"Access denied\"); } }\n\nPHP Memcached client\n\nThere are 2 very popular PHP memcached clients – Memcache and Memcached. Memcached is the newer one and supports both the Binary Memcached protocol and SASL authentication. To enable Memcached and SASL, add the following to the php.ini file:\n\nextension=memcached.so memcached.use_sasl = 1\n\nThe following PHP code uses the PHP Memcached client library to connect to a Coherence cluster, add an entry and then retrieve it again.\n\n<?php $m = new Memcached(); $m->setOption(Memcached::OPT_BINARY_PROTOCOL, true); $m->setSaslAuthData(\"username\", \"password\"); if (!count($m->getServerList())) { // List of Coherence proxy service nodes $m->addServer('127.0.0.1', 11211); } print_r($m->getVersion()); $m->set('test', 'test_string'); print_r('Cache value: ' . $m->get('test')); ?>\n\nPutting it all together\n\nThis is easy to try out for yourself. First download and install Coherence, PHP and the PHP Memcached client library. Then in a test dir create an operational configuration file called tangosol-coherence-override.xml with the operational configuration details outlined above. Similarly create a cache configuration file called coherence-cache-config.xml with the specified cache configuration for Memcached clients. Finally, in the test dir create a dir called memcached and in it create a file called PasswordIdentityAsserter.java with the code listing above.\n\nOn the PHP side just save the example PHP code above to a file called test.php. Now we are ready to test everything.\n\nTo startup Coherence we will need to refer to the Coherence libraries. To make this easier define an environment variable for the location of the Coherence installation called COH_HOME. Below we will assume a Linux/OSX environment but its very similar on Windows.\n\nBuild the PasswordIdentityAsserter by running the following command in your test dir:\n\njavac –cp $COH_HOME/lib/coherence.jar memcached/PasswordIdentityAsserter.java\n\nNow start a Coherence cache server to store the Memcached client data:\n\njava -cp .:$COH_HOME/lib/coherence.jar com.tangosol.net.DefaultCacheServer\n\nLastly test that the PHP client can connect, store and read data securely via the Coherence Memcached Adaptor:\n\nphp test.php\n\nThe output should be:\n\nArray ( [127.0.0.1:11211] => 12.1.3 ) Cache value: test_string\n\nThis shows that the value inserted into Coherence by the PHP client, test_string, has successfully been read back.\n\nFor more information, please watch:\n\nAll content listed on this page is the property of Oracle Corp. Redistribution not allowed without written permission"
    }
}
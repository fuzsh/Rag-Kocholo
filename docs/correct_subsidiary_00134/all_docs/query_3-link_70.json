{
    "id": "correct_subsidiary_00134_3",
    "rank": 70,
    "data": {
        "url": "https://antony137.rssing.com/chan-8248299/all_p3.html",
        "read_more_link": "",
        "language": "en",
        "title": "Oracle Antony Reynolds' Blog",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://pixel.quantserve.com/pixel/p-KygWsHah2_7Qa.gif",
            "https://static.icivics.org/sites/default/files/constitution.jpg",
            "https://blogs.oracle.com/reynolds/resource/Scripts/Scripts.png",
            "https://www.packtpub.com/sites/default/files/782x300_Main_banner.jpg",
            "https://dgdsbygo8mp3h.cloudfront.net/sites/default/files/imagecache/productview_larger/3883EN_cov.jpg",
            "https://blogs.oracle.com/peoplesoft/resource/O_Support_clr.bmp",
            "https://a.dilcdn.com/bl/wp-content/uploads/sites/2/2013/05/GIF-FANTASIA-BROOM2.gif",
            "https://blogs.oracle.com/reynolds/resource/Rules/ShipmentStructure.png",
            "https://blogs.oracle.com/reynolds/resource/Rules/ShipmentGlobals.png",
            "https://blogs.oracle.com/reynolds/resource/Rules/ShipmentInitialActions.png",
            "https://blogs.oracle.com/reynolds/resource/Rules/ShipmentFunction.png",
            "https://blogs.oracle.com/reynolds/resource/Rules/ShipmentRule.png",
            "https://blogs.oracle.com/reynolds/resource/Rules/ShipmentTests.png",
            "https://2.bp.blogspot.com/_0Gws3D3l1TE/TAZj0KhRr3I/AAAAAAAAAfI/AlsGvqondNw/s320/Grace's_Avatar.png",
            "https://blogs.oracle.com/reynolds/resource/BinaryData/OpaqueSchema.png",
            "https://blogs.oracle.com/reynolds/resource/BinaryData/BinaryFormat.png",
            "https://blogs.oracle.com/reynolds/resource/BinaryData/BinarySchema_OutputMessage.png",
            "https://blogs.oracle.com/reynolds/resource/BinaryData/BinaryProcess.png",
            "https://blogs.oracle.com/reynolds/resource/BinaryData/BinaryPipeline.png",
            "https://www2.sfdcstatic.com/common/assets/img/logo-company.png",
            "https://blogs.oracle.com/reynolds/resource/SFDC/OldProcess.png",
            "https://blogs.oracle.com/reynolds/resource/OEP_HA/OEP_HA_NoHA.png",
            "https://blogs.oracle.com/reynolds/resource/OEP_HA/OEP_HA_HA.png",
            "https://blogs.oracle.com/reynolds/resource/OEP_HA/EPNnonHA.png",
            "https://blogs.oracle.com/reynolds/resource/OEP_HA/EPNHAtest.png",
            "https://blogs.oracle.com/reynolds/resource/OEP_HA/EPNHAInput.png",
            "https://blogs.oracle.com/reynolds/resource/OEP_HA/EPNHAOutput.png",
            "https://blogs.oracle.com/reynolds/resource/OEP_HA/EPNHAFull.png",
            "https://blogs.oracle.com/reynolds/resource/PerfTuningTool/SnapshotFlow.png",
            "https://blogs.oracle.com/reynolds/resource/PerfTuningTool/SoaSnapshotDiagram.png",
            "https://blogs.oracle.com/reynolds/resource/PerfTuningTool/SampleComposite.png",
            "https://blogs.oracle.com/reynolds/resource/PerfTuningTool/SampleAsyncBPELProcess.jpg",
            "https://blogs.oracle.com/reynolds/resource/2000th-Book-Home-Page-Banner.png",
            "https://blogs.oracle.com/reynolds/resource/SingleQueue/SingleQueueFlow.png",
            "https://blogs.oracle.com/reynolds/resource/CoherenceAdapter/Targeting.png",
            "https://blogs.oracle.com/reynolds/resource/CoherenceAdapter/ConnectionFactoryList.png",
            "https://blogs.oracle.com/reynolds/resource/CoherenceAdapter/OutboundConnectionGroup.png",
            "https://blogs.oracle.com/reynolds/resource/CoherenceAdapter/JNDIName.png",
            "https://blogs.oracle.com/reynolds/resource/CoherenceAdapter/ConnectionFactoryProperties.png",
            "https://blogs.oracle.com/reynolds/resource/CoherenceAdapter/CoherenceComposite.jpg",
            "https://blogs.oracle.com/reynolds/resource/CoherenceAdapter/CoherenceTestXSD.png",
            "https://blogs.oracle.com/reynolds/resource/CoherenceAdapter/PutOperation.png",
            "https://blogs.oracle.com/reynolds/resource/CoherenceAdapter/GetOperation.png",
            "https://blogs.oracle.com/reynolds/resource/CoherenceAdapter/RemoveOperation.png",
            "https://blogs.oracle.com/reynolds/resource/CoherenceAdapter/RemoveAllOperation.png",
            "https://blogs.oracle.com/reynolds/resource/CoherenceAdapter/ListOperation.png",
            "https://blogs.oracle.com/reynolds/resource/CoherenceAdapter/ListKeysOperation.png",
            "https://blogs.oracle.com/reynolds/resource/CoherenceAdapter/CleanupTestSuite.png",
            "https://blogs.oracle.com/reynolds/resource/CoherenceAdapter/InitTestSuite.png",
            "https://blogs.oracle.com/reynolds/resource/CoherenceAdapter/MainTestSuite.png",
            "https://blogs.oracle.com/reynolds/resource/CoherenceAdapter/RemoveTestSuite.png",
            "https://blogs.oracle.com/reynolds/resource/CoherenceAdapter/ValidateRemoveTestSuite.png",
            "https://blogs.oracle.com/reynolds/resource/MultiDomain/Super.png",
            "https://blogs.oracle.com/reynolds/resource/MultiDomain/Multi.png",
            "https://blogs.oracle.com/reynolds/resource/MultiDomain/SharedService.png",
            "https://blogs.oracle.com/reynolds/resource/docker1/image04.png",
            "https://blogs.oracle.com/reynolds/resource/docker1/image07.png",
            "https://blogs.oracle.com/reynolds/resource/docker1/image00.jpg",
            "https://blogs.oracle.com/reynolds/resource/docker1/image06.jpg",
            "https://blogs.oracle.com/reynolds/resource/docker1/image03.png",
            "https://blogs.oracle.com/reynolds/resource/docker1/image01.png",
            "https://blogs.oracle.com/reynolds/resource/docker1/image02.png",
            "https://blogs.oracle.com/reynolds/resource/docker1/image05.png",
            "https://blogs.oracle.com/reynolds/resource/docker2/image00.png",
            "https://blogs.oracle.com/reynolds/resource/docker2/image02.png",
            "https://blogs.oracle.com/reynolds/resource/docker2/image01.png",
            "https://blogs.oracle.com//reynolds/resource/docker1/image04.png",
            "https://blogs.oracle.com//reynolds/resource/docker1/image07.png",
            "https://cdn.app.compendium.com/uploads/user/e7c690e8-6ff9-102a-ac6d-e4aebca50425/f4a5b21d-66fa-4885-92bf-c4e81c06d916/Image/8bc2657305d3c74a4a913ca6b632bdf1/o_support_clr.bmp",
            "https://cdn.app.compendium.com/uploads/user/e7c690e8-6ff9-102a-ac6d-e4aebca50425/f4a5b21d-66fa-4885-92bf-c4e81c06d916/Image/dd977acf5d2b8e0047d45d22cf3de2cf/gif_fantasia_broom2.gif",
            "https://cdn.app.compendium.com/uploads/user/e7c690e8-6ff9-102a-ac6d-e4aebca50425/f4a5b21d-66fa-4885-92bf-c4e81c06d916/Image/9a71d27be072be97a950f710bba4650d/shipmentstructure.png",
            "https://cdn.app.compendium.com/uploads/user/e7c690e8-6ff9-102a-ac6d-e4aebca50425/f4a5b21d-66fa-4885-92bf-c4e81c06d916/Image/9cfc5464e33e814bbd8faad34f7d8a51/shipmentglobals.png",
            "https://cdn.app.compendium.com/uploads/user/e7c690e8-6ff9-102a-ac6d-e4aebca50425/f4a5b21d-66fa-4885-92bf-c4e81c06d916/Image/e0e321187a8e642fc21f18c5564959f0/shipmentinitialactions.png",
            "https://cdn.app.compendium.com/uploads/user/e7c690e8-6ff9-102a-ac6d-e4aebca50425/f4a5b21d-66fa-4885-92bf-c4e81c06d916/Image/afb31166bde824dee917ea2284565741/shipmentfunction.png",
            "https://cdn.app.compendium.com/uploads/user/e7c690e8-6ff9-102a-ac6d-e4aebca50425/f4a5b21d-66fa-4885-92bf-c4e81c06d916/Image/39cf7cc6a6d1eec4464cc1682167b8d0/shipmentrule.png",
            "https://cdn.app.compendium.com/uploads/user/e7c690e8-6ff9-102a-ac6d-e4aebca50425/f4a5b21d-66fa-4885-92bf-c4e81c06d916/Image/95bd6eae9bf48956c30456138ccf0cd2/shipmenttests.png",
            "https://4.bp.blogspot.com/-aZzamp_aOzU/UhTjnrtlo1I/AAAAAAAAAVo/KMNDjN9CuVs/s640/Celestial+Blue.jpg",
            "https://augustacrime.com/wp-content/uploads/2019/07/Jessica-Carpenter-37-Simple-battery-150x150.jpg",
            "https://busyteacher.org/uploads/posts/2016-12/thumbs/1482019946_california-worksheet-0.png",
            "https://augustacrime.com/wp-content/uploads/2017/01/imageCRESTONCURRY.jpg",
            "https://www.greytrix.com/blogs/sagex3/wp-content/uploads/2021/11/Fig02-Export-customization-screen-1024x500.png",
            "https://thepost.s3.amazonaws.com/wp-content/uploads/2014/08/0CA0F25I-150x150.jpg",
            "https://www.learncbse.in/wp-content/uploads/2017/08/NCERT-Solutions-for-Class-9th-Sanskrit-Chapter-1-अपठित-अवबोधनम्-2.jpg",
            "https://i.imgur.com/yTvNsOQl.jpg",
            "https://busyteacher.org/uploads/posts/2012-11/thumbs/1353086477_make-or-do-collocations-key-0.png",
            "https://1.bp.blogspot.com/-m_gxW9wUrow/YWpAej2ComI/AAAAAAAADwI/lPbemMAKTPcFQMerHxF4zzzDiJkzDqjeACLcBGAsYHQ/w300-h400/WhatsApp%2BImage%2B2021-10-16%2Bat%2B11.00.18%2BAM.jpeg",
            "https://e2e.ti.com//resized-image/__size/1230x0/__key/communityserver-discussions-components-files/6/7608._F764D653_.PNG",
            "https://i.imgur.com/E2aWYSh.jpg",
            "https://4.bp.blogspot.com/-D2ybkIpqREU/Wti4IoCYwLI/AAAAAAAABEg/UTVmhPWy1QcrZg6t-w8mJpp49Ho6MfwfQCLcBGAs/s640/Anasuya%2B3.png",
            "https://www.digitalkhabar.in/wp-content/uploads/Happy-Birthday-Bhabhi-in-Hindi.jpg",
            "https://2.bp.blogspot.com/-7TLcIsTWCRM/WJdhZJ8T1vI/AAAAAAAANlA/7-XTO95WE1494iVWxC5MIpWXckxuscj3QCLcB/s640/meenakshi%2Bjoshi%2Banchor.jpg",
            "https://chrisukorg.files.wordpress.com/2015/02/perry.jpg?w=529&h=511",
            "https://3.bp.blogspot.com/-HBHyCcGBgf8/Wn1yke7zLmI/AAAAAAAAEhI/K7xGK0DnW94EUDg4PXG7m9d5uDemlygEgCLcBGAs/s400/class12-bilogy-ncert-solutions-in-hindi-ch3-2.png",
            "https://s3.amazonaws.com/nixle/uploads/pub_media/md/user24872-1464275450-media1_a5a7a6_240_180_PrsMe_.jpeg",
            "https://lh5.ggpht.com/-drHHw9Z5ZwY/Tmp1gKWmZ1I/AAAAAAAALFc/8dMBz_U8mJw/s400/dragon_ball_73.gif",
            "https://3.bp.blogspot.com/-NP1u31H_hVE/WIJo02ELdQI/AAAAAAAAADY/eCMDy6ZuNoESiAXT9rhyePg9W9gujKiKwCEw/s400/p.txt.jpg",
            "https://www.rappler.com/tachyon/2021/07/PCIJ-Illustration-for-green-bills-story-july-17-2021.png?fit=449%2C449",
            "https://www.thesun.ie/wp-content/uploads/sites/3/2024/07/NINTCHDBPICT000919459510.jpg?strip=all&w=738",
            "https://www.thesun.co.uk/wp-content/uploads/2024/07/2018-love-island-couple-adam-421397728.jpg?strip=all&w=676",
            "https://jpcdn.it/img/small/71850111dc29343c90e84c90a0de1a2e.jpg",
            "https://cdn-images-1.medium.com/max/1024/0*hX1Okh3RlOSKECGM.png",
            "https://www.thesun.co.uk/wp-content/uploads/2024/07/1st-pics-eastenders-patsy-palmer-919437030.jpg?strip=all&w=746",
            "https://community.cadence.com/resized-image/__size/1280x960/__key/communityserver-discussions-components-files/28/pastedimage1721900432893v1.png",
            "https://i.etsystatic.com/5772952/r/il/059bcf/1488310855/il_570xN.1488310855_96je.jpg",
            "https://www.the-sun.com/wp-content/uploads/sites/6/2024/07/walmart-store-receipt-walmart-brand-912546864_4cf2a7.jpg?strip=all&w=960",
            "https://media.adverts.ie/eyJidWNrZXQiOiJtZWRpYS5hZHNpbWcuY29tIiwia2V5IjoiYzU0ZTQ1MjJhMDgyMGNlZjExYzNhNDcyODQ0YzQ0NmY5YjAwMGZiMzEwNGJlN2ZkNGZlYWU3YzE0ODkyMGVkMi5qcGciLCJvdXRwdXRGb3JtYXQiOiJqcGVnIiwiZWRpdHMiOnsicmVzaXplIjp7IndpZHRoIjoyMjcsImhlaWdodCI6MTg3fX19?signature=12517ddb0860daea228c84c6763d11c504f91785b6d74441e8272264aff45f8b"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "//www.rssing.com/favicon.ico",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Postscript on Scripts\n\nMore Scripts for SOA Suite\n\nOver time I have evolved my startup scripts and thought it would be a good time to share them. They are available for download here. I have finally converted to using WLST, which has a number of advantages. To me the biggest advantage is that the output and log files are automatically written to a consistent location in the domain directory or node manager directory. In addition the WLST scripts wait for the component to start and then return, this lets us string commands together without worrying about the dependencies.\n\nThe following are the key scripts (available for download here):\n\nScriptDescriptionPre-ReqsStops when\n\nTask CompletestartWlstNodeManager.shStarts Node Manager using WLSTNoneYesstartNodeManager.shStarts Node ManagerNoneYesstopNodeManager.shStops Node Manager using WLSTNode Manager runningYesstartWlstAdminServer.shStarts Admin Server using WLSTNode Manager runningYesstartAdminServer.shStarts Admin ServerNoneNostopAdminServer.shStops Admin ServerAdmin Server runningYesstartWlstManagedServer.shStarts Managed Server using WLSTNode Manager runningYesstartManagedServer.shStarts Managed ServerNoneNostopManagedServer.shStops Managed ServerAdmin Server runningYes\n\nSamples\n\nTo start Node Manager and Admin Server\n\nstartWlstNodeManager.sh ; startWlstAdminServer.sh\n\nTo start Node Manager, Admin Server and SOA Server\n\nstartWlstNodeManager.sh ; startWlstAdminServer.sh ; startWlstManagedServer soa_server1\n\nNote that the Admin server is not started until the Node Manager is running, similarly the SOA server is not started until the Admin server is running.\n\nNode Manager Scripts\n\nstartWlstNodeManager.sh\n\nUses WLST to start the Node Manager. When the script completes the Node manager will be running.\n\nstartNodeManager.sh\n\nThe Node Manager is started in the background and the output is piped to the screen. This causes the Node Manager to continue running in the background if the terminal is closed. Log files, including a .out capturing standard output and standard error, are placed in the <WL_HOME>/common/nodemanager directory, making them easy to find. This script pipes the output of the log file to the screen and keeps doing this until terminated, Terminating the script does not terminate the Node Manager.\n\nstopNodeManager.sh\n\nUses WLST to stop the Node Manager. When the script completes the Node manager will be stopped.\n\nAdmin Server Scripts\n\nstartWlstAdminServer.sh\n\nUses WLST to start the Admin Server. The Node Manager must be running before executing this command. When the script completes the Admin Server will be running.\n\nstartAdminServer.sh\n\nThe Admin Server is started in the background and the output is piped to the screen. This causes the Admin Server to continue running in the background if the terminal is closed. Log files, including the .out capturing standard output and standard error, are placed in the same location as if the server had been started by Node Manager, making them easy to find. This script pipes the output of the log file to the screen and keeps doing this until terminated, Terminating the script does not terminate the server.\n\nstopAdminServer.sh\n\nStops the Admin Server. When the script completes the Admin Server will no longer be running.\n\nManaged Server Scripts\n\nstartWlstManagedServer.sh <MANAGED_SERVER_NAME>\n\nUses WLST to start the given Managed Server. The Node Manager must be running before executing this command. When the script completes the given Managed Server will be running.\n\nstartAdminServer.sh <MANAGED_SERVER_NAME>\n\nThe given Managed Server is started in the background and the output is piped to the screen. This causes the given Managed Server to continue running in the background if the terminal is closed. Log files, including the .out capturing standard output and standard error, are placed in the same location as if the server had been started by Node Manager, making them easy to find. This script pipes the output of the log file to the screen and keeps doing this until terminated, Terminating the script does not terminate the server.\n\nstopAdminServer.sh <MANAGED_SERVER_NAME>\n\nStops the given Managed Server. When the script completes the given Managed Server will no longer be running.\n\nUtility Scripts\n\nThe following scripts are not called directly but are used by the previous scripts.\n\n_fmwenv.sh\n\nThis script is used to provide information about the Node Manager and WebLogic Domain and must be edited to reflect the installed FMW environment, in particular the following values must be set:\n\nDOMAIN_NAME – the WebLogic domain name.\n\nNM_USERNAME – the Node Manager username.\n\nNM_PASSWORD – the Node Manager password.\n\nMW_HOME – the location where WebLogic and other FMW components are installed.\n\nThe following values may also need changing:\n\nDOMAIN_HOME – the location of the WebLogic domain directory, defaults to ${MW_HOME}/user_projects/domains/${DOMAIN_NAME}\n\nNM_LISTEN_HOST – the Node Manager listening hostname, defaults to the hostname of the machine it is running on.\n\nNM_LISTEN_PORT – the Node Manager listening port.\n\n_runWLst.sh\n\nThis script runs the WLST script passed in environment variable ${SCRIPT} and takes its configuration from _fmwenv.sh. It dynamically builds a WLST properties file in the /tmp directory to pass parameters into the scripts. The properties filename is of the form <DOMAIN_NAME>.<PID>.properties.\n\n_runAndLog.sh\n\nThis script runs the command passed in as an argument, writing standard out and standard error to a log file. The log file is rotated between invocations to avoid losing the previous log files. The log file is then tailed and output to the screen. This means that this script will never finish by itself.\n\nWLST Scripts\n\nThe following WLST scripts are used by the scripts above, taking their properties from /tmp/<DOMAIN_NAME>.<PID>.properties:\n\nstartNodeManager.py\n\nstopNodeManager.py\n\nstartServer.py\n\nRelationships\n\nThe dependencies and relationships between my scripts and the built in scripts are shown in the diagram below.\n\n↧\n\n↧\n\n$5 eBook Bonanza\n\nPackt eBooks $5 Offer\n\nPackt Publishing just told me about their Christmas offer, get eBooks for $5.\n\nFrom December 19th, customers will be able to get any eBook or Video from Packt for just $5. This offer covers a myriad of titles in the 1700+ range where customers will be able to grab as many as they like until January 3rd 2014 – more information is available at http://bit.ly/1jdCr2W\n\nIf you haven’t bought the SOA Developers Cookbook then now is a great time to do so!\n\n↧\n\nSupporting the Team\n\nSOA Support Team Blog\n\nSome of my former colleagues in support have created a blog to help answer common problems for customers. One way they are doing this is by creating better landing zones within My Oracle Support (MOS). I just used the blog to locate the landing zone for database related issues in SOA Suite. I needed to get the purge scripts working on 11.1.1.7 and I couldn’t find the patches needed to do that. A quick look on the blog and I found a suitable entry that directed me to the Oracle Fusion Middleware (FMW) SOA 11g Infrastructure Database: Installation, Maintenance and Administration Guide (Doc ID 1384379.1) in MOS. Lots of other useful stuff on the blog so stop by and check it out, great job Shawn, Antonella, Maria & JB.\n\n↧\n\nCleaning Up After Yourself\n\nMaintaining a Clean SOA Suite Test Environment\n\nFun blog entry with Fantasia animated gifs got me thinking like Mickey about how nice it would be to automate clean up tasks.\n\nI don’t have a sorcerers castle to clean up but I often have a test environment which I use to run tests, then after fixing problems that I uncovered in the tests I want to run them again. The problem is that all the data from my previous test environment is still there.\n\nNow in the past I used VirtualBox snapshots to rollback to a clean state, but this has a problem that it not only loses the environment changes I want to get rid of such as data inserted into tables, it also gets rid of changes I want to keep such as WebLogic configuration changes and new shell scripts. So like Mickey I went in search of some magic to help me.\n\nCleaning Up SOA Environment\n\nMy first task was to clean up the SOA environment by deleting all instance data from the tables. Now I could use the purge scripts to do this, but that would still leave me with running instances, for example 800 Human Workflow Tasks that I don’t want to deal with. So I used the new truncate script to take care of this. Basically this removes all instance data from your SOA Infrastructure, whether or not the data is live. This can be run without taking down the SOA Infrastructure (although if you do get strange behavior you may want to restart SOA). Some statistics, such are service and reference statistics, are kept since server startup, so you may want to restart your server to clear that data. A sample script to run the truncate SQL is shown below.\n\n#!/bin/sh\n\n# Truncate the SOA schemas, does not truncate BAM.\n\n# Use only in development and test, not production.\n\n# Properties to be set before running script\n\n# SOAInfra Database SID\n\nDB_SID=orcl\n\n# SOA DB Prefix\n\nSOA_PREFIX=DEV\n\n# SOAInfra DB password\n\nSOAINFRA_PASSWORD=welcome1\n\n# SOA Home Directory\n\nSOA_HOME=/u01/app/fmw/Oracle_SOA1\n\n# Set DB Environment\n\n. oraenv << EOF\n\n${DB_SID}\n\nEOF\n\n# Run Truncate script from directory it lives in\n\ncd ${SOA_HOME}/rcu/integration/soainfra/sql/truncate\n\n# Run the truncate script\n\nsqlplus ${SOA_PREFIX}_soainfra/${SOAINFRA_PASSWORD} @truncate_soa_oracle.sql << EOF\n\nexit\n\nEOF\n\nAfter running this script all your SOA composite instances and associated workflow instances will be gone.\n\nCleaning Up BAM\n\nThe above example shows how easy it is to get rid of all the runtime data in your SOA repository, however if you are using BAM you still have all the contents of your BAM objects from previous runs. To get rid of that data we need to use BAM ICommand’s clear command as shown in the sample script below:\n\n#!/bin/sh\n\n# Set software locations\n\nFMW_HOME=/home/oracle/fmw\n\nexport JAVA_HOME=${FMW_HOME}/jdk1.7.0_17\n\nBAM_CMD=${FMW_HOME}/Oracle_SOA1/bam/bin/icommand\n\n# Set objects to purge\n\nBAM_OBJECTS=/path/RevenueEvent /path/RevenueViolation\n\n# Clean up BAM\n\nfor name in ${BAM_OBJECTS}\n\ndo\n\n${BAM_CMD} -cmd clear -name ${name} -type dataobject\n\ndone\n\nAfter running this script all the rows of the listed objects will be gone.\n\nReady for Inspection\n\nUnlike the hapless Mickey, our clean up scripts work reliably and do what we want without unexpected consequences, like flooding the castle.\n\n↧\n\nList Manipulation in Rules\n\nGenerating Lists from Rules\n\nRecently I was working with a customer that wanted to use rules to do validation. The idea was to pass in a document to the rules engine and get back a list of violations, or an empty list if there were no violations. Turns out that there were a coupe more steps required than I expected so thought I would share my solution in case anyone else is wondering how to return lists from the rules engine.\n\nThe Scenario\n\nFor the purposes of this blog I modeled a very simple shipping company document that has two main parts. The Package element contains information about the actual item to be shipped, its weight, type of package and destination details. The Billing element details the charges applied.\n\nFor the purpose of this blog I want to validate the following:\n\nA residential surcharge is applied to residential addresses.\n\nA residential surcharge is not applied to non-residential addresses.\n\nThe package is of the correct weight for the type of package.\n\nThe Shipment element is sent to the rules engine and the rules engine replies with a ViolationList element that has all the rule violations that were found.\n\nCreating the Return List\n\nWe need to create a new ViolationList within rules so that we can return it from within the decision function. To do this we create a new global variable – I called it ViolationList – and initialize it. Note that I also had some globals that I used to allow changing the weight limits for different package types.\n\nWhen the rules session is created it will initialize the global variables and assert the input document – the Shipment element. However within rules our ViolationList variable has an uninitialized internal List that is used to hold the actual List of Violation elements. We need to initialize this to an empty RL.list in the Decision Functions “Initial Actions” section.\n\nWe can then assert the global variable as a fact to make it available to be the return value of the decision function. After this we can now create the rules.\n\nAdding a Violation to the List\n\nIf a rule fires because of a violation then we need add a Violation element to the list. The easiest way to do this without having the rule check the ViolationList directly is to create a function to add the Violation to the global variable VioaltionList.\n\nThe function creates a new Violation and initializes it with the appropriate values before appending it to the list within the ViolationList.\n\nWhen a rule fires then it just necessary to call the function to add the violation to the list.\n\nIn the example above if the address is a residential address and the surcharge has not been applied then the function is called with an appropriate error code and message.\n\nHow it Works\n\nEach time a rule fires we can add the violation to the list by calling the function. If multiple rules fire then we will get multiple violations in the list. We can access the list from a function because it is a global variable. Because we asserted the global variable as a fact in the decision function initialization function it is picked up by the decision function as a return value. When all possible rules have fired then the decision function will return all asserted ViolationList elements, which in this case will always be 1 because we only assert it in the initialization function.\n\nWhat Doesn’t Work\n\nA return from a decision function is always a list of the element you specify, so you may be tempted to just assert individual Violation elements and get those back as a list. That will work if there is at least one element in the list, but the decision function must always return at least one element. So if there are no violations then you will get an error thrown.\n\nAlternative\n\nInstead of having a completely separate return element you could have the ViolationList as part of the input element and then return the input element from the decision function. This would work but now you would be copying most of the input variables back into the output variable. I prefer to have a cleaner more function like interface that makes it easier to handle the response.\n\nDownload\n\nHope this helps someone. A sample composite project is available for download here. The composite includes some unit tests. You can run these from the EM console and then look at the inputs and outputs to see how things work.\n\n↧\n\n↧\n\nGoing Native with JCA Adapters\n\nFormatting JCA Adapter Binary Contents\n\nSometimes you just need to go native and play with binary data rather than XML. This occurs commonly when using JCA adapters, the file to be written is in binary format, or the TCP messsages written by the Socket Adapter are in binary format. Although the adapter has no problem converting Base64 data into raw binary, it is a little tricky to get that data into base64 format in the first place, so this blog entry will explain how.\n\nAdapter Creation\n\nWhen creating most adapters (application & DB being the exceptions) you have the option of choosing the message format. By making the message format “opaque” you are telling the adapter wizard that the message data will be provided as a base-64 encoded string and the adapter will convert this to binary and deliver it.\n\nThis results in a WSDL message defined as shown below:\n\n<wsdl:types>\n\n<schema targetNamespace=\"http://xmlns.oracle.com/pcbpel/adapter/opaque/\"\n\nxmlns=\"http://www.w3.org/2001/XMLSchema\">\n\n<element name=\"opaqueElement\" type=\"base64Binary\" />\n\n</schema>\n\n</wsdl:types>\n\n<wsdl:message name=\"Write_msg\">\n\n<wsdl:part name=\"opaque\" element=\"opaque:opaqueElement\"/>\n\n</wsdl:message>\n\nThe Challenge\n\nThe challenge now is to convert out data into a base-64 encoded string. For this we have to turn to the service bus and MFL.\n\nWithin the service bus we use the MFL editor to define the format of the binary data. In our example we will have variable length strings that start with a 1 byte length field as well as 32-bit integers and 64-bit floating point numbers.\n\nThe example below shows a sample MFL file to describe the above data structure:\n\n<?xml version='1.0' encoding='windows-1252'?>\n\n<!DOCTYPE MessageFormat SYSTEM 'mfl.dtd'>\n\n<!-- Enter description of the message format here. -->\n\n<MessageFormat name='BinaryMessageFormat' version='2.02'>\n\n<FieldFormat name='stringField1' type='String' delimOptional='y' codepage='UTF-8'>\n\n<LenField type='UTinyInt'/>\n\n</FieldFormat>\n\n<FieldFormat name='intField' type='LittleEndian4' delimOptional='y'/>\n\n<FieldFormat name='doubleField' type='LittleEndianDouble' delimOptional='y'/>\n\n<FieldFormat name='stringField2' type='String' delimOptional='y' codepage='UTF-8'>\n\n<LenField type='UTinyInt'/>\n\n</FieldFormat>\n\n</MessageFormat>\n\nNote that we can define the endianess of the multi-byte numbers, in this case they are specified as little endian (Intel format).\n\nI also created an XML version of the MFL that can be used in interfaces.\n\nThe XML version can then be imported into a WSDL document to create a web service.\n\nFull Steam Ahead\n\nWe now have all the pieces we need to convert XML to binary and deliver it via an adapter using the process shown below:\n\nWe receive the XML request, in the sample code, the sample delivers it as a web service.\n\nWe then convert the request data into MFL format XML using an XQuery and store the result in a variable (mflVar).\n\nWe then convert the MFL formatted XML into binary data (internally this is held as a java byte array) and store the result in a variable (binVar).\n\nWe then convert the byte array to a base-64 string using javax.xml.bind.DatatypeConverter.printBase64Binary and store the result in a variable (base64Var).\n\nFinally we replace the original $body contents with the output of an XQuery that matches the adapter expected XML format.\n\nThe diagram below shows the OSB pipeline that implements the above.\n\nA Wrinkle\n\nUnfortunately we can only call static Java methods that reside in a jar file imported into service bus, so we have to provide a wrapper for the printBase64Binary call. The below Java code was used to provide this wrapper:\n\npackage antony.blog;\n\nimport javax.xml.bind.DatatypeConverter;\n\npublic class Base64Encoder {\n\npublic static String base64encode(byte[] content) {\n\nreturn DatatypeConverter.printBase64Binary(content);\n\n}\n\npublic static byte[] base64decode(String content) {\n\nreturn DatatypeConverter.parseBase64Binary(content);\n\n}\n\n}\n\nWrapping Up\n\nSample code is available here and consists of the following projects:\n\nBinaryAdapter – JDeveloper SOA Project that defines the JCA File Adapter\n\nOSBUtils – JDeveloper Java Project that defines the Java wrapper for DataTypeConverter\n\nBinaryFileWriter – Eclipse OSB Project that includes everything needed to try out the steps in this blog.\n\nThe OSB project needs to be customized to have the logical directory name point to something sensible. The project can be tested using the normal OSB console test screen.\n\nThe following sample input (note 16909060 is 0x01020304)\n\n<bin:OutputMessage xmlns:bin=\"http://www.example.org/BinarySchema\">\n\n<bin:stringField1>First String</bin:stringField1>\n\n<bin:intField>16909060</bin:intField>\n\n<bin:doubleField>1.5</bin:doubleField>\n\n<bin:stringField2>Second String</bin:stringField2>\n\n</bin:OutputMessage>\n\nGenerates the following binary data file – displayed using “hexdump –C”. The int is highlighted in yellow, the double in orange and the strings and their associated lengths in green with the length in bold.\n\n$ hexdump -C 2.bin\n\n00000000 0c 46 69 72 73 74 20 53 74 72 69 6e 6704 03 02 |.First String...|\n\n00000010 0100 00 00 00 00 00 f8 3f0d 53 65 63 6f 6e 64 |........?.Second|\n\n00000020 20 53 74 72 69 6e 67 | String|\n\nAlthough we used a web service writing through to a file adapter we could have equally well used the socket adapter to send the data to a TCP endpoint. Similarly the source of the data could be anything. The same principle can be applied to decode binary data, just reverse the steps and use Java method parseBase64Binary instead of printBase64Binary.\n\n↧\n\nClear Day for Cloud Adapters\n\nsalesforce.com Adapter Released\n\nYesterday Oracle released their cloud adapter for salesforce.com (SFDC) so I thought I would talk a little about why you might want it. I had previously integrated with SFDC using BPEL and the SFDC web interface, so in this post I will explore why the adapter might be a better approach.\n\nWhy?\n\nSo if I can interface to SFDC without the adapter why would I spend money on the adapter? There are a number of reasons and in this post I will just explain the following 3 benefits:\n\nAuto-Login\n\nNon-Ploymorphic Operations\n\nOperation Wizards\n\nLets take each one in turn.\n\nAuto-Login\n\nThe first obvious benefit is how you connect and make calls to SFDC. To perform an operation such as query an account or update an address the SFDC interface requires you to do the following:\n\nInvoke a login method which returns a session ID to placed in the header on all future calls and the actual endpoint to call.\n\nInvoke the actual operation using the provided endpoint and passing the session ID provided.\n\nWhen finished with calls invoke the logout operation.\n\nNow these are not unreasonable demands. The problem comes when you try to implement this interface.\n\nBefore calling the login method you need the credentials. These need to be obtained from somewhere, I set them as BPEL preferences but there are other ways to store them. Calling the login method is not a problem but you need to be careful in how you make subsequent calls.\n\nFirst all subsequent calls must override the endpoint address with the one returned from the login operation. Secondly the provided session ID must be placed into a custom SOAP header. So you have to copy the session ID into a custom SOAP Header and provide that header to the invoke operation. You also have to override the endpointURI property in the invoke with provided endpoint.\n\nFinally when you have finished performing operations you have to logout.\n\nIn addition to the number of steps you have to code there is the problem of knowing when to logout. The simplest thing to do is for each operation you wish to perform execute the login operatoin folloed y the actual working operation and then do a logout operation. The trouble with this is that you are now making 3 calls every time you want to perform an operation against SFDC. This causes additional latency in processing the request.\n\nThe adapter hides all this from you, hiding the login/logout operations and allowing connections to be re-used, reducing the number of logins required. The adapter makes the SFDC call look like a call to any other web service while the adapter uses a session cache to avoid repeated logins.\n\nNon-Polymorphic Operations\n\nThe standard operations in the SFDC interface provide a base object return type, the sObject. This could be an Account or a Campaign for example but the operations always return the base sObject type, leaving it to the client to make sure they process the correct return type. Similarly requests also use polymorphic data types. This often requires in BPEL that the sObject returned is copied to a variable of a more specific type to simplify processing of the data. If you don’t do this then you can still query fields within the specific object but the SOA tooling cannot check it for you.\n\nThe adapter identifies the type of request and response and also helps build the request for you with bind parameters. This means that you are able to build your process to actually work with the real data structures, not the abstract ones. This is another big benefit in my view!\n\nOperation Wizards\n\nThe SFDC API is very powerful. Translation: the SFDC API is very complex. With great power comes complexity to paraphrase Uncle Ben (amongst others). The adapter groups the operations into logical collections and then provides additional help in selecting from within those collections of operations and providing the correct parameters for them.\n\nInstalling\n\nInstallation takes place in two parts. The Design time is deployed into JDeveloper and the run time is deployed into the SOA Suite Oracle Home. The adapter is available for download here and the installation instructions and documentation are here. Note that you will need OPatch to install the adapter. OPatch can be downloaded from Oracle Support Patch 6880880. Don’t use the OPatch that ships with JDeveloper and SOA Suite. If you do you may see an error like:\n\nUncaught exception\n\noracle.classloader.util.AnnotatedNoClassDefFoundError:\n\nMissing class: oracle.tip.tools.ide.adapters.cloud.wizard.CloudAdapterWizard\n\nYou will want the OPatch 11.1.0.x.x. Make sure you download the correct 6880880, it must be for 11.1.x as that is the version of JDeveloper and SOA Suite and it must be for the platform you are running on.\n\nApart from getting the right OPatch the installation is very straight forward.\n\nSo don’t be afraid of SFDC integration any more, cloud integratoin is clear with the SFDC adapter.\n\n↧\n\nClustering Events\n\nSetting up an Oracle Event Processing Cluster\n\nRecently I was working with Oracle Event Processing (OEP) and needed to set it up as part of a high availability cluster. OEP uses Coherence for quorum membership in an OEP cluster. Because the solution used caching it was also necessary to include access to external Coherence nodes. Input messages need to be duplicated across multiple OEP streams and so a JMS Topic adapter needed to be configured. Finally only one copy of each output event was desired, requiring the use of an HA adapter. In this blog post I will go through the steps required to implement a true HA OEP cluster.\n\nOEP High Availability Review\n\nThe diagram below shows a very simple non-HA OEP configuration:\n\nEvents are received from a source (JMS in this blog). The events are processed by an event processing network which makes use of a cache (Coherence in this blog). Finally any output events are emitted. The output events could go to any destination but in this blog we will emit them to a JMS queue.\n\nOEP provides high availability by having multiple event processing instances processing the same event stream in an OEP cluster. One instance acts as the primary and the other instances act as secondary processors. Usually only the primary will output events as shown in the diagram below (top stream is the primary):\n\nThe actual event processing is the same as in the previous non-HA example. What is different is how input and output events are handled. Because we want to minimize or avoid duplicate events we have added an HA output adapter to the event processing network. This adapter acts as a filter, so that only the primary stream will emit events to out queue. If the processing of events within the network depends on how the time at which events are received then it is necessary to synchronize the event arrival time across the cluster by using an HA input adapter to synchronize the arrival timestamps of events across the cluster.\n\nOEP Cluster Creation\n\nLets begin by setting up the base OEP cluster. To do this we create new OEP configurations on each machine in the cluster. The steps are outlined below. Note that the same steps are performed on each machine for each server which will run on that machine:\n\nRun ${MW_HOME}/ocep_11.1/common/bin/config.sh.\n\nMW_HOME is the installation directory, note that multiple Fusion Middleware products may be installed in this directory.\n\nWhen prompted “Create a new OEP domain”.\n\nProvide administrator credentials.\n\nMake sure you provide the same credentials on all machines in the cluster.\n\nSpecify a “Server name” and “Server listen port”.\n\nEach OEP server must have a unique name.\n\nDifferent servers can share the same “Server listen port” unless they are running on the same host.\n\nProvide keystore credentials.\n\nMake sure you provide the same credentials on all machines in the cluster.\n\nConfigure any required JDBC data source.\n\nProvide the “Domain Name” and “Domain location”.\n\nAll servers must have the same “Domain name”.\n\nThe “Domain location” may be different on each server, but I would keep it the same to simplify administration.\n\nMultiple servers on the same machine can share the “Domain location” because their configuration will be placed in the directory corresponding to their server name.\n\nCreate domain!\n\nConfiguring an OEP Cluster\n\nNow that we have created our servers we need to configure them so that they can find each other. OEP uses Oracle Coherence to determine cluster membership. Coherence clusters can use either multicast or unicast to discover already running members of a cluster. Multicast has the advantage that it is easy to set up and scales better (see http://www.ateam-oracle.com/using-wka-in-large-coherence-clusters-disabling-multicast/) but has a number of challenges, including failure to propagate by default through routers and accidently joining the wrong cluster because someone else chose the same multicast settings. We will show how to use both unicast and multicast to discover the cluster.\n\nMulticast DiscoveryUnicast DiscoveryCoherence multicast uses a class D multicast address that is shared by all servers in the cluster. On startup a Coherence node broadcasts a message to the multicast address looking for an existing cluster. If no-one responds then the node will start the cluster.Coherence unicast uses Well Known Addresses (WKAs). Each server in the cluster needs a dedicated listen address/port combination. A subset of these addresses are configured as WKAs and shared between all members of the cluster. As long as at least one of the WKAs is up and running then servers can join the cluster. If a server does not find any cluster members then it checks to see if its listen address and port are in the WKA list. If it is then that server will start the cluster, otherwise it will wait for a WKA server to become available.\n\nSet a common cluster multicast listen address shared by all servers in the config.xml file.\n\nAdd the following to the <cluster> element:\n\n<cluster>\n\n…\n\n<!—For us in Coherence multicast only! –>\n\n<multicast-address>239.255.200.200</multicast-address>\n\n<multicast-port>9200</multicast-port>\n\n</cluster>\n\nThe “multicast-address” must be able to be routed through any routers between servers in the cluster.\n\nOptionally you can specify the bind address of the server, this allows you to control port usage and determine which network is used by Coherence\n\nCreate a “tangosol-coherence-override.xml” file in the ${DOMAIN}/{SERVERNAME}/config directory for each server in the cluster.\n\n<?xml version='1.0'?>\n\n<coherence>\n\n<cluster-config>\n\n<unicast-listener>\n\n<!—This server Coherence address and port number –>\n\n<address>192.168.56.91</address>\n\n<port>9200</port>\n\n</unicast-listener>\n\n</cluster-config>\n\n</coherence>\n\nConfigure the Coherence WKA cluster discovery.\n\nCreate a “tangosol-coherence-override.xml” file in the ${DOMAIN}/{SERVERNAME}/config directory for each server in the cluster.\n\n<?xml version='1.0'?>\n\n<coherence>\n\n<cluster-config>\n\n<unicast-listener>\n\n<!—WKA Configuration –>\n\n<well-known-addresses>\n\n<socket-address id=\"1\">\n\n<address>192.168.56.91</address>\n\n<port>9200</port>\n\n</socket-address>\n\n<socket-address id=\"2\">\n\n<address>192.168.56.92</address>\n\n<port>9200</port>\n\n</socket-address>\n\n</well-known-addresses>\n\n<!—This server Coherence address and port number –>\n\n<address>192.168.56.91</address>\n\n<port>9200</port>\n\n</unicast-listener>\n\n</cluster-config>\n\n</coherence>\n\nList at least two servers in the <socket-address> elements.\n\nFor each <socket-address> element there should be a server that has corresponding <address> and <port> elements directly under <well-known-addresses>.\n\nOne of the servers listed in the <well-known-addresses> element must be the first server started.\n\nNot all servers need to be listed in <well-known-addresses>, but see previous point.\n\nYou should now have a working OEP cluster. Check the cluster by starting all the servers.\n\nLook for a message like the following on the first server to start to indicate that another server has joined the cluster:\n\n<Coherence> <BEA-2049108> <The domain membership has changed to [server2, server1], the new domain primary is \"server1\">\n\nLog on to the Event Processing Visualizer of one of the servers –http://<hostname>:<port>/wlevs. Select the cluster name on the left and then select group “AllDomainMembers”. You should see a list of all the running servers in the “Servers of Group – AllDomainMembers” section.\n\nSample Application\n\nNow that we have a working OEP cluster let us look at a simple application that can be used as an example of how to cluster enable an application. This application models service request tracking for hardware products. The application we will use performs the following checks:\n\nIf a new service request (identified by SRID) arrives (indicated by status=RAISE) then we expect some sort of follow up in the next 10 seconds (seconds because I want to test this quickly). If no follow up is seen then an alert should be raised.\n\nFor example if I receive an event (SRID=1, status=RAISE) and after 10 seconds I have not received a follow up message (SRID=1, status<>RAISE) then I need to raise an alert.\n\nIf a service request (identified by SRID) arrives and there has been another service request (identified by a different SRID) for the same physcial hardware (identified by TAG) then an alert should be raised.\n\nFor example if I receive an event (SRID=2, TAG=M1) and later I receive another event for the same hardware (SRID=3, TAG=M1) then an alert should be raised.\n\nNote use case 1 is nicely time bounded – in this case the time window is 10 seconds. Hence this is an ideal candidate to be implemented entirely in CQL.\n\nUse case 2 has no time constraints, hence over time there could be a very large number of CQL queries running looking for a matching TAG but a different SRID. In this case it is better to put the TAGs into a cache and search the cache for duplicate tags. This reduces the amount of state information held in the OEP engine.\n\nThe sample application to implement this is shown below:\n\nMessages are received from a JMS Topic (InboundTopicAdapter). Test messages can be injected via a CSV adapter (RequestEventCSVAdapter). Alerts are sent to a JMS Queue (OutboundQueueAdapter), and also printed to the server standard output (PrintBean). Use case 1 is implemented by the MissingEventProcessor. Use case 2 is implemented by inserting the TAG into a cache (InsertServiceTagCacheBean) using a Coherence event processor and then querying the cache for each new service request (DuplicateTagProcessor), if the same tag is already associated with an SR in the cache then an alert is raised. The RaiseEventFilter is used to filter out existing service requests from the use case 2 stream.\n\nThe non-HA version of the application is available to download here.\n\nWe will use this application to demonstrate how to HA enable an application for deployment on our cluster.\n\nA CSV file (TestData.csv) and Load generator properties file (HADemoTest.prop) is provided to test the application by injecting events using the CSV Adapter.\n\nNote that the application reads a configuration file (System.properties) which should be placed in the domain directory of each event server.\n\nDeploying an Application\n\nBefore deploying an application to a cluster it is a good idea to create a group in the cluster. Multiple servers can be members of this group. To add a group to an event server just add an entry to the <cluster> element in config.xml as shown below:\n\n<cluster>\n\n…\n\n<groups>HAGroup</groups>\n\n</cluster>\n\nMultiple servers can be members of a group and a server can be a member of multiple groups. This allows you to have different levels of high availability in the same event processing cluster.\n\nDeploy the application using the Visualizer. Target the application at the group you created, or the AllDomainMembers group.\n\nTest the application, typically using a CSV Adapter. Note that using a CSV adapter sends all the events to a single event server. To fix this we need to add a JMS output adapter (OutboundTopicAdapter) to our application and then send events from the CSV adapter to the outbound JMS adapter as shown below:\n\nSo now we are able to send events via CSV to an event processor that in turn sends the events to a JMS topic. But we still have a few challenges.\n\nManaging Input\n\nFirst challenge is managing input. Because OEP relies on the same event stream being processed by multiple servers we need to make sure that all our servers get the same message from the JMS Topic. To do this we configure the JMS connection factory to have an Unrestricted Client ID. This allows multiple clients (OEP servers in our case) to use the same connection factory. Client IDs are mandatory when using durable topic subscriptions. We also need each event server to have its own subscriber ID for the JMS Topic, this ensures that each server will get a copy of all the messages posted to the topic. If we use the same subscriber ID for all the servers then the messages will be distributed across the servers, with each server seeing a completely disjoint set of messages to the other servers in the cluster. This is not what we want because each server should see the same event stream. We can use the server name as the subscriber ID as shown in the below excerpt from our application:\n\n<wlevs:adapter id=\"InboundTopicAdapter\" provider=\"jms-inbound\">\n\n…\n\n<wlevs:instance-property name=\"durableSubscriptionName\"\n\nvalue=\"${com_bea_wlevs_configuration_server_ClusterType.serverName}\" />\n\n</wlevs:adapter>\n\nThis works because I have placed a ConfigurationPropertyPlaceholderConfigurer bean in my application as shown below, this same bean is also used to access properties from a configuration file:\n\n<bean id=\"ConfigBean\"\n\nclass=\"com.bea.wlevs.spring.support.ConfigurationPropertyPlaceholderConfigurer\">\n\n<property name=\"location\" value=\"file:../Server.properties\"/>\n\n</bean>\n\nWith this configuration each server will now get a copy of all the events.\n\nAs our application relies on elapsed time we should make sure that the timestamps of the received messages are the same on all servers. We do this by adding an HA Input adapter to our application.\n\n<wlevs:adapter id=\"HAInputAdapter\" provider=\"ha-inbound\">\n\n<wlevs:listener ref=\"RequestChannel\" />\n\n<wlevs:instance-property name=\"keyProperties\"\n\nvalue=\"EVID\" />\n\n<wlevs:instance-property name=\"timeProperty\" value=\"arrivalTime\"/>\n\n</wlevs:adapter>\n\nThe HA Adapter sets the given “timeProperty” in the input message to be the current system time. This time is then communicated to other HAInputAdapters deployed to the same group. This allows all servers in the group to have the same timestamp in their event. The event is identified by the “keyProperties” key field.\n\nTo allow the downstream processing to treat the timestamp as an arrival time then the downstream channel is configured with an “application-timestamped” element to set the arrival time of the event. This is shown below:\n\n<wlevs:channel id=\"RequestChannel\" event-type=\"ServiceRequestEvent\">\n\n<wlevs:listener ref=\"MissingEventProcessor\" />\n\n<wlevs:listener ref=\"RaiseEventFilterProcessor\" />\n\n<wlevs:application-timestamped>\n\n<wlevs:expression>arrivalTime</wlevs:expression>\n\n</wlevs:application-timestamped>\n\n</wlevs:channel>\n\nNote the property set in the HAInputAdapter is used to set the arrival time of the event.\n\nSo now all servers in our cluster have the same events arriving from a topic, and each event arrival time is synchronized across the servers in the cluster.\n\nManaging Output\n\nNote that an OEP cluster has multiple servers processing the same input stream. Obviously if we have the same inputs, synchronized to appear to arrive at the same time then we will get the same outputs, which is central to OEPs promise of high availability. So when an alert is raised by our application it will be raised by every server in the cluster. If we have 3 servers in the cluster then we will get 3 copies of the same alert appearing on our alert queue. This is probably not what we want. To fix this we take advantage of an HA Output Adapter. unlike input where there is a single HA Input Adapter there are multiple HA Output Adapters, each with distinct performance and behavioral characteristics. The table below is taken from the Oracle® Fusion Middleware Developer's Guide for Oracle Event Processing and shows the different levels of service and performance impact:\n\nTable 24-1 Oracle Event Processing High Availability Quality of Service\n\nHigh Availability OptionMissed Events?Duplicate Events?Performance OverheadSection 24.1.2.1, \"Simple Failover\"Yes (many)Yes (few)NegligibleSection 24.1.2.2, \"Simple Failover with Buffering\"Yes (few)Foot 1Yes (many)LowSection 24.1.2.3, \"Light-Weight Queue Trimming\"NoYes (few)Low-MediumFoot 2 Section 24.1.2.4, \"Precise Recovery with JMS\"NoNoHigh\n\nI decided to go for the lightweight queue trimming option. This means I won’t lose any events, but I may emit a few duplicate events in the event of primary failure. This setting causes all output events to be buffered by secondary's until they are told by the primary that a particular event has been emitted. To configure this option I add the following adapter to my EPN:\n\n<wlevs:adapter id=\"HAOutputAdapter\" provider=\"ha-broadcast\">\n\n<wlevs:listener ref=\"OutboundQueueAdapter\" />\n\n<wlevs:listener ref=\"PrintBean\" />\n\n<wlevs:instance-property name=\"keyProperties\" value=\"timestamp\"/>\n\n<wlevs:instance-property name=\"monotonic\" value=\"true\"/>\n\n<wlevs:instance-property name=\"totalOrder\" value=\"false\"/>\n\n</wlevs:adapter>\n\nThis uses the time of the alert (timestamp property) as the key to be used to identify events which have been trimmed. This works in this application because the alert time is the time of the source event, and the time of the source events are synchronized using the HA Input Adapter. Because this is a time value then it will increase, and so I set monotonic=”true”. However I may get two alerts raised at the same timestamp and in that case I set totalOrder=”false”.\n\nI also added the additional configuration to config.xml for the application:\n\n<ha:ha-broadcast-adapter>\n\n<name>HAOutputAdapter</name>\n\n<warm-up-window-length units=\"seconds\">15</warm-up-window-length>\n\n<trimming-interval units=\"millis\">1000</trimming-interval>\n\n</ha:ha-broadcast-adapter>\n\nThis causes the primary to tell the secondary's which is its latest emitted alert every 1 second. This will cause the secondary's to trim from their buffers all alerts prior to and including the latest emitted alerts. So in the worst case I will get one second of duplicated alerts. It is also possible to set a number of events rather than a time period. The trade off here is that I can reduce synchronization overhead by having longer time intervals or more events, causing more memory to be used by the secondary's or I can cause more frequent synchronization, using less memory in the secondary's and generating fewer duplicate alerts but there will be more communication between the primary and the secondary's to trim the buffer.\n\nThe warm-up window is used to stop a secondary joining the cluster before it has been running for that time period. The window is based on the time that the EPN needs to be running to be have the same state as the other servers. In our example application we have a CQL that runs for a period of 10 seconds, so I set the warm up window to be 15 seconds to ensure that a newly started server had the same state as all the other servers in the cluster. The warm up window should be greater than the longest query window.\n\nAdding an External Coherence Cluster\n\nWhen we are running OEP as a cluster then we have additional overhead in the servers. The HA Input Adapter is synchronizing event time across the servers, the HA Output adapter is synchronizing output events across the servers. The HA Output adapter is also buffering output events in the secondary’s. We can’t do anything about this but we can move the Coherence Cache we are using outside of the OEP servers, reducing the memory pressure on those servers and also moving some of the processing outside of the server. Making our Coherence caches external to our OEP cluster is a good idea for the following reasons:\n\nAllows moving storage of cache entries outside of the OEP server JVMs hence freeing more memory for storing CQL state.\n\nAllows storage of more entries in the cache by scaling cache independently of the OEP cluster.\n\nMoves cache processing outside OEP servers.\n\nTo create the external Coherence cache do the following:\n\nCreate a new directory for our standalone Coherence servers, perhaps at the same level as the OEP domain directory.\n\nCopy the tangosol-coherence-override.xml file previously created for the OEP cluster into a config directory under the Coherence directory created in the previous step.\n\nCopy the coherence-cache-config.xml file from the application into a config directory under the Coherence directory created in the previous step.\n\nAdd the following to the tangosol-coherence-override.xml file in the Coherence config directory:\n\n<coherence>\n\n<cluster-config>\n\n<member-identity>\n\n<cluster-name>oep_cluster</cluster-name>\n\n<member-name>Grid1</member-name>\n\n</member-identity>\n\n…\n\n</cluster-config>\n\n</coherence>\n\nImportant Note: The <cluster-name> must match the name of the OEP cluster as defined in the <domain><name> element in the event servers config.xml.\n\nThe member name is used to help identify the server.\n\nDisable storage for our caches in the event servers by editing the coherence-cache-config.xml file in the application and adding the following element to the caches:\n\n<distributed-scheme>\n\n<scheme-name>DistributedCacheType</scheme-name>\n\n<service-name>DistributedCache</service-name>\n\n<backing-map-scheme>\n\n<local-scheme/>\n\n</backing-map-scheme>\n\n<local-storage>false</local-storage>\n\n</distributed-scheme>\n\nThe local-storage flag stops the OEP server from storing entries for caches using this cache schema.\n\nDo not disable storage at the global level (-Dtangosol.coherence.distributed.localstorage=false) because this will disable storage on some OEP specific cache schemes as well as our application cache. We don’t want to put those schemes into our cache servers because they are used by OEP to maintain cluster integrity and have only one entry per application per server, so are very small. If we put those into our Coherence Cache servers we would have to add OEP specific libraries to our cache servers and enable them in our coherence-cache-config.xml, all of which is too much trouble for little or no benefit.\n\nIf using Unicast Discovery (this section is not required if using Multicast) then we want to make the Coherence Grid be the Well Known Address servers because we want to disable storage of entries on our OEP servers, and Coherence nodes with storage disabled cannot initialize a cluster. To enable the Coherence servers to be primaries in the Coherence grid do the following:\n\nChange the unicast-listener addresses in the Coherence servers tangosol-coherence-override.xml file to be suitable values for the machine they are running on – typically change the listen address.\n\nModify the WKA addresses in the OEP servers and the Coherence servers tangosol-coherence-override.xml file to match at least two of the Coherence servers listen addresses.\n\nThe following table shows how this might be configured for 2 OEP servers and 2 Cache servers\n\nNote that the OEP servers do not listen on the WKA addresses, using different port numbers even though they run on the same servers as the cache servers.\n\nAlso not that the Coherence servers are the ones that listen on the WKA addresses.\n\nNow that the configuration is complete we can create a start script for the Coherence grid servers as follows:\n\n#!/bin/sh\n\nMW_HOME=/home/oracle/fmw\n\nOEP_HOME=${MW_HOME}/ocep_11.1\n\nJAVA_HOME=${MW_HOME}/jrockit_160_33\n\nCACHE_SERVER_HOME=${MW_HOME}/user_projects/domains/oep_coherence\n\nCACHE_SERVER_CLASSPATH=${CACHE_SERVER_HOME}/HADemoCoherence.jar:${CACHE_SERVER_HOME}/config\n\nCOHERENCE_JAR=${OEP_HOME}/modules/com.tangosol.coherence_3.7.1.6.jar\n\nJAVAEXEC=$JAVA_HOME/bin/java\n\n# specify the JVM heap size\n\nMEMORY=512m\n\nif [[ $1 == '-jmx' ]]; then\n\nJMXPROPERTIES=\"-Dcom.sun.management.jmxremote -Dtangosol.coherence.management=all -Dtangosol.coherence.management.remote=true\"\n\nshift\n\nfi\n\nJAVA_OPTS=\"-Xms$MEMORY -Xmx$MEMORY $JMXPROPERTIES\"\n\n$JAVAEXEC -server -showversion $JAVA_OPTS -cp \"${CACHE_SERVER_CLASSPATH}:${COHERENCE_JAR}\" com.tangosol.net.DefaultCacheServer $1\n\nNote that I put the tangosol-coherence-override and the coherence-cache-config.xml files in a config directory and added that directory to my path (CACHE_SERVER_CLASSPATH=${CACHE_SERVER_HOME}/HADemoCoherence.jar:${CACHE_SERVER_HOME}/config) so that Coherence would find the override file.\n\nBecause my application uses in-cache processing (entry processors) I had to add a jar file containing the required classes for the entry processor to the classpath (CACHE_SERVER_CLASSPATH=${CACHE_SERVER_HOME}/HADemoCoherence.jar:${CACHE_SERVER_HOME}/config).\n\nThe classpath references the Coherence Jar shipped with OEP to avoid versoin mismatches (COHERENCE_JAR=${OEP_HOME}/modules/com.tangosol.coherence_3.7.1.6.jar).\n\nThis script is based on the standard cache-server.sh script that ships with standalone Coherence.\n\nThe –jmx flag can be passed to the script to enable Coherence JMX management beans.\n\nWe have now configured Coherence to use an external data grid for its application caches. When starting we should always start at least one of the grid servers before starting the OEP servers. This will allow the OEP server to find the grid. If we do start things in the wrong order then the OEP servers will block waiting for a storage enabled node to start (one of the WKA servers if using Unicast).\n\nSummary\n\nWe have now created an OEP cluster that makes use of an external Coherence grid for application caches. The application has been modified to ensure that the timestamps of arriving events are synchronized and the output events are only output by one of the servers in the cluster. In event of failure we may get some duplicate events with our configuration (there are configurations that avoid duplicate events) but we will not lose any events. The final version of the application with full HA capability is shown below:\n\nFiles\n\nThe following files are available for download:\n\nOracle Event Processing\n\nIncludes Coherence\n\nNone-HA version of application\n\nIncludes test file TestData.csv and Load Test property file HADemoTest.prop\n\nIncludes Server.properties.Antony file to customize to point to your WLS installation\n\nHA version of application\n\nIncludes test file TestData.csv and Load Test property file HADemoTest.prop\n\nIncludes Server.properties.Antony file to customize to point to your WLS installation\n\nOEP Cluster Files\n\nIncludes config.xml\n\nIncludes tangosol-coherence-override.xml\n\nIncludes Server.properties that will need customizing for your WLS environment\n\nCoherence Cluster Files\n\nIncludes tangosol-coherence-override.xml and coherence-cache-configuration.xml\n\nincludes cache-server.sh start script\n\nIncludes HADemoCoherence.jar with required classes for entry processor\n\nReferences\n\nThe following references may be helpful:\n\nOracle Complex Event Processing High Availability White Paper\n\nAdditional background reading with some good explanations.\n\nOracle® Fusion Middleware Administrator's Guide for Oracle Event Processing\n\nAdministering Multi-Server Domains With Oracle Coherence\n\nIntroduction to Multi-Server Domains\n\nDeploying Applications to Multi-Server Domains\n\nOracle® Fusion Middleware Developer's Guide for Oracle Event Processing\n\nTesting Applications With the Load Generator and csvgen Adapter\n\nDeveloping Applications for High Availability\n\nSchema Reference: Server Configuration wlevs_server_config.xsd\n\nOracle® CEP CQL Language Reference\n\nOracle Fusion Middleware Java API Reference for Oracle Event Processing\n\nClass ConfigurationPropertyPlaceholderConfigurer\n\nOracle® Coherence Developer's Guide\n\nConfiguring Multicast Communication\n\nSpecifying a Cluster Member's Unicast Address\n\nUsing Well Known Addresses\n\nConfiguring Caches\n\nOperational Configuration Elements\n\nwell-known-addresses\n\nCache Configuration Elements\n\ndistributed-scheme\n\n↧\n\nThe Impact of Change\n\nMeasuring Impact of Change in SOA Suite\n\nMormon prophet Thomas S. Monson once said:\n\nWhen performance is measured, performance improves. When performance is measured and reported, the rate of performance accelerates.\n\n(LDS Conference Report, October 1970, p107)\n\nLike everything in life, a SOA Suite installation that is monitored and tracked has a much better chance of performing well than one that is not measured. With that in mind I came up with tool to allow the measurement of the impact of configuration changes on database usage in SOA Suite. This tool can be used to assess the impact of different configurations on both database growth and database performance, helping to decide which optimizations offer real benefit to the composite under test.\n\nBasic Approach\n\nThe basic approach of the tool is to take a snapshot of the number of rows in the SOA tables before executing a composite. The composite is then executed. After the composite has completed another snapshot is taken of the SOA tables. This is illustrated in the diagram below:\n\nAn example of the data collected by the tool is shown below:\n\nTest NameTotal Tables ChangedTotal Rows AddedNotesAsyncTest11315Async Interaction with simple SOA composite, one retry to send response.AsyncTest21213Async interaction with simple SOA composite, no retries on sending response.AsyncTest31213Async interaction with simple SOA composite, no callback address provided.OneWayTest11213One-Way interaction with simple SOA composite.SyncTest177Sync interaction with simple SOA composite.\n\nNote that the first three columns are provided by the tool, the fourth column is just an aide-memoir to identify what the test name actually did. The tool also allows us to drill into the data to get a better look at what is actually changing as shown in the table below:\n\nTest NameTable NameRows AddedAsyncTest1AUDIT_COUNTER1AsyncTest1AUDIT_DETAILS1AsyncTest1AUDIT_TRAIL2AsyncTest1COMPOSITE_INSTANCE1AsyncTest1CUBE_INSTANCE1AsyncTest1CUBE_SCOPE1AsyncTest1DLV_MESSAGE1AsyncTest1DOCUMENT_CI_REF1AsyncTest1DOCUMENT_DLV_MSG_REF1AsyncTest1HEADERS_PROPERTIES1AsyncTest1INSTANCE_PAYLOAD1AsyncTest1WORK_ITEM1AsyncTest1XML_DOCUMENT2\n\nHere we have drilled into the test case with the retry of the callback to see what tables are actually being written to.\n\nFinally we can compare two tests to see difference in the number of rows written and the tables updated as shown below:\n\nTest NameBase Test NameTable NameRow DifferenceAsyncTest1AsyncTest2AUDIT_TRAIL1\n\nHere are the additional tables referenced by this test\n\nTest NameBase Test NameAdditional Table NameRows AddedAsyncTest1AsyncTest2WORK_ROWS1\n\nHow it Works\n\nI created a database stored procedure, soa_snapshot.take_soa_snaphot(test_name, phase). that queries all the SOA tables and records the number of rows in each table. By running the stored procedure before and after the execution of a composite we can capture the number of rows in the SOA database before and after a composite executes. I then created a view that shows the difference in the number of rows before and after composite execution. This view has a number of sub-views that allow us to query specific items. The schema is shown below:\n\nThe different tables and views are:\n\nCHANGE_TABLE\n\nUsed to track number of rows in SOA schema, each test case has two or more phases. Usually phase 1 is before execution and phase 2 is after execution.\n\nThis only used by the stored procedure and the views.\n\nDELTA_VIEW\n\nUsed to track changes in number of rows in SOA database between phases of a test case. This is a view on CHANGE_TABLE. All other views are based off this view.\n\nSIMPLE_DELTA_VIEW\n\nProvides number of rows changed in each table.\n\nSUMMARY_DELTA_VIEW\n\nProvides a summary of total rows and tables changed.\n\nDIFFERENT_ROWS_VIEW\n\nProvides a summary of differences in rows updated between test cases\n\nEXTRA_TABLES_VIEW\n\nProvides a summary of the extra tables and rows used by a test case.\n\nThis view makes use of a session context, soa_ctx, which holds the test case name and the baseline test case name. This context is initialized by calling the stored procedure soa_ctx_pkg.set(testCase, baseTestCase).\n\nI created a web service wrapper to the take_soa_snapshot procedure so that I could use SoapUI to perform the tests.\n\nSample Output\n\nHow many rows and tables did a particular test use?\n\nHere we can see how many rows in how many tables changed as a result of running a test:\n\n-- Display the total number of rows and tables changed for each test\n\nselect * from summary_delta_view\n\norder by test_name;\n\nTEST_NAME TOTALDELTAROWS TOTALDELTASIZE TOTALTABLES\n\n-------------------- -------------- -------------- -----------\n\nAsyncTest1 15 0 13\n\nAsyncTest1noCCIS 15 0 13\n\nAsyncTest1off 8 0 8\n\nAsyncTest1prod 13 0 12\n\nAsyncTest2 13 0 12\n\nAsyncTest2noCCIS 13 0 12\n\nAsyncTest2off 7 0 7\n\nAsyncTest2prod 11 0 11\n\nAsyncTest3 13 0 12\n\nAsyncTest3noCCIS 13 65536 12\n\nAsyncTest3off 7 0 7\n\nAsyncTest3prod 11 0 11\n\nOneWayTest1 13 0 12\n\nOneWayTest1noCCI 13 65536 12\n\nOneWayTest1off 7 0 7\n\nOneWayTest1prod 11 0 11\n\nSyncTest1 7 0 7\n\nSyncTest1noCCIS 7 0 7\n\nSyncTest1off 2 0 2\n\nSyncTest1prod 5 0 5\n\n20 rows selected\n\nWhich tables grew during a test?\n\nHere for a given test we can see which tables had rows inserted.\n\n-- Display the tables which grew and show the number of rows they grew by\n\nselect * from simple_delta_view\n\nwhere test_name='AsyncTest1'\n\norder by table_name;\n\nTEST_NAME TABLE_NAME DELTAROWS DELTASIZE\n\n-------------------- ------------------------------ ---------- ----------\n\nAsyncTest1 AUDIT_COUNTER 1 0\n\nAsyncTest1 AUDIT_DETAILS 1 0\n\nAsyncTest1 AUDIT_TRAIL 2 0\n\nAsyncTest1 COMPOSITE_INSTANCE 1 0\n\nAsyncTest1 CUBE_INSTANCE 1 0\n\nAsyncTest1 CUBE_SCOPE 1 0\n\nAsyncTest1 DLV_MESSAGE 1 0\n\nAsyncTest1 DOCUMENT_CI_REF 1 0\n\nAsyncTest1 DOCUMENT_DLV_MSG_REF 1 0\n\nAsyncTest1 HEADERS_PROPERTIES 1 0\n\nAsyncTest1 INSTANCE_PAYLOAD 1 0\n\nAsyncTest1 WORK_ITEM 1 0\n\nAsyncTest1 XML_DOCUMENT 2 0\n\n13 rows selected\n\nWhich tables grew more in test1 than in test2?\n\nHere we can see the differences in rows for two tests.\n\n-- Return difference in rows updated (test1)\n\nselect * from different_rows_view\n\nwhere test1='AsyncTest1' and test2='AsyncTest2';\n\nTEST1 TEST2 TABLE_NAME DELTA\n\n-------------------- -------------------- ------------------------------ ----------\n\nAsyncTest1 AsyncTest2 AUDIT_TRAIL 1\n\nWhich tables were used by test1 but not by test2?\n\nHere we can see tables that were used by one test but not by the other test.\n\n-- Register base test case for use in extra_tables_view\n\n-- First parameter (test1) is test we expect to have extra rows/tables\n\nbegin soa_ctx_pkg.set('AsyncTest1', 'AsyncTest2'); end;\n\n/\n\nanonymous block completed\n\n-- Return additional tables used by test1\n\ncolumn TEST2 FORMAT A20\n\nselect * from extra_tables_view;\n\nTEST1 TEST2 TABLE_NAME DELTAROWS\n\n-------------------- -------------------- ------------------------------ ----------\n\nAsyncTest1 AsyncTest2 WORK_ITEM 1\n\nResults\n\nI used the tool to find out the following. All tests were run using SOA Suite 11.1.1.7.\n\nThe following is based on a very simple composite as shown below:\n\nEach BPEL process is basically the same as the one shown below:\n\nImpact of Fault Policy Retry Being Executed Once\n\nSettingTotal Rows Written Total Tables UpdatedNo Retry1312One Retry1513\n\nWhen a fault policy causes a retry then the following additional database rows are written:\n\nTable NameNumber of RowsAUDIT_TRAIL1WORK_ITEM1\n\nImpact of Setting Audit Level = Development Instead of Production\n\nSettingTotal Rows Written Total Tables UpdatedDevelopment1312Production1111\n\nWhen the audit level is set at development instead of production then the following additional database rows are written:\n\nTable NameNumber of RowsAUDIT_TRAIL1WORK_ITEM1\n\nImpact of Setting Audit Level = Production Instead of Off\n\nSettingTotal Rows Written Total Tables UpdatedProduction1111Off77\n\nWhen the audit level is set at production rather than off then the following additional database rows are written:\n\nTable NameNumber of RowsAUDIT_COUNTER1AUDIT_DETAILS1AUDIT_TRAIL1COMPOSITE_INSTANCE1\n\nImpact of Setting Capture Composite Instance State\n\nSettingTotal Rows Written Total Tables UpdatedOn1312Off1312\n\nWhen capture composite instance state is on rather than off then no additional database rows are written, note that there are other activities that occur when composite instance state is captured:\n\nImpact of Setting oneWayDeliveryPolicy = async.cache or sync\n\nSettingTotal Rows Written Total Tables Updatedasync.persist1312async.cache77sync77\n\nWhen choosing async.persist (the default) instead of sync or async.cache then the following additional database rows are written:\n\nTable NameNumber of RowsAUDIT_DETAILS1DLV_MESSAGE1DOCUMENT_CI_REF1DOCUMENT_DLV_MSG_REF1HEADERS_PROPERTIES1XML_DOCUMENT1\n\nAs you would expect the sync mode behaves just as a regular synchronous (request/reply) interaction and creates the same number of rows in the database. The async.cache also creates the same number of rows as a sync interaction because it stores state in memory and provides no restart guarantee.\n\nCaveats & Warnings\n\nThe results above are based on a trivial test case. The numbers will be different for bigger and more complex composites. However by taking snapshots of different configurations you can produce the numbers that apply to your composites.\n\nThe capture procedure supports multiple steps in a test case, but the views only support two snapshots per test case.\n\nCode Download\n\nThe sample project I used us available here.\n\nThe scripts used to create the user (createUser.sql), create the schema (createSchema.sql) and sample queries (TableCardinality.sql) are available here.\n\nThe Web Service wrapper to the capture state stored procedure is available here.\n\nThe sample SoapUI project that I used to take a snapshot, perform the test and take a second snapshot is available here.\n\n↧\n\n↧\n\nPackt Publishing Buy One Get One Free Offer\n\nPackt Publishing celebrates their 2000th title with a Buy One Get One Free Offer\n\nGreat time to get those Packt books you’ve been thinking of buying, like the SOA Suite 11g Developers Guide or the SOA Suite 11g Developers Cookbook.\n\n↧\n\nNot Just a Cache\n\nCoherence as a Compute Grid\n\nCoherence is best known as a data grid, providing distributed caching with an ability to move processing to the data in the grid. Less well known is the fact that Coherence also has the ability to function as a compute grid, distributing work across multiple servers in a cluster. In this entry, which was co-written with my colleague Utkarsh Nadkarni, we will look at using Coherence as a compute grid through the use of the Work Manager API and compare it to manipulating data directly in the grid using Entry Processors.\n\nCoherence Distributed Computing Options\n\nThe Coherence documentation identifies several methods for distributing work across the cluster, see Processing Data in a Cache. They can be summarized as:\n\nEntry Processors\n\nAn InvocableMap interface, inherited by the NamedCache interface, provides support for executing an agent (EntryProcessor or EntryAggregator) on individual entries within the cache.\n\nThe entries may or may not exist, either way the agent is executed once for each key provided, or if no key is provided then it is executed once for each object in the cache.\n\nIn Enterprise and Grid editions of Coherence the entry processors are executed on the primary cache nodes holding the cached entries.\n\nAgents can return results.\n\nOne agent executes multiple times per cache node, once for each key targeted on the node.\n\nInvocation Service\n\nAn InvocationService provides support for executing an agent on one or more nodes within the grid.\n\nExecution may be targeted at specific nodes or at all nodes running the Invocation Service.\n\nAgents can return results.\n\nOne agent executes once per node.\n\nWork Managers\n\nA WorkManager class provides a grid aware implementation of the commonJ WorkManager which can be used to run tasks across multiple threads on multiple nodes within the grid.\n\nWorkManagers run on multiple nodes.\n\nEach WorkManager may have multiple threads.\n\nTasks implement the Work interface and are assigned to specific WorkManager threads to execute.\n\nEach task is executed once.\n\nThree Models of Distributed Computation\n\nThe previous section listing the distributed computing options in Coherence shows that there are 3 distinct execution models:\n\nPer Cache Entry Execution (Entry Processor)\n\nExecute the agent on the entry corresponding to a cache key.\n\nEntries processed on a single thread per node.\n\nParallelism across nodes.\n\nPer Node Execution (Invocation Service)\n\nExecute the same agent once per node.\n\nAgent processed on a single thread per node.\n\nParallelism across nodes.\n\nPer Task Execution (Work Manager)\n\nEach task executed once.\n\nParallelism across nodes and across threads within a node.\n\nThe entry processor is good for operating on individual cache entries. It is not so good for working on groups of cache entries.\n\nThe invocation service is good for performing checks on a node, but is limited in its parallelism.\n\nThe work manager is good for operating on groups of related entries in the cache or performing non-cache related work in parallel. It has a high degree of parallelism.\n\nAs you can see the primary choice for distributed computing comes down to the Work Manager and the Entry Processor.\n\nDifferences between using Entry Processors and Work Managers in Coherence\n\nAspectEntry ProcessorsWork ManagersDegree of parallelizationIs a function of the number of Coherence nodes. EntryProcessors are run concurrently across all nodes in a cluster. However, within each node only one instance of the entry processor executes at a time.Is a function of the number of Work Manager threads. The Work is run concurrently across all threads in all Work Manager instances.TransactionalityTransactional. If an EntryProcessor running on one node does not complete (say, due to that node crashing), the entries targeted will be executed by an EntryProcessor on another node.Not transactional. The specification does not explicitly specify what the response should be if a remote server crashes during an execution. Current implementation uses WORK_COMPLETED with WorkCompletedException as a result. In case a Work does not run to completion, it is the responsibility of the client to resubmit the Work to the Work Manager. How is the Cache accessed or mutated?Operations against the cache contents are executed by (and thus within the localized context of) a cache.Accesses and changes to the cache are done directly through the cache API.Where is the processing performed?In the same JVM where the entries-to-be-processed reside.In the Work Manager server. This may not be the same JVM where the entries-to-be-processed reside.Network TrafficIs a function of the size of the EntryProcessor. Typically, the size of an EntryProcessor is much smaller than the size of the data transferred across nodes in the case of a Work Manager approach. This makes the EntryProcessor approach more network-efficient and hence more scalable. One EntryProcessor is transmitted to each cache node.Is a function of the\n\nNumber of Work Objects, of which multiple may be sent to each server.\n\nSize of the data set transferred from the Backing Map to the Work Manager Server.\n\nDistribution of “Tasks”Tasks are moved to the location at which the entries-to-be-processed are being managed. This may result in a random distribution of tasks. The distribution tends to get equitable as the number of entries increases.Tasks are distributed equally across the threads in the Work Manager Instances.Implementation of the EntryProcessor or Work class.Create a class that extends AbstractProcessor. Implement the process method. Update the cache item based on the key passed in to the process method.Create a class that is serializable and implements commonj.work.Work. Implement the run method.Implementation of “Task”In the process method, update the cache item based on the key passed into the process method.In the run method, do the following:\n\nGet a reference to the named cache\n\nDo the Work – Get a reference to the Cache Item; change the cache item; put the cache item back into the named cache.\n\nCompletion NotificationWhen the NamedCache.invoke method completes then all the entry processors have completed executing.When a task is submitted for execution it executes asynchronously on the work manager threads in the cluster. Status may be obtained by registering a commonj.work.WorkListener class when calling the WorkManager.schedule method. This will provide updates when the Work is accepted, started and completed or rejected. Alternatively the WorkManager.waitForAll and WorkManager.waitForAny methods allow blocking waits for either all or one result respectively.Returned Resultsjava.lang.Object – when executed on one cache item. This returns result of the invocation as returned from the EntryProcessor.\n\njava.util.Map – when executed on a collection of keys. This returns a Map containing the results of invoking the EntryProcessor against each of the specified keys. commonj.work.WorkItem - There are three possible outcomes\n\nThe Work is not yet complete. In this case, a null is returned by WorkItem.getResult.\n\nThe Work started but completed with an exception. This may have happened due to a Work Manager Instance terminating abruptly. This is indicated by an exception thrown by WorkItem.getResult.\n\nThe Work Manager instance indicated that the Work is complete and the Work ran to completion. In this case, WorkItem.getResult returns a non-null and no exception is thrown by WorkItem.getResult.\n\nError HandlingFailure of a node results in all the work assigned to that node being executed on the new primary. This may result in some work being executed twice, but Coherence ensures that the cache is only updated once per item.Failure of a node results in the loss of scheduled tasks assigned to that node. Completed tasks are sent back to the client as they complete.\n\nFault Handling Extension\n\nEntry processors have excellent error handling within Coherence. Work Managers less so. In order to provide resiliency on node failure I implemented a “RetryWorkManager” class that detects tasks that have failed to complete successfully and resubmits them to the grid for another attempt.\n\nA JDeveloper project with the RetryWorkManager is available for download here. It includes sample code to run a simple task across multiple work manager threads.\n\nTo create a new RetryWorkManager that will retry failed work twice then you would use this:\n\nWorkManager = new RetryWorkManager(\"WorkManagerName\", 2); // Change for number of retries, if no retry count is provided then the default is 0.\n\nYou can control the number of retries at the individual work level as shown below:\n\nWorkItem workItem = schedule(work); // Use number of retries set at WorkManager creation\n\nWorkItem workItem = schedule(work, workListener); // Use number of retries set at WorkManager creation\n\nWorkItem workItem = schedule(work, 4); // Change number of retries\n\nWorkItem workItem = schedule(work, workListener, 4); // Change number of retries\n\nCurrently the RetryWorkManager defaults to having 0 threads. To change use this constructor:\n\nWorkItem workItem = schedule(work, workListener, 3, 4); // Change number of threads (3) and retries (4)\n\nNote that none of this sample code is supported by Oracle in any way, and is provided purely as a sample of what can be done with Coherence.\n\nHow the RetryWorkManager Works\n\nThe RetryWorkManager delegates most operations to a Coherence WorkManager instance. It creates a WorkManagerListener to intercept status updates. On receiving a WORK_COMPLETED callback the listener checks the result to see if the completion is due to an error. If an error occurred and there are retries left then the work is resubmitted. The WorkItem returned by scheduling an event is wrapped in a RetryWorkItem. This RetryWorkItem is updated with a new Coherence WorkItem when the task is retried. If the client registers a WorkManagerListener then the RetryWorkManagerListener delegates non-retriable events to the client listener. Finally the waitForAll and waitForAny methods are modified to deal with work items being resubmitted in the event of failure.\n\nSample Code for EntryProcessor and RetryWorkManager\n\nThe downloadable project contains sample code for running the work manager and an entry processor.\n\nThe demo implements a 3-tier architecture\n\nCoherence Cache Servers\n\nCan be started by running RunCacheServer.cmd\n\nRuns a distributed cache used by the Task to be executed in the grid\n\nCoherence Work Manager Servers\n\nCan be started by running RunWorkManagerServer.cmd\n\nTakes no parameters\n\nRuns two threads for executing tasks\n\nCoherence Work Manager Clients\n\nCan be started by running RunWorkManagerClient.cmd\n\nTakes three parameters currently\n\nWork Manager name - should be \"AntonyWork\" - default is \"AntonyWork\"\n\nNumber of tasks to schedule - default is 10\n\nTime to wait for tasks to complete in seconds - default is 60\n\nThe task stores the number of times it has been executed in the cache, so multiple runs will see the counter incrementing. The choice between EntryProcessor and WorkManager is controlled by changing the value of USE_ENTRY_PROCESSOR between false and true in the RunWorkManagerClient.cmd script.\n\nThe SetWorkManagerEnv.cmd script should be edited to point to the Coherence home directory and the Java home directory.\n\nSummary\n\nIf you need to perform operations on cache entries and don’t need to have cross-checks between the entries then the best solution is to use an entry processor. The entry processor is fault tolerant and updates to the cached entity will be performed once only.\n\nIf you need to perform generic work that may need to touch multiple related cache entries then the work manager may be a better solution. The extensions I created in the RetryWorkManager provide a degree of resiliency to deal with node failure without impacting the client.\n\nThe RetryWorkManager can be downloaded here.\n\n↧\n\nOne Queue to Rule them All\n\nUsing a Single Queue for Multiple Message Types with SOA Suite\n\nProblem Statement\n\nYou use a single JMS queue for sending multiple message types / service requests. You use a single JMS queue for receiving multiple message types / service requests. You have multiple SOA JMS Adapter interfaces for reading and writing these queues. In a composite it is random which interface gets a message from the JMS queue. It is not a problem having multiple adapter instances writing to a single queue, the problem is only with having multiple readers because each reader gets the first message on the queue.\n\nBackground\n\nThe JMS Adapter is unaware of who receives the messages. Each adapter instance just takes the message from the queue and delivers it to its own configured interface, one interface per adapter instance. The SOA infrastructure is then responsible for routing that message, usually via a database table and an in memory notification message, to a component within a composite. Each message will create a new composite but the BPEL engine and Mediator engine will attempt to match callback messages to the appropriate Mediator or BPEL instance.\n\nNote that message type, including XML document type, has nothing to do with the preceding statements.\n\nThe net result is that if you have a sequence of two receives from the same queue using different adapters then the messages will be split equally between the two adapters, meaning that half the time the wrong adapter will receive the message. This blog entry looks at how to resolve this issue.\n\nNote that the same problem occurs whenever you have more than 1 adapter listening to the same queue, whether they are in the same composite or different composites. The solution in this blog entry is also relevant to this use case.\n\nSolutions\n\nIn order to correctly deliver the messages to the correct interface we need to identify the interface they should be delivered to. This can be done by using JMS properties. For example the JMSType property can be used to identify the type of the message. A message selector can be added to the JMS inbound adapter that will cause the adapter to filter out messages intended for other interfaces. For example if we need to call three services that are implemented in a single application:\n\nService 1 receives messages on the single outbound queue from SOA, it send responses back on the single inbound queue.\n\nSimilarly Service 2 and Service 3 also receive messages on the single outbound queue from SOA, they send responses back on the single inbound queue.\n\nFirst we need to ensure the messages are delivered to the correct adapter instance. This is achieved as follows:\n\naThe inbound JMS adapter is configured with a JMS message selector. The message selector might be \"JMSType='Service1'\" for responses from Service 1. Similarly the selector would be \"JMSType='Service2'\" for the adapter waiting on a response from Service 2. The message selector ensures that each adapter instance will retrieve the first message from the queue that matches its selector.\n\nThe sending service needs to set the JMS property (JMSType in our example) that is used in the message selector.\n\nNow our messages are being delivered to the correct interface we need to make sure that they get delivered to the correct Mediator or BPEL instance. We do this with correlation. There are several correlation options:\n\nWe can do manual correlation with a correlation set, identifying parts of the outbound message that uniquely identify our instance and matching them with parts of the inbound message to make the correlation.\n\nWe can use a Request-Reply JMS adapter which by default expects the response to contain a JMSCorrelationID equal to the outgoing JMSMessageID. Although no configuration is required for this on the SOA client side, the service needs to copy the incoming JMSMessageID to the outgoing JMSCorrelationID.\n\nSpecial Case - Request-Reply Synchronous JMS Adapter\n\nWhen using a synchronous Request-Reply JMS adapter we can omit to specify the message selector because the Request-Reply JMS adapter will immediately do a listen with a message selector for the correlation ID rather than processing the incoming message asynchronously.\n\nThe synchronous request-reply will block the BPEL process thread and hold open the BPEL transaction until a response is received, so this should only be used when you expect the request to be completed in a few seconds.\n\nThe JCA Connection Factory used must point to a non-XA JMS Connection Factory and must have the isTransacted property set to “false”. See the documentation for more details.\n\nSample\n\nI developed a JDeveloper SOA project that demonstrates using a single queue for multiple incoming adapters. The overall process flow is shown in the picture below. The BPEL process on the left receives messages from the jms/TestQueue2 and sends messages to the jms/Test Queue1. A Mediator is used to simulate multiple services and also provide a web interface to initiate the process. The correct adapter is identified by using JMS message properties and a selector.\n\nThe flow above shows that the process is initiated from EM using a web service binding on mediator. The mediator, acting as a client, posts the request to the inbound queue with a JMSType property set to \"Initiate\".\n\nModelClientBPELServiceInbound RequestClient receives web service request and posts the request to the inbound queue with JMSType='Initiate'The JMS adapter with a message selector \"JMSType='Initiate'\" receives the message and causes a composite to be created. The composite in turn causes the BPEL process to start executing.\n\nThe BPEL process then sends a request to Service 1 on the outbound queue.\n\nKey Points\n\nInitiate message can be used to initate a correlation set if necessary\n\nSelector required to distinguish initiate messages from other messages on the queue\n\nService 1 receives the request and sends a response on the inbound queue with JMSType='Service1' and JMSCorrelationID= incoming JMS Message ID.Separate Request and Reply AdaptersThe JMS adapter with a message selector \"JMSType='Service1'\" receives the message and causes a composite to be created. The composite uses a correlation set to in turn deliver the message to BPEL which correlates it with the existing BPEL process.\n\nThe BPEL process then sends a request to Service 2 on the outbound queue.\n\nKey Points\n\nSeparate request & reply adapters require a correlation set to ensure that reply goes to correct BPEL process instance\n\nSelector required to distinguish service 1 response messages from other messages on the queue\n\nService 2 receives the request and sends a response on the inbound queue with JMSType='Service2' and JMSCorrelationID= incoming JMS Message ID.Asynchronous Request-Reply AdapterThe JMS adapter with a message selector \"JMSType='Service2'\" receives the message and causes a composite to be created. The composite in turn delivers the message to the existing BPEL process using native JMS correlation.\n\nKey Point\n\nAsynchronous request-reply adapter does not require a correlation set, JMS adapter auto-correlates using CorrelationID to ensure that reply goes to correct BPEL process instance\n\nSelector still required to distinguish service 2 response messages from other messages on the queue\n\nThe BPEL process then sends a request to Service 3 on the outbound queue using a synchronous request-reply.\n\nService 3 receives the request and sends a response on the inbound queue with JMSType='Service2' and JMSCorrelationID= incoming JMS Message ID.Synchronous Request-Reply AdapterThe synchronous JMS adapter receives the response without a message selector and correlates it to the BPEL process using native JMS correlation and sends the overall response to the outbound queue.\n\nKey Points\n\nSynchronous request-reply adapter does not require a correlation set, JMS adapter auto-correlates using CorrelationID to ensure that reply goes to correct BPEL process instance\n\nSelector also not required to distinguish service 3 response messages from other messages on the queue because the synchronous adapter is doing a selection on the expected CorrelationID\n\nOutbound ResponseClient receives the response on an outbound queue.\n\nSummary\n\nWhen using a single JMS queue for multiple purposes bear in mind the following:\n\nIf multiple receives use the same queue then you need to have a message selector. The corollary to this is that the message sender must add a JMS property to the message that can be used in the message selector.\n\nWhen using a request-reply JMS adapter then there is no need for a correlation set, correlation is done in the adapter by matching the outbound JMS message ID to the inbound JMS correlation ID. The corollary to this is that the message sender must copy the JMS request message ID to the JMS response correlation ID.\n\nWhen using a synchronous request-reply JMS adapter then there is no need for the message selector because the message selection is done based on the JMS correlation ID.\n\nSynchronous request-reply adapter requires a non-XA connection factory to be used so that the request part of the interaction can be committed separately to the receive part of the interaction.\n\nSynchronous request-reply JMS adapter should only be used when the reply is expected to take just a few seconds. If the reply is expected to take longer then the asynchronous request-reply JMS adapter should be used.\n\nDeploying the Sample\n\nThe sample is available to download here and makes use of the following JMS resources:\n\nJNDIResource;Notesjms/TestQueueQueueOutbound queue from the BPEL processjms/TestQueue2QueueInbound queue to the BPEL processeis/wls/TestQueueJMS Adapter Connector FactoryThis can point to an XA or non-XA JMS Connection Factory such as weblogic.jms.XAConnectionFactoryeis/wls/TestQueueNone-XA JMS Adapter Connector FactoryThis must point to a non-XA JMS Connection Factory such as weblogic.jms.ConnectionFactory and must have isTransacted set to “false”\n\nTo run the sample then just use the test facility in the EM console or the soa-infra application.\n\n↧\n\nCoherence Adapter Configuration\n\nSOA Suite 12c Coherence Adapter\n\nThe release of SOA Suite 12c sees the addition of a Coherence Adapter to the list of Technology Adapters that are licensed with the SOA Suite. In this entry I provide an introduction to configuring the adapter and using the different operations it supports.\n\nThe Coherence Adapter provides access to Oracles Coherence Data Grid. The adapter provides access to the cache capabilities of the grid, it does not currently support the many other features of the grid such as entry processors – more on this at the end of the blog.\n\nPreviously if you wanted to use Coherence from within SOA Suite you either used the built in caching capability of OSB or resorted to writing Java code wrapped as a Spring component. The new adapter significantly simplifies simple cache access operations.\n\nConfiguration\n\nWhen creating a SOA domain the Coherence adapter is shipped with a very basic configuration that you will probably want to enhance to support real requirements. In this section I look at the configuration required to use Coherence adapter in the real world.\n\nActivate Adapter\n\nThe Coherence Adapter is not targeted at the SOA server by default, so this targeting needs to be performed from within the WebLogic console before the adapter can be used.\n\nCreate a cache configuration file\n\nThe Coherence Adapter provides a default connection factory to connect to an out-of-box Coherence cache and also a cache called adapter-local. This is helpful as an example but it is good practice to only have a single type of object within a Coherence cache, so we will need more than one. Without having multiple caches then it is hard to clean out all the objects of a particular type. Having multiple caches also allows us to specify different properties for each cache. The following is a sample cache configuration file used in the example.\n\n<?xml version=\"1.0\"?>\n\n<!DOCTYPE cache-config SYSTEM \"cache-config.dtd\">\n\n<cache-config>\n\n<caching-scheme-mapping>\n\n<cache-mapping>\n\n<cache-name>TestCache</cache-name>\n\n<scheme-name>transactional</scheme-name>\n\n</cache-mapping>\n\n</caching-scheme-mapping>\n\n<caching-schemes>\n\n<transactional-scheme>\n\n<scheme-name>transactional</scheme-name>\n\n<service-name>DistributedCache</service-name>\n\n<autostart>true</autostart>\n\n</transactional-scheme>\n\n</caching-schemes>\n\n</cache-config>\n\nThis defines a single cache called TestCache. This is a distributed cache, meaning that the entries in the cache will distributed across the grid. This enables you to scale the storage capacity of the grid by adding more servers. Additional caches can be added to this configuration file by adding additional <cache-mapping> elements.\n\nThe cache configuration file is reference by the adapter connection factory and so needs to be on a file system accessed by all servers running the Coherence Adapter. It is not referenced from the composite.\n\nCreate a Coherence Adapter Connection Factory\n\nWe find the correct cache configuration by using a Coherence Adapter connection factory. The adapter ships with a few sample connection factories but we will create new one. To create a new connection factory we do the following:\n\nOn the Outbound Connection Pools tab of the Coherence Adapter deployment we select New to create the adapter.\n\nChoose the javax.resource.cci.ConnectionFactory group.\n\nProvide a JNDI name, although you can use any name something along the lines of eis/Coherence/Test is a good practice (EIS tells us this an adapter JNDI, Coherence tells us it is the Coherence Adapter, and then we can identify which adapter configuration we are using).\n\nIf requested to create a Plan.xml then make sure that you save it in a location available to all servers.\n\nFrom the outbound connection pool tab select your new connection factory so that you can configure it from the properties tab.\n\nSet the CacheConfigLocation to point to the cache configuration file created in the previous section.\n\nSet the ClassLoaderMode to CUSTOM.\n\nSet the ServiceName to the name of the service used by your cache in the cache configuration file created in the previous section.\n\nSet the WLSExtendProxy to false unless your cache configuration file is using an extend proxy.\n\nIf you plan on using POJOs (Plain Old Java Objects) with the adapter rather than XML then you need to point the PojoJarFile at the location of a jar file containing your POJOs.\n\nMake sure to press enter in each field after entering your data. Remember to save your changes when done.\n\nYou may will need to stop and restart the adapter to get it to recognize the new connection factory.\n\nOperations\n\nTo demonstrate the different operations I created a WSDL with the following operations:\n\nput – put an object into the cache with a given key value.\n\nget – retrieve an object from the cache by key value.\n\nremove – delete an object from the cache by key value.\n\nlist – retrieve all the objects in the cache.\n\nlistKeys – retrieve all the keys of the objects in the cache.\n\nremoveAll – remove all the objects from the cache.\n\nI created a composite based on this WSDL that calls a different adapter reference for each operation. Details on configuring the adapter within a composite are provided in the Configuring the Coherence Adapter section of the documentation.\n\nI used a Mediator to map the input WSDL operations to the individual adapter references.\n\nSchema\n\nThe input schema is shown below.\n\nThis type of pattern is likely to be used in all XML types stored in a Coherence cache. The XMLCacheKey element represents the cache key, in this schema it is a string, but could be another primitive type. The other fields in the cached object are represented by a single XMLCacheContent field, but in a real example you are likely to have multiple fields at this level. Wrapper elements are provided for lists of elements (XMLCacheEntryList) and lists of cache keys (XMLCacheEntryKeyList). XMLEmpty is used for operation that don’t require an input.\n\nPut Operation\n\nThe put operation takes an XMLCacheEntry as input and passes this straight through to the adapter. The XMLCacheKey element in the entry is also assigned to the jca.coherence.key property. This sets the key for the cached entry. The adapter also supports automatically generating a key, which is useful if you don’t have a convenient field in the cached entity. The cache key is always returned as the output of this operation.\n\nGet Operation\n\nThe get operation takes an XMLCacheKey as input and assigns this to the jca.coherence.key property. This sets the key for the entry to be retrieved.\n\nRemove Operation\n\nThe remove operation takes an XMLCacheKey as input and assigns this to the jca.coherence.key property. This sets the key for the entry to be deleted.\n\nRemoveAll Operation\n\nThis is similar to the remove operation but instead of using a key as input to the remove operation it uses a filter. The filter could be overridden by using the jca.coherence.filter property but for this operation it was permanently set in the adapter wizard to be the following query:\n\nkey() != \"\"\n\nThis selects all objects whose key is not equal to the empty string. All objects should have a key so this query should select all objects for deletion.\n\nNote that there appears to be a bug in the return value. The return value is entry rather than having the expected RemoveResponse element with a Count child element. Note the documentation states that\n\nWhen using a filter for a Remove operation, the Coherence Adapter does not report the count of entries affected by the remove operation, regardless of whether the remove operation is successful.\n\nWhen using a key to remove a specific entry, the Coherence Adapter does report the count, which is always 1 if a Coherence Remove operation is successful.\n\nAlthough this could be interpreted as meaning an empty part is returned, an empty part is a violation of the WSDL contract.\n\nList Operation\n\nThe list operation takes no input and returns the result list returned by the adapter. The adapter also supports querying using a filter. This filter is essentially the where clause of a Coherence Query Language statement. When using XML types as cached entities then only the key() field can be tested, for example using a clause such as:\n\nkey() LIKE “Key%1”\n\nThis filter would match all entries whose key starts with “Key” and ends with “1”.\n\nListKeys Operation\n\nThe listKeys operation is essentially the same as the list operation except that only the keys are returned rather than the whole object.\n\nTesting\n\nTo test the composite I used the new 12c Test Suite wizard to create a number of test suites. The test suites should be executed in the following order:\n\nCleanupTestSuite has a single test that removes all the entries from the cache used by this composite.\n\nInitTestSuite has 3 tests that insert a single record into the cache. The returned key is validated against the expected value.\n\nMainTestSuite has 5 tests that list the elements and keys in the cache and retrieve individual inserted elements. This tests that the items inserted in the previous test are actually in the cache. It also tests the get, list and listAll operations and makes sure they return the expected results.\n\nRemoveTestSuite has a single test that removes an element from the cache and tests that the count of removed elements is 1.\n\nValidateRemoveTestSuite is similar to MainTestSuite but verifies that the element removed by the previous test suite has actually been removed.\n\nUse Case\n\nOne example of using the Coherence Adapter is to create a shared memory region that allows SOA composites to share information. An example of this is provided by Lucas Jellema in his blog entry First Steps with the Coherence Adapter to create cross instance state memory.\n\nHowever there is a problem in creating global variables that can be updated by multiple instances at the same time. In this case the get and put operations provided by the Coherence adapter support a last write wins model. This can be avoided in Coherence by using an Entry Processor to update the entry in the cache, but currently entry processors are not supported by the Coherence Adapter. In this case it is still necessary to use Java to invoke the entry processor.\n\nSample Code\n\nThe sample code I refer to above is available for download and consists of two JDeveloper projects, one with the cache config file and the other with the Coherence composite.\n\nCoherenceConfig has the cache config file that must be referenced by the connection factory properties.\n\nCoherenceSOA has a composite that supports the WSDL introduced at the start of this blog along with the test cases mentioned at the end of the blog.\n\nThe Coherence Adapter is a really exciting new addition to the SOA developers toolkit, hopefully this article will help you make use of it.\n\n↧\n\n↧\n\nSlicing the EDG\n\nDifferent SOA Domain Configurations\n\nIn this blog entry I would like to introduce three different configurations for a SOA environment. I have omitted load balancers and OTD/OHS as they introduce a whole new round of discussion. For each possible deployment architecture I have identified some of the advantages.\n\nSuper Domain\n\nThis is a single EDG style domain for everything needed for SOA/OSB. It extends the standard EDG slightly but otherwise assumes a single “super” domain.\n\nThis is basically the SOA EDG. I have broken out JMS servers and Coherence servers to improve scalability and reduce dependencies.\n\nKey Points\n\nSeparate JMS allows those servers to be kept up separately from rest of SOA Domain, allowing JMS clients to post messages even if rest of domain is unavailable.\n\nJMS servers are only used to host application specific JMS destinations, SOA/OSB JMS destinations remain in relevant SOA/OSB managed servers.\n\nSeparate Coherence servers allow OSB cache to be offloaded from OSB servers.\n\nUse of Coherence by other components as a shared infrastructure data grid service.\n\nCoherence cluster may be managed by WLS but more likely run as a standalone Coherence cluster.\n\nBenefits\n\nSingle Administration Point (1 Admin Server)\n\nClosely follows EDG with addition of application specific JMS servers and standalone Coherence servers for OSB caching and application specific caches.\n\nCoherence grid can be scaled independent of OSB/SOA.\n\nJMS queues provide for inter-application communication.\n\nDrawbacks\n\nPatching is an all or nothing affair.\n\nStartup time for SOA may be slow if large number of composites deployed.\n\nMultiple Domains\n\nThis extends the EDG into multiple domains, allowing separate management and update of these domains. I see this type of configuration quite often with customers, although some don't have OWSM, others don't have separate Coherence etc.\n\nSOA & BAM are kept in the same domain as little benefit is obtained by separating them.\n\nKey Points\n\nSeparate JMS allows those servers to be kept up separately from rest of SOA Domain, allowing JMS clients to post messages even if other domains are unavailable.\n\nJMS servers are only used to host application specific JMS destinations, SOA/OSB JMS destinations remain in relevant SOA/OSB managed servers.\n\nSeparate Coherence servers allow OSB cache to be offloaded from OSB servers.\n\nUse of Coherence by other components as a shared infrastructure data grid service.\n\nCoherence cluster may be managed by WLS but more likely run as a standalone Coherence cluster.\n\nBenefits\n\nFollows EDG but in separate domains and with addition of application specific JMS servers and standalone Coherence servers for OSB caching and application specific caches.\n\nCoherence grid can be scaled independent of OSB/SOA.\n\nJMS queues provide for inter-application communication.\n\nPatch lifecycle of OSB/SOA/JMS are no longer lock stepped.\n\nJMS may be kept running independently of other domains allowing applications to insert messages fro later consumption by SOA/OSB.\n\nOSB may be kept running independent of other domains, allowing service virtualization to continue independent of other domains availability.\n\nAll domains use same OWSM policy store (MDS-WSM).\n\nDrawbacks\n\nMultiple domains to manage and configure.\n\nMultiple Admin servers (single view requires use of Grid Control)\n\nMultiple Admin servers/WSM clusters waste resources.\n\nAdditional homes needed to enjoy benefits of separate patching.\n\nCross domain trust needs setting up to simplify cross domain interactions.\n\nStartup time for SOA may be slow if large number of composites deployed.\n\nShared Service Environment\n\nThis model extends the previous multiple domain arrangement to provide a true shared service environment.\n\nThis extends the previous model by allowing multiple additional SOA domains and/or other dom"
    }
}
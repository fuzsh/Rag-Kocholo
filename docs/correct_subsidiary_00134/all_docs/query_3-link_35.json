{
    "id": "correct_subsidiary_00134_3",
    "rank": 35,
    "data": {
        "url": "https://www.datanucleus.org/products/accessplatform/jdo/persistence.html",
        "read_more_link": "",
        "language": "en",
        "title": "JDO Persistence Guide (v5.2)",
        "top_image": "https://www.datanucleus.org/images/datanucleus_favicon.png",
        "meta_img": "https://www.datanucleus.org/images/datanucleus_favicon.png",
        "images": [
            "https://www.datanucleus.org/products/accessplatform/images/datanucleus_icon.png",
            "https://www.datanucleus.org/products/accessplatform/images/javadoc.png",
            "https://www.datanucleus.org/products/accessplatform/images/nucleus_extension.png",
            "https://www.datanucleus.org/products/accessplatform/images/nucleus_extension.png",
            "https://www.datanucleus.org/products/accessplatform/images/nucleus_extension.png",
            "https://www.datanucleus.org/products/accessplatform/images/nucleus_extensionpoint.png",
            "https://www.datanucleus.org/products/accessplatform/images/nucleus_extensionpoint.png",
            "https://www.datanucleus.org/products/accessplatform/images/nucleus_extensionpoint.png",
            "https://www.datanucleus.org/products/accessplatform/images/nucleus_extension.png",
            "https://www.datanucleus.org/products/accessplatform/images/nucleus_extension.png",
            "https://www.datanucleus.org/products/accessplatform/images/nucleus_extensionpoint.png",
            "https://www.datanucleus.org/products/accessplatform/images/nucleus_extension.png",
            "https://www.datanucleus.org/products/accessplatform/images/nucleus_extension.png",
            "https://www.datanucleus.org/products/accessplatform/images/nucleus_extension.png",
            "https://www.datanucleus.org/products/accessplatform/images/nucleus_extension.png",
            "https://www.datanucleus.org/products/accessplatform/images/nucleus_extension.png",
            "https://www.datanucleus.org/products/accessplatform/images/nucleus_extension.png",
            "https://www.datanucleus.org/products/accessplatform/images/nucleus_extension.png",
            "https://www.datanucleus.org/products/accessplatform/images/nucleus_extension.png",
            "https://www.datanucleus.org/products/accessplatform/images/nucleus_extensionpoint.png",
            "https://www.datanucleus.org/products/accessplatform/images/javadoc.png",
            "https://www.datanucleus.org/products/accessplatform/images/nucleus_extension.png",
            "https://www.datanucleus.org/products/accessplatform/images/nucleus_extension.png",
            "https://www.datanucleus.org/products/accessplatform/images/nucleus_extension.png",
            "https://www.datanucleus.org/products/accessplatform/images/nucleus_extension.png",
            "https://www.datanucleus.org/products/accessplatform/images/javadoc.png",
            "https://www.datanucleus.org/products/accessplatform/images/javadoc.png",
            "https://www.datanucleus.org/products/accessplatform/images/javadoc.png",
            "https://www.datanucleus.org/products/accessplatform/images/javadoc.png",
            "https://www.datanucleus.org/products/accessplatform/images/javadoc.png",
            "https://www.datanucleus.org/products/accessplatform/images/javadoc.png",
            "https://www.datanucleus.org/products/accessplatform/images/nucleus_extension.png",
            "https://www.datanucleus.org/products/accessplatform/images/javadoc.png",
            "https://www.datanucleus.org/products/accessplatform/images/nucleus_extension.png",
            "https://www.datanucleus.org/products/accessplatform/images/nucleus_extension.png",
            "https://www.datanucleus.org/products/accessplatform/images/nucleus_extension.png",
            "https://www.datanucleus.org/products/accessplatform/images/nucleus_extension.png",
            "https://www.datanucleus.org/products/accessplatform/images/nucleus_extension.png",
            "https://www.datanucleus.org/products/accessplatform/images/nucleus_extension.png",
            "https://www.datanucleus.org/products/accessplatform/images/nucleus_extensionpoint.png",
            "https://www.datanucleus.org/products/accessplatform/images/javadoc.png",
            "https://www.datanucleus.org/products/accessplatform/images/jdo_object_lifecycle.png",
            "https://www.datanucleus.org/products/accessplatform/images/jdo_object_lifecycle_2.png",
            "https://www.datanucleus.org/products/accessplatform/images/javadoc.png",
            "https://www.datanucleus.org/products/accessplatform/images/nucleus_extension.png",
            "https://www.datanucleus.org/products/accessplatform/images/nucleus_extension.png",
            "https://www.datanucleus.org/products/accessplatform/images/javadoc.png",
            "https://www.datanucleus.org/products/accessplatform/images/javadoc.png",
            "https://www.datanucleus.org/products/accessplatform/images/nucleus_extension.png",
            "https://www.datanucleus.org/products/accessplatform/images/javadoc.png",
            "https://www.datanucleus.org/products/accessplatform/images/javadoc.png",
            "https://www.datanucleus.org/products/accessplatform/images/javadoc.png",
            "https://www.datanucleus.org/products/accessplatform/images/lifecyclelistener_pm.png",
            "https://www.datanucleus.org/products/accessplatform/images/lifecyclelistener_pmf.png",
            "https://www.datanucleus.org/products/accessplatform/images/nucleus_extension.png",
            "https://www.datanucleus.org/products/accessplatform/images/nucleus_extension.png",
            "https://www.datanucleus.org/products/accessplatform/images/nucleus_extension.png",
            "https://www.datanucleus.org/products/accessplatform/images/mx4j.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            "JDO",
            "JPA",
            "Persistence",
            "Java",
            "RDBMS",
            "MongoDB",
            "Cassandra"
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "DataNucleus JDO/JPA/REST Persistence of Java Objects",
        "meta_lang": "",
        "meta_favicon": "https://www.datanucleus.org/images/datanucleus_favicon.png",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Whichever way we wish to obtain the PersistenceManagerFactory we have defined a series of properties to give the behaviour of the PersistenceManagerFactory. The first property specifies to use the DataNucleus implementation, and the following 3 properties (javax.jdo.option.Connection???) define the datastore that it should connect to. There are many properties available. Some of these are standard JDO properties, and some are DataNucleus extensions.\n\nPersistenceManagerFactory for Persistence-Unit\n\nWhen designing an application you can usually nicely separate your persistable objects into independent groupings that can be treated separately, perhaps within a different DAO object, if using DAOs. JDO uses the (JPA) idea of a persistence-unit. A persistence-unit provides a convenient way of specifying a set of metadata files, and classes, and jars that contain all classes to be persisted in a grouping. The persistence-unit is named, and the name is used for identifying it. Consequently this name can then be used when defining what classes are to be enhanced, for example.\n\nTo define a persistence-unit you first need to add a file persistence.xml to the META-INF/ directory of the CLASSPATH (this may mean WEB-INF/classes/META-INF when using a web-application in such as Tomcat). This file will be used to define your persistence-unit(s). Lets show an example\n\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?> <persistence xmlns=\"http://xmlns.jcp.org/xml/ns/persistence\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://xmlns.jcp.org/xml/ns/persistence http://xmlns.jcp.org/xml/ns/persistence/persistence_2_1.xsd\" version=\"2.1\"> <persistence-unit name=\"OnlineStore\"> <class>mydomain.samples.metadata.store.Product</class> <class>mydomain.samples.metadata.store.Book</class> <class>mydomain.samples.metadata.store.CompactDisc</class> <class>mydomain.samples.metadata.store.Customer</class> <class>mydomain.samples.metadata.store.Supplier</class> <exclude-unlisted-classes/> <properties> <property name=\"datanucleus.ConnectionURL\" value=\"jdbc:h2:mem:datanucleus\"/> <property name=\"datanucleus.ConnectionUserName\" value=\"sa\"/> <property name=\"datanucleus.ConnectionPassword\" value=\"\"/> </properties> </persistence-unit> <persistence-unit name=\"Accounting\"> <mapping-file>/mydomain/samples/metadata/accounts/package.jdo</mapping-file> <properties> <property name=\"datanucleus.ConnectionURL\" value=\"jdbc:h2:mem:datanucleus\"/> <property name=\"datanucleus.ConnectionUserName\" value=\"sa\"/> <property name=\"datanucleus.ConnectionPassword\" value=\"\"/> </properties> </persistence-unit> </persistence>\n\nIn this example we have defined 2 persistence-unit(s). The first has the name \"OnlineStore\" and contains 5 classes (annotated). The second has the name \"Accounting\" and contains a metadata file called package.jdo in a particular package (which will define the classes being part of that unit). This means that once we have defined this we can reference these persistence-unit(s) in our persistence operations. You can find the XSD for persistence.xml here.\n\nThere are several sub-elements of this persistence.xml file\n\nprovider - Not used by JDO\n\njta-data-source - JNDI name for JTA connections (make sure you set transaction-type as JTA on the persistence-unit for this). You can alternatively specify JDO standard javax.jdo.option.ConnectionFactoryName to the same end.\n\nnon-jta-data-source - JNDI name for non-JTA connections. You can alternatively specify JDO standard javax.jdo.option.ConnectionFactory2Name to the same end.\n\nshared-cache-mode - Defines the way the L2 cache will operate. ALL means all entities cached. NONE means no entities will be cached. ENABLE_SELECTIVE means only cache the entities that are specified. DISABLE_SELECTIVE means cache all unless specified. UNSPECIFIED leaves it to DataNucleus.\n\nvalidation-mode - Defines the validation mode for Bean Validation. AUTO, CALLBACK or NONE.\n\njar-file - name of a JAR file to scan for annotated classes to include in this persistence-unit.\n\nmapping-file - name of an XML \"mapping\" file containing persistence information to be included in this persistence-unit. This is the JDO XML Metadata file (package.jdo) (not the ORM XML Metadata file)\n\nclass - name of an annotated class to include in this persistence-unit\n\nproperties - properties defining the persistence factory to be used.\n\nexclude-unlisted-classes - when this is specified then it will only load metadata for the classes/mapping files listed.\n\nUse with JDO\n\nJDO accepts the \"persistence-unit\" name to be specified when creating the PersistenceManagerFactory, like this\n\nPersistenceManagerFactory pmf = JDOHelper.getPersistenceManagerFactory(\"MyPersistenceUnit\");\n\nMetadata loading using persistence unit\n\nWhen you specify a PMF using a persistence.xml it will load the metadata for all classes that are specified directly in the persistence unit, as well as all classes defined in JDO XML metadata files that are specified directly in the persistence unit. If you don’t have the exclude-unlisted-classes set to true then it will also do a CLASSPATH scan to try to find any other annotated classes that are part of that persistence unit. To set the CLASSPATH scanner to a custom version use the persistence property datanucleus.metadata.scanner and set it to the classname of the scanner class.\n\nDynamically generated Persistence-Unit\n\nDataNucleus allows an extension to the JDO API to dynamically create persistence-units at runtime. Use the following code sample as a guide. Obviously any classes defined in the persistence-unit need to have been enhanced.\n\nimport org.datanucleus.metadata.PersistenceUnitMetaData; import org.datanucleus.api.jdo.JDOPersistenceManagerFactory; PersistenceUnitMetaData pumd = new PersistenceUnitMetaData(\"dynamic-unit\", \"RESOURCE_LOCAL\", null); pumd.addClassName(\"mydomain.test.A\"); pumd.setExcludeUnlistedClasses(); pumd.addProperty(\"javax.jdo.option.ConnectionURL\", \"jdbc:hsqldb:mem:nucleus\"); pumd.addProperty(\"javax.jdo.option.ConnectionUserName\", \"sa\"); pumd.addProperty(\"javax.jdo.option.ConnectionPassword\", \"\"); pumd.addProperty(\"datanucleus.schema.autoCreateAll\", \"true\"); PersistenceManagerFactory pmf = new JDOPersistenceManagerFactory(pumd, null);\n\nIt should be noted that if you call pumd.toString(); then this returns the text that would have been found in a persistence.xml file.\n\nLevel 2 Cache\n\nThe PersistenceManagerFactory has an optional cache of all objects across all _PersistenceManager_s. This cache is called the Level 2 (L2) cache, and JDO doesn’t define whether this should be enabled or not. With DataNucleus it defaults to enabled. The user can configure the L2 cache if they so wish; by use of the persistence property datanucleus.cache.level2.type. You set this to \"type\" of cache required. You currently have the following options.\n\nsoft - use the internal (soft reference based) L2 cache. This is the default L2 cache in DataNucleus. Provides support for the JDO interface of being able to put objects into the cache, and evict them when required. This option does not support distributed caching, solely running within the JVM of the client application. Soft references are held to non pinned objects.\n\nweak - use the internal (weak reference based) L2 cache. Provides support for the JDO interface of being able to put objects into the cache, and evict them when required. This option does not support distributed caching, solely running within the JVM of the client application. Weak references are held to non pinned objects.\n\njavax.cache - a simple wrapper to the Java standard \"javax.cache\" Temporary Caching API.\n\nEHCache - a simple wrapper to EHCache’s caching product.\n\nEHCacheClassBased - similar to the EHCache option but class-based.\n\nRedis - an L2 cache using Redis.\n\nOracle Coherence - a simple wrapper to Oracle’s Coherence caching product. Oracle’s caches support distributed caching, so you could, in principle, use DataNucleus in a distributed environment with this option.\n\nspymemcached - a simple wrapper to the \"spymemcached\" client for memcached caching product.\n\nxmemcached - a simple wrapper to the \"xmemcached\" client for memcached caching product.\n\ncacheonix - a simple wrapper to the Cacheonix distributed caching software.\n\nOSCache - a simple wrapper to OSCache’s caching product.\n\nnone - turn OFF L2 caching.\n\nThe weak, soft and javax.cache caches are available in the datanucleus-core plugin. The EHCache, OSCache, Coherence, Cacheonix, and Memcache caches are available in the datanucleus-cache plugin.\n\nIn addition you can control the mode of operation of the L2 cache. You do this using the persistence property datanucleus.cache.level2.mode. The default is UNSPECIFIED which means that DataNucleus will cache all objects of entities unless the entity is explicitly marked as not cacheable. The other options are NONE (don’t cache ever), ALL (cache all entities regardless of annotations), ENABLE_SELECTIVE (cache entities explicitly marked as cacheable), or DISABLE_SELECTIVE (cache entities unless explicitly marked as not cacheable - i.e same as our default).\n\nObjects are placed in the L2 cache when you commit() the transaction of a PersistenceManager. This means that you only have datastore-persisted objects in that cache. Also, if an object is deleted during a transaction then at commit it will be removed from the L2 cache if it is present.\n\nThe L2 cache is a DataNucleus allowing you to provide your own cache where you require it. Use the examples of the EHCache, Coherence caches etc as reference.\n\nControlling the Level 2 Cache\n\nThe majority of times when using a JDO-enabled system you will not have to take control over any aspect of the caching other than specification of whether to use a L2 Cache or not. With JDO and DataNucleus you have the ability to control which objects remain in the cache. This is available via a method on the PersistenceManagerFactory.\n\nPersistenceManagerFactory pmf = JDOHelper.getPersistenceManagerFactory(props); DataStoreCache cache = pmf.getDataStoreCache();\n\nThe DataStoreCache interface provides methods to control the retention of objects in the cache. You have 3 groups of methods\n\nevict - used to remove objects from the L2 Cache\n\npin - used to pin objects into the cache, meaning that they will not get removed by garbage collection, and will remain in the L2 cache until removed.\n\nunpin - used to reverse the effects of pinning an object in the L2 cache. This will mean that the object can thereafter be garbage collected if not being used.\n\nThese methods can be called to pin objects into the cache that will be much used. Clearly this will be very much application dependent, but it provides a mechanism for users to exploit the caching features of JDO. If an object is not \"pinned\" into the L2 cache then it can typically be garbage collected at any time, so you should utilise the pinning capability for objects that you wish to retain access to during your application lifetime. For example, if you have an object that you want to be found from the cache you can do\n\nPersistenceManagerFactory pmf = JDOHelper.getPersistenceManagerFactory(props); DataStoreCache cache = pmf.getDataStoreCache(); cache.pinAll(MyClass.class, false); PersistenceManager pm = pmf.getPersistenceManager(); Transaction tx = pm.currentTransaction(); try { tx.begin(); pm.makePersistent(myObject); tx.commit(); } finally { if (tx.isActive()) { tx.close(); } }\n\nThereafter, whenever something refers to myObject, it will find it in the L2 cache. To turn this behaviour off, the user can either unpin it or evict it.\n\nJDO allows control over which classes are put into a L2 cache. You do this by specifying the cacheable attribute to false (defaults to true). So with the following specification, no objects of type MyClass will be put in the L2 cache.\n\n@Cacheable(\"false\") public class MyClass { ... }\n\nor using XML metadata\n\n<class name=\"MyClass\" cacheable=\"false\"> ... </class>\n\nJDO allows you control over which fields of an object are put in the L2 cache. You do this by specifying the cacheable attribute to false (defaults to true). This setting is only required for fields that are relationships to other persistable objects. Like this\n\npublic class MyClass { ... Collection values; @Cacheable(\"false\") Collection elements; }\n\nor using XML metadata\n\n<class name=\"MyClass\"> <field name=\"values\"/> <field name=\"elements\" cacheable=\"false\"/> ... </class>\n\nSo in this example we will cache \"values\" but not \"elements\". If a field is cacheable then\n\nIf it is a persistable object, the \"identity\" of the related object will be stored in the L2 cache for this field of this object\n\nIf it is a Collection of persistable elements, the \"identity\" of the elements will be stored in the L2 cache for this field of this object\n\nIf it is a Map of persistable keys/values, the \"identity\" of the keys/values will be stored in the L2 cache for this field of this object\n\nWhen pulling an object in from the L2 cache and it has a reference to another object DataNucleus uses the \"identity\" to find that object in the L1 or L2 caches to re-relate the objects.\n\nL2 Cache using javax.cache\n\nDataNucleus provides a simple wrapper to any compliant javax.cache implementation, for example Apache Ignite or HazelCast. To enable this you should put a \"javax.cache\" implementation in your CLASSPATH, and set the persistence properties\n\ndatanucleus.cache.level2.type=javax.cache datanucleus.cache.level2.cacheName={cache name}\n\nAs an example, you could simply add the following to a Maven POM, together with those persistence properties above to use HazelCast \"javax.cache\" implementation\n\n<dependency> <groupId>javax.cache</groupId> <artifactId>cache-api</artifactId> <version>1.0.0</version> </dependency> <dependency> <groupId>com.hazelcast</groupId> <artifactId>hazelcast</artifactId> <version>3.7.3</version> </dependency>\n\nL2 Cache using EHCache\n\nDataNucleus provides a simple wrapper to EHCache’s own API caches (not the javax.cache API variant). To enable this you should set the persistence properties\n\ndatanucleus.cache.level2.type=ehcache datanucleus.cache.level2.cacheName={cache name} datanucleus.cache.level2.configurationFile={EHCache configuration file (in classpath)}\n\nThe EHCache plugin also provides an alternative L2 Cache that is class-based. To use this you would need to replace \"ehcache\" above with \"ehcacheclassbased\".\n\nL2 Cache using Spymemcached/Xmemcached\n\nDataNucleus provides a simple wrapper to Spymemcached caches and Xmemcached caches. To enable this you should set the persistence properties\n\ndatanucleus.cache.level2.type=spymemcached [or \"xmemcached\"] datanucleus.cache.level2.cacheName={prefix for keys, to avoid clashes with other memcached objects} datanucleus.cache.level2.expireMillis=... datanucleus.cache.level2.memcached.servers=...\n\ndatanucleus.cache.level2.memcached.servers is a space separated list of memcached hosts/ports, e.g. host:port host2:port. datanucleus.cache.level2.expireMillis if not set or set to 0 then no expire\n\nL2 Cache using Cacheonix\n\nDataNucleus provides a simple wrapper to Cacheonix. To enable this you should set the persistence properties\n\ndatanucleus.cache.level2.type=cacheonix datanucleus.cache.level2.cacheName={cache name}\n\nNote that you can optionally also specify\n\ndatanucleus.cache.level2.expiryMillis={expiry-in-millis} datanucleus.cache.level2.configurationFile={Cacheonix configuration file (in classpath)}\n\nand define a cacheonix-config.xml like\n\n<?xml version=\"1.0\"?> <cacheonix> <local> <localCache name=\"mydomain.MyClass\"> <store> <lru maxElements=\"1000\" maxBytes=\"1mb\"/> <expiration timeToLive=\"60s\"/> </store> </localCache> <localCache name=\"datanucleus\"> <store> <lru maxElements=\"1000\" maxBytes=\"10mb\"/> <expiration timeToLive=\"60s\"/> </store> </localCache> <localCache name=\"default\" template=\"true\"> <store> <lru maxElements=\"10\" maxBytes=\"10mb\"/> <overflowToDisk maxOverflowBytes=\"1mb\"/> <expiration timeToLive=\"1s\"/> </store> </localCache> </local> </cacheonix>\n\nL2 Cache using Redis\n\nDataNucleus provides a simple L2 cache using Redis. To enable this you should set the persistence properties\n\ndatanucleus.cache.level2.type=redis datanucleus.cache.level2.cacheName={cache name} datanucleus.cache.level2.clearAtClose={true | false, whether to clear at close} datanucleus.cache.level2.expiryMillis={expiry-in-millis} datanucleus.cache.level2.redis.database={database, or use the default '1'} datanucleus.cache.level2.redis.timeout={optional timeout, or use the default of 5000} datanucleus.cache.level2.redis.sentinels={comma-separated list of sentinels, optional (use server/port instead)} datanucleus.cache.level2.redis.server={server, or use the default of \"localhost\"} datanucleus.cache.level2.redis.port={port, or use the default of 6379}\n\nL2 Cache using OSCache\n\nDataNucleus provides a simple wrapper to OSCache’s caches. To enable this you should set the persistence properties\n\ndatanucleus.cache.level2.type=oscache datanucleus.cache.level2.cacheName={cache name}\n\nL2 Cache using Oracle Coherence\n\nDataNucleus provides a simple wrapper to Oracle’s Coherence caches. This currently takes the NamedCache interface in Coherence and instantiates a cache of a user provided name. To enabled this you should set the following persistence properties\n\ndatanucleus.cache.level2.type=coherence datanucleus.cache.level2.cacheName={coherence cache name}\n\nThe Coherence cache name is the name that you would normally put into a call to CacheFactory.getCache(name). You have the benefits of Coherence’s distributed/serialized caching. If you require more control over the Coherence cache whilst using it with DataNucleus, you can just access the cache directly via\n\nJDODataStoreCache cache = (JDODataStoreCache)pmf.getDataStoreCache(); NamedCache tangosolCache = ((TangosolLevel2Cache)cache.getLevel2Cache()).getTangosolCache();\n\nLevel 2 Cache implementation\n\nObjects in a Level 2 cache are keyed by their JDO \"identity\". Consequently only persistable objects with an identity will be L2 cached. In terms of what is cached, the persistable object is represented by a CachedPC object. This stores the class of the persistable object, the \"id\", \"version\" (if present), and the field values (together with which fields are present in the L2 cache). If a field is/contains a relation, the field value will be the \"id\" of the related object (rather than the object itself). If a field is/contains an embedded persistable object, the field value will be a nested CachedPC object representing that object.\n\nThe thing to remember when using DataNucleus is that the schema is under your control. DataNucleus does not impose anything on you as such, and you have the power to turn on/off all schema components. Some Java persistence tools add various types of information to the tables for persisted classes, such as special columns, or meta information. DataNucleus is very unobtrusive as far as the datastore schema is concerned. It minimises the addition of any implementation artifacts to the datastore, and adds nothing (other than any datastore identities, and version columns where requested) to any schema tables.\n\nSchema Generation : Validation\n\nDataNucleus can check any existing schema against what is implied by the MetaData.\n\nThe property datanucleus.schema.validateTables provides a way of telling DataNucleus to validate any tables that it needs against their current definition in the datastore. If the user already has a schema, and want to make sure that their tables match what DataNucleus requires (from the MetaData definition) they would set this property to true. This can be useful for example where you are trying to map to an existing schema and want to verify that you’ve got the correct MetaData definition.\n\nThe property datanucleus.schema.validateColumns provides a way of telling DataNucleus to validate any columns of the tables that it needs against their current definition in the datastore. If the user already has a schema, and want to make sure that their tables match what DataNucleus requires (from the MetaData definition) they would set this property to true. This will validate the precise column types and widths etc, including defaultability/nullability settings. Please be aware that many JDBC drivers contain bugs that return incorrect column detail information and so having this turned off is sometimes the only option (dependent on the JDBC driver quality).\n\nThe property datanucleus.schema.validateConstraints provides a way of telling DataNucleus to validate any constraints (primary keys, foreign keys, indexes) that it needs against their current definition in the datastore. If the user already has a schema, and want to make sure that their table constraints match what DataNucleus requires (from the MetaData definition) they would set this property to true.\n\nRead-Only\n\nIf your datastore is read-only (you can’t add/update/delete any data in it), obviously you could just configure your application to not perform these operations. An alternative is to set the PMF as \"read-only\". You do this by setting the persistence property javax.jdo.option.ReadOnly to true.\n\nFrom now on, whenever you perform a persistence operation that implies a change in datastore data, the operation will throw a JDOReadOnlyException.\n\nDataNucleus provides an additional control over the behaviour when an attempt is made to change a read-only datastore. The default behaviour is to throw an exception. You can change this using the persistence property datanucleus.readOnlyDatastoreAction with values of \"EXCEPTION\" (default), and \"IGNORE\". \"IGNORE\" has the effect of simply ignoring all attempted updates to readonly objects.\n\nYou can take this read-only control further and specify it just on specific classes. Like this\n\n@Extension(vendorName=\"datanucleus\", key=\"read-only\", value=\"true\") public class MyClass {...}\n\nSchema Adaption\n\nAs time goes by during the development of your DataNucleus JDO powered application you may need to add fields, update field mappings, or delete fields. In an ideal world the JDO provider would take care of this itself. However this is actually not part of the JPA standard and so you are reliant on what features the JDO provider possesses.\n\nDataNucleus can cope with added fields, if you have the relevant persistence properties enabled. In this case look at datanucleus.schema.autoCreateTables, datanucleus.schema.autoCreateColumns, datanucleus.schema.autoCreateConstraints, and datanucleus.rdbms.dynamicSchemaUpdates (with this latter property of use where you have interface field(s) and a new implementation of that interface is encountered at runtime).\n\nIf you update or delete a field with an RDBMS datastore then you will need to update your schema manually. With non-RDBMS datastores deletion of fields is supported in some situations.\n\nYou should also consider making use of tools like Flyway and Liquibase since these are designed for exactly this role.\n\nRDBMS : Datastore Schema SPI\n\nThe JDO API doesn’t provide a way of accessing the schema of the datastore itself (if it has one). In the case of RDBMS it is useful to be able to find out what columns there are in a table, or what data types are supported for example. DataNucleus Access Platform provides an API for this.\n\nThe first thing to do is get your hands on the DataNucleus StoreManager and from that the StoreSchemaHandler. You do this as follows\n\nimport org.datanucleus.api.jdo.JDOPersistenceManagerFactory; import org.datanucleus.store.StoreManager; import org.datanucleus.store.schema.StoreSchemaHandler; [assumed to have \"pmf\"] ... StoreManager storeMgr = ((JDOPersistenceManagerFactory)pmf).getStoreManager(); StoreSchemaHandler schemaHandler = storeMgr.getSchemaHandler();\n\nSo now we have the StoreSchemaHandler what can we do with it? Well start with the javadoc for the implementation that is used for RDBMS\n\nRDBMS : Datastore Types Information\n\nSo we now want to find out what JDBC/SQL types are supported for our RDBMS. This is simple.\n\nimport org.datanucleus.store.rdbms.schema.RDBMSTypesInfo; Connection conn = (Connection)pm.getDataStoreConnection().getNativeConnection(); RDBMSTypesInfo typesInfo = schemaHandler.getSchemaData(conn, \"types\");\n\nAs you can see from the javadocs for RDBMSTypesInfo we can access the JDBC types information via the \"children\". They are keyed by the JDBC type number of the JDBC type (see java.sql.Types). So we can just iterate it\n\nIterator jdbcTypesIter = typesInfo.getChildren().values().iterator(); while (jdbcTypesIter.hasNext()) { JDBCTypeInfo jdbcType = (JDBCTypeInfo)jdbcTypesIter.next(); Iterator sqlTypesIter = jdbcType.getChildren().values().iterator(); while (sqlTypesIter.hasNext()) { SQLTypeInfo sqlType = (SQLTypeInfo)sqlTypesIter.next(); ... inspect the SQL type info } }\n\nRDBMS : Column information for a table\n\nHere we have a table in the datastore and want to find the columns present. So we do this\n\nimport org.datanucleus.store.rdbms.schema.RDBMSTableInfo; Connection conn = (Connection)pm.getDataStoreConnection().getNativeConnection(); RDBMSTableInfo tableInfo = schemaHandler.getSchemaData(conn, \"columns\", new Object[] {catalogName, schemaName, tableName});\n\nAs you can see from the javadocs for RDBMSTableInfo we can access the columns information via the \"children\".\n\nIterator columnsIter = tableInfo.getChildren().iterator(); while (columnsIter.hasNext()) { RDBMSColumnInfo colInfo = (RDBMSColumnInfo)columnsIter.next(); ... }\n\nRDBMS : Index information for a table\n\nHere we have a table in the datastore and want to find the indices present. So we do this\n\nimport org.datanucleus.store.rdbms.schema.RDBMSTableInfo; Connection conn = (Connection)pm.getDataStoreConnection().getNativeConnection(); RDBMSTableIndexInfo tableInfo = schemaHandler.getSchemaData(conn, \"indices\", new Object[] {catalogName, schemaName, tableName});\n\nAs you can see from the javadocs for RDBMSTableIndexInfo we can access the index information via the \"children\".\n\nIterator indexIter = tableInfo.getChildren().iterator(); while (indexIter.hasNext()) { IndexInfo idxInfo = (IndexInfo)indexIter.next(); ... }\n\nRDBMS : ForeignKey information for a table\n\nHere we have a table in the datastore and want to find the FKs present. So we do this\n\nimport org.datanucleus.store.rdbms.schema.RDBMSTableInfo; Connection conn = (Connection)pm.getDataStoreConnection().getNativeConnection(); RDBMSTableFKInfo tableInfo = schemaHandler.getSchemaData(conn, \"foreign-keys\", new Object[] {catalogName, schemaName, tableName});\n\nAs you can see from the javadocs for RDBMSTableFKInfo we can access the foreign-key information via the \"children\".\n\nIterator fkIter = tableInfo.getChildren().iterator(); while (fkIter.hasNext()) { ForeignKeyInfo fkInfo = (ForeignKeyInfo)fkIter.next(); ... }\n\nRDBMS : PrimaryKey information for a table\n\nHere we have a table in the datastore and want to find the PK present. So we do this\n\nimport org.datanucleus.store.rdbms.schema.RDBMSTableInfo; Connection conn = (Connection)pm.getDataStoreConnection().getNativeConnection(); RDBMSTablePKInfo tableInfo = schemaHandler.getSchemaData(conn, \"primary-keys\", new Object[] {catalogName, schemaName, tableName});\n\nAs you can see from the javadocs for RDBMSTablePKInfo we can access the foreign-key information via the \"children\".\n\nIterator pkIter = tableInfo.getChildren().iterator(); while (pkIter.hasNext()) { PrimaryKeyInfo pkInfo = (PrimaryKeyInfo)pkIter.next(); ... }\n\nBy default with JDO implementations when you open a PersistenceManagerFactory and obtain a PersistenceManager DataNucleus knows nothing about which classes are to be persisted to that datastore (unless you created the PMF using a persistence-unit). JDO implementations only load the Meta-Data for any class when the class is first enlisted in a PersistenceManager operation. For example you call makePersistent on an object. The first time a particular class is encountered DataNucleus will dynamically load the Meta-Data for that class. This typically works well since in an application in a particular operation the PersistenceManagerFactory may well not encounter all classes that are persistable to that datastore. The reason for this dynamic loading is that JDO implementations can’t be expected to scan through the whole Java CLASSPATH for classes that could be persisted there. That would be inefficient.\n\nThere are situations however where it is desirable for DataNucleus to have knowledge about what is to be persisted, or what subclasses of a candidate are possible on executing a query, so that it can load the Meta-Data at initialisation of the persistence factory and hence when the classes are encountered for the first time nothing needs doing. There are several ways of achieving this\n\nAutoStartMechanism : SchemaTable (RDBMS only)\n\nWhen using an RDBMS datastore the SchemaTable auto-start mechanism stores the list of classes (and their tables, types and version of DataNucleus) in a datastore table NUCLEUS_TABLES. This table is read at startup of DataNucleus, and provides DataNucleus with the necessary knowledge it needs to continue persisting these classes. This table is continuously updated during a session of a DataNucleus-enabled application.\n\nIf the user changes their persistence definition a problem can occur when starting up DataNucleus. DataNucleus loads up its existing data from NUCLEUS_TABLES and finds that a table/class required by the NUCLEUS_TABLES data no longer exists. There are 3 options for what DataNucleus will do in this situation. The property datanucleus.autoStartMechanismMode defines the behaviour of DataNucleus for this situation.\n\nChecked will mean that DataNucleus will throw an exception and the user will be expected to manually fix their database mismatch (perhaps by removing the existing tables).\n\nQuiet (the default) will simply remove the entry from NUCLEUS_TABLES and continue without exception.\n\nIgnored will simply continue without doing anything.\n\nThe default database schema used the SchemaTable is described below:\n\nTABLE : NUCLEUS_TABLES ( COLUMN : CLASS_NAME VARCHAR(128) PRIMARY KEY, -- Fully qualified persistent Class name COLUMN : TABLE_NAME VARCHAR(128), -- Table name COLUMN : TYPE VARCHAR(4), -- FCO | SCO COLUMN : OWNER VARCHAR(2), -- 1 | 0 COLUMN : VERSION VARCHAR(20), -- DataNucleus version COLUMN : INTERFACE_NAME VARCHAR(255) -- Fully qualified persistent Class type -- of the persistent Interface implemented )\n\nIf you want to change the table name (from NUCLEUS_TABLES) you can set the persistence property datanucleus.rdbms.schemaTable.tableName\n\nNow that we have our PersistenceManagerFactory, providing the connection for our persistence context to our datastore, we need to obtain a PersistenceManager (PM) to manage the persistence of objects. Here we describe the majority of operations that you will are likely to need to know about.\n\nFinding an object by its identity\n\nOnce you have persisted an object, it has an \"identity\". This is a unique way of identifying it. You can obtain the identity by calling\n\nObject lincolnID = pm.getObjectId(lincoln);\n\nAlternatively you can create an identity to represent this object by calling\n\nObject lincolnID = pm.newObjectIdInstance(Person.class, 1);\n\nSo what ? Well the identity can be used to retrieve the object again at some other part in your application. So you pass the identity into your application, and the user clicks on some button on a web page and that button corresponds to a particular object identity. You can then go back to your data layer and retrieve the object as follows\n\nPerson lincoln = (Person)pm.getObjectById(lincolnID);\n\nA DataNucleus extension is to pass in a String form of the identity to the above method. It accepts identity strings of the form\n\n{fully-qualified-class-name}:{key}\n\n{discriminator-name}:{key}\n\nwhere the key is the identity toString() value (datastore-identity) or the result of PK.toString() (application-identity). So for example we could input\n\nObject obj = pm.getObjectById(\"mydomain.Person:1\");\n\nThere is, of course, a bulk load variant too\n\nObject[] objs = pm.getObjectsById(ids);\n\nWhen you call the method getObjectById if an object with that identity is found in the cache then a call is, by default, made to validate it still exists. You can avoid this call to the datastore by setting the persistence property datanucleus.findObject.validateWhenCached to false.\n\nFinding an object by its class and unique key field value(s)\n\nWhilst the primary way of looking up an object is via its identity, in some cases a class has a unique key (maybe comprised of multiple field values). This is sometimes referred to as a natural id. This is not part of the JDO API, however DataNucleus makes it available. Let’s take an example\n\n@PersistenceCapable @Unique(name=\"MY_NAME_IDX\", members={\"firstName\", \"lastName\"}) public class Person { @PrimaryKey long id; LocalDate dob; String firstName; String lastName; int age; ... }\n\nHere we have a Person class with an identity defined as a long, but also with a unique key defined as the composite of the firstName and lastName (in most societies it is possible to duplicate names amongst people, but we just take this as an example).\n\nNow to access a Person object based on the firstName and lastName we do the following\n\nJDOPersistenceManager jdopm = (JDOPersistenceManager)pm; Person p = jdopm.getObjectByUnique(Person.class, {\"firstName\", \"lastName\"}, {\"George\", \"Jones\"});\n\nand we retrieve the Person \"George Jones\".\n\nDetaching a persisted Object\n\nAs long as your persistable class is detachable (see the mapping guide) then you can detach objects of that type. Being detached means that your object is no longer managed by a particular PersistenceManager and hence usable in other tiers of your application. In this case you want to detach the object (and its related sub-objects) so that they can be passed across to the part of the application that requires it. To do this you do\n\nPerson detachedLincoln = pm.detachCopy(lincoln);\n\nThe detached object is like the original object except that it has no StateManager connected, and it stores its JDO identity and version. It retains a list of all fields that are modified while it is detached. This means that when you want to \"attach\" it to the data-access layer it knows what to update.\n\nSome things to be aware of with the detachment process.\n\nCalling detachCopy on an object that is not detachable will return a transient instance that is a COPY of the original, so use the COPY thereafter.\n\nCalling detachCopy on an object that is detachable will return a detached instance that is a COPY of the original, so use this COPY thereafter\n\nA detached object retains the id of its datastore entity. Detached objects should be used where you want to update the objects and attach them later (updating the associated object in the datastore. If you want to create copies of the objects in the datastore with their own identities you should use makeTransient instead of detachCopy.\n\nCalling detachCopy will detach all fields of that object that are in the current Fetch Groups for that class for that PersistenceManager.\n\nBy default the fields of the object that will be detached are those in the Default Fetch Group.\n\nYou should choose your Fetch Group carefully, bearing in mind which object(s) you want to access whilst detached. Detaching a relation field will detach the related object as well.\n\nIf you don’t detach a field of an object, you cannot access the value for that field while the object is detached.\n\nIf you don’t detach a field of an object, you can update the value for that field while detached, and thereafter you can access the value for that field.\n\nDetaching objects used by a transaction\n\nTo make the detachment process transparent you can set the persistence property datanucleus.DetachAllOnCommit to true and when you commit your transaction all objects enlisted in the transaction will be detached. If you just want to apply this setting for a PersistenceManager then there is a setDetachAllOnCommit method on the PersistenceManager.\n\nThis only has any effect when performing operations in a transaction.\n\nDetach objects on close of the PersistenceManager\n\nA further variation is known as \"detachOnClose\" and means that if enabled (setting persistence property datanucleus.DetachOnClose to true), when you close your PersistenceManager you are opting to have all instances currently cached in the Level 1 Cache of that PersistenceManager to be detached automatically.\n\nThis will not work in a JavaEE environment when using JCA.\n\nIt is recommended that you use \"DetachAllOnCommit\" instead of this wherever possible since that is standard JDO and would work in all JavaEE environments also.\n\nDetached Fields\n\nWhen an object is detached it is typically passed to a different layer of an application and potentially changed. During the course of the operation of the system it may be required to know what is loaded in the object and what is dirty (has been changed since detaching). DataNucleus provides an extension to allow interrogation of the detached object.\n\nString[] loadedFieldNames = NucleusJDOHelper.getLoadedFields(obj, pm); String[] dirtyFieldNames = NucleusJDOHelper.getDirtyFields(obj, pm);\n\nSo you have access to the names of the fields that were loaded when detaching the object, and also to the names of the fields that have been updated since detaching.\n\nSerialization of Detachable classes\n\nDuring enhancement of Detachable classes, a field called jdoDetachedState is added to the class definition. This field allows reading and changing tracking of detached objects while they are not managed by a PersistenceManager.\n\nWhen serialization occurs on a Detachable object, the jdoDetachedState field is written to the serialized object stream. On deserialize, this field is written back to the new deserialized instance. This process occurs transparently to the application. However, if deserialization occurs with an un-enhanced version of the class, the detached state is lost.\n\nSerialization and deserialization of Detachable classes and un-enhanced versions of the same class is only possible if the field serialVersionUID is added. It’s recommended during development of the class, to define the serialVersionUID and make the class implement the java.io.Serializable interface.\n\nCascading Operations\n\nWhen you have relationships between entities, and you persist one entity, by default the related entity will be persisted. This is referred to as persistence-by-reachability.\n\nLet’s use our example above, and create new Person and Account objects.\n\nPerson lincoln = new Person(1, \"Abraham\", \"Lincoln\"); Account acct1 = new Account(1, lincoln);\n\nnow to persist them both we have two options. Firstly with the default cascade setting\n\npm.makePersistent(acct1);\n\nThis will persist the Account object and since it refers to the Person object, that will be persisted also.\n\nDataNucleus allows you to disable cascading of persist/update operations by using the @Extension metadata. So if we change our class like this\n\n@PersistenceCapable public class Account { @PrimaryKey long id; @Extension(vendorName=\"datanucleus\", key=\"cascade-persist\", value=\"false\") @Extension(vendorName=\"datanucleus\", key=\"cascade-update\", value=\"false\") Person person; }\n\nnow when we do this\n\nem.persist(acct1);\n\nit will not persist the related Person object (but will likely throw an exception due to it being present).\n\nManaged Relationships\n\nAs previously mentioned, users should really set both sides of a bidirectional relation. DataNucleus provides a good level of managed relations in that it will attempt to correct any missing information in relations to make both sides consistent. What it provides is defined below\n\nFor a 1-1 bidirectional relation, at persist you should set one side of the relation and the other side will be set to make it consistent. If the respective sides are set to inconsistent objects then an exception will be thrown at persist. At update of owner/non-owner side the other side will also be updated to make them consistent.\n\nFor a 1-N bidirectional relation and you only specify the element owner then the collection must be Set-based since DataNucleus cannot generate indexing information for you in that situation (you must position the elements). At update of element or owner the other side will also be updated to make them consistent. At delete of element the owner collection will also be updated to make them consistent. If you are using a List you MUST set both sides of the relation\n\nFor an M-N bidirectional relation, at persist you MUST set one side and the other side will be populated at commit/flush to make them consistent.\n\nThis management of relations can be turned on/off using a persistence property datanucleus.manageRelationships. If you always set both sides of a relation at persist/update then you could safely turn it off.\n\nWhen performing management of relations there are some checks implemented to spot typical errors in user operations e.g add an element to a collection and then remove it (why?!). You can disable these checks using datanucleus.manageRelationshipsChecks, set to false.\n\nLevel 1 Cache\n\nEach PersistenceManager maintains a cache of the objects that it has encountered (or have been \"enlisted\") during its lifetime. This is termed the Level 1 (L1) Cache. It is enabled by default and you should only ever disable it if you really know what you are doing. There are inbuilt types for the L1 Cache available for selection. DataNucleus supports the following types of L1 Cache :-\n\nweak - uses a weak reference backing map. If JVM garbage collection clears the reference, then the object is removed from the cache.\n\nsoft - uses a soft reference backing map. If the map entry value object is not being actively used, then garbage collection may garbage collect the reference, in which case the object is removed from the cache.\n\nstrong - uses a normal HashMap backing. With this option all references are strong meaning that objects stay in the cache until they are explicitly removed by calling remove() on the cache.\n\nnone - will turn off L1 caching. Only ever use this where the cache is of no use and you are performing bulk operations and not requiring objects returned\n\nYou can specify the type of L1 cache by providing the persistence property datanucleus.cache.level1.type. You set this to the value of the type required. If you want to remove objects from the L1 cache programmatically you should use the pm.evict or pm.evictAll methods.\n\nObjects are placed in the L1 cache (and updated there) during the course of the transaction. This provides rapid access to the objects in use in the users application and is used to guarantee that there is only one object with a particular identity at any one time for that PersistenceManager. When the PersistenceManager is closed the L1 cache is cleared.\n\nThe L1 cache is a DataNucleus allowing you to provide your own cache where you require it.\n\nMultithreaded PersistenceManagers\n\nA PersistenceManagerFactory is designed to be thread-safe. A PersistenceManager is not. JDO provides a persistence property javax.jdo.option.Multithreaded that acts as a hint to the PMF to provide PersistenceManager(s) that are usable with multiple threads. While DataNucleus makes efforts to make this PersistenceManager usable with multiple threads, it is not guaranteed to work multi-threaded in all situations, particularly around second class collection/map fields.\n\nConsider the difficulties in operating a PM multithreaded. A PM has one transaction. If one thread starts it then all operations from all threads that come in will be on that transaction, until it is committed. Timing issues will abound.\n\nRegarding datastore connections, you have 1 connection in use during a transaction, and 1 available for use non-transactionally. If working non-transactionally this connection will be opened and closed repeatedly unless datanucleus.connection.nontx.releaseAfterUse is set to false. This will lead to timing issues around when the connection is released.\n\nIt is strongly recommended that any PM is operated single-threaded.\n\nPersistenceManagerProxy\n\nAs we have already described for normal persistence, you perform all operations using a PersistenceManager, needing to obtain this when you want to start datastore operations.\n\nIn some architectures (e.g in a web environment) it can be convenient to maintain a single PersistenceManager for use in a servlet init() method to initialise a static variable. Alternatively for use in a SessionBean to initialise a static variable. The JDO API provides a \"proxy\" object that can be used for this purpose. Thereafter you just refer to the proxy. The proxy isn’t the actual PersistenceManager just a proxy, delegating to the real object. If you call close() on the proxy the real PM will be closed, and when you next invoke an operation on the proxy it will create a new PM delegate and work with that.\n\nTo create a PM proxy is simple\n\nPersistenceManager pm = pmf.getPersistenceManagerProxy();\n\nSo we have our proxy, and now we can perform operations in the same way as we do with any PersistenceManager.\n\nDuring the persistence process, an object goes through lifecycle changes. Below we demonstrate the primary object lifecycle changes for JDO. JDO has a very high degree of flexibility and so can be configured to operate in different modes. The mode most consistent with JPA is shown below (this has the persistence property datanucleus.DetachAllOnCommit set to true).\n\nSo a newly created object is transient. You then persist it and it becomes persistent. You then commit the transaction and it is detached for use elsewhere in the application. You then attach any changes back to persistence and it becomes persistent again. Finally when you delete the object from persistence and commit that transaction it is in transient state.\n\nPersistence operations performed by the PersistenceManager are typically managed in a transaction, allowing operations to be grouped together. A Transaction forms a unit of work. The Transaction manages what happens within that unit of work, and when an error occurs the Transaction can roll back any changes performed. Transactions can be managed by the users application, or can be managed by a framework (such as Spring), or can be managed by a JavaEE container. These are described below.\n\nTransactions with lots of data\n\nOccasionally you may need to persist large amounts of data in a single transaction. Since all objects need to be present in Java memory at the same time, you can get OutOfMemory errors, or your application can slow down as swapping occurs. You can alleviate this by changing how you flush/commit the persistent changes.\n\nOne way is to do it like this, where possible,\n\nPersistenceManager pm = pmf.getPersistenceManager(); Transaction tx = pm.currentTransaction(); try { tx.begin(); for (int i=0; i<100000; i++) { Wardrobe wardrobe = new Wardrobe(); wardrobe.setModel(\"3 doors\"); pm.makePersistent(wardrobe); if (i % 10000 == 0) { pm.flush(); } } tx.commit(); } finally { if (tx.isActive()) { tx.rollback(); } pm.close(); }\n\nAnother way, if one object is causing the persist of a huge number of related objects, is to just persist some objects without relations first, flush, and then form the relations. This then allows the above process to be utilised, manually flushing at intervals.\n\nYou can additionally consider evicting objects from the Level 1 Cache, since they will, by default, be cached until commit.\n\nPessimistic (Datastore) Locking\n\nPessimistic locking is suitable for short lived operations where no user interaction is taking place and so it is possible to block access to datastore entities for the duration of the transaction. By default DataNucleus does not currently lock the objects fetched with pessimistic locking, but you can configure this behaviour for RDBMS datastores by setting the persistence property datanucleus.SerializeRead to true. This will result in all SELECT …​ FROM …​ statements being changed to be SELECT …​ FROM …​ FOR UPDATE. This will be applied only where the underlying RDBMS supports the \"FOR UPDATE\" syntax. This can be done on a transaction-by-transaction basis by doing\n\nTransaction tx = pm.currentTransaction(); tx.setSerializeRead(true);\n\nAlternatively, on a per query basis, you would do\n\nQuery q = pm.newQuery(...); q.setSerializeRead(true);\n\nWith pessimistic locking DataNucleus will grab a datastore connection at the first operation, and maintain it for the duration of the transaction. A single connection is used for the transaction (with the exception of any Value Generation operations which need datastore access, so these can use their own connection).\n\nIn terms of the process of pessimistic (datastore) locking, we demonstrate this below.\n\nOperation DataNucleus process Datastore process\n\nSo here whenever an operation is performed, DataNucleus pushes it straight to the datastore. Consequently any queries will always reflect the current state of all objects in use. However this mode of operation has no version checking of objects and so if they were updated by external processes in the meantime then they will overwrite those changes.\n\nIt should be noted that DataNucleus provides two persistence properties that allow an amount of control over when flushing happens with pessimistic locking\n\nPersistence property datanucleus.flush.mode when set to MANUAL will try to delay all datastore operations until commit/flush.\n\nPersistence property datanucleus.datastoreTransactionFlushLimit represents the number of dirty objects before a flush is performed. This defaults to 1.\n\nOptimistic Locking\n\nOptimistic locking is the other option in JDO. It is suitable for longer lived operations maybe where user interaction is taking place and where it would be undesirable to block access to datastore entities for the duration of the transaction. The assumption is that data altered in this transaction will not be updated by other transactions during the duration of this transaction, so the changes are not propagated to the datastore until commit()/flush(). The data is checked just before commit to ensure the integrity in this respect. The most convenient way of checking data for updates is to maintain a column on each table that handles optimistic locking data. The user will decide this when generating their MetaData.\n\nRather than placing version/timestamp columns on all user datastore tables, JDO allows the user to notate particular classes as requiring optimistic treatment. This is performed by specifying in MetaData or annotations the details of the field/column to use for storing the version - see versioning. With JDO the version is added in a surrogate column, whereas a vendor extension allows you to have a field in your class ready to store the version.\n\nWhen the version is stored in a surrogate column in the datastore, JDO provides a helper method for accessing this version. You can call\n\nJDOHelper.getVersion(object);\n\nand this returns the version as an Object (typically Long or Timestamp). It will return null for a transient object, and will return the version for a persistent object. If the object is not persistable then it will also return null.\n\nIn terms of the process of optimistic locking, we demonstrate this below.\n\nOperation DataNucleus process Datastore process\n\nHere no changes make it to the datastore until the user either commits the transaction, or they invoke flush(). The impact of this is that when performing a query, by default, the results may not contain the modified objects unless they are flushed to the datastore before invoking the query. Depending on whether you need the modified objects to be reflected in the results of the query governs what you do about that. If you invoke flush() just before running the query the query results will include the changes. The obvious benefit of optimistic locking is that all changes are made in a block and version checking of objects is performed before application of changes, hence this mode copes better with external processes updating the objects.\n\nPlease note that for some datastores (e.g RDBMS) the version check followed by update/delete is performed in a single statement. See also :-\n\nConnection Pooling\n\nWhen you create a PersistenceManagerFactory using a connection URL, driver name, and the username/password, this does not necessarily pool the connections (so they would be efficiently opened/closed when needed to utilise datastore resources in an optimum way). For some of the supported datastores DataNucleus allows you to utilise a connection pool to efficiently manage the connections to the datastore when specifying the datastore via the URL. We currently provide support for the following\n\nRDBMS : Apache DBCP v2, we allow use of externally-defined DBCP2, but also provide a builtin DBCP v2.x\n\nRDBMS : C3P0\n\nRDBMS : BoneCP\n\nRDBMS : HikariCP\n\nRDBMS : Tomcat\n\nRDBMS : Manually creating a DataSource for a 3rd party software package\n\nRDBMS : Custom Connection Pooling Plugins for RDBMS using the DataNucleus ConnectionPoolFactory interface\n\nRDBMS : Using JNDI, and lookup a connection DataSource.\n\nLDAP : Using JNDI\n\nYou need to specify the persistence property datanucleus.connectionPoolingType to be whichever of the external pooling libraries you wish to use (or \"None\" if you explicitly want no pooling). DataNucleus provides two sets of connections to the datastore - one for transactional usage, and one for non-transactional usage. If you want to define a different pooling for nontransactional usage then you can also specify the persistence property datanucleus.connectionPoolingType.nontx to whichever is required.\n\nRDBMS : JDBC driver properties with connection pool\n\nIf using RDBMS and you have a JDBC driver that supports custom properties, you can still use DataNucleus connection pooling and you need to s pecify the properties in with your normal persistence properties, but add the prefix datanucleus.connectionPool.driver. to the property name that the driver requires. For example, if an Oracle JDBC driver accepts defaultRowPrefetch, then you would specify something like\n\ndatanucleus.connectionPool.driver.defaultRowPrefetch=50\n\nand it will pass in defaultRowPrefetch as \"50\" into the driver used by the connection pool.\n\nRDBMS : C3P0\n\nDataNucleus allows you to utilise a connection pool using C3P0 to efficiently manage the connections to the datastore. C3P0 is a third-party library providing connection pooling. This is accessed by specifying the persistence property datanucleus.connectionPoolingType to C3P0.\n\nSo the PMF will use connection pooling using C3P0. To do this you will need the c3p0 JAR to be in the CLASSPATH.\n\nIf you want to configure C3P0 further you can include a c3p0.properties in your CLASSPATH - see the C3P0 documentation for details. You can also specify persistence properties to control the actual pooling. The currently supported properties for C3P0 are shown below\n\n# Pooling of Connections datanucleus.connectionPool.maxPoolSize=5 datanucleus.connectionPool.minPoolSize=3 datanucleus.connectionPool.initialPoolSize=3 # Pooling of PreparedStatements datanucleus.connectionPool.maxStatements=20\n\nRDBMS : BoneCP\n\nDataNucleus allows you to utilise a connection pool using BoneCP to efficiently manage the connections to the datastore. BoneCP is a third-party library providing connection pooling. This is accessed by specifying the persistence property datanucleus.connectionPoolingType to BoneCP.\n\nSo the PMF will use connection pooling using BoneCP. To do this you will need the bonecp JAR (and slf4j, google-collections) to be in the CLASSPATH.\n\nYou can also specify persistence properties to control the actual pooling. The currently supported properties for BoneCP are shown below\n\n# Pooling of Connections datanucleus.connectionPool.maxPoolSize=5 datanucleus.connectionPool.minPoolSize=3 # Pooling of PreparedStatements datanucleus.connectionPool.maxStatements=20\n\nRDBMS : HikariCP\n\nDataNucleus allows you to utilise a connection pool using HikariCP to efficiently manage the connections to the datastore. HikariCP is a third-party library providing connection pooling. This is accessed by specifying the persistence property datanucleus.connectionPoolingType to HikariCP.\n\nSo the PMF will use connection pooling using HikariCP. To do this you will need the hikaricp JAR (and slf4j, javassist as required) to be in the CLASSPATH.\n\nYou can also specify persistence properties to control the actual pooling. The currently supported properties for HikariCP are shown below\n\n# Pooling of Connections datanucleus.connectionPool.maxPoolSize=5 datanucleus.connectionPool.idleTimeout=180 datanucleus.connectionPool.leakThreshold=1 datanucleus.connectionPool.maxLifetime=240\n\nRDBMS : Tomcat\n\nDataNucleus allows you to utilise a connection pool using Tomcat JDBC Pool to efficiently manage the connections to the datastore. This is accessed by specifying the persistence property datanucleus.connectionPoolingType to tomcat.\n\nSo the PMF will use a DataSource with connection pooling using Tomcat. To do this you will need the tomcat-jdbc JAR to be in the CLASSPATH.\n\nYou can also specify persistence properties to control the actual pooling, like with the other pools.\n\nRDBMS : Manually create a DataSource ConnectionFactory\n\nWe could have used the built-in DBCP2 support which internally creates a DataSource ConnectionFactory, alternatively the support for external DBCP, C3P0, HikariCP, BoneCP etc, however we can also do this manually if we so wish. Let’s demonstrate how to do this with one of the most used pools Apache Commons DBCP\n\nWith DBCP you need to generate a javax.sql.DataSource, which you will then pass to DataNucleus. You do this as follows\n\nClass.forName(dbDriver); ObjectPool connectionPool = new GenericObjectPool(null); ConnectionFactory connectionFactory = new DriverManagerConnectionFactory(dbURL, dbUser, dbPassword); KeyedObjectPoolFactory kpf = new StackKeyedObjectPoolFactory(null, 20); PoolableConnectionFactory pcf = new PoolableConnectionFactory(connectionFactory, connectionPool, kpf, null, false, true); DataSource ds = new PoolingDataSource(connectionPool); Map properties = new HashMap(); properties.put(\"datanucleus.ConnectionFactory\", ds); PersistenceManagerFactory pmf = JDOHelper.createPersistenceManagerFactory(\"myPersistenceUnit\", properties);\n\nNote that we haven’t passed the dbUser and dbPassword to the PMF since we no longer need to specify them - they are defined for the pool so we let it do the work. As you also see, we set the data source for the PMF. Thereafter we can sit back and enjoy the performance benefits. Please refer to the documentation for DBCP for details of its configurability (you will need commons-dbcp, commons-pool, and commons-collections in your CLASSPATH to use this above example).\n\nRDBMS : Lookup a DataSource using JNDI\n\nDataNucleus allows you to use connection pools (java.sql.DataSource) bound to a javax.naming.InitialContext with a JNDI name. You first need to create the DataSource in the container (application server/web server), and secondly you specify the jta-data-source in the persistence-unit with the DataSource JNDI name. Please read more about this in RDBMS DataSources.\n\nLDAP : JNDI\n\nIf using an LDAP datastore you can use the following persistence properties to enable connection pooling\n\ndatanucleus.connectionPoolingType=JNDI\n\nOnce you have turned connection pooling on if you want more control over the pooling you can also set the following persistence properties\n\ndatanucleus.connectionPool.maxPoolSize : max size of pool\n\ndatanucleus.connectionPool.initialPoolSize : initial size of pool\n\nMultitenancy via Discriminator in Table\n\nApplicable to RDBMS, HBase, MongoDB, Neo4j, Cassandra\n\nIf you specify the persistence property datanucleus.tenantId as an identifier for your user-group/application then DataNucleus will know that it needs to provide a tenancy discriminator to all primary tables of persisted classes. This discriminator is then used to separate the data of the different user-groups.\n\nBy default this will add a column TENANT_ID to each primary table, of String-based type. You can control this by specifying extension metadata for each persistable class\n\n<class name=\"MyClass\"> <extension vendor-name=\"datanucleus\" key=\"multitenancy-column-name\" value=\"TENANT\"/> <extension vendor-name=\"datanucleus\" key=\"multitenancy-column-length\" value=\"24\"/> ... </class>\n\nor using annotations\n\n@PersistenceCapable @MultiTenant(column=\"TENANT\", columnLength=24) public class MyClass { ... }\n\nIn all subsequent use of DataNucleus, any \"insert\" to the primary \"table\"(s) will also include the TENANT column value. Additionally any query will apply a WHERE clause restricting to a particular value of TENANT column.\n\nIf you have enabled multi-tenancy as above but want to disable multitenancy on a class, just specify the following metadata on the class in question\n\n<class name=\"MyClass\"> <extension vendor-name=\"datanucleus\" key=\"multitenancy-disable\" value=\"true\"/> ... </class>\n\nor using annotations\n\n@PersistenceCapable @MultiTenant(disable=true) public class MyClass { ... }\n\nNote that the Tenant ID can be set in one of three ways.\n\nPer PersistenceManagerFactory : just set the persistence property datanucleus.tenantId when you start up the PMF, and all access for this PMF will use this Tenant ID\n\nPer PersistenceManager : set the persistence property datanucleus.tenantId when you start up the PMF as the default Tenant ID, and set a property on any PM that you want a different Tenant ID specifying for. Like this\n\nPersistenceManager pm = pmf.getPersistenceManager(); ... // All operations will apply to default tenant specified in persistence property for PMF pm.close(); PersistenceManager pm1 = pmf.getPersistenceManager(); pm1.setProperty(\"datanucleus.tenantId\", \"John\"); ... // All operations will apply to tenant \"John\" pm1.close(); PersistenceManager pm2 = pmf.getPersistenceManager(); pm2.setProperty(\"datanucleus.tenantId\", \"Chris\"); ... // All operations will apply to tenant \"Chris\" pm2.close();\n\nPer datastore access : When creating the PMF set the persistence property datanucleus.tenantProvider and set it to an instance of org.datanucleus.store.schema.MultiTenancyProvider\n\npublic interface MultiTenancyProvider { String getTenantId(ExecutionContext ec); }\n\nNow the programmer can set a different Tenant ID for each datastore access, maybe based on some session variable for example?\n\nSo we are validating that instances of the Person class will have an \"id\" that is not null and that the \"name\" field is not null and between 3 and 80 characters. If it doesn’t validate then at persist/update an exception will be thrown. You can add bean validation annotations to classes marked as @PersistenceCapable.\n\nA further use of the Bean Validation annotations @Size(max=…​) and @NotNull is that if you specify these then you have no need to specify the equivalent JDO \"length\" and \"allowsNull\" since they equate to the same thing. This is enabled via the persistence property datanucleus.metadata.javaxValidationShortcuts.\n\nWhen an object is retrieved from the datastore by JDO typically not all fields are retrieved immediately. This is because for efficiency purposes only particular field types are retrieved in the initial access of the object, and then any other objects are retrieved when accessed (lazy loading). The group of fields that are loaded is called a fetch group. There are 3 types of \"fetch groups\" to consider\n\nDefault Fetch Group\n\nJDO provides an initial fetch group, comprising the fields that will be retrieved when an object is retrieved if the user does nothing to define the required behaviour. By default the default fetch group comprises all fields of the following types (as per JDO spec) :-\n\nprimitives : boolean, byte, char, double, float, int, long, short\n\nObject wrappers of primitives : Boolean, Byte, Character, Double, Float, Integer, Long, Short\n\njava.lang.String, java.lang.Number, java.lang.Enum\n\njava.math.BigDecimal, java.math.BigInteger\n\njava.util.Date\n\nDataNucleus adds in many other types to the default fetch group as per the mapping guide.\n\nRelation fields are not present, by default, in the default fetch group.\n\nIf you wish to change the Default Fetch Group for a class you can update the Meta-Data for the class as follows\n\n@Persistent(defaultFetchGroup=\"true\") SomeType fieldX;\n\nor using XML metadata\n\n<class name=\"MyClass\"> ... <field name=\"fieldX\" default-fetch-group=\"true\"/> </class>\n\nWhen a PersistenceManager is created it starts with a FetchPlan of the \"default\" fetch group. That is, if we call\n\nCollection fetchGroups = fp.getGroups();\n\nthis will have one group, called \"default\". At runtime, if you have been using other fetch groups and want to revert back to the default fetch group at any time you simply do\n\nfp.setGroup(FetchPlan.DEFAULT);\n\nNamed Fetch Groups\n\nAs mentioned above, JDO allows specification of users own fetch groups. These are specified in the MetaData of the class. For example, if we have the following class\n\nclass MyClass { String name; HashSet coll; MyOtherClass other; }\n\nand we want to have the other field loaded whenever we load objects of this class, we define our MetaData as\n\n@PersistenceCapable @FetchGroup(name=\"otherfield\", members={@Persistent(name=\"other\")}) public class MyClass { ... }\n\nor using XML metadata\n\n<package name=\"mydomain\"> <class name=\"MyClass\"> <field name=\"name\"> <column length=\"100\" jdbc-type=\"VARCHAR\"/> </field> <field name=\"coll\" persistence-modifier=\"persistent\"> <collection element-type=\"mydomain.Address\"/> <join/> </field> <field name=\"other\" persistence-modifier=\"persistent\"/> <fetch-group name=\"otherfield\"> <field name=\"other\"/> </fetch-group> </class> </package>\n\nSo we have defined a fetch group called \"otherfield\" that just includes the field with name other. We can then use this at runtime in our persistence code.\n\nPersistenceManager pm = pmf.getPersistenceManager(); pm.getFetchPlan().addGroup(\"otherfield\"); ... (load MyClass object)\n\nBy default the FetchPlan will include the default fetch group. We have changed this above by adding the fetch group \"otherfield\", so when we retrieve an object using this PersistenceManager we will be retrieving the fields name AND other since they are both in the current FetchPlan. We can take the above much further than what is shown by defining nested fetch groups in the MetaData. In addition we can change the FetchPlan just before any PersistenceManager operation to control what is fetched during that operation. The user has full flexibility to add many groups to the current Fetch Plan. This gives much power and control over what will be loaded and when. A big improvement over the \"default\" fetch group.\n\nThe FetchPlan applies not just to calls to PersistenceManager.getObjectById(), but also to PersistenceManager.newQuery(), PersistenceManager.getExtent(), PersistenceManager.detachCopy and much more besides.\n\nDynamic Fetch Groups\n\nThe mechanism above provides static fetch groups defined in XML or annotations. That is great when you know in advance what fields you want to fetch. In some situations you may want to define your fields to fetch at run time.\n\nYou can define a FetchGroup on the PMF, or on the PM. For example, on the PMF as follows\n\nimport org.datanucleus.FetchGroup; FetchGroup grp = myPMF.getFetchGroup(MyClass.class, \"TestGroup\"); grp.addMember(\"field1\").addMember(\"field2\"); myPMF.addFetchGroups(grp); ... fp.addGroup(\"TestGroup\");\n\nSo we use the DataNucleus PMF as a way of creating a FetchGroup, and then register that FetchGroup with the PMF for use by all PMs. We then enable our FetchGroup for use in the FetchPlan by using its group name (as we do for a static group).\n\nAlternatively, on the PM\n\nimport org.datanucleus.FetchGroup; FetchGroup grp = myPM.getFetchGroup(MyClass.class, \"TestGroup\"); grp.addMember(\"field1\").addMember(\"field2\"); ... fp.addGroup(\"TestGroup\");\n\nThe FetchGroup allows you to add/remove the fields necessary so you have full API control over the fields to be fetched.\n\nFetch Depth\n\nThe basic fetch group defines which fields are to be fetched. It doesn’t explicitly define how far down an object graph is to be fetched. JDO provides two ways of controlling this.\n\nThe first is to set the maxFetchDepth for the FetchPlan. This value specifies how far out from the root object the related objects will be fetched. A positive value means that this number of relationships will be traversed from the root object. A value of -1 means that no limit will be placed on the fetching traversal. The default is 1. Let’s take an example\n\npublic class MyClass1 { MyClass2 field1; ... } public class MyClass2 { MyClass3 field2; ... } public class MyClass3 { MyClass4 field3; ... }\n\nand we want to detach field1 of instances of MyClass1, down 2 levels - so detaching the initial \"field1\" MyClass2 object, and its \"field2\" MyClass3 instance. So we define our fetch-groups like this\n\n<class name=\"MyClass1\"> ... <fetch-group name=\"includingField1\"> <field name=\"field1\"/> </fetch-group> </class> <class name=\"MyClass2\"> ... <fetch-group name=\"includingField2\"> <field name=\"field2\"/> </fetch-group> </class>\n\nand we then define the maxFetchDepth as 2, like this\n\npm.getFetchPlan().setMaxFetchDepth(2);\n\nA further refinement to this global fetch depth setting is to control the fetching of recursive fields. This is performed via a MetaData setting \"recursion-depth\". A value of 1 means that only 1 level of objects will be fetched. A value of -1 means there is no limit on the amount of recursion. The default is 1. Let’s take an example\n\npublic class Directory { Collection children; ... }\n\n<class name=\"Directory\"> <field name=\"children\"> <collection element-type=\"Directory\"/> </field> <fetch-group name=\"grandchildren\"> <field name=\"children\" recursion-depth=\"2\"/> </fetch-group> ... </class>\n\nSo when we fetch a Directory, it will fetch 2 levels of the children field, hence fetching the children and the grandchildren.\n\nJDO defines a mechanism whereby a persistable class can be marked as a listener for lifecycle events. Alternatively a separate listener class can be defined to receive these events. Thereafter when entities of the particular class go through lifecycle changes events are passed to the provided methods. Let’s look at the two different mechanisms\n\nInstance Callbacks\n\nJDO defines an interface for persistable classes so that they can be notified of events in their own lifecycle and perform any additional operations that are needed at these checkpoints. This is a complement to the Lifecycle Listeners interface which provides listeners for all objects of particular classes, with the events sent to a listener. With InstanceCallbacks the persistable class is the destination of the lifecycle events. As a result the Instance Callbacks method is more intrusive than the method of Lifecycle Listeners in that it requires methods adding to each class that wishes to receive the callbacks.\n\nThe InstanceCallbacks interface is documented here.\n\nTo give an example of this capability, let us define a class that needs to perform some operation just before it’s object is deleted.\n\npublic class MyClass implements InstanceCallbacks { String name; ... (class methods) public void jdoPostLoad() {} public void jdoPreClear() {} public void jdoPreStore() {} public void jdoPreDelete() { } }\n\nWe have implemented InstanceCallbacks and have defined the 4 required methods. Only one of these is of importance in this example.\n\nThese methods will be called just before storage in the data store (jdoPreStore), just before clearing (jdoPreClear), just after being loaded from the datastore (jdoPostLoad) and just before being deleted (jdoPreDelete).\n\nJDO also has 2 additional callbacks to complement InstanceCallbacks. These are AttachCallback and DetachCallback . If you want to intercept attach/detach events your class can implement these interfaces. You will then need to implement the following methods\n\npublic interface AttachCallback { public void jdoPreAttach(); public void jdoPostAttach(Object attached); } public interface DetachCallback { public void jdoPreDetach(); public void jdoPostDetach(Object detached); }\n\nLifecycle Listeners\n\nJDO defines an interface for the PersistenceManager and PersistenceManagerFactory whereby a user can register a listener for persistence events. The user provides a listener for either all classes, or a set of defined classes, and the JDO implementation calls methods on the listener when the required events occur. This provides the user application with the power to monitor the persistence process and, where necessary, append related behaviour. Specifying the listeners on the PersistenceManagerFactory has the benefits that these listeners will be added to all PersistenceManagers created by that factory, and so is for convenience really. This facility is a complement to the Instance Callbacks facility which allows interception of events on an instance by instance basis. The Lifecycle Listener process is much less intrusive than the process provided by Instance Callbacks, allowing a class external to the persistence process to perform the listening.\n\nThe InstanceLifecycleListener interface is documented here.\n\nTo give an example of this capability, let us define a Listener for our persistence process.\n\npublic class LoggingLifecycleListener implements CreateLifecycleListener, DeleteLifecycleListener, LoadLifecycleListener, StoreLifecycleListener { public void postCreate(InstanceLifecycleEvent event) { log.info(\"Lifecycle : create for \" + ((Persistable)event.getSource()).dnGetObjectId()); } public void preDelete(InstanceLifecycleEvent event) { log.info(\"Lifecycle : preDelete for \" + ((Persistable)event.getSource()).dnGetObjectId()); } public void postDelete(InstanceLifecycleEvent event) { log.info(\"Lifecycle : postDelete for \" + ((Persistable)event.getSource()).dnGetObjectId()); } public void postLoad(InstanceLifecycleEvent event) { log.info(\"Lifecycle : load for \" + ((Persistable)event.getSource()).dnGetObjectId()); } public void preStore(InstanceLifecycleEvent event) { log.info(\"Lifecycle : preStore for \" + ((Persistable)event.getSource()).dnGetObjectId()); } public void postStore(InstanceLifecycleEvent event) { log.info(\"Lifecycle : postStore for \" + ((Persistable)event.getSource()).dnGetObjectId()); } }\n\nHere we’ve provided a listener to receive events for CREATE, DELETE, LOAD, and STORE of objects. These are the main event types and in our simple case above we will simply log the event. All that remains is for us to register this listener with the PersistenceManager, or PersistenceManagerFactory\n\npm.addInstanceLifecycleListener(new LoggingLifecycleListener(), null);\n\nWhen using this interface the user should always remember that the listener is called within the same transaction as the operation being reported and so any changes they then make to the objects in question will be reflected in that objects state.\n\nRegister the listener with the PersistenceManager or PersistenceManagerFactory provide different effects. Registering with the PersistenceManagerFactory means that all PersistenceManagers created by it will have the listeners registered on the PersistenceManagerFactory called. Registering the listener with the PersistenceManager will only have the listener called only on events raised only by the PersistenceManager instance.\n\nThe above diagram displays the sequence of actions for a listener registered only in the PersistenceManager. Note that a second PersistenceManager will not make calls to the listener registered in the first PersistenceManager.\n\nThe above diagram displays the sequence of actions for a listener registered in the PersistenceManagerFactory. All events raised in a PersistenceManager obtained from the PersistenceManagerFactory will make calls to the listener registered in the PersistenceManagerFactory.\n\nDataNucleus supports the following instance lifecycle listener types\n\nAttachLifecycleListener - all attach events\n\nClearLifecycleListener - all clear events\n\nCreateLifecycelListener - all object create events\n\nDeleteLifecycleListener - all object delete events\n\nDetachLifecycleListener - all detach events\n\nDirtyLifecycleListener - all dirty events\n\nLoadLifecycleListener - all load events\n\nStoreLifecycleListener - all store events\n\nThe default JDO lifecycle listener StoreLifecycleListener only informs the listener of the object being stored. It doesn’t provide information about the fields being stored in that event. DataNucleus extends the JDO specification and on the \"preStore\" event it will return an instance of org.datanucleus.api.jdo.FieldInstanceLifecycleEvent (which extends the JDO InstanceLifecycleEvent) and provides access to the names of the fields being stored.\n\npublic class FieldInstanceLifecycleEvent extends InstanceLifecycleEvent { ... public String[] getFieldNames() ... }\n\nIf the store event is the persistence of the object then this will return all field names. If instead just particular fields are being stored then you just receive those fields in the event. So the only thing to do to utilise this DataNucleus extension is cast the received event to org.datanucleus.FieldInstanceLifecycleEvent\n\nThe JavaEE framework is widely used, providing a container within which java processes operate and it provides mechanisms for, amongst other things, transactions (JTA), and for connecting to other (3rd party) utilities (using Java Connector Architecture, JCA). DataNucleus Access Platform can be utilised within a JavaEE environment either in the same way as you use it for JavaSE, or via this JCA system, and we provide a Resource Adaptor (RAR file) containing this JCA adaptor allowing Access Platform to be used with the likes of WebLogic and JBoss. Instructions are provided for the following JavaEE servers\n\nDataNucleus Resource Adaptor and transactions\n\nA great advantage of DataNucleus implementing the ManagedConnection interface is that the JavaEE container manages transactions for you (no need to call the begin/commit/rollback-methods).\n\nCurrently local transactions and distributed (XA) transactions are supported.\n\nWithin a JavaEE environment, JDO transactions are nested in JavaEE transactions. All you have to do is to declare that a method needs transaction management. This is done in the EJB meta data. Here you will see, how a SessionBean implementation could look like. The EJB meta data is defined in a file called ejb-jar.xml and can be found in the META-INF directory of the jar you deploy. Suppose you deploy a bean called DataNucleusBean, your ejb-jar.xml should contain the following configuration elements:\n\n<session> <ejb-name>DataNucleusBean</ejb-name> ... <transaction-type>Container</transaction-type> ... <session>\n\nImagine your bean defines a method called testDataNucleusTrans():\n\n<container-transaction> <method > <ejb-name>DataNucleusBean</ejb-name> ... <method-name>testDataNucleusTrans</method-name> </method> <trans-attribute>Required</trans-attribute> </container-transaction>\n\nYou hereby define that transaction management is required for this method. The container will automatically begin a transaction for this method. It will be committed if no error occurs or rolled back otherwise. A potential SessionBean implementation containing methods to retrieve a PersistenceManager then could look like this:\n\npublic abstract class DataNucleusBean implements SessionBean { public void ejbCreate() throws CreateException { } public void ejbRemove() throws EJBException, RemoteException { } private static final String PMF_JNDI_NAME = \"java:/datanucleus1\"; private InitialContext getInitialContext() throws NamingException { InitialContext initialContext = new InitialContext(); return initialContext; } private PersistenceManagerFactory getPersitenceManagerFactory(InitialContext context) throws NamingException { return (PersistenceManagerFactory) context.lookup(PMF_JNDI_NAME); } public PersistenceManager getPersistenceManager() throws NamingException { return getPersitenceManagerFactory(getInitialContext()).getPersistenceManager(); } public void testDataNucleusTrans() throws Exception { PersistenceManager pm = getPersistenceManager() try { } finally { pm.close(); } } }\n\nMake sure that you close the PersistenceManager in your bean methods. If you don’t, the JavaEE server will usually close it for you (one of the advantages), but of course not without a warning or error message.\n\nThese instructions were adapted from a contribution by a DataNucleus user Alexander Bieber\n\nGeneral configuration\n\nA resource adapter has one central configuration file /META-INF/ra.xml which is located within the rar file and which defines the default values for all instances of the resource adapter (i.e. all instances of PersistenceManagerFactory). Additionally, it uses one or more deployment descriptor files (in JBoss, for example, they are named *-ds.xml) to set up the instances. In these files you can override the default values from the ra.xml.\n\nSince it is bad practice (and inconvenient) to edit a library’s archive (in this case the datanucleus-jdo-jca-5.2.rar) for changing the configuration (it makes updates more complicated, for example), it is recommended, not to edit the ra.xml within DataNucleus' rar file, but instead put all your configuration into your deployment descriptors. This way, you have a clean separation of which files you maintain (your deployment descriptors) and which files are maintained by others (the libraries you use and which you simply replace in case of an update).\n\nNevertheless, you might prefer to declare default values in the ra.xml in certain circumstances, so here’s an example:\n\n<?xml version=\"1.0\" encoding=\"UTF-8\"?> <!DOCTYPE connector PUBLIC \"-//Sun Microsystems, Inc.//DTD Connector 1.0//EN\" \"http://java.sun.com/dtd/connector_1_0.dtd\"> <connector> <display-name>DataNucleus Connector</display-name> <description></description> <vendor-name>DataNucleus Team</vendor-name> <spec-version>1.0</spec-version> <eis-type>JDO Adaptor</eis-type> <version>1.0</version> <resourceadapter> <managedconnectionfactory-class>org.datanucleus.jdo.connector.ManagedConnectionFactoryImpl</managedconnectionfactory-class> <connectionfactory-interface>javax.resource.cci.ConnectionFactory</connectionfactory-interface> <connectionfactory-impl-class>org.datanucleus.jdo.connector.PersistenceManagerFactoryImpl</connectionfactory-impl-class> <connection-interface>javax.resource.cci.Connection</connection-interface> <connection-impl-class>org.datanucleus.jdo.connector.PersistenceManagerImpl</connection-impl-class> <transaction-support>LocalTransaction</transaction-support> <config-property> <config-property-name>ConnectionFactoryName</config-property-name> <config-property-type>java.lang.String</config-property-type> <config-property-value>jdbc/ds</config-property-value> </config-property> <authentication-mechanism> <authentication-mechanism-type>BasicPassword</authentication-mechanism-type> <credential-interface>javax.resource.security.PasswordCredential</credential-interface> </authentication-mechanism> <reauthentication-support>false</reauthentication-support> </resourceadapter> </connector>\n\nTo define persistence properties you should make use of persistence.xml or jdoconfig.xml and refer to the documentation for persistence properties for full details of the properties.\n\nJBoss 3.0/3.2\n\nTo use DataNucleus on JBoss (Ver 3.2) the first thing that you will require is the datanucleus-jdo-jca-5.2.rar file. You should put this in the deploy directory (${JBOSS}/server/default/deploy/) of your JBoss installation.\n\nYou then create a file, also in the deploy directory with name datanucleus-ds.xml. To give a guide on what this file will typically include, see the following\n\n<?xml version=\"1.0\" encoding=\"UTF-8\"?> <connection-factories> <tx-connection-factory> <jndi-name>datanucleus</jndi-name> <adapter-display-name>DataNucleus Connector</adapter-display-name> <config-property name=\"ConnectionDriverName\" type=\"java.lang.String\">com.mysql.jdbc.Driver</config-property> <config-property name=\"ConnectionURL\" type=\"java.lang.String\">jdbc:mysql://localhost/yourdbname</config-property> <config-property name=\"UserName\" type=\"java.lang.String\">yourusername</config-property> <config-property name=\"Password\" type=\"java.lang.String\">yourpassword</config-property> </tx-connection-factory> <tx-connection-factory> <jndi-name>datanucleus1</jndi-name> <adapter-display-name>DataNucleus Connector</adapter-display-name> <config-property name=\"ConnectionDriverName\" type=\"java.lang.String\">com.mysql.jdbc.Driver</config-property> <config-property name=\"ConnectionURL\" type=\"java.lang.String\">jdbc:mysql://localhost/yourdbname1</config-property> <config-property name=\"UserName\" type=\"java.lang.String\">yourusername</config-property> <config-property name=\"Password\" type=\"java.lang.String\">yourpassword</config-property> </tx-connection-factory> <tx-connection-factory> <jndi-name>datanucleus2</jndi-name> <adapter-display-name>DataNucleus Connector</adapter-display-name> <config-property name=\"ConnectionDriverName\" type=\"java.lang.String\">com.mysql.jdbc.Driver</config-property> <config-property name=\"ConnectionURL\" type=\"java.lang.String\">jdbc:mysql://localhost/yourdbname2</config-property> <config-property name=\"UserName\" type=\"java.lang.String\">yourusername</config-property> <config-property name=\"Password\" type=\"java.lang.String\">yourpassword</config-property> </tx-connection-factory> </connection-factories>\n\nThis example creates 3 connection factories to MySQL databases, but you can create as many or as few as you require for your system to whichever databases you prefer (as long as they are supported by DataNucleus). With the above definition we can then use the JNDI names java:/datanucleus, java:/datanucleus1, and java:/datanucleus2 to refer to our datastores.\n\nNote, that you can use separate deployment descriptor files. That means, you could for example create the three files datanucleus1-ds.xml, datanucleus2-ds.xml and datanucleus3-ds.xml with each declaring one PersistenceManagerFactory instance. This is useful (or even required) if you need a distributed configuration. In this case, you can use JBoss' hot deployment feature and deploy a new PersistenceManagerFactory, while the server is running (and working with the existing PMFs): If you create a new *-ds.xml file (instead of modifying an existing one), the server does not undeploy anything (and thus not interrupt ongoing work), but will only add the new connection factory to the JNDI.\n\nYou are now set to work on DataNucleus-enabling your actual application. As we have said, you can use the above JNDI names to refer to the datastores, so you could do something like the following to access the PersistenceManagerFactory to one of your databases.\n\nimport javax.jdo.PersistenceManagerFactory; InitialContext context = new InitialContext(); PersistenceManagerFactory pmf = (PersistenceManagerFactory)context.lookup(\"java:/datanucleus1\");\n\nThese instructions were adapted from a contribution by a DataNucleus user Marco Schulze.\n\nJBoss 4.0\n\nWith JBoss 4.0 there are some changes in configuration relative to JBoss 3.2 in order to allow use some new features of JCA 1.5. Here you will see how to configure JBoss 4.0 to use with DataNucleus JCA adapter for DB2.\n\nTo use DataNucleus on JBoss 4.0 the first thing that you will require is the datanucleus-jdo-jca-5.2.rar file. You should put this in the deploy directory (\"${JBOSS}/server/default/deploy/\") of your JBoss installation. Additionally, you have to remember to put any JDBC driver files to lib directory (\"${JBOSS}/server/default/lib/\") if JBoss does not have them installed by default. In case of DB2 you need to copy db2jcc.jar and db2jcc_license_c.jar.\n\nYou then create a file, also in the deploy directory with name datanucleus-ds.xml. To give a guide on what this file will typically include, see the following\n\n<?xml version=\"1.0\" encoding=\"UTF-8\"?> <connection-factories> <tx-connection-factory> <jndi-name>datanucleus</jndi-name> <rar-name>datanucleus-jca-version}.rar</rar-name> <connection-definition>javax.resource.cci.ConnectionFactory</connection-definition> <config-property name=\"ConnectionDriverName\" type=\"java.lang.String\">com.ibm.db2.jcc.DB2Driver</config-property> <config-property name=\"ConnectionURL\" type=\"java.lang.String\">jdbc:derby:net://localhost:1527/\"directory_of_your_db_files\"</config-property> <config-property name=\"UserName\" type=\"java.lang.String\">app</config-property> <config-property name=\"Password\" type=\"java.lang.String\">app</config-property> </tx-connection-factory> </connection-factories>\n\nYou are now set to work on DataNucleus-enabling your actual application. You can use the above JNDI name to refer to the datastores, and so you could do something like the following to access the PersistenceManagerFactory to one of your databases.\n\nimport javax.jdo.PersistenceManagerFactory; InitialContext context=new InitialContext(); PersistenceManagerFactory pmFactory=(PersistenceManagerFactory)context.lookup(\"java:/datanucleus\");\n\nThese instructions were adapted from a contribution by a DataNucleus user Maciej Wegorkiewicz\n\nDataNucleus jars are OSGi bundles, and as such, can be deployed in an OSGi environment. Being an OSGi environment care must be taken with respect to class-loading. In particular the persistence property datanucleus.primaryClassLoader will need setting. Please refer to the following guide(s) for assistance until a definitive guide can be provided\n\nPlease make use of the OSGi sample for JDO in case it is of use. Use of OSGi is notorious for class loading oddities, so it may be necessary to refine this sample for your situation. We welcome any feedback to improve it.\n\nHOWTO Use Datanucleus with OSGi and Spring DM\n\nThis guide was written by Jasper Siepkes.\n\nThis guide is based on my personal experience and is not the authoritative guide to using DataNucleus with OSGi and Spring DM. I’ve updated this guide to use DataNucleus 3.x and Eclipse Gemini (formerly Spring DM). I haven’t extensively tested it yet. This guide explains how to use DataNucleus, Spring, OSGi and the OSGi blueprint specification together. This guide assumes the reader is familiar with concepts like OSGi, Spring, JDO, DataNucleus etc. This guide only explains how to wire these technologies together and not how they work. Now there have been a lot of (name) changes in over a short course of time. Some webpages might not have been updated yet so to undo some of the confusion created here is the deal with Eclipse Gemini. Eclipse Gemini started out as Spring OSGi, which was later renamed to Spring Dynamic Modules or Spring DM for short. Spring DM is NOT to be confused with Spring DM Server. Spring DM Server is a complete server product with management UI and tons of other features. Spring DM is the core of Spring DM Server and provides only the service / dependency injection part. At some point in time the Spring team decided to donate their OSGi efforts to the Eclipse foundation. Spring DM became Eclipse Gemini and Spring DM Server became Eclipse Virgo. The whole Spring OSGi / Spring DM / Eclipse Gemini later became standardised as the OSGi Blueprint specification. To summarise: Spring OSGi = Spring DM = Eclipse Gemini, Spring DM Server = Eclipse Virgo.\n\nTechnologies used in this guide are:\n\nIDE (Eclipse 3.7)\n\nOSGi (Equinox 3.7.1)\n\nJDO (DataNucleus 3.x)\n\nDependency Injection (Spring 3.0.6)\n\nOSGi Blueprint (Eclipse Gemini BluePrint 1.0.0)\n\nDatastore (PostgreSQL 8.3, altough any datastore supported by DataNucleus can be used)\n\nWe are going to start by creating a clean OSGi target platform. Start by creating an empty directory which is going to house all the bundles for our target platform.\n\nStep 1 : Adding OSGi\n\nThe first ingredient we are adding to our platform is the OSGi implementation. In this guide we will use Eclipse Equinox as our OSGi implementation. However one could also use Apache Felix, Knoplerfish, Concierge or any other compatible OSGi implementation for this purpose. Download the org.eclipse.osgi_3.7.1.R37x_v20110808-1106.jar (\"Framework Only\" download) from the Eclipse Equinox website and put in the target platform.\n\nStep 2 - Adding DI\n\nWe are now going to add the Spring, Spring ORM, Spring JDBC, Spring Transaction and Spring DM bundles to our target platform. Download the Spring Community distribution from their website (spring-framework-3.0.6.RELEASE.zip). Extract the following files to our target platform directory:\n\norg.springframework.aop-3.0.6.RELEASE.jar\n\norg.springframework.asm-3.0.6.RELEASE.jar\n\norg.springframework.aspects-3.0.6.RELEASE.jar\n\norg.springframework.beans-3.0.6.RELEASE.jar\n\norg.springframework.context.support-3.0.6.RELEASE.jar\n\norg.springframework.context-3.0.6.RELEASE.jar\n\norg.springframework.core-3.0.6.RELEASE.jar\n\norg.springframework.expression-3.0.6.RELEASE.jar\n\norg.springframework.jdbc-3.0.6.RELEASE.jar\n\norg.springframework.orm-3.0.6.RELEASE.jar\n\norg.springframework.spring-library-3.0.6.RELEASE.libd\n\norg.springframework.transaction-3.0.6.RELEASE.jar\n\nStep 3 - Adding OSGi Blueprint\n\nDownload the Eclipse Gemini release from their website (gemini-blueprint-1.0.0.RELEASE.zip) and extract the following files to our target platform:\n\ngemini-blueprint-core-1.0.0.RELEASE.jar\n\ngemini-blueprint-extender-1.0.0.RELEASE.jar\n\ngemini-blueprint-io-1.0.0.RELEASE.jar\n\nStep 4 - Adding ORM\n\nWe are now going to add JDO and DataNucleus to our target platform.\n\ndatanucleus-core-XXX.jar\n\ndatanucleus-api-jdo-XXX.jar\n\ndatanucleus-rdbms-XXX.jar\n\njavax.jdo-3.2.0-m5.jar\n\nStep 5 - Adding miscellaneous bundles\n\nThe following bundles are dependencies of our core bundles and can be downloaded from the Spring Enterprise Bundle Repository\n\ncom.springsource.org.aopalliance-1.0.0.jar (Dependency of Spring AOP, the core AOP bundle. )\n\ncom.springsource.org.apache.commons.logging-1.1.1.jar (Dependency of various Spring bundles, logging abstraction library.)\n\ncom.springsource.org.postgresql.jdbc4-8.3.604.jar (PostgreSQL JDBC driver, somewhat dated.)\n\nWe now have a basic target platform. This is how the directory housing the target platform looks on my PC:\n\n$ ls -las 4 drwxrwxr-x 2 siepkes siepkes 4096 Oct 22 15:28 . 4 drwxrwxr-x 3 siepkes siepkes 4096 Oct 22 15:29 .. 8 -rw-r----- 1 siepkes siepkes 4615 Oct 22 15:27 com.springsource.org.aopalliance-1.0.0.jar 68 -rw-r----- 1 siepkes siepkes 61464 Oct 22 15:28 com.springsource.org.apache.commons.logging-1.1.1.jar 472 -rw-r----- 1 siepkes siepkes 476053 Oct 22 15:28 com.springsource.org.postgresql.jdbc4-8.3.604.jar 312 -rw-r----- 1 siepkes siepkes 314358 Oct 2 11:36 datanucleus-api-jdo-5.0.1.jar 1624 -rw-r----- 1 siepkes siepkes 1658797 Oct 2 11:36 datanucleus-core-5.0.1.jar 1400 -rw-r----- 1 siepkes siepkes 1427439 Oct 2 11:36 datanucleus-rdbms-5.0.1.jar 572 -rw-r----- 1 siepkes siepkes 578205 Aug 22 22:37 gemini-blueprint-core-1.0.0.RELEASE.jar 180 -rw-r----- 1 siepkes siepkes 178525 Aug 22 22:37 gemini-blueprint-extender-1.0.0.RELEASE.jar 32 -rw-r----- 1 siepkes siepkes 31903 Aug 22 22:37 gemini-blueprint-io-1.0.0.RELEASE.jar 208 -rw-r--r-- 1 siepkes siepkes 208742 Oct 2 11:36 javax.jdo-3.2.0-m5.jar 1336 -rw-r----- 1 siepkes siepkes 1363464 Oct 22 14:26 org.eclipse.osgi_3.7.1.R37x_v20110808-1106.jar 320 -rw-r----- 1 siepkes siepkes 321428 Aug 18 16:50 org.springframework.aop-3.0.6.RELEASE.jar 56 -rw-r----- 1 siepkes siepkes 53082 Aug 18 16:50 org.springframework.asm-3.0.6.RELEASE.jar 36 -rw-r----- 1 siepkes siepkes 35557 Aug 18 16:50 org.springframework.aspects-3.0.6.RELEASE.jar 548 -rw-r----- 1 siepkes siepkes 556590 Aug 18 16:50 org.springframework.beans-3.0.6.RELEASE.jar 660 -rw-r----- 1 siepkes siepkes 670258 Aug 18 16:50 org.springframework.context-3.0.6.RELEASE.jar 104 -rw-r----- 1 siepkes siepkes 101450 Aug 18 16:50 org.springframework.context.support-3.0.6.RELEASE.jar 380 -rw-r----- 1 siepkes siepkes 382184 Aug 18 16:50 org.springframework.core-3.0.6.RELEASE.jar 172 -rw-r----- 1 siepkes siepkes 169752 Aug 18 16:50 org.springframework.expression-3.0.6.RELEASE.jar 384 -rw-r----- 1 siepkes siepkes 386033 Aug 18 16:50 org.springframework.jdbc-3.0.6.RELEASE.jar 332 -rw-r----- 1 siepkes siepkes 334743 Aug 18 16:50 org.springframework.orm-3.0.6.RELEASE.jar 4 -rw-r----- 1 siepkes siepkes 1313 Aug 18 16:50 org.springframework.spring-library-3.0.6.RELEASE.libd 232 -rw-r----- 1 siepkes siepkes 231913 Aug 18 16:50 org.springframework.transaction-3.0.6.RELEASE.jar\n\nStep 6 - Set up Eclipse\n\nHere I will show how one can create a base for an application with our newly created target platform.\n\nCreate a Target Platform in Eclipse by going to 'Window' → 'Preferences' → 'Plugin Development' → 'Target Platform' and press the 'Add' button. Select 'Nothing: Start with an empty target platform', give the platform a name and point it to the directory we put all the jars/bundles in. When you are done press the 'Finish' button. Indicate to Eclipse we want to use this new platform by ticking the checkbox in front of our newly created platform in the 'Target Platform' window of the 'Preferences' screen.\n\nCreate a new project in Eclipse by going to 'File' → 'New…​' → 'Project' and Select 'Plug-in Project' under the 'Plugin development' leaf. Give the project a name (I’m going to call it 'nl.siepkes.test.project.a' in this example). In the radiobox options 'This plugin is targetted to run with:' select 'An OSGi framework' → 'standard'. Click 'Next'. Untick the 'Generate an activator, a Java class that…​.' and press 'Finish'.\n\nObviously Eclipse is not the mandatory IDE for the steps described above. Other technologies can be used instead. For this guide I used Eclipse because it is easy to explain, but for most of my projects I use Maven. If you have the Spring IDE plugin installed (which is advisable if you use Spring) you can add a Spring Nature to your project by right clicking your project and then clicking 'Spring Tools' → 'Add Spring Nature'. This will enable error detection in your Spring bean configuration file.\n\nCreate a directory called 'spring' in your 'META-INF' directory. In this directory create a Spring bean configuration file by right clicking the directory and click 'New…​' → 'Other…​'. A menu called 'New' will popup, select 'Spring Bean Configuration File'. Call the file beans.xml.\n\nIt is important to realize that the Datanucleus plugin system uses the Eclipse extensions system and NOT the plain OSGi facilities. There are two ways to make the DataNucleus plugin system work in a plain OSGi environment:\n\nTell DataNucleus to use a simplified plugin manager which does not use the Eclipse plugin system (called \"OSGiPluginRegistry\").\n\nAdd the Eclipse plugin system to the OSGi platform.\n\nWe are going to use the simplified plugin manager. The upside is that its easy to setup. The downside is that is less flexible then the Eclipse plugin system. The Eclipse plugin system allowes you to manage different version of DataNucleus plugins. With the simplified plugin manager you can have only one version of a DataNucleus plugin in your OSGi platform at any given time.\n\nDeclare a Persistence Manager Factory Bean inside the beans.xml:\n\n<bean id=\"pmf\" class=\"nl.siepkes.util.DatanucleusOSGiLocalPersistenceManagerFactoryBean\"> <property name=\"jdoProperties\"> <props> <prop key=\"javax.jdo.PersistenceManagerFactoryClass\">org.datanucleus.api.jdo.JDOPersistenceManagerFactory</prop> <prop key=\"javax.jdo.option.ConnectionURL\">jdbc:postgresql://localhost/testdb</prop> <prop key=\"javax.jdo.option.ConnectionUserName\">foo</prop> <prop key=\"javax.jdo.option.ConnectionPassword\">bar</prop> <prop key=\"datanucleus.schema.autoCreateAll\">true</prop> <prop key=\"datanucleus.schema.validateAll\">tru"
    }
}
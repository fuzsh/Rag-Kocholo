{
    "id": "correct_subsidiary_00134_2",
    "rank": 12,
    "data": {
        "url": "https://medium.com/oracle-coherence/hello-coherence-part-1-6900de3f9ceb",
        "read_more_link": "",
        "language": "en",
        "title": "Hello Coherence, Part 1",
        "top_image": "https://miro.medium.com/v2/resize:fit:1097/0*Isooqs0jsBAeWcEF.png",
        "meta_img": "https://miro.medium.com/v2/resize:fit:1097/0*Isooqs0jsBAeWcEF.png",
        "images": [
            "https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png",
            "https://miro.medium.com/v2/resize:fill:88:88/1*eTIHw_lbiq1bW-uinwlYhg.png",
            "https://miro.medium.com/v2/resize:fill:48:48/1*sfReS1plQGlKXa3VrGu0xw.png",
            "https://miro.medium.com/v2/resize:fill:144:144/1*eTIHw_lbiq1bW-uinwlYhg.png",
            "https://miro.medium.com/v2/resize:fill:64:64/1*sfReS1plQGlKXa3VrGu0xw.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Aleks Seovic",
            "aseovic.medium.com"
        ],
        "publish_date": "2020-09-05T00:50:11.573000+00:00",
        "summary": "",
        "meta_description": "This article was originally published in Java Magazine, on August 14, 2020. It has since been updated to cover the latest Coherence CE 22.06 and Helidon 2.5 releases. Oracle Coherence is nearly 20…",
        "meta_lang": "en",
        "meta_favicon": "https://miro.medium.com/v2/1*m-R_BkNf1Qjr1YbyOIJY2w.png",
        "meta_site_name": "Medium",
        "canonical_link": "https://medium.com/oracle-coherence/hello-coherence-part-1-6900de3f9ceb",
        "text": "Use the open source Oracle Coherence Community Edition to create stateful applications that are as easy to scale, if not easier, than the stateless applications you are building today.\n\nAleks Seovic\n\n·\n\nFollow\n\nPublished in\n\nOracle Coherence\n\n·\n\n22 min read\n\n·\n\nSep 5, 2020\n\n--\n\nThis article was originally published in Java Magazine, on August 14, 2020. It has since been updated to cover the latest Coherence CE 22.06 and Helidon 2.5 releases.\n\nOracle Coherence is nearly 20 years old. It started as a distributed caching product and then evolved into an in-memory data grid. It’s an essential tool for improving the performance and scalability of Java EE applications, and it’s widely used for large-scale projects.\n\nThink of it as a scalable, concurrent, fault-tolerant java.util.Map implementation that is partitioned across multiple JVMs, machines, and even data centers. That is a major oversimplification, of course, but it is sufficient for now.\n\nFor most of its history, Oracle Coherence was a commercial product with a fairly high price tag. That limited its appeal mainly to corporate users and took it out of consideration for many applications, including smaller or open source projects that didn’t absolutely need the features it offers.\n\nAll that changed on June 25, 2020, when Oracle released Coherence Community Edition (CE), an open source version of the product.\n\nCoherence CE may often be the best option you have when building modern, cloud native applications and services. Along with my colleague Harvey Raja, I recently wrote A Gentle Introduction to Coherence, covering what the platform is and why you should consider using it for your next application, so I will not repeat that information here.\n\nIn this article, I will focus on how to build scalable stateful applications using Coherence CE.\n\nExtending the example To Do List service application\n\nIn this article, I will extend the To Do List application used as a quick start example on the Coherence CE website (see Figure 1). I’m doing this for two reasons:\n\nIt has a very simple domain model that everyone understands, so this article can focus on the usage of Coherence.\n\nDespite its simplicity, the sample application demonstrates many Coherence features from basic reads and writes, to queries and aggregations, to in-place processing and events.\n\nFigure 1. Screenshot of the To Do List application\n\nI’ll make my application a lot more interesting than the quick start example, though: In addition to the Helidon REST service implemented there, my project will do the following:\n\nAdd support for server-sent events (SSEs) to the REST API to broadcast Coherence events to REST clients.\n\nImplement a React-based web front end that will use the Helidon REST API, which will be served by the Helidon Web Server as well.\n\nConfigure Coherence CE’s gRPC server and implement a JavaFX front end that uses the native Java client to interact with Coherence over gRPC.\n\nDemonstrate how all the components above work together.\n\nDeploy the application into a Kubernetes cluster using Coherence Operator.\n\nAlthough I will use Helidon, JavaFX, and React to implement the application, the focus will be very much on Coherence CE usage and APIs. Everything else is secondary, but you are more than welcome to explore the source code for the complete application, which is available on GitHub.\n\nFair warning: I am not a JavaFX or React expert, so some of the UI-related code may be suboptimal.\n\nAs you can see from the list above, there’s too much to fit into a single article. So, I will implement the REST API back end for the application here, and I’ll leave front-end implementations and operational aspects, such as deployment and monitoring, for future articles in this series.\n\nImplementing the REST API\n\nThe first step is to create a Maven project with all the needed Helidon and Coherence CE dependencies. Do that by following the instructions in the Helidon MP Tutorial, and then add a few additional dependencies.\n\nThe resulting POM file should look like this:\n\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n\n<project xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n\nxmlns=\"http://maven.apache.org/POM/4.0.0\"\n\nxsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n\n<modelVersion>4.0.0</modelVersion> <parent>\n\n<groupId>io.helidon.applications</groupId>\n\n<artifactId>helidon-mp</artifactId>\n\n<version>2.5.1</version>\n\n<relativePath/>\n\n</parent> <groupId>com.oracle.coherence.examples</groupId>\n\n<artifactId>todo-list-helidon-server</artifactId>\n\n<version>1.0.0-SNAPSHOT</version> <properties>\n\n<coherence.groupId>com.oracle.coherence.ce</coherence.groupId>\n\n<coherence.version>22.06.1</coherence.version>\n\n</properties> <dependencies>\n\n<!-- Helidon dependencies-->\n\n<dependency>\n\n<groupId>io.helidon.microprofile.bundles</groupId>\n\n<artifactId>helidon-microprofile</artifactId>\n\n</dependency>\n\n<dependency>\n\n<groupId>org.glassfish.jersey.media</groupId>\n\n<artifactId>jersey-media-json-binding</artifactId>\n\n</dependency>\n\n<dependency>\n\n<groupId>org.glassfish.jersey.media</groupId>\n\n<artifactId>jersey-media-sse</artifactId>\n\n</dependency> <!-- Coherence CE dependencies -->\n\n<dependency>\n\n<groupId>${coherence.groupId}</groupId>\n\n<artifactId>coherence-cdi-server</artifactId>\n\n<version>${coherence.version}</version>\n\n</dependency>\n\n<dependency>\n\n<groupId>${coherence.groupId}</groupId>\n\n<artifactId>coherence-mp-config</artifactId>\n\n<version>${coherence.version}</version>\n\n</dependency>\n\n</dependencies> <build>\n\n<plugins>\n\n<plugin>\n\n<groupId>org.jboss.jandex</groupId>\n\n<artifactId>jandex-maven-plugin</artifactId>\n\n<executions>\n\n<execution>\n\n<id>make-index</id>\n\n</execution>\n\n</executions>\n\n</plugin>\n\n</plugins>\n\n</build></project>\n\nIn addition to the Helidon MicroProfile Bundle, I have added Eclipse Jersey support for JSON-B serialization and SSEs. The code also configures the Jandex plugin to index the application’s classes at build time to speed up Contexts and Dependency Injection (CDI) startup.\n\nOn the Coherence CE side, there are two added modules:\n\nCoherence CDI Server, which provides a CDI extension that starts the Coherence CE server within the application process, enables the injection of Coherence maps into Helidon services, and maps Coherence events to CDI events so they can be handled using standard CDI observers\n\nCoherence MicroProfile Config, which will configure Coherence CE using the Helidon MP Config implementation\n\nI used a Maven property to specify not only the Coherence CE version but also the Maven groupId for Coherence CE dependencies. This is a recommended practice, because it allows you to easily switch between the open source version (Coherence CE) and the commercial versions (Oracle Coherence Enterprise Edition or Oracle Coherence Grid Edition), which have a different groupId. Everything else in your code, from artifact names to package and class names, can stay exactly the same.\n\nIn addition to creating a Maven project, it’s necessary to create a few configuration files. First, turn the application into a proper CDI bean archive by creating the META-INF/beans.xml file:\n\n<?xml version=\"1.0\" encoding=\"UTF-8\"?><beans xmlns=\"http://xmlns.jcp.org/xml/ns/javaee\"\n\nxmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n\nxsi:schemaLocation=\"http://xmlns.jcp.org/xml/ns/javaee\n\nhttp://xmlns.jcp.org/xml/ns/javaee/beans_2_0.xsd\"\n\nversion=\"2.0\"\n\nbean-discovery-mode=\"annotated\">\n\nIt’s necessary to configure Java Logging to see log output from Helidon and Coherence CE. To configure it, add the logging.properties file to the src/main/resources directory:\n\nhandlers=io.helidon.common.HelidonConsoleHandler# Global default logging level. Can be overriden by specific handlers and loggers\n\n.level=CONFIG# Formatter configuration\n\njava.util.logging.SimpleFormatter.format=%1$tY.%1$tm.%1$td %1$tH:%1$tM:%1$tS %4$s %3$s !thread!: %5$s%6$s%n\n\nImplementing the domain model\n\nTo implement the REST API for the To Do List application, the application will need two things: a data model representing a single task in a to-do list and a JAX-RS resource that provides the necessary REST endpoints.\n\nThe first one is a trivial plain old Java object (POJO) with a handful of attributes that are very much self-explanatory:\n\npublic class Task\n\nimplements Serializable\n\n{\n\nprivate String id;\n\nprivate long createdAt;\n\nprivate String description;\n\nprivate Boolean completed; /**\n\n* Construct Task instance.\n\n*\n\n* @param description task description\n\n*/\n\npublic Task(String description)\n\n{\n\nthis.id = UUID.randomUUID().toString().substring(0, 6);\n\nthis.createdAt = System.currentTimeMillis();\n\nthis.description = description;\n\nthis.completed = false;\n\n} // accessors omitted for brevity\n\n}\n\nNote that this POJO is serializable using Java serialization, which is what Coherence CE will use as a storage format, and using JSON-B, which will be used by the REST and gRPC APIs. Coherence CE supports pluggable serializers for both storage and the client/server communication. I could have used JSON for both. The application could also have used JSON as a transport format and Coherence Portable Object Format (POF) as a storage format.\n\nThe goal here is to demonstrate that you can use different serialization formats for client/server communication and storage, with Coherence CE automatically converting objects as necessary. It is a bit too early to talk about Coherence POF, though; that will come up later.\n\nImplementing Task Repository\n\nIn addition to a low-level NamedMap API, Coherence CE provides a higher level Repository API. To make data access code as simple as possible, we will implement TaskRepository:\n\n@ApplicationScoped\n\npublic class TaskRepository extends AbstractRepository<String, Task>\n\n{\n\n@Inject\n\nprivate NamedMap<String, Task> tasks; protected NamedMap<String, Task> getMap()\n\n{\n\nreturn tasks;\n\n} protected String getId(Task task)\n\n{\n\nreturn task.getId();\n\n} protected Class<? extends Task> getEntityType()\n\n{\n\nreturn Task.class;\n\n}\n\n}\n\nThe implementation above is as simple as it gets: we implement AbstractRepository, define the ID and entity types for it, and inject a NamedMap that will be used as a backing store for our repository implementation.\n\nCoherence CE’s NamedMap is an extension of java.util.Map, an interface most developers know well. This is both good and bad. It is good because developers already know how to do many things with it. It is bad because developers may do things in a suboptimal way when a better alternative is readily available.\n\nTo dig deeper, unlike Java’s Map, Coherence CE’s NamedMap is a distributed data structure. Data in it is not stored within local buckets, as it is in a Java HashMap, for example. Instead, it is stored within partitions, which can be distributed across many JVMs, machines, or even data centers. This means that some of the operations you take for granted when you work with a local map are not nearly as efficient when you work with a NamedMap.\n\nTake iteration as an example: You wouldn’t think twice about iterating over the entries of a HashMap. However, iterating over a NamedMap can be an extremely expensive operation, potentially moving gigabytes or even terabytes of data across the network, deserializing it, processing it, garbage collecting it, and so on.\n\nFortunately, most things work as you would expect, and the Coherence CE team has done a lot of work to make sure that’s the case. For example, the team has reimplemented many of the Map methods to use Coherence CE primitives, such as aggregators and entry processors, in a way that is suitable for a distributed Mapimplementation. Similarly, the developers have completely reimplemented the Stream API on top of Coherence CE aggregators, effectively allowing you to perform stream operations in parallel across many machines. Distributed lambdas have been reimplemented to allow them to be serialized using any of the supported serialization formats in a way that can be safely used in a distributed environment, with potentially different versions of those lambdas being present on different cluster members.\n\nThe point is that even though the Coherence CE developers have done all that optimization, you still need to remember that you are working with a distributed data structure and that most calls will end up on the network. Therefore, you should use Coherence CE features that allow you to minimize the impact of that, and Repository API is one of those features, as you’ll see in a minute.\n\nImplementing the Service\n\nWith the data model and the repository implementations in place, we can start implementing business logic for our task management service.\n\nWhile we could do this directly within our JAX-RS resource that will be used to provide REST endpoints for our service, that would not be ideal. We also want to provide gRPC and GraphQL endpoints, so implementing business logic in a way that allows us to reuse it for all three API implementations seems more appropriate.\n\n@ApplicationScoped\n\npublic class ToDoListService\n\n{\n\n@Inject\n\nprotected TaskRepository tasks; // TBD\n\n}\n\nFirst, the code needs some basic CRUD functionality: the ability to add, update, and remove individual tasks and to retrieve a list of all tasks.\n\npublic Task createTask(String description)\n\n{\n\nObjects.requireNonNull(description, \"description is required\");\n\nreturn tasks.save(new Task(description));\n\n}\n\nThe method above creates a new Task instance with a given description and saves it to the repository. The return value from the save method is fully populated instance of the created Task object.\n\nThe methods to find and remove existing tasks are almost as simple:\n\npublic Task findTask(String id)\n\n{\n\nreturn Optional\n\n.ofNullable(tasks.get(id))\n\n.orElseThrow(() -> new TaskNotFoundException());\n\n}public Task deleteTask(String id)\n\n{\n\nreturn Optional\n\n.ofNullable(tasks.removeById(id, true))\n\n.orElseThrow(() -> new TaskNotFoundException());\n\n}\n\nThe only complication is that we need to throw a TaskNotFoundException if the task with the specified ID doesn’t exist in the repository. This is an application exception, which will be mapped to an appropriate HTTP response (404 Not Found, in this case) using JAX-RS exception mapper.\n\nNow let’s implement the logic that will allow us to fetch a list of tasks and to remove tasks based on their completion status:\n\npublic Collection<Task> findTasks(Boolean completed)\n\n{\n\nFilter<Task> filter = completed == null\n\n? always()\n\n: equal(Task::getCompleted, completed); return tasks.getAllOrderedBy(filter, Task::getCreatedAt);\n\n}public boolean deleteCompletedTasks()\n\n{\n\nreturn tasks.removeAll(isTrue(Task::getCompleted));\n\n}\n\nThis code shows an example of an optimization I mentioned earlier. Instead of fetching all the tasks from the server and filtering them on the client, I use TaskRepository.getAllOrderedBy method, which uses Coherence Filter API to execute the query in parallel across the cluster members and returns only the subset of the tasks with the specified completion status. It also sorts the results based on task creation time, so the results return to the clients in a consistent order.\n\nIf the client doesn’t specify the completion status to query on, we return all tasks by passing an AlwaysFilter instance to the method.\n\nSimilarly, we use TaskRepository.removeAll method with a Filter that evaluates to true if the task is completed, in order to remove all completed tasks from the repository in parallel.\n\nFinally, let’s look at a slightly more interesting piece of functionality: mutating operations:\n\npublic Task updateDescription(String id, String description)\n\n{\n\nTask task = tasks.update(id, Task::setDescription, description);\n\nreturn Optional\n\n.ofNullable(task)\n\n.orElseThrow(() -> new NotFoundException());\n\n}public Task updateCompletionStatus(String id, boolean completed)\n\n{\n\nTask task = tasks.update(id, Task::setCompleted, completed);\n\nreturn Optional\n\n.ofNullable(task)\n\n.orElseThrow(() -> new NotFoundException());\n\n}\n\nJust like with findTask and deleteTask, we throw a NotFoundException if the task with the specified ID doesn’t exist in the repository. However, that is not the “interesting” part — what is interesting is how the update itself is performed.\n\nUnlike many data stores, Coherence doesn’t force you to use read-modify-write idiom in order to mutate data in the cluster — as a matter of fact, it discourages you from doing so.\n\nFor example, I could’ve written the code above as:\n\npublic Task updateDescription(String id, String description)\n\n{\n\nTask task = tasks.get(id); // read\n\nif (task == null)\n\n{\n\nthrow new NotFoundException();\n\n} task.setDescription(description); // modify\n\nreturn tasks.save(task); // write\n\n}\n\nBut I didn’t, and you shouldn’t…\n\nThere are two problems withe the code above:\n\nIt is not correct from a concurrency perspective — multiple processes could retrieve the same task from the cluster, modify it in a different way, and write it back. The last one to write would win, and all the changes made by other processes would be lost. Sure, we could use explicit locking to avoid that, but that is a very expensive solution that turns a 6-network hop operation into a 14-network hop operation, as locking and unlocking require additional network calls to both primary and backup members.\n\nIt is inefficient even without locking, as it moves potentially large object over the wire three times, and requires 6 network hops on its own (2 for get and 4 for save).\n\nWhat we did instead was this:\n\nTask task = tasks.update(id, Task::setDescription, description);\n\nThere is a lot happening there, so let’s break it down:\n\nFind the primary copy of the task with the specified id, wherever it may be in the cluster (likely on a remote cluster member)\n\nModify it by calling setDescription method on it, passing specified description as an argument\n\nReturn updated Task to the caller.\n\nThis way we don’t need explicit lock, as Coherence will obtain a lightweight, implicit lock on the primary copy of the object automatically before modifying it. It also reduces the number of times we move the object over the wire from three to two (from primary to backup, and from primary to the caller).\n\nThe update method used above barely scratches the surface of what you can do. You could also use one of more powerful update overloads to update multiple attributes in one call, create entity instance if it doesn’t exist, or even avoid sending the updated value back to the caller if you don’t need it, further reducing the amount of data moving over the wire.\n\nUnderstanding Entry Processors\n\nThis feature of the Repository API builds on one of the most important distributed computing primitives you will ever use with Coherence: entry processors.\n\nEntry processors allow you to send functions into the cluster and process data in place, which completely eliminates the need to move the data over the network in most cases. In other words, the function moves to the data, not the other way around!\n\nTo some extent, you can compare entry processors to stored procedures: entry processors efficiently process large data sets in parallel without moving any data over the network (apart from any processing results, which you can optionally return). However, there are two important differences between them:\n\n1. Entry processors are written in Java instead of SQL, because Java is to Coherence what SQL is to a database, and this makes them much easier to write and use (for Java developers, at least) than stored procedures could ever be.\n\n2. Entry processors can be predefined via an existing Java class implementation that is available on both the client and the server. Or, they can be created dynamically via Java lambdas, in which case they are shipped from the client to the server, bytecode and all, and registered as versioned classes to support multiple versions of the same lambda across the cluster. The example above uses a simple method reference, but I’ll show a lambda example shortly.\n\nBy the way, entry processors are guaranteed to execute exactly once, even in the case of failures of primary or backup members, as long as the primary member and all its backups do not fail at the same time. This is an incredibly difficult guarantee to provide in a distributed system, and it’s a huge differentiator between Coherence and products from some of Oracle’s competitors, who may have a similar feature without the same guarantees Coherence provides.\n\nI cannot stress enough how important and powerful entry processors are. In theory, you could implement every other method in the NamedMap API using entry processors, and as a matter of fact, I did use them to implement many of the default Map methods that were added in Java 8, and most of the AsyncNamedMap API.\n\nWith that, we are done with the service implementation. It is time to put some API facades in front of it, so it can be used by REST, gRPC and GraphQL clients.\n\nImplementing REST endpoints\n\nNow that we have a service that provides the functionality we want to expose to the clients, writing a JAX-RS facade that will do that is trivial. But first, let’s define the API that we want to provide to the clients:\n\nPOST /api/tasks # create a new task\n\nGET /api/tasks[?completed] # fetch tasks based on completion state\n\nDELETE /api/tasks/:id # delete specified task\n\nDELETE /api/tasks # delete completed tasks\n\nPUT /api/tasks/:id # update specified task\n\nGET /api/tasks/events # register for events (SSE)\n\nThe first four of these endpoints do nothing more than delegate to our service:\n\n@Path(\"/api/tasks\")\n\n@ApplicationScoped\n\npublic class ToDoListRestApi\n\n{\n\n@Inject\n\nprivate ToDoListService api; @POST\n\n@Consumes(APPLICATION_JSON)\n\npublic Task createTask(JsonObject task)\n\n{\n\nreturn api.createTask(task.getString(\"description\"));\n\n} @GET\n\n@Produces(APPLICATION_JSON)\n\npublic Collection<? extends Task>\n\ngetTasks(@QueryParam(\"completed\") Boolean completed)\n\n{\n\nreturn api.getTasks(completed);\n\n} @DELETE\n\n@Path(\"{id}\")\n\npublic Task deleteTask(@PathParam(\"id\") String id)\n\n{\n\nreturn api.deleteTask(id);\n\n} @DELETE\n\npublic boolean deleteCompletedTasks()\n\n{\n\nreturn api.deleteCompletedTasks();\n\n} // TBD\n\n}\n\nThe functionality to update a task is slightly more complex, as we need to determine whether to update description or a completion status based on the PUT payload:\n\n@PUT\n\n@Path(\"{id}\")\n\n@Consumes(APPLICATION_JSON)\n\npublic Task updateTask(@PathParam(\"id\") String id, JsonObject task)\n\n{\n\nif (task.containsKey(\"description\"))\n\n{\n\nreturn api.updateDescription(\n\nid, task.getString(\"description\"));\n\n}\n\nelse if (task.containsKey(\"completed\"))\n\n{\n\nreturn api.updateCompletionStatus(\n\nid, task.getBoolean(\"completed\"));\n\n} throw new IllegalArgumentException(\n\n\"either description or completion status must be specified\");\n\n}\n\nThat’s almost it — the only remaining task is to implement support for SSE, so clients can be notified when any of the existing tasks change or when tasks are added or deleted.\n\nImplementing Server-Sent Events\n\nCoherence provides a rich, powerful event model: Applications can observe server-side events, which are raised on the member that owns the entry that triggered the event, and client-side events, which allow you to observe the events happening across the cluster.\n\nWhat’s needed is to provide an SSE endpoint at /api/tasks/events that the REST API clients can use to register for a stream of change events to update their local state whenever the set of tasks in the TaskRepository changes. All the clients should receive the same stream of events after the registration. The application leverages SSE broadcast support in JAX-RS to accomplish that:\n\n@Inject\n\nprivate TaskRepository tasks;@Inject\n\nprivate Cluster cluster;@Context\n\nprivate Sse sse;\n\nprivate SseBroadcaster broadcaster;@PostConstruct\n\nvoid createBroadcaster()\n\n{\n\nthis.broadcaster = sse.newBroadcaster(); // TODO: convert Coherence events to SSE events\n\n}@GET\n\n@Path(\"events\")\n\n@Produces(MediaType.SERVER_SENT_EVENTS)\n\npublic void registerEventListener(@Context SseEventSink eventSink)\n\n{\n\nbroadcaster.register(eventSink); Member member = cluster.getLocalMember();\n\neventSink.send(sse.newEvent(\"begin\", member.toString()));\n\n}\n\nOnce this code is added to the JAX-RS resource, the clients will be able to register with the SSE broadcaster. However, while the clients can register to observe the SSE events, the application isn’t actually listening to Coherence events, or sending any SSE events yet.\n\nTo fix the first issue and start listening to Coherence events, we need to register an event listener with the TaskRepository within the createBroadcaster method above:\n\n@PostConstruct\n\nvoid createBroadcaster()\n\n{\n\nthis.broadcaster = sse.newBroadcaster(); tasks.addListener(\n\ntasks.listener()\n\n.onInsert(this::sendInsert)\n\n.onUpdate(this::sendUpdate)\n\n.onRemove(this::sendDelete)\n\n.build());\n\n}\n\nEach of the event handlers that we registered above will be called with a Task that caused the event as an argument whenever the repository changes.\n\nThe SSE broadcaster expects us to publish OutboundSseEvent instances with the event name and the relevant data as the JSON payload, so we need to convert the Task we received into the OutboundSseEvent and publish it:\n\nprivate void sendInsert(Task task)\n\n{\n\nbroadcaster.broadcast(createEvent(\"insert\", task));\n\n}private void sendUpdate(Task task)\n\n{\n\nbroadcaster.broadcast(createEvent(\"update\", task));\n\n}private void sendDelete(Task task)\n\n{\n\nbroadcaster.broadcast(createEvent(\"delete\", task));\n\n}private OutboundSseEvent createEvent(String name, Task task)\n\n{\n\nreturn sse.newEventBuilder()\n\n.name(name)\n\n.data(Task.class, task)\n\n.mediaType(APPLICATION_JSON_TYPE)\n\n.build();\n\n}\n\nThat concludes the REST API implementation. Let’s move on to the gRPC API.\n\nImplementing gRPC endpoints\n\nAs ubiquitous as REST is, it is not the most efficient, or the simplest way to access remote services. To provide an alternate way of accessing tasks managed by our service, we will also create a gRPC facade for it.\n\nTo accomplish that we will use Helidon gRPC framework:\n\n<dependency>\n\n<groupId>io.helidon.microprofile.grpc</groupId>\n\n<artifactId>helidon-microprofile-grpc-server</artifactId>\n\n</dependency>\n\nThis allows us to implement gRPC service the same way we’ve implemented our REST service: via an annotated Java class.\n\nJakarta gRPC\n\nHelidon gRPC is on its way to become an official Jakarta EE specification, via recently introduced Jakarta RPC project, led by yours truly.\n\n@Grpc(name = \"examples.json.ToDoList\")\n\n@GrpcMarshaller(\"jsonb\")\n\n@ApplicationScoped\n\npublic class ToDoListGrpcApiJson\n\n{\n\n@Inject\n\nprivate ToDoListService api;\n\n@Inject\n\nprotected TaskRepository tasks;\n\n// TBD\n\n}\n\nAs you can see, our gRPC service is just a CDI bean with a @Grpc and @GrpcMarshaller annotations, which define the service name clients should use to connect to it, and the marshaller to use (JSONB in this case), respectively.\n\nJust like with our REST facade, most of the functionality can be implemented by simply delegating to the injected ToDoListService instance:\n\n@Unary\n\npublic Task createTask(String description)\n\n{\n\nreturn api.createTask(description);\n\n}@ServerStreaming\n\npublic Stream<Task> getAllTasks()\n\n{\n\nreturn api.getTasks(null).stream();\n\n}@ServerStreaming\n\npublic Stream<Task> getTasks(boolean completed)\n\n{\n\nreturn api.getTasks(completed).stream();\n\n}@Unary\n\npublic Task findTask(String id)\n\n{\n\nreturn api.findTask(id);\n\n}@Unary\n\npublic Task deleteTask(String id)\n\n{\n\nreturn api.deleteTask(id);\n\n}@Unary\n\npublic boolean deleteCompletedTasks()\n\n{\n\nreturn api.deleteCompletedTasks();\n\n}\n\nIn a similar fashion, event streaming can be implemented by simply wiring gRPC @ServerStreaming methods to a repository listener:\n\n@ServerStreaming\n\npublic void onInsert(StreamObserver<Task> observer)\n\n{\n\ntasks.addListener(\n\ntasks.listener()\n\n.onInsert(observer::onNext)\n\n.build());\n\n}@ServerStreaming\n\npublic void onUpdate(StreamObserver<Task> observer)\n\n{\n\ntasks.addListener(\n\ntasks.listener()\n\n.onUpdate(observer::onNext)\n\n.build());\n\n}@ServerStreaming\n\npublic void onRemove(StreamObserver<Task> observer)\n\n{\n\ntasks.addListener(\n\ntasks.listener()\n\n.onRemove(observer::onNext)\n\n.build());\n\n}\n\nIn this case there is no need to convert received Task instances to a wrapper event type — we simply stream them back via three different event channels, one for each event type.\n\nFinally, in order to implement methods that allow us to update description and completion status, we need to define request messages that encapsulate task identifier and the value to update, as gRPC methods only accept a single argument:\n\n@Unary\n\npublic Task updateDescription(UpdateDescriptionRequest request)\n\n{\n\nreturn api.updateDescription(request.id, request.description);\n\n}@Unary\n\npublic Task updateCompletionStatus(\n\nUpdateCompletionStatusRequest request)\n\n{\n\nreturn api.updateCompletionStatus(\n\nrequest.id, request.completed);\n\n}// ---- request messages -------------------------------public static class UpdateDescriptionRequest\n\n{\n\npublic String id;\n\npublic String description;\n\n}public static class UpdateCompletionStatusRequest\n\n{\n\npublic String id;\n\npublic boolean completed;\n\n}\n\nThat’s pretty much it for our gRPC facade implementation. Time to wrap it up by implementing the final, GraphQL facade.\n\nImplementing GraphQL endpoints\n\nGraphQL is an increasingly popular way to access data from all kinds of data sources, so our amazing To Do List service needs to support it.\n\nTo accomplish that, we will leverage Helidon’s implementation of Eclipse MicroProfile GraphQL specification. Let’s add a dependency on it to our pom.xml:\n\n<dependency>\n\n<groupId>io.helidon.microprofile.graphql</groupId>\n\n<artifactId>helidon-microprofile-graphql-server</artifactId>\n\n</dependency>\n\nJust like with our REST and gRPC implementations, GraphQL facade is nothing more than a thin wrapper around our ToDoListService:\n\n@GraphQLApi\n\n@ApplicationScoped\n\npublic class ToDoListGraphQLApi\n\n{\n\n@Inject\n\nprivate ToDoListService api;\n\n//----- API methods -------------------------------\n\n@Mutation\n\n@Description(\"Create a task with the given description\")\n\npublic Task createTask(@Name(\"description\") @NonNull\n\nString description)\n\n{\n\nreturn api.createTask(description);\n\n} @Query\n\n@Description(\"Query tasks\")\n\npublic Collection<Task> getTasks(@Name(\"completed\")\n\nBoolean completed)\n\n{\n\nreturn api.getTasks(completed);\n\n} @Query\n\n@Description(\"Find a given task using the task id\")\n\npublic Task findTask(@Name(\"id\") @NonNull String id)\n\nthrows NotFoundException\n\n{\n\nreturn api.findTask(id);\n\n} @Mutation\n\n@Description(\"Delete a task and return its details\")\n\npublic Task deleteTask(@Name(\"id\") @NonNull String id)\n\nthrows NotFoundException\n\n{\n\nreturn api.deleteTask(id);\n\n} @Mutation\n\n@Description(\"Remove all completed tasks\")\n\npublic boolean deleteCompletedTasks()\n\n{\n\nreturn api.deleteCompletedTasks();\n\n} @Mutation\n\n@Description(\"Update task description\")\n\npublic Task updateDescription(\n\n@Name(\"id\") @NonNull String id,\n\n@Name(\"description\") @NonNull String description)\n\nthrows NotFoundException\n\n{\n\nreturn api.updateDescription(id, description);\n\n} @Mutation\n\n@Description(\"Update task completion status\")\n\npublic Task updateCompletionStatus(\n\n@Name(\"id\") @NonNull String id,\n\n@Name(\"completed\") boolean completed)\n\nthrows NotFoundException\n\n{\n\nreturn api.updateCompletionStatus(id, completed);\n\n}\n\n}\n\nAt the moment Eclipse MicroProfile GraphQL specification doesn’t support events, so we will not provide support for them in our GraphQL service facade either.\n\nWe have implemented a simple task management service with three API facades: REST, gRPC and GraphQL. We have also discussed various Coherence CE features along the way.\n\nBut does it work?\n\nRunning the server\n\nThe easiest way to run the server is to simply run it within your IDE of choice, which in my case is IntelliJ IDEA (see Figure 2):\n\nFigure 2. Server run configuration in IntelliJ IDEA\n\nOnce you have the run configuration defined, click the Run button and Helidon will bootstrap both its own web server, which will serve the REST API, and a Coherence CE cluster member that will be used to store the data.\n\nWait…What!? That is correct: Coherence CE is not a server that needs to be started, unlike most data stores you may already be familiar with from relational databases, such as Oracle Database and MySQL, or NoSQL key-value data stores such as Mongo and Redis. It is a Java library, which can be easily embedded into any Java application! (It can even be embedded into Node.js applications using GraalVM.)\n\nThat’s not to say that there is nothing that needs to be started. Coherence CE (the library) provides a set of services that need to be started in order for it to form the cluster and manage data, for example, the Cluster service and a PartitionedService. However, because these services are essentially just Java classes that implement a com.tangosol.util.Service interface, they can be easily started programmatically within the application.\n\nThat’s exactly what happens here.\n\nThe Helidon io.helidon.microprofile.cdi.Main class will simply configure logging and bootstrap the CDI container. The CDI container will then look for all the available CDI extensions, and it will discover both the Helidon Web Server extension and the Coherence CE extension, among others. Both of these will observe an @Initialized event that will be fired by the CDI container once it’s ready for use and will then start the Helidon Web Server and the Coherence CE server, respectively.\n\nThis architecture provides two significant benefits:\n\nIt allows you to reduce complexity by effectively combining an application serverand a data store into a single stateful application server. This makes the architecture much simpler, because there are fewer elements to provision, scale, monitor, and manage. This tends to work quite well for simple applications and services. That is what I mean when I talk about the combination of Helidon and Coherence CE as a stateful microservices platform.\n\nThe architecture makes it easy to debug the code that executes within Coherence CE, such as entry processors, especially now that the actual, fully documented source code for Coherence CE is available on GitHub and on Maven Central.\n\nNow, that doesn’t mean you always have to run the application that way. You can certainly choose to split the application server and the data store using Coherence CE roles and then manage them separately, but this is truly a deployment-time decision, not a development-time one. You can develop the application, run it within the IDE as a single process for testing and debugging, and even package it into a single Docker image. Then later you can choose to run separate app server and storage tiers by changing how you deploy the application to Kubernetes.\n\nYou can also run different microservices in a single Coherence CE cluster, or in multiple clusters, or even as a quasi-monolith by running all the services in every cluster member. The possibilities are endless, and the best option depends on your particular use case.\n\nHowever, it is hard to overstate how much simpler the typical developer workflow is by simply embedding the data store into the app. Try it and you’ll see.\n\nGo back to actually running the To Do List server. If you clicked on the Run button earlier, you should see log output similar to the following towards the end:\n\n2020.07.30 05:55:53 INFO io.helidon.microprofile.server.ServerCdiExtension Thread[main,5,main]: Server started on http://localhost:7001 (and all other host addresses) in 12215 milliseconds (since JVM startup).\n\nThat output means the Helidon Web Server was successfully started and can be accessed on port 7001. You should also see the following a bit higher up in the log:\n\n2020.07.30 05:55:53 CONFIG org.glassfish.jersey.server.ApplicationHandler Thread[main,5,main]: Jersey application initialized.\n\nRoot Resource Classes:\n\ncom.oracle.coherence.examples.todo.server.rest.ToDoListRestApi\n\nThat output means the JAX-RS resource was discovered and deployed by Helidon.\n\nFinally, you should be able to scroll a bit higher up and find the following:\n\n2020.07.30 05:55:52 INFO coherence Thread[Logger@9258732 20.06,3,main]: (thread=DefaultCacheServer, member=1, up=10.634):\n\nServices\n\n(\n\n// omitted for brevity\n\n)\n\nStarted DefaultCacheServer...\n\nThat output means the Coherence CE server was started successfully and the application can actually store data in Coherence.\n\nMuch more information is logged by both Helidon and Coherence CE, so feel free to explore. However, these three sections in the log are critical, and if they are present, the To Do List service is ready to use.\n\nTesting the REST API\n\nI could’ve implemented automated tests for the REST API using JUnit and REST Assured, but for the time being, I use the curl tool to see if everything works as expected.\n\nThere’s no data in the application yet, so I’ll create some tasks using the POST endpoint:\n\n$ curl -i -X POST -H \"Content-Type: application/json\" \\\n\n-d '{\"description\": \"Learn Coherence\"}' \\\n\nhttp://localhost:7001/api/tasks\n\nHTTP/1.1 204 No Content\n\n$ curl -i -X POST -H \"Content-Type: application/json\" \\\n\n-d '{\"description\": \"Write an article\"}' \\\n\nhttp://localhost:7001/api/tasks\n\nHTTP/1.1 204 No Content\n\nBased on the HTTP response codes, that seems to be working. To check, run this command:\n\n$ curl -i -X GET -H \"Accept: application/json\" \\\n\nhttp://localhost:7001/api/tasks\n\nHTTP/1.1 200 OK\n\nContent-Type: application/json[{\"completed\":false,\"createdAt\":1596105616507,\"description\":\"Learn Coherence\",\"id\":\"9c6d9a\"}\n\n,{\"completed\":false,\"createdAt\":1596105656378,\"description\":\"Write an article\",\"id\":\"a3f764\"}]\n\nThat output looks good. Can this application complete a task?\n\n$ curl -i -X PUT -H \"Content-Type: application/json\" \\\n\n-d '{\"completed\": true}' \\\n\nhttp://localhost:7001/api/tasks/a3f764\n\nHTTP/1.1 200 OK\n\nContent-Type: application/json{\"completed\":true,\"createdAt\":1596105656378,\"description\":\"Write an article\",\"id\":\"a3f764\"}\n\nAwesome. The output shows that worked as well.\n\nConclusion\n\nThe article created a To Do List application using Coherence CE and Helidon, and it appears to be working correctly. I’ll leave the deletion of completed tasks and the deletion of tasks by ID as an exercise for readers, but you get the idea: you can store, update, delete, and retrieve tasks from Coherence CE using the REST, gRPC and GraphQL APIs developed earlier.\n\nAlthough everything is working fine as long as you have the server up and running, you’ll notice that if you shut down the server you will lose all the data.\n\nThis is expected, because you haven’t configured Coherence CE to persist data to disk yet. So at the moment, data is stored only in memory and it’s lost if you restart the cluster, which currently has a single member. (I know: It’s not much of a “cluster” at this point, is it?)\n\nOne way to fix that is by starting more members, which will automatically enable high-availability mode and create backups of the tasks. Unfortunately, I can’t do that at the moment. Any additional members would fail to start because of TCP bind exceptions, because Helidon will attempt to start the web server on the same port (7001). That problem will go away in the final article of this series, which deploys the application to Kubernetes.\n\nYou can also prevent data loss on restart by enabling disk persistence, which can be accomplished by passing the -Dcoherence.distributed.peristence.mode=active system property on the command line, but I wouldn’t worry about it for now. It is faster to start the server with persistence disabled, and it’s usually simpler to start testing with a clean slate.\n\nIt is now time to build a decent looking front end for the application, so nobody will have to use curl to manage tasks ever again. However, I’ll leave that for the next article in this series."
    }
}
{
    "id": "dbpedia_6759_0",
    "rank": 72,
    "data": {
        "url": "https://www.mdpi.com/2076-328X/14/8/692",
        "read_more_link": "",
        "language": "en",
        "title": "Dimensions of Alexithymia and Identification of Emotions in Masked and Unmasked Faces",
        "top_image": "https://pub.mdpi-res.com/behavsci/behavsci-14-00692/article_deploy/html/images/behavsci-14-00692-g001-550.jpg?1723187381",
        "meta_img": "https://pub.mdpi-res.com/behavsci/behavsci-14-00692/article_deploy/html/images/behavsci-14-00692-g001-550.jpg?1723187381",
        "images": [
            "https://pub.mdpi-res.com/img/design/mdpi-pub-logo-black-small1.svg?da3a8dcae975a41c?1723640743",
            "https://pub.mdpi-res.com/img/design/mdpi-pub-logo-black-small1.svg?da3a8dcae975a41c?1723640743",
            "https://pub.mdpi-res.com/img/journals/behavsci-logo.png?8600e93ff98dbf14",
            "https://pub.mdpi-res.com/bundles/mdpisciprofileslink/img/unknown-user.png?1723640743",
            "https://www.mdpi.com/bundles/mdpisciprofileslink/img/unknown-user.png",
            "https://pub.mdpi-res.com/img/design/orcid.png?0465bc3812adeb52?1723640743",
            "https://www.mdpi.com/bundles/mdpisciprofileslink/img/unknown-user.png",
            "https://www.mdpi.com/bundles/mdpisciprofileslink/img/unknown-user.png",
            "https://www.mdpi.com/behavsci/behavsci-14-00692/article_deploy/html/images/behavsci-14-00692-g001-550.jpg",
            "https://www.mdpi.com/behavsci/behavsci-14-00692/article_deploy/html/images/behavsci-14-00692-g001.png",
            "https://www.mdpi.com/behavsci/behavsci-14-00692/article_deploy/html/images/behavsci-14-00692-g002-550.jpg",
            "https://www.mdpi.com/behavsci/behavsci-14-00692/article_deploy/html/images/behavsci-14-00692-g002.png",
            "https://pub.mdpi-res.com/img/table.png",
            "https://pub.mdpi-res.com/img/table.png",
            "https://pub.mdpi-res.com/img/table.png",
            "https://pub.mdpi-res.com/img/table.png",
            "https://pub.mdpi-res.com/img/table.png",
            "https://pub.mdpi-res.com/img/table.png",
            "https://pub.mdpi-res.com/img/design/mdpi-pub-logo-white-small.png?71d18e5f805839ab?1723640743"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Thomas Suslow",
            "Anette Kersting",
            "Charlott Maria Bodenschatz",
            "Charlott Maria"
        ],
        "publish_date": "2024-08-09T00:00:00",
        "summary": "",
        "meta_description": "Alexithymia, a multifaceted personality construct, is known to be related to difficulties in the decoding of emotional facial expressions, especially in case of suboptimal stimuli. The present study investigated whether and which facets of alexithymia are related to impairments in the recognition of emotions in faces with face masks. Accuracy and speed of emotion recognition were examined in a block of faces with and a block of faces without face masks in a sample of 102 healthy individuals. The order of blocks varied between participants. Emotions were recognized better and faster in unmasked than in masked faces. Recognition performance was worst and slowest for participants starting the task with masked faces. In the whole sample, there were no correlations of alexithymia facets with accuracy and speed of emotion recognition for masked and unmasked faces. In participants starting the task with masked faces, the facet externally oriented thinking was positively correlated with reaction latencies of correct responses for masked faces. Our findings indicate that an externally oriented thinking style could be linked to a less efficient identification of emotions from faces wearing masks when task difficulty is high and support the utility of a facet approach in alexithymia research.",
        "meta_lang": "en",
        "meta_favicon": "https://pub.mdpi-res.com/img/mask-icon-128.svg?c1c7eca266cd7013?1723640743",
        "meta_site_name": "MDPI",
        "canonical_link": "https://www.mdpi.com/2076-328X/14/8/692",
        "text": "Department of Psychosomatic Medicine and Psychotherapy, University of Leipzig Medical Center, 04103 Leipzig, Germany\n\n*\n\nAuthor to whom correspondence should be addressed.\n\nBehav. Sci. 2024, 14(8), 692; https://doi.org/10.3390/bs14080692\n\nSubmission received: 7 June 2024 / Revised: 19 July 2024 / Accepted: 5 August 2024 / Published: 9 August 2024\n\nAbstract\n\n:\n\nAlexithymia, a multifaceted personality construct, is known to be related to difficulties in the decoding of emotional facial expressions, especially in case of suboptimal stimuli. The present study investigated whether and which facets of alexithymia are related to impairments in the recognition of emotions in faces with face masks. Accuracy and speed of emotion recognition were examined in a block of faces with and a block of faces without face masks in a sample of 102 healthy individuals. The order of blocks varied between participants. Emotions were recognized better and faster in unmasked than in masked faces. Recognition performance was worst and slowest for participants starting the task with masked faces. In the whole sample, there were no correlations of alexithymia facets with accuracy and speed of emotion recognition for masked and unmasked faces. In participants starting the task with masked faces, the facet externally oriented thinking was positively correlated with reaction latencies of correct responses for masked faces. Our findings indicate that an externally oriented thinking style could be linked to a less efficient identification of emotions from faces wearing masks when task difficulty is high and support the utility of a facet approach in alexithymia research.\n\n1. Introduction\n\nThe accurate identification of emotions in other people’s faces is crucial for social life, since it promotes smooth social interactions and helps to build social relations and bonds [1,2,3]. The perception of facial emotions is complex and appears to involve a number of different processes and brain systems [4,5]. It is assumed that the identification of emotional facial expressions could be based on the processing of single or multiple features of a face, which are diagnostically relevant for an emotion [6,7]. There is also evidence that holistic processing is used for the recognition of emotional facial expressions [8]. It has also been argued that the ability to recognize facially expressed emotions of other individuals relies, at least in part, on processes which simulate the same emotional state in the observer [9]. This means, when people perceive an emotional facial expression, they partially activate the respective emotion in themselves, providing a basis for the recognition of that emotion [10]. In general, accuracy of facial emotion recognition is rather high for most of the basic emotions [11,12] and linked to their frequency in everyday life [13]. The relevance of recognizing emotions in others’ faces makes the identification of personality factors influencing facial emotion recognition highly important.\n\nDuring the COVID-19 pandemic, social life has been shaped by containment measures such as mask wearing and social distancing. Face masks reduce viral transmission and have been an important component in preventing the spread of COVID-19 [14]. However, the wearing of face masks has adverse effects on the capacity to read facial expressions, making it more difficult to recognize quality and intensity of other people’s emotions [15,16]. In recent years, substantial evidence has accumulated that face masks hamper the recognition of basic emotions such as happiness, sadness, disgust, and fear [17,18,19,20].\n\nAlexithymia is a multifaceted personality trait primarily defined by difficulties in identifying and verbalizing one’s feelings and an externally oriented thinking style [21]. It has been shown that individuals with alexithymic characteristics do not have only difficulties in identifying their own emotions but also those of other people. Alexithymia has been found to be linked to impairments in recognizing others’ emotional facial expressions even when these expressions are very intense and displayed for a long duration [22,23]—but see Pandey and Mandal [24] and McDonald and Prkachin [25] for null results. Findings concerning alexithymia-related difficulties in perceiving emotions in faces are more consistent in research with suboptimal presentation of facial stimuli, i.e., showing faces with temporal constraints (for example for only 66 or 100 ms) [26,27,28] or in degraded quality (as blurred images) [29]. According to the systematic review of Grynberg et al. [30] alexithymic individuals’ difficulties in decoding facial emotions are neither limited to a specific emotional valence nor a specific emotional quality. That is, alexithymia appears to be associated, for example, with impairments in identifying facial happiness as well as with difficulties in recognizing facial sadness, fear, anger, or disgust.\n\nIn a recent integrative review of the literature on cognitive–emotional processing in alexithymia, Luminet et al. [31] underline the importance of considering the alexithymia facets separately. There is evidence for differential relationships of the alexithymia facets with cognitive–emotional functioning across various domains (attention, appraisal, memory, language, and behavior). The influence of the externally oriented thinking facet seems rather distinct from the influence of difficulties in identifying and describing feelings. Externally oriented thinking appears to be a central facet involved in emotion processing impairments, in particular, concerning attentional and appraisal processes [31,32] (see also [27,29,33,34]).\n\nThree previous studies based on samples of non-clinical individuals have so far investigated the effect of alexithymia on emotion identification in faces with face masks. Fuchs et al. [35] compared highly alexithymic with non-alexithymic individuals in an emotion recognition task in which masked faces expressing happiness, fear, and disgust were presented. Study participants were classified as alexithymic or non-alexithymic using the cut-offs provided by Bagby and Taylor [36]. In the former study [35], no group differences were observed for accuracy and speed of emotion recognition in masked faces. Verroca et al. [37] focused their dimensional analysis on recognition accuracy and examined alexithymia as a unitary construct, i.e., using the total alexithymia score. In their recognition task, which was conducted online, participants had to identify six basic emotions (i.e., happiness, surprise, anger, disgust, fear, and sadness) in blocks of unmasked and masked faces with a randomized order of blocks across participants. Verroca et al. [37] found no association between overall alexithymia and accuracy in identifying emotions from masked faces. The only investigation that followed a facet approach of alexithymia was conducted by Maiorana et al. [38]. In this study, masked faces with expressions of happiness, anger, and sadness were presented as stimuli. The authors found no correlations between alexithymia facets and speed of emotion identification in faces with masks. The null findings in this study might be due to a rather small sample size (N = 31). In addition, it can be argued that the authors presented different types of emotional facial stimuli to their participants (unmasked faces, masked faces, lower face parts, upper face parts) in a counterbalanced order (in a block format) but did not consider the effect of order on the relation between alexithymia and emotion recognition in masked faces.\n\nThe main objective of the present study was to explore the recognition of emotions in faces with a face mask as a function of alexithymia facets. In our study, we administered a block of faces with face masks and as a control condition a block of faces without face masks. It was expected that face masks have a negative impact on the recognition of facial emotions. In the present study, we varied the order of blocks between participants. First, as in the previous dimensional alexithymia studies [37,38], we analyzed emotion recognition in the whole sample regardless of the order of blocks in which masked and unmasked faces were presented. Second, we conducted separate correlation analyses for individuals who started the recognition task with masked faces and those who started with unmasked faces.\n\nIt can be assumed that when participants are first confronted with the identification of emotions in masked faces the task would be more difficult for them compared to the condition in which participants are first confronted with the identification of emotions in unmasked faces. In the latter case, participants become familiar with the task-relevant emotion qualities and the response format. It can be hypothesized that the identification of masked facial emotions is most difficult when the task starts with masked faces. As the duration of facial stimulus presentation was rather long in our study (with a maximum of 5 s) we expected that alexithymia would be related to a slowing of response latencies. The association between alexithymia and response latency should be most pronounced for the most difficult task condition, i.e., masked faces presented as the first experimental block. In our emotion recognition task, five different categories of facial expressions were shown, i.e., happy, sad, fearful, disgusted, and neutral faces.\n\nFollowing the recommendations of a recent review [31] we administered a dimensional facet approach to examine processes of facial emotion recognition in alexithymia. Results from previous research indicate that the facet externally oriented thinking is primarily involved in emotion processing impairments [27,29,31]. Against this background, we assumed that especially externally oriented thinking is related to difficulties in facial emotion recognition. We assessed and controlled participants’ dispositional anxiety and level of depressive symptoms since these variables have been found to be associated with alexithymia [39,40]. Participants’ intelligence was measured in our study as it can influence decoding of facial emotions [41]. Since we were interested in the speed of emotion recognition, which involves motor responses by pressing buttons on a keyboard, the Trail Making Test Part B [42] was used to assess visuomotor processing speed in a task with non-emotional stimuli.\n\n2. Materials and Methods\n\n2.1. Participants\n\nOne hundred and five young volunteers participated in the present study. The final sample consisted of one hundred and two participants (seventy women) with a mean age of 24.05 years (SD: 4.25) and a mean school education of 12.21 years (SD: 1.01). Three participants were removed from the analyses because of slow response times in the emotion recognition task (see for details Section 2.5 below). Participants were recruited via public notices and online advertisements. All participants were native German speakers and had normal vision as tested with a Snellen eye chart. According to self-report they were free of any lifetime history of psychiatric or neurological disorders and did not use psychotropic medication. About three quarters of the participants were university students from various disciplines. The study protocol was approved by the local ethics committee. Before the experiment, informed written consent was obtained from each participant.\n\n2.2. Questionnaires and Tests\n\nThe 20-Item Toronto Alexithymia Scale (TAS-20) is a self-report measure that assesses three facets of alexithymia [43,44]: difficulty identifying feelings (DIF), difficulty describing feelings (DDF), and externally oriented thinking (EOT). Items are rated on a 5-point scale. The total alexithymia score is the sum of responses to all 20 items with a range of possible scores from 20 to 100. Higher TAS-20 scores indicate higher alexithymia. In the present sample, Cronbach’s alphas for the subscales DIF, DDF, and EOT and the total score were 0.75, 0.85, 0.64, and 0.85 which were similar to the coefficients reported by Bagby et al. [43] for a sample of university students: 0.79, 0.75, 0.66, and 0.80.\n\nThe Beck Depression Inventory (BDI-II) is a 21-item, self-rated scale that assesses characteristic attitudes and symptoms of depression during the preceding two weeks [45,46]. Each item is rated on a 4-point scale ranging from 0 to 3. The maximum total score of the BDI-II is 63. Cronbach’s alpha was 0.77 for the BDI-II in our sample.\n\nThe State-Trait Anxiety Inventory (STAI) trait version was administered to assess participants’ trait anxiety [47,48]. Trait anxiety refers to a general tendency to respond with anxiety to perceived threats and is a relatively stable personality characteristic. The STAI contains 20 items that are scored on a 4-point scale (from 1 to 4). Total scores range between 20 and 80. In our study, Cronbach’s alpha was 0.89 for the STAI.\n\nThe Trail Making Test Part B (TMT-B) assesses executive functioning, visual attention, and visuomotor processing speed [42]. In this paper-and-pencil test, participants have to connect numbers and letters in ascending order (twenty-five items in all). The TMT is scored by how long it takes to complete the test (in seconds). Higher scores indicate slower switching between numbers and letters.\n\nThe Mehrfachwahl-Wortschatz-Intelligenztest Version B (MWT-B) is a multiple-choice vocabulary intelligence test [49] that measures aspects of general intelligence, specifically crystallized, verbal intelligence. The MWT-B consists of 37 items. Each item comprises one real word and four pronounceable pseudo-words. The subject has the task to identify and cross out the real word. Sum scores of correct responses are calculated and can be converted to IQ scores.\n\n2.3. Emotion Recognition Task: Stimuli and Procedure\n\nFacial stimuli comprised 100 color photographs of twenty young models (ten women) from the MPI FACES database [50]. Each model was shown with five different facial expressions (happy, sad, fearful, disgusted, and neutral). Ten models (five women) were presented without a mask as original FACES images. The photos of ten other models (five women) were digitally adapted by superimposing a light blue mask on the original faces. The masks were adapted to match the width and length of the faces so that they covered faces from the upper nose downwards. The display size of each face photo on the screen was 10.2 cm high × 8 cm wide.\n\nThe emotion recognition task consisted of 100 trials divided into two experimental blocks. In one block, faces without a mask were shown (50 trials) and, in another block, faces with a mask were displayed (50 trials). About half of the sample saw first the block with the original (unmasked) faces and then the block with the faces wearing masks (n = 54, “start with unmasked”). The other half of the sample saw the blocks in reverse order (n = 51, “start with masked”). At the beginning of the task, participants were instructed that they would see faces on the screen expressing fear, disgust, sadness, or happiness. They were also informed that some faces in the task have neutral expressions. Participants were told to identify the expressed emotions and to respond primarily as accurately but also as quickly as possible. We registered responses and reaction latencies of participants. Experimental trials in each block were presented in an individual random order.\n\nEach trial in the emotion recognition task had the same structure: after the appearance of a central fixation cross, which was shown for 1000 ms, a face stimulus was displayed for a maximum of 5000 ms. Participants responded by button press in a forced choice manner. Responses were given on a keyboard using the number keys 1 (happiness), 2 (sadness), 3 (neutral), 4 (disgust), 5 (fear). Each emotion was assigned to one key. The expression categories and the assigned numbers were shown at the bottom of the screen in black letters during the entire experiment.\n\nDuring the task participants were seated in a chair at approximately 60 cm in front of the screen. The computer-based stimulus presentation and response registration were realized via PsychoPy 3 version 2020.2.4 [51] on a Dell Latitude E6540 with a 15.6-inch screen.\n\n2.4. General Procedure\n\nAt the beginning of the experiment, participants completed a sociodemographic questionnaire and performed a vision test (Snellen eye chart). Subsequently, participants performed the computer-based emotion recognition task. At the end of the session, the questionnaires and tests were administered in a fixed order: BDI-II, STAI trait, MWT-B, TAS-20, and TMT-B. All testing sessions were conducted individually in a quiet laboratory room.\n\n2.5. Statistical Analyses\n\nThe dependent variables in the emotion recognition task were accuracy and response time. Accuracy was scored as the number of correctly identified emotional expressions. Response times were calculated for all correct trials across expression conditions, resulting in an overall response latency score for masked and unmasked faces. In addition, we calculated the mean number of hits and the mean response latency for correct responses for each expression condition. The 105 participants of our study gave 10,500 responses in the emotion recognition task. Of them, 9694 responses were correct (92.3%). Response latencies were in no trial below 500 ms. Thus, there was no evidence for unrealistically fast decisions. We excluded very slow responses or responders from data analysis. Thirteen response latencies were above 10 s and were removed from further data processing. To identify slow responders, we analyzed the resulting overall response latencies of all participants. A criterion of three standard deviations from the mean was used to find outliers. The upper cut-off score was 3278 ms for the overall response latency for unmasked faces and 3606 ms for the overall response latency for masked faces. In this way, three participants were identified as slow responders (two women, one man) and excluded from data analysis so that our final sample consisted of one hundred and two individuals. In our study, no participant exhibited overall response latency scores, which were three standard deviations below the means.\n\nKolmogorov–Smirnov tests were applied to assess normality of distribution. There was a significant departure from normality for the majority of the investigated variables (i.e., DIF, DDF, EOT, BDI-II, STAI-T, MWT-B, TMT-B, overall hits (unmasked faces), overall hits (masked faces), RT (masked faces), all ps < 0.05). Against this background, we administered non-parametric tests to examine the effect of face masks on accuracy and speed of correct facial emotion recognition (Wilcoxon tests for paired samples). To examine the effect of order of presentation on accuracy and speed of emotion recognition we used Mann–Whitney U tests comparing the participants who started the task with masked faces with the participants who started with unmasked faces. Mann–Whitney U tests were also used to compare these two participants groups on the questionnaire (TAS-20, BDI-II, and STAI-T) and test variables (TMT-B and MWT-B). Spearman rank correlation analysis was performed to examine the relationships between TAS-20, measures of negative affect, visuomotor processing speed, intelligence, and performance in the emotion recognition task. All calculations were made with SPSS 29.0 (IBM Corp., Armonk, NY, USA). Results were considered significant at p < 0.05, two-tailed. To adjust for multiple testing in our main correlation analyses concerning the associations of alexithymia facets with emotion recognition parameters, we used a conservative significance threshold of 0.05/24 = 0.00208 (i.e., dividing the conventional p-level by the product of the two masking conditions (masked, unmasked), the two block sequences (start with masked, start with unmasked), the two recognition parameters (accuracy, latency), and the three alexithymia facets (DIF, DDF, and EOT)). In these correlation analyses, one-tailed testing was applied because the hypotheses had a direction.\n\nWe calculated an (a priori) power analysis with the program G*Power 3.1 [52] to determine the required sample size to detect correlations between the alexithymia facet externally oriented thinking and emotion recognition in masked faces. We based our analysis on the large effect size of 0.58 observed in healthy individuals for the relation between externally oriented thinking and recognition of emotional facial expressions in degraded faces (see Kätsyri et al. [29], Table 2). To detect a medium effect of r = 0.58 with an alpha value of 0.05, two-tailed, and a statistical power of 0.95, the required sample size is 28 (type of power analysis: a priori—compute required sample size given α, power, and effect size). Thus, the number of participants in the subgroups of our study should be at least 28.\n\n3. Results\n\n3.1. Emotion Recognition Performance\n\nParticipants’ recognition performance (number of correct responses or hits) is presented in Figure 1 as a function of masks and order of blocks. We analyzed in the whole sample whether number of correct responses differed between the masked and the unmasked face condition. According to the results of a Wilcoxon test, the number of correctly identified facial emotions was significantly higher in the unmasked (mean: 9.55, SD: 0.39) compared to the masked face condition (mean: 8.91, SD: 0.48), Z = −7.85, p < 0.001.\n\nAccording to the results of a Mann–Whitney U test, participants starting with masked faces showed no differences in number of hits for unmasked faces compared to participants starting with unmasked faces, Z = −1.31, p = 0.19. In contrast, participants starting with masked faces had a lower number of hits for masked faces compared to those starting with unmasked faces, Z = −2.86, p < 0.005.\n\nResponse times for correct responses in the emotion recognition task are shown in Figure 2 as a function of masks and order of blocks. In the whole sample, response latencies of correct responses differed between the masked and the unmasked face condition. Response times were significantly lower in the unmasked (mean: 1.96 s, SD: 0.35) compared to the masked face condition (mean: 2.15 s, SD: 0.39), Z = −4.11, p < 0.001.\n\nParticipants starting with unmasked faces were slower in identifying emotions in unmasked faces compared to participants starting with masked faces, Z = −5.51, p < 0.001. Moreover, participants who began the recognition task with unmasked faces were faster to identify emotions in masked faces compared to those who started with masked faces, Z = −3.01, p < 0.005. Interestingly, results from Wilcoxon tests indicated that response times were faster in the second block irrespective of masking type encountered in the first block. In individuals starting with unmasked faces response latencies were higher for unmasked faces than for masked faces, Z = −2.76, p < 0.01, whereas in individuals starting with masked faces response latencies were higher for masked faces than for unmasked faces, Z = −6.09, p < 0.001 (see Figure 2).\n\n3.2. Correlations of Alexithymia with Psychological Tests\n\nThe correlation analysis indicated that the sum score and the subscales DIF and DDF of the TAS-20 were positively related to depressed symptoms as assessed by the BDI-II (see Table 1 for details). Moreover, the sum score and all subscales of the TAS-20 were positively correlated with trait anxiety (STAI-T). The sum score and the subscales DIF and DDF of the TAS-20 showed negative correlations with the TMT-B. Finally, no significant correlations were observed between TAS-20 scales and intelligence (MWT-B) (see Table 1).\n\n3.3. Correlations of Alexithymia with Recognition Performance\n\nThe results of Spearman rank correlations between TAS-20 scales and emotion recognition are shown in Table 2. In the total sample, when administering the adjusted p-level of 0.00208 we found no significant correlations of the TAS-20 sum score and the scales DIF, DDF, and EOT with the overall number of correct responses and the overall response latencies for the masked and unmasked expression conditions.\n\n3.4. Correlations of Affectivity, Psychomotor Functioning, and Intelligence with Recognition Performance\n\nNo correlations were found between trait anxiety, psychomotor functioning, intelligence, and the overall number of hits and the overall response latencies for unmasked and masked faces. Participants’ level of depressive symptoms as assessed by the BDI-II was positively correlated with RT for correct responses in the unmasked condition but not with RT in the masked condition (see Table 3).\n\n3.5. Comparisons between Participants Starting with Unmasked Faces and Participants Starting with Masked Faces Concerning Alexithymia, Affectivity, Psychomotor Functioning, and Intelligence\n\nAccording to the results of Mann–Whitney U tests, participants starting with unmasked faces in the emotion recognition task (n = 52; 34 women) did not differ from participants starting with masked faces (n = 50; 36 women) on the TAS-20 scales (see Table 4). Moreover, no group differences were observed for level of depressive symptoms, trait anxiety, and intelligence. Participants starting with unmasked faces manifested slower psychomotor functioning compared to those starting with masked faces (see Table 4).\n\n3.6. Correlations of Alexithymia with Recognition Performance in Participants Starting with Unmasked Faces and Participants Starting with Masked Faces\n\nIn the sample of participants who started the emotion recognition task with unmasked faces, there were no significant correlations of the TAS-20 scales with the overall number of correct responses and the overall response latencies for the unmasked and masked expression conditions (see Table 5). In the sample of participants starting with the masked faces, there were also no correlations of the TAS-20 scales with the overall number of correct responses for the unmasked and masked expression conditions. However, EOT showed a significant positive correlation with overall response latencies for the masked expression condition (see Table 5).\n\nAdditional correlation analyses were carried out in the sample of participants who started with masked faces to explore the relationships between EOT and response latencies for the five masked emotional expression conditions (see Table 6). There was only a significant positive correlation of EOT with response time for masked disgusted faces. No other significant correlations were observed.\n\nTo compare response latencies between masked facial expression conditions in participants starting with masked faces we conducted Wilcoxon tests. The results indicated that response time for correctly identified masked disgust was significantly longer than the response times for correctly identified masked neutral, happy, fearful, and sad expressions (all ps < 0.05). Moreover, response time for correctly identified masked sadness was significantly longer than the response times for correctly identified masked neutral and happy facial expressions (ps < 0.001) but did not differ from response time for masked fear. This means that when considering response latency of correct responses masked disgust was the most difficult emotion to identify followed by masked sadness and masked fear.\n\nFinally, we compared hit rates between masked facial expression conditions in participants starting with masked faces using Wilcoxon tests. Hit rate for masked sad faces (6.88 (SD = 1.42)) was significantly lower than hit rates for masked neutral (9.94, SD = 0.24), fearful (9.30, SD = 1.33), disgusted (8.84, SD = 1.48), and happy faces (8.82, SD = 0.66) (all ps < 0.001). Hit rate for masked disgust did not differ from hit rates for masked happy and fearful facial expressions. Thus, when considering the number of correct responses masked sadness was the most difficult emotion to identify in the emotion recognition task.\n\n4. Discussion\n\nThe correct identification of emotions in facial expressions is important for successful interaction with other people [1,2,3]. Since the start of the COVID-19 pandemic, face masks were used to prevent the spreading of the coronavirus and became part of public life. Unfortunately, the occlusion of the mouth and nose has an adverse impact on facial emotion recognition. The identification of basic emotions in other people’s facial expressions is substantially worse for masked compared to unmasked faces though it leaves inferring basic emotional expressions clearly above chance level [18,19]. The covering of the lower face hides facial features relevant for emotion recognition [7,53] and it could also disrupt holistic face processing [16]. The data of our study clearly confirm that emotions are recognized better and faster in unmasked faces than in masked faces. Moreover, we observed that recognition performance was worst and slowest for participants who started the task with masked faces.\n\nIn the current study, we examined whether alexithymia dimensions are associated with difficulties in the recognition of emotions in faces with a face mask. We focused in our analyses on alexithymia facets since results from previous research on cognitive–emotional processing in alexithymia highlighted the utility of a facet approach [31,32]. In the whole sample, we found no correlation of alexithymia facets with accuracy or speed of emotion recognition for masked and unmasked faces in the emotion recognition task. Our null findings concerning recognition speed in the whole sample correspond to those observed in the only previous investigation, which followed a facet approach to study the relations between alexithymia and emotion decoding in faces wearing masks [38]. In this study, no correlations were detected between alexithymia facets and speed of emotion identification in masked faces. Interestingly, Maiorana et al. [38] presented different types of emotional facial stimuli, i.e., unmasked faces, masked faces, lower face parts, and upper face parts, in a counterbalanced block format. However, they pooled the recognition data for the conditions across block positions and did not analyze the effect of order on recognition performance. It is likely that participants who began the task with emotion recognition in masked faces responded more slowly compared to those who encountered the masked face stimuli at the end of the recognition task. The aggregation of data across block positions could have made it more difficult to reveal associations between alexithymia facets and speed of emotion decoding. Future research on alexithymia and emotion perception should pay more attention to the difficulty and variability of difficulty in emotion recognition tasks with multiple stimulus conditions, in particular, when samples with rather low levels of alexithymic characteristics are examined.\n\nConsistent with our hypothesis, we found externally oriented thinking to be linked to high response latencies for correct identifications of emotions in masked faces. This association was revealed in participants who started the task with masked faces, i.e., the most difficult task condition. This correlation was of medium to large effect size. Thus, the alexithymia facet externally oriented thinking seems to be linked to slower emotion recognition in faces wearing masks when participants are unprepared, i.e., when they are not familiar with the task. This relationship did not emerge for participants who started the task with the unmasked faces. It is conceivable that being familiar with the task requirements may attenuate impairments in emotion perception linked to externally oriented thinking. According to the results of additional analyses externally oriented thinking was found to be related to long response times in the recognition of masked disgust. Thus, high levels of externally oriented thinking could be associated with slowed recognition of masked disgusted expressions. However, it seems advisable to interpret this finding cautiously in terms of emotion specificity or valence. In our experiment, masked disgust was the most difficult emotional quality to detect when considering latencies of correct responses. Against this background it is possible that the correlations of externally oriented thinking with decoding of disgust in masked faces are due to their general recognition difficulty.\n\nImportantly, in our study individuals starting the recognition task with masked faces did not differ from those starting with unmasked faces with regard to alexithymic characteristics, level of depressive symptoms, trait anxiety, and intelligence. A group difference was revealed for psychomotor functioning. However, psychomotor functioning was not correlated with response latencies in the masked (and unmasked) expression conditions of our emotion recognition task.\n\nOverall, the present data suggest that externally oriented thinking could be associated with less efficient emotion processing in faces wearing masks. The covering of the lower face by face masks reduces the available information on the expressed emotions. For individuals high in externally oriented thinking such a shortage of available information seems to cause problems in rapidly identifying the correct emotional expression. This alexithymia facet could be characterized by less efficient reading out and use of perceptual and lexical emotional information in the labeling of facial expressions. Our results are in line with and confirm the conclusions of recent reviews in the research field that in particular externally oriented thinking is crucially connected with impairments in attentional and appraisal processes during emotion perception [27,31,32,33]. The observed rather high correlation between externally oriented thinking and emotion recognition performance in our study confirms the relation reported by Kätsyri et al. [29], who presented blurred faces briefly (with a duration varying from 0.8 to 2 s). Interestingly, in both studies facial stimuli were characterized by a removal of information on facial features which are assumed to be relevant for emotion recognition. In the study of Kätsyri et al. the removal of information by image degradation with a loss of details concerned the surface of the whole face, whereas in our study the lower half of the face was completely covered and not available for processes of emotion decoding. A substantial reduction in the amount of available visual information on facial features may lead, in particular in individuals high in externally oriented thinking, to a slowing of emotion recognition processes.\n\nIn a previous eye-tracking study [35], we compared identification of emotions in masked faces between highly alexithymic and non-alexithymic individuals. Fuchs et al. [35] found that highly alexithymic participants did not differ from non-alexithymic participants in accuracy and speed of emotion decoding. In the recognition task administered, facial stimuli were also selected from the FACES database and consisted of happy, disgusted, fearful, and neutral expressions. Different from the present study, sadness was not included as a facial expression category. In the study of Fuchs et al. and the present study, the masks administered resembled light blue surgical face masks and covered the faces from the upper nose downwards. The overall hit rate for masked faces in Fuchs et al.’s study (0.87, SD = 0.08) was similar to that observed in the present study (0.89, SD = 0.05). The overall response latency for correct responses for masked faces was substantially higher (3.55 s, SD = 0.46) than in the present study (2.15 s, SD = 0.39). This difference in response latency could be due to different response modes and/or differences in stimulus presentation time: in Fuchs et al.’s study, expression categories were shown on the screen for only 2 s and participants made their decisions by mouse clicking on response fields, while in the present study facial stimuli were presented for a maximum of 5 s and participants gave their answers by pressing number keys on a keyboard. The latter response mode and longer duration of stimulus presentation might have accelerated responding in the present study. In both studies, participants were young and healthy adult individuals. The mean age of participants in Fuchs et al.’s study (24.37, SD = 4.67) was similar to that in the present study (24.05, SD = 4.25). There was no overlap of samples between studies. The most important difference between studies concerned the analytical approach. Fuchs et al. followed a categorical approach of analysis treating alexithymia as a unitary construct and assigning selected participants to extreme groups (defined by presence or absence of alexithymia) based on the TAS-20 total score, whereas in the present study a dimensional facet approach of analysis was chosen. Highly alexithymic individuals can differ considerably from one another in the degree of specific facets and represent a rather heterogeneous group. Some alexithymic persons may be primarily characterized by difficulties in describing their feelings, while others can be primarily characterized by an externally oriented thinking style. The different patterns of results between studies indicate that a dimensional facet approach could be more useful to detect alexithymia-related impairments in emotion decoding than a categorical approach [35]. Recently, research has begun to pay more attention to analyzing the influence of individual facets of alexithymia through subscale scores, because facet scores allow clearer identification of which alexithymia features are particularly related to specific emotion processing impairments [31,32]. Therefore, it seems advisable to prioritize a facet approach over a categorical or total score approach in studies on alexithymia and emotion perception.\n\nIn our study, none of the alexithymia facets was related to recognition accuracy for masked and unmasked faces in the total sample or the subsamples. This finding could be due to the relatively long stimulus presentation time in our experiment, which may have contributed to the high overall percentage of correct responses in our recognition task (95.5% for unmasked faces and 89.1% for masked faces). Study participants had considerable time to make their decisions. It would be interesting to examine in future studies emotion identification under conditions of time pressure by using a response-window technique and short stimulus presentation times. Possibly, under such experimental conditions, emotion processing impairments associated with externally oriented thinking would become manifest in the form of errors. The present data indicate that, not surprisingly, order of presentation can affect task difficulty and especially in samples characterized by low levels of alexithymia it seems advantageous to administer difficult recognition tasks to increase the probability to detect alexithymia-related impairments in emotion decoding.\n\nEmotional facial expressions of other people provide useful clues to their motives, intentions, and attitudes [54]. The ability to read facial expressions is important for social functioning, as it helps to coordinate relationships with other people [55]. Unsurprisingly, deficits in decoding emotions from faces have been found to be linked to interpersonal difficulties and a reduced ability to create and to preserve positive social relationships [56,57]. There is evidence that an externally oriented thinking style goes along with impairments in interpersonal relations. Individuals high in externally oriented thinking have difficulties to empathize and appear self-centered [58,59]. Impaired perception of facial emotions might contribute to dysfunctional interpersonal relationships in alexithymic individuals. When communicating with people wearing face masks interpersonal misunderstanding may occur more frequently in externally oriented individuals and could exacerbate their social problems. Beyond the pandemic, our findings could be of importance for clinicians who use face masks in their everyday professional life. Successful nurse–patient or doctor–patient communications require correct and fast interpretation of emotional states. If clinicians encounter patients characterized by externally oriented thinking and have to communicate with them while wearing face masks, it could be advantageous to intensify or extend the duration of their facial emotions or to make greater use of vocal emotional communication and verbal descriptions of emotions.\n\nThe results of our study may point to a positive aspect concerning the modifiability of alexithymia-related difficulties in recognizing emotions from masked faces. Externally oriented thinking was not linked to emotion recognition in masked faces when immediately before emotion identification was “trained” using unmasked facial expressions. However, before firm conclusions can be drawn on this point, there is a need to further investigate the effect of training interventions on emotion recognition abilities in alexithymia. Smartphones represent flexible devices that can play an important role in training facial emotion identification. Recently, Lukas et al. [60] developed a psychological intervention to reduce alexithymia, which combines psychoeducation with a smartphone-based emotion recognition skills training.\n\nA limitation of our study is that alexithymia was only assessed with a self-report measure. Self-descriptive tests assessing alexithymia have been criticized, since they depend on the abilities to describe and attend to one’s feelings correctly [61]. However, over recent decades, research has provided considerable evidence for the reliability and validity of the TAS-20 [62]. Moreover, it can be argued that individuals with higher levels of education could be well aware of their own alexithymic characteristics due to the integration of negative social feedback concerning their difficulties in perceiving, feeling, and communicating emotions regarding their self-concept. It has to be mentioned as limitations that our study participants were healthy, young, and predominantly female individuals and that the mean TAS-20 alexithymia sum score was relatively low in our sample. This limits the generalizability of our results. Range-restriction effects associated with using a relatively homogeneous healthy sample could have reduced the magnitude of relationships observed. Another limitation is that our masked faces represent artificial stimuli, which may not have ecological validity. Our facial stimuli were created by graphically imposing an image of a blue face mask over original photos of emotional facial expressions taken from the MPI FACES database [50]. However, it is worth outlining that recently Grenville and Dwyer [63] compared the accuracy of emotion recognition from graphically created facial stimuli with that from stimuli where the emotional expressions were posed by people who wore face masks. Interestingly, the authors observed quite similar effects on emotion recognition for both types of facial stimuli. It is also a limitation of our investigation that static images of intense facial expressions were administered in our recognition experiment, yet emotional facial expressions in daily life are in general dynamic and frequently more subtle. Finally, it can be criticized that we showed different models in the two masking conditions of our emotion recognition task. To control for differences in expression intensity or purity of expression it would have been methodologically stronger to present the same facial stimuli across masking conditions.\n\nIn sum, the present study provides further evidence that occluding lower face parts by face masks has a negative impact on the recognition of basic emotions. The alexithymia facet externally oriented thinking seems to be related to a slowed identification of emotions from faces wearing masks when task difficulty is high. These difficulties in recognizing emotions could lead to less understanding of others’ states, attitudes, and intentions and contribute to the development of problems in interactions with mask-wearing people. For a comprehensive understanding of the impact face masks exert on emotion recognition it seems necessary to consider the personality trait alexithymia. The present findings suggest that a specific facet of alexithymia could be linked to impairments in decoding emotions from faces wearing masks, demonstrating the utility of a facet approach in alexithymia research.\n\nAuthor Contributions\n\nConceptualization, T.S., C.M.B. and A.K.; methodology, C.M.B. and T.S.; formal analysis, T.S.; investigation, C.M.B.; data curation, C.M.B. and T.S.; writing—original draft preparation, T.S.; writing—review and editing, C.M.B. and A.K.; visualization, T.S.; project administration, T.S.; funding acquisition, A.K. All authors have read and agreed to the published version of the manuscript.\n\nFunding\n\nThis research received no external funding.\n\nInstitutional Review Board Statement\n\nThe study was conducted in accordance with the Declaration of Helsinki and approved by the Ethics Committee of the Medical Faculty at the University of Leipzig (reference number: 18,614ek, date: 23 September 2020).\n\nInformed Consent Statement\n\nInformed consent was obtained from all subjects involved in the study.\n\nData Availability Statement\n\nThe datasets used and analyzed during the current study are available from the corresponding author on reasonable request.\n\nAcknowledgments\n\nWe thank Minh Nguyen and Luis Lichtenberg for their help in data acquisition.\n\nConflicts of Interest\n\nThe authors declare no conflicts of interest.\n\nReferences\n\nHareli, S.; Halhal, M.; Hess, U. Dyadic dynamics: The impact of emotional responses to facial expressions on the perception of power. Front. Psychol. 2018, 9, 1993. [Google Scholar] [CrossRef] [PubMed]\n\nNiedenthal, P.M.; Brauer, M. Social functionality of human emotion. Annu. Rev. Psychol. 2012, 63, 259–285. [Google Scholar] [CrossRef] [PubMed]\n\nFrith, C. Role of facial expressions in social interactions. Philos. Trans. R. Soc. Lond. B Biol. Sci. 2009, 364, 3453–3458. [Google Scholar] [CrossRef] [PubMed]\n\nBeaudry, O.; Roy-Charland, A.; Perron, M.; Cormier, I.; Tapp, R. Featural processing in recognition of emotional facial expressions. Cogn. Emot. 2014, 28, 416–432. [Google Scholar] [CrossRef]\n\nSchirmer, A.; Adolphs, R. Emotion perception from face, voice, and touch: Comparisons and convergence. Trends Cogn. Sci. 2017, 21, 216–228. [Google Scholar] [CrossRef] [PubMed]\n\nScheller, E.; Büchel, C.; Gamer, M. Diagnostic Features of Emotional Expressions Are Processed Preferentially. PLoS ONE 2012, 7, e41792. [Google Scholar] [CrossRef] [PubMed]\n\nSchurgin, M.W.; Nelson, J.; Iida, S.; Ohira, H.; Chiao, J.Y.; Franconeri, S.L. Eye movements during emotion recognition in faces. J. Vis. 2014, 14, 1–16. [Google Scholar] [CrossRef] [PubMed]\n\nTobin, A.; Favelle, S.; Palermo, R. Dynamic facial expressions are processed holistically, but not more holistically than static facial expressions. Cogn. Emot. 2016, 30, 1208–1221. [Google Scholar] [CrossRef]\n\nHeberlein, A.S.; Atkinson, A.P. Neuroscientific evidence for simulation and shared substrates in emotion recognition: Beyond faces. Emot. Rev. 2009, 1, 162–177. [Google Scholar] [CrossRef]\n\nRoss, P.; Atkinson, A.P. Expanding simulation models of emotional understanding: The case for different modalities, body-state simulation prominence, and developmental trajectories. Front. Psychol. 2020, 11, 309. [Google Scholar] [CrossRef] [PubMed]\n\nSurcinelli, P.; Andrei, F.; Montebarocci, O.; Grandi, S. Emotion recognition of facial expressions presented in profile. Psychol. Rep. 2022, 125, 2623–2635. [Google Scholar] [CrossRef] [PubMed]\n\nGoeleven, E.; De Raedt, R.; Leyman, L.; Verschuere, B. The Karolinska directed emotional faces: A validation study. Cogn. Emot. 2008, 22, 1094–1118. [Google Scholar] [CrossRef]\n\nCalvo, M.G.; Gutiérrez-García, A.; Fernández-Martín, A.; Nummenmaa, L. Recognition of facial expressions of emotion is related to their frequency in everyday life. J. Nonverb. Behav. 2014, 38, 549–567. [Google Scholar] [CrossRef]\n\nCatching, A.; Capponi, S.; Yeh, M.T.; Bianco, S.; Andino, R. Examining the interplay between face mask usage, asymptomatic transmission, and social distancing on the spread of COVID-19. Sci. Rep. 2021, 11, 15998. [Google Scholar] [CrossRef]\n\nEscelsior, A.; Amadeo, M.B.; Esposito, D.; Rosina, A.; Trabucco, A.; Inuggi, A.; Pereira da Silva, B.; Serafini, G.; Gori, M.; Amore, M. COVID-19 and psychiatric disorders: The impact of face masks in emotion recognition face masks and emotion recognition in psychiatry. Front. Psychiatry 2022, 13, 932791. [Google Scholar] [CrossRef] [PubMed]\n\nPavlova, M.A.; Sokolov, A.A. Reading covered faces. Cereb. Cortex 2022, 32, 249–265. [Google Scholar] [CrossRef] [PubMed]\n\nPazhoohi, F.; Forby, L.; Kingstone, A. Facial masks affect emotion recognition in the general population and individuals with autistic traits. PLoS ONE 2021, 16, e0257740. [Google Scholar] [CrossRef] [PubMed]\n\nGrundmann, F.; Epstude, K.; Scheibe, S. Face masks reduce emotion-recognition accuracy and perceived closeness. PLoS ONE 2021, 16, e0249792. [Google Scholar] [CrossRef] [PubMed]\n\nGrahlow, M.; Rupp, C.I.; Derntl, B. The impact of face masks on emotion recognition performance and perception of threat. PLoS ONE 2022, 17, e0262840. [Google Scholar] [CrossRef] [PubMed]\n\nLeitner, M.C.; Meurer, V.; Hutzler, F.; Schuster, S.; Hawelka, S. The effect of masks on the recognition of facial expressions: A true-to-life study on the perception of basic emotions. Front. Psychol. 2022, 13, 933438. [Google Scholar] [CrossRef] [PubMed]\n\nTaylor, G.J.; Bagby, R.M. An overview of the alexithymia construct. In Handbook of Emotional Intelligence; Bar-On, R., Parker, J.D.A., Eds.; Jossey-Bass: San Francisco, CA, USA, 2000; pp. 40–67. [Google Scholar]\n\nParker, J.D.; Taylor, G.J.; Bagby, R.M. Alexithymia and the recognition of facial expressions of emotion. Psychother. Psychosom. 1993, 59, 197–202. [Google Scholar] [CrossRef] [PubMed]\n\nMann, L.S.; Wise, T.N.; Trinidad, A.; Kohanski, R. Alexithymia, affect recognition, and the five-factor model of personality in normal subjects. Psychol. Rep. 1994, 74, 563–567. [Google Scholar] [CrossRef] [PubMed]\n\nPandey, R.; Mandal, M.K. Processing of facial expressions of emotion and alexithymia. Br. J. Clin. Psychol. 1997, 36, 631–633. [Google Scholar] [CrossRef] [PubMed]\n\nMcDonald, P.W.; Prkachin, K.M. The expression and perception of facial emotion in alexithymia: A pilot study. Psychosom. Med. 1990, 52, 199–210. [Google Scholar] [CrossRef] [PubMed]\n\nParker, P.D.; Prkachin, K.M.; Prkachin, G.C. Processing of facial expressions of negative emotion in alexithymia: The influence of temporal constraint. J. Pers. 2005, 73, 1087–1107. [Google Scholar] [CrossRef] [PubMed]\n\nPrkachin, G.C.; Casey, C.; Prkachin, K.M. Alexithymia and perception of facial expressions of emotion. Pers. Individ. Diff. 2009, 46, 412–417. [Google Scholar] [CrossRef]\n\nIhme, K.; Sacher, J.; Lichev, V.; Rosenberg, N.; Kugel, H.; Rufer, M.; Grabe, H.J.; Pampel, A.; Lepsien, J.; Kersting, A.; et al. Alexithymic features and the labeling of brief emotional facial expressions—An fMRI study. Neuropsychologia 2014, 64, 289–299. [Google Scholar] [CrossRef]\n\nKätsyri, J.; Saalasti, S.; Tiippana, K.; von Wendt, L.; Sams, M. Impaired recognition of facial emotions from low-spatial frequencies in Asperger syndrome. Neuropsychologia 2008, 46, 1888–1897. [Google Scholar] [CrossRef] [PubMed]\n\nGrynberg, D.; Chang, B.; Corneille, O.; Maurage, P.; Vermeulen, N.; Berthoz, S.; Luminet, O. Alexithymia and the processing of emotional facial expressions (EFEs): Systematic review, unanswered questions and further perspectives. PLoS ONE 2012, 7, e42429. [Google Scholar] [CrossRef] [PubMed]\n\nLuminet, O.; Nielson, K.A.; Ridout, N. Cognitive-emotional processing in alexithymia: An integrative review. Cogn. Emot. 2021, 35, 449–487. [Google Scholar] [CrossRef]\n\nLuminet, O.; Nielson, K.A.; Ridout, N. Having no words for feelings: Alexithymia as a fundamental personality dimension at the interface of cognition and emotion. Cogn. Emot. 2021, 35, 435–448. [Google Scholar] [CrossRef] [PubMed]\n\nGrynberg, D.; Vermeulen, N.; Luminet, O. Amplification of attentional blink by distress-related facial expressions: Relationships with alexithymia and affectivity. Int. J. Psychol. 2014, 49, 371–380. [Google Scholar] [CrossRef] [PubMed]\n\nDonges, U.S.; Suslow, T. Alexithymia and automatic processing of emotional stimuli: A systematic review. Rev. Neurosci. 2017, 28, 247–264. [Google Scholar] [CrossRef] [PubMed]\n\nFuchs, M.; Kersting, A.; Suslow, T.; Bodenschatz, C.M. Recognizing and looking at masked emotional faces in alexithymia. Behav. Sci. 2024, 14, 343. [Google Scholar] [CrossRef] [PubMed]\n\nBagby, R.M.; Taylor, G.J. Measurement and validation of the alexithymia construct. In Disorders of Affect Regulation. Alexithymia in Medical and Psychiatric Illness; Taylor, G.J., Bagby, R.M., Parker, J.D.A., Eds.; Cambridge University Press: Cambridge, UK, 1997; pp. 46–66. [Google Scholar]\n\nVerroca, A.; de Rienzo, C.M.; Gambarota, F.; Sessa, P. Mapping the perception-space of facial expressions in the era of face masks. Front. Psychol. 2022, 13, 956832. [Google Scholar] [CrossRef] [PubMed]\n\nMaiorana, N.; Dini, M.; Poletti, B.; Tagini, S.; Reitano, M.R.; Pravettoni, G.; Priori, A.; Ferrucci, R. The effect of surgical masks on the featural and configural processing of emotions. Int. J. Environ. Res. Public Health 2022, 19, 2420. [Google Scholar] [CrossRef] [PubMed]\n\nLi, S.; Zhang, B.; Guo, Y.; Zhang, J. The association between alexithymia as assessed by the 20-item Toronto Alexithymia Scale and depression: A meta-analysis. Psychiatry Res. 2015, 227, 1–9. [Google Scholar] [CrossRef] [PubMed]\n\nSuslow, T.; Donges, U.-S. Alexithymia components are differentially related to explicit negative affect but not associated with explicit positive affect or implicit affectivity. Front. Psychol. 2017, 8, 1758. [Google Scholar] [CrossRef] [PubMed]\n\nMontebarocci, O.; Surcinelli, P.; Rossi, N.; Baldaro, B. Alexithymia, verbal ability and emotion recognition. Psychiatr. Q. 2011, 82, 245–252. [Google Scholar] [CrossRef] [PubMed]\n\nReitan, R.M.; Wolfson, D. The Halstead-Reitan Neuropsychological Test Battery: Therapy and Clinical Interpretation; Neuropsychological Press: Tucson, AZ, USA, 1985. [Google Scholar]\n\nBagby, R.M.; Parker, J.D.A.; Taylor, G.J. The Twenty-Item Toronto Alexithymia Scale. I. Item selection and cross-validation of the factor structure. J. Psychosom. Res. 1994, 38, 23–32. [Google Scholar] [CrossRef]\n\nBach, M.; Bach, D.; de Zwaan, M.; Serim, M.; Böhmer, F. Validation of the German version of the 20-item Toronto Alexithymia Scale in normal persons and psychiatric patients. Psychother. Psychosom. Med. Psychol. 1996, 46, 23–28. [Google Scholar] [PubMed]\n\nBeck, A.T.; Steer, R.A.; Brown, G.K. Manual for the Beck Depression Inventory-II; Psychological Corporation: San Antonio, TX, USA, 1996. [Google Scholar]\n\nHautzinger, M.; Keller, F.; Kühner, C. BDI-II. Beck Depressions-Inventar Revision; Harcourt Test Services: Frankfurt, Germany, 2006. [Google Scholar]\n\nSpielberger, C.D.; Gorsuch, R.L.; Lushene, R.E. Manual for the State-Trait Anxiety Inventory; Consulting Psychologists Press: Palo Alto, CA, USA, 1970. [Google Scholar]\n\nLaux, L.; Glanzmann, P.; Schaffner, P.; Spielberger, C.D. Das State-Trait-Angstinventar: Theoretische Grundlagen und Handanweisungen; Beltz Testgesellschaft: Weinheim, Germany, 1981. [Google Scholar]\n\nLehrl, S. Manual zum MWT-B, 5th ed.; Spitta: Balingen, Germany, 2005. [Google Scholar]\n\nEbner, N.C.; Riediger, M.; Lindenberger, U. FACES—A database of facial expressions in young, middle-aged, and older women and men: Development and validation. Behav. Res. Method. 2010, 42, 351–362. [Google Scholar] [CrossRef] [PubMed]\n\nPeirce, J.W.; MacAskill, M. Building Experiments in PsychoPy; Sage: London, UK, 2018. [Google Scholar]\n\nFaul, F.; Erdfelder, E.; Buchner, A.; Lang, A.G. Statistical power analyses using G*power 3.1: Tests for correlation and regression analyses. Behav. Res. Methods 2009, 41, 1149–1160. [Google Scholar] [CrossRef] [PubMed]\n\nCalvo, M.G.; Fernández-Martín, A.; Gutiérrez-García, A.; Lundqvist, D. Selective eye fixations on diagnostic face regions of dynamic emotional expressions: KDEF-dyn database. Sci. Rep. 2018, 8, 17039. [Google Scholar] [CrossRef] [PubMed]\n\nKeltner, D.; Haidt, J. Social functions of emotions at four levels of analysis. Cogn. Emot. 1999, 13, 505–521. [Google Scholar] [CrossRef]\n\nTanzer, M.; Shahar, G.; Avidan, G. Project PAVE (Personality and Vision Experimentation): Role of personal and interpersonal resilience in the perception of emotional facial expression. Front. Hum. Neurosci. 2014, 8, 602. [Google Scholar] [CrossRef] [PubMed]\n\nRyff, C.D. Happiness is everything, or is it? Explorations on the meaning of psychological well-being. J. Pers. Soc. Psychol. 1989, 57, 1069–1081. [Google Scholar] [CrossRef]\n\nStaff, A.I.; Luman, M.; van der Oord, S.; Bergwerff, C.E.; van den Hoofdakker, B.J.; Oosterlaan, J. Facial emotion recognition impairment predicts social and emotional problems in children with (subthreshold) ADHD. Eur. Child Adolesc. Psychiatry 2022, 31, 715–727. [Google Scholar] [CrossRef] [PubMed]\n\nWeinryb, R.M.; Gustavsson, J.P.; Hellström, C.; Andersson, E.; Broberg, A.; Gunnar, R. Interpersonal problems and personality characteristics: Psychometric studies of the Swedish version of the IIP. Pers. Individ. Diff. 1996, 20, 13–23. [Google Scholar] [CrossRef]\n\nVanheule, S.; Vandenbergen, J.; Verhaeghe, P.; Desmet, M. Interpersonal problems in alexithymia: A study in three primary care groups. Psychol. Psychother. 2010, 83, 351–362. [Google Scholar] [CrossRef] [PubMed]\n\nLukas, C.A.; Trevisi Fuentes, H.; Berking, M. Smartphone-based emotion recognition skills training for alexithymia—A randomized controlled pilot study. Internet Interv. 2019, 17, 100250. [Google Scholar] [CrossRef] [PubMed]\n\nLane, R.D.; Weihs, K.L.; Herring, A.; Hishaw, A.; Smith, R. Affective agnosia: Expansion of the alexithymia construct and a new opportunity to integrate and extend Freud’s legacy. Neurosci. Biobehav. Rev. 2015, 55, 594–611. [Google Scholar] [CrossRef] [PubMed]\n\nBagby, R.M.; Parker, J.D.A.; Taylor, G.J. Twenty-five years with the 20-item Toronto alexithymia scale. J. Psychosom. Res. 2020, 131, 109940. [Google Scholar] [CrossRef] [PubMed]\n\nGrenville, E.; Dwyer, D.M. Face masks have emotion-dependent dissociable effects on accuracy and confidence in identifying facial expressions of emotion. Cogn. Res. Princ. Implic. 2022, 7, 15. [Google Scholar] [CrossRef] [PubMed]\n\nFigure 1. Number of hits (correct responses) in the emotion recognition task as a function of mask and order of blocks, i.e., start with unmasked vs. start with masked faces (error bars: standard error).\n\nFigure 2. Response times (in seconds) for correct answers in the emotion recognition task as a function of mask and order of blocks, i.e., start with unmasked vs. start with masked faces (error bars: standard error).\n\nTable 1. Correlations between TAS-20 scales and psychological tests (Spearman rank) with descriptive statistics (N = 102).\n\nVariableTAS-20\n\nSum ScoreTAS-20\n\nDIFTAS-20\n\nDDFTAS-20\n\nEOTMeanSDTAS-20 (sum score)---0.82 ***0.88 ***0.67 *** 41.4810.65TAS-20 DIF------0.62 ***0.29 ***14.044.38TAS-20 DDF---------0.43 ***11.824.59TAS-20 EOT------------15.624.32BDI-II0.29 **0.34 ***0.22 *0.178.745.13STAI-T 0.47 **0.51***0.36 ***0.22 *38.298.76TMT-B−0.25 *−0.24 *−0.21 *−0.1156.8720.01MWT-B IQ −0.02−0.04−0.01−0.06109.2711.50\n\nTable 2. Correlations of the TAS-20 scales with overall number of correct responses (hits) and overall response latencies for the unmasked and masked expression conditions in the emotion recognition task (Spearman rank) (N = 102).\n\nVariableTAS-20\n\nSum ScoreTAS-20\n\nDIFTAS-20\n\nDDFTAS-20\n\nEOTUnmasked hits0.03−0.040.010.03Masked hits−0.05−0.02−0.140.06Unmasked RT0.010.000.08−0.02Masked RT0.160.060.210.09\n\nTable 3. Correlations of the BDI-II, STAI-T, TMT-B, and MWT-B with overall number of correct responses (hits) and overall response latencies for the unmasked and masked expression conditions in the emotion recognition task (Spearman rank) (N = 102).\n\nVariableBDI-IISTAI-TTMT-BMWT-B IQUnmasked hits0.100.15−0.050.11Masked hits0.04−0.03−0.010.00Unmasked RT0.22 *0.090.19−0.04Masked RT0.010.000.02−0.07\n\nTable 4. Comparison between participants starting with unmasked faces (n = 52, 34 women) and participants starting with masked faces (n = 50, 36 women) concerning alexithymia, affectivity, psychomotor functioning, and intelligence (Mann–Whitney U tests) with descriptive statistics.\n\nVariableStart with\n\nUnmasked Faces\n\nMean (SD)Start with\n\nMasked Faces\n\nMean (SD)ZpTAS-20 (sum score)40.60 (10.44)42.40 (10.89)−1.090.27TAS-20 DIF13.62 (4.49)14.48 (4.26)−1.140.25TAS-20 DDF11.63 (4.29)12.02 (4.91)−0.320.75TAS-20 EOT15.35 (4.27)15.9 (4.40)−0.630.53BDI-II9.15 (4.96)8.30 (5.30)−0.960.34STAI-T 38.00 (8.96)38.60 (8.62)−0.660.51TMT-B61.54 (23.38)52.02 (14.64)−2.000.046 *MWT-B IQ 108.13 (11.55)110.46 (11.44)−1.310.19\n\nTable 5. Correlations of the TAS-20 scales with overall number of correct responses (hits) and overall response latencies for the unmasked and masked expression conditions in participants starting with unmasked faces (n = 52) and participants starting with masked faces in the emotion recognition task (n = 50) (Spearman rank).\n\nVariableTAS-20\n\nSum ScoreTAS-20\n\nDIFTAS-20\n\nDDFTAS-20\n\nEOTStart with unmasked facesUnmasked hits−0.07−0.07−0.03−0.16Masked hits0.100.14−0.050.17Unmasked RT−0.05−0.010.08−0.18Masked RT−0.06−0.050.15−0.23Start with masked facesUnmasked hits0.150.000.070.20Masked hits−0.11−0.07−0.210.01Unmasked RT0.260.180.220.24Masked RT0.330.080.270.41*\n\nTable 6. Correlations of the TAS-20 scales with response latencies for the masked emotional expression conditions (Spearman rank) in participants starting with the masked faces in the emotion recognition task (n = 50) with descriptive statistics.\n\nVariableTAS-20\n\nSum ScoreTAS-20\n\nDIFTAS-20\n\nDDFTAS-20\n\nEOTMeanSDMasked Happy RT0.100.13−0.010.112.100.37Masked Neutral RT0.250.090.250.231.690.41Masked Sad RT0.250.140.160302.440.63Masked Disgusted RT0.380.12300.50 *2.700.75Masked Fearful RT0.14−0.080.150.182.400.65\n\nDisclaimer/Publisher’s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.\n\n© 2024 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/).\n\nShare and Cite\n\nMDPI and ACS Style\n\nSuslow, T.; Kersting, A.; Bodenschatz, C.M. Dimensions of Alexithymia and Identification of Emotions in Masked and Unmasked Faces. Behav. Sci. 2024, 14, 692. https://doi.org/10.3390/bs14080692\n\nAMA Style\n\nSuslow T, Kersting A, Bodenschatz CM. Dimensions of Alexithymia and Identification of Emotions in Masked and Unmasked Faces. Behavioral Sciences. 2024; 14(8):692. https://doi.org/10.3390/bs14080692\n\nChicago/Turabian Style\n\nSuslow, Thomas, Anette Kersting, and Charlott Maria Bodenschatz. 2024. \"Dimensions of Alexithymia and Identification of Emotions in Masked and Unmasked Faces\" Behavioral Sciences 14, no. 8: 692. https://doi.org/10.3390/bs14080692\n\nNote that from the first issue of 2016, this journal uses article numbers instead of page numbers. See further details here.\n\nArticle Metrics\n\nNo\n\nNo\n\nArticle Access Statistics\n\nFor more information on the journal statistics, click here.\n\nMultiple requests from the same IP address are counted as one view."
    }
}
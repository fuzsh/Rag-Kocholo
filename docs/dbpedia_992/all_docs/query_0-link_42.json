{
    "id": "dbpedia_992_0",
    "rank": 42,
    "data": {
        "url": "https://www.windmill.dev/changelog/log-disk-distributed-storage-compaction",
        "read_more_link": "",
        "language": "en",
        "title": "Large log disk and Distributed storage compaction",
        "top_image": "https://www.windmill.dev/assets/images/windmill_alone-f9a1aca26f10cc7f693a6666d39e3124.png",
        "meta_img": "https://www.windmill.dev/assets/images/windmill_alone-f9a1aca26f10cc7f693a6666d39e3124.png",
        "images": [
            "https://www.windmill.dev/img/windmill.svg",
            "https://www.windmill.dev/img/windmill.svg",
            "https://www.windmill.dev/assets/images/windmill_alone-f9a1aca26f10cc7f693a6666d39e3124.png",
            "https://www.windmill.dev/img/windmill.svg",
            "https://www.windmill.dev/img/soc.png.webp"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2024-03-23T00:00:00+00:00",
        "summary": "",
        "meta_description": "One of our large scale customers noticed that their database disk usage was much much higher than they anticipated. After investigation, we realized that our use of the database for streaming was very suboptimal in a few ways due to the nature of update in postgres. When you update a row in Postgres, it will actually keep the prior row as a dead tuple until it is collected. It doesn't matter in much case but it will if you're appending a few log lines to a 25MB log row, every 500ms. <br/><br/> **We have completely refactored the way we deal with logs in major ways  and starting on 1.295.0 you should feel comfortable having extremely large logs on Windmill** <br/><br/> First action we took was to extract the logs from the queue table to a separate table. That was to avoid update unrelated to logs creating dead tuples. Second action was to make the streaming rate from the worker adaptive to the the duration of the job, a longer job does not need to update its log every 500ms, every 2.5s is reasonable for jobs of more than 10s, 5s for 60s+, etc... <br/><br/> But that was still not enough, even every 2.5s an update on a 25mb log would create lots of heavy dead tuples. And 25MB is not that large, our customer should feel confident streaming GBs of logs per job with jobs that run for months. <br/><br/> So we completely revisited the way we store logs to only treat the database as a buffer for streaming purpose rather than long term storage. We keep the db as a 5000 char buffer to still provide the same instant preview as before but <br/><br/> 1. On EE, the logs will be streamed to S3 if you connected your instance to S3, everything is seamless and you can still download the entire log, Windmill will take care of streaming from S3 <br/><br/> 2. non EE, the excess log (>10000 chars)  will be stored on disk of the worker (mount /tmp/windmill/logs to persist those). <br/><br/> **Now the db only stores at most 5Kb of logs per job rows, reducing the pressure on it by order of magnitudes, while users can now run jobs with unlimited logs with minimal impact on the worker or db. And the logs are still as live as before**",
        "meta_lang": "en",
        "meta_favicon": "/img/docs_logo.svg",
        "meta_site_name": "",
        "canonical_link": "https://www.windmill.dev/changelog/log-disk-distributed-storage-compaction",
        "text": "One of our large scale customers noticed that their database disk usage was much much higher than they anticipated. After investigation, we realized that our use of the database for streaming was very suboptimal in a few ways due to the nature of update in postgres. When you update a row in Postgres, it will actually keep the prior row as a dead tuple until it is collected. It doesn't matter in much case but it will if you're appending a few log lines to a 25MB log row, every 500ms.\n\nWe have completely refactored the way we deal with logs in major ways and starting on 1.295.0 you should feel comfortable having extremely large logs on Windmill\n\nFirst action we took was to extract the logs from the queue table to a separate table. That was to avoid update unrelated to logs creating dead tuples. Second action was to make the streaming rate from the worker adaptive to the the duration of the job, a longer job does not need to update its log every 500ms, every 2.5s is reasonable for jobs of more than 10s, 5s for 60s+, etc...\n\nBut that was still not enough, even every 2.5s an update on a 25mb log would create lots of heavy dead tuples. And 25MB is not that large, our customer should feel confident streaming GBs of logs per job with jobs that run for months.\n\nSo we completely revisited the way we store logs to only treat the database as a buffer for streaming purpose rather than long term storage. We keep the db as a 5000 char buffer to still provide the same instant preview as before but\n\n1. On EE, the logs will be streamed to S3 if you connected your instance to S3, everything is seamless and you can still download the entire log, Windmill will take care of streaming from S3\n\n2. non EE, the excess log (>10000 chars) will be stored on disk of the worker (mount /tmp/windmill/logs to persist those).\n\nNow the db only stores at most 5Kb of logs per job rows, reducing the pressure on it by order of magnitudes, while users can now run jobs with unlimited logs with minimal impact on the worker or db. And the logs are still as live as before"
    }
}
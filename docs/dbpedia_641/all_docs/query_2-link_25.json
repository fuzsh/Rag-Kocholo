{
    "id": "dbpedia_641_2",
    "rank": 25,
    "data": {
        "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9903828/",
        "read_more_link": "",
        "language": "en",
        "title": "Population genomics of the critically endangered kākāpō",
        "top_image": "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/pmc-card-share.jpg?_=0",
        "meta_img": "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/pmc-card-share.jpg?_=0",
        "images": [
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/favicons/favicon-57.png",
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/icon-dot-gov.svg",
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/icon-https.svg",
            "https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/logos/AgencyLogo.svg",
            "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/logo-cgen.gif",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9903828/bin/fx1.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9903828/bin/gr1.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9903828/bin/gr2.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9903828/bin/gr3.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9903828/bin/gr4.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Jason Howard",
            "Erich D. Jarvis",
            "Bruce C. Robertson",
            "Love Dalén",
            "Nicolas Dussex",
            "Tom van der Valk",
            "Hernán E. Morales",
            "Christopher W. Wheat",
            "David Díez-del-Molino",
            "Johanna von Seth"
        ],
        "publish_date": "2021-10-13T00:00:00",
        "summary": "",
        "meta_description": "The kākāpō is a flightless parrot endemic to New Zealand. Once common in the archipelago, only 201 individuals remain today, most of them descending from an isolated island population. We report the first genome-wide analyses of ...",
        "meta_lang": "en",
        "meta_favicon": "https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon.ico",
        "meta_site_name": "PubMed Central (PMC)",
        "canonical_link": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9903828/",
        "text": "Quantification and statistical analysis\n\nDe-novo assembly and annotation\n\nThe kākāpō assembly was generated with the Vertebrate Genomes Project (VGP) v1.6 assembly pipeline18 using a combination of PacBio and Hi-C libraries (see Methods S1). The final assembly size was of 1.17 Gb, with a scaffold N50 of 83.2Mb and assigned to 26 chromosomes (24 autosomes and two sex chromosomes; see Methods S1). We identified Z and W chromosomes from the assembled genome by blasting all scaffolds against the Z-chromosome of zebra finch (v3.2.4, Taeniopygia guttata; GenBank: GCA_000151805.2) and W-chromosome of chicken (v5.0, Gallus gallus, GenBank: GCA_000002315.5) using BLAST+ 2.5.0.35 The BLAST+ parameters were set as: -evalue = 1e-10; -word_size = 15; -max_target_seqs = 1000. We excluded the identified Z chromosome ({\"type\":\"entrez-nucleotide\",\"attrs\":{\"text\":\"CM013763.1\",\"term_id\":\"1560007597\",\"term_text\":\"CM013763.1\"}}CM013763.1; 101.23Mb) and W chromosome ({\"type\":\"entrez-nucleotide\",\"attrs\":{\"text\":\"CM013773.1\",\"term_id\":\"1560007459\",\"term_text\":\"CM013773.1\"}}CM013773.1; 35.7Mb), from all downstream analyses in order to avoid bias associated with analyses relying on heterozygosity estimates. We also visually examined genome coverage estimated with Qualimap v2.2.136 (see below) for males and females alignments to confirm the identity of the Z and W chromosomes. Males had on average ∼15X and ∼0X for the Z and W chromosome, respectively, and females had on average ∼7X and ∼7X for the Z and W chromosome, respectively. We identified CpG sites using a custom script masking CG sites14,37 and masked repetitive elements in the genome assembly using RepeatMasker v4.0.738 applying the repeat element library of the aves database.\n\nWe annotated the assembly using the MESPA pipeline39 (see Methods S1). Briefly, we collapsed reference protein sets for zebra finch (Taeniopygia guttata; GenBank: GCA_000151805.2) to 90% coverage following Uniprot90 guidelines using a custom script to only retain sequences with at least 90% sequence identity to, and 80% overlap with, the longest sequence. We then generated an annotation in gff format and extracted 85% (13,175 out of 15,342) high quality kākāpō protein models using zebra finch as a reference protein set. We refined this annotation using the BRAKER2 v2.1.1 pipeline40, 41, 42 and used the resulting zebra finch proteome to predict kākāpō proteins with the exon-aware, protein-to-genome aligner SPALN2.43 We then extracted CDSs and protein sequences from this annotation with cufflinks v2.2.144,45 gffread command using the -V option to exclude genes with in-frame STOP codons. We identified 16,171 kākāpō gene models with a mean length of 1,514bp (Median = 672; min = 50; max = 26,940) to be used in downstream analyses. Finally, we performed a functional annotation of these gene models using the eggNOG-mapper v4.5.146 and obtained 15,699 annotated gene models (see Methods S1).\n\nTwo other annotations not used in downstream analyses were also generated using the Ensembl gene annotation system77 and NCBI Eukaryotic Genome Annotation Pipeline78 (see Methods S1).\n\nHistorical and modern data processing\n\nAll data processing and analyses were performed on resources provided by the Swedish National Infrastructure for Computing (SNIC) at UPPMAX, Uppsala University. Raw historical sequence data were demultiplexed using bcl2Fastq v2.17.1 with default settings (Illumina Inc.). We merged forward and reverse sequencing reads before mapping as recommended for damaged and short reads.79 We used SeqPrep v1.147 to trim adapters and merge paired-end reads using default settings. We made a minor modification to the source code, which enabled to choose the best quality score of the overlapping bases in the merged region instead of aggregating the scores, following Palkopoulou et al.12 We mapped the merged reads against the reference genome using the BWA v0.7.13 aln algorithm48 with deactivated seeding (-l 16,500), allowing more substitutions (-n 0.01) and allowing up to two gaps (-o 2). We used the BWA samse command to generate alignments in SAM format and Samtools v1.349 to convert these alignments to BAM format, sort and index them. Finally, we removed PCR duplicate reads using a custom python script that takes into account both start and end position of the reads.12 Even though all historical genomes were USER-treated75,76 during library preparation to remove post-mortem DNA damage, we used mapDamage v2.050 on the 13 historical samples to estimate damage patterns (Figure S4).\n\nFor modern samples, we trimmed forward and reverse reads to remove Illumina adapters using Trimmomatic v0.32 with default settings51 and then mapped them to the reference genome using BWA mem v0.7.13.48 Samtools was used for sorting, indexing, and removing duplicates from the alignments.\n\nNext, we processed historical and modern bam files using the same approach. We used Picard v1.141 to assign read group information including library, lane and sample identity to each bam file. We then re-aligned reads around indels using GATK IndelRealigner v3.4.0,52 and only kept reads with mapping quality mapQ ≥ 30 for subsequent analysis. For each genome, we estimated the depth of coverage using Qualimap v2.2.1.36 After this filtering, average genome coverage ranged from 11.8 and 18.2 (average = 15.3) and from 10.3 to 27.7 (average = 14.2) for modern and historical genomes, respectively (Table S1).\n\nWe called variants in historical and modern genomes separately for each individual using bcftools mpileup v1.3and bcftools call v1.349,53 using a minimum depth of coverage (DP4) of 1/3X of the average coverage (i.e., 5X) and removed SNPs with base quality QV < 30 and those within 5bp of indels. We also filtered out SNPs in heterozygous state with an allelic balance (i.e., number of reads displaying the reference allele/depth) of < 0.2 and > 0.8 in order to avoid biases caused by contamination, mapping or sequencing error.\n\nWe removed the Z and W chromosomes, hard masked all identified CpG sites and repeat regions using BEDtools v2.27.1.54 After merging all 49 individual vcf. files we obtained 2,785,380 high quality SNP calls. We then used PLINK v1.955 to filter variants not covered in all of the 49 individuals resulting in a total of 880,370 high quality SNPs that were used in all downstream analyses (i.e., population structure, demography, genome-wide diversity and inbreeding, mutational load estimation).\n\nPopulation structure\n\nWe first used the R package SNPRelate to perform a principal component analysis (PCA) based on the genetic covariance matrix calculated from the genotypes56 using our filtered SNP dataset.\n\nSecond, we used the program ADMIXTURE v1.3.057 to identify genetic clusters (K = 1-4) within our dataset. This program estimates ancestry in a model-based manner where individuals are considered unrelated and uses a cross-validation procedure to determine the best number of possible genetic groups present in the dataset.\n\nThird, we constructed a phylogenetic tree using RapidNJ v2.3.258 based on the neighbor-joining method.80 This method calculates the distance matrix of Dij between each pair of individuals (i and j) with the following formula:\n\nDij=∑m=1MdijL\n\nWhere, M is the number of segregating sites in i and j, L is the length of regions, dij is the distance between individuals i and j at given site. dij = 0, when individuals i and j are both homozygous for the same allele (AA/AA); dij = 0.5, when at least one of the genotypes of an individual i or j is heterozygous (Aa/AA, AA/Aa or Aa/Aa); and dij = 1, when individuals i and j are both homozygous but for different alleles (AA/aa or aa/AA).\n\nSince all three methods agreed in the main population structure within the specimens in our dataset and showed a clear distinction between the Stewart Island and the mainland population (Figure S5), we used the identified clusters for all downstream analyses. All mislabelled specimens (i.e., VM5, AUC2, LEI2, AUS1) were analyzed as part of the population they were genetically assigned to.\n\nDemographic reconstruction and population divergence\n\nWe used the Pairwise Sequentially Markovian Coalescent (PSMC v0.6.5)21 model to estimate temporal changes in effective population sizes (Ne) of kākāpō. We generated consensus sequences for all autosomes of a subset of historical and modern genomes using the Samtools mpileup v1.349 command and the ‘vcf2fq’ command from vcfutils.pl. We filtered for base and mapping quality below 30, and depth below 1/3X of the average coverage for each specimen. We set N (the number of iterations) = 30, t (Tmax) = 15 and p (atomic time interval) = 64 (4+25∗2+4+6, for each of which parameters are estimated with 28 free interval parameters). To estimate the substitutions rate per site/year, we used TimeTree,81 which estimates the substitution rate based on automated literature searches. We aligned 135 birds genomes and assumed a divergence time of 25 my BP between the kea and kākāpō lineages.82 We obtained an estimate of 0.89 × 10−9 substitutions/site/year.\n\nIn order to scale population parameters, we assumed a generation time of 15 years making for a rate (μ) of 1.33 × 10−8 substitutions/site/generation which is biologically realistic in a large and natural kākāpō population (data not shown; see Methods S1).\n\nSecond, we reconstructed the population history of kākāpō and estimated divergence times between the Stewart Island (N = 35) and the mainland (N = 11; excluding TEP11, AUC2 and VM5 which formed their own cluster; and S5) populations and their effective population sizes (Ne) using a composite-likelihood method based on the site frequency spectrum83 implemented in fastsimcoal2 v2.6.59,60 We obtained a folded site frequency spectrum by converting the vcf file filtered for missing data (880,370 SNPs) into Arlequin format in PGDspider61 and then by converting the resulting Arlequin file into a joint Site Frequency Spectrum (joint 2D-SFS) in Arlequin v3.5.62 We also collapsed all SFS entries less than 5 in a single category (command line option –C5). We designed four competing scenarios including a post-glacial population size change (bottleneck or expansion) and a divergence event of the Stewart Island population from the Mainland: (a) Post-glacial divergence, (b) Post-glacial divergence followed by Stewart Island population expansion, (c) Post-glacial divergence followed by Mainland population expansion and (d) Post-glacial divergence followed by Stewart Island and Mainland populations expansion (Figure S6). The latter population size change was not constrained in the model in order to allow for either a bottleneck or population expansion to occur but is referred to as an expansion since it was supported by the simulations (Figure S8; Table S3). For each scenario, we carried out 50 replicate runs with the following settings: -n 100000 -m -q -M 0.001 -l 10 -L 40. Initial prior distributions followed a log-uniform distribution for population sizes (Npre-glac: 103 –105; NMain: 103 –105; and NStewart: 102 –104; Nglac-main: 103 –105; and Nglac-Stewart: 102 –104), timing of glacial bottleneck (TBOT: 102 – 105), timing of divergence (TDIV: 5×102 – 1.5×103), timing of terminal expansion for both populations (TEXP: 5×102 – 1.5×103). The data was modeled as FREQ (1 bp simulated for each locus), with the number of independent chromosomes equal to the total number of loci (including monomorphic loci) characterized. We used the same substitution rate and generation time as mentioned above for the PSMC. We then used the range in parameter estimates across the initial 50 runs as the prior distribution for another 50 replicates within each scenario, until no further increase in likelihood was detected. The parameter values from the final run with the highest likelihood for each scenario were then used for 50 additional runs with –n = 1000000 to obtain a final estimate of the maximum observed likelihood. We assessed the best fitting scenario by Akaike’s information criterion (AIC) score84 and with the AIC’s weight (w), as described in Excoffier et al.60 (Table S2). We then used the parameter values from the best-fitting scenario to simulate 100 parametric bootstraps datasets. In order to obtain confidence intervals for parameter estimates, we used the ∗.tpl and initial prior distribution ∗.est files that led to the best replicate and ran 50 replicates per simulated dataset, making for a total of 5000 parameter estimates (Table S3). We changed the data type to DNA (1 bp), with the number of chromosomal segments equalling the total number of loci in the SFS (including monomorphic sites).\n\nThird, we used the multiple sequential Markovian coalescent (MSMC2) model63 based on phased haplotypes from the two populations to infer changes in kākāpō Ne. We used Beagle v5.164 on default settings to phase the SNP-calls. Genome mappability masks and multi-sample input files were obtained using msmc-tools following the pipelines described in Schiffels and Wang.63 MSMC2 was then run using the five genomes with highest coverage for each population and using default settings. We used the same substitution rate (μ) and generation time as those described for the PSMC for scaling.\n\nFinally, we estimated the split time (T), assuming no coalescent events since divergence between the mainland and Stewart Island using the PSMC approach applied to a pseudo-diploid Z chromosome genome as described in Palkopoulou et al.12 We extracted the Z-chromosomes from one mainland historical (CAN1) and one modern Stewart Island (Ruth) female. We generated a Z chromosome haploid consensus sequence for each these two females and merged them into a pseudo-diploid sequence using the seqtk mergefa command. We then applied the PSMC method on the pseudo-diploid Z chromosome to estimate changes in Ne over time. Finally, we rescaled the pseudo-diploid Z chromosome curve to 0.25 consistent with the effective population size of chromosome Z relative to that of autosomes (sex-chromosome/autosome ratio: 0.75). We ran the analysis using the same quality filters, parameters (i.e., 64 discrete time intervals) and the same substitution rate as above for the PSMC on autosomes. As a comparison, we also ran the analysis using fewer discrete intervals (i.e., 49 = 6+4+3+13∗2+4+6 or 37 = 2+2+1+15∗2+2) as recommended by Prado-Martinez et al.85 in order to avoid underestimation of the split time.\n\nGenomic diversity and inbreeding\n\nWe first estimated genome-wide population-level nucleotide diversity (π)86 in mainland and Stewart Island birds with vcftools65 using a sliding window of 10kbp.\n\nSecond, we used mlRho v2.766 to estimate the mutation rate (θ), which approximates the per site heterozygosity under the infinite sites model and uses bam files as input. We first filtered out bases and reads with quality below 30, and positions with root-mean-squared mapping quality below 30 from the historical and modern bam files. Because high or low coverage in some regions resulting from structural variation can create erroneous mapping to the reference genome and false heterozygous sites, for each specimen, we also filtered out sites with depth lower than five times and higher than two times the average coverage across all our specimens. We then estimated the individual θ as the number of heterozygote sites per 1,000bp. The maximum likelihood approach implemented in mlRho has been shown to provide unbiased estimates of average within-individual heterozygosity at high coverage.66,87\n\nThird, we estimated individual inbreeding coefficients, by estimating the number and length of Runs of homozygosity (ROH). ROH are long tracts of the genome with very little or no heterozygote sites that can inform about recent and past population events and can be used to estimate individual inbreeding levels.88 We used PLINK v1.955 to identify ROH and per sample inbreeding coefficients (FROH). We first converted the filtered multi-individual vcf. file comprising 35 Stewart Island and 14 mainland individuals into a ped file and identified ROH in autosomal chromosomes. We used a sliding window size of 100 SNPs (homozyg-window-snp 100). We assumed a window to be homozygous if there were not more than 1 heterozygous site per window (homozyg-window-het 1). Moreover, if at least 5% of all windows that included a given SNP were defined as homozygous, the SNP was defined as being in a homozygous segment of a chromosome (homozyg-window-threshold 0.05). This threshold was chosen to ensure that the edges of a ROH are properly delimited. Furthermore, a homozygous segment was defined as a ROH if all of the following conditions were met: the segment included ≥ 25 SNPs (homozyg-snp 25) and covered ≥ 100kb (homozyg-kb 100); the minimum SNP density was one SNP per 50kb (homozyg-density 50); the maximum distance between two neighboring SNPs was ≤ 1,000kb (homozyg-gap 1,000). For the number of heterozygous sites within ROH, we set the value at 750 (homozyg-het 750) in order to prevent sequencing errors to cut ROH. Based on these results, we estimated the inbreeding coefficient FROH estimated as the overall proportion of the genome contained in ROH.\n\nWhile we were mainly interested in estimating the relative difference between mainland and Stewart Island birds, we also assessed the robustness of our results to the various parameters used and to potential sequencing errors, by running the same analysis using more stringent parameters. Specifically, we varied the number of heterozygous sites per ROH segment (homozyg-het 1), at least one SNP in a ROH per 100kb (homozyg-density 100) and the maximum distance between two neighboring SNPs (homozyg-gap 500).\n\nWe statistically compared heterozygosity, FROH between mainland and Stewart Island kākāpō using a Welch’s two-sample t tests in R.89\n\nMutational load estimation\n\nWe estimated mutational load in mainland and Stewart Island kākāpō genomes using two approaches. First, we measured the relative mutational load in each individual as the number of derived alleles at sites that are under strict evolutionary constraints (i.e., highly conserved) and thus likely to be deleterious using genomic evolutionary rate profiling scores (GERP) with the GERP++ software67 and following van der Valk et al.90 We included both heterozygous (counted as one allele) and homozygous positions (counted as two alleles) even though the mutational effect of heterozygous positions depends on additional assumptions about the dominance coefficient. GERP identifies constrained elements in multiple alignments by quantifying the amount of substitution deficits (e.g., substitutions that would have occurred if the element were neutral DNA, but did not occur because the element has been under functional constraint) by accounting for phylogenetic divergence. High GERP scores (> 1) represent highly conserved regions whereas low scores (< 1) are putatively neutral.\n\nTo identify genomic regions under strong evolutionary constraint in the kākāpō we obtained 135 published bird genomes from NCBI (Figure S16). We used TimeTree81 to estimate the divergence times among these genomes as described above. Each of these genomes were then converted into fastq-format (50 bp reads) and realigned against the kākāpō assembly using BWA mem v0.7.13,48 slightly lowering mismatch and gaps penalty scores (-B 3, -O 4,4). Additionally, we filtered out all reads from the processed bam files aligning to more than one genomic location using Samtools.49 Next, we converted each alignment file to fasta-format using htsbox v1.0 -R -q 30 -Q 30 -l 35 -s 1. GERP++ was then used to calculate conservation scores for each site in the genome for which at least three bird species could be accurately aligned to the kākāpō reference (Figure S17). The kea genome (N. notabilis) alignment was used for the ancestral allele inference.91,92\n\nTo estimate the mutational load of each individual we obtained the total number of derived alleles stratified by GERP-score within highly conserved regions of the kākāpō genome (excluding sites with missing genotypes). The individual relative mutational load was then calculated as the sum of the number of all derived alleles above GERP-score of two (as these are considered to be deleterious) multiplied by their GERP-score, divided by the total number of derived alleles by individual (including those below a GERP-score of one). Higher values indicate that a relatively larger proportion of derived alleles is found at conserved genomic sites, thus indicating higher mutational load. We statistically compared GERP-scores between mainland and Stewart Island kākāpō using a Welch’s two-sample t tests in R.89\n\nSecond, we estimated mutational load in coding regions for mainland and Stewart Island kākāpō genomes using SNPeff v4.3.68 We used our dataset filtered for missing genotypes (880,370 SNPs) to avoid any bias due to sequencing stochasticity when estimating the difference in mutational load between populations and the annotation of 15,699 genes from the MESPA pipeline (see Methods S1) for this analysis. In order to avoid a reference bias when identifying synonymous and non-synonymous variants, we replaced the reference allele with the ancestral allele by using kea (N. notabilis) as reference and using a custom script as described above. After replacing the reference allele, we obtained a total of 406,510 SNPs.\n\nWe generated a database for kākāpō using the protein sequences extracted from our annotation. We further removed any gene model with in-frame STOP codons using the -V option of gffread from the cufflinks v2.2.144,45 package. We first identified putative deleterious variants in four different impact categories as defined in the SNPeff manual: a) Low: mostly harmless or unlikely to change protein behavior (i.e., synonymous variants); b) Moderate: non-disruptive variants that might change protein effectiveness (i.e., missense variants; Table S4); c) High: variant assumed to have high (disruptive) impact in the protein, probably causing protein truncation, loss of function (LoF) or triggering nonsense mediated decay (i.e., stop codons, splice donor variant and splice acceptor, start codon lost; Table S5); d) Modifier: usually non-coding variants or variants affecting non-coding genes, where predictions are difficult or there is no evidence of impact (i.e., downstream or upstream variants).68 Next, we identified the number of variants in these four categories separated by homozygous and heterozygous state. Because we only used sites covered in all individuals, we counted the number of variants in these four categories separated by homozygous and heterozygous state and did not need to use bootstrapping of allele counts. We then compared the number of each of these variants in mainland and Stewart Island kākāpō using a Welch’s two-sample t tests in R.89\n\nWe then estimated the difference in frequency of variants of all impact categories listed above between mainland and Stewart Island kākāpō using a similar approach to the one described by Xue et al.25 and van der Valk et al.10 For each category of variants, we calculated at each site i the observed allele frequency in Population x as fxi = dxi / nxi, where nxi, is the total number of alleles called in population x and dxi is the total number of called derived alleles. Similarly, we define fyi for population y. For each C category of variants we estimated:\n\nFreqpop−x(C)=∑i∈Cfix(1−fiy)\n\nWe then calculated the Rxy = Freqpop-x / Freqpop-y ratio, where a value of 1 corresponds to no change in frequency, > 1 a decrease in frequency in population y relative to population x and < 1 to an increase in frequency in population y. relative to population x We estimated the variance in the Rxy ratio by running a Jackknife approach in blocks of 1000 from the set of sites in each category of mutation. The Rxy ratio only included sites where at least one out of all alleles is derived in both populations.\n\nTo check for annotation bias, we performed the same analysis using a consensus mainland historical genome. We modified our modern high quality genome by changing SNPs and indels to the historical state using the genome polishing software Pilon v.1.2269 with quality filtering (–minmq 20 –minqual 20) and by mapping merged reads from individual LEI2, which had the highest coverage of the historical genomes (Table S1) using BWA mem v0.7.13.48 A second annotation for the historical genome was generated with the MESPA pipeline (see Methods S1), by using the historical genome as the reference with all other steps identical. The raw data was then mapped to this consensus and the variant calling performed as described above. After filtering for missing genotypes, we obtained 834,420 SNPs. Finally, we also replaced the reference allele with the ancestral allele by using kea (N. notabilis) in order to avoid reference bias as described above obtained a total of 371,886 SNPs. Results were consistent with those based on data mapped to the modern assembly (Figure S21).\n\nPurging of recessive deleterious variants (i.e., LoF alleles) is expected to lead to different signatures in homozygous (i.e., runs of homozygosity; ROH) and non-homozygous tracts within individuals. Since the individuals in this study were adults when sampled, recessive LoF variants with a deleterious effect on viability or survival in early infancy should thus be less common in homozygous tracts, where they have been exposed to purifying selection, than elsewhere in the genome. To test this hypothesis, we measured the number of LoF variants sites in homozygous and heterozygous portions of the genome and controlled for differing amounts of homozygosity within individuals by normalizing the rates of LoF variant sites by the rates of synonymous homozygous variant sites in the same regions obtained from the SNPeff output. We then assessed significance of the difference between relative rates of LoF variants in the homozygous and non-homozygous portions of the genomes using a paired t test in R.89\n\nGene Ontology\n\nWe performed a functional analysis for genes with LoF variants identified in SNPeff and based on Jane’s annotation (Table S6). We obtained the gene IDs associated with each LoF allele identified in the SNPeff analysis from our functional annotation. We then assessed the functional classification of these LoF variants with a Gene Ontology analysis in Panther v16.070 using chicken as reference set. Because identifying mutational load in birds that survived the peak of the 1990s bottleneck is highly valuable to guide future conservation actions for kākāpō, we performed this analysis only on survivor birds (i.e., 35 Stewart Island birds and Richard Henry; Tables S6 and S7).\n\nForward simulations\n\nSince the effect of drift and purifying selection are dependent on Ne8, we estimated changes in mutational load under contrasting demographic scenarios to assess their respective roles in declining populations. To test whether our results were consistent with purging of deleterious mutations in the Stewart Island population, we performed forward simulations recapitulating the demographic history of mainland and Stewart Island kākāpō. We also simulated scenarios for hypothetical stable and severely bottlenecked populations to model a weak and strong effect of drift, respectively.\n\nWe performed individual-based simulations with SLiM 371 using the non-Wright-Fisher (non-WF) implementation. As opposed to Wright-Fisher models, which operates under a more restrictive set of assumptions, non-WF models are fully customizable in terms of mate choice, reproduction, survival and population regulation, which allowed us to approximate the kākāpō life-history traits in a more realistic way based on Powlesland et al.4 We controlled the sex ratio to simulate the observed skewed sex ratio of kākāpō in the wild (∼2:1 in favor of males). We controlled time to sexual maturity by only allowing individuals to reproduce after females reached sexual maturity between 7 and 11 years old and males slightly sooner, between 5 and 7 years old. We simulated the known variance in reproductive success by allowing more experienced males (i.e., older males) to form pairs more readily as a function of their age. Pairs produced clutches in accordance with clutch-sizes observed in the wild, using random draws from a normal distribution (mean = 3, sd = 1.5) each pair produced between two and four individuals, and rarely more than four and less than two (including zero to represent inviable eggs). This mating scheme revealed that in our simulations, approximately a third of the individuals produced all the offspring in a given generation. Therefore, we simulated 2.8 times more individuals than our target effective population size. In non-WF simulations generations are overlapping (as in nature) and the average generation time is an emerging property of the simulation in function of the life-history parameters used. We recorded the full genealogy of 500 simulations steps and estimated that in average the distance between parents and offspring nodes was of ∼16 (sd = 2) simulation steps. This is remarkably consistent with the estimated generation time for kākāpō, estimated around 15 years. Thus, each simulation step can be thought as one year (the total simulation time was 25,000 years) and the generation time in our simulation to be in average 16.5 years.\n\nWe simulated 3,291 genes across the 23 fully assembled chromosome in relative proportions and positions as observed in the genome assembly, representing 20% of the total kākāpō exome. Each in-silico gene had a length of 1.5kb adding to a total amount of 4,936,500 base pairs simulated for each individual. We used a per-base, per-generation mutation rate of 1.33x10−08. A recombination rate of 1x10−9 was used between genes, but no recombination was allowed within genes. Neutral and deleterious mutations occurred at a relative proportion of 1:2.31.72 Selection coefficients of deleterious mutations were drawn from a gamma distribution (mean = −0.024, sd = 0.14), and simulations were performed independently for fully recessive (h = 0), partially recessive (h = 0.25) or additive (h = 0.5) dominance coefficients.\n\nWe simulated four distinct scenarios that spanned 25,000 years and that modeled distinct population trajectories since the LGM with Ne estimates from the PSMC used as priors: (i) a Stewart Island scenario modeled a decline to a long-term Ne of ∼1,000; (ii) a Mainland scenario modeled a decline to a long-term Ne of ∼6,000; (iii) an Extreme scenario modeled a sustained LGM decline to a long-term Ne of ∼100 to specifically simulate a strong effect of drift; (iv) a Stable scenario modeled a constant Ne of ∼10,000 and was used as a control, where the effect of drift should be weak.\n\nWe first performed a burn-in simulation step to obtain a fully coalesced population. Since our initial population size of N ∼28,000 with overlapping generations could take a very long time to reach coalescence, we sped-up this stage of the simulation by scaling-down population size and scaling-up recombination/mutation rates and selection coefficients by a factor of 10 as recommended in the SLiM 3 manual. We ran the burn-in simulation for 100,000 steps and collected the entire genealogy by the means of tree-sequence recording93 to confirm the tree had a single root with pyslim (i.e., has reached full-coalescence94). We then loaded the tree-sequence to start a new simulation where the scaling factors were removed. We first ran 10,000 generations and kept track of the trend of nucleotide diversity to confirm the scaling change had not disrupted the mutation-selection equilibrium (Figure S27). After 10,000 steps we varied the carrying capacity of the simulation to follow the different trajectories of our demographic scenarios for 25,000 steps (Figure 4) . We randomly sub-sampled 200 individuals from the last simulation step to compare the same sampling effort across all scenarios and models. We counted derived mutations for mutation classes of weakly deleterious (−0.001 ≤ s < 0), mildly deleterious (−0.01 ≤ s < −0.001) and strongly deleterious (s < −0.01) selection coefficients. We calculated additive genetic load as in Pedersen et al.95 by adding the sum of selection coefficients for homozygous mutations and the sum of selection coefficients times the dominance coefficients for heterozygous mutations."
    }
}
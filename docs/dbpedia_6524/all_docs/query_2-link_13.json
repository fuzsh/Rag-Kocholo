{
    "id": "dbpedia_6524_2",
    "rank": 13,
    "data": {
        "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9708014/",
        "read_more_link": "",
        "language": "en",
        "title": "From population- to patient-based prediction of in-hospital mortality in heart failure using machine learning",
        "top_image": "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/pmc-card-share.jpg?_=0",
        "meta_img": "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/pmc-card-share.jpg?_=0",
        "images": [
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/favicons/favicon-57.png",
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/icon-dot-gov.svg",
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/icon-https.svg",
            "https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/logos/AgencyLogo.svg",
            "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/logo-ehjdh.jpg",
            "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/corrauth.gif",
            "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/corrauth.gif",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9708014/bin/ztac012ga1.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Sebastian König",
            "Vincent Pellissier",
            "Sven Hohenstein",
            "Johannes Leiner",
            "Andreas Meier-Hellmann",
            "Ralf Kuhlen",
            "Gerhard Hindricks",
            "Andreas Bollmann"
        ],
        "publish_date": "2022-06-16T00:00:00",
        "summary": "",
        "meta_description": "Utilizing administrative data may facilitate risk prediction in heart failure inpatients. In this short report, we present different machine learning models that predict in-hospital mortality on an individual basis utilizing this widely available data ...",
        "meta_lang": "en",
        "meta_favicon": "https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon.ico",
        "meta_site_name": "PubMed Central (PMC)",
        "canonical_link": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9708014/",
        "text": "Since the overall prevalence of heart failure (HF) increases, an improved prognostic assessment is critical for the optimal allocation of resources in patient care. Several scores were introduced to predict HF-related in-hospital and long-term mortality but are, however, not routinely used for various reasons.1,2 Considering data availability to be critical, our group introduced models to calculate expected in-hospital mortality rates based on machine learning (ML) algorithms that implemented widely available administrative data and achieved good performance metrices despite the omission of variables from laboratory tests or imaging.3 Initially focusing on a retrospective analysis of large cohorts for research and quality management purposes, we now adapted different ML models to follow a patient-based approach to mortality prediction and compared them with classic regression analysis.\n\nWe used administrative data of 86 Helios hospitals in Germany including inpatient cases from 1 January 2016 to 31 December 2018. All cases with ICD-10 encoded the main diagnosis of HF were examined in accordance with prior publications.4 Cases with urgent (non-elective) hospital admission and hospital discharge type other than hospital transfer were further studied. NYHA class and comorbidities according to the Elixhauser comorbidity score were identified from encoded secondary diagnoses (see Supplementary material online, Table S1). Cases with missing NYHA class information (n = 5315) were discarded to facilitate adequate model calibration. Different from the previous approach, variables to be captured retrospectively (length-of-stay, length-of-intensive care unit-stay, duration of mechanical ventilation) were excluded and additional variables (presence of a cardiac implantable electronic device: Z95.0; acute myocardial infarction: I21, I23; acute kidney injury: N17, N99.0; coronary artery disease: I22, I25; pneumonia: J10.0, J11.0, J12–18) were considered for model improvement. The data set was randomly split (block-stratified design per outcome and hospital with all cases of a given patient being allocated to the same subset) using 75% for model development and 25% for model testing. Variables being highly sparse/unbalanced (near-zero variance, ratio most frequent/second most frequent variable >95/5) were removed and the remaining variables were scaled and centred (see Supplementary material online, Table S2). Five algorithms were evaluated: logistic regression [generalized linear models (GLM)], random forest (RF), gradient boosting machine (GBM), single-layer neural network (NNET), and extreme gradient boosting (XGBoost). After model tuning (Bayesian model-based optimization) and selecting appropriate hyper-parameters, the predictive abilities of each algorithm were assessed by receiver operating characteristics (ROC) area under the curves (AUCs), precision–recall curve, calibration plots, and F1 statistics. Receiver operating characteristics area under the curves were compared using DeLong’s test. To account for non-linearities, age was entered a polynomial, which degree was considered a hyper-parameter and tuned as described before. The strength of interactions was computed using the partial dependence function and those with higher values were added to the GLM (see Supplementary material online, Figure S1). The individual importance of included variables was juxtaposed for the original and adapted models (see Supplementary material online). Patients’ data were stored in a pseudonymized form and data use was approved by the local ethics committee. Considering the retrospective analysis of pseudonymized clinical routine data, individual informed consent was not obtained.\n\nIn total, 59 074 inpatient cases (mean age 77.6 ± 11.1 years, 51.9% female, 89.4% NYHA Class III/IV) from 69 hospitals (59.6% from high-volume centres) were included. Baseline characteristics were balanced between the training and testing data set as described previously.3 The in-hospital mortality was 6.2%. Applying models within the testing data set, model performance measurements (ROC AUCs) including confidence intervals (CIs) for GLM and ML models were: GLM 0.853 (95% CI 0.842–0.863), RF 0.851 (95% CI 0.840–0.862), GBM 0.855 (95% CI 0.844–0.865), NNET 0.836 (95% CI 0.823–0.849), and XGBoost 0.856 (95% CI 9.846–0.867). Results of the area under the precision–recall curves (AUPRCs) and other discrimination metrices per model are shown in . Computing pairwise comparisons of ROC AUCs, all models outperformed NNET (each P < 0.001), GBM (P = 0.016), and XGBoost (P = 0.001) were superior to RF, and XGBoost revealed a higher ROC AUC compared with GLM (P = 0.023). Uncalibrated Brier scores were 0.051–0.052 for all algorithms without significant improvements after recalibration.\n\nOur analysis demonstrated good performance metrices across tested algorithms for in-hospital mortality prediction in HF patients using administrative data. Focusing on a more practice-oriented approach and therefore discarding variables that need to be assessed at the cases’ end, calculated performance metrices were inferior compared with previously presented models.3 However, with respect to AUCs, our algorithms outperformed other models for in-hospital mortality prediction based both on clinical and billing data.5,6 We refused to implement clinical data into our models in order to guarantee broad applicability that is not dependent on laboratory results or imaging findings. Moreover, the addition of those information did not result in further improvement of model performance in a previous comparable study.5\n\nConcerning different methodological approaches, similar findings with gradient boosting models performing best within ML algorithms but without significantly outreaching results of regression models were reported for other cardiovascular cohorts. Those, however, investigated different endpoints which impede comparability.7,8 When looking at the results of our previous study, the former benefits of all ML algorithms over GLM were no longer verifiable and the clinical relevance of the small difference in ROC AUC between GLM and XGBoost remains questionable. This is likely a consequence of the nature of excluded variables with ML models formerly better accounting for the assumed non-linear influence on in-hospital death. This inconsistency reflects previous studies’ results comparing classical regression models with ML algorithms with some reporting equivalence and others reporting superiority of the latter statistical methods.7–10 However, differing study populations, investigated endpoints, and applied ML models have to be considered in this context and a generally valid statement cannot be made with certainty based on the existing data.\n\nTaking the similar performance of classical regression and ML models in our analysis, is it even worth implementing ML methods in such prediction analyses? Approaching this question calls for the consideration of various aspects. Machine learning techniques can implicitly model complex relations between variables. For example, non-linearities or interactions have to be explicitly specified when using regression techniques, while they are implicit in tree-based or neural network approaches. Moreover, missing values can also be handled implicitly by XGBoost models, with the algorithm gradually performing imputation. Conversely, regressions rely on the pre-specified imputation of missing values. In our analysis, cases with missing values were removed to ensure a fair comparison of methods, but for an application in clinical practice, the ability of handling missing data might prove advantageous for ML algorithms. These advantages come at the cost of a less clear interpretability and the challenges of computing such algorithms."
    }
}
{
    "id": "dbpedia_8579_2",
    "rank": 45,
    "data": {
        "url": "https://simonschreibt.de/gat/renderhell-book2/",
        "read_more_link": "",
        "language": "en",
        "title": "Render Hell – Book II",
        "top_image": "https://simonschreibt.de/wp-content/uploads/2013/09/blog_icon_fav1.gif",
        "meta_img": "https://simonschreibt.de/wp-content/uploads/2013/09/blog_icon_fav1.gif",
        "images": [
            "https://simonschreibt.de/wp-content/uploads/2024/03/header.png",
            "https://data.simonschreibt.de/assets/blog_icon_support.png",
            "https://data.simonschreibt.de/assets/blog_icon_support.png",
            "https://data.simonschreibt.de/assets/steam_smallCapsule_01.png",
            "https://data.simonschreibt.de/assets/flag_ch.png",
            "https://data.simonschreibt.de/assets/flag_ru.png",
            "https://data.simonschreibt.de/gat049/banner_book_02.jpg",
            "https://data.simonschreibt.de/assets/face_simon.png",
            "https://data.simonschreibt.de/gat049/parallel_pipeline_01.png",
            "https://data.simonschreibt.de/gat049/parallel_pipeline_02.png",
            "https://data.simonschreibt.de/gat049/parallel_pipeline_03.png",
            "https://data.simonschreibt.de/gat049/parallel_pipeline_04.png",
            "http://www.legitreviews.com/images/reviews/1100/Fermi_Die.jpg",
            "https://data.simonschreibt.de/gat049/fermipipeline.png",
            "https://data.simonschreibt.de/assets/face_simon.png",
            "https://data.simonschreibt.de/assets/face_simon.png",
            "https://data.simonschreibt.de/gat049/pipeline_workdistribution_03_maxwell.png",
            "https://data.simonschreibt.de/assets/face_simon.png",
            "https://data.simonschreibt.de/assets/face_simon.png",
            "https://data.simonschreibt.de/assets/icon_update_01.png",
            "https://data.simonschreibt.de/gat049/update1/thumbnail_youtube_01_forward.jpg",
            "https://data.simonschreibt.de/assets/steam_smallCapsule_01.png",
            "https://data.simonschreibt.de/assets/supportsimon_achievement_logo_01.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2015-08-16T19:03:20+02:00",
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "https://simonschreibt.de/wp-content/uploads/2013/09/blog_icon_fav1.gif",
        "meta_site_name": "",
        "canonical_link": "https://simonschreibt.de/gat/renderhell-book2/",
        "text": "Pipeline in Detail\n\nMost of the constructive feedback I received about this article was “Nice explanation, but your pipeline is 6 years old!”. I wasn’t sure what that exactly meant until Christoph Kubisch joined my fight in the Render Hell. He is a Developer Technology Engineer working for NVIDIA and whatever question I had, he answered it. And believe me, I had a lot! :)\n\nOur explanation is mostly based on NVIDIA architecture due the fact that Christoph is working for NVIDIA. I hope you don’t misunderstand this as an advertisement. We both just share the passion to explain stuff and I was really happy to have someone who could help me with that technical knowledge.\n\nBe aware that we omitted and simplified some details.\n\nTwo major points of the pipeline will be explained below, which weren’t totally wrong explained here in book I, but might not be clear enough:\n\n1. Not everything is done by the “tiny” GPU cores!\n\n2. There can be several parallel running pipelines!\n\nBelow I’ll go into detail about it. Have fun!\n\n1. Not everything is done by the “tiny” GPU cores!\n\nIn the pipeline example above it might seem like every pipeline stage is done by the GPU Cores – This is NOT the case! In fact, most of the stuff is not done by them. In the new section in “1. Copy the data into system memory for fast access” you already saw that several components are necessary to just bring the data to a core. So what work do the cores actually do?\n\nLet’s observe one:\n\nA core can receive commands and data. Then it executes the command by calculating the data in a floating point unit (FP UNIT) or an integer unit (INT UNIT). So you could say: A core can calculate pixels and vertices (maybe also other calculations like physics but let’s focus on graphic rendering).\n\nOther important stuff like distribute the render-tasks, calculate tessellation, culling and preparing the fragments for the pixel shader, depth testing and writing pixels into the frame buffer are NOT done by the cores. This work is done by special, not programmable hardware blocks which are also placed in the GPU.\n\nOk, after we know that now, let’s move on to the next major point I have to clear out:\n\n2. There can be several parallel running pipelines!\n\nFirst, I’ll give you a short example what I mean with that headline. If you’re still thirsty after this, I’ll take you into an even more detailed explanation.\n\nBut first, let’s recap:\n\nIf we would only have one GPU Core, what could we calculate with it?\n\nCorrect: Nothing! Because the core needs someone who assigns him some work. This is done by a Streaming Multiprocessor which can handle a stream of vertices/pixels which belong to one shader. OK, with that and one core we could calculate one vertex/pixel at a time:\n\nOf course, if we increase the amount of cores, there can be more vertices/pixels calculated at the same time. But only if they belong to the same shader!\n\nThis was already explained in my first attempt of explaining the pipeline! But now it gets interesting: What if we would add another Streaming Multiprocessor which cares about half of the cores?\n\nNow we can not only calculate vertices/pixels in parallel, we can also take care about 2 shader streams at the same time! This means, we can for example run two different pixel shaders at the same time or run a vertex shader AND a pixel shader at the same time!\n\nThis rough example shall just give you a glimpse about that several different hardware blocks are involved and they all work in parallel so the pipeline is more flexible than I described in my first attempt.\n\nAnyone still thirsty? Then let’s get into more detail!\n\n3. In-Depth look into the pipeline stages\n\n3.1 Overview\n\nFirst of all: Why do we need a flexible/parallel pipeline? The reason is, that you can’t foresee, what workload you’ll have. Especially with tessellation it might be, that there are suddenly 100.000 more polygons on the screen than one frame before. Therefore you need a flexible pipeline which handles totally different workloads.\n\nDON’T WORRY!\n\nPlease don’t be afraid by looking at the two following images (like I am when i read a Wikipedia article and see formulas between the text)! Yes, this stuff isn’t easy and even complex charts only show what a programmer needs to know and hide a lot of the “real” complexity. I only show it to give you a rough understand how complicated this stuff is. :)\n\nThe following image shows a GPU. I have no idea about what is what, but it’s kind of beautiful, isn’t it?\n\nAnd here’s an image from Christoph Kubisch’ Article “Life of a triangle”. It shows parts of the work which is happening on a GPU in structured graphics.\n\nI hope you’ve a rough idea about the complexity now and will realize how much the explanations below are simplified. Let’s now have a detailed look on the whole pipeline.\n\n3.2 Application Stage\n\nIt starts with the application or a game telling the driver that it wants something rendered.\n\n3.3 Driver Stage\n\nThe driver receives the commands from the application and puts them into a command buffer (this was already explained here in book I). After a while (or when the programmer forces it), the commands are send to the GPU.\n\n3.4 Read commands\n\nNow we’re on the graphic card! The Host Interface reads commands to make them available for further use.\n\n3.5 Data Fetch\n\nSome of the commands sent to the GPU can contain data or are instructions to copy data. The GPU typically has a dedicated engine to deal with copying data from RAM to VRAM and vice versa. This data could be anything filling vertex buffers, textures or other shader parameter buffers. Our frame would typically start with some camera matrices being sent.\n\nNow that all ingredients are ready the Gigathread Engine comes into play, it creates a thread for every vertex/pixel and bundles them into a package. NVIDIA calls such a package: Thread Block. Additional threads may also be created for vertices after tessellation or for geometry shaders (will be explained later). Thread blocks are then distributed to the Streaming Multiprocessors.\n\n3.6 Vertex Fetch\n\nA Streaming Multiprocessor is a collection of different hardware units and one of them is the Polymorph Engine. For the sake of simplicity I present them like separate guys. :) The Polymorph Engine gets the needed data and copies it into caches so cores can work faster. Why it is useful to copy data into caches was explained here in book I.\n\n3.7 Shader Execution\n\nThe main purpose of the Streaming Multiprocessor is executing program code written by the application developer, also called shaders. There is multiple types of shaders but each kind can run on any of these Streaming Multiprocesors and they all follow the same execution logic.\n\nThe Streaming Multiprocessor now takes his big Thread Block which he received from the Gigathread Engine and separates it into smaller chunks of work. He splits the Thread Block into heaps of 32 Threads. Such a smaller heap is called: Warp. In total, a Maxwell Streaming Multiprocessor can “hold” 64 of such warps. In my example and Maxwell GPUs there are 32 dedicated cores to work on the 32 Threads.\n\nOne Warp is then taken to be worked on. At this point the hardware should have all necessary data loaded into the registers so that the cores can work with it. We simplify the illustration here a bit: Maxwell’s Streaming Processor for example has 4 warp schedulers, which each would let one warp work and manage a subset of the Streaming Multiprocessor’s warps.\n\nThe actual work begins now. The cores itself never see the whole shader-code but only one command of it at the time. They do their work and then the next command is given to them by the Streaming Multiprocessor. All cores execute the same command but on different data (vertices/pixels). It’s not possible that some cores work on command A and some on command B at the same time. This method is called lock-step.\n\nThis lock-step method gets important if you have an IF-statement in your shader code which executes either one code block or another.\n\nThis IF-Statement makes some of our cores execute the left code and some the right code. It can’t be done at the same time (like explained above). First one code-side would be executed and some cores would be “sleeping” and then the other side is executed afterwards. Kayvon explains this here at 43:58 “What about conditional execution?”.\n\nIn my example 16 pixels/vertices would be worked on but it’s of course possible to have IF-statements where only ONE pixel/vertex is calculated and the other 31 cores get masked out. The same applies to loops, if only one core has to stay in the loop long, all the others become idle. This phenomenon is also called “divergent threads” and should be minimized. Ideally all threads in the warp hit the same side of the IF-condition, because then we can entirely jump over the non-active side.\n\nBut why can the Streaming Processor hold 64 warps, when the cores can only work on very few at a time?? This is because sometimes you can’t progress because you have to wait for some data. For example we need to calculate the lighting with the normal from our normal map texture. Even if the normal was in the cache it takes a while to access it, and if it wasn’t it can take a good while. Pro’s call this Memory Stall and this is greatly explained by Kayvon. Instead of doing nothing, the current warp will just be exchanged with one, where the necessary data is ready for use:\n\nThe explanation above was a bit simplified. Modern GPU architecture doesn’t only allow a Streaming Processor working at one Warp at the time. Look for example at this image showing a Streaming Processor (SMM) based on Maxwell architecture: One Streaming Processor has access to four Warp schedulers, each controlling 32 Cores. This makes him able to work on four Warps completely parallel. The book keeping for the work state of multiple threads is kept independently and is reflected by how many Threads the SM can hold in parallel, as noted on top.\n\nBut what are our threads actually working on? For example a Vertex Shader!\n\n3.8 Vertex Shader\n\nThe vertex shader takes care of one vertex and modifies it how the programmer wants. Unlike normal software (e.g. Mail-Program) where you run one instance of the program and hand over a lot data which get taken care of (e.g. handling all the Mails), you run one instance of a vertex shader program for every vertex which then runs in in one thread managed by the Streaming Multiprocessor.\n\nOur vertex shader transforms vertices or its parameters (pos, color, uv) like you want:\n\nSome stages are only executed when Tessellation is used. Click the link below, if you want to see what happens when Tessellation gets into the game."
    }
}
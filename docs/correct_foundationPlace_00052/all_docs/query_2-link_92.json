{
    "id": "correct_foundationPlace_00052_2",
    "rank": 92,
    "data": {
        "url": "https://arxiv.org/html/2312.06718v3",
        "read_more_link": "",
        "language": "en",
        "title": "Large Scale Foundation Models for Intelligent Manufacturing Applications: A Survey",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://arxiv.org/html/x1.png",
            "https://arxiv.org/html/x2.png",
            "https://arxiv.org/html/extracted/5312886/Figure/case1.png",
            "https://arxiv.org/html/x3.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            "Large Sacle Foundation Models",
            "Intelligent Manufacturing",
            "Survey"
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "License: arXiv.org perpetual non-exclusive license\n\narXiv:2312.06718v3 [cs.AI] 22 Dec 2023\n\nLarge Scale Foundation Models for Intelligent Manufacturing Applications: A Survey\n\nHaotian Zhang, Semujju Stuart Dereck, Zhicheng Wang, Xianwei Lv, Kang Xu, Liang Wu, Ye Jia, Jing Wu, Zhuo Long, Wensheng Liang, X.G. Ma , and Ruiyan Zhuang This work was supported in part by the Guangdong Basic and Applied Basic Research Fund Project under Grant 2022A1515140121; and in part by the Initiation Funding of Foshan Graduate School of Innovation, Northeastern University under Grant 200076421002 Haotian Zhang, Zhicheng Wang, Xianwei Lv, Kang Xu, Liang Wu, and X. Ma are with the College of Information Science and Engineering, Northeastern University, Shenyang 110819, China, and the Foshan Graduate School, Northeastern University, Foshan 528311, China. (E-mail:2200966@stu.neu.edu.cn, 2390108@stu.neu.edu.cn,2270888@stu.neu.edu.cn, maxg@mail.neu.edu.cn).Wensheng Liang is with the School of mechanical engineering and automation, South China University of Technology, Shenyang 110819, China. (E-mail:2200385@stu.neu.edu.cn).Semujju Stuart Dereck is with the School of Software Engineering, South China University of Technology, Guangzhou 510006, China. (E-mail:stuartsemujju@gmail.com).Jing Wu is with the Departement of Computer and Information Science, Qinghai University of Science and Technology, Xining, China. (E-mail:mirror_a3@hotmail.com).Ruiyan Zhuang is with the Enterprise AI, Midea Group, Foshan 528311, China. (E-mail:zhuangry@midea.com).\n\nAbstract\n\nAlthough the applications of artificial intelligence especially deep learning had greatly improved various aspects of intelligent manufacturing, they still face challenges for wide employment due to the poor generalization ability, difficulties to establish high-quality training datasets, and unsatisfactory performance of deep learning methods. The emergence of large scale foundational models(LSFMs) had triggered a wave in the field of artificial intelligence, shifting deep learning models from single-task, single-modal, limited data patterns to a paradigm encompassing diverse tasks, multimodal, and pre-training on massive datasets. Although LSFMs had demonstrated powerful generalization capabilities, automatic high-quality training dataset generation and superior performance across various domains, applications of LSFMs on intelligent manufacturing were still in their nascent stage. A systematic overview of this topic was lacking, especially regarding which challenges of deep learning can be addressed by LSFMs and how these challenges can be systematically tackled. To fill this gap, this paper systematically expounded current statue of LSFMs and their advantages in the context of intelligent manufacturing. and compared comprehensively with the challenges faced by current deep learning models in various intelligent manufacturing applications. We also outlined the roadmaps for utilizing LSFMs to address these challenges. Finally, case studies of applications of LSFMs in real-world intelligent manufacturing scenarios were presented to illustrate how LSFMs could help industries, improve their efficiency.\n\nIndex Terms:\n\nLarge Sacle Foundation Models, Intelligent Manufacturing, Survey\n\nI Introduction\n\nManufacturing industries are mainstays of a nation’s economy, and several countries had announced strategic roadmaps to promote applications of new techniques of manufacturing to ensure their leadership in this area, e.g., Industry 4.0 of Germany[1], the Smart Manufacturing Leadership Coalition (SMLC) in the U.S[2], and China Manufacturing 2025[3]. Over the past several decades, manufacturing was getting more intelligent by deploying new technologies such as sensors, internet of things (loT), robotics, digital twins, and cyber-physical systems (CPSs)[4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], wherein unprecedented amount of data were continuously generated and captured at all stages of manufacturing processes. Therefore, optimal data-processing algorithms were highly desired to realize efficient fault diagnosis and predictive maintenance, quality control, human operations, process optimization, and many other smart decision-makings needed for intelligent manufacturing[16, 17, 18, 19, 20]. Statistics showed that 82% of the industrial activities using intelligent manufacturing technologies obtained improved efficiency and performance[21, 16]\n\nThese improvements of intelligent manufacturing largely attributed to the implementations of various machine learning algorithms with increasing scale and complexities of manufacturing data, wherein many advanced data-driven methods had been studied and employed to enable large-scale data processing capabilities with high efficiency and powerful decision-making abilities for highly non-linear optimizations, both of which were commonly required in sophisticated manufacturing activities. Table 1 listed some review papers in this area[22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87].\n\nTraditional machine learning methods, such as support vector machines, K-nearest neighbors. Naive Bayes, etc., could improve performance of decision-making[88, 89, 90], production line schedule[91, 92], machine maintenance arrangements[93, 94], failure prediction[95, 96, 97], quality assessment[98, 99], and defect detection[100, 101] in manufacturing to certain extents. However, they relied heavily on handcrafting feature engineering to represent data with domain knowledge and were lack of capabilities to handle highly non-linear relationships among large-scale data, limiting their applications in intelligent manufacturing[102, 103].\n\nDeep learning, as one of the advanced machine learning methods, could enable automatic feature extractions and pattern recognitions from high-dimensional and nonlinear raw data by employing multi-level neural network architectures, making them more adaptive to the complicated data treatments of intelligent manufacturing. ln the last decade, deep learning methods was the mainstream data-driven approaches used in various areas of intelligent manufacturing, such as health management (PHM)[104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124], quality control[125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141], robotic[142, 143, 144, 145, 146, 147, 148], and human activity recognition[65, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164].\n\nAlthough deep learning showed high level abstract representation abilities of feature learning, with great end-to-end decision-making modeling capabilities and significantly, reduced need for human-labor, allowing it to greatly push the development of intelligent manufacturing, it still faced significant difficulties when being employed[165, 44, 166, 167, 168]. Firstly, the performance of small customized deep learning models tailored for specific patterns and objectives was limited. These models suffered from issues like limited generalization, poor interpretability, vulnerability to attacks, and could not meet the requirements of enterprises for intelligent production and management, especially in complex tasks with diverse data[169, 170, 171, 172, 173, 174] . In addition, they could only handle individual tasks in a scattered and loose coupling manner[175, 176, 177].\n\nSecondly, growing requirements of data scale and dataset establishment cost constrained the performance of deep learning models. As a data-driven approach, deep learning models rely on fitting the relationships between inputs and outputs wherein the volume and quality of the training datasets play critical roles[165]. Although the use of new technologies like sensors and the Internet of Things had made it possible to efficiently collect a massive amount of data[178, 179, 180, 181, 182, 183], these data were often unevenly distributed, noisy, lacking of labels, and contained a substantial amount of unstructured ones. As a result, these data were inadequate for training good deep learning models. Meanwhile, deep learning models were insufficient to handle large scale data with high efficiency.\n\nRecent emergence of large scale foundation models[184, 185, 186, 187, 188] were usually trained through extensive self-supervision and had exhibited robust generalization capabilities, exceptional zero-shot performance, and impressive multi-modal fusion abilities, as evidenced by successes in a wide range of downstream tasks, such as natural language processing, computer visions, etc.[189, 190, 191, 192, 193, 194, 195, 196]. Although the endeavor of using LSFMs to tackle challenges in intelligent manufacturing was just beginning, some progress had already been made attempted. [197, 198] discussed the potential applications of LSFMs in industrial manufacturing, but limited to specific industrial tasks or specific LSFM. Ji et al.[199] presented a quantitative comparison of the performance of vision foundational models in concealed scenarios with state-of-the-art deep learning models. Ogundare et al.[200] proposed a study on the resilience and efficiency of industrial automation and control systems generated by large language models (LLMs).\n\nAlthough LSFMs showed great potentials in intelligent manufacturing, wherein powerful generalization ability, automatic high-quality training dataset generation and superior performance were highly desirable, studies in this areas remain in its early stages and a systematic review of LSFMs for intelligent manufacturing applications is still not available. This paper presented technical roadmaps for using LSFMs in intelligent manufacturing where deep learning methods met great obstacles. Our work aimed to provide guiding directions and discussions to help understand how LSFMs can benefit intelligent manufacturing.\n\nThe rest of this paper was organized as follows. Section II described the challenges encountered by deep learning models in intelligent manufacturing. In Section III, we initially provided a brief overview of the current progresses of LSFMs, and subsequently we discussed the technical advantages of LSFMs in intelligent manufacturing that addressed the challenges faced by deep learning. Section IV delineated the roadmaps of applying LSFMs in intelligent manufacturing. Finally, in Section V, we illustrated how LSFMs could make progresses in intelligent manufacturing through several cases we applied in real manufacturing scenes.\n\nII Progresses and challenges of Deep Learning in Intelligent Manufacturing\n\nPioneering work of artificial neural network started around Dartmouth workshops in 1956[201], and 1960s-1990s witnessed the development of machine learning when back propagation[202], Boltzmann machine[203],K-nearest neighbor[204],support vector machine[205], among many other classical machine learning methods were invented to solve classification and regression issues in various scientific and industrial areas.\n\nThe couple of decades around AD 2000 saw the evolution of deep learning which could extract non-linear and complicated features automatically from raw data with the helps of high-scale modeling and end-to-end optimization capabilities. Popular deep learning algorithms such as recurrent neural networks(RNN)[206], long short-term memory(LSTM)[207],convolution neural networks(CNN)[208], deep belief network[209], deep auto encoders, etc., were developed and widely used in various areas of intelligent manufacturing, e.g., prognostics and health management (PHM), quality control, human activity recognition, and industrial robotics.\n\nII-A Progresses of deep learning in Intelligent Manufacturing\n\nII-A1 Prognostics and health management (PHM)\n\nPHM applies advanced technologies and methodologies for the monitoring and assessment of the health status of systems or equipments. The implementation of PHM aimed to avoid equipment failures and production halts, thereby enhancing production efficiency and reducing maintenance costs [210]. Traditional PHM methods primarily relied on machine learning technologies, including Support Vector Machines (SVM) [211], Random Forests [212], Principal Component Analysis (PCA) [213], Particle Filtering [214], and Hidden Markov Models (HMM)[215]. These machine learning technologies often relied on expertise knowledge to manually select and extract desired features from data[216, 217].\n\nNevertheless, with the manufacturing industry’s progressive shift towards intelligent operations, novel challenges to traditional machine learning based PHM methods were introducted including data processing capacity, immediacy of response, and the adaptability to a broad range of scenarios.\n\nIn this context, deep learning methods, specifically those based on neural network models such as CNNs[218], RNNs[219], and LSTMs[220], have demonstrated considerable improvements in PHM. For instance, Belmiloud et al. [221] utilized Wavelet Packet Decomposition (WPD) alongside deep CNNs for feature extraction from bearing data, facilitating Remaining Useful Life (RUL) prediction. Li et al. [222] and Zhu et al. [223] investigated multi-scale feature extraction techniques, wherein CNNs could represent various facets of the original data via interconnected convolutional and pooling layers more efficiently. Furthermore, some methods have enhanced PHM performance by combining different deep learning algorithms. For example, the CNN-LSTM model applied by Yue et al. [224] was to address icing issues on wind turbine blades. Subsequently, Chen et al. [225] extended its application to the prognosis of the same issue, demonstrating its potential in resolving PHM challenges in specific industrial applications.\n\nAlthough deep learning-based PHM strategies had achieved commendable progresses, they faced substantial challenges for broader applications [27], including the accurate acquisition and establishment of industrial datasets [226], poor interpretability of PHM systems [227], and lack of effective and stable capabilities to integrate with pre-existing industrial frameworks [35]. These identified challenges not only delineated the trajectory for future research but also underscored the need for more innovative solutions to propel the development and applications of PHM technologies.\n\nII-A2 Quality control\n\nThe core logics of traditional quality control methodologies was the utilization of statistical analysis and rule formulation to ensure adherence of product quality to established standards and requirements. In the early stages of development, Shewhart [228] implemented control charts plotting sample means and standard deviations to monitor the state of control within production processes. The subsequent adoption of Six Sigma markedly reduced the rate of defects in industrial processes, establishing itself as a pivotal tool in traditional quality control [229]. However, these conventional methods harbored inherent limitations. Firstly their static nature rendered them ineffectual in adapting to changes and fluctuations within production processes, consequently impeding the transferability of quality control models [230]. Additionally, their reliance on predefined rules and experiential knowledge proved insufficient in fully harnessing the extensive data generated in intelligent manufacturing, leading to inaccuracies in quality control and defect detection [231]. Furthermore, their capacity to process unstructured and image data was significantly constrained [232].\n\nIn recent years, the focal point of research in the quality control domain has shifted towards the applications and enhancement of deep learning technologies. Deep learning models demonstrated exceptional capability in processing complex unstructured data, such as images and sounds, which significantly enhances their performance in product defect detection and classification [131, 233, 45]. Moreover, the profound feature learning ability of deep learning enables the automatic extraction of valuable features from extensive data sets, substantially improving the accuracy and efficiency of quality control measures [234, 235]. For instance, Zhang et al. [236] developed an efficient method for surface defect detection in electronic components using Convolutional Neural Networks (CNN). In a similar vein, Essien et al. [237] employed LSTM models to model industrial production processes, achieving real-time quality control and fault prediction, thereby reducing downtime. Concurrently, Stricker et al. [238] proposed an adaptive control methodology based on deep reinforcement learning, capable of dynamically adjusting quality control strategies based on real-time data, underscoring the potential of deep learning in adaptive quality control applications.\n\nIn summary, deep learning has achieved significant milestones in the realm of quality control. It has not only enhanced the accuracy of product defect detection but also improved the efficiency and stability of production processes. These advancements have introduced novel tools and methodologies to the industrial sector, promising further improvements in product quality and reductions in production costs [139, 239, 240].\n\nII-A3 Human action recognition(HAR)\n\nIn intelligent manufacturing, HAR referred to the utilization of technology and methods to monitor, analyze, and identify the behaviors and movements of employees in work environment, and was applied to enhance safety, quality control, efficiency, and productivity. In recent years, deep learning technology had made significant strides in the field of HAR within industrial manufacturing[241, 242, 243, 244, 245]. This progress was largely due to its potent feature learning capabilities and the ability to capture complex data patterns. Manufacturers collected motion data from various sensors and utilized deep learning models such as CNNs[246, 247], RNNs[248], and LSTMs[249, 247] to monitor and analyze workers’ activities in real-time. These HAR systems were capable of identifying unsafe work behaviors and issuing timely warnings, which reduced workplace injuries and increased the safety of the production environment. For quality control, HAR helped in detecting and correcting operational errors by monitoring deviations from standard operating procedures, thereby reducing the incidence of defects. Additionally, by analyzing workers’ workflows, HARs could uncover inefficiencies within the production process, leading to the optimization of operational procedures and an increase in production efficiency. In terms of equipment maintenance, HARs were monitored to prevent equipment failures and minimize downtime. HAR could also be used to identify training needs, providing a foundation for improving worker skills and minimizing operational errors.[250, 251, 252, 253]\n\nOn the technical front, multimodal learning emerged as a key development in HARs, enhancing the accuracy and robustness of activity recognition by combining visual and other sensor data. Transfer learning and domain adaptation techniques were employed to mitigate the annotation scarcity problem by leveraging pre-trained models and cross-domain knowledge[254, 255], thus reducing the dependency on extensive labeled data. To address the class imbalance issue, researchers adopted strategies like resampling techniques, cost-sensitive learning, and ensemble learning. Unsupervised and semi-supervised learning methods also found applications in HARs, especially when labeled data was scarce[255, 256]. In summary, deep learning had shown enormous potential in the realm of industrial HAR. However, there was a continuous need to resolve challenges related to data, algorithms, and privacy[257, 258, 259, 260, 261, 262]. Future research was expected to focus on developing more efficient algorithms, improving the generalization capabilities of models, and advancing the widespread applications of HAR technology in industrial production, meanwhile protecting worker privacy.\n\nII-A4 Industrial Robotics\n\nIndustrial robots were automated mechanical devices typically controlled by computer programs and used to perform various repetitive, precise, or high risk industrial tasks. These robots usually had multi-axis motion capabilities, equipped with various sensors and tools, capable of executing tasks in industrial fields such as manufacturing, assembly, and handling. Deep learning (DL) technology had marked the advent of a new era of intelligence in manufacturing with its progress in industrial robotics applications. The core advantage of deep learning lay in its exceptional data processing and pattern recognition capabilities, which enabled robots to perform more complex and flexible tasks within intricate manufacturing settings[263][264]. For instance, by leveraging CNNs, robots had been capable of performing precise visual inspections, identifying manufacturing defects in real-time, and classifying them. This automated quality control not only improved product consistency but also significantly reduced the need for manual inspections. In the realm of predictive maintenance, deep learning had enabled robots to anticipate potential equipment failures and carry out maintenance proactively by analyzing historical operational data. This was achieved through Recurrent Neural Networks (RNNs) and Long Short-Term Memory networks (LSTMs), which could handle and predict complex patterns in time-series data.[265, 266, 267]. Moreover, Deep Reinforcement Learning (DRL) technology was being utilized to develop autonomous robots capable of operating in unmanned or hazardous environments, thereby enhancing the safety and efficiency of operations.[268] Deep learning also played a crucial role in the control and optimization of assembly robots. Through deep learning algorithms, robots had been able to adapt to changing assembly conditions, collaborate with human operators, and learn from past experiences to improve future performance. These technologies were also used to monitor the actions of robots, identifying potential safety risks and further enhancing workplace safety[269, 270].\n\nDespite the significant advantages that deep learning applications brought to the field of industrial robotics, such as increased production efficiency, waste reduction, improved product quality and safety, there were also challenges and drawbacks. Deep learning models typically required large amounts of labeled data for training, which could be difficult and costly to obtain in real industrial environments. Additionally, the decision-making process of deep learning models often missed transparency, which could be problematic in industrial applications that demanded high reliability. Moreover, the computational intensity of deep learning algorithms could pose challenges for real-time applications[271][272]. In conclusion, the application of deep learning technology in the field of industrial robotics was achieving significant breakthroughs in the automation and intelligentization of production. However, to realize its full potential, further research and development were needed in the areas of algorithm interpretability, data efficiency, and computational optimization.\n\nII-B Challenges of Deep Learning in Intelligent Manufacturing\n\nII-B1 Poor generalization capability\n\nOverfitting. Overfitting refers to the phenomenon where a model excessively adapts to the training dataset during the training process, resulting in the model performing well on the training data but having poor generalization ability for new data. This issue should be particularly noted in the application scenarios of intelligent manufacturing because production manufacturing often involves relatively uniform samples and fixed tasks. Models trained under such conditions, even if they achieve high accuracy, should be suspected of being caused by overfitting. Overfitting severely damages the model’s generalization, making even slight changes in samples or the environment likely to cause the model to fail.\n\nThe applications of deep learning algorithms in manufacturing encountered challenges, particularly in terms of poor generalization performance[273, 131, 274, 134, 135, 275] . The intricacies of manufacturing scenes introduced various domain shifts, often occurring in diverse and complex conditions. These shifts posed hurdles for deep learning applications, highlighting the need for robust adaptation strategies to ensure effective and reliable performance across the dynamic landscape of manufacturing processes.\n\nPoor domain knowledge integration. The challenge of poor domain knowledge integration stands as a significant hurdle in the application of deep learning to intelligent manufacturing. Domain knowledge, include expertise about specific manufacturing processes, materials, and industry-specific constraints, was invaluable for creating effective and dependable AI solutions. However, integrating this crucial knowledge into deep learning models was non-trivial for several reasons:\n\nImplicit and tacit knowledge: Often, domain expertise resides within experts in a form that is implicit or tacit [105, 276, 277], This posed a formidable challenge in terms of explicit articulation and encoding into deep learning models. This inherent complexity hindered the seamless integration of critical knowledge for effective deep learning applications in intelligent manufacturing.\n\nInterdisciplinary nature of domain knowledge: Intelligent manufacturing often spanned multiple disciplines, such as mechanical engineering, materials science, automation, and more[274]. The integration of expertise from these diverse domains was inherently complex and necessitated a well-defined methodology for deep learning models. Effectively incorporating insights from various disciplines into the fabric of intelligent manufacturing posed a critical obstacle to achieve seamless and holistic domain knowledge integration.\n\nData-driven approach vs. Knowledge-driven approach: Deep learning models traditionally leaned heavily on training datasets to learn patterns, and finding a balance with the integration of crucial domain knowledge [131] became challenging, especially in scenarios where labeled data were scarce. Striking an effective equilibrium between these contrasting approaches was essential for ensuring the robust integration of domain knowledge in intelligent manufacturing applications.\n\nPoor unstructured data representation. Unstructured data encompassed information lacking of a predefined data model or specific organization. In manufacturing environments, this category included data from diverse sources such as images, videos, text documents, and sensor readings. The challenges of using poor unstructured data in the context of deep learning for intelligent manufacturing were discussed below.\n\nLack of standardization: The absence of standardization in data formats and labeling conventions increased the complexity of pre-processing and analysis when employing deep learning in intelligent manufacturing [278]. The lack of uniformity across these aspects also posed a substantial obstacle to the seamless integration of unstructured data, requiring additional efforts in normalization and compatibility for effective utilization in deep learning models.\n\nHigh dimensionality and high heterogeneity: The high dimensionality and high heterogeneity of data often existed in various levels of intelligent manufacturing of various time periods, products, fabricationlines, etc., such as images or videos, characterized by an extensive amount of information [65, 139, 149, 276], posed a significant challenge. Dealing with these complex datasets in the context of intelligent manufacturing required computationally intensive processes and extra-special processing.\n\nFeature extraction: The task of feature extraction from unstructured data, involving the identification of meaningful patterns in images or relevant information from text, could be a non-trivial challenge demanding extra domain-specific expertise [65]. This required specialized knowledge to discern and capture pertinent features, emphasizing the complexity of representing unstructured data effectively for intelligent manufacturing.\n\nData volume and storage: The volume of data generated by unstructured sources, such as images or videos, posed a significant logistical challenge in terms of efficient storage, retrieval, and processing [279]. Dealing with these substantial datasets required careful considerations to ensure optimal management and utilization of deep learning.\n\nII-B2 Limited high-quality training dataset\n\nLimited labeling resources. The challenge of limited high-quality training datasets for deep learning in intelligent manufacturing was accentuated by the scarcity of labeling resources. The process of annotating data for training could be exceptionally time-consuming with high costs, especially for special industrial dataset establishment wherein high expertise was needed. This scarcity in labeling resources imposed significant restrictions on both the size and diversity of the training dataset [126, 125, 127, 128, 129, 130, 131, 132, 133].\n\nVariety and complexity. The challenge of a limited high-quality training dataset could be further intensified by the variety and complexity nature of manufacturing environments, wherein diverse operating conditions, various product types, and a multitude of defect types might result in a high level of diversity in the training data [279]. Incorporating this diversity was essential for training deep learning models capable of effectively generalizing and adapting to the intricate and varied intelligent manufacturing scenarios\n\nImbalance and rarity. The imbalance and rarity of certain faults or defects in the manufacturing process often existed wherein specific issues occurred infrequently resulting in severe class imbalance issues within the dataset, posing a significant challenge for deep learning models to deliver accurate performance [65].\n\nSpecificity and uniqueness. Some manufacturing processes were highly specialized or even unique in particular industries[280]. Acquiring relevant data for constructing a training set became particularly challenging when dealing with these processes. This specificity and uniqueness presented a barrier to assemble a comprehensive dataset, crucial for training deep learning models to effectively grasp the intricacies of these specialized manufacturing procedures.\n\nAnonymity and privacy concerns. Anonymity and privacy concerns were significant for indusrtial companies. This restricted special algorithms like federal data sharing for research or other collaborative purposes, wherein this constraint significantly limited the availability of public datasets, crucial for efficient training of deep learning models [281].\n\nDynamic environments. Dynamic environments in manufacturing ariseD as processes evolved over time, influenced by factors such as changes in raw materials, equipments and production techniques. This evolution introduced a mismatch between training datasets and data obtained from actual operating conditions [104]. Adapting deep learning models to these dynamic shifts became difficult due to ever-changing manufacturing settings.\n\nII-B3 Unsatisfactory performance\n\nSingle task, single modal Most of the current deep learning algorithms could be effective only on a single task after sufficient training onspecially prepared datasets, as illustrated in Fig1. When the same algorithm was applied on diffrtrnt tasks, using a different training dataset, its accuracy and efficiency usually decreased dramaticlly[131, 136, 128, 130, 132, 133]. This restricted their performance in complex tasks and hindered their ability to effectively utilize diverse information. Even the deep learning models specifically designed for multitasking often only involved coupling multiple models together[282, 283].\n\nUnsatisfactory accuracy Accuracy referred to the ability of a model to make correct predictions or decisions. In manufacturing environments, accuracy was crucial for tasks such as defect detection, quality control, and process optimization. Although deep learning models achieved remarkably high accuracy on specific tasks in cases where there were sufficient high-quality training datasets[289, 290]. Certain situations still significantly diminished the accuracy of deep learning, including insufficient data labeling, data fluctuations, excessively complex process objects, and noise interference, etc.[129, 131, 139, 284, 80]\n\nLack of interpretability Interpretability referred to the ability to understand and explain the decisions made by a deep learning model. In many manufacturing scenarios, it was crucial to have a clear understanding of why a model made a particular prediction or decision, and a transparent decision-making process could facilitate subsequent improvements [65] and aid in the identification and analysis of failures when they occurred [131].\n\nIndusrtial network security In intelligent manufacturing, employed deep learning networks could be vulnerable to malicious adversarial attacks, wherein the utilization of adversarial samples, which where imperceptible to the human eye, might have the neural networks product entirely incorrect outputs [285, 286]. This posed challenges for the deployment of deep learning algorithms in practice, as such attacks might go unnoticed and cause direct financial losses to businesses [287, 288].\n\nIII Background for Large Scale Foundation Models\n\nIII-A Progresses on Large-Scale Foundation Models(LSFMs)\n\nFoundation models were designed to be trained on large-scale datasets, i.e., with parameters over billions to hundreds of billions and were named for the first time recently[184]. These models could have fixed most parameters after pre-training and adapted to a wide range of downstream applications through fine-tuning. In fact, large scale foundation models(LSFMs) had achieved revolutional progresses in the field of natural language processing[291], computer vision[292] etc.\n\nSubsequent fine-tuning processes could enhance its factual accuracy and ensure its performance to align with desired behaviors. In various professional and academic benchmark tests, GPT-4 had demonstrated performance comparable to human being levels, partcularly in fields such as human-machine interaction, education, healthcare, and law. The LlaMA model[299] was currently the most popular open-source LLM, available in four sizes: 7B, 13B, 30B, and 65B. As LlaMA was pretrained on English corpora, it often necessitated fine-tuning with instructions or data from the target language when used, giving rise to a series of expanded models[300, 301, 302] that constituted the LlaMA family.\n\nFor visual models, the Segment Anything Model (SAM)[292] had ushered computer vision into the era of LSFMs. SAM was composed of three main components: task, model, and data. Firstly, SAM’s tasks involved designing a versatile model capable of transferring zero-shot capabilities to new image tasks and distributions. Inspired by ideas from LLMs, segmentation tasks were designed to be promptable so that it could refine valid segmentation masks from a given prompt. Secondly, SAM’s model framework comprised three parts: an image encoder, a prompt encoder, and a mask decoder. The image encoder utilized a ViT[303] based on the MAE[304], wherein each image was processed once to obtain embeddings before running prompts. The prompt encoder handled sparse (points, boxes, text) and dense (masks) prompt inputs. The mask decoder employed a bidirectional Transformer decoder with self-attention and cross-attention mechanisms based on prompt images to map image embeddings, prompt embeddings, and output labelings onto masks. Lastly, recognizing the inadequacy of existing semantic segmentation datasets for training LSFMs, SAM introduced a data engine, a data generation approach involving iterative training-annotation cycles to obtain a substantial amount of annotated data. Establishing the SA-1B dataset, which consisted of 11 millions of images and 1.1 billions of masks with 99.1% of them generated automatically, making it the largest segmentation dataset currently available. Compared to SAM, Segment Everything Everywhere All at Once(SEEM) model[305] offered broader interactional and semantic capabilities. It not only facilitated segmentation and category recognition in images and videos but also supported a wider range of input modalities, including points, bounding boxes, scribbles, text, speeches, and more. This attributed to SEEM’s utilization of a unified prompting encoder, which encoded all types of prompts into a unified feature space. For the convenient deployment of visual foundation models, the unified feature optimization(UFO) model[306] squeezed multiple tasks into a medium-sized model and further pruned it when transferring to down-stream tasks. This approach ensured flexible deployment of the UFO while retaining the advantages of large-scale pre-training.\n\nMultimodal information processing was one of the important features and advantages of LSFMs and many multimodal methods were proposed recently for various downstream tasks. CLIP[307] connected text and images by creating a transferable visual model and taking a significant step towards breaking the traditional paradigm of computer vision. It learned the matching relationship between text-image pairs through contrastive pre-training methods, involving both Text Encoder and Image Encoder models. BLIP[308] and GLIP series[309, 310], made improvements based on CLIP by incorporating both image-text matching and image-text generation tasks during pre-training. This approach could generate images from text prompts or generate text descriptions from images. Grounding DINO[311] combined the strengths of CLIP and DINO, enabling it to detect arbitrary objects based on human input. Diffusion model was also widely used in multimodal models[312, 313, 314]. Inspired by non-equilibrium thermodynamics, the diffusion model applied forward diffusion to perturb the distribution of data and then learned to recover the data distribution through reverse diffusion.Not confined to aligning only images and text, DreamFusion[315] and Magic3D[316] linked text with 3D content, ImageBind[317] connected six data modalities: images, text, audio, depth, thermal data, and IMU data, forming a unified embedding space, and NExT-GPT[318] could combine multimodal input to generate content. PaLM-E[319] combined language, vision, and robot commands for specific reasoning, visual language, and language tasks, texts could be generated to regulate low-level commands, facilitating control and planning of robots.\n\nIII-B Advantages of Large Scale Foundation Models(LSFMs)\n\nIII-B1 Powerful generalization ability\n\nLarge scale datasets with high dimensions and complexity. Although traditional deep learning models excelled in the applications of intelligent manufacturing, due to constraints in data scale and model expressive capabilities, these models could only perform well on specific tasks and were unable to give satisfactory performance on other tasks. In contrast, LSFMs, with significantly increased parameter quantities, e.g. ChatGPT with 1.42 trillions of parameters in its training dataset, greatly enhanced the model’s expressive power, allowing for better modeling of general knowledge of massive training data containing enough variety, complexity, imbalance, rarity, specificity, uniqueness, anonymity, etc.. This meant that a single LSFM could adapt well to various downstream tasks, and researchers needed only to train one model, or even directly used publically available LSFMs to comprehensively address numerous industrial tasks. Bubeck et al.[273] considered that, given the breadth and depth of GPT-4’s capabilities, it could reasonably be regarded as an early version of Artificial General Intelligence (AGI) system, as it could tackle novel and challenging tasks spanning various fields such as mathematics, coding, visual, medical, legal, psychology, and demonstrated performance compareable to human being levels. Ji et al.[320] investigated the segmentation performance of SAM in anomaly surface defect detection tasks, and empirical evidence showed that SAM performed acceptably across different industrial tasks and human-machine interaction yielded even better results.\n\nSuperior data human-machine interaction capability. The emergence of multimodal LSFMs further enhanced the interaction capabilities of intelligent agents with humans, allowing for a more diverse range of tasks to be performed based on human prompts or instructions. SAM[292] was designed and trained to be promptable, supporting multimodal prompts such as text, keypoints, and bounding boxes, wherein users could easily segment specified objects using prompts like a point, a box, or a sentence. It even accepted input prompts from other systems, like visual focus information from AR/VR headsets, to select corresponding objects. The multimodal capability also allowed people to directly control robots using natural language[319, 321, 322, 323], for example, by saying ”Take the chips from the drawer,” which involved multiple steps of action. In the field of robot operation, visual-language tasks, and language tasks, LSFMs trained for multitasking exhibited a higher level of performance compared to deep learning models trained for a single task. This led to high cross-task transfer efficiency in robot tasks, effectively extending the capabilities of robot control models and improving their performance and flexibility when handling complex tasks.\n\nIII-B2 Superior performance\n\nMulti-task, multi-modal. Compared to deep learning models designed for specific tasks on specific datasets, LSFMs had significant advantages and could integrate data from multi modals, achieving superior performance across multi tasks. Initially, the multi-task applicability of LSFMs significantly reduced research and development time and costs in practical intelligent manufacturing applications. Secondly, leveraging multi-modal inputs, which encompassed diverse data produced in manufacturing scenarios such as visual, linguistic, time series, and expert prior knowledge, often resulted in better performance compared to single-modal models[324, 325]. Moreover, the multi-task, multi-modal nature broadened the application boundaries of LSFMs in intelligent manufacturing. For instance, while traditional object detection algorithms could only detect fixed categories, LSFMs accomplished detection on open-set object[311], which undoubtedly better suited real-world application scenarios.\n\nSuperior accuracy. The improvements brought by LSFMs were evident in two aspects. On one hand, pre-trained models themselves already exhibited high accuracy. GPT-4, for example, achieved scores approximately in the top 10% of test-takers in lawyer exams simulating[291]. Zero-shot CLIP outperformed fully supervised methods on 16 out of 27 tested datasets[307]. SAM delivered optimal performance in single-point effective masking and instance segmentation tasks, and its performance in object proposals surpassed baseline models for medium and large objects, as well as rare and common objects. It only fell short on small and frequent objects. In edge detection tasks, SAM lagged behind state-of-the-art edge detection algorithms but outperformed classical zero-shot transfer methods[292].\n\nOn the other hand, while directly applying LSFMs in some scenarios might not be as effective as training dedicated neural networks for those specific application scenarios, LSFMs could further enhance their performance through fine-tuning. To address SAM’s suboptimal performance when segmenting obscured, weakly bounded, low-contrast, small, and irregularly shaped target objects, Wang et al.[326] used shadow data combined with sparse cues for fine-tuning SAM. They also incorporated a Long Short-Term Memory network, surpassing state-of-the-art techniques[327] and improving the Mean Absolute Error (MAE) and Intersection over Union (IoU) by 17.2% and 3.3%, respectively. For medical images typically characterized by weak boundaries and irregular shapes, SAMed[328] applied a low-rank-based (LoRA) fine-tuning strategy to SAM’s image encoder and finetuned it together with the prompt encoder and the mask decoder on labeled medical image segmentation datasets. It performed on par with state-of-the-art methods[329, 330] on the Synapse multi-organ segmentation dataset.\n\nLogical reasoning ability. For LSFMs, it was discovered that when model size exceeded a certain threshold, the model could offer unpredictable emergent abilities, such as autonomous logical reasoning abilities. This was referred to as Chain-of-Thought (CoT)[331, 332], which wasn’t simply about constructing input-output pairs as prompts but involved incorporating intermediate reasoning steps into the prompts, leading to the final output. In essence, LSFMs would break down complex tasks into finer sub-tasks and then formulate strategies to accomplish these sub-tasks. Leveraging this feature, LSFMs could tackle more complex problems that were previously challenging for deep learning models. Suzgun et al.[332] selected 23 challenging tasks from the BIG-Bench evaluation method[333], including causal reasoning, word sorting, and action comprehension, wherein prior language model evaluations did not surpass average human performance. However, when CoT was applied, Pa-LM[319] exceeded human average performance in 10 tasks, and Codex[334] surpassed human average performance in 17 tasks. In reality, LSFMs could assist humans in performing more advanced activities[335], such as tasks related to intelligent manufacturing, like production planning and material allocation scheduling by employing logical reasoning ability of LSFMs.\n\nSuperior analysis capability . Natural language is one of the most common and frequent means of information exchange among humans. The powerful natural language capabilities of LLMs implies many potential application advantages. Unlike explicit instructions, natural language can be ambiguous and carry multi-layered information. [273] discovered that LSFMs not only had the ability to understand natural language but also could infer the underlying beliefs, emotions, desires, intentions, and knowledge, among other psychological states behind the language. As a crucial aspect of intelligence, in addition to understanding others’ language, LSFMs also had the capability to comprehend their own behavior to some extent. In [273], the quality of explanations was assessed using output consistency and process consistency, wherein GPT-4 exhibited reliable output consistency with a lack of processing consistency, providing reasonable explanations about how predictions were made and thus deepening the understanding of the tasks themselves.\n\nIII-B3 Automatic high-quality training dataset generation\n\nSemi-supervised / self-supervised. Self-supervised methods had already been used in various LSFMs for automatic training dataset establishment. The self-supervised training mode of LSFMs significantly reduced the need for accurate data labels, making it easier to obtain large amounts of unlabeled data. For instance, BERT[336] employed portions of unannotated text from the BooksCorpus and Wikipedia. It utilized two self-supervised tasks for training, and the trained model achieved optimal performance on 11 downstream tasks through fine-tuning. CLIP[307] used 400 million ”image-text pairs,” while Wu et al.’s Text-to-Image 2.0[337] employed 650 million ”image-text pairs” for training. Additionally, the automatically labeled data generated during self-supervised training could be collected for wide research and industrial applications. For example, the SAM model[292] built the largest segmentation dataset to date, known as SA-1B which had 1.1 billions of masks and 99.1%of them was geb=nerated automatically. In fact, it became a classic dataset for future computer vision segmentation model training and evaluation, whereinover 300academic papers used SAM and SA-1B for various segmentation tasks after its release in March 2023.\n\nSuperior generative capability. The generative LSFMs[314, 291] changed the landscape of AI by generating new data through learning the distribution of existing data, giving AI a sense of ”creativity.” Generative LSFMs were not only used for generating dialogues and images but also found applications in smart manufacturing to automatically generate high-quality training data. Whitehouse et al.[338] attempted to expand multilingual commonsense reasoning datasets using LLMs, integrating datasets generated by LLMs significantly enhanced the performance of the trained model. Sen et al.[339] utilized LLM to generate Counterfactually Augmented Data (CADs), which was commonly employed to train models for robustness against false features, achieving performance almost comparable to manually generated CADs.\n\nIV Roadmaps of LSFMs for intelligent manufacturing applications\n\nIV-A Roadmaps for powerful generalization ability\n\nIV-A1 Pre-training combined with fine-tuning\n\nAs model parameter and size surpassed a certain threshold, these models not only exhibited emergent performance improvements but also acquired functionalities like logical reasoning abilities, absent in smaller-scale models [293, 344]. The combination of pre-training with fine-tuning in LSFMs offered various possibilities to address issues encountered by traditional small-scale deep learning methods in intelligent manufacturing.\n\nLSFMs pre-trained on diverse general datasets reduced their reliance on limited, task-specific datasets, hence mitigating overfitting risks despite the large parameter count of the models. Kahatapitiya et al. [345], acknowledging the limited availability of video-text matching data, applied a pre-trained image-text model to the video domain for video-text matching instead of training from scratch. Additionally, specific fine-tuning strategies could enhance model generalization to further avoid potential model overfitting during fine-tuning with small sample learning. Song et al. [346] proposed a fine-tuning method called Feature Discriminant Alignment (FD Align) to enhance model generalization by maintaining consistency in pseudo-features, demonstrating effectiveness in within-distribution (ID) and out-of-distribution (OOD) tasks.\n\nIV-A2 Building structured data through LSFMs\n\nLSFMs cloud be used to extract and comprehend intricate unstructured data, encoding it into manageable structured formats, for instance, handling unstructured text data within work orders [347]. Deep Generative Models (DGM) and models like VIT [303] were designed to uncover complex high-dimensional probability distributions from unstructured data to extract more abstract, more intricate features. Oliveira et al. [348] outlined four types of DGM: Energy-based Models (EBM), Generative Adversarial Networks (GAN), Variational Autoencoders (VAE), and Autoregressive models, and how they were applied to SCM optimization.\n\nIV-A3 Embedding knowledge through prompts\n\nOnce expert knowledge was encoded, it could be fused with input texts or image features, thereby improving the accuracy of output [349]. Many LSMFs, such as ChatGPT and SAM, inherently included manual prompt encoding, allowing for the fusion of domain knowledge without modifying the models through prompts. For example, for abstract human behavior activities, it might be difficult for models to describe them all at once. Therefore, it could be guided to generate activity descriptions related to objects initially, emphasizing crucial objects to distinguish similar activities. Subsequently, it could identify the activity classes of human activities and help explain the contexts [350]. Furthermore, LSFMs could even gather relevant domain knowledge during training by collecting case studies [351].\n\nIV-A4 Using multimodal LSFMs\n\nIntelligent manufacturing, usually yielded a multiple forms of data, encompassing free-text maintenance logs, images, audio and video recordings. The inherent diversity of these data posed a formidable challenge for singular modalities within deep learning models. LSFMs such as Visual-GPT [352] and ImageBind [317] had emerged as viable solutions. These models exceled in the simultaneous encoding of a spectrum of data, including images, text, audio, depth, thermal, IMU data, and time-series signal data [353, 354]. This expanded capacity could not only enrich the range of data captured in intelligent manufacturing but also endow LSFMs with distinct functionalities such as cross-modal retrieval, modal fusion via arithmetic operations, and cross-modal detection and generation. Leveraging these expansive LSFMs facilitates the precise handling of unstructured data and the synthesis of diverse structured data sources. In complex industrial environments characterized by multiple disturbances, LSFMs demonstrated enhanced robustness compared to conventional single-mode deep learning methods.\n\nIV-A5 Regularization and ensemble learning\n\nLSFMs can solve overfitting problems through methods such as regularization, and ensemble learning. Regularization can limit the complexity of the models, pruning can remove unnecessary nodes and connections, and ensemble learning can combine the prediction results of multiple models to improve the generalization ability of the models.\n\nAlthough many LSFMs like GPT-3 and PaLM did not use dropout [355] during training, it still had a significant effect on LSFMs. For instance, by using dropout during the training process Galactica [295] achieved a 120 billion parameter model without overfitting. Moreover, to alleviated the reduction in training speed for LSFMs due to dropout, gradually introducing dropout into the process during training could yield performance comparable to using dropout consistently throughout [356].\n\nIV-A6 Continual Learning/Life-Long Learning\n\nMost current deep learning models in intelligent manufacturing assumed that the normal mode remained unchanged. However, variations in the manufacturing environment occurred frequently.Continuous learning/lifelong learning involves acquiring and identifying new knowledge while retaining previously learned knowledge. LSFMs possessed robust capabilities for continuous learning by collecting past task outcomes as experience. Through this process, LSFMs continually enhanced themselves using prior knowledge [357, 351]. The continual learning feature of LSFMs allowed them to continually accumulate new knowledge during actual production processes to adapt to potential changes in complex real-world environments[357, 351]. This ability was beneficial in preventing models trained on fixed patterns from experiencing overfitting. Applying specific constraints to this process further enhances the model’s performance and stability [358].\n\nIV-A7 LSFM assisted construction of knowledge graph\n\nKnowledge graphs are forms of expression that acquires knowledge by understanding graph structures [359], and were highly effective. However, knowledge Graph Engineering (KGE) required an in-depth understanding of graph structure, logic, and knowledge content, causing a lot of work. The contextual comprehension and representation capabilities of deep learning methods were unsatisfied, especially when encountering entirely new or rare knowledge. Using the knowledge comprehension abilities and advanced reasoning skills of LLMs, it was possible to automatically generate knowledge graphs in professional domains[360], and was expected to enhance the model’s understanding of specific domain knowledge achieved by synergizing the knowledge graphs with pre-trained language models[361].\n\nIV-B Roadmaps foor automatic high-quality training dataset generation\n\nIV-B1 Generation of higher-quality datasets\n\nGenerative models such as diffusion could potentially facilitate the generation of higher-quality synthetic data compared to traditional data synthesis methods [314]. Using a text-to-image diffusion model could generated realistic image variations for data augmentation. Unlike simple augmentation methods such as concatenation, rotation, flipping, augmentation based on the diffusion model could alter higher-level semantic attributes, like the paint job on a truck[362]. To addressed the issue of requiring a large amount of data for training the diffusion model itself, Wang and colleagues transformed the two-dimensional diffusion model into three dimensions using chain rules, enabling the generation of three-dimensional object data[363]. Moreover, Transform could be used to weighted average or score the results of multiple prediction models, and to learn and simulate historical data to obtain more robust prediction results.\n\nIn Section V, we demonstrated how we used LSFMs to achieve low-cost, automated action recognition data annotation on industrial production lines.\n\nIV-B2 Improving data quality\n\nHigh-quality data was crucial for both model training and decision-making in intelligent manufacturing, wherein raw data often had issues such as missing values, outliers, and repeated values. LSFMs could be used to automatically remove impurity data, reduce prediction errors, and improve data quality. For instance, BLIP [308] relied on intermediate training models to automatically remove poorly matched image-text pairs from the dataset during training and improved the text annotations of certain images. Jain et al.[364] using LLM to enhance the structural organization and readability of the code used as training data. Models fine-tuned on the cleaned data perform significant improvements in the algorithmic code generation tasks.\n\nIV-B3 Zero-shot and few-shot\n\nA major challenge in industrial defect detection is the lack of abnormal samples, and the abnormal situations of industrial products are often diverse and unpredictable. The LSFM could effectively achieve zero sample detection or few sample detection. Gu et al. [324] explored the use of Large Vision-Language Models(LVLMs) to solve industrial anomaly detection problems and proposed a new LVLM based industrial anomaly detection method, AnomalyGPT. On the MVTec anomaly detection dataset, AnomalyGPT could achieve the most advanced performance of 86.1% accuracy, 94.1% image level AUC and 95.3% pixel level AUC with only one normal shot. This application method no longer requires to collect abnormal samples or create datasets for each task to train specific models, and only needed a small amount of data fine-tuning to achieve good detection results. In predictive maintenance, for instance, Leite et al. [365] employed LLMs to classify credibility signals, which were commonly used to assess the authenticity of predictive contents. The LLM-based approach outperformed state-of-the-art classifiers on two misinformation datasets without the need for any ground-truth labels.\n\nIV-B4 Pre-training combined with fine-tuning\n\nAltough some preliminary work provided datasets for intelligent manufacturing scenarios such as HAR [366, 367, 368], quality control[369, 370, 371], and PHM [372, 373], these datasets were characterized by small scales, narrow coverage, singular scenes, simple operating conditions, and uneven data distributions. LSFMs pre-trained on extensive data could identify general features of real-world entities, providing an efficient solution for achieving accurate and flexible intelligent manufacturing in data-limited environments [293]. Pre-trained models trained on large scale data were then fine-tuned on smaller-scale data to enhance model accuracy and generalizability. For instance, Sun et al. [374]. used BERT in medical texts and achieved good performance with just a small dataset for fine-tuning. Similarly, Radford et al. [298]. demonstrated GPT’s transfer learning abilities across various tasks.\n\nIV-C Roadmaps for superior performance\n\nIV-C1 Improvements by prompts\n\nGenerally, after training was completed, deep learning models no longer accepted ’guidance’ and instead operated on the basis of trained parameters for inference. However, LSFMs had superior data integration capability that could enhance output performance by leveraging various forms of prompts. Ji et al. [320] found that the quality of prompts had a crucial impact on the accuracy of LSFMs. To address SAM’s suboptimal segmentation performance on small scales and irregular boundaries, multiple prompts could be employed to derive more precise segmentation outcomes from distributions[375]. Specifically, Deng et al. [376] utilized Monte Carlo simulations with prior distribution parameters to estimate SAM’s predicted distribution. This approach allowed for estimating arbitrary uncertainties by considering multiple predictions from individual images. Alternatively, networks could also be employed to acquire enhanced cues, generating augmented cues from inputting original cues to yield masks and subsequently outputting augmented cues. By merging these cues as new hints, segmentation performance could be enhanced [377]. It is also worth carefully handling decoupling mask generation and prompt embedding to prevent misleading prompt from having adverse effects on mask generation [374].\n\nIV-C2 Enhanced input data\n\nIn LSFMs, the term ”foundation” indicating that LSFMs could easily be used as the foundation to combine with other algorithms. This ensured that even in situations where LSFMs performed unsatisfactorily when being used alone, good performance could still be guaranteed by combining them with other algorithms. VLM exhibited strong robustness against various corruptions, yet some corruptions led to model performance degradation, such as corruption related to blur [378]. Additionally, SAM’s performance was proven to be inadequate in concealed and camouflaged scenes [199, 379]. Fortunately, extensive prior research has been conducted in both deblurring [380, 381, 382] and target detection techniques for concealed and camouflaged scenes [383, 384]. As one of the characteristics and advantages of LSFMs, VLM could effortlessly combine with other models, using preprocessed data as input or incorporating detection boxes from other object detectors as prompts.\n\nIV-C3 Cross modal pre-training\n\nLSMFs overcome the limitations of single task and single modal in deep learning, and could achieve multi-task and multi-modal applications with unified model, after cross modal pre-training[385]. By leveraging contrastive loss during training to establish correlations between image and text features, it was possible to achieve open-set object recognition and detection [309, 310, 311]. This could prevent tasks from being constrained by pre-defined categories in training. To achieve satisfactory pre-training performance, success relied on both the scale of cross-modal datasets [385, 386] and the model’s ability to leverage weakly aligned data [387]. Li et al. [388] employed a pre-trained model for weakly supervised label classification to measure semantic similarity in videos within an industrial system. By incorporating an enhanced cross-modal Transformer block, they maximized the utilization of interactive information between video and texture features.\n\nIV-C4 Pre-training combined with fine-tuning\n\nCompared to the unsatisfactory accuracy achieved by deep learning in situations with limited data and complex processes, large scale pre-training not only bestows powerful generalization capabilities on LSFMs but also endows them with the potential for higher accuracy [293, 344]. While directly using pre-trained LSFMs may not always outperform specially designed deep neural networks [320], fine-tuning them with specific intelligent manufacturing domain dataset data effectively can improve their accuracy [389, 390], potentially surpassing existing deep learning models. Technologies like P-Tuning [391], Lora [392], QLora [393], facilitated the fine-tuning process in LSFMs.\n\nApart from that, training deep learning models on integrated datasets increased the privacy risk of data leakage. Using pre-trained LSFMs emerged as a viable solution to enhance data security, reducing the privacy risk brought by the extensive data requirements for training models from scratch. These pre-trained models could achieve effective results with minimal fine-tuning, thereby reducing exposure to sensitive data. During the fine-tuning stage, a limited segment of the LSFM network required adjustment, introducing differential privacy techniques. Specifically, techniques proposed by Abadi et al. [394] were applied during the fine-tuning process. These measures could maintain the privacy of the data involved in fine-tuning LSFM, ensuring a more secure training environment.\n\nIV-C5 Adopting distributed learning\n\nData in manufacturing are not as readily available as those in natural language and other areas, hence the adoption of distributed learning methods [395] could be beneficial in both training and security aspects for LSFMs used in intelligent manufacturing wherein data for training could be obtained from different lines, factories, or even countries. Distributed learning methods, such as federated learning, involved the local processing of data from each party, with only intermediate results (like gradients) being aggregated for model updates. This enabled clients (devices or organizations) to collaboratively train machine learning models without exposing their data, greatly increasing data usage efficiency [396, 397]. Combining with such technologies could empower LSFMs to surpass traditional methods not only in performance but also in providing a more secure data processing framework when dealing with sensitive indusrtial information.\n\nIV-C6 Using LSFM’s own output for interpretation\n\nDeep learning models were often considered as ’black boxes’ due to their decision-making processes being highly abstract and non-intuitive. LSFMs, especially LLMs, demonstrated remarkable contextual comprehension in tasks, hence attempting to use LLMs to explain models was a potentially feasible. In a study by Bubeck et al. [273], it was found that LLMs exhibited strong result consistency in their outputs, implying that the model followed a fixed “thinking” pattern. Therefore, asking questions like “Please explain the reasons behind your prediction” to chat-GPT was proven to be effective, when being preceded by reasonable prior questions. This idea could also be applied to models based on encoder structures [398], to address the limitations of AE by performing bias analysis on reconstructed input features to obtain explanations [399].\n\nIV-C7 Using LLM to explain other models\n\nLLMs had powerful text capabilities, and it was envisioned to leverage the knowledge obtained from LLMs to interpret other neural networks. To achieve this, LLMs were used to summarize and score the outputs of the models under analysis [400]. In addition, LLMs could be employed to generate or match counterfactuals, simulating or estimating different choices in an event or behavior to better understand the model’s predicted outcomes [401]. Alternatively, embedding LLMs directly into the model training could allow for efficient inference while achieving good interpretability [402].\n\nIV-C8 Visualize the running process\n\nEnabling the extraction of intermediate feature maps from neural network outputs could assist in understanding the features being focused on, even though these feature maps could be still highly abstract. Visualizing attention could provide a more intuitive explanation over feature maps by employing self-attention mechanisms with token-linking in its architecture. The strength of the attention links could intuitively be considered as indicators of each token’s contribution to classification. The visualizing attentions aided in understanding the interested parts to the models [403]. Considering that LSFMs are mostly built on transformer structures, visualizing the output of attentions to improve the interpretability of LSFMs is promising.\n\nMoreover, it was possible to visualize the data through the output of LSFMs with the helps of interpretable auto prompting [343]. For intelligent manufacturing LSFMs could integrate scattered and complex supply chain data and present the data through data analysis and link visualization, allowing supply chain managers to have a clearer understanding of the operation of the entire supply chain [404, 405].\n\nIV-C9 Using LSFMs detect malicious perturbations\n\nUnlike humans, deep learning models might have been more inclined to utilize information such as textures and colors for inference [406], leading to a heightened sensitivity of deep neural networks towards imperceptible malicious perturbations (referred to as adversarial attacks). The emergence of LSFMs brought promising solutions for this issue[407]. On one hand, relying on superior generative capabilities, LSFMs could potentially make adversarial attacks more efficient [408], while, on the other hand, LSFMs were more likely to detect hidden malicious perturbations with the superior extraction helps of their capabilities. Meanwhile, inconsistencies between images containing imperceptible perturbations and diffusion-reconstructed images could be identified using diffusion models, thereby recognizing the malicious attacks concealed within input samples [409]. LLMs, a self defense mechanism utilized the LLM output content as re-input, could be used to inspect whether the output content was harmful [410], randomly dropping a portion of the input and aggregating outputs from various dropping could be used collectively to determine if a request was malicious [411]. In practice, Mireshghallah et al. [412] combined homomorphic encryption with LSFMs, to allow model inference on encrypted data, thus offering further data protection.\n\nV Cases of application of LSFMs in intelligent manufacturing\n\nEventhough LSFMs have intrinsic advantages in terms of powerful generalizations, abilities to establish high-quality training datasets automatically, and superior performance, it is not straight forward to employ them in intelligent manufacturing scenarios. Researchers need to work closely with big manufacturing companies where large-scale industrial data are available to build training datasets and in-situ LSFMs can be tested in real manufacturing lines. Moreover, issues like transferabilities, costs, real-time performance, etc., also require careful investigation before LSFMs can be widely used in large-scale intelligent manufacturing scenes.\n\nIn this section, we elucidated a couple of real applications of LSFMs in manufacturing lines of Midea Group to show how LSFMs could help industries improve their efficiency and cut down their cost.\n\nV-A Case: PCB defect inspection\n\nAs an implementation form of intelligent manufacturing, industrial intelligent detection technology based on multimodal large models, especially in the field of defect detection in high-density electronic components, is gradually becoming a research hotspot in the industrial production field. This technology can effectively integrate and analyze various types of data, such as visual images, text information, point cloud data, etc., to achieve comprehensive perception and in-depth understanding of electronic devices. On this basis, by applying complex algorithms and computational models, the intelligent detection system can extract and fuse multimodal features, optimize production processes, detect equipment failures, ensure product quality, and ultimately achieve the goal of cost reduction and efficiency improvement. In addition, this technology can also support rapid product switching and personalized customization, improving the responsiveness and flexibility of enterprises to market changes.\n\nTraditional methods only achieved a recognition accuracy rate of around 80%, unsatisfactory for real manufacturing requirements. Furthermore, direct applications of SAM could introduce issues, such as unlabelled segmented targets,segmentation errors,missed segmentations,under-segmentation and over-segmentation. In reality, we significantly enhance the accuracy of automatic component position detection to 97% by freezing the image encoder layer and fine-tuning with LoRA. This improvement had the potential to reduce 2000 production line employees, i.e. an annual cost reduction of 150 million yuan for Midea Group.\n\nV-B Case: Industrial human action recognition\n\nWith the rapid development of intelligent manufacturing, industrial human action recognitions (IHAR) drew great attentions[413] and started to play important roles in many industrial scenarios, such as quality control[414], safety monitoring[415], and process optimization[416]. Nevertheless, as IHAR scale increased in practice, the growing diversity of industrial actions involved in manufacture lines (MLs) raised complexity of data distribution and transferability across different MLs.\n\nConstructing a generic industrial dataset for IHAR model training was challenging or even unrealistic[417], and up until now, manual annotations were still a mainstream approach for obtaining high quality industrial data, especially when supervised deep learning(DL) models were used. These made it very costly to employ conventional DL-Based IHAR methods, especially when dealing with large-scale industrial data.\n\nIn this work, we proposed a LSFM-based low-cost and real-time industrial human action recognition (LRIHAR) model, with the least human intervention. Grounding DINO[311] and BLIP2[418] were used for action detection and recognition of large-scale industrial human actions, respectively. After obtaining sufficient boxed action pictures, YOLOv5 was trained as the detector for real deployment. In order to get high accuracy, we trained ViT-L for classification using low-rank adaptation (LoRA) fine-tuning method[392]. Finally, knowledge distillation (KD)[419] was used to distill low algorithm call time and highly generalized ViT-S models to classify actions selected from YOLO. To summarize, our main contributions were:\n\n(1)Grounding DINO and BLIP2 was jointly used to facilitate automatic annotations and industrial dataset establishment, wherein annotation costs was saved by more than 80% with superior generalization over traditional meth- ods, speeding up training process and IHAR deployment.\n\n(2)LoRA and KD was used to reduce time for training and response, respectively, wherein 96.84% classification accuracy was achieved, outperforming retrained ResNet-18 in all scenarios with call time less than 10ms, realizing real-time IHAR.\n\n(3)Comprehensive experiments on large scale industrial data from three industrial MLs of Midea Group showed that the LRIHAR achieved 98.19% detection accuracy on average, and surpassed the conventional ResNet-18 by 4.4% on classification accuracy. Moreover, the LRIHAR showed superior AC cost saving, real-time performance, and generalization capabilities, simultaneously, facilitating its applications in large scale industrial scenarios.\n\nVI Conclusion\n\nLSFMs demonstrated powerful generalization capabilities, the ability to automatically generate high-quality training datasets, and superior performance, and was able to transform AI from a paradigm of single modal, single task, and training on limited data to a pattern of multimodal, multitask, and pre-training on massive data followed by fine-tuning, and were bound to bring about a new wave of transformation in intelligent manufacturing.\n\nSince research on applying LSFMs on intelligent manufacturing was still in its early stages and it was lack of systematic directional guidance in this areas, this paper summarized the progresses and challenges of deep learning in intelligent manufacturing, as well as the progresses of LSFMs and their potential advantages in intelligent manufacturing applications. Accordingly, this paper comprehensively discussed how to construct an LSFM system suitable for the intelligent manufacturing domain from the perspectives of generalization, data, and performance, and illustrated how the application of LSFMs could help enterprises enhance efficiency and reduce costs by presenting the real-world applications of LSFMs in the manufacturing lines of Midea Group.\n\nAcknowledgment\n\nWe would like to thank to Midea Group for providing us with data samples and a testing platform for our case study. We appreciate Zhenrui Wu for completing the drawing of Figure2 and collecting the references. We also thank Shuai Li, Wensheng Liang, Liu Junwei and Kang Xu for the technical support they provided for our case study.\n\nReferences\n\n[1] Andreja Rojko. Industry 4.0 concept: Background and overview. International journal of interactive mobile technologies, 11(5), 2017.\n\n[2] Michelle Bryner. Smart manufacturing: The next revolution. Chemical Engineering Progress, 108(10):4–12, 2012.\n\n[3] Jost Wübbeke, Mirjam Meissner, Max J Zenglein, Jaqueline Ives, and Björn Conrad. Made in china 2025. Mercator Institute for China Studies. Papers on China, 2(74):4, 2016.\n\n[4] Arkadeep Kumar. Methods and materials for smart manufacturing: additive manufacturing, internet of things, flexible sensors and soft robotics. Manufacturing Letters, 15:122–125, 2018.\n\n[5] Andreas Schütze, Nikolai Helwig, and Tizian Schneider. Sensors 4.0–smart sensors and measurement technology enable industry 4.0. Journal of Sensors and Sensor systems, 7(1):359–371, 2018.\n\n[6] Priyanshi Gupta, Chaitanya Krishna, Rahul Rajesh, Arushi Ananthakrishnan, A Vishnuvardhan, Shrey Shaileshbhai Patel, Chinmay Kapruan, Stavan Brahmbhatt, Tarun Kataray, Deva Narayanan, et al. Industrial internet of things in intelligent manufacturing: a review, approaches, opportunities, open challenges, and future directions. International Journal on Interactive Design and Manufacturing (IJIDeM), pages 1–23, 2022.\n\n[7] Georgios Lampropoulos, Kerstin Siakas, and Theofylaktos Anastasiadis. Internet of things (iot) in industry: contemporary application domains, innovative technologies and intelligent manufacturing. people, 6(7), 2018.\n\n[8] Yajun Lu and Joe Cecil. An internet of things (iot)-based collaborative framework for advanced manufacturing. The International Journal of Advanced Manufacturing Technology, 84:1141–1152, 2016.\n\n[9] Khalid Hasan Tantawi, Alexandr Sokolov, and Omar Tantawi. Advances in industrial robotics: From industry 3.0 automation to industry 4.0 collaboration. In 2019 4th Technology Innovation Management and Engineering Science International Conference (TIMES-iCON), pages 1–4. IEEE, 2019.\n\n[10] Prahar M Bhatt, Rishi K Malhan, Aniruddha V Shembekar, Yeo Jung Yoon, and Satyandra K Gupta. Expanding capabilities of additive manufacturing through use of robotics technologies: A survey. Additive manufacturing, 31:100933, 2020.\n\n[11] Ruchi Goel and Pooja Gupta. Robotics and industry 4.0. A Roadmap to Industry 4.0: Smart Production, Sharp Business and Sustainable Development, pages 157–169, 2020.\n\n[12] Werner Kritzinger, Matthias Karner, Georg Traar, Jan Henjes, and Wilfried Sihn. Digital twin in manufacturing: A categorical literature review and classification. Ifac-PapersOnline, 51(11):1016–1022, 2018.\n\n[13] Yuqian Lu, Chao Liu, I Kevin, Kai Wang, Huiyue Huang, and Xun Xu. Digital twin-driven smart manufacturing: Connotation, reference model, applications and research issues. Robotics and computer-integrated manufacturing, 61:101837, 2020.\n\n[14] Jay Lee, Behrad Bagheri, and Hung-An Kao. A cyber-physical systems architecture for industry 4.0-based manufacturing systems. Manufacturing letters, 3:18–23, 2015.\n\n[15] Tianyue Wang, Bingtao Hu, Yixiong Feng, Xiaoxie Gao, Chen Yang, and Jianrong Tan. Data augmentation-based manufacturing quality prediction approach in human cyber-physical systems. Journal of Manufacturing Science and Engineering, 145(12), 2023.\n\n[16] Shahriar Akter, Grace McCarthy, Shahriar Sajib, Katina Michael, Yogesh K Dwivedi, John D’Ambra, and Kathy Ning Shen. Algorithmic bias in data-driven innovation in the age of ai, 2021.\n\n[17] Cihan H Dagli. Artificial neural networks for intelligent manufacturing. Springer Science & Business Media, 2012.\n\n[18] Bo-hu Li, Bao-cun Hou, Wen-tao Yu, Xiao-bing Lu, and Chun-wei Yang. Applications of artificial intelligence in intelligent manufacturing: a review. Frontiers of Information Technology & Electronic Engineering, 18:86–96, 2017.\n\n[19] Junliang Wang, Chuqiao Xu, Jie Zhang, and Ray Zhong. Big data analytics for intelligent manufacturing systems: A review. Journal of Manufacturing Systems, 62:738–752, 2022.\n\n[20] Jie ZHANG, Junliang WANG, Youlong LYU, and Jinsong BAO. Big data driven intelligent manufacturing. China Mechanical Engineering, 30(02):127, 2019.\n\n[21] Smart Manufacturing Coalition. Manufacturing growth continues despite uncertain economy, according to asq outlook survey; 2013.\n\n[22] Chong Leong Gan. Prognostics and health management of electronics: Fundamentals, machine learning, and the internet of things: John wiley & sons ltd.(2018). pp. 731, isbn: 9781119515326 (print), 9781119515326 (online). Life Cycle Reliability and Safety Engineering, 9(2):225–226, 2020.\n\n[23] Jaskaran Singh, Moslem Azamfar, Fei Li, and Jay Lee. A systematic review of machine learning algorithms for prognostics and health management of rolling element bearings: fundamentals, concepts and applications. Measurement Science and Technology, 32(1):012001, 2020.\n\n[24] Lorenzo Polverino, Raffaele Abbate, Pasquale Manco, Donato Perfetto, Francesco Caputo, Roberto Macchiaroli, and Mario Caterino. Machine learning for prognostics and health management of industrial mechanical systems and equipment: A systematic literature review. International Journal of Engineering Business Management, 15:18479790231186848, 2023.\n\n[25] DENG Weikun, Khanh TP NGUYEN, Kamal MEDJAHER, GOGU Christian, and Jérôme MORIO. Physics-informed machine learning in prognostics and health management: state of the art and challenges. Applied Mathematical Modelling, 124:325–352, 2023.\n\n[26] Dong Wang, Kwok-Leung Tsui, and Qiang Miao. Prognostics and health management: A review of vibration based bearing and gear health indicators. Ieee Access, 6:665–676, 2017.\n\n[27] Kwok L Tsui, Nan Chen, Qiang Zhou, Yizhen Hai, Wenbin Wang, et al. Prognostics and health management: A review on data driven approaches. Mathematical Problems in Engineering, 2015, 2015.\n\n[28] Marcos Leandro Hoffmann Souza, Cristiano André da Costa, and Gabriel de Oliveira Ramos. A machine-learning based data-oriented pipeline for prognosis and health management systems. Computers in Industry, 148:103903, 2023.\n\n[29] Thamo Sutharssan, Stoyan Stoyanov, Chris Bailey, and Chunyan Yin. Prognostic and health management for engineering systems: a review of the data-driven approach and algorithms. The Journal of engineering, 2015(7):215–222, 2015.\n\n[30] Alican Dogan and Derya Birant. Machine learning and data mining in manufacturing. Expert Systems with Applications, 166:114060, 2021.\n\n[31] Rory Coulter and Lei Pan. Intelligent agents defending for an iot world: A review. Computers & Security, 73:439–458, 2018.\n\n[32] Meng Zhang, Fei Tao, Ying Zuo, Feng Xiang, Lihui Wang, and AYC Nee. Top ten intelligent algorithms towards smart manufacturing. Journal of Manufacturing Systems, 71:158–171, 2023.\n\n[33] Simon Fahle, Christopher Prinz, and Bernd Kuhlenkötter. Systematic review on machine learning (ml) methods for manufacturing processes–identifying artificial intelligence (ai) methods for field application. Procedia CIRP, 93:413–418, 2020.\n\n[34] Behnoush Rezaeianjouybari and Yi Shang. Deep learning for prognostics and health management: State of the art, challenges, and opportunities. Measurement, 163:107929, 2020.\n\n[35] Liangwei Zhang, Jing Lin, Bin Liu, Zhicong Zhang, Xiaohui Yan, and Muheng Wei. A review on deep learning applications in prognostics and health management. Ieee Access, 7:162415–162438, 2019.\n\n[36] Yang Hu, Xuewen Miao, Yong Si, Ershun Pan, and Enrico Zio. Prognostics and health management: A review from the perspectives of design, development and decision. Reliability Engineering & System Safety, 217:108063, 2022.\n\n[37] Andre Listou Ellefsen, Vilmar Æsøy, Sergey Ushakov, and Houxiang Zhang. A comprehensive survey of prognostics and health management based on deep learning for autonomous ships. IEEE Transactions on Reliability, 68(2):720–740, 2019.\n\n[38] Ying Zhang and Yan-Fu Li. Prognostics and health management of lithium-ion battery using deep learning methods: A review. Renewable and Sustainable Energy Reviews, 161:112282, 2022.\n\n[39] Prashant Kumar, Salman Khalid, and Heung Soo Kim. Prognostics and health management of rotating machinery of industrial robot with deep learning applications—a review. Mathematics, 11(13):3008, 2023.\n\n[40] Ikram Remadna, Sadek Labib Terrissa, Ryad Zemouri, and Soheyb Ayad. An overview on the deep learning based prognostic. In 2018 International Conference on Advanced Systems and Electric Technologies (IC_ASET), pages 196–200. IEEE, 2018.\n\n[41] Nikhil M Thoppil, V Vasu, and CSP Rao. Deep learning algorithms for machinery health prognostics using time-series data: A review. Journal of Vibration Engineering & Technologies, pages 1–23, 2021.\n\n[42] PLS Jayalaxmi, Rahul Saha, Gulshan Kumar, Mauro Conti, and Tai-Hoon Kim. Machine and deep learning solutions for intrusion detection and prevention in iots: A survey. IEEE Access, 2022.\n\n[43] Jiawen Xu, Matthias Kovatsch, Denny Mattern, Filippo Mazza, Marko Harasic, Adrian Paschke, and Sergio Lucia. A review on ai for smart manufacturing: Deep learning challenges and solutions. Applied Sciences, 12(16):8239, 2022.\n\n[44] Anbesh Jamwal, Rajeev Agrawal, and Monica Sharma. Deep learning for manufacturing sustainability: Models, applications in industry 4.0 and implications. International Journal of Information Management Data Insights, 2(2):100107, 2022.\n\n[45] Jinjiang Wang, Yulin Ma, Laibin Zhang, Robert X Gao, and Dazhong Wu. Deep learning for smart manufacturing: Methods and applications. Journal of manufacturing systems, 48:144–156, 2018.\n\n[46] Rishi Malhan and Satyandra K Gupta. The role of deep learning in manufacturing applications: Challenges and opportunities. Journal of Computing and Information Science in Engineering, 23(6), 2023.\n\n[47] Jian Qin, Fu Hu, Ying Liu, Paul Witherell, Charlie CL Wang, David W Rosen, Timothy W Simpson, Yan Lu, and Qian Tang. Research and application of machine learning for additive manufacturing. Additive Manufacturing, 52:102691, 2022.\n\n[48] Minh-Quang Tran, Hoang-Phuong Doan, Viet Q Vu, and Lien T Vu. Machine learning and iot-based approach for tool condition monitoring: A review and future prospects. Measurement, page 112351, 2022.\n\n[49] Fengyang He, Lei Yuan, Haochen Mu, Montserrat Ros, Donghong Ding, Zengxi Pan, and Huijun Li. Research and application of artificial intelligence techniques for wire arc additive manufacturing: a state-of-the-art review. Robotics and Computer-Integrated Manufacturing, 82:102525, 2023.\n\n[50] Md Sazzadur Rahman, Tapotosh Ghosh, Nahid Ferdous Aurna, M Shamim Kaiser, Mehrin Anannya, and ASM Sanwar Hosen. Machine learning and internet of things in industry 4.0: A review. Measurement: Sensors, page 100822, 2023.\n\n[51] AS Rajesh, MS Prabhuswamy, and Srinivasan Krishnasamy. Smart manufacturing through machine learning: a review, perspective, and future directions to the machining industry. Journal of Engineering, 2022, 2022.\n\n[52] Yaohua Deng, Zilin Zhang, Hao Huang, Xiali Liu, and Jiamin Tang. Research on intelligent maintenance decision for flexible electronic manufacturing equipment based on deep reinforcement learning. In 2023 IEEE 16th International Conference on Electronic Measurement & Instruments (ICEMI), pages 1–4. IEEE, 2023.\n\n[53] Di Wu, Peilei Zhang, Zhishui Yu, Yanfeng Gao, Hua Zhang, Huabin Chen, Shanben Chen, and YingTao Tian. Progress and perspectives of in-situ optical monitoring in laser beam welding: Sensing, characterization and modeling. Journal of Manufacturing Processes, 75:767–791, 2022.\n\n[54] Zhibin Zhao, Qiyang Zhang, Xiaolei Yu, Chuang Sun, Shibin Wang, Ruqiang Yan, and Xuefeng Chen. Applications of unsupervised deep transfer learning to intelligent fault diagnosis: A survey and comparative study. IEEE Transactions on Instrumentation and Measurement, 70:1–28, 2021.\n\n[55] MARIA Di Summa, MARIA ELENA Griseta, NICOLA Mosca, COSIMO Patruno, MASSIMILIANO Nitti, VITO Renò, and ETTORE Stella. A review on deep learning techniques for railway infrastructure monitoring. IEEE Access, 2023.\n\n[56] Dhiraj Neupane and Jongwon Seok. Bearing fault detection and diagnosis using case western reserve university dataset with deep learning approaches: A review. IEEE Access, 8:93155–93178, 2020.\n\n[57] Wuyi Ming, Chen Cao, Guojun Zhang, Hongmei Zhang, Fei Zhang, Zhiwen Jiang, and Jie Yuan. Application of convolutional neural network in defect detection of 3c products. IEEE Access, 9:135657–135674, 2021.\n\n[58] Adriano Scibilia, Nicola Pedrocchi, and Luigi Fortuna. Modeling nonlinear dynamics in human-machine interaction. IEEE Access, 2023.\n\n[59] Sreenivasan Ramasamy Ramamurthy and Nirmalya Roy. Recent trends in machine learning for human activity recognition—a survey. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 8(4):e1254, 2018.\n\n[60] Sen Qiu, Hongkai Zhao, Nan Jiang, Zhelong Wang, Long Liu, Yi An, Hongyu Zhao, Xin Miao, Ruichen Liu, and Giancarlo Fortino. Multi-sensor information fusion based on machine learning for real applications in human activity recognition: State-of-the-art and research challenges. Information Fusion, 80:241–265, 2022.\n\n[61] Binh Nguyen, Yves Coelho, Teodiano Bastos, and Sridhar Krishnan. Trends in human activity recognition with focus on machine learning and power requirements. Machine Learning with Applications, 5:100072, 2021.\n\n[62] Farzana Kulsoom, Sanam Narejo, Zahid Mehmood, Hassan Nazeer Chaudhry, Ayesha Butt, and Ali Kashif Bashir. A review of machine learning-based human activity recognition for diverse applications. Neural Computing and Applications, 34(21):18289–18324, 2022.\n\n[63] Ferhat Attal, Samer Mohammed, Mariam Dedabrishvili, Faicel Chamroukhi, Latifa Oukhellou, and Yacine Amirat. Physical human activity recognition using wearable sensors. Sensors, 15(12):31314–31338, 2015.\n\n[64] Oscar D Lara and Miguel A Labrador. A survey on human activity recognition using wearable sensors. IEEE communications surveys & tutorials, 15(3):1192–1209, 2012.\n\n[65] Kaixuan Chen, Dalin Zhang, Lina Yao, Bin Guo, Zhiwen Yu, and Yunhao Liu. Deep learning for sensor-based human activity recognition: Overview, challenges, and opportunities. ACM Computing Surveys (CSUR), 54(4):1–40, 2021.\n\n[66] Fuqiang Gu, Mu-Huan Chung, Mark Chignell, Shahrokh Valaee, Baoding Zhou, and Xue Liu. A survey on deep learning for human activity recognition. ACM Computing Surveys (CSUR), 54(8):1–34, 2021.\n\n[67] Shibo Zhang, Yaxuan Li, Shen Zhang, Farzad Shahabi, Stephen Xia, Yu Deng, and Nabil Alshurafa. Deep learning in human activity recognition with wearable sensors: A review on advances. Sensors, 22(4):1476, 2022.\n\n[68] Henry Friday Nweke, Ying Wah Teh, Mohammed Ali Al-Garadi, and Uzoma Rita Alo. Deep learning algorithms for human activity recognition using mobile and wearable sensor networks: State of the art and research challenges. Expert Systems with Applications, 105:233–261, 2018.\n\n[69] E Ramanujam, Thinagaran Perumal, and S Padmavathi. Human activity recognition with smartphone and wearable sensors using deep learning techniques: A review. IEEE Sensors Journal, 21(12):13029–13040, 2021.\n\n[70] Xinyu Li, Yuan He, and Xiaojun Jing. A survey of deep learning-based human activity recognition in radar. Remote Sensing, 11(9):1068, 2019.\n\n[71] Pranjal Kumar and Siddhartha Chauhan. Human activity recognition with deep learning: Overview, challenges & possibilities. CCF Transactions on Pervasive Computing and Interaction, 339(3):1–29, 2021.\n\n[72] Nida Saddaf Khan and Muhammad Sayeed Ghani. A survey of deep learning based models for human activity recognition. Wireless Personal Communications, 120(2):1593–1635, 2021.\n\n[73] SWETHA Danthala, SEERAMSRINIVASA Rao, KASIPRASAD Mannepalli, and Dhantala Shilpa. Robotic manipulator control by using machine learning algorithms: A review. International Journal of Mechanical and Production Engineering Research and Development, 8(5):305–310, 2018.\n\n[74] Zhihao Liu, Quan Liu, Wenjun Xu, Lihui Wang, and Zude Zhou. Robot learning towards smart robotic manufacturing: A review. Robotics and Computer-Integrated Manufacturing, 77:102360, 2022.\n\n[75] Weiyu Wang and Keng Siau. Artificial intelligence, machine learning, automation, robotics, future of work and future of humanity: A review and research agenda. Journal of Database Management (JDM), 30(1):61–79, 2019.\n\n[76] Francesco Semeraro, Alexander Griffiths, and Angelo Cangelosi. Human–robot collaboration and machine learning: A systematic review of recent research. Robotics and Computer-Integrated Manufacturing, 79:102432, 2023.\n\n[77] Jörg Marvin Gülzow, Patrick Paetzold, and Oliver Deussen. Recent developments regarding painting robots for research in automatic painting, artificial creativity, and machine learning. Applied Sciences, 10(10):3396, 2020.\n\n[78] Mohit Sajwan and Simranjit Singh. A review on the effectiveness of machine learning and deep learning algorithms for collaborative robot. Archives of Computational Methods in Engineering, pages 1–20, 2023.\n\n[79] Yukiyasu Domae. Recent trends in the research of industrial robots and future outlook. Journal of Robotics and Mechatronics, 31(1):57–62, 2019.\n\n[80] Kriti Aggarwal, Sunil K Singh, Muskaan Chopra, Sudhakar Kumar, and Francesco Colace. Deep learning in robotics for strengthening industry 4.0.: opportunities, challenges and future directions. Robotics and AI for Cybersecurity and Critical Infrastructure in Smart Cities, pages 1–19, 2022.\n\n[81] Mohsen Soori, Behrooz Arezoo, and Roza Dastres. Artificial intelligence, machine learning and deep learning in advanced robotics, a review. Cognitive Robotics, 2023.\n\n[82] João Pedro Carvalho de Souza, Luís F Rocha, Paulo Moura Oliveira, A Paulo Moreira, and José Boaventura-Cunha. Robotic grasping: from wrench space heuristics to deep learning policies. Robotics and Computer-Integrated Manufacturing, 71:102176, 2021.\n\n[83] J Hernavs, M Ficko, L Berus, R Rudolf, and S Klančnik. Deep learning in industry 4.0–brief overview. J. Prod. Eng, 21(2):1–5, 2018.\n\n[84] Katarina Valaskova, Peter Ward, and Lucia Svabova. Deep learning-assisted smart process planning, cognitive automation, and industrial big data analytics in sustainable cyber-physical production systems. Journal of Self-Governance and Management Economics, 9(2):9–20, 2021.\n\n[85] Artur Cordeiro, Luís F Rocha, Carlos Costa, Pedro Costa, and Manuel F Silva. Bin picking approaches based on deep learning techniques: A state-of-the-art survey. In 2022 IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC), pages 110–117. IEEE, 2022.\n\n[86] Shehan Caldera, Alexander Rassau, and Douglas Chai. Review of deep learning methods in robotic grasp detection. Multimodal Technologies and Interaction, 2(3):57, 2018.\n\n[87] Haonan Duan, Peng Wang, Yayu Huang, Guangyun Xu, Wei Wei, and Xiaofei Shen. Robotics dexterous grasping: The methods based on point cloud and deep learning. Frontiers in Neurorobotics, 15:658280, 2021.\n\n[88] Seema Rawat, Aakankshu Rawat, Deepak Kumar, and A Sai Sabitha. Application of machine learning and data visualization techniques for decision support in the insurance sector. International Journal of Information Management Data Insights, 1(2):100012, 2021.\n\n[89] Georg Meyer, Gediminas Adomavicius, Paul E Johnson, Mohamed Elidrisi, William A Rush, JoAnn M Sperl-Hillen, and Patrick J O’Connor. A machine learning approach to improving dynamic decision making. Information Systems Research, 25(2):239–263, 2014.\n\n[90] Rahul Rai, Manoj Kumar Tiwari, Dmitry Ivanov, and Alexandre Dolgui. Machine learning in manufacturing and industry 4.0 applications, 2021.\n\n[91] Yuanyuan Li, Stefano Carabelli, Edoardo Fadda, Daniele Manerba, Roberto Tadei, and Olivier Terzo. Machine learning and optimization for production rescheduling in industry 4.0. The International Journal of Advanced Manufacturing Technology, 110:2445–2463, 2020.\n\n[92] Satie L Takeda-Berger, Enzo Morosini Frazzon, Eike Broda, and Michael Freitag. Machine learning in production scheduling: An overview of the academic literature. In International Conference on Dynamics in Logistics, pages 409–419. Springer, 2020.\n\n[93] Andrea Coraddu, Luca Oneto, Aessandro Ghio, Stefano Savio, Davide Anguita, and Massimo Figari. Machine learning approaches for improving condition-based maintenance of naval propulsion plants. Proceedings of the Institution of Mechanical Engineers, Part M: Journal of Engineering for the Maritime Environment, 230(1):136–153, 2016.\n\n[94] Marina Paolanti, Luca Romeo, Andrea Felicetti, Adriano Mancini, Emanuele Frontoni, and Jelena Loncarski. Machine learning approach for predictive maintenance in industry 4.0. In 2018 14th IEEE/ASME International Conference on Mechatronic and Embedded Systems and Applications (MESA), pages 1–6. IEEE, 2018.\n\n[95] João R Campos, Marco Vieira, and Ernesto Costa. Exploratory study of machine learning techniques for supporting failure prediction. In 2018 14th European Dependable Computing Conference (EDCC), pages 9–16. IEEE, 2018.\n\n[96] Kadir Celikmih, Onur Inan, and Harun Uguz. Failure prediction of aircraft equipment using machine learning with a hybrid data preparation method. Scientific Programming, 2020:1–10, 2020.\n\n[97] Pier Francesco Orrù, Andrea Zoccheddu, Lorenzo Sassu, Carmine Mattia, Riccardo Cozza, and Simone Arena. Machine learning approach using mlp and svm algorithms for the fault prediction of a centrifugal pump in the oil and gas industry. Sustainability, 12(11):4776, 2020.\n\n[98] Long Xu, Weisi Lin, and C-C Jay Kuo. Visual quality assessment by machine learning. Springer, 2015.\n\n[99] Iker Pastor-López, Borja Sanz, Alberto Tellaeche, Giuseppe Psaila, José Gaviria de la Puerta, and Pablo G Bringas. Quality assessment methodology based on machine learning with small datasets: Industrial castings defects. Neurocomputing, 456:622–628, 2021.\n\n[100] Chung-Chi Huang and Xin-Pu Lin. Study on machine learning based intelligent defect detection system. In MATEC Web of Conferences, volume 201, page 01010. EDP Sciences, 2018.\n\n[101] Tao Zhang, Biyun Ding, Xin Zhao, Ganjun Liu, and Zhibo Pang. Learningadd: Machine learning based acoustic defect detection in factory automation. Journal of Manufacturing Systems, 60:48–58, 2021.\n\n[102] Manjunath Jogin, MS Madhulika, GD Divya, RK Meghana, S Apoorva, et al. Feature extraction using convolution neural networks (cnn) and deep learning. In 2018 3rd IEEE international conference on recent trends in electronics, information & communication technology (RTEICT), pages 2319–2323. IEEE, 2018.\n\n[103] Gonzalo Farias, Sebastián Dormido-Canto, Jesús Vega, Giuseppe Rattá, Héctor Vargas, Gabriel Hermosilla, Luis Alfaro, and Agustín Valencia. Automatic feature extraction in large fusion databases by using deep learning approach. Fusion Engineering and Design, 112:979–983, 2016.\n\n[104] Tiago Zonta, Cristiano André da Costa, Felipe A Zeiser, Gabriel de Oliveira Ramos, Rafael Kunst, and Rodrigo da Rosa Righi. A predictive maintenance model f"
    }
}
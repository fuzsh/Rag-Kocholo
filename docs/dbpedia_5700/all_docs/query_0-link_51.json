{
    "id": "dbpedia_5700_0",
    "rank": 51,
    "data": {
        "url": "https://aws.amazon.com/rekognition/faqs/",
        "read_more_link": "",
        "language": "en",
        "title": "Amazon Rekognition – frequently asked questions",
        "top_image": "https://a0.awsstatic.com/libra-css/images/logos/aws_logo_smile_1200x630.png",
        "meta_img": "https://a0.awsstatic.com/libra-css/images/logos/aws_logo_smile_1200x630.png",
        "images": [],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "Find answers to your most frequently asked questions for building visual analysis for your applications with Amazon Rekognition.",
        "meta_lang": "en",
        "meta_favicon": "https://a0.awsstatic.com/libra-css/images/site/fav/favicon.ico",
        "meta_site_name": "Amazon Web Services, Inc.",
        "canonical_link": "https://aws.amazon.com/rekognition/faqs/",
        "text": "Q: What is Amazon Rekognition?\n\nAmazon Rekognition is a service that makes it easy to add powerful visual analysis to your applications. Rekognition Image lets you easily build powerful applications to search, verify, and organize millions of images. Rekognition Video lets you extract motion-based context from stored or live stream videos and helps you analyze them.\n\nRekognition Image is an image recognition service that detects objects, scenes, activities, landmarks, faces, dominant colors, and image quality. Rekognition Image also extracts text, recognizes celebrities, and identifies inappropriate content in images. It also allows you to search and compare faces.\n\nRekognition Video is a video recognition service that detects activities, understands the movement of people in frame, and recognizes objects, celebrities, and inappropriate content in videos stored in Amazon S3 and live video streams. Rekognition Video detects persons and tracks them through the video even when their faces are not visible, or as the whole person might go in and out of the scene. For example, this could be used in an application that sends a real-time notification when someone delivers a package to your door. Rekognition Video allows you also to index metadata like objects, activities, scene, landmarks, celebrities, and faces that make video search easy.\n\nQ: Can I use Custom Labels for analyzing faces, customized text detection?\n\nNo. Custom Labels is meant for finding objects and scenes in images. Custom Labels is not designed for analyzing faces, customized text detection. You should use other Rekognition APIs for these tasks. Please refer to the documentation for face analysis, Text detection.\n\nQ: Can I use Custom Labels for finding unsafe image content?\n\nYes. Custom Labels is meant for finding objects and scenes in images. Custom Labels, when trained to detect your use case specific unsafe image content, can detect unsafe image content specific to your use case. Please also refer to the documentation for Moderation API to detect generic unsafe image content.\n\nQ: How many images are needed to train a custom model?\n\nThe number of images required to train a custom model depends on the variability of the custom labels you want the model to predict and the quality of the training data. For example, a distinct logo overlaid on an image can be detected with 1-2 training images, while a more subtle logo required to be detected under many variations (scale, viewpoint, deformations) may need in the order of tens to hundreds of training examples with high quality annotations. If you already have a high number of labeled images, we recommend training a model with as many images as you have available. Please refer to the documentation for limits on maximum training dataset size.\n\nAlthough hundreds of images may sometimes be required to train a custom model with high accuracy, with Custom Labels you can first train a model with tens of images per label, review your test results to understand where it does not work, and accordingly add new training images and train again to iteratively improve your model.\n\nQ: How many inference compute resources should I provision for my custom model?\n\nThe number of parallel inference compute resources needed depends on how many images you need to process at a given point in time. The throughput of a single resource will depend factors including the size of the images, the complexity of those images (how many detected objects are visible), and the complexity of your custom model. We recommend that you monitor the frequency at which you need provision your custom model, and the number of images that need to be processed at a single time, in order to schedule provisioning of your custom model most efficiently.\n\nIf you expect to process images periodically (e.g. once a day or week, or scheduled times during the day), you should Start provisioning your custom model at a scheduled time, process all your images, and then Stop provisioning. If you don’t stop provisioning, you will be charged even if no images are processed.\n\nQ: My training has failed. Will I be charged?\n\nNo. You will not be charged for the compute resources if your training fails.\n\nQ: What is Content Moderation?\n\nAmazon Rekognition’s Content Moderation API uses deep learning to detect explicit or suggestive adult content, violent content, weapons, visually disturbing content, drugs, alcohol, tobacco, hate symbols, gambling, and rude gestures in image and videos. Beyond flagging an image or video based on presence of inappropriate or offensive content, Amazon Rekognition also returns a hierarchical list of labels with confidence scores. These labels indicate specific sub-categories of the type of content detected, thus providing more granular control to developers to filter and manage large volumes of user generated content (UGC). This API can be used in moderation workflows for applications such as social and dating sites, photo sharing platforms, blogs and forums, apps for children, e-commerce site, entertainment and online advertising services.\n\nQ: What types of inappropriate, offensive, and unwanted content does Amazon Rekognition detect?\n\nYou can find a full list of content categories detected by Amazon Rekognition here.\n\nAmazon Rekognition returns a hierarchy of labels, as well as a confidence score for each detected label. For instance, given an inappropriate image, Rekognition may return “Explicit Nudity” with a confidence score as a top-level label. Developers can use this metadata to flag content at a high level, for example, when all types of explicit adult content is to be flagged. In the same response, Rekognition also returns second level of granularity by providing additional context like “Graphic Male Nudity” with its own confidence score. Developers can use this information to build more complex filtering logic to serve different geographies and demographics.\n\nPlease note that the Content Moderation API is not an authority on, or in any way purports to be an exhaustive filter of inappropriate and offensive content. Furthermore, this API does not detect whether an image includes illegal content (such as child pornography) or unnatural adult content.\n\nIf you require other types of inappropriate content to be detected in images, please reach out to us using the feedback process outlined later in this section.\n\nQ: How can I know which model version I am currently using?\n\nAmazon Rekognition makes regular improvement to its models. To keep track of model version, you can use the 'ModerationModelVersion' field in the API response.\n\nQ: How can I ensure that Amazon Rekognition meets accuracy goals for my image or video moderation use case?\n\nAmazon Rekognition’s Content Moderation models have been and tuned and tested extensively, but we recommend that you measure the accuracy on your own data sets to gauge performance.\n\nYou can use the ‘MinConfidence’ parameter in your API requests to balance detection of content (recall) vs the accuracy of detection (precision). If you reduce ‘MinConfidence’, you are likely to detect most of the inappropriate content, but are also likely to pick up content that is not actually inappropriate. If you increase ‘MinConfidence’ you are likely to ensure that all your detected content is truly inappropriate but some content may not be tagged.\n\nQ: How can I give feedback to Rekognition to improve its Content Moderation APIs?\n\nPlease send us your requests through AWS Customer Support. Amazon Rekognition continuously expands the types of inappropriate content detected based on customer feedback. Please note that illegal content (such as child pornography) will not be accepted through this process.\n\nQ: How do I add faces to a collection for search?\n\nTo add a face to an existing face collection, use the IndexFaces API. This API accepts an image in the form of an S3 object or image byte array and adds a vector representation of the faces detected to the face collection. IndexFaces also returns a unique FaceId and face bounding box for each of the face vectors added.\n\nMultiple face vectors of the same person can be aggregated to create and store user vectors using the CreateUser and AssociateFaces APIs. User vectors are more robust representations than single face vectors because they contain multiple face vectors with varying degrees of lighting, sharpness, poses, appearance differences, etc. Face search with user vectors can improve accuracy significantly compared to face search with single face vectors. User vectors are stored in the same collection as the associated face vectors.\n\nQ: How do I delete faces from a collection?\n\nTo delete a face from an existing face collection, use the DeleteFaces API. This API operates on the face collection supplied (using a CollectionId) and removes the entries corresponding to the list of FaceIds. If the FaceID is associated with a user vector, you will first need to use the DisassociateFaces API call to remove it from the user vector. Alternatively, you can delete the user vector from the collection using the DeleteUser API.\n\nFor more information on adding and deleting faces, please refer to our Managing Collections example.\n\nQ: How do I search for a user within a face collection?\n\nOnce you have created users and associated FaceIDs, you can search by using either an image (SearchUsersByImage), a UserId (SearchUsers), or a FaceID (SearchUsers). These APIs take in an input face and return a set of users that match, ordered by similarity score with the highest similarity first. For more details, please refer to our Searching Users example.\n\nQ: What are Amazon Rekognition Streaming Video Events?\n\nAmazon Rekognition Streaming Video Events uses machine learning to detect objects from connected camera to provide actionable alerts in real time. Amazon Rekognition Streaming Video events work with your new and existing Kinesis Video Streams to process video streams (up to 120 seconds per motion event) and notify you as soon a desired object of interest in detected. You can use these notifications to\n\nSend Smart Alerts to your end users such as “a package was detected at the front door.”\n\nProvide home automation capabilities such as “turning on the garage light when a person is detected.”\n\nIntegrate with smart assistants such as Echo devices to provide Alexa announcements when an object is detected.\n\nProvide Smart Search capabilities such as search for all video clips where a package was detected.\n\nQ: How does Amazon Rekognition Streaming Video Events work?\n\nYou can use your new or existing Kinesis Video Streams to get started with Amazon Rekognition Streaming Video events. When configuring your stream processor settings for Amazon Rekognition you can choose the desired labels (person, pet, or package) that you want to detect, the duration for the video (up to 120 seconds per motion event) that Rekognition should process for each event, and/or the region of interest on the frame that you want to process through Rekognition. Rekognition Streaming Video Event APIs process video only when you send a notification to Rekognition to start processing the video stream.\n\nWhen motion is detected on a connected camera, you send a notification to Rekognition to start processing the video stream. Rekognition processes the corresponding Kinesis Video Stream, post motion detection, to look for the desired objects specified by you. As soon as a desired object is detected, Amazon Rekognition will send you a notification. This notification includes the object detected, bounding box, zoomed in image of the object, and the time stamp.\n\nQ: What labels can Amazon Rekognition Streaming Video Events support?\n\nAmazon Rekognition Streaming Video Events can support people, pets, and packages.\n\nQ: What pets and package types can Amazon Rekognition Streaming Video APIs detect?\n\nAmazon Rekognition Streaming Video Event APIs support dogs and cats for pet detection. The API can detect medium and large cardboard boxes with high accuracy. The API also detects smaller boxes, bubble mailer envelopes, and folders but may miss some of these objects occasionally.\n\nQ: Will I be charged separately for each label detected? Can I choose which labels to opt into?\n\nNo, you will not be charged separately for each label. You will be charged for the duration of streaming video processed by Rekognition. You can either opt into specific labels (pet, package) or choose to opt in to all three labels (people, pet, package) while configuring your stream processing settings.\n\nQ: Do I need to stream video continuously to Amazon Rekognition?\n\nNo, you do not have to stream video continuously to Amazon Rekognition.\n\nQ: Should I create new Kinesis Video Streams (KVS) to use Streaming Video Events?\n\nAmazon Rekognition Streaming Video Events works with both new and existing Kinesis Video Streams. Simply integrate the relevant KVS streams with Amazon Rekognition Streaming Video Events API to get started with video analysis on KVS streams.\n\nQ: When will Amazon Rekognition send me a notification?\n\nAmazon Rekognition starts processing the video stream post motion detection. You can configure the duration for processing this video stream (up to 120 seconds per event). As soon as Amazon Rekognition detects the object of interest in the video stream, Rekognition will send you a notification. This notification includes the type of object detected, the bounding box, a zoomed in image of the object detected, and a time stamp.\n\nQ: What resolution and fps is support for label detection?\n\nIn order to keep costs and latency low Amazon Rekognition Streaming Video Events support 1080p or lower resolution video streams. Rekognition processes the video stream at 5 fps.\n\nQ: What codecs and file format is supported for streaming video?\n\nAmazon Rekognition Video supports H.264 files in MPEG-4 (.mp4) or MOV format.\n\nQ: What is the maximum duration of the video processed per event?\n\nYou can process up to 120 seconds of video per event.\n\nQ: Can I choose a particular area of the frame to be processed for my video stream?\n\nYes, as a part of configuring your StreamProcessor you can choose the region of interest that you want to process on your frame. Amazon Rekognition will only process that particular area of the frame.\n\nQ: How many concurrent video streams can I process with Amazon Rekognition?\n\nAmazon Rekognition Streaming Video Events can support 600 concurrent sessions per AWS customer. Please reach out to your account manager if you need to increase this limit.\n\nQ: What types of entities can Amazon Rekognition Video detect?\n\nAmazon Rekognition Video can detect objects, scenes, landmarks, faces, celebrities, text, and inappropriate content in videos. You can also search for faces appearing in a video using your own repository or collection of face images.\n\nQ: What types file formats and codecs does Amazon Rekognition Video support?\n\nAmazon Rekognition Video supports H.264 files in MPEG-4 (.mp4) or MOV format. If your video files use a different codec, you can transcode them into H.264 using AWS Elemental MediaConvert.\n\nQ: How do Amazon Rekognition Video asynchronous APIs work?\n\nAmazon Rekognition Video can process videos stored in an Amazon S3 bucket. You can use an asynchronous set of operations: you start video analysis by calling a Start operation such as StartLabelDetection for detecting objects and scenes. The completion status of the request is published to an Amazon Simple Notification Service (SNS) topic. To get the completion status from the Amazon SNS topic, you can use an Amazon Simple Queue Service (SQS) queue or an AWS Lambda function. Once you have the completion status, you call a Get operation such as GetLabelDetection to get the results of the request. For a list of available Amazon Rekognition Video APIs, please see this page.\n\nQ: How can I find the timeline for each detection in a video?\n\nAmazon Rekognition Video returns label results by timestamps or video segments. You can choose how you want to organize these results by using the AggregateBy input parameter in GetLabelDetection API.\n\nWhen label results are organized by timestamps, each label will be returned every time Amazon Rekognition Video detects that label in the video timeline. For example, if ‘Dog’ is detected at 2000ms and 4000ms, Amazon Rekognition Video will return 2 label entries for ‘Dog’, one with 2000ms and another with 4000ms.\n\nWhen label results are organized by video segments, Amazon Rekognition Video returns the video segment for when a label is detected across multiple consecutive frames. A video segment is defined by a start timestamp, an end timestamp, and a duration. For example, if ‘Dog’ is detected in 2 consecutive frames at 2000ms and 4000ms, Amazon Rekognition Video will return 1 label entry for ‘Dog’ with start timestamp 2000ms, end timestamp 4000ms, and duration 2000ms.\n\nTo learn more about timestamps and segments and see a sample API response, visit Detecting labels in a video.\n\nQ: How many concurrent video analysis jobs can I run with Amazon Rekognition Video?\n\nYou can process up to 20 concurrent jobs with Amazon Rekognition Video. For more details on limits, please see our limits page.\n\nQ: What video resolution should I use?\n\nAmazon Rekognition Video automatically handles a wide range of video resolutions and video quality. We recommend using a 720p (1280×720 pixels) to 1080p (1920x1080 pixels), or their equivalent resolutions in other aspect ratios for optimum results. Very low resolution (such as QVGA or 240p) and very low-quality videos may adversely impact results quality.\n\nQ: What is People Pathing?\n\nWith Rekognition Video, you can find the path of each person across the video timeline. Rekognition Video detects persons even when the camera is in motion and, for each person, returns a bounding box and the face, along with face attributes and timestamps. For retail applications, this allows to generate customer insights anonymously, such as how customers move across aisles in a shopping mall or how long they are waiting in checkout lines.\n\nQ: What types of media analysis segments can Amazon Rekognition Video detect?\n\nAmazon Rekognition Video can detect the following types of segments or entities for media analysis:\n\nBlack frames: Videos often contain a short duration of empty black frames with no audio that are used as cues to insert advertisements, or to demarcate the end of a program segment such as a scene or the opening credits. With Amazon Rekognition Video, you can detect such black frame sequences to automate ad insertion, package content for VOD, and demarcate various program segments or scenes. Black frames with audio (such as fade outs or voiceovers) are considered as content and not returned.\n\nCredits: Amazon Rekognition Video helps you automatically identify the exact frames where the opening and closing credits start and end for a movie or TV show. With this information, you can generate ‘binge markers’ or interactive viewer prompts such as ‘Next Episode’ or ‘Skip Intro’ in VOD applications. Amazon Rekognition Video is trained to handle a wide variety of opening and end credit styles ranging from simple rolling credits to more challenging credits alongside content, credits on scenes, or stylized credits in anime content.\n\nShots: A shot is a series of interrelated consecutive pictures taken contiguously by a single camera and representing a continuous action in time and space. With Amazon Rekognition Video, you can detect the start, end, and duration of each shot, as well as a count all the shots in a piece of content. Shot metadata can be used for applications such as creating promotional videos using selected shots, generating a set of preview thumbnails that avoid transitional content between shots, and inserting ads in spots that don’t disrupt viewer experience, such as the middle of a shot when someone is speaking.\n\nColor Bars: Amazon Rekognition Video allows you to detect sections of video that display SMPTE or EBU color bars, which are a set of colors displayed in specific patterns to ensure color is calibrated correctly on broadcast monitors, programs, and on cameras. For more information about SMPTE color bars, see SMPTE color bar. This metadata is useful to prepare content for VOD applications by removing color bar segments from the content, or to detect issues such as loss of broadcast signals in a recording, when color bars are shown continuously as a default signal instead of content.\n\nSlates: Slates are sections, typically at the beginning of a video, that contain text metadata about the episode, studio, video format, audio channels, and more. Amazon Rekognition can identify the start and end such slates, making it easy for operators to use the text metadata or to simply remove the slate when preparing content for final viewing.\n\nStudio logos: Studio logos are sequences that show the logos or emblems of the production studio involved in making the show. Amazon Rekognition can identify such sequences, making it easy for operators to review them for identifying studios.\n\nContent: Content refers to the portions of the TV show or movie that contain the program or related elements. Black frames, credits, color bars, slates, and studio logos are not considered to be content. Amazon Rekognition Video enables you to detect the start and end of each content segment in the video, which enables multiple uses such as finding the program run time or finding certain segments that serve specific purposes. For example, a quick recap of the previous episode at the beginning of the video is a type of content. Similarly, bonus post-credit content can appear after the credits have finished. And, some videos may have ‘textless’ content at the end of the video, which are a set of all program content that contains overlaid text, but with that text removed to enable internationalization in another language. Once all the content segments are detected with Amazon Rekognition Video, you can apply specific domain knowledge such as ‘my videos always start with a recap’ to further categorize each segment or to send them for human review.\n\nAmazon Rekognition Video provides the start, end, duration, and timecodes for each detected entity, and provides timestamp (milliseconds), SMPTE format code, and frame number options for each.\n\nQ: How do I get started with media analysis using Amazon Rekognition Video?\n\nMedia analysis features are available through the Amazon Rekognition Video segment detection API. This is an asynchronous API composed of two operations: StartSegmentDetection to start the analysis, and GetSegmentDetection to get the analysis results. To get started, please refer to the documentation.\n\nIf you want to visualize the results of media analysis or even try out other Amazon AI services like Amazon Transcribe with your own videos, please use the Media Insights application – a serverless framework and demo application to easily generate insights and develop applications for your video, audio, text, and image resources, using AWS Machine Learning and Media services. You can easily spin up your own demo application using the supplied AWS CloudFormation template, to try out your own videos and visualize analysis results.\n\nQ: What is a frame accurate timecode?\n\nFrame accurate timecodes provide the exact frame number for a relevant segment of video or entity. Media companies commonly process timecodes using the SMPTE (Society of Motion Picture and Television Engineers) format hours:minutes:seconds:frame number, for example, 00:24:53:22.\n\nQ: Is Amazon Rekognition Video segment detection frame accurate?\n\nYes, the Amazon Rekognition Video segment detection API provides frame accurate SMPTE timecodes, as well as millisecond timestamps for the start and end of each detection.\n\nQ: What types of frame rate formats can Amazon Rekognition Video segment detection handle?\n\nAmazon Rekognition Video segment detection automatically handles integer, fractional and drop frame standards for frame rates between 15 and 60fps. For example, common frame rates such as 23.976 fps, 25fps, 29.97 fps and 30fps are supported by segment detection. Frame rate information is utilized to provide frame accurate timecodes in each case.\n\nQ: What filtering options can I apply?\n\nYou can specify the minimum confidence for each segment type while making the API request. For example, you can filter out any segment below 70% confidence score. For black frame detection, you can also control the maximum pixel luminance that you consider to be a black pixel, for example, a value of 40 for a color range of 0 to 255. Further, you can also control what percentage of pixels in a frame need to meet this black pixel luminance criteria for the frame to be classified as a black frame, for example, 99%. These filters allow you to account for varied video quality and formats when detecting black frames. For example, videos reclaimed from tape archives might be noisy and have a different black level compared to a modern digital video. For more details, please refer to this page.\n\nQ: Are image and video inputs processed by Amazon Rekognition stored, and how are they used by AWS?\n\nAmazon Rekognition may store and use image and video inputs processed by the service solely to provide and maintain the service and, unless you opt out as provided below, to improve and develop the quality of Amazon Rekognition and other Amazon machine-learning/artificial-intelligence technologies. Use of your content is important for continuous improvement of your Amazon Rekognition customer experience, including the development and training of related technologies. We do not use any personally identifiable information that may be contained in your content to target products, services or marketing to you or your end users. Your trust, privacy, and the security of your content are our highest priority and we implement appropriate and sophisticated technical and physical controls, including encryption at rest and in transit, designed to prevent unauthorized access to, or disclosure of, your content and ensure that our use complies with our commitments to you. Please see https://aws.amazon.com/compliance/data-privacy-faq/ for more information. You may opt out of having your image and video inputs used to improve or develop the quality of Amazon Rekognition and other Amazon machine-learning/artificial-intelligence technologies by using an AWS Organizations opt-out policy. For information about how to opt out, see Managing AI services opt-out policy.\n\nQ: Is the content processed by Amazon Rekognition moved outside the AWS region where I am using Amazon Rekognition?\n\nAny content processed by Amazon Rekognition is encrypted and stored at rest in the AWS region where you are using Amazon Rekognition. Unless you opt out as provided below, some portion of content processed by Amazon Rekognition may be stored in another AWS region solely in connection with the continuous improvement and development of your Amazon Rekognition customer experience and other Amazon machine-learning/artificial-intelligence technologies. You can request deletion of image and video inputs associated with your account by contacting AWS Support. Your trust, privacy, and the security of your content are our highest priority and we implement appropriate and sophisticated technical and physical controls, including encryption at rest and in transit, designed to prevent unauthorized access to, or disclosure of, your content and ensure that our use complies with our commitments to you. Please see https://aws.amazon.com/compliance/data-privacy-faq/ for more information. Your content will not be stored in another AWS region if you opt out of having your content used to improve and develop the quality of Amazon Rekognition and other Amazon machine-learning/artificial-intelligence technologies. For information about how to opt out, see Managing AI services opt-out policy."
    }
}
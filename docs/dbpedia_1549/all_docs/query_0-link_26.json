{
    "id": "dbpedia_1549_0",
    "rank": 26,
    "data": {
        "url": "https://developer.amazon.com/en-US/docs/alexa/custom-skills/speech-synthesis-markup-language-ssml-reference.html",
        "read_more_link": "",
        "language": "en",
        "title": "Speech Synthesis Markup Language (SSML) Reference",
        "top_image": "https://ds6yc8t7pnx74.cloudfront.net/en-US/alexa/techdoc-template.thumb.800.480.png?ck=1625002623",
        "meta_img": "https://ds6yc8t7pnx74.cloudfront.net/en-US/alexa/techdoc-template.thumb.800.480.png?ck=1625002623",
        "images": [
            "https://d7qzviu3xw2xc.cloudfront.net/alexa/assets/images/Alexa_Logo_RGB_BLUE.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "You can use Speech Synthesis Markup Language (SSML) in your output speech response to control how Alexa generates the speech. For example, you can add pauses and other speech ef...",
        "meta_lang": "en",
        "meta_favicon": "https://m.media-amazon.com/images/G/01/AlexaDevPortal/favicon-32x32._CB1544644721_.png",
        "meta_site_name": "Amazon Alexa",
        "canonical_link": "https://developer.amazon.com/en-US/docs/alexa/custom-skills/speech-synthesis-markup-language-ssml-reference.html",
        "text": "Note: Sign in to the developer console to build or publish your skill.\n\nYou can use Speech Synthesis Markup Language (SSML) in your output speech response to control how Alexa generates the speech. For example, you can add pauses and other speech effects.\n\nAbout SSML\n\nWhen the your skill returns a response to a request, you provide text that the Alexa service converts to speech. Alexa automatically handles normal punctuation, such as pausing after a period, or speaking a sentence ending in a question mark as a question.\n\nHowever, sometimes you might want additional control over how Alexa generates the speech from the text in your response. For example, you might want a longer pause within the speech, or you might want Alexa to read a string of digits as a standard telephone number. The Alexa Skills Kit provides this type of control with Speech Synthesis Markup Language (SSML) support.\n\nSSML is a markup language that provides a standard way to mark up text for the generation of synthetic speech. The Alexa Skills Kit supports a subset of the tags defined in the SSML specification. For the list of supported tags, see Supported SSML Tags.\n\nUse SSML in your response\n\nTo use SSML, construct your output speech with the supported SSML tags. When you send a response from your service, you must indicate that the speech is in SSML rather than plain text. If you construct the JSON response directly, provide the marked-up text in the outputSpeech property and set the type to SSML instead of PlainText. Use the ssml property instead of text for the marked-up text:\n\nYou can use SSML with both the normal output speech response and any re-prompt included in the response.\n\nIf you use the Alexa Skills Kit SDK for Node.js or Alexa Skills Kit SDK for Java, the SDK wraps the SSML in the <speak> tag automatically.\n\nThe following example shows the SSML within <speak> tags.\n\nIn the JSON output for the SSML, either escape quotation marks within the output, or use an appropriate mix of single and double quotation marks. The following example wraps the response in double quotation marks and uses single quotation marks for attributes.\n\nImportant: Unpronounceable Unicode characters aren't allowed in SSML.\n\nIf you use Alexa Presentation Language (APL) for audio, you can use the Speech component to render SSML. Set the content property to the SSML text, enclosed with <speak> tags. Set the contentType property to SSML.\n\nThe following example shows an APL for audio document. The first Speech component renders plain text. The second Speech component renders SSML.\n\nFor more about APL for audio, see APL for Audio Reference.\n\nSupported SSML tags\n\nThe Alexa Skills Kit supports the following SSML tags, listed in alphabetic order.\n\n<amazon:domain>\n\n<amazon:effect>\n\n<amazon:emotion>\n\n<audio>\n\n<break>\n\n<emphasis>\n\n<lang>\n\n<p>\n\n<phoneme>\n\n<prosody>\n\n<s>\n\n<say-as>\n\n<speak>\n\n<sub>\n\n<voice>\n\n<w>\n\nNote that the Alexa service strips out any unsupported SSML tags included in the text you provide.\n\nYou can combine most supported tags with each other to apply multiple effects on the speech. For instance, this example uses both the <say-as> and <amazon:emotion> tags. This tells Alexa to speak the entire string in an \"excited\" voice, and speak the provided number as individual digits:\n\nYou can't combine all tags. The following tags can't be applied to the same speech:\n\n<amazon:domain>\n\nYou must combine <amazon:domain name=\"conversational\"> with the <voice> tag and the Matthew or Joanna voice. The conversational style doesn't work with other voices, and it doesn't work on its own without <voice>.\n\nYou can combine <amazon:domain name=\"news\"> with the <voice> tag and the Matthew, Joanna, and Lupe voices. The news style doesn't work with other voices.\n\nYou can't combine <amazon:domain name=\"long-form\">, <amazon:domain name=\"music\">, or <amazon:domain name=\"fun\"> with <voice>.\n\n<amazon:emotion>\n\nspeechcons\n\nSpeechcons use the <say-as> tag with interpret-as set to interjection, for example: <say-as interpret-as=\"interjection\">wow</say-as>.\n\nYou can combine <say-as> with other tags when you use other values for the interpret-as attribute. For example, you could combine <amazon:emotion> or <emphasis> with <say-as interpret-as=\"ordinal\">1</say-as>.\n\n<voice>\n\nYou can combine <voice> with the <amazon:domain> tag with the restrictions noted before.\n\nYou can't combine <voice> with any of the other tags listed here.\n\n<emphasis>\n\n<prosody> with the pitch attribute (for example, <prosody pitch=\"x-low\">…</pitch>)\n\nFor example, the following examples don't work:\n\nInvalid SSML: <voice> used within <amazon:emotion>\n\nInvalid SSML: <amazon:emotion> used within <voice>\n\nIncompatible voice used with conversational or news style\n\nInvalid: Conversational style used without <voice> tag\n\nYou can use the incompatible tags in the same <speak> string, as long as they're not applied to the same text string. For example, the following combination is valid:\n\nIn this example, the first line is spoken in a disappointed voice, the second line is spoken in the Kendra voice, and the final line uses the disappointed voice again.\n\namazon:domain\n\nNote: The <amazon:domain> tag is available in the following locales: English (US), English (UK), English (CA), English (AU), German (DE), and Japanese (JP). Not all styles are available in all locales.\n\nApplies different speaking styles to the speech. The styles are curated text-to-speech voices that use different variations of intonation, emphasis, pausing, and other techniques to match the speech to the content. For example, the news style makes Alexa's voice sound like what you might expect to hear in a TV or radio newscast, and was built primarily for customers to listen to news articles and other news-based content.\n\nThe <amazon:domain> tag takes the following required parameters:\n\nAttribute Possible Values\n\nname\n\nThe name of the speaking style to apply to the speech. Available styles:\n\nconversational – Style voices to sound more conversational and less formal, more like how people sound when they speak to friends and family. The conversational style is available in English (US), Italian (IT), and Japanese (JP) skills. You can also use this style with Amazon Polly voices. For Amazon Polly, conversational requires the <voice> tag and the Matthew or Joanna voices.\n\nlong-form – Style the speech for long-form content such as podcasts, articles, and blogs. The long-form style can't be used with the <voice> tag. The long-form style is available in English (US) skills.\n\nmusic – Style the speech for talking about music, video, or other multi-media content. The music style can't be used with the <voice> tag. The music style is available in English (US), English (CA), English (UK), and German (DE) skills.\n\nnews – Style the speech similar to what you hear when listening to the news on the radio or television. The news style can be combined with the <voice> tag and the Matthew, Joanna, and Lupe voices. The news style is available in English (US) and English (AU) skills.\n\nfun – Style the speech to sound more friendly and animated in greetings, animation or children stories. The fun style is available in Japanese (JP) skills.\n\nExamples\n\nnews\n\nmusic\n\nlong-form\n\nfun\n\nnews combined with voice\n\nThis example uses two different voices in the same response.\n\nconversational combined with voice\n\nThe <amazon:domain name=\"conversational\"> works with the <voice> tag and the Matthew and Joanna voices. You can't use conversational without the <voice> tag.\n\nYou can combine <amazon:domain> with all other tags, except for those listed in incompatible tags.\n\nBest practices for the amazon:domain tag\n\nThese recommendations can help you build a better experience with the <amazon:domain> tag:\n\nUse the default Alexa voice without the <amazon:domain> tag in the intro to your skill. This sets a \"baseline,\" so that the specialized speaking styled responses later have more impact.\n\nDon't overdo the use of the speaking styles, as this might create a poor or unpleasant user experience. For example, don't switch between different speaking styles frequently.\n\nTest how your responses sound with a device or the simulator in the developer console and verify that speaking style is appropriate for the response.\n\namazon:effect\n\nApplies Amazon-specific effects to the speech.\n\nAttribute Possible values\n\nname\n\nThe name of the effect to apply to the speech.\n\nAccepted values:\n\nwhispered: Applies a whispering effect to the speech.\n\namazon:emotion\n\nNote: The <amazon:emotion> tag is available in the following locales: English (US), English (UK), German (DE), and Japanese (JP).\n\nThe <amazon:emotion> tag causes Alexa to express emotion when speaking. The emotion effects are useful for stories, games, news updates and other narrative content. For instance, in a game, you might use the \"excited\" emotion for correct answers and the \"disappointed\" emotion for incorrect answers.\n\nThe <amazon:emotion> tag takes the following required parameters:\n\nAttribute Possible values\n\nExamples\n\nYou can combine <amazon:emotion> with all other tags, except for those listed in incompatible tags.\n\nFor example, this adds a three-second pause in the middle of speech with the \"excited\" emotion:\n\nThis example uses <prosody> to increase the volume of the \"disappointed\" speech.\n\nBest practices for the <amazon:emotion> tag\n\nThese recommendations can help you build a better experience with the <amazon:emotion> tag:\n\nUse the default Alexa voice without the <amazon:emotion> tag in the intro to your skill. This sets a \"baseline,\" so that the emotional responses later can have more impact.\n\nDon't overuse emotional responses, as this might create a poor or unpleasant experience. Consider these guidelines:\n\nDon't switch between excited and disappointed frequently.\n\nDon't use the emotions in every response.\n\nTry the medium intensity initially, and then adjust the intensity as needed. Using medium in most instances gives you more options for adjusting the intensity up or down depending on the situation.\n\nTest how your responses sound with a device or the simulator in the developer console and make sure that the voice is appropriate for the response.\n\naudio\n\nThe <audio> tag lets you provide the URL for an MP3 file that the Alexa service can play. Use the <audio> tag to embed short, pre-recorded audio within your response. For example, you could include sound effects alongside your text-to-speech responses, or provide a response that uses a voice associated with your brand.\n\nTip: For a library of sound effects you can use with the <audio> tag, see the Alexa Skills Kit Sound Library.\n\nAttribute Possible values\n\nsrc\n\nSpecifies the URL for the MP3 file. Note the following requirements and limitations:\n\nThe MP3 must be hosted at an Internet-accessible HTTPS endpoint. HTTPS is required, and the domain hosting the MP3 file must present a valid, trusted SSL certificate. You can't use self-signed certificates.\n\nThe MP3 must not contain any customer-specific or other sensitive information.\n\nThe MP3 must be a valid MP3 file (MPEG version 2).\n\nFor your speech response, the audio file can't be longer than 240 seconds.\n\nThe combined total time for all audio files in the outputSpeech property of the response can't be more than 240 seconds.\n\nThe combined total time for all audio files in the reprompt property of the response can't be more than 90 seconds.\n\nThe bit rate must be 48 kbps. Note that this bit rate gives a good result when used with spoken content, but is generally not a high enough quality for music.\n\nThe sample rate must be 22050 Hz, 24000 Hz, or 16000 Hz.\n\nUse converter software to convert your MP3 files to the required codec version (MPEG version 2) and bit rate (48 kbps).\n\nInclude the <audio> tag within your text-to-speech response within the <speak> tag. Alexa plays the MP3 at the specified point within the text to speech. For example:\n\nWhen Alexa renders this response, it sounds like this:\n\nAlexa: Welcome to Ride Hailer.\n\n(the specified amzn_sfx_car_accelerate_01.mp3 audio file plays)\n\nAlexa: You can order a ride, or request a fare estimate. Which do you want?\n\nA single response sent by your service can include multiple <audio> tags according to the following limits:\n\nNo more than five audio files can be used in a single response.\n\nThe combined total time for all audio files in the outputSpeech property of the response can't be more than 240 seconds.\n\nThe combined total time for all audio files in the reprompt property of the response can't be more than 90 seconds.\n\nConverting audio files to an Alexa-friendly format\n\nYou can use converter software to convert your MP3 files to the required codec version (MPEG version 2) and bit rate (48 kbps). One option is a command-line tool, FFmpeg.\n\nThis sample command converts the provided <input-file> to an MP3 file that works with the <audio> tag. This version uses 16000 as the sample rate:\n\nYou might get better quality by increasing the sample rate to 24000 like this:\n\nSee the documentation for FFmpeg for details about command line options.\n\nAnother option is Audacity:\n\nOpen the file to convert.\n\nSet the Project Rate in the lower-left corner to 16000.\n\nClick File > Export Audio and change the Save as type to MP3 Files.\n\nClick Options, set the Quality to 48 kbps and the Bit Rate Mode to Constant.\n\nThis requires the Lame library, which can be found at: http://lame.buanzo.org/#lamewindl.\n\nHosting the audio files for your skill\n\nThe MP3 files you use to provide audio must be hosted on an endpoint that uses HTTPS. The endpoint must provide an SSL certificate signed by an Amazon-approved certificate authority. Many content hosting services provide this. For example, you could host your files at a service such as Amazon Simple Storage Service (Amazon S3) (an Amazon Web Services offering).\n\nYou aren't required to authenticate the requests for the audio files. Therefore, you must not include any customer-specific or sensitive information in these audio files. For example, building a custom MP3 file in response to a user's request, and including sensitive information within the audio, isn't allowed.\n\nFor optimal performance, Amazon recommends that you host your MP3 files for SSML responses in close proximity to where your skill is hosted. For example, if the Lambda function for your skill is hosted in the US West (Oregon) region, you will get better performance if you upload your MP3s to a US West (Oregon) S3 bucket.\n\nIn addition to using S3 for hosting, Amazon recommends that you use a content delivery network (CDN) such as AWS CloudFront for hosting media assets to prevent throttling under high load.\n\nHTTP Live Streaming (HLS) of audio files\n\nAlexa supports SSML <audio> tags that point toward HTTP Live Streaming (HLS) streams, provided that the audio data conforms to the listed specifications. Due to the streaming approach that Alexa uses, there is no benefit to using HLS streams instead of statically served MP3 files. Furthermore, unlike with statically served MP3 files, an SSML response that contains an HLS stream that violates the 240-second duration limit fails silently. This silent failure means that the playback stops before the limit is hit, no error message is generated on the customer device, and the skill doesn't receive an error request. If your skill uses SSML responses that contain HLS streams, make sure that you take particular care to test the audio returned in its responses.\n\nbreak\n\nRepresents a pause in the speech. Set the length of the pause with the strength or time attributes.\n\nImportant: Break tag silence can't exceed 10 seconds, including scenarios with consecutive break tags. SSML with more than 10 seconds of silence isn't rendered to the user.\n\nAttribute Possible Values\n\nstrength\n\nThe strength or length to pause.\n\nAccepted values:\n\nnone: Don't output a pause. Use this to remove a pause that would normally occur, such as after a period.\n\nx-weak: Don't output a pause. Equivalent to none.\n\nweak: Treat adjacent words as if separated by a single comma. Equivalent to medium.\n\nmedium: Treat adjacent words as if separated by a single comma.\n\nstrong: Make a sentence break. Equivalent to using the <s> tag.\n\nx-strong: Make a paragraph break. Equivalent to using the <p> tag.\n\ntime\n\nDuration of the pause; up to 10 seconds (10s) or 10000 milliseconds (10000ms). Include the unit with the time (s or ms).\n\nThe default is medium. This is used if you don't specify any attributes, or if you provide any unsupported attribute values.\n\nemphasis\n\nEmphasize the tagged words or phrases. Emphasis changes rate and volume of the speech. More emphasis is spoken louder and slower. Less emphasis is quieter and faster.\n\nAttribute Possible values\n\nlevel\n\nLevel of emphasis to apply.\n\nAccepted values:\n\nstrong: Increase the volume and slow down the speaking rate so the speech is louder and slower.\n\nmoderate: Increase the volume and slow down the speaking rate, but not as much as when set to strong. This is used as a default when level isn't provided.\n\nreduced: Decrease the volume and speed up the speaking rate. The speech is softer and faster.\n\nYou can combine <emphasis> with all other tags, except for those listed in incompatible tags.\n\nNote: When you modify the speech with the <emphasis> tag, Alexa uses a legacy text-to-speech system, which might change the speech sound quality.\n\nlang\n\nUse <lang> to specify the language model and rules to speak the tagged content as if it were written in the language specified by the xml:lang attribute. Words and phrases in other languages usually sound better when enclosed with the <lang> tag. This is useful for short phrases in other languages, such as the names of restaurants or shops.\n\nThe following example shows the SSML to pronounce \"Paris\" using the language code fr-FR, which refers to the French language as spoken in France.\n\nAlexa adapts the pronunciation to use the sounds available in the original language of the skill, so it might not sound exactly like a native speaker. To achieve a more natural voice than what you get with the <lang> tag alone, use the <lang> tag and the <voice> tag together. With the <voice>, you can select a voice customized for a specific language. Make sure that the language of the tagged text matches the <lang> attribute, and that the <voice> is specific to the language of the text.\n\nFor example, consider the French phrase \"J'adore chanter\" in an English (US) skill. The following examples show how Alexa speaks this phrase without the <lang> tag, with the <lang> tag alone, and with both <lang> and <voice>.\n\nWithout any tags, Alexa speaks the phrase with English-like pronunciation.\n\nWith the <lang xml:lang='fr-FR'> tag, Alexa uses French pronunciation with sounds available in English for a \"French-like\" pronunciation. A perfect French pronunciation would include an uvular trill (/R/) in the word \"adore.\" The French-like English pronunciation achieved with the <lang> tag uses the corresponding /r/ sound instead.\n\nFor a better French pronunciation, use the <voice> tag with a French voice. The following example uses the Celine voice.\n\nSupported locales for the xml:lang attribute\n\nThe <lang> tag supports the following locales:\n\nde-DE\n\nen-AU\n\nen-CA\n\nen-GB\n\nen-IN\n\nen-US\n\nes-ES\n\nes-MX\n\nes-US\n\nfr-CA\n\nfr-FR\n\nhi-IN\n\nit-IT\n\nja-JP\n\npt-BR\n\np\n\nRepresents a paragraph. This tag provides extra-strong breaks before and after the tag. This is equivalent to specifying a pause with <break strength=\"x-strong\"/>.\n\nphoneme\n\nProvides a phonemic/phonetic pronunciation for the contained text. For example, people might pronounce words like \"pecan\" differently.\n\nAttribute Possible values\n\nWhen you use , Alexa uses the pronunciation provided in the `ph` attribute instead of the text contained within the tag. However, you should still provide human-readable text within the tags. In the following example, the word \"pecan\" shown within the tags is never spoken. Instead, Alexa speaks the text provided in the `ph` attribute:\n\nAdditional examples of writing words with a phonetic alphabet:\n\nWord IPA X-SAMPA\n\nbottle\n\nˈbɑ.təl\n\n\"bA.t@l\n\nfrozen\n\nˈfɹoʊ.zən\n\n\"fr\\oU.z@n\n\nblossom\n\nˈblɑ.səm\n\n\"blA.s@m\n\nSupported symbols\n\nThe following tables list the supported symbols for use with the <phoneme> tag. The symbols are specific to the skill's language.\n\nArabic (SA)\n\nDutch (NL)\n\nEnglish (AU)\n\nEnglish (CA)\n\nEnglish (IN)\n\nEnglish (UK)\n\nEnglish (US)\n\nFrench (CA)\n\nFrench (FR)\n\nGerman (DE)\n\nHindi (IN)\n\nItalian (IT)\n\nJapanese (JP)\n\nPortuguese (BR)\n\nprosody\n\nModifies the volume, pitch, and rate of the tagged speech.\n\nAttribute Possible values\n\nrate\n\nModify the rate of the speech.\n\nAccepted values:\n\nx-slow, slow, medium, fast, x-fast: Set the rate to a predefined value.\n\nn%: specify a percentage to increase or decrease the speed of the speech:\n\n100% indicates no change from the normal rate.\n\nPercentages greater than 100% increase the rate.\n\nPercentages below 100% decrease the rate.\n\nThe minimum value you can provide is 20%.\n\npitch\n\nRaise or lower the tone (pitch) of the speech.\n\nAccepted values:\n\nx-low, low, medium, high, x-high: Set the pitch to a predefined value.\n\n+n%: Increase the pitch by the specified percentage. For example: +10%, +5%. The maximum value allowed is +50%. A value higher than +50% is rendered as +50%.\n\n-n%: Decrease the pitch by the specified percentage. For example: -10%, -20%. The smallest value allowed is -33.3%. A value lower than -33.3% is rendered as -33.3%.\n\nNote: When you modify the speech with the pitch tag, Alexa uses a legacy text-to-speech system, which might change the speech sound quality.\n\nvolume\n\nChange the volume for the speech.\n\nAccepted values:\n\nsilent, x-soft, soft, medium, loud, x-loud: Set volume to a predefined value for current voice.\n\n+ndB: Increase volume relative to the current volume level. For example, +0dB means no change of volume. +6dB is approximately twice the current amplitude. The maximum positive value is about +4.08dB.\n\n-ndB: Decrease the volume relative to the current volume level. For example, -6dB means approximately half the current amplitude.\n\nYou can combine <prosody> with all other tags when you set the rate and/or volume attributes. When you use the pitch attribute, you can't combine <prosody> with the tags shown in incompatible tags.\n\ns\n\nRepresents a sentence. This tag provides strong breaks before and after the tag.\n\nThis is equivalent to:\n\nEnding a sentence with a period (.).\n\nSpecifying a pause with <break strength=\"strong\"/>.\n\nsay-as\n\nDescribes how the text should be interpreted. This lets you provide additional context to the text and eliminate any ambiguity on how Alexa renders the text. Indicate how Alexa should interpret the text with the interpret-as attribute.\n\nAttribute Possible values\n\ninterpret-as\n\nSpecify how to interpret the text.\n\nAccepted values:\n\ncharacters, spell-out: Spell out each letter.\n\ncardinal, number: Interpret the value as a cardinal number.\n\nordinal: Interpret the value as an ordinal number.\n\ndigits: Spell each digit separately .\n\nfraction: Interpret the value as a fraction. This works for both common fractions (such as 3/20) and mixed fractions (such as 1+1/2).\n\nunit: Interpret a value as a measurement. The value should be either a number or fraction followed by a unit (with no space in between) or just a unit.\n\ndate: Interpret the value as a date. Specify the format with the format attribute.\n\ntime: Interpret a value such as 1'21\" as duration in minutes and seconds.\n\ntelephone: Interpret a value as a 7-digit or 10-digit telephone number. This can also handle extensions (for example, 2025551212x345).\n\naddress: Interpret a value as part of street address.\n\ninterjection: Interpret the value as an interjection. Alexa speaks the text in a more expressive voice. For optimal results, only use the supported interjections and surround each speechcon with a pause. For example: <say-as interpret-as=\"interjection\">Wow.</say-as>. Speechcons are supported for the languages listed below.\n\nexpletive: \"Bleep\" out the content inside the tag.\n\nformat\n\nApplies when interpret-as is set to date. Set to one of the following to indicate format of the date:\n\nmdy\n\ndmy\n\nymd\n\nmd\n\ndm\n\nym\n\nmy\n\nd\n\nm\n\ny\n\nAlternatively, if you provide the date in YYYYMMDD format, the format attribute is ignored.\n\nInclude question marks (?) for portions of the date to leave out. For instance, Alexa speaks <say-as interpret-as=\"date\">????0922</say-as> as \"September twenty-second.\" For an example, see Example: Provide a date without the year.\n\nAlexa attempts to interpret the provided text correctly based on the formatting even without this tag. For example, if your output speech includes \"202-555-1212\", Alexa interprets the number as a phone number and speaks each individual digit, with a brief pause for each dash. The <say-as interpret-as=\"telephone\"> tag isn't necessary. However, if you provided the text \"2025551212\", but you wanted Alexa to speak it as a phone number, you must use <say-as interpret-as=\"telephone\">.\n\nExample: Telephone number\n\nThe following example shows the telephone attribute for interpret-as.\n\nExample: Spell out words and numbers\n\nThe following example shows how to use interpret-as to spell out words and numbers. Note that Alexa interprets the number 12345 as a cardinal number automatically.\n\nExample: Use an interjection (speechcon)\n\nThe following example shows the difference between normal speech and a speechcon.\n\nThe following example shows how Alexa interprets a date with question marks (?).\n\nSupported speechcons\n\nSpeechcons are language specific. See the following pages for the available speechcons for each skill language:\n\nEnglish (AU)\n\nEnglish (CA)\n\nEnglish (IN)\n\nEnglish (UK)\n\nEnglish (US)\n\nFrench (CA)\n\nFrench (FR)\n\nGerman (DE)\n\nHindi (IN)\n\nItalian (IT)\n\nJapanese (JP)\n\nPortuguese (BR)\n\nSpanish (ES)\n\nSpanish (MX)\n\nSpanish (US)\n\nspeak\n\nThe root element of an SSML document. When using SSML with the Alexa Skills Kit, surround the text to be spoken with the <speak> tag.\n\nsub\n\nPronounce the specified word or phrase as a different word or phrase. Specify the pronunciation to substitute with the alias attribute.\n\nAttribute Possible values\n\nThe following example replaces the abbreviated chemical elements with the full words.\n\nvoice\n\nSpeak the text with the specified Amazon Polly voice. Each listed voice has its own individual character. See Best Practices for Using Amazon Polly Voices for advice on how to use different voices in your skill to create a good user experience.\n\nYou can combine <voice> with all other tags, except for those listed in incompatible tags.\n\nAttribute Possible values\n\nSupported Amazon Polly voices\n\nThis table lists the Amazon Polly voices supported by Alexa. Voice names don't contain accented characters. Use a voice supported for the skill locale or use the voice with the <lang> tag.\n\nTo comply with Alexa skill policies, don't expose the Amazon-assigned name of an Amazon Polly voice to users.\n\nLocale Supported voices\n\nEnglish, American (en-US)\n\nIvy, Joanna, Joey, Justin, Kendra, Kimberly, Matthew, Salli\n\nEnglish, Australian (en-AU)\n\nNicole, Russell\n\nEnglish, British (en-GB)\n\nAmy, Brian, Emma\n\nEnglish, Indian (en-IN)\n\nAditi, Raveena\n\nEnglish, Welsh (en-GB-WLS)\n\nGeraint\n\nFrench, Canadian (fr-CA)\n\nChantal\n\nFrench, France (fr-FR)\n\nCeline, Lea, Mathieu\n\nGerman (de-DE)\n\nHans, Marlene, Vicki\n\nHindi (hi-IN)\n\nAditi\n\nItalian (it-IT)\n\nCarla, Giorgio, Bianca\n\nJapanese (ja-JP)\n\nMizuki, Takumi\n\nPortuguese, Brazilian (pt-BR)\n\nVitoria, Camila, Ricardo\n\nSpanish, American (es-US)\n\nPenelope, Lupe, Miguel\n\nSpanish, Castilian (es-ES)\n\nConchita, Enrique, Lucia\n\nSpanish, Mexican (es-MX)\n\nMia\n\nExample: Standard Alexa voice and a specified Amazon Polly voice\n\nIn this example, assume this sample is from an en-US skill, and because Kendra is an en-US voice, no <lang> tag is required. If this sample was from a skill that doesn't have an en-US locale, then the <lang> tag should be added and set to en-US.\n\nExample: Different voices in a dialog\n\nThe following example provides a dialog between an en-US voice and an en-GB voice, such as might occur if a story with two different characters were being read. The standard Alexa voice, which varies by locale, speaks the first and last sentence.\n\nExample: French content in an English skill\n\nIn the following example, assume the locale of this skill is for an English-speaking region. Because Celine is an fr-FR voice, and you want this content spoken in French, set the <lang> tag to fr-FR.\n\nTips for using Amazon Polly voices\n\nAlthough all Amazon Polly voices use approximately the same volume, users might perceive some voices as louder or quieter than Alexa voices. Use the prosody tag to modify the volume, rate, and pitch of the voice you have chosen. Use other SSML tags to modify the spoken output.\n\nYou can enhance your skills with responses that include one or more Amazon Polly voices, as well as the default Alexa voice, and you can choose specific voices for specific responses. Refer to User Experience Guidelines for the Use of Amazon Polly Voices in Your Skills for guidance on using Amazon Polly voices in your skills.\n\nThere is no charge for Alexa developers to use Amazon Polly voices.\n\nThe locale of a skill refers to a combination of region and language, and all of the Amazon Polly voices are tagged with a locale. For example, the en-AU locale refers to the English language in Australia, whereas en-IN refers to the English language in India. You select the locale of your skill when you first create it.\n\nTo achieve the best results, if the voice you select is for a different locale than that specified by your skill, use the <lang> tag to specify the language. See more about the lang tag.\n\nBe mindful of the user experience if you combine voices from different locales in your skill responses.\n\nNode.js sample code for voice\n\nIf building a Node.js skill, this switchVoice function can wrap speech output with <voice> tags to get a specific voice. If you use the Alexa Skills Kit SDK for Node.js, the SDK automatically wraps speech output in the <speak> tag.\n\nHere is some sample speech output from a skill using multiple voices with the switchVoice function.\n\nIf you want all of the skill responses to be in a particular voice, make sure that all speech outputs from the skill are specified as SSML and are wrapped with the appropriate <voice> tag.\n\nw\n\nSimilar to <say-as>, the <w> tag customizes the pronunciation of words by specifying the word's part of speech.\n\nAttribute Possible values\n\nrole\n\nSpecify the part of speech for the word.\n\nAccepted values:\n\namazon:VB: Interpret the word as a verb (present simple).\n\namazon:VBD: Interpret the word as a past participle.\n\namazon:NN: Interpret the word as a noun.\n\namazon:SENSE_1: Use the non-default sense of the word. For example, the noun \"bass\" is pronounced differently depending on meaning. The \"default\" meaning is the lowest part of the musical range. The alternate sense, which is still a noun, is a freshwater fish. Specifying <speak><w role=\"amazon:SENSE_1\">bass</w>\"</speak> renders the non-default pronunciation (freshwater fish).\n\nThe following example shows the amazon:VB and amazon:VBD values for role.\n\nThe follow example shows the amazon:SENSE_1 value for role.\n\nNote that these tags previously used the ivona namespace in the attribute names. The tags are backwards compatible, so existing SSML written with the ivona namespace continues to work.\n\nAlexa Skills Kit Sound Library\n\nBest Practices for Amazon Polly Voices\n\nSpeechcons\n\nWas this page helpful?"
    }
}
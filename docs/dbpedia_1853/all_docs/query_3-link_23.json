{
    "id": "dbpedia_1853_3",
    "rank": 23,
    "data": {
        "url": "https://arxiv.org/html/2404.03635v2",
        "read_more_link": "",
        "language": "en",
        "title": "WorDepth: Variational Language Prior for Monocular Depth Estimation",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://arxiv.org/html/x1.png",
            "https://arxiv.org/html/x2.png",
            "https://arxiv.org/html/x3.png",
            "https://arxiv.org/html/x4.png",
            "https://arxiv.org/html/x5.png",
            "https://arxiv.org/html/x6.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "HTML conversions sometimes display errors due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.\n\nfailed: epic\n\nAuthors: achieve the best HTML results from your LaTeX submissions by following these best practices.\n\nLicense: CC BY 4.0\n\narXiv:2404.03635v2 [cs.CV] 05 Apr 2024\n\nWorDepth: Variational Language Prior for Monocular Depth Estimation\n\nZiyao Zeng1 Daniel Wang1 Fengyu Yang1 Hyoungseob Park1 Yangchao Wu2\n\nStefano Soatto2 Byung-Woo Hong3 Dong Lao2 Alex Wong1\n\n1Yale University 2University of California, Los Angeles Chung-Ang University3\n\n1{ziyao.zeng, daniel.wang.dhw33, fengyu.yang, hyoungseob.park, alex.wong}@yale.edu\n\n2 wuyangchao1997@g.ucla.edu 2{soatto,lao}@cs.ucla.edu 3hong@cau.ac.kr\n\nAbstract\n\nThree-dimensional (3D) reconstruction from a single image is an ill-posed problem with inherent ambiguities, i.e. scale. Predicting a 3D scene from text description(s) is similarly ill-posed, i.e. spatial arrangements of objects described. We investigate the question of whether two inherently ambiguous modalities can be used in conjunction to produce metric-scaled reconstructions. To test this, we focus on monocular depth estimation, the problem of predicting a dense depth map from a single image, but with an additional text caption describing the scene. To this end, we begin by encoding the text caption as a mean and standard deviation; using a variational framework, we learn the distribution of the plausible metric reconstructions of 3D scenes corresponding to the text captions as a prior. To “select” a specific reconstruction or depth map, we encode the given image through a conditional sampler that samples from the latent space of the variational text encoder, which is then decoded to the output depth map. Our approach is trained alternatingly between the text and image branches: in one optimization step, we predict the mean and standard deviation from the text description and sample from a standard Gaussian, and in the other, we sample using a (image) conditional sampler. Once trained, we directly predict depth from the encoded text using the conditional sampler. We demonstrate our approach on indoor (NYUv2) and outdoor (KITTI) scenarios, where we show that language can consistently improve performance in both. Code: https://github.com/Adonis-galaxy/WorDepth.\n\n1 Introduction\n\nThe process of imaging is a surjection from a 3D scene to the 2D image domain, where infinitely many 3D scenes can map to the same image. Its inverse problem, estimating the 3D scene structure from a single image, i.e., monocular depth estimation, is therefore ill-posed with inherent ambiguity, such as the scale of the reconstruction. Consequently, induction is necessary, and depth estimation becomes drawing a scene with maximum likelihood from the distribution of all possible scenes, conditioned on the image. This conditional scene distribution is learned by a deep neural network on a chosen training set. While an ideal training set should accurately reflect this distribution, practical challenges arise due to the scarcity of well-established large-scale depth datasets. A crucial question arises: Can any priors, other than the training set, be leveraged to calibrate the learned scene distribution to true real-world statistics?\n\nThese priors may come in many forms, from generic priors such as local smoothness and connectivity [19, 22, 102, 67] or object orientation [15] that may be imposed as a part of the training objective (regularizer) to specific inductive biases realized as architectural designs (layers) [65] or a collection object shapes [14]. While generic priors are suitable for a wide variety of scenes, they typically lack specificity, i.e., size or shape of objects within a specific 3D scene. On the other hand, specific network designs may backfire when the assumption motivating the design does not hold, i.e., using specifics about camera parameters for reconstruction. We consider a more flexible source of priors – language – that is closely tied to semantics, and often shape (and functionality) [31, 32, 4]. Consider a text description of “A bedroom with a bed and a table” as in Fig. 1: One can imagine a probable 3D scene containing a bed and a table as the primary objects. In fact, there exist infinitely many 3D scenes compatible with the description, as there are ambiguities in terms of the scene layout and the precise shape of the bed and table. Yet, one may surmise that the scale of the scene is closely related to the objects (and their typical sizes) populating it. This lends to a prior that is specific for a given scene, yet, generic enough without assumptions on the camera used or the shapes within the imaged 3D scene.\n\nHence, the question at hand becomes whether two inherently ambiguous modalities (camera image and text descriptions) can be exploited for their complementary strengths: In the image, one can observe the layout and object shapes populating the 3D scene; in a text caption, one has strong priors about the scale (and coarse shapes) of the scene. Our work aims to resolve the respective ambiguities of the two modalities by using language to reduce the solution space to yield metric-scaled reconstructions as 2.5D depth maps.\n\nTo test the feasibility of this approach, we consider the ill-posed inverse problem of monocular depth estimation, where one predicts a depth map from a single image. Instead of using just an image, we also assume a text description or caption describing the 3D scene captured within the image. Note that we do not make any assumption regarding the source of the description, i.e., it can be dictated by humans or generated by a model. But for practicality, we use an image captioner (ExpansionNet v2 [25]) to generate a brief, concise description of the image.\n\nTo exploit the inherent ambiguity of text captions, where a single description can generate infinitely many 3D scenes, we choose to encode the caption using a variational auto-encoder (VAE) as a mean and standard deviation of the plausible scene layout distribution. By sampling a noise vector from a standard Gaussian and using the reparameterization trick customary in VAEs, we can draw from the latent distribution and decode it into a metric-scaled depth map. Yet, to choose a particular depth map amongst the many possible, one must rely on the image. This is facilitated by a conditional sampler that predicts the noise vector from the given image in place of the one sampled from a Gaussian to be used in the reparameterization step. Consequently, this substitution enables one to sample the most probable depth map, adhering to the scene arrangement and object shapes observed in the image, from the learned distribution. This naturally lends to an alternating optimization process between the (text-)VAE and conditional sampler.\n\nIn one alternation, one would predict the mean and standard deviation from the text caption and optimize the text-VAE branch for depth by minimizing a loss with respect to ground truth on the depth map sampled using a standard Gaussian (similar to traditional VAEs). In the other alternation, one would still use the mean and standard deviation predicted by the text-VAE, but instead, use the conditional sampler to “select” a specific depth map compatible with the image, and again, minimize a loss on the output depth. Note: that depending on the alternation, either the text-VAE or the conditional sampler is frozen. At test-time, one no longer needs to sample from the Gaussian and may directly predict depth using the text-VAE with the conditional sampler (see Fig. 2). In another mode, one may use the text-VAE alone to generate plausible scenes for a given caption.\n\nOur contributions are as follows: (i) We propose a variational framework that leverages complementary strengths of two inherently ambiguous modalities for monocular depth estimation; we term our approach, WorDepth. (ii) We introduce an image-based conditional sampler that models the use of language as a conditional prior. (iii) We achieve the state-of-the-art on indoor (NYU Depth V2 [58]) and outdoor (KITTI [20]) benchmarks. (iv) To the best of our knowledge, we are the first to treat language as a variational prior for monocular depth estimation.\n\n2 Related Work\n\nMonocular depth estimation trains by minimizing loss between depth predictions and ground-truth depth maps [7, 2, 17, 52, 80, 78, 46, 35, 54, 61, 86, 84, 66]. Specifically, DORN [16] employs a spacing-increasing discretization strategy for depth estimation as an ordinal regression problem. AdaBins [2] introduces a transformer block that segments the depth range into adaptive bins. ASTransformer [7] incorporates an Attention-based Up-sample Block to enhance detailed texture features. DepthFormer [40] employs hierarchical aggregation and heterogeneous interaction modules for effective feature affinity and modeling. RPSF [47] presents a differentiable model of the aperture mask. However, deriving semantics solely from visual cues is challenging because of scale ambiguity and the limited size of fully annotated training datasets. We use language as a prior to ground predictions to metric scale. When ground-truth depth is not available, self-supervised approaches [70, 27, 36, 85, 3, 96, 100, 63, 62, 51, 64, 15, 94] rely on geometric constraints, often established via from various modalities, including lidar [79, 67, 69, 68, 44, 50, 72] and radar [59], or through deliberate design. Arising from training, if done at a large scale, is a prior on the scene that can be exploited for semantic tasks [33]. On the other hand, we consider language as a semantic prior to enhance the effectiveness of monocular depth estimation.\n\nVariational and generative methods focus on the ambiguous nature of monocular depth estimation, many involving Diffusion or VAE models for modeling this ambiguity [10, 57, 56, 73, 5, 83, 41]. DepthGen [56] uses a depth pre-trained diffusion model, which generates depth estimations conditioned on images, and shows that the model is capable of generating multiple plausible depth maps when depth is ambiguous. DDVM [57] uses a similar approach and designed a training pipeline that can produce both depth maps and optical flow outputs with a diffusion model. [73] trained a VAE model that outputs a probability distribution over scene depth given an image, which can then be combined with additional inputs for more accurate depth estimations. VDN [10] models depth as a distribution with its variance interpreted as uncertainty. The CodeSLAM model [5] also employed a VAE conditioned on image intensities for depth estimation. However, although these work explored the idea of uncertainty in depth estimation, and even combined other modalities of inputs [73], none have experimented with language priors, and most VAE-based approaches use images to obtain the mean of the modeled distribution, which is fundamentally different from WorDepth.\n\nFoundation models [53, 37, 38, 6, 48, 21, 104, 23, 77, 49, 98] acquire a comprehensive understanding of languages, images, and other data types through pre-training under substantial and diverse datasets, thus forming an effective baseline for downstream tasks [75, 76, 39, 12, 89, 74, 8, 81, 71, 2, 82, 42, 95, 92, 93]. To leverage foundation models for monocular depth estimation, TADP [30] uses captions created by AI to enhance the correlation between text and images in diffusion-based vision models. VPD [97] leverages a diffusion-based pipeline with cross-attention between text and images. Dinov2 [48] trains a ViT [11] with 1B parameters using an automatically built image dataset under contrastive learning objectives. Unlike methods that rely on foundation models for feature extraction, WorDepth is potentially more efficient for industrial applications.\n\nVision-language models are designed to build connections between visual and language inputs. CLIP [53] conducts contrastive learning between text-image pairs, empowering various tasks like few-shot image classification [18, 88, 87, 101], image segmentation [99, 55], object detection [103, 55], and 3D perception [90, 105, 91, 26]. In light of the powerful emerging ability brought by recent vision-language models, some works have tried to apply the vision-language model for monocular depth estimation. DepthCLIP [91] leverages the semantic depth response of CLIP [53] with a depth projection scheme to conduct zero-shot adaptation from the semantic language response to monocular depth estimation. Furthermore, [26] extends DepthCLIP with learnable prompts and depth codebook to narrow the depth domain gap among different scenes. Likewise, [1] modifies DepthCLIP [91] using continuous learnable tokens in place of discrete human-language words. Additionally, VPD [97] exploits the high-fidelity embedding of a pre-trained text-to-image diffusion model in monocular depth estimation. However, existing methods using vision-language models rely on implicit modeling. Conversely, WorDepth explicitly models language as a prior for depth estimation and exploits strong priors regarding the size of objects described in text captions to better ground monocular depth (often scaleless) to metric scale.\n\n3 Method\n\nGiven an RGB image x:Ω⊂ℝ2→ℝ3:𝑥Ωsuperscriptℝ2→superscriptℝ3x:\\Omega\\subset\\mathbb{R}^{2}\\rightarrow\\mathbb{R}^{3}italic_x : roman_Ω ⊂ blackboard_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT → blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT, monocular depth estimation aims to infer a dense depth map y:Ω⊂ℝ2→ℝ+:𝑦Ωsuperscriptℝ2→subscriptℝy:\\Omega\\subset\\mathbb{R}^{2}\\rightarrow\\mathbb{R}_{+}italic_y : roman_Ω ⊂ blackboard_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT → blackboard_R start_POSTSUBSCRIPT + end_POSTSUBSCRIPT using a parameterized function hℎhitalic_h realized as a neural network, i.e., y:=h⁢(⋅)assign𝑦ℎ⋅y:=h(\\cdot)italic_y := italic_h ( ⋅ ). We consider a supervised dataset 𝒟={x(m),𝐭(m),y*(m)}m=1M𝒟superscriptsubscriptsuperscript𝑥𝑚superscript𝐭𝑚superscript𝑦absent𝑚𝑚1𝑀\\mathcal{D}=\\{x^{(m)},\\textbf{t}^{(m)},y^{*(m)}\\}_{m=1}^{M}caligraphic_D = { italic_x start_POSTSUPERSCRIPT ( italic_m ) end_POSTSUPERSCRIPT , t start_POSTSUPERSCRIPT ( italic_m ) end_POSTSUPERSCRIPT , italic_y start_POSTSUPERSCRIPT * ( italic_m ) end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_m = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT with M𝑀Mitalic_M samples, where y*:Ω⊂ℝ2→ℝ+:superscript𝑦Ωsuperscriptℝ2→subscriptℝy^{*}:\\Omega\\subset\\mathbb{R}^{2}\\rightarrow\\mathbb{R}_{+}italic_y start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT : roman_Ω ⊂ blackboard_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT → blackboard_R start_POSTSUBSCRIPT + end_POSTSUBSCRIPT denotes the ground-truth depth map, and t the text caption describing the image.\n\n3.1 Text variational auto-encoder\n\nTo incorporate language priors to monocular depth estimation, we first design a variational auto-encoder (VAE) to learn the latent distribution of possible depth maps as described by the text caption. This VAE is comprised of the text encoder from a pre-trained vision-language model, CLIP [53], which by default offers a shared latent space between vision and text embeddings, followed by a multi-layer perceptron (MLP) to estimate the mean μ^∈ℝd^𝜇superscriptℝ𝑑\\hat{\\mu}\\in\\mathbb{R}^{d}over^ start_ARG italic_μ end_ARG ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT and standard deviation σ^∈ℝd^𝜎superscriptℝ𝑑\\hat{\\sigma}\\in\\mathbb{R}^{d}over^ start_ARG italic_σ end_ARG ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT of the latent distribution of plausible scenes based on the text encoding. Note that the CLIP text encoder is frozen at all times and never updated when training WorDepth. Specifically, given a text caption 𝐭={t1,t2,…}𝐭subscript𝑡1subscript𝑡2…\\textbf{t}=\\{t_{1},t_{2},...\\}t = { italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … }, we first encode it using the CLIP text encoder and estimate the mean and standard deviation as (μ^,σ^)=gψ⁢(𝐭)∈ℝ2×d^𝜇^𝜎subscript𝑔𝜓𝐭superscriptℝ2𝑑(\\hat{\\mu},\\hat{\\sigma})=g_{\\psi}(\\textbf{t})\\in\\mathbb{R}^{2\\times d}( over^ start_ARG italic_μ end_ARG , over^ start_ARG italic_σ end_ARG ) = italic_g start_POSTSUBSCRIPT italic_ψ end_POSTSUBSCRIPT ( t ) ∈ blackboard_R start_POSTSUPERSCRIPT 2 × italic_d end_POSTSUPERSCRIPT using a multi-layer perceptron (MLP). To sample from the distribution parameterized by μ^^𝜇\\hat{\\mu}over^ start_ARG italic_μ end_ARG and σ^^𝜎\\hat{\\sigma}over^ start_ARG italic_σ end_ARG, we first draw a noise vector ϵ∈ℝditalic-ϵsuperscriptℝ𝑑\\epsilon\\in\\mathbb{R}^{d}italic_ϵ ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT from a standard Gaussian ϵ∼𝒩⁢(0,1)similar-toitalic-ϵ𝒩01\\epsilon\\sim\\mathcal{N}(0,1)italic_ϵ ∼ caligraphic_N ( 0 , 1 ). Then, we use ϵitalic-ϵ\\epsilonitalic_ϵ to sample from the latent distribution via the reparameterization trick [29], z^=μ^+ϵ⋅σ^^𝑧^𝜇⋅italic-ϵ^𝜎\\hat{z}=\\hat{\\mu}+\\epsilon\\cdot\\hat{\\sigma}over^ start_ARG italic_z end_ARG = over^ start_ARG italic_μ end_ARG + italic_ϵ ⋅ over^ start_ARG italic_σ end_ARG. We refer to this module as a text variational auto-encoder (text-VAE). To generate a depth map y^^𝑦\\hat{y}over^ start_ARG italic_y end_ARG from the sample z^∈ℝd^𝑧superscriptℝ𝑑\\hat{z}\\in\\mathbb{R}^{d}over^ start_ARG italic_z end_ARG ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT, we first duplicate z^^𝑧\\hat{z}over^ start_ARG italic_z end_ARG along the horizontal and vertical axes to yield a d×h×w𝑑ℎ𝑤d\\times h\\times witalic_d × italic_h × italic_w latent (choice of design to be discussed below in Sec. 3.2) and feed it through a depth decoder to yield y^=hϕ⁢(z^)∈ℝ+H×W^𝑦subscriptℎitalic-ϕ^𝑧superscriptsubscriptℝ𝐻𝑊\\hat{y}=h_{\\phi}(\\hat{z})\\in\\mathbb{R}_{+}^{H\\times W}over^ start_ARG italic_y end_ARG = italic_h start_POSTSUBSCRIPT italic_ϕ end_POSTSUBSCRIPT ( over^ start_ARG italic_z end_ARG ) ∈ blackboard_R start_POSTSUBSCRIPT + end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_H × italic_W end_POSTSUPERSCRIPT, where we overload z^^𝑧\\hat{z}over^ start_ARG italic_z end_ARG as the spatially duplicated latent, and H𝐻Hitalic_H and W𝑊Witalic_W denote the height and width of the depth map, preset as hyperparameters to match the desired image dimensions.\n\nTo train our text-VAE and depth decoder, we minimize\n\nℒVAE=ℒSI⁢(y*,y^)+α⋅ℒKL⁢(μ^,σ^)subscriptℒVAEsubscriptℒSIsuperscript𝑦^𝑦⋅𝛼subscriptℒKL^𝜇^𝜎\\displaystyle\\mathcal{L}_{\\text{VAE}}=\\mathcal{L}_{\\text{SI}}(y^{*},\\hat{y})+% \\alpha\\cdot\\mathcal{L}_{\\text{KL}}(\\hat{\\mu},\\hat{\\sigma})caligraphic_L start_POSTSUBSCRIPT VAE end_POSTSUBSCRIPT = caligraphic_L start_POSTSUBSCRIPT SI end_POSTSUBSCRIPT ( italic_y start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT , over^ start_ARG italic_y end_ARG ) + italic_α ⋅ caligraphic_L start_POSTSUBSCRIPT KL end_POSTSUBSCRIPT ( over^ start_ARG italic_μ end_ARG , over^ start_ARG italic_σ end_ARG ) (1)\n\nwith respect to ψ𝜓\\psiitalic_ψ and ϕitalic-ϕ\\phiitalic_ϕ, where ℒSIsubscriptℒSI\\mathcal{L}_{\\text{SI}}caligraphic_L start_POSTSUBSCRIPT SI end_POSTSUBSCRIPT is the scale invariant loss (Eq. 3), ℒKLsubscriptℒKL\\mathcal{L}_{\\text{KL}}caligraphic_L start_POSTSUBSCRIPT KL end_POSTSUBSCRIPT the KL divergence loss (Eq. 4) as detailed in Section 3.3, and α𝛼\\alphaitalic_α the weight of the KL divergence term.\n\n3.2 Image-based conditional sampler\n\nWhile our text-VAE can predict plausible metric-scaled depth maps from text captions, we are interested in the depth map corresponding to a specific image. To do so, we treat text-VAE as the latent prior distribution of the plausible scene layouts. Predicting depth y~~𝑦\\tilde{y}over~ start_ARG italic_y end_ARG for a specific image x𝑥xitalic_x requires sampling the latent corresponding to the depth map of the 3D scene layout with the highest likelihood to be compatible with the observed image, i.e., prior conditioned on the image. To this end, we introduce an image-based conditional sampler that will predict the sample ϵ~~italic-ϵ\\tilde{\\epsilon}over~ start_ARG italic_ϵ end_ARG in place of ϵ∼𝒩⁢(0,1)similar-toitalic-ϵ𝒩01\\epsilon\\sim\\mathcal{N}(0,1)italic_ϵ ∼ caligraphic_N ( 0 , 1 ) drawn from the standard Gaussian. Using the reparameterization trick as before, we will use ϵ~~italic-ϵ\\tilde{\\epsilon}over~ start_ARG italic_ϵ end_ARG to select the latent vector z~~𝑧\\tilde{z}over~ start_ARG italic_z end_ARG to be decoded by the depth decoder.\n\nSpecifically, our image-based conditional sampler utilizes a Swin-L transformer backbone to encode an image x∈ℝ3×H×W𝑥superscriptℝ3𝐻𝑊x\\in\\mathbb{R}^{3\\times H\\times W}italic_x ∈ blackboard_R start_POSTSUPERSCRIPT 3 × italic_H × italic_W end_POSTSUPERSCRIPT. We chose this design to exploit the locality of the tokens produced by Swin-L. The tokens are then encoded into h×wℎ𝑤h\\times witalic_h × italic_w number of local samples ϵ~∈ℝd×h×w~italic-ϵsuperscriptℝ𝑑ℎ𝑤\\tilde{\\epsilon}\\in\\mathbb{R}^{d\\times h\\times w}over~ start_ARG italic_ϵ end_ARG ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_h × italic_w end_POSTSUPERSCRIPT to be used to sample from the latent distribution of our text-VAE; in other words, we perform “patch-wise” selection from latent distribution for more granular predictions. To do so, we additionally include μ^^𝜇\\hat{\\mu}over^ start_ARG italic_μ end_ARG and σ^^𝜎\\hat{\\sigma}over^ start_ARG italic_σ end_ARG as part of its input. We note that μ^^𝜇\\hat{\\mu}over^ start_ARG italic_μ end_ARG and σ^^𝜎\\hat{\\sigma}over^ start_ARG italic_σ end_ARG have been detached from the computational graph and treated as input. We refer to this module as our conditional sampler ϵ~=fφ⁢(x,μ^,σ^)~italic-ϵsubscript𝑓𝜑𝑥^𝜇^𝜎\\tilde{\\epsilon}=f_{\\varphi}(x,\\hat{\\mu},\\hat{\\sigma})over~ start_ARG italic_ϵ end_ARG = italic_f start_POSTSUBSCRIPT italic_φ end_POSTSUBSCRIPT ( italic_x , over^ start_ARG italic_μ end_ARG , over^ start_ARG italic_σ end_ARG ), which aims to estimate the most probable latent variable of text-VAE. Thus, the scene layout latent vector is now given by z~=μ^+ϵ~⋅σ^~𝑧^𝜇⋅~italic-ϵ^𝜎\\tilde{z}=\\hat{\\mu}+\\tilde{\\epsilon}\\cdot\\hat{\\sigma}over~ start_ARG italic_z end_ARG = over^ start_ARG italic_μ end_ARG + over~ start_ARG italic_ϵ end_ARG ⋅ over^ start_ARG italic_σ end_ARG, and the predicted depth y~=hϕ⁢(z~)~𝑦subscriptℎitalic-ϕ~𝑧\\tilde{y}=h_{\\phi}(\\tilde{z})over~ start_ARG italic_y end_ARG = italic_h start_POSTSUBSCRIPT italic_ϕ end_POSTSUBSCRIPT ( over~ start_ARG italic_z end_ARG ). As an implementation detail, we note that skip connections from the encoder fφsubscript𝑓𝜑f_{\\varphi}italic_f start_POSTSUBSCRIPT italic_φ end_POSTSUBSCRIPT are injected into hϕsubscriptℎitalic-ϕh_{\\phi}italic_h start_POSTSUBSCRIPT italic_ϕ end_POSTSUBSCRIPT by concatenation; when training text-VAE (Sec. 3.1), feature maps of skip connections are of the same size, but populated with zeros instead.\n\nTo train the conditional sampler, we minimize the same loss (Eq. 1) as that of text-VAE:\n\nℒCS=ℒSI⁢(y*,y~)+β⋅ℒKL⁢(μ~,σ~)subscriptℒCSsubscriptℒSIsuperscript𝑦~𝑦⋅𝛽subscriptℒKL~𝜇~𝜎\\displaystyle\\mathcal{L}_{\\text{CS}}=\\mathcal{L}_{\\text{SI}}(y^{*},\\tilde{y})+% \\beta\\cdot\\mathcal{L}_{\\text{KL}}(\\tilde{\\mu},\\tilde{\\sigma})caligraphic_L start_POSTSUBSCRIPT CS end_POSTSUBSCRIPT = caligraphic_L start_POSTSUBSCRIPT SI end_POSTSUBSCRIPT ( italic_y start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT , over~ start_ARG italic_y end_ARG ) + italic_β ⋅ caligraphic_L start_POSTSUBSCRIPT KL end_POSTSUBSCRIPT ( over~ start_ARG italic_μ end_ARG , over~ start_ARG italic_σ end_ARG ) (2)\n\nwith respect to φ𝜑\\varphiitalic_φ and ϕitalic-ϕ\\phiitalic_ϕ. With a batch size of b𝑏bitalic_b, the number of ϵ~~italic-ϵ\\tilde{\\epsilon}over~ start_ARG italic_ϵ end_ARG is b×h×w𝑏ℎ𝑤b\\times h\\times witalic_b × italic_h × italic_w, while μ~~𝜇\\tilde{\\mu}over~ start_ARG italic_μ end_ARG and σ~~𝜎\\tilde{\\sigma}over~ start_ARG italic_σ end_ARG are the sample mean and standard deviation of ϵ~~italic-ϵ\\tilde{\\epsilon}over~ start_ARG italic_ϵ end_ARG over a batch. We impose a KL divergence loss as regularization so that the estimated ϵ~~italic-ϵ\\tilde{\\epsilon}over~ start_ARG italic_ϵ end_ARG does not drift from the standard Gaussian, which also serves to improve training stability.\n\n3.3 Training Loss\n\nScale invariant loss. We minimize a supervised loss using ground truth y*superscript𝑦y^{*}italic_y start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT. To improve training stability over diverse scenes, we use the scale-invariant depth loss [13]:\n\nℒSI⁢(y,y*)=1Ne⁢∑(i,j)∈Ωe⁢(i,j)2−γNe2⁢(∑(i,j)∈Ωe⁢(i,j))2,subscriptℒSI𝑦superscript𝑦1subscript𝑁𝑒subscript𝑖𝑗Ω𝑒superscript𝑖𝑗2𝛾superscriptsubscript𝑁𝑒2superscriptsubscript𝑖𝑗Ω𝑒𝑖𝑗2\\mathcal{L}_{\\text{SI}}(y,y^{*})=\\frac{1}{N_{e}}\\sum_{(i,j)\\in\\Omega}e(i,j)^{2% }-\\frac{\\gamma}{N_{e}^{2}}(\\sum_{(i,j)\\in\\Omega}e(i,j))^{2},caligraphic_L start_POSTSUBSCRIPT SI end_POSTSUBSCRIPT ( italic_y , italic_y start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ) = divide start_ARG 1 end_ARG start_ARG italic_N start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT end_ARG ∑ start_POSTSUBSCRIPT ( italic_i , italic_j ) ∈ roman_Ω end_POSTSUBSCRIPT italic_e ( italic_i , italic_j ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT - divide start_ARG italic_γ end_ARG start_ARG italic_N start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ( ∑ start_POSTSUBSCRIPT ( italic_i , italic_j ) ∈ roman_Ω end_POSTSUBSCRIPT italic_e ( italic_i , italic_j ) ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , (3)\n\nwhere e⁢(i,j)=log⁡y⁢(i,j)−log⁡y*⁢(i,j)𝑒𝑖𝑗𝑦𝑖𝑗superscript𝑦𝑖𝑗e(i,j)=\\log y(i,j)-\\log y^{*}(i,j)italic_e ( italic_i , italic_j ) = roman_log italic_y ( italic_i , italic_j ) - roman_log italic_y start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ( italic_i , italic_j ), ΩΩ\\Omegaroman_Ω denotes the image space, Nesubscript𝑁𝑒N_{e}italic_N start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT the number of pixels, y𝑦yitalic_y the predicted depth, and γ𝛾\\gammaitalic_γ the scaling factor to control the sensitivity of the loss.\n\nKullback-Leibler (KL) divergence loss. Following [29], we employ the KL Divergence loss as a regularizer, which biases the predicted latent distribution (parameterized by mean μ𝜇\\muitalic_μ and standard deviation σ𝜎\\sigmaitalic_σ) towards a standard Gaussian distribution. We apply the Kullback-Leibler divergence loss to μ𝜇\\muitalic_μ and σ𝜎\\sigmaitalic_σ as follows:\n\nℒKL⁢(μ,σ)=−log⁡(σ)+σ2+μ22−12.subscriptℒKL𝜇𝜎𝜎superscript𝜎2superscript𝜇2212\\mathcal{L}_{\\text{KL}}(\\mu,\\sigma)=-\\log(\\sigma)+\\frac{\\sigma^{2}+\\mu^{2}}{2}% -\\frac{1}{2}.caligraphic_L start_POSTSUBSCRIPT KL end_POSTSUBSCRIPT ( italic_μ , italic_σ ) = - roman_log ( italic_σ ) + divide start_ARG italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_μ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG 2 end_ARG - divide start_ARG 1 end_ARG start_ARG 2 end_ARG . (4)\n\n3.4 Optimizing Wordepth\n\nTraining Wordepth involves optimizing text-VAE with our conditional sampler: One may choose to first train text-VAE until convergence (i.e., optimize for ψ*,ϕ*superscript𝜓superscriptitalic-ϕ\\psi^{*},\\phi^{*}italic_ψ start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT , italic_ϕ start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT), then freeze ψ*,ϕ*superscript𝜓superscriptitalic-ϕ\\psi^{*},\\phi^{*}italic_ψ start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT , italic_ϕ start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT, and finally train the image-based conditional sample (i.e., optimize for φ*superscript𝜑\\varphi^{*}italic_φ start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT). However, we find that doing so often results in the conditional sampler being trapped in a suboptimal local minimum. Moreover, this introduces the inconvenience of an extra stage of training. Instead, we propose an alternating optimization scheme to train the text-VAE with conditional sampler. In one alternating step, we freeze the conditional sampler and train the text-VAE and depth decoder following the procedure in Sec. 3.1, i.e., predicting μ^^𝜇\\hat{\\mu}over^ start_ARG italic_μ end_ARG and σ^^𝜎\\hat{\\sigma}over^ start_ARG italic_σ end_ARG from text caption t and using the reparameterization trick with an ϵitalic-ϵ\\epsilonitalic_ϵ drawn from a standard Gaussian to sample the latent vector. In the next alternating step, we freeze text-VAE and train the conditional sampler with the depth decoder following Sec. 3.2, i.e., predicting μ^^𝜇\\hat{\\mu}over^ start_ARG italic_μ end_ARG and σ^^𝜎\\hat{\\sigma}over^ start_ARG italic_σ end_ARG using the frozen text-VAE and sample from the latent distribution using ϵ~~italic-ϵ\\tilde{\\epsilon}over~ start_ARG italic_ϵ end_ARG predicted from the image. These alternating steps are repeated with a ratio of p𝑝pitalic_p (for optimizing text-VAE) to 1−p1𝑝1-p1 - italic_p (for optimizing the conditional sampler).\n\nInference. Once trained, we no longer require drawing ϵitalic-ϵ\\epsilonitalic_ϵ from a standard Gaussian. Instead, at test time, the inference step simply follows Sec. 3.2. In another mode, if one wants to generate depth maps from text captions, one can discard the conditional sampler branch and directly sample from a standard Gaussian instead.\n\n4 Experiments\n\nDatasets. We evaluate our method on indoor (NYU Depth V2 [58]) and outdoor (KITTI [20]) scenarios. NYU Depth V2 contains 480×\\times×640 images with depth values from 1×10−31superscript1031\\times 10^{-3}1 × 10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT to 10 meters. We follow [34] for the dataset partition, which contains 24,231 train images and 654 test images. KITTI contains 352×\\times×1216 images where depth values from 1×10−31superscript1031\\times 10^{-3}1 × 10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT to 80 meters. We adopt the Eigen Split [13] consisting of 23,488 training images and 697 testing images. Following [86, 2], after cleaning out samples without valid ground truth, we have 652 valid images for testing.\n\nNetwork Architecture. We use the ResNet-50 [24] version of CLIP [53] text encoder to extract text features. We use ExpansionNet-v2 [25] for captioning images for efficiency. We set the dimension d𝑑ditalic_d of the latent space of the text-VAE and image-based conditional sampler to be 128. As for the image-based conditional sampler, we use a Swin-L Transformer backbone [45] pre-trained on ImageNet [9]. For the text-VAE, given CLIP features of size 1024, we use a 3-layer MLP with hidden dimensions of 512, 256, and 128 to encode text features. For the depth decoder, there are 3 convolutional up-sampling and refinement layers. For depth prediction, we attach 3 skip connections from the conditional sampler to the depth decoder between corresponding layers. When optimizing for text-VAE by our alternating optimization scheme (Sec. 3.4), we sample ϵ∼𝒩⁢(0,1)similar-toitalic-ϵ𝒩01\\epsilon\\sim\\mathcal{N}(0,1)italic_ϵ ∼ caligraphic_N ( 0 , 1 ) from a standard Gaussian; as an implementation detail, all values passed from the skip connections are set to be zero.\n\nHyperparameters. We use the Adam [28] optimizer without weight decay. The learning rate is reduced from 3×10−53superscript1053\\times 10^{-5}3 × 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT to 1×10−51superscript1051\\times 10^{-5}1 × 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT by a cosine learning rate scheduler. The model is trained for 50 epochs for both KITTI and NYU Depth V2 under this scheduler. γ𝛾\\gammaitalic_γ for scale-invariant loss is set to 0.85, and the weights α𝛼\\alphaitalic_α and β𝛽\\betaitalic_β for KL-Divergence are set to 1×10−31superscript1031\\times 10^{-3}1 × 10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT. We set the probability p𝑝pitalic_p to optimizing text-VAE branch to 1%. Data augmentation includes random gamma within [0.9,1.1]0.91.1[0.9,1.1][ 0.9 , 1.1 ], random brightness within [0.75,1.25]0.751.25[0.75,1.25][ 0.75 , 1.25 ] for NYU Depth V2 [58] and [0.9,1.1]0.91.1[0.9,1.1][ 0.9 , 1.1 ] for KITTI [20], random color intensity within [0.9,1.1]0.91.1[0.9,1.1][ 0.9 , 1.1 ] for each channel, random horizontal flipping with 50% probability, and random rotations within [−2.5,2.5]2.52.5[-2.5,2.5][ - 2.5 , 2.5 ] degrees.\n\nEvaluation metrics. Following [43, 7], we evaluate WorDepth and baseline methods quantitatively using mean absolute relative error (Abs Rel), root mean square error (RMSE), absolute error in log space (log10)subscript10\\left(\\log_{10}\\right)( roman_log start_POSTSUBSCRIPT 10 end_POSTSUBSCRIPT ), logarithmic root mean square error (RMSElogsubscriptRMSE\\text{RMSE}_{\\log}RMSE start_POSTSUBSCRIPT roman_log end_POSTSUBSCRIPT) and threshold accuracy (δi)subscript𝛿𝑖\\left(\\delta_{i}\\right)( italic_δ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ). The evaluation metrics are summarized in the Supp. Mat. For qualitative results and comparisons, see Fig. 3 and 4, where the error map shows the absolute relative error.\n\nQuantitative results. We show results on NYU Depth V2 in Tab. 1, where we improve over the baseline and existing works across all evaluation metrics. We want to highlight that WorDepth significantly excels in terms of the threshold accuracy δ<1.25𝛿1.25\\delta<1.25italic_δ < 1.25, which measures the proportion of predictions deviating from the ground truth within a specific range. We note that while existing methods often produce high fidelity shapes (i.e., ordinal relationships of points) in depth maps, the scale tends to be off – leading to lower δ<1.25𝛿1.25\\delta<1.25italic_δ < 1.25. Our gain in the δ<1.25𝛿1.25\\delta<1.25italic_δ < 1.25 metric indicates that a greater proportion of depth estimations align closely with the ground truth, thanks to better scale estimated based on objects that populate the scene, thereby yielding depth values in ranges closer to that of ground truth.\n\nTab. 2 shows the results on the KITTI dataset, using the Eigen Split [13] partition. WorDepth also achieves state-of-the-art performance. Like NYU Depth V2, WorDepth improves the threshold accuracy δ<1.25𝛿1.25\\delta<1.25italic_δ < 1.25, however, the relative performance gain on this metric is not as pronounced as on NYU Depth V2. This difference can be due to the wider range of object sizes and shapes that may populate an outdoor scene that are attributed to the same equivalence class of an object. For example, the term “car” may refer to a sedan, a coupe, or a hatchback – all exhibit different sizes (coupes are smaller than sedans) and shapes (hatchbacks have an elevated and connect trunk). While text captions give flexibility between generality and specificity as a prior, in cases where captions tend to be vague, the explicit reliance (by modeling as a conditional prior) on them may backfire, leading to incorrect shapes and sizes. Nonetheless, conditioning on the image resolve such cases to a degree and usage of the prior leads to more benefits than harm.\n\nQualitative comparisons. We show representative visual examples comparing WorDepth with a baseline method AdaBins [2] on the NYU Depth V2 dataset in Fig. 3, to highlight the benefit of the language prior.\n\nFrom the error map where brighter regions indicate larger errors, it is evident that WorDepth predicts more accurate depth for objects mentioned in the text description, like “chairs and tables” in the first row, “a window” in the second row, “a shower curtain” in the third row, “a man” in the fourth row, and “a group of desks” in the last row. Note that errors maps of WorDepth shows improvement uniformly across the image regions; this implies that our method estimates a better scale than existing ones, thanks to priors about object size (and coarse shapes) from text captions. Knowing that a certain object exists within an image reduces the problem to “placing” the object in the 3D scene based on its shape and location in the image. We showed that scale can be inferred from language, which can narrow down the solution space of depth prediction, leading to improved accuracy.\n\nA similar pattern is also evident in KITTI (Fig. 4). Examples include improved accuracy for “a wall” shown in the first column, “trees” in the second column, and “a group of cars” alongside “a large building” in the last column. This observation is intriguing because, for example, the text “a wall” is ambiguous by itself, especially in outdoor scenes, where the wall could be any size or distance away from the camera, 1 or 100 meters. However, the text description of a scene, either from a human annotator or a deep neural network, inherently carries biases that emphasize “a wall” when its size (tall or wide enough) or depth falls within a specific range while ignoring it when it falls within another range. The resulting prior embedded in the text description may convey more scale information than initially apparent.\n\nOptimizing with different alternation ratios. As a sensitivity study, we investigate how different ratios of alternating optimization steps between text-VAE and conditional sampler have an effect on the performance of WorDepth. We find that optimizing text-VAE with a lower ratio will lead to a more deterministic model, which is anticipated. On the other hand, optimizing text-VAE more frequently enables the model to learn a better variational prior on the depth maps corresponding to text captions, which facilitates the generation of diverse prior depth maps. However, this comes at the cost of training time as the conditional sampler takes more steps to converge and, given a fixed number of steps, results in more blurry predictions. We identify the ratio at 1% in updating text-VAE to be the best empirically (Tab. 3). Ratios exceeding 10% notably degrades performance given a fixed training length because of fewer updates to the sampler. Note that at 100%, where we do not condition the image, caption to depth generation still yields reasonable results as the text captions produce plausible statistics that match the ground truth depth. On the other hand, without the modeling language as a variational prior (at 0%, where we train both text-VAE and conditional optimizer jointly as a direct map from single image and caption to depth), performance degrade to do the lack of the prior.\n\nZero-shot Generalization. Given the smaller domain gap in language descriptions across different scenes compared to images, we conduct a zero-shot transfer experiment to highlight our improved generalization ability. We train the model on the NYU Depth V2 [58] and test it on the Sun-RGBD [60] without fine-tuning. As shown in Tab. 4, WorDepth outperforms baseline methods by a substantial margin, indicating the transferability of language priors which underscores the robustness of text descriptions in handling scene variability. This suggests that language descriptions may offer a more stable basis for generalization across diverse data domains than direct visual signals.\n\n5 Discussion\n\nIn this study, we seek to answer the question of whether language can be used to calibrate the learned scene distribution to true real-world statistics. The answer is yes, which is valuable for circumventing the long-standing problem of scale ambiguity in monocular depth or structure-from-motion problems. The approach is a first in leveraging complementary properties of two modalities with inherent ambiguities for the 3D reconstruction, to address the deficits in one another. We show that by exploiting the layout/scene ambiguity in language as a strength via our variational approach, we can ground predictions to metric scale. This opens up new avenue in how one can address the issue of scale in 3D reconstruction as well as provide a direct framework to extending the many works that currently are limited to relative or scaleless depth predictions.\n\nLimitations. Generic regularizers typically yield little gains, but do little harm; specific regularizers can provide larger boosts but are limited in their applications. While using language as a prior gives flexibility between the two, specificity in the caption controls the degree of regularization imposed. Naturally, vague captions give little to no information on object shape or size, so there is little to be gained; specific, but incorrect captions may misfire, barring any malicious intent. As WorDepth relies on the quality of the caption, it is susceptible to inaccuracies stemming from descriptions provided by the image captioner. Its ease of use also opens up vulnerabilities from malicious users who may choose captions to steer predictions incorrectly.\n\nReferences\n\nAuty and Mikolajczyk [2023] Dylan Auty and Krystian Mikolajczyk. Learning to prompt clip for monocular depth estimation: Exploring the limits of human language. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2039–2047, 2023.\n\nBhat et al. [2021] Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka. Adabins: Depth estimation using adaptive bins. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4009–4018, 2021.\n\nBian et al. [2021] Jia-Wang Bian, Huangying Zhan, Naiyan Wang, Zhichao Li, Le Zhang, Chunhua Shen, Ming-Ming Cheng, and Ian Reid. Unsupervised scale-consistent depth learning from video. International Journal of Computer Vision, 129(9):2548–2564, 2021.\n\nBiederman and Ju [1988] Irving Biederman and Ginny Ju. Surface versus edge-based determinants of visual recognition. Cognitive psychology, 20(1):38–64, 1988.\n\nBloesch et al. [2018] Michael Bloesch, Jan Czarnowski, Ronald Clark, Stefan Leutenegger, and Andrew J Davison. Codeslam—learning a compact, optimisable representation for dense visual slam. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2560–2568, 2018.\n\nCaron et al. [2021] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 9650–9660, 2021.\n\nChang et al. [2021] Wenjie Chang, Yueyi Zhang, and Zhiwei Xiong. Transformer-based monocular depth estimation with attention supervision. In 32nd British Machine Vision Conference (BMVC 2021), 2021.\n\nChen et al. [2023] Jiaben Chen, Renrui Zhang, Dongze Lian, Jiaqi Yang, Ziyao Zeng, and Jianbo Shi. iquery: Instruments as queries for audio-visual sound separation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14675–14686, 2023.\n\nDeng et al. [2009] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009.\n\nDikov and van Vugt [2022] Georgi Dikov and Joris van Vugt. Variational depth networks: Uncertainty-aware monocular self-supervised depth estimation. In European Conference on Computer Vision, pages 43–60. Springer, 2022.\n\nDosovitskiy et al. [2020] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n\nDou et al. [2024] Yiming Dou, Fengyu Yang, Yi Liu, Antonio Loquercio, and Andrew Owens. Tactile-augmented radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024.\n\nEigen et al. [2014] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from a single image using a multi-scale deep network. Advances in neural information processing systems, 27, 2014.\n\nFei and Soatto [2018] Xiaohan Fei and Stefano Soatto. Visual-inertial object detection and mapping. In Proceedings of the European conference on computer vision (ECCV), pages 301–317, 2018.\n\nFei et al. [2019] Xiaohan Fei, Alex Wong, and Stefano Soatto. Geo-supervised visual depth prediction. IEEE Robotics and Automation Letters, 4(2):1661–1668, 2019.\n\nFu et al. [2018a] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, and Dacheng Tao. Deep ordinal regression network for monocular depth estimation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2002–2011, 2018a.\n\nFu et al. [2018b] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, and Dacheng Tao. Deep ordinal regression network for monocular depth estimation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2002–2011, 2018b.\n\nGao et al. [2021] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao. Clip-adapter: Better vision-language models with feature adapters. arXiv preprint arXiv:2110.04544, 2021.\n\nGarg et al. [2016] Ravi Garg, Vijay Kumar Bg, Gustavo Carneiro, and Ian Reid. Unsupervised cnn for single view depth estimation: Geometry to the rescue. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VIII 14, pages 740–756. Springer, 2016.\n\nGeiger et al. [2012] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In 2012 IEEE conference on computer vision and pattern recognition, pages 3354–3361. IEEE, 2012.\n\nGirdhar et al. [2023] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15180–15190, 2023.\n\nGodard et al. [2017] Clément Godard, Oisin Mac Aodha, and Gabriel J Brostow. Unsupervised monocular depth estimation with left-right consistency. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 270–279, 2017.\n\nGuo et al. [2023] Ziyu Guo, Renrui Zhang, Xiangyang Zhu, Yiwen Tang, Xianzheng Ma, Jiaming Han, Kexin Chen, Peng Gao, Xianzhi Li, Hongsheng Li, et al. Point-bind & point-llm: Aligning point cloud with multi-modality for 3d understanding, generation, and instruction following. arXiv preprint arXiv:2309.00615, 2023.\n\nHe et al. [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.\n\nHu et al. [2022] Jia Cheng Hu, Roberto Cavicchioli, and Alessandro Capotondi. Expansionnet v2: Block static expansion in fast end to end training for image captioning. arXiv preprint arXiv:2208.06551, 2022.\n\nHu et al. [2023] Xueting Hu, Ce Zhang, Yi Zhang, Bowen Hai, Ke Yu, and Zhihai He. Learning to adapt clip for few-shot monocular depth estimation. arXiv preprint arXiv:2311.01034, 2023.\n\nJi et al. [2021] Pan Ji, Runze Li, Bir Bhanu, and Yi Xu. Monoindoor: Towards good practice of self-supervised monocular depth estimation for indoor environments. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12787–12796, 2021.\n\nKingma and Ba [2014] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n\nKingma and Welling [2013] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.\n\nKondapaneni et al. [2023] Neehar Kondapaneni, Markus Marks, Manuel Knott, Rogério Guimarães, and Pietro Perona. Text-image alignment for diffusion-based perception. arXiv preprint arXiv:2310.00031, 2023.\n\nLandau et al. [1988] Barbara Landau, Linda B Smith, and Susan S Jones. The importance of shape in early lexical learning. Cognitive development, 3(3):299–321, 1988.\n\nLandau et al. [1998] Barbara Landau, Linda Smith, and Susan Jones. Object shape, object function, and object name. Journal of memory and language, 38(1):1–27, 1998.\n\nLao et al. [2022] Dong Lao, Fengyu Yang, Daniel Wang, Hyoungseob Park, Samuel Lu, Alex Wong, and Stefano Soatto. On the viability of monocular depth pre-training for semantic segmentation. arXiv preprint arXiv:2203.13987, 2022.\n\nLee et al. [2019a] Jin Han Lee, Myung-Kyu Han, Dong Wook Ko, and Il Hong Suh. From big to small: Multi-scale local planar guidance for monocular depth estimation. arXiv preprint arXiv:1907.10326, 2019a.\n\nLee et al. [2019b] Jin Han Lee, Myung-Kyu Han, Dong Wook Ko, and Il Hong Suh. From big to small: Multi-scale local planar guidance for monocular depth estimation. arXiv preprint arXiv:1907.10326, 2019b.\n\nLi et al. [2021] Boying Li, Yuan Huang, Zeyu Liu, Danping Zou, and Wenxian Yu. Structdepth: Leveraging the structural regularities for self-supervised indoor depth estimation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12663–12673, 2021.\n\nLi et al. [2022a] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International Conference on Machine Learning, pages 12888–12900. PMLR, 2022a.\n\nLi et al. [2023a] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023a.\n\nLi et al. [2023b] Yong-Lu Li, Xiaoqian Wu, Xinpeng Liu, Yiming Dou, Yikun Ji, Junyi Zhang, Yixing Li, Jingru Tan, Xudong Lu, and Cewu Lu. From isolated islands to pangea: Unifying semantic space for human action understanding. arXiv preprint arXiv:2304.00553, 2023b.\n\nLi et al. [2022b] Zhenyu Li, Zehui Chen, Xianming Liu, and Junjun Jiang. Depthformer: Exploiting long-range correlation and local information for accurate monocular depth estimation. arXiv preprint arXiv:2203.14211, 2022b.\n\nLiang et al. [2023a] Yiqing Liang, Numair Khan, Zhengqin Li, Thu Nguyen-Phuoc, Douglas Lanman, James Tompkin, and Lei Xiao. Gaufre: Gaussian deformation fields for real-time dynamic novel view synthesis, 2023a.\n\nLiang et al. [2023b] Yiqing Liang, Eliot Laidlaw, Alexander Meyerowitz, Srinath Sridhar, and James Tompkin. Semantic attention flow fields for monocular dynamic scene decomposition. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023b.\n\nLiu et al. [2023] Ce Liu, Suryansh Kumar, Shuhang Gu, Radu Timofte, and Luc Van Gool. Va-depthnet: A variational approach to single image depth prediction. arXiv preprint arXiv:2302.06556, 2023.\n\nLiu et al. [2022] Tian Yu Liu, Parth Agrawal, Allison Chen, Byung-Woo Hong, and Alex Wong. Monitored distillation for positive congruent depth completion. In European Conference on Computer Vision, pages 35–53. Springer, 2022.\n\nLiu et al. [2021] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012–10022, 2021.\n\nLong et al. [2021] Xiaoxiao Long, Cheng Lin, Lingjie Liu, Wei Li, Christian Theobalt, Ruigang Yang, and Wenping Wang. Adaptive surface normal constraint for depth estimation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 12849–12858, 2021.\n\nMel et al. [2022] Mazen Mel, Muhammad Siddiqui, and Pietro Zanuttigh. End-to-end learning for joint depth and image reconstruction from diffracted rotation. arXiv preprint arXiv:2204.07076, 2022.\n\nOquab et al. [2023] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023.\n\nPan et al. [2024] Zixuan Pan, Zihao Wei, and Andrew Owens. Efficient vision-language pre-training by cluster masking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024.\n\nPark et al. [2024] Hyoungseob Park, Anjali Gupta, and Alex Wong. Test-time adaptation for depth completion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024.\n\nPeng et al. [2021] Rui Peng, Ronggang Wang, Yawen Lai, Luyang Tang, and Yangang Cai. Excavating the potential capacity of self-supervised monocular depth estimation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15560–15569, 2021.\n\nQi et al. [2018] Xiaojuan Qi, Renjie Liao, Zhengzhe Liu, Raquel Urtasun, and Jiaya Jia. Geonet: Geometric neural network for joint depth and surface normal estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 283–291, 2018.\n\nRadford et al. [2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748–8763. PMLR, 2021.\n\nRanftl et al. [2021] René Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In Proceedings of the IEEE/CVF international conference on computer vision, pages 12179–12188, 2021.\n\nRao et al. [2021] Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong Tang, Zheng Zhu, Guan Huang, Jie Zhou, and Jiwen Lu. Denseclip: Language-guided dense prediction with context-aware prompting. arXiv preprint arXiv:2112.01518, 2021.\n\nSaxena et al. [2023] Saurabh Saxena, Abhishek Kar, Mohammad Norouzi, and David J Fleet. Monocular depth estimation using diffusion models. arXiv preprint arXiv:2302.14816, 2023.\n\nSaxena et al. [2024] Saurabh Saxena, Charles Herrmann, Junhwa Hur, Abhishek Kar, Mohammad Norouzi, Deqing Sun, and David J Fleet. The surprising effectiveness of diffusion models for optical flow and monocular depth estimation. Advances in Neural Information Processing Systems, 36, 2024.\n\nSilberman et al. [2012] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support inference from rgbd images. In Computer Vision–ECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part V 12, pages 746–760. Springer, 2012.\n\nSingh et al. [2023] Akash Deep Singh, Yunhao Ba, Ankur Sarker, Howard Zhang, Achuta Kadambi, Stefano Soatto, Mani Srivastava, and Alex Wong. Depth estimation from camera image and mmwave radar point cloud. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9275–9285, 2023.\n\nSong et al. [2015] Shuran Song, Samuel P Lichtenberg, and Jianxiong Xiao. Sun rgb-d: A rgb-d scene understanding benchmark suite. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 567–576, 2015.\n\nUpadhyay et al. [2023] Rishi Upadhyay, Howard Zhang, Yunhao Ba, Ethan Yang, Blake Gella, Sicheng Jiang, Alex Wong, and Achuta Kadambi. Enhancing diffusion models with 3d perspective geometry constraints. ACM Transactions on Graphics (TOG), 42(6):1–15, 2023.\n\nWang et al. [2023a] Ruoyu Wang, Zehao Yu, and Shenghua Gao. Planedepth: Self-supervised depth estimation via orthogonal planes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21425–21434, 2023a.\n\nWang et al. [2023b] Youhong Wang, Yunji Liang, Hao Xu, Shaohui Jiao, and Hongkai Yu. Sqldepth: Generalizable self-supervised fine-structured monocular depth estimation. arXiv preprint arXiv:2309.00526, 2023b.\n\nWong and Soatto [2019] Alex Wong and Stefano Soatto. Bilateral cyclic constraint and adaptive regularization for unsupervised monocular depth prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5644–5653, 2019.\n\nWong and Soatto [2021] Alex Wong and Stefano Soatto. Unsupervised depth completion with calibrated backprojection layers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12747–12756, 2021.\n\nWong et al. [2020a] Alex Wong, Safa Cicek, and Stefano Soatto. Targeted adversarial perturbations for monocular depth prediction. Advances in neural information processing systems, 33:8486–8497, 2020a.\n\nWong et al. [2020b] Alex Wong, Xiaohan Fei, Stephanie Tsuei, and Stefano Soatto. Unsupervised depth completion from visual inertial odometry. IEEE Robotics and Automation Letters, 5(2):1899–1906, 2020b.\n\nWong et al. [2021a] Alex Wong, Safa Cicek, and Stefano Soatto. Learning topology from synthetic data for unsupervised depth completion. IEEE Robotics and Automation Letters, 6(2):1495–1502, 2021a.\n\nWong et al. [2021b] Alex Wong, Xiaohan Fei, Byung-Woo Hong, and Stefano Soatto. An adaptive framework for learning unsupervised depth completion. IEEE Robotics and Automation Letters, 6(2):3120–3127, 2021b.\n\nWu et al. [2022] Cho-Ying Wu, Jialiang Wang, Michael Hall, Ulrich Neumann, and Shuochen Su. Toward practical monocular indoor depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3814–3824, 2022.\n\nWu and Yang [2023] Shaokai Wu and Fengyu Yang. Boosting detection in crowd analysis via underutilized output features. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 15609–15618, 2023.\n\nWu et al. [2023] Yangchao Wu, Tian Yu Liu, Hyoungseob Park, Stefano Soatto, Dong Lao, and Alex Wong. Augundo: Scaling up augmentations for unsupervised depth completion. arXiv preprint arXiv:2310.09739, 2023.\n\nXia et al. [2020] Zhihao Xia, Patrick Sullivan, and Ayan Chakrabarti. Generating and exploiting probabilistic monocular depth estimates. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 65–74, 2020.\n\nYang and Ma [2022] Fengyu Yang and Chenyang Ma. Sparse and complete latent organization for geospatial semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1809–1818, 2022.\n\nYang et al. [2022] Fengyu Yang, Chenyang Ma, Jiacheng Zhang, Jing Zhu, Wenzhen Yuan, and Andrew Owens. Touch and go: Learning from human-collected vision and touch. Neural Information Processing Systems (NeurIPS) - Datasets and Benchmarks Track, 2022.\n\nYang et al. [2023] Fengyu Yang, Jiacheng Zhang, and Andrew Owens. Generating visual scenes from touch. International Conference on Computer Vision (ICCV), 2023.\n\nYang et al. [2024] Fengyu Yang, Chao Feng, Ziyang Chen, Hyoungseob Park, Daniel Wang, Yiming Dou, Ziyao Zeng, Xien Chen, Rit Gangopadhyay, Andrew Owens, and Alex Wong. Binding touch to everything: Learning unified multimodal tactile representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024.\n\nYang et al. [2021] Guanglei Yang, Hao Tang, Mingli Ding, Nicu Sebe, and Elisa Ricci. Transformer-based attention networks for continuous pixel-wise prediction. In Proceedings of the IEEE/CVF International Conference on Computer vision, pages 16269–16279, 2021.\n\nYang et al. [2019] Yanchao Yang, Alex Wong, and Stefano Soatto. Dense depth posterior (ddp) from single image and sparse range. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3353–3362, 2019.\n\nYin et al. [2019] Wei Yin, Yifan Liu, Chunhua Shen, and Youliang Yan. Enforcing geometric constraints of virtual normal for depth prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5684–5693, 2019.\n\nYou et al. [2022] Chenyu You, Weicheng Dai, Fenglin Liu, Yifei Min, Haoran Su, Xiaoran Zhang, Xiaoxiao Li, David A Clifton, Lawrence Staib, and James S Duncan. Mine your own anatomy: Revisiting medical image segmentation with extremely limited labels. arXiv preprint arXiv:2209.13476, 2022.\n\nYou et al. [2023] Chenyu You, Weicheng Dai, Yifei Min, Lawrence Staib, and James S Duncan. Implicit anatomical rendering for medical image segmentation with stochastic experts. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 561–571. Springer, 2023.\n\nYou et al. [2024] Chenyu You, Weicheng Dai, Yifei Min, Fenglin Liu, David Clifton, S Kevin Zhou, Lawrence Staib, and James Duncan. Rethinking semi-supervised medical image segmentation: A variance-reduction perspective. Advances in Neural Information Processing Systems, 36, 2024.\n\nYu et al. [2023] Shangbin Yu, Renyan Zhang, Shuaiye Ma, and Xinfang Jiang. Monocular depth estimation network based on swin transformer. In Journal of Physics: Conference Series, page 012019. IOP Publishing, 2023.\n\nYu et al. [2020] Zehao Yu, Lei Jin, and Shenghua Gao. P 2 net: Patch-match and plane-regularization for unsupervised indoor depth estimation. In European Conference on Computer Vision, pages 206–222. Springer, 2020.\n\nYuan et al. [2022] Weihao Yuan, Xiaodong Gu, Zuozhuo Dai, Siyu Zhu, and Ping Tan. Neural window fully-connected crfs for monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3916–3925, 2022.\n\nZhang et al. [2021a] Renrui Zhang, Rongyao Fang, Peng Gao, Wei Zhang, Kunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li. Tip-adapter: Training-free clip-adapter for better vision-language modeling. arXiv preprint arXiv:2111.03930, 2021a.\n\nZhang et al. [2021b] Renrui Zhang, Longtian Qiu, Wei Zhang, and Ziyao Zeng. Vt-clip: Enhancing vision-language models with visual-guided texts. arXiv preprint arXiv:2112.02399, 2021b.\n\nZhang et al. [2021c] Renrui Zhang, Ziyao Zeng, Ziyu Guo, Xinben Gao, Kexue Fu, and Jianbo Shi. Dspoint: Dual-scale point cloud recognition with high-frequency fusion. arXiv preprint arXiv:2111.10332, 2021c.\n\nZhang et al. [2022a] Renrui Zhang, Ziyu Guo, Wei Zhang, Kunchang Li, Xupeng Miao, Bin Cui, Yu Qiao, Peng Gao, and Hongsheng Li. Pointclip: Point cloud understanding by clip. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8552–8562, 2022a.\n\nZhang et al. [2022b] Renrui Zhang, Ziyao Zeng, Ziyu Guo, and Yafeng Li. Can language understand depth? In Proceedings of the 30th ACM International Conference on Multimedia, pages 6868–6874, 2022b.\n\nZhang et al. [2023] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. ICLR 2024, 2023.\n\nZhang et al. [2024] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? arXiv preprint arXiv:2403.14624, 2024.\n\nZhao et al. [2022a] Chaoqiang Zhao, Youmin Zhang, Matteo Poggi, Fabio Tosi, Xianda Guo, Zheng Zhu, Guan Huang, Yang Tang, and Stefano Mattoccia. Monovit: Self-supervised monocular depth estimation with a vision transformer. In 2022 International Conference on 3D Vision (3DV), pages 668–678. IEEE, 2022a.\n\nZhao et al. [2022b] Hanbin Zhao, Fengyu Yang, Xinghe Fu, and Xi Li. Rbc: Rectifying the biased context in continual semantic segmentation. ECCV, 2022b.\n\nZhao et al. [2020] Wang Zhao, Shaohui Liu, Yezhi Shu, and Yong-Jin Liu. Towards better generalization: Joint depth-pose learning without posenet. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9151–9161, 2020.\n\nZhao et al. [2023] Wenliang Zhao, Yongming Rao, Zuyan Liu, Benlin Liu, Jie Zhou, and Jiwen Lu. Unleashing text-to-image diffusion models for visual perception. arXiv preprint arXiv:2303.02153, 2023.\n\nZheng et al. [2024] Chenhao Zheng, Jieyu Zhang, Aniruddha Kembhavi, and Ranjay Krishna. Iterated learning improves compositionality in large vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024.\n\nZhou et al. [2021a] Chong Zhou, Chen Change Loy, and Bo Dai. Denseclip: Extract free dense labels from clip. arXiv preprint arXiv:2112.01071, 2021a.\n\nZhou et al. [2019] Junsheng Zhou, Yuwang Wang, Kaihuai Qin, and Wenjun Zeng. Moving indoor: Unsupervised video depth learning in challenging environments. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8618–8627, 2019.\n\nZhou et al. [2021b] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. arXiv preprint arXiv:2109.01134, 2021b.\n\nZhou et al. [2017] Tinghui Zhou, Matthew Brown, Noah Snavely, and David G Lowe. Unsupervised learning of depth and ego-motion from video. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1851–1858, 2017.\n\nZhou et al. [2022] Xingyi Zhou, Rohit Girdhar, Armand Joulin, Phillip Krähenbühl, and Ishan Misra. Detecting twenty-thousand classes using image-level supervision. arXiv preprint arXiv:2201.02605, 2022.\n\nZhu et al. [2023a] Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, HongFa Wang, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zongwei Li, et al. Languagebind: Extending video-language pretraining to n-modality by language-based semantic alignment. arXiv preprint arXiv:2310.01852, 2023a.\n\nZhu et al. [2023b] Xiangyang Zhu, Renrui Zhang, Bowei He, Ziyu Guo, Ziyao Zeng, Zipeng Qin, Shanghang Zhang, and Peng Gao. Pointclip v2: Prompting clip and gpt for powerful 3d open-world learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2639–2650, 2023b.\n\n\\thetitle\n\nSupplementary Material\n\nAppendix A Evaluation metrics.\n\nDrawing on [43, 7], our evaluation of WorDepth alongside comparison methods involves a quantitative assessment through several metrics. These include mean absolute relative error (Abs Rel), root mean square error (RMSE), absolute error in log space (log10)subscript10\\left(\\log_{10}\\right)( roman_log start_POSTSUBSCRIPT 10 end_POSTSUBSCRIPT ), logarithmic root mean square error (RMSElogsubscriptRMSE\\text{RMSE}_{\\log}RMSE start_POSTSUBSCRIPT roman_log end_POSTSUBSCRIPT) and threshold accuracy (δi)subscript𝛿𝑖\\left(\\delta_{i}\\right)( italic_δ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ). The evaluation metrics are summarized in Table 5 for details.\n\nAppendix B Ablation on Model Architecture\n\nHowever, it’s equally crucial to avoid excessively large hidden variables. A relatively constrained dimensionality acts as a form of regularization, compelling the text-VAE to focus on extracting features crucial for depth decoding. Additionally, a limited hidden dimension prompts the model to learn not just the distribution mean but also its variance. This aspect is particularly important when mapping a text description to multiple scenes, such scenes’ text features are encoded with identical distribution means but exhibit significant variance.\n\nWe established hidden variables d𝑑ditalic_d of 32, 64, 128, 256, 512, and 1024 for training WorDepth. It was observed that the optimal hidden dimension is 128, striking a balance between capturing sufficient geometric features of scenes while maintaining effective regularization. Deviating from this optimal size, either too small or too large, adversely impacts performance.\n\nAppendix C Additional Visualization on NYU Depth V2\n\nIn this section, as illustrated in Figure 5, We present additional visualizations comparing WorDepth with a baseline method AdaBins [2] on the NYU Depth V2 [58] dataset, emphasizing the advantages gained from integrating the language prior. Compared with AdaBins, the error map, with its brighter regions highlighting larger errors, clearly demonstrates that WorDepth achieves more precise depth predictions for objects identified in the text description. For instance: “a sink and a bath tub” in the first row, “a white bath tub” in the second row, “a wooden dresser” in the third row, “a bed” in the fourth row, “a bunk bed” in the fifth row, “an unmade bed with clothes on top of it” in the sixth row, “a couch and a table” in the seventh row, “a table and chairs” in the eighth row, “a blender on a counter” in the ninth row, “chairs” in the tenth row, and “machine on top of a wooden table” in the last row.\n\nAppendix D Additional Visualization on KITTI\n\nThis section, depicted in Figure 6, showcases visualizations of Monocular Depth Estimation in outdoor scenarios with the KITTI dataset [20] using Eigen Split [13], comparing with Adabins [2]. Due to the limited variety of objects in outdoor scenes, our method captures fewer objects compared to indoor scenes. However, when salient objects and scenes are present outdoors, our method gains a preliminary understanding of their scale. This understanding aids in enhancing monocular depth estimation for these objects. The error map’s brighter regions, which emphasize greater absolute relative errors, unequivocally show that WorDepth outperforms AdaBins in making more accurate depth predictions for objects and scenes mentioned in the text description. For instance: “two white trucks” in the upper right, “a woman riding a scooter” in the lower left, “buildings” in the lower middle, and “forest with tree” in the lower left."
    }
}
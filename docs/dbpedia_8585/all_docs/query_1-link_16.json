{
    "id": "dbpedia_8585_1",
    "rank": 16,
    "data": {
        "url": "https://carpentries-incubator.github.io/python_packaging/instructor/03-building-and-installing.html",
        "read_more_link": "",
        "language": "en",
        "title": "Python Packaging: Building and Installing Packages using setuptools",
        "top_image": "https://carpentries-incubator.github.io/python_packaging/favicon-32x32.png",
        "meta_img": "https://carpentries-incubator.github.io/python_packaging/favicon-32x32.png",
        "images": [
            "https://carpentries-incubator.github.io/python_packaging/assets/images/incubator-logo.svg",
            "https://carpentries-incubator.github.io/python_packaging/assets/images/incubator-logo-sm.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2023-07-04T00:00:00",
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "../apple-touch-icon.png",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Building and Installing Packages using setuptools\n\nLast updated on 2024-04-16 | Edit this page\n\nEstimated time: 20 minutes\n\nIntroduction\n\nIn the first lesson, we showed how to use the PYTHONPATH environment variable to enable us to import our modules and packages from anywhere on our system. There are a few disadvantages to this method:\n\nIf we have two different versions of a package on our system at once, it can be tedious to manually update PYTHONPATH whenever we want to switch between them.\n\nIf we have multiple Python environments on our system (using tools such as venv or conda), setting PYTHONPATH will affect all of them. This can lead to unexpected dependency conflicts that can be very hard to debug.\n\nIf we share our software with others and require them to update their own PYTHONPATH, they will need to install any requirements for our package separately, which can be error prone.\n\nIt would be preferable if we could install our package using pip, the same way that we would normally install external Python packages. However, if we enter the top level directory of our project and try the following:\n\nBASH\n\n$ cd /path/to/my/workspace/epi_models $ python3 -m pip install .\n\nWe get the following error:\n\nOUTPUT\n\nERROR: Directory '.' is not installable. Neither 'setup.py' nor 'pyproject.toml' found.\n\nIn order to make our project installable, we need to add the either the file pyproject.toml or setup.py to our project. For modern Python projects, it is recommended to write only pyproject.toml. This was introduced by PEP 517, PEP 518 and PEP 621 as a standard way to define a Python project, and all tools that build, install, and publish Python packages are expected to use it.\n\nWhat issetup.py?\n\nsetup.py serves a similar role to pyproject.toml, but it is no longer recommended for use. The lesson on the history of build tools explains how it works and why the community has moved away from it.\n\nBy making our project pip-installable, weâ€™ll also make it very easy to publish our packages on public repositories â€“ this will be covered in our lesson on package publishing. After publishing our work, our users will be able to download and install our package using pip from any machine of their chocie!\n\nTo begin, weâ€™ll introduce the concept of a â€˜Python environmentâ€™, and how these can help us manage our workflows.\n\nManaging Python Environments\n\nWhen working with Python, it can sometimes be beneficial to install packages to an isolated environment instead of installing them globally. Usually, this is done to manage competing dependencies:\n\nProject B might depend upon Project A, but may have been written to use version 1.0.\n\nProject C might also depend upon Project A, but may instead only work with version 2.0.\n\nIf we install Project A globally and choose version 2.0, then Project B will not work. Similarly, if we choose version 1.0, Project C will not work.\n\nA good way to handle these sorts of conflicts is to instead use virtual environments for each project. A number of tools have been developed to manage virtual environments, such as venv, which is a standard built-in Python tool, and conda, which is a powerful third-party tool. Weâ€™ll focus on venv here, but both tools work similarly.\n\nCallout\n\nYou can pip install packages into a conda virtual environment, so much of the advice in this lesson will still apply if you prefer to use conda.\n\nIf weâ€™re using Linux, we can find which Python environment weâ€™re using by calling:\n\nBASH\n\n$ which python3\n\nIf weâ€™re using the default system environment, the result is something like the following:\n\nOUTPUT\n\n/usr/bin/python3\n\nTo create a new virtual environment using venv, we can call:\n\nBASH\n\n$ python3 -m venv /path/to/my/env\n\nThis will create a new directory at the location /path/to/my/env. Note that this can be a relative path, so just calling python3 -m venv myenv will create the virtual environment in the directory ./myenv. We can then â€˜activateâ€™ the virtual environment using:\n\nBASH\n\n$ source /path/to/my/env/bin/activate\n\nChecking which Python weâ€™re running should now give a different result:\n\nBASH\n\n$ which python3\n\nOUTPUT\n\n/path/to/my/env/bin/python3\n\nIf we now install a new package, it will be installed within our new virtual environment instead of being installed to the system libraries. For example:\n\nBASH\n\n$ python3 -m pip install numpy\n\nWe should now find NumPy installed at the following location (note that the Python version may not match yours):\n\nBASH\n\n$ ls /path/to/my/env/lib/python3.8/site-packages/numpy\n\nsite-packages is a standard location to store installed Python packages. We can see this by analysing Pythonâ€™s import path:\n\nPYTHON\n\n>>> import sys >>> print(sys.path)\n\n['', '/usr/lib/python38.zip', '/usr/lib/python3.8', '/usr/lib/python3.8/lib-dynload', '/path/to/my/env/lib/python3.8/site-packages']\n\nIf we no longer wish to use this virtual environment, we can return to the system environment by calling:\n\nBASH\n\n$ deactivate\n\nVirtual environments are very useful when weâ€™re testing our code, as they allow us to create a fresh Python environment without any of the installed packages we normally use in our work. This will be important later when we add dependencies to our package, as this allows us to test whether our users will be able to install and run our code properly using a fresh environment.\n\nAn Overview of TOML files\n\npyproject.toml is a TOML file, which stands for â€˜Tomâ€™s Obvious Minimal Langaugeâ€™ (named for its developer, Thomas Preston-Werner, who cofounded GitHub). There are many configuration file formats in common usage, such as YAML, JSON, and INI, but the Python community chose TOML as it provides some benefits over the competition:\n\nDesigned to be human writable and human readable.\n\nCan map unambiguously to a hash table (a dict in Python).\n\nIt has a formal specification, so has an unambiguous set of rules.\n\nA TOML file contains a series of key = value pairs, which may be grouped into sections using a header enclosed in square brackets, such as [section name]. The values are typed, unlike some other formats where all values are strings. The available types are strings, integers, floats, booleans, and dates. It is possible to store lists of values in arrays, or store a series of key-value pairs in tables. For example:\n\nTOML\n\n# file: mytoml.toml int_val = 5 float_val = 0.5 string_val = \"hello world\" bool_val = true date_val = 2023-01-01T08:00:00 array = [1, 2, 3] inline_table = {key = \"value\"} # Section headings allow us to define tables over # multiple lines [header_table] name = \"John\" dob = 2002-03-05 # We can define subtables using dot notation [header_table.subtable] foo = \"bar\"\n\nWe can read this using the toml library in Python:\n\nBASH\n\n$ python3 -m pip install toml\n\nPYTHON\n\n>>> import toml >>> with open(\"mytoml.toml\", \"r\") as f: ... data = toml.load(f) >>> print(data)\n\nThe result is a dictionary object, with TOML types converted to their corresponding Python types:\n\n{ 'int_val': 5, 'float_val': 0.5, 'string_val': 'hello world', 'bool_val': True, 'date_val': datetime.datetime(2023, 1, 1, 8, 0), 'array': [1, 2, 3], 'inline_table': {'key': 'value'}, 'header_table': { 'name': 'John', 'dob': datetime.date(2002, 3, 5), 'subtable': { 'foo': 'bar' } } }\n\nCallout\n\nSince Python 3.11, tomllib is part of Pythonâ€™s standard library. It works the same as above, but youâ€™ll need to import tomllib instead of toml.\n\nInstalling our package with pyproject.toml\n\nFirst, we will show how to write a relatively minimal pyproject.toml file so that we can install our projects using pip. We will then cover some additional tricks that can be achieved with this file:\n\nUse alternative directory structures\n\nInclude any data files needed by our code\n\nGenerate an executable so that our scripts can be run directly from the command line\n\nConfigure our development tools.\n\nTo make our package pip-installable, we should add the file pyproject.toml to the top-level epi_models directory:\n\nðŸ“ epi_models\n\n|\n\n|____ðŸ“œ pyproject.toml\n\n|____ðŸ“¦ epi_models\n\n|\n\n|____ðŸ“œ __init__.py\n\n|____ðŸ“œ __main__.py\n\n|\n\n|____ðŸ“ models\n\n| |\n\n| |____ðŸ“œ __init__.py\n\n| |____ðŸ“œ SIR.py\n\n| |____ðŸ“œ SEIR.py\n\n| |____ðŸ“œ SIS.py\n\n| |____ðŸ“œ utils.py\n\n|\n\n|____ðŸ“ plotting\n\n|\n\n|____ðŸ“œ __init__.py\n\n|____ðŸ“œ plot_SIR.py\n\n|____ðŸ“œ plot_SEIR.py\n\n|____ðŸ“œ plot_SIS.py\n\nThe first section in our pyproject.toml file should specify which build system we wish to use, and additionally specify any version requirements for packages used to build our code. This is necessary to avoid a circular dependecy problem that occurred with earlier Python build systems, in which the user had to run an install program to determine the projectâ€™s dependencies, but needed to already have the correct build tool installed to run the install program â€“ see the lesson on historical build tools for more detail. We will choose to use setuptools, which requires the following:\n\nrequires is set to a list of strings, each of which names a dependency of the build system and (optionally) its minimum version. This uses the same version syntax as pip.\n\nbuild-backend is set to a sub-module of setuptools which implements the PEP 517 build interface.\n\nWith our build system determined, we can add some metadata that defines our project. At a minimum, we should specify the name of the package, its version, and our dependencies:\n\nThatâ€™s all we need! Weâ€™ll discuss versioning in our lesson on publishing. With this done, we can install our package using:\n\nBASH\n\n$ python3 -m pip install .\n\nThis will automatically download and install our dependencies, and our package will be importable regardless of which directory weâ€™re in.\n\nThe installed package can be found in the directory /path/to/my/env/lib/python3.8/site-packages/epi_models along with a new directory, epi_models-0.1.0.dist-info, which simply contains metadata describing our project. If we look inside our installed package, weâ€™ll see that our files have been copied, and there is also a __pycache__ directory:\n\nBASH\n\n$ ls /path/to/my/env/lib/python3.8/site-packages/epi_models\n\n__init__.py __main__.py models plotting __pycache__\n\nThe __pycache__ directory contains Python bytecode, which is a lower-level version of Python that is understood by the Python Virtual Machine (PVM). All of our Python code is converted to bytecode when it is run or imported, and by pre-compiling our package it can be imported much faster. If we look into the directories models and plotting, weâ€™ll see those have been compiled to bytecode too.\n\nIf we wish to uninstall, we may call:\n\nBASH\n\n$ python3 -m pip uninstall epi_models\n\nWe can also create an â€˜editable installâ€™, in which any changes we make to our code are instantly recognised by any codes importing it â€“ this mode can be very useful when developing our code, especially when working on documentation or tests.\n\nBASH\n\n$ python3 -m pip install -e . $ # Or... $ python3 -m pip install --editable .\n\nCallout\n\nThe ability to create editable installs from a pyproject.toml-only build was standardised in PEP 660, and only recently implemented in pip. You may need to upgrade to use this feature:\n\nBASH\n\n$ python3 -m pip install --upgrade pip\n\nThere are many other options we can add to our pyproject.toml to better describe our project. PEP 621 defines a minimum list of possible metadata that all build tools should support, so weâ€™ll stick to that list. Each build tool will also define synonyms for some metadata entries, and additional tool-specific metadata. Some of the recommended core metadata keys are described below:\n\nTOML\n\n# file: pyproject.toml [project] # name: String, REQUIRED name = \"my_project\" # version: String, REQUIRED # Should follow PEP 440 rules # Can be provided dynamically, see the lesson on publishing version = \"1.2.3\" # description: String # A simple summary of the project description = \"My wonderful Python package\" # readme: String # Full description of the project. # Should be the path to your README file, relative to pyproject.toml readme = \"README.md\" # requires-python: String # The Python version required by the project requires-python = \">=3.8\" # license: Table # The license of your project. # Can be provided as a file or a text description. # Discussed in the lesson on publishing license = {file = \"LICENSE.md\"} # or... license = {text = \"BDS 3-Clause License\"} # authors: Array of Tables # Can also be called 'maintainers'. # Each entry can have a name and/or an email authors = [ {name = \"My Name\", email = \"my.email@email.net\"}, {name = \"My Friend\", email = \"their.email@email.net\"}, ] # urls: Table # Should describe where to find useful info for your project urls = {source = \"github.com/MyProfile/my_project\", documentation = \"my_project.readthedocs.io/en/latest\"} # dependencies: Array of Strings # A list of requirements for our package dependencies = [ \"numpy >= 1.20\", \"pyyaml\", ]\n\nNote that some of the longer tables in our TOML file can be written using non-inline tables if it improved readability:\n\nTOML\n\n[project.urls] Source = \"github.com/MyProfile/my_project\", Documentation = \"my_project.readthedocs.io/en/latest\",\n\nAlternative Directory Structures\n\nsetuptools provides some additional tools to help us install our package if they use a different layout to the â€˜flatâ€™ layout we covered so far. A popular alternative layout is the src-layout:\n\nðŸ“ epi_models\n\n|\n\n|____ðŸ“œ pyproject.toml\n\n|____ðŸ“ src\n\n|\n\n|____ðŸ“¦ epi_models\n\n|\n\n|____ðŸ“œ __init__.py\n\n|____ðŸ“œ __main__.py\n\n|____ðŸ“ models\n\n|____ðŸ“ plotting\n\nThe main benefit of this choice is that setuptools wonâ€™t accidentally bundle any utility modules stored in the top-level directory with our package. It can also be neater when one project contains multiple packages. Note that directories and files with special names are excluded by default regardless of which layout we choose, such as test/, docs/, and setup.py.\n\nWe can also disable automatic package discovery and explicitly list the packages we wish to install:\n\nTOML\n\n# file: pyproject.toml [tool.setuptools] packages = [\"my_package\", \"my_other_package\"]\n\nNote that this is not part of the PEP 621 standard, and therefore instead of being listed under the [project] header, it is a method specific to setuptools. Finally, we may set up custom package discovery:\n\nTOML\n\n# file: pyproject.toml [tool.setuptools.packages.find] where = [\"my_directory\"] include = [\"my_package\", \"my_other_package\"] exclude = [\"my_package.tests*\"]\n\nHowever, for ease of use, it is recommended to stick to either the flat layout or the src layout.\n\nPackage Data\n\nSometimes our code requires some non-.py files in order to function properly, but these would not be picked up by automatic package discovery. For example, the project may store default input data in .json files. These could be included with your package by adding the following to pyproject.toml:\n\nTOML\n\n# file: pyproject.toml [tool.setuptools.package-data] epi_models = [\"*.json\"]\n\nNote that this would grab only .json files in the top-level directory of our project. To include data files from all packages and sub-packages, we should instead write:\n\nTOML\n\n# file: pyproject.toml [tool.setuptools.package-data] \"*\" = [\"*.json\"]\n\nInstalling Scripts\n\nIf our package contains any scripts and/or a __main__.py file, we can run those from anywhere on our system after installation:\n\nBASH\n\n$ python3 -m epi_models $ python3 -m epi_models.plotting.plot_SIR\n\nWith a little extra work, we can also install a simplified interface that doesnâ€™t require python3 -m in front. This is how tools like pip can be invoked using two possible methods:\n\nBASH\n\n$ python3 -m pip # Invoke with python $ pip # Invoke via console-scripts entrypoint\n\nThis can be achieved by adding a table scripts under the [project] header:\n\nTOML\n\n# file: pyproject.toml [project] scripts = {epi_models = \"epi_models.__main__:main\"} # Alternative form: [project.scripts] epi_models = \"epi_models.__main__:main\"\n\nThis syntax means that we should create a console script epi_models, and that running it should call the function main() from the file epi_models/__main__.py. This will require a slight modification to our __main__.py file. All thatâ€™s necessary is to move everything from the script into a function main() that takes no arguments, and then to call main() at the bottom of the script:\n\nPYTHON\n\n# file: main.py def main(): # Put the __main__ script here... main()\n\nThis will allow us to run our package as a script directly from the command line\n\nBASH\n\n$ python3 -m pip install . $ epi_models --help\n\nNote that weâ€™ll still be able to run our code using the longer form:\n\nBASH\n\n$ python3 -m epi_models --help\n\nIf we have multiple scripts in our package, these can all be given invidual console scripts. However, these will also need to have a function name as an entry point:\n\nTOML\n\n# file: pyproject.toml [project.scripts] epi_models = \"epi_models.__main__:main\" epi_models_sir = \"epi_models.plotting.plot_SIR:main\"\n\nSo how do these scripts work? When we activate a virtual environment, a new entry is added to our PATH environment variable linking to /path/to/my/env/bin/:\n\nBASH\n\nPATH = \"/path/to/my/env/bin:${PATH}\"\n\nAfter installing our console scripts, we can find a new file in this directory with the name we assigned to it. For example, /path/to/my/env/bin/epi_models:\n\nPYTHON\n\n#!/path/to/my/env/bin/python3 # -*- coding: utf-8 -*- import re import sys from epi_models.__main__ import main if __name__ == '__main__': sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0]) sys.exit(main())\n\nInstalling our project has automatically created a new Python file that can be run as a command line script due to the hash-bang (#!) on the top line, and all it does it import our main function and run it. As itâ€™s contained with the bin/ directory of our Python environment, itâ€™s available for use as long weâ€™re using that environment, but as soon as we call deactivate, it is removed from our PATH.\n\nSetting Dependency Versions\n\nEarlier, when setting dependencies in our pyproject.toml, we chose to specify a minimum requirement for numpy, but not for pyyaml:\n\nThis indicates that pip should install any version of numpy greater than 1.20, but that any version of pyyaml will do. If our installed numpy version is less than 1.20, or if it isnâ€™t installed at all, pip will upgrade to the latest version thatâ€™s compatible with the rest of our installed packages and our Python version. Weâ€™ll cover software versioning in more detail in the lesson on publishing, but now weâ€™ll simply cover some ways to specify which software versions we need:\n\nTOML\n\n\"numpy >= 1.20\" # Must be at least 1.20 \"numpy > 1.20\" # Must be greater than 1.20 \"numpy == 1.20\" # Must be exactly 1.20 \"numpy <= 1.20\" # Must be 1.20 at most \"numpy < 1.20\" # Must be less than 1.20 \"numpy == 1.*\" # Must be any version 1\n\nIf we separate our clauses with commas, we can combine these requirements:\n\nTOML\n\n# At least 1.20, less than 1.22, and not the release 1.21.3 \"numpy => 1.20, < 1.22, != 1.21.3\"\n\nA useful shorthand is the â€˜compatible releaseâ€™ clause:\n\nTOML\n\n\"numpy ~= 1.20\" # Must be a release compatible with 1.20\n\nThis is equivalent to:\n\nTOML\n\n\"numpy >= 1.20, == 1.*\"\n\nThat is, we require anything which is version 1, provided itâ€™s greater than 1.20. This would include version 1.25, but exlude version 2.0. Weâ€™ll come back to this later when we discuss publishing.\n\nOptional Dependencies\n\nSometimes we might have dependencies that only make sense for certain kind of user. For example, a developer of our library might need any libraries we use to run unit tests or build documentation, but an end user would not. These can be added as optional-dependencies:\n\nThese dependencies can be installed by adding the name of each optional dependency group in square brackets after telling pip what we want to install:\n\nBASH\n\n$ pip install .[test] # Include testing dependencies $ pip install .[doc] # Include documentation dependencies $ pip install .[test,doc] # Include all dependencies"
    }
}
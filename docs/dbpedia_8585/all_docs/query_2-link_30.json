{
    "id": "dbpedia_8585_2",
    "rank": 30,
    "data": {
        "url": "https://opentelemetry.io/docs/kubernetes/operator/automatic/",
        "read_more_link": "",
        "language": "en",
        "title": "Injecting Auto-instrumentation",
        "top_image": "https://opentelemetry.io/img/social/logo-wordmark-001.png",
        "meta_img": "https://opentelemetry.io/img/social/logo-wordmark-001.png",
        "images": [],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2024-08-05T19:22:09-04:00",
        "summary": "",
        "meta_description": "An implementation of auto-instrumentation using the OpenTelemetry Operator.",
        "meta_lang": "en",
        "meta_favicon": "/favicons/favicon.ico",
        "meta_site_name": "OpenTelemetry",
        "canonical_link": "https://opentelemetry.io/docs/kubernetes/operator/automatic/",
        "text": "An implementation of auto-instrumentation using the OpenTelemetry Operator.\n\nThe OpenTelemetry Operator supports injecting and configuring auto-instrumentation libraries for .NET, Java, Node.js, Python, and Go services.\n\nInstallation\n\nFirst, install the OpenTelemetry Operator into your cluster.\n\nYou can do this with the Operator release manifest, the Operator helm chart, or with Operator Hub.\n\nIn most cases, you will need to install cert-manager. If you use the helm chart, there is an option to generate a self-signed cert instead.\n\nIf you want to use Go auto-instrumentation, you need to enable the feature gate. See Controlling Instrumentation Capabilities for details.\n\nCreate an OpenTelemetry Collector (Optional)\n\nIt is a best practice to send telemetry from containers to an OpenTelemetry Collector instead of directly to a backend. The Collector helps simplify secret management, decouples data export problems (such as a need to do retries) from your apps, and lets you add additional data to your telemetry, such as with the k8sattributesprocessor component. If you chose not to use a Collector, you can skip to the next section.\n\nThe Operator provides a Custom Resource Definition (CRD) for the OpenTelemetry Collector which is used to create an instance of the Collector that the Operator manages. The following example deploys the Collector as a deployment (the default), but there are other deployment modes that can be used.\n\nWhen using the Deployment mode the operator will also create a Service that can be used to interact with the Collector. The name of the service is the name of the OpenTelemetryCollector resource prepended to -collector. For our example that will be demo-collector.\n\nThe above command results in a deployment of the Collector that you can use as an endpoint for auto-instrumentation in your pods.\n\nConfigure Automatic Instrumentation\n\nTo be able to manage automatic instrumentation, the Operator needs to be configured to know what pods to instrument and which automatic instrumentation to use for those pods. This is done via the Instrumentation CRD.\n\nCreating the Instrumentation resource correctly is paramount to getting auto-instrumentation working. Making sure all endpoints and env vars are correct is required for auto-instrumentation to work properly.\n\n.NET\n\nThe following command will create a basic Instrumentation resource that is configured specifically for instrumenting .NET services.\n\nBy default, the Instrumentation resource that auto-instruments .NET services uses otlp with the http/protobuf protocol. This means that the configured endpoint must be able to receive OTLP over http/protobuf. Therefore, the example uses http://demo-collector:4318, which will connect to the http port of the otlpreceiver of the Collector created in the previous step.\n\nExcluding auto-instrumentation\n\nBy default, the .NET auto-instrumentation ships with many instrumentation libraries. This makes instrumentation easy, but could result in too much or unwanted data. If there are any libraries you do not want to use you can set the OTEL_DOTNET_AUTO_[SIGNAL]_[NAME]_INSTRUMENTATION_ENABLED=false where [SIGNAL] is the type of the signal and [NAME] is the case-sensitive name of the library.\n\nLearn more\n\nFor more details, see .NET Auto Instrumentation docs.\n\nJava\n\nThe following command creates a basic Instrumentation resource that is configured for instrumenting Java services.\n\nBy default, the Instrumentation resource that auto-instruments Java services uses otlp with the http/protobuf protocol. This means that the configured endpoint must be able to receive OTLP over http via protobuf payloads. Therefore, the example uses http://demo-collector:4318, which connects to the http port of the otlpreceiver of the Collector created in the previous step.\n\nExcluding auto-instrumentation\n\nBy default, the Java auto-instrumentation ships with many instrumentation libraries. This makes instrumentation easy, but could result in too much or unwanted data. If there are any libraries you do not want to use you can set the OTEL_INSTRUMENTATION_[NAME]_ENABLED=false where [NAME] is the name of the library. If you know exactly which libraries you want to use, you can disable the default libraries by setting OTEL_INSTRUMENTATION_COMMON_DEFAULT_ENABLED=false and then use OTEL_INSTRUMENTATION_[NAME]_ENABLED=true where [NAME] is the name of the library. For more details, see Suppressing specific instrumentation.\n\nLearn more\n\nFor more details, see Java agent Configuration.\n\nNode.js\n\nThe following command creates a basic Instrumentation resource that is configured for instrumenting Node.js services.\n\nBy default, the Instrumentation resource that auto-instruments Node.js services uses otlp with the grpc protocol. This means that the configured endpoint must be able to receive OTLP over grpc. Therefore, the example uses http://demo-collector:4317, which connects to the grpc port of the otlpreceiver of the Collector created in the previous step.\n\nExcluding instrumentation libraries\n\nBy default, the Node.js zero-code instrumentation has all the instrumentation libraries enabled.\n\nTo enable only specific instrumentation libraries you can use the OTEL_NODE_ENABLED_INSTRUMENTATIONS environment variable as documented in the Node.js zero-code instrumentation documentation.\n\nTo keep all default libraries and disable only specific instrumentation libraries you can use the OTEL_NODE_DISABLED_INSTRUMENTATIONS environment variable. For details, see Excluding instrumentation libraries.\n\nNote\n\nIf both environment variables are set, OTEL_NODE_ENABLED_INSTRUMENTATIONS is applied first, and then OTEL_NODE_DISABLED_INSTRUMENTATIONS is applied to that list. Therefore, if the same instrumentation is included in both lists, that instrumentation will be disabled.\n\nLearn more\n\nFor more details, see Node.js auto-instrumentation.\n\nPython\n\nThe following command will create a basic Instrumentation resource that is configured specifically for instrumenting Python services.\n\nBy default, the Instrumentation resource that auto-instruments Python services uses otlp with the http/protobuf protocol (gRPC is not supported at this time). This means that the configured endpoint must be able to receive OTLP over http/protobuf. Therefore, the example uses http://demo-collector:4318, which will connect to the http port of the otlpreceiver of the Collector created in the previous step.\n\nAs of operator v0.67.0, the Instrumentation resource automatically sets OTEL_EXPORTER_OTLP_TRACES_PROTOCOL and OTEL_EXPORTER_OTLP_METRICS_PROTOCOL to http/protobuf for Python services. If you use an older version of the Operator you MUST set these env variables to http/protobuf, or Python auto-instrumentation will not work.\n\nAuto-instrumenting Python logs\n\nBy default, Python logs auto-instrumentation is disabled. If you would like to enable this feature, you must to set the OTEL_LOGS_EXPORTER and OTEL_PYTHON_LOGGING_AUTO_INSTRUMENTATION_ENABLED environment variables as follows:\n\nNote that OTEL_LOGS_EXPORTER must be explicitly set to otlp_proto_http, otherwise it defaults to gRPC.\n\nExcluding auto-instrumentation\n\nBy default, the Python auto-instrumentation ships with many instrumentation libraries. This makes instrumentation easy, but can result in too much or unwanted data. If there are any packages you do not want to instrument, you can set the OTEL_PYTHON_DISABLED_INSTRUMENTATIONS environment variable.\n\nLearn more\n\nSee the Python agent Configuration docs for more details.\n\nGo\n\nThe following command creates a basic Instrumentation resource that is configured specifically for instrumenting Go services.\n\nBy default, the Instrumentation resource that auto-instruments Go services uses otlp with the http/protobuf protocol. This means that the configured endpoint must be able to receive OTLP over http/protobuf. Therefore, the example uses http://demo-collector:4318, which connects to the http/protobuf port of the otlpreceiver of the Collector created in the previous step.\n\nThe Go auto-instrumentation does not support disabling any instrumentation. See the Go Auto-Instrumentation repository for more details.\n\nNow that your Instrumentation object is created, your cluster has the ability to auto-instrument services and send data to an endpoint. However, auto-instrumentation with the OpenTelemetry Operator follows an opt-in model. In order to activate automatic instrumentation, you’ll need to add an annotation to your deployment.\n\nAdd annotations to existing deployments\n\nThe final step is to opt in your services to automatic instrumentation. This is done by updating your service’s spec.template.metadata.annotations to include a language-specific annotation:\n\n.NET: instrumentation.opentelemetry.io/inject-dotnet: \"true\"\n\nGo: instrumentation.opentelemetry.io/inject-go: \"true\"\n\nJava: instrumentation.opentelemetry.io/inject-java: \"true\"\n\nNode.js: instrumentation.opentelemetry.io/inject-nodejs: \"true\"\n\nPython: instrumentation.opentelemetry.io/inject-python: \"true\"\n\nThe possible values for the annotation can be\n\n\"true\" - to inject Instrumentation resource with default name from the current namespace.\n\n\"my-instrumentation\" - to inject Instrumentation CR instance with name \"my-instrumentation\" in the current namespace.\n\n\"my-other-namespace/my-instrumentation\" - to inject Instrumentation CR instance with name \"my-instrumentation\" from another namespace \"my-other-namespace\".\n\n\"false\" - do not inject\n\nAlternatively, the annotation can be added to a namespace, which will result in all services in that namespace to opt-in to automatic instrumentation. See the Operators auto-instrumentation documentation for more details.\n\nOpt-in a Go Service\n\nUnlike other languages’ auto-instrumentation, Go works via an eBPF agent running via a sidecar. When opted in, the Operator will inject this sidecar into your pod. In addition to the instrumentation.opentelemetry.io/inject-go annotation mentioned above, you must also supply a value for the OTEL_GO_AUTO_TARGET_EXE environment variable. You can set this environment variable via the instrumentation.opentelemetry.io/otel-go-auto-target-exe annotation.\n\nThis environment variable can also be set via the Instrumentation resource, with the annotation taking precedence. Since Go auto-instrumentation requires OTEL_GO_AUTO_TARGET_EXE to be set, you must supply a valid executable path via the annotation or the Instrumentation resource. Failure to set this value causes instrumentation injection to abort, leaving the original pod unchanged.\n\nSince Go auto-instrumentation uses eBPF, it also requires elevated permissions. When you opt in, the sidecar the Operator injects will require the following permissions:\n\nTroubleshooting\n\nIf you run into problems trying to auto-instrument your code, here are a few things that you can try.\n\nDid the Instrumentation resource install?\n\nAfter installing the Instrumentation resource, verify that it installed correctly by running this command, where <namespace> is the namespace in which the Instrumentation resource is deployed:\n\nSample output:\n\nDo the OTel Operator logs show any auto-instrumentation errors?\n\nCheck the OTel Operator logs for any errors pertaining to auto-instrumentation by running this command:\n\nWere the resources deployed in the right order?\n\nOrder matters! The Instrumentation resource needs to be deployed before deploying the application, otherwise the auto-instrumentation won’t work.\n\nRecall the auto-instrumentation annotation:\n\nThe annotation above tells the OTel Operator to look for an Instrumentation object in the pod’s namespace. It also tells the Operator to inject Python auto-instrumentation into the pod.\n\nWhen the pod starts up, the annotation tells the Operator to look for an Instrumentation object in the pod’s namespace, and to inject auto-instrumentation into the pod. It adds an init-container to the application’s pod, called opentelemetry-auto-instrumentation, which is then used to injects the auto-instrumentation into the app container.\n\nIf the Instrumentation resource isn’t present by the time the application is deployed, however, the init-container can’t be created. Therefore, if the application is deployed before deploying the Instrumentation resource, the auto-instrumentation will fail.\n\nTo make sure that the opentelemetry-auto-instrumentation init-container has started up correctly (or has even started up at all), run the following command:\n\nWhich should output something like this:\n\nIf the output is missing Created and/or Started entries for opentelemetry-auto-instrumentation, then it means that there is an issue with your auto-instrumentation. This can be the result of any of the following:\n\nThe Instrumentation resource wasn’t installed (or wasn’t installed properly).\n\nThe Instrumentation resource was installed after the application was deployed.\n\nThere’s an error in the auto-instrumentation annotation, or the annotation in the wrong spot — see #4 below.\n\nBe sure to check the output of kubectl get events for any errors, as these might help point to the issue.\n\nIs the auto-instrumentation annotation correct?\n\nSometimes auto-instrumentation can fail due to errors in the auto-instrumentation annotation.\n\nHere are a few things to check for:\n\nIs the auto-instrumentation for the right language? For example, when instrumenting a Python application, make sure that the annotation doesn’t incorrectly say instrumentation.opentelemetry.io/inject-java: \"true\" instead.\n\nIs the auto-instrumentation annotation in the correct location? When defining a Deployment, annotations can be added in one of two locations: spec.metadata.annotations, and spec.template.metadata.annotations. The auto-instrumentation annotation needs to be added to spec.template.metadata.annotations, otherwise it won’t work.\n\nWas the auto-instrumentation endpoint configured correctly?\n\nThe spec.exporter.endpoint attribute of the Instrumentation resource defines where to send data to. This can be an OTel Collector, or any OTLP endpoint. If this attribute is left out, it defaults to http://localhost:4317, which, most likely won’t send telemetry data anywhere.\n\nWhen sending telemetry to an OTel Collector located in the same Kubernetes cluster, spec.exporter.endpoint should reference the name of the OTel Collector Service.\n\nFor example:\n\nHere, the Collector endpoint is set to http://demo-collector.opentelemetry.svc.cluster.local:4317, where demo-collector is the name of the OTel Collector Kubernetes Service. In the above example, the Collector is running in a different namespace from the application, which means that opentelemetry.svc.cluster.local must be appended to the Collector’s service name, where opentelemetry is the namespace in which the Collector resides."
    }
}
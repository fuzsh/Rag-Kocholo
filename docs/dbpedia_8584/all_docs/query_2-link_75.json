{
    "id": "dbpedia_8584_2",
    "rank": 75,
    "data": {
        "url": "https://blogs.gnome.org/tvb/author/tvb/page/2/",
        "read_more_link": "",
        "language": "en",
        "title": "tvb – Page 2 – Tristan's World",
        "top_image": "https://blogs.gnome.org/tvb/files/2016/03/aboriginal-ybd-cropped-1024x831.png",
        "meta_img": "",
        "images": [
            "https://blogs.gnome.org/tvb/files/2016/05/endless-300x162.png",
            "https://blogs.gnome.org/tvb/files/2016/01/logo.png",
            "https://blogs.gnome.org/tvb/files/2016/03/aboriginal-ybd-cropped-1024x831.png",
            "https://blogs.gnome.org/tvb/files/2016/01/logo.png",
            "https://blogs.gnome.org/tvb/files/2016/01/gnome-korean-300x236.png",
            "https://blogs.gnome.org/tvb/files/2016/01/gnome-applications-300x277.png",
            "http://blogs.gnome.org/tvb/files/2013/12/contact-saving.png",
            "http://blogs.gnome.org/tvb/files/2013/12/filter-by-long-email-address-prefix.png",
            "http://blogs.gnome.org/tvb/files/2013/12/filter-by-long-given-name-prefix.png",
            "http://blogs.gnome.org/tvb/files/2013/12/memory-usage-rss.png",
            "http://blogs.gnome.org/tvb/files/2013/10/cursor-positions-alphabetic.png",
            "http://blogs.gnome.org/tvb/files/2013/10/example-browser-en-US.png",
            "http://blogs.gnome.org/tvb/files/2013/10/example-browser-ja-JP.png",
            "http://blogs.gnome.org/tvb/files/2013/08/sponsored-badge-shadow.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2016-06-24T08:41:16+00:00",
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "The last couple of months at Codethink have been an exploration into cross compiling, or rather, cross compiling without the hassle of cross compiling.\n\nIn brief, this post is about an experimental technique for cross building operating systems we’ve come up with, in which we use a virtual machine to run the builds, a cross compiler over distccd to do the heavy lifting and a virtfs 9p mount to share the build directory with the guest build slave.\n\nLets start at the beginning\n\nIn a recent post, I showcased a build of GNOME from scratch. This was created using the ybd build tool to build GNOME from Baserock YAML definitions.\n\nOnce we had a working system, I was asked if I could repeat that for arm. There was already a build story for building arm with Baserock definitions, but getting off the ground to bootstrap it was difficult, and the whole system needs to be built inside an arm emulator or on arm hardware. We started looking at improving the build story for cross compilation.\n\nWe examined a few approaches…\n\nFull traditional cross compile\n\nSome projects, such as yocto or buildroot, provide techniques for cross compiling an entire OS from scratch.\n\nI did a writeup on the complications involved in cross building systems\n\nin this email, but in summary:\n\nThe build process is complex, packages need to be compiled for both the $host and $target all the way up the stack, since modules tend to provide tooling which needs to run on the build host, usually by other modules which depend on it (i.e. icu-config or pkg-config).\n\nBuilding involves trickery, one needs to setup the build environment very specifically so that host tools are run in the build scripts of a given module, this setup varies from module to module depending on the kind of build scripts they use.\n\nThe further up the stack you get, the more modules tend to expect a self hosting (or native) build environment. This means there is a lot of friction in maintaining something like buildroot, it can involve in some cases very strange autogen/configure incantations and in worse cases, downstream patches need to be maintained just to get it to work.\n\nSometimes you even encounter projects which compile C programs that are not distributed, but only used to generating header files and the like during the build, and often these programs are not compiled specifically with $HOST_CC but directly with $CC.\n\nIn any case, this was obviously not a viable option. If one wants to be able to build the bleeding edge on a regular basis, cross compiling all the way up the stack involves too much friction.\n\nThe scratchbox2 project\n\nThis was an avenue which shows promise indeed. The scratchbox project allows one to setup a build environment that is completely tweaked for optimal build performance, using qemu user mode emulation, and much, much more.\n\nI took a look at the internals PDF document and, while I remain impressed, I just don’t think this is the right fit.\n\nThe opening statement of the referred pdf says:\n\nDocumenting a system as complex as Scratchbox 2 is not an easy task.\n\nAnd this is no understatement by any means. Scratchbox’s internal design is extremely difficult to grasp, there are many moving parts and details to this build environment; all of which, at least at face value, I perceive to be potential points of failure.\n\nScratchbox 2 inserts itself in between the qemu user mode emulator and the host operating system and makes decisions, based on configuration data the user needs to provide, about what tooling can be used during a build, and what paths are really going to be accessed.\n\nIn short, scratchbox 2 will sometimes call host tools and run them directly without emulation, and sometimes it will use target tools in user mode emulation, these are managed by a virtual filesystem “view” and both execution modes will see the underlying filesystem in different ways. This way you basically get the fastest solution possible: you run a host cross compiler to build binaries for the target at build time, you run host built coreutils and shells and perl and such at configure time, and if you are well configured, you presumably only ever run target binaries in user emulation when those are tools which were built in your build and need to run during the build.\n\nScratchbox is what you get when you try to get the maximum performance out of a virtualized native build environment. And it is a remarkable feat, but I have reservations about depending on something as complex as this:\n\nWill be able to easily repeat the same build I did today in 10 years from now and easily obtain the same result ?\n\nIf something ever goes wrong, will it always be possible to find an engineer who is easily capable of fixing it ?\n\nWhen creating entirely new builds, how much effort is going to go into setting up and configuring the scratchbox environment ?\n\nBut we’re getting closer, scratchbox2 provides a virtualized environment so that when compiling, your upstream packages believe that they are in a native environment, removing that friction with upstreams and allowing one to upgrade modules without maintaining obscure build instructions and downstream patches.\n\nThe Aboriginal Linux project\n\nThis is the approach we took as a starting point, it’s not the fastest as a build environment but has some values which align quite nicely with our goals.\n\nWhat Aboriginal Linux provides, is mostly just a hand full of shell scripts which allow one to bootstrap for a given architecture quite elegantly.\n\nWhen running the Aboriginal build itself, you just have to tell it what the host and target architectures are, and after the build completes, you end up with the following ingredients:\n\nA statically linked, relocatable $host -> $target cross compiler\n\nThis is interesting, you get a gcc which you can untar on any machine\n\nof the given $host architecture and it will compile for $target\n\nA minimal system image to run on the target\n\nThis includes:\n\nA minimal kernel configured for that arch\n\nBusybox / Toybox for your basic utilities\n\nBash for your basic shell utilities\n\nA native compiler for the target arch\n\ndistcc\n\nAn init.sh to boot the system\n\nA set of scripts to launch your kernel & rootfs under qemu\n\nThese scripts are generated for your specific target arch so they “just work”, and they setup the guest so that distcc is plugged into the cross compiler you just built.\n\nA couple of nice things about Aboriginal\n\nMinimal build requirements\n\nRunning the Aboriginal scripts and getting a build requires:\n\nar, as, nm, ranlib, ld, cc, gcc, g++, objdump, make and sh\n\nThe build starts out by setting up an environment which has access to these, and only these binaries.\n\nThis highly controlled early stage build environment is attractive to me because I think the chances are very high that in 10 years I can launch the same build script and get a working result, this is to be at the very base of our stack.\n\nElegant configuration for bootstrapping targets\n\nSupporting a target architecture in Aboriginal Linux is a bit tricky but once it’s done it should remain reliable and repeatable.\n\nAboriginal keeps some target configuration files which are sourced at various stages of the build in order to determine:\n\nExtra compiler flags for building binutils & gcc and libc\n\nExtra configuration options for the kernel build\n\nMagical obscure qemu incantation for bringing up the OS in qemu\n\nGetting a compiler, kernel and emulator tuple to work is a delicate dance of matching up these configurations exactly. Once that is done however, it should basically keep working forever.\n\nThe adventure begins\n\nWhen I started testing things, I first just wanted a proof of concept, lets see if we can build our stack from within the Aboriginal Linux emulator.\n\nIn my first attempts, I built all the dependencies I needed to run python and git, which are basically the base requirements for running the ybd build tool. This was pretty smooth sailing except that I had to relocate everything I was building into a home directory (read-only root). By the time I started to build baserock’s definitions though I hit a wall. I, quite innocently, wanted to just go ahead and build glibc with Aboriginal’s compiler, thinking no big deal right ? Boy was I wrong.\n\nFirst problem was that glibc, seems to care a great deal about what compiler is used to build it, and the last GPLv2 version of gcc was not going to cut it. Surprisingly, the errors I encountered were not about the compiler not supporting a recent C standard or such, it was explicitly about gcc – glibc has a deep longing desire to be compiled with gcc, and a moderately recent version of it at that.\n\nAboriginal Linux had frozen at the very latest releases (and even git commits) of packages which were still available under GPLv2. It took some convincing but since that toolchain is getting old, Rob Landley agreed that it would be desirable, in a transitional period until llvm is ready, to have an optional build mode allowing one to build Aboriginal Linux using the newer GPLv3 contaminated toolchain.\n\nSo, I set myself to work and, hoping that it would just cost me a weekend (wrong again), cooked up a branch which supports an option to compile Aboriginal with GCC 5.3 and binutils 2.25.1. A report of the changes this branch introduced can be found on the aboriginal mailing list.\n\nIn this time I became intimately acquainted with building compilers and cross compilers. As I mentioned, Aboriginal has a very neat build model which bootstraps everything, running build.sh basically runs like:\n\nCROSS_COMPILER_HOST=i686 SYSIMAGE_TYPE=ext2 ./build.sh armv5l\n\nSo essentially you choose the host arch and target arch (both of which need to have support, i.e. a description file like this one in the aboriginal sources), and then the build runs in stages, sourcing the description files for the said architecture depending on what it’s building along the way.\n\nBecause I spent considerable time on this, and am still sufficiently fascinated, I’m going to give a break down of the process here.\n\n1.) Build host tooling\n\nFirst we create a host directory where we will install tools we want to use during the build, we intentionally symlink to only a few minimal host tools that we require be on your system, these are your host compilers, linkers, a functional shell and make implementation.\n\nThen, we build toybox, busybox, e2fsprogs and distcc, basically any tools which we actually have a chance of running on your host.\n\n2.) Build a stage 1 cross compiler for ${target}\n\nThis is the compiler we’re going to use to build everything that is going to run on your target qemu guest image, including of course the native compiler.\n\nIn this step we build gcc, musl libc and then gcc again, we build gcc again in order to complete the runtime and get libstdc++.\n\nPrevious versions of Aboriginal did not require this second build of gcc, but since GCC folks decided to start using C++, we need a C++ capable cross compiler to build the native compiler.\n\n3.) Build a stage 1 cross compiler for ${host}\n\nThis is the first step towards building your statically linked and relocatable cross compiler, which you’ll want to be plugging into distcc and using on any machine of the given ${host} arch.\n\nThis step is run in exactly the same way as the previous step, except that we are building a cross compiler from your real host -> ${host}\n\n4.) Build the full ${host} -> ${TARGET} cross compiler\n\nIn this stage we will use the cross compiler we built in the previous step, in order to build a cross compiler which runs on ${host} and produces code for ${target}. Neither of these have to be the host arch you are actually running on, you could be building a cross compiler for arm machines to build x86 code on a mips, if you were that sadistic.\n\nIn this second stage compiler the setup is a bit different, we start out by compiling musl libc right away and just build gcc once, since we already have a full compiler and we already have musl libc compiled for the target ${host}.\n\nNote: This is actually called a “Canadian Cross”, and no, I was also surprised to find out that it is not named after a tattoo one gets when joining a fringe religious group in Canada.\n\n5.) Build the native compiler\n\nNow, in exactly the same way we build the Canadian Cross, we’re going to build a native compiler using the stage 1 cross compiler for ${target}.\n\nThis compiler is cross compiled to run on target, and configured to produce code for the same target, so it is a cross compiled native compiler.\n\n6.) Build the kernel and system image\n\nFinally, we use our stage 1 cross compiler again to compile the kernel which will run in qemu, and the root filesystem which has a few things in it. The root filesystem includes toybox, busybox, make, bash and distcc.\n\nWe wrap this up with a few scripts, an init.sh to run on the resulting guest image and a run-emulator.sh script which is generated to just “know” how to properly bring up this guest.\n\nA word on the ccwrap compiler frontend\n\nBefore moving on, I should say a word or two about the compiler frontend ccwrap.c.\n\nI mentioned before that the cross compiler Aboriginal creates is statically linked and relocatable. This is achieved with the said frontend to the compiler tooling, who’s purpose in life is to fight GCC’s desire to hard code itself into the location you’ve compiled it to, tooth and nail.\n\nccwrap counters gcc’s tactics by sitting in place of gcc, cc, g++, c++ and cpp, and figuring out the real location of standard includes and linking stubs, and then calling into the original gcc binaries using a modified set of command line arguments; adding -nostdinc and -nostdlib where necessary, and providing the include paths and stubs to the command line.\n\nThis is a violent process, and gcc puts up a good fight, but the result is that the cross compiler you generate can be untarred anywhere on any host of the correct ${host} architecture, and it will just run and create binaries for ${target}, building and linking against musl libc by default (more on libc further down).\n\nTo port this all to work with new GCC and binutils versions, I needed to find the right patches for gcc and binutils, these are all mostly upstream already in unreleased versions of gcc and binutils. Then I had to reconstruct the building of the stage 1 compilers so that it builds with C++ support, and finally iron out remaining kinks.\n\nThis part was all pretty fun to wrap my head around, hope it was also enjoyable to read about 🙂\n\nThe travel from musl libc to glibc\n\nSo after all that, we have an Aboriginal Linux setup which is capable of building glibc, but the ride is not over ! When building a whole operating system, there is a small chance that someone out there used C++, if we’re going to distribute a glibc based system, we’re probably also going to want to have a libstdc++ that is actually linked against that glibc.\n\nWell, that was what I was thinking, in fact; it runs deeper than this, gcc itself provides libgcc.a and it’s start/end stubs which compliment the host libc’s start/end stubs, but also provides a shared library and a libgcc_eh.so which need to be linked against the host libc.\n\nIn any case, at this stage I was a bit worried that the musl-linked gcc compiler I had might not be capable of building and linking programs against the new glibc. Of course it should work, this is just a standards compliant compiler on one hand and a standard C library on the other, but seeing that gcc / glibc entanglement runs so deeply, we had to be sure.\n\nAfter some time building and rebuilding glibc and gcc on a puny armv5l qemu emulator I found the magic concoction which makes the build pass. For glibc the build pretty much runs smoothly, you first have to install the appropriate linux kernel includes and tell glibc that –with-headers=/usr/include, lest it tread off the beaten path, and go searching obscure host-triple prefixed paths all on it’s own.\n\nTo build the gcc runtimes (so that you get the desired libstdc++), you actually have to build gcc as if you were building a cross compiler.\n\nIn the armv5l transition from musl libc to GNU libc, you would tell it that:\n\n--build=armv5l-thingamajiggie-musleabi --host=armv5l-thingamajiggie-musleabi --target=armv5l-thingamajiggie-gnueabi\n\nWith this setup, it will build all the host tooling using the existing musl libc which our existing compiler is hardwired to use, but when building the runtimes, it will look into ${prefix} and find the glibc we previously compiled, linking the gcc runtimes against the fresh glibc.\n\nAnd yeah, it’s actually important to specify ‘-musleabi’ and ‘-gnueabi’ in the host triples specifically, gcc’s build scripts will parse the triples and behave differently depending on what suffix you give it.\n\nIn my setup, I did not want to use the new compiler, just the runtimes. So I did a custom install of the gcc runtimes in precisely the way that the aboriginal frontend expects to find them.\n\nAt this stage, we can now use environment variables to tell the Aboriginal compiler frontend how to behave, telling it the runtime linker we want to use and where it should look for it’s start stubs and end stubs and such.\n\nOnce we have installed glibc and new gcc runtimes into a chroot staging area on the target emulator, we can now set the following env vars:\n\nCCWRAP_DYNAMIC_LINKER=/lib/ld.so CCWRAP_TOPDIR=/usr\n\nAnd gcc will look for standard headers and library paths in /usr and use the dynamic linker installed by glibc.\n\nNow we can compile C and C++ programs, against glibc and glibc based libstdc++, using our nifty compiler which was built against, and statically linked, to musl libc.\n\nWhat we have done with this ?\n\nThe next step was integrating all of this into the YBD build tool and use the Aboriginal compilers and emulator image to virtually cross-compile baserock definitions from whatever host you are running.\n\nWhat we have now is a build model that looks something like this:\n\nI’ll just take a bit more space here to give a run down of what each component is doing.\n\nYBD Builder\n\nThe YBD builder tool remains mostly unchanged in my downstream branch.\n\nMostly it differs inasmuch as it no longer performs the builds in a chroot sandbox, but instead marshals those builds to slaved Aboriginal guests running in qemu emulators (plural of course, because we want to parallelize the builds as much as dependencies and host resources allow).\n\nWhat YBD does is basically:\n\nClones the sources to be built from git, all sources are normalized into git repositories hosted on the trove.\n\nStages dependencies, i.e. results of previous builds into a sysroot in a temporary directory for a build, this is done in the virtfs staging grounds.\n\nStages the git repository into the build directory\n\nTells a running emulator that it’s time to build\n\nWaits for the result\n\nIf successful, collects the build results and creates an “artifact” (tarball).\n\nAlso, of course YBD parses the YAML definitions format and constructs and navigates a dependency graph.\n\nIPC Interpretor / Modified Init.sh\n\nThis component currently lives in the aboriginal controller repository, but should eventually be migrated into the YBD build tool itself as it makes little sense to have this many moving parts.\n\nThis is essentially some host side shell scripts, and some guest side shell scripts. The guest is launched in a specific way so as to run in the background and listen to commands over the virtio serial port (this IPC needs to be fixed, it’s a shaky thing and should probably be done over the actual network instead of the serial ports).\n\nBuild Sandbox\n\nThe build sandbox is just your basic chroot calling shell script, except that it is a bit peculiar in the way it does things.\n\nIt conditionally stages toybox/busybox if and only if tools are not already found in the staging area\n\nIt stages statically linked binaries only and is perfectly operational in the absence of any libc\n\nWell, not all that peculiar I guess.\n\nVirtfs 9p shared directory\n\nHere is another, really fun part of this experimental build process.\n\nQemu has support for exporting a shared directory which can be accessed by the guest kernel if it is compiled with:\n\nCONFIG_VIRTIO=y CONFIG_VIRTIO_PCI=y CONFIG_VIRTIO_PCI_LEGACY=y CONFIG_NET_9P=y CONFIG_NET_9P_VIRTIO=y\n\nWhen a guest mounts -t 9p the exported directory, qemu will basically just perform the reads and writes on the guests behalf.\n\nMore interestingly, qemu provides a few security models, the basic being passthrough, which just reads and writes using the qemu launching user’s credentials. In any case, qemu can only access the underlying filesystem using the credentials it has. However qemu does provide a security model called “mapped” (or “mapped-file” which we ended up using).\n\nFirstly, of course the shared directory is practical because it allows the host running YBD tool to stage things in the same directory where they will be built by the emulator, but things become interesting when the emulator is installing files under specific uids/gids, or creating device files which should be shipped in the resulting OS – basically anything that normally requires root.\n\nUsing the “mapped-file” security model allows the guest emulator to believe that it, as root, can manipulate the 9p mounted filesystem as root for all intents and purposes. On the actual underlying filesystem that qemu is writing to, everything will be created in mode 0600 and belong to the user running qemu, but extra metadata about the files qemu creates are going to go into corresponding files in a .virtfs_metadata directory.\n\nThe solution we came up with (I had much help in this area from Rob Taylor), was to write a small translation layer which allows us to also interact with the virtfs staging directory on the host side. What this translation layer does is basically:\n\nCollect build results and create “real” tarballs from these results. The regular user is not allowed to create device files or files which belong to root, but it is at least allowed to own a tarball containing such files\n\nThe reverse of the previous; stage the content of a real tarball into a virtfs staging ground, so that files are extracted under the users credentials but the correct virtfs metadata is created so that the guest (build slave) will see the right thing\n\nStage files and directories into the virtfs staging grounds. This part is required for extracted git repositories which we intend to build.\n\nThis way, the whole operating system image can potentially be built from scratch by a regular user on the host.\n\nSummary\n\nAt this, unfinished stage, I have built over 300 of the ~420 components which go into the basic GNOME system using this method of compilation to build for armv5l on my x86_64 laptop. The only build instructions which needed to be changed in order to build these were the base compiler and glibc builds, and a couple of minor changes to get some packages to build on armv5l.\n\nMost of the kinks have been ironed out, I still have to build over 100 high level components and deploy and test the resulting image, but the higher up the stack you get the less problems you tend to encounter, I presume we’re through the worst of it.\n\nPerformance wise, this will never be as fast as scratchbox, however it’s possible that we explore qemu’s user mode emulation at some point. The problem with performance is the more you optimize here, the more nasty hacks you introduce (if say, you want to run host perl while building in the emulator), and the less comprehensive build system you end up with. We will try to keep a nice balance here and prioritize on repeatability and the convenience this can offer in terms of bootstrapping an OS with baserock build instructions on new architectures.\n\nI can say however that regarding performance, libtool is probably next on the chopping block. It serves basically no purpose when building on linux, and building libtool objects costs about 8 to 10 times the time as simply compiling a regular object over distcc.\n\nI will have to put this work down for a while as I have other work landing on my plate which requires attention, so I hope there will be an army of developers to continue this work in my absence 🙂\n\nIf you would like to try and repeat this work, a HOWTO can be found at the bottom of this email. Note that in that email, we had not yet tried the virtfs mapped security model which solves the problem of building as a regular user, however the instructions to get a build off the ground are still valid.\n\nFor now I see this as an interesting research project, we have tried some pretty new and interesting things, I am curious to see where this will lead us.\n\nAnd, special thanks are owed to Rob Landley for giving me pointers along the way while navigating the Aboriginal build system, and for being generally entertaining in #toybox in freenode. Also thanks to Rob Taylor for digging into the qemu sources and coming up with the wild idea of man handling the virtfs mapped metadata.\n\nThis is one of those back to work posts you intend to write and then kick yourself for forgetting… after a few starts this week I finally managed to squeeze in the time to finish this post.\n\nLast week thanks to Codethink, I was able to travel to Brussels and attend the DX Hackfest followed by FOSDEM. What follows is a run down of things we did there.\n\nDay 0\n\nThe Hackfest started on the 27th so I had arrived in Brussels on the 26th bright and early, after around 16 hours of travel including the layover. Feeling hungry, I stumbled out of my hotel room which was downtown by Sainte-Catherine square to fetch a kebab sandwhich. I was thoroughly enjoying my messy pita and fries at a small kebab shack beside the church and by coincidence Juan Pablo was moseying by, admiring the view and taking pictures of the church. With a healthy streak of spicy mayonnaise dripping down my face I called out his name so as not to miss him.\n\nJuan and I had a bit of a chance to talk about what things Glade we could accomplish in our short time in Brussels.\n\nOf course, property bindings came up, which is something that we have wanted for a long time, and Denis Washington had attempted before as his gsoc project.\n\nNo, we did not implement that, but here are a few reasons why we determined it was a no go for a few days of intense hacking:\n\nProperty Sensitivity\n\nGlade has a notion about object properties having a sensitive or insensitive state, this is determined and driven by the widget adaptor of the object type owning a given property. This is typically used in cases where it makes no sense to set a given property, for instance we make the GtkLabel’s wrap mode property insensitive when the label is not set to wrap.\n\nWhen considering that a property can be set up as a binding target, it stands to reason that the bound property editor should also be insensitive, as it makes no sense to give it a value if it’s value is driven by another property. Further, it may also make no sense to allow binding of a property at all if the given target property is meaningless in the current widget’s configuration. So, for instance when setting a GtkButton to use custom content instead of the icon name & label, we would have to undoably clear the binding state of the icon name property as well as it’s value.\n\nCut, Copy & Paste\n\nWhen we cut, copy and paste in Glade we do so with branches of an object hierarchy. Some interesting new cases we would have to handle include:\n\nWhen we paste a hierarchy in which contains a property source/target pair, we should have the new target property re-routed to the copied source object property.\n\nWhen we paste a hierarchy which contains a bound property for which the source object is outside of the pasted hierarchy, we should maintain that binding in the pasted hierarchy so that it continues to reference the same out-of-hierarchy source.\n\nWhen we paste a hierarchy which contains a bound property for which the source object is outside of the pasted hierarchy, but paste it in a separate project / glade file, the originally bound property should be cleared as it refers to a source property that is now in a different project.\n\nSo, after having considered some of the complexities of this, we decided not to be over ambitious and set our sights on lower hanging fruit.\n\nDay 1\n\nOn day one we met up relatively bright and early at the betacowork space where the hackfest took place. Some of the morning was spent looking at the agenda and seeing if there were specific things people wanted to talk about, however, as Glade has a huge todo list it makes little sense to think too far ahead about bright and shiny desirable features so I did not add anything to the agenda.\n\nJuan and I had decided that we can absolutely support glade files which do not always specify the ID field, which GtkBuilder has not been requiring for some time now. The benefit of adding this seemingly mundane feature to Glade is mostly better support for Glade files in the wild. Since the ID field is not required by GtkBuilder anymore, it turns out that many hand written files in the wild can no longer be loaded in Glade.\n\nWe spent around an hour discussing what issues we might face, and decided the path of least resistance would be to always have an ID internally under a special prefix __glade_unnamed_, so we just avoid serialization of the ID of those objects which are unnamed and we invent them as we load files that omit the ID.\n\nFurther, we ensure at all times that if an object is referred to as a property of another object, it must always have an explicit name. We achieve the rollover when running the object selection dialog, if any object is selected as a property of another object; the referred object is given a traditional name like label1 undoably while assigning that reference.\n\nBy the end of the day this was working pretty well…\n\nDay 2\n\nBy now we thought we had pretty much everything covered for the ID’less widgets, and then we encountered the <action-widgets> of GtkDialog and GtkInfoBar.\n\nThese have the unfortunate history of being implemented in an odd way, and I’m not sure how far back this dates, but historically you would denote an action widget by giving it a Response ID integer property and placing the widget in the action area. Since some version of GTK+ 3.x (or possibly even 3.0 ?) we need to refer to these action widgets by their ID in the Glade file and serialize an <action-widgets> node containing those references.\n\nThis should ideally be changed in Glade so that the dialog & infobar have actual references to the action widgets (consequently forcing them to have an ID), and probably have another object selection dialog allowing one to select widgets inside of the GtkDialog / GtkInfoBar hierarchy as action widgets. If however the <action-widgets> semantic is newer than GTK+ 3.0 then it gets quite tricky to make this switch and still support the older semantics of adding buttons with response IDs into the action area.\n\nIn any case, we settled on simply forcing the action widgets to have an ID at save time, without any undo support, for the singular case of GtkDialog/GtkInfoBar action widgets, disturbingly this also includes autosave, and annoyingly modifies the Glade datamodel without any undoable user interaction, but it’s the corner case hack.\n\nAfter this road block, and ironing out a few other obstacles (like serializing the ID’s even if they dont exist when launching the preview, which requires an ID to preview)… we were able to at least nail this feature by the end of Day 2.\n\nI also closed this bug by ensuring we dont handle scroll events in the already scrolling property editor, something we probably should have done many years ago.\n\nAlso, Juan Pablo revived the old school logo (for those who recall the flaming globe logo) in Glade’s workspace so the workspace is a little more fancy. This tribute to the older logo has in fact has been present for years in the loading screen. Unfortunately… there is only a small number of users who work on projects which contain thousands of widgets, so most of you have been missing out on the awesome old logo tribute, which will now appear in it’s full glory in the background of Glade’s workspace.\n\nDay 3\n\nBy now we are getting a bit tired, this post hasn’t covered the more gory details but as we were in Brussels, of course we had the responsibility of sampling every kind of beer. By around 4 pm I was falling asleep at my desk, but before that I was able to make a pass through the GTK+ widget catalog and update it with new deprecations and newly added properties and signals, in some cases updating the custom editors to integrate the new properties nicely. For instance GtkLabel now has a “lines” property which is only sensitive and relevant if ellipsis and word wrapping are enabled simultaneously.\n\nWe also fixed a few more bugs.\n\nFOSDEM\n\nAnd then there was FOSDEM, my first time attending this conference, I was planning on sleeping in but managed to arrive around 10am.\n\nI enjoyed hanging around the booths and mingling mostly, which led to a productive conversation with Andre Klapper about various bug tracking and workflow solutions. I attended some talks in the distros dev room; Sam Thursfield gave his talk about the benefits of using declarative and structured data to represent build and integration instructions in build systems. I also enjoyed some libreoffice talks.\n\nBy the end of the second day and just in the nick of time, I was informed that if I had not gotten a waffle from a proper waffle van at the venue, then I had not “really been to FOSDEM”. I hurried along and was lucky enough to catch one of the last waffles off of a closing van, which was indeed the most delicious waffle I’ve ever tasted.\n\nI guess the conclusion is that waffles are not what FOSDEM is all about, and that’s a good thing – I’d rather be eating a waffle at a conference about free software, than writing free software at a conference about waffles.\n\nThis post is a sort of summary of the work we’ve done on the Addressbook for Evolution Data Server this year at Openismus. As I demonstrated in my previous post, Evolution’s addressbook is now riddled with rich locale sensitive features so I won’t cover the sorting features, you can refer to the other post for that.\n\nUnderstanding Phone Numbers\n\nThis is another really nice feature which I admit has been driving me up the wall for the last year. I’ll try to sum it up one last time here so hopefully I don’t have to ever concern myself too much with the topic again 😉\n\nBefore I get into it, I should start with describing the problem which we’ve addressed (I can’t say solved as I don’t believe there really is any ultimate solution for this problem).\n\nSo, this problem is particular to the implementation of a hand phone device, which implies the following conditions:\n\nUsers will enter any kind of data into the addressbook and call it a phone number. This most typically involves phone numbers entered as local numbers, sometimes it includes fully qualified international numbers for contacts who live abroad, and can also include text such as “ask jenny for her number next time”, as a rule, anything goes.\n\nAddressbooks are typically a collection of vCards, this is a point of interest as using a standard format for contacts allows one to send vCards back and forth, which means that you cannot consider yourself in control of the data you store on your device. Instead a vCard can come from a synchronization of contacts from your laptop’s email client, or passed over bluetooth, the vCard can come from anywhere, containing any data it pleases and calling that a phone number.\n\nWhen initiating an outbound call, the cellular modem firmware will send a string of numbers to the carrier, this will either succeed or fail. It’s tempting at this stage to consider and store the result of this operation, but unclear if the modem firmware will tell you the fully qualified number of the successful outbound call, and unreasonable to attempt a call just to determine such a thing.\n\nWhen the firmware announces an incoming call, there will be a fully qualified E.164 phone number available (E.164 is a standard international phone number format).\n\nTwo obvious use cases now present a difficulty:\n\nLet’s say the user starts entering a phone number somewhere in the UI, perhaps with the intent of initiating a phone call, or just with the intent of searching for a contact. We know that the user entered ‘just about anything’ as a phone number, we know that the vCard might have come from an external source (the same user might not have entered the phone number to begin with), and of course, we want to find the correct contact, and we want to find that contact right now.\n\nLet’s imagine also, just for a moment, that your hand phone implementation might actually receive a phone call (not all of your users are so popular that they actually receive calls, but this is, after all, what your device is for ;-)). So now we have access to a fully qualified E.164 number for the incoming call, and an addressbook full of these vCards, which have, ya know, whatever the user decided to enter as a phone number. We of course want to find the single unique matching contact for that incoming phone number, and we want it even more righter and nower than in the other use case listed above (just in case, you know designers, maybe they want something crazy like displaying the full name of the contact who’s calling you instead of the phone number).\n\nA common trick of the trade to handle these cases is to perform a simple heuristic which involves normalizing the number in your vCards (to strip out spaces and characters such as ‘-‘ and ‘.’) and then perform a simple suffix match against the incoming caller’s fully qualified phone number. This was admittedly a motivation behind our implementation of optimized suffix matching last year.\n\nBut let’s pretend that you’re not satisfied with this heuristic, you know you can do better, and you want to impress the crowd. Well now you can do just that ! assuming of course that you use Evolution’s addressbook in your device architecture (but of course you do, what else would you use ?).\n\nLocale sensitive phone number interpretations\n\nAgain the International Components for Unicode (ICU) libraries come to the rescue, this time with the libphonenumber helper library on google code, which is now an optional dependency for Evolution Data Server (my painful experience compiling this heap of libphonenumber code begs me to make an unrelated comment: somebody school these guys in the ways of Automake, this CMake experience is just about the worst offence you can make to a downstream packager, or someone like me, who just needs to build it).\n\nSo what is a locale sensitive interpretation of a phone number ? Why do we need that ?\n\nSome countries have different call prefixes for outgoing calls to leave the country. I believe this is ‘0’ in North America but it can vary. Even more confusing, take a country like Brazil with many competing cellular carriers, in the case of Brazil you have multiple valid calling prefixes to place a call outside of the country, each one with a different, competing rate.\n\nSome locales, like in all locale sensitive affairs, have different preferences about how a phone number is composed, i.e. do we use a space or a dash or a decimal point as a separator ? do we interpret a trailing number preceded by a decimal as an extension code ? or as the final component of a local phone number ?\n\nSome locales permit entirely different character sets to be used for numbers, and users might very well prefer to enter phone numbers in their native language script rather than entering standard numeric characters.\n\nUsing libphonenumber allows us to make the best possible guess of what the fully qualified E.164 number would be for an arbitrary string entered by the user in a given locale. It also provides us with useful information such as whether the phone number contained an extension code, whether the number had a leading 0 (special case for Italy) and whether the parsed phone number string had a country code. Actually libphonenumber will also make a guess at what the country code would be, but we’re not interested by that feature.\n\nTo leverage this feature in Evolution’s addressbook, one should make use of the following APIs:\n\nThe function e_phone_number_is_supported() can be called at runtime to determine if your installation of Evolution Data Server was compiled with support for phone numbers (as libphonenumber is only a soft dependency, this should be checked).\n\nWhen creating a new addressbook, the ESourceBackendSummarySetup should be used to configure the E_CONTACT_TEL field into the addressbook ‘quick search’ configuration, and it should be configured with the special index E_BOOK_INDEX_PHONE so that interpreted phone numbers will be stored in SQLite for quick phone number matching.\n\nWhether your addressbook is configured for quick phone number searches or not, so long as phone numbers are supported in your build then you have the capacity of leveraging the phone number matching semantics in the regular way with EBookQuery.\n\nThree queries exist which can be used for phone number matching, at differing match strengths.\n\nA typical query that you would use to search for, say equality at the national number strength level, would look like this:\n\nEBookQuery *query = e_book_query_field_test (E_CONTACT_TEL, E_BOOK_QUERY_EQUALS_NATIONAL_PHONE_NUMBER, \"user entered, or E.164 number\");\n\nWith the above query one can use the regular APIs to fetch a contact list, filter a view, or filter a cursor.\n\nAddressbooks on Crystal Meth\n\nOne thing that is really important of course in contacts databases, is speed, lots and lots of speed 🙂\n\nWhile last year we’ve made considerable improvements, and already gained much speed for the addressbook, now contacts are inserted, deleted, modified and as specially fetched righter and nower than ever.\n\nI should start by mentioning that in the last two weeks I’ve committed a complete rewrite of the old SQLite code which handles addressbooks, finally overcoming this bug, and turning what used to look like this (shudders) into something much more intelligible (sigh of relief). The net results in performance, can be observed here.\n\nWhile I’d love to go over the vast improvements I’ve made in the code, and new features added, let’s just go through some highlights of the new benchmarks as I’ve been writing this post for a few hours already 😉\n\nFirst some facts about the addressbook configuration, since that makes a great difference in the meaning of the benchmark output.\n\nE_CONTACT_GIVEN_NAME – Forced fallback search routines (no index)\n\nE_CONTACT_FAMILY_NAME – Prefix Index, Suffix Index\n\nE_CONTACT_EMAIL – Prefix Index, Suffix Index\n\nE_CONTACT_TEL – Prefix Index, Suffix Index\n\nThe red line in the benchmarks is what is now in EDS master (although marked as “Experimental” in the charts). The comparisons are based on addressbooks opened in Direct Read Access mode, i.e. books are opened with e_book_client_connect_direct().\n\nAnd now some of the highlights:\n\nIn the above chart we’re basically seeing the time we save by using prepared statements for batch inserts, instead of generating the same query again and again.\n\nPrefix searches on contact fields which have multiple values (stored in a separate table) such as email addresses and phone numbers, now have good performance for addressbooks containing over 200,000 contacts (I was unable to test 409,600 contacts, as the benchmarks themselves require a lot of memory to keep a set of vCards and EContacts in memory).\n\nHere we see no noticeable change in the speed of fetching contacts with the fallback routine (i.e. no data to compare in SQLite columns, vCard parsing involved).\n\nThis is interesting only because in my new code base, we no longer fetch all contacts into memory and compare the list one by one, but instead install a function into SQLite to perform the fallback routine over the course of a full table scan. This is much more friendly to your memory consumption, and comes at no decrease in the performance of queries which hit the fallback routine.\n\nHere we see the memory usage of the entire benchmark process including the evolution-addressbook-factory process over the course of the benchmarks (each dot from left to right is a measurement taken after each benchmark runs). Note that we have 200,000 contacts and vCards loaded into memory for the sake of the benchmarks to verify results for each speed test (hence the initial spikes at the beginning of the benchmark progress).\n\nThe noticeable drop in memory usage can be attributed to how the new backend is more friendly on system memory when performing fallback search routines.\n\nTake a look at the full benchmarks including the csv file here. I should note that the ‘fetch-all-contacts.png’ benchmark indicated a bug where I was not detecting a special case (a wildcard query which should simply list all results), that has been fixed and the benchmark for it doesn’t apply with current Evolution Data Server master.\n\nAnyway, it’s late and time for pizza 🙂\n\nI hope you’ve all enjoyed the show !"
    }
}
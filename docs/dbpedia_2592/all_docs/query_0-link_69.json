{
    "id": "dbpedia_2592_0",
    "rank": 69,
    "data": {
        "url": "https://deepmind.google/technologies/veo/",
        "read_more_link": "",
        "language": "en",
        "title": "Veo",
        "top_image": "https://lh3.googleusercontent.com/4sPA7BgwdC6KEY9o2KQrQYEldPzpitmmZNM0pfe5bx4WZ7hghc8EBlebilkOn60DDxU20OlU-6xE3dxg8DM9ZkxYDCSdRgYIIHz9eHfcKE08zVQxLg=w1200-h630-n-nu",
        "meta_img": "https://lh3.googleusercontent.com/4sPA7BgwdC6KEY9o2KQrQYEldPzpitmmZNM0pfe5bx4WZ7hghc8EBlebilkOn60DDxU20OlU-6xE3dxg8DM9ZkxYDCSdRgYIIHz9eHfcKE08zVQxLg=w1200-h630-n-nu",
        "images": [
            "https://lh3.googleusercontent.com/3UPL0KnJKzLo3eVxfiGlAEq-AMje_o41v4pOw4wButeGrZOagF5-7SsPrNP7t0U2EKvCgURjXpYD4Kk4XU_oOQ8DefRnBC5DMP-2mAHESlFaUhHQ=w264-h156-n-nu",
            "https://lh3.googleusercontent.com/J44wtQW-XRL5gMfR6mZdToqEAAYp6HFF9ZcATYPavT3g7irE9pyc2pmUliYkrVGH50lyOKRfO9k9Mm1T4PLoHee8E8O_ImiTkC1JJ-Cn_ydCuO5VzA=w264-h156-n-nu",
            "https://lh3.googleusercontent.com/4amJbS1Q5bh_CoBHPAc4NEn0Q13izqrskMETkJl3h2Jdku08GryCCjW6BM59OKj1-Q7-8ZFCWlgu7tIMzjRBIXImy8wlgTOxYgJ88fQvYJTye07C=w264-h156-n-nu",
            "https://lh3.googleusercontent.com/Lzihw4F171DQeSgZ9q0MUONzbt1BkbK1sOgnqvLAV3AUIQQ1UJ4niEXOTgWiiyKZrJaCpE4Q6APwV8RRQj7a86_2yDlbIV6WUzD6S_Gu2mjuZDyVWqo=w264-h156-n-nu",
            "https://lh3.googleusercontent.com/3UPL0KnJKzLo3eVxfiGlAEq-AMje_o41v4pOw4wButeGrZOagF5-7SsPrNP7t0U2EKvCgURjXpYD4Kk4XU_oOQ8DefRnBC5DMP-2mAHESlFaUhHQ=w264-h156-n-nu",
            "https://lh3.googleusercontent.com/J44wtQW-XRL5gMfR6mZdToqEAAYp6HFF9ZcATYPavT3g7irE9pyc2pmUliYkrVGH50lyOKRfO9k9Mm1T4PLoHee8E8O_ImiTkC1JJ-Cn_ydCuO5VzA=w264-h156-n-nu",
            "https://lh3.googleusercontent.com/3U6k20oOgX_zopp5fr-zK0D8v-NAI0Hd-G9eGmGsiZjtbK5mmeZGtw3DOShtNQk3jxrkDkXkOf0JxhD0i_zFKfVFhwKKS7YpUORufWUEDhwVnRR0=w512",
            "https://lh3.googleusercontent.com/J9jWtsWOAN3_oWBfxSK4LfP3MYN5FdSbt1zRxwOV51AeAN88SNNeeQzz3M9DXBmdz4Pd6zYjXXEGwlXpYZS-das1xY6lGP_nP5pN3XwpeD-fNU5orQ=w616",
            "https://lh3.googleusercontent.com/Jlt_8KiodXVEZatcRfpzvPqoO5MsqZkWazuB6Yl2mYkUFOxPRhhRYW3ClM_C_FYaWb0foKHj-oorE_hfqCb0P8xevsjOd3FNrwpLyiSpcLbNrV3N=w1070",
            "https://i.ytimg.com/vi_webp/dKAVFLB75xs/hqdefault.webp",
            "https://lh3.googleusercontent.com/-IJMoak_pZ1ttNhxD31f9LQ0KSF5h_NxIbG2J6Wv4w0BYztxo7C8g87WZ8xOUunOxYmFKRi8Jokg35MLGZTJEf-DQdA_440RghMvGZB1qzZkMgxC=w400-h225-n-nu",
            "https://lh3.googleusercontent.com/YPfZ3rUBlBJ5JJsGrE65a0F-lhwdLr9_qTvHekeDVNkt4tFdYo72v0OFgWX7MeCaBjgmacBAB6MuObvb4kiVmiWFwKSDCbVZUv9Xx9NdUEOiWoeBBw=w400-h225-n-nu",
            "https://lh3.googleusercontent.com/CyPZ9uwh9jBTEAtAD2lMhl83jgwrsqVwyG0zMV83J0hLNDsFaYMsJ8m_cP1yJZTRwKNLh16cudFluLubrCu1a2iGZIbpKQNCtZkdL1CWydUL76lAiQ=w400-h225-n-nu",
            "https://lh3.googleusercontent.com/WxZ_GLt_PyebmygyOGsxp2k9cBj-PSdJc5Nf4QaGLnATswjtQSnPgZIRc0zjgEKOqY8xVkMuh-CmViEFVXS-tVLwXpcmteiWamuTsUyDsqjy0nNZ=w24-h24-n-nu",
            "https://lh3.googleusercontent.com/HvuX0l6UzIGYiveqBCvDWLtnN8g3XO7YyjHNw5ZiskbFs8S77vyR5nc4lKPw64JHfYIuo7iQa6ISkqA2ClSOemDRcau61AViJhLiN4r5cG-R28ZXc7Q=w24-h24-n-nu",
            "https://lh3.googleusercontent.com/dUglGoYw3VFKI6E2rc1QA-yV7KzQVn45G19fHpmESk_yoYAz9MJg21kWRQdZmOynWaAePurAcyzWS2WyN2S0c4DPWd4ECF3Yiv3UtPRyxgqBGHVxoX4=w24-h24-n-nu",
            "https://lh3.googleusercontent.com/TuY1hbTmfWU4khBVemv986-SIxyR7ojQr5LYoTiRsQ5lJBHq06qUS9h0bic_sHEE1zVdE4BtxUN8VTBah-ZJCigqsKHyJfeYQzKqF6pw5SAe-2-1=w24-h24-n-nu"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2024-08-05T00:00:00",
        "summary": "",
        "meta_description": "Veo is our most capable video generation model to date. It generates high-quality, 1080p resolution videos that can go beyond a minute, in a wide range of cinematic and visual styles.",
        "meta_lang": "en",
        "meta_favicon": "https://www.gstatic.com/images/branding/productlogos/google_deepmind/v3/web-96dp/logo_google_deepmind_color_2x_web_96dp.png",
        "meta_site_name": "Google DeepMind",
        "canonical_link": "https://deepmind.google/technologies/veo/",
        "text": "Our most capable generative video model\n\nSign up to try on VideoFX\n\nVeo is our most capable video generation model to date. It generates high-quality, 1080p resolution videos that can go beyond a minute, in a wide range of cinematic and visual styles.\n\nIt accurately captures the nuance and tone of a prompt, and provides an unprecedented level of creative control — understanding prompts for all kinds of cinematic effects, like time lapses or aerial shots of a landscape.\n\nOur video generation model will help create tools that make video production accessible to everyone. Whether you're a seasoned filmmaker, aspiring creator, or educator looking to share knowledge, Veo unlocks new possibilities for storytelling, education and more.\n\nOver the coming weeks some of these features will be available to select creators through VideoFX, a new experimental tool at labs.google. You can join the waitlist now.\n\nIn the future, we’ll also bring some of Veo’s capabilities to YouTube Shorts and other products.\n\nGreater understanding of language and vision\n\nTo produce a coherent scene, generative video models need to accurately interpret a text prompt and combine this information with relevant visual references.\n\nWith advanced understanding of natural language and visual semantics, Veo generates video that closely follows the prompt. It accurately captures the nuance and tone in a phrase, rendering intricate details within complex scenes.\n\nControls for film-making\n\nWhen given both an input video and editing command, like adding kayaks to an aerial shot of a coastline, Veo can apply this command to the initial video and create a new, edited video.\n\nIn addition, it supports masked editing, enabling changes to specific areas of the video when you add a mask area to your video and text prompt.\n\nVeo can also generate a video with an image as input along with the text prompt. By providing a reference image in combination with a text prompt, it conditions Veo to generate a video that follows the image’s style and user prompt’s instructions.\n\nThe model is also able to make video clips and extend them to 60 seconds and beyond. It can do this either from a single prompt, or by being given a sequence of prompts which together tell a story.\n\nPrompts:\n\nA fast-tracking shot through a bustling dystopian sprawl with bright neon signs, flying cars and mist, night, lens flare, volumetric lighting.\n\nA fast-tracking shot through a futuristic dystopian sprawl with bright neon signs, starships in the sky, night, volumetric lighting.\n\nA neon hologram of a car driving at top speed, speed of light, cinematic, incredible details, volumetric lighting.\n\nThe cars leave the tunnel, back into the real world city Hong Kong.\n\nConsistency across video frames\n\nMaintaining visual consistency can be a challenge for video generation models. Characters, objects, or even entire scenes can flicker, jump, or morph unexpectedly between frames, disrupting the viewing experience.\n\nVeo's cutting-edge latent diffusion transformers reduce the appearance of these inconsistencies, keeping characters, objects and styles in place, as they would in real life.\n\nBuilt upon years of video generation research\n\nVeo builds upon years of generative video model work including Generative Query Network (GQN), DVD-GAN, Imagen-Video, Phenaki, WALT, VideoPoet and Lumiere, and also our Transformer architecture and Gemini.\n\nTo help Veo understand and follow prompts more accurately, we have also added more details to the captions of each video in its training data. And to further improve performance, the model uses high-quality, compressed representations of video (also known as latents) so it’s more efficient too. These steps improve overall quality and reduce the time it takes to generate videos.\n\nResponsible by design\n\nIt's critical to bring technologies like Veo to the world responsibly. Videos created by Veo are watermarked using SynthID, our cutting-edge tool for watermarking and identifying AI-generated content, and passed through safety filters and memorization checking processes that help mitigate privacy, copyright and bias risks.\n\nVeo’s future will be informed by our work with leading creators and filmmakers. Their feedback helps us improve our generative video technologies and makes sure they benefit the wider creative community and beyond.\n\nAcknowledgements\n\nThis work was made possible by the exceptional contributions of: Abhishek Sharma, Adams Yu, Ali Razavi, Andeep Toor, Andrew Pierson, Ankush Gupta, Austin Waters, Aäron van den Oord, Daniel Tanis, Dumitru Erhan, Eric Lau, Eleni Shaw, Gabe Barth-Maron, Greg Shaw, Han Zhang, Henna Nandwani, Hernan Moraldo, Hyunjik Kim, Irina Blok, Jakob Bauer, Jeff Donahue, Junyoung Chung, Kory Mathewson, Kurtis David, Lasse Espeholt, Marc van Zee, Matt McGill, Medhini Narasimhan, Miaosen Wang, Mikołaj Bińkowski, Mohammad Babaeizadeh, Mohammad Taghi Saffar, Nando de Freitas, Nick Pezzotti, Pieter-Jan Kindermans, Poorva Rane, Rachel Hornung, Robert Riachi, Ruben Villegas, Rui Qian, Sander Dieleman, Serena Zhang, Serkan Cabi, Shixin Luo, Shlomi Fruchter, Signe Nørly, Srivatsan Srinivasan, Tobias Pfaff, Tom Hume, Vikas Verma, Weizhe Hua, William Zhu, Xinchen Yan, Xinyu Wang, Yelin Kim, Yuqing Du and Yutian Chen.\n\nWe extend our gratitude to Aida Nematzadeh, Alex Cullum, Anja Hauth, April Lehman, Aäron van den Oord, Benigno Uria, Charlie Chen, Charlie Nash, Charline Le Lan, Conor Durkan, Cristian Țăpuș, David Bridson, David Ding, David Steiner, Emanuel Taropa, Evgeny Gladchenko, Frankie Garcia, Gavin Buttimore, Geng Yan, Golnaz Ghiasi, Greg Shaw, Hadi Hashemi, Harsha Vashisht, Hartwig Adam, Huisheng Wang, Jacob Austin, Jacob Kelly, Jacob Walker, Jim Lin, Jonas Adler, Joost van Amersfoort, Jordi Pont-Tuset, Josh V. Dillon, Josh Newlan, Junlin Zhang, Junwhan Ahn, Katie Zhang, Kelvin Xu, Kristian Kjems, Lois Zhou, Luis C. Cobo, Maigo Le, Malcolm Reynolds, Marcus Wainwright, Mary Cassin, Mateusz Malinowski, Matt Smart, Matt Young, Mingda Zhang, Minh Giang, Moritz Dickfeld, Nancy Xiao, Nelly Papalampidi, Nikhil Khadke, Nir Shabat, Oliver Woodman, Ollie Purkiss, Oskar Bunyan, Patrice Oehen, Pauline Luc, Pete Aykroyd, Petko Georgiev, Phil Chen, Rakesh Shivanna, Ramya Ganeshan, Richard Nguyen, RJ Mical, Robin Strudel, Rohan Anil, Sam Haves, Shanshan Zheng, Sholto Douglas, Siddhartha Brahma, Tatiana López, Tejash Desai, Thang Luong, Victor Gomes, Vighnesh Birodkar, Xin Chen, Yaroslav Ganin, Yi-Ling Wang, Yifeng Lu, Yilin Ma, Yori Zwols, Yu Qiao, Yuchen Liang, Yukun Zhu, Yusuf Aytar and Zu Kim for their invaluable partnership in developing and refining key components of this project.\n\nSpecial thanks to Douglas Eck, Oriol Vinyals, Eli Collins, Koray Kavukcuoglu and Demis Hassabis for their insightful guidance and support throughout the research process.\n\nWe also acknowledge the many other individuals who contributed across Google DeepMind and our partners at Google."
    }
}
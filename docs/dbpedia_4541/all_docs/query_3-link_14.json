{
    "id": "dbpedia_4541_3",
    "rank": 14,
    "data": {
        "url": "https://www.californialawreview.org/print/deep-fakes-a-looming-challenge-for-privacy-democracy-and-national-security",
        "read_more_link": "",
        "language": "en",
        "title": "Deep Fakes: A Looming Challenge for Privacy, Democracy, and National Security — California Law Review",
        "top_image": "http://static1.squarespace.com/static/640d6616cc8bbb354ff6ba65/t/644f71da355434081f5cda33/1682928090401/Picture1.png?format=1500w",
        "meta_img": "http://static1.squarespace.com/static/640d6616cc8bbb354ff6ba65/t/644f71da355434081f5cda33/1682928090401/Picture1.png?format=1500w",
        "images": [
            "https://images.squarespace-cdn.com/content/v1/640d6616cc8bbb354ff6ba65/aa8a3aee-35dd-45a5-a1ab-5ac277e6269e/Picture1.png",
            "https://images.squarespace-cdn.com/content/v2/namespaces/memberAccountAvatars/libraries/640d661470d90357bac88cd4/379319c592c54cd3b4db0ed95cb33136/379319c592c54cd3b4db0ed95cb33136.jpeg?format=300w",
            "https://images.squarespace-cdn.com/content/v1/640d6616cc8bbb354ff6ba65/e03cb8d1-4518-4ab5-b714-dbdc5df31c38/Slogan_Dark.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "California Law Review"
        ],
        "publish_date": "2023-04-02T01:02:00-07:00",
        "summary": "",
        "meta_description": "Harmful lies are nothing new. But the ability to distort reality has taken an exponential leap forward with “deep fake” technology. This capability makes it possible to create audio and video of real people saying and doing things they never said or did. Machine learning techniques are escalating th",
        "meta_lang": "en",
        "meta_favicon": "https://images.squarespace-cdn.com/content/v1/640d6616cc8bbb354ff6ba65/e3140403-f712-4147-838c-7646cf68d51c/favicon.ico?format=100w",
        "meta_site_name": "California Law Review",
        "canonical_link": "https://www.californialawreview.org/print/deep-fakes-a-looming-challenge-for-privacy-democracy-and-national-security",
        "text": "Several factors combined to limit the harm from this fakery. First, the genuine image already was in wide circulation and available at its original source. This made it fast and easy to fact-check the fakes. Second, the intense national attention associated with the post-Parkland gun control debate and, especially, the role of students like Emma in that debate, ensured that journalists paid attention to the issue, spending time and effort to debunk the fakes. Third, the fakes were of poor quality (though audiences inclined to believe their message might disregard the red flags).\n\nEven with those constraints, though, many believed the fakes, and harm ensued. Our national dialogue on gun control has suffered some degree of distortion; Emma has likely suffered some degree of anguish over the episode; and other Parkland victims likely felt maligned and discredited. Falsified imagery, in short, has already exacted significant costs for individuals and society. But the situation is about to get much worse, as this Article shows.\n\nTechnologies for altering images, video, or audio (or even creating them from scratch) in ways that are highly -realistic and difficult to detect are maturing rapidly. As they ripen and diffuse, the problems illustrated by the Emma González episode will expand and generate significant policy and legal challenges. Imagine a deep fake video, released the day before an election, making it appear that a candidate for office has made an inflammatory statement. Or what if, in the wake of the Trump-Putin tête-à-tête at Helsinki in 2018, someone circulated a deep fake audio recording that seemed to portray President Trump as promising not to take any action should Russia interfere with certain NATO allies. Screenwriters are already building such prospects into their plotlines.[6] The real world will not lag far behind.\n\nPornographers have been early adopters of the technology, interposing the faces of celebrities into sex videos. This has given rise to the label “deep fake” for such digitized impersonations. We use that label here more broadly, as shorthand for the full range of hyper-realistic digital falsification of images, video, and audio.\n\nThis full range will entail, sooner rather than later, a disturbing array of malicious uses. We are by no means the first to observe that deep fakes will migrate far beyond the pornography context, with great potential for harm.[7] We do, however, provide the first comprehensive survey of these harms and potential responses to them. We break new ground by giving early warning regarding the powerful incentives that deep fakes produce for privacy-destructive solutions.\n\nThis Article unfolds as follows. Part I begins with a description of the technological innovations pushing deep fakes into the realm of hyper-realism and making them increasingly difficult to debunk. It then discusses the amplifying power of social media and the confounding influence of cognitive biases.\n\nPart II surveys the benefits and the costs of deep fakes. The upsides of deep fakes include artistic exploration and educative contributions. The downsides of deep fakes, however, are as varied as they are costly. Some harms are suffered by individuals or groups, such as when deep fakes are deployed to exploit or sabotage individual identities and corporate opportunities. Others impact society more broadly, such as distortion of policy debates, manipulation of elections, erosion of trust in institutions, exacerbation of social divisions, damage to national security, and disruption of international relations. And, in what we call the “liar’s dividend,” deep fakes make it easier for liars to avoid accountability for things that are in fact true.\n\nPart III turns to the question of remedies. We survey an array of existing or potential solutions involving civil and criminal liability, agency regulation, and “active measures” in special contexts like armed conflict and covert action. We also discuss technology-driven market responses, including not just the promotion of debunking technologies, but also the prospect of an alibi service, such as privacy-destructive life logging. We find, in the end, that there are no silver-bullet solutions. Thus, we couple our recommendations with warnings to the public, policymakers, and educators.\n\nI. Technological Foundations of the Deep-Fakes Problem\n\nDigital impersonation is increasingly realistic and convincing. Deep-fake technology is the cutting-edge of that trend. It leverages machine-learning algorithms to insert faces and voices into video and audio recordings of actual people and enables the creation of realistic impersonations out of digital whole cloth.[8] The end result is realistic-looking video or audio making it appear that someone said or did something. Although deep fakes can be created with the consent of people being featured, more often they will be created without it. This Part describes the technology and the forces ensuring its diffusion, virality, and entrenchment.\n\nA. Emergent Technology for Robust Deep Fakes\n\nDoctored imagery is neither new nor rare. Innocuous doctoring of images—such as tweaks to lighting or the application of a filter to improve image quality—is ubiquitous. Tools like Photoshop enable images to be tweaked in both superficial and substantive ways.[9] The field of digital forensics has been grappling with the challenge of detecting digital alterations for some time.[10] Generally, forensic techniques are automated and thus less dependent on the human eye to spot discrepancies.[11] While the detection of doctored audio and video was once fairly straightforward,[12] the emergence of generative technology capitalizing on machine learning promises to shift this balance. It will enable the production of altered (or even wholly invented) images, videos, and audios that are more realistic and more difficult to debunk than they have been in the past. This technology often involves the use of a “neural network” for machine learning. The neural network begins as a kind of tabula rasa featuring a nodal network controlled by a set of numerical standards set at random.[13] Much as experience refines the brain’s neural nodes, examples train the neural network system.[14] If the network processes a broad array of training examples, it should be able to create increasingly accurate models.[15] It is through this process that neural networks categorize audio, video, or images and generate realistic impersonations or alterations.[16]\n\nTo take a prominent example, researchers at the University of Washington have created a neural network tool that alters videos so speakers say something different from what they originally said.[17] They demonstrated the technology with a video of former President Barack Obama (for whom plentiful video footage was available to train the network) that made it appear that he said things that he had not.[18]\n\nBy itself, the emergence of machine learning through neural network methods would portend a significant increase in the capacity to create false images, videos, and audio. But the story does not end there. Enter “generative adversarial networks,” otherwise known as GANs. The GAN approach, invented by Google researcher Ian Goodfellow, brings two neural networks to bear simultaneously.[19] One network, known as the generator, draws on a dataset to produce a sample that mimics the dataset.[20] The other network, the discriminator, assesses the degree to which the generator succeeded.[21] In an iterative fashion, the assessments from the discriminator inform the assessments of the generator. The result far exceeds the speed, scale, and nuance of what human reviewers could achieve.[22] Growing sophistication of the GAN approach is sure to lead to the production of increasingly convincing deep fakes.[23]\n\nThe same is true with respect to generating convincing audio fakes. In the past, the primary method of generating audio entailed the creation of a large database of sound fragments from a source, which would then be combined and reordered to generate simulated speech. New approaches promise greater sophistication, including Google DeepMind’s “Wavenet” model,[24] Baidu’s DeepVoice,[25] and GAN models.[26] Startup Lyrebird has posted short audio clips simulating Barack Obama, Donald Trump, and Hillary Clinton discussing its technology with admiration.[27]\n\nIn comparison to private and academic efforts to develop deep-fake technology, less is currently known about governmental research.[28] Given the possible utility of deep-fake techniques for various government purposes—including the need to defend against hostile uses—it is a safe bet that state actors are conducting classified research in this area. However, it is unclear whether classified research lags behind or outpaces commercial and academic efforts. At the least, we can say with confidence that industry, academia, and governments have the motive, means, and opportunity to push this technology forward at a rapid clip.\n\nB. Diffusion of Deep-Fake Technology\n\nThe capacity to generate persuasive deep fakes will not stay in the hands of either technologically sophisticated or responsible actors.[29] For better or worse, deep-fake technology will diffuse and democratize rapidly.\n\nAs Benjamin Wittes and Gabriella Blum explained in The Future of Violence: Robots and Germs, Hackers and Drones, technologies—even dangerous ones—tend to diffuse over time.[30] Firearms developed for state-controlled armed forces are now sold to the public for relatively modest prices.[31] The tendency for technologies to spread only lags if they require scarce inputs that function (or are made to function) as chokepoints to curtail access.[32] Scarcity as a constraint on diffusion works best where the input in question is tangible and hard to obtain; such as plutonium or highly enriched uranium to create nuclear weapons.[33]\n\nOften though, the only scarce input for a new technology is the knowledge behind a novel process or unique data sets. Where the constraint involves an intangible resource like information, preserving secrecy requires not only security against theft, espionage, and mistaken disclosure, but also the capacity and will to keep the information confidential.[34] Depending on the circumstances, the relevant actors may not want to keep the information to themselves and, indeed, may have affirmative commercial or intellectual motivation to disperse it, as in the case of academics or business enterprises.[35]\n\nConsequently, the capacity to generate deep fakes is sure to diffuse rapidly no matter what efforts are made to safeguard it. The capacity does not depend on scarce tangible inputs, but rather on access to knowledge like GANs and other approaches to machine learning. As the volume and sophistication of publicly available deep-fake research and services increase, user-friendly tools will be developed and propagated online, allowing diffusion to reach beyond experts. Such diffusion has occurred in the past both through commercial and black-market means, as seen with graphic manipulation tools like Photoshop and malware services on the dark web.[36] User-friendly capacity to generate deep fakes likely will follow a similar course on both dimensions.[37]\n\nIndeed, diffusion has begun for deep-fake technology. The recent wave of attention generated by deep fakes began after a Reddit user posted a tool inserting the faces of celebrities into porn videos.[38] Once Fake App, “a desktop app for creating photorealistic faceswap videos made with deep learning,” appeared online, the public adopted it in short order.[39] Following the straightforward steps provided by Fake App, a New York Times reporter created a semi-realistic deep-fake video of his face on actor Chris Pratt’s body with 1,861 images of himself and 1,023 images of Chris Pratt.[40] After enlisting the help of someone with experience blending facial features and source footage, the reporter created a realistic video featuring him as Jimmy Kimmel.[41] This portends the diffusion of ever more sophisticated versions of deep-fake technology.\n\nC. Fueling the Fire\n\nThe capacity to create deep fakes comes at a perilous time. No longer is the public’s attention exclusively in the hands of trusted media companies. Individuals peddling deep fakes can quickly reach a massive, even global, audience. As this section explores, networked phenomena, rooted in cognitive bias, will fuel that effort.[42]\n\nTwenty-five years ago, the practical ability of individuals and organizations to distribute images, audio, and video (whether authentic or not) was limited. In most countries, a handful of media organizations disseminated content on a national or global basis. In the U.S., the major television and radio networks, newspapers, magazines, and book publishers controlled the spread of information.[43] While governments, advertisers, and prominent figures could influence mass media, most were left to pursue local distribution of content. For better or worse, relatively few individuals or entities could reach large audiences in this few-to-many information distribution environment.[44]\n\nThe information revolution has disrupted this content distribution model.[45] Today, innumerable platforms facilitate global connectivity. Generally speaking, the networked environment blends the few-to-many and many-to-many models of content distribution, democratizing access to communication to an unprecedented degree.[46] This reduces the overall amount of gatekeeping, though control still remains with the companies responsible for our digital infrastructure.[47] For instance, content platforms have terms-of-service agreements, which ban certain forms of content based on companies’ values.[48] They experience pressure from, or adhere to legal mandates of, governments to block or filter certain information like hate speech or “fake news.”[49]\n\nAlthough private companies have enormous power to moderate content (shadow banning it, lowering its prominence, and so on), they may decline to filter or block content that does not amount to obvious illegality. Generally speaking, there is far less screening of content for accuracy, quality, or suppression of facts or opinions that some authority deems undesirable.\n\nContent not only can find its way to online audiences, but can circulate far and wide, sometimes going viral both online and, at times, amplifying further once picked up by traditional media. A variety of cognitive heuristics help fuel these dynamics. Three phenomena in particular—the “information cascade” dynamic, human attraction to negative and novel information, and filter bubbles—help explain why deep fakes may be especially prone to going viral.\n\nFirst, consider the “information cascade” dynamic.[50] Information cascades result when people stop paying sufficient attention to their own information, relying instead on what they assume others have reliably determined and then passing that information along.. Because people cannot know everything, they often rely on what others say, even if it contradicts their own knowledge.[51] At a certain point, people stop paying attention to their own information and look to what others know.[52] And when people pass along what others think, the credibility of the original claim snowballs.[53] As the cycle repeats, the cascade strengthens.[54]\n\nSocial media platforms are a ripe environment for the formation of information cascades spreading content of all stripes. From there, cascades can spill over to traditional mass-audience outlets that take note of the surge of social media interest and as a result cover a story that otherwise they might not have.[55] Social movements have leveraged the power of information cascades, including Black Lives Matter activists[56] and the Never Again movement of the Parkland High School students.[57] Arab Spring protesters spread videos and photographs of police torture.[58] Journalist Howard Rheingold refers to positive information cascades as “smart mobs.”[59] But not every mob is smart or laudable, and the information cascade dynamic does not account for such distinctions. The Russian covert action program to sow discord in the United States during the 2016 election provides ample demonstration.[60]\n\nSecond, our natural tendency to propagate negative and novel information may enable viral circulation of deep fakes. Negative and novel information “grab[s] our attention as human beings and [] cause[s] us to want to share that information with others—we’re attentive to novel threats and especially attentive to negative threats.”[61] Data scientists, for instance, studied 126,000 news stories shared on Twitter from 2006 to 2010, using third-party fact-checking sites to classify them as true or false.[62] According to the study, hoaxes and false rumors reached people ten times faster than accurate stories.[63] Even when researchers controlled for differences between accounts originating rumors, falsehoods were 70 percent more likely to get retweeted than accurate news.[64] The uneven spread of fake news was not due to bots, which in fact retweeted falsehoods at the same frequency as accurate information.[65] Rather, false news spread faster due to people retweeting inaccurate news items.[66] The study’s authors hypothesized that falsehoods had greater traction because they seemed more “novel” and evocative than real news.[67] False rumors tended to elicit responses expressing surprise and disgust, while accurate stories evoked replies associated with sadness and trust.[68]\n\nWith human beings seemingly more inclined to spread negative and novel falsehoods, the field is ripe for bots to spur and escalate the spreading of negative misinformation.[69] Facebook estimates that as many as 60 million bots may be infesting its platform.[70] Bots were responsible for a substantial portion of political content posted during the 2016 election.[71] Bots also can manipulate algorithms used to predict potential engagement with content.\n\nNegative information not only is tempting to share, but is also relatively “sticky.” As social science research shows, people tend to credit—and remember—negative information far more than positive information.[72] Coupled with our natural predisposition towards certain stimuli like sex, gossip, and violence, that tendency provides a welcome environment for harmful deep fakes.[73] The Internet amplifies this effect, which helps explain the popularity of gossip sites like TMZ.com.[74] Because search engines produce results based on our interests, they tend to feature more of the same—more sex and more gossip.[75]\n\nThird, filter bubbles further aggravate the spread of false information. Even without the aid of technology, we naturally tend to surround ourselves with information confirming our beliefs. Social media platforms supercharge this tendency by empowering users to endorse and re-share content.[76] Platforms’ algorithms highlight popular information, especially if it has been shared by friends, and surround us with content from relatively homogenous groups.[77] As endorsements and shares accumulate, the chances for an algorithmic boost increase. After seeing friends’ recommendations online, individuals tend to pass on those recommendations to their own networks.[78] Because people tend to share information with which they agree, social media users are surrounded by information confirming their preexisting beliefs.[79] This is what we mean by “filter bubble.”[80]\n\nFilter bubbles can be powerful insulators against the influence of contrary information. In a study of Facebook users, researchers found that individuals reading fact-checking articles had not originally consumed the fake news at issue, and those who consumed fake news in the first place almost never read a fact-check that might debunk it.[81]\n\nTaken together, common cognitive biases and social media capabilities are behind the viral spread of falsehoods and decay of truth. They have helped entrench what amounts to information tribalism, and the results plague public and private discourse. Information cascades, natural attraction to negative and novel information, and filter bubbles provide an all-too-welcoming environment as deep-fake capacities mature and proliferate.\n\nII. Costs and Benefits\n\nDeep-fake technology can and will be used for a wide variety of purposes. Not all will be antisocial; some, in fact, will be profoundly prosocial. Nevertheless, deep fakes can inflict a remarkable array of harms, many of which are exacerbated by features of the information environment explored above.\n\nA. Beneficial Uses of Deep-Fake Technology\n\nHuman ingenuity no doubt will conceive many beneficial uses for deep-fake technology. For now, the most obvious possibilities for beneficial uses fall under the headings of education, art, and the promotion of individual autonomy.\n\n1. Education\n\nDeep-fake technology creates an array of opportunities for educators, including the ability to provide students with information in compelling ways relative to traditional means like readings and lectures. This is similar to an earlier wave of educational innovation made possible by increasing access to ordinary video.[82] With deep fakes, it will be possible to manufacture videos of historical figures speaking directly to students, giving an otherwise unappealing lecture a new lease on life.[83]\n\nCreating modified content will raise interesting questions about intellectual property protections and the reach of the fair use exemption. Setting those obstacles aside, the educational benefits of deep fakes are appealing from a pedagogical perspective in much the same way that is true for the advent of virtual and augmented reality production and viewing technologies.[84]\n\nThe technology opens the door to relatively cheap and accessible production of video content that alters existing films or shows, particularly on the audio track, to illustrate a pedagogical point. For example, a scene from a war film could be altered to make it seem that a commander and her legal advisor are discussing application of the laws of war, when in the original the dialogue had nothing to do with that—and the scene could be re-run again and again with modifications to the dialogue tracking changes to the hypothetical scenario under consideration. If done well, it would surely beat just having the professor asking students to imagine the shifting scenario out of whole cloth.[85]\n\nThe educational value of deep fakes will extend beyond the classroom. In the spring of 2018, Buzzfeed provided an apt example when it circulated a video that appeared to feature Barack Obama warning of the dangers of deep-fake technology itself.[86] One can imagine deep fakes deployed to support educational campaigns by public-interest organizations such as Mothers Against Drunk Driving.\n\n2. Art\n\nThe potential artistic benefits of deep-fake technology relate to its educational benefits, though they need not serve any formal educational purpose. Thanks to the use of existing technologies that resurrect dead performers for fresh roles, the benefits to creativity are already familiar to mass audiences.[87] For example, the startling appearance of the long-dead Peter Cushing as the venerable Grand Moff Tarkin in 2016’s Rogue One was made possible by a deft combination of live acting and technical wizardry. That prominent illustration delighted some and upset others.[88] The Star Wars contribution to this theme continued in The Last Jedi when Carrie Fisher’s death led the filmmakers to fake additional dialogue using snippets from real recordings.[89]\n\nNot all artistic uses of deep-fake technologies will have commercial potential. Artists may find it appealing to express ideas through deep fakes, including, but not limited to, productions showing incongruities between apparent speakers and their apparent speech. Video artists might use deep-fake technology to satirize, parody, and critique public figures and public officials. Activists could use deep fakes to demonstrate their point in a way that words alone could not.\n\n3. Autonomy\n\nJust as art overlaps with education, deep fakes implicate self-expression. But not all uses of deep fakes for self-expression are best understood as art. Some may be used to facilitate “avatar” experiences for a variety of self-expressive ends that might best be described in terms of autonomy.\n\nPerhaps most notably, deep-fake audio technology holds promise to restore the ability of persons suffering from certain forms of paralysis, such as ALS, to speak with their own voice.[90] Separately, individuals suffering from certain physical disabilities might interpose their faces and that of consenting partners into pornographic videos, enabling virtual engagement with an aspect of life unavailable to them in a conventional sense.[91]\n\nThe utility of deep-fake technology for avatar experiences, which need not be limited to sex, closely relates to more familiar examples of technology. Video games, for example, enable a person to have or perceive experiences that might otherwise be impossible, dangerous, or otherwise undesirable if pursued in person. The customizable avatars from Nintendo Wii (known as “Mii”) provide a familiar and non-threatening example. The video game example underscores that the avatar scenario is not always a serious matter, and sometimes boils down to no more and no less than the pursuit of happiness.\n\nDeep-fake technology confers the ability to integrate more realistic simulacrums of one’s own self into an array of media, thus producing a stronger avatar effect. For some aspects of the pursuit of autonomy, this will be a very good thing (as the book and film Ready Player One suggests, albeit with reference to a vision of advanced virtual reality rather than deep-fake technology). Not so for others, however. Indeed, as we describe below, the prospects for the harmful use of deep-fake technology are legion.\n\nB. Harmful Uses of Deep-Fake Technology\n\nHuman ingenuity, alas, is not limited to applying technology to beneficial ends. Like any technology, deep fakes also will be used to cause a broad spectrum of serious harms, many of them exacerbated by the combination of networked information systems and cognitive biases described above.\n\n1. Harm to Individuals or Organizations\n\nLies about what other people have said or done are as old as human society, and come in many shapes and sizes. Some merely irritate or embarrass, while others humiliate and destroy; some spur violence. All of this will be true with deep fakes as well, only more so due to their inherent credibility and the manner in which they hide the liar’s creative role. Deep fakes will emerge as powerful mechanisms for some to exploit and sabotage others.\n\nExploitation\n\nThere will be no shortage of harmful exploitations. Some will be in the nature of theft, such as stealing people’s identities to extract financial or some other benefit. Others will be in the nature of abuse, commandeering a person’s identity to harm them or individuals who care about them. And some will involve both dimensions, whether the person creating the fake so intended or not.\n\nAs an example of extracting value, consider the possibilities for the realm of extortion. Blackmailers might use deep fakes to extract something of value from people, even those who might normally have little or nothing to fear in this regard, who (quite reasonably) doubt their ability to debunk the fakes persuasively, or who fear that any debunking would fail to reach far and fast enough to prevent or undo the initial damage.[92] In that case, victims might be forced to provide money, business secrets, or nude images or videos (a practice known as sextortion) to prevent the release of the deep fakes.[93] Likewise, fraudulent kidnapping claims might prove more effective in extracting ransom when backed by video or audio appearing to depict a victim who is not in fact in the fraudster’s control.\n\nNot all value extraction takes a tangible form. Deep-fake technology can also be used to exploit an individual’s sexual identity for other’s gratification.[94] Thanks to deep-fake technology, an individual’s face, voice, and body can be swapped into real pornography.[95] A subreddit (now closed) featured deep-fake sex videos of female celebrities and amassed more than 100,000 users.[96] As one Reddit user asked, “I want to make a porn video with my ex-girlfriend. But I don’t have any high-quality video with her, but I have lots of good photos.”[97] A Discord user explained that he made a “pretty good” video of a girl he went to high school with, using around 380 photos scraped from her Instagram and Facebook accounts.[98]\n\nThese examples highlight an important point: the gendered dimension of the exploitation of deep fakes. In all likelihood, the majority of victims of fake sex videos will be female. This has been the case for cyber stalking and non-consensual pornography, and likely will be the case for deep-fake sex videos.[99]\n\nOne can easily imagine deep-fake sex videos subjecting individuals to violent, humiliating sex acts. This shows that not all such fakes will be designed primarily, or at all, for the creator’s sexual or financial gratification. Some will be nothing less than cruel weapons meant to terrorize and inflict pain. Of deep-fake sex videos, Mary Anne Franks has astutely said, “If you were the worst misogynist in the world, this technology would allow you to accomplish whatever you wanted.”[100]\n\nWhen victims discover that they have been used in deep-fake sex videos, the psychological damage may be profound—whether or not this was the video creator’s aim. Victims may feel humiliated and scared.[101] Deep-fake sex videos force individuals into virtual sex, reducing them to sex objects. As Robin West has observed, threats of sexual violence “literally, albeit not physically, penetrates the body.”[102] Deep-fake sex videos can transform rape threats into a terrifying virtual reality. They send the message that victims can be sexually abused at whim. Given the stigma of nude images, especially for women and girls, individuals depicted in fake sex videos also may suffer collateral consequences in the job market, among other places, as we explain in more detail below in our discussion of sabotage.[103]\n\nThese examples are but the tip of a disturbing iceberg. Like sexualized deep fakes, imagery depicting non-sexual abuse or violence might also be used to threaten, intimidate, and inflict psychological harm on the depicted victim (or those who care for that person). Deep fakes also might be used to portray someone, falsely, as endorsing a product, service, idea, or politician. Other forms of exploitation will abound.\n\nSabotage\n\nIn addition to inflicting direct psychological harm on victims, deep-fake technology can be used to harm victims along other dimensions due to their utility for reputational sabotage. Across every field of competition—workplace, romance, sports, marketplace, and politics—people will have the capacity to deal significant blows to the prospects of their rivals.\n\nIt could mean the loss of romantic opportunity, the support of friends, the denial of a promotion, the cancellation of a business opportunity, and beyond. Deep-fake videos could depict a person destroying property in a drunken rage. They could show people stealing from a store; yelling vile, racist epithets; using drugs; or any manner of antisocial or embarrassing behavior like sounding incoherent. Depending on the circumstances, timing, and circulation of the fake, the effects could be devastating.\n\nIn some instances, debunking the fake may come too late to remedy the initial harm. For example, consider how a rival might torpedo the draft position of a top pro sports prospect by releasing a compromising deep-fake video just as the draft begins. Even if the video is later doubted as a fake, it could be impossible to undo the consequences (which might involve the loss of millions of dollars) because once cautious teams make other picks, the victim may fall into later rounds of the draft (or out of the draft altogether).[104]\n\nThe nature of today’s communication environment enhances the capacity of deep fakes to cause reputational harm. The combination of cognitive biases and algorithmic boosting increases the chances for salacious fakes to circulate. The ease of copying and storing data online—including storage in remote jurisdictions—makes it much harder to eliminate fakes once they are posted and shared. These considerations combined with the ever-improving search engines increase the chances that employers, business partners, or romantic interests will encounter the fake.\n\nOnce discovered, deep fakes can be devastating to those searching for employment. Search results matter to employers.[105] According to a 2009 Microsoft study, more than 90 percent of employers use search results to make decisions about candidates, and in more than 77 percent of cases, those results have a negative result. As the study explained, employers often decline to interview or hire people because their search results featured “inappropriate photos.”[106] The reason for those results should be obvious. It is less risky and expensive to hire people who do not have the baggage of damaged online reputations. This is especially true in fields where the competition for jobs is steep.[107] There is little reason to think the dynamics would be significantly different with respect to romantic prospects.[108]\n\nDeep fakes can be used to sabotage business competitors. Deep-fake videos could show a rival company’s chief executive engaged in any manner of disreputable behavior, from purchasing illegal drugs to hiring underage prostitutes to uttering racial epithets to bribing government officials. Deep fakes could be released just in time to interfere with merger discussions or bids for government contracts. As with the sports draft example, mundane business opportunities could be thwarted even if the videos are ultimately exposed as fakes.\n\n2. Harm to Society\n\nDeep fakes are not just a threat to specific individuals or entities. They have the capacity to harm society in a variety of ways. Consider the following:\n\nFake videos could feature public officials taking bribes, displaying racism, or engaging in adultery.\n\nPoliticians and other government officials could appear in locations where they were not, saying or doing things that they did not.[109]\n\nFake audio or video could involve damaging campaign material that claims to emanate from a political candidate when it does not.[110]\n\nFake videos could place them in meetings with spies or criminals, launching public outrage, criminal investigations, or both.\n\nSoldiers could be shown murdering innocent civilians in a war zone, precipitating waves of violence and even strategic harms to a war effort.[111]\n\nA deep fake might falsely depict a white police officer shooting an unarmed black man while shouting racial epithets.\n\nA fake audio clip might “reveal” criminal behavior by a candidate on the eve of an election.\n\nFalsified video appearing to show a Muslim man at a local mosque celebrating the Islamic State could stoke distrust of, or even violence against, that community.\n\nA fake video might portray an Israeli official doing or saying something so inflammatory as to cause riots in neighboring countries, potentially disrupting diplomatic ties or sparking a wave of violence.\n\nFalse audio might convincingly depict U.S. officials privately “admitting” a plan to commit an outrage overseas, timed to disrupt an important diplomatic initiative.\n\nA fake video might depict emergency officials “announcing” an impending missile strike on Los Angeles or an emergent pandemic in New York City, provoking panic and worse.\n\nAs these scenarios suggest, the threats posed by deep fakes have systemic dimensions. The damage may extend to, among other things, distortion of democratic discourse on important policy questions; manipulation of elections; erosion of trust in significant public and private institutions; enhancement and exploitation of social divisions; harm to specific military or intelligence operations or capabilities; threats to the economy; and damage to international relations.\n\nDistortion of Democratic Discourse\n\nPublic discourse on questions of policy currently suffers from the circulation of false information.[112] Sometimes lies are intended to undermine the credibility of participants in such debates, and sometimes lies erode the factual foundation that ought to inform policy discourse. Even without prevalent deep fakes, information pathologies abound. But deep fakes will exacerbate matters by raising the stakes for the “fake news” phenomenon in dramatic fashion (quite literally).[113]\n\nMany actors will have sufficient interest to exploit the capacity of deep fakes to skew information and thus manipulate beliefs. As recent actions by the Russian government demonstrate, state actors sometimes have such interests.[114] Other actors will do it as a form of unfair competition in the battle of ideas. And others will do it simply as a tactic of intellectual vandalism and fraud. The combined effects may be significant, including but not limited to the disruption of elections. But elections are vulnerable to deep fakes in a separate and distinctive way as well, as we will explore in the next section.\n\nDemocratic discourse is most functional when debates build from a foundation of shared facts and truths supported by empirical evidence.[115] In the absence of an agreed upon reality, efforts to solve national and global problems become enmeshed in needless first-order questions like whether climate change is real.[116] The large-scale erosion of public faith in data and statistics has led us to a point where the simple introduction of empirical evidence can alienate those who have come to view statistics as elitist.[117] Deep fakes will allow individuals to live in their own subjective realities, where beliefs can be supported by manufactured “facts.” When basic empirical insights provoke heated contestation, democratic discourse has difficulty proceeding. In a marketplace of ideas flooded with deep-fake videos and audio, truthful facts will have difficulty emerging from the scrum.\n\nManipulation of Elections\n\nIn addition to the ability of deep fakes to inject visual and audio falsehoods into policy debates, a deeply convincing variation of a long-standing problem in politics, deep fakes can enable a particularly disturbing form of sabotage: distribution of a damaging, but false, video or audio about a political candidate. The potential to sway the outcome of an election is real, particularly if the attacker is able to time the distribution such that there will be enough window for the fake to circulate but not enough window for the victim to debunk it effectively (assuming it can be debunked at all). In this respect, the election scenario is akin to the NBA draft scenario described earlier. Both involve decisional chokepoints: narrow windows of time during which irrevocable decisions are made, and during which the circulation of false information therefore may have irremediable effects.\n\nThe 2017 election in France illustrates the perils. In this variant of the operation executed against the Clinton campaign in the United States in 2016, the Russians mounted a covert-action program that blended cyber-espionage and information manipulation in an effort to prevent the election of Emmanuel Macron as President of France in 2017.[118] The campaign included theft of large numbers of digital communications and documents, alteration of some of those documents in hopes of making them seem problematic, and dumping a lot of them on the public alongside aggressive spin. The effort ultimately fizzled for many reasons, including: poor tradecraft that made it easy to trace the attack; smart defensive work by the Macron team, which planted their own false documents throughout their own system to create a smokescreen of distrust; a lack of sufficiently provocative material despite an effort by the Russians to engineer scandal by altering some of the documents prior to release; and mismanagement of the timing of the document dump, which left enough time for the Macron team and the media to discover and point out all these flaws.[119]\n\nIt was a bullet dodged, yes, but a bullet nonetheless. The Russians could have acted with greater care, both in terms of timing and tradecraft. They could have produced a more-damning fake document, for example, dropping it just as polls opened. Worse, they could have distributed a deep fake consisting of seemingly-real video or audio evidence persuasively depicting Macron speaking or doing something shocking.\n\nThis version of the deep-fake threat is not limited to state-sponsored covert action. States may have a strong incentive to develop and deploy such tools to sway elections, but there will be no shortage of non-state actors and individuals motivated to do the same. The limitation on such interventions has much more to do with means than motive, as things currently stand. The diffusion of the capacity to produce high-quality deep fakes will erode that limitation, empowering an ever-widening circle of participants to inject false-but-compelling information into a ready and willing information-sharing environment. If executed and timed well enough, such interventions are bound to tip an outcome sooner or later—and in a larger set of cases they will at least cast a shadow of illegitimacy over the election process itself.\n\nEroding Trust in Institutions\n\nDeep fakes will erode trust in a wide range of both public and private institutions and such trust will become harder to maintain. The list of public institutions for which this will matter runs the gamut, including elected officials, appointed officials, judges, juries, legislators, staffers, and agencies. One can readily imagine, in the current climate especially, a fake-but-viral video purporting to show FBI special agents discussing ways to abuse their authority to pursue a Trump family member. Conversely, we might see a fraudulent video of ICE officers speaking with racist language about immigrants or acting cruelly towards a detained child. Particularly where strong narratives of distrust already exist, provocative deep fakes will find a primed audience.\n\nPrivate sector institutions will be just as vulnerable. If an institution has a significant voice or role in society, whether nationally or locally, it is a potential target. More to the point, such institutions already are subject to reputational attacks, but soon will have to face abuse in the form of deep fakes that are harder to debunk and more likely to circulate widely. Religious institutions are an obvious target, as are politically-engaged entities ranging from Planned Parenthood to the NRA.[120]\n\nExacerbating Social Divisions\n\nThe institutional examples relate closely to significant cleavages in American society involving identity and policy commitments. Indeed, this is what makes institutions attractive targets for falsehoods. As divisions become entrenched, the likelihood that opponents will believe negative things about the other side—and that some will be willing to spread lies towards that end—grows.[121] However, institutions will not be the only ones targeted with deep fakes. We anticipate that deep fakes will reinforce and exacerbate the underlying social divisions that fueled them in the first place.\n\nSome have argued that this was the actual—or at least the original—goal of the Russian covert action program involving intervention in American politics in 2016. The Russians may have intended to enhance American social divisions as a general proposition, rendering us less capable of forming consensus on important policy questions and thus more distracted by internal squabbles.[122] Texas is illustrative.[123] Russia promoted conspiracy theories about federal military power during the innocuous, “Jade Helm” training exercises.[124] Russian operators organized an event in Houston to protest radical Islam and a counter-protest of that event;[125] they also promoted a Texas independence movement.[126] Deep fakes will strengthen the hand of those who seek to divide us in this way.\n\nDeep fakes will not merely add fuel to the fire sustaining divisions. In some instances, the emotional punch of a fake video or audio might accomplish a degree of mobilization-to-action that written words alone could not.[127] Consider a situation of fraught, race-related tensions involving a police force and a local community. A sufficiently inflammatory deep fake depicting a police officer using racial slurs, shooting an unarmed person, or both could set off substantial civil unrest, riots, or worse. Of course, the same deep fake might be done in reverse, falsely depicting a community leader calling for violence against the police. Such events would impose intangible costs by sharpening societal divisions, as well as tangible costs for those tricked into certain actions and those suffering from those actions.\n\nUndermining Public Safety\n\nThe foregoing example illustrates how a deep fake might be used to enhance social divisions and to spark actions—even violence—that fray our social fabric. But note, too, how deep fakes can undermine public safety.\n\nA century ago, Justice Oliver Wendell Holmes warned of the danger of falsely shouting fire in a crowded theater.[128] Now, false cries in the form of deep fakes go viral, fueled by the persuasive power of hyper-realistic evidence in conjunction with the distribution powers of social media.[129] The panic and damage Holmes imagined may be modest in comparison to the potential unrest and destruction created by a well-timed deep fake.[130]\n\nIn the best-case scenario, real public panic might simply entail economic harms and hassles. In the worst-case scenario, it might involve property destruction, personal injuries, and/or death. Deep fakes increase the chances that someone can induce a public panic.\n\nThey might not even need to capitalize on social divisions to do so. In early 2018, we saw a glimpse of how a panic might be caused through ordinary human error when an employee of Hawaii’s Emergency Management Agency issued a warning to the public about an incoming ballistic missile.[131] Less widely noted, we saw purposeful attempts to induce panic when the Russian Internet Research Agency mounted a sophisticated and well-resourced campaign to create the appearance of a chemical disaster in Louisiana and an Ebola outbreak in Atlanta.[132] There was real but limited harm in both of these cases, though the stories did not spread far because they lacked evidence and the facts were easy to check.\n\nWe will not always be so lucky as malicious attempts to spread panic grow. Deep fakes will prove especially useful for such disinformation campaigns, enhancing their credibility. Imagine if the Atlanta Ebola story had been backed by compelling fake audio appearing to capture a phone conversation with the head of the Centers for Disease Control and Prevention describing terrifying facts and calling for a cover-up to keep the public calm.\n\nUndermining Diplomacy\n\nDeep fakes will also disrupt diplomatic relations and roil international affairs, especially where the fake is circulated publicly and galvanizes public opinion. The recent Saudi-Qatari crisis might have been fueled by a hack that injected fake stories with fake quotes by Qatar’s emir into a Qatari news site.[133] The manipulator behind the lie could then further support the fraud with convincing video and audio clips purportedly gathered by and leaked from some unnamed intelligence agency.\n\nA deep fake put into the hands of a state’s intelligence apparatus may or may not prompt a rash action. After all, the intelligence agencies of the most capable governments are in a good position to make smart decisions about what weight to give potential fakes. But not every state has such capable institutions, and, in any event, the real utility of a deep fake for purposes of sparking an international incident lies in inciting the public in one or more states to believe that something shocking really did occur or was said. Deep fakes thus might best be used to box in a government through inflammation of relevant public opinion, constraining the government’s options, and perhaps forcing its hand in some particular way. Recalling the concept of decisional chokepoints, for example, a well-timed deep fake calculated to inflame public opinion might be circulated during a summit meeting, making it politically untenable for one side to press its agenda as it otherwise would have, or making it too costly to reach and announce some particular agreement.\n\nJeopardizing National Security\n\nThe use of deep fakes to endanger public safety or disrupt international relations can also be viewed as harming national security. But what else belongs under that heading?\n\nMilitary activity—especially combat operations—belongs under this heading as well, and there is considerable utility for deep fakes in that setting. Most obviously, deep fakes have utility as a form of disinformation supporting strategic, operational, or even tactical deception. This is a familiar aspect of warfare, famously illustrated by the efforts of the Allies in Operation Bodyguard to mislead the Axis regarding the location of what became the D-Day invasion of June 1944.[134] In that sense, deep fakes will be (or already are) merely another instrument in the toolkit for wartime deception, one that combatants will both use and have used against them.\n\nCritically, deep fakes may prove to have special impact when it comes to the battle for hearts and minds where a military force is occupying or at least operating amidst a civilian population, as was the case for the U.S. military for many years in Iraq and even now in Afghanistan. In that context, we have long seen contending claims about civilian casualties—including, at times, the use of falsified evidence to that effect. Deep fakes are certain to be used to make such claims more credible. At times, this will merely have a general impact in the larger battle of narratives. Nevertheless, such general impacts can matter a great deal in the long term and can spur enemy recruitment or enhance civilian support to the enemy. And, at times, it will spark specific violent reactions. One can imagine circulation of a deep-fake video purporting to depict American soldiers killing local civilians and seeming to say disparaging things about Islam in the process, precipitating an attack by civilians or even a host-state soldier or police officer against nearby U.S. persons.\n\nDeep fakes pose similar problems for the activities of intelligence agencies. The experience of the United States since the Snowden leaks in 2013 demonstrates that the public, both in the United States and abroad, can become very alarmed about reports that the U.S. Intelligence Community has a particular capability, and this can translate into significant pressure to limit or abolish that capability both from an internal U.S. perspective and in terms of diplomatic relations. Whether those pressures resulted in changes that went too far in the case of the Snowden revelations is not our concern here. Our point is that this dynamic could be exploited if one wished to create distractions for an intelligence agency or generate conditions that would lead a society to limit what that agency is authorized to do. None of that would be easily done, but deep fakes make the prospect of a strategic operation to bedevil a competing state’s intelligence services more plausible.[135]\n\nThe list of potential national security harms associated with deep fakes can go on, depending on one’s definition of national security. In a recent report, the Belfer Center highlighted the national security implications of sophisticated forgeries.[136] An adversary could acquire real and sensitive documents through cyber-espionage and release the real documents along with forgeries. Deep-fake video and audio could be “leaked” to verify the forgeries. Foreign policy could be changed in response to convincing deep fakes and forgeries.[137]\n\nUndermining Journalism\n\nAs the capacity to produce deep fakes spreads, journalists increasingly will encounter a dilemma: when someone provides video or audio evidence of a newsworthy event, can its authenticity be trusted? That is not a novel question, but it will be harder to answer as deep fakes proliferate. News organizations may be chilled from rapidly reporting real, disturbing events for fear that the evidence of them will turn out to be fake.[138]\n\nIt is not just a matter of honest mistakes becoming more frequent: one can expect instances in which someone tries to trap a news organization using deep fakes. We already have seen many examples of “stings” pursued without the benefit of deep-fake technology.[139] Convincing deep fakes will make such stings more likely to succeed. Media entities may grow less willing to take risks in that environment, or at least less willing to do so in timely fashion. Without a quick and reliable way to authenticate video and audio, the press may find it difficult to fulfill its ethical and moral obligation to spread truth.\n\nThe Liar’s Dividend: Beware the Cry of Deep-Fake News\n\nWe conclude our survey of the harms associated with deep fakes by flagging another possibility, one different in kind from those noted above. In each of the preceding examples, the harm stems directly from the use of a deep fake to convince people that fictional things really occurred. But not all lies involve affirmative claims that something occurred (that never did): some of the most dangerous lies take the form of denials.\n\nDeep fakes will make it easier for liars to deny the truth in distinct ways. A person accused of having said or done something might create doubt about the accusation by using altered video or audio evidence that appears to contradict the claim. This would be a high-risk strategy, though less so in situations where the media is not involved and where no one else seems likely to have the technical capacity to expose the fraud. In situations of resource-inequality, we may see deep fakes used to escape accountability for the truth.\n\nDeep fakes will prove useful in escaping the truth in another equally pernicious way. Ironically, liars aiming to dodge responsibility for their real words and actions will become more credible as the public becomes more educated about the threats posed by deep fakes. Imagine a situation in which an accusation is supported by genuine video or audio evidence. As the public becomes more aware of the idea that video and audio can be convincingly faked, some will try to escape accountability for their actions by denouncing authentic video and audio as deep fakes. Put simply: a skeptical public will be primed to doubt the authenticity of real audio and video evidence. This skepticism can be invoked just as well against authentic as against adulterated content.\n\nHence what we call the liar’s dividend: this dividend flows, perversely, in proportion to success in educating the public about the dangers of deep fakes. The liar’s dividend would run with the grain of larger trends involving truth skepticism. Most notably, recent years have seen mounting distrust of traditional sources of news. That distrust has been stoked relentlessly by President Trump and like-minded sources in television and radio; the mantra “fake news” has become an instantly recognized shorthand for a host of propositions about the supposed corruption and bias of a wide array of journalists, and a useful substitute for argument when confronted with damaging factual assertions. Whether one labels this collection of attitudes postmodernist or nihilist,[140] the fact remains that it has made substantial inroads into public opinion in recent years.\n\nAgainst that backdrop, it is not difficult to see how “fake news” will extend to “deep-fake news” in the future. As deep fakes become widespread, the public may have difficulty believing what their eyes or ears are telling them—even when the information is real. In turn, the spread of deep fakes threatens to erode the trust necessary for democracy to function effectively.[141]\n\nThe combination of truth decay and trust decay accordingly creates greater space for authoritarianism. Authoritarian regimes and leaders with authoritarian tendencies benefit when objective truths lose their power.[142] If the public loses faith in what they hear and see and truth becomes a matter of opinion, then power flows to those whose opinions are most prominent—empowering authorities along the way.[143]\n\nCognitive bias will reinforce these unhealthy dynamics. As Part II explored, people tend to believe facts that accord with our preexisting beliefs.[144] As research shows, people often ignore information that contradicts their beliefs and interpret ambiguous evidence as consistent with their beliefs.[145] People are also inclined to accept information that pleases them when given the choice.[146] Growing appreciation that deep fakes exist may provide a convenient excuse for motivated reasoners to embrace these dynamics, even when confronted with information that is in fact true.\n\nIII. What Can Be Done? Evaluating Technical, Legal, and Market Responses\n\nWhat can be done to ameliorate these harms? Part III reviews various possibilities. To start, we explore the prospects for technological solutions that would facilitate the detection and debunking of deep fakes. We then describe current and potential proposals for criminal and civil liability. With law in mind, we discuss the role of regulators and identify ways in which the government might respond to deep fakes. In the shadow of these possibilities, we anticipate new services the market might spawn to protect individuals from harm associated with deep fakes—and the considerable threat to privacy such services themselves might entail.\n\nA. Technological Solutions\n\nTechnology has given us deep fakes – but might it also provide us with a capacity for debunking them and limiting their harmful potential? An efficient and generally effective method for rapid detection of deep fakes would go far toward resolving this topic as a matter of pressing public-policy concern. Unfortunately, the challenges are daunting. For example, detection software would have to keep pace with innovations in deep-fake technology to retain efficacy. Moreover, if such technology existed and could be deployed through social media platforms, it would only reduce the systemic harms described above, but by no means eliminate them. Such developments might not protect individuals from deep fakes involving narrow or even isolated distribution.[147] Further, detection software might not disabuse certain people’s faith in deep fakes, especially those under the profound sway of cognitive bias. At the least though, the impact of harmful deep fakes might be cabined while beneficial uses could continue unabated.\n\nAt any rate, it is far from clear that such technology will emerge in the near future. There are a number of projects—academic and corporate—aimed at creating counterfeit-proof systems for authenticating content or otherwise making it easier to confirm credible provenance.[148] Such systems, however, are tailored to particular products rather than video or audio technologies generally. They will therefore have only limited use until one program becomes ubiquitous and effective enough for dominant platforms to incorporate them into their content-screening systems—and, indeed, to make use of them mandatory for posting. Additionally, these systems will have to withstand users’ efforts to bypass them.\n\nFor now, we are left to seek a generally applicable technology that can detect manipulated content without an expectation that the content comes with an internal certification. Professor Hany Farid, the pioneer of PhotoDNA, a technology that identifies and blocks child pornography, warns: “We’re decades away from having forensic technology that . . . [could] conclusively tell a real from a fake . . . If you really want to fool the system you will start building into the deepfake ways to break the forensic system.”[149] The defense, in short, is currently faring poorly in the deep-fake technology arms race.\n\nAs problems associated with deep fakes begin to accumulate, we might expect developments that could alter the current balance of power between technologies that create deep fakes and those that detect them. For example, growing awareness of the problem might produce the conditions needed for grantmaking agencies like the National Science Foundation and the Defense Advanced Research Projects Agency (DARPA) to begin steering funds toward scalable detection systems that can be commercialized or even provided freely. DARPA has an initial project in the form of a contest pitting GAN methods for generating deep fakes against would-be detection algorithms. The DARPA project manager is skeptical about the prospects for detection, however, given current technical capacities.[150]\n\nEmerging market forces might encourage companies to invest in such capabilities on their own or in collaboration with each other and with academics (a possibility that we revisit below). For now, however, it would be foolish to trust that technology will deliver a debunking solution that is scalable and reliable enough to minimize the harms deep fakes might cause.\n\nB. Legal Solutions\n\nIf technology alone will not save us, might the law? Would a combination of criminal and civil liability meaningfully deter and redress the harms that deep fakes seem poised to cause? We examine the possibilities under existing and potential law.\n\n1. Problems with an Outright Ban\n\nNo current criminal law or civil liability regime bans the creation or distribution of deep fakes. A threshold question is whether such a law would be normatively appealing and, if so, constitutionally permissible.\n\nA flat ban is not desirable because digital manipulation is not inherently problematic. Deep fakes exact significant harm in certain contexts but not in all. A prohibition of deep fakes would bar routine modifications that improve the clarity of digital content. It would chill experimentation in diverse fields, from history and science to art and education.\n\nCrafting a law prohibiting destructive applications of deep-fake technology while excluding beneficial ones would be difficult, but perhaps not impossible. For example, what if a law required proof of a deep-fake creator’s intent to deceive and evidence of serious harm as a way to reduce concerns about chilling public discourse? Under such a proposal, concerns over speech still remain. The very existence of a general prohibition of deep fakes, even with those guardrails, would cast a significant shadow, potentially diminishing expression crucial to self-governance and democratic culture. The American free speech tradition warns against government having the power to pick winners and losers in the realm of ideas because it will “tend to act on behalf of the ideological powers that be.”[151] As James Weinstein notes, we should be especially wary of entrusting government officials with the power to determine the veracity of factual claims “made in the often highly ideological context of public discourse . . . .”[152] A deep-fakes ban would raise the specter of penalties for parodies of would-be or current office holders.\n\nAlthough self-serving prosecutions are not inevitable, they are a real possibility.[153] Dislike of minority or unpopular viewpoints, combined with ambiguity surrounding a deep-fake creator’s intent, might result in politicized enforcement.[154] This might inhibit engagement in political discourse specifically, and in democratic culture more generally.[155] The “‘risk of censorious selectivity by prosecutors’” [will] . . . distort perspectives made available” to the public.[156] It is far better to forego an outright ban of deep fakes than to run the risk of its abuse.\n\nEven if these normative concerns could be overcome, it is unlikely that a flat ban on deep fakes could withstand constitutional challenge. Deep fakes implicate freedom of expression, even though they involve intentionally false statements.[157] In the landmark 1964 decision New York Times v. Sullivan,[158] the Supreme Court held that false speech enjoys constitutional protection insofar as its prohibition would chill truthful speech.[159]\n\nIn 2012, in United States v. Alvarez,[160] the Court went even further. In the plurality and concurring opinions, the Court concluded that “falsity alone” does not remove expression from First Amendment protection.[161] As Justice Kennedy’s plurality noted, falsehoods generally warrant protection because they inspire rebuttal and “reawaken respect” for valuable ideas in public discourse.[162] Central to this point is faith in the public’s willingness to counter lies and engage in reasoned discourse.\n\nWhile all nine Justices agreed that the harmful effect of false factual statements could be regulated, they differed in the particulars.[163] The plurality opinion took the position that false statements can be proscribed if the speakers intended to cause “legally cognizable harm” of a kind traditionally understood as falling outside the First Amendment’s protection.[164] The concurrence posited that a law aimed at regulating harm-causing falsehoods may be permissible if it does not disproportionately damage First Amendment interests.[165] The dissent would have denied First Amendment protection to false factual statements that inflict harm and serve no legitimate purpose.[166] The court reached consensus that regulation of false statements involving history, politics, literature, and other matters of public concern requires strict scrutiny review.[167]\n\nThe opinions in Alvarez, taken together, would seem to preclude a sweeping ban on deep fakes while leaving considerable room for carefully tailored prohibitions of certain harmful deep fakes. As the plurality underscored in Alvarez, certain categories of speech are not covered by the First Amendment due to their propensity to bring about serious harms and their slight contribution to free speech values.[168] Some deep fakes will fall into those categories and thus could be subject to regulation. This includes defamation of private persons, fraud, true threats, and the imminent-and-likely incitement of violence.[169] Speech integral to criminal conduct like extortion, blackmail, and perjury has long been understood to enjoy no First Amendment protection.[170]\n\nConsider as an illustration laws banning the impersonation of government officials (such as law enforcement officers or agency officials). As Helen Norton insightfully explains, these statutes are “largely uncontroversial as a First Amendment matter in great part because they address real (if often intangible) harm to the public as well as to the individual target.”[171] Lies about the source of speech—whether a public official is actually speaking—do not serve free speech values.[172] Quite the opposite, they deny listeners the ability to assess the quality and credibility of the speech, undermining democratic self-governance and the search for truth.[173] From a normative perspective, therefore, a surgical approach to criminal and civil liability may result in a more attractive balance of costs and benefits than a deep-fake ban perspective. And so we turn now to a discussion of specific possibilities, starting with civil liability.\n\n2. Specific Categories of Civil Liability\n\nGiven that deep fakes cannot and should not be banned on a generalized basis, the question remains whether their creators and distributors in particular contexts should be subject to civil liability for the harms they cause. This section reviews relevant existing laws and possible improvements.\n\nThreshold Obstacles\n\nBefore reviewing the prospects for particular theories of liability, we note two threshold problems.\n\nThe first involves attribution. Civil liability cannot ameliorate harms caused by deep fakes if plaintiffs cannot tie them to their creators. The attribution problem arises in the first instance because the metadata relevant for ascertaining a deep fake’s provenance might be insufficient to identify the person who generated it. It arises again when the creator or someone else posts a deep fake on social media or otherwise injects it into the marketplace of information. A careful distributor of a deep fake may take pains to be anonymous, including but not limited to using technologies like Tor.[174] When these technologies are employed, the IP addresses connected to posts may be impossible to find and trace back to the responsible parties.[175] In such cases, a person or entity aggrieved by a deep fake may have no practical recourse against its creator, leaving only the possibility of seeking a remedy from the owner of platforms that enabled circulation of the content.\n\nA second obstacle arises when the creator of the deep fake—or the platform circulating it—is outside the United States and thus beyond the effective reach of US legal process, or in a jurisdiction where local legal action is unlikely to be effective. Therefore, even if attribution is known, it still may be impossible to use civil remedies effectively. While limitations of civil liability exist in many settings, the global nature of online platforms makes it a particular problem in the deep-fake context.\n\nMoreover, regardless of whether perpetrators can be identified or reside in the US, civil suits are expensive. Victims usually bear the heavy costs of bringing civil claims and may be hesitant to initiate lawsuits if deep-fake generators are effectively judgment-proof.[176] Worse, the “Streisand Effect” is likely to overhang the decision to sue when the deep fake is embarrassing or reputationally harmful. Lawsuits attract publicity; unless the victim is permitted to sue under a pseudonym, filing a claim may exacerbate the victim’s harm.[177]\n\nSuing the Creators of Deep Fakes\n\nThreshold attribution and liability hurdles are not always fatal for would-be plaintiffs. When a victim decides to sue the creator of a deep fake, several bodies of law come into play, including intellectual property and tort law.\n\nFirst, consider copyright law. Some deep fakes exploit copyrighted content, opening the door to monetary damages and a notice-and-takedown procedure that can result in removal of the offending content.[178] A copyright owner is the person who took a photograph. Thus, if a deep fake involves a photo that the victim took of herself, the victim might have a copyright claim against the creator of the deep fake.[179]\n\nThe prospects for success, however, are uncertain. A court will have to determine whether the deep fake is a “fair use” of the copyrighted material, intended for educational, artistic, or other expressive purposes. Whether the fake is sufficiently transformed from the original to earn fair use protection is a highly fact-specific inquiry for which a judicial track record does not yet exist.[180]\n\nTort law also includes concepts that could be used to address deep-fake scenarios. Most obviously, victims can sue for defamation. Where the alleged defamation concerns private individuals rather than public figures, states may permit plaintiffs to prevail based on a showing that the falsehood was made negligently.[181] Public officials and public figures are subject to a higher requirement of showing clear and convincing evidence of actual malice—knowledge or reckless disregard for the possibility that the deep fakes were false.[182] In addition to defamation, the closely related tort of placing a person in a “false light”—or recklessly creating a harmful and false implication about someone in a public setting—has clear potential for the deep fake context.[183]\n\nVictims may also sue in tort for intentional infliction of emotional distress. This requires proof of “extreme and outrageous conduct.”[184] Creating and circulating humiliating content like deep-fake sex videos would likely amount to “extreme and outrageous conduct” because it falls outside the norms of decency by most accounts.[185]\n\nAnother prospect is the “right of publicity” in tort law, which permits compensation for the misappropriation of someone’s likeness for commercial gain.[186] The commercial-gain element sharply limits the utility of this model: the harms associated with deep fakes do not typically generate direct financial gain for their creators.[187] This is likely true, for example, of deep fakes posted to harm rivals or ex-lovers. Only in core cases, such as a business using deep-fake technology to make it seem a particular person endorsed their product or service, might this approach prove useful in stemming abuse. Further, the expressive value of some deep fakes may constitute a further hurdle to liability; courts often dismiss right of publicity claims concerning newsworthy matters on free-speech grounds.[188]\n\nOther privacy-focused torts seem relevant at first blush, yet are a poor fit on close inspection.[189] The “public disclosure of private fact” tort, for example, allows individuals to recover for publication of private, “non-newsworthy” information that would highly offend the reasonable person.[190] While deep fakes may meet the offense standard, using a person’s face in a deep-fake video does not amount to the disclosure of private information if the source image was publicly available.[191] The “intrusion-on-seclusion” tort is likewise ill-suited to the deep-fake scenario. It narrowly applies to defendants who “intruded into a private place, or . . . invaded a private seclusion that the plaintiff has thrown about his person or affairs.”[192] Deep-fakes usually will not involve invasions of spaces (either physical or conceptual like email inboxes) in which individuals have a reasonable expectation of privacy.\n\nTherefore, current options for imposing liability on creators of deep fakes have mixed potential. Civil liability is most robust in relation to defamation, false light, and intentional infliction of emotional distress, with more limited prospects for copyright infringement and right of publicity claims.\n\nSuing the Platforms\n\nIt will be challenging to achieve individualized accountability for harmful deep fakes, but creators are not the only parties that might bear responsibility. Given the key role that content platforms play in enabling the distribution of deep fakes, and the fact that creators of harmful deep fakes in some cases may be difficult to find and deter, the most efficient and effective way to mitigate harm may be to impose liability on platforms.[193] In some contexts, this may be the only realistic possibility for deterrence and redress.\n\nOnline platforms already have an incentive to screen content, thanks to the impact of moral suasion, market dynamics, and political pressures.[194] They do not currently face significant civil liability risk for user-generated content, however, for the reasons explained below.\n\nIn 1996, Congress provided platforms with a liability shield in the form of Section 230 of the Communications Decency Act (CDA). The law provided an immunity from liability to online platforms for hosting harmful content, albeit with an exception for content that violates federal criminal law, the Electronic Communications Privacy Act, and intellectual property law.[195]\n\nSection 230 protects platforms in important ways. First, consider a situation in which an online platform displays content that links to another source (such as a news article or blog post) or is user-generated (such as a customer review posted on Yelp). Now, imagine that the content is defamatory or otherwise actionable. Can the plaintiff sue the online platform that helped it see the light of day? Not under Section 230. Section 230(c)(1) expressly forbids treating the platform as a “publisher” or “speaker” of someone else’s problematic content. As courts have interpreted Section 230, online platforms enjoy immunity from liability for user-generated content even if they deliberately encouraged the posting of that content.[196]\n\nNext, consider a situation in which an online platform decides not to allow users to post whatever they wish, but to instead screen and block certain harmful content. Might the act of filtering become the basis of liability? If so, platforms might be loath to do any screening at all. Section 230(c)(2) was meant to remove the disincentive to self-regulation that liability otherwise might produce.[197] Simply put, it forbids civil suits against platforms based on the good-faith act of filtering to screen out offensive content, whether in the nature of obscenity, harassment, violence, or otherwise.[198]\n\nIn crafting Section 230, the bill’s sponsors thought they were devising a safe harbor for online service providers that would enable the growth of the then-emerging “Internet.”[199] Representative Chris Cox, for example, became interested after reading about a trial court decision holding Prodigy, an online services company, liable as a publisher of defamatory comments because it tried but failed to filter profanity on its bulletin boards.[200] A key goal of the legislation was to help “clean up” the Internet by making it easier for willing platforms to filter out offensive material, removing the risk that doing so would incur civil liability by casting them in a publisher’s role.[201]\n\nAt the time, sponsors Senators James Exon and Slade Gorton sought to combat online pornography and make the “Internet” safe for kids.[202] Representatives Cox and Ron Wyden, another sponsor, argued that, if “this amazing new thing—the Internet—[was] going to blossom,” companies should not be “punished for trying to keep things clean.”[203]\n\nThis intent is clear in the language of Section 230(c)(2), which expressly concerns platforms engaged in “good faith” editorial activity involving the blocking and filtering of offensive user-posted content. The speaker and publisher liability provision of Section 230, however, lacks this narrowing language and has become a foundation for courts to interpret Section 230 immunity broadly.[204]\n\nNo doubt, Section 230’s immunity provision has been beneficial for digital expression and democratic culture. It has provided breathing room for the development of online services and innumerable opportunities for speech and discourse.[205] Its supporters contend that without immunity, search engines, social networks, and microblogging services might not have emerged.[206] We agree; the fear of publisher liability would likely have inhibited the Internet’s early growth.[207]\n\nHowever, an overbroad reading of Section 230 has “given online platforms a free pass to ignore illegal activities, to deliberately repost illegal material, and to solicit unlawful activities while ensuring that abusers cannot be identified.”[208] The permissive interpretation of Section 230 eliminates “incentives for better behavior by those in the best position to minimize harm.”[209] The results have been two-fold. On one hand, the law has created an open environment for hosting and distributing user-generated online content. On the other, it has generated an environment in which it is exceptionally hard to hold providers accountable, even in egregious circumstances involving systematic disinformation and falsehoods.[210]\n\nCourts have extended the immunity provision to a remarkable array of scenarios. They include instances where a provider republished content knowing it violated the law;[211] solicited illegal content while ensuring that those responsible could not be identified;[212] altered its user interface to ensure that criminals could were not caught;[213] and sold dangerous products.[214] In this way, Section 230 has evolved into a super-immunity that, among other things, prevents the best-positioned entities to respond to most harmful content. This would have seemed absurd to the CDA’s drafters.[215] The law’s overbroad interpretation means that platforms have no liability-based reason to take down illicit material, and that victims have no legal leverage to insist otherwise.[216] Rebecca Tushnet aptly expressed it a decade ago: Section 230 ensures that platforms enjoy “power without responsibility.”[217]\n\nUnfortunately, platforms’ power now includes the ability to ignore the propagation of damaging deep fakes. To be sure, some platforms do not need civil liability exposure to take action against deep-fake generated harms; market pressures and morals are enough. In most cases, however, these forces are insufficient to spur response.\n\nShould Section 230 be amended to extend liability to a wider-range of circumstances? In 2018, lawmakers modified the statute by enacting the Allow States and Victims to Fight Online Sex Trafficking Act (“FOSTA”) to address websites’ facilitation of sex trafficking.[218] FOSTA added a new exception to Section 230 immunity, similar to the provision preserving the ability to sue for intellectual property claims. Now, plaintiffs, including state attorneys general, acting on behalf of victims, may avoid Section 230 immunity when suing platforms for knowingly assisting, supporting, or facilitating sex trafficking offenses.\n\nFOSTA did not become law without controversy. Some decried the erosion of Section 230 over concerns that greater liability exposure for online platforms would result in a decrease in outlets, and more self-censorship by those remaining.[219] Others criticized FOSTA’s language as indeterminate, potentially resulting in less filtering rather than more.[220] On the other hand, the FOSTA debate also raises the question whether Congress instead erred by not going far enough in carving out exceptions to Section 230 immunity.\n\nSection 230 should be amended to allow a limited degree of platform liability relating to deep fakes. [221] Building on prior work in which one of us (Citron) proposed a similar change in an article co-authored with Benjamin Wittes, we propose that Section 230(c)(1) protections to platforms be conditional rather than automatic.[222] To qualify, an entity must demonstrate that it has taken “reasonable steps” to ensure that its platform is not being used for illegal ends. Platforms that meet this relatively-undemanding requirement will continue to enjoy the protections of Section 230, but others will not and hence may be treated as a publisher of user-generated content that they host.[223]\n\nTo be sure, such an amendment would raise hard questions regarding the metes and bounds of reasonableness. The scope of the duty would need to track salient differences among online entities. For example, “ISPs and social networks with millions of postings a day cannot plausibly respond to complaints of abuse immediately, let alone within a day or two,”[224] yet “they may be able to deploy technologies to detect content previously deemed unlawful.”[225] Inevitably, the “duty of care will evolve as technology improves.”[226]\n\nThis proposed amendment would be useful as a means to incentivize platforms to take reasonable steps to minimize the most-serious harms that might follow from user-posted or user-distributed deep fakes. If the reasonably available technical and other means for detection and removal of harmful fakes are limited, so too will be the obligation on the part of the platform.[227] But as those means improve, so would the incentive to use them.[228]\n\nWe recognize that this proposal runs risks, beyond the usual challenges associated with common law development of a novel standard of care. For example, opening the door to liability may over-deter platforms that are uncertain about the standard of care (and fearful of runaway juries imposing massive damages). This might drive sites to shutter (or to never emerge), and it might cause undue private censorship at the sites that remain. Free expression, innovation, and commerce all would suffer, on this view.\n\nTo ameliorate these concerns, this proposal can be cabined along several dimensions. First, the amendment to Section 230 could include a sunset provision paired with data-gathering requirements that would empower Congress to make an informed decision on renewal.[229] Data-gathering should include the type and frequency of content removed by platforms as well as the extent to which platforms use automation to filter or block certain types of content. This would permit Congress to assess whether the law was resulting in overbroad private censorship, and acting as a Heckler’s veto. Second, the amendment could include carefully tailored damages caps. Third, the amendment could be paired with a federal anti-SLAAP provision, which would deter frivolous lawsuits designed to silence protected speech. Last, the amendment could include an exhaustion-of-remedies provision pursuant to which plaintiffs, as a precondition to suit, must first provide notice to the platform regarding the allegedly improper content. The platform would have a specified window of time to examine and respond to the objection.\n\nIn sum, a reasonably calibrated standard of care combined with safeguards could reduce opportunities for abuses without interfering unduly with the further development of a vibrant Internet. It would also avoid unintentionally turning innocent platforms into involuntary insurers for those injured through their sites. Approaching the problem with the goal of setting an appropriate standard more readily allows differentiation between kinds of online actors, and a separate rule for websites designed to facilitate illegality in contrast to large ISPs linking millions to the Internet. That said, features used to control the scope of platform liability are only a partial solution to the deep-fakes challenge. Other policy responses will be necessary.\n\n3. Specific Categories of Criminal Liability\n\nCivil liability is not the only means through which the legal system can discourage the creation and distribution of harmful deep fakes. Criminal liability is another possibility. Can it close some of the gaps identified above?\n\nOnly to a limited extent. The criminal liability model in theory does have the capacity to overcome some of the most significant limits on the civil liability model. Being judgment proof might spare someone from fear of civil suit, for example, but it is no protection from being sent to prison and bearing the other consequences of criminal conviction.[230] And whereas the identification and service of process on the creator or distributor of a harmful deep fake often will be beyond the practical reach of would-be private plaintiffs, law enforcement entities have greater investigative capacities (in addition to the ability to seek extradition). It is far from clear, though, that these notional advantages can be brought to bear effectively in practice.\n\nTo some extent, the capacity of criminal law is a question of setting law enforcement priorities and allocating resources accordingly. So far, law enforcement’s track record is not promising. Notwithstanding notable exceptions, law enforcement, on the whole, has had a lackluster response to online abuse. In particular, state and local law enforcement agencies often fail to pursue cyberstalking complaints adequately because they lack training in the relevant laws and in the investigative techniques necessary to track down online abusers (federal prosecutors—including especially DOJ’s Computer Crimes and Intellectual Property Section—have a much stronger record, but their capabilities do not scale easily).[231] Although a wide range of deep fakes might warrant criminal charges, only the most extreme cases are likely to attract the attention of law enforcement.\n\nApart from questions of investigative and prosecutorial will, the prospects for criminal liability also depend on the scope of criminal laws themselves. To what extent do existing laws actually cover deep fakes, and to what extent might new ones do so?\n\nA number of current criminal statutes—concerning cyber stalking, impersonation, and defamation—are potentially relevant. Posting deep fakes in connection with the targeting of individuals, for example, might violate the federal cyberstalking laws, 18 U.S.C. § 2261A, or analogous state statutes. Under federal law, it is a felony to use any “interactive computer service or electronic communication service” to “intimidate”[232] a person in ways “reasonably expected to cause substantial emotional distress . . . .”[233] This reflects the fact that, even when cyberstalking victims do not fear bodily harm, “their lives are totally disrupted . . . in the most insidious and frightening ways.”[234] Defendants can be punished for up to five years in prison and fined up to $250,000, with additional sentencing requirements for repeat offenders and for defendants whose offense violates a restraining order.[235] Some deep fakes will fit this bill.\n\nImpersonation crimes may be applicable as well. Several states make it a crime, for example, to knowingly and credibly impersonate another person online with intent to “harm[], intimidat[e], threaten[], or defraud[]” that person.[236] And while the “harm, intimidate, threaten” portion of such statutes to some extent tracks the cyberstalking statute described above, its extension to “fraud” opens the door to a wider, though uncertain, range of potential applications. In certain jurisdictions, creators of deep fakes could also face charges for criminal defamation if they posted videos knowing that they were fake or if they were reckless as to their truth or falsity.[237] Similarly, using someone’s face in a violent deep-fake sex video might support charges for both impersonation and defamation if the defendant intended to terrorize or harm the person and knew the video was fake.\n\nThe foregoing examples concern harm to specific individuals, but some harms flowing from deep fakes will be distributed broadly across society. A pernicious example of the latter is a deep fake calculated to spur an audience to violence. Some platforms ban content calling for violence, but not all do.[238] Could the creator of such a deep fake be prosecuted under a statute like 18 U.S.C. § 2101, which criminalizes the use of facilities of interstate commerce, such as the Internet, with intent to incite a riot? Incitement charges must comport with the First Amendment constraints identified in Brandenburg, including that the speech in question be likely to produce imminent lawless action.[239] This leaves many deep fakes beyond the law’s reach even though they may have played a role in violence.\n\nCan criminal law be helpful in limiting harms from deep fakes in the particularly sensitive context of elections? Although lies have long plagued the democratic process, deep fakes present a troubling development. Some states have criminalized the intentional use of lies to impact elections.[240] These experiments have run into constitutional hurdles, however.\n\nFree speech scholar Helen Norton explains that while political candidates’ lies “pose . . . harms to their listeners . . . and may also . . . undermine public confidence in the integrity of the political process,” laws forbidding such lies “threaten significant First Amendment harms because they regulate expression in a context in which we especially fear government overreaching and partisan abuse.”[241] As the Court underscored in Brown v. Hartlage,[242] the “State’s fear that voters might make an ill-advised choice does not provide the State with a compelling justification for limiting speech.”[243] Not surprisingly, courts therefore have struck down periodic attempts to ban election-related lies.[244] The entry of deep fakes into the mix may not change that result. As explored above, however, criminal laws banning the impersonation of government officials or candidates for office may overcome constitutional challenge.[245]\n\nUltimately, criminal liability is not likely to be a particularly effective tool against deep fakes that pertain to elections. The most capable actors with motive and means to deploy deep fakes in a high-impact manner in an election setting will include the intelligence services of foreign governments engaging in such activity as a form of covert action, as we saw with Russia in relation to the American election of 2016. The prospect of a criminal prosecution in the United States will mean little to foreign government agents involved in such activity so long as they are not likely to end up in US custody (though it might mean something more to private actors through whom those agencies sometimes choose to act, at least if they intend to travel abroad).[246]\n\nC. Administrative Agency Solutions\n\nThe foregoing analysis suggests that prosecutors and private plaintiffs can and likely will play an important role in curbing harms from deep fakes, but also that this role has significant limitations. We therefore turn to consider the potential contributions of other actors, starting with administrative agencies.\n\nGenerally speaking, agencies can advance public policy goals through rulemaking, adjudication, or both.[247] Agencies do not enjoy plenary jurisdiction to use these tools in relation to any subject they wish. Typically, their field of operation is defined—with varying degrees of specificity—by statute. And thus we might begin by asking which agencies have the most plausible grounds for addressing deep fakes.\n\nAt the federal level, three candidates stand out: the Federal Trade Commission (“FTC”), the Federal Communications Commission (“FCC”), and the Federal Election Commission (“FEC”). On close inspection, however, their potential roles appear quite limited.\n\n1. The FTC\n\nConsider the Federal Trade Commission and its charge to regulate and litigate in an effort to minimize deceptive or unfair commercial acts and practices.[248] For that matter, consider the full range of state actors (often a state’s Attorney General’s Office) that play a similar role. Bearing that charge in mind, can these entities intervene in the deep fake context?\n\nA review of current areas of FTC activity suggests limited possibilities. Most deep fakes will not take the form of advertising, but some will. That subset will implicate the FTC’s role in protecting consumers from fraudulent advertising relating to “food, drugs, devices, services, or cosmetics.”[249] Some deep fakes will be in the nature of satire or parody, without intent or even effect of misleading consumers into believing a particular person (a celebrity or some other public figure) is endorsing the product or service in question. That line will be crossed in some instances, however. If such a case involves a public figure who is aware of the fraud and both inclined to and capable of suing on their own behalf for misappropriation of likeness, there is no need for the FTC or a state agency to become involved. Those conditions will not always be met, though, especially when the deep-fake element involves a fraudulent depiction of something other than a specific person’s words or deeds; there would be no obvious private plaintiff. The FTC and state attorneys general (state AGs) can play an important role in that setting.\n\nBeyond deceptive advertising, the FTC has authority to investigate unfair and deceptive commercial acts and practices under Section 5 of the Federal Trade Commission Act.[250] Much like Section 5 of the Federal Trade Commission Act, state UDAP laws (enforced by state AGs) prohibit deceptive commercial acts and practices and unfair trade acts and practices whose costs exceed their benefits.[251] UDAP laws empower attorneys general to seek civil penalties, injunctive relief, and attorneys’ fees and costs.[252]\n\nActing in that capacity, for example, the FTC previously investigated and reached a settlement with Facebook regarding its treatment of user data—and is now doing so again in the aftermath of public furor over the Cambridge Analytica debacle.[253] In response to the problem of fake news in general and deep-fake news in particular, the FTC might contemplate asserting a role under the rubric of “unfair or deceptive acts or practices in or affecting commerce.”[254] Any such efforts would face several obstacles, however. First, Section 230 of the Communications Decency Act as currently written would shield platforms at least to some extent from liability for publishing users’ deep fakes. Second, it is not clear this would be a proper interpretation of the FTC’s jurisdiction. Professor David Vladeck, formerly head of the FTC’s Bureau of Consumer Protection, has expressed doubt about the FTC’s jurisdiction to regulate sites purveying fake news.[255] Vladeck argues, “[f]ake news stories that get circulated or planted or tweeted around are not trying to induce someone to purchase a product; they’re trying to induce someone to believe an idea.”[256] Finally, the prospect of a government entity attempting to distinguish real news from fake news—and suppressing the latter—raises the First Amendment concerns described above in relation to election-lies laws.\n\nMight a different agency at least have a stronger jurisdictional claim to become involved in some settings? This brings us to the Federal Communications Commission.\n\n2. The FCC\n\nIf any regulatory agency is to play a role policing against harms from deep fakes circulating online, the FCC at first blush might seem a natural fit. It has a long tradition of regulating the communications of broadcasters, and many have observed that the major social media platforms of the twenty-first century occupy a place in our information ecosystem similar to the central role that radio and television broadcasters enjoyed in the twentieth century.[257] Similar thinking led the FCC in 2015 to break new ground by reclassifying Internet service providers as a “telecommunications service” rather than an “information service,” thus opening the door to more extensive regulation.[258] Amidst intense controversy, however, the FCC in late 2017 reversed course on this position on ISPs,[259] and in any event never asserted that so-called “edge providers” like Facebook also should be brought under the “telecommunications service” umbrella.[260]\n\nAs things stand, the FCC appears to lack jurisdiction (not to mention interest) over content circulated via social media. However, concern over fake news, incitement, radicalization, or any number of other hot-button issues might at some point tip the scales either for the FCC to reinterpret its own authority or for Congress to intervene. For the moment, however, this pathway appears closed, leaving the FCC’s role in relation to deep fakes limited to potential efforts to deter their appearance on radio or television.\n\n3. The FEC\n\nA third federal agency with a plausible stake in the topic of deep fakes is the Federal Election Commission. Plainly, its jurisdiction would touch upon deep fakes only as they relate to elections—a narrow, but important, subfield. Whether and how the FEC might act in relation to deep fakes even in that setting, however, is unclear.\n\nThe FEC regulates campaign speech, but not in ways that would speak directly to the deep-fake scenario. In particular, the FEC does not purport to regulate the truth of campaign-related statements, nor is it likely to assert or receive such jurisdiction a"
    }
}
{
    "id": "dbpedia_3098_1",
    "rank": 53,
    "data": {
        "url": "https://encord.com/blog/seggpt-segment-everything-in-context-explainer/",
        "read_more_link": "",
        "language": "en",
        "title": "SegGPT: Segmenting everything in context [Explained]",
        "top_image": "https://images.prismic.io/encord/41884f66-9209-48a8-96e0-c3b754ecf446_SegGPT-thumbnail.+%282%29.png?auto=compress%2Cformat&fit=max",
        "meta_img": "https://images.prismic.io/encord/41884f66-9209-48a8-96e0-c3b754ecf446_SegGPT-thumbnail.+%282%29.png?auto=compress%2Cformat&fit=max",
        "images": [
            "https://images.prismic.io/encord/41884f66-9209-48a8-96e0-c3b754ecf446_SegGPT-thumbnail.+%282%29.png?auto=compress%2Cformat&fit=max&w=906&h=638",
            "https://images.prismic.io/encord/41884f66-9209-48a8-96e0-c3b754ecf446_SegGPT-thumbnail.+%282%29.png?auto=compress%2Cformat&fit=max&w=906&h=638",
            "https://encord.com/static/VectorDesktop-d6a994f2c668a0332ba39898992e598f.png",
            "https://encord.com/static/VectorTablet-5246b4eeb12ce3a011a59f9a65313af7.png",
            "https://encord.com/static/VectorDesktop-d6a994f2c668a0332ba39898992e598f.png",
            "https://encord.cdn.prismic.io/encord/ZmrVVZm069VX1tfd_Union.svg",
            "https://images.prismic.io/encord/c594c9c7-3edf-4357-9307-92b9ab4d4548_1629105749470.jfif?auto=compress%2Cformat&fit=max&w=80&h=80",
            "https://images.prismic.io/encord/c594c9c7-3edf-4357-9307-92b9ab4d4548_1629105749470.jfif?auto=compress%2Cformat&fit=max&w=80&h=80",
            "https://images.prismic.io/encord/6c7ffd77-e01d-4776-b9a7-57b13790cd97_SegGPT_compared_previous_models.png?auto=compress,format",
            "https://images.prismic.io/encord/8c90155a-681f-40c8-89db-716560dfbca1_SegGPT_In_Context_Coloring.png?auto=compress,format",
            "https://images.prismic.io/encord/3e84e0ee-1094-4211-b396-3321862ffbad_Segmenting+everything+in+context+-+SegGPT.png?auto=compress,format",
            "https://images.prismic.io/encord/751d2685-a772-4407-8d8c-25078b6eca66_Segment+Anything+and+Segment+Everything+in+context.png?auto=compress,format",
            "https://images.prismic.io/encord/e25bcca7-d3c9-45cb-8593-f75c851431ae_Auto-segmentation+with+SegGPT+in+Hugging+Face.png?auto=compress,format",
            "https://images.prismic.io/encord/ef3347ba-d07c-4049-a4d5-4fe4bb0fbe5b_Instance+Segmentation+in+COCO.png?auto=compress,format",
            "https://encord.com/static/VectorDesktop-d6a994f2c668a0332ba39898992e598f.png",
            "https://encord.com/static/VectorTablet-5246b4eeb12ce3a011a59f9a65313af7.png",
            "https://encord.com/static/VectorDesktop-d6a994f2c668a0332ba39898992e598f.png",
            "https://encord.cdn.prismic.io/encord/ZmrVVZm069VX1tfd_Union.svg",
            "https://images.prismic.io/encord/c594c9c7-3edf-4357-9307-92b9ab4d4548_1629105749470.jfif?auto=compress%2Cformat&fit=max&w=80&h=80",
            "https://images.prismic.io/encord/c594c9c7-3edf-4357-9307-92b9ab4d4548_1629105749470.jfif?auto=compress%2Cformat&fit=max&w=80&h=80",
            "https://images.prismic.io/encord/ZgXBBMt2UUcvBQ2P_VisualizationsinDatabricks.png?auto=format%2Ccompress&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/ZgXBBMt2UUcvBQ2P_VisualizationsinDatabricks.png?auto=format%2Ccompress&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/ZgKwrccYqOFdyFc1_Mora-Banner.png?auto=format%2Ccompress&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/ZgKwrccYqOFdyFc1_Mora-Banner.png?auto=format%2Ccompress&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/65e096f89c42d04f7d96a37a_image-3-.png?auto=format%2Ccompress&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/65e096f89c42d04f7d96a37a_image-3-.png?auto=format%2Ccompress&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/65e0b7bd27237c2bb829aa4d_image-44-.png?auto=format%2Ccompress&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/65e0b7bd27237c2bb829aa4d_image-44-.png?auto=format%2Ccompress&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/32022a01-5deb-4084-9ab2-608bdf44d18f_Gemini+1.5.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/32022a01-5deb-4084-9ab2-608bdf44d18f_Gemini+1.5.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/af42bef8-9530-4d95-806d-2440ad79b04c_image4.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/af42bef8-9530-4d95-806d-2440ad79b04c_image4.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/a2da04e9-4214-403a-a463-0ebc7b2ef929_image+%2875%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/a2da04e9-4214-403a-a463-0ebc7b2ef929_image+%2875%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/2bd1cb87-8b2b-473f-85f5-11d97e1420e3_What+is+Model+Observability+-+Encord.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/2bd1cb87-8b2b-473f-85f5-11d97e1420e3_What+is+Model+Observability+-+Encord.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/7afd5c49-7289-4edf-8b97-87a1973c0d51_image+%2831%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/7afd5c49-7289-4edf-8b97-87a1973c0d51_image+%2831%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/0cab0829-77a1-4589-ae0a-6e6ffa83b73d_image+%2828%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/0cab0829-77a1-4589-ae0a-6e6ffa83b73d_image+%2828%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/613edaae-6914-403d-bf6f-39cdf80c1786_image5.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/613edaae-6914-403d-bf6f-39cdf80c1786_image5.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/424f6050-6d3f-49ab-b807-8465b455622c_Cross+Entropy+Loss+Functions.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/424f6050-6d3f-49ab-b807-8465b455622c_Cross+Entropy+Loss+Functions.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/6155fefd-96d8-476c-8b9b-f579671f51b5_Fine-tuning+vs+training.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/6155fefd-96d8-476c-8b9b-f579671f51b5_Fine-tuning+vs+training.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/1e3870ac-9b13-4749-b84a-e809733f6f5a_Mean+Average+Precision+in+Object+Detection-+Encord.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/1e3870ac-9b13-4749-b84a-e809733f6f5a_Mean+Average+Precision+in+Object+Detection-+Encord.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/760fff56-285d-43d6-8612-d410688a6c71_Vision+Language+Models.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/760fff56-285d-43d6-8612-d410688a6c71_Vision+Language+Models.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/acf3e8ac-56e3-4c2a-a64e-3c866a5a5043_image+%2821%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/acf3e8ac-56e3-4c2a-a64e-3c866a5a5043_image+%2821%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/692f0d83-ef27-4932-b131-7fcb9b40b530_ChatGPT.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/692f0d83-ef27-4932-b131-7fcb9b40b530_ChatGPT.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/f9a534e0-7fd1-4c97-894d-ffa02549447e_Scale+Alternatives.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/f9a534e0-7fd1-4c97-894d-ffa02549447e_Scale+Alternatives.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/bae5445b-042c-4876-a96e-00356a8851b7_Sudoku.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/bae5445b-042c-4876-a96e-00356a8851b7_Sudoku.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/76851426-6533-4e6f-a7b3-ef67b07f16ef_DALL-E+3+Blog+Post+Image.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/76851426-6533-4e6f-a7b3-ef67b07f16ef_DALL-E+3+Blog+Post+Image.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/cff8838f-a3f4-4d1e-aead-942047579fdb_image+%2820%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/cff8838f-a3f4-4d1e-aead-942047579fdb_image+%2820%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/8a55a37e-6c18-454e-a366-e7b8aff5daf5_image3.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/8a55a37e-6c18-454e-a366-e7b8aff5daf5_image3.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/f3cd75d4-c3ba-4d3e-9fe6-fc49d0b547c6_image9.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/f3cd75d4-c3ba-4d3e-9fe6-fc49d0b547c6_image9.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/2a41f7b2-7943-4629-b71a-7c03791f521a_OOD.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/2a41f7b2-7943-4629-b71a-7c03791f521a_OOD.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/0ee8d42b-fe26-4ad6-8a06-be74eff3565b_image2.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/0ee8d42b-fe26-4ad6-8a06-be74eff3565b_image2.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/c6ec5dd2-e45d-4b6a-9c8b-5cdef30aef63_Recent+papers.jpg?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/c6ec5dd2-e45d-4b6a-9c8b-5cdef30aef63_Recent+papers.jpg?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/d9a505d2-9fb8-425d-bd8f-cd4ffa0001b7_image9.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/d9a505d2-9fb8-425d-bd8f-cd4ffa0001b7_image9.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/f1f5d3a3-06b3-4ea6-a450-bc900661a077_Barlow-Twins.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/f1f5d3a3-06b3-4ea6-a450-bc900661a077_Barlow-Twins.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/cab9b6c5-bf6d-4575-9de3-4f4d90238e2f_Vision-Transformers-%28ViT%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/cab9b6c5-bf6d-4575-9de3-4f4d90238e2f_Vision-Transformers-%28ViT%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/d9880bf1-9792-481b-89af-cf8c34d1af54_image2.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/d9880bf1-9792-481b-89af-cf8c34d1af54_image2.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/4d5afeba-9695-4704-9fd8-13e34abe37f0_Transfer+Learning.jpg?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/4d5afeba-9695-4704-9fd8-13e34abe37f0_Transfer+Learning.jpg?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/065b7cde-b140-43c5-94ee-033d29bc3c34_image2.jpg?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/065b7cde-b140-43c5-94ee-033d29bc3c34_image2.jpg?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/cfa0df6b-b85b-4bed-93be-1b8895e06da8_image2.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/cfa0df6b-b85b-4bed-93be-1b8895e06da8_image2.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/991bfbf1-90c6-45fe-b1f1-900c7a41d547_Data+Curation.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/991bfbf1-90c6-45fe-b1f1-900c7a41d547_Data+Curation.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/3268a966-e19e-484e-bf64-646e64387956_Hyperparameter+Tuning.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/3268a966-e19e-484e-bf64-646e64387956_Hyperparameter+Tuning.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/402bca8e-ea61-4668-956e-f69eade05ff1_Part+2.jpg?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/402bca8e-ea61-4668-956e-f69eade05ff1_Part+2.jpg?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/f04dedd4-e636-41ca-a868-72b2c74969d6_Part+1.jpg?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/f04dedd4-e636-41ca-a868-72b2c74969d6_Part+1.jpg?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/9eb3fbc1-3768-4a7b-a624-759fcd327ad2_Deep+Learning.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/9eb3fbc1-3768-4a7b-a624-759fcd327ad2_Deep+Learning.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/93d426cc-ec5e-4c66-88b3-54ef05c098d7_Time+series+w+RNN.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/93d426cc-ec5e-4c66-88b3-54ef05c098d7_Time+series+w+RNN.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/384870c1-ada4-4d59-8450-919dbaf86b04_image+%2844%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/384870c1-ada4-4d59-8450-919dbaf86b04_image+%2844%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/3fcce73d-4116-4e1c-9a27-d99ca637a27a_image1.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/3fcce73d-4116-4e1c-9a27-d99ca637a27a_image1.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/80e2444d-26ec-4504-bde1-b5a15092e9ab_PUG+Blog+Banner.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/80e2444d-26ec-4504-bde1-b5a15092e9ab_PUG+Blog+Banner.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/d7ed7c81-e1a5-4665-b5e8-889d6ebf8f17_ML+modeling+vs+ML+observability.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/d7ed7c81-e1a5-4665-b5e8-889d6ebf8f17_ML+modeling+vs+ML+observability.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/583430bd-6888-4ad7-a91c-4a8e10d8096d_Model+Inference.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/583430bd-6888-4ad7-a91c-4a8e10d8096d_Model+Inference.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/b23e44bf-aea5-404a-ba3e-5c4d74116982_Data+factory.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/b23e44bf-aea5-404a-ba3e-5c4d74116982_Data+factory.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/044b6a04-3077-4069-9dc6-80d1bca6b52a_image+%2840%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/044b6a04-3077-4069-9dc6-80d1bca6b52a_image+%2840%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/fbdaee27-be5a-48c6-a2c6-eeb16967a103_image+%2838%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/fbdaee27-be5a-48c6-a2c6-eeb16967a103_image+%2838%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/31000dbf-703e-4d39-a761-572e25e38adb_BIas+in+machine+learning+%281%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/31000dbf-703e-4d39-a761-572e25e38adb_BIas+in+machine+learning+%281%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/af0365e1-0625-4343-b39f-63e077deabce_Per-SAM+vs+Mask+RCNN.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/af0365e1-0625-4343-b39f-63e077deabce_Per-SAM+vs+Mask+RCNN.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/7d53a172-8e30-423a-924f-abfcc64c8c0e_image6.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/7d53a172-8e30-423a-924f-abfcc64c8c0e_image6.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/c53a4e69-7209-48b6-9190-cc578af755f0_image9.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/c53a4e69-7209-48b6-9190-cc578af755f0_image9.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/5accaa76-6fad-437b-8fbb-94b9544c3789_image7.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/5accaa76-6fad-437b-8fbb-94b9544c3789_image7.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/e0cf64c6-71b5-4c3f-abc3-4f3769cde785_KL+Divergence.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/e0cf64c6-71b5-4c3f-abc3-4f3769cde785_KL+Divergence.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/746ef267-ecdf-405c-9393-90bceb1f9a36_Synthetic+data.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/746ef267-ecdf-405c-9393-90bceb1f9a36_Synthetic+data.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/11b9026c-edc4-4d23-b6f3-09bd0ede3e28_image+%2835%29+2.jpg?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/11b9026c-edc4-4d23-b6f3-09bd0ede3e28_image+%2835%29+2.jpg?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/4586fc5c-4788-47b5-a443-8ef3bb5155c2_Llama+v2.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/4586fc5c-4788-47b5-a443-8ef3bb5155c2_Llama+v2.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/959d564b-c80a-499f-86b4-b248e804d4eb_image9.gif?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/959d564b-c80a-499f-86b4-b248e804d4eb_image9.gif?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/703be18a-dd25-4306-a7b5-e3873d4a25ce_F1+Blog+Banner.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/703be18a-dd25-4306-a7b5-e3873d4a25ce_F1+Blog+Banner.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/f5e1a224-fa73-4e58-984d-deb579885fc6_contrastive+learning+banner.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/f5e1a224-fa73-4e58-984d-deb579885fc6_contrastive+learning+banner.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/c0657d28-5216-4170-9b91-3ca0fe9ec2c1_Semantic+Seg.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/c0657d28-5216-4170-9b91-3ca0fe9ec2c1_Semantic+Seg.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/29257b2c-bcdf-454d-8d32-4b3221eabeaa_Guide+to+RLHF.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/29257b2c-bcdf-454d-8d32-4b3221eabeaa_Guide+to+RLHF.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/8dd324c7-4f12-4c52-95fa-511499a88cb9_image+%2812%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/8dd324c7-4f12-4c52-95fa-511499a88cb9_image+%2812%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/1f526e45-d0ed-495d-b19f-c7b97725f6f0_image+%2813%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/1f526e45-d0ed-495d-b19f-c7b97725f6f0_image+%2813%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/ba91424d-edd4-4cfe-84b7-94e6e8e1f131_image+%2811%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/ba91424d-edd4-4cfe-84b7-94e6e8e1f131_image+%2811%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/2f442132-bc37-48dd-af91-302b083d04bb_image3.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/2f442132-bc37-48dd-af91-302b083d04bb_image3.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/3147e4cb-22e3-4d8d-a84c-3407186b5e4b_Frame+1000001737.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/3147e4cb-22e3-4d8d-a84c-3407186b5e4b_Frame+1000001737.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/94455529-c0f1-4f5f-a790-ec363800c7fb_image+%289%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/94455529-c0f1-4f5f-a790-ec363800c7fb_image+%289%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/a50a93cc-595a-411f-85a7-0ae8418f379c_image%284%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/a50a93cc-595a-411f-85a7-0ae8418f379c_image%284%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/88cd5fa0-e8e9-49e1-a6e1-a89aa4c4f7ca_vector+similarity+search.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/88cd5fa0-e8e9-49e1-a6e1-a89aa4c4f7ca_vector+similarity+search.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/54de7c45-c97d-4c87-8f1d-118e509dfa97_1.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/54de7c45-c97d-4c87-8f1d-118e509dfa97_1.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/c66851e9-9b1c-4137-874c-9a9e06a8cd84_How+to+Measure+Model+Performance+in+Computer+Vision+A+Comprehensive+Guide.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/c66851e9-9b1c-4137-874c-9a9e06a8cd84_How+to+Measure+Model+Performance+in+Computer+Vision+A+Comprehensive+Guide.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/b2fc8322-f612-4d59-b261-cc1749d7001f_image2.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/b2fc8322-f612-4d59-b261-cc1749d7001f_image2.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/a82e415e-a411-453c-8309-250c0c9a2011_Data+refinement+strategies.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/a82e415e-a411-453c-8309-250c0c9a2011_Data+refinement+strategies.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/6a1e49ec-5e0f-42ae-b930-9289b01a1471_Best+Video+Annotation+tools.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/6a1e49ec-5e0f-42ae-b930-9289b01a1471_Best+Video+Annotation+tools.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/df774eb1-d9df-4767-b152-18f3f56f9d57_Thumbnail+ImageBind+Explained.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/df774eb1-d9df-4767-b152-18f3f56f9d57_Thumbnail+ImageBind+Explained.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/2b3bbc92-bb9e-4adb-8cec-0c46b59be923_self-supervised+learning+explained.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/2b3bbc92-bb9e-4adb-8cec-0c46b59be923_self-supervised+learning+explained.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/00c9db11-7475-4752-8077-aef48eeac950_VFMs+explained+%281%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/00c9db11-7475-4752-8077-aef48eeac950_VFMs+explained+%281%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/362b5e92-1eb2-46a5-91f7-77541e148fbd_DINOv2+self-supervised+learning_+img1.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/362b5e92-1eb2-46a5-91f7-77541e148fbd_DINOv2+self-supervised+learning_+img1.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/7d9206e9-0d93-4737-99e3-b873faf9a062_full+guide+to+foundation+models.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/7d9206e9-0d93-4737-99e3-b873faf9a062_full+guide+to+foundation+models.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/0d886cbe-db86-4006-b509-554359d6b2c0_Object+Detection_.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/0d886cbe-db86-4006-b509-554359d6b2c0_Object+Detection_.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/c906cc2b-0bdf-4722-9b59-18af3e952c97_Segment-all-models-thumbnail..png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/c906cc2b-0bdf-4722-9b59-18af3e952c97_Segment-all-models-thumbnail..png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/7df045db-935f-443c-8693-70988a033754_344167390_A_dream_of_a_giant_photorealistic_ginger_cat_is__playing_in_the_road_stopping_traffic_photographed_by_David_LaChapelle_.webp?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/7df045db-935f-443c-8693-70988a033754_344167390_A_dream_of_a_giant_photorealistic_ginger_cat_is__playing_in_the_road_stopping_traffic_photographed_by_David_LaChapelle_.webp?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/e3dfc705-deaf-46a6-96e1-d86c184fee32_One+Shot+Learning+Blog+Header.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/e3dfc705-deaf-46a6-96e1-d86c184fee32_One+Shot+Learning+Blog+Header.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/b1d66c7a-52be-467f-b6e6-c354a927f83a_HAL_closeup.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/b1d66c7a-52be-467f-b6e6-c354a927f83a_HAL_closeup.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/f7c346f4-7ef4-4262-8071-dca0765b5468_image%284%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/f7c346f4-7ef4-4262-8071-dca0765b5468_image%284%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/fc12e910-7c35-4181-90d5-a8f6d6544eab_Data_curation_thumbnail.webp?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/fc12e910-7c35-4181-90d5-a8f6d6544eab_Data_curation_thumbnail.webp?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/1fac39a2-d52d-40d6-a5e5-7e54ddf0fed0_5+Bias.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/1fac39a2-d52d-40d6-a5e5-7e54ddf0fed0_5+Bias.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/a04dd173-3f60-4301-ab9a-86f66145ebb5_piret-ilver-98MbUldcDJY-unsplash.jpg?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/a04dd173-3f60-4301-ab9a-86f66145ebb5_piret-ilver-98MbUldcDJY-unsplash.jpg?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/a93e02b6-9a7d-4d76-bb74-9726b2a07661_Car.webp?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/a93e02b6-9a7d-4d76-bb74-9726b2a07661_Car.webp?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/2bc7fa42-e2e9-4128-a624-e1a4f6134b00_Frame+487.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/2bc7fa42-e2e9-4128-a624-e1a4f6134b00_Frame+487.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/d2c5c481-9603-4046-94ae-1d061b5d1aeb_4+things+to+do.webp?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/d2c5c481-9603-4046-94ae-1d061b5d1aeb_4+things+to+do.webp?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/ee5778d3-a7de-418f-955a-be59ce078f04_Data+Approximation.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/ee5778d3-a7de-418f-955a-be59ce078f04_Data+Approximation.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/5a676cee-9d13-480d-96fb-ac788de33034_data+augmentation.webp?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/5a676cee-9d13-480d-96fb-ac788de33034_data+augmentation.webp?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/1385e64d-ff53-4f15-b46c-2d4b434413c2_Object+Tracking.webp?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/1385e64d-ff53-4f15-b46c-2d4b434413c2_Object+Tracking.webp?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/e7c86d33-b673-4ace-b870-15b7cbd55a55_Startup.webp?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/e7c86d33-b673-4ace-b870-15b7cbd55a55_Startup.webp?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/5068d72b-3862-43e4-9cbb-042fba220ff6_6+steps+to+models.webp?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/5068d72b-3862-43e4-9cbb-042fba220ff6_6+steps+to+models.webp?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/e581a6d9-82eb-4c07-967d-3b63f8878eba_Introduction+to+synthetic+training+data.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/e581a6d9-82eb-4c07-967d-3b63f8878eba_Introduction+to+synthetic+training+data.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/900afe57-9cc1-4662-a27b-fdd8fa4d6ca6_Sythetic+Training+Data.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/900afe57-9cc1-4662-a27b-fdd8fa4d6ca6_Sythetic+Training+Data.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/b974b751-f42d-40cc-9c48-15851bc2577b_2700.jpg?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/b974b751-f42d-40cc-9c48-15851bc2577b_2700.jpg?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/59553dbd-c28f-45f3-b374-427840480fc5_1000.jpg?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/59553dbd-c28f-45f3-b374-427840480fc5_1000.jpg?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/818c6d29-a479-4fed-8f1c-30d101fa00b1_3100.webp?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/818c6d29-a479-4fed-8f1c-30d101fa00b1_3100.webp?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/201d1f7d-b74f-4804-bfc4-5c5731fb61f1_Mirco-models.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/201d1f7d-b74f-4804-bfc4-5c5731fb61f1_Mirco-models.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/2ae70f11-966d-4c1e-ac11-ce842ecfeeac_6368ddfad96697ba16a6b234_Best+Datasets-p-800.webp?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/2ae70f11-966d-4c1e-ac11-ce842ecfeeac_6368ddfad96697ba16a6b234_Best+Datasets-p-800.webp?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/57bd343a-7e54-4653-a716-f8fbd88d1afc_image+%284%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/57bd343a-7e54-4653-a716-f8fbd88d1afc_image+%284%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://cdn.drata.com/badge/soc2-dark.png",
            "https://images.prismic.io/encord/d5a5f02e-d8df-49c2-9413-5633a8e75e7d_soc2-certificate.png?auto=compress,format",
            "https://encord.cdn.prismic.io/encord/ZoZ1tR5LeNNTwyYw_g22024.svg",
            "https://dc.ads.linkedin.com/collect/?pid=4241362&fmt=gif"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Nikolaj Buhl"
        ],
        "publish_date": "2023-04-13T22:41:55+00:00",
        "summary": "",
        "meta_description": "Discover BAAI’s Segment Everything in Context (SegGPT), its impact on computer vision and medical imaging, and how it compares to previous AI models.",
        "meta_lang": "en",
        "meta_favicon": "/apple-touch-icon.png",
        "meta_site_name": "",
        "canonical_link": "https://encord.com/blog/seggpt-segment-everything-in-context-explainer/",
        "text": "Visualizations in Databricks\n\nWith data becoming a pillar stone of a company’s growth strategy, the market for visualization tools is growing rapidly, with a projected compound annual growth rate (CAGR) of 10.07% between 2023 and 2028. The primary driver of these trends is the need for data-driven decision-making, which involves understanding complex data patterns and extracting actionable insights to improve operational efficiency. PowerBI and Tableau are traditional tools with interactive workspaces for creating intuitive dashboards and exploring large datasets. However, other platforms are emerging to address the ever-changing nature of the modern data ecosystem. In this article, we will discuss the visualizations offered by Databricks - a modern enterprise-scale platform for building data, analytics, and artificial intelligence (AI) solutions. Databricks Databricks is an end-to-end data management and model development solution built on Apache Spark. It lets you create and deploy the latest generative AI (Gen AI) and large language models (LLMs). The platform uses a proprietary Mosaic AI framework to streamline the model development process. It provides tools to fine-tune LLMs seamlessly through enterprise data and offers a unified service for experimentation through foundation models. In addition, it features Databricks SQL, a state-of-the-art lakehouse for cost-effective data storage and retrieval. It lets you centrally store all your data assets in an open format, Delta Lake, for effective governance and discoverability. Further, Databricks SQL has built-in support for data visualization, which lets you extract insights from datasets directly from query results in the SQL editor. Users also benefit from the visualization tools featured in Databricks Notebooks, which help you build interactive charts by using the Plotly library in Python. Through these visualizations, Databricks offers robust data analysis for monitoring data assets critical to your AI models. So, let’s discuss in more detail the types of chart visualizations, graphs, diagrams, and maps available on Databricks to help you choose the most suitable visualization type for your use case. Effective visualization can help with effortless data curation. Learn more about how you can use data curation for computer vision Visualizations in Databricks As mentioned earlier, Databricks provides visualizations through Databricks SQL and Databricks Notebooks. The platform lets you run multiple SQL queries to perform relevant aggregations and apply filters to visualize datasets according to your needs. Databricks also allows you to configure settings related to the X and Y axes, legends, missing values, colors, and labels. Users can also download visualizations in PNG format for documentation purposes. The following sections provide an overview of the various visualization types available in these two frameworks, helping you select the most suitable option for your project. Bar Chart Bar charts are helpful when you want to compare the frequency of occurrence of different categories in your dataset. For instance, you can draw a bar chart to compare the frequency of various age groups, genders, ethnicities, etc. Additionally, bar charts can be used to view the sum of the prices of all orders placed in a particular month and group them by priority. Bar chart The result will show the months on the X-axis and the sum of all the orders categorized by priority on the Y-axis. Line Line charts connect different data points through straight lines. They are helpful when users want to analyze trends over some time. The charts usually show time on the X-axis and some metrics whose trajectory you want to explore on the Y-axis. Line chart For instance, you can view changes in the average price of orders over the years grouped by priority. The trends can help you predict the most likely future values, which can help you with financial projections and budget planning. Pie Chart Pie charts display the proportion of different categories in a dataset. They divide a circle into multiple segments, each showing the proportion of a particular category, with the segment size proportional to the category’s percentage of the total. Pie chart For instance, you can visualize the proportion of orders for each priority. The visualization is helpful when you want a quick overview of data distribution across different segments. It can help you analyze demographic patterns, market share of other products, budget allocation, etc. Scatter Plot A scatter plot displays each data point as a dot representing a relationship between two variables. Users can also control the color of each dot to reflect the relationship across different groups. Scatter Plot For instance, you can plot the relationship between quantity and price for different color-coded item categories. The visualization helps in understanding the correlation between two variables. However, users must interpret the relationship cautiously, as correlation does not always imply causation. Deeper statistical analysis is necessary to uncover causal factors. Area Charts Area charts combine line and bar charts by displaying lines and filling the area underneath with colors representing particular categories. They show how the contribution of a specific category changes relative to others over time. Area Charts For instance, you can visualize which type of order priority contributed the most to revenue by plotting the total price of different order priorities across time. The visualization helps you analyze the composition of a specific metric and how that composition varies over time. It is particularly beneficial in analyzing sales growth patterns for different products, as you can see which product contributed the most to growth across time. Box Chart Box charts concisely represent data distributions of numerical values for different categories. They show the distribution’s median, skewness, interquartile, and value ranges. Box Chart For instance, the box can display the median price value through a line inside the box and the interquartile range through the top and bottom box enclosures. The extended lines represent minimum and maximum price values to compute the price range. The chart helps determine the differences in distribution across multiple categories and lets you detect outliers. You can also see the variability in values across different categories and examine which category was the most stable. Bubble Chart Bubble charts enhance scatter plots by allowing you to visualize the relationship of three variables in a two-dimensional grid. The bubble position represents how the variable on the X-axis relates to the variable on the Y-axis. The bubble size represents the magnitude of a third variable, showing how it changes as the values of the first two variables change. Bubble chart The visualization is helpful for multi-dimensional datasets and provides greater insight when analyzing demographic data. However, like scatter plots, users must not mistake correlation for causation. Combo Chart Combo charts combine line and bar charts to represent key trends in continuous and categorical variables. The categorical variable is on the X-axis, while the continuous variable is on the Y-axis. Combo Chart For instance, you can analyze how the average price varies with the average quantity according to shipping date. The visualization helps summarize complex information involving relationships between three variables on a two-dimensional graph. However, unambiguous interpretation requires careful configuration of labels, colors, and legends. Heatmap Chart Heatmap charts represent data in a matrix format, with each cell having a different color according to the numerical value of a specific variable. The colors change according to the value intensity, with lower values typically having darker and higher values having lighter colors. Heatmap chart For instance, you can visualize how the average price varies according to order priority and order status. Heatmaps are particularly useful in analyzing correlation intensity between two variables. They also help detect outliers by representing unusual values through separate colors. However, interpreting the chart requires proper scaling to ensure colors do not misrepresent intensities. Histogram Histograms display the frequency of particular value ranges to show data distribution patterns. The X-axis contains the value ranges organized as bins, and the Y-axis shows the frequency of each bin. Histogram For instance, you can visualize the frequency of different price ranges to understand price distribution for your orders. The visualization lets you analyze data spread and skewness. It is beneficial in deeper statistical analysis, where you want to derive probabilities and build predictive models. Pivot Tables Pivot tables can help you manipulate tabular displays through drag-and-drop options by changing aggregation records. The option is an alternative to SQL filters for viewing aggregate values according to different conditions. Pivot Tables For instance, you can group total orders by shipping mode and order category. The visualization helps prepare ad-hoc reports and provides important summary information for decision-making. Interactive pivot tables also let users try different arrangements to reveal new insights. Choropleth Map Visualization Choropleth map visualization represents color-coded aggregations categorized according to different geographic locations. Regions with higher value intensities have darker colors, while those with lower intensities have lighter shades. Choropleth map visualization For instance, you can visualize the total revenue coming from different countries. This visualization helps determine global presence and highlight disparities across borders. The insights will allow you to develop marketing strategies tailored to regional tastes and behavior. Funnel Visualization Funnel visualization depicts data aggregations categorized according to specific steps in a pipeline. It represents each step from top to bottom with a bar and the associated value as a label overlay on each bar. It also displays cumulative percentage values showing the proportion of the aggregated value resulting from each stage. Funnel Visualization For instance, you can determine the incoming revenue streams at each stage of the ordering process. This visualization is particularly helpful in analyzing marketing pipelines for e-commerce sites. The tool shows the proportion of customers who view a product ad, click on it, add it to the cart, and proceed to check out. Cohort Analysis Cohort analysis offers an intuitive visualization to track the trajectory of a particular metric across different categories or cohorts. Cohort Analysis For instance, you can analyze the number of active users on an app that signed up in different months of the year. The rows will depict the months, and the columns will represent the proportion of active users in a particular cohort as they move along each month. The visualization helps in retention analysis as you can determine the proportion of retained customers across the user lifecycle. Counter Display Databricks allows you to configure a counter display that explicitly shows how the current value of a particular metric compares with the metric’s target value. Counter display For instance, you can check how the average total revenue compares against the target value. In Databricks, the first row represents the current value, and the second is the target. The visualization helps give a quick snapshot of trending performance and allows you to quantify goals for better strategizing. Sankey Diagrams Sankey diagrams show how data flows between different entities or categories. It represents flows through connected links representing the direction, with entities displayed as nodes on either side of a two-dimensional grid. The width of the connected links represents the magnitude of a particular value flowing from one entity to the other. Sankey Diagram For instance, you can analyze traffic flows from one location to the other. Sankey diagrams can help data engineering teams analyze data flows from different platforms or servers. The analysis can help identify bottlenecks, redundancies, and resource constraints for optimization planning. Sunburst Sequence The sunburst sequence visualizes hierarchical data through concentric circles. Each circle represents a level in the hierarchy and has multiple segments. Each segment represents the proportion of data in the hierarchy. Furthermore, it color codes segments to distinguish between categories within a particular hierarchy. Sunburst Sequence For instance, you can visualize the population of different world regions through a sunburst sequence. The innermost circle represents a continent, the middle one shows a particular region, and the outermost circle displays the country within that region. The visualization helps data science teams analyze relationships between nested data structures. The information will allow you to define clear data labels needed for model training. Table A table represents data in a structured format with rows and columns. Databricks offers additional functionality to hide, reformat, and reorder data. Tables help summarize information in structured datasets. You can use them for further analysis through SQL queries. Word Cloud Word cloud visualizations display words in different sizes according to their frequency in textual data. For instance, you can analyze customer comments or feedback and determine overall sentiment based on the highest-occurring words. Word Cloud While word clouds help identify key themes in unstructured textual datasets, they can suffer from oversimplification. Users must use word clouds only as a quick overview and augment textual analysis with advanced natural language processing techniques. Visualization is critical to efficient data management. Find out the top tools for data management for computer vision Visualizations in Databricks: Key Takeaways With an ever-increasing data volume and variety, visualization is becoming critical for quickly communicating data-based insights in a simplified manner. Databricks is a powerful tool with robust visualization types for analyzing complex datasets. Below are a few key points to remember regarding visualization in Databricks. Databricks SQL and Databricks Notebooks: Databricks offers advanced visualizations through Databricks SQL and Databricks Notebooks as a built-in functionality. Visualization configurations: Users can configure multiple visualization settings to produce charts, graphs, maps, and diagrams per their requirements. Visualization types: Databricks offers multiple visualizations, including bar charts, line graphs, pie charts, scatter plots, area graphs, box plots, bubble charts, combo charts, heatmaps, histograms, pivot tables, choropleth maps, funnels, cohort tables, counter display, Sankey diagrams, sunburst sequences, tables, and word clouds.\n\nMar 28 2024\n\n10 M\n\nMicrosoft MORA: Multi-Agent Video Generation Framework\n\nWhat is Mora? Mora is a multi-agent framework designed for generalist video generation. Based on OpenAI's Sora, it aims to replicate and expand the range of generalist video generation tasks. Sora, famous for making very realistic and creative scenes from written instructions, set a new standard for creating videos that are up to a minute long and closely match the text descriptions given. Mora distinguishes itself by incorporating several advanced visual AI agents into a cohesive system. This lets it undertake various video generation tasks, including text-to-video generation, text-conditional image-to-video generation, extending generated videos, video-to-video editing, connecting videos, and simulating digital worlds. Mora can mimic Sora’s capabilities using multiple visual agents, significantly contributing to video generation. In this article, you will learn: Mora's innovative multi-agent framework for video generation. The importance of open-source collaboration that Mora enables. Mora's approach to complex video generation tasks and instruction fidelity. About the challenges in video dataset curation and quality enhancement. TL; DR Mora's novel approach uses multiple specialized AI agents, each handling different aspects of the video generation process. This innovation allows various video generation tasks, showcasing adaptability in creating detailed and dynamic video content from textual descriptions. Mora aims to fix the problems with current models like Sora, which is closed-source and does not let anyone else use it or do more research in the field, even though it has amazing text-to-video conversion abilities 📝🎬. Unfortunately, Mora still has problems with dataset quality, video fidelity, and ensuring that outputs align with complicated instructions and people's preferences. These problems show where more work needs to be done in the future. OpenAI Sora’s Closed-Source Nature The closed-source nature of OpenAI's Sora presents a significant challenge to the academic and research communities interested in video generation technologies. Sora's impressive capabilities in generating realistic and detailed videos from text descriptions have set a new standard in the field. Related: New to Sora? Check out our detailed explainer on the architecture, relevance, limitations, and applications of Sora. However, the inability to access its source code or detailed architecture hinders external efforts to replicate or extend its functionalities. This limits researchers from fully understanding or replicating its state-of-the-art performance in video generation. Here are the key challenges highlighted due to Sora's closed-source nature: Inaccessibility to Reverse-Engineer Without access to Sora's source code, algorithms, and detailed methodology, the research community faces substantial obstacles in dissecting and understanding the underlying mechanisms that drive its exceptional performance. This lack of transparency makes it difficult for other researchers to learn from and build upon Sora's advancements, potentially slowing down the pace of innovation in video generation. Extensive Training Datasets Sora's performance is not just the result of sophisticated modeling and algorithms; it also benefits from training on extensive and diverse datasets. But the fact that researchers cannot get their hands on similar datasets makes it very hard to copy or improve Sora's work. High-quality, large-scale video datasets are crucial for training generative models, especially those capable of creating detailed, realistic videos from text descriptions. However, these datasets are often difficult to compile due to copyright issues, the sheer volume of data required, and the need for diverse, representative samples of the real world. Creating, curating, and maintaining high-quality video datasets requires significant resources, including copyright permissions, data storage, and management capabilities. Sora's closed nature worsens these challenges by not providing insights into compiling the datasets, leaving researchers to navigate these obstacles independently. Computational Power Creating and training models like Sora require significant computational resources, often involving large clusters of high-end GPUs or TPUs running for extended periods. Many researchers and institutions cannot afford this much computing power, which makes the gap between open-source projects like Mora and proprietary models like Sora even bigger. Without comparable computational resources, it becomes challenging to undertake the necessary experimentation—with different architectures and hyperparameters—and training regimes required to achieve similar breakthroughs in video generation technology. Learn more about these limitations in the technical paper. Evolution: Text-to-Video Generation Over the years, significant advancements in text-to-video generation technology have occurred, with each approach and architecture uniquely contributing to the field's growth. Here's a summary of these evolutionary stages, as highlighted in the discussion about text-to-video generation in the Mora paper: GANs (Generative Adversarial Networks) Early attempts at video generation leveraged GANs, which consist of two competing networks: a generator that creates images or videos that aim to be indistinguishable from real ones, and a discriminator that tries to differentiate between the real and generated outputs. Despite their success in image generation, GANs faced challenges in video generation due to the added complexity of temporal coherence and higher-dimensional data. Generative Video Models Moving beyond GANs, the field saw the development of generative video models designed to produce dynamic sequences. Generating realistic videos frame-by-frame and maintaining temporal consistency is a challenge, unlike in static image generation. Auto-Regressive Transformers Auto-regressive transformers were a big step forward because they could generate video sequences frame-by-frame. These models predicted each new frame based on the previously generated frames, introducing a sequential element that mirrors the temporal progression of videos. But this approach often struggled with long-term coherence over longer sequences. Large-Scale Diffusion Models Diffusion models, known for their capacity to generate high-quality images, were extended to video generation. These models gradually refine a random noise distribution toward a coherent output. They apply this iterative denoising process to the temporal domain of videos. Related: Read our guide on HuggingFace’s Dual-Stream Diffusion Net for Text-to-Video Generation. Image Diffusion U-Net Adapting the U-Net architecture for image diffusion models to video content was critical. This approach extended the principles of image generation to videos, using a U-Net that operates over sequences of frames to maintain spatial and temporal coherence. 3D U-Net Structure The change to a 3D U-Net structure allowed for more nuance in handling video data, considering the extra temporal dimension. This change also made it easier to model time-dependent changes, improving how we generate coherent and dynamic video content. Latent Diffusion Models (LDMs) LDMs generate content in a latent space rather than directly in pixel space. This approach reduces computational costs and allows for more efficient handling of high-dimensional video data. LDMs have shown that they can better capture the complex dynamics of video content. Diffusion Transformers Diffusion transformers (DiT) combine the strengths of transformers in handling sequential data with the generative capabilities of diffusion models. This results in high-quality video outputs that are visually compelling and temporally consistent. Useful: Stable Diffusion 3 is an example of a multimodal diffusion transformer model that generates high-quality images and videos from text. Check out our explainer on how it works. AI Agents: Advanced Collaborative Multi-agent Structures The paper highlights the critical role of collaborative, multi-agent structures in developing Mora. It emphasizes their efficacy in handling multimodal tasks and improving video generation capabilities. Here's a concise overview based on the paper's discussion on AI Agents and their collaborative frameworks: Multimodal Tasks Advanced collaborative multi-agent structures address multimodal tasks involving processing and generating complex data across different modes, such as text, images, and videos. These structures help integrate various AI agents, each specialized in handling specific aspects of the video generation process, from understanding textual prompts to creating visually coherent sequences. Cooperative Agent Framework (Role-Playing) The cooperative agent framework, characterized by role-playing, is central to the operation of these multi-agent structures. Each agent is assigned a unique role or function in this framework, such as prompt enhancement, image generation, or video editing. By defining these roles, the framework ensures that an agent with the best skills for each task is in charge of that step in the video generation process, increasing overall efficiency and output quality. Multi-Agent Collaboration Strategy The multi-agent collaboration strategy emphasizes the orchestrated interaction between agents to achieve a common goal. In Mora, this strategy involves the sequential and sometimes parallel processing of tasks by various agents. For instance, one agent might enhance an initial text prompt, convert it into another image, and finally transform it into a video sequence by yet another. This collaborative approach allows for the flexible and dynamic generation of video content that aligns with user prompts. AutoGen (Generic Programming Framework) A notable example of multi-agent collaboration in practice is AutoGen. This generic programming framework is designed to automate the assembly and coordination of multiple AI agents for a wide range of applications. Within the context of video generation, AutoGen can streamline the configuration of agents according to the specific requirements of each video generation task to generate complex video content from textual or image-based prompts. Mora drone to butterfly flythrough shot. | Image Source. Role of an AI Agent The paper outlines the architecture involving multiple AI agents, each serving a specific role in the video generation process. Here's a closer look at the role of each AI agent within the framework: Illustration of how to use Mora to conduct video-related tasks Prompt Selection and Generation Agent This agent is tasked with processing and optimizing textual prompts for other agents to process them further. Here are the key techniques used for Mora: GPT-4: This agent uses the generative capabilities of GPT-4 to generate high-quality prompts that are detailed and rich in context. Prompt Selection: This involves selecting or enhancing textual prompts to ensure they are optimally prepared for the subsequent video generation process. This step is crucial for setting the stage for generating images and videos that closely align with the user's intent. Good Read: Interested in GPT-4 Vision alternatives? Check out our blog post. Text-to-Image Generation Agent This agent uses a retrained large text-to-image model to convert the prompts into initial images. The retraining process ensures the model is finely tuned to produce high-quality images, laying a strong foundation for the video generation process. Image-to-Image Generation Agent This agent specializes in image-to-image generation, taking initial images and editing them based on new prompts or instructions. This ability allows for a high degree of customization and improvement in video creation. Image-to-Video Generation Agent This agent transforms static images into dynamic video sequences, extending the visual narrative by generating coherent frames. Here are the core techniques and models: Core Components: It incorporates two pre-trained models: GPT-3 for understanding and generating text-based instructions, and Stable Diffusion for translating these instructions into visual content. Prompt-to-Prompt Technique: The prompt-to-prompt technique guides the transformation from an initial image to a series of images that form a video sequence. Classifier-Free Guidance: Classifier-free guidance is used to improve the fidelity of generated videos to the textual prompts so that the videos remain true to the users' vision. Text-to-Video Generation Agent: This role is pivotal in transforming static images into dynamic videos that capture the essence of the provided descriptions. Stable Video Diffusion (SVD) and Hierarchical Training Strategy: A model specifically trained to understand and generate video content, using a hierarchical training strategy to improve the quality and coherence of the generated videos. Video Connection Agent This agent creates seamless transitions between two distinct video sequences for a coherent narrative flow. Here are the key techniques used: Pre-Trained Diffusion-Based T2V Model: This model uses a pre-trained diffusion-based model specialized in text-to-video (T2V) tasks to connect separate video clips into a cohesive narrative. Text-Based Control: This method uses textual descriptions to guide the generation of transition videos that seamlessly connect disparate video clips, ensuring logical progression and thematic consistency. Image-to-Video Animation and Autoregressive Video Prediction: These capabilities allow the agent to animate still images into video sequences, predict and generate future video frames based on previous sequences, and create extended and coherent video narratives. Mora’s Video Generation Process Mora's video-generation method is a complex, multi-step process that uses the unique capabilities of specialized AI agents within its framework. This process allows Mora to tackle various video generation tasks, from creating videos from text descriptions to editing and connecting existing videos. Here's an overview of how Mora handles each task: Mora’s video generation process. Text-to-Video Generation This task begins with a detailed textual prompt from the user. Then, the Text-to-Image Generation Agent converts the prompts into initial static images. These images serve as the basis for the Image-to-Video Generation Agent, which creates dynamic sequences that encapsulate the essence of the original text and produce a coherent video narrative. Text-Conditional Image-to-Video Generation This task combines textual prompts with a specific starting image. Mora first improves the input with the Prompt Selection and Generation Agent, ensuring that the text and image are optimally prepared for video generation. Then, the Image-to-Video Generation Agent takes over, generating a video that evolves from the initial image and aligns with the textual description. Extend Generated Videos To extend an existing video, Mora uses the final frame of the input video as a launchpad. The Image-to-Video Generation Agent crafts additional sequences that logically continue the narrative from the last frame, extending the video while maintaining narrative and visual continuity. Video-to-Video Editing In this task, Mora edits existing videos based on new textual prompts. The Image-to-Image Generation Agent first edits the video's initial frame according to the new instructions. Then, the Image-to-Video Generation Agent generates a new video sequence from the edited frame, adding the desired changes to the video content. Connect Videos Connecting two videos involves creating a transition between them. Mora uses the Video Connection Agent, which analyzes the first video's final frame and the second's initial frame. It then generates a transition video that smoothly links the two segments into a cohesive narrative flow. Simulating Digital Worlds Mora generates video sequences in this task that simulate digital or virtual environments. The process involves appending specific style cues (e.g., \"in digital world style\") to the textual prompt, guiding the Image-to-Video Generation Agent to create a sequence reflecting the aesthetics of a digital realm. This can involve stylistically transforming real-world images into digital representations or generating new content within the specified digital style. See Also: Read our explainer on Google’s Video Gaming Companion: Scalable Instructable Multiworld Agent [SIMA]. Mora: Experimental Setup As detailed in the paper, the experimental setup for evaluating Mora is comprehensive and methodically designed to assess the framework's performance across various dimensions of video generation. Here's a breakdown of the setup: Baseline The baseline for comparison includes existing open-sourced models that showcase competitive performance in video generation tasks. These models include Videocrafter, Show-1, Pika, Gen-2, ModelScope, LaVie-Interpolation, LaVie, and CogVideo. These models are a reference point for evaluating Mora's advancements and position relative to the current state-of-the-art video generation. Basic Metrics The evaluation framework comprises several metrics to quantify Mora's performance across different dimensions of video quality and condition consistency: Video Quality Measurement Object Consistency: Measures the stability of object appearances across video frames. Background Consistency: Assesses the uniformity of the background throughout the video. Motion Smoothness: Evaluates the fluidity of motion within the video. Aesthetic Score: Gauges the artistic and visual appeal of the video. Dynamic Degree: Quantifies the video's dynamic action or movement level. Imaging Quality: Assesses the overall visual quality of the video, including clarity and resolution. Video Condition Consistency Metric Temporal Style: Measures how consistently the video reflects the temporal aspects (e.g., pacing, progression) described in the textual prompt. Appearance Style: Evaluates the adherence of the video's visual style to the descriptions provided in the prompt, ensuring that the generated content matches the intended appearance. Self-Defined Metrics Video-Text Integration (VideoTI): Measures the model’s fidelity to textual instructions by comparing text representations of input images and generated videos. Temporal Consistency (TCON): Evaluates the coherence between an original video and its extended version, providing a metric for assessing the integrity of extended video content. Temporal Coherence (Tmean): Quantifies the correlation between the intermediate generated and input videos, measuring overall temporal coherence. Video Length: This parameter quantifies the duration of the generated video content, indicating the model's capacity for producing videos of varying lengths. Implementation Details The experiments use high-performance hardware, specifically TESLA A100 GPUs with substantial VRAM. This setup ensures that Mora and the baseline models are evaluated under conditions allowing them to fully express their video generation capabilities. The choice of hardware reflects the computational intensity of training and evaluating state-of-the-art video generation models. Mora video generation - Fish underwater flythrough Limitations of Mora The paper outlines several limitations of the Mora framework. Here's a summary of these key points: Curating High-Quality Video Datasets Access to high-quality video datasets is a major challenge for training advanced video generation models like Mora. Copyright restrictions and the sheer volume of data required make it difficult to curate diverse and representative datasets that can train models capable of generating realistic and varied video content. Read Also: The Full Guide to Video Annotation for Computer Vision. Quality and Length Gaps While Mora demonstrates impressive capabilities, it has a noticeable gap in quality and maximum video length compared to state-of-the-art models like Sora. This limitation is particularly evident in tasks requiring the generation of longer videos, where maintaining visual quality and coherence becomes increasingly challenging. Simulating videos in Mora vs in Sora. Instruction Following Capability Mora sometimes struggles to precisely follow complex or detailed instructions, especially when generating videos that require specific actions, movements, or directionality. This limitation suggests that further improvement in understanding and interpreting textual prompts is needed. Human Visual Preference Alignment The experimental results may not always align with human visual preferences, particularly in scenarios requiring the generation of realistic human movements or the seamless connection of video segments. This misalignment highlights the need to incorporate a more nuanced understanding of physical laws and human dynamics into the video-generation process. Mora Vs. Sora: Feature Comparisons The paper compares Mora and OpenAI's Sora across various video generation tasks. Here's a detailed feature comparison based on their capabilities in different aspects of video generation: Check out the project repository on GitHub. Mora Multi-Agent Framework: Key Takeaways The paper \"Mora: Enabling Generalist Video Generation via a Multi-Agent Framework\" describes Mora, a new framework that advances video technology. Using a multi-agent approach, Mora is flexible and adaptable across various video generation tasks, from creating detailed scenes to simulating complex digital worlds. Because it is open source, it encourages collaboration, which leads to new ideas, and lets the wider research community add to and improve its features. Even though Mora has some good qualities, it needs high-quality video datasets, video quality, length gaps, trouble following complicated instructions correctly, and trouble matching outputs to how people like to see things. Finding solutions to these problems is necessary to make Mora work better and be used in more situations. Continuing to improve and develop Mora could change how we make video content so it is easier for creators and viewers to access and have an impact.\n\nMar 26 2024\n\n8 M\n\nQwen-VL and Qwen-VL-Chat: Introduction to Alibaba’s AI Models\n\nQwen-VL is a series of open-source large vision-language models (LVLMs), offering a potent combination of advanced capabilities and accessibility. As an open-source project, Qwen-VL not only democratizes access to cutting-edge AI technology but also positions itself as a formidable competitor to established models from tech giants like OpenAI’s GPT-4V and Google’s Gemini. In the competitive landscape of LVLMs, Qwen-VL has quickly risen to the forefront, securing its place as a leader on the OpenVLM leaderboard. This leaderboard, which encompasses 38 different VLMs including GPT-4V, Gemini, QwenVLPlus, LLaVA, and others, serves as a comprehensive benchmark for evaluating model performance across 13 distinct multimodal tasks. OpenVLM Leaderboard Qwen-VL's performance across these benchmarks underscores its versatility and robustness in handling various vision-language tasks with unparalleled accuracy and efficiency. By leading the charge on the OpenVLM leaderboard, Qwen-VL sets a new standard for excellence in the field, pushing the boundaries of what is possible with LVLMs and paving the way for future advancements in multimodal AI research. Introduction to Large-scale Vision Language Models (LVLMs) Large Language Models (LLMs) have attracted attention in recent years for their remarkable text generation and comprehension capabilities in the field of generative AI. However, their limitation to processing text alone has constrained their utility in various applications. In response to this limitation, a new class of models known as Large Vision Language Models (LVLMs) has come up, aiming to integrate visual data with textual information to address vision-centric tasks. LVLMs improve conventional LLMs by integrating vision language learning, thus extending their applicability to include image datasets. However, despite their promising potential, open-source LVLM implementations encounter hurdles such as inadequate training and optimization when compared to proprietary models. Also, understanding visual content still remains a significant challenge for existing LVLM frameworks. Overview of Qwen-VL The Qwen-VL series represents a significant advancement in Large Vision Language Models (LVLMs), designed to overcome the limitations of existing models and equip LLMs with visual processing capabilities. Built upon the Alibaba Cloud’s 7 billion parameter model, Qwen-7B language model, the Qwen-VL series introduces a visual receptor architecture comprising a language-aligned visual encoder and a position-aware adapter. This architecture enables Qwen-VL models to effectively process visual inputs, generate responses based on prompts, and perform various vision-language tasks such as image recognition, image captioning, visual question answering, and visual grounding. Qwen-VL models demonstrate leading performance on vision-centric benchmarks and support multiple languages, including English and Chinese. For more information on VLMs, read the blog Guide to Vision-Language Models (VLMs) Key Features of Qwen-VL Qwen-VL models demonstrate good accuracy on a wide range of vision-centric understanding benchmarks, surpassing other SOTA models of similar scales. They excel not only in conventional benchmarks such as captioning and question-answering but also in recently introduced dialogue benchmarks. Here are the key features of Qwen-VL: Multi-lingual Support: Similar to Qwen-LM, Qwen-VLs are trained on multilingual image-text data, with a substantial corpus in English and Chinese. This enables Qwen-VLs to naturally support English, Chinese, and other multilingual instructions. Multi-image Capability: During training, Qwen-VLs can handle arbitrary interleaved image-text data as inputs, allowing them to compare, understand, and analyze context when multiple images are provided. Fine-grained Visual Understanding: Qwen-VLs exhibit highly competitive fine-grained visual understanding abilities, thanks to their higher-resolution input size and fine-grained corpus used during training. Compared to existing vision-language generalists, Qwen-VLs demonstrate superior performance in tasks such as grounding, text-reading, text-oriented question answering, and fine-grained dialogue comprehension. Vision-centric Understanding: This allows the model to comprehensively interpret and process visual information. With advanced architecture integrating a language-aligned visual encoder and position-aware adapter, Qwen-VL excels in tasks like image captioning, question answering, and visual grounding. Its fine-grained analysis ensures precise interpretation of visual content, making Qwen-VL highly effective in vision-language tasks and real-world applications. Design Structure of Qwen-VL Beginning with the foundation of Qwen-LM, the model is enhanced with visual capacity through several key components: Visual Receptor: Qwen-VL incorporates a carefully designed visual receptor, which includes a visual encoder and adapter. This component is responsible for processing image inputs and extracting fixed-length sequences of image features. Input-Output Interface: The model's input-output interface is optimized to differentiate between image and text feature inputs. Special tokens are utilized to delineate image feature input, ensuring seamless integration of both modalities. 3-stage Training Pipeline: Qwen-VL employs a sophisticated 3-stage training pipeline to optimize model performance. This pipeline encompasses comprehensive training stages aimed at fine-tuning the model's parameters and enhancing its ability to comprehend and generate responses for both text and image inputs. Multilingual Multimodal Cleaned Corpus: Qwen-VL is trained on a diverse multilingual multimodal corpus, which includes cleaned data encompassing both textual and visual information. This corpus facilitates the model's ability to understand and generate responses in multiple languages while effectively processing various types of visual content. Model Architecture of Qwen-VL The architecture of Qwen-VL comprises three key components, each contributing to the model's robustness in processing both text and visual inputs. Large Language Model Qwen-VL leverages a large language model as its foundational component. This machine learning model is initialized with pre-trained weights obtained from Qwen-7B, ensuring a strong linguistic foundation for the model's language processing capabilities. Visual Encoder Qwen-VL employs the Vision Transformer (ViT) architecture, utilizing pre-trained weights from Openclip's ViT-bigG. During both training and inference, input images are resized to a specific resolution. The visual encoder processes these images by dividing them into patches with a stride of 14, thereby generating a set of image features that encapsulate visual information. Position-aware Vision-Language Adapter To address efficiency concerns arising from long sequences of image features, Qwen-VL introduces a vision-language adapter. This adapter is designed to compress the image features, enhancing computational efficiency. It consists of a single-layer cross-attention module initialized randomly. This module utilizes a group of trainable embeddings as query vectors and the image features from the visual encoder as keys for cross-attention operations. By employing this mechanism, the visual feature sequence is compressed to a fixed length of 256. To preserve positional information crucial for fine-grained image comprehension, 2D absolute positional encodings are incorporated into the query-key pairs of the cross-attention mechanism. This ensures that positional details are retained during the compression process. The compressed image feature sequence of length 256 is then fed into the large language model, enabling Qwen-VL to effectively process both textual and visual inputs and perform a wide range of vision-language tasks with high accuracy and efficiency. Training Pipeline of Qwen-VL series For more information, read the official paper released on Arxiv: Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond. Performance of Qwen-VL against State-of-The-Art LVLMs The performance of Qwen-VL models, particularly Qwen-VL-Max, surpasses SOTA models such as Gemini Ultra and GPT-4V in various text-image multimodal tasks. Compared to the open-source version of Qwen-VL, these models achieve comparable results to Gemini Ultra and GPT-4V, while significantly outperforming previous best results from open-source models. Performance of Qwen-VL-Plus and Qwen-VL-Max against other LVLM In particular, Qwen-VL-Max demonstrates superior performance over GPT-4V from OpenAI and Gemini from Google in tasks related to Chinese question answering and Chinese text comprehension. This achievement highlights the advanced capabilities of Qwen-VL-Max and its potential to establish new benchmarks in multimodal AI research and application. It should also be noted that most SOTA models are not trained on chinese language. Capabilities of Qwen-VL Qwen-VL exhibits a diverse range of capabilities that enable it to effectively comprehend and interact with visual and textual information, as well as reason and learn from its environment. These capabilities include: Basic Recognition Capabilities Qwen-VL demonstrates strong basic recognition capabilities, accurately identifying and describing various elements within images, including common objects, celebrities, landmarks, and intricate details. Recognition capabilities of Qwen-VL Visual Agent Capability As a visual agent, Qwen-VL is capable of providing detailed background information, answering questions, and analyzing complex visual content. It can also compose poetry in multiple languages inspired by visual stimuli and analyze everyday screenshots. Visual Agent Capabilities of Qwen-VL Visual Reasoning Capability Qwen-VL possesses advanced visual reasoning capabilities, extending beyond content description to comprehend and interpret intricate representations such as flowcharts, diagrams, and other symbolic systems. It excels in problem-solving and reasoning tasks, including mathematical problem-solving and profound interpretations of charts and graphs. Qwen-VL has advanced visual reasoning capabilities Text Information Recognition and Processing Qwen-VL exhibits enhanced text information recognition and processing abilities, efficiently extracting information from tables and documents, reformatting it to meet customized output requirements, and effectively identifying and converting dense text. It also supports images with extreme aspect ratios, ensuring flexibility in processing diverse visual content. Advanced text information recognition and processing abilities of Qwen-VL Few-shot Learning on Vision-Language Tasks Qwen-VL demonstrates satisfactory in-context learning (few-shot learning) ability, achieving superior performance on vision-language tasks such as question answering and image captioning compared to models with similar numbers of parameters. Its performance rivals even larger models, showcasing its adaptability and efficiency in learning from limited data. For more information on few-shot learning, read the blog Few Shot Learning in Computer Vision: Approaches & Uses Qwen-VL Availability Qwen-VL, including Qwen-VL-Plus and Qwen-VL-Max, is now readily accessible through various platforms, offering researchers and developers convenient access to its powerful capabilities: HuggingFace: Users can access Qwen-VL-Plus and Qwen-VL-Max through the Huggingface Spaces and Qwen website, enabling seamless integration into their projects and workflows. Dashscope APIs: The APIs of Qwen-VL-Plus and Qwen-VL-Max are available through the Dashscope platform, providing developers with the flexibility to leverage its capabilities for their AI applications. Detailed documentation and quick-start guides are available on the Dashscope platform for easy integration. QianWen Web Portal: By logging into the Tongyi QianWen web portal and switching to \"Image Understanding\" mode, users can harness the latest Qwen-VL-Max capabilities for image understanding tasks. This mode offers additional functionalities tailored specifically for image processing and understanding. ModelScope: The Qwen-VL-Chat demo is available on modelscope. GitHub Repository: The code and model weights of both Qwen-VL and Qwen-VL-Chat are openly available to download on GitHub, allowing researchers and developers to explore, modify, and utilize them freely. The commercial use of these resources is permitted, enabling their integration into commercial projects and applications. Qwen-VL-Chat Qwen-VL-Chat, as a generalist multimodal LLM-based AI assistant, supports complex interactions, including multiple image inputs, multi-round question answering, and creative capabilities. Unlike traditional vision-language chatbots, Qwen-VL-Chat's alignment techniques enable it to comprehend and respond to complex visual and textual inputs with superior accuracy and flexibility. Here's how Qwen-VL-Chat stands out in real-world dialog benchmarks and compares with existing models: Qwen-VL-Chat Vs. Vision-Language Chat Performance of Qwen-VL against other generalist models across various tasks Qwen-VL-Chat's advanced capabilities are evaluated using the TouchStone benchmark, which assesses overall text-image dialogue capability and alignment with humans. Unlike conventional models like chatGPT or Bard, Qwen-VL-Chat excels in handling direct image input, thanks to fine-grained image annotations provided by human labeling. With a comprehensive coverage of 300+ images, 800+ questions, and 27 categories, including attribute-based Q&A, celebrity recognition, writing poetry, summarizing multiple images, product comparison, and math problem solving, Qwen-VL-Chat achieves superior performance in understanding and responding to complex visual and textual inputs. You can find the official tutorial to implement Qwen-VL-Chat on your own on Github. Real-world Dialog Benchmark Qwen-VL-Chat's outstanding results in other multimodal benchmarks, such the MME Benchmark and Seed-Bench, demonstrate that its performance evaluation extends beyond the TouchStone benchmark. In both the perceptual and cognition tracks, Qwen-VL-Chat obtains state-of-the-art scores in the MME Benchmark, an extensive evaluation of multimodal large language models. The Qwen series, which includes Qwen-VL-Chat, achieves state-of-the-art performance in Seed-Bench, a benchmark consisting of 19K multiple-choice questions with precise human annotations. Qwen-VL: What’s Next? The release of the Qwen-VL series represents a significant stride forward in large-scale multilingual vision-language models, with the goal of advancing multimodal research. Qwen-VL has demonstrated its superiority over comparable artificial intelligence models across various benchmarks, facilitating multilingual complex conversations, multi-image interleaved conversations, grounding in Chinese, and fine-grained recognition. Looking ahead, the focus is on further enhancing Qwen-VL's capabilities in several key dimensions: Multi-modal Generation The team plans to integrate Qwen-VL with more modalities, including speech and video. By expanding its scope to encompass these modalities, Qwen-VL will enhance its ability to understand and generate content across a wider range of inputs. Multi-Modal Generation This generative AI model will be further developed to excel in multi-modal generation, particularly in generating high-fidelity images and fluent speech. By enhancing its ability to generate content across multiple modalities with high fidelity and fluency, Qwen-VL will advance the state-of-the-art in multimodal AI systems. Augmentation of Model Size and Training Data Efforts are underway to scale up the model size, training data, and resolution of Qwen-VL. This enhancement aims to enable Qwen-VL to handle more complex and intricate relationships within multimodal data, leading to more nuanced and comprehensive understanding and generation of content.\n\nFeb 29 2024\n\n8 M\n\nGemini 1.5: Google's Generative AI Model with Mixture of Experts Architecture\n\nIn December 2023, Google launched the Gemini 1.0 family of models that outperformed state-of-the-art (SoTA) models in multimodal AI capabilities. Fast-forward to February 2024, and the Google Deepmind research team has launched Gemini 1.5 Pro with up to 10 million context windows! Not only that, it maintains near-perfect across the entire context and uses a mixture-of-experts (MoE) architecture for more efficient training & higher-quality responses. In this article, you will learn about: The superior performance benchmarks of Gemini 1.5 Why it performs better than SoTA at textual, visual, and audio capabilities How well it handles long-context tasks, especially with MoE as it’s architectural backbone How you can get started using it Before we jump into it, let’s set the tone with an overview of the MoE architecture that backs Gemini 1.5. TL;DR Gemini 1.5 is a Sparse mixture-of-experts (MoE) multimodal model with a context window of up to 10 million tokens. It excels at long-term recall and retrieval; generalizes zero-shot to long instructions like analyzing 3 hours of video, and 22 hours of audio with near-perfect recall. It performs better than Gemini 1.0 Pro and 1.0 Ultra but performs worse than 1.0 Ultra for audio and vision. Although there are no detailed insights on the model size, architectural experiments, or the number of experts, the model performs well at in-context memorization and generalization Mixture-of-Experts (MoE) Architecture Gemini 1.5 Pro uses a mixture-of-experts (MoE) architecture for efficient training & higher-quality responses, building on a long line of Google research efforts on sparse models. At its core, MoE diverges from traditional deep learning and Transformer architectures by introducing a dynamic routing mechanism that selectively activates different subsets of parameters (referred to as \"experts\") depending on the input data. It learns to selectively activate only the most relevant expert pathways in its neural network for nuanced and contextually aware outputs. This approach enables the model to scale more effectively in terms of computational efficiency and capacity without a linear increase in computational demands. In the context of Gemini 1.5, the MoE architecture contributes to efficient training and serving. Concentrating computational resources on the most relevant parts of the model for each input allows for faster convergence and improved performance without necessitating the proportional increase in computational power typically associated with scaling up the model size. Gemini 1.5 - Model Functionalities Gemini 1.5 drops with some impressive functionalities that beat SoTA models: Huge context window that spans up to 10 million-token context length Reduced training compute with the mixture-of-experts architecture Superior performance compared to Gemini 1.0 models, GPT-4, and other SoTA Huge Context Window A model’s “context window” comprises tokens, the building blocks for processing a user’s query. Tokens can be entire parts or subsections of words, images, videos, audio, or code. The bigger a model’s context window, the more information it can take in and process at a given prompt. Gemini 1.5 is a highly capable multimodal model with token context lengths ranging from 128K to 1 million token context lengths for production applications and up to 10 million for research. This unlocks a lot of use cases: Across reasoning about long text documents Making sense of an hour of video (full movies) 11 hours of audio Entire podcast series 700,000 words 30,000 lines of code simultaneously These capabilities are several times greater than other AI models, including OpenAI’s GPT-4, which powers ChatGPT. Context lengths of foundation models with Gemini 1.5 scaling up to 10 million tokens in research Reduced Training Compute The training compute required to train Gemini 1.5 were TPUv4 accelerators of multiple 4096-chip pods. This underscored the model's reliance on high-performance computing resources to perform well, but it also needed training efficiency techniques with the MoE architecture to be optimal. Gemini 1.5 significantly reduced compute requirements for training despite the larger context windows. This achievement is pivotal in the progress of AI model training efficiency, addressing one of the most pressing challenges in the field: the environmental and economic costs associated with training large-scale AI models. The reduction in training compute is primarily down to the Mixture-of-Experts (MoE) architectural backbone, which Gemini 1.5 uses to optimize computational resources. Beyond that, Gemini 1.5 incorporates state-of-the-art techniques such as sparsity in the model's parameters, which means that only a subset of the model's weights is updated during each training step. This approach reduces the computational load, leading to faster training times and lower energy consumption. According to the technical report, combining those processes to train the model led to remarkable performance without the proportional increase in resource consumption typically seen in less advanced models. Recalling and Reasoning Google Gemini 1.5 Pro sets a new standard in AI's ability to recall and reason across extensive multimodal contexts. The ten million-token context window—the largest of any foundational model, so far—enables Gemini 1.5 Pro to demonstrate unparalleled proficiency in synthesizing and interpreting vast amounts of information. Gemini 1.5 Pro achieves near-perfect recall in complex retrieval tasks across long text documents, videos, and audio, which shows its understanding of the input. In tests from the report, Gemini 1.5 Pro learned new languages from sparse instructional materials 🤯. This model's proficiency in recalling specific details from large datasets and its capability to apply this knowledge in reasoning tasks usher in a new era in AI applications—ranging from academic research and comprehensive code analysis to nuanced content creation. Superior Performance Benchmark Gemini 1.5 Pro demonstrates remarkable improvements over state-of-the-art (SotA) models, including GPT-4V, in tasks spanning text, code, vision, and audio. Some of the benchmarks for which Gemini 1.5 Pro achieves SotA accuracy include 1H-VideoQA and EgoSchema. This indicates Gemini 1.5 Pro's advanced long-context multimodal understanding. Learn more about how OpenAI’s GPT-Vision is expected to compare to the Gemini family of models in our explainer blog post. In core text evaluations, Gemini 1.5 Pro consistently outperforms its predecessors (Gemini 1.0 Pro and Ultra) in various domains such as Math, Science & Reasoning, Coding, Multilinguality, and Instruction Following. The model shows substantial improvements, particularly in Math and Science Reasoning, where it outperforms Gemini 1.0 Ultra, and in Coding tasks, it sets a new SotA accuracy benchmark on EgoSchema. Gemini 1.5 Pro's performance in multilingual evaluations highlights its enhanced ability to process and understand multiple languages. It shows significant improvements over both Gemini 1.0 models and other specialist models like USM and Whisper in speech understanding tasks. Needle In A Haystack (NIAH) Evaluation The Needle In A Haystack (NIAH) evaluation showcases Gemini 1.5 Pro's capability to retrieve specific information (\"needle\") from a massive amount of data (\"haystack\") across different modalities. This evaluation underscores the model's efficiency in long-context understanding and recall accuracy. Gemini 1.5 Pro achieves near-perfect “needle” recall (>99.7%) up to 1M tokens of “haystack” in all modalities (i.e., text, video audio) and maintains this recall performance when extending to 10 M tokens across modalities Context Window - Text Modality: Recall to Token Count Gemini 1.5 Pro excels in the text modality, with the model achieving over 99% recall for up to 10 million tokens, or approximately 7 million words. This capacity for deep, nuanced understanding and recall from vast quantities of text sets a new benchmark for AI performance in natural language processing. It can sift through large volumes of text to find specific information. Text needle-in-a-haystack task comparison between Gemini 1.5 Pro and GPT-4 Turbo The model demonstrates high recall rates for identifying exact text segments within extensive documents. Context Window - Audio Modality: Recall to Token Count Gemini 1.5 Pro demonstrates an exceptional ability to recall information from audio data, achieving near-perfect recall (>99.7%) up to 2 million tokens, equivalent to approximately 22 hours of audio content. It was able to recall and identify specific audio segments (\"needles\") embedded within long audio streams (\"haystacks\"). Audio version of the needle-in-a-haystack experiment comparing Gemini 1.5 Pro and a combination of Whisper and GPT-4 Turbo This represents a significant advancement over combining two SoTA models like Whisper + GPT-4 Turbo in a recall-to-token count comparison, which struggles with long-context audio processing. Context Window - Video Modality: Recall to Token Count Gemini 1.5 Pro maintains high recall performance in the video modality, successfully retrieving information from video data up to 2.8 million tokens, correlating to around 3 hours of video content. The \"Video Needle In A Haystack\" task tested the model's performance in recalling specific video frames from lengthy videos. This is critical for tasks requiring detailed understanding and analysis of long-duration video sequences. It can accurately pinpoint and recall specific moments or information from extensive video sequences. Multineedle in Haystack Test The researchers created a generalized version of the needle in a haystack test, where the model must retrieve 100 different needles hidden in the context window. The results? Gemini 1.5 Pro’s performance was above that of GPT-4 Turbo at small context lengths and remains relatively steady across the entire 1M context window. At the same time, the GPT-4 Turbo model drops off more quickly (and cannot go past 128k tokens). Multineedle in Haystack Test Textual Capabilities of Gemini 1.5 Mathematical and Scientific Textual Reasoning Gemini 1.5 Pro shows a +28.9% improvement over Gemini 1.0 Pro and a +5.2% improvement over Gemini 1.0 Ultra. This indicates a substantial increase in its ability to handle complex reasoning and problem-solving tasks. This proficiency is attributed to its extensive training dataset, which includes a wide array of scientific literature and mathematical problems, so the model can grasp and apply complex concepts accurately. Coding In Coding tasks, Gemini 1.5 Pro marked a +8.9% improvement over 1.0 Pro and +0.2% over 1.0 Ultra, showcasing its superior algorithmic understanding and code generation capabilities. The model can 𝐚𝐜𝐜𝐮𝐫𝐚𝐭𝐞𝐥𝐲 𝐚𝐧𝐚𝐥𝐲𝐳𝐞 an entire code library in a single prompt, without the need to fine-tune the model, including understanding and reasoning over small details that a developer might easily miss. Problem Solving Capability across 100,633 lines of code Instructional Understanding Gemini 1.5 Pro excels in Instruction Following, surpassing the 1.0 series in comprehending and executing complex (+9.2% over 1.0 Pro and +2.5% over 1.0 Ultra), multi-step instructions across various data formats and tasks. This indicates its advanced natural language understanding and ability to process and apply knowledge in a contextually relevant manner. Multilinguality The model also shows improvements in handling multiple languages, with a +22.3% improvement over 1.0 Pro and a slight +6.7% improvement over 1.0 Ultra. This highlights its capacity for language understanding and translation across diverse linguistic datasets. This makes it an invaluable tool for global communication and preserving and revitalizing endangered languages. Kalamang has almost no online presence. Machine Translation from One Book (MTOB: arxiv.org/abs/2309.16575) is a recently introduced benchmark evaluating the ability of a learning system to learn to translate Kalamang from just a single book. Gemini 1.5 Pro still translates the user prompt with astonishing accuracy. Visual Capabilities of Gemini 1.5 The model's multimodal understanding is outstanding in Image and Video Understanding tasks. Gemini 1.5 Pro's performance in these areas reflects its ability to interpret and analyze visual data, making it an indispensable tool for tasks requiring a nuanced understanding of text and media. Image and Video Understanding For image understanding, there's a +6.5% improvement over 1.0 Pro but a -4.1% difference compared to 1.0 Ultra. In video understanding, however, Gemini 1.5 Pro shows a significant +16.9% improvement over 1.0 Pro and +3.8% over 1.0 Ultra, indicating robust enhancements in processing and understanding visual content. These are some areas Gemini 1.5 performs great at: Contextual Understanding: Gemini 1.5 integrates visual data with textual descriptions, enabling it to understand the context and significance of visual elements in a comprehensive manner. This allows for nuanced interpretations that go beyond mere object recognition. Video Analysis: For video content, Gemini 1.5 demonstrates an advanced ability to track changes over time, recognize patterns, and predict outcomes. This includes understanding actions, events, and even the emotional tone of scenes and providing detailed analyses of video data. Image Processing: In image understanding, Gemini 1.5 utilizes state-of-the-art techniques to analyze and interpret images. This includes recognizing and categorizing objects, understanding spatial relationships, and extracting meaningful information from still visuals. Audio Capabilities of Gemini 1.5 Speech Recognition and Translation In an internal YouTube video-based benchmark, Gemini 1.5 Pro was evaluated on 15-minute segments, showing a remarkable ability to understand and transcribe speech with a word error rate (WER) significantly lower than that of its predecessors and other contemporary models. This capability is especially notable given the challenges posed by long audio segments, where the model maintains high accuracy without the need for segmentation or additional preprocessing. Gemini 1.5 Pro also performed well at translating spoken language from one language to another, maintaining the meaning and context of the original speech. This is particularly important for applications that require real-time or near-real-time translation. Overall, there are mixed results in the audio domain, with a +1.2% improvement in speech recognition over 1.0 Pro but a -5.0% change compared to 1.0 Ultra. In speech translation, Gemini 1.5 Pro shows a slight +0.3% improvement over 1.0 Pro but a -2.2% difference compared to 1.0 Ultra. Gemini 1.5 Core capabilities performance over its predecessor, Gemini 1.0 series of models, Gemini 1.0 Pro and Gemini 1.0 Ultra Long Context Understanding Gemini 1.5 Pro significantly expands the context length to multiple millions of tokens, enabling the model to process larger inputs effectively. This is a substantial improvement over models like Claude 2.1, which has a 200k token context window. Gemini 1.5 Pro maintains a 100% recall at 200k tokens and shows minimal reduction in recall up to 10 million tokens, highlighting its superior ability to manage and analyze extensive data sets. In one example, the model analyzed long, complex text documents, like Victor Hugo’s five-volume novel “Les Misérables” (1382 pages, 732k tokens). The researchers demonstrated multimodal capabilities by coarsely sketching a scene and saying, “Look at the event in this drawing. What page is this on?” With the entire text of Les Misérables in the prompt (1382 pages, 732k tokens), Gemini 1.5 Pro can identify and locate a famous scene from a hand-drawn sketch In another example, Gemini 1.5 Pro analyzed and summarized the 402-page transcripts from Apollo 11’s mission to the moon. “One small step for man, one giant leap for mankind.” Demo of Long Context Understanding Prompt In-Context Learning and the Machine Translation from One Book (MTOB) Benchmark Gemini 1.5 Pro can adapt and generate accurate responses based on minimal instruction. This capability is especially evident in complex tasks requiring understanding nuanced instructions or learning new concepts from a limited amount of information in the prompt. Gemini 1.5 Pro's in-context learning capabilities show its performance on the challenging Machine Translation from One Book (MTOB) benchmark. This benchmark tests the model's ability to learn to translate a new language from a single source of instructional material. In the MTOB benchmark, Gemini 1.5 Pro was tasked with translating between English and Kalamang, a language with a limited online presence and fewer than 200 speakers. Despite these challenges, the report showed that Gemini 1.5 Pro achieved translation quality comparable to that of human learners with the same instructional materials. This underscores the model's potential to support language learning and translation for underrepresented languages, opening new avenues for research and application in linguistics and beyond. Gemini 1.5 Pro Vs. Gemini Ultra While Gemini 1.5 Pro (2024) and Gemini Ultra (2023) are at the forefront of AI research and application, Gemini Pro 1.5 introduces several key advancements that differentiate it from Gemini Ultra. The table below provides an overview and comparison of both models. Use Cases Analyzing Lengthy Videos Analyzing videos is another great capability brought by the fact that Gemini models are naturally multimodal, and this becomes even more compelling with long contexts. In the technical report, Gemini 1.5 Pro was able to analyze movies, like Buster Keaton’s silent 45-minute “Sherlock Jr.” movie. Using one frame per second, the researchers turned the movie into an input context of 684k tokens. The model can then answer fairly complex questions about the video content, such as: “Tell me some key information from the piece of paper that is removed from the person’s pocket and the timecode of that moment.” Or, a very cursory line drawing of something that happened, combined with “What is the timecode when this happens?” Gemini 1.5 analyzing and reasoning over the 45-minute “Sherlock Jr.” movie You can see this interaction here: Multimodal prompting with a 44-minute movie Navigating Large and Unfamiliar Codebases As another code-related example, imagine you’re unfamiliar with a large codebase and want the model to help you understand the code or find where a particular functionality is implemented. In another example, the model can ingest an entire 116-file JAX code base (746k tokens) and help users identify the specific spot in the code that implements the backward pass for auto differentiation. It’s easy to see how the long context capabilities can be invaluable when diving into an unfamiliar code base or working with one you use daily. According to a technical lead, many Gemini team members have been finding it very useful to use Gemini 1.5 Pro’s long context capabilities on our Gemini code base. Gemini 1.5 navigating large and unfamiliar codebases What’s Next? According to a Google blog post, Gemini 1.5 Pro is currently in private preview, and its general availability with a standard 128,000-token context window will come later. Developers and enterprise customers can sign up to try Gemini 1.5 Pro with a context window of up to an experimental 1 million tokens via AI Studio and Google Vertex AI to upload hundreds of pages of text, entire code repos, and long videos and let Gemini reason across them. Try Gemini 1.5 Pro with a context window of up to an experimental 1 million tokens via AI Studio and Google Vertex AI That’s all for now. In the meantime, check out our resources on multimodal AI: Introduction to Multimodal Deep Learning GPT-4 Vision Alternatives Top Multimodal Annotation Tools\n\nFeb 17 2024\n\n10 M\n\nOpenAI Releases New Text-to-Video Model, Sora\n\nOpenAI has responded to the recent debut of Google's Lumiere, a space-time diffusion model for video generation, by unveiling its own creation: Sora. The diffusion model can transform short text descriptions into high-definition video clips up to one minute long. How Does Sora Work? Sora is a diffusion model that starts with a video that resembles static noise. Over many steps, the output gradually transforms by removing the noise. By providing the model with the foresight of multiple frames concurrently, OpenAI has resolved the complex issue of maintaining subject consistency, even when it momentarily disappears from view. OpenAI Sora - AI Video Output Similar to GPT models, Sora uses a transformer architecture. Images and videos are represented as patches, collections of smaller units of data. By representing the data in the same manner, OpenAI was able to train diffusion transformers on a wide range of data of different durations, resolutions, and aspect ratios. Sora leverages the recaptioning techniques from DALL-E3 and as such, the model follows the user’s text instructions closely. Technical overview of OpenAI’s Sora OpenAI has released a few technical details on how the state-of-the-art diffusion model for video generation. Here are the key methodologies and features employed in Sora’s architecture. Video Generated by OpenAI's Sora Unified Representation for Large-Scale Training Sora focuses on transforming visual data into a unified representation conducive to large-scale training of generative models. Unlike previous approaches that often concentrate on specific types of visual data or fixed-size videos, Sora embraces the variability inherent in real-world visual content. By training on videos and images of diverse durations, resolutions, and aspect ratios, Sora becomes a generalist model capable of generating high-quality videos and images spanning a wide range of characteristics. Patch-Based Representations Inspired by the use of tokens in large language models (LLMs), Sora adopts a patch-based representation of visual data. This approach effectively unifies diverse modalities of visual data, facilitating scalable and efficient training of generative models. Patches have demonstrated their effectiveness in modeling visual data, enabling Sora to handle diverse types of videos and images with ease. Turning Visual Data into Patches Video Compression Network To convert videos into patches, Sora first compresses the input videos into a lower-dimensional latent space, preserving both temporal and spatial information. This compression is facilitated by a specialized video compression network, which reduces the dimensionality of visual data while maintaining its essential features. The compressed representation is subsequently decomposed into spacetime patches, which serve as transformer tokens for Sora's diffusion transformer architecture. Diffusion Transformer Sora leverages a diffusion transformer architecture, demonstrating remarkable scalability as video models. Diffusion transformers have proven effective across various domains, including language modeling, computer vision, and image generation. Sora's diffusion transformer architecture enables it to effectively handle video generation tasks, with sample quality improving significantly as training compute increases. Scaling Transformers for Video Generation Native Size Training for High-Quality Video Generation Sora benefits from training on data at its native size, rather than resizing, cropping, or trimming videos to standardized dimensions. This approach offers several advantages, including sampling flexibility, improved framing and composition, and enhanced language understanding. By training on videos at their native aspect ratios, Sora achieves superior composition and framing, resulting in high-quality video generation. Language Understanding and Text-to-Video Generation Training Sora for text-to-video generation involves leveraging advanced language understanding techniques, including re-captioning and prompt generation using models like DALL·E and GPT. Highly descriptive video captions improve text fidelity and overall video quality, enabling Sora to generate high-quality videos accurately aligned with user prompts. Capabilities of Sora OpenAI’s Sora can generate intricate scenes encompassing numerous characters, distinct forms of motion, and precise delineations of subject and background. As OpenAI states “The model understands not only what the user has asked for in the prompt, but also how those things exist in the physical world.” Capabilities of OpenAI Sora Here is an extensive list of capabilities of Sora that OpenAI demonstrated. This definitely says a lot about how powerful it is as a text-to-video tool for creating content generation and simulation tasks. Prompting with Images and Videos Sora's flexibility extends to accepting inputs beyond text prompts, including pre-existing images or videos. Glimpse of Prompt Generated Artwork of an Art Gallery by OpenAI's Sora Animating DALL-E Images Sora can generate videos from static images produced by DALL·E, showcasing its ability to seamlessly animate still images and bring them to life through dynamic video sequences. Current techniques for animating images utilize neural-based rendering methods to produce lifelike animations. However, despite these advancements, achieving precise and controllable image animation guided by text remains a challenge, especially for open-domain images taken in diverse real-world environments. Models like AnimateDiff, AnimateAnything, etc have also demonstrated promising results for animating static images. Extending Generated Videos Sora is adept at extending videos, whether forward or backward in time, to create seamless transitions or produce infinite loops. This capability enables Sora to generate videos with varying starting points while converging to a consistent ending, enhancing its utility in video editing tasks. Video-to-Video Editing Leveraging diffusion models like SDEdit, Sora enables zero-shot style and environment transformation of input videos, showcasing its capability to manipulate video content based on text prompts and editing techniques. Connecting Videos Sora facilitates gradual interpolation between two input videos, facilitating seamless transitions between videos with different subjects and scene compositions. This feature enhances Sora's ability to create cohesive video sequences with diverse visual content. Image Generation Sora is proficient in generating images by arranging patches of Gaussian noise in spatial grids with a temporal extent of one frame, offering flexibility in generating images of variable sizes up to 2048 x 2048 resolution. Photorealistic Image Generation Capability of OpenAI Sora Simulation Capabilities At scale, Sora exhibits amazing simulation capabilities, enabling it to simulate aspects of people, animals, environments, and digital worlds without explicit inductive biases. These capabilities include: 3D Consistency: Generating videos with dynamic camera motion, ensuring consistent movement of people and scene elements through three-dimensional space. Long-Range Coherence and Object Permanence: Effectively modeling short- and long-range dependencies, maintaining temporal consistency even when objects are occluded or leave the frame. Interacting with the World: Simulating actions that affect the state of the world, such as leaving strokes on a canvas or eating a burger with persistent bite marks. Simulating Digital Worlds: Simulating artificial processes, including controlling players in video games like Minecraft while rendering high-fidelity worlds and dynamics. Limitations of Sora Limitation of OpenAI's Sora - Glass Shattering Effect OpenAI acknowledged that the current AI model has known weaknesses, including: Struggling to accurately simulate complex space Understand some instances of cause and effect Confuse spatial details of a prompt Precise descriptions of events over time Safety Considerations of Sora OpenAI is currently working with a team of red teamers to test the AI model prior to making Sora available to OpenAI users. These red teamers consist of domain experts familiar with misinformation, hateful content, and bias. In their release, OpenAI has stated that they will not only leverage existing safety methods leveraged for the release of DALL-E3 but also going one step further to build tools to detect misleading content, including a detection classifier that can identify a video generated by Sora. Once the model is released in OpenAI’s products, they will include C2PA metadata and be monitored by their text and image classifiers: input prompts that violate their usage policy will be rejected and video outputs will be reviewed frame by frame. In addition to all these safety precautions, OpenAI has also stated they will engage policymakers, educators, and artists to understand concerns and identify use cases for the model. Text-to-video synthesis with Sora Noteworthy Text to Video Generation Models Google’s Lumiere Google’s recent introduction of its text-to-video diffusion model, Lumiere is truly remarkable as well. It is designed to generate realistic, diverse, and coherent motion in videos. Lumiere’s capabilities include: text-to-video generation image-to-video generation stylized generation text-based video editing animating content of an image within a user-provided region Video inpainting Unlike traditional approaches that rely on cascaded designs involving distant keyframe generation and subsequent temporal super-resolution, Lumiere introduces Space-Time I-Net architecture. This architecture allows Lumiere to generate the entire temporal duration of the video at once, streamlining the synthesis process and improving global temporal consistency. Google Lumiere's Prompt Generated AI Video By incorporating spatial and temporal down- and up-sampling techniques and leveraging pre-trained text-to-image diffusion models, Lumiere achieves remarkable results in generating full-frame-rate, low-resolution videos. This approach not only enhances the overall visual quality of the synthesized videos but also facilitates a wide range of content creation tasks and video editing applications, including image-to-video conversion, video inpainting, and stylized generation. For more information, read the paper Lumiere: A Space-Time Diffusion Model for Video Generation. Stability AI’s Stable Video Diffusion Stability AI introduced Stable Video Diffusion, a latent video diffusion model designed for state-of-the-art text-to-video and image-to-video generation tasks. Leveraging recent advancements in latent diffusion models (LDMs) initially trained for 2D image synthesis, Stability AI extends its capabilities to generate high-resolution videos by incorporating temporal layers and fine-tuning them on specialized video datasets. Stable Video Diffusion Stability AI addresses the lack of standardized training methods by proposing and evaluating three key stages for successful training of video LDMs: text-to-image pretraining, video pretraining, and high-quality video finetuning. Emphasizing the importance of a meticulously curated pretraining dataset for achieving high-quality video synthesis, Stability AI presents a systematic curation process, including strategies for captioning and data filtering, to train a robust base model. The Stable Video Diffusion model demonstrates the effectiveness of finetuning the base model on high-quality data, resulting in a text-to-video model that competes favorably with closed-source video generation methods. The base model not only provides a powerful motion representation for downstream tasks such as image-to-video generation but also exhibits adaptability to camera motion-specific LoRA modules. It also showcases the versatility of its model by demonstrating its strong multi-view 3D-prior capabilities, serving as a foundation for fine-tuning a multi-view diffusion model that generates multiple views of objects in a feedforward manner. This approach outperforms image-based methods while requiring a fraction of their compute budget, highlighting the efficiency and effectiveness of Stable Video Diffusion in generating high-quality videos across various applications. For more information, read the paper Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets. Meta’s Make-A-Video Meta two years ago introduced Make-A-Video. Make-A-Video leverages paired text-image data to learn representations of the visual world and utilize unsupervised learning on unpaired video data to capture realistic motion. This innovative approach offers several advantages: It expedites the training of text-to-video models by leveraging pre-existing visual and multimodal representations It eliminates the need for paired text-video data It inherits the vast diversity of aesthetic and fantastical depictions from state-of-the-art image generation models. Meta's Make-A-Video Generated Graphic Make-A-Video is a simple yet effective architecture that builds on text-to-image models with novel spatial-temporal modules. First, full temporal U-Net and attention tensors are decomposed and approximated in space and time. Then, a spatial-temporal pipeline is designed to generate high-resolution and frame-rate videos, incorporating a video decoder, interpolation model, and two super-resolution models to enable various applications beyond text-to-video synthesis. Despite the limitations of text describing images, Make-A-Video demonstrates surprising effectiveness in generating short videos. By extending spatial layers to include temporal information and incorporating new attention modules, Make-A-Video accelerates the T2V training process and enhances visual quality. Sora: Key Highlights With a SOTA diffusion model, Sora empowers users to effortlessly transform text descriptions into captivating high-definition video clips, revolutionizing the way we bring ideas to life. Here are the key highlights of Sora: Sora's Architecture: Utilizes a diffusion model and transformer architecture for efficient training. Sora's Methodologies: Sora uses methodologies like unified representation, patch-based representations, video compression network, and diffusion transformer. Capabilities: Includes image and video prompting, DALL·E image animation, video extension, editing, image generation, etc. Limitations: Weaknesses in simulating complex space and understanding causality. Sora's Safety Considerations: Emphasizes safety measures like red team testing, content detection, and engagement with stakeholders. Other significant text-to-video models: Lumiere, Stable Video Diffusion, and Make-A-Video.\n\nFeb 15 2024\n\n3 M\n\nA Guide to Machine Learning Model Observability\n\nArtificial intelligence (AI) is solving some of society's most critical issues. But, lack of transparency and visibility in building models make AI a black box. Users cannot understand what goes on behind the scenes when AI answers a question, gives a prediction, or makes a critical decision. What happens when large language models (LLMs), like GPT-4 and LLaMA, make errors, but users fail to realize the mistake? Or how badly is the model’s credibility affected when users identify errors in the model outcome? Customers lose trust, and the company behind the model can face serious legal and financial consequences. Remember how a minor factual error in Bard’s demo reportedly cost Google $100 billion in market value? That’s where model observability comes into play. It helps understand how a model reaches an outcome using different techniques. In this article, you will: Understand model observability and the importance Challenges related to model observability How model observability applies to modern AI domains, like computer vision and natural language processing (NLP) What is Model Observability? Model observability is a practice to validate and monitor machine learning (ML) model performance and behavior by measuring critical metrics, indicators, and processes to ensure that the model works as expected in production. It involves end-to-end event logging and tracing to track and diagnose issues quickly during training, inference, and decision-making cycles. It lets you monitor and validate training data to check if it meets the required quality standards. It also assists in model profiling, detecting bias and anomalies that could affect the ML model's performance Through observability, ML engineers can conduct root-cause analysis to understand the reason behind a particular issue. This practice allows for continuous performance improvement using a streamlined ML workflow that enables scalability and reduces time to resolution. A significant component of observability is model explainability, which operates under explainable AI or XAI. XAI facilitates root-cause analysis by enabling you to investigate a model’s decision-making process. XAI optimizes model development, debugging, and testing cycles using tools and frameworks. ML model observability is often used interchangeably with ML monitoring. However, an emphasis on model explainability and a focus on why a specific issue occurred makes ML observability broader than ML monitoring. While model monitoring only tells you where and what the problem is, observability goes further by helping you understand the primary reason behind a particular situation. Learn more about the difference between model monitoring and model observability by reading our detailed article ML Observability vs. ML Monitoring Significance of Model Observability ML models in production can encounter several issues that can cause performance degradation and result in poor user experience. These issues can go unnoticed if model observability practices are not followed. With a robust model observability pipeline, data scientists can prevent such problems and speed up the development lifecycle. Below are a few factors that can go wrong during production and warrant the need for model observability. Data Drift Data drift occurs when the statistical properties of a machine learning model’s training data change. It can be a covariate shift where input feature distributions change or a model drift where the relationship between the input and target variables becomes invalid. The divergence between the real-world and training data distribution can occur for multiple reasons, such as changes in underlying customer behavior, changes in the external environment, demographic shifts, product upgrades, etc. Data Drift Performance Degradation As the machine learning application attracts more users, model performance can deteriorate over time due to model overfitting, outliers, adversarial attacks, and changing data patterns. Data Quality Ensuring consistent data quality during production is challenging as it relies heavily on data collection methods, pipelines, storage platforms, pre-processing techniques, etc. Problems such as missing data, labeling errors, disparate data sources, privacy restraints, inconsistent formatting, and lack of representativeness can severely damage data quality and cause significant prediction errors. Let’s discuss how model observability helps. Faster Detection and Resolution of Issues Model observability solutions track various data and performance metrics in real-time to quickly notify teams if particular metrics breach thresholds and enhance model interpretability to fix the root cause of the problem. Regulatory Compliance Since observability tools maintain logs of model behavior, datasets, performance, predictions, etc., they help with compliance audits and ensure that the model development process aligns with regulatory guidelines. Moreover, they help detect bias by making the model decision-making process transparent through XAI. Fostering Customer Trust Model observability enables you to build unbiased models with consistent behavior and reduces major model failures. Customers begin to trust applications based on these models and become more willing to provide honest feedback, allowing for further optimization opportunities. Now, let’s discuss how model observability improves the ML pipeline for different domains. Model Observability in Large Language Models (LLMs) and Computer Vision (CV) Standard machine learning observability involves model validation, monitoring, root-cause analysis, and improvement. During validation, an observability platform evaluates model performance on unseen test datasets and assesses whether a model suffers from bias or variance. It helps you detect issues during production through automated metrics. Finally, insights from the root-cause analysis help you improve the model and prevent similar problems from occurring again. While the flow above applies to all ML models, monitoring and evaluation methods can differ according to model complexity. For instance, LLMs and CV models process unstructured data and have complex inference procedures. So, model observability requires advanced explainability, evaluation, and monitoring techniques to ensure that the models perform as expected. Let’s discuss the main challenges of such models and understand the critical components required to make model observability effective in these domains. Model Observability in Large Language Models LLMs can suffer from unique issues, as discussed below. Hallucinations: Occurs when LLMs generate non-sensical or inaccurate responses to user prompts. No single ground truth: A significant challenge in evaluating an LLM's response is the absence of a single ground truth. LLMs can generate multiple plausible answers, and assessing which is accurate is problematic. Response quality: While responses may be factually accurate, they may not be relevant to a user’s prompt. Also, the language may be ambiguous with an inappropriate tone. The challenge here is to monitor response and prompt quality together, as a poorly crafted prompt will generate sub-optimal responses. Jailbreaks: Specific prompts can cause LLMs to disregard security protocols and generate harmful, biased, and offensive responses. Cost of retraining: Retraining LLMs is necessary to ensure that they generate relevant responses using the latest information. However, retraining is costly. It demands a robust infrastructure involving advanced hardware, personnel expenses, data management costs, etc. You can mitigate the above challenges using a tailored model observability strategy that will allow for better evaluation techniques. Below are common techniques to evaluate LLMs. User feedback: Users can quickly identify and report problems, such as bias, misinformation, and unethical LLM responses to specific prompts. Collecting and assessing user feedback can help improve response quality. Embedding visualization: Comparing the model response and input prompt embeddings in a semantic space can reveal how close the responses to particular prompts are. The method can help evaluate response relevance and accuracy. A Word Embedding Plot Showcasing Relevance of Similar Words Prompt engineering: You can enhance LLM performance by investigating how it responds to several prompt types. It can also help reveal jailbreaking prompts early in the development process. Retrieval systems: A retrieval system can help you assess whether an LLM fetches the correct information from relevant sources. You can experiment by feeding different data sources into the model and trying various user queries to see if the LLM retrieves relevant content. Fine-tuning: Instead of re-training the entire LLM from scratch, you can fine-tune the model on domain-specific input data. Model Observability in Computer Vision Similar to LLMs, CV models have specific issues that require adapting model observability techniques for effectiveness. Below are a few problems with CV modeling. Image drift: Image data drift occurs when image properties change over time. For instance, certain images may have poor lighting, different background environments, different camera angles, etc. Occlusion: Occlusion happens when another object blocks or hides an image's primary object of interest. It causes object detection models to classify objects wrongly and reduces model performance. Occlusion Lack of annotated samples: CV models often require labeled images for training. However, finding sufficient domain-specific images with correct labels is challenging. Labeling platforms like Encord can help you mitigate these issues. Sensitive use cases: CV models usually operate in safety-critical applications like medical diagnosis and self-driving cars. Minor errors can lead to disastrous consequences. As such, model observability in CV must have the following components to address the problems mentioned above. Monitoring metrics: Use appropriate metrics to measure image quality and model performance. Specialized workforce: Domain experts must be a part of the model observability pipeline to help annotators with the image labeling process. Quality of edge devices: Image data collection through edge devices, such as remote cameras, drones, sensors, etc., requires real-time monitoring methods to track a device’s health, bandwidth, latency, etc. Label quality: Ensuring high label quality is essential for effective model training. Automation in the labeling process with a regular review system can help achieve quality standards. Smart labeling techniques, such as active, zero-shot, and few-shot learning, should be part of the observability framework to mitigate annotation challenges. Domain adaptation: CV observability should help indicate when fine-tuning a CV model is suitable by measuring the divergence between source and target data distribution. It should also help inform the appropriate adaptation technique. Want to learn how active learning can solve your CV data annotation challenges? Read our detailed article A Practical Guide to Active Learning for Computer Vision. Now, let’s explore the two crucial aspects of model observability: monitoring and explainability. We’ll discuss various techniques that you can em"
    }
}
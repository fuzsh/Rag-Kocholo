{
    "id": "correct_subsidiary_00146_2",
    "rank": 70,
    "data": {
        "url": "https://www.techtarget.com/searchdatamanagement/definition/data-management",
        "read_more_link": "",
        "language": "en",
        "title": "What is data management and why is it important? Full guide",
        "top_image": "https://cdn.ttgtmedia.com/ITKE/images/logos/TTlogo-379x201.png",
        "meta_img": "https://cdn.ttgtmedia.com/ITKE/images/logos/TTlogo-379x201.png",
        "images": [
            "https://cdn.ttgtmedia.com/rms/onlineImages/stedman_craig.jpg",
            "https://cdn.ttgtmedia.com/rms/onlineimages/data_management-key_parts_of_process_mobile.png",
            "https://cdn.ttgtmedia.com/rms/onlineimages/data_management-rdbms_dbms_intersect_mobile.png",
            "https://cdn.ttgtmedia.com/rms/onlineimages/data_warehouse_vs_data_lake_vs_data_lakehouse-f_mobile.png",
            "https://cdn.ttgtmedia.com/rms/onlineimages/data_management-data_integration_mobile.png",
            "https://cdn.ttgtmedia.com/rms/onlineimages/architecting_a_data_pipeline-f_mobile.png",
            "https://cdn.ttgtmedia.com/rms/onlineImages/data_management-job_requirements_mobile.jpg",
            "https://cdn.ttgtmedia.com/visuals/digdeeper/1.jpg",
            "https://cdn.ttgtmedia.com/rms/onlineimages/pratt_mary.jpg",
            "https://cdn.ttgtmedia.com/rms/onlineimages/competition_a299069360_searchsitetablet_520X173.jpg",
            "https://cdn.ttgtmedia.com/rms/onlineImages/botelho_bridget.jpg",
            "https://cdn.ttgtmedia.com/visuals/searchDataManagement/integration_technology/datamanagement_article_015_searchsitetablet_520X173.jpg",
            "https://cdn.ttgtmedia.com/rms/onlineImages/mullins_craig_02.jpg",
            "https://cdn.ttgtmedia.com/rms/onlineimages/storage_g539954410_searchsitetablet_520X173.jpg",
            "https://cdn.ttgtmedia.com/rms/onlineimages/armstrong_adam.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Craig Stedman",
            "Mary Pratt",
            "Bridget Botelho",
            "Craig Mullins",
            "Adam Armstrong"
        ],
        "publish_date": "2024-05-28T00:00:00+00:00",
        "summary": "",
        "meta_description": "Data management is a set of disciplines and techniques used to process, store and organize data. Learn about the data management process in this guide.",
        "meta_lang": "en",
        "meta_favicon": "/favicon.ico",
        "meta_site_name": "Data Management",
        "canonical_link": "https://www.techtarget.com/searchdatamanagement/definition/data-management",
        "text": "Data management tools and techniques\n\nA wide range of technologies, tools and techniques can be used in the data management process. The following options are available for different aspects of managing data.\n\nDatabase management systems\n\nA database management system (DBMS) is the primary technology used to deploy and administer databases. It's software that acts as an interface between databases and the DBAs, end users and applications that access them. The most prevalent type of DBMS is the relational database management system (RDBMS). Relational databases organize data into tables with rows and columns that contain database records. Related records in different tables are connected through the use of primary and foreign keys, avoiding the need to create duplicate data entries.\n\nRelational databases are built around the SQL programming language and a rigid data model best suited to structured data. They also support the ACID properties -- atomicity, consistency, isolation and durability -- for ensuring data integrity and guaranteeing that transactions are completed correctly. That has all made them the top database choice for transaction processing applications.\n\nHowever, other types of DBMS technologies have emerged as viable alternatives to RDBMSes for different data workloads. Most are categorized as NoSQL databases, which don't impose rigid requirements on data models and database schemas. As a result, they can better store unstructured and semistructured data, such as sensor data, internet clickstream records and network, server and application logs.\n\nThere are four main types of NoSQL systems:\n\nDocument databases that store data in document-like structures.\n\nKey-value databases that pair unique keys and associated values.\n\nWide-column stores with tables that have a large number of columns.\n\nGraph databases that connect related data elements in a graph-like format.\n\nNoSQL has become something of a misnomer, though. While NoSQL databases don't rely on SQL, many now support elements of it and offer some level of ACID compliance. Once meant literally, the term more commonly stands for \"not only SQL\" today.\n\nAdditional database and DBMS options include in-memory databases that store data in a server's memory to boost I/O performance -- with both relational and NoSQL technologies available -- and SQL-based columnar databases designed for analytics applications. Special-purpose databases can be used, too. Notable ones are time series databases that store time-stamped data sequentially; vector databases that support similarity searches in unstructured data sets; and ledger databases that create immutable transaction records. Hierarchical and network databases that run on mainframes and were first developed in the late 1960s are also still available for use.\n\nOrganizations can deploy databases in on-premises or cloud-based systems. With cloud databases, they have a choice between self-managed deployments and database as a service (DBaaS) environments that are managed for them by database vendors.\n\nBig data management\n\nNoSQL databases are often used in big data systems because of their ability to store and manage various data types -- structured, unstructured and semistructured. Big data environments are also commonly built around various open source technologies, including the following:\n\nThe Spark processing engine.\n\nHadoop, a distributed processing framework with a built-in file system that stores data across clusters of commodity servers.\n\nThe HBase database and Hive data warehouse software, which both run on top of Hadoop.\n\nThe Kafka, Flink, Storm and Samza stream processing platforms.\n\nDrill, Presto and Trino, three SQL query engines designed for use in big data applications.\n\nIncreasingly, big data systems are also being deployed in the cloud, using object storage technologies such as Amazon Simple Storage Service (S3), Azure Blob Storage and Google's Cloud Storage.\n\nData warehouses and data lakes\n\nThe two most widely used repositories for managing analytics data are data warehouses and data lakes. A data warehouse -- the more traditional method -- typically is based on a relational or columnar database. It stores structured data that has been pulled together from different operational systems and prepared for analysis. The primary data warehouse use cases are BI querying and enterprise reporting, which enable business analysts and executives to analyze sales, inventory management and other KPIs.\n\nAn enterprise data warehouse includes data from systems across an organization. In large companies, individual subsidiaries and business units might build their own data warehouses. Data marts are another option. They're smaller versions of data warehouses that contain subsets of an organization's data for specific departments or groups of users. In one deployment approach, an existing data warehouse is used to create different data marts; in another, the data marts are built first and then used to populate a data warehouse.\n\nData lakes store pools of big data for use in predictive modeling, machine learning, AI and other data science applications. At first, they were mostly built on Hadoop clusters, but S3 and other cloud object storage services are increasingly being used for data lakes. They're sometimes also deployed on NoSQL databases, and different platforms can be combined in a distributed data lake environment. The data can be processed for analysis when it's ingested, but a data lake often contains raw data stored as is. In that case, data scientists and other analysts typically do their own data preparation work for specific applications.\n\nA third platform option for storing and processing analytical data has also emerged: the data lakehouse. As its name indicates, it combines elements of data lakes and data warehouses. Data lakehouses merge the flexible data storage, scalability and lower cost of a data lake with the querying capabilities and more rigorous data management structure of a data warehouse.\n\nThat enables them to support both BI applications and advanced analytics, essentially by adding data warehousing functionality on top of a data lake. However, data lakehouse platforms are still maturing and might not offer the full capabilities of separate data warehouses and data lakes. They also add new management complexity, including the need for strong metadata management to support the combined functionality.\n\nData integration\n\nThe most widely used data integration technique is extract, transform and load. ETL pulls data from source systems, converts it into a consistent format and then loads the integrated data into a data warehouse or other target system. However, data integration platforms now also support a variety of other integration methods. That includes extract, load and transform (ELT), a variation on ETL that leaves data as is when it's loaded into the target platform. ELT is a common choice for data integration in data lakes and other big data systems.\n\nETL and ELT are batch integration processes that run at scheduled intervals. Data management teams can also do real-time data integration, using methods such as change data capture and streaming data integration. The former applies changes in databases to a data warehouse or other repository as they're made, while the latter integrates streams of real-time data on a continuous basis. Data virtualization is another integration option; it uses an abstraction layer to create a virtual view of data from different systems instead of physically loading the data into a data warehouse.\n\nData modeling\n\nData modelers create a series of conceptual, logical and physical data models that document data sets in a visual form and map them to business requirements for transaction processing and analytics. Common techniques for modeling data include the development of entity relationship diagrams, data mappings and schemas in a variety of model types. Data models often must be updated when new data sources are added or when an organization's information requirements change.\n\nData governance\n\nData governance is primarily an organizational process; software products that help manage data governance programs are available, but they're an optional element. While the programs are often led by data management or governance professionals, they usually include a data governance committee made up of business executives. The committee, or council in some cases, collectively makes decisions on common data definitions and corporate standards for creating, formatting and using data.\n\nAnother key aspect of governance initiatives is data stewardship, which involves overseeing data sets and ensuring that end users comply with the approved data policies. Data steward can be a full- or part-time position, depending on the size of an organization and the scope of its governance program. Data stewards can also come from both business operations and the IT department; either way, a close knowledge of the data they oversee is normally a prerequisite.\n\nData quality\n\nData governance is closely associated with data quality improvement efforts. Ensuring that data quality levels are high is a key part of effective data governance, and metrics that document improvements in data quality are central to demonstrating the business value of governance programs. Key data quality techniques supported by various software tools include the following:\n\nData profiling, which scans data sets to identify outlier values that might be errors.\n\nData cleansing, also known as data scrubbing, which fixes data errors by modifying or deleting bad data.\n\nData validation, which checks data against preset quality rules.\n\nMaster data management\n\nMDM is also affiliated with data governance and data quality management, although it hasn't been adopted as widely as they have. That's partly due to the complexity of MDM programs, which mostly limits them to large organizations. MDM creates a central registry of master data for selected data domains -- what's often called a golden record. The master data is stored in an MDM hub, which feeds the data to analytics systems for consistent analysis and reporting enterprise-wide. The hub can also be configured to push updated master data back to source systems.\n\nData observability\n\nData observability is an emerging process that can augment data quality and data governance initiatives by providing a more complete picture of data health in an organization. Adapted from observability practices in IT systems, data observability monitors data sets and the data pipelines that deliver them to end users, identifying issues that need to be addressed. Data observability tools can be used to automate monitoring, alerting and root cause analysis procedures, as well as to plan and prioritize problem-resolution work.\n\nData management history, evolution and trends\n\nThe first flowering of data management was driven by IT professionals looking to solve the problem of garbage in, garbage out in the earliest computers after recognizing that the machines made errors when they were fed inaccurate or inadequate data. Mainframe-based hierarchical databases became available in the 1960s, bringing more formality to the process of managing data.\n\nThe relational database emerged in the 1970s and cemented its place at the center of the data management ecosystem during the 1980s. The idea of the data warehouse was conceived late in that decade, and early adopters began deploying data warehouses in the mid-1990s. By the early 2000s, relational software was a dominant technology, with a virtual lock on database deployments.\n\nBut Hadoop became available in 2006 and was followed by the Spark processing engine and various other big data technologies. NoSQL databases also started to become available in the same time frame. While relational platforms are still the most widely used data store by far, the rise of those alternatives and the data lake environments they enable gave organizations a broader set of data management choices. The addition of the data lakehouse concept in 2017 further expanded the options.\n\nAll these choices have made many data environments more complex. That's spurring the development of new technologies and processes designed to make them easier to manage. In addition to data observability, they include data fabric, an architectural framework that aims to better unify data assets by automating integration processes and making them reusable. There's also data mesh, a decentralized architecture that gives data ownership and management responsibilities to individual business domains, with federated governance to agree on organizational standards and policies.\n\nNone of those three approaches is widely used yet, though. In its 2023 Hype Cycle report on data management technologies, consulting firm Gartner said data fabrics and data observability tools have been adopted by less than 5% of their target user audiences. It predicted that data observability was still two to five years away from mainstream adoption, while data fabric was five to 10 years away. Data mesh has a higher adoption rate of between 5% and 20% of targeted users, but Gartner expects its core capabilities to eventually be subsumed by data fabrics -- a prediction that data mesh proponents dispute.\n\nThe following are some other notable data management trends:\n\nCloud data management technologies are becoming pervasive -- and pushing innovation. Cloud database services now account for more than half of overall DBMS revenue and almost all the growth in that market, according to Gartner. In the Hype Cycle report, it said the broad migration to DBaaS deployments in the cloud is accompanied by \"an extremely high pace of innovation and change\" on data management technologies as a whole. That includes the development of data ecosystems, alternatively referred to as modern data stacks. They incorporate different tools into a unified data management environment to ease technology integration requirements for users, particularly in the cloud. For companies that can't or aren't ready to fully migrate, hybrid cloud architectures that combine cloud and on-premises systems -- for example, hybrid data warehouse environments -- are also an option.\n\nAugmented data management capabilities aim to streamline processes. Data management software vendors are adding augmented functionality to aid in data quality, database management, data integration, data cataloging and other operations. For example, AI and machine learning technologies can be used in data management to automate repetitive tasks, identify issues and suggest actions. Augmented features are also being applied in FinOps, a process for managing operational costs and decisions on technology investments in data platforms as well as other IT systems.\n\nGenerative AI adds both new data management opportunities and needs. The emergence and rapid adoption of generative AI (GenAI) tools offers more opportunities to incorporate AI into data management processes. For example, GenAI can help data management teams write SQL queries, create data quality rules and classify, label and annotate text data. It also provides a conversational interface for using natural language to perform data management tasks. On the other hand, GenAI adds huge new data sets that further expand data management work. In addition, it's driving increased use of vector databases, which store numerical representations of unstructured data. A niche technology since the early 2000s, vector databases are a good fit for the text data used in the large language models that underpin GenAI tools, such as ChatGPT.\n\nThe growth of edge computing is also creating new data management requirements. As organizations increasingly use remote sensors and IoT devices to collect and process data in edge computing environments, vendors are developing edge data management capabilities. Moving data management outside of cloud and on-premises data centers enables real-time analytics applications on edge data. But it requires new processes for managing and governing the data.\n\nCraig Stedman is an industry editor who creates in-depth packages of content on analytics, data management, cybersecurity and other technology areas for TechTarget Editorial.\n\nJack Vaughan, a former senior news writer at TechTarget, contributed to this article."
    }
}
{
    "id": "dbpedia_4957_3",
    "rank": 59,
    "data": {
        "url": "https://arxiv.org/html/2406.01852v2",
        "read_more_link": "",
        "language": "en",
        "title": "Non-uniformity is All You Need: Efficient and Timely Encrypted Traffic Classification With ECHO",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://arxiv.org/html/extracted/5644590/chapters/introduction/figs/dist_5606_uniform.png",
            "https://arxiv.org/html/extracted/5644590/chapters/introduction/figs/dist_5606_non_uniform.png",
            "https://arxiv.org/html/x1.png",
            "https://arxiv.org/html/x2.png",
            "https://arxiv.org/html/x3.png",
            "https://arxiv.org/html/x4.png",
            "https://arxiv.org/html/x5.png",
            "https://arxiv.org/html/x6.png",
            "https://arxiv.org/html/x7.png",
            "https://arxiv.org/html/x8.png",
            "https://arxiv.org/html/x9.png",
            "https://arxiv.org/html/x10.png",
            "https://arxiv.org/html/x11.png",
            "https://arxiv.org/html/x12.png",
            "https://arxiv.org/html/x13.png",
            "https://arxiv.org/html/x14.png",
            "https://arxiv.org/html/x15.png",
            "https://arxiv.org/html/x16.png",
            "https://arxiv.org/html/x17.png",
            "https://arxiv.org/html/x18.png",
            "https://arxiv.org/html/x19.png",
            "https://arxiv.org/html/x20.png",
            "https://arxiv.org/html/x21.png",
            "https://arxiv.org/html/x22.png",
            "https://arxiv.org/html/x23.png",
            "https://arxiv.org/html/x24.png",
            "https://arxiv.org/html/x25.png",
            "https://arxiv.org/html/x26.png",
            "https://arxiv.org/html/x27.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Shilo Daum , Tal Shapira , Anat Bremler-Barr and David Hay Part of this work was done while D. Hay was on a sabbatical at Princeton University, NJ, USA\n\nAbstract.\n\nWith 95% of Internet traffic now encrypted, an effective approach to classifying this traffic is crucial for network security and management. This paper introduces ECHO—a novel optimization process for ML/DL-based encrypted traffic classification. ECHO targets both classification time and memory utilization and incorporates two innovative techniques.\n\nThe first component, HO (Hyperparameter Optimization of binnings), aims at creating efficient traffic representations. While previous research often uses representations that map packet sizes and packet arrival times to fixed-sized bins, we show that non-uniform binnings are significantly more efficient. These non-uniform binnings are derived by employing a hyperparameter optimization algorithm in the training stage. HO significantly improves accuracy given a required representation size, or, equivalently, achieves comparable accuracy using smaller representations.\n\nThen, we introduce EC (Early Classification of traffic), which enables faster classification using a cascade of classifiers adapted for different exit times, where classification is based on the level of confidence. EC reduces the average classification latency by up to 90%. Remarkably, this method not only maintains classification accuracy but also, in certain cases, improves it.\n\nUsing three publicly available datasets, we demonstrate that the combined method, Early Classification with Hyperparameter Optimization (ECHO), leads to a significant improvement in classification efficiency.\n\n1. Introduction\n\nInternet traffic classification is a critical challenge in network management, security, and optimization. Traffic classification can be used for application identification (Shapira and Shavitt, 2019), traffic categorization (Draper-Gil et al., 2016; Akbari et al., 2021), identifying encryption methods like VPN or Tor (Lashkari et al., 2017), detecting malicious activities such as Distributed Denial of Service (DDoS) attacks (Wang et al., 2018; Sharafaldin et al., 2018; Bronzino et al., 2021), and device fingerprinting (Engelberg and Wool, 2021).\n\nIn the past, classification methods such as port-based classification (using the predefined ports associated with specific services or applications) or signature-based Deep Packet Inspection (DPI) were considered sufficient (cf. (Finsterbusch et al., 2014; Aceto et al., 2010)).\n\nHowever, the use of random ports, shared port usage between applications, and most importantly, the widespread adoption of encryption protocols—such as TLS/SSL encryption, VPN tunneling, and anonymous communication technologies (e.g. Tor (Enghardt et al., 2020; Moore and Papagiannaki, 2005))—have significantly hindered the ability to inspect and classify traffic based on these approaches.\n\nThus, new approaches were introduced to deal with encrypted traffic, mostly focused on extracting features from each flow (namely, the flow representation) and applying Machine Learning (ML) or Deep Learning (DL) methods to classify it. As most data is obfuscated, these methods typically use as input only the size of packets, their inter-arrival times, and their direction.\n\nWhile many proposed methods claim high accuracy in classification, implementing these approaches in real-life scenarios can be challenging due to the large volume of traffic (namely, hundreds of Gb/s in large ISPs) and the huge number of concurrent connections (namely, millions of new connection per second ). Thus, storing flow content, headers, or representations requires considerable memory resources. Moreover, the deployment of DL or large ML models demands significant computational power.\n\nTo reduce both memory and compute, flow representations often aggregate values, and each packet size (or arrival time) is mapped to a bin. The representation then typically holds a counter for each bin, tracking the number of packets whose size (or arrival time) falls within that bin. This is most commonly done in a uniform manner with all bins being the same size. However, such coarse-grained uniform binning might fail to capture subtle nuances in traffic patterns.\n\nThis paper first explores non-uniform binnings, as real-world traffic data often exhibit non-uniform patterns. We introduce variable-sized bins, each representing an interval of values, allowing for a more fine-grained and adaptive approach to capturing non-uniform traffic patterns, with smaller representations. Examples of uniform and non-uniform binnings of the packet size distribution for a sampled flow are shown in Figure 1.\n\nThe selection of bins can be approached either manually or algorithmically. However, as there are usually multiple classes and a significant number of flows, manual selection of bin boundaries may be impractical and imprecise. Employing statistical methods for bin selection (or selecting only a subset of the available features) might introduce an improvement over uniform binning. Nonetheless, model-agnostic techniques might fail to capture complex relations between features. Therefore, we propose a flexible, data-aware and model-aware approach.\n\nInnovatively, to determine the binning, we employ Hyperparameter Optimization (HO). In this method, we apply Bayesian optimization methods to optimize the bin boundaries in the training stage. Our results show that our HO method outperforms other optimization methods, such as feature selection and statistical bin selection across all classification tasks, by up to 15% in accuracy. The full description of the proposed method is in Section 4.\n\nAnother aspect we optimize is the collection time; that is, the time required for collecting properties from the flow until a classification is made. Many existing approaches classify flows only after they end, after a predefined amount of time has elapsed, or after a specific number of packets have been received. Typically, the collection time is orders of magnitude longer than the inference time, thus dominating the overall classification time.\n\nWe notice that some flows contain indicative features useful for classification at an early stage, while others require more time for accurate classification. Thus, we introduce Early Classification (EC), a novel setup performing classification in multiple exit times and making a final prediction only upon reaching a certain level of confidence in the classification. Our system is adaptive, classifying some flows at earlier stages, and waiting longer for flows with low confidence in earlier classifications. As our system may invoke multiple classifiers for each flow, we need to create multiple different representations, which might be prohibitive when the number of concurrent flows is large. Consequently, we present a method for creating additive traffic representations, such that the process of updating the representations between classifiers does not incur any additional memory requirements. Additionally, we propose a method that creates tradeoffs between accuracy and collection time, allowing users to tailor these parameters to their specific requirements. The EC setup is presented in Section 5.\n\nIn conclusion, this work makes two significant contributions. First, we introduce HO. Our exploration showcases the effectiveness of this strategy, which allows us to improve the classification accuracy by 5%–20% in multiple classification tasks, or, alternatively, reduce memory requirements and computational overheads (as smaller classifiers are required) by 90%.\n\nOur second contribution is by presenting EC, a novel method enabling early traffic classification with an adaptive classification approach. Using this method, the required time for classification can be reduced by up to an order of magnitude, with minimal decline in accuracy, and possibly even an improvement.\n\nFinally, we combine the two methods to create ECHO classifiers, achieving both an accuracy boost of up to 10%, and a reduction in the average collection time by 95%.\n\nThe two suggested techniques can be adapted easily to many existing approaches for traffic classification, substantially enhancing their performance.\n\nThe rest of this paper is organized as follows: We discuss related work in Section 2. Section 3 explains the networking model and defines the problem of encrypted traffic classification. Section 4 introduces HO, where Bayesian hyperparameter optimization yields non-uniform binnings for packet sizes and arrival times. Section 5 outlines the Early Classification (EC) setup. Subsequently, in Section 6, we present the combined ECHO approach. Finally, Section 7 presents the evaluation results based on three publicly-available datasets, followed by conclusions drawn in Section 8. Note that additional information, including details about our datasets and comparison between different models and representations, is given in the appendices.\n\n2. Related Work\n\nEncrypted traffic classification has been extensively studied in recent years (cf. (Zhao et al., 2021; Salman et al., 2020) and references therein). Modern classifiers predominantly utilize ML or DL techniques over representations created from the encrypted traffic. Commonly used representations are statistical features extracted from the traffic (Williams et al., 2006; Draper-Gil et al., 2016; Lashkari et al., 2017; Anderson and McGrew, 2017), time-series of the first packets of the flow(Bernaille et al., 2006a; Yang et al., 2021), and distributional representations, which include dist (Engelberg and Wool, 2021)—2 pairs of distribution vectors representing packet sizes and arrival times per traffic direction—and flowpic (Horowicz et al., 2022; Shapira and Shavitt, 2019; Finamore et al., 2023)—a two dimensional representation where each element in the matrix captures the number of packet of a certain size arriving at a certain time. Notice that distributional representations often outperform other representations (Finamore et al., 2023) (see further comparison results in Appendix B), and therefore, are employed in this paper.\n\nMany methods and representations require substantial memory and compute resources, which makes them impractical for large networks.\n\nTo mitigate the memory and compute requirements, some works (e.g., (van Ede et al., 2020; Moore and Zuev, 2005; Ertam and Avcı, 2017; Shafiq et al., 2018; Williams et al., 2006; Fahad et al., 2013; Bronzino et al., 2021; Holland et al., 2021)), have shown how to extract certain features (e.g., flow statistics) from the traffic, or reduce feature dimensionality using feature selection.\n\nNon-uniform binnings (as we suggest in this work) were rarely considered in the context of flow representations. There are two notable works in this area. Barradas et al. (Barradas et al., 2021) have suggested an optimization process that consists of creating uniform binning for packet size distribution and pruning bins using feature selection methods, where the final selection of features is a subset of the original uniform bins. Garcia et al. (Garcia and Korhonen, 2018) have proposed a statistical algorithm based on the Kolmogorov-Smirnov statistical test for selecting histogram bins, aiming at maximizing the separation between classes (measured using the Jensen-Shanon Distance).\n\nOur research focuses on the introduction of non-uniform binnings derived by a Bayesian hyperparameter optimization process. We note that in the field of traffic classification, most previous literature regarding Bayesian hyperparameter optimization is focused on selecting hyperparameters for classifiers (Bergstra et al., 2011, 2013), but not for flow representations. HO, our strategy for creating non-uniform representations, outperforms all previously suggested optimization methods in all explored classification tasks, and can be plugged into many of the previous works to enhance their performance and reduce their memory footprint, as detailed in Section 4.\n\nAs for early classification, some works have suggested using the first few packets of the flow for early classification of flows (Bernaille et al., 2006b, c, a; Yang et al., 2021; Wang et al., 2017b; Lotfollahi et al., 2017; Lopez-Martin et al., 2017; Liu et al., 2019; Zou et al., 2018). While for some classification tasks, the first few packets of a connection suffice, this is not always the case and can lead to less accurate classification (Finamore et al., 2023), this trend is also observable in our experiments (Appendix B).\n\nA first step towards early classification was suggested in (Wang et al., 2023; Sivanathan et al., 2019), where two classifiers are used: the first classifier is based on simpler classification rules, while the second classifier employs more complex rules or a larger model, allowing for more adaptive classification schemes. To the best of our knowledge, in the context of encrypted traffic classification, no extensive research was conducted on building confidence-based classifiers, where a classification is made only after reaching a threshold of certainty.\n\n3. Problem Statement\n\nWe first describe our networking model and define precisely the problem at hand.\n\nData is transmitted over the network from a source to a destination as a flow (namely, a timestamped sequence of packets) over communication links. Typically, many flows share the same communication link. Thus, when observing a communication link, we first need to map the packets to their corresponding flows.\n\nWe note that flows are commonly defined by a five-tuple (namely, the following five header fields in an IP packet: source and destination IP addresses, source and destination ports, and protocol) (Nguyen and Armitage, 2008; Williams et al., 2006). However, this definition is not rigid and can be seamlessly modified to meet different settings or requirements (e.g., in case parts of the five-tuple will be obfuscated in the future or to address a different classification task).\n\nThis work focuses on classifying flows that traverse the network. Thus, when a flow (with identifier q𝑞qitalic_q) is initiated or requires classification, a flow representation rqsubscript𝑟𝑞r_{q}italic_r start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT is generated to capture relevant properties of q𝑞qitalic_q. Since encryption techniques obfuscate most features, we assume only packet sizes, arrival times, and packet directions are available. We aim to classify a large number (namely, millions) of flows simultaneously, implying that, at any given time, many flow representations are being collected and stored in the system.\n\nFlow information is collected for a predefined time τ𝜏\\tauitalic_τ, where the representation rqsubscript𝑟𝑞r_{q}italic_r start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT is built on the fly. Then, rqsubscript𝑟𝑞r_{q}italic_r start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT is passed to a classifier f𝑓fitalic_f, which predicts the label f⁢(rq)𝑓subscript𝑟𝑞f(r_{q})italic_f ( italic_r start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ) of flow q𝑞qitalic_q. Upon classification, the appropriate actions may be taken based on the predicted class, such as generating an alert, adding the information to a database, or blocking certain activities. Subsequently, the flow is marked as classified. Alternatively, time-limited classifications can be employed, where the classification remains valid for a specific duration, after which the flow should be reclassified to ensure up-to-date classification results. Figure 2 illustrates the collection and classification process.\n\n4. Hyperparameter Optimization (HO) of Binnings\n\nIn this section, we present HO, a novel strategy to create non-uniform binnings that enable efficient classification. We have explored both non-uniformity in the packet size distribution vector and the arrival-time distribution vector.\n\nBinning consists of reducing the granularity of the representation such that each original value is mapped to a bin. Each bin represents an interval of the original values. Formally, a uniform representation of size N𝑁Nitalic_N for values in [0,x)0𝑥[0,x)[ 0 , italic_x ) represents the data in N𝑁Nitalic_N bins, where bin i∈{1,…,N}𝑖1…𝑁i\\in\\{1,\\ldots,N\\}italic_i ∈ { 1 , … , italic_N } is for values in [(i−1)⋅xN,i⋅xN)⋅𝑖1𝑥𝑁⋅𝑖𝑥𝑁[\\frac{(i-1)\\cdot x}{N},\\frac{i\\cdot x}{N})[ divide start_ARG ( italic_i - 1 ) ⋅ italic_x end_ARG start_ARG italic_N end_ARG , divide start_ARG italic_i ⋅ italic_x end_ARG start_ARG italic_N end_ARG ). A non-uniform N𝑁Nitalic_N-bin representation has additional degrees of freedom to choose the boundaries by defining a vector\n\nB=[b0,…,bN],𝐵subscript𝑏0…subscript𝑏𝑁B=[b_{0},\\ldots,b_{N}],italic_B = [ italic_b start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , … , italic_b start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ] ,\n\nsuch that 0=b0<b1<…<bN−1<bN=x0subscript𝑏0subscript𝑏1…subscript𝑏𝑁1subscript𝑏𝑁𝑥0=b_{0}<b_{1}<\\ldots<b_{N-1}<b_{N}=x0 = italic_b start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT < italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT < … < italic_b start_POSTSUBSCRIPT italic_N - 1 end_POSTSUBSCRIPT < italic_b start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT = italic_x. In such a representation, bin i∈{1,…,N}𝑖1…𝑁i\\in\\{1,\\ldots,N\\}italic_i ∈ { 1 , … , italic_N } is for values in [bi−1,bi)subscript𝑏𝑖1subscript𝑏𝑖[b_{i-1},b_{i})[ italic_b start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT , italic_b start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ). In general, we examine two additional properties of binnings: data-aware and model-aware binnings.\n\nA binning is data-aware if, in the creation process, it takes properties of the traffic into account. A simple example of data-aware binning is to employ Feature Selection methods and decide not to store some of the (fixed-sized) bins, based on the data (Barradas et al., 2021).\n\nA more sophisticated data-aware binning is to use Statistical methods to select variable-sized bins (represented as a vector B𝐵Bitalic_B) that create a separation between the distributions of the different classes of the data. Inspired by (Garcia and Korhonen, 2018), we use a multiclass version of the Jensen-Shanon distance metric (Lin, 1991), where classes are compared in an one-vs-all manner.\n\nA model-aware binning, which we are first to propose, corresponds to the specific classifier that will eventually use the representations for classification. This approach allows us to tune the representation by the anticipated performance of the final model, distinguishing it from all other binning operations (see Table 1).\n\nOrthogonal to the information available to the binning process is the way the different elements in B𝐵Bitalic_B are selected, where we distinguish between two approaches. The naive greedy approach (as used, for example, in (Garcia and Korhonen, 2018)) iteratively selects a single bin boundary, maximizing the objective function given previous selections.\n\nHO, on the other hand, employs a hyperparameter optimization algorithm for binning selection. Hyperparameter optimization, in general, refers to the process of finding the best combination of hyperparameters (configuration settings that are not learned in the training stage, but rather set before training the classifier) for a given classifier. In our context, the hyperparameters are the selected bins for the representation, the search space includes all possible combinations of bin boundaries, and the objective we aim to maximize in the Bayesian process is the accuracy of a trained model over the validation set. While many hyperparameter optimization approaches exist, we focus on the Tree Parzen Estimators (TPE) method, as it shows superiority over other approaches in past experiments (Bergstra et al., 2011, 2013).\n\nWe note that, unlike (Garcia and Korhonen, 2018), we apply TPE also on the Statistical approach, where the objective function corresponds to the Jensen-Shannon distance derived from the possible binnings. While such bin selection is better than the greedy approach, it is still inferior to HO as the Statistical approach is not model-aware.\n\nFor the experiments in this work, we use k𝑘kitalic_k-fold cross-validation (KFCV) (Kohavi, 1995). Therefore, to validate the TPE process, we use nested cross-validation (NCV) (Cawley and Talbot, 2010), an extension of KFCV for hyperparameter optimization. NCV consists of an outer loop for standard KFCV, selecting training and test sets in each iteration. In the inner loop, we use the training data to iteratively select optimal hyperparameters through another KFCV process. We use k=5𝑘5k=5italic_k = 5 in both the outer and inner loops, implying that for each evaluation in the inner loop, 64%percent6464\\%64 % of the flows are used for training, 16%percent1616\\%16 % for validating the hyperparameters by calculating the objective function, and 20%percent2020\\%20 % for testing (in the outer loop). It is important to note that different iterations of the outer loop might yield different hyperparameter values.\n\nNotice that in our settings, the optimization process is done offline on labeled data, and therefore, has fewer constraints regarding computational power or latency.\n\nOnce the optimization process finishes and an optimal result is achieved, the real-time classifier is adapted and uses the optimal hyperparameters (namely, the bin boundaries) achieved by the offline optimizer.\n\nAfter selecting the bin boundaries, to create our representations without any computational overhead, we create a mapping, using a small direct-access array of size x𝑥xitalic_x, between every possible packet size (or discretized arrival time) to its bin index; this implies that the process of building our representations (which is done online, packet by packet) does not incur additional computations (only one memory access).\n\nTo capture changes in traffic patterns or in the underlying applications, we propose to run HO periodically, upon degradation of accuracy, or manually if a change in the network condition is known. The process should run in the background without interfering with ongoing classifications.\n\nThe results achieved using HO, as well as a comprehensive comparison to the alternative methods, are shown in Section 7.1.\n\n5. Early Classification (EC) of Traffic\n\nIn this section, we will describe the system setup enabling early traffic classification. EC not only improves the Quality of Service (QoS) and the network security but also saves memory requirements, as flow representations are stored for shorter periods.\n\nThis method is based on the assumption that some flows have indicative features at earlier stages and there is no need to wait for a long time to reach an accurate classification. Therefore, in the proposed setup, classification occurs upon reaching a sufficient confidence level, without the need to wait several seconds and collect the full traffic representation. For assessing models’ confidence, we exploit the fact that many classifiers output the predicted results as a vector f′⁢(r)=usuperscript𝑓′𝑟𝑢f^{\\prime}(r)=uitalic_f start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( italic_r ) = italic_u, where u𝑢uitalic_u consists of the predicted probability that a sample is in each class. The predicted class for some sample is then f⁢(r)=arg⁡maxi=1C⁡f′⁢(r)𝑓𝑟superscriptsubscript𝑖1𝐶superscript𝑓′𝑟f(r)=\\arg\\max_{i=1}^{C}f^{\\prime}(r)italic_f ( italic_r ) = roman_arg roman_max start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_C end_POSTSUPERSCRIPT italic_f start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( italic_r ). For EC, we define the confidence of the classification, denoted by βqsubscript𝛽𝑞\\beta_{q}italic_β start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT, as the maximal argument in the resulting vector; namely,\n\nβq=maxi=1C⁡f′⁢(rq).subscript𝛽𝑞superscriptsubscript𝑖1𝐶superscript𝑓′subscript𝑟𝑞\\beta_{q}=\\max_{i=1}^{C}f^{\\prime}(r_{q}).italic_β start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT = roman_max start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_C end_POSTSUPERSCRIPT italic_f start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( italic_r start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ) .\n\nWe note that this approach can be replaced with several different methods for assessing model confidence, as discussed by (LEI, 2014).\n\nThe core idea behind EC is that classification occurs in multiple exit times τ1,…,τm⁢a⁢xsubscript𝜏1…subscript𝜏𝑚𝑎𝑥\\tau_{1},\\ldots,\\tau_{max}italic_τ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_τ start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT, and the final prediction is made when the confidence value exceeds a predefined threshold β𝛽\\betaitalic_β, or if the maximal time τm⁢a⁢xsubscript𝜏𝑚𝑎𝑥\\tau_{max}italic_τ start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT has elapsed.\n\nFor employing an efficient EC system, we first train (offline) different classifiers f1,…,fm⁢a⁢xsubscript𝑓1…subscript𝑓𝑚𝑎𝑥f_{1},\\ldots,f_{max}italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_f start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT for multiple durations in a logarithmic scale (namely, τi+1=2⋅τisubscript𝜏𝑖1⋅2subscript𝜏𝑖\\tau_{i+1}=2\\cdot\\tau_{i}italic_τ start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT = 2 ⋅ italic_τ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT) up to the maximal duration τm⁢a⁢xsubscript𝜏𝑚𝑎𝑥\\tau_{max}italic_τ start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT selected for the classification task.\n\nIn the online phase, we iteratively build the traffic representations. First, we build the distribution vectors with regard to the first exit time and feed it into the first classifier. If the sample is not classified with sufficient confidence, we keep the packet-sizes distribution vector unchanged and keep collecting packets to it. For the arrival-times distribution vector, in order to adjust the representation to the next classifier, we first merge each bin 2⁢i−12𝑖12i-12 italic_i - 1 with its adjacent bin 2⁢i2𝑖2i2 italic_i, leaving the second half of each vector empty. Then, we count arriving packets into the second halves of these vectors, where each bin corresponds to the time that passed since the beginning of the collection process. Formally, for d⁢(τi)𝑑subscript𝜏𝑖d({\\tau_{i}})italic_d ( italic_τ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) being a distribution vector of the arrival times for the exit time τisubscript𝜏𝑖\\tau_{i}italic_τ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT (for a single side of the traffic), the values in the updated vector d⁢(τi+1)𝑑subscript𝜏𝑖1d({\\tau_{i+1}})italic_d ( italic_τ start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT ) for the next exit time will be:\n\nd⁢(τi+1)j={d⁢(τi)2⁢j−1+d⁢(τi)2⁢j:j≤N2#⁢(packets in ⁢[j−1N⋅2⁢τi,jN⋅2⁢τi)):else.𝑑subscriptsubscript𝜏𝑖1𝑗cases𝑑subscriptsubscript𝜏𝑖2𝑗1𝑑subscriptsubscript𝜏𝑖2𝑗:absent𝑗𝑁2#packets in ⋅𝑗1𝑁2subscript𝜏𝑖⋅𝑗𝑁2subscript𝜏𝑖:absentelsed({\\tau_{i+1}})_{j}=\\begin{cases}d({\\tau_{i}})_{2j-1}+d({\\tau_{i}})_{2j}&:j% \\leq\\frac{N}{2}\\\\ \\#(\\text{packets in }[\\frac{j-1}{N}\\cdot 2\\tau_{i},\\frac{j}{N}\\cdot 2\\tau_{i})% )&:\\text{else}\\\\ \\end{cases}.italic_d ( italic_τ start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT = { start_ROW start_CELL italic_d ( italic_τ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT 2 italic_j - 1 end_POSTSUBSCRIPT + italic_d ( italic_τ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT 2 italic_j end_POSTSUBSCRIPT end_CELL start_CELL : italic_j ≤ divide start_ARG italic_N end_ARG start_ARG 2 end_ARG end_CELL end_ROW start_ROW start_CELL # ( packets in [ divide start_ARG italic_j - 1 end_ARG start_ARG italic_N end_ARG ⋅ 2 italic_τ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , divide start_ARG italic_j end_ARG start_ARG italic_N end_ARG ⋅ 2 italic_τ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) end_CELL start_CELL : else end_CELL end_ROW .\n\nUsing this method, no additional memory is required, and the representations are built and adjusted to the different classifiers on the fly. This process is illustrated in Figure 3.\n\nTo minimize the negative effect this process might have on classification, we select the confidence threshold β𝛽\\betaitalic_β as the accuracy of a baseline model that classifies all flow after τm⁢a⁢xsubscript𝜏𝑚𝑎𝑥\\tau_{max}italic_τ start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT.\n\nSurprisingly, this scheme not only reduces the average classification time but in some cases also improves the overall classification accuracy. This might occur due to the multiple exit options, allowing the models to identify timely behaviors that are more discernible in earlier classifiers.\n\nAdditionally, we explore the tradeoffs between the average collection time and the overall classification accuracy by altering the classification threshold β𝛽\\betaitalic_β with small α𝛼\\alphaitalic_α values, making early predictions if the confidence is higher than β−α𝛽𝛼\\beta-\\alphaitalic_β - italic_α.\n\nAs shown in Appendix B.2, the classification throughput highly depends on the selected classifier and the representation size. Therefore, if EC is deployed on edge devices, at high network rates, or if a strong classifier is selected, there will be a tradeoff between the ability to perform early classification and the rate of classification, as this method requires (possibly) employing multiple classifiers on each flow until it reaches the final prediction. Additionally, as the collection time is significantly higher than the inference time of our classifiers, we neglect the inference time in these comparisons. .\n\nThe results exploring EC setups are shown in Section 7.2.\n\n6. Early Classification with Hyperparameter Optimization (ECHO)\n\nAs we observed that HO improves the performance of models, we wish to create those representations for EC setups as well. For the packet-size distribution vector, we adopt the non-uniform bins of an optimized HO model (a single binning for all classifiers). However, as the arrival time distribution vector are updated in a particular way between EC classifiers (recall Figure 3), customized arrival-times bins cannot be used. The EC setup employing HO on the packet size dimension with uniform time bins is denoted by ECHO.\n\nYet, we do propose a non-uniform alternative for the arrival time bins, while still maintaining the ability to update the representations in real-time. In this alternative setup, denoted by EC Log, we will use logarithmic bins, that is, for every exit time τisubscript𝜏𝑖\\tau_{i}italic_τ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and N𝑁Nitalic_N bins with boundaries B=[b0,…,bN]𝐵subscript𝑏0…subscript𝑏𝑁B=[b_{0},\\ldots,b_{N}]italic_B = [ italic_b start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , … , italic_b start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ], where b0=0⁢s,bN=τiformulae-sequencesubscript𝑏00𝑠subscript𝑏𝑁subscript𝜏𝑖b_{0}=0s,\\>b_{N}=\\tau_{i}italic_b start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = 0 italic_s , italic_b start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT = italic_τ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, and the other boundaries (0<j<N0𝑗𝑁0<j<N0 < italic_j < italic_N):\n\nbj=τi⋅2−j.subscript𝑏𝑗⋅subscript𝜏𝑖superscript2𝑗b_{j}={\\tau_{i}}\\cdot{2^{-j}}.italic_b start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT = italic_τ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ⋅ 2 start_POSTSUPERSCRIPT - italic_j end_POSTSUPERSCRIPT .\n\nUsing this selection, if the different exit times are defined logarithmically (τi+1=2⋅τisubscript𝜏𝑖1⋅2subscript𝜏𝑖\\tau_{i+1}=2\\cdot\\tau_{i}italic_τ start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT = 2 ⋅ italic_τ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT), to update the arrival-times vector distribution we simply sum the two values with the smallest indexes, and shift all values one index below. Formally, for d⁢(τi)𝑑subscript𝜏𝑖d({\\tau_{i}})italic_d ( italic_τ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) being a single distribution vector of arrival times created at time τisubscript𝜏𝑖{\\tau_{i}}italic_τ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, d⁢(τi+1)𝑑subscript𝜏𝑖1d({\\tau_{i+1}})italic_d ( italic_τ start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT ) will be the updated vector for the next classifier:\n\nd⁢(τi+1)j={d⁢(τi)1+d⁢(τi)2:j=1#⁢(packets in ⁢[τi,τi+1)):j=Nd⁢(τi)j−1:else.𝑑subscriptsubscript𝜏𝑖1𝑗cases𝑑subscriptsubscript𝜏𝑖1𝑑subscriptsubscript𝜏𝑖2:absent𝑗1#packets in subscript𝜏𝑖subscript𝜏𝑖1:absent𝑗𝑁𝑑subscriptsubscript𝜏𝑖𝑗1:absentelsed({\\tau_{i+1}})_{j}=\\begin{cases}d({\\tau_{i}})_{1}+d({\\tau_{i}})_{2}&:j=1\\\\ \\#(\\text{packets in }[\\tau_{i},\\tau_{i+1}))&:j=N\\\\ d({\\tau_{i}})_{j-1}&:\\text{else}\\\\ \\end{cases}.italic_d ( italic_τ start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT = { start_ROW start_CELL italic_d ( italic_τ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_d ( italic_τ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_CELL start_CELL : italic_j = 1 end_CELL end_ROW start_ROW start_CELL # ( packets in [ italic_τ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_τ start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT ) ) end_CELL start_CELL : italic_j = italic_N end_CELL end_ROW start_ROW start_CELL italic_d ( italic_τ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_j - 1 end_POSTSUBSCRIPT end_CELL start_CELL : else end_CELL end_ROW .\n\nIn Section 7.3, we have evaluated different selections of packet-size bins for the ECHO setup, and explored the proposed method for the logarithmic arrival-times bins (EC Log).\n\n7. Experimental Results\n\nIn this section, we provide experimental results investigating the HO strategy for creating efficient traffic representations (Section 4), the EC method for timely traffic classification (Section 5) and the ECHO approach that combines the two strategies (Section 6). We also demonstrate their superiority over alternatives that were previously proposed in the literature.\n\nOur results are structured as follows: First, we examine the effectiveness of HO in traffic classification (Section 7.1). Here, we analyze how the representations created using HO impact classification accuracy and memory utilization. Our investigation highlights the advantages of HO compared to traditional uniform binning and alternative approaches, and provides in-depth insights into the reasons behind the success of the proposed method.\n\nSection 7.2 explores EC methods and how they affect the accuracy and reduce the average collection time of the system.\n\nFinally, we combine the HO method with EC to create efficient classifiers, with high accuracy and with minimal time required for classification (Section 7.3).\n\nIn this work, we use the Logistic Regression (L⁢R𝐿𝑅LRitalic_L italic_R) (Pearl and Reed, 1920) classifier for building the models. L⁢R𝐿𝑅LRitalic_L italic_R classifiers offer the highest classification throughput compared to other explored classifiers (a detailed comparison in Appendix B), therefore enabling real-time classification for large volumes of traffic. However, we note that in our experiments, similar trends were observed for different ML and DL classifiers.\n\nWe use three publicly-available datasets (Rezaei and Liu, 2020; Draper-Gil et al., 2016; Lashkari et al., 2017; Naas and Fesl, 2023), where classification tasks include application identification (e.g., distinguishing between YouTube and Google Music), encryption identification (e.g., distinguishing between different types of VPN), and traffic categorization (e.g. distinguishing between video and VoIP). The different classification tasks we tackle are presented in Table 2, and a thorough description of the datasets, including our preprocessing stage, is in Appendix A.\n\nIt is important to note that our preprocessing ensures that the datasets are balanced across all classification tasks, and the difference between accuracy (percentage of correctly classified samples over the test set) and alternative utility metrics (e.g. f1 score, precision, recall) is negligible. Consequently, we evaluate our models and representations based solely on the achieved accuracy using k𝑘kitalic_k-fold Cross Validation (KFCV) (Kohavi, 1995) with k=5𝑘5k=5italic_k = 5.\n\n7.1. Hyperparameter Optimization (HO) of Binnings\n\nAs observed in our experiments, larger representations improve classification accuracy, but come at the cost of a higher memory footprint and usually more computational requirements.\n\nWe compare several methods for selecting binnings over the packet size dimension, including our suggested novel strategy:\n\nUniform.:\n\nAll bins are of equal size; namely, bi=i⋅xSsubscript𝑏𝑖⋅𝑖𝑥𝑆b_{i}=\\frac{i\\cdot x}{S}italic_b start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = divide start_ARG italic_i ⋅ italic_x end_ARG start_ARG italic_S end_ARG, where S𝑆Sitalic_S is the number of bins.\n\nFeature Selection.:\n\nInspired by (Barradas et al., 2021), we employ feature-selection methods on larger uniform representations, to create a smaller pruned representation. Given the required number of bins N𝑁Nitalic_N, we apply feature selection on multiple uniform representations with different original sizes: N′∈{10,20,50,100,200,500,1500},superscript𝑁′1020501002005001500N^{\\prime}\\in\\{10,20,50,100,200,500,1500\\},italic_N start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ∈ { 10 , 20 , 50 , 100 , 200 , 500 , 1500 } , and select the model with the highest achieved accuracy. This representation still uses fixed-size bins, however, it is data-aware and selects features with respect to their statistical significance.\n\nStatistical.:\n\nAs explained in Section 4, and inspired by (Garcia and Korhonen, 2018), we use the TPE method for optimization of binning, where the objective function is to maximize a multiclass version of the Jensen-Shanon distance metric (Lin, 1991). This method maximizes the statistical separation between the different classes independently of the selected classifier.\n\nHO Sizes.:\n\nOur novel strategy employed on the packet size distribution, using the TPE method with the objective being the accuracy of a trained model, as explained in detail in Section 4.\n\nThroughout all our experiments, we set the number of iterations of the TPE method to 200, as we observed convergence in this iterative process. Note that we have also explored greedy sampling methods for hyperparameter selection, where we progressively built the hyperparameter configuration by selecting the option in each subsequent bin that maximized the objective function given the previously chosen options. However, as greedy selection methods do not have as many degrees of freedom, and showed no noticeable advantage over the TPE method, we focus our discussion here on the TPE.\n\nThe results are shown in Figure 4. Evidently, the HO Sizes approach is able to capture the nuances and patterns in smaller representations, leading to improved classification accuracy throughout all classification tasks. Additionally, statistical optimization (Statistical) shows an advantage over uniform representations; however, this trend is not consistent for all tasks and sizes. Finally, the Feature Selection method introduces a degradation in accuracy compared to uniform representations. This might be caused by the loss of information in the feature selection process, in the inherent limitation of selecting only sampled features for a single uniform representation (of larger size).\n\nWe note that introducing non-uniformity in the packet sizes dimension leads to a significant enhancement in the model’s accuracy, particularly evident when employing the HO Sizes approach. We further explore non-uniformity by introducing non-uniformity in the arrival-times dimension. We compare the following representations: Uniform representations, HO Sizes - that use non-uniform bins for the sizes dimension and uniform bins for the arrival-times dimension (as in Figure 4), HO Times - that use non-uniform bins for the arrival time dimension and uniform bins for the sizes dimension, and HO Both that include optimized binning for both dimensions (with different parameters for each dimension). Results are shown in Figure 5. We see that by itself, non-uniformity in the arrival-times dimension shows some advantage over uniform representations. However, this improvement is limited, and when combined with non-uniformity in the packet-sizes dimension, there is little to no advantage in model accuracy compared to HO Sizes.\n\n7.1.1. Explainability\n\nAn important property of our binning strategy is its explainability. This is done by looking at the distribution of packet sizes (or arrival times) for each class, and observing what information is given by the new bin boundaries. For example, for the VPN Services task over 5 bins, Figure 6 depicts the selected bin boundaries of one of the runs of HO Sizes, along with the packet sizes distribution of each class. It is easy to see that bin boundaries are correlated to the data distribution. Specifically, notice that one of the boundaries is at 168 bytes. In retrospect, this appears to distinguish between typical SSTP packets and typical OpenVPN packets. Similarly, the bin boundary of 76 bytes appears to distinguish between L2TP IPsec traffic (which has many smaller packets) and WireGuard traffic (with bigger packets).\n\nThese boundaries demonstrate the strength of the HO approach, as coarse-grained uniform representations commonly group these values together resulting in misclassifications; for example, Figure 7 shows a confusion matrix of a classifier using coarse a flow representation with 5 uniform bins.\n\nWe can see that the classes that are misclassified are the classes for which the boundaries selected by HO Sizes provide the necessary separation. An optimized representation employing an L⁢R𝐿𝑅LRitalic_L italic_R classifier with the same number of bins achieved an accuracy of 98%percent9898\\%98 %, in contrast to all other binning methods achieving accuracy lower than 90%percent9090\\%90 %, as the Uniform and Feature Selection approaches are limited to bins of equal size, and the bins selected by the Statistical approach were correlated to the overall data distribution but were unable to capture small nuances that affected the final classification performance.\n\n7.2. Early Classification (EC) of Traffic\n\nThis section explores our EC setup, which is based on training multiple classifiers f1,…,fm⁢a⁢xsubscript𝑓1…subscript𝑓𝑚𝑎𝑥f_{1},\\ldots,f_{max}italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_f start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT for multiple exit times τ1,…,τm⁢a⁢xsubscript𝜏1…subscript𝜏𝑚𝑎𝑥\\tau_{1},\\ldots,\\tau_{max}italic_τ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_τ start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT. When a classifier fisubscript𝑓𝑖f_{i}italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT (1≤i<m⁢a⁢x1𝑖𝑚𝑎𝑥1\\leq i<max1 ≤ italic_i < italic_m italic_a italic_x) achieves a confidence value larger than a threshold β𝛽\\betaitalic_β, the classification stops and the label predicted by fisubscript𝑓𝑖f_{i}italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is reported. Otherwise, after the maximal exit time τm⁢a⁢xsubscript𝜏𝑚𝑎𝑥\\tau_{max}italic_τ start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT, the label predicted by fm⁢a⁢xsubscript𝑓𝑚𝑎𝑥f_{max}italic_f start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT is reported (regardless of its confidence). A detailed explanation can be found in Section 5. In this section, for ease of presentation, we used a pseudo-logarithmic scale (1⁢m⁢s,2⁢m⁢s,5⁢m⁢s,10⁢m⁢s⁢…)1𝑚𝑠2𝑚𝑠5𝑚𝑠10𝑚𝑠…(1ms,2ms,5ms,10ms\\ldots)( 1 italic_m italic_s , 2 italic_m italic_s , 5 italic_m italic_s , 10 italic_m italic_s … ).\n\nFirst, to gain a sense of the behavior of the confidence for the different classifiers, we train the classifiers f1,…,fm⁢a⁢xsubscript𝑓1…subscript𝑓𝑚𝑎𝑥f_{1},\\ldots,f_{max}italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_f start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT and checked their classification accuracy and coverage (the proportion of instances for which the classifier provides a prediction) for possible confidence thresholds.\n\nThe results for a single classification task (namely, VPN Services with 5 bins) are shown in Figure 8.\n\nSome significant insights emerge from these results. First, even with short exit times, many samples have been classified with high confidence, thus validating the underlying principle of our method: many flows can be classified at an early stage, thereby reducing the average classification time and saving memory resources. Second, we observe an improvement in model coverage and accuracy over time, indicating that subsequent classifiers generally classify with higher accuracy and cover a broader range of flows. Yet, some flows are correctly classified in earlier classifiers but are misclassified in subsequent ones. This indicates that there are timely features that become hard to recognize after a longer period.\n\nFollowing this experiment, we proceeded to evaluate the EC setup, where we determined the initial classification threshold β𝛽\\betaitalic_β based on the accuracy of a baseline model that classifies all flows after the maximal time τm⁢a⁢xsubscript𝜏𝑚𝑎𝑥\\tau_{max}italic_τ start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT.\n\nFor the same classification task as shown in Figure 8 (VPN Services with 5 uniform bins), the EC model results are shown in Figure 9.\n\nWe can see two phenomena. First, the average classification time for flows stands at 1.011.011.011.01 seconds, in contrast to the baseline model’s classification time of 5555 seconds, representing a notable improvement of approximately 80%percent8080\\%80 % in classification speed.\n\nMoreover, the final accuracy of the EC model surpasses that of the baseline model. This means that not only we can save classification time, but EC can potentially improve the model’s accuracy. As previously discussed, this improvement could stem from timely features identified by earlier models, which may go unnoticed by subsequent ones.\n\nFigure 10 compares models utilizing EC for different tasks, where each value is normalized against a uniform binning model with the appropriate number of bins. These figures highlight the inherent trade-offs in employing E⁢C𝐸𝐶ECitalic_E italic_C: While larger representations facilitate faster classification, there is a slight compromise in accuracy compared to baseline models. On the other hand, smaller representations have the potential to surpass the performance of baseline models, as explained above.\n\nThe rationale behind these results lies in the fact that larger representations encapsulate finer details about the flows, thereby boosting the confidence of earlier models. Nevertheless, the notable enhancement in reducing classification time comes with the potential of misclassification at these early stages.\n\nWe also compare the results of the EC setup with a lower classification threshold (β−α𝛽𝛼\\beta-\\alphaitalic_β - italic_α). The results of this comparison for one classification task are shown in Figure 11.\n\nBy opting for a lower threshold, a greater number of flows undergo early-stage classification, thereby reducing the average classification time. However, while the classification time is reduced, there is a notable decrease in accuracy. This tradeoff is controlled by selecting the threshold parameter, giving the flexibility to fine-tune this threshold according to specific requirements and constraints. In this experiment, it is evident that up to an α𝛼\\alphaitalic_α value of 5%percent55\\%5 %, the decline in accuracy is negligible (lower than 1%percent11\\%1 %), while there is a reduction of 25%percent2525\\%25 % in collection time.\n\n7.3. Early Classification with Hyperparameter Optimization (ECHO)\n\nHaving noted the effectiveness of HO in enhancing model accuracy (Section 7.1), we aim to extend these representations to EC setups as well.\n\nFirst, we will compare two methods for the bins of the arrival times distribution, the simple EC with uniform time and size bins, and the EC Log method with logarithmic time bins. The full description of those methods and how they are updated to allow real-time classification without memory overhead is in Section 6.\n\nThe comparison results for different numbers of bins and different α𝛼\\alphaitalic_α values are shown in Figure 12. The results for the two methods are similar. Yet, a noteworthy trend is that with smaller representations, the performance of EC Log is inferior. Conversely, for larger representations, EC Log tends to exhibit higher accuracy, surpassing the accuracy of the uniform EC. We observe that while introducing non-uniformity in the time dimension enhances accuracy, its impact is relatively minor compared to the non-uniformity seen in the packet-sizes dimension, as depicted earlier in Figure 5.\n\nIn the following experiment, we evaluate the creation of EC models with non-uniform bins in the packet size dimension, namely ECHO. As explained in Section 6, this process does not require any additional memory or computational power. For the ECHO models in this comparison, β𝛽\\betaitalic_β is selected as the accuracy of a baseline uniform model, and α𝛼\\alphaitalic_α is 5%percent55\\%5 % based on previous observations (Figures 11 and 12). The selected α𝛼\\alphaitalic_α value strikes a balance between average exit time and accuracy, with similar trends noted for other values of α𝛼\\alphaitalic_α.\n\nFigure 13 compares different selections of bins for the EC model, where each selection is based on an HO optimization over a classifier with the corresponding time scope f1,…,fm⁢a⁢xsubscript𝑓1…subscript𝑓𝑚𝑎𝑥f_{1},\\ldots,f_{max}italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_f start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT. Figure 13a compares the accuracy of the ECHO setups, it is noticable that ECHO significantly improves the model’s accuracy compared to the uniform EC model. However, the accuracy results are still lower than an HO setup (with non-uniform packet size bins) that classifies all flows after τm⁢a⁢xsubscript𝜏𝑚𝑎𝑥\\tau_{max}italic_τ start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT.\n\nMoreover, even though no specific classifier has a clear advantage in average accuracy, there is a smaller standard deviation for subsequent classifiers, implying that the binnings of these classifiers are more reliable and consistent, and therefore, are preferable.\n\nIn Figure 13b, we compare the same classifiers in terms of average exit time. Here, for all binnings, the ECHO models show significantly faster average classification times (with no advantage to any specific selection). Recall that the threshold value for the EC models is based on a baseline uniform model, and as shown earlier, HO significantly improves accuracy, which is then reflected as well in higher confidence in classification. Thus enabling faster classification but with a lower accuracy than a full HO model.\n\n8. Conclusions\n\nThis paper highlights the fact that many traffic classification techniques have considerable room for improvement. The techniques applied in the ECHO approach can seamlessly be combined with many existing traffic classification methods, and reduce both their memory footprint and their collection time without compromising any aspect of the classification process.\n\nWe show that by employing hyperparameter optimization methods to create non-uniform flow representations, we can significantly reduce the memory overhead of representations without compromising model accuracy. Furthermore, it boosts the model’s throughput as smaller and more efficient classifiers are used.\n\nWe also demonstrate that building a setup that includes multiple exit times with multiple classifiers, can significantly reduce the average required time for classification with no additional memory requirements.\n\nIn this paper, we utilize the TPE algorithm to optimize the variable-sized bins. Although this process is conducted offline, in future work, we aim to investigate other methods with improved time efficiency. We also wish to explore whether other flow representations could be optimized in terms of memory usage.\n\nIn addition, we aim to delve into different methods to create the setup for early traffic classification, and different methods for assessing the models’ confidence as part of the early classification process.\n\n8.1. Reproducability\n\nOur code, as well as the processed datasets, will be publicly available to the community upon completion of the double-blind review process of our paper.\n\n8.2. Acknowledgments\n\nPart of this work has been supported by Israel Innovation Authority within the ENTM consortium.\n\nReferences\n\n(1)\n\nAceto et al. (2019) Giuseppe Aceto, Domenico Ciuonzo, Antonio Montieri, and Antonio Pescapè. 2019. MIMETIC: Mobile encrypted traffic classification using multimodal deep learning. Computer Networks 165 (2019), 106944.\n\nAceto et al. (2010) Giuseppe Aceto, Alberto Dainotti, Walter de Donato, and Antonio Pescapé. 2010. PortLoad: Taking the Best of Two Worlds in Traffic Classification. 2010 INFOCOM IEEE Conference on Computer Communications Workshops (2010), 1–5.\n\nAkbari et al. (2021) Iman Akbari, Mohammad A. Salahuddin, Leni Ven, Noura Limam, Raouf Boutaba, Bertrand Mathieu, Stephanie Moteau, and Stephane Tuffin. 2021. A Look Behind the Curtain: Traffic Classification in an Increasingly Encrypted Web. Proc. ACM Meas. Anal. Comput. Syst. 5, 1, Article 04 (feb 2021), 26 pages.\n\nAnderson and McGrew (2017) Blake Anderson and David McGrew. 2017. Machine Learning for Encrypted Malware Traffic Classification: Accounting for Noisy Labels and Non-Stationarity. In KDD’17. 1723–1732. https://doi.org/10.1145/3097983.3098163\n\nAnonymous (2023) Anonymous. 2023. Private Communincation..\n\nBarradas et al. (2021) Diogo Barradas, Nuno Santos, Luís Rodrigues, Salvatore Signorello, Fernando M. V. Ramos, and André Madeira. 2021. FlowLens: Enabling Efficient Flow Classification for ML-based Network Security Applications. In NDSS’21.\n\nBatista et al. (2004) Gustavo E. A. P. A. Batista, Ronaldo C. Prati, and Maria Carolina Monard. 2004. A Study of the Behavior of Several Methods for Balancing Machine Learning Training Data. SIGKDD Explor. Newsl. 6, 1 (jun 2004), 20–29. https://doi.org/10.1145/1007730.1007735\n\nBergstra et al. (2011) James Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. 2011. Algorithms for Hyper-Parameter Optimization. In Advances in Neural Information Processing Systems, Vol. 24.\n\nBergstra et al. (2013) James Bergstra, Daniel Yamins, and David Cox. 2013. Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures. In ICML’13 (Proceedings of Machine Learning Research, Vol. 28). PMLR, 115–123.\n\nBernaille et al. (2006a) Laurent Bernaille, Renata Teixeira, Ismael Akodkenou, Augustin Soule, and Kave Salamatian. 2006a. Traffic Classification on the Fly. SIGCOMM Comput. Commun. Rev. 36, 2 (apr 2006), 23–26.\n\nBernaille et al. (2006b) Laurent Bernaille, Renata Teixeira, Ismael Akodkenou, Augustin Soule, and Kave Salamatian. 2006b. Traffic classification on the fly. SIGCOMM Comput. Commun. Rev. 36, 2 (apr 2006), 23–26. https://doi.org/10.1145/1129582.1129589\n\nBernaille et al. (2006c) Laurent Bernaille, Renata Cruz Teixeira, and Kave Salamatian. 2006c. Early application identification. In Conference on Emerging Network Experiment and Technology. https://api.semanticscholar.org/CorpusID:496969\n\nBronzino et al. (2021) Francesco Bronzino, Paul Schmitt, Sara Ayoubi, Hyojoon Kim, Renata Teixeira, and Nick Feamster. 2021. Traffic Refinery: Cost-Aware Data Representation for Machine Learning on Network Traffic. Proc. ACM Meas. Anal. Comput. Syst. 5, 3, Article 40 (dec 2021), 24 pages.\n\nCawley and Talbot (2010) Gavin C. Cawley and Nicola L.C. Talbot. 2010. On Over-Fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation. J. Mach. Learn. Res. 11 (aug 2010), 2079–2107.\n\nCortes and Vapnik (1995) Corinna Cortes and Vladimir Naumovich Vapnik. 1995. Support-Vector Networks. Machine Learning 20 (1995), 273–297.\n\nDraper-Gil et al. (2016) Gerard Draper-Gil, Arash Habibi Lashkari, Mohammad Saiful Islam Mamun, and Ali A Ghorbani. 2016. Characterization of encrypted and VPN traffic using time-related features. In ICISSP. 407–414.\n\nEngelberg and Wool (2021) Aviv Engelberg and Avishai Wool. 2021. Classification of Encrypted IoT Traffic Despite Padding and Shaping. arXiv:2110.11188 [cs.CR] https://arxiv.org/abs/2110.11188\n\nEnghardt et al. (2020) Reese Enghardt, Tommy Pauly, Colin Perkins, Kyle Rose, and Christopher A. Wood. 2020. A Survey of the Interaction between Security Protocols and Transport Services. RFC 8922.\n\nErtam and Avcı (2017) Fatih Ertam and Engin Avcı. 2017. A new approach for internet traffic classification: GA-WK-ELM. Measurement 95 (2017), 135–142. https://doi.org/10.1016/j.measurement.2016.10.001\n\nFahad et al. (2013) Adil Fahad, Zahir Tari, Ibrahim Khalil, Ibrahim Habib, and Hussein Alnuweiri. 2013. Toward an efficient and scalable feature selection approach for internet traffic classification. Computer Networks 57, 9 (2013), 2040–2057. https://doi.org/10.1016/j.comnet.2013.04.005\n\nFinamore et al. (2023) Alessandro Finamore, Chao Wang, Jonatan Krolikowski, José Manuel Navarro, Fuxing Chen, and Dario Rossi. 2023. Replication: Contrastive Learning and Data Augmentation in Traffic Classification Using a Flowpic Input Representation. Proceedings of the 2023 ACM on Internet Measurement Conference (2023). https://api.semanticscholar.org/CorpusID:262046478\n\nFinsterbusch et al. (2014) Michael Finsterbusch, Chris Richter, Eduardo Rocha, Jean-Alexander Muller, and Klaus Hanssgen. 2014. A Survey of Payload-Based Traffic Classification Approaches. IEEE Communications Surveys & Tutorials 16, 2 (2014), 1135–1156. https://doi.org/10.1109/SURV.2013.100613.00161\n\nFukushima (1980) Kunihiko Fukushima. 1980. Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. Biological Cybernetics 36 (1980), 193–202.\n\nGarcia and Korhonen (2018) Johan Garcia and Topi Korhonen. 2018. Efficient Distribution-Derived Features for High-Speed Encrypted Flow Classification. Proceedings of the 2018 Workshop on Network Meets AI & ML (2018). https://api.semanticscholar.org/CorpusID:51929029\n\nHaffner et al. (2005) Patrick Haffner, Subhabrata Sen, Oliver Spatscheck, and Dongmei Wang. 2005. ACAS: Automated Construction of Application Signatures. In ACM SIGCOMM MineNet’05. 197–202.\n\nHo (1995) Tin Kam Ho. 1995. Random decision forests. In Proceedings of 3rd International Conference on Document Analysis and Recognition, Vol. 1. 278–282 vol.1. https://doi.org/10.1109/ICDAR.1995.598994\n\nHolland et al. (2021) Jordan Holland, Paul Schmitt, Nick Feamster, and Prateek Mittal. 2021. New Directions in Automated Traffic Analysis. In ACM CCS’21. 3366–3383.\n\nHorowicz et al. (2022) Eyal Horowicz, Tal Shapira, and Yuval Shavitt. 2022. A Few Shots Traffic Classification with Mini-FlowPic Augmentations. In ACM IMC’22 (Nice, France). 647–654. https://doi.org/10.1145/3517745.3561436\n\nHughes (1968) Gordon P. Hughes. 1968. On the mean accuracy of statistical pattern recognizers. IEEE Trans. Inf. Theory 14 (1968), 55–63. https://api.semanticscholar.org/CorpusID:206729491\n\nJacobs et al. (2022) Arthur S. Jacobs, Roman Beltiukov, Walter Willinger, Ronaldo A. Ferreira, Arpit Gupta, and Lisandro Z. Granville. 2022. AI/ML for Network Security: The Emperor Has No Clothes. In Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security (Los Angeles, CA, USA) (CCS ’22). Association for Computing Machinery, New York, NY, USA, 1537–1551. https://doi.org/10.1145/3548606.3560609\n\nKohavi (1995) Ron Kohavi. 1995. A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection. In International Joint Conference on Artificial Intelligence.\n\nLashkari et al. (2017) Arash Habibi Lashkari, Gerard Draper-Gil, Mohammad Saiful Islam Mamun, and Ali A. Ghorbani. 2017. Characterization of Tor Traffic using Time based Features. In ICISSP.\n\nLEI (2014) JING LEI. 2014. Classification with confidence. Biometrika 101, 4 (2014), 755–769. http://www.jstor.org/stable/43304686\n\nLi et al. (2018) Rui Li, Xi Xiao, Shiguang Ni, Haitao Zheng, and Shutao Xia. 2018. Byte Segment Neural Network for Network Traffic Classification. 2018 IEEE/ACM 26th International Symposium on Quality of Service (IWQoS) (2018), 1–10. https://api.semanticscholar.org/CorpusID:59232416\n\nLin (1991) Jianhua Lin. 1991. Divergence measures based on the Shannon entropy. IEEE Trans. Inf. Theory 37 (1991), 145–151. https://api.semanticscholar.org/CorpusID:12121632\n\nLiu et al. (2019) Chang Liu, Longtao He, Gang Xiong, Zigang Cao, and Zhen Li. 2019. FS-Net: A Flow Sequence Network For Encrypted Traffic Classification. IEEE INFOCOM 2019 - IEEE Conference on Computer Communications (2019), 1171–1179. https://api.semanticscholar.org/CorpusID:86515190\n\nLopez-Martin et al. (2017) Manuel Lopez-Martin, Belen Carro, Antonio Sanchez-Esguevillas, and Jaime Lloret. 2017. Network Traffic Classifier With Convolutional and Recurrent Neural Networks for Internet of Things. IEEE Access 5 (2017), 18042–18050. https://doi.org/10.1109/ACCESS.2017.2747560\n\nLotfollahi et al. (2017) Mohammad Lotfollahi, Mahdi Jafari Siavoshani, Ramin Shirali Hossein Zade, and Mohammdsadegh Saberian. 2017. Deep packet: a novel approach for encrypted traffic classification using deep learning. Soft Computing 24 (2017), 1999 – 2012. https://api.semanticscholar.org/CorpusID:35187639\n\nMoore and Papagiannaki (2005) Andrew W. Moore and Konstantina Papagiannaki. 2005. Toward the Accurate Identification of Network Applications. In PAM’05. 41–54.\n\nMoore and Zuev (2005) Andrew W. Moore and Denis Zuev. 2005. Internet Traffic Classification Using Bayesian Analysis Techniques. SIGMETRICS Perform. Eval. Rev. 33, 1 (jun 2005), 50–60.\n\nNaas and Fesl (2023) Mohamed Naas and Jan Fesl. 2023. A novel dataset for encrypted virtual private network traffic analysis. Data in Brief 47 (2023), 108945.\n\nNguyen and Armitage (2008) Thuy T.T. Nguyen and Grenville Armitage. 2008. A survey of techniques for internet traffic classification using machine learning. IEEE Communications Surveys & Tutorials 10, 4 (2008), 56–76. https://doi.org/10.1109/SURV.2008.080406\n\nPearl and Reed (1920) Raymond Pearl and Lowell J. Reed. 1920. On the Rate of Growth of the Population of the United States since 1790 and Its Mathematical Representation1. Proceedings of the National Academy of Sciences 6, 6 (1920), 275–288. https://doi.org/10.1073/pnas.6.6.275\n\nRezaei and Liu (2020) Shahbaz Rezaei and Xin Liu. 2020. How to Achieve High Classification Accuracy with Just a Few Labels: A Semi-supervised Approach Using Sampled Packets. arXiv:1812.09761 [cs.NI] https://arxiv.org/abs/1812.09761\n\nRoughan et al. (2004) Matthew Roughan, Subhabrata Sen, Oliver Spatscheck, and Nick G. Duffield. 2004. Class-of-service mapping for QoS: a statistical signature-based approach to IP traffic classification. In ACM/SIGCOMM Internet Measurement Conference.\n\nSalman et al. (2020) Ola Salman, Imad H Elhajj, Ayman Kayssi, and Ali Chehab. 2020. A review on machine learning–based approaches for Internet traffic classification. Annals of Telecommunications 75, 11 (2020), 673–710.\n\nShafiq et al. (2018) Muhammad Shafiq, Xiangzhan Yu, Ali Kashif Bashir, Hassan Nazeer Chaudhry, and Dawei Wang. 2018. A machine learning approach for feature selection traffic classification using security analysis. The Journal of Supercomputing 74 (2018), 4867–4892.\n\nShapira and Shavitt (2019) Tal Shapira and Yuval Shavitt. 2019. Flowpic: Encrypted internet traffic classification is as easy as image recognition. In IEEE Infocom Workshops. 680–687.\n\nSharafaldin et al. (2018) Iman Sharafaldin, Arash Habibi Lashkari, and Ali A. Ghorbani. 2018. Toward Generating a New Intrusion Detection Dataset and Intrusion Traffic Characterization. In ICISSP.\n\nSivanathan et al. (2019) Arunan Sivanathan, Hassan Habibi Gharakheili, Franco Loi, Adam Radford, Chamith Wijenayake, Arun Vishwanath, and Vijay Sivaraman. 2019. Classifying IoT Devices in Smart Environments Using Network Traffic Characteristics. IEEE Transactions on Mobile Computing 18 (2019), 1745–1759. https://api.semanticscholar.org/CorpusID:70082542\n\nvan Ede et al. (2020) Thijs van Ede, Riccardo Bortolameotti, Andrea Continella, Jingjing Ren, Daniel J. Dubois, Martina Lindorfer, David R. Choffnes, Maarten van Steen, and Andreas Peter. 2020. FlowPrint: Semi-Supervised Mobile-App Fingerprinting on Encrypted Network Traffic. Proceedings 2020 Network and Distributed System Security Symposium (2020). https://api.semanticscholar.org/CorpusID:211265114\n\nWang et al. (2018) Wei Wang, Yiqiang Sheng, Jinlin Wang, Xuewen Zeng, Xiaozhou Ye, Yongzhong Huang, and Ming Zhu. 2018. HAST-IDS: Learning Hierarchical Spatial-Temporal Features Using Deep Neural Networks to Improve Intrusion Detection. IEEE Access 6 (2018), 1792–1806. https://doi.org/10.1109/ACCESS.2017.2780250\n\nWang et al. (2017a) Wei Wang, Ming Zhu, Jinlin Wang, Xuewen Zeng, and Zhongzhen Yang. 2017a. End-to-end encrypted traffic classification with one-dimensional convolution neural networks. In 2017 IEEE International Conference on Intelligence and Security Informatics (ISI). 43–48. https://doi.org/10.1109/ISI.2017.8004872\n\nWang et al. (2017b) Wei Wang, Ming Zhu, Jinlin Wang, Xuewen Zeng, and Zhongzhen Yang. 2017b. End-to-end encrypted traffic classification with one-dimensional convolution neural networks. 2017 IEEE International Conference on Intelligence and Security Informatics (ISI) (2017), 43–48. https://api.semanticscholar.org/CorpusID:3720713\n\nWang et al. (2023) Yipeng Wang, Huijie He, Yingxu Lai, and Alex X. Liu. 2023. A Two-Phase Approach to Fast and Accurate Classification of Encrypted Traffic. IEEE/ACM Transactions on Networking 31, 3 (2023), 1071–1086. https://doi.org/10.1109/TNET.2022.3209979\n\nWilliams et al. (2006) Nigel Williams, Sebastian Zander, and Grenville Armitage. 2006. A preliminary performance comparison of five machine learning algorithms for practical IP traffic flow classification. SIGCOMM Comput. Commun. Rev. 36 (10 2006), 5–16. https://doi.org/10.1145/1163593.1163596\n\nXu et al. (2023) Yuwei Xu, Jie Cao, Kehui Song, Qiao Xiang, and Guang Cheng. 2023. FastTraffic: A lightweight method for encrypted traffic fast classification. Computer Networks 235 (2023), 109965. https://doi.org/10.1016/j.comnet.2023.109965\n\nYang et al. (2021) Lixuan Yang, Alessandro Finamore, Feng Jun, and Dario Rossi. 2021. Deep Learning and Zero-Day Traffic Classification: Lessons Learned From a Commercial-Grade Dataset. IEEE TNSM 18, 4 (2021), 4103–4118. https://doi.org/10.1109/TNSM.2021.3122940\n\nZhao et al. (2021) Jingjing Zhao, Xuyang Jing, and Witold Pedrycz. 2021. Network traffic classification for data fusion: A survey. Information Fusion 72 (02 2021). https://doi.org/10.1016/j.inffus.2021.02.009\n\nZou et al. (2018) Zhuang Zou, Jingguo Ge, Hongbo Zheng, Yulei Wu, Chunjing Han, and Zhongjiang Yao. 2018. Encrypted Traffic Classification with a Convolutional Long Short-Term Memory Neural Network. 2018 IEEE 20th International Conference on High Performance Computing and Communications; IEEE 16th International Conference on Smart City; IEEE 4th International Conference on Data Science and Systems (HPCC/SmartCity/DSS) (2018), 329–334. https://api.semanticscholar.org/CorpusID:59233070\n\nAppendix A Datasets\n\nThis section provides an overview of the three publicly available datasets used for training and testing our models, and the different classification tasks for each dataset.\n\nQuic traffic dataset (Rezaei and Liu, 2020).\n\nThe Quic traffic dataset was recorded at UC Davis and consists of the traffic of 5 Google services (Google Drive, YouTube, Google Docs, Google Search, and Google Music) under several operating systems (including different Windows and Ubuntu distributions). The flow counts are shown in Table 3.\n\nFor this dataset, we consider one classification task, which we call Quic Applications, of classifying flows to one of the different applications.\n\nISCX combined traffic dataset.\n\nThis dataset consists of 2 network captures produced by the University of New Brunswick (Draper-Gil et al., 2016; Lashkari et al., 2017). The captures were filtered to include only the traffic related to the actual label of the flow. Classes with an insufficient number of samples (e.g., chat) were discarded. The processed dataset consists of 4 traffic categories (VoIP, video, file transfer, and browsing) with different applications for each category.\n\nFor these categories, we have 3 encryption techniques: non-VPN, VPN (for all classes except browsing), and TOR. As proposed in the original papers, the dataset is sampled using a 15-second non-overlapping time window and every sample is issued as a different flow. Table 4 shows the number of flows for each category and each type of encryption, while Table 5 delves into specific applications within the video and VoIP categories.\n\nFor this dataset, we consider three classification tasks: (i) ISCX Applications in which we classify the different applications in Table 5; (ii) ISCX Encryption, in which we classify the different encryption protocols (namely, VPN, Non-VPN, or TOR); and (iii) ISCX Categories, in which we obtain the category (namely, video, VoIP, file transfer, or browsing) for the flow.\n\nVPN services dataset (Naas and Fesl, 2023).\n\nThis dataset consists of packet captures produced and processed by the University of South Bohemia. The processed dataset contains 5 traffic categories (email, video conferencing, SSH, streaming, and non-streaming) using different applications for the streaming and non-streaming categories (the specific applications are not available in the dataset). Traffic from all categories is recorded over VPN and non-VPN connections, and using 7 different VPN settings (L2TP, L2TP IPsec, OpenVPN, PPTP, SSTP, WireGuard, and non-VPN).\n\nSince the dataset contains a low volume of email, video conferencing, and SSH traffic, we only use the streaming and non-streaming categories. We also exclude the traffic recorded over PPTP VPN as it contains significantly fewer samples than the other VPN services. The number of flows in each category for the filtered dataset is shown in Table 6.\n\nFor this dataset, we consider two classification tasks. First, VPN Services, is the task of classifying the specific VPN in use. Secondly, Streaming or Not is the binary task of categorizing the flows of the dataset into streaming/non-streaming categories.\n\nA.1. Preprocessing\n\nWe filter the samples using three parameters: minimal number of packets, minimal volume (in bytes), and minimal duration. For every dataset, we have selected these parameters with regard to similar experiments done in the literature and to our observations of the traffic patterns. The selected parameters for each dataset are shown in Table 7.\n\nAdditionally, as some of our datasets are imbalanced, we have employed random undersampling (Batista et al., 2004) to balance the data. This balancing procedure was applied to both the training and test sets to ensure equal representation of each class.\n\nAppendix B Representations and Models Comparison\n\nIn this section, we thoroughly compare different representations with various sizes (detailed below), employing 4 different classifiers, Random Forest (RF) (Ho, 1995), Logistic Regression (LR) (Pearl and Reed, 1920), Support Vector Machine (SVM) (Cortes and Vapnik, 1995), and Convolutional Neural Networks (CNNs, denoted here by DEEP) (Fukushima, 1980). Although ECHO can work with any distributional representation and classifier, this comparison justify our selection of representation and model in Section 7.\n\nRecall that we denote the selected representation type as r𝑟ritalic_r, where rqsubscript𝑟𝑞r_{q}italic_r start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT is the calculated representation for the flow with identifier q𝑞qitalic_q. We compare the following four common types of flow representations.\n\nDistribution vectors,:\n\ndenoted by d⁢i⁢s⁢t⁢(N)𝑑𝑖𝑠𝑡𝑁dist(N)italic_d italic_i italic_s italic_t ( italic_N ). This representation consists of 4 distribution vectors of size N𝑁Nitalic_N, containing, for each flow direction, counters of the arrival times and packet sizes, where each value is mapped to one of the N𝑁Nitalic_N bins, as suggested by (Engelberg and Wool, 2021). d⁢i⁢s⁢t⁢(N)𝑑𝑖𝑠𝑡𝑁dist(N)italic_d italic_i italic_s italic_t ( italic_N ) works under the classifiers D⁢E⁢E⁢P𝐷𝐸𝐸𝑃DEEPitalic_D italic_E italic_E italic_P 1D-CNN, R⁢F𝑅𝐹RFitalic_R italic_F, L⁢R𝐿𝑅LRitalic_L italic_R, and S⁢V⁢M𝑆𝑉𝑀SVMitalic_S italic_V italic_M. The size of d⁢i⁢s⁢t⁢(N)𝑑𝑖𝑠𝑡𝑁dist(N)italic_d italic_i italic_s italic_t ( italic_N ) is 4⁢N4𝑁4N4 italic_N bytes as we store 4 vectors of size N𝑁Nitalic_N, using 8-bit integers.\n\nTime series,:\n\ndenoted by t⁢s⁢(N)𝑡𝑠𝑁ts(N)italic_t italic_s ( italic_N ). Time series of arrival time, packet size, and packet direction of N𝑁Nitalic_N packets, as proposed by (Bernaille et al., 2006a; Yang et al., 2021). t⁢s⁢(N)𝑡𝑠𝑁ts(N)italic_t italic_s ( italic_N ) works under the classifiers D⁢E⁢E⁢P𝐷𝐸𝐸𝑃DEEPitalic_D italic_E italic_E italic_P 1D-CNN, R⁢F𝑅𝐹RFitalic_R italic_F, L⁢R𝐿𝑅LRitalic_L italic_R, and S⁢V⁢M𝑆𝑉𝑀SVMitalic_S italic_V italic_M. The size of t⁢s⁢(N)𝑡𝑠𝑁ts(N)italic_t italic_s ( italic_N ) is approximately 6⁢N6𝑁6N6 italic_N bytes as we store the arrival time (a 32-bit float), packet size (11 bits for sizes up to 1500), and direction (a boolean value) of N𝑁Nitalic_N packets.\n\nFlowPic,:\n\ndenoted by f⁢p⁢(N×N)𝑓𝑝𝑁𝑁fp(N\\times N)italic_f italic_p ( italic_N × italic_N ). The FlowPic representation is a two-dimensional distribution matrix of size N×N𝑁𝑁N\\times Nitalic_N × italic_N of arrival times and packet sizes, as proposed by (Horowicz et al., 2022; Shapira and Shavitt, 2019; Engelberg and Wool, 2021; Finamore et al., 2023). As in d⁢i⁢s⁢t⁢(N)𝑑𝑖𝑠𝑡𝑁dist(N)italic_d italic_i italic_s italic_t ( italic_N ), each packet size and arrival time is mapped to one of N𝑁Nitalic_N bins. Then, element (i𝑖iitalic_i,j𝑗jitalic_j) in the matrix holds the number of packets whose arrival times are mapped to (arrival time) bin i𝑖iitalic_i, and packet sizes are mapped to (packet size) bin j𝑗jitalic_j.\n\nf⁢p⁢(N×N)𝑓𝑝𝑁𝑁fp(N\\times N)italic_f italic_p ( italic_N × italic_N ) can be used under the classifiers D⁢E⁢E⁢P𝐷𝐸𝐸𝑃DEEPitalic_D italic_E italic_E italic_P 2D-CNN, R⁢F𝑅𝐹RFitalic_R italic_F, L⁢R𝐿𝑅LRitalic_L italic_R, and S⁢V⁢M𝑆𝑉𝑀SVMitalic_S italic_V italic_M. The size of f⁢p⁢(N×N)𝑓𝑝𝑁𝑁fp(N\\times N)italic_f italic_p ( italic_N × italic_N ) is 2⋅N2⋅2superscript𝑁22\\cdot N^{2}2 ⋅ italic_N start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bytes as we store a two-dimensional vector of size N𝑁Nitalic_N for each traffic side, using 8-bit integers.\n\nStatistical Features,:\n\ndenoted by s⁢t⁢s𝑠𝑡𝑠stsitalic_s italic_t italic_s. Statistical representation, containing 33 basic statistical features, as proposed by (Williams et al., 2006; Draper-Gil et al., 2016; Lashkari et al., 2017; Anderson and McGrew, 2017) and others. The features are the basic minimum, maximum, mean, median, and standard deviation of the packet sizes and packet relative arrival times, for all packets and each traffic direction independently, as well as the number of packets for each direction. This representation is only used with simpler ML classifiers: R⁢F𝑅𝐹RFitalic_R italic_F, L⁢R𝐿𝑅LRitalic_L italic_R, and S⁢V⁢M𝑆𝑉𝑀SVMitalic_S italic_V italic_M. The size of s⁢t⁢s𝑠𝑡𝑠stsitalic_s italic_t italic_s is 132132132132 bytes if all statistics are stored as 32-bit float values (this can be reduced by setting specific types to each value) .\n\nB.1. Flow Representations\n\nThe results for the different classification tasks are shown in Tables 8–13. For brevity, we show only the accuracy and exclude the std results of the k𝑘kitalic_k-fold process. Additionally, we note that the neural network architecture of the D⁢E⁢E⁢P𝐷𝐸𝐸𝑃DEEPitalic_D italic_E italic_E italic_P CNNs is not relevant for small d⁢i⁢s⁢t𝑑𝑖𝑠𝑡distitalic_d italic_i italic_s italic_t and t⁢s𝑡𝑠tsitalic_t italic_s representations (as it includes pooling layers), as well as s⁢t⁢s𝑠𝑡𝑠stsitalic_s italic_t italic_s representations (which have no temporal structure).\n\nWe observe some common trends for most of the tasks and representations:\n\nFirst, d⁢i⁢s⁢t𝑑𝑖𝑠𝑡distitalic_d italic_i italic_s italic_t representations achieve the highest accuracy for most classification tasks (with two anomalies favoring f⁢p𝑓𝑝fpitalic_f italic_p), with the D⁢E⁢E⁢P𝐷𝐸𝐸𝑃DEEPitalic_D italic_E italic_E italic_P CNN classifier achieving the highest accuracy. It is apparent that the distributional features in d⁢i⁢s⁢t𝑑𝑖𝑠𝑡distitalic_d italic_i italic_s italic_t and f⁢p𝑓𝑝fpitalic_f italic_p representations hold meaningful information for classification more than raw values as a time series or a collection of statistical values.\n\nMoreover, larger representations tend to achieve higher accuracy, this is due to the granularity of the representations which allows identifying intrinsic behaviors unnoticeable in smaller or more coarse representations .\n\nFinally, small L⁢R𝐿𝑅LRitalic_L italic_R classifiers tend to achieve a very low accuracy compared to other classifiers. However, this gap is significantly narrowed for larger representations, where L⁢R𝐿𝑅LRitalic_L italic_R classifiers with larger representations achieve nearly equivalent results to those achieved by stronger D⁢E⁢E⁢P𝐷𝐸𝐸𝑃DEEPitalic_D italic_E italic_E italic_P classifiers.\n\nB.2. Models’ Deployment Considerations\n\nIn this section, we discuss the real-time deployment consideration of our classifiers. Particularly, we examine the memory requirements and the throughput of different representations and classifiers. First, we compare the memory required for storing all flow representations until classification. This is factored by the flow rate in the network, the collection time τ𝜏\\tauitalic_τ, and by the representation size. We note that the memory required for storing the parameters of most classifiers is negligible compared to the flow representations.\n\nTable 14 shows an approximation of the total required memory for storing flow representations. We can see that some representations (like larger d⁢i⁢s⁢t𝑑𝑖𝑠𝑡distitalic_d italic_i italic_s italic_t and f⁢p𝑓𝑝fpitalic_f italic_p representations) may be impractical for real-time use on edge devices in a large network.\n\nThe second objective we explore is the throughput of the models. That is, the number of flows processed by a classifier within a specified timeframe, measured based on given computational capabilities. Specifically, we measured the number of flows classified within a 60-second period, using batches of 1000 samples. This method allows classifiers to perform batch operations more efficiently than single-sample classification (e.g. matrix multiplications).\n\nTable 15 shows the throughput of different classifiers for different representations. It only includes the throughput calculated on models trained on the ISCX APPS classification task, however, as the required computational power merely changes across datasets or classification tasks, we observed similar results for the other tasks. The absolute numbers strongly depend on the deployment setting which in our case was an otherwise unloaded machine with an Intel core i7 processor (@ 2.80GHz 1.69GHz) with 16GB RAM, using Python’s Keras for D⁢E⁢E⁢P𝐷𝐸𝐸𝑃DEEPitalic_D italic_E italic_E italic_P CNNs (with tensorflow CPU backend) and SKlearn for R⁢F𝑅𝐹RFitalic_R italic_F, L⁢R𝐿𝑅LRitalic_L italic_R, and S⁢V⁢M𝑆𝑉𝑀SVMitalic_S italic_V italic_M classifiers.\n\nWe can see that despite the high accuracy achieved by D⁢E⁢E⁢P𝐷𝐸𝐸𝑃DEEPitalic_D italic_E italic_E italic_P CNN classifiers, the classification throughput may be impractical if deployed in a large ISP network with millions of new flows per second. On the other hand, we can see that as expected, L⁢R𝐿𝑅LRitalic_L italic_R classifiers outperform other classifiers in terms of throughput by an order of magnitude."
    }
}
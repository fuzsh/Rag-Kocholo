{
    "id": "dbpedia_6669_0",
    "rank": 34,
    "data": {
        "url": "https://medium.com/%40othornton414/jupyter-notebook-for-ml-technical-program-managers-7d977debedf7",
        "read_more_link": "",
        "language": "en",
        "title": "Jupyter Notebook for ML Technical Program Managers",
        "top_image": "https://miro.medium.com/v2/da:true/resize:fit:1200/0*xI7A-WVz8_z8vqY1",
        "meta_img": "https://miro.medium.com/v2/da:true/resize:fit:1200/0*xI7A-WVz8_z8vqY1",
        "images": [
            "https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png",
            "https://miro.medium.com/v2/da:true/resize:fill:88:88/0*05GT2fuDx2SmirAa",
            "https://miro.medium.com/v2/da:true/resize:fill:144:144/0*05GT2fuDx2SmirAa"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Oliver Thornton",
            "medium.com"
        ],
        "publish_date": "2023-08-24T00:37:12.299000+00:00",
        "summary": "",
        "meta_description": "Lately companies are starting to identify a very specific set of skills related to the machine learning world and data science. They are now considered a key value for ML TPMs. One of these is skills…",
        "meta_lang": "en",
        "meta_favicon": "https://miro.medium.com/v2/5d8de952517e8160e40ef9841c781cdc14a5db313057fa3c3de41c6f5b494b19",
        "meta_site_name": "Medium",
        "canonical_link": "https://medium.com/@othornton414/jupyter-notebook-for-ml-technical-program-managers-7d977debedf7",
        "text": "Lately companies are starting to identify a very specific set of skills related to the machine learning world and data science. They are now considered a key value for ML TPMs. One of these is skills is the ability to dig deeper into the data to better understand patterns, extract information, and combine datasets.\n\nTechnical Program/Project Manager (TPMs) are constantly challenged with the need of analyzing and correlating large amounts of data coming from different sources, and specially from Excel, data extracts, etc. If you fall into that category, this article is for you.\n\nAnd that is where the Python Pandas library comes into the play. In this write up I will share the basics on using Pandas DataFrame for manipulating and analyzing data to obtain insights or views that otherwise would require a lot of work using Excel or Google worksheets.\n\nFor this post I will use small data sets such that we can keep track of what is happening. Notice that you should be able to scale the same process for larger data sets.\n\nI will walk you through an end to end scenario that will show in a practical manner how to make use of Jupyter Notebook to read, filter, and connect data using Python Panda’s DataFrames.\n\n⚠️ Assumptions for this article: You have basic understanding of Python language, know how to run basic python programs, or have access to a Jupyter Notebook either on your laptop or a cloud/share environment.\n\nScenario\n\nLet’s assume you are given sales information and need to analyze the data to provide insights such as total per customer, items sales, location of customers, etc.\n\nThe data comes in three separate tables: Sales, Customer, and Products.\n\nData sets can be imported from Excel. For simplicity I included the code required for the import, but will not be using it. I will populate the data as code.\n\nPython Pandas — The basics\n\nPandas is a popular Python library used for data science as it offers, among other things, an easy way to manipulate and understand data when available in table format (rows / columns).\n\nPandas makes use of an structure name DataFrames, which are two-dimensional data structures (grid) where each row contains values, and each column correspond to the label of that value. Potentially each column could be of a different data type. If you are familiar with Excel spreadsheets, you are already familiar with what a DataFrame looks like.\n\nWith that very simple definition in mind, let’s look at our sales table and dive into the code:\n\nIn this example, each row contains values and the columns contain the label for that value. To create this table we can use the following code:\n\n# Import Pandas library\n\n# In order to have access to the DataFrame structure and functions within it\n\n# you must first import Pandas library into your program.\n\nimport pandas as pd\n\n# SALES DATA\n\n# Create a list containing the values of each column\n\nsales = [\n\n['0100212',1001,9001,5,24.99,'2023-5-1' ],\n\n['0100212',1001,9023,1,124.19,'2023-5-1' ],\n\n['0223432',1003,9001,3,28.99,'2023-6-12' ],\n\n['0223432',1003,9024,3,12.97,'2023-6-12' ],\n\n['0100123',1004,9001,5,24.99,'2023-6-21' ],\n\n['0100123',1005,9027,1,99.99,'2023-6-21' ],\n\n['0221152',1011,9002,3,14.99,'2023-7-3' ],\n\n['0221152',1011,9035,3,8.85,'2023-7-3' ]\n\n]\n\n# converts list into a pandas dataframe and assign name to each column\n\nsalesdf = pd.DataFrame(sales, columns=['Account','Order','Item_id','Quantity','Unit_price','Date'])\n\nThe output of this code is the table displayed above. Notice that we used the Pandas DataFrame library to create our sales-DataFrame. There are multiple ways of creating these structures, but for practical purposes on this article, I will use this approach.\n\nMoving forward all the code will have comments so that code and documentation stay together.\n\nBefore we move forward, let’s create the remaining DataFrames for Customers and Products.\n\n# Creates Customer Data List\n\ncust = [\n\n['0100212','John Smith','1 Royal Way','Las Vegas','Nevada'],\n\n['0100123','Jane Foster','201 Main Street','Seattle','Washington'],\n\n['0230432','Bob Anderson','45 Pine Drive','Salem','Oregon'],\n\n['0200133','Charles Peterson','10-2023 Somewhere Street','Everette','Washington'],\n\n['0221152','Mary Popins','1 Heaven Way','San Diego','California'],\n\n['0223432','Peter Foster','123 Artist Drive','Santa Monica','California']\n\n]\n\n# Converts the list into a DataFrame, and assign names to the columns\n\ncustdf=pd.DataFrame(cust, columns=['Cust Account','Name','Address','City','State'])\n\ncustdf\n\n# Creates Products Data List\n\nproducts = [\n\n[9001,'Booster ABC',24.99],\n\n[9002,'Safety belt RB',19.99],\n\n[9003,'Dash flower', 3.95],\n\n[9023,'Pad Safety ARB',135.25],\n\n[9024,'Rear deflector Stop',15.99],\n\n[9027,'Front Dash Bumper',99.99],\n\n[9035,'Vanity Mirror SQ',9.95]\n\n]\n\n# Converts the list into a DataFrame, and assign names to the columns\n\nprodsdf = pd.DataFrame(products, columns=['Item','Description','Sug_price'])\n\nprodsdf\n\nThe result of the previous code will be our three datasets properly converted to DFs.\n\nNow that we have our data structures in place, and properly converted to Pandas DataFrameworks, let’s start understanding the basic usage of DataFrames. For that, the first mental model we need to form is that a DataFrame is just a bi-dimensional array, so we should be able to get to the individual elements of the array if we know the row/column coordinates.\n\nAccessing and retrieving elements in a DataFrame (DF)\n\nThe first column of a DF, in our case above a sequential number, corresponds to the “index” to that specific row. Notice that the index starts with zero (0) and not one (1). So to retrieve “John Smith”’s record we need to use: custdf.loc(0) Where 0 is the index. The following code illustrates the output.\n\n# Retrieves and print the content of record index 0\n\nprint(custdf.loc[0])\n\nCust Account 0100212\n\nName John Smith\n\nAddress 1 Royal Way\n\nCity Las Vegas\n\nState Nevada\n\nName: 0, dtype: object\n\nThis type of access can become difficult for very large DFs, specially when we know there could be a different access key, in this case for example customer number. Pandas let you change the index column very easily using the command DF.set_index(). In our case we want to use: custdf.set_index('Cust Account')\n\nImportant Detail — In the code below we created a NEW copy of the same DF with Account Number as index. The original DF (custdf) still has the sequential number for index. To change the index on custdf permanently we need to use: custdf.set_index('Cust Account', inplace=True) . Let's not change it for now as having 2 similar DFs will help us illustrate the next point.\n\n# Creates a new dataframe named tmpdf with 'Cust Account' column as index\n\ntmpdf=custdf.set_index('Cust Account')\n\n# Changing the value for Bob Anderson Address on tmpdf\n\ntmpdf.loc['0230432','Address'] = '55 Olson Pl.'\n\n# display tmpdf dataframe - Notice address change\n\nprint (\"TMPDF DATAFRAME\")\n\ndisplay(tmpdf)\n\n# display original custdf dataframe - Notice address did not change for Bob Anderson\n\nprint (\"\\nCUSTDF DATAFRAME\")\n\ndisplay(custdf)\n\nIn the code, I illustrate:\n\n* the index for TPMDF is now the Cust Account number. The sequential index remains on CUSTDF.\n\n* how to access individual elements through the row/column combination,\n\n* how to update the value of that specific cell, and\n\n* the fact that Bob Anderson’s address is different in the original and copy DFs.\n\nIf you want to retrieve more than one record or set of records, Pandas makes it very easy. All you have to do is specify what you need:\n\n# Retrieve specif records by given a list of indexes separated by comma\n\nprint (\"Retrieving specific records by index:\")\n\ndisplay(tmpdf.loc[['0100212','0230432']])\n\n# Retrieves a range of indexes - This is sequential on the list\n\nprint(\"\\nor a range (also called slice) of the list \\n\")\n\ndisplay(tmpdf.loc['0100212':'0230432'])\n\nNow, if we want to retrieve specific columns and not all of them. We can easily do so by specifying which columns are of interest. In the case of our new dataframe tmpdf the first column (index) is going to be the \"Cust Account\". If we run the same command for custdf we will see a numeric index instead. Of course we could retrieve only a subset of the rows. Let’s see:\n\n# Display only 2 columns for tmpdf\n\ndisplay(tmpdf[['Name','State']])\n\n# Display the same 2 columns for custdf\n\ndisplay(custdf[['Name','State']])\n\n# Display only 2 columns for a slice of the data\n\nprint(\"\\nOnly a slice of the dataframe and show specific columns\")\n\ndisplay(tmpdf.loc['0100212':'0230432'][['Name','State']])\n\nSearching for elements using LOC\n\nHaving an index that we are familiar with is very useful, but what happens if we are interested in rows that meet a specific criteria and not just the index?. For example all customers who live Washington. In that case we need to specify the search criteria, as well as if we want only specific columns.\n\n# Search the tmpdf DataFrame and prints all rows wher customer State is equal to Washington\n\nprint (\"All columns for Washington State\\n\")\n\n# The LOC attribute allows us to do simple and complex searches\n\ndisplay(tmpdf.loc[tmpdf['State'] == 'Washington'])\n\n# Same search but prints only a subset of the columns\n\nprint (\"----\")\n\nprint (\"Showing only columns Name and City\\n\")\n\ndisplay(tmpdf.loc[tmpdf['State'] == 'Washington'][['Name','City']])\n\nYou can have more complex searches using the .isin function, or using logical long expressions. Let’s look at the following examples:\n\n# List all records where State is either Washington or Oregon\n\nprint(\"Customers in Washington or Oregon\")\n\ndisplay(tmpdf.loc[tmpdf['State'].isin( ['Washington','Oregon'])])\n\n# List all records where State is either Washington or Oregon\n\nprint(\"\\nCustomers in Washington or San Diego\")\n\ndisplay(tmpdf.loc[(tmpdf['State'] == 'Washington') | (tmpdf['City'] == 'San Diego')])\n\n# Or just those who are neither Washington or Oregon\n\nprint(\"\\nCustomers NEITER in Washington NOR Oregon\")\n\ndisplay(tmpdf.loc[~tmpdf['State'].isin( ['Washington','Oregon'])])\n\nI don’t want to sound overly enthusiastic but the possibilities are almost endless. As bonus, let me extend for a minute to show an example using regular expressions. Assume you want all customers with name starting with J or lastname Foster.\n\n# using regular expressions. Name starting with letter J\n\ndisplay(tmpdf.loc[(tmpdf['Name'].str.contains(r'(^J)') )])\n\n# using regular expressions. Lastname is Foster\n\ndisplay(tmpdf.loc[(tmpdf['Name'].str.contains(r'(Foster)') )])\n\nPretty cool eh? Using regular expressions would be an article on its ow, but I encourage you to explore this feature a lot more.\n\nConnecting DataFrames\n\nBack to our problem. Now that we have the basics (please not that there is A LOT more than what I just showed) lets move onto one of the most interesting features of Pandas DataFrames which is the possibility of connecting multiple DFs and working with them to obtain insights.\n\nBefore we dive into details, our first step is to get our datasets ready. And, as discussed before, the first step is to change index to the column that we want to use as “key” to access the data. For Customers and Sales we decided to use ‘Account Number’ and for Products we will use ‘Product Id’.\n\n# Update the index on the customer data set such that we can access by account number\n\ncustdf.set_index('Cust Account',inplace=True)\n\ndisplay(custdf.head(5))\n\n# Update the index on the Sales data set such that we can access by account number\n\nsalesdf.set_index('Account',inplace=True)\n\ndisplay(salesdf.head(5))\n\n# Update the index on the Products data set such that we can access by Item id\n\nprodsdf.set_index('Item',inplace=True)\n\ndisplay(prodsdf.head(5))\n\nSo, how are all the three DataFrames connected? Rather than use words let me use an image. As you can see all the three tables contain information that will enable you to join the data and obtain insights. And that is what I will show you how to do.\n\nSimple Join (2 DataFrames)\n\nWe now have 2 DFs with the same key field (customers -> Account Number, sales -> Account). Let’s assume that you want to get all the customer information for all sales. In this case a simple df.join() command for those tables will produce that result.\n\nThe way your join looks will depend on which DF you use as starting point. See below for details on both examples.\n\nThe first block of code will show you all the customers and their corresponding transactions from the sales DF. Note that those customers without transactions will show values as NaN\n\nprint(\"Dataframe.Join using Customer --> Sales\")\n\ncustdf.join(salesdf)\n\nThe second block of code will display all Sales records and their corresponding customers. In this case those customers without transactions will NOT be displayed.\n\nprint(\"Dataframe.Join using Sales --> Customer\")\n\nsalesdf.join(custdf)\n\nWe can join DFs and sort them by specific columns:\n\n# Sort values by Customer account, and then orders within each account\n\ncustdf.join(salesdf).sort_values(by=['Cust Account','Order'])\n\nFor easier read we can use the group functionality for DataFrames:\n\n# Sort the result by 'Account' and 'Order', then groups by Account > Name > State > Order > Item\n\ncustdf.join(salesdf).sort_values(by=['Cust Account','Order']).groupby(['Cust Account','Name','State','Order','Item_id']).mean('Unit_price')\n\nGoing advance: Joining 3+ DataFrames\n\nNow that we are familiar with the join function, it is time to move to the next step and start getting some good insights.\n\nIn general joining 3 DFs is just the same as above. It is just an iterative process of joining 2 DFs at a time: update index to match each other, and join.\n\nAs before, easier if we justlook at the code:\n\n# STEP 1\n\n# Create NEW dataframe with Sales and Customers. Both tables are indexed by Account number\n\nsalescustdf=salesdf.join(custdf).sort_values(by=['Account','Order'])\n\nsalescustdf\n\n# STEP 1.5\n\n# Display the new table as group for easier reading - Nothing new up to here\n\n# Group the results by Account > Customer > Order > Item Id\n\nsalescustdf.sort_values(by=['Account','Order']).groupby(['Account','Name','State','Order','Item_id']).mean('Unit_price')\n\n# STEP 2\n\n# Set the index of the NEW JOINED CustomerSales DataFrame to match the index of Products\n\n# For the next step needs to index by Item Id to be able to join the Products table\n\nsalescustdf.set_index('Item_id',inplace=True)\n\nsalescustdf\n\nBefore we go to the next step, we need to reset the index to a numeric value, and then, in preparation for the upcoming join, set “Item_Id” as the new index.\n\nOur Products table is already using Item Id as index, so all what is left to do is joining salescustdf with products (prodsdf):\n\n# Reset the index to a default numeric value\n\nsalescustdf.reset_index(inplace=True)\n\nsalescustdf\n\n# Set the index of SalesCustDF to Item_id\n\nsalescustdf.set_index('Item_id',inplace=True)\n\nsalescustdf\n\n# Created a new DFs with all the 3 tables together\n\nalldfs=salescustdf.join(prodsdf)\n\nalldfs\n\nAnd finally.. let’s get some insights..\n\nWe now have a DF with all our data merged together. Let’s start answering some questions:\n\n🎯 List all accounts and sales, grouped by customer and order number\n\n# List all accounts and sales grouped by customer and order number\n\n# Group the results by Account > Customer > Order > Item Id\n\nalldfs.sort_values(by=['Account','Order']).groupby(['Account','Name','State','Order','Item_id','Description']).mean('Unit_price')\n\n🎯 List All Sales by State:\n\n# Sales by State\n\nalldfs.sort_values(by=['Account','Order']).groupby(['State','Account','Name','Order','Item_id','Description']).mean('Unit_price')\n\n🎯 Add new column and list Totals for all Sales per Item\n\n# Add a column to calculate the total of each line item (unit x sale price)\n\nalldfs['Total Item']=alldfs['Quantity']*alldfs['Unit_price']\n\nalldfs\n\n🎯 Present summary of sales by State\n\nalldfs.sort_values(by=['Account','Order']).groupby(['State','Account','Name','Order','Item_id','Description']).sum('Total Item')\n\n🎯 List number of items sold by state\n\n# Which item sold by state\n\nalldfs.groupby(['State','Item_id','Description','Quantity'])[['Description']].sum('Quantity')\n\n🎯 List total of sales per account\n\n# Show the total per Account\n\nalldfs.groupby(['Account'])[['Name','Total Item']].sum()\n\n🎯 List how many units of each item were sold, calculate the average price, total sales. Note that some items were not sold at the Suggested Price\n\n# How many units were sold of every item, what was the average price, and total sold. Notice how we can have different operations on different columns.\n\nalldfs.groupby(['Item_id','Description','Sug_price']).agg({'Unit_price':'mean', 'Quantity': 'sum', 'Total Item': 'sum'})\n\n🎯 Show which State had and the most sales and what was the mean price paid. Also, show which Customers spent the most\n\n# find wich state had most sales\n\nalldfs.groupby(['State']).agg({'Unit_price':'mean', 'Quantity': 'sum', 'Total Item': 'sum'}).sort_values(by = 'Total Item', ascending = False)\n\n# Which customer spent the most\n\nalldfs.groupby(['Account','Name','City','State']).agg({'Total Item':'sum'}).sort_values(by='Total Item', ascending=False)\n\n🎯 Show which products where sold at different priced from Suggested Price\n\n# Find which products were sold at different price from RSP\n\n# - NOTICE THAT Item_id is not called out as it is already the default index\n\nalldfs[alldfs['Unit_price'] != alldfs['Sug_price']][['Description','Unit_price','Sug_price']]\n\nWe could go on and on with the insights but I hope this gets you started on your own journey through data exploration and insights.\n\nAnd remember, be kind to one another!"
    }
}
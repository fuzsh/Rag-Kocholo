{
    "id": "dbpedia_4607_2",
    "rank": 61,
    "data": {
        "url": "https://dl.acm.org/doi/10.1145/3613904.3642447",
        "read_more_link": "",
        "language": "en",
        "title": "Sharenting on TikTok: Exploring Parental Sharing Behaviors and the Discourse Around Children’s Online Privacy",
        "top_image": "https://dl.acm.org/cms/asset/5a7300cb-db68-4c64-914f-a794a820bd09/3613904.cover.jpg",
        "meta_img": "https://dl.acm.org/cms/asset/5a7300cb-db68-4c64-914f-a794a820bd09/3613904.cover.jpg",
        "images": [
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/acm-dl-logo-white-1ecfb82271e5612e8ca12aa1b1737479.png",
            "https://dl.acm.org/doi/10.1145/specs/products/acm/releasedAssets/images/acm-logo-1-ad466e729c8e2a97780337b76715e5cf.png",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/action/showDoPubAsset?doi=10.1145/contrib-81100481887&format=rel-imgonly&assetId=apu-medium.jpg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/footer-logo1-45ae33115db81394d8bd25be65853b77.png",
            "https://dl.acm.org/cms/10.1145/3613904.3642447/asset/ecf7ff4b-923c-476a-9140-4070ac1c2f78/assets/images/medium/chi24-555-fig1.jpg",
            "https://dl.acm.org/cms/10.1145/3613904.3642447/asset/f91ad6ce-f071-4481-ba68-bcb9791e0335/assets/images/medium/chi24-555-fig2.jpg",
            "https://dl.acm.org/cms/10.1145/3613904.3642447/asset/66939275-809e-472a-ac1c-ec5ab26bcdff/assets/images/medium/chi24-555-fig4.jpg",
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/Default_image_lazy-0687af31f0f1c8d4b7a22b686995ab9b.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/action/showDoPubAsset?doi=10.1145/contrib-81100639942&format=rel-imgonly&assetId=dsc_0976_close_up.jpg",
            "https://dl.acm.org/action/showDoPubAsset?doi=10.1145/contrib-81100531194&format=rel-imgonly&assetId=maxwilson10x82.jpg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/action/showDoPubAsset?doi=10.1145/contrib-81100288406&format=rel-imgonly&assetId=portraits_2019-2020_colour1.jpeg",
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/loader-7e60691fbe777356dc81ff6d223a82a6.gif",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/action/showDoPubAsset?doi=10.1145/contrib-81100481887&format=rel-imgonly&assetId=apu-medium.jpg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/acm-logo-dl-8437178134fce530bc785276fc316cbf.png",
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/acm-logo-3-10aed79f3a6c95ddb67053b599f029af.png"
        ],
        "movies": [
            "https://iframe.videodelivery.net/eyJraWQiOiI3YjgzNTg3NDZlNWJmNDM0MjY5YzEwZTYwMDg0ZjViYiIsImFsZyI6IlJTMjU2In0.eyJzdWIiOiIzNDJkM2ZjMDU2NGFkYjUzOWQ2NWRlNTY4N2VmYzU4YSIsImV4cCI6MTcyNDg4MjM4OSwia2lkIjoiN2I4MzU4NzQ2ZTViZjQzNDI2OWMxMGU2MDA4NGY1YmIifQ.DtogvvCvgBczzQqStj8p_-kM30tEnxpTdleaA4DEkTrB7d2XfbbULbVjO8zaYZutLb5UZ_IPz-iFHgiTxT6vQ_UgWTdK_NaDP-sCrjxJd_cUkpzSS8Ruz1nXspkto1CePTarEg5EE8rA7IDtZGvf5_GmYEoXwXMtgoiM_FVFI7Xqjxb6gSJgGcE9WCB3ukLBBKlOn5RN2tPW4mR90klJ1js-Tkbs9wDuoRUAQsYVGC-tQqyMPoGAWpzIh-SenRTX1wYTF8GTaDwQBsHnm97aQM007HTrKw4i9dIcsU7tdRACoBK_IZPAA6hByvd_aWMqOxyvio6BH9bBO4HXZ5fLRg?poster=https%3A%2F%2Fvideodelivery.net%2FeyJraWQiOiI3YjgzNTg3NDZlNWJmNDM0MjY5YzEwZTYwMDg0ZjViYiIsImFsZyI6IlJTMjU2In0.eyJzdWIiOiIzNDJkM2ZjMDU2NGFkYjUzOWQ2NWRlNTY4N2VmYzU4YSIsImV4cCI6MTcyNDg4MjM4OSwia2lkIjoiN2I4MzU4NzQ2ZTViZjQzNDI2OWMxMGU2MDA4NGY1YmIifQ.DtogvvCvgBczzQqStj8p_-kM30tEnxpTdleaA4DEkTrB7d2XfbbULbVjO8zaYZutLb5UZ_IPz-iFHgiTxT6vQ_UgWTdK_NaDP-sCrjxJd_cUkpzSS8Ruz1nXspkto1CePTarEg5EE8rA7IDtZGvf5_GmYEoXwXMtgoiM_FVFI7Xqjxb6gSJgGcE9WCB3ukLBBKlOn5RN2tPW4mR90klJ1js-Tkbs9wDuoRUAQsYVGC-tQqyMPoGAWpzIh-SenRTX1wYTF8GTaDwQBsHnm97aQM007HTrKw4i9dIcsU7tdRACoBK_IZPAA6hByvd_aWMqOxyvio6BH9bBO4HXZ5fLRg%2Fthumbnails%2Fthumbnail.jpg%3Ftime%3D10.0s"
        ],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Data & Information Sciences",
            "University of Wisconsin-Madison",
            "United States https:",
            "orcid.org",
            "Indiana University",
            "University of Washington",
            "Sophie Stephenson",
            "Christopher Nathaniel Page",
            "Miranda Wei",
            "Apu Kapadia"
        ],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "/pb-assets/head-metadata/apple-touch-icon-1574252172393.png",
        "meta_site_name": "ACM Conferences",
        "canonical_link": "https://dl.acm.org/doi/10.1145/3613904.3642447",
        "text": "Abstract\n\nSince the inception of social media, parents have been sharing information about their children online. Unfortunately, this “sharenting” can expose children to several online and offline risks. Although researchers have studied sharenting on multiple platforms, sharenting on short-form video platforms like TikTok—where posts can contain detailed information, spread quickly, and spark considerable engagement—is understudied. Thus, we provide a targeted exploration of sharenting on TikTok. We analyzed 328 TikTok videos that demonstrate sharenting and 438 videos where TikTok creators discuss sharenting norms. Our results indicate that sharenting on TikTok indeed creates several risks for children, not only within individual posts but also in broader patterns of sharenting that arise when parents repeatedly use children to generate viral content. At the same time, creators voiced sharenting concerns and boundaries that reflect what has been observed on other platforms, indicating the presence of cross-platform norms. Promisingly, we observed that TikTok users are engaging in thoughtful conversations around sharenting and beginning to shift norms toward safer sharenting. We offer concrete suggestions for designers and platforms based on our findings.\n\nFigure 1:\n\n1 Introduction\n\nA vast majority (82%) of parents who use social media have posted text, photos, or videos about their children online [6]. This phenomenon is known as sharenting (a combination of “sharing” and “parenting”). Although sharenting is not a new phenomenon—for example, the first “mommy blogs” emerged two decades ago [33]— social media has given parents a new space to share memories, proud moments, funny anecdotes, and other snippets of their children’s lives. By sharing about their children, parents express love, share memories with family and friends [16, 52, 67], and form community [8, 13, 27, 95], among other advantages.\n\nDespite its benefits, sharenting is controversial because it presents several risks to children who are the subject of sharenting. Revealing a child’s name, location, or other personal information could prove dangerous offline if strangers (or acquaintances) use that information to gain the child’s trust [4, 13, 65]. Online, photos that parents share can be appropriated for illicit use [48], children’s information can be leveraged by adversaries like identity thieves [4, 19, 65], and content may embarrass children or lead to bullying [4, 43, 92]. Some critics also believe that young children are unlikely to understand the implications of sharenting and thus cannot consent to the creation of this digital footprint [8, 52, 92]. Therefore sharenting, despite its benefits, also comes with privacy and security risks.\n\nGiven its controversial nature, prior work has investigated sharenting on Facebook [7, 16, 52, 59, 65], Instagram [14, 57, 65], and social media broadly [2, 4, 8, 13, 27, 65, 67, 68, 75, 96] (§ 2.1). However, the latest variety of sharenting—sharenting on short-form video platforms, and specifically on TikTok—also deserves attention. Parents are using TikTok to share short videos about talented children [97], create family-centered content [53, 79], and illustrate the realities of parenting children with disabilities [55], to name a few examples. Sharenting in this new context merits attention for three reasons. First, most TikTok posts are videos, which contain rich and detailed information. Second, TikTok is designed to broadcast content to the largest possible audience [24, 26, 31]. This wider reach increases the potential for privacy violations, especially when creators suddenly go viral [86] and are then incentivized to post similar content going forward [34]. Finally, TikTok’s short, relatively unpolished videos are easy and fast to create [30]. Because of these affordances, parents on TikTok can disseminate detailed information about their children quickly to a large audience. This may explain why some parents on TikTok are choosing to sharent in more privacy-conscious ways or to stop sharenting altogether [46, 47, 54, 56, 61].\n\nIn this study, we performed a targeted exploration of sharenting on TikTok. As a widely popular platform containing multi-modal, interconnected content, TikTok is a rich source of qualitative data; by studying TikTok, we can understand more organically than is possible in interviews the ways in which people sharent and how they discuss sharenting on the platform [98]. We sought to answer the following research questions:\n\nRQ1:\n\nCharacterizing Sharenting on TikTok (§4): In what ways do creators share content about their children on TikTok? For example, what information do they share, and in what context? By studying this behavior, we can understand sharenting norms on TikTok and how they might differ from sharenting norms on other platforms.\n\nRQ2:\n\nCharacterizing Discourse Around Sharenting (§5): What is the discourse about sharenting on TikTok? Examining creators’ discussions about sharenting will help us understand the current concerns and practices around sharenting, and compare these findings with prior work.\n\nTo characterize sharenting on TikTok (RQ1), we used TikTok’s manual search functionality to collect 328 examples of sharenting on TikTok and examined them across several characteristics. These TikTok videos (represented in Fig. 1) showed children across a range of ages and shared information about them, including personally identifiable information (PII) and information about sensitive topics such as menstruation. Surprisingly, 97% of the videos in our dataset did not mention whether the children consented1 to the post—and although five videos claimed the child gave consent, another five videos showed a child actively objecting to being filmed. Although our sample does not necessarily represent the entire sharenting space on TikTok, this analysis gave us a lens into the nature of sharenting on the platform.\n\nNext, we took a step back to analyze the discourse about sharenting among TikTok creators (RQ2). We collected 438 TikTok videos that discuss sharenting and analyzed them with inductive methods. We found that the conversation around sharenting on TikTok is interconnected and generally anti-sharenting, which may be influenced by a self-selection bias among creators who are critical of sharenting. These videos raised several concerns about the potential consequences of sharenting (e.g., that predators and others may misuse the content) as well as the potential to violate children’s right to privacy, autonomy, and informed consent. In all, although some of these creators’ concerns about sharenting were consistent with prior work, we uncovered specific concerns about TikTok-style sharenting that require scrutiny. Promisingly, we observed that TikTok also fosters critical conversations in which creators are setting new, more privacy-preserving norms.\n\n2 Background & Related Work\n\nOur work contributes to broader research on sharenting as well as a growing body of work studying TikTok.\n\n2.1 Sharenting\n\nSharenting is the practice of posting about one’s child on social media platforms. The precise definition of sharenting varies, but in this paper we define sharenting as any instance where a parent or other caregiver shares visual or written information about their child (aged 0-17) online.\n\nSharenting is commonplace among parents. In 2012, an estimated 75% of parents had sharented on Facebook [7]; later, in 2020, a survey of 3,640 parents indicated 82% of parents had sharented [6]. As a result, sharenting has been studied through surveys [4, 7, 13, 16, 59, 65, 67, 68, 96], interviews/focus groups [2, 8, 14, 16, 27, 52, 75, 95], analyses of sharenting posts [14, 27, 57, 59, 65, 90], and legal analyses [22, 32, 92]. Some works focused on specific platforms, including Facebook [7, 16, 52, 59, 65], Instagram [14, 57, 65], Twitter (now called X) [27], and VK (a Russian equivalent of Facebook) [90]; others studied sharenting without focusing on a specific platform [2, 4, 8, 13, 27, 65, 67, 68, 75, 96]. We summarize these works in the rest of this subsection.\n\nBenefits and risks. Sharenting can benefit both parents and children. Parents sharent to find community, support, and advice through peers [8, 13, 16, 27, 52, 67, 92, 95], often by presenting authentic narratives that challenge mainstream portrayals of parenthood [44]. Sharenting can also help parents archive memories [52], display their identity as a parent [14, 52], and discuss controversial topics [16]—and because they believe others love seeing their posts [16]. More broadly, sharenting helps parents advocate for child-related causes [92], raise awareness and create community for marginalized groups (e.g., children with disabilities and their parents) [92], challenge gender roles in parenting [14], or raise money in a crisis [92]. However, some have argued that today’s sharenting is less about community or advocacy and more about acquiring views and monetizing social media content [33, 44]. Finally, anecdotal evidence indicates that sharenting might alert a community to problematic or abusive parenting [21].\n\nDespite its benefits, sharenting increases the risk that children will encounter online and offline threats. For instance, sharenting often reveals children’s names, faces, and birthdates, and can yield even more information when combined with public records [65]. This information could be misused by dangerous actors within the child’s social circle [25, 28, 43, 58, 65, 92], identity thieves [4, 19], private companies and data brokers [4, 43, 65, 92], or even governments and “surveillant authorities” [65, pg. 776]. Parents also sometimes share embarrassing content that could lead to bullying by children or adults [4, 13, 28, 43, 52, 92], and sharented images may be misused by predators or others [4, 13, 16, 43, 52, 65, 92]. Unfortunately, parents are not always aware of these risks [75, 92]. In extreme cases, parents’ drive to create viral sharenting content can itself create an unsafe environment for their children [63]. Besides explicit threats, some believe that sharenting is problematic in principle. One issue is that young children cannot give informed consent to the sharing because they do not understand its implications [52]. Thus, by sharenting, parents create a semi-permanent digital footprint for their child without their child’s consent [16, 52]. In spite of these risks, recent work found that parents’ social media usage is strongly correlated with sharenting frequency, which suggests that some parents still see sharenting posts as no different from any other social media post [4].\n\nParents’ and children’s boundaries. Given these concerns, parents believe some types of sharenting are acceptable, while others are not. Generally, parents feel okay sharing positive portrayals [2, 27, 52, 68], milestones [2, 27], things done as a family [27], and cute, proud, or funny moments [2, 52]. In contrast, content that provides private information (name, face, birthday, location, etc.) [13, 27, 65, 68], content that could be sexualized [27, 52, 65], negative or embarrassing content [13, 27, 52, 68], and divisive content [27] are considered inappropriate. Despite holding these boundaries, in one study, many parents admitted to sharing embarrassing, inappropriate, or location-revealing content [13].\n\nAlthough Moser et al. found that children believe the frequency of their parents’ sharenting is “about right” [68, p. 5223], most research suggests that children disapprove of sharenting [39, 75, 96] and sometimes find it “embarrassing and useless” [96, p. 1]. Children prefer positive, infrequent, and non-intimate sharenting, and they prefer when parents ask for their consent to post [68, 75]. They are more concerned about photos than other types of posts [68].\n\nLaws and regulations. In the U.S., children who are the subject of sharenting have sparse legal protections. The most relevant laws are Illinois Senate Bill 1782 [88] and Washington House Bill 1627 [81, 93] (currently in committee), which guarantee compensation for children of for-profit family vlogs (video blogs). These laws protect children’s property rights but not their rights to autonomy and privacy. Aside from these protections, parents typically have exclusive control over the disclosure of their child’s information and it is assumed that they will do what is best for their child [92]. The Children’s Online Privacy Protection Act (COPPA) [17], for example, allows parents to control the information collected about their children (under 13) online. Outside of the U.S., the UN Convention on the Rights of the Child [69]—which the U.S. has not ratified—recognizes that children’s “autonomous nature can be recognized at a young age” [92, pg. 864]. Further, the Right to Be Forgotten, part of the EU’s General Data Protection Regulation (GDPR) [29], allows people to remove links about them from search results [91], which could be adapted to apply to sharenting [92]. However, some argue that the GDPR is insufficient because it, too, places parents in control of children’s digital identities [22].\n\n2.2 TikTok\n\nThis study centers sharenting on TikTok, a popular social media platform [40] consisting mostly of short-form videos (“TikToks”). TikTok is designed to show content that interests each user rather than content from people they explicitly follow. The primary TikTok interface, the For You Page (FYP), gives users an infinite feed of curated content based on their previous viewing activity. This format allows content to spread quickly to new audiences and has led to a culture of trying to go viral on the platform [38]. Because of this dynamic, some argue TikTok is television, not social media [24].\n\nOn TikTok, creators can add to others’ videos and create video replies to text comments. They can also interact with videos via likes, comments, and saves (which bookmark the content so the user can easily access it later). Within TikToks, creators can add sounds and embed text, images, and videos from other sources. These features make TikTok a rich source of qualitative data, and accordingly, researchers have begun to study TikTok from a variety of angles [1, 9, 23, 45, 62, 74, 76, 85, 89, 98, 100].\n\nAnecdotal evidence suggests that TikTok is at once a repository for sharenting videos [53, 55, 79, 97] and a space where creators discuss and reason about sharenting [46, 47, 54, 56, 61]. Despite this, prior research focused primarily on Facebook or Instagram, or on sharenting behaviors across many platforms. In a recent survey study focused on predictors of sharenting behavior, 10% of participants had sharented on TikTok [4]; however, the study did not further explore sharenting on TikTok. We argue that sharenting on TikTok deserves targeted attention due to the platform’s aforementioned affordances and use cases.2\n\n3 Method\n\nTo understand sharenting and sharenting discourse on TikTok, we analyzed 328 videos that demonstrate sharenting and 438 videos that contribute to the discourse around sharenting. We conducted searches using TikTok’s existing functionality—i.e., no third-party tools were used to identify or select videos.\n\n3.1 Data Collection\n\nTo begin, the first and second authors spent a week exploring sharenting-related content on TikTok. We used sharenting-related search terms and tags, such as “sharenting” and “#momlife,” and expanded these terms as we identified new relevant videos. We also looked for videos that added onto relevant videos and explored other posts from creators who posted relevant videos. This process yielded 89 relevant videos: 42 examples of sharenting (hereafter “sharenting videos”) and 47 videos that contribute to discourse (hereafter “discourse videos”).\n\nBased on this initial investigation, we selected three sharenting-related search terms (“kids,” “family,” and “parenting”) and three discourse-related search terms (“sharenting,” “not sharing my kids,” and “kids are not content”). The discourse-related terms do favor an anti-sharenting perspective, but this reflects our earlier observations. We manually searched for these six terms in June 2023, collecting up to 300 videos for each. We noted each video’s URL; creator; current number of likes, comments, and saves; and caption. In total, we selected an initial set of 1,461 videos over two days.\n\nNext, we organized the videos into three categories: sharenting videos, discourse videos, and irrelevant videos. The inclusion requirements for sharenting videos were (1) a child was present in the video or the video shared specific information about a child, such as a child’s name or a story about a child; and (2) the video appeared to have been originally posted by the child’s parent or another caregiver. For discourse videos, our inclusion requirements were that the video discussed someone’s opinion on or experiences with sharenting or the video was a news story or other reposted content that was shared to add to the discourse. We excluded videos that were not in English. We discussed and reached agreement on the criteria for inclusion [60], then used the criteria to identify 308 unique sharenting videos and 409 unique discourse videos.\n\nMany videos in our dataset used TikTok’s built-in stitch and duet features: stitches show a clip from an existing video followed by new content, while duets show an existing video side-by-side with new content. Our dataset contained stitches or duets, so we also collected the original videos used in those compilations. Some videos were stitches of stitches or duets of duets, so we repeated this process until no new relevant videos were linked in the dataset. Doing so added 20 more sharenting videos and 29 more discourse videos, for a total of 328 sharenting videos and 438 discourse videos. 20 videos were both sharenting videos and discourse videos. A list of URLs for all 746 videos is available to researchers upon request.\n\n3.2 Characterizing Sharenting on TikTok\n\nWe began by characterizing our dataset of sharenting videos. Our goal was to understand the contents of the dataset across many different characteristics; thus, we first defined which characteristics we were interested in learning about based on our initial investigation and our research questions. These characteristics (Table 1) included the approximate ages of the children shown in the videos, the information shared about those children, and video type (e.g., stitch or duet), among others. Within each characteristic, we used open coding to generate codes organically from the data. For some characteristics, the codes were inherently limited—e.g., there are only a few possible video types—but others, like the information shared about children, were much more open-ended. We also allowed for miscellaneous codes to arise outside of the chosen characteristics.\n\nTo begin the coding process, the first and second authors independently coded a sample of 100 sharenting videos, then met to compare codes, augment our characteristics of interest, and define an initial codebook. We used this to independently code a different sample of 50 sharenting videos, then met to seek agreement before proceeding [60]. Finally, we divided the full set of 328 videos between the two coders for analysis. In the interest of collaborative qualitative data analysis (CQA) [82], we met regularly to address uncertainty and make any necessary changes to the codebook. Our final (abbreviated) codebook is shown in Table 1.\n\nTo code a single video, we watched it in its entirety and read the caption. We also looked at the creator’s page and used their username, bio, and recent videos to gain further context and add codes about the creator. Because many videos featured more than one child, we coded the maximum visibility of any children in the video—e.g., if the video showed one child’s head from behind and another child’s face, we would code that as “full face.” When coding a reposted video, we searched for information about the original creator when available; when coding a stitch, we ignored all content from the stitched video and coded only the content that was added by the current creator.\n\n3.3 Analyzing Sharenting Discourse on TikTok\n\nWhile our goal for the sharenting dataset was to summarize characteristics of interest, our goal for the discourse dataset was to explore the dataset from the ground up. Thus, for this stage of the work, we did not define any characteristics of interest and instead generated all themes from the data. Specifically, we adapted Kuckartz’s thematic qualitative text analysis methodology [51], using multiple iterations of inductive coding to construct high-level thematic categories. Compared to our analysis of sharenting videos, this analysis required more rounds of coding with fewer videos per round because the discourse videos were longer and more detailed.\n\nTo begin, the first and second authors separately watched a sample of 50 discourse videos and created individual codebooks for the sample. We discussed our codes and defined an initial shared codebook with main topical categories like “Cons of sharenting” and “Sharenting boundaries.” We then independently coded another 25 discourse videos using this initial codebook and met again to discuss, updating and augmenting the codebook as needed. At this stage, we refined the top-level categories and began to generate subcategories, such as “online threats” and “offline threats” underneath “Cons of sharenting.” After a third round of coding with another 25 videos, we reached agreement [60] on a detailed, multi-layer codebook, and all 438 videos were divided between the two coders. Once the coding process was complete, we finished with a category-based analysis of the key thematic categories (§ 5.3–§ 5.6) in which we compared subcategories within each thematic category and assessed relationships between categories [51].\n\nWhen coding, we used the same procedure as before, although we treated stitches, duets, and other reposted content differently. In the sharenting videos, we wanted to understand how parents and other caregivers shared about their children, so we ignored content from others. With the discourse videos, however, we were exploring the conversation around sharenting, and stitches, duets, and reposted content are all important parts of that conversation. Thus, we coded all content in the video, regardless of its origin.\n\n3.4 Ethical Considerations\n\nOur IRB deemed the study not to be human subjects research. However, IRB exemption is not sufficient for research involving social media due to the potential ethical concerns of using “public” posts [12, 101]. Although a post may technically be available to the public, the person who posted it likely did not anticipate that it would be taken out of context and compiled with similar content in a research study [12, 71]. Thus, we took measures to protect the anonymity of the creators in our study, avoiding naming specific creators and showing artistic renderings of videos rather than providing screenshots. Similar to prior work, our goal is to understand the broad trends in sharenting and conversations about sharenting on TikTok—not to call out or study any specific creators [98].\n\n3.5 Positionality Statement\n\nSince the authors were the “data collection instrument” [10] in this research, our positionality impacted our approach, analysis, and results. In particular, all of the authors are computer security and privacy researchers, and our interest in the topic stemmed from the potential security and privacy concerns that arise from sharenting. We generally believe that while some forms of sharenting can be acceptable (e.g., private posting of respectful content), many forms of sharenting are problematic (e.g., posting publicly or violating explicit non-consent).\n\nTwo of the authors are parents who sharent minimally; one of us shares non-controversial content to private social media platforms, while the other shares only in private group chats and only if their child consents. The other three authors, including the two coders, are not parents. All of us have been the subject of sharenting in some form, although two authors were only sharented as adults. Two authors use TikTok outside of work, two authors used TikTok only for this project, and the fifth only views videos sent to them by someone else. The authors post infrequently on social media.\n\n3.6 Limitations\n\nFirst, we do not claim that our results represent what the average user will come across on TikTok. Because users’ experiences on TikTok are heavily influenced by their algorithm, the sharenting content users see will vary [45]. The first and second authors created TikTok accounts used only for the study which returned identical search results, but these search results and the content we collected may differ from what users see on personalized accounts. Also, by collecting English-language posts only, we missed some types of sharenting and discourse from other cultures and contexts.\n\nFurther, although our search terms were designed based on an initial exploration of the content, they may have biased our dataset toward family and parenting content, and we may have overlooked videos that do not contain related terms. Additionally, the discourse search terms lean anti-sharenting, which was reflected in the dataset (§ 5). We attempted to address this by searching for pro-sharenting content but did not see common themes within the pro-sharenting videos in our dataset that could be used to generate additional search terms. Thus, our discourse dataset remained mostly anti-sharenting. We hypothesize that this is due in part to self-selection bias, where creators who feel strongly against sharenting may be more likely to create content about it. However, we also believe that pro-sharenting content is represented by the sharenting videos themselves, while the discourse challenges this status quo (§ 6.1).\n\nFinally, as mentioned, we could only approximate several characteristics of the videos (e.g., age, creator’s relationship to the child, and whether the video was scripted). In particular, we could not know the gender of the children in most videos; even if the creator used pronouns or gendered words, those may not have reflected the child’s gender [87], especially if that child was transgender or gender-nonconforming and not out to their parents. Thus, we only report pronouns or gendered words used for children in the videos.\n\n4 Sharenting On TikTok\n\nTable 1:\n\nTable 2:\n\nWe analyzed a varied subset of 328 sharenting videos on TikTok. Below, we describe our dataset in detail across many dimensions, including information about the children shared in these videos, the type of content within the videos, and the creators posting these videos. Table 1 summarizes our findings.\n\nDescriptive statistics (Table 2). Videos were posted between March 2020 and June 2023 and were 37 seconds long on average. In total, the videos had approximately 415 million likes, 3 million comments, and 29 million saves when they were collected. The videos came from 306 creators; the most frequently occurring creator, a viral video account, contributed 7 of the videos. More than 85% of the videos (283) appear to only contain content by the creator who posted them; we refer to these as “original videos.” Of the rest, 40 videos were reposts of other creators’ videos and five were stitches.\n\n4.1 Types of Sharenting Videos\n\nOur dataset contains a wide variety of content, from comedy to trends to sharenting discourse. Fig. 1 provides artist renderings of five types of sharenting videos we observed.\n\nAbout half of the videos in our dataset were comedy videos. Many of these videos were unscripted footage of children doing humorous things, such as pranking a parent, play fighting, saying silly things, or clutching a squirrel (Fig. 1a). However, a few were comedic at the children’s expense—for example, by making fun of children who are scared, upset, uninformed, getting pranked (Fig. 1d), or suffering minor injuries. These videos were often reposted by viral video accounts, indicating that they can have a wide reach on TikTok. In addition to comedy about children, a few videos joked about parenting.\n\nParenting content was also common in the dataset. The most common type of parenting video in our dataset was feel-good videos, which usually show sweet or poignant family moments and children being cute. Another type was educational videos where creators advise on how to parent a certain way or show real-life examples of a specific style of parenting (Fig. 1c). A few videos talked about struggles creators face as parents, and while a couple of these videos were lighthearted (e.g., one parent’s video about how life changed after they had kids) others showed more difficult moments, such as parents’ struggles with fatigue, lack of support, financial trouble, or extreme misbehavior. Finally, eight videos were vlogs (video blogs) showing families in their daily life: doing daily routines, making dinner, going shopping, etc.\n\nAnother 46 videos in our dataset followed TikTok trends and challenges. Many of these videos took part in family-centered trends like family introduction videos, where family members are introduced to the audience while dancing or lip-syncing (Fig. 1b), or family games, such as the “Who’s most likely to” trend. Others took part in child-focused trends, such as where parents show their children and who they’re named after. Finally, in a few videos, creators participated in generic trends (e.g., dances) as a family.\n\nFinally, twenty sharenting videos in our dataset also contributed to the discourse around sharenting, which we discuss in § 5.\n\n4.2 Age and Gender of Sharented Children\n\nA majority of the videos (66%) showed or discussed a single child, while the rest included two or more, with up to 11 children in a single video. The children’s ages ranged from infant to adolescent. Toddlers and preschoolers were the most frequently occurring age groups in the dataset, with each age group appearing in around 120 videos (37%); school-aged children were involved in over 80 videos (25%), while infants and adolescents each appeared in around 40 videos (12%). Many of the videos did not refer to children with any pronouns or gendered language, but 133 videos (41%) referenced children using she/her pronouns and words like “daughter” and 102 videos (31%) used he/him pronouns and words like “son.” Only one video in our dataset referred to a child with they/them pronouns.\n\n4.3 Information Shared in the Videos\n\nGiven the risks of sharenting—from identity theft to the child’s embarrassment—it is important to understand how much information is typically conveyed about children in TikToks. Here, we describe the types of information sharing that occurred frequently in our dataset, as well as notable outliers. Given that privacy is contextual [71], this can help us understand the current norms on TikTok and which types of sharenting may be seen as (in)appropriate.\n\nVisible presence of children. Almost all of the videos in our dataset showed a child’s face, body, or voice in some way (Fig. 2). Even in videos that did not show any children, many captured children’s voices—for example, videos in which parents interacted with their children, but showed only their own faces in the video. Thus, as with other forms of social media, it appears normal for children of all ages to visibly or audibly appear in TikTok videos.\n\nA few outliers not only showed children’s physical forms but showed them in states of distress or undress. Thirty-three videos (10%) showed a child crying, screaming, throwing a tantrum, or otherwise in distress (Fig. 1e). In addition, 10 videos (3%) showed infants or toddlers wearing swimsuits or dressed only in diapers. Although seeing a young child in a diaper may not be concerning depending on the context, some parents worry about the potential for this type of footage to be misused by predators online (§ 5). In our dataset, these were relatively uncommon, indicating that they may indeed be considered less appropriate.\n\nFigure 2:\n\nInformation shared about children. In addition to the physical presence of a child, most of the videos shared at least some information about a child. Information about a child’s family—whether they have siblings, names of family members, family rules and routines—was shared in 86 videos (26%) and was especially common in trends where the intent is to introduce family members. Many videos (22%) also shared PII, including children’s names, birthdays, or medical information such as injuries, ailments, and disabilities. Others shared a child’s misbehavior (22%), often because it was considered funny, and 6% of videos revealed information about sensitive topics like menstruation, bathroom habits, crushes, and past experiences of abuse. Additionally, while some videos took place in more public spaces, a majority of the videos (72%) appear to have been filmed in a home, and 40% appear to have been filmed in private rooms such as a bedroom or bathroom.\n\nPrivacy-preserving videos. Although not the norm in our dataset, several videos took a more privacy-preserving approach. For instance, 26 videos (8%) did not show children’s bodies or voices at all (although they did share information about the children) while 120 videos (37%) showed children but did not reveal anything about them. (In either case, however, children may still find the content embarrassing.) Sometimes, privacy-preserving practices seemed more deliberate; in a small number of videos (6%), creators obscured their child’s image with physical objects (e.g., sunglasses) or digital artifacts (e.g., emojis and text), or positioned the child’s face out of view of the camera. Nonetheless, these videos did not necessarily reflect the creators’ usual posting behavior; in many cases, more information could be found in the creators’ other videos.\n\nOutliers and controversial content. Contrasting with the privacy-preserving videos, we labeled 14 videos as potentially controversial based on our intuitions and parents’ concerns from prior work (§ 2.1). These videos contained child nudity or partial nudity (e.g., children in diapers), children swearing, parents swearing around children, children lip-syncing to explicit songs, and content with overt political messaging. We also flagged a video where a parent (upon the request of a fan) asked a young child if they found a sixty-year-old celebrity attractive, as well as a video where a mother made her young child repeat an action multiple times to get the perfect scene for a TikTok video.\n\nDue to the subjectivity of the term “controversial,” we analyzed the comments on these videos to understand what other users thought. We did not observe many critical comments on the videos, except for the videos of children swearing or lip-syncing to explicit music; commenters had mixed reactions on whether the explicit content was appropriate for the children. The most intriguing comments appeared on the video where the child repeated an action multiple times to get the perfect TikTok video. Commenters on this video were not critical of the parent who posted, but of the child; the commenters empathized that it is frustrating to create content with children, and agreed that it was “irritating” that the child couldn’t get the scene right the first time.\n\n4.4 Children’s Roles in the Videos\n\nIn most videos (82%), children played a starring role, although some videos (47%) included children in supporting roles (e.g., in only part of the video) or in the background (10%). (In several videos, multiple children were involved, and the children played different roles in the video.) On the other hand, children usually did not seem to be active participants in creating the video. For example, in almost all videos (84%), it appeared that the children were filmed while engaging in normal activities, in contrast to videos like skits or trends, where it was clear the child knew they were being filmed and actively participated (17%).\n\nFor each video, we noted whether anyone in the video or the video’s caption mentioned that the child consented to the sharenting post. Only five videos in the entire dataset (2%) stated that the children gave consent. Although there is debate about whether these children were able to give informed consent (§ 6.3), this shows that a few parents were considering their children’s wishes when posting. Unfortunately, in an equal number of videos, parents actively violated a child’s wishes by posting a video in which they are dissenting. For example, in one video, the creator laughs at their child while the child cries and begs the creator to stop recording.\n\n4.5 Accounts that Post Sharenting Videos\n\nThis dataset only includes videos posted by caregivers—parents, grandparents, other family members, and nannies of the children in the videos. However, these caregivers posted a variety of accounts, and some videos in our dataset were reposted by others on TikTok.\n\nAbout half of the videos in the dataset (56%) came from accounts focused on parenting or families. The most prevalent creator type was parent bloggers (37%), whose content focuses primarily on their life as a parent (or grandparent, for three videos). In these accounts, the parent is the main character but children do appear in videos. A few videos (3%) also came from parenting educators who use their platform to teach others about specific parenting methods. On the other hand, a small number of videos came from family accounts (13%) and child accounts (3%), where children play more of a central role. Family accounts treat the whole family as main characters and feature content from all members. Child accounts, on the other hand, exclusively feature content about a young child. These accounts often pretend the child runs the account and typically post funny or feel-good videos of the child.\n\nThe other half of the videos were posted by accounts not specifically related to families. These include health and fitness accounts, comedy influencers, product sellers, and many accounts without a clear theme. Viral video accounts were particularly prevalent, accounting for 10% of videos. We saw two types of viral video accounts in our dataset: those reposting all types of viral videos and those that specifically repost viral videos of children. Videos from these accounts were usually comedy videos and they received more likes, comments, and saves on average compared to the entire dataset.\n\n5 Sharenting Discourse On TikTok\n\nFigure 3:\n\nNow, we turn our attention to another side of sharenting: discussions amongst TikTok creators about the appropriateness of sharenting. Analyzing these discussions deepens our understanding of creators’ concerns and community norms for sharenting in the context of this popular platform.\n\n5.1 Overview of Discourse Dataset\n\nWe identified 438 videos contributing to the discourse about sharenting on TikTok. These videos were posted between September 2019 and June 2023. The videos lasted 55 seconds on average and had a total of around 18 million likes, 259 thousand comments, and 738 thousand saves at the time they were collected (Table 3).\n\nThe videos in the discourse dataset came from 255 unique creators. Unlike the sharenting dataset, some creators contributed a large number of discourse videos to this dataset—for example, the top three creators contributed 96, 38, and 19 videos, respectively. This is a limitation of our dataset and thus we focused less on quantitative measures in this section, instead using the terminology in Table 4 to refer to different frequencies of codes. However, these prolific creators also provided more nuanced views on sharenting and brought in other creators’ perspectives via stitches, duets, and comment replies—all of which helped us reach theoretical sufficiency [11, 20]. We also emphasize that the “overrepresentation” of these creators may accurately reflect the way discourse happens on TikTok, where influencers often spearhead movements or trends (see, e.g., [49]). Indeed, the most prolific of these creators portrays themself as an anti-sharenting influencer.\n\nTable 3:\n\nTable 4:\n\nVideo formats. While the sharenting dataset contained mostly original videos, our discourse dataset contained a wider variety of interconnected content. About half of the discourse videos interacted with other content using built-in methods (stitches, duets (Fig. 3a), and replies to comments) or by informally embedding other content in the video itself. Often, creators interacted with other sharenting discourse to augment another creator’s argument or simply spread it to a larger audience. However, creators also interacted with sharenting content, usually to criticize it and use it as an example of what not to do. In both cases, these interactions supported the creator’s arguments for or against sharenting. Fig. 3 shows artistic renderings of some of the interconnected videos we observed, while Table 8 in the Appendix details the types of content embedded within stitches, duets, and original videos in the dataset.\n\nChildren appeared often in the stitched/dueted/embedded content, and sometimes creators chose to blur or cover the child when using that content (Fig. 3b). However, an equal number of creators in our dataset simply included the original content as-is (Fig. 3c). We found it surprising that these creators shared images of children given that they were often discussing their opposition to sharenting or to the specific sharenting content they were interacting with.\n\n5.2 Categories of Discourse Videos\n\nAlmost all the videos—all but 20—were against sharenting in some way. About half of the videos generally addressed sharenting and its potential harms, including some which fun of sharenting, provided tips and advice to parents for safer sharenting, and discussed the creator’s own (negative) experiences being sharented. We also observed videos where creators reacted to specific sharenting content—usually by criticizing someone’s sharenting practices, but occasionally by promoting examples of sharenting the creator found acceptable. Finally, in some videos, creators discussed their own sharenting choices and the rationale behind those choices.\n\nWhile anti-sharenting videos were dominant (unsurprisingly, due to our search terms), there were a few videos that speak positively about sharenting. These creators discussed the potential benefits of sharenting, discuss why they choose to sharent, joked about people who don’t share or who ask them not to share, and encouraged others to sharent. In three videos (which are also sharenting videos), creators defiantly showed their children in response to people who advised them not to sharent.\n\n5.3 Concerns About Sharenting\n\nSince a majority of the discourse videos were definitively anti-sharenting, the creators of these videos raised many concerns with sharenting. Their concerns fell into two main groups: concerns about the online and offline consequences of sharenting, and concerns about sharenting on principle. We list the concerns in Table 5.\n\nOnline consequences. About half of the discourse videos mentioned potential online consequences of sharenting. The most common concern in the dataset was sexual predators. Some used the number of interactions on sharenting videos, especially saves, as evidence that the videos are being misused by predators. The creators worried that predators or others may misuse child-centered content by saving it, putting it on other sites (including websites related to child sexual abuse), or creating deepfake photos or audio.\n\nBeyond predatory behavior, creators pointed out that sharented children may be harassed by the audience, especially if they belong to a marginalized group (e.g., if the child is trans). Alternatively, viewers can develop parasocial relationships with children in these videos, where viewers are incredibly invested in the child’s life and believe they have a personal relationship with the child; creators found this troubling. Finally, creators were concerned that sharenting can create a digital footprint for the child—a semi-permanent record of their life that will persist as they grow up.\n\nTable 5:\n\nOffline consequences. In addition to online consequences, creators discussed ways that sharenting could impact a child’s life offline. The most common offline concern was mental health impacts, from embarrassment to lifelong trauma. Along these lines, creators were also worried that sharenting could impact a child’s relationships—e.g., if their parent is more concerned with posting viral content than spending time with their child. Children may also be bullied by their peers or adults, further impacting their mental health. And a small group of creators chose not to sharent out of concern for the evil eye: a curse that can bring “a beam of bad juju or bad luck,” especially to babies and children [83].\n\nIn addition to mental health risks, creators brought up safety and financial risks that could arise from sharenting. Due to sharenting, creators argued, a stranger could learn information about a child that reveals their location or allows the stranger to gain the child’s trust. For example, if parents share about a child’s broken arm, a stranger with this knowledge could pretend to be a doctor on the case and ask the child to come with them. Finally, a few creators worried that sharenting could lead to theft of the child’s identity, impacting their finances as well as other aspects of their life.\n\nPrinciples: children’s autonomy, privacy, and consent. Creators were concerned about sharenting not only because of its consequences, but also on principle. In our dataset, a large cohort of creators contributed to the “kids are not content” movement—the idea that sharenting (in some forms) is exploitative and uses children’s lives as a means to get views and make money. Typically, the concerns were about more extreme forms of sharenting, where parents share very frequently, share more intimate information, and make their child a primary focus of their content, often with the goal of monetizing their content. Some creators posited that adopted and foster children are particularly vulnerable to this type of exploitation—see, e.g., the case of the Fantastic Adventures YouTube channel, where a mother was accused of abusing several foster children who starred in the channel [63].\n\nAlthough this extreme sharenting was a major concern, some creators opposed any form of sharenting, including less invasive forms. Over 100 videos in our dataset mentioned that children have rights to privacy or autonomy, arguing that sharenting in many cases violates those rights. Related to this, nearly 100 videos discussed how sharenting should not occur unless the child consents. Specifically, many of these videos argued that many children are not capable of giving informed consent because they do not understand the potential long-term repercussions of being shared on a public platform. These creators believed that sharenting should not happen at all until the child is able to provide informed consent (although creators did not specify when informed consent is possible). More generally, a few creators stated that children are a vulnerable group that deserves respect and deserves to enjoy childhood without thinking about their presence on social media.\n\nOther concerns. Beyond concerns about consequences and principles, the videos mentioned a smattering of other reasons not to sharent. A few videos brought up that sharenting is largely unregulated and until protections are in place for children it should be limited. On the other hand, a few creators noted that sharenting is often prohibited by adoption and foster care policies. Videos also referenced potential interpersonal issues with sharenting, including a risk of irritating followers or triggering people who struggle with conception. Finally, some parents stated that they would simply rather keep their children to themselves.\n\n5.4 Pro-Sharenting Arguments\n\nNot all of the videos were against sharenting. In a few videos, creators mentioned benefits of sharenting which echoed findings from prior work (§ 2.1); e.g., that sharenting creates community and raises awareness for causes. A few videos gave counterarguments in response to the aforementioned concerns about sharenting. Some of the counterarguments focused on predators—for example, creators reason that we should blame the predators misusing content, not parents who post that content. To those who say sharenting is exploitative, a few creators argued that actual child abuse should be addressed instead, and that parents have a right to share. In response to concerns about consent, creators argued that parents can give consent for their children in other contexts, and that in many cases, children do understand the implications and affirmatively consent. Finally, some dismissed concerns about sharenting because they compare it to children acting in movies or TV and claim that the content can always be deleted later.\n\nTable 6:\n\nTable 7:\n\n5.5 Sharenting Norms and Opinions\n\nAlmost all discourse videos in the dataset offered an opinion on which types of sharenting are acceptable or unacceptable. These opinions were either shared as personal boundaries—e.g., a creator describing how they will share their child differently in the future—or as stances on how sharenting should be done in general. Sometimes, creators also mentioned specific sharenting trends, account types (e.g., child accounts), and even specific creators which go against these norms. Table 6 and Table 7 visualize these “norms of appropriateness” [72] and the number of unique videos and creators who mentioned each norm.\n\nInformation-specific norms. Some norms are centered around specific information (physical and non-physical) that should (not) be shared. For instance, several creators gave opinions about how children should physically appear in videos. The most common opinions about physical sharing were that children should not be shown when partially nude, distressed, or experiencing abuse. Children’s faces were another common concern; several creators believed that a child’s face should not appear online and, in fact, a few videos mentioned that sharenting is acceptable as long as the child’s face is not shown.\n\nCreators also gave opinions about the types of information that should or should not be shared about children online, the most common being that sensitive topics like menstruation, toilet-related content, body measurements, trauma, and anything likely to embarrass a child should be avoided. Along these lines, a few videos mentioned that “personal information” should not be shared. The sentiment of the content also matters: a few agreed that negative information should not be shared, while others noted that positive or fun things can be shared. Finally, creators were wary of sharing identifying information. This includes PII like a child’s name, birth date, or birth time (potentially shared via a birth announcement); medical information; information that could reveal location, including first day of school photos, report cards, routines & activities, and vacations; and usernames for digital platforms. Broadly, a few videos warned parents not to share “identifying information,” although they did not specify which details they would consider to be identifying.\n\nIn a few cases, creators specified which information is acceptable to share. One type of acceptable information relates to parents’ lives, not children’s. For example, some creators believe it is okay to share parenting content, clothes, and room decorations. Some also believe that it is okay to share videos that do not impact the child’s privacy, which includes things like trends, pranks, and parents telling stories about their children without showing the event as it occurred. Interestingly, in direct opposition to the opinions stated in other videos, some creators believe it is okay to share a child’s name, birthday, and routines.\n\nNorms around how sharenting occurs. On a different note, how sharenting occurs impacts whether creators believe it is acceptable. Publicness was mentioned in a few videos; a handful of videos urged parents not to post on public accounts (especially with a large audience), while others conceded that posting privately is okay. The platform also matters, as a few creators believe that posting on TikTok or YouTube is not okay but posting on Instagram or Facebook or sharing directly with family is acceptable. Finally, creators cautioned parents not to share images (usually in relation to concerns that predators will view those images), and two videos warned parents that using hashtags on private sharenting posts may make those posts available to a wider audience.\n\n5.6 Setting Personal Sharenting Boundaries\n\nOften, creators of these videos described their personal sharenting boundaries. These creators provided further details about how they made sharenting decisions and the challenges they faced.\n\nThe boundary-setting process. Creators discussed how they set boundaries around sharenting. For instance, a common theme was that creators changed their mind about sharenting at some point. Often, these creators were persuaded by TikTok discourse or were influenced by eye-opening events such as learning that their child had been digitally kidnapped—a phenomenon where strangers steal photos of children online and pretend to be the children or their parents. In contrast, some creators made their sharenting decisions before their first child was even born.\n\nMaking decisions about sharenting can be challenging, according to a few videos. Some challenges are personal—for influencers with kids, for example, it is difficult to avoid content with children in it because their children are a big part of their life. There are also interpersonal challenges. For instance, 10 videos mentioned that setting boundaries can create tensions with family and friends, especially when they do not understand the reasons why a parent has chosen not to share. Finally, the audience can make things difficult; audience members can become frustrated because creators do not share their children, and when the size of the audience changes, it can be difficult to adjust habits accordingly.\n\nWhen boundaries break down. These opinions regarding sharenting aren’t necessarily set in stone. In our dataset, we identified several examples where creators made one or more videos adamantly expressing their anti-sharenting views and boundaries, then later abruptly changed their stance. Upon deeper investigation, we identified at least seven creators in our dataset who were adamant in their anti-sharenting opinions and later published TikTok videos breaking the same boundaries they put forth for themselves. In each identified case, the parent never overtly expressed their reasons for changing their behavior.\n\n6 Discussion\n\nWe aimed to understand sharenting norms on TikTok by examining the ways sharenting occurs on TikTok and the ways creators discuss sharenting. Our analysis of 746 TikTok videos provides new insight into the sharenting ecosystem on TikTok (§ 6.1), how sharenting issues on TikTok echo broader debates about sharenting (§ 6.2–§ 6.3), and implications for research, design, and policy (§ 6.4–§ 6.5).\n\n6.1 Characterizing Sharenting on TikTok\n\nWe provided a detailed understanding of sharenting behaviors and discourse on TikTok. Here, we highlight three key characteristics of the sharenting ecosystem on TikTok.\n\nRisky individual posts. Our results exemplify how TikTok’s affordances may lead to risky sharenting posts. First, TikTok enables sharenting content to reach a wider audience. Benchmarks show that TikTok content receives nearly 10 times higher engagement on average than other platforms [26] and the videos in our dataset received a high amount of engagement indeed, with an average of 1.3 million likes per video. Several videos were reposted by viral video accounts, which garnered even more interactions. Further, the number of videos in our dataset with broad appeal—particularly those intended to be humorous or those that took part in trending challenges (§ 4.1)—indicates that some parents may be posting with the intention of reaching wide audiences. This type of behavior modification has been observed on Twitter, where people who experience a viral event subsequently craft their content to be similar to those in viral posts [34].\n\nWe also observed that some TikTok trends encourage information-revealing posts. For example, we saw trends where creators introduce their families with names and ages or show a video of their infant child and the person they were named after. This encourages creators to reveal information that commonly appears in security questions and (poorly designed) passwords. A similar problem exists on Facebook, where “innocent” quizzes ask users to share information about themselves such as their mother’s maiden name [50]. Although prior work has shown that children’s names, faces, and birthdays are often provided in sharenting posts on other platforms [65], we provide evidence that this type of sharing is actively encouraged by TikTok trends.\n\nExploitative patterns. Beyond individual posts, our work identifies concerns about exploitative patterns of sharenting; namely, the “kids are not content” movement. Prior work has focused on the potential ramifications and benefits of casual posting—for example, a mother who shares photos and stories about her child on Facebook to keep relatives up to date. Accordingly, the motivations for sharenting mentioned in prior work do not include gaining online popularity or making money, unless the purpose is raising money in a crisis [92] (§ 2.1). In contrast, TikTok creators are most concerned about influencer-style sharenting and the risks that arise when parents repeatedly use their children as tools for internet fame and fortune. Their concern indicates the rise of a more extreme form of sharenting than has been previously studied. We hypothesize that TikTok’s affordances, and a culture of virality [38], exacerbate and encourage this type of sharenting [34].\n\nDiscourse and norm-setting.. Although we found that sharenting on TikTok can be risky, we also showed that TikTok enables critical conversations about sharenting that challenge the status quo. Creators are confronting sharenting directly, engaging with each other’s views, and tightening their boundaries in response to these discussions. This visible norm-setting is particularly interesting given that norms are usually implicit in online communities [99] and that people prefer to use invisible sanctions online [80]. We see these explicit sanctions as evidence that creators are collaboratively shifting norms toward more privacy-preserving sharenting, on TikTok and beyond. Our method of analyzing TikTok posts enabled us to closely observe this norm-setting process, which is not as visible on platforms with slower-spreading discourse.\n\n6.2 Echoing Cross-Platform Concerns\n\nAdditionally, our results from TikTok echo commonly-held concerns and boundaries that were raised in prior work and in the context of other platforms. For example, the creators in our discourse dataset shared concerns about predators [4, 43, 65, 92], safety risks [65, 92], and consent [16, 52, 91]. There also seems to be some consensus about what types of sharenting are unacceptable. As in prior work, we observed general disapproval for content that provides private or identifying information [13, 27, 65, 68], nudity and content that could be sexualized [27, 52, 65], negative content (including children in distress) [52, 68], and content that could be embarrassing for the children currently or later [13, 27]. We can consider these repeated norms as a lower bound of what types of sharenting can be considered acceptable across platforms.\n\nOur results also support Fox and Hoy’s observation that brands sometimes encourage sharenting [27]. Fox and Hoy first observed this phenomenon on Twitter, where they saw mothers sharenting (sometimes with PII) in response to brand engagement. Likewise, a few creators in our discourse dataset mentioned that brands seek partnerships with parents on TikTok because they can sell children’s products or simply because children generate views. In fact, sites exist with the express purpose of connecting brands with “momfluencers” [66]. Worryingly, we see hints that brands may actively encourage norm-breaking sharenting. For example, in the discourse dataset, one creator described how a brand asked her to make a video of her child in the bath to market their bath products.\n\n6.3 The Issue of Informed Consent\n\nThe aforementioned concerns are firmly situated in the online context: concerns about what parents post, how that content may be viewed and manipulated, and how brands interact with sharenting content. However, our results indicate that some sharenting concerns are situated outside the internet altogether. In particular, children’s right to informed consent is a broad consideration that has been raised both in our results and in prior work [16, 52, 91].\n\nOne concern is about when children can give informed consent. Although many creators in the discourse dataset emphasized the importance of informed consent, they did not offer concrete guidelines about when a child can give informed consent. Some creators in our dataset talked about children not being “old enough” to consent, indicating that age reflects a child’s ability to give informed consent; on the other hand, some creators argued that we should approach this question on a per-child basis. One creator, for example, argued that a parent will know whether their child has the maturity level and necessary background information to be able to give informed consent—and that different children may reach this stage at different ages.\n\nLegally, the age at which children can consent in specific contexts such as medical matters [5] is clearly defined. However, in the academic world, the question of children’s consent is far from solved. Scholars in developmental psychology and health contexts continue to study the ethics of children’s consent, particularly whether and when children can give consent to participate in research (e.g., [18, 37, 64]). A review article of empirical studies on children’s ability to consent finds no singular criteria for consent and instead illustrates relevant considerations such as children’s cognitive development, parents’ beliefs about child autonomy, situational factors, and children’s experience with the particular decision they are making [64]. Children’s consent in online settings is also an active area of research, given that children are less likely to understand the long-term implications of information being shared online [70] and that younger children (under nine years old) cannot “engage with the internet in a safe and beneficial manner” [41, p.4]. Our work demonstrates that TikTok creators are grappling with these ambiguities in the discourse around sharenting (§ 5).\n\nTo make matters more complex, some argue that until children can give informed consent, parents have the power to give consent on behalf of their children. For example, parents already give consent for their children in medical contexts, on school permission slips, and for research studies [73]. This fundamental tension between parental rights and children’s autonomy and privacy has been raised in prior work [8, 22, 32, 64, 92]. Thus, perhaps the root of this debate is actually different views on parenting; namely, how much responsibility and control parents feel they should have over their children’s privacy, and when they believe that control passes to their children. All told, the issue is more nuanced than simply preventing any sharenting before informed consent is provided and likely requires a sociological solution rather than a technical one.\n\n6.4 Future Research Directions\n\nThis area is ripe with research opportunities. To learn more about sharenting behaviors, interview studies could clarify parents’ perceived norms around when children can give consent, illuminate strategies for setting sharenting boundaries (and challenges faced when doing so), and shed light on how brand engagement impacts when and how people sharent. It would also be fascinating to explore how sharenting is correlated with other factors—for example, how much parents value other people’s privacy [36]. Our results provide foundational context for these conversations. For example, we identified several patterns and challenges associated with setting sharenting boundaries that could be used to structure an interview study on the topic.\n\nResearchers should also investigate the role of other stakeholders. For example, an analysis of sponsored sharenting posts or posts which interact with brands could clarify the role of brands. Qualitative work with creators who contribute to sharenting discourse could illuminate what inspires people to speak out about sharenting, especially when it deviates from their usual content. These other perspectives matter because sharenting impacts more people online than just parents and children—for instance, the creators who saw sharenting content, reacted negatively to it, and felt compelled to speak up against it.\n\nBroadly, we encourage researchers to take advantage of TikTok as a resource for rich qualitative data. In particular, our results indicate that TikTok can be a resource for studying norm-setting and frank interactions between creators that are difficult to access in other contexts.\n\n6.5 Design and Policy Implications\n\nBased on our results, we suggest design and policy interventions that support the cross-platform sharenting norms identified in § 6.2. For example, since common concerns include sharing (1) identifying information [13, 27, 65, 68] and (2) images of children [68], design interventions that obscure children’s faces in sharenting content would support existing norms. Toward this goal, designers can take inspiration from the privacy-preserving strategies we observed from some creators (e.g., blurring or obscuring children’s faces). For example, as a lower bound, platforms can provide an easy-to-apply face-blurring filter that parents can use when sharenting, or more sophisticated transforms that maintain the viewers’ satisfaction or increase the amount of redaction with time [35, 78]. As an upper bound, platforms can even attempt to identify children’s faces in content and blur their faces by default. The ability to identify children’s faces could also be used to flag if parents post many sharenting videos within a short time window, which could trigger a message to the parent about community norms. TikTok already uses (opaque) methods to scan videos and compare creators’ claimed age to the age they appear in their content [77], which could be repurposed for these use cases. However, the benefits of these techniques must be weighed against the privacy risks of collecting biometric data [84].\n\nMore subtly, platforms could build in tools to scaffold conversations around children’s consent. We identified children’s consent as a major concern of TikTok creators, but since there is little consensus on an age of consent for sharenting, we recommend that TikTok and other social media platforms encourage creators to consider the consent of all parties who appear in their content. To do so, TikTok could include an extra step before creators post, asking (1) if the creator has obtained consent from everyone in the video and (2) if they believe the children in the video understand the potential repercussions of being shared. Visual indicators become less powerful over time [42], and prior work has shown that such privacy warnings can even backfire [3]; however, this type of intervention still fosters discussion about children’s right to consent and signals to creators that they should consider these new norms.\n\nThe aforementioned interventions apply when sharenting content is created, but platforms also have a role to play after sharenting content is posted. For example, multiple creators in our dataset discussed how they manually detect and hide sharenting content from their For You Page, which we see as a form of invisible sanction [80]. TikTok could support these users by flagging sharenting content (e.g., by identifying sharenting-related tags and captions or using age verification) and allowing users to hide all sharenting content from their feed if they choose.\n\nFinally, our results indicate a need for policy that aligns with sharenting norms. The newest sharenting laws in the U.S. [81, 88] take a good first step by mandating that a percentage of funds generated from sharenting must go to the children who are being shared. While these laws aim to mitigate harm after the fact, additional policies should be put in place to prevent norm-breaking sharenting. For instance, laws could restrict brand involvement in sharenting or even prevent creators from monetizing sharenting content at all. On the other hand, to combat risky individual posts, laws could prevent parents from posting content where children are in distress or actively dissenting to the content. Children should also be able to request that their parents’ content be taken down later if they find it violated their privacy—essentially, a stronger version of California’s existing “Eraser Bill,” which requires social media platforms to give minors the option to delete content they previously posted [15]. In the meantime, this effort could begin with TikTok reinterpreting (and, importantly, enforcing) its existing Community Guidelines, since they already prohibit any content that exposes youth to exploitation [94].\n\n7 Conclusions\n\nWhen parents post about their children on social media, they expose those children to online and offline risks. Prior work has studied such sharenting on text- and image-based platforms. Still, little is known about sharenting on short-form video platforms like TikTok, which enable a wealth of information to be shared with a wider audience. Thus, we performed an exploratory study of sharenting on TikTok, analyzing 328 TikTok videos which demonstrate sharenting and 438 TikTok videos which discuss sharenting.\n\nOur findings shed new light on sharenting on TikTok and social media as a whole. First, we highlight how TikTok’s format appears to present different risks to children, indicating that we must be wary of the ways that new modalities could impact the privacy of children. For instance, some TikTok trends encourage parents to reveal information about children such as their name, age, and members of their immediate family. However, we also found substantial evidence of a concerning trend: repetitive patterns of sharenting when parents share frequent, intimate posts about their children with the goal of creating viral content. Promisingly, we found that creators are confronting these issues through critical, interconnected conversations.\n\nFurther, our results deepen the human-computer interaction community’s understanding of sharenting agnostic of any one social media platform. We identified concerns (e.g., predators), boundaries (e.g., embarrassing content), and influences (e.g., brand involvement) that have been raised in prior work and appear to be cross-platform issues. One cross-platform concern, children’s informed consent, was particularly salient in our results—our findings reflect a lack of consensus about when informed consent can be given and indicate that some sharenting interventions will be societal, not technical. Finally, we encourage further research that approaches sharenting from multiple perspectives; design interventions that align with changing norms, such as face-blurring filters that enable parents to easily hide their children’s faces; and legislation that not only addresses the consequences of repetitive sharenting but aims to prevent it from happening.\n\nAcknowledgments\n\nWe thank the anonymous reviewers, whose comments helped us improve the paper. Additional thanks to Akira Ohiso for creating the illustrations for Fig. 1 and Fig. 3. We appreciate Susan Zvacek, Majed Almansoori, Rose Ceccio, Rahul Chatterjee, Inyoung Cheong, and Rachel McAmis for providing feedback on the draft. This work was done as part of the Center for Privacy and Security for Marginalized and Vulnerable Populations (PRISM), supported by the National Science Foundation under Awards 2205171 and 2207019. This work was also supported in part by the NSF under Award 2222242.\n\n8 Appendix\n\n8.1 Supplementary Figures\n\nTable 8:\n\nTable 8 gives an overview of the types of content embedded within discourse videos.\n\nSupplemental Material\n\nMP4 File - Video Presentation\n\nVideo Presentation\n\nDownload\n\n37.96 MB\n\nTranscript\n\nReferences\n\n[1]\n\nSamantha J Albucker and Shari R Lipner. 2023. Social media creators are far from nailing it: A cross-sectional analysis of 100 longitudinal melanonychia TikTok videos shows poor educational content and lack of skin of color representation. J. Cutan. Med. Surg. 27, 2 (March 2023), 170–173. https://doi.org/10.1177/12034754231159649\n\n[2]\n\nTawfiq Ammari, Priya Kumar, Cliff Lampe, and Sarita Schoenebeck. 2015. Managing children’s online identities: How parents decide what to disclose about their children online. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI ’15) (Seoul, Republic of Korea, April 2015). ACM, New York, NY, 1895–1904. https://doi.org/10.1145/2702123.2702325\n\n[3]\n\nMary Jean Amon, Rakibul Hasan, Kurt Hugenberg, Bennett I Bertenthal, and Apu Kapadia. 2020. Influencing photo sharing decisions on social media: A case of paradoxical findings. In 2020 IEEE Symposium on Security and Privacy (SP) (San Francisco, CA, May 2020). IEEE, New York, NY, 1350–1366. https://doi.org/10.1109/SP40000.2020.00006\n\n[4]\n\nMary Jean Amon, Nika Kartvelishvili, Bennett I. Bertenthal, Kurt Hugenberg, and Apu Kapadia. 2022. Sharenting and children’s privacy in the United States: Parenting style, practices, and perspectives on sharing young children’s photos on social media. Proc. ACM Hum.-Comput. Interact. 6, CSCW1, Article 116 (April 2022), 30 pages. https://doi.org/10.1145/3512963\n\n[5]\n\nNational District Attorneys Association. 2013. Minor Consent to Medical Treatment Laws. Retrieved February 19, 2024 from https://ndaa.org/wp-content/uploads/Minor-Consent-to-Medical-Treatment-2.pdf.\n\n[6]\n\nBrooke Auxier, Monica Anderson, Andrew Perrin, and Erica Turner. 2020. Parenting Children in the Age of Screens. Pew Research Center. Retrieved June 6, 2023 from https://www.pewresearch.org/internet/2020/07/28/parenting-children-in-the-age-of-screens/.\n\n[7]\n\nMitchell K Bartholomew, Sarah J Schoppe-Sullivan, Michael Glassman, Claire M Kamp Dush, and Jason M Sullivan. 2012. New parents’ Facebook use at the transition to parenthood. Fam. Relat. 61, 3 (July 2012), 455–469. http://dx.doi.org/10.1111/j.1741-3729.2012.00708.x\n\n[8]\n\nAlicia Blum-Ross and Sonia Livingstone. 2017. “Sharenting,” parent blogging, and the boundaries of the digital self. Popular Communication 15, 2 (April 2017), 110–125. https://doi.org/10.1080/15405702.2016.1223300\n\n[9]\n\nMaximilian Boeker and Aleksandra Urman. 2022. An Empirical Investigation of Personalization Factors on TikTok. In Proceedings of the ACM Web Conference 2022 (WWW ’22)’ (Virtual Event, Lyon, France, April 2022). ACM, New York, NY, 2298–2309. https://doi.org/10.1145/3485447.3512102\n\n[10]\n\nBrian Bourke. 2014. Positionality: Reflecting on the research process. The Qualitative Report 19, 33 (2014), 1–9. https://doi.org/10.46743/2160-3715/2014.1026\n\n[11]\n\nVirginia Braun and Victoria Clarke. 2021. To saturate or not to saturate? Questioning data saturation as a useful concept for thematic analysis and sample-size rationales. Qualitative Research in Sport, Exercise and Health 13, 2 (March 2021), 201–216. https://doi.org/10.1080/2159676X.2019.1704846\n\n[12]\n\nAmber M Buck and Devon F Ralston. 2021. I didn’t sign up for your research study: The ethics of using “public” data. Computers and Composition 61 (Sept. 2021), 102655. https://doi.org/10.1016/j.compcom.2021.102655\n\n[13]\n\nC. S. Mott Children’s Hospital.2015. Parents on social media: Likes and dislikes of sharenting. National Poll on Children’s Health 2, 23 (2015), 2 pages. https://mottpoll.org/reports-surveys/parents-social-media-likes-and-dislikes-sharenting\n\n[14]\n\nMario Campana, Astrid Van den Bossche, and Bryoney Miller. 2020. #dadtribe: Performing sharenting labour to commercialise involved fatherhood. J. Macromarketing 40, 4 (Dec. 2020), 475–491. https://doi/pdf/10.1177/0276146720933334\n\n[15]\n\nElectronic Privacy Information Center. 2023. State Right to be Forgotten Policy. Retrieved December 1, 2023 from https://epic.org/state-right-to-be-forgotten-policy/.\n\n[16]\n\nCharlotte Chalklen and Heather Anderson. 2017. Mothering on Facebook: Exploring the privacy/openness paradox. Social Media + Society 3, 2 (April 2017), 1–10. https://doi.org/10.1177/2056305117707187\n\n[17]\n\nFederal Trade Commission. 2013. Children’s Online Privacy Protection Rule (“COPPA”). Retrieved August 23, 2023 from https://www.ftc.gov/legal-library/browse/rules/childrens-online-privacy-protection-rule-coppa.\n\n[18]\n\nHortense Cotrim, Cristina Granja, Ana Sofia Carvalho, Carlos Cotrim, and Rui Martins. 2021. Children’s understanding of informed assents in research studies. Healthcare (Basel) 9, 7 (July 2021), 871. http://dx.doi.org/10.3390/healthcare9070871\n\n[19]\n\nSean Coughlan. 2018. ’Sharenting’ puts young at risk of online fraud. BBC News. https://www.bbc.com/news/education-44153754\n\n[20]\n\nIan Dey. 1999. Grounding Grounded Theory: Guidelines for Qualitative Inquiry. Emerald Group Publishing Limited, Leeds, UK.\n\n[21]\n\nLindsay Dodgson and Michelle Mark. 2023. Who is Ruby Franke? The YouTuber mom had viewers concerned for years before her arrest on child-abuse claims. Insider. Retrieved September 1, 2023 from https://www.insider.com/who-is-ruby-franke-youtube-mom-arrested-child-abuse-claim-2023-8.\n\n[22]\n\nSheila Donovan. 2020. ‘sharenting’: The forgotten children of the GDPR. Peace Human Rights Governance 4, 1 (2020), 35–59. https://phrg.padovauniversitypress.it/2020/1/2\n\n[23]\n\nJared Duval, Ferran Altarriba Bertran, Siying Chen, Melissa Chu, Divya Subramonian, Austin Wang, Geoffrey Xiang, Sri Kurniawan, and Katherine Isbister. 2021. Chasing play on TikTok from populations with disabilities to inspire playful and inclusive technology design. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (CHI ’21) (Yokohama, Japan, May 2021). ACM, New York, NY, 1–15. https://doi.org/10.1145/3411764.3445303\n\n[24]\n\nDaniel Faltesek, Elizbeth Graalum, Bailey Breving, Elselucia Knudsen, Jessica Lucas, Sierra Young, and Felix Eduardo Varas Zambrano. 2023. TikTok as television. Social Media + Society 9, 3 (Jul 2023), 13 pages. https://doi.org/10.1177/20563051231194576\n\n[25]\n\nAdina Farrukh, Rebecca Sadwick, and John Villasenor. 2014. Youth internet safety: Risks, responses, and research recommendations. Center for Technology Innovation at Brookings. Retrieved February 19, 2023 from https://www.brookings.edu/wp-content/uploads/2016/06/Youth-Internet-Safety_v07.pdf.\n\n[26]\n\nBlair Feehan. 2023. 2023 Social Media Industry Benchmark Report. Rival IQ. Retrieved September 6, 2023 from https://www.rivaliq.com/blog/social-media-industry-benchmark-report/.\n\n[27]\n\nAlexa K Fox and Mariea Grubbs Hoy. 2019. Smart devices, smart decisions? Implications of parents’ sharenting for children’s online privacy: An investigation of mothers. J. Public Policy Mark. 38, 4 (Oct. 2019), 414–432. https://doi.org/10.1177/0743915619858\n\n[28]\n\nDiana Freed, Natalie N Bazarova, Sunny Consolvo, Eunice J Han, Patrick Gage Kelley, Kurt Thomas, and Dan Cosley. 2023. Understanding Digital-Safety Experiences of Youth in the U.S. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (CHI ’23)’ (Hamburg, Germany, April 2023). ACM, New York, NY, 1–15. https://doi.org/10.1145/3544548.3581128\n\n[29]\n\nGDPR. 2018. Art. 17 GDPR – Right to erasure (‘right to be forgotten’). Retrieved August 30, 2023 from https://gdpr-info.eu/art-17-gdpr/.\n\n[30]\n\nWerner Geyser. 2022. What Is TikTok? – Everything You Need to Know in 2023. Influencer Marketing Hub. Retrieved September 12, 2023 from https://influencermarketinghub.com/what-is-tiktok/.\n\n[31]\n\nAnupam Gupta. 2020. How TikTok’s unique mix of social and broadcast media techniques sets it apart. VentureBeat. Retrieved September 13, 2023 from https://venturebeat.com/business/how-tiktoks-unique-mix-of-social-and-broadcast-media-techniques-sets-it-apart/.\n\n[32]\n\nKeltie Haley. 2020. Sharenting and the (potential) right to be forgotten. Ind. LJ 95, 3 (2020), 1005. https://www.repository.law.indiana.edu/ilj/vol95/iss3/9\n\n[33]\n\nHanisha Harjani and Emily Silver. 2023. The Sunday Story: Permission to share. Up First. https://www.npr.org/2023/07/06/1186221489/the-sunday-story-permission-to-share\n\n[34]\n\nRakibul Hasan, Cristobal Cheyre, Yong-Yeol Ahn, Roberto Hoyle, and Apu Kapadia. 2022. The impact of viral posts on visibility and behavior: A longitudinal study of scientists on twitter. In Proceedings of The International AAAI Conference on Web and Social Media (ICWSM ’22) (Atlanta, Georgia, June 2022), Vol. 16. AAAI Press, Palo Alto, CA, 323–334. https://doi.org/10.1609/icwsm.v16i1.19295\n\n[35]\n\nRakibul Hasan, Yifang Li, Eman Hassan, Kelly Caine, David J. Crandall, Roberto Hoyle, and Apu Kapadia. 2019. Can privacy be satisfying? On improving viewer satisfaction for privacy-enhanced photos using aesthetic transforms. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (CHI ’19) (Glasgow, Scotland, UK, May 2019). ACM, New York, NY, 1–13. https://doi.org/10.1145/3290605.3300597\n\n[36]\n\nRakibul Hasan, Rebecca Weil, Rudolf Siegel, and Katharina Krombholz. 2023. A psychometric scale to measure individuals’ value of other people’s privacy (VOPP). In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (CHI ’23) (Hamburg, Germany, April 2023). ACM, New York, NY, 1–14. https://doi.org/10.1145/3544548.3581496\n\n[37]\n\nIrma M Hein, Martine C De Vries, Pieter W Troost, Gerben Meynen, Johannes B Van Goudoever, and Ramón J L Lindauer. 2015. Informed consent instead of assent is appropriate in children from the age of twelve: Policy implications of new findings on children’s competence to consent to clinical research. BMC Med. Ethics 16, 1 (Nov. 2015), 76. http://dx.doi.org/10.1186/s12910-015-0067-z\n\n[38]\n\nAlex Hern. 2022. How TikTok’s algorithm made it a success: ‘It pushes the boundaries’. The Guardian. https://www.theguardian.com/technology/2022/oct/23/tiktok-rise-algorithm-popularity\n\n[39]\n\nAlexis Hiniker, Sarita Y. Schoenebeck, and Julie A. Kientz. 2016. Not at the dinner table: parents’ and children’s perspectives on family technology rules. In Proceedings of the 19th ACM Conference on Computer-Supported Cooperative Work & Social Computing (CSCW ’16) (San Francisco, California, February 27–March 2, 2016). ACM, New York, NY, 1376–1389. https://doi.org/10.1145/2818048.2819940\n\n[40]\n\nRae Hodge. 2021. TikTok hits 3 billion downloads. CNET. Retrieved August 30 from https://www.cnet.com/tech/services-and-software/tiktok-hits-3-billion-downloads/.\n\n[41]\n\nDonell Holloway, Lelia Green, and Sonia Livingstone. 2013. Zero to eight: young children and their internet use. LSE, London: EU Kids Online. https://ro.ecu.edu.au/ecuworks2013/929\n\n[42]\n\nAndrew Huberman. 2022. The Science of Setting & Achieving Goals. Huberman Lab Podcast. Retrieved September 12, 2023 from https://hubermanlab.com/the-science-of-setting-and-achieving-goals/.\n\n[43]\n\nHuffPost. 2013. Angry Mom Uncovers ’Toddler Bashing’ Facebook Group That Makes Fun Of ’Ugly’ Babies. Retrieved June 29, 2023 from https://www.huffpost.com/entry/toddler-bashing-facebook-group-ugly-babies_n_4241706.\n\n[44]\n\nAndrea Hunter. 2016. Monetizing the mommy: mommy blogs and the audience commodity. Inf. Commun. Soc. 19, 9 (Sept. 2016), 1306–1320. https://doi.org/10.1080/1369118X.2016.1187642\n\n[45]\n\nShaheen Kanthawala, Kelley Cotter, Kali Foyle, and Julia R DeCook. 2022. It’s the methodology for me: A systematic review of early approaches to studying TikTok. In Proceedings of the 55th Hawaii International Conference on System Sciences (HICSS) (Maui, Hawaii, January 2022). HICSS, Maui, Hawaii, 1–17.\n\n[46]\n\nKieran Press-Reynolds. 2023. A new wave of parent influencers and mom TikTokers are choosing not to spotlight their children on the internet anymore. Yahoo. Retrieved June 15, 2023 from https://www.yahoo.com/now/wave-parent-influencers-mom-tiktokers-165102270.html.\n\n[47]\n\nKatie Kindelan. 2022. Parents remove videos of their kids from TikTok after ‘Wren Eleanor’ warning. ABC News. Retrieved August 30, 2023 from https://www.goodmorningamerica.com/family/story/wren-eleanor-tiktok-trend-sees-parents-removing-photos-87486106.\n\n[48]\n\nSharon Kirkey. 2017. Do you know where your child’s image is? Pedophiles sharing photos from parents’ social media accounts. National Post. https://nationalpost.com/news/canada/photos-shared-on-pedophile-sites-taken-from-parents-social-media-accounts\n\n[49]\n\nLara Kobilke. 2022. All those glamazons we subscribe to. Mapping a network of key influencers spreading the art of drag on YouTube. In Drag in the Global Digital Public Sphere: Queer Visibility, Online Discourse and Political Change, Niall Brennan and David Gudelunas (Eds.). Taylor & Francis Group, London, UK.\n\n[50]\n\nBrian Krebs. 2018. Don’t Give Away Historic Details About Yourself. Krebs On Security. Retrieved August 23, 2023 from https://krebsonsecurity.com/2018/04/dont-give-away-historic-details-about-yourself/.\n\n[51]\n\nUdo Kuckartz. 2014. Qualitative Text Analysis: A Guide to Methods, Practice and Using Software. SAGE, New York, NY.\n\n[52]\n\nPriya Kumar and Sarita Schoenebeck. 2015. The modern day baby book: Enacting good mothering and stewarding privacy on facebook. In Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work & Social Computing (CSCW ’15)’ (Vancouver, BC, Canada, March 2015). ACM, New York, NY, 1302–1312. https://doi.org/10.1145/2675133.2675149\n\n[53]\n\nJoseph Lamour. 2023. Mom of 12 responds to detractors after serving her kids nachos in a kiddie pool. Today. Retrieved August 3, 2023 from https://www.today.com/food/trends/mom-feeds-kids-nachos-in-baby-pool-rcna83950.\n\n[54]\n\nFortesa Latifi. 2023. Influencer Parents and The Kids Who Had Their Childhood Made Into Content. Teen Vogue. Retrieved May 12, 2023 from https://www.teenvogue.com/story/influencer-parents-children-social-media-impact.\n\n[55]\n\nFortesa Latifi. 2023. ‘Medical moms’ share their kids’ illnesses with millions. At what cost?The Washington Post. https://www.washingtonpost.com/parenting/2023/05/11/tiktok-medical-mamas/\n\n[56]\n\nFortesa Latifi. 2023. Parenting influencers try something new: Giving their kids privacy. The Washington Post. https://www.washingtonpost.com/parenting/2023/08/01/parenting-influencers-children-privacy/\n\n[57]\n\nEffie Le Moignan, Shaun Lawson, Duncan A Rowland, Jamie Mahoney, and Pam Briggs. 2017. Has Instagram fundamentally altered the ’family snapshot’?. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (CHI ’17) (Denver, Colorado, May 2017). ACM, New York, NY, 4935–4947. https://doi.org/10.1145/3025453.3025928\n\n[58]\n\nSonia Livingstone. 2013. Online risk, harm and vulnerability: reflections on the evidence base for child Internet safety policy. ZER: Journal of Communication Studies 18, 35 (2013), 13–28.\n\n[59]\n\nMuge Marasli, Er Suhendan, Nergis Hazal Yilmazturk, and Figen Cok. 2016. Parents’ shares on social networking sites about their children: Sharenting. Anthropologist 24, 2 (2016), 399–406. https://doi.org/10.1080/09720073.2016.11892031\n\n[60]\n\nNora McDonald, Sarita Schoenebeck, and Andrea Forte. 2019. Reliability and inter-rater reliability in qualitative research: Norms and guidelines for CSCW and HCI practice. Proc. ACM Hum.-Comput. Interact. 3, CSCW (Nov. 2019), 1–23. https://doi.org/10.1145/3359174\n\n[61]\n\nNora McInerny. 2022. Why I Stopped Posting Photos of My Kid on Social Media. Time. https://time.com/6211473/stopped-posting-photos-kids-social-media/\n\n[62]\n\nJuan Carlos Medina Serrano, Orestis Papakyriakopoulos, and Simon Hegelich. 2020. Dancing to the partisan beat: A first analysis of political communication on TikTok. In Proceedings of the 12th ACM Conference on Web Science (Southampton, UK, July 2020). ACM, New York, NY, 257–266. https://doi.org/10.1145/3394231.3397916\n\n[63]\n\nKatie Mettler. 2019. This ‘YouTube Mom’ was accused of torturing the show’s stars — her own kids. She died before standing trial. The Washington Post. https://www.washingtonpost.com/crime-law/2019/11/13/popular-youtube-mom-who-was-charged-with-child-abuse-has-died/\n\n[64]\n\nVictoria A Miller, Dennis Drotar, and Eric Kodish. 2004. Children’s competence for assent and consent: a review of empirical findings. Ethics Behav. 14, 3 (2004), 255–295. http://dx.doi.org/10.1207/s15327019eb1403_3\n\n[65]\n\nTehila Minkus, Kelvin Liu, and Keith W Ross. 2015. Children seen but not heard: When parents compromise children’s online privacy. In Proceedings of the 24th International Conference on World Wide Web (WWW ’15)’ (Florence, Italy, May 2015). WWW ’15, Republic and Canton of Geneva, Switzerland, 776–786. https://doi.org/10.1145/2736277.2741124\n\n[66]\n\n[66] momfluence. 2023. Retrieved August 29, 2023 from https://www.momfluence.co/.\n\n[67]\n\nMeredith Ringel Morris. 2014. Social networking site use by mothers of young children. In Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work & Social Computing (CSCW ’14) (Baltimore, Maryland, February 2014). ACM, New York, NY, 1272–1282. https://doi.org/10.1145/2531602.2531603\n\n[68]\n\nCarol Moser, Tianying Chen, and Sarita Y Schoenebeck. 2017. Parents’ and children’s preferences about parents sharing about children on social media. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (CHI ’17) (Denver, Colorado, May 2017). ACM, New York, NY, 5221–5225. https://doi.org/10.1145/3025453.3025587\n\n[69]\n\nUnited Nations. 1989. Convention on the Rights of the Child. Retrieved August 30, 2023 from https://www.ohchr.org/en/instruments-mechanisms/instruments/convention-rights-child.\n\n[70]\n\nHeather Ng Osborn. 2008. Targeting bad behavior: Why federal regulators must treat online behavioral marketing as spyware. Hastings Comm. & Ent. LJ 31 (2008), 369. https://heinonline.org/HOL/LandingPage?handle=hein.journals/hascom31&div=21\n\n[71]\n\nHelen Nissenbaum. 2004. Privacy as contextual integrity. Wash Law Rev. 79 (2004), 119.\n\n[72]\n\nHelen Nissenbaum. 2009. Privacy in Context. Stanford University Press, Redwood City, CA.\n\n[73]\n\nUS Department of Health and Human Services. 2023. Research with Children FAQs. Retrieved August 29, 2023 from https://www.hhs.gov/ohrp/regulations-and-policy/guidance/faq/children-research/.\n\n[74]\n\nKatherine O’Toole. 2023. Collaborative creativity in TikTok music duets. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (CHI ’23) (Hamburg, Germany, April 2023). ACM, New York, NY, 1–16. https://doi.org/10.1145/3544548.3581380\n\n[75]\n\nGaëlle Ouvrein and Karen Verswijvel. 2019. Sharenting: Parental adoration or public humiliation? A focus group study on adolescents’ experiences with sharenting against the background of their own impression management. Child. Youth Serv. Rev. 99 (April 2019), 319–327. https://doi.org/10.1016/jchildyouth.2019.02.011\n\n[76]\n\nOrestis Papakyriakopoulos, Christelle Tessono, Arvind Narayanan, and Mihir Kshirsagar. 2022. How algorithms shape the distribution of political advertising: Case studies of Facebook, Google, and TikTok. In Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society (Oxford, UK, May 2022). ACM, New York, NY, 532–546. https://doi.org/10.1145/3514094.3534166\n\n[77]\n\nSarah Perez. 2023. TikTok CEO says company scans public videos to determine users’ ages. TechCrunch. https://techcrunch.com/2023/03/23/tiktok-ceo-says-company-scans-public-videos-to-determine-users-ages/\n\n[78]\n\nSabid Bin Habib Pias, Imtiaz Ahmad, Taslima Akter, Apu Kapadia, and Adam J. Lee. 2022. Decaying photos for enhanced privacy: User perceptions towards temporal redactions and ’trusted’ platforms. Proc. ACM Hum.-Comput. Interact. 6, CSCW2, Article 437 (November 2022), 30 pages. https://doi.org/10.1145/3555538\n\n[79]\n\nElaine Pofeldt. 2023. This Father Of Four Turned To YouTube And TikTok When He Needed To Make A Living. Now His Family Has 2.2 Million Followers. Forbes Magazine. https://www.forbes.com/sites/elainepofeldt/2023/02/27/this-father-of-four-turned-to-youtube-and-tiktok-when-he-needed-to-make-a-living-now-his-family-has-22-million-followers/?sh=910d92a557cb\n\n[80]\n\nYasmeen Rashidi, Apu Kapadia, Christena Nippert-Eng, and Norman Makoto Su. 2020. “It’s easier than causing confrontation”: Sanctioning strategies to maintain social norms and privacy on social media. Proc. ACM Hum.-Comput. Interact. 4, CSCW1 (May 2020), 1–25. https://doi.org/10.1145/3392827\n\n[81]\n\nRep. Kristine Reeves, Rep. Tana Senn, Rep. Liz Berry, Rep. Amy Walen, Rep. Sharon Wylie, and Rep. Shelley Kloba. 2023. House Bill 1627: Protecting the interests of minor children featured on for-profit family vlogs. https://app.leg.wa.gov/billsummary?BillNumber=1627&Year=2023&Initiative=false.\n\n[82]\n\nK. Andrew R. Richards and Michael A. Hemphill. 2018. A practical guide to collaborative qualitative data analysis. J. Teach. Phys. Educ. 37, 2 (2018), 225–231.\n\n[83]\n\nDave Roos. 2023. The Evil Eye’s Ancient Power Still Resonates Today. HowStuffWorks. Retrieved August 16, 2023 from https://people.howstuffworks.com/evil-eye-meaning.htm.\n\n[84]\n\nEmma Roth. 2023. Online age verification is coming, and privacy is on the chopping block. The Verge. Retrieved September 6, 2023 from https://www.theverge.com/23721306/online-age-verification-privacy-laws-child-safety.\n\n[85]\n\nAnastasia Schaadhardt, Yue Fu, Cory Gennari Pratt, and Wanda Pratt. 2023. “Laughing so I don’t cry”: How TikTok users employ humor and compassion to connect around psychiatric hospitalization. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (CHI ’23) (Hamburg, Germany, April 2023). ACM, New York, NY, 1–13. https://doi.org/10.1145/3544548.3581559\n\n[86]\n\nJoseph S Schafer, Annie Denton, Chloe Seelhoff, Jordyn Vo, and Kate Starbird. 2023. Viral privacy: Contextual integrity as a lens to understand content creators’ privacy perceptions and needs after sudden attention. In DIS ’23 Workshop on Multi-Stakeholder Privacy and Safety on Content Creation Platforms (Pittsburgh, PA, July 2023). ACM, New York, NY, 1–4.\n\n[87]\n\nMorgan Klaus Scheuerman, Katta Spiel, Oliver L Haimson, Foad Hamidi, and Stacy M Branham. 2020. HCI Gender Guidelines. Morgan Klaus Scheuerman. Retrieved August 28, 2023 from https://www.morgan-klaus.com/gender-guidelines.html.\n\n[88]\n\nSen. David Koehler. 2023. Senate Bill 1782: Child Labor - Online Content. https://www.ilga.gov/legislation/BillStatus.asp?DocNum=1782&GAID=17&DocTypeID=SB&LegId=146603&SessionID=112&GA=103\n\n[89]\n\nLanyu Shang, Ziyi Kou, Yang Zhang, and Dong Wang. 2021. A multimodal misinformation detector for COVID-19 short videos on TikTok. In 2021 IEEE International Conference on Big Data (Big Data) (Orlando, FL, December 2021). IEEE, New York, NY, 899–908. https://doi.org/10.1109/BigData52589.2021.9671928\n\n[90]\n\nElizaveta Sivak and Ivan Smirnov. 2019. Parents mention sons more often than daughters on social media. Proc. Natl. Acad. Sci. U. S. A. 116, 6 (Feb. 2019), 2039–2041. http://dx.doi.org/10.1073/pnas.1804996116\n\n[91]\n\nStacey Steinberg. 2022. How parents can share smarter on social media. YouTube, uploaded by TEDx Talks. Retrieved June 27, 2023 from https://www.youtube.com/watch?v=uNbonhBgW_Q.\n\n[92]\n\nStacey B Steinberg. 2016. Sharenting: Children’s privacy in the age of social media. Emory LJ 66 (2016), 839. https://scholarlycommons.law.emory.edu/elj/vol66/iss4/2\n\n[93]\n\nMorgan Sung. 2023. How one teen is urging legislators in Washington state to help protect kids from being exploited on vlogs. NBC News. Retrieved May 12, 2023 from https://www.nbcnews.com/tech/social-media/child-influencers-exploitation-bill-hearing-washington-state-hb1627-rcna70479.\n\n[94]\n\nTikTok. 2023. Community Guidelines. Retrieved August 30, 2023 from https://www.tiktok.com/community-guidelines/en/.\n\n[95]\n\nAustin L Toombs, Kellie Morrissey, Emma Simpson, Colin M Gray, John Vines, and Madeline Balaam. 2018. Supporting the complex social lives of new parents. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (CHI ’18) (Montreal, QC, Canada, April 2018). ACM, New York, NY, 1–13. https://doi.org/10.1145/3173574.3173994\n\n[96]\n\nKaren Verswijvel, Michel Walrave, Kris Hardies, and Wannes Heirman. 2019. Sharenting, is it a good or a bad thing? Understanding how adolescents think and feel about sharenting on social network sites. Child. Youth Serv. Rev. 104 (Sept. 2019), 319–327. https://doi.org/10.1016/j.childyouth.2019.104401\n\n[97]\n\nAri Wasserman. 2023. Who is ‘Baby Gronk’? The father of the social media sensation shares his plan. The Athletic. Retrieved June 8, 2023 from https://theathletic.com/4592818/2023/06/08/baby-gronk-father-college-football-qa/.\n\n[98]\n\nMiranda Wei, Eric Zeng, Tadayoshi Kohno, and Franziska Roesner. 2022. Anti-privacy and anti-security advice on TikTok: Case studies of technology-enabled surveillance and control in intimate partner and parent-child relationships. In Eighteenth Symposium on Usable Privacy and Security (SOUPS 2022) (Boston, MA, August 2022). USENIX Association, Berkeley, CA, 447–462. https://www.usenix.org/conference/soups2022/presentation/wei\n\n[99]\n\nGalen Weld, Amy X. Zhang, and Tim Althoff. 2022. What makes online communities ‘better’? Measuring values, consensus, and conflict across thousands of subreddits. Proceedings of the International AAAI Conference on Web and Social Media 16, 1 (2022), 1121–1132. https://doi.org/10.1609/icwsm.v16i1.19363\n\n[100]\n\nAlex J Xu, Jacob Taylor, Tian Gao, Rada Mihalcea, Veronica Perez-Rosas, and Stacy Loeb. 2021. TikTok and prostate cancer: Misinformation and quality of information using validated questionnaires. BJU Int. 128, 4 (Oct. 2021), 435–437. http://dx.doi.org/10.1111/bju.15403\n\n[101]\n\nMichael Zimmer. 2010. “But the data is already public”: On the ethics of research in Facebook. Ethics Inf. Technol. 12, 4 (Dec. 2010), 313–325. https://doi.org/10.1007/s10676-010-9227-5\n\nIndex Terms\n\nSharenting on TikTok: Exploring Parental Sharing Behaviors and the Discourse Around Children’s Online Privacy\n\nHuman-centered computing\n\nCollaborative and social computing\n\nCollaborative and social computing theory, concepts and paradigms\n\nSocial media\n\nSecurity and privacy\n\nHuman and societal aspects of security and privacy\n\nSocial aspects of security and privacy\n\nSocial and professional topics\n\nUser characteristics\n\nAge\n\nChildren\n\nRecommendations\n\nSharenting and Children's Privacy in the United States: Parenting Style, Practices, and Perspectives on Sharing Young Children's Photos on Social Media\n\nCSCW1\n\nParents posting photos and other information about children on social media is increasingly common and a recent source of controversy. We investigated characteristics that predict parental sharing behavior by collecting information from "
    }
}
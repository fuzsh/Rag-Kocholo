{
    "id": "dbpedia_1503_0",
    "rank": 41,
    "data": {
        "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3998690/",
        "read_more_link": "",
        "language": "en",
        "title": "Reproducibility and Prognosis of Quantitative Features Extracted from CT Images",
        "top_image": "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/pmc-card-share.jpg?_=0",
        "meta_img": "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/pmc-card-share.jpg?_=0",
        "images": [
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/favicons/favicon-57.png",
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/icon-dot-gov.svg",
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/icon-https.svg",
            "https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/logos/AgencyLogo.svg",
            "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/logo-transoncol.gif",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3998690/bin/tlo0701_0072_fig001.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3998690/bin/tlo0701_0072_fig002.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3998690/bin/tlo0701_0072_fig003.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3998690/bin/tlo0701_0072_fig004.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3998690/bin/tlo0701_0072_fig005a.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3998690/bin/tlo0701_0072_fig005b.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3998690/bin/tlo0701_0072_fig006.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3998690/bin/tlo0701_0072_fig007a.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3998690/bin/tlo0701_0072_fig007b.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3998690/bin/tlo0701_0072_fig008.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3998690/bin/tlo0701_0072_fig009.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Yoganand Balagurunathan",
            "Yuhua Gu",
            "Hua Wang",
            "Virendra Kumar",
            "Olya Grove",
            "Sam Hawkins",
            "Jongphil Kim",
            "Dmitry B Goldgof",
            "Lawrence O Hall",
            "Robert A Gatenby"
        ],
        "publish_date": "2014-02-11T00:00:00",
        "summary": "",
        "meta_description": "We study the reproducibility of quantitative imaging features that are used to describe tumor shape, size, and texture from computed tomography (CT) scans of non-small cell lung cancer (NSCLC). CT images are dependent on various scanning factors. We focus ...",
        "meta_lang": "en",
        "meta_favicon": "https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon.ico",
        "meta_site_name": "PubMed Central (PMC)",
        "canonical_link": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3998690/",
        "text": "Segmentation of Tumors\n\nWe used Definiens Developer XD© (Munich, Germany) as the image analysis platform to perform tumor segmentation and feature extraction. Definiens is based on the Cognition Network Technology [14,15] that allows the development and execution of image analysis applications. Here, the Lung Tumor Analysis application was used [16]. Lung Tumor Analysis is a semiautomated three-dimensional “Click&Grow” approach for segmentation of tumors under the guidance of an operator. To perform the seed-based segmentation of a target lesion, the latter has to be completely within a lung-image object. In cases where a medical expert concluded that the automated preprocessing described above failed to accurately identify the border between a target lesion and the pleural wall, it was necessary to enable correction of the automated lung segmentation.\n\nThe manual segmentation process required many human interactions to get the “correct” segmentation boundaries. In our study, we used a trained radiologist to assist in the manual segmentation process. Consequently, we developed an automatic single-click ensemble segmentation (SCES) algorithm [11]. In brief, the SCES algorithm uses the initial seed point to automatically generate multiple seed points with region growing. It makes use of the “Click&Grow” algorithm by using a manually selected initial seed point to define a small circumscribed area within the tumor boundary, within which multiple seed points are automatically generated. An ensemble segmentation is obtained from the multiple regions that were grown from these multiple seed points. In this algorithm, an ensemble segmentation refers to a set of different input segmentations (multiple runs using the same segmentation technique but different initializations) that are combined to generate a “consensus” segmentation.\n\nOnce the segmentation of all target lesions was deemed sufficiently accurate, statistics for each lesion, such as volume, center of gravity, and average density, all readily available as object features within the commercial cognitive network language, were extracted. shows a comparison of the segmentation masks between segmentations and repeats.\n\nIn total, 64 lesions were segmented, i.e., 2 per patient, and quantitative values of image features were extracted from each segmented volume. shows a Bland-Altman plot of conventional size measurements (long diameter, longest diameter*short axis, and volume), estimated after manual and ensemble segmentations ( ). The volume distribution showed a diverse population with relatively high variability in midsized to smaller sized tumors. Half the samples had small volume ≤ 4 cm3 tumors, whereas the rest of them are larger (the largest group close to 120 cm3). shows an example of a patient tumor delineated with manual segmentation for test/retest scans.\n\nTable 1\n\nFeature NameManual CCCEnsemble CCCManual and Ensemble CCCUnivariate: (RECIST)Longest axis (cm)0.9963 (0.967, 0.9998)0.9920 (0.9509, 0.9997)0.9943 (0.9523, 0.9997)Short axis (cm)0.9951 (0.9047, 0.9952)0.9868 (0.9078, 0.9995)0.9878 (0.9084, 0.9995)Bivariate: (WHO)(Longest axis *Short axis)0.9855 (0.9384, 0.9997)0.9757 (0.8342, 0.9988)0.9807 (0.8848, 0.9992)Volumetric:Volume (cm3)0.9934 (0.9571, 0.9997)0.9913 (0.9552, 0.9997)0.9924 (0.9574, 0.9998)\n\nImage features. We extracted several types of image features, both in 2D and 3D with most of our analysis directed on using 3D features. In this work, we present 3D features that are broadly divided into the following two classes: nontexture and texture features. Each of these classes can be subdivided into several categories on the basis of their functional description, which also facilitate analysis and presentation. Specifically, nontexture class includes tumor size, tumor shape, and tumor location categories, and texture class includes pixel histogram, run length, co-occurrence, Laws, and wavelet feature categories. We note that texture features have been shown to be good descriptors of the tumor and some have shown to be useful in survival prediction [17]. In this study, we have used 219 three-dimensional and 111 two-dimensional image features. Most of these were implemented within the Definiens platform [18], whereas some were computed by implementing the algorithms in C/C++ (former Bell Labs USA) and MATLAB (Mathworks, Natick, MA). All the features were obtained from the ROI (i.e., after the segmentation). The 2D features are expected to have lower variability in measurement due to limited span of the ROI; in this repeated experiment, matching slices between test/retest has been a challenge, which adds to measurement noise. In this study, our focus has been geared toward 3D features, which provide better description of the tumor volume region.\n\nTexture descriptors provide measure of properties such as smoothness, coarseness, and regularity, as no standard description exists. Typically, the following three principal approaches are used to describe texture: statistical, structural, and spectral [18]. Our features cover all the categories and use most popular approaches for texture computation. A brief description of each feature category is provided, and additional information is provided in the Supplemental section.\n\nNontexture features. Tumor size, shape, and location descriptors make up our nontexture features.\n\nTumor size category contains features that can be broadly categorized as univariate (longest diameter, short axis, width, and other size measurements), bivariate (area, length by width, length by thickness, and other size measurements), and volume measurements both in pixel units and in native resolution (centimeters).\n\nTumor shape category feature measures circularity of the tumor in various forms: compactness, largest elliptical fit in the tumor region, asymmetry, density, and compactness. Asymmetry is a measure of variance from round shape (disproportional length). It is computed as a ratio of smallest and largest Eigen values of the tumor. Density describes spatial distribution of the pixels with respect to cubical object. The density is higher when the volume of the object follows a cubical shape (lower when it is like a filament). Compactness measures the cuboid occupied by the object computed as a ratio of the first three Eigen values to the number of voxels in the tumor. The “MacSpic” feature measures the number of countable spiculations in the tumor.\n\nTumor location category feature measures tumor position with respect to anatomic structure of the lung. The Attached to Pleural is a binary flag that tells if the tumor (in 3D) is attached to lung wall. The Main direction feature is a measure of the angle between the best-fit line on the centers of gravity for each 2D slice to the z-axis.\n\nTexture features. In CT images, texture is typically attributed to gray-level changes seen by a radiologist. In other types of imaging, in addition to gray-level changes, texture is well characterized in transformed domain (kernel based or functional mapping). These features have been shown to be useful in medicine [19]. We categorized histogram, run length, co-occurrences, Laws kernel, and wavelet-based features as textures.\n\nPixel intensity histogram features are computed on the pixel intensity (in Hounsfield units or HU) for the region (voxel) of interest. First and higher-order statistics, entropy, and energy on the tumor volumes are reported as features.\n\nRun-length and co-occurrence features may find some correlation to radiologist-visualized texture. The run length is defined as a measure of contiguous gray levels along a specific orientation. Fine textures tend to have short run length, whereas coarser texture will have longer run lengths with similar gray level. These features capture coarseness in 3D-image structure and have been found useful in a number of texture analyses [20,21]. If R (k,p) is the run-length matrix n1 by n2, at gray-level k, then the number of such lengths equals p, along an orientation, in the volume (x,y,z). One useful measure of run length in this study has been the measure of nonuniformity (RunLGLN) that measures extent of smoothness or similarity in the image.\n\nRunLGLN=(1n∑k=1n1(∑p=1n2R(k,p)2)\n\nWe compute 11 different run-length metrics, each of which has a property to capture gray-level variations in the tumor. The co-occurrence matrix contains the frequency of one gray-level intensity appearing in a specified spatial relationship with another gray-level intensity in a given range. The co-occurrence matrix is first constructed, and different formulations of the values are then calculated where measurements include contrast, energy, homogeneity, entropy, mean, and maximum probability [22].\n\nLaws features are constructed from a set of five one-dimensional kernels each designed to reflect a different type of structure in the image [23]. The kernel has the ability to enhance certain regions of the image. About 125 features are computed on different sets of kernel and orientation.\n\nWavelet features are kernel-based functions that decompose the image (3D) into orthogonal components. We used Daubechies (Coiflet) wavelets in this study [24,25]. Statistics on the decomposition have been widely used in image texture identification. In this study, we calculate two metrics (energy and entropy) along each direction of the 3D-image volume with two levels of decomposition yielding 30 features.\n\nTwo-dimensional features. Traditionally, image processing has been carried out on 2D gray-scale images; however, most proposed features could be extended to 3D. The advantage of 2D features includes easier interpretability and visualization. In this study, the tumors were delineated in 3D using the segmentation methods described in the previous section. Slices (2D) in the ROI were matched between test/retest scans by a trained radiologist. The identification criteria were based on the anatomic structure of the lung. On a 2D slice, we extracted 110 2D features that describe the shape, size, and texture of the lesion. Additionally, 2D measurements are an estimate of the true tumor size that is dependent on segmentation methods (see Supporting Analysis section, Tables O1–O3). The slice matching between test and retest experiments adds additional variations that may not be uniform across the features, making it difficult to discern test/retest variability.\n\nRepeatable and representative features. Finding features that are consistent in repeated experiments is a prerequisite step, which is followed by a redundancy reduction step to obtain an informative set. We tested the consistency between the test and retest experiments. For each image feature, the CCCTreT was computed to quantify reproducibility between two scans performed on each patient. The CCCTreT measures deviation from the 45° line, which is appropriate for repeated experiments and shown to be superior to the Pearson correlation coefficient [26]. On this set of highly reproducible features, the next step was to select the features with a large interpatient variability, using the “dynamic range” metric. The normalized DR for a feature was defined as the inverse of the average difference between measurements to the observed biologic (interpatient) range:\n\nDR=(1−1n∑i=1n|Test(i)−Retest(i)|Max−Min)\n\nwhere i refers to an individual sample from the n patient cases; the maximum and minimum are computed on the entire sample set. The DR runs from 0 to 1. Values close to 1 are preferred and imply that the feature has a large biologic range relative to reproducibility. Increasing variation between the test-retest repeats will lead to a reduction in the DR value. Screening for a large DR will eliminate features that show greater variability in the repeat scans compared to the range of the coverage. The last step is to eliminate redundancies, on the basis of the calculation of dependencies within the group. We computed the R2Bet between the remaining features to quantify the dependency. The R2 has a range of 0 to 1 and is a ratio of known variance measured by linear model to total variance between two variables, where one is the outcome and the other is used to form the predictor. Values close to 1 would mean that the data points are close to the fitted line (i.e., closer to dependency) [24,25]. The R2 of simple regression is equal to the square of the Pearson correlation coefficient [27,28]. The features were grouped on the basis of the R2Bet between them; in this subset, one representative that had the highest DR was picked. The procedure was repeated recursively to cover all the features, resulting in a most representative group. This was carried out in two ways: done independently for each category and across categories.\n\nWe implemented different cutoff values for R2Bet to consider the feature as linearly dependent with any other features in the list. Because the purpose of this third filter is to eliminate redundancies (and not necessarily identify independence), features with R2Bet values in the range ≥0.75 to ≥0.99 were found. This was repeated for different segmentation methods (manual, ensemble, and common between).\n\nConcordance in Repeated Experiment\n\nThe 219 extracted features (3D and 110 2D) were first compared in a test/retest experiment using the CCCTreT, which is a stringent measure of reproducibility. A CCCTreT value ≥ 0.75 indicates that the data are of acceptable reproducibility. At a second level of analysis, the DR was computed as described in Materials and Methods section, and this metric will identify those features with the largest biologic range relative to their reproducibility. For our data set, we examined various cutoffs and, with stringent limits, there were 72 (manual, M) and 106 (ensemble, E) 3D features that had a CCCTreT ≥ 0.90 and DR ≥ 0.90. Of these, 48 (∼22%) of the features were common between segmentations (in total of 219 features; see ). In the 2D set, 51 (manual, M) and 28 (ensemble, E) features had a CCCTreT ≥ 0.90 and DR ≥ 0.90. Of these, 17 (15.3%) of the features were common between the segmentations. Details are reported in Supporting Analysis section, Tables O2 and O3. It was interesting to note that some of the texture features (wavelet and Laws kernel) were not repeated in the ensemble segmentation. This was attributed to feature outliers due to segmentation boundary differences, which resulted in adding different regions in test/retest. When the outlier samples were removed (in the samples with wavelet layer 1 energy > 20%), there was an 8% increase in the number of ensemble features (see ).\n\nTable 3\n\nFeature CategoryNo. of Features: CCCTreT and DR ≥ 0.90Manual SegmentationEnsemble SegmentationCommon (Manual and Ensemble)(A) CCCTreT and DR: 3D featuresC1: Tumor size12 (92.31%)11 (84.62%)11 (84.62%)C2: Tumor shape6 (50%)4 (33.33%)4 (33.33%)C3: Tumor location11 (78.57%)8 (57.14%)8 (57.14%)C4: Histogram6 (75%)3 (37.5%)3 (37.5%)C5: Run length and co-occurrence6 (35.29%)6 (35.29%)6 (35.29%)C6: Laws16 (12.8%)74 (59.2%)16 (12.8%)C7: Wavelets15 (50%)0 (0%)0 (0%)All Categories72 (32.88%)106 (48.4%)48 (21.92%)(B) CCCTreT and DR: Filtered 3D featuresC1: Tumor size12 (92.31%)12 (92.31%)8 (61.54%)C2: Tumor shape6 (50%)8 (66.67%)7 (58.33%)C3: Tumor location11 (78.57%)6 (42.86%)5 (35.71%)C4: Histogram3 (37.5%)1 (12.5%)1 (12.5%)C5: Run length and co-occurrence4 (23.53%)5 (29.41%)1 (5.88%)C6: Laws13 (10.4%)68 (54.4%)8 (6.4%)C7: Wavelets19 (63.33%)23 (76.67%)19 (63.33%)All Categories68 (31.05%)123 (56.16%)49 (22.37%)\n\nThese concordance and DR filtering procedures will result in obtaining a set of features that are reproducible with a large range compared to the variability between the test and retest experiments. However, these resulting features may have interdependencies. Therefore, we used the R2Bet between the features to quantify the levels of similarity. In this approach, if a feature of interest is linearly predicted by any other feature in the filtered feature set, the two were grouped together, repeated to cover all pairs. In the group of interdependent features, the one having the largest DR was chosen as the representative feature for the group, and the rest were removed. The procedure was repeated to cover the entire subset to form the reduced set. The cutoff level to reduce features based on linear dependency is critical and is subject to change with the sample size and the tumor shape and texture. This redundancy reduction can be carried out categorywise or by combining the categories; both were attempted. Finding reproducible features categorywise with redundancy reduction will help us form an informative feature set that will translate to a similar range of tumors. We performed the filtering at a few different levels: at R2Bet ≥ 0.95, there were 29 common features between segmentation methods for CCCTreT and DR ≥ 0.90. After removing the category bounds, there were 23 common features.\n\nshows the ordered distribution plot for the DRs along with the distribution of concordance coefficients. The features' concordance and DR criteria were computed for manual and automatic segmentation independently, and the common features were obtained later. We tested interdependency by computing the R2Bet between the image features in each of the filtered sets and followed the described procedure to find a representative feature for a group of highly dependent features (see Discussion section).\n\nshows feature details, whereas Table W3 provides features with categorywise reduction. When category boundaries were removed, more feature reduction was observed. , A and C, and Table W5 show feature details without category restrictions. shows a clusterogram heat map of representative features that are common between segmentations. The feature value was averaged in the test/retest experiment and across segmentations (manual and ensemble). The concordance and DR was set at: CCCTreT and DR ≥ 0.90. The representative features were outlined for both types of segmentations with feature reduction cutoff of R2Bet ≥ 0.95.\n\nTable 4\n\n(A) No. of Representative Features (CCCTreT and DR ≥ 0.90; Combine R2Bet ≥ 0.95)CategoryRedundancy Reduction CategorywiseRedundancy Reduction across All CategoriesAll SamplesAfter Outlier RemovalAll SamplesAfter Outlier RemovalC1: Tumor size9879C2: Tumor shape3523C3: Tumor location6565C4: Histogram3121C5: Gray scale4121C6: Laws4646C7: Wavelets0505Total29312330\n\n(B) (i) Representative Feature (CCCTreT ≥ 0.90 and DR ≥ 0.90) Obtained at R2Bet ≥ 0.95 (All Samples)(F No.: Suffix No. represents feature index in the total list of 219.)Category C1: Representative features (9):F1:LongDia; F2:ShortAx-LongDia; F3:ShortAx; F6:Vol-cm; F33:Area-Pxl; F36:Width-Pxl; F37:Thickness-Pxl; F38:Length-Pxl; and F41:Border-Leng-PxlCategory C2: Representative features (3):F14:9c-3D-Compact; F25:Density; and F30:Shape-IndexCategory C3: Representative features (6):F8:8a-3D-Attch-Pleural; F9:8b-3D-Bord-to-Lung; F15:9d-3D-AV-Dist-COG-to-Border; F16:9e-3D-SD-Dist-COG-to-Border; F17:9f-3D-Min-Dist-COG-to-Border; and F19:10a-3D-Relat-Vol-AirspacesCategory C4: Representative features (3):F4:Mn-Hu; F186:Hist-Energy-L1; and F187:Hist-Entropy-L1Category C5: Representative features (4):F44:AvgCooC-Constrast; F48:AvgGLN; F51:AvgLRE; and F54:AvgRLNCategory C6: Representative features (4):F67:3D-Laws-9; F74:3D-Laws-16; F103:3D-Laws-45; and F128:3D-Laws-79Category C7: None\n\n(B) (ii) Removing OutliersRepresentative feature (CCCTreT ≥ 0.90 and DR ≥ 0.90) obtained at R2Bet ≥ 0.95 (prefix represents feature index in the total list of 219).Category C1: Representative features (8):F1:LongDia; F2:ShortAx-LongDia; F3:ShortAx; F6:Vol-cm; F34:Volume-pxl; F36:Width-Pxl; F37:Thickness-Pxl; and F38:Length-PxlCategory C2: Representative features (5):F13:9b-3D-Circularity; F14:9c-3D-Compact; F25:Density; F30:Shape-Index; and F32:RectangularFitCategory C3: Representative features (5):F9:8b-3D-Bord-to-Lung; F12:9a-3D-FractionalAnisotropy; F16:9e-3D-SD-Dist-COG-to-Border; F17:9f-3D-Min-Dist-COG-to-Border; and F18:9g-3D-Max-Dist-COG-to-BorderCategory C4: Representative features (1):F186:Hist-Energy-L1Category C5: Representative features (1):F48:AvgGLN; F51Category C6: Representative features (6):F62:3D-Laws-4; F68:3D-Laws-10; F69:3D-Laws-11; F72:3D-Laws-14; F143:3D-Laws-94; and F182:3D-Laws-133Category C7: Representative features (5):F197:3D-WaveP2-L2-8; F206:3D-WaveP1-L2-17; F208:3D-WaveP1-L2-19; F211:3D-WaveP1- L2-22; and F216:3D-WaveP1-L2-27\n\n(C) Common between Manual and Ensemble: Across Categories (All Samples)Concordance cutoff: CCCTreT ≥ 0.90 and DR ≥ 0.90Representative features (23): Combine features with R2Bet ≥ 0.95C1: Tumor size:F1:LongDia; F2:ShortAx-LongDia; F3:ShortAx; F6:Vol-cm; F33:Area-Pxl; F37:Thickness-Pxl; and F38:Length-PxlC2: Tumor shape:F25:Density and F30:Shape-IndexC3: Location:F8:8a-3D-Attch-Pleural; F9:8b-3D-Bord-to-Lung; F15:9d-3D-AV-Dist-COG-to-Border; F16:9e-3D-SD-Dist-COG-to-Border; F17:9f-3D-Min-Dist-COG-to-Border; and F19:10a-3D-Relat-Vol-AirspaceC4: Pixel intensity histogramF4:Mn-Hu and F187:Hist-Entropy-L1C5: Co-occurrence and run length:F48:AvgGLN; F51:AvgLREC6: Laws features:F67:3D-Laws-9; F74:3D-Laws-16; F103:3D-Laws-45; and F128:3D-Laws-79"
    }
}
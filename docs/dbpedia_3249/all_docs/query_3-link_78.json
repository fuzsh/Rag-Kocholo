{
    "id": "dbpedia_3249_3",
    "rank": 78,
    "data": {
        "url": "https://pdfcoffee.com/download/cheat-pdf-pdf-free.html",
        "read_more_link": "",
        "language": "en",
        "title": "PDFCOFFEE.COM",
        "top_image": "https://pdfcoffee.com/img/cheat-pdf-pdf-free.jpg",
        "meta_img": "https://pdfcoffee.com/img/cheat-pdf-pdf-free.jpg",
        "images": [
            "https://pdfcoffee.com/pdfcoffee/assets/img/pdfcoffee_logo.png",
            "https://pdfcoffee.com/img/200x200/cheat-pdf-pdf-free.jpg",
            "https://pdfcoffee.com/pdfcoffee/assets/img/thumbnail_placeholder.png",
            "https://pdfcoffee.com/pdfcoffee/assets/img/thumbnail_placeholder.png",
            "https://pdfcoffee.com/pdfcoffee/assets/img/thumbnail_placeholder.png",
            "https://pdfcoffee.com/pdfcoffee/assets/img/thumbnail_placeholder.png",
            "https://pdfcoffee.com/pdfcoffee/assets/img/thumbnail_placeholder.png",
            "https://pdfcoffee.com/pdfcoffee/assets/img/thumbnail_placeholder.png",
            "https://pdfcoffee.com/pdfcoffee/assets/img/thumbnail_placeholder.png",
            "https://pdfcoffee.com/pdfcoffee/assets/img/thumbnail_placeholder.png",
            "https://pdfcoffee.com/pdfcoffee/assets/img/thumbnail_placeholder.png",
            "https://pdfcoffee.com/pdfcoffee/assets/img/pdfcoffee_logo.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Guest"
        ],
        "publish_date": null,
        "summary": "",
        "meta_description": "Dan’s Cheat Sheets Documentation Release 1Dan PoirierJul 21, 2019 Contents1Android information32Ansible...",
        "meta_lang": "en",
        "meta_favicon": "https://pdfcoffee.com/pdfcoffee/assets/img/apple-icon-57x57.png",
        "meta_site_name": "pdfcoffee.com",
        "canonical_link": "https://pdfcoffee.com/cheat-pdf-pdf-free.html",
        "text": "Citation preview\n\nDan’s Cheat Sheets Documentation Release 1\n\nDan Poirier\n\nJul 21, 2019\n\nContents\n\n1\n\nAndroid information\n\n3\n\n2\n\nAnsible\n\n7\n\n3\n\nAWS\n\n49\n\n4\n\nBootstrap\n\n51\n\n5\n\nDebian\n\n53\n\n6\n\nDiet\n\n57\n\n7\n\nDjango\n\n63\n\n8\n\nDocker\n\n127\n\n9\n\nElasticsearch\n\n129\n\n10 Elixir\n\n135\n\n11 Git\n\n141\n\n12 Google APIs\n\n147\n\n13 Gulp Tasks\n\n149\n\n14 HAProxy\n\n151\n\n15 Haskell\n\n153\n\n16 i3\n\n157\n\n17 IPv6\n\n159\n\n18 Javascript\n\n161\n\n19 JIRA\n\n169\n\n20 Kobo ereader\n\n171\n\ni\n\n21 Linux Notes, Misc.\n\n175\n\n22 LIRC: linux infrared remote control\n\n177\n\n23 LVM\n\n179\n\n24 LXDE\n\n181\n\n25 Logitech Harmony\n\n183\n\n26 MPD\n\n205\n\n27 MySQL with Django\n\n207\n\n28 Nginx\n\n211\n\n29 OpenSSL\n\n215\n\n30 Org mode (Emacs)\n\n221\n\n31 Postfix\n\n223\n\n32 Postgres\n\n225\n\n33 Python\n\n231\n\n34 Raspberry Pi\n\n251\n\n35 Release - how to\n\n255\n\n36 reStructuredText\n\n257\n\n37 Rsync\n\n263\n\n38 Salt Stack\n\n265\n\n39 Socat\n\n267\n\n40 Tmux\n\n269\n\n41 Travis CI\n\n271\n\n42 Video\n\n273\n\n43 X11 Window System\n\n275\n\n44 Virtualbox\n\n277\n\n45 Vue\n\n279\n\n46 Possibly surprising things in Vue\n\n283\n\n47 YAML\n\n287\n\n48 Indices and tables\n\n289\n\nIndex\n\n291\n\nii\n\nDan’s Cheat Sheets Documentation, Release 1\n\nContents:\n\nContents\n\n1\n\nDan’s Cheat Sheets Documentation, Release 1\n\n2\n\nContents\n\nCHAPTER\n\n1\n\nAndroid information\n\nContents:\n\n1.1 Nexus 6P This information is specific to the Google/Huawei Nexus 6P. See also more universal android info. Code name: angler There’s lots of information about the Nexus 6P that is nicely gathered in this thread on xda-developers. SEE ALSO: Android universal instructions\n\n1.1.1 Booting into different modes Enter safe mode (this will boot with 3rd-party apps disabled): 1. Have Your Nexus 6P Powered on and at the Home Screen 2. Press and Hold the Power Button until You See the ‘Power Off’ Dialog Screen Pop Up, 3. Let Go of the Power Button 4. Then Tap and Hold Your Finger on the “Power Off’ Option. After Holding for a Few Seconds You’ll be Asked if You Want to Reboot Into Safe Mode. 5. Simply Tap the ‘OK’ Option to Reboot Your Nexus 6P Into Safe Mode. Then Wait for the Nexus 6P to Reboot Enter fastboot aka bootloader mode (https://www.androidexplained.com/nexus-6p-fastboot-mode/):\n\n3\n\nDan’s Cheat Sheets Documentation, Release 1\n\nThis lets you execute certain ADB and Fastboot commands from your computer, get to recovery mode from here. 1. Power Down the Nexus 6P 2. Once the Device is Off, Press and Hold the Power Button and Volume Down Button at the Same Time 3. Continue Holding These Two Buttons Until you See the Special Fastboot Mode Screen 4. When You are in Fastboot Mode, You Can Let Go of These Two Buttons Enter recovery mode This lets you do a factory reset, wipe the cache partition or sideload an OTA update, etc. Also, “reboot bootloader” goes back to fastboot/bootloader mode. 1. Boot the Nexus 6P into Fastboot Mode 2. Once in Fastboot Mode, Press the volume Down Button Twice. This Should Highlight the ‘Recovery’ Option 3. Press the Power Button to Select This Option. This Will Take You to a Black Screen with a Green Android Laying Down 4. Press and Hold the the Power Button, Then Press the Volume Up Button, You’ll Immediately be Taken to the Recovery Mode Menu Here’s a kernel? https://forum.xda-developers.com/nexus-6p/development/kernel-purez-kernel-v2-0-purezandroid-t3636909 https://github.com/purezandroid/purez-kernel-angler\n\n1.1.2 Installing/replacing stuff Custom recovery This is a pre-req to install custom ROM. The following information is not specific to any Android device. See the links just above for device-specific info. Android universal instructions (custom roms etc) See device-specific pages for how to get into recovery mode, fastboot mode, etc etc\n\n1.2 Installing adb and fastboot on a computer (not the android, a linux or windows or something) https://developer.android.com/studio/releases/platform-tools.html Just download the zip, unpack, and put the binaries on your PATH or something.\n\n1.3 Custom recovery (Pre-req to install custom ROM) https://www.xda-developers.com/how-to-install-twrp/ Download from https://twrp.me/Devices/ Before:\n\n4\n\nChapter 1. Android information\n\nDan’s Cheat Sheets Documentation, Release 1\n\n• Install adb and fastboot on a computer • Enable USB debugging on the android device To install TWRP: 1. Boot into bootloader mode (e.g. fastboot mode) 2. “fastboot flash recovery twrp-2.8.x.x-xxx.img” 3. “fastboot reboot” ALTERNATIVELY if device is already rooted, you can install and use TWRP manager\n\n1.4 Custom ROM http://www.android.gs/install-custom-roms-android-devices-guide/ THIS WIPES ALL YOUR DATA 1. Copy zipfiles to phone file system 2. Boot into recovery mode 3. select “wipe data factory reset”, “wipe cache partition” and “wipe dalvick cache”. 4. Choose “install zip from SD card” and “choose zip from SD card”. Pick the update file and flash the same. 5. Optional: repeat this operation for applying the Google Apps package. 6. Reboot\n\n1.5 Kernel (as opposed to the whole ROM)\n\n1.5.1 With fastboot 1. Plug in the phone and boot to fastboot mode (back+power or camera+power). Wait until the screen with skating ‘droids appears, 2. press the back button (the center bar should say “Fastboot” or “Fastboot USB”) 3. On the computer: fastboot flash boot boot.img fastboot reboot\n\n1.4. Custom ROM\n\n5\n\nDan’s Cheat Sheets Documentation, Release 1\n\n6\n\nChapter 1. Android information\n\nCHAPTER\n\n2\n\nAnsible\n\nThis is growing into a minimal Ansible reference of sorts, since Ansible’s own docs have nothing like a reference. • Ansible. • list of keys that common playbook objects can take. • Release tarballs • Ansible documentation for older releases Quickies: To check the ubuntu version, ansible_distribution_version|float < 18 (ansible_distribution_version is e.g. “16.04”): \"ansible_distribution_release\": \"bionic\" \"ansible_distribution_version\": \"18.04\"\n\n2.1 Running Ansible tasks in the background Example: - name: start collectstatic in the background command: \"{{ install_root }}/env/bin/python manage.py collectstatic --noinput -v 0\" args: chdir: \"{{ install_root }}/webapp\" async: 1000 poll: 0 register: collectstatic_bg ################################################################################ # # PUT TASKS HERE THAT DON'T NEED TO BE RUN BEFORE COLLECTSTATIC CAN START, # AND THAT WON'T AFFECT THE BACKGROUND COLLECTSTATIC. (continues on next page)\n\n7\n\nDan’s Cheat Sheets Documentation, Release 1\n\n(continued from previous page)\n\n# ################################################################################ - name: clean up local tarball delegate_to: 127.0.0.1 # Run on localhost run_once: yes # only once become: no # Don't need sudo file: state: absent path: \"{{ tarball }}\" - name: migrate command: \"{{ install_root }}/env/bin/python manage.py migrate --noinput\" args: chdir: \"{{ install_root }}/webapp\" - name: install tasks command: \"{{ install_root }}/env/bin/python manage.py installtasks --traceback\" args: chdir: \"{{ install_root }}/webapp\" ################################################################################ # # Check every 'delay' seconds, up to 'retries' times, until collectstatic is done # ################################################################################ - name: wait for collectstatic to finish async_status: jid={{ collectstatic_bg.ansible_job_id }} register: job_result until: job_result.finished retries: 80 delay: 15 ################################################################################ # # PUT TASKS AFTER THIS THAT CAN'T RUN UNTIL COLLECTSTATIC IS DONE # ################################################################################\n\n2.2 Blocks • Blocks doc • A blog post about blocks • Blog post with examples • Complete list of possible keywords Blocks can be used anywhere a task can (mostly?). They allow applying task keys to a group of tasks without having to repeat them over and over. They also provide a form of error handling. Syntax: block: - - (continues on next page)\n\n8\n\nChapter 2. Ansible\n\nDan’s Cheat Sheets Documentation, Release 1\n\n(continued from previous page)\n\nwhen: become: true become_user: .... [rescue: - debug: msg=\"This task runs if there's an error in the block\" - ... always: - debug: msg=\"I always run\" ... more tasks .. ]\n\n2.3 Conditionals, return values, and registered variables Ansible conditionals doc\n\n2.3.1 Conditional tasks See Task.\n\n2.4 Return Values and registered variables To create a variable with results from a task, use register: - name: any task command: whatever register: varname\n\nThen in later tasks, you can use varname in conditionals like when. The variable is actually an object with lots of useful items in it. Some of them: • .changed - boolean, whether the task changed anything • .failed - boolean, true if the task failed • .skipped - boolean, true if the task was skipped • .result - ? (depends on the task?) • .results - If this key exists, it indicates that a loop was present for the task and that it contains a list of the normal module ‘result’ per item. more on common return values. There are also useful filters: • is succeeded - boolean, true if task succeeded • is failed - boolean, true if task failed • is skipped = boolean, true if the task was skipped succeeded is probably the most useful here - the others just duplicate attributes.\n\n2.3. Conditionals, return values, and registered variables\n\n9\n\nDan’s Cheat Sheets Documentation, Release 1\n\n2.5 Configuration 2.5.1 Configuration file Syntax .ini file See The Ansible Configuration File doc. Ansible uses the first config file it finds on this list: • ANSIBLE_CONFIG (an environment variable) • ansible.cfg (in the current directory) • .ansible.cfg (in the home directory) • /etc/ansible/ansible.cfg Some useful vars in the [defaults] section: any_errors_fatal If true, stop immediately if any task fails. Default value of False only stops for the host that failed. The playbook invocation will eventually report failure, but the error itself might be thousands of lines back in the output. Recommend changing this to True. display_args_to_stdout If true, more information displayed as tasks execute. Default: False. error_on_undefined_vars If true, task fails if any undefined vars are encountered, which seems like it ought to be the default behavior, but it’s not. hostfile This is the default location of the inventory file, script, or directory that Ansible will use to determine what hosts it has available to talk to: hostfile = /etc/ansible/hosts use_persistent_connections Default False. Whether to use persistent connections. (Yes, this is in the defaults section.) private_role_vars Makes role variables inaccessible from other roles. This was introduced as a way to reset role variables to default values if a role is used more than once in a playbook. Default: False\n\n10\n\nChapter 2. Ansible\n\nDan’s Cheat Sheets Documentation, Release 1\n\nretry_files_enabled Default true. If True, create .retry files on failure. These are generally useless so I change this to False to not clutter up my file system with them. roles_path The roles path indicate additional directories beyond the ‘roles/’ subdirectory of a playbook project to search to find Ansible roles. For instance, if there was a source control repository of common roles and a different repository of playbooks, you might choose to establish a convention to checkout roles in /opt/mysite/roles like so: roles_path = /opt/mysite/roles Additional paths can be provided separated by colon characters, in the same way as other pathstrings: roles_path = /opt/mysite/roles:/opt/othersite/roles Roles will be first searched for in the playbook directory. Should a role not be found, it will indicate all the possible paths that were searched. Some useful vars in the [inventory] section: any_unparsed_is_failed Default false. If ‘true’, it is a fatal error when any given inventory source cannot be successfully parsed by any available inventory plugin; otherwise, this situation only attracts a warning. Some useful vars in the [ssh_connection] section: pipelining Pipelining, if supported by the connection plugin, reduces the number of network operations required to execute a module on the remote server, by executing many Ansible modules without actual file transfer. This can result in a very significant performance improvement when enabled. However this conflicts with privilege escalation (become). For example, when using ‘sudo:’ operations you must first disable ‘requiretty’ in /etc/sudoers on all managed hosts, which is why it is disabled by default. scp_if_ssh Preferred method to use when transferring files over ssh. When set to smart, Ansible will try them until one succeeds or they all fail. If set to True, it will force ‘scp’, if False it will use ‘sftp’. ssh_args If set, this will override the Ansible default ssh arguments.In particular, users may wish to raise the ControlPersist time to encourage performance. A value of 30 minutes may be appropriate.Be aware that if -o ControlPath is set in ssh_args, the control path setting is not used.\n\n2.5. Configuration\n\n11\n\nDan’s Cheat Sheets Documentation, Release 1\n\nWarning: If you set this, the default setting is completely overridden, so you should include it (possibly edited): -C -o ControlMaster=auto -o ControlPersist=60s Example: ssh_args = -C -o ControlMaster=auto -o ControlPersist=300s -o ForwardAgent=yes -o ˓→ControlPath=./ansible_ssh_conn_%h\n\n2.6 Host Patterns See ansible host patterns doc for host patterns. : “all” = all hosts in inventory file “*” = all hosts in inventory file “~” = use regex syntax for this pattern “:” = include all hosts that match pattern1 OR pattern2 “:&” = include all hosts that match pattern1 AND pattern2 “:!” = include all hosts that match pattern1 but NOT pattern2\n\n2.7 Inventory 2.7.1 Inventory directory Whatever directory the Inventory file is in.\n\n2.7.2 Inventory file Default /etc/ansible/hosts Change set ANSIBLE_HOSTS in environment ansible-playbook -i ... set hostfile in configuration Syntax .ini file, except initial lines don’t need to be in a section The inventory file is basically a list of hostnames or IP addresses, one per line. hostname:port or address:port.\n\nCan include port with\n\nRanges: Including [m:n] in a line will repeat the line for every value from m through n. m and n can be numbers or letters: [mygroup] host[1:25]\n\nHost Pre-defined variables: Can specify per-host options after hostname on the same line. E.g.: jumper ansible_ssh_port=5555 ansible_ssh_host=192.168.1.50\n\nSee also Variables files. Group Pre-defined variables: add [groupname:vars] section and put var definitions in it, one per line. Example:\n\n12\n\nChapter 2. Ansible\n\nDan’s Cheat Sheets Documentation, Release 1\n\n[all:vars] project_name=\"PeterPan\" environment_name=staging\n\nSee also Variables files. Groups of groups: add [newgroupname:children] and put other group names in it, one per line: [group3] host13 host14 [group3:children] group1 group2 [group3:vars] group3_var1=27 group3_var2=\"Hello, World\"\n\n2.8 Invoking 2.8.1 Ad-hoc To run an ad-hoc command, use ansible. (But you almost always will want to run a playbook; see below.) Examples of ad-hoc commands: $ # $ # $ # $ $\n\nansible all -m ping as bruce ansible all -m ping -u bruce as bruce, sudoing to root ansible all -m ping -u bruce --sudo as bruce, sudoing to batman ansible all -m ping -u bruce --sdo --sudo-user batman ansible all -a \"/bin/echo hello\"\n\nHelp: Usage: ansible [options] Options: -a MODULE_ARGS, --args=MODULE_ARGS module arguments -k, --ask-pass ask for SSH password --ask-su-pass ask for su password -K, --ask-sudo-pass ask for sudo password --ask-vault-pass ask for vault password -B SECONDS, --background=SECONDS run asynchronously, failing after X seconds (default=N/A) -C, --check don't make any changes; instead, try to predict some of the changes that may occur -c CONNECTION, --connection=CONNECTION connection type to use (default=smart) (continues on next page)\n\n2.8. Invoking\n\n13\n\nDan’s Cheat Sheets Documentation, Release 1\n\n(continued from previous page)\n\n-f FORKS, --forks=FORKS specify number of parallel processes to use (default=5) -h, --help show this help message and exit -i INVENTORY, --inventory-file=INVENTORY specify inventory host file (default=/etc/ansible/hosts) -l SUBSET, --limit=SUBSET further limit selected hosts to an additional pattern --list-hosts outputs a list of matching hosts; does not execute anything else -m MODULE_NAME, --module-name=MODULE_NAME module name to execute (default=command) -M MODULE_PATH, --module-path=MODULE_PATH specify path(s) to module library (default=/usr/share/ansible/) -o, --one-line condense output -P POLL_INTERVAL, --poll=POLL_INTERVAL set the poll interval if using -B (default=15) --private-key=PRIVATE_KEY_FILE use this file to authenticate the connection -S, --su run operations with su -R SU_USER, --su-user=SU_USER run operations with su as this user (default=root) -s, --sudo run operations with sudo (nopasswd) -U SUDO_USER, --sudo-user=SUDO_USER desired sudo user (default=root) -T TIMEOUT, --timeout=TIMEOUT override the SSH timeout in seconds (default=10) -t TREE, --tree=TREE log output to this directory -u REMOTE_USER, --user=REMOTE_USER connect as this user (default=poirier) --vault-password-file=VAULT_PASSWORD_FILE vault password file -v, --verbose verbose mode (-vvv for more, -vvvv to enable connection debugging) --version show program's version number and exit\n\n2.8.2 Playbooks To run a playbook, use ansible-playbook. Here’s the help from 2.0.1.0: Usage: ansible-playbook playbook.yml Options: --ask-become-pass ask for privilege escalation password -k, --ask-pass ask for connection password --ask-su-pass ask for su password (deprecated, use become) -K, --ask-sudo-pass ask for sudo password (deprecated, use become) --ask-vault-pass ask for vault password -b, --become run operations with become (nopasswd implied) --become-method=BECOME_METHOD privilege escalation method to use (default=sudo), valid choices: [ sudo | su | pbrun | pfexec | runas | doas ] (continues on next page)\n\n14\n\nChapter 2. Ansible\n\nDan’s Cheat Sheets Documentation, Release 1\n\n(continued from previous page)\n\n--become-user=BECOME_USER run operations as this user (default=root) -C, --check don't make any changes; instead, try to predict some of the changes that may occur -c CONNECTION, --connection=CONNECTION connection type to use (default=smart) -D, --diff when changing (small) files and templates, show the differences in those files; works great with --check -e EXTRA_VARS, --extra-vars=EXTRA_VARS set additional variables as key=value or YAML/JSON --flush-cache clear the fact cache --force-handlers run handlers even if a task fails -f FORKS, --forks=FORKS specify number of parallel processes to use (default=5) -h, --help show this help message and exit -i INVENTORY, --inventory-file=INVENTORY specify inventory host path (default=/etc/ansible/hosts) or comma separated host list. -l SUBSET, --limit=SUBSET further limit selected hosts to an additional pattern --list-hosts outputs a list of matching hosts; does not execute anything else --list-tags list all available tags --list-tasks list all tasks that would be executed -M MODULE_PATH, --module-path=MODULE_PATH specify path(s) to module library (default=None) --new-vault-password-file=NEW_VAULT_PASSWORD_FILE new vault password file for rekey --output=OUTPUT_FILE output file name for encrypt or decrypt; use - for stdout --private-key=PRIVATE_KEY_FILE, --key-file=PRIVATE_KEY_FILE use this file to authenticate the connection --scp-extra-args=SCP_EXTRA_ARGS specify extra arguments to pass to scp only (e.g. -l) --sftp-extra-args=SFTP_EXTRA_ARGS specify extra arguments to pass to sftp only (e.g. -f, -l) --skip-tags=SKIP_TAGS only run plays and tasks whose tags do not match these values --ssh-common-args=SSH_COMMON_ARGS specify common arguments to pass to sftp/scp/ssh (e.g. ProxyCommand) --ssh-extra-args=SSH_EXTRA_ARGS specify extra arguments to pass to ssh only (e.g. -R) --start-at-task=START_AT_TASK start the playbook at the task matching this name --step one-step-at-a-time: confirm each task before running -S, --su run operations with su (deprecated, use become) -R SU_USER, --su-user=SU_USER run operations with su as this user (default=root) (deprecated, use become) -s, --sudo run operations with sudo (nopasswd) (deprecated, use become) -U SUDO_USER, --sudo-user=SUDO_USER (continues on next page)\n\n2.8. Invoking\n\n15\n\nDan’s Cheat Sheets Documentation, Release 1\n\n(continued from previous page)\n\ndesired sudo user (default=root) (deprecated, use become) --syntax-check perform a syntax check on the playbook, but do not execute it -t TAGS, --tags=TAGS only run plays and tasks tagged with these values -T TIMEOUT, --timeout=TIMEOUT override the connection timeout in seconds (default=10) -u REMOTE_USER, --user=REMOTE_USER connect as this user (default=None) --vault-password-file=VAULT_PASSWORD_FILE vault password file -v, --verbose verbose mode (-vvv for more, -vvvv to enable connection debugging) --version show program's version number and exit\n\n2.8.3 Hosts pulling config Ansible-pull (ansible-pull doc) is a small script that will checkout a repo of configuration instructions from git, and then run ansible-playbook against that content. Assuming you load balance your checkout location, ansible-pull scales essentially infinitely. Help from ansible-pull 2.0.1.0: Usage: ansible-pull -U [options] Options: --accept-host-key adds the hostkey for the repo url if not already added --ask-become-pass ask for privilege escalation password -k, --ask-pass ask for connection password --ask-su-pass ask for su password (deprecated, use become) -K, --ask-sudo-pass ask for sudo password (deprecated, use become) --ask-vault-pass ask for vault password -C CHECKOUT, --checkout=CHECKOUT branch/tag/commit to checkout. Defaults to behavior of repository module. -c CONNECTION, --connection=CONNECTION connection type to use (default=smart) -d DEST, --directory=DEST directory to checkout repository to -e EXTRA_VARS, --extra-vars=EXTRA_VARS set additional variables as key=value or YAML/JSON -f, --force run the playbook even if the repository could not be updated --full Do a full clone, instead of a shallow one. -h, --help show this help message and exit -i INVENTORY, --inventory-file=INVENTORY specify inventory host path (default=/etc/ansible/hosts) or comma separated host list. -l SUBSET, --limit=SUBSET further limit selected hosts to an additional pattern --list-hosts outputs a list of matching hosts; does not execute anything else -m MODULE_NAME, --module-name=MODULE_NAME (continues on next page)\n\n16\n\nChapter 2. Ansible\n\nDan’s Cheat Sheets Documentation, Release 1\n\n(continued from previous page)\n\nRepository module name, which ansible will use to check out the repo. Default is git. -M MODULE_PATH, --module-path=MODULE_PATH specify path(s) to module library (default=None) --new-vault-password-file=NEW_VAULT_PASSWORD_FILE new vault password file for rekey -o, --only-if-changed only run the playbook if the repository has been updated --output=OUTPUT_FILE output file name for encrypt or decrypt; use - for stdout --private-key=PRIVATE_KEY_FILE, --key-file=PRIVATE_KEY_FILE use this file to authenticate the connection --purge purge checkout after playbook run --scp-extra-args=SCP_EXTRA_ARGS specify extra arguments to pass to scp only (e.g. -l) --sftp-extra-args=SFTP_EXTRA_ARGS specify extra arguments to pass to sftp only (e.g. -f, -l) --skip-tags=SKIP_TAGS only run plays and tasks whose tags do not match these values -s SLEEP, --sleep=SLEEP sleep for random interval (between 0 and n number of seconds) before starting. This is a useful way to disperse git requests --ssh-common-args=SSH_COMMON_ARGS specify common arguments to pass to sftp/scp/ssh (e.g. ProxyCommand) --ssh-extra-args=SSH_EXTRA_ARGS specify extra arguments to pass to ssh only (e.g. -R) -t TAGS, --tags=TAGS only run plays and tasks tagged with these values -T TIMEOUT, --timeout=TIMEOUT override the connection timeout in seconds (default=10) -U URL, --url=URL URL of the playbook repository -u REMOTE_USER, --user=REMOTE_USER connect as this user (default=None) --vault-password-file=VAULT_PASSWORD_FILE vault password file -v, --verbose verbose mode (-vvv for more, -vvvv to enable connection debugging) --verify-commit verify GPG signature of checked out commit, if it fails abort running the playbook. This needs the corresponding VCS module to support such an operation --version show program's version number and exit\n\n2.9 Loops See http://docs.ansible.com/playbooks_loops.html\n\n2.9.1 Iterating with nested loops Write a task: 2.9. Loops\n\n17\n\nDan’s Cheat Sheets Documentation, Release 1\n\n- module: args with_subelements: - thelist - fieldname\n\nThen Ansible will essentially do this: for thing in thelist: item.0 = thing for fieldvalue in get(thing, fieldname): item.1 = fieldvalue EXECUTE (module, args)\n\nIn other words, it’ll iterate over the first value as a list, call it item.0, then get the list from that value’s field named ‘fieldname’, and iterate over that as well, calling it item.1. Presumably you could nest this deeper. Example from the docs. With variables: --users: - name: alice authorized: - /tmp/alice/onekey.pub - /tmp/alice/twokey.pub - name: bob authorized: - /tmp/bob/id_rsa.pub\n\nYou can write tasks: - user: name={{ item.name }} state=present generate_ssh_key=yes with_items: \"{{users}}\" - authorized_key: \"user={{ item.0.name }} key='{{ lookup('file', item.1) }}'\" with_subelements: - users - authorized\n\n2.10 Playbook 2.10.1 Playbook directory Default current dir\n\n2.10.2 Playbook Syntax A YAML file defining a list of Play s and Playbook include s: -\n\n18\n\ninclude: include:\n\nChapter 2. Ansible\n\nDan’s Cheat Sheets Documentation, Release 1\n\nTemplating A playbook is rendered as a Jinja2 template (using variables in playbooks doc) before processing it, but playbooks should not use loops and conditionals.\n\n2.10.3 Playbook include A playbook can include other playbooks: - include:\n\nNote that, unlike Task include s, playbook includes cannot set variables.\n\n2.10.4 Play Complete list of possible keys A dictionary: hosts: hosta:pattern1:pattern2 # required vars: var1: value1 var2: value2 roles: - - {role: , var1: value1, tags: ['tag1', 'tag2']} - role: var1: value1 var2: value2 tags: - tag1 - tag2 tags: - - remote_user: username sudo: yes|no sudo_user: username tasks: - - include: - include: tags: [tag1, tag2] - handlers: - - include: - notify: - - vars_files: - - [, , ...] (ansible loads the first one found) - strategy: linear|free serial: |\"%\"\n\n2.10. Playbook\n\n19\n\nDan’s Cheat Sheets Documentation, Release 1\n\nRequired keys: hosts A string, containing one or more Host Patterns s separated by colons Optional keys: handlers list of Handler s and Task include s. pre_tasks list of Task s and Task include s. These are executed before roles. roles list of names of Role s to include in the play. You can add parameters, tags, and conditionals: roles: - common - { role: foo_app_instance, dir: '/opt/a', tags: [\"bar\", \"baz\"] } - { role: foo_app_instance, dir: '/opt/b', when: \"ansible_os_family == 'RedHat' ˓→\" }\n\nserial Set how many hosts at a time to run at a time. The default is to run tasks on all of a play’s machines at once. See also strategy. strategy How plays are run on multiple hosts. The default is “linear”, where each task is run on up to serial hosts in parallel, and then Ansible waits for them all to complete before starting the next task on all the hosts. “free” lets each host run independently, starting its next task as soon as it finishes the previous one, regardless of how far other hosts have gotten. tags see Tags. tasks list of Task s and Task include s. These are executed after the roles. post_tasks list of Task s and Task include s. These are executed after the tasks. notify list of names of Handler s to trigger when done, but only if something changed vars A dictionary defining additional Pre-defined variables remote_user user to login as remotely sudo yes|no sudo_user user to sudo to remotely\n\n2.10.5 Running a playbook ansible-playbook [options] ansible-playbook playbook.yml –start-at=”install packages” The above will start executing your playbook at a task named “install packages”. ansible-playbook playbook.yml –step This will cause ansible to stop on each task, and ask if it should execute that task.\n\n2.11 Roles 2.11.1 Role A role is a directory with specified contents. The role directory must be in one of the directories on the roles_path and its name is used to refer to the role elsewhere. Complete list of possible keywords 20\n\nChapter 2. Ansible\n\nDan’s Cheat Sheets Documentation, Release 1\n\nInside the role’s top-level directory, you might see a tree like this (most of this is optional). defaults/main.yml variables within will be defined at the lowest priority (can be overridden by variables declared anywhere else, even inventory variables) files/ Any copy tasks can reference files in roles/x/files/ without having to path them relatively or absolutely Any script tasks can reference scripts in roles/x/files/ without having to path them relatively or absolutely handlers/main.yml handlers listed therein will be added to the play library/ modules here (directories) will be available in the role, and any roles called after this role meta/main.yml Role dependencies file tasks/main.yml Tasks file Any include tasks can reference files in roles/x/tasks/ without having to path them relatively or absolutely templates/ Any template tasks can reference files in roles/x/templates/ without having to path them relatively or absolutely vars/main.yml variables listed therein will be added to the play. These override almost any other variables except those on the command line, so this is really better for the role’s “constants” than variables :-)\n\n2.11.2 Role dependencies file Syntax YAML file Templating Jinja2 Contents A dictionary The role dependencies file defines what other roles this role depends on. Keys: dependencies A list of Dependency dictionary s allow-duplicates yes|no Defaults to no, preventing the same role from being listed as a dependency more than once. Set to yes if you want to list the same role with different variables. Example: --dependencies: - role: role1 - role: role2 varname: value\n\n2.11.3 Dependency dictionary Required keys: role name of role, or quoted path to role file, or quoted repo URL:\n\n2.11. Roles\n\n21\n\nDan’s Cheat Sheets Documentation, Release 1\n\nrole: postgres role: ‘/path/to/common/roles/foo’ role: ‘git+http://git.example.com/repos/role-foo,v1.1,foo’ role: ‘/path/to/tar/file.tgz„friendly-name’ Optional keys: any parameters for the role - these define Pre-defined variables\n\n2.11.4 Embedding modules in roles\n\n2.12 Secrets Ansible handles secrets using a feature called Vault. Vault lets you encrypt any of your .yml files, but typically you’d apply it to files containing variable definitions, then use the variables’ values as needed elsewhere. Vault provides subcommands that let you encrypt a file in place, decrypt a file in place, edit a file that’s encrypted in one step, etc. When ansible is running your playbook or whatever, any time it comes across a .yml file that appears to be encrypted, it will decrypt it (in memory) and use the decrypted contents, fairly transparently. You can have as many of your files encrypted as you want. However, all the encrypted files have to use the same password.\n\n2.12.1 Providing the password to Ansible 1) Have Ansible prompt for it by passing --ask-vault-pass. Most secure, but inconvenient. 2) Put it plaintext in a well-protected file, and pass --vault-password-file . Most insecure, but more convenient than the prompt. 3) Write a script or program that outputs the password on stdout, mark it executable, and pass that: --vault-password-file . This makes it possible to use a local system keychain or something, which might be more secure than the other options. Or worse. . .\n\n2.12.2 Ways to use it One approach I’ve used is to have a single encrypted secrets.yml file in my base directory containing all my secret variables, and another file with very restrictive permissions (and outside of source control) containing my password, then add these arguments when running ansible: --extra-vars @secrets.yml --vault-password-file path/to/passfile\n\nThe advantage of that is that if I don’t need the secrets, I can leave all that off and Ansible will run fine. (As opposed to having the encrypted file automatically read by Ansible every time.) I’m not sure if that will scale, though.\n\n22\n\nChapter 2. Ansible\n\nDan’s Cheat Sheets Documentation, Release 1\n\n2.12.3 Limitations • This is symmetric encryption. In other words, anyone with the password can encrypt and decrypt a file. • All the encrypted files must be encrypted using the same password. • That means you have to protect the decrypting password (the only password) very carefully, and makes providing it to Ansible awkward. Links: • Vault • Not logging secrets • How to upload encrypted file using ansible vault? • Managing Secrets with Ansible Vault – The Missing Guide (Part 1 of 2) • Managing Secrets with Ansible Vault – The Missing Guide (Part 2 of 2)\n\n2.13 synchronize The Ansible synchronize module gets its own page because it is a bitch. (Update: apparently some of these bad behaviors were bugs in Ansible 2.0.0.x, but I’m keeping this page around for history.) Let me count the ways: • By default, it tries to become locally the user you’ve specified using the become_user variable that you have said you want to become remotely. [Apparently that was a bug in 2.0.0.x and works correctly in 1.9.x and 2.0.1+.] • Then it does not try to remotely become the user you’ve specified; you have to hack it by setting rsync_path: \"sudo rsync\". [I have not tried this again with 2.0.1+.] • Unlike every other Ansible module, the owner and group options are booleans, not the names or numbers of users and groups. If true, it’ll try to copy the owner of the local files, but if you want to specify the ownership of the target files yourself, you’ll have to fix it afterward. Here’s a working example: - name: sync source from local directory synchronize: dest: \"{{ source_dir }}\" src: \"{{ local_project_dir }}/\" delete: yes rsync_path: \"sudo rsync\" # Use sudo on the remote system recursive: true rsync_opts: - \"--exclude=.git\" - \"--exclude=*.pyc\" become: no # stops synchronize trying to sudo locally\n\nNOTE: Ansible 2.0.1 fixed numerous bugs in synchronize: • Fixes a major compatibility break in the synchronize module shipped with 2.0.0.x. That version of synchronize ran sudo on the controller prior to running rsync. In 1.9.x and previous, sudo was run on the host that rsync connected to. 2.0.1 restores the 1.9.x behaviour.\n\n2.13. synchronize\n\n23\n\nDan’s Cheat Sheets Documentation, Release 1\n\n• Additionally, several other problems with where synchronize chose to run when combined with delegate_to were fixed. In particular, if a playbook targetted localhost and then delegated_to a remote host the prior behavior (in 1.9.x and 2.0.0.x) was to copy files between the src and destination directories on the delegated host. This has now been fixed to copy between localhost and the delegated host. • Fix a regression where synchronize was unable to deal with unicode paths. • Fix a regression where synchronize deals with inventory hosts that use localhost but with an alternate port.\n\n2.14 Tags When you apply tags to things, you can then control whether they’re executed by adding command line options.\n\n2.14.1 How to tag things Plays and tasks have optional tags attributes where you can specify a list of tags. Here are some tagged Task s: tasks: - module: parm1=a parm2=b tags: - packages - module2: parm1=x parm2=y tags: - configuration\n\nAnd here’s a playbook with some tagged Play s: - hosts: all tags: - foo - bar roles: - role1 - role2\n\nYou can also apply tags when invoking a role from a playbook: roles: - { role: webserver, port: 5000, tags: [ 'web', 'foo' ] }\n\nand when including tasks: - include: foo.yml tags: [web,foo]\n\n2.14.2 What tags do Adding a tag to a play or task says that if ansible is invoked with --tags=x,y,z, that the tagged play or task will only be executed if at least one of its tags is included in the list of tags from the command line. Specifying --tags=all is equivalent to the default behavior, where all playbooks and tasks are run regardless of their tags.\n\n24\n\nChapter 2. Ansible\n\nDan’s Cheat Sheets Documentation, Release 1\n\nSpecifying --tags=tagged runs only things that have some tag, while --tags=untagged runs only things that have no tag. You could alternatively invoke ansible with --skip-tags=a,b,c and it will execute all plays and tasks that are not tagged with a, b, or c. Presumably --skip-tags=tagged does the opposite of --tags=tagged, and --skip-tags=untagged does the opposite of --tags=untagged. If a play or task is tagged always, then it will be executed unless ansible is invoked with skip-tags=always.\n\n2.15 Task 2.15.1 Tasks file Syntax YAML FILE Templating Jinja2 Content A list of task definitions, task includes, and Blocks.\n\n2.15.2 Task include Anywhere there can be a task definition, you can also use a task include: - include: [options]\n\nThe path is relative to the Playbook directory, or the file is also searched for in the tasks directory of a role. [options] is an optional list of additional variable settings, e.g.: - include: tasks/footasks.yml vara=1 varb=2 varc=3\n\nYou can use an expanded syntax with a vars setting to set more complicated values: - include: wordpress.yml vars: wp_user: timmy some_list_variable: - alpha - beta - gamma\n\nOr use this more compact but apparently equivalent syntax: - { include: wordpress.yml, wp_user: timmy, ssh_keys: [ 'keys/one.txt', 'keys/two.txt ˓→' ] }\n\n2.15.3 Task ansible tasks doc, complete list of possible keywords A dictionary:\n\n2.15. Task\n\n25\n\nDan’s Cheat Sheets Documentation, Release 1\n\nname: string # optional but highly recommended module: args # required; the \"action\" environment: dictionary remote_user: username sudo: yes|no sudo_user: username otheroption: othervalue # depending on module tags: - -\n\nRequired keys: name text modulename options Optional keys that can be used on any task: environment dictionary (in YAML, or variable containing dictionary) ignore_errors if true, continue even if task fails register store result of task in . See also when for some ways to use. remote_user user to login as remotely sudo yes|no sudo_user user to sudo to remotely tags list of tags to associate with the task when expression controls whether task is executed ansible when doc: when: when: not\n\nSpecial filters for checking result of a prior task: when: |failed when: |skipped when: |success\n\nAdditional keys might be required and optional depending on the module being used.\n\n2.15.4 Handler Same syntax as a Task, it just gets triggered under different circumstances.\n\n2.16 Variables 2.16.1 Pre-defined variables Ansible defines some variables for you These are not mentioned when you list Facts (see below) - go figure.\n\n26\n\nChapter 2. Ansible\n\nDan’s Cheat Sheets Documentation, Release 1\n\ninventory_hostname is the name of the current host as you’ve configured it in your Ansible inventory file, regardless of the system’s actual hostname. If you have a long FQDN, inventory_hostname_short also contains the part up to the first period, without the rest of the domain.\n\n2.16.2 Variables Some variables alter the behavior of ansible (see http://docs.ansible.com/intro_inventory.html# list-of-behavioral-inventory-parameters for a list). You can set some of these using environment variables (ansible variables doc). CORRECTION: Use ansible_ssh_user, not ansible_user. Any of them can be used anywhere Jinja2 templating is in effect. Places to define variables: • inventory • playbook • included files and roles • local facts • ansible command line (--extra-vars \"foo=1 bar=2\" or --extra-vars @filepath.json or --extra-vars @filepath.yml) See also “Variable Precedence”, a little farther down. . .\n\n2.16.3 Variables file A variables file (doc) is a file that defines values of Pre-defined variables. Syntax YAML defining a single dictionary Templating The file does not appear to undergo template expansion, but the values of variables do??\n\n2.16.4 Variables files Ansible will look in Inventory directory and Playbook directory for directories named host_vars or group_vars. Inside those directories, you can put a single Variables file with the same name as a host or group (respectively) and Ansible will use those Pre-defined variables definitions. Or a file named all that will be used for all hosts or groups. Or you can create a directory with the same name as a host or group and Ansible will use all the files in that directory as Variables file s. You can also include vars files from a Play (ansible variable files doc).\n\n2.16.5 Variable precedence docs From 2.0 on, from lowest priority to highest - in other words, if a variable is defined in two places, the place that’s farther down in this list takes precedence.\n\n2.16. Variables\n\n27\n\nDan’s Cheat Sheets Documentation, Release 1\n\n• role defaults [1] • inventory file or script group vars [2] • inventory group_vars/all [3] • playbook group_vars/all [3] • inventory group_vars/* [3] • playbook group_vars/* [3] • inventory file or script host vars [2] • inventory host_vars/* • playbook host_vars/* • host facts / cached set_facts [4] • inventory host_vars/* [3] • playbook host_vars/* [3] • host facts • play vars • play vars_prompt • play vars_files • role vars (defined in role/vars/main.yml) • block vars (only for tasks in block) • task vars (only for the task) • include_vars • set_facts / registered vars • role (and include_role) params • include params • extra vars (defined on command line with -e, always win precedence) [1] Tasks in each role will see their own role’s defaults. Tasks defined outside of a role will see the last role’s defaults. [2] (1, 2) Variables defined in inventory file or provided by dynamic inventory. [3] (1, 2, 3, 4, 5, 6) Includes vars added by ‘vars plugins’ as well as host_vars and group_vars which are added by the default vars plugin shipped with Ansible. [4] When created with set_facts’s cacheable option, variables will have the high precedence in the play, but will be the same as a host facts precedence when they come from the cache.\n\n2.16.6 Facts Ansible automatically defines a whole bunch of variables with information about the system that it’s running on (the system the plays and tasks are running on, not the system you’re controlling ansible from). You can add to the facts with config files called local facts (ansible local facts doc) though I don’t know how that’s any better than putting variables in all the other places you can set them. . . To see a list of all of the facts that are available about a machine, you can run the “setup” module as an ad-hoc action: ansible -m setup hostname\n\n28\n\nChapter 2. Ansible\n\nDan’s Cheat Sheets Documentation, Release 1\n\nThis will print out a dictionary of all of the facts that are available for that particular host. Facts output is an example from one of my machines. The Ansible docs used to show an example of this output, but apparently they’ve removed or moved that. And here’s an example. The top of the output will look like: staging-web2 | SUCCESS => { \"ansible_facts\": { \"ansible_all_ipv4_addresses\": [ \"10.132.77.14\", \"138.197.111.207\", \"10.17.0.12\" ], \"ansible_all_ipv6_addresses\": [\n\nIgnore the \"ansible_facts\" part of that. To reference any of these variable, start with the next level. E.g. {{ ansible_all_ipv4_addresses[1] }}. ALTERNATIVELY, you can access the same variables as items in the ansible_facts dictionary, only without the individual keys prefixed by ansible_ (or so the docs say https://docs.ansible.com/ansible/latest/reference_ appendices/config.html#inject-facts-as-vars) and this should work even if INJECT_FACTS_AS_VARS has been set False).\n\n2.17 Ansible Galaxy 2.17.1 Links • Galaxy doc • popular galaxy roles and recent activity • search galaxy\n\n2.17.2 Role specification Format when installing roles from galaxy: • username.rolename[,version] • scm+repo_url[,version] • tarball_url Versions represent tags in the role’s source repository. E.g.: user2.role2 user1.role1,v1.0.0 user1.role2,master git+http://bitbucket.org/willthames/git-ansible-galaxy https://some.webserver.example.com/files/master.tar.gz\n\n2.17. Ansible Galaxy\n\n29\n\nDan’s Cheat Sheets Documentation, Release 1\n\n2.17.3 Ways of installing Command-line List roles on the command line: ansible-galaxy install user2.role2 user1.role1,v1.0.9\n\nSimple file List roles in a file, one per line. Example file: # file: roles.txt user2.role2 user1.role1,v1.0.0\n\nAnd install with -r: ansible-galaxy install -r roles.txt\n\nYAML file Use a YAML file to provide more control. The YAML file should contain a list of dictionaries. Each dictionary specifies a role to install. Keys can include: src (required) a role specification as above. (Since there’s a separate dictionary key for version, I don’t know whether you can include version here, or if you’re required to list it separately as version.) path Where to install (directory, can be relative) version version to install. e.g. master or v1.4. name install as a different name scm default git but could say hg and then in src provide a URL to a mercurial repository. Example: # install_roles.yml # from galaxy - src: yatesr.timezone # from github - src: https://github.com/bennojoy/nginx # from github installing to a relative path - src: https://github.com/bennojoy/nginx path: vagrant/roles/ # from github, overriding the name and specifying a specific tag - src: https://github.com/bennojoy/nginx version: master name: nginx_role # from a webserver, where the role is packaged in a tar.gz (continues on next page)\n\n30\n\nChapter 2. Ansible\n\nDan’s Cheat Sheets Documentation, Release 1\n\n(continued from previous page)\n\n- src: https://some.webserver.example.com/files/master.tar.gz name: http-role # from bitbucket, if bitbucket happens to be operational right now :) - src: git+http://bitbucket.org/willthames/git-ansible-galaxy version: v1.4 # from bitbucket, alternative syntax and caveats - src: http://bitbucket.org/willthames/hg-ansible-galaxy scm: hg\n\nAnd again install with -r: ansible-galaxy install -r install_roles.yml\n\n2.18 Facts output Example output of ‘ansible moth -m setup’ (abridged): moth | SUCCESS => { \"ansible_facts\": { \"ansible_all_ipv4_addresses\": [ \"172.26.163.45\", \"10.28.4.5\" ], \"ansible_all_ipv6_addresses\": [ \"fcae:c8f4:7d93:35ee:162::1\", \"fe80::b0f5:3cff:fe75:cff0\", \"fe80::33e1:2:9db5:7dc0\" ], \"ansible_apparmor\": { \"status\": \"enabled\" }, \"ansible_architecture\": \"x86_64\", \"ansible_bios_date\": \"05/17/2016\", \"ansible_bios_version\": \"G6ETB4WW (2.74 )\", \"ansible_cmdline\": { \"BOOT_IMAGE\": \"/boot/vmlinuz-4.15.0-38-generic\", \"ro\": true, \"root\": \"UUID=cd980b7e-8c51-4b68-9a36-25bacd7d5ebf\" }, \"ansible_date_time\": { \"date\": \"2019-01-11\", \"day\": \"11\", \"epoch\": \"1547232398\", \"hour\": \"13\", \"iso8601\": \"2019-01-11T18:46:38Z\", \"iso8601_basic\": \"20190111T134638560137\", \"iso8601_basic_short\": \"20190111T134638\", \"iso8601_micro\": \"2019-01-11T18:46:38.560229Z\", \"minute\": \"46\", \"month\": \"01\", \"second\": \"38\", \"time\": \"13:46:38\", (continues on next page)\n\n2.18. Facts output\n\n31\n\nDan’s Cheat Sheets Documentation, Release 1\n\n(continued from previous page)\n\n\"tz\": \"EST\", \"tz_offset\": \"-0500\", \"weekday\": \"Friday\", \"weekday_number\": \"5\", \"weeknumber\": \"01\", \"year\": \"2019\" }, \"ansible_default_ipv4\": { \"address\": \"10.28.4.5\", \"alias\": \"wlp3s0\", \"broadcast\": \"10.28.4.255\", \"gateway\": \"10.28.4.1\", \"interface\": \"wlp3s0\", \"macaddress\": \"84:3a:4b:73:6c:f8\", \"mtu\": 1500, \"netmask\": \"255.255.255.0\", \"network\": \"10.28.4.0\", \"type\": \"ether\" }, \"ansible_default_ipv6\": {}, \"ansible_device_links\": { \"ids\": { \"sda\": [ \"ata-SanDisk_SD5SG2256G1052E_124928400505\", \"wwn-0x5001b44821e51879\" ], \"sda1\": [ \"ata-SanDisk_SD5SG2256G1052E_124928400505-part1\", \"wwn-0x5001b44821e51879-part1\" ] }, \"labels\": {}, \"masters\": {}, \"uuids\": { \"sda1\": [ \"cd980b7e-8c51-4b68-9a36-25bacd7d5ebf\" ] } }, \"ansible_devices\": { \"loop0\": { \"holders\": [], \"host\": \"\", \"links\": { \"ids\": [], \"labels\": [], \"masters\": [], \"uuids\": [] }, \"model\": null, \"partitions\": {}, \"removable\": \"0\", \"rotational\": \"1\", \"sas_address\": null, \"sas_device_handle\": null, \"scheduler_mode\": \"none\", \"sectors\": \"693784\", (continues on next page)\n\n32\n\nChapter 2. Ansible\n\nDan’s Cheat Sheets Documentation, Release 1\n\n(continued from previous page)\n\n\"sectorsize\": \"512\", \"size\": \"338.76 MB\", \"support_discard\": \"4096\", \"vendor\": null, \"virtual\": 1 }, ... \"sda\": { \"holders\": [], \"host\": \"SATA controller: Intel Corporation 7 Series Chipset Family 6˓→port SATA Controller [AHCI mode] (rev 04)\", \"links\": { \"ids\": [ \"ata-SanDisk_SD5SG2256G1052E_124928400505\", \"wwn-0x5001b44821e51879\" ], \"labels\": [], \"masters\": [], \"uuids\": [] }, \"model\": \"SanDisk SD5SG225\", \"partitions\": { \"sda1\": { \"holders\": [], \"links\": { \"ids\": [ \"ata-SanDisk_SD5SG2256G1052E_124928400505-part1\", \"wwn-0x5001b44821e51879-part1\" ], \"labels\": [], \"masters\": [], \"uuids\": [ \"cd980b7e-8c51-4b68-9a36-25bacd7d5ebf\" ] }, \"sectors\": \"500115456\", \"sectorsize\": 512, \"size\": \"238.47 GB\", \"start\": \"2048\", \"uuid\": \"cd980b7e-8c51-4b68-9a36-25bacd7d5ebf\" } }, \"removable\": \"0\", \"rotational\": \"0\", \"sas_address\": null, \"sas_device_handle\": null, \"scheduler_mode\": \"cfq\", \"sectors\": \"500118192\", \"sectorsize\": \"512\", \"size\": \"238.47 GB\", \"support_discard\": \"512\", \"vendor\": \"ATA\", \"virtual\": 1, \"wwn\": \"0x5001b44821e51879\" } }, \"ansible_distribution\": \"Ubuntu\", (continues on next page)\n\n2.18. Facts output\n\n33\n\nDan’s Cheat Sheets Documentation, Release 1\n\n(continued from previous page)\n\n\"ansible_distribution_file_parsed\": true, \"ansible_distribution_file_path\": \"/etc/os-release\", \"ansible_distribution_file_variety\": \"Debian\", \"ansible_distribution_major_version\": \"18\", \"ansible_distribution_release\": \"bionic\", \"ansible_distribution_version\": \"18.04\", \"ansible_dns\": { \"nameservers\": [ \"127.0.0.53\" ], \"search\": [ \"mynet\" ] }, \"ansible_domain\": \"zero\", \"ansible_effective_group_id\": 1000, \"ansible_effective_user_id\": 1000, \"ansible_env\": { \"BASH_ENV\": \"/home/poirier/dotfiles/bash/.bashenvrc\", \"DBUS_SESSION_BUS_ADDRESS\": \"unix:path=/run/user/1000/bus\", \"HOME\": \"/home/poirier\", \"LANG\": \"en_US.UTF-8\", \"LOGNAME\": \"poirier\", \"MAIL\": \"/var/mail/poirier\", \"PATH\": \"/home/poirier/.pyenv/plugins/pyenv-virtualenv/shims:/home/ ˓→poirier/.pyenv/shims:/home/poirier/.pyenv/bin:/home/poirier/.pyenv/plugins/pyenv˓→virtualenv/shims:/home/poirier/.pyenv/shims:/home/poirier/.pyenv/bin:/usr/local/ ˓→sbin:/usr/local/bin:/home/poirier/.local/bin:/home/poirier/.yarn/bin:/home/poirier/ ˓→bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games\", \"PWD\": \"/home/poirier\", \"PYENV_ROOT\": \"/home/poirier/.pyenv\", \"PYENV_SHELL\": \"bash\", \"PYENV_VIRTUALENV_INIT\": \"1\", \"SHELL\": \"/bin/bash\", \"SHLVL\": \"1\", \"SSH_AUTH_SOCK\": \"/tmp/ssh-pH3QNOurzC/agent.26808\", \"SSH_CLIENT\": \"127.0.0.1 33550 22\", \"SSH_CONNECTION\": \"127.0.0.1 33550 127.0.0.1 22\", \"TZ\": \"America/New_York\", \"USER\": \"poirier\", \"XDG_RUNTIME_DIR\": \"/run/user/1000\", \"XDG_SESSION_ID\": \"11860\", \"_\": \"/bin/sh\" }, \"ansible_fips\": false, \"ansible_form_factor\": \"Notebook\", \"ansible_fqdn\": \"moth.zero\", \"ansible_hostname\": \"moth\", \"ansible_interfaces\": [ \"zt7nnjxkbi\", \"wlp3s0\", \"wwp0s20u4i6\", \"lo\" ], \"ansible_is_chroot\": false, \"ansible_iscsi_iqn\": \"\", \"ansible_kernel\": \"4.15.0-38-generic\", (continues on next page)\n\n34\n\nChapter 2. Ansible\n\nDan’s Cheat Sheets Documentation, Release 1\n\n(continued from previous page)\n\n\"ansible_lo\": { \"active\": true, \"device\": \"lo\", \"ipv4\": { \"address\": \"127.0.0.1\", \"broadcast\": \"host\", \"netmask\": \"255.0.0.0\", \"network\": \"127.0.0.0\" }, \"ipv6\": [ { \"address\": \"::1\", \"prefix\": \"128\", \"scope\": \"host\" } ], \"mtu\": 65536, \"promisc\": false, \"type\": \"loopback\" }, \"ansible_local\": {}, \"ansible_lsb\": { \"codename\": \"bionic\", \"description\": \"Ubuntu 18.04.1 LTS\", \"id\": \"Ubuntu\", \"major_release\": \"18\", \"release\": \"18.04\" }, \"ansible_machine\": \"x86_64\", \"ansible_machine_id\": \"d3e0714b1ee94fbd8512d59db7d1cf3f\", \"ansible_memfree_mb\": 145, \"ansible_memory_mb\": { \"nocache\": { \"free\": 1830, \"used\": 5847 }, \"real\": { \"free\": 145, \"total\": 7677, \"used\": 7532 }, \"swap\": { \"cached\": 32, \"free\": 7397, \"total\": 7629, \"used\": 232 } }, \"ansible_memtotal_mb\": 7677, \"ansible_mounts\": [ { \"block_available\": 16474502, \"block_size\": 4096, \"block_total\": 61271111, \"block_used\": 44796609, \"device\": \"/dev/sda1\", \"fstype\": \"ext4\", (continues on next page)\n\n2.18. Facts output\n\n35\n\nDan’s Cheat Sheets Documentation, Release 1\n\n(continued from previous page)\n\n\"inode_available\": 14238018, \"inode_total\": 15630336, \"inode_used\": 1392318, \"mount\": \"/\", \"options\": \"rw,relatime,errors=remount-ro,data=ordered\", \"size_available\": 67479560192, \"size_total\": 250966470656, \"uuid\": \"cd980b7e-8c51-4b68-9a36-25bacd7d5ebf\" }, { \"block_available\": 0, \"block_size\": 131072, \"block_total\": 116, \"block_used\": 116, \"device\": \"/dev/loop1\", \"fstype\": \"squashfs\", \"inode_available\": 0, \"inode_total\": 1721, \"inode_used\": 1721, \"mount\": \"/snap/gnome-logs/43\", \"options\": \"ro,nodev,relatime\", \"size_available\": 0, \"size_total\": 15204352, \"uuid\": \"N/A\" }, { \"block_available\": 0, \"block_size\": 131072, \"block_total\": 116, \"block_used\": 116, \"device\": \"/dev/loop2\", \"fstype\": \"squashfs\", \"inode_available\": 0, \"inode_total\": 1720, \"inode_used\": 1720, \"mount\": \"/snap/gnome-logs/40\", \"options\": \"ro,nodev,relatime\", \"size_available\": 0, \"size_total\": 15204352, \"uuid\": \"N/A\" }, { \"block_available\": 0, \"block_size\": 131072, \"block_total\": 104, \"block_used\": 104, \"device\": \"/dev/loop5\", \"fstype\": \"squashfs\", \"inode_available\": 0, \"inode_total\": 1598, \"inode_used\": 1598, \"mount\": \"/snap/gnome-characters/139\", \"options\": \"ro,nodev,relatime\", \"size_available\": 0, \"size_total\": 13631488, \"uuid\": \"N/A\" }, (continues on next page)\n\n36\n\nChapter 2. Ansible\n\nDan’s Cheat Sheets Documentation, Release 1\n\n(continued from previous page)\n\n{ \"block_available\": 0, \"block_size\": 131072, \"block_total\": 1116, \"block_used\": 1116, \"device\": \"/dev/loop4\", \"fstype\": \"squashfs\", \"inode_available\": 0, \"inode_total\": 27651, \"inode_used\": 27651, \"mount\": \"/snap/gnome-3-26-1604/64\", \"options\": \"ro,nodev,relatime\", \"size_available\": 0, \"size_total\": 146276352, \"uuid\": \"N/A\" }, { \"block_available\": 0, \"block_size\": 131072, \"block_total\": 18, \"block_used\": 18, \"device\": \"/dev/loop8\", \"fstype\": \"squashfs\", \"inode_available\": 0, \"inode_total\": 1269, \"inode_used\": 1269, \"mount\": \"/snap/gnome-calculator/238\", \"options\": \"ro,nodev,relatime\", \"size_available\": 0, \"size_total\": 2359296, \"uuid\": \"N/A\" }, { \"block_available\": 0, \"block_size\": 131072, \"block_total\": 1128, \"block_used\": 1128, \"device\": \"/dev/loop6\", \"fstype\": \"squashfs\", \"inode_available\": 0, \"inode_total\": 27638, \"inode_used\": 27638, \"mount\": \"/snap/gnome-3-26-1604/70\", \"options\": \"ro,nodev,relatime\", \"size_available\": 0, \"size_total\": 147849216, \"uuid\": \"N/A\" }, { \"block_available\": 0, \"block_size\": 131072, \"block_total\": 117, \"block_used\": 117, \"device\": \"/dev/loop7\", \"fstype\": \"squashfs\", \"inode_available\": 0, \"inode_total\": 1720, (continues on next page)\n\n2.18. Facts output\n\n37\n\nDan’s Cheat Sheets Documentation, Release 1\n\n(continued from previous page)\n\n\"inode_used\": 1720, \"mount\": \"/snap/gnome-logs/45\", \"options\": \"ro,nodev,relatime\", \"size_available\": 0, \"size_total\": 15335424, \"uuid\": \"N/A\" }, { \"block_available\": 0, \"block_size\": 131072, \"block_total\": 30, \"block_used\": 30, \"device\": \"/dev/loop10\", \"fstype\": \"squashfs\", \"inode_available\": 0, \"inode_total\": 724, \"inode_used\": 724, \"mount\": \"/snap/gnome-system-monitor/51\", \"options\": \"ro,nodev,relatime\", \"size_available\": 0, \"size_total\": 3932160, \"uuid\": \"N/A\" }, { \"block_available\": 0, \"block_size\": 131072, \"block_total\": 18, \"block_used\": 18, \"device\": \"/dev/loop11\", \"fstype\": \"squashfs\", \"inode_available\": 0, \"inode_total\": 1270, \"inode_used\": 1270, \"mount\": \"/snap/gnome-calculator/222\", \"options\": \"ro,nodev,relatime\", \"size_available\": 0, \"size_total\": 2359296, \"uuid\": \"N/A\" }, { \"block_available\": 0, \"block_size\": 131072, \"block_total\": 338, \"block_used\": 338, \"device\": \"/dev/loop9\", \"fstype\": \"squashfs\", \"inode_available\": 0, \"inode_total\": 36056, \"inode_used\": 36056, \"mount\": \"/snap/gtk-common-themes/701\", \"options\": \"ro,nodev,relatime\", \"size_available\": 0, \"size_total\": 44302336, \"uuid\": \"N/A\" }, { \"block_available\": 0, (continues on next page)\n\n38\n\nChapter 2. Ansible\n\nDan’s Cheat Sheets Documentation, Release 1\n\n(continued from previous page)\n\n\"block_size\": 131072, \"block_total\": 18, \"block_used\": 18, \"device\": \"/dev/loop12\", \"fstype\": \"squashfs\", \"inode_available\": 0, \"inode_total\": 1269, \"inode_used\": 1269, \"mount\": \"/snap/gnome-calculator/260\", \"options\": \"ro,nodev,relatime\", \"size_available\": 0, \"size_total\": 2359296, \"uuid\": \"N/A\" }, { \"block_available\": 0, \"block_size\": 131072, \"block_total\": 30, \"block_used\": 30, \"device\": \"/dev/loop13\", \"fstype\": \"squashfs\", \"inode_available\": 0, \"inode_total\": 724, \"inode_used\": 724, \"mount\": \"/snap/gnome-system-monitor/54\", \"options\": \"ro,nodev,relatime\", \"size_available\": 0, \"size_total\": 3932160, \"uuid\": \"N/A\" }, { \"block_available\": 0, \"block_size\": 131072, \"block_total\": 1126, \"block_used\": 1126, \"device\": \"/dev/loop14\", \"fstype\": \"squashfs\", \"inode_available\": 0, \"inode_total\": 27631, \"inode_used\": 27631, \"mount\": \"/snap/gnome-3-26-1604/74\", \"options\": \"ro,nodev,relatime\", \"size_available\": 0, \"size_total\": 147587072, \"uuid\": \"N/A\" }, { \"block_available\": 0, \"block_size\": 131072, \"block_total\": 30, \"block_used\": 30, \"device\": \"/dev/loop15\", \"fstype\": \"squashfs\", \"inode_available\": 0, \"inode_total\": 747, \"inode_used\": 747, \"mount\": \"/snap/gnome-system-monitor/57\", (continues on next page)\n\n2.18. Facts output\n\n39\n\nDan’s Cheat Sheets Documentation, Release 1\n\n(continued from previous page)\n\n\"options\": \"ro,nodev,relatime\", \"size_available\": 0, \"size_total\": 3932160, \"uuid\": \"N/A\" }, { \"block_available\": 0, \"block_size\": 131072, \"block_total\": 104, \"block_used\": 104, \"device\": \"/dev/loop19\", \"fstype\": \"squashfs\", \"inode_available\": 0, \"inode_total\": 1597, \"inode_used\": 1597, \"mount\": \"/snap/gnome-characters/117\", \"options\": \"ro,nodev,relatime\", \"size_available\": 0, \"size_total\": 13631488, \"uuid\": \"N/A\" }, { \"block_available\": 0, \"block_size\": 131072, \"block_total\": 104, \"block_used\": 104, \"device\": \"/dev/loop21\", \"fstype\": \"squashfs\", \"inode_available\": 0, \"inode_total\": 1597, \"inode_used\": 1597, \"mount\": \"/snap/gnome-characters/124\", \"options\": \"ro,nodev,relatime\", \"size_available\": 0, \"size_total\": 13631488, \"uuid\": \"N/A\" }, { \"block_available\": 0, \"block_size\": 131072, \"block_total\": 274, \"block_used\": 274, \"device\": \"/dev/loop23\", \"fstype\": \"squashfs\", \"inode_available\": 0, \"inode_total\": 27298, \"inode_used\": 27298, \"mount\": \"/snap/gtk-common-themes/808\", \"options\": \"ro,nodev,relatime\", \"size_available\": 0, \"size_total\": 35913728, \"uuid\": \"N/A\" }, { \"block_available\": 0, \"block_size\": 131072, \"block_total\": 277, (continues on next page)\n\n40\n\nChapter 2. Ansible\n\nDan’s Cheat Sheets Documentation, Release 1\n\n(continued from previous page)\n\n\"block_used\": 277, \"device\": \"/dev/loop28\", \"fstype\": \"squashfs\", \"inode_available\": 0, \"inode_total\": 27345, \"inode_used\": 27345, \"mount\": \"/snap/gtk-common-themes/818\", \"options\": \"ro,nodev,relatime\", \"size_available\": 0, \"size_total\": 36306944, \"uuid\": \"N/A\" }, { \"block_available\": 0, \"block_size\": 131072, \"block_total\": 706, \"block_used\": 706, \"device\": \"/dev/loop18\", \"fstype\": \"squashfs\", \"inode_available\": 0, \"inode_total\": 12808, \"inode_used\": 12808, \"mount\": \"/snap/core/5897\", \"options\": \"ro,nodev,relatime\", \"size_available\": 0, \"size_total\": 92536832, \"uuid\": \"N/A\" }, { \"block_available\": 0, \"block_size\": 131072, \"block_total\": 193, \"block_used\": 193, \"device\": \"/dev/loop22\", \"fstype\": \"squashfs\", \"inode_available\": 0, \"inode_total\": 20404, \"inode_used\": 20404, \"mount\": \"/snap/heroku/3669\", \"options\": \"ro,nodev,relatime\", \"size_available\": 0, \"size_total\": 25296896, \"uuid\": \"N/A\" }, { \"block_available\": 0, \"block_size\": 131072, \"block_total\": 194, \"block_used\": 194, \"device\": \"/dev/loop24\", \"fstype\": \"squashfs\", \"inode_available\": 0, \"inode_total\": 20430, \"inode_used\": 20430, \"mount\": \"/snap/heroku/3677\", \"options\": \"ro,nodev,relatime\", \"size_available\": 0, (continues on next page)\n\n2.18. Facts output\n\n41\n\nDan’s Cheat Sheets Documentation, Release 1\n\n(continued from previous page)\n\n\"size_total\": 25427968, \"uuid\": \"N/A\" }, { \"block_available\": 0, \"block_size\": 131072, \"block_total\": 2711, \"block_used\": 2711, \"device\": \"/dev/loop0\", \"fstype\": \"squashfs\", \"inode_available\": 0, \"inode_total\": 2577, \"inode_used\": 2577, \"mount\": \"/snap/pycharm-professional/107\", \"options\": \"ro,nodev,relatime\", \"size_available\": 0, \"size_total\": 355336192, \"uuid\": \"N/A\" }, { \"block_available\": 0, \"block_size\": 131072, \"block_total\": 716, \"block_used\": 716, \"device\": \"/dev/loop3\", \"fstype\": \"squashfs\", \"inode_available\": 0, \"inode_total\": 12810, \"inode_used\": 12810, \"mount\": \"/snap/core/6034\", \"options\": \"ro,nodev,relatime\", \"size_available\": 0, \"size_total\": 93847552, \"uuid\": \"N/A\" }, { \"block_available\": 0, \"block_size\": 131072, \"block_total\": 716, \"block_used\": 716, \"device\": \"/dev/loop17\", \"fstype\": \"squashfs\", \"inode_available\": 0, \"inode_total\": 12810, \"inode_used\": 12810, \"mount\": \"/snap/core/6130\", \"options\": \"ro,nodev,relatime\", \"size_available\": 0, \"size_total\": 93847552, \"uuid\": \"N/A\" }, { \"block_available\": 0, \"block_size\": 131072, \"block_total\": 2712, \"block_used\": 2712, \"device\": \"/dev/loop20\", (continues on next page)\n\n42\n\nChapter 2. Ansible\n\nDan’s Cheat Sheets Documentation, Release 1\n\n(continued from previous page)\n\n\"fstype\": \"squashfs\", \"inode_available\": 0, \"inode_total\": 2577, \"inode_used\": 2577, \"mount\": \"/snap/pycharm-professional/109\", \"options\": \"ro,nodev,relatime\", \"size_available\": 0, \"size_total\": 355467264, \"uuid\": \"N/A\" }, { \"block_available\": 0, \"block_size\": 131072, \"block_total\": 194, \"block_used\": 194, \"device\": \"/dev/loop25\", \"fstype\": \"squashfs\", \"inode_available\": 0, \"inode_total\": 21319, \"inode_used\": 21319, \"mount\": \"/snap/heroku/3685\", \"options\": \"ro,nodev,relatime\", \"size_available\": 0, \"size_total\": 25427968, \"uuid\": \"N/A\" }, { \"block_available\": 60329506, \"block_size\": 131072, \"block_total\": 90023018, \"block_used\": 29693512, \"device\": \"syn.mynet:/volume1\", \"fstype\": \"nfs4\", \"inode_available\": 731447206, \"inode_total\": 731684864, \"inode_used\": 237658, \"mount\": \"/v\", \"options\": \"rw,relatime,vers=4.0,rsize=131072,wsize=131072,namlen=255, ˓→soft,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=10.28.4.5,local_lock=none, ˓→addr=10.28.4.15\", \"size_available\": 7907509010432, \"size_total\": 11799497015296, \"uuid\": \"N/A\" }, { \"block_available\": 0, \"block_size\": 131072, \"block_total\": 2713, \"block_used\": 2713, \"device\": \"/dev/loop26\", \"fstype\": \"squashfs\", \"inode_available\": 0, \"inode_total\": 2583, \"inode_used\": 2583, \"mount\": \"/snap/pycharm-professional/112\", \"options\": \"ro,nodev,relatime\", \"size_available\": 0, (continues on next page)\n\n2.18. Facts output\n\n43\n\nDan’s Cheat Sheets Documentation, Release 1\n\n(continued from previous page)\n\n\"size_total\": 355598336, \"uuid\": \"N/A\" }, { \"block_available\": 0, \"block_size\": 131072, \"block_total\": 867, \"block_used\": 867, \"device\": \"/dev/loop16\", \"fstype\": \"squashfs\", \"inode_available\": 0, \"inode_total\": 10113, \"inode_used\": 10113, \"mount\": \"/snap/bitwarden/16\", \"options\": \"ro,nodev,relatime\", \"size_available\": 0, \"size_total\": 113639424, \"uuid\": \"N/A\" } ], \"ansible_nodename\": \"moth\", \"ansible_os_family\": \"Debian\", \"ansible_pkg_mgr\": \"apt\", \"ansible_processor\": [ \"0\", \"GenuineIntel\", \"Intel(R) Core(TM) i7-3667U CPU @ 2.00GHz\", \"1\", \"GenuineIntel\", \"Intel(R) Core(TM) i7-3667U CPU @ 2.00GHz\", \"2\", \"GenuineIntel\", \"Intel(R) Core(TM) i7-3667U CPU @ 2.00GHz\", \"3\", \"GenuineIntel\", \"Intel(R) Core(TM) i7-3667U CPU @ 2.00GHz\" ], \"ansible_processor_cores\": 2, \"ansible_processor_count\": 1, \"ansible_processor_threads_per_core\": 2, \"ansible_processor_vcpus\": 4, \"ansible_product_name\": \"3443CTO\", \"ansible_product_serial\": \"NA\", \"ansible_product_uuid\": \"NA\", \"ansible_product_version\": \"ThinkPad X1 Carbon\", \"ansible_python\": { \"executable\": \"/usr/bin/python3\", \"has_sslcontext\": true, \"type\": \"cpython\", \"version\": { \"major\": 3, \"micro\": 7, \"minor\": 6, \"releaselevel\": \"final\", \"serial\": 0 }, \"version_info\": [ (continues on next page)\n\n44\n\nChapter 2. Ansible\n\nDan’s Cheat Sheets Documentation, Release 1\n\n(continued from previous page)\n\n3, 6, 7, \"final\", 0\n\n] }, \"ansible_python_version\": \"3.6.7\", \"ansible_real_group_id\": 1000, \"ansible_real_user_id\": 1000, \"ansible_selinux\": { \"status\": \"Missing selinux Python library\" }, \"ansible_selinux_python_present\": false, \"ansible_service_mgr\": \"systemd\", \"ansible_ssh_host_key_ecdsa_public\": ˓→\"AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBOag0MplD833lb8uTuna9XSgqzBb/ ˓→chnOJ+JJd5IY3LPkML9vgYGsqM5TzCNIyTJU1Yu0NIAr7viQOYv5nOFVYA=\", \"ansible_ssh_host_key_ed25519_public\": ˓→\"AAAAC3NzaC1lZDI1NTE5AAAAIHlbGrt1d2303ouFG685QFq+DU1xBogZ3zfpba+/EPi6\", \"ansible_ssh_host_key_rsa_public\": \"AAAAB3NzaC1yc2EAAAADAQABAAABAQDOzkRKqn4P/ ˓→7q5Yn8vipd5BcwL0nmIpvYmyivH4Y9kci8q1KU71JxQWlFm4kuX9KgrQyY8sI2R0GkIF0jzFiA0Lyd4u7wjPJPIeCwbNn5q54ai ˓→dGFbcK3aSFKkRaiRin9hO1UK27w1dBQ+NsBITM5EBLNdhvdeZqp5ie1QAFqVsfwsVvRHUpY6tsGOx9IhLb7yc4HC6j1iuhjIvpV ˓→\", \"ansible_swapfree_mb\": 7397, \"ansible_swaptotal_mb\": 7629, \"ansible_system\": \"Linux\", \"ansible_system_capabilities\": [ \"\" ], \"ansible_system_capabilities_enforced\": \"True\", \"ansible_system_vendor\": \"LENOVO\", \"ansible_uptime_seconds\": 5980956, \"ansible_user_dir\": \"/home/poirier\", \"ansible_user_gecos\": \"Dan Poirier,,,\", \"ansible_user_gid\": 1000, \"ansible_user_id\": \"poirier\", \"ansible_user_shell\": \"/bin/bash\", \"ansible_user_uid\": 1000, \"ansible_userspace_architecture\": \"x86_64\", \"ansible_userspace_bits\": \"64\", \"ansible_virtualization_role\": \"host\", \"ansible_virtualization_type\": \"kvm\", \"ansible_wlp3s0\": { \"active\": true, \"device\": \"wlp3s0\", \"ipv4\": { \"address\": \"10.28.4.5\", \"broadcast\": \"10.28.4.255\", \"netmask\": \"255.255.255.0\", \"network\": \"10.28.4.0\" }, \"ipv6\": [ { \"address\": \"fe80::33e1:2:9db5:7dc0\", \"prefix\": \"64\", \"scope\": \"link\" (continues on next page)\n\n2.18. Facts output\n\n45\n\nDan’s Cheat Sheets Documentation, Release 1\n\n(continued from previous page)\n\n} ], \"macaddress\": \"84:3a:4b:73:6c:f8\", \"module\": \"iwlwifi\", \"mtu\": 1500, \"pciid\": \"0000:03:00.0\", \"promisc\": false, \"type\": \"ether\" }, \"ansible_wwp0s20u4i6\": { \"active\": false, \"device\": \"wwp0s20u4i6\", \"macaddress\": \"56:42:fa:89:f2:0b\", \"module\": \"cdc_mbim\", \"mtu\": 1500, \"pciid\": \"3-4:1.6\", \"promisc\": false, \"type\": \"ether\" }, \"ansible_zt7nnjxkbi\": { \"active\": true, \"device\": \"zt7nnjxkbi\", \"ipv4\": { \"address\": \"172.26.163.45\", \"broadcast\": \"172.26.255.255\", \"netmask\": \"255.255.0.0\", \"network\": \"172.26.0.0\" }, \"ipv6\": [ { \"address\": \"fcae:c8f4:7d93:35ee:162::1\", \"prefix\": \"40\", \"scope\": \"global\" } ], \"macaddress\": \"b2:f5:3c:75:cf:f0\", \"mtu\": 2800, \"promisc\": false, \"speed\": 10, \"type\": \"ether\" }, \"gather_subset\": [ \"all\" ], \"module_setup\": true }, \"changed\": false }\n\nMisc. stuff I need to file somewhere:\n\n2.19 Ad-hoc command ansible Host Patterns -m [options] e.g. 46\n\nChapter 2. Ansible\n\nDan’s Cheat Sheets Documentation, Release 1\n\n$ ansible all -m ping –ask-pass Shortcut to run a command: $ ansible all -a “/bin/echo hello” options: see output of “ansible –help” for now See ansible ad-hoc commands doc for ad-hoc commands.\n\n2.19. Ad-hoc command\n\n47\n\nDan’s Cheat Sheets Documentation, Release 1\n\n48\n\nChapter 2. Ansible\n\nCHAPTER\n\n3\n\nAWS\n\nContents:\n\n3.1 S3 3.1.1 Access control • How S3 evaluates access control • Guidelines for Using the Available Access Policy Options “The only recommended use case for the bucket ACL is to grant write permission to the Amazon S3 Log Delivery group”. . . “In general, you can use either a user policy or a bucket policy to manage permissions.” Here’s a bucket policy to grant some IAM user complete access to a bucket: { \"Statement\": [ { \"Sid\":\"PublicReadForGetBucketObjects\", \"Effect\":\"Allow\", \"Principal\": { \"AWS\": \"*\" }, \"Action\":[\"s3:GetObject\"], \"Resource\":[\"arn:aws:s3:::BUCKET-NAME/*\" ] }, { \"Action\": \"s3:*\", \"Effect\": \"Allow\", \"Resource\": [ (continues on next page)\n\n49\n\nDan’s Cheat Sheets Documentation, Release 1\n\n(continued from previous page)\n\n\"arn:aws:s3:::BUCKET-NAME\", \"arn:aws:s3:::BUCKET-NAME/*\" ], \"Principal\": { \"AWS\": [ \"USER-ARN\" ] } } ] }\n\nWhat about read-only access? Let’s see. . . seems like s3auth.com used this example: { \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [\"s3:GetObject\", \"s3:GetBucketWebsite\"], \"Resource\": [ \"arn:aws:s3:::bucket-1.example.com/*\", \"arn:aws:s3:::bucket-2.example.com/*\" ] } ] }\n\n3.1.2 Updating metadata to improve response headers for caching Install s3cmd, then do it like this: s3cmd --recursive modify \\ --add-header=\"Expires: Thu, 31 Dec 2099 20:00:00 GMT\" \\ --add-header=\"Cache-Control: max-age=94608000\" \\ s3://caktus-website-production-2015/media/community_logos\n\nYou can use s3cmd ls to get a list of the buckets you can access.\n\n50\n\nChapter 3. AWS\n\nCHAPTER\n\n4\n\nBootstrap\n\nWarning: THIS IS NOT DONE AND PROBABLY WRONG. The grid. SO what does a class col-SIZE-N mean? Each SIZE has a BREAKPOINT: xs: -1 sm: 750px md: 970px lg: 1170px Call the window width WIDTH. For a single class col-SIZE-N: if WIDTH >= BREAKPOINT(SIZE), then ELEMENT-WIDTH=WIDTH*N/12 display INLINE (same line as previous element if possible) else ELEMENT-WIDTH=100% display BLOCK (element gets its own line)\n\nWhat if we have col-SIZE1-N1 and col-SIZE2-N2, with BREAKPOINT(SIZE1) < BREAKPOINT(SIZE2)?: IF WIDTH >= BREAKPOINT(SIZE2), then ELEMENT_WIDTH = WIDTH * N2 / 12 INLINE elif WIDTH >= BREAKPOINT(SIZE1), then ELEMENT_WIDTH = WIDTH * N1 / 12 INLINE else: BLOCK display\n\nand so forth - just look at the class with the largest size\n\n51\n\nDan’s Cheat Sheets Documentation, Release 1\n\nNOTE: Since all widths are >= the breakpoint of XS, then if XS is present, the element will ALWAYS be laid out inline. Though col-xs-12 is pretty much equivalent to not having an XS class, right???????????/\n\n52\n\nChapter 4. Bootstrap\n\nCHAPTER\n\n5\n\nDebian\n\n5.1 Timezone Set system timezone: sudo dpkg-reconfigure tzdata\n\n5.2 Services update-rc.d: • Make a service run in its default runlevels: update-rc.d defaults\n\nor: update-rc.d enable\n\n• Make a service not run in any runlevel: update-rc.d disable\n\nMaking a new init script: • Read /etc/init.d/README, which will point to other docs • Copy /etc/init.d/skeleton and edit it.\n\n53\n\nDan’s Cheat Sheets Documentation, Release 1\n\n5.3 Packages • List packages that match a pattern: dpkg -l • List contents of a package: dpkg -L packagename • Show packages that installed files matching pattern: dpkg -S pattern • Show info about an installed package: dpkg-query -s packagename • show info about a package that is known: apt-cache showpkg packagename • Reconfigure a package: dpkg-reconfigure packagename • Change alternatives: update-alternatives ...\n\n5.4 Alternatives Change ‘alternatives’ default browser or editor: sudo update-alternatives --set x-www-browser /usr/bin/chromium-browser sudo update-alternatives --set editor /usr/bin/emacs24\n\nIf you get an error like: update-alternatives: error: alternative /snap/bin/firefox for x-www-browser not ˓→registered; not setting\n\nthen you can add the new alternative with: sudo update-alternatives --install /usr/bin/x-www-browser x-www-browser /snap/bin/ ˓→firefox 50\n\nand then try again. Be prompted for which alternative you prefer for a link group: sudo update-alternatives --config x-www-browser\n\nFind out what the top-level link groups are: sudo update-alternatives --get-selections\n\nSet xdg program to open/browse a directory (DOES NOT WORK) (do NOT use sudo): xdg-mime default /usr/share/applications/Thunar.desktop x-directory/normal\n\nChange ‘xdg’ default browser (for user): xdg-settings get default-web-browser xdg-settings set default-web-browser google-chrome.desktop xdg-settings set default-web-browser firefox.desktop\n\nInstall without any prompts (http://askubuntu.com/questions/146921/how-do-i-apt-get-y-dist-upgrade-without-a-grub-config-prompt): sudo DEBIAN_FRONTEND=noninteractive apt-get -y \\ -o Dpkg::Options::=\"--force-confdef\" -o Dpkg::Options::=\"--force-confold\" \\\n\n54\n\nChapter 5. Debian\n\nDan’s Cheat Sheets Documentation, Release 1\n\n5.5 Desktop applications Put your own .desktop files in ~/.local/share/applications. Archlinux on desktop entries Desktop file spec To let the system know about new or changed desktop files: update-desktop-database [directory]\n\nLaunch the application from command line that has a .desktop file somewhere: gtk-launch\n\n5.5. Desktop applications\n\n55\n\nDan’s Cheat Sheets Documentation, Release 1\n\n56\n\nChapter 5. Debian\n\nCHAPTER\n\n6\n\nDiet\n\n6.1 Foods with low glycemic index • Breads • Dense wholegrain breads • White corn tortillas • Grain and seed breads • Fruit Loaf such as Raisin • Multigrain breads (look for breads where you can see lots of grains) • Authentic Sourdough bread • Breakfast Cereals • Traditional porridge oats • Muesli* • Bircher Muesli • Wholegrain high fibre cereals • Vegetables • Sweetcorn • Silverbeet • Carrots • Zucchini • Peas, frozen or fresh • Snowpeas • CarismaTM Potatoes* 57\n\nDan’s Cheat Sheets Documentation, Release 1\n\n• Green Beans • Broccoli • Eggplant • Cauliflower • Squash • Capsicum • Salad Vegetables • Celery • Leeks • Tomatoes • Mushrooms – very low carb or no GI rating • Butternut Pumpkin (lower GI) • Avocadoes • Drinks • Milo® • Skim Latte • Sustagen® • Soy Drinks • Fruit Smoothies • Fruit Juice • Snacks • Grain & Fruit bars • Wholegrain crackers • Nut & Seed bars • Dried fruit and nuts • Legumes • Split Peas; Green or red Lentils • Baked Beans • Canned & Dried beans – kidney, cannellini, butter, borlotti, chickpeas • Spreads • Fruit Spreads • Hummus • Nut butters • Main Meal Carbs • Doongara Low GI White rice • Fresh Noodles – Hokkein, Udon, Rice\n\n58\n\nChapter 6. Diet\n\nDan’s Cheat Sheets Documentation, Release 1\n\n• Low GI Brown rice* • Soba Noodles • Basmati rice (lower GI) • Buckwheat • Pasta, cooked al dente* • Vermicelli • Pearl Couscous* • Bulgur • Quinoa* • Semolina • Pearl Barley • Cracked Wheat • Fruit • Apples* • Pears* • Bananas • Kiwi Fruit • Grapes* • Mango • Strawberries • Oranges • Peaches • Grapefruits • Apricots • Berries, fresh or frozen • Plums • Dried fruits such as prunes, raisins, sultanas, apricots • Canned Fruit in natural juice • Dairy Foods • Reduced fat milk • Reduced fat custard • Reduced fat yoghurt, plain or fruit flavoured • Low fat ice-cream*\n\n6.1. Foods with low glycemic index\n\n59\n\nDan’s Cheat Sheets Documentation, Release 1\n\n6.2 For lowering triglycerides • Decrease or eliminate: • Sweets • Alcohol • Refined carbohydrates: • White rice • bread and pasta made from white flour or semolina • Saturated fats and fried foods: • high fat meats • skin on poultry • sauces and spreads • Trans fatty acids and hidden fats: • hydrogenated vegetable oil • regular fat meats • lunchmeats • hot dogs • fatty snack foods • Eat more: • omega 3 fatty acids: • fatty fish • salmon • mackerel • sardines • tuna • trout • ground flax seed • flaxseed oil • soy products • legumes • walnuts • dark leafy green vegetables • high fiber foods: • beans • whole grains • ground flaxseed\n\n60\n\nChapter 6. Diet\n\nDan’s Cheat Sheets Documentation, Release 1\n\n• pumpkin seeds • rice bran • oat bran • fruits and vegetables • Eat more plant foods: Vegetable proteins such as • dried beans, • peas, and • soy products; • White poultry, prepared without the skin, is also a good source of protein without a lot of fat content.\n\n6.2. For lowering triglycerides\n\n61\n\nDan’s Cheat Sheets Documentation, Release 1\n\n62\n\nChapter 6. Diet\n\nCHAPTER\n\n7\n\nDjango\n\nThese are just things I always find myself looking up, so I try to make some notes of the most important parts that I can refer back to. Contents:\n\n7.1 Admin 7.1.1 URLs List {{ app_label }}_{{ model_name }}_changelist Change {{ app_label }}_{{ model_name }}_change object_id https://docs.djangoproject.com/en/stable/ref/contrib/admin/#reversing-admin-urls\n\n7.1.2 Customize top-right corner of admin pages Create your own templates/admin/base_site.html that comes ahead of the admin’s default one in the templates path. At least in Django 1.8+, this gives you a “View site” link for free: {% extends \"admin/base.html\" %} {% block title %}{{ title }} | {{ site_title|default:_('Django site admin') }}{% ˓→endblock %} {% block branding %} {{ site_header|default:_('Django ˓→administration') }} {% endblock %} {% block userlinks %} Clear cache / (continues on next page)\n\n63\n\nDan’s Cheat Sheets Documentation, Release 1\n\n(continued from previous page)\n\n{{ block.super }} {% endblock userlinks %}\n\nPrior to Django 1.8: {% extends \"admin/base.html\" %} {% block title %}{{ title }} | Caktus Admin{% endblock %} {% block branding %}Caktus Admin{% endblock %} {% block nav-global %}\n\nReturn to Caktus Home | Clear cache\n\n{% endblock %}\n\n7.2 Applications https://docs.djangoproject.com/en/stable/ref/applications/#django.apps.AppConfig In __init__.py: # programs/__init__.py default_app_config = 'programs.apps.ProgramsConfig'\n\nIn apps.py: # programs/apps.py from django.apps import AppConfig class ProgramsConfig(AppConfig): name = 'programs' # required: must be the Full dotted path to the app label = 'programs' # optional: app label, must be unique in Django project verbose_name = \"Rock ’n’ roll\" # optional def ready(): \"\"\" This runs after all models have been loaded, but you may not modify the database in here. Here's a trick to run something after each migration, which is often good enough. \"\"\" from django.db.models.signals import post_migrate post_migrate.connect(`callable`)\n\n64\n\nChapter 7. Django\n\nDan’s Cheat Sheets Documentation, Release 1\n\n7.3 Celery (Yes, I know Celery isn’t Django-specific.) http://docs.celeryproject.org/en/latest/\n\n7.3.1 Useful settings http://docs.celeryproject.org/en/latest/configuration.html CELERY_ALWAYS_EAGER: If this is True, all tasks will be executed locally by blocking until the task returns. apply_async() and Task.delay() will return an EagerResult instance, which emulates the API and behavior of AsyncResult, except the result is already evaluated. That is, tasks will be executed locally instead of being sent to the queue. CELERY_EAGER_PROPAGATES_EXCEPTIONS: If this is True, eagerly executed tasks (applied by task.apply(), or when the CELERY_ALWAYS_EAGER setting is enabled), will propagate exceptions. It’s the same as always running apply() with throw=True. CELERY_IGNORE_RESULT: Whether to store the task return values or not (tombstones). If you still want to store errors, just not successful return values, you can set CELERY_STORE_ERRORS_EVEN_IF_IGNORED. CELERYD_HIJACK_ROOT_LOGGER: By default any previously configured handlers on the root logger will be removed. If you want to customize your own logging handlers, then you can disable this behavior by setting CELERYD_HIJACK_ROOT_LOGGER = False. CELERYBEAT_SCHEDULE: In each task, you can add an ‘options’ dictionary and set ‘expires’ to a number of seconds. If the task doesn’t run within that time, it’ll be discarded rather than run when it finally gets to a worker. This can help a lot with periodic tasks when workers or the queue gets hung up for a while and then unjammed - without this, the workers will have to work through a huge backlog of the same periodic tasks over and over, for no reason. Example: CELERYBEAT_SCHEDULE = { 'process_new_scans': { 'task': 'tasks.process_new_scans', 'schedule': timedelta(minutes=15), 'options': { 'expires': 10*60, # 10 minutes } }, }\n\nCELERY_DEFAULT_QUEUE: In the absence of more complicated configuration, celery will use this queue name for everything. Handy when multiple instances of a site are sharing a queue manager: CELERY_DEFAULT_QUEUE = 'queue_%s' % INSTANCE\n\n7.4 django-compressor django-compressor docs Warning: much of the documentation is casual about saying things that are only true in some scenarios, without making clear that that’s the case.\n\n7.3. Celery\n\n65\n\nDan’s Cheat Sheets Documentation, Release 1\n\n7.4.1 ACTUALLY USING Here are some practical scenarios for using django-compressor. For what to put in your templates, you can go by the django-compressor documentation, and be sure to use {% static %} and not STATIC_URL. For what to put in your settings. . . it’s a lot more complicated. Set the compressor filters and precompilers however you want. For the rest, keep reading. Scenario: Development using runserver, DEBUG, not offline If DEBUG is True, then compressor won’t even do anything and so everything should just work. Scenario: Running using local files, not offline This is the typical small server situation. You unpack your project on the server, run collectstatic, point nginx or some other server at STATIC_ROOT and go. Example settings: # Django settings DEBUG = False STATIC_ROOT = '/var/www/static/' STATIC_URL = '/static/' # set compressor filters and precompilers as desired. # leave other compressor settings at defaults. # nginx settings location /static { alias /var/www/static; }\n\nScenario: running using local files, with offline Like the previous scenario, but you want compressor to do all its work at deploy time so the results are cached and ready to go immediately when you start your server. # Django settings like before, plus: COMPRESS_OFFLINE = True\n\nNow at deploy time you have more steps: $ python manage.py collectstatic $ python manage.py compress\n\nRun compress after collectstatic so that compressor can find its input files. It’ll write its output files under {STATIC_ROOT}/CACHE, and get them from there at runtime. Scenario: running with storage on the network, with offline In this scenario, you’re putting your static files somewhere off of the server where you’re running Django. For example, S3. Or just your own static file server somewhere. Whatever.\n\n66\n\nChapter 7. Django\n\nDan’s Cheat Sheets Documentation, Release 1\n\nLet’s start with how this would be setup without django-compressor, then we can modify it to add django-compressor. # settings/no_compressor.py STATIC_ROOT = None # Unused STATIC_URL = None # Unused STATIC_FILE_STORAGE = 'package.of.FileStorageClass'\n\nAt deploy time you can just run collectstatic, and all your static files will be pushed to the network: $ python manage.py collectstatic\n\nAnd at runtime, {% static %} will ask your file storage class to come up with a URL for each file, which will turn out to be on your other server, or S3, or whatever. Now, suppose we want to add compressor with offline processing (not using offline makes no sense with network storage). Here are the settings you can use at runtime for that, assuming things have been prepared correctly: # settings/deployed.py # Django settings we'll use in production STATIC_ROOT = None # Unused STATIC_URL = None # Unused STATIC_FILE_STORAGE = 'path.to.network.filestorage' COMPRESS_ENABLED = True COMPRESS_OFFLINE = True\n\nThe preparation is the tricky part. It turns out that for compress to work, a copy of the static files must be gathered in a local directory first. Most of the tools we might use to compile, compress, etc. are going to read local files and write local output. To gather the static files into a local directory, we might, for example, use a different settings file that uses the default file storage class, and run collectstatic. E.g.: # settings/gather.py # Django settings when first running collectstatic from .deployed import * # Override a few settings to make storage local STATIC_ROOT = '/path/to/tmp/dir' STATIC_URL = None # Unused STATIC_FILE_STORAGE = 'django.core.files.storage.FileSystemStorage' $ python manage.py collectstatic --settings=settings.gather\n\nAfter running collectstatic with these settings, all your source static files will be gathered under ‘/path/to/tmp/dir’. Now you could run compress, and the resulting files would be added under /path/to/tmp/dir. There’s an important gotcha that will cause problems, though - for compressor to match up the output it makes now with what it’ll be looking for later, the contents of each {% compress %} tag must be identical now to what it’ll be then, which means the URLs must point at the production file server. We can accomplish this by setting STATIC_URL before running the compress: # settings/compress.py # Django settings when running compress from .deployed import * # Override a few settings to make storage local, but URLs look remote STATIC_ROOT = '/path/to/tmp/dir' (continues on next page)\n\n7.4. django-compressor\n\n67\n\nDan’s Cheat Sheets Documentation, Release 1\n\n(continued from previous page)\n\nSTATIC_URL = 'https://something.s3.somewhere/static/' # URL prefix for runtime STATIC_FILE_STORAGE = 'django.core.files.storage.FileSystemStorage' $ python manage.py compress --settings=settings.compress\n\nThe problem now is to get all these files onto the remote server. You could just use rsync or s3cmd or something, which will work fine. But for maximum flexibility, let’s figure out a way to do it using Django. Our approach will be to tell Django that our SOURCE static files are in ‘/path/to/tmp/dir’, and we want them collected using our production file storage class, which will put them where we want them. # Django settings when running collectstatic again after compress, # to copy the resulting files to the network # settings/copy_upstream.py from .deployed import * # Set up for network file storage # Tell collectstatic to use the files we collected and compressed STATICFILES_FINDERS = ['django.contrib.staticfiles.finders.FileSystemFinder'] STATICFILES_DIRS = ['/path/to/tmp/dir'] $ python manage.py collectstatic --settings=settings.copy_upstream\n\nThat should copy things to the network. Then if you run using the ‘deployed’ settings, things should work! TODO: TEST THAT!!!!!!!!!!!!!!!!!!!! Other approaches The compressor docs suggest a different approach – hack the storage class you’re using so when you run collectstatic, it saves a copy of each file into a local directory in addition to pushing it upstream. Then you can use the same storage class for collectstatic, compress, and runtime.\n\n7.4.2 More detailed notes Cache For some things, compressor uses the cache named by COMPRESS_CACHE_BACKEND, which defaults to None, which gives us the default Django cache. Principles of compression Whether compressor is processing templates offline ahead of time or at runtime, there are some common principles. First, if COMPRESS_ENABLED is False, the {% compress %} tag will simply render as its contents; compressor won’t change anything. Otherwise, compressor will 1. parse the contents of the tag and figure out which css and javascript files would be included 2. fetch those files (See “accessing the files to be compressed”) 3. run those files through any configured preprocessors 4. concatenate the result and save it using COMPRESS_STORAGE\n\n68\n\nChapter 7. Django\n\nDan’s Cheat Sheets Documentation, Release 1\n\n5. at rendering, the tag and contents will be replaced with one or two HTML elements that will load the compressed file instead of the original ones. Offline If COMPRESS_OFFLINE is True, compressor expects all uses of {% compress ... %} in templates to have been pre-processed by running manage.py compress ahead of time, which puts the results in compressor’s offline cache. If anything it needs at run-time is not found there, things break/throw errors/render wrong etc. Note: If COMPRESS_OFFLINE is True and files have not been pre-compressed, compressor will not compress them at runtime. Things will break. The offline cache manifest is a json file, stored using COMPRESS_STORAGE, in the subdirectory COMPRESS_OUTPUT_DIR (default: CACHE), using the filename COMPRESS_OFFLINE_MANIFEST (default: manifest.json). The keys in the offline cache manifest are generated from the template content inside each compress tag, not the contents of the compressed files. So, you must arrange to re-run the offline compression anytime your content files might have changed, or it’ll be serving up compressed files generated from the old file contents. Note: It sounds like you must also be sure the contents of the compress tags don’t change between precompressing and runtime, for example by changing the URL prefix! The values in the offline cache manifest are paths of the compressed files in COMPRESS_STORAGE. Note: RECOMMENDATION FROM DOCS: make COMPRESS_OFFLINE_MANIFEST change depending on the current code revision, so that during deploys, servers running different versions of the code will each use the manifest appropriate for the version of the code they’re running. Otherwise, servers might use the wrong manifest and strange things could happen.\n\nNot offline If COMPRESS_OFFLINE is False, compressor will look in COMPRESS_STORAGE for previously processed results, but if not found, will create them on the fly and save them to use again. Storage Compressor uses a Django storage class for some of its operations, controlled by the setting COMPRESS_STORAGE. The default storage class is compressor.storage.CompressorFileStorage, which is a subclass of the standard filesystem storage class. It uses COMPRESS_ROOT as the base directory in the local filesystem to store files in, and builds URLs by prefixing file paths within the storage with COMPRESS_URL. If you change COMPRESS_STORAGE, then ignore anything in the docs about COMPRESS_ROOT and COMPRESS_URL as they won’t apply anymore (except in a few cases. . . see exceptions noted as they come up, below).\n\n7.4. django-compressor\n\n69\n\nDan’s Cheat Sheets Documentation, Release 1\n\nAccessing the files to be compressed For each file to be compressed, compressor starts with the URL from the rendered original content inside the compress tag. For example, if part of the content is , then it extracts \"http://example.com/foo.js\" as the URL. It checks that the URL starts with COMPRESS_STORAGE’s base_url, or if accessing that fails (quite possible since base_url is not a standard part of the file storage class API), uses COMPRESS_URL. Note: This is a place where compressor can use COMPRESS_URL even if it’s not using its default storage. If the URL doesn’t start with that string, compressor throws a possibly misleading error, “’%s’ isn’t accessible via COMPRESS_URL (‘%s’) and can’t be compressed”. Otherwise, compressor tries to come up with a local filepath to access the file, as follows: • Try to get a local filepath from COMPRESS_STORAGE using .path(). • If that’s not implemented (for example, for remote storages), it tries again using compressor.storage. CompressorFileStorage (regardless of what COMPRESS_STORAGE is set to), so basically it’s going to look for it under COMPRESS_ROOT. • If it still can’t get a local filepath, throws an error: “’%s’ could not be found in the COMPRESS_ROOT ‘%s’%s” which is very misleading if you’re not using a storage class that looks at COMPRESS_ROOT.\n\n7.5 Data fixtures Export/dump data to use as a fixture: python manage.py dumpdata --format=yaml --natural app.model >data.yaml\n\nLoad it again: python manage.py loaddata data.yaml\n\n7.5.1 Natural keys https://docs.djangoproject.com/en/stable/topics/serialization/#natural-keys from django.db import models class PersonManager(models.Manager): def get_by_natural_key(self, first_name, last_name): return self.get(first_name=first_name, last_name=last_name) class Person(models.Model): objects = PersonManager() ... def natural_key(self): return (self.first_name, self.last_name) class Meta: unique_together = (('first_name', 'last_name'),)\n\n70\n\nChapter 7. Django\n\nDan’s Cheat Sheets Documentation, Release 1\n\nDependencies If part of the natural key is a reference to another model, then that model needs to be deserialized first: class Book(models.Model): name = models.CharField(max_length=100) author = models.ForeignKey(Person) def natural_key(self): return (self.name,) + self.author.natural_key() natural_key.dependencies = ['example_app.person']\n\n7.6 Databases 7.6.1 Performance From Django 1.6 on, always add CONN_MAX_AGE to database settings to enable persistent connections. 300 is a good starting value (5 minutes). None will keep them open indefinitely. BUT - keep in mind that every open connection to Postgres consumes database server resources. So you might want instead to run pgbouncer locally.\n\n7.7 Django Debug Toolbar Install/config Install: pip install django-debug-toolbar\n\nsettings.py: DEBUG = True INTERNAL_IPS = ['127.0.0.1'] INSTALLED_APPS += [ 'debug_toolbar', ] # The order of MIDDLEWARE and MIDDLEWARE_CLASSES is important. You should include # the Debug Toolbar middleware as early as possible in the list. However, it must # come after any other middleware that encodes the response’s content, such as # GZipMiddleware. MIDDLEWARE = [ 'debug_toolbar.middleware.DebugToolbarMiddleware', ] + MIDDLEWARE\n\nurls.py: from django.conf import settings from django.conf.urls import include, url if settings.DEBUG: import debug_toolbar urlpatterns += [ (continues on next page)\n\n7.6. Databases\n\n71\n\nDan’s Cheat Sheets Documentation, Release 1\n\n(continued from previous page)\n\nurl(r'^__debug__/', include(debug_toolbar.urls)), ]\n\n7.8 Dokku Readying a Django project for deploying to Dokku. This lists the things to add or change to easily deploy a Django application to Dokku. It started out not trying to cover all of setting up a site on Dokku, only the parts relevant to a Django project – but it has grown. Still, you should read the Dokku getting started docs, then use this as a cheatsheet to quickly enable existing Django projects to deploy on Dokku. Start with the pages in this list, in order, then come back to this page and continue reading:\n\n7.8.1 Dokku server administration This page has information for those who have to set up and maintain a Dokku server. If you’re just using one, you can ignore this. Initial install The Dokku docs recommend setting up a new OS install of a supported operating system, then running the Dokku install script. Experience suggests that that approach is more likely to work than trying to install Dokku on a system that has already had some configuration done for other things. Simple hostnames The simple way to set up hostnames is: • Pick a hostname you can control, e.g. dokku.me. • During initial setup of Dokku, configure that as the server’s name. • Create a DNS A record pointing dokku.me at the server’s IP address. • Add a wildcard entry for *.dokku.me at the same address. • For each app you put on that server, give the app the same name you want to use for its subdomain. For example, an app named foo would be accessible on the internet at foo.dokku.me, without having to make any more changes to your DNS settings. Managing users In other words, who can mess with the apps on a dokku server? The way this currently works is that everybody ends up sshing to the server as the dokku user to do things. To let them do that, we want to add a public key for them to the dokku config, by doing this (from any system): $ cat /path/to/ssh_keyfile.pub | ssh dokku ssh-keys:add\n\n72\n\nChapter 7. Django\n\nDan’s Cheat Sheets Documentation, Release 1\n\nThe is just to identify the different keys. I suggest using the person’s typical username. Just remember there will not be a user of that name on the dokku server. When it’s time to revoke someone’s access: $ ssh dokku ssh_keys:remove\n\nand now you see why the is useful. For now, there’s not a simple way to limit particular users to particular apps or commands.\n\n7.8.2 Files Setting up files in a Django project for deploying it to Dokku requirements.txt There needs to be a requirements.txt file at the top level. If you prefer to keep your requirements somewhere else, the top-level one can just look like: -r path/to/real/requirements.txt\n\nWherever your requirements are, add the latest versions of: dj-database-url gunicorn whitenoise\n\nsettings Add a .../deploy.py settings file, e.g. /settings/deploy.py. It can start out looking like this (edit the top line if your main settings file isn’t base.py): # Settings when deployed to Dokku from .base import * # noqa import dj_database_url # Disable Django's own staticfiles handling in favour of WhiteNoise, for # greater consistency between gunicorn and `./manage.py runserver`. See: # http://whitenoise.evans.io/en/stable/django.html#using-whitenoise-in-development INSTALLED_APPS.remove('django.contrib.staticfiles') INSTALLED_APPS.extend([ 'whitenoise.runserver_nostatic', 'django.contrib.staticfiles', ]) MIDDLEWARE.remove('django.middleware.security.SecurityMiddleware') MIDDLEWARE = [ 'django.middleware.security.SecurityMiddleware', 'whitenoise.middleware.WhiteNoiseMiddleware', ] + MIDDLEWARE # Update database configuration with $DATABASE_URL. db_from_env = dj_database_url.config(conn_max_age=500) (continues on next page)\n\n7.8. Dokku\n\n73\n\nDan’s Cheat Sheets Documentation, Release 1\n\n(continued from previous page)\n\nDATABASES['default'].update(db_from_env) # Honor the 'X-Forwarded-Proto' header for request.is_secure() SECURE_PROXY_SSL_HEADER = ('HTTP_X_FORWARDED_PROTO', 'https') # Allow all host headers (feel free to make this more specific) ALLOWED_HOSTS = ['*'] # Simplified static file serving. # https://warehouse.python.org/project/whitenoise/ STATICFILES_STORAGE = 'whitenoise.storage.CompressedManifestStaticFilesStorage'\n\nwsgi.py Find your wsgi.py file. 1. Edit to change the default settings module to .settings.deploy (the path to the new settings file you created above). 2. Add to the end: from whitenoise.django import DjangoWhiteNoise application = DjangoWhiteNoise(application)\n\nProcfile Create Procfile (more on dokku Procfile) in the top directory. For our simple case, it can just contain one line, starting with web: and containing the command to start gunicorn for our site: web: gunicorn {{ project_name }}.wsgi\n\nSee also the section on running Celery and other processes. runtime.txt Create runtime.txt in the top directory. It only needs one line, e.g.: python-3.6.1\n\nThis has to be specific. E.g. python-3.5.2 or python-3.6.1 might work if the dokku server supports it, but python-3.5 or python-3.6 probably won’t. app.json Create app.json in the top-level project directory. You might see examples on the Interwebs with lots of things in app.json (because Heroku uses app.json for lots of things), but as of this writing, dokku ignores everything but scripts.dokku.predeploy and scripts.dokku.postdeploy. Example: { \"scripts\": { \"dokku\": { (continues on next page)\n\n74\n\nChapter 7. Django\n\nDan’s Cheat Sheets Documentation, Release 1\n\n(continued from previous page)\n\n\"predeploy\": \"python manage.py migrate --noinput\" } } }\n\nNote: Dokku automatically runs collectstatic for you, so you don’t need to do that from app.json.\n\nbuildpacks If your app is not pure Python - e.g. if it uses node - you’ll need to override the automatic buildpack detection, because it only works for a single application type. Do this by adding a top-level .buildpacks file, containing links to the buildpacks to use: https://github.com/heroku/heroku-buildpack-nodejs.git https://github.com/heroku/heroku-buildpack-python.git https://github.com/heroku/heroku-buildpack-apt\n\nHeroku maintains a list of buildpacks.\n\n7.8.3 Postgres with Dokku There’s nothing Django-specific about this, but I’m including it just because we probably want to do it on every single Django deploy. To install the postgresql plugin, inside your server run (because plugins must be installed as root): $ sudo dokku plugin:install https://github.com/dokku/dokku-postgres.git\n\nNow you need to create a database, and link the database to the app. You can do this from your own system: $ ssh dokku postgres:create example-database $ ssh dokku postgres:link example-database django-tutorial\n\nNow when dokku runs your app, it’ll set an env var to tell it where its DB is, e.g.: DATABASE_URL=postgres://user:pass"
    }
}
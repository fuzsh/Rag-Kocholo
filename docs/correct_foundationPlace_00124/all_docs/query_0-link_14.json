{
    "id": "correct_foundationPlace_00124_0",
    "rank": 14,
    "data": {
        "url": "https://github.com/maqboolkhan/Kazi",
        "read_more_link": "",
        "language": "en",
        "title": "Uni Paderborn Semester Project",
        "top_image": "https://opengraph.githubassets.com/96e37da9e0019430098602307d3f481023d0c7499ea3c1c9a9977d7e6191a40d/maqboolkhan/Kazi",
        "meta_img": "https://opengraph.githubassets.com/96e37da9e0019430098602307d3f481023d0c7499ea3c1c9a9977d7e6191a40d/maqboolkhan/Kazi",
        "images": [],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "Kazi - The naive fact checker - Uni Paderborn Semester Project  - GitHub - maqboolkhan/Kazi: Kazi - The naive fact checker - Uni Paderborn Semester Project",
        "meta_lang": "en",
        "meta_favicon": "https://github.com/fluidicon.png",
        "meta_site_name": "GitHub",
        "canonical_link": "https://github.com/maqboolkhan/Kazi",
        "text": "The naive fact checker based on Wikipedia. Using NLTK and Wekipedia Python packages.\n\nFaktenpr√ºfung\n\nName Matriculation Number Maqbool Ur Rahim Khan 6843364 Ali Azmi 6844310 Muhammad Hamza 6845542\n\nPython3\n\nPip3\n\nOur setup uses Python3 which is aliased as python3. If Python 3 command is not available as python3 then setup.py won't work.\n\nIn order to execute the training data set, please run the following command.\n\nIn order to execute test, please run the following command.\n\nPlease make sure that the spellings of train and test are correct.\n\nFirst of all, we perform Name Entity (NE) on each sentence using NLTK, but NE done by NLTK is not perfect. Therefore, we wrote our own rules for NE in our project.\n\nConsider the following sample sentence taken from the train.tsv file:\n\nAfter performing NE, we will get Valve Corporation's and Kirkland, Washington. As these sentences are quite straight forward, so we will assume the first token as Subject and the second as Object. Now, the remaining tokens are place, is and here you can easily see that after removing the stop word is, we are left with place, and so we can assume it to be the Predicate.\n\nNow we take the Subject and search its corresponding Wikipedia page. Once we have the page, we seach our Object in that respective page. If the search result returns true, we consider our fact as TRUE and if the search result returns false, we consider our fact as FALSE.\n\nSince the approach is Naive, one can simply question; why don't we use predicate to check our facts?\n\nAnd the reason is each fact or sentence can be written in multiple ways.\n\nExample:\n\nQuaid-e-Azam was born in Pakistan (true fact)\n\nthis sentence can also be written as:\n\nThe birth place of Quaid-e-Azam is Pakistan (true fact)\n\nThe Predicate born and birth place are quite different. It's difficult to know what predicate is available on Wikipedia, there is a possibility of having a synoynym of the same word.\n\nSecondly, the sentences on Wikipedia are not that simple as compared to our data and the distance between the Subject, Object and Predicate can make a sentence evaluation quite unreliable and without using predicate, surprisingly our project gives 65% of precision.\n\nIn this case we are simply returning false!!!\n\nFollowing are the sample run of the trained data set.\n\nQuaid-e-Azam was born in India\n\nAnsoo Lake is situated in Sindh province\n\nQuetta is the capital city of Pakistan"
    }
}
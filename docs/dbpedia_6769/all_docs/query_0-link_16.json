{
    "id": "dbpedia_6769_0",
    "rank": 16,
    "data": {
        "url": "https://worldwidescience.org/topicpages/i/intact%2Bsensory%2Bmodalities.html",
        "read_more_link": "",
        "language": "en",
        "title": "intact sensory modalities: Topics by WorldWideScience.org",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://worldwidescience.org/sites/www.osti.gov/files/public/image-files/WWSlogo_wTag650px-min.png",
            "https://worldwidescience.org/sites/www.osti.gov/files/public/image-files/OSTIlogo.svg",
            "https://worldwidescience.org/sites/www.osti.gov/files/public/image-files/ICSTIlogo.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Acquired auditory-visual synesthesia: A window to early cross-modal sensory interactions\n\nDirectory of Open Access Journals (Sweden)\n\nPegah Afra\n\n2009-01-01\n\nFull Text Available Pegah Afra, Michael Funke, Fumisuke MatsuoDepartment of Neurology, University of Utah, Salt Lake City, UT, USAAbstract: Synesthesia is experienced when sensory stimulation of one sensory modality elicits an involuntary sensation in another sensory modality. Auditory-visual synesthesia occurs when auditory stimuli elicit visual sensations. It has developmental, induced and acquired varieties. The acquired variety has been reported in association with deafferentation of the visual system as well as temporal lobe pathology with intact visual pathways. The induced variety has been reported in experimental and post-surgical blindfolding, as well as intake of hallucinogenic or psychedelics. Although in humans there is no known anatomical pathway connecting auditory areas to primary and/or early visual association areas, there is imaging and neurophysiologic evidence to the presence of early cross modal interactions between the auditory and visual sensory pathways. Synesthesia may be a window of opportunity to study these cross modal interactions. Here we review the existing literature in the acquired and induced auditory-visual synesthesias and discuss the possible neural mechanisms.Keywords: synesthesia, auditory-visual, cross modal\n\nExperienced Sensory Modalities in Dream Recall\n\nOpenAIRE\n\nå²¡ç°, æ\n\n2000-01-01\n\nThe purpose of the present study is to survey the frequency of visual, auditory, kinaesthetic, cutaneous, organic, gustatory, and olfactory experience in dream recall. A total of 1267 undergraduate students completed a dream recall frequency questionnaire, which contained a question about dream recall frequency and about recall frequency of seven sensory modalities. Results showed that seven sensory modalities were divided into two groups; normally perceived sensory modalities in dreaming, wh...\n\nMixed-Modality Stimulation to Evoke Two Modalities Simultaneously in One Channel for Electrocutaneous Sensory Feedback.\n\nScience.gov (United States)\n\nChoi, Kyunghwan; Kim, Pyungkang; Kim, Kyung-Soo; Kim, Soohyun\n\n2017-12-01\n\nOne of the long-standing challenges in upper limb prosthetics is restoring the sensory feedback that is missing due to amputation. Two approaches have previously been presented to provide various types of sensory information to users, namely, multi-modality sensory feedback and using an array of single-modality stimulators. However, the feedback systems used in these approaches were too bulky to be embedded in prosthesis sockets. In this paper, we propose an electrocutaneous sensory feedback method that is capable of conveying two modalities simultaneously with only one electrode. The stimulation method, which we call mixed-modality stimulation, utilizes the phenomenon in which the superposition of two electric pulse trains of different frequencies is able to evoke two different modalities (i.e., pressure and tapping) at the same time. We conducted psychophysical experiments in which healthy subjects were required to recognize the intensity of pressure or the frequency of tapping from mixed-modality or two-channel stimulations. The results demonstrated that the subjects were able to discriminate the features of the two modalities in one electrode during mixed-modality stimulation and that the accuracies of successful recognitions (mean Â± standard deviation) for the two feedback variables were 84.3 Â± 7% for mixed-modality stimulation and 89.5 Â± 6% for two-channel dual-modality stimulation, showing no statistically significant difference. Therefore, mixed-modality stimulation is an attractive method for modulating two modalities independently with only one electrode, and it could be used for implementing a compact sensory feedback system that is able to provide two different types of sensory information from prosthetics.\n\nDorsal and ventral streams across sensory modalities\n\nInstitute of Scientific and Technical Information of China (English)\n\nAnna Sedda; Federica Scarpina\n\n2012-01-01\n\nIn this review,we describe the current models of dorsal and ventral streams in vision,audition and touch.Available theories take their first steps from the model of Milner and Goodale,which was developed to explain how human actions can be efficiently carried out using visual information.Since then,similar concepts have also been applied to other sensory modalities.We propose that advances in the knowledge of brain functioning can be achieved through models explaining action and perception patterns independently from sensory modalities.\n\nMultistability in perception: binding sensory modalities, an overview\n\nScience.gov (United States)\n\nSchwartz, Jean-Luc; Grimault, Nicolas; HupÃ©, Jean-Michel; Moore, Brian C. J.; Pressnitzer, Daniel\n\n2012-01-01\n\nThis special issue presents research concerning multistable perception in different sensory modalities. Multistability occurs when a single physical stimulus produces alternations between different subjective percepts. Multistability was first described for vision, where it occurs, for example, when different stimuli are presented to the two eyes or for certain ambiguous figures. It has since been described for other sensory modalities, including audition, touch and olfaction. The key features of multistability are: (i) stimuli have more than one plausible perceptual organization; (ii) these organizations are not compatible with each other. We argue here that most if not all cases of multistability are based on competition in selecting and binding stimulus information. Binding refers to the process whereby the different attributes of objects in the environment, as represented in the sensory array, are bound together within our perceptual systems, to provide a coherent interpretation of the world around us. We argue that multistability can be used as a method for studying binding processes within and across sensory modalities. We emphasize this theme while presenting an outline of the papers in this issue. We end with some thoughts about open directions and avenues for further research. PMID:22371612\n\nMultistability in perception: binding sensory modalities, an overview.\n\nScience.gov (United States)\n\nSchwartz, Jean-Luc; Grimault, Nicolas; HupÃ©, Jean-Michel; Moore, Brian C J; Pressnitzer, Daniel\n\n2012-04-05\n\nThis special issue presents research concerning multistable perception in different sensory modalities. Multistability occurs when a single physical stimulus produces alternations between different subjective percepts. Multistability was first described for vision, where it occurs, for example, when different stimuli are presented to the two eyes or for certain ambiguous figures. It has since been described for other sensory modalities, including audition, touch and olfaction. The key features of multistability are: (i) stimuli have more than one plausible perceptual organization; (ii) these organizations are not compatible with each other. We argue here that most if not all cases of multistability are based on competition in selecting and binding stimulus information. Binding refers to the process whereby the different attributes of objects in the environment, as represented in the sensory array, are bound together within our perceptual systems, to provide a coherent interpretation of the world around us. We argue that multistability can be used as a method for studying binding processes within and across sensory modalities. We emphasize this theme while presenting an outline of the papers in this issue. We end with some thoughts about open directions and avenues for further research.\n\nThe contribution of cutaneous and kinesthetic sensory modalities in haptic perception of orientation.\n\nScience.gov (United States)\n\nFrisoli, Antonio; Solazzi, Massimiliano; Reiner, Miriam; Bergamasco, Massimo\n\n2011-06-30\n\nThe aim of this study was to understand the integration of cutaneous and kinesthetic sensory modalities in haptic perception of shape orientation. A specific robotic apparatus was employed to simulate the exploration of virtual surfaces by active touch with two fingers, with kinesthetic only, cutaneous only and combined sensory feedback. The cutaneous feedback was capable of displaying the local surface orientation at the contact point, through a small plate indenting the fingerpad at contact. A psychophysics test was conducted with SDT methodology on 6 subjects to assess the discrimination threshold of angle perception between two parallel surfaces, with three sensory modalities and two shape sizes. Results show that the cutaneous sensor modality is not affected by size of shape, but kinesthetic performance is decreasing with smaller size. Cutaneous and kinesthetic sensory cues are integrated according to a Bayesian model, so that the combined sensory stimulation always performs better than single modalities alone. Copyright Â© 2010 Elsevier Inc. All rights reserved.\n\nPerception of Scenes in Different Sensory Modalities: A Result of Modal Completion.\n\nScience.gov (United States)\n\nGruber, Ronald R; Block, Richard A\n\n2017-01-01\n\nDynamic perception includes amodal and modal completion, along with apparent movement. It fills temporal gaps for single objects. In 2 experiments, using 6 stimulus presentation conditions involving 3 sensory modalities, participants experienced 8-10 sequential stimuli (200 ms each) with interstimulus intervals (ISIs) of 0.25-7.0 s. Experiments focused on spatiotemporal completion (walking), featural completion (object changing), auditory, completion (falling bomb), and haptic changes (insect crawling). After each trial, participants judged whether they experienced the process of \"happening \" or whether they simply knew that the process must have occurred. The phenomenon was frequency independent, being reported at short ISIs but not at long ISIs. The phenomenon involves dynamic modal completion and possibly also conceptual processes.\n\nFrom Sensory Signals to Modality-Independent Conceptual Representations: A Probabilistic Language of Thought Approach.\n\nDirectory of Open Access Journals (Sweden)\n\nGoker Erdogan\n\n2015-11-01\n\nFull Text Available People learn modality-independent, conceptual representations from modality-specific sensory signals. Here, we hypothesize that any system that accomplishes this feat will include three components: a representational language for characterizing modality-independent representations, a set of sensory-specific forward models for mapping from modality-independent representations to sensory signals, and an inference algorithm for inverting forward models-that is, an algorithm for using sensory signals to infer modality-independent representations. To evaluate this hypothesis, we instantiate it in the form of a computational model that learns object shape representations from visual and/or haptic signals. The model uses a probabilistic grammar to characterize modality-independent representations of object shape, uses a computer graphics toolkit and a human hand simulator to map from object representations to visual and haptic features, respectively, and uses a Bayesian inference algorithm to infer modality-independent object representations from visual and/or haptic signals. Simulation results show that the model infers identical object representations when an object is viewed, grasped, or both. That is, the model's percepts are modality invariant. We also report the results of an experiment in which different subjects rated the similarity of pairs of objects in different sensory conditions, and show that the model provides a very accurate account of subjects' ratings. Conceptually, this research significantly contributes to our understanding of modality invariance, an important type of perceptual constancy, by demonstrating how modality-independent representations can be acquired and used. Methodologically, it provides an important contribution to cognitive modeling, particularly an emerging probabilistic language-of-thought approach, by showing how symbolic and statistical approaches can be combined in order to understand aspects of human perception.\n\nUni-, bi- and tri-modal warning signals: effects of temporal parameters and sensory modality on perceived urgency\n\nNARCIS (Netherlands)\n\nvan Erp, Johannes Bernardus Fransiscus; Toet, Alexander; Janssen, Joris B.\n\nMulti-sensory warnings can potentially enhance risk communication. Hereto we investigated how temporal signal parameters affect perceived urgency within and across modalities. In an experiment, 78 observers rated the perceived urgency of uni-, bi-, and/or tri-modal stimuli as function of 25\n\nUni-, bi- and tri-modal warning signals : Effects of temporal parameters and sensory modality on perceived urgency\n\nNARCIS (Netherlands)\n\nErp, J.B.F. van; Toet, A.; Janssen, J.B.\n\n2015-01-01\n\nMulti-sensory warnings can potentially enhance risk communication. Hereto we investigated how temporal signal parameters affect perceived urgency within and across modalities. In an experiment, 78 observers rated the perceived urgency of uni-, bi-, and/or tri-modal stimuli as function of 25\n\nAutomatic selective attention as a function of sensory modality in aging.\n\nScience.gov (United States)\n\nGuerreiro, Maria J S; Adam, Jos J; Van Gerven, Pascal W M\n\n2012-03-01\n\nIt was recently hypothesized that age-related differences in selective attention depend on sensory modality (Guerreiro, M. J. S., Murphy, D. R., & Van Gerven, P. W. M. (2010). The role of sensory modality in age-related distraction: A critical review and a renewed view. Psychological Bulletin, 136, 975-1022. doi:10.1037/a0020731). So far, this hypothesis has not been tested in automatic selective attention. The current study addressed this issue by investigating age-related differences in automatic spatial cueing effects (i.e., facilitation and inhibition of return [IOR]) across sensory modalities. Thirty younger (mean age = 22.4 years) and 25 older adults (mean age = 68.8 years) performed 4 left-right target localization tasks, involving all combinations of visual and auditory cues and targets. We used stimulus onset asynchronies (SOAs) of 100, 500, 1,000, and 1,500 ms between cue and target. The results showed facilitation (shorter reaction times with valid relative to invalid cues at shorter SOAs) in the unimodal auditory and in both cross-modal tasks but not in the unimodal visual task. In contrast, there was IOR (longer reaction times with valid relative to invalid cues at longer SOAs) in both unimodal tasks but not in either of the cross-modal tasks. Most important, these spatial cueing effects were independent of age. The results suggest that the modality hypothesis of age-related differences in selective attention does not extend into the realm of automatic selective attention.\n\nSensory-specific satiety is intact in amnesics who eat multiple meals.\n\nScience.gov (United States)\n\nHiggs, Suzanne; Williamson, Amy C; Rotshtein, Pia; Humphreys, Glyn W\n\n2008-07-01\n\nWhat is the relationship between memory and appetite? We explored this question by examining preferences for recently consumed food in patients with amnesia. Although the patients were unable to remember having eaten, and were inclined to eat multiple meals, we found that sensory-specific satiety was intact in these patients. The data suggest that sensory-specific satiety can occur in the absence of explicit memory for having eaten and that impaired sensory-specific satiety does not underlie the phenomenon of multiple-meal eating in amnesia. Overeating in amnesia may be due to disruption of learned control by physiological aftereffects of a recent meal or to problems utilizing internal cues relating to nutritional state.\n\nParallel pathways for cross-modal memory retrieval in Drosophila.\n\nScience.gov (United States)\n\nZhang, Xiaonan; Ren, Qingzhong; Guo, Aike\n\n2013-05-15\n\nMemory-retrieval processing of cross-modal sensory preconditioning is vital for understanding the plasticity underlying the interactions between modalities. As part of the sensory preconditioning paradigm, it has been hypothesized that the conditioned response to an unreinforced cue depends on the memory of the reinforced cue via a sensory link between the two cues. To test this hypothesis, we studied cross-modal memory-retrieval processing in a genetically tractable model organism, Drosophila melanogaster. By expressing the dominant temperature-sensitive shibire(ts1) (shi(ts1)) transgene, which blocks synaptic vesicle recycling of specific neural subsets with the Gal4/UAS system at the restrictive temperature, we specifically blocked visual and olfactory memory retrieval, either alone or in combination; memory acquisition remained intact for these modalities. Blocking the memory retrieval of the reinforced olfactory cues did not impair the conditioned response to the unreinforced visual cues or vice versa, in contrast to the canonical memory-retrieval processing of sensory preconditioning. In addition, these conditioned responses can be abolished by blocking the memory retrieval of the two modalities simultaneously. In sum, our results indicated that a conditioned response to an unreinforced cue in cross-modal sensory preconditioning can be recalled through parallel pathways.\n\nAccommodating Students' Sensory Learning Modalities in Online Formats\n\nScience.gov (United States)\n\nAllison, Barbara N.; Rehm, Marsha L.\n\n2016-01-01\n\nOnline classes have become a popular and viable method of educating students in both K-12 settings and higher education, including in family and consumer sciences (FCS) programs. Online learning dramatically affects the way students learn. This article addresses how online learning can accommodate the sensory learning modalities (sight, hearing,â¦\n\nCommon Sense in Choice: The Effect of Sensory Modality on Neural Value Representations\n\nScience.gov (United States)\n\n2018-01-01\n\nAbstract Although it is well established that the ventromedial prefrontal cortex (vmPFC) represents value using a common currency across categories of rewards, it is unknown whether the vmPFC represents value irrespective of the sensory modality in which alternatives are presented. In the current study, male and female human subjects completed a decision-making task while their neural activity was recorded using functional magnetic resonance imaging. On each trial, subjects chose between a safe alternative and a lottery, which was presented visually or aurally. A univariate conjunction analysis revealed that the anterior portion of the vmPFC tracks subjective value (SV) irrespective of the sensory modality. Using a novel cross-modality multivariate classifier, we were able to decode auditory value based on visual trials and vice versa. In addition, we found that the visual and auditory sensory cortices, which were identified using functional localizers, are also sensitive to the value of stimuli, albeit in a modality-specific manner. Whereas both primary and higher-order auditory cortices represented auditory SV (aSV), only a higher-order visual area represented visual SV (vSV). These findings expand our understanding of the common currency network of the brain and shed a new light on the interplay between sensory and value information processing. PMID:29619408\n\nCommon Sense in Choice: The Effect of Sensory Modality on Neural Value Representations.\n\nScience.gov (United States)\n\nShuster, Anastasia; Levy, Dino J\n\n2018-01-01\n\nAlthough it is well established that the ventromedial prefrontal cortex (vmPFC) represents value using a common currency across categories of rewards, it is unknown whether the vmPFC represents value irrespective of the sensory modality in which alternatives are presented. In the current study, male and female human subjects completed a decision-making task while their neural activity was recorded using functional magnetic resonance imaging. On each trial, subjects chose between a safe alternative and a lottery, which was presented visually or aurally. A univariate conjunction analysis revealed that the anterior portion of the vmPFC tracks subjective value (SV) irrespective of the sensory modality. Using a novel cross-modality multivariate classifier, we were able to decode auditory value based on visual trials and vice versa. In addition, we found that the visual and auditory sensory cortices, which were identified using functional localizers, are also sensitive to the value of stimuli, albeit in a modality-specific manner. Whereas both primary and higher-order auditory cortices represented auditory SV (aSV), only a higher-order visual area represented visual SV (vSV). These findings expand our understanding of the common currency network of the brain and shed a new light on the interplay between sensory and value information processing.\n\nIdentification and integration of sensory modalities: Neural basis and relation to consciousness\n\nNARCIS (Netherlands)\n\nPennartz, C.M.A.\n\n2009-01-01\n\nA key question in studying consciousness is how neural operations in the brain can identify streams of sensory input as belonging to distinct modalities, which contributes to the representation of qualitatively different experiences. The basis for identification of modalities is proposed to be\n\nSensory modality of smoking cues modulates neural cue reactivity.\n\nScience.gov (United States)\n\nYalachkov, Yavor; Kaiser, Jochen; GÃ¶rres, Andreas; Seehaus, Arne; Naumer, Marcus J\n\n2013-01-01\n\nBehavioral experiments have demonstrated that the sensory modality of presentation modulates drug cue reactivity. The present study on nicotine addiction tested whether neural responses to smoking cues are modulated by the sensory modality of stimulus presentation. We measured brain activation using functional magnetic resonance imaging (fMRI) in 15 smokers and 15 nonsmokers while they viewed images of smoking paraphernalia and control objects and while they touched the same objects without seeing them. Haptically presented, smoking-related stimuli induced more pronounced neural cue reactivity than visual cues in the left dorsal striatum in smokers compared to nonsmokers. The severity of nicotine dependence correlated positively with the preference for haptically explored smoking cues in the left inferior parietal lobule/somatosensory cortex, right fusiform gyrus/inferior temporal cortex/cerebellum, hippocampus/parahippocampal gyrus, posterior cingulate cortex, and supplementary motor area. These observations are in line with the hypothesized role of the dorsal striatum for the expression of drug habits and the well-established concept of drug-related automatized schemata, since haptic perception is more closely linked to the corresponding object-specific action pattern than visual perception. Moreover, our findings demonstrate that with the growing severity of nicotine dependence, brain regions involved in object perception, memory, self-processing, and motor control exhibit an increasing preference for haptic over visual smoking cues. This difference was not found for control stimuli. Considering the sensory modality of the presented cues could serve to develop more reliable fMRI-specific biomarkers, more ecologically valid experimental designs, and more effective cue-exposure therapies of addiction.\n\nPerceptual load interacts with stimulus processing across sensory modalities.\n\nScience.gov (United States)\n\nKlemen, J; BÃ¼chel, C; Rose, M\n\n2009-06-01\n\nAccording to perceptual load theory, processing of task-irrelevant stimuli is limited by the perceptual load of a parallel attended task if both the task and the irrelevant stimuli are presented to the same sensory modality. However, it remains a matter of debate whether the same principles apply to cross-sensory perceptual load and, more generally, what form cross-sensory attentional modulation in early perceptual areas takes in humans. Here we addressed these questions using functional magnetic resonance imaging. Participants undertook an auditory one-back working memory task of low or high perceptual load, while concurrently viewing task-irrelevant images at one of three object visibility levels. The processing of the visual and auditory stimuli was measured in the lateral occipital cortex (LOC) and auditory cortex (AC), respectively. Cross-sensory interference with sensory processing was observed in both the LOC and AC, in accordance with previous results of unisensory perceptual load studies. The present neuroimaging results therefore warrant the extension of perceptual load theory from a unisensory to a cross-sensory context: a validation of this cross-sensory interference effect through behavioural measures would consolidate the findings.\n\nLarge-Scale Brain Networks Supporting Divided Attention across Spatial Locations and Sensory Modalities.\n\nScience.gov (United States)\n\nSantangelo, Valerio\n\n2018-01-01\n\nHigher-order cognitive processes were shown to rely on the interplay between large-scale neural networks. However, brain networks involved with the capability to split attentional resource over multiple spatial locations and multiple stimuli or sensory modalities have been largely unexplored to date. Here I re-analyzed data from Santangelo et al. (2010) to explore the causal interactions between large-scale brain networks during divided attention. During fMRI scanning, participants monitored streams of visual and/or auditory stimuli in one or two spatial locations for detection of occasional targets. This design allowed comparing a condition in which participants monitored one stimulus/modality (either visual or auditory) in two spatial locations vs. a condition in which participants monitored two stimuli/modalities (both visual and auditory) in one spatial location. The analysis of the independent components (ICs) revealed that dividing attentional resources across two spatial locations necessitated a brain network involving the left ventro- and dorso-lateral prefrontal cortex plus the posterior parietal cortex, including the intraparietal sulcus (IPS) and the angular gyrus, bilaterally. The analysis of Granger causality highlighted that the activity of lateral prefrontal regions were predictive of the activity of all of the posteriors parietal nodes. By contrast, dividing attention across two sensory modalities necessitated a brain network including nodes belonging to the dorsal frontoparietal network, i.e., the bilateral frontal eye-fields (FEF) and IPS, plus nodes belonging to the salience network, i.e., the anterior cingulated cortex and the left and right anterior insular cortex (aIC). The analysis of Granger causality highlights a tight interdependence between the dorsal frontoparietal and salience nodes in trials requiring divided attention between different sensory modalities. The current findings therefore highlighted a dissociation among brain networks\n\nLarge-Scale Brain Networks Supporting Divided Attention across Spatial Locations and Sensory Modalities\n\nDirectory of Open Access Journals (Sweden)\n\nValerio Santangelo\n\n2018-02-01\n\nFull Text Available Higher-order cognitive processes were shown to rely on the interplay between large-scale neural networks. However, brain networks involved with the capability to split attentional resource over multiple spatial locations and multiple stimuli or sensory modalities have been largely unexplored to date. Here I re-analyzed data from Santangelo et al. (2010 to explore the causal interactions between large-scale brain networks during divided attention. During fMRI scanning, participants monitored streams of visual and/or auditory stimuli in one or two spatial locations for detection of occasional targets. This design allowed comparing a condition in which participants monitored one stimulus/modality (either visual or auditory in two spatial locations vs. a condition in which participants monitored two stimuli/modalities (both visual and auditory in one spatial location. The analysis of the independent components (ICs revealed that dividing attentional resources across two spatial locations necessitated a brain network involving the left ventro- and dorso-lateral prefrontal cortex plus the posterior parietal cortex, including the intraparietal sulcus (IPS and the angular gyrus, bilaterally. The analysis of Granger causality highlighted that the activity of lateral prefrontal regions were predictive of the activity of all of the posteriors parietal nodes. By contrast, dividing attention across two sensory modalities necessitated a brain network including nodes belonging to the dorsal frontoparietal network, i.e., the bilateral frontal eye-fields (FEF and IPS, plus nodes belonging to the salience network, i.e., the anterior cingulated cortex and the left and right anterior insular cortex (aIC. The analysis of Granger causality highlights a tight interdependence between the dorsal frontoparietal and salience nodes in trials requiring divided attention between different sensory modalities. The current findings therefore highlighted a dissociation among\n\nEffects of Arousal on Mouse Sensory Cortex Depend on Modality\n\nDirectory of Open Access Journals (Sweden)\n\nDaisuke Shimaoka\n\n2018-03-01\n\nFull Text Available Summary: Changes in arousal modulate the activity of mouse sensory cortex, but studies in different mice and different sensory areas disagree on whether this modulation enhances or suppresses activity. We measured this modulation simultaneously in multiple cortical areas by imaging mice expressing voltage-sensitive fluorescent proteins (VSFP. VSFP imaging estimates local membrane potential across large portions of cortex. We used temporal filters to predict local potential from running speed or from pupil dilation, two measures of arousal. The filters provided good fits and revealed that the effects of arousal depend on modality. In the primary visual cortex (V1 and auditory cortex (Au, arousal caused depolarization followed by hyperpolarization. In the barrel cortex (S1b and a secondary visual area (LM, it caused only hyperpolarization. In all areas, nonetheless, arousal reduced the phasic responses to trains of sensory stimuli. These results demonstrate diverse effects of arousal across sensory cortex but similar effects on sensory responses. : Shimaoka etÂ al. use voltage-sensitive imaging to show that the effects of arousal on the mouse cortex are markedly different across areas and over time. In all the sensory areas studied, nonetheless, arousal reduced the phasic voltage responses to trains of sensory stimuli. Keywords: cerebral cortex, cortical state, locomotion, sensory processing, widefield imaging\n\nSensory impairments of the lower limb after stroke: a pooled analysis of individual patient data.\n\nScience.gov (United States)\n\nTyson, Sarah F; Crow, J Lesley; Connell, Louise; Winward, Charlotte; Hillier, Susan\n\n2013-01-01\n\nTo obtain more generalizable information on the frequency and factors influencing sensory impairment after stroke and their relationship to mobility and function. A pooled analysis of individual data of stroke survivors (N = 459); mean (SD) age = 67.2 (14.8) years, 54% male, mean (SD) time since stroke = 22.33 (63.1) days, 50% left-sided weakness. Where different measurement tools were used, data were recorded. Descriptive statistics described frequency of sensory impairments, kappa coefficients investigated relationships between sensory modalities, binary logistic regression explored the factors influencing sensory impairments, and linear regression assessed the impact of sensory impairments on activity limitations. Most patients' sensation was intact (55%), and individual sensory modalities were highly associated (Îº = 0.60, P sensory impairment (P analysis showed sensation of the lower limb is grossly preserved in most stroke survivors but, when present, it affects function. Sensory modalities are highly interrelated; interventions that treat the motor system during functional tasks may be as effective at treating the sensory system as sensory retraining alone.\n\nThree exploratory studies of relations between young adults' preference for activities involving a specific sense modality and sensory attributes of early memories.\n\nScience.gov (United States)\n\nWestman, A S; Stuve, M\n\n2001-04-01\n\nThree studies explored whether young adults' preference for using a sense modality, e.g., hearing, correlated with presence or clarity of attributes of that sense modality in earliest memories from childhood, elementary school, or high school. In Study 1, 75 graduates or seniors in fine arts, fashion merchandising, music, conducting, or dance showed no greater frequency or clarity of any modality's sensory attributes. In Study 2, 213 beginning university students' ratings of current importance of activities emphasizing a sense modality correlated with sensory contents of recollections only for smell and taste. In Study 3, 102 beginning students' ratings of current enjoyment in using a sense modality and sensory contents of recollections were correlated and involved every modality except vision.\n\nEnvironmental enrichment of young adult rats (Rattus norvegicus) in different sensory modalities has long-lasting effects on their ability to learn via specific sensory channels.\n\nScience.gov (United States)\n\nDolivo, Vassilissa; Taborsky, Michael\n\n2017-05-01\n\nSensory modalities individuals use to obtain information from the environment differ among conspecifics. The relative contributions of genetic divergence and environmental plasticity to this variance remain yet unclear. Numerous studies have shown that specific sensory enrichments or impoverishments at the postnatal stage can shape neural development, with potential lifelong effects. For species capable of adjusting to novel environments, specific sensory stimulation at a later life stage could also induce specific long-lasting behavioral effects. To test this possibility, we enriched young adult Norway rats with either visual, auditory, or olfactory cues. Four to 8 months after the enrichment period we tested each rat for their learning ability in 3 two-choice discrimination tasks, involving either visual, auditory, or olfactory stimulus discrimination, in a full factorial design. No sensory modality was more relevant than others for the proposed task per se, but rats performed better when tested in the modality for which they had been enriched. This shows that specific environmental conditions encountered during early adulthood have specific long-lasting effects on the learning abilities of rats. Furthermore, we disentangled the relative contributions of genetic and environmental causes of the response. The reaction norms of learning abilities in relation to the stimulus modality did not differ between families, so interindividual divergence was mainly driven by environmental rather than genetic factors. (PsycINFO Database Record (c) 2017 APA, all rights reserved).\n\nRelationship of Sensory Modality to Retention of Episodic Memory.\n\nScience.gov (United States)\n\nMorris, Jeri; Woodworth, Craig; Swier-Vosnos, Amy; Rossini, Edward; Jackson, Ilana\n\n2015-01-01\n\nThis study investigated the difference between episodic memory for verbal information presented in an oral format versus equivalent material presented in a written format. The study utilized the Logical Memory subtest of the Wechsler Memory Scales-Fourth Edition and the recently validated Morris Revision-IV Paragraphs. In a sample of 97 normal participants, auditory and visual memory performances were found to be significantly correlated (r = .651, p memory for these two sensory modalities in normal participants.\n\nGender differences in emotion recognition: Impact of sensory modality and emotional category.\n\nScience.gov (United States)\n\nLambrecht, Lena; Kreifelts, Benjamin; Wildgruber, Dirk\n\n2014-04-01\n\nResults from studies on gender differences in emotion recognition vary, depending on the types of emotion and the sensory modalities used for stimulus presentation. This makes comparability between different studies problematic. This study investigated emotion recognition of healthy participants (N = 84; 40 males; ages 20 to 70 years), using dynamic stimuli, displayed by two genders in three different sensory modalities (auditory, visual, audio-visual) and five emotional categories. The participants were asked to categorise the stimuli on the basis of their nonverbal emotional content (happy, alluring, neutral, angry, and disgusted). Hit rates and category selection biases were analysed. Women were found to be more accurate in recognition of emotional prosody. This effect was partially mediated by hearing loss for the frequency of 8,000 Hz. Moreover, there was a gender-specific selection bias for alluring stimuli: Men, as compared to women, chose \"alluring\" more often when a stimulus was presented by a woman as compared to a man.\n\nMatching of motor-sensory modality in the rodent femoral nerve model shows no enhanced effect on peripheral nerve regeneration\n\nScience.gov (United States)\n\nKawamura, David H.; Johnson, Philip J.; Moore, Amy M.; Magill, Christina K.; Hunter, Daniel A.; Ray, Wilson Z.; Tung, Thomas HH.; Mackinnon, Susan E.\n\n2010-01-01\n\nThe treatment of peripheral nerve injuries with nerve gaps largely consists of autologous nerve grafting utilizing sensory nerve donors. Underlying this clinical practice is the assumption that sensory autografts provide a suitable substrate for motoneuron regeneration, thereby facilitating motor endplate reinnervation and functional recovery. This study examined the role of nerve graft modality on axonal regeneration, comparing motor nerve regeneration through motor, sensory, and mixed nerve isografts in the Lewis rat. A total of 100 rats underwent grafting of the motor or sensory branch of the femoral nerve with histomorphometric analysis performed after 5, 6, or 7 weeks. Analysis demonstrated similar nerve regeneration in motor, sensory, and mixed nerve grafts at all three time points. These data indicate that matching of motor-sensory modality in the rat femoral nerve does not confer improved axonal regeneration through nerve isografts. PMID:20122927\n\nSub-threshold cross-modal sensory interaction in the thalamus: lemniscal auditory response in the medial geniculate nucleus is modulated by somatosensory stimulation.\n\nScience.gov (United States)\n\nDonishi, T; Kimura, A; Imbe, H; Yokoi, I; Kaneoke, Y\n\n2011-02-03\n\nRecent studies have highlighted cross-modal sensory modulations in the primary sensory areas in the cortex, suggesting that cross-modal sensory interactions occur at early stages in the hierarchy of sensory processing. Multi-modal sensory inputs from non-lemniscal thalamic nuclei and cortical inputs from the secondary sensory and association areas are considered responsible for the modulations. On the other hand, there is little evidence of cross-sensory modal sensitivities in lemniscal thalamic nuclei. In the present study, we were interested in a possibility that somatosensory stimulation may affect auditory response in the ventral division (MGV) of the medial geniculate nucleus (MG), a lemniscal thalamic nucleus that is considered to be dedicated to auditory uni-modal processing. Experiments were performed on anesthetized rats. Transcutaneous electrical stimulation of the hindpaw, which is thought to evoke nociception and seems unrelated to auditory processing, modulated unit discharges in response to auditory stimulation (noise bursts). The modulation was observed in the MGV and non-lemniscal auditory thalamic nuclei such as the dorsal and medial divisions of the MG. The major effect of somatosensory stimulation was suppression. The most robust suppression was induced by electrical stimuli given simultaneously with noise bursts or preceding noise bursts by 10 to 20 ms. The results indicate that the lemniscal (MGV) and non-lemniscal auditory nuclei are subject to somatosensory influence. In everyday experience intense somatosensory stimuli such as pain interrupt our ongoing hearing or interfere with clear recognition of sound. The modulation of lemniscal auditory response by somatosensory stimulation may underlie such cross-modal disturbance of auditory perception as a form of cross-modal switching of attention. Copyright ÃÂ© 2011 IBRO. Published by Elsevier Ltd. All rights reserved.\n\nWorking memory resources are shared across sensory modalities.\n\nScience.gov (United States)\n\nSalmela, V R; Moisala, M; Alho, K\n\n2014-10-01\n\nA common assumption in the working memory literature is that the visual and auditory modalities have separate and independent memory stores. Recent evidence on visual working memory has suggested that resources are shared between representations, and that the precision of representations sets the limit for memory performance. We tested whether memory resources are also shared across sensory modalities. Memory precision for two visual (spatial frequency and orientation) and two auditory (pitch and tone duration) features was measured separately for each feature and for all possible feature combinations. Thus, only the memory load was varied, from one to four features, while keeping the stimuli similar. In Experiment 1, two gratings and two tones-both containing two varying features-were presented simultaneously. In Experiment 2, two gratings and two tones-each containing only one varying feature-were presented sequentially. The memory precision (delayed discrimination threshold) for a single feature was close to the perceptual threshold. However, as the number of features to be remembered was increased, the discrimination thresholds increased more than twofold. Importantly, the decrease in memory precision did not depend on the modality of the other feature(s), or on whether the features were in the same or in separate objects. Hence, simultaneously storing one visual and one auditory feature had an effect on memory precision equal to those of simultaneously storing two visual or two auditory features. The results show that working memory is limited by the precision of the stored representations, and that working memory can be described as a resource pool that is shared across modalities.\n\nA magnetoencephalography study of multi-modal processing of pain anticipation in primary sensory cortices.\n\nScience.gov (United States)\n\nGopalakrishnan, R; Burgess, R C; Plow, E B; Floden, D P; Machado, A G\n\n2015-09-24\n\nPain anticipation plays a critical role in pain chronification and results in disability due to pain avoidance. It is important to understand how different sensory modalities (auditory, visual or tactile) may influence pain anticipation as different strategies could be applied to mitigate anticipatory phenomena and chronification. In this study, using a countdown paradigm, we evaluated with magnetoencephalography the neural networks associated with pain anticipation elicited by different sensory modalities in normal volunteers. When encountered with well-established cues that signaled pain, visual and somatosensory cortices engaged the pain neuromatrix areas early during the countdown process, whereas the auditory cortex displayed delayed processing. In addition, during pain anticipation, the visual cortex displayed independent processing capabilities after learning the contextual meaning of cues from associative and limbic areas. Interestingly, cross-modal activation was also evident and strong when visual and tactile cues signaled upcoming pain. Dorsolateral prefrontal cortex and mid-cingulate cortex showed significant activity during pain anticipation regardless of modality. Our results show pain anticipation is processed with great time efficiency by a highly specialized and hierarchical network. The highest degree of higher-order processing is modulated by context (pain) rather than content (modality) and rests within the associative limbic regions, corroborating their intrinsic role in chronification. Copyright Â© 2015 IBRO. Published by Elsevier Ltd. All rights reserved.\n\nTimescale- and Sensory Modality-Dependency of the Central Tendency of Time Perception.\n\nScience.gov (United States)\n\nMurai, Yuki; Yotsumoto, Yuko\n\n2016-01-01\n\nWhen individuals are asked to reproduce intervals of stimuli that are intermixedly presented at various times, longer intervals are often underestimated and shorter intervals overestimated. This phenomenon may be attributed to the central tendency of time perception, and suggests that our brain optimally encodes a stimulus interval based on current stimulus input and prior knowledge of the distribution of stimulus intervals. Two distinct systems are thought to be recruited in the perception of sub- and supra-second intervals. Sub-second timing is subject to local sensory processing, whereas supra-second timing depends on more centralized mechanisms. To clarify the factors that influence time perception, the present study investigated how both sensory modality and timescale affect the central tendency. In Experiment 1, participants were asked to reproduce sub- or supra-second intervals, defined by visual or auditory stimuli. In the sub-second range, the magnitude of the central tendency was significantly larger for visual intervals compared to auditory intervals, while visual and auditory intervals exhibited a correlated and comparable central tendency in the supra-second range. In Experiment 2, the ability to discriminate sub-second intervals in the reproduction task was controlled across modalities by using an interval discrimination task. Even when the ability to discriminate intervals was controlled, visual intervals exhibited a larger central tendency than auditory intervals in the sub-second range. In addition, the magnitude of the central tendency for visual and auditory sub-second intervals was significantly correlated. These results suggest that a common modality-independent mechanism is responsible for the supra-second central tendency, and that both the modality-dependent and modality-independent components of the timing system contribute to the central tendency in the sub-second range.\n\nTimescale- and Sensory Modality-Dependency of the Central Tendency of Time Perception.\n\nDirectory of Open Access Journals (Sweden)\n\nYuki Murai\n\nFull Text Available When individuals are asked to reproduce intervals of stimuli that are intermixedly presented at various times, longer intervals are often underestimated and shorter intervals overestimated. This phenomenon may be attributed to the central tendency of time perception, and suggests that our brain optimally encodes a stimulus interval based on current stimulus input and prior knowledge of the distribution of stimulus intervals. Two distinct systems are thought to be recruited in the perception of sub- and supra-second intervals. Sub-second timing is subject to local sensory processing, whereas supra-second timing depends on more centralized mechanisms. To clarify the factors that influence time perception, the present study investigated how both sensory modality and timescale affect the central tendency. In Experiment 1, participants were asked to reproduce sub- or supra-second intervals, defined by visual or auditory stimuli. In the sub-second range, the magnitude of the central tendency was significantly larger for visual intervals compared to auditory intervals, while visual and auditory intervals exhibited a correlated and comparable central tendency in the supra-second range. In Experiment 2, the ability to discriminate sub-second intervals in the reproduction task was controlled across modalities by using an interval discrimination task. Even when the ability to discriminate intervals was controlled, visual intervals exhibited a larger central tendency than auditory intervals in the sub-second range. In addition, the magnitude of the central tendency for visual and auditory sub-second intervals was significantly correlated. These results suggest that a common modality-independent mechanism is responsible for the supra-second central tendency, and that both the modality-dependent and modality-independent components of the timing system contribute to the central tendency in the sub-second range.\n\nEffects of electrode size and spacing on sensory modalities in the phantom thumb perception area for the forearm amputees.\n\nScience.gov (United States)\n\nLi, P; Chai, G H; Zhu, K H; Lan, N; Sui, X H\n\n2015-01-01\n\nTactile sensory feedback plays a key role in accomplishing the dexterous manipulation of prosthetic hands for the amputees, and the non-invasive transcutaneous electrical nerve stimulation (TENS) of the phantom finger perception (PFP) area would be an effective way to realize sensory feedback clinically. In order to realize the high-spatial-resolution tactile sensory feedback in the PFP region, we investigated the effects of electrode size and spacing on the tactile sensations for potentially optimizing the surface electrode array configuration. Six forearm-amputated subjects were recruited in the psychophysical studies. With the diameter of the circular electrode increasing from 3 mm to 12 mm, the threshold current intensity was enhanced correspondingly under different sensory modalities. The smaller electrode could potentially lead to high sensation spatial resolution. Whereas, the smaller the electrode, the less the number of sensory modalities. For an Î¦-3 mm electrode, it is even hard for the subject to perceive any perception modalities under normal stimulating current. In addition, the two-electrode discrimination distance (TEDD) in the phantom thumb perception area decreased with electrode size decreasing in two directions of parallel or perpendicular to the forearm. No significant difference of TEDD existed along the two directions. Studies in this paper would guide the configuration optimization of the TENS electrode array for potential high spatial-resolution sensory feedback.\n\nIs it me? Self-recognition bias across sensory modalities and its relationship to autistic traits.\n\nScience.gov (United States)\n\nChakraborty, Anya; Chakrabarti, Bhismadev\n\n2015-01-01\n\nAtypical self-processing is an emerging theme in autism research, suggested by lower self-reference effect in memory, and atypical neural responses to visual self-representations. Most research on physical self-processing in autism uses visual stimuli. However, the self is a multimodal construct, and therefore, it is essential to test self-recognition in other sensory modalities as well. Self-recognition in the auditory modality remains relatively unexplored and has not been tested in relation to autism and related traits. This study investigates self-recognition in auditory and visual domain in the general population and tests if it is associated with autistic traits. Thirty-nine neurotypical adults participated in a two-part study. In the first session, individual participant's voice was recorded and face was photographed and morphed respectively with voices and faces from unfamiliar identities. In the second session, participants performed a 'self-identification' task, classifying each morph as 'self' voice (or face) or an 'other' voice (or face). All participants also completed the Autism Spectrum Quotient (AQ). For each sensory modality, slope of the self-recognition curve was used as individual self-recognition metric. These two self-recognition metrics were tested for association between each other, and with autistic traits. Fifty percent 'self' response was reached for a higher percentage of self in the auditory domain compared to the visual domain (tâ=â3.142; Pâself-recognition bias across sensory modalities (Ï = -0.165, Pâ=â0.204). Higher recognition bias for self-voice was observed in individuals higher in autistic traits (Ï AQ = 0.301, P = 0.008). No such correlation was observed between recognition bias for self-face and autistic traits (Ï AQ = -0.020, P = 0.438). Our data shows that recognition bias for physical self-representation is not related across sensory modalities. Further, individuals with higher autistic traits were better able\n\nMental Imagery Induces Cross-Modal Sensory Plasticity and Changes Future Auditory Perception.\n\nScience.gov (United States)\n\nBerger, Christopher C; Ehrsson, H Henrik\n\n2018-04-01\n\nCan what we imagine in our minds change how we perceive the world in the future? A continuous process of multisensory integration and recalibration is responsible for maintaining a correspondence between the senses (e.g., vision, touch, audition) and, ultimately, a stable and coherent perception of our environment. This process depends on the plasticity of our sensory systems. The so-called ventriloquism aftereffect-a shift in the perceived localization of sounds presented alone after repeated exposure to spatially mismatched auditory and visual stimuli-is a clear example of this type of plasticity in the audiovisual domain. In a series of six studies with 24 participants each, we investigated an imagery-induced ventriloquism aftereffect in which imagining a visual stimulus elicits the same frequency-specific auditory aftereffect as actually seeing one. These results demonstrate that mental imagery can recalibrate the senses and induce the same cross-modal sensory plasticity as real sensory stimuli.\n\nSensory Deprivation during Early Postnatal Period Alters the Density of Interneurons in the Mouse Prefrontal Cortex\n\nDirectory of Open Access Journals (Sweden)\n\nHiroshi Ueno\n\n2015-01-01\n\nFull Text Available Early loss of one sensory system can cause improved function of other sensory systems. However, both the time course and neuronal mechanism of cross-modal plasticity remain elusive. Recent study using functional MRI in humans suggests a role of the prefrontal cortex (PFC in cross-modal plasticity. Since this phenomenon is assumed to be associated with altered GABAergic inhibition in the PFC, we have tested the hypothesis that early postnatal sensory deprivation causes the changes of inhibitory neuronal circuit in different regions of the PFC of the mice. We determined the effects of sensory deprivation from birth to postnatal day 28 (P28 or P58 on the density of parvalbumin (PV, calbindin (CB, and calretinin (CR neurons in the prelimbic, infralimbic, and dorsal anterior cingulate cortices. The density of PV and CB neurons was significantly increased in layer 5/6 (L5/6. Moreover, the density of CR neurons was higher in L2/3 in sensory deprived mice compared to intact mice. These changes were more prominent at P56 than at P28. These results suggest that long-term sensory deprivation causes the changes of intracortical inhibitory networks in the PFC and the changes of inhibitory networks in the PFC may contribute to cross-modal plasticity.\n\nNaturalizing aesthetics: brain areas for aesthetic appraisal across sensory modalities.\n\nScience.gov (United States)\n\nBrown, Steven; Gao, Xiaoqing; Tisdelle, Loren; Eickhoff, Simon B; Liotti, Mario\n\n2011-09-01\n\nWe present here the most comprehensive analysis to date of neuroaesthetic processing by reporting the results of voxel-based meta-analyses of 93 neuroimaging studies of positive-valence aesthetic appraisal across four sensory modalities. The results demonstrate that the most concordant area of activation across all four modalities is the right anterior insula, an area typically associated with visceral perception, especially of negative valence (disgust, pain, etc.). We argue that aesthetic processing is, at its core, the appraisal of the valence of perceived objects. This appraisal is in no way limited to artworks but is instead applicable to all types of perceived objects. Therefore, one way to naturalize aesthetics is to argue that such a system evolved first for the appraisal of objects of survival advantage, such as food sources, and was later co-opted in humans for the experience of artworks for the satisfaction of social needs. Copyright Â© 2011 Elsevier Inc. All rights reserved.\n\nPharmacologic attenuation of cross-modal sensory augmentation within the chronic pain insula\n\nScience.gov (United States)\n\nHarte, Steven E.; Ichesco, Eric; Hampson, Johnson P.; Peltier, Scott J.; Schmidt-Wilcke, Tobias; Clauw, Daniel J.; Harris, Richard E.\n\n2016-01-01\n\nAbstract Pain can be elicited through all mammalian sensory pathways yet cross-modal sensory integration, and its relationship to clinical pain, is largely unexplored. Centralized chronic pain conditions such as fibromyalgia are often associated with symptoms of multisensory hypersensitivity. In this study, female patients with fibromyalgia demonstrated cross-modal hypersensitivity to visual and pressure stimuli compared with age- and sex-matched healthy controls. Functional magnetic resonance imaging revealed that insular activity evoked by an aversive level of visual stimulation was associated with the intensity of fibromyalgia pain. Moreover, attenuation of this insular activity by the analgesic pregabalin was accompanied by concomitant reductions in clinical pain. A multivariate classification method using support vector machines (SVM) applied to visual-evoked brain activity distinguished patients with fibromyalgia from healthy controls with 82% accuracy. A separate SVM classification of treatment effects on visual-evoked activity reliably identified when patients were administered pregabalin as compared with placebo. Both SVM analyses identified significant weights within the insular cortex during aversive visual stimulation. These data suggest that abnormal integration of multisensory and pain pathways within the insula may represent a pathophysiological mechanism in some chronic pain conditions and that insular response to aversive visual stimulation may have utility as a marker for analgesic drug development. PMID:27101425\n\nNeural organization of linguistic short-term memory is sensory modality-dependent: evidence from signed and spoken language.\n\nScience.gov (United States)\n\nPa, Judy; Wilson, Stephen M; Pickell, Herbert; Bellugi, Ursula; Hickok, Gregory\n\n2008-12-01\n\nDespite decades of research, there is still disagreement regarding the nature of the information that is maintained in linguistic short-term memory (STM). Some authors argue for abstract phonological codes, whereas others argue for more general sensory traces. We assess these possibilities by investigating linguistic STM in two distinct sensory-motor modalities, spoken and signed language. Hearing bilingual participants (native in English and American Sign Language) performed equivalent STM tasks in both languages during functional magnetic resonance imaging. Distinct, sensory-specific activations were seen during the maintenance phase of the task for spoken versus signed language. These regions have been previously shown to respond to nonlinguistic sensory stimulation, suggesting that linguistic STM tasks recruit sensory-specific networks. However, maintenance-phase activations common to the two languages were also observed, implying some form of common process. We conclude that linguistic STM involves sensory-dependent neural networks, but suggest that sensory-independent neural networks may also exist.\n\nEvidence of a visual-to-auditory cross-modal sensory gating phenomenon as reflected by the human P50 event-related brain potential modulation.\n\nScience.gov (United States)\n\nLebib, Riadh; Papo, David; de Bode, Stella; BaudonniÃ¨re, Pierre Marie\n\n2003-05-08\n\nWe investigated the existence of a cross-modal sensory gating reflected by the modulation of an early electrophysiological index, the P50 component. We analyzed event-related brain potentials elicited by audiovisual speech stimuli manipulated along two dimensions: congruency and discriminability. The results showed that the P50 was attenuated when visual and auditory speech information were redundant (i.e. congruent), in comparison with this same event-related potential component elicited with discrepant audiovisual dubbing. When hard to discriminate, however, bimodal incongruent speech stimuli elicited a similar pattern of P50 attenuation. We concluded to the existence of a visual-to-auditory cross-modal sensory gating phenomenon. These results corroborate previous findings revealing a very early audiovisual interaction during speech perception. Finally, we postulated that the sensory gating system included a cross-modal dimension.\n\nRole of secondary sensory cortices in emotional memory storage and retrieval in rats.\n\nScience.gov (United States)\n\nSacco, Tiziana; Sacchetti, Benedetto\n\n2010-08-06\n\nVisual, acoustic, and olfactory stimuli associated with a highly charged emotional situation take on the affective qualities of that situation. Where the emotional meaning of a given sensory experience is stored is a matter of debate. We found that excitotoxic lesions of auditory, visual, or olfactory secondary sensory cortices impaired remote, but not recent, fear memories in rats. Amnesia was modality-specific and not due to an interference with sensory or emotional processes. In these sites, memory persistence was dependent on ongoing protein kinase Mzeta activity and was associated with an increased activity of layers II-IV, thus suggesting a synaptic strengthening of corticocortical connections. Lesions of the same areas left intact the memory of sensory stimuli not associated with any emotional charge. We propose that secondary sensory cortices support memory storage and retrieval of sensory stimuli that have acquired a behavioral salience with the experience.\n\nA Ventral Visual Stream Reading Center Independent of Sensory Modality and Visual Experience\n\nDirectory of Open Access Journals (Sweden)\n\nLior Reich\n\n2011-10-01\n\nFull Text Available The Visual Word Form Area (VWFA is a ventral-temporal-visual area that develops expertise for visual reading. It encodes letter-strings irrespective of case, font, or location in the visual-field, with striking anatomical reproducibility across individuals. In the blind, reading can be achieved using Braille, with a comparable level-of-expertise to that of sighted readers. We investigated which area plays the role of the VWFA in the blind. One would expect it to be at either parietal or bilateral occipital cortex, reflecting the tactile nature of the task and crossmodal plasticity, respectively. However, according to the notion that brain areas are task specific rather than sensory-modality specific, we predicted recruitment of the left-hemispheric VWFA, identically to the sighted and independent of visual experience. Using fMRI we showed that activation during Braille reading in congenitally blind individuals peaked in the VWFA, with striking anatomical consistency within and between blind and sighted. The VWFA was reading-selective when contrasted to high-level language and low-level sensory controls. Further preliminary results show that the VWFA is selectively activated also when people learn to read in a new language or using a different modality. Thus, the VWFA is a mutlisensory area specialized for reading regardless of visual experience.\n\nSensory modality specificity of neural activity related to memory in visual cortex.\n\nScience.gov (United States)\n\nGibson, J R; Maunsell, J H\n\n1997-09-01\n\nPrevious studies have shown that when monkeys perform a delayed match-to-sample (DMS) task, some neurons in inferotemporal visual cortex are activated selectively during the delay period when the animal must remember particular visual stimuli. This selective delay activity may be involved in short-term memory. It does not depend on visual stimulation: both auditory and tactile stimuli can trigger selective delay activity in inferotemporal cortex when animals expect to respond to visual stimuli in a DMS task. We have examined the overall modality specificity of delay period activity using a variety of auditory/visual cross-modal and unimodal DMS tasks. The cross-modal DMS tasks involved making specific long-term memory associations between visual and auditory stimuli, whereas the unimodal DMS tasks were standard identity matching tasks. Delay activity existed in auditory/visual cross-modal DMS tasks whether the animal anticipated responding to visual or auditory stimuli. No evidence of selective delay period activation was seen in a purely auditory DMS task. Delay-selective cells were relatively common in one animal where they constituted up to 53% neurons tested with a given task. This was only the case for up to 9% of cells in a second animal. In the first animal, a specific long-term memory representation for learned cross-modal associations was observed in delay activity, indicating that this type of representation need not be purely visual. Furthermore, in this same animal, delay activity in one cross-modal task, an auditory-to-visual task, predicted correct and incorrect responses. These results suggest that neurons in inferotemporal cortex contribute to abstract memory representations that can be activated by input from other sensory modalities, but these representations are specific to visual behaviors.\n\nShort-Term Effect of Prosthesis Transforming Sensory Modalities on Walking in Stroke Patients with Hemiparesis\n\nScience.gov (United States)\n\nSekiguchi, Yusuke; Honda, Keita; Ishiguro, Akio\n\n2016-01-01\n\nSensory impairments caused by neurological or physical disorders hamper kinesthesia, making rehabilitation difficult. In order to overcome this problem, we proposed and developed a novel biofeedback prosthesis called Auditory Foot for transforming sensory modalities, in which the sensor prosthesis transforms plantar sensations to auditory feedback signals. This study investigated the short-term effect of the auditory feedback prosthesis on walking in stroke patients with hemiparesis. To evaluate the effect, we compared four conditions of auditory feedback from plantar sensors at the heel and fifth metatarsal. We found significant differences in the maximum hip extension angle and ankle plantar flexor moment on the affected side during the stance phase, between conditions with and without auditory feedback signals. These results indicate that our sensory prosthesis could enhance walking performance in stroke patients with hemiparesis, resulting in effective short-term rehabilitation. PMID:27547456\n\nThe sensory substrate of multimodal communication in brown-headed cowbirds: are females sensory 'specialists' or 'generalists'?\n\nScience.gov (United States)\n\nRonald, Kelly L; Sesterhenn, Timothy M; Fernandez-Juricic, Esteban; Lucas, Jeffrey R\n\n2017-11-01\n\nMany animals communicate with multimodal signals. While we have an understanding of multimodal signal production, we know relatively less about receiver filtering of multimodal signals and whether filtering capacity in one modality influences filtering in a second modality. Most multimodal signals contain a temporal element, such as change in frequency over time or a dynamic visual display. We examined the relationship in temporal resolution across two modalities to test whether females are (1) sensory 'specialists', where a trade-off exists between the sensory modalities, (2) sensory 'generalists', where a positive relationship exists between the modalities, or (3) whether no relationship exists between modalities. We used female brown-headed cowbirds (Molothrus ater) to investigate this question as males court females with an audiovisual display. We found a significant positive relationship between female visual and auditory temporal resolution, suggesting that females are sensory 'generalists'. Females appear to resolve information well across multiple modalities, which may select for males that signal their quality similarly across modalities.\n\nSensory perception: lessons from synesthesia: using synesthesia to inform the understanding of sensory perception.\n\nScience.gov (United States)\n\nHarvey, Joshua Paul\n\n2013-06-01\n\nSynesthesia, the conscious, idiosyncratic, repeatable, and involuntary sensation of one sensory modality in response to another, is a condition that has puzzled both researchers and philosophers for centuries. Much time has been spent proving the condition's existence as well as investigating its etiology, but what can be learned from synesthesia remains a poorly discussed topic. Here, synaesthesia is presented as a possible answer rather than a question to the current gaps in our understanding of sensory perception. By first appreciating the similarities between normal sensory perception and synesthesia, one can use what is known about synaesthesia, from behavioral and imaging studies, to inform our understanding of \"normal\" sensory perception. In particular, in considering synesthesia, one can better understand how and where the different sensory modalities interact in the brain, how different sensory modalities can interact without confusion - the binding problem - as well as how sensory perception develops.\n\nOscillatory neuronal activity reflects lexical-semantic feature integration within and across sensory modalities in distributed cortical networks.\n\nScience.gov (United States)\n\nvan Ackeren, Markus J; Schneider, Till R; MÃ¼sch, Kathrin; Rueschemeyer, Shirley-Ann\n\n2014-10-22\n\nResearch from the previous decade suggests that word meaning is partially stored in distributed modality-specific cortical networks. However, little is known about the mechanisms by which semantic content from multiple modalities is integrated into a coherent multisensory representation. Therefore we aimed to characterize differences between integration of lexical-semantic information from a single modality compared with two sensory modalities. We used magnetoencephalography in humans to investigate changes in oscillatory neuronal activity while participants verified two features for a given target word (e.g., \"bus\"). Feature pairs consisted of either two features from the same modality (visual: \"red,\" \"big\") or different modalities (auditory and visual: \"red,\" \"loud\"). The results suggest that integrating modality-specific features of the target word is associated with enhanced high-frequency power (80-120 Hz), while integrating features from different modalities is associated with a sustained increase in low-frequency power (2-8 Hz). Source reconstruction revealed a peak in the anterior temporal lobe for low-frequency and high-frequency effects. These results suggest that integrating lexical-semantic knowledge at different cortical scales is reflected in frequency-specific oscillatory neuronal activity in unisensory and multisensory association networks. Copyright Â© 2014 the authors 0270-6474/14/3314318-06$15.00/0.\n\nCross-modal decoupling in temporal attention.\n\nScience.gov (United States)\n\nMÃ¼hlberg, Stefanie; Oriolo, Giovanni; Soto-Faraco, Salvador\n\n2014-06-01\n\nPrior studies have repeatedly reported behavioural benefits to events occurring at attended, compared to unattended, points in time. It has been suggested that, as for spatial orienting, temporal orienting of attention spreads across sensory modalities in a synergistic fashion. However, the consequences of cross-modal temporal orienting of attention remain poorly understood. One challenge is that the passage of time leads to an increase in event predictability throughout a trial, thus making it difficult to interpret possible effects (or lack thereof). Here we used a design that avoids complete temporal predictability to investigate whether attending to a sensory modality (vision or touch) at a point in time confers beneficial access to events in the other, non-attended, sensory modality (touch or vision, respectively). In contrast to previous studies and to what happens with spatial attention, we found that events in one (unattended) modality do not automatically benefit from happening at the time point when another modality is expected. Instead, it seems that attention can be deployed in time with relative independence for different sensory modalities. Based on these findings, we argue that temporal orienting of attention can be cross-modally decoupled in order to flexibly react according to the environmental demands, and that the efficiency of this selective decoupling unfolds in time. Â© 2014 Federation of European Neuroscience Societies and John Wiley & Sons Ltd.\n\nSensory feedback in upper limb prosthetics.\n\nScience.gov (United States)\n\nAntfolk, Christian; D'Alonzo, Marco; RosÃ©n, Birgitta; Lundborg, GÃ¶ran; Sebelius, Fredrik; Cipriani, Christian\n\n2013-01-01\n\nOne of the challenges facing prosthetic designers and engineers is to restore the missing sensory function inherit to hand amputation. Several different techniques can be employed to provide amputees with sensory feedback: sensory substitution methods where the recorded stimulus is not only transferred to the amputee, but also translated to a different modality (modality-matched feedback), which transfers the stimulus without translation and direct neural stimulation, which interacts directly with peripheral afferent nerves. This paper presents an overview of the principal works and devices employed to provide upper limb amputees with sensory feedback. The focus is on sensory substitution and modality matched feedback; the principal features, advantages and disadvantages of the different methods are presented.\n\nCross-Modal Correspondences Enhance Performance on a Colour-to-Sound Sensory Substitution Device.\n\nScience.gov (United States)\n\nHamilton-Fletcher, Giles; Wright, Thomas D; Ward, Jamie\n\nVisual sensory substitution devices (SSDs) can represent visual characteristics through distinct patterns of sound, allowing a visually impaired user access to visual information. Previous SSDs have avoided colour and when they do encode colour, have assigned sounds to colour in a largely unprincipled way. This study introduces a new tablet-based SSD termed the âCreoleâ (so called because it combines tactile scanning with image sonification) and a new algorithm for converting colour to sound that is based on established cross-modal correspondences (intuitive mappings between different sensory dimensions). To test the utility of correspondences, we examined the colourâsound associative memory and object recognition abilities of sighted users who had their device either coded in line with or opposite to soundâcolour correspondences. Improved colour memory and reduced colour-errors were made by users who had the correspondence-based mappings. Interestingly, the colourâsound mappings that provided the highest improvements during the associative memory task also saw the greatest gains for recognising realistic objects that also featured these colours, indicating a transfer of abilities from memory to recognition. These users were also marginally better at matching sounds to images varying in luminance, even though luminance was coded identically across the different versions of the device. These findings are discussed with relevance for both colour and correspondences for sensory substitution use.\n\nThe Improved Sensitivity to Crossmodal Asynchrony Caused by Voluntary Action: Comparing Combinations of Sensory Modalities\n\nDirectory of Open Access Journals (Sweden)\n\nNorimichi Kitagawa\n\n2011-10-01\n\nFull Text Available The brain has to assess the fine temporal relationship between voluntary actions and their sensory effects to achieve precise spatiotemporal control of body movement. Recently we found that voluntary action improved the subsequent perceptual temporal discrimination between somatosensory and auditory events. In voluntary condition, participants actively pressed a button and a noise burst was presented at various onset asynchronies relative to the button press. The participants made either âsound-firstâ or âtouch-firstâ responses. We found that the temporal order judgment performance in the voluntary condition (as indexed by just noticeable difference was significantly better than that when their finger was passively stimulated (passive condition. Temporal attention and comparable involuntary movement did not explain the improvement caused by the voluntary action. The results suggest that predicting sensory consequences via a âforwardâ model enhances perceptual temporal resolution for precise control of the body. The present study examined whether this improved temporal sensitivity caused by the voluntary action is also observed for the other combinations of sensory modalities. We compared the effects of voluntary action on the temporal sensitivity between auditory-somatosensory, visual-somatosensory, and somatosensory-somatosensory stimulus pairs.\n\nAssociative learning changes cross-modal representations in the gustatory cortex.\n\nScience.gov (United States)\n\nVincis, Roberto; Fontanini, Alfredo\n\n2016-08-30\n\nA growing body of literature has demonstrated that primary sensory cortices are not exclusively unimodal, but can respond to stimuli of different sensory modalities. However, several questions concerning the neural representation of cross-modal stimuli remain open. Indeed, it is poorly understood if cross-modal stimuli evoke unique or overlapping representations in a primary sensory cortex and whether learning can modulate these representations. Here we recorded single unit responses to auditory, visual, somatosensory, and olfactory stimuli in the gustatory cortex (GC) of alert rats before and after associative learning. We found that, in untrained rats, the majority of GC neurons were modulated by a single modality. Upon learning, both prevalence of cross-modal responsive neurons and their breadth of tuning increased, leading to a greater overlap of representations. Altogether, our results show that the gustatory cortex represents cross-modal stimuli according to their sensory identity, and that learning changes the overlap of cross-modal representations.\n\nStrain differences of the effect of enucleation and anophthalmia on the size and growth of sensory cortices in mice.\n\nScience.gov (United States)\n\nMassÃ©, Ian O; Guillemette, Sonia; LaramÃ©e, Marie-Eve; Bronchti, Gilles; Boire, Denis\n\n2014-11-07\n\nAnophthalmia is a condition in which the eye does not develop from the early embryonic period. Early blindness induces cross-modal plastic modifications in the brain such as auditory and haptic activations of the visual cortex and also leads to a greater solicitation of the somatosensory and auditory cortices. The visual cortex is activated by auditory stimuli in anophthalmic mice and activity is known to alter the growth pattern of the cerebral cortex. The size of the primary visual, auditory and somatosensory cortices and of the corresponding specific sensory thalamic nuclei were measured in intact and enucleated C57Bl/6J mice and in ZRDCT anophthalmic mice (ZRDCT/An) to evaluate the contribution of cross-modal activity on the growth of the cerebral cortex. In addition, the size of these structures were compared in intact, enucleated and anophthalmic fourth generation backcrossed hybrid C57Bl/6JÃZRDCT/An mice to parse out the effects of mouse strains and of the different visual deprivations. The visual cortex was smaller in the anophthalmic ZRDCT/An than in the intact and enucleated C57Bl/6J mice. Also the auditory cortex was larger and the somatosensory cortex smaller in the ZRDCT/An than in the intact and enucleated C57Bl/6J mice. The size differences of sensory cortices between the enucleated and anophthalmic mice were no longer present in the hybrid mice, showing specific genetic differences between C57Bl/6J and ZRDCT mice. The post natal size increase of the visual cortex was less in the enucleated than in the anophthalmic and intact hybrid mice. This suggests differences in the activity of the visual cortex between enucleated and anophthalmic mice and that early in-utero spontaneous neural activity in the visual system contributes to the shaping of functional properties of cortical networks. Copyright Â© 2014 Elsevier B.V. All rights reserved.\n\nDesigning sensory-substitution devices: Principles, pitfalls and potential1.\n\nScience.gov (United States)\n\nKristjÃ¡nsson, Ãrni; Moldoveanu, Alin; JÃ³hannesson, Ãmar I; Balan, Oana; Spagnol, Simone; ValgeirsdÃ³ttir, VigdÃ­s Vala; Unnthorsson, RÃºnar\n\n2016-09-21\n\nAn exciting possibility for compensating for loss of sensory function is to augment deficient senses by conveying missing information through an intact sense. Here we present an overview of techniques that have been developed for sensory substitution (SS) for the blind, through both touch and audition, with special emphasis on the importance of training for the use of such devices, while highlighting potential pitfalls in their design. One example of a pitfall is how conveying extra information about the environment risks sensory overload. Related to this, the limits of attentional capacity make it important to focus on key information and avoid redundancies. Also, differences in processing characteristics and bandwidth between sensory systems severely constrain the information that can be conveyed. Furthermore, perception is a continuous process and does not involve a snapshot of the environment. Design of sensory substitution devices therefore requires assessment of the nature of spatiotemporal continuity for the different senses. Basic psychophysical and neuroscientific research into representations of the environment and the most effective ways of conveying information should lead to better design of sensory substitution systems. Sensory substitution devices should emphasize usability, and should not interfere with other inter- or intramodal perceptual function. Devices should be task-focused since in many cases it may be impractical to convey too many aspects of the environment. Evidence for multisensory integration in the representation of the environment suggests that researchers should not limit themselves to a single modality in their design. Finally, we recommend active training on devices, especially since it allows for externalization, where proximal sensory stimulation is attributed to a distinct exterior object.\n\nA review of invasive and non-invasive sensory feedback in upper limb prostheses.\n\nScience.gov (United States)\n\nSvensson, Pamela; Wijk, Ulrika; BjÃ¶rkman, Anders; Antfolk, Christian\n\n2017-06-01\n\nThe constant challenge to restore sensory feedback in prosthetic hands has provided several research solutions, but virtually none has reached clinical fruition. A prosthetic hand with sensory feedback that closely imitates an intact hand and provides a natural feeling may induce the prosthetic hand to be included in the body image and also reinforces the control of the prosthesis. Areas covered: This review presents non-invasive sensory feedback systems such as mechanotactile, vibrotactile, electrotactile and combinational systems which combine the modalities; multi-haptic feedback. Invasive sensory feedback has been tried less, because of the inherent risk, but it has successfully shown to restore some afferent channels. In this review, invasive methods are also discussed, both extraneural and intraneural electrodes, such as cuff electrodes and transverse intrafascicular multichannel electrodes. The focus of the review is on non-invasive methods of providing sensory feedback to upper-limb amputees. Expert commentary: Invoking embodiment has shown to be of importance for the control of prosthesis and acceptance by the prosthetic wearers. It is a challenge to provide conscious feedback to cover the lost sensibility of a hand, not be overwhelming and confusing for the user, and to integrate technology within the constraint of a wearable prosthesis.\n\nThe Sense of Agency Is More Sensitive to Manipulations of Outcome than Movement-Related Feedback Irrespective of Sensory Modality.\n\nDirectory of Open Access Journals (Sweden)\n\nNicole David\n\nFull Text Available The sense of agency describes the ability to experience oneself as the agent of one's own actions. Previous studies of the sense of agency manipulated the predicted sensory feedback related either to movement execution or to the movement's outcome, for example by delaying the movement of a virtual hand or the onset of a tone that resulted from a button press. Such temporal sensorimotor discrepancies reduce the sense of agency. It remains unclear whether movement-related feedback is processed differently than outcome-related feedback in terms of agency experience, especially if these types of feedback differ with respect to sensory modality. We employed a mixed-reality setup, in which participants tracked their finger movements by means of a virtual hand. They performed a single tap, which elicited a sound. The temporal contingency between the participants' finger movements and (i the movement of the virtual hand or (ii the expected auditory outcome was systematically varied. In a visual control experiment, the tap elicited a visual outcome. For each feedback type and participant, changes in the sense of agency were quantified using a forced-choice paradigm and the Method of Constant Stimuli. Participants were more sensitive to delays of outcome than to delays of movement execution. This effect was very similar for visual or auditory outcome delays. Our results indicate different contributions of movement- versus outcome-related sensory feedback to the sense of agency, irrespective of the modality of the outcome. We propose that this differential sensitivity reflects the behavioral importance of assessing authorship of the outcome of an action.\n\nWhat is Sensory about Multi-Sensory Enhancement of Vision by Sounds?\n\nDirectory of Open Access Journals (Sweden)\n\nAlexis PÃ©rez-Bellido\n\n2011-10-01\n\nFull Text Available Can auditory input influence the sensory processing of visual information? Many studies have reported cross-modal enhancement in visual tasks, but the nature of such gain is still unclear. Some authors argue for âhigh-orderâ expectancy or attention effects, whereas others propose âlow-orderâ stimulus-driven multisensory integration. The present study applies a psychophysical analysis of reaction time distributions in order to disentangle sensory changes from other kind of high-order (not sensory-specific effects. Observers performed a speeded simple detection task on Gabor patches of different spatial frequencies and contrasts, with and without accompanying sounds. The data were adjusted using chronometric functions in order to separate changes is sensory evidence from changes in decision or motor times. The results supported the existence of a stimulus unspecific auditory-induced enhancement in RTs across all types of visual stimuli, probably mediated by higher-order effects (eg, reduction of temporal uncertainty. Critically, we also singled out a sensory gain that was selective to low spatial frequency stimuli, highlighting the role of the magno-cellular visual pathway in multisensory integration for fast detection. The present findings help clarify previous mixed findings in the area, and introduce a novel form to evaluate cross-modal enhancement.\n\nShould healthy eating programmes incorporate interaction with foods in different sensory modalities? A review of the evidence.\n\nScience.gov (United States)\n\nDazeley, Paul; Houston-Price, Carmel; Hill, Claire\n\n2012-09-01\n\nCommercial interventions seeking to promote fruit and vegetable consumption by encouraging preschool- and school-aged children to engage with foods with 'all their senses' are increasing in number. We review the efficacy of such sensory interaction programmes and consider the components of these that are likely to encourage food acceptance. Repeated exposure to a food's flavour has robust empirical support in terms of its potential to increase food intake. However, children are naturally reluctant to taste new or disliked foods, and parents often struggle to provide sufficient taste opportunities for these foods to be adopted into the child's diet. We therefore explore whether prior exposure to a new food's non-taste sensory properties, such as its smell, sound, appearance or texture, might facilitate the food's introduction into the child's diet, by providing the child with an opportunity to become partially familiar with the food without invoking the distress associated with tasting it. We review the literature pertaining to the benefits associated with exposure to foods through each of the five sensory modalities in turn. We conclude by calling for further research into the potential for familiarisation with the visual, olfactory, somaesthetic and auditory properties of foods to enhance children's willingness to consume a variety of fruits and vegetables.\n\nShort-term memory for event duration: modality specificity and goal dependency.\n\nScience.gov (United States)\n\nTakahashi, Kohske; Watanabe, Katsumi\n\n2012-11-01\n\nTime perception is involved in various cognitive functions. This study investigated the characteristics of short-term memory for event duration by examining how the length of the retention period affects inter- and intramodal duration judgment. On each trial, a sample stimulus was followed by a comparison stimulus, after a variable delay period (0.5-5Â s). The sample and comparison stimuli were presented in the visual or auditory modality. The participants determined whether the comparison stimulus was longer or shorter than the sample stimulus. The distortion pattern of subjective duration during the delay period depended on the sensory modality of the comparison stimulus but was not affected by that of the sample stimulus. When the comparison stimulus was visually presented, the retained duration of the sample stimulus was shortened as the delay period increased. Contrarily, when the comparison stimulus was presented in the auditory modality, the delay period had little to no effect on the retained duration. Furthermore, whenever the participants did not know the sensory modality of the comparison stimulus beforehand, the effect of the delay period disappeared. These results suggest that the memory process for event duration is specific to sensory modality and that its performance is determined depending on the sensory modality in which the retained duration will be used subsequently.\n\nSensing Nepal in a Peer Student Created Multi-sensory Environment\n\nOpenAIRE\n\nSthapit, Erose; Kansakar, Sajina\n\n2010-01-01\n\nThe idea that learning experienced through all the senses is helpful in reinforcing memory has a long history in pedagogy. From the earliest teaching guides, educators have embraced a range of multi-sensory techniques in order to make learning richer and more motivating for learners. The term multisensory is used to refer to any learning activity that combines two or more sensory strategies/ modalities simultaneously to take in or express information. The sensory modalities include visual (si...\n\nFlexibility and Stability in Sensory Processing Revealed Using Visual-to-Auditory Sensory Substitution\n\nScience.gov (United States)\n\nHertz, Uri; Amedi, Amir\n\n2015-01-01\n\nThe classical view of sensory processing involves independent processing in sensory cortices and multisensory integration in associative areas. This hierarchical structure has been challenged by evidence of multisensory responses in sensory areas, and dynamic weighting of sensory inputs in associative areas, thus far reported independently. Here, we used a visual-to-auditory sensory substitution algorithm (SSA) to manipulate the information conveyed by sensory inputs while keeping the stimuli intact. During scan sessions before and after SSA learning, subjects were presented with visual images and auditory soundscapes. The findings reveal 2 dynamic processes. First, crossmodal attenuation of sensory cortices changed direction after SSA learning from visual attenuations of the auditory cortex to auditory attenuations of the visual cortex. Secondly, associative areas changed their sensory response profile from strongest response for visual to that for auditory. The interaction between these phenomena may play an important role in multisensory processing. Consistent features were also found in the sensory dominance in sensory areas and audiovisual convergence in associative area Middle Temporal Gyrus. These 2 factors allow for both stability and a fast, dynamic tuning of the system when required. PMID:24518756\n\nModulation of shark prey capture kinematics in response to sensory deprivation.\n\nScience.gov (United States)\n\nGardiner, Jayne M; Atema, Jelle; Hueter, Robert E; Motta, Philip J\n\n2017-02-01\n\nThe ability of predators to modulate prey capture in response to the size, location, and behavior of prey is critical to successful feeding on a variety of prey types. Modulating in response to changes in sensory information may be critical to successful foraging in a variety of environments. Three shark species with different feeding morphologies and behaviors were filmed using high-speed videography while capturing live prey: the ram-feeding blacktip shark, the ram-biting bonnethead, and the suction-feeding nurse shark. Sharks were examined intact and after sensory information was blocked (olfaction, vision, mechanoreception, and electroreception, alone and in combination), to elucidate the contribution of the senses to the kinematics of prey capture. In response to sensory deprivation, the blacktip shark demonstrated the greatest amount of modulation, followed by the nurse shark. In the absence of olfaction, blacktip sharks open the jaws slowly, suggestive of less motivation. Without lateral line cues, blacktip sharks capture prey from greater horizontal angles using increased ram. When visual cues are absent, blacktip sharks elevate the head earlier and to a greater degree, allowing them to overcome imprecise position of the prey relative to the mouth, and capture prey using decreased ram, while suction remains unchanged. When visual cues are absent, nurse sharks open the mouth wider, extend the labial cartilages further, and increase suction while simultaneously decreasing ram. Unlike some bony fish, neither species switches feeding modalities (i.e. from ram to suction or vice versa). Bonnetheads failed to open the mouth when electrosensory cues were blocked, but otherwise little to no modulation was found in this species. These results suggest that prey capture may be less plastic in elasmobranchs than in bony fishes, possibly due to anatomical differences, and that the ability to modulate feeding kinematics in response to available sensory information varies\n\nThe functional upregulation of piriform cortex is associated with cross-modal plasticity in loss of whisker tactile inputs.\n\nDirectory of Open Access Journals (Sweden)\n\nBing Ye\n\nFull Text Available Cross-modal plasticity is characterized as the hypersensitivity of remaining modalities after a sensory function is lost in rodents, which ensures their awareness to environmental changes. Cellular and molecular mechanisms underlying cross-modal sensory plasticity remain unclear. We aim to study the role of different types of neurons in cross-modal plasticity.In addition to behavioral tasks in mice, whole-cell recordings at the excitatory and inhibitory neurons, and their two-photon imaging, were conducted in piriform cortex. We produced a mouse model of cross-modal sensory plasticity that olfactory function was upregulated by trimming whiskers to deprive their sensory inputs. In the meantime of olfactory hypersensitivity, pyramidal neurons and excitatory synapses were functionally upregulated, as well as GABAergic cells and inhibitory synapses were downregulated in piriform cortex from the mice of cross-modal sensory plasticity, compared with controls. A crosswire connection between barrel cortex and piriform cortex was established in cross-modal plasticity.An upregulation of pyramidal neurons and a downregulation of GABAergic neurons strengthen the activities of neuronal networks in piriform cortex, which may be responsible for olfactory hypersensitivity after a loss of whisker tactile input. This finding provides the clues for developing therapeutic strategies to promote sensory recovery and substitution.\n\nShared cross-modal associations and the emergence of the lexicon\n\nOpenAIRE\n\nCuskley, Christine F.\n\n2013-01-01\n\nThis thesis centres around a sensory theory of protolanguage emergence, or STP. The STP proposes that shared biases to make associations between sensory modalities provided the basis for the emergence of a shared protolinguistic lexicon. Crucially, this lexicon would have been grounded in our perceptual systems, and thus fundamentally non-arbitrary. The foundation of such a lexicon lies in shared cross-modal associations: biases shared among language users to map properties in ...\n\nPARO robot affects diverse interaction modalities in group sensory therapy for older adults with dementia.\n\nScience.gov (United States)\n\nÅ abanoviÄ, Selma; Bennett, Casey C; Chang, Wan-Ling; Huber, Lesa\n\n2013-06-01\n\nWe evaluated the seal-like robot PARO in the context of multi-sensory behavioral therapy in a local nursing home. Participants were 10 elderly nursing home residents with varying levels of dementia. We report three principle findings from our observations of interactions between the residents, PARO, and a therapist during seven weekly therapy sessions. Firstly, we show PARO provides indirect benefits for users by increasing their activity in particular modalities of social interaction, including visual, verbal, and physical interaction, which vary between primary and non-primary interactors. Secondly, PARO's positive effects on older adults' activity levels show steady growth over the duration of our study, suggesting they are not due to short-term \"novelty effects.\" Finally, we show a variety of ways in which individual participants interacted with PARO and relate this to the \"interpretive flexibility\" of its design.\n\nThalamic control of sensory selection in divided attention.\n\nScience.gov (United States)\n\nWimmer, Ralf D; Schmitt, L Ian; Davidson, Thomas J; Nakajima, Miho; Deisseroth, Karl; Halassa, Michael M\n\n2015-10-29\n\nHow the brain selects appropriate sensory inputs and suppresses distractors is unknown. Given the well-established role of the prefrontal cortex (PFC) in executive function, its interactions with sensory cortical areas during attention have been hypothesized to control sensory selection. To test this idea and, more generally, dissect the circuits underlying sensory selection, we developed a cross-modal divided-attention task in mice that allowed genetic access to this cognitive process. By optogenetically perturbing PFC function in a temporally precise window, the ability of mice to select appropriately between conflicting visual and auditory stimuli was diminished. Equivalent sensory thalamocortical manipulations showed that behaviour was causally dependent on PFC interactions with the sensory thalamus, not sensory cortex. Consistent with this notion, we found neurons of the visual thalamic reticular nucleus (visTRN) to exhibit PFC-dependent changes in firing rate predictive of the modality selected. visTRN activity was causal to performance as confirmed by bidirectional optogenetic manipulations of this subnetwork. Using a combination of electrophysiology and intracellular chloride photometry, we demonstrated that visTRN dynamically controls visual thalamic gain through feedforward inhibition. Our experiments introduce a new subcortical model of sensory selection, in which the PFC biases thalamic reticular subnetworks to control thalamic sensory gain, selecting appropriate inputs for further processing.\n\nNo Sensory Compensation for Olfactory Memory: Differences between Blind and Sighted People\n\nOpenA"
    }
}
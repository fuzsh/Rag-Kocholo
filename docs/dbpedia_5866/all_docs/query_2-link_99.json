{
    "id": "dbpedia_5866_2",
    "rank": 99,
    "data": {
        "url": "https://www.local695.com/magazine_page_type/features/page/9/",
        "read_more_link": "",
        "language": "en",
        "title": "Features – Page 9",
        "top_image": "https://www.local695.com/wp-content/uploads/2020/09/cropped-favicon-e1600041568693-32x32.jpg",
        "meta_img": "https://www.local695.com/wp-content/uploads/2020/09/cropped-favicon-e1600041568693-32x32.jpg",
        "images": [
            "https://www.local695.com/wp-content/uploads/2019/12/2493_D044_00212_GRD2028329.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/2493_D051_00362.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/2493_D020_00050RJPG.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/2493_D025_00208.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/2493_D025_003412028129.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/2493_D062_0012920282292028129.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/2493_D034_00164RJPG.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/alexlowe_with_offcamera_readers.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/120-20Selaa20202880-BT.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/520-20Neve20542220Mixer20201.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/620-20Jim20Webb20Mixer2020web.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/320-20Sonosax2020SX-S.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/220-20Cooper2020CS-106.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/4-20Audio2020Developments20AD2003120web.jpg",
            "https://magazine.local695.com/sites/default/files/7%20-%20Bruce%20Bisenz%20%20Mixer%20web.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/820-20Studer201692020Mixer.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/server_roomweb.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/4117_D021_11177_R_CROP1532471367web.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/4117_D011_05049_R_CROPweb.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/4117_D001_00101_R_CROPweb.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/4117_PP_D002_02137_R1531331238web.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/4117_D018_09780_Rweb.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/4117_D017_09462_R1531331226web.jpg",
            "https://magazine.local695.com/sites/default/files/4117_D017_09176_R1531331225web_0.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/4117_D007_03281_R1531331226web.jpg",
            "https://magazine.local695.com/sites/default/files/4117_D025_13343_R_CROP1531331236web.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/rev-1-ASIB-15481r_WEB.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/rev-1-ASIB-08455_WEB.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/IMG_2111web-1.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/rev-1-ASIB-TRL-9090r_WEB.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/IMG_2120web.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/IMG_2136JPG-scaled.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/IMG_2122JPG-1-1024x768.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/IMG_2252JPG-1-768x1024.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/IMG_2313web.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/mi6-ff-h055rb_web.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/mdj-12106-web.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/mi6-ff-u017rweb.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/mcj-web.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/IMG_4354JPG.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/mdj-14928r_web.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/mdj-04679-srweb.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/Jim20McBride_TestingJPG.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/Lloyd20and20Hosea.jpeg",
            "https://www.local695.com/wp-content/uploads/2019/12/Lloyd20and20me20Norway.jpeg",
            "https://www.local695.com/wp-content/uploads/2019/12/Chris20Munro202_HaloNEW-883x1024.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/mi6-ff-uhd015WEB.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/Chris20Munro202_Helicopter-922x1024.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/4-COVER-IST_March_1953201.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/1-IST_March_19532014.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/2-IST_March_19532015.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/3-IST_March_19532020.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/220-20Western20Electric20Manuals202.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/320-20Western20Electric2022-C.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/820-20RCA20BC-5.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/620-20Westrex20RA-148520Tea20Cart.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/13-20Nagra20BMII20Mixer-Rear.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/1520-20Sela20Mixer.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/4-20Westrex20RA-152420MixerJPG.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/920-Girardin20MT3420Mixer.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/1020-20Westrex20RA-1518-A20Stereophonic20Mixer_Page_1.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/14-20Sennheiser20M101201.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/IMG_1267Disney20Ranch_1.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/10348380_716721975069736_1860001400706681188_n_2.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/1535618_10202874286449034_944775256_n2028129.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/69112_450614874783_6057355_n.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/Prizzi27s20Honor.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/Down20n20Out.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/IMG_1266DTLA.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/17015868_10211978967780377_2160972636079554480_o.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/IMG_2887.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/Y-16A-Kendra-Bates-showing-up-for-work-471x345.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/YWC20Hike202.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/PAC20fundraiser201.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/YWC20Hike201.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/Figure-6.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/January-1934-Cover.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/Cover-Fall-2010.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/RCA-KU-3A.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/CHINHDA.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/ChinhdaHenryEmbry.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/AoLooCart.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/20180520-DSC_6323.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/ProRes-RAW-_-inline-featured-640x360@2x-1024x576.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/02GUIDE7-jumbo.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/nsnwdecon16.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/IMG_Bus-view.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/IMG_7751.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/IMG_6010.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/bus.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/yellowstone_day13_em_195-5_R.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/SEP18_102_R.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/OCT12_YS_056_R.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/OCT26_YS_315_R.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/IMG_1162.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/IMG_1609.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/fullsizeoutput_13c4.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/IMG_1254.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/IMG_1507.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/DF-24970_R_crop.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/DF-00309-640x227.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/DF-12297-640x430.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/NOR_D01_053017_9465-641x387.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/NOR_D11_061317_1672-Edit_R-640x287.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/NOR_D18_062217_064158-640x427.jpg",
            "https://www.local695.com/wp-content/uploads/2019/12/NOR_D00_052317_7961_R-640x390.jpg",
            "https://www.local695.com/wp-content/uploads/2020/03/footer-play-stop-7.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "https://www.local695.com/wp-content/uploads/2020/09/cropped-favicon-e1600041568693-32x32.jpg",
        "meta_site_name": "",
        "canonical_link": "https://www.local695.com/magazine_page_type/features/page/9/",
        "text": "Mission Critical\n\nSound Mixes an Emotional Journey for Damien Chazelle’s First Man\n\nby Daron James\n\nOn July 16, 1969, Apollo 11 launched from Florida’s Kennedy Space Center carrying three astronauts—Neil Armstrong, Edwin Aldrin, and Michael Collins—their destination; the moon, a mere 240,000 miles away. Four days later at 10:56 PM ET, Neil Armstrong stepped onto the lunar surface uttering his now famous words to a billion people listening at home.\n\nThat’s one small step for man, one giant leap for mankind.\n\nIn First Man, Director Damien Chazelle (Whiplash, La La Land) viscerally explores the story behind the mission to the moon, immersing us in the life of Neil Armstrong (Ryan Gosling)—his marriage to Janet (Claire Foy), being a father of three, and the tribulations leading up to the historic event.\n\nVisually, Chazelle and Cinematographer Linus Sandgren leaned on a dynamic style tapping three different film formats to distinguish story elements. 16mm emphasized Armstrong’s early life and spacecraft interiors. 35mm captured their El Lago, Texas, home, NASA, and shuttle exteriors. When the Apollo 11 door opens up the moon, it shifts from 16mm to 70mm IMAX.\n\n“The film was broken into two halves,” says Production Sound Mixer Mary H. Ellis CAS, known her work on Prisoners and Baby Driver. “The first half was Armstrong’s life on the ground and the second half was the spacecraft work and moon landing.” Along with Ellis were Boom Operator James Peterson, Sound Utility Nikki Dengel and Sound Playback Alexander Lowe and Raegan Wexler.\n\nAn early rehearsal before production introduced the shooting style to the sound crew. Chazelle, wanting a realistic portrayal, proposed that all camera movement—except when on the moon—be handheld, cinéma vérité style.\n\nOne rehearsed scene intimately placed Armstrong’s two-year-old daughter Karen (Lucy Stafford) in his arms, hugging her as he circled. Only the two actors, director, cinematographer and Peterson were allowed in the room. To record audio, Peterson was given a Sound Devices 788T to place around his neck to track the rehearsal, which ended up in the final version of the film. “We put a lav pretty much on everyone all day every day, but we never wired Lucy. Damien didn’t want her being aware of any of us,” notes Ellis. “The rehearsal helped a lot. It allowed James to get used to Linus’s body language operating the camera as he would spin 180° and widen out the lens often.”\n\nShot primarily in Atlanta, Georgia, on practical locations and stages, production did travel to Edwards Air Force Base in California to recreate Armstrong’s X-15 flight take-off and landing that opens the film and another day at the Johnson Space Center in Houston.\n\nThe busiest days for sound took place on the mission control set, a vast replica of the Johnson Space Center by Production Designer Nathan Crawley. The complex scene brought us inside the command center as the cosmonauts rocket toward the moon. As many as twenty-three actors needed to be wired at once in order to cover the dialog. Ellis brought in Production Sound Mixer Michael P. Clark (Stranger Things, The Walking Dead) to help head the task.\n\n“I insisted on a rehearsal two days prior to shooting as there would be a limited amount of time on production days,” says Ellis. “We wanted to just slap the mic on the actors and find out how everything was.” It allowed the sound mixer to create a seating chart of the actors where Ellis mixed the top eight and Clark took the other fifteen.\n\n“I knew once Damien got going, he would upgrade non-speaking parts to speaking ones, so I warned everyone about it. We had to be careful when stealing a microphone from someone to be sure they were not going to play in a specific part,” Ellis continues. “All the actors were fitted with a Sanken COS-11D. Each wire had its own ISO track and the mix was kept consistent no matter what happened on the page.”\n\nTo record the dialog and for communication between the director and actors, an intricate setup was configured that included off-camera readers. “Alex [Lowe] was fundamental in all of this,” says Ellis. Lowe created three different mix options to route through. Sound also accounted for each actor’s preference in terms of who they wanted to hear and what they wanted for playback. For instance, Ben Owen, who played John Hodge, wanted to hear all twenty-three microphones at once in his earwig to feel the sense of urgency and chaos in the room.\n\nRecording dialog inside each spacecraft was a different technical story. An early concern for sound was the multitude of spacesuits and helmets as wardrobe. The film moves from 1961 through 1969 and details five missions, including the X-15 flight, Gemini V, Gemini VIII, Apollo 1, and Apollo 11. Costumer Mary Zophres researched and duplicated each look, even creating two suits for Apollo 11, one for each actor and the other for their stunt double.\n\n“In prep I spoke with Whit Norris and Mark Weingarten, people who had done helmet movies before to find out what they’ve accomplished, but I learned they didn’t have to worry about the period piece part of it like we did. We didn’t have as many wiring options so we planned different strategies for when we could get our hands on the helmets,” says Ellis. “We ended up buying four new mics and had a quick release made right at their neckline because the minute the actors could, they would take off their helmets.”\n\nThe spacecraft modules were built for actual size. They were tiny, and once an actor was inside, it was impossible to adjust the wireless microphone. “Our other concern was about airflow and how loud it was going to be inside the helmet. You have to have enough air for the actors so they don’t pass out but it can lead to condensation,” says Ellis.\n\n“Instead of a regulated system, they had an air compressor pushing air into the helmet. It was all or nothing and very loud,” notes Lowe. “Mary sent me separate feeds that I gated open when they talked or reduced air noise. I sent the actors a feed of their own off-camera reader, the feed of the other actors, but not themselves, and mission control comms to all. When Damien talked, it shut down every feed, including their own so they could hear his direction. I also routed the First ADs voice of god to any one of them if needed. All this was done each day. I had to break it all down every night and set it up again the next day. It took two hours.”\n\nEarwigs were not used inside the helmets because if they went out, 108 dB of white noise would blast into the actor’s ear. Instead, Comteks were hardwired inside a prop earwig and set to the earwig frequency and surveillance systems for sound to have complete control over. “The great thing about this was the batteries lasted all day as the actors could be in the capsules for up to seven hours. Also, I could change a battery without taking off the suit in case one failed, though that never happened,” says Lowe.\n\nAdditionally, the original launch day recordings from NASA came into play on set when actors wanted to listen to the delivery. “Ryan was very particular about mimicking Neil’s inflections, specifically when we were on the moon,” notes Lowe. “I fed Ryan a recording of Neil and he would work out his moves with the dialog.”\n\nTo find the right mic placement for Gosling, the actor was all about experimenting and finding the right levels. “Ryan doesn’t like to do any ADR, so we needed to find the right balance between the air level and audio level so he wasn’t looping two months of capsule work,” says Ellis. Another point of emphasis was placing plant mics as ISO tracks outside the gimbals as they got creakier for post.\n\nFor its moon landing, production took over the Vulcan Rock Quarry, a rock quarry south of Atlanta. The shoot took place outdoors in December and at night. Sound approached the work utilizing wires instead of booms to give the actors solitude. “It was a real internal moment for them so we wanted to give them as much space as possible,” says Ellis.\n\nReflecting on the show, Ellis admits Sandgren gave them some challenging situations. “He would always come over to say sorry but he didn’t have to. He had an amazing team and we were able to have this really great dance together thanks to the crew I had around me.”\n\nAll photos: Daniel McFadden/Universal Pictures and DreamWorks Pictures\n\nThe 1970’s\n\nWhile the 1960’s saw some further advances in the techniques of both production sound recording and re-recording, it wasn’t until the 1970’s that some of the nascent technologies developed for music recording began to make inroads into the film industry. Although stereo and surround sound were nothing new (going all the way back to the early 1950’s), the films released in either four-track 35mm Cinemascope mag or six-track 70mm mag were limited to major roadshow titles like 2001: A Space Odyssey and Woodstock. Prints were extremely expensive, and the number of theaters equipped to run either 35mm mag or 70mm were typically limited to major cities. And even with the advent of these technologies, theater loudspeaker systems hadn’t really evolved much past the technologies of the late 1940’s and early 1950’s. Despite the extraordinary quality of 70mm magnetic, the Academy curve was still the norm, with its severe rolloff of high frequencies.\n\nOther changes were beginning to take place in the 1970’s as well. Audiences had become more sophisticated in relation to sound. A new generation of music listeners had become accustom to high-quality home sound systems, FM radio began to take off, the quality of the compact cassette improved, and those with the means invested in recorders to listen to four-track reel-to-reel releases. Audiences of this generation were not going to be satisfied with the sound of a theater system developed two decades ago. Commensurately, theater attendance was in decline, and studios were looking for ways to attract a younger audience.\n\nIt was against this backdrop that a number of advances in film sound took place. Most notable among these was the introduction of Dolby noise reduction in the post-production stages (first used on Stanley Kubrick’s A Clockwork Orange in 1971). While the film was originally released in Academy mono (due primarily to Kubrick’s concern regarding how many theaters would be able to play stereo optical), it was clear from the tests done at Elstree Studios that the quality of sound could be markedly improved if the process could be applied to the optical track itself. Further development was done at Dolby Labs over the next few years, which culminated with the release of Lisztomania in 1975 and Star Wars in 1977.\n\nAlso notable was the introduction of Sensurround by Universal Studios, which was first used for the movie Earthquake in 1974.\n\nAnd perhaps most important in the realm of production sound recording, 1975 marked the year that Robert Altman’s movie Nashville was released, significant both for its use of multitrack dialog (with stellar work by mixer Jim Webb), in addition to live multitrack music recording (utilizing a remote truck built by the author).\n\nWhile multitrack dialog recording was not exactly new per se (having been used for the production of three-channel Cinemascope films), the use of multitrack for production sound would mostly be limited to Robert Altman films for nearly two decades. It did, however, help to spur a move to a more sophisticated approach to production sound, which was still largely done on mono Nagra recorders (despite the introduction of the stereo Nagra 1970).\n\nWith the introduction of op-amp technologies, mixer designs began to take on a significant change in design philosophy during the 1970’s. These advances, along with more sophisticated printed circuit board designs and smaller components, made possible more compact mixers with less current draw than their predecessors. It also heralded the adoption of a modular approach to console design, with components separated into input modules, master modules, buss assignment modules, and monitor modules. While these approaches were at first destined for the music and broadcast world, it wasn’t long before they were adopted by manufacturers engaged in designing mixers for the film industry. This was due in no small part to the increase in the channel counts of film dubbing stages, which were beginning to increase with the advent of Dolby stereo in 1975.\n\nThe same approach was also used for smaller production sound mixers, with more limited facilities. The 1970’s would also mark an era that would see a more ready adoption of European film sound equipment by US sound mixers. Although companies such as Sennheiser and Neumann had made inroads into the United States with their microphones (primarily for music recording), and Nagra with portable recorders, up until the seventies, if you walked into most film sound operations, nearly everything you saw was of US manufacture.\n\nIn the early seventies, there were still not many choices when it came to lightweight production mixers (the Nagra BM-T and Sela 2880-BT not withstanding). For stage work, it was still common in Hollywood to see mixers made by both Westrex and RCA dating back at least a decade (with many custom variants) used on set. As the move to location shooting became more prevalent, sound mixers started looking for alternatives to the bulky production boards typically used for stage work.\n\nHowever, there were some alternatives for those who wanted to take a bit different approach, which in many cases involved doing a bit of customizing. Notable among these were the following:\n\nThe Sennheiser M101 mixer, a four-input, mono-output mixer with built-in battery supply and T power, which was first introduced in 1969, but took a little time to catch on in the US market. Some enterprising individuals would also customize these boards into a six-input configuration.\n\nThe Stellavox AMI mixer, a five-input, two-output mixer introduced in 1971. Designed by former Nagra engineer Georges Quellet, this was intended as a companion piece to the Stellavox SP7 recorder.\n\nThe Audio Developments AD031 “Pico” mixer, which could be supplied in a few different configurations, and utilized a 24-volt power supply.\n\nThe Neve 5422 “suitcase mixer” brought to market in 1977, and intended primarily for use in location music recording and broadcast.\n\nThe Studer 169/269 series mixers, introduced circa 1978, and which could be ordered in a variety of configurations. Intended primarily as a location broadcast console for the European market, this console could be either AC- or battery-powered. While prized for its sonics by music engineers, it was only used by a handful of production mixers in the States (due in no small part to its size and weight).\n\nAs amplifier technology evolved and components became smaller, it allowed designers the luxury of adding more features, including three-band equalization, better high-pass filters, better mic preamps, and more sophisticated signal routing. It also marked the move away from the traditional four-input mixer, which had dominated production sound recording for nearly four decades. Still, production sound equipment had to be portable, which limited the sort of features that would be found standard on even fairly rudimentary re-recording consoles of the period.\n\nA (very few) ambitious sound mixers also took it upon themselves to build or commission mixers to their liking from scratch or perform significant modifications to mixers that were designed for other purposes.\n\nThe 1970’s also saw an extensive adoption of straight-line faders, which had moved from wire-wound designs to carbon composition resistive elements. While early straight-line faders were prone to problems when used under unfavorable conditions, the new faders were both smaller and more reliable. In addition, sound mixers who began their careers in music recording or post production were more receptive to using them for production work. By the end of the decade, nobody except Sela were manufacturing location mixers with rotary faders.\n\nThe 1980’s\n\nDespite the fact that the seventies saw a host of developments in film sound recording, it didn’t translate into very many changes in the sound mixing techniques and equipment used for production work. Most of the advances made in the previous decade were in the area of re-recording, as well as the advent of Dolby Stereo on optical tracks (Stereo Variable Area), which allowed studios to release titles in L/C/R/S stereo without the need for four-track magnetic release prints. Since the optical tracks could be printed and processed on standard laboratory equipment, it greatly reduced the costs associated with making a stereo release.\n\nAs such (with the notable exception of Robert Altman), most production sound packages still consisted of a four- or six-input mixer, perhaps mated with a Nagra stereo recorder with Dolby noise reduction, and four channels of wireless. And for most productions, this was sufficient. Even with the introduction of the Sony PCM-F1 in 1981 and DAT in 1987 (both being two-channel formats), there was no compelling reason to change the basic approach used for production recording.\n\nWhile some sound mixers (including this author) opted to use somewhat larger consoles intended for broadcast and remote music recording, there weren’t really many options available to the industry until the introduction of the Sonosax SX-S in 1983, and the Cooper CS-106 mixer in 1989. Like most equipment destined for the highly specialized film market, these mixers were designed by individuals who had a dedication to producing high-quality sound recording equipment specifically for the film industry.\n\nThe Sonosax SX-S mixer was the brainchild of Swiss engineer Jacques Sax, who had begun his career as a live sound mixer. Frustrated with what was available on the market at the time, he took it upon himself to design something that was more to his liking, beginning with the SX-B mixer in 1980, and culminating with the current SX-ST series consoles.\n\nThe Cooper 106 was designed and built by Andy Cooper, who besides being a bright designer, was also cognizant of the particular needs of the film production market. So instead of designing something that he “thought” represented the needs of production mixers, he actually went out and took the time to talk with notable mixers of the era (a lesson that some manufacturers still have yet to learn).\n\nThe Cooper CS-106 marked a fairly significant departure from anything else available at the time. With straight-line faders, the option for seven inputs, three-band EQ, a lightweight chassis, DC powering, sophisticated monitoring and signal routing functions, the Cooper mixer embodied much of what production mixers had been looking for at the time.\n\nFilm sound being a very small slice of the overall worldwide audio market, larger manufacturers simply weren’t interested in developing a highly labor- and design-intensive console for a small market segment, when there were much bigger rewards to be reaped in the studio, sound reinforcement, and broadcast markets. Many of the consoles built by Sonosax and Cooper Sound are still in use nearly three decades later, which attests to Jacques and Andy’s strengths as careful designers who understood the rigors of film production.\n\nThere were of course, other options available in the eighties. Audio Developments continued with their line of portable mixers, which included the AD 062 and AD 075 series. Sony actually introduced a twelve-input mixer, the MXP-61, which had some features such as 12-volt T-powered mic inputs, which were clearly aimed at the film production market, but didn’t generate a lot of sales.\n\nThere were also some entries in the portable “bag rig” market, most notably by the British company SQN, which introduced the SQN-4S mixer.\n\nBeing the highly individual craft that production recording is, many sound mixers weren’t content with what was offered on the commercial market, and opted to design something that suited their personal approach to production recording, or make extensive modifications to stock consoles. Not everyone who sat at a mixing board had the kind of electronic background to undertake this sort of task however.\n\nHighly customized mixer designed and built by Bruce Bisenz in the 1980’s, utilizing Nagra mic preamps. Note the modified Altec graphic equalizer, with one octave band intended for dialog EQ, and the group buss assignments. Photo courtesy Bruce Bisenz\n\nAmong the few who took on this challenge during the seventies and eighties include David Ronne, an Academy Award-winning production mixer (who also designed the RollLogic remote control). Bruce Bisenz, who built a highly customized console from the ground up, and Jim Webb, who commissioned a console to his liking that was built by Jack Cashin. The list goes on…\n\nThere were also sound mixers such as Nelson Stoll, Ray Cymoszinski, Michael Evje, and others who decided they loved the big sound of the Neve consoles, and took it upon themselves to modify the boards to their liking for film work. Others (including the author) opted for the modular configurations offered by the Studer 169/269 series consoles.\n\nThe important thing to note in this regard is that every one of these sound mixers had a particular approach to the challenges of doing production sound under all kinds of conditions, and wanted a console that would give them the most flexibility and best sound quality for their style. In a world that has now become defined by the stock offering of various manufacturers, the “signature sound” that many mixers had sought to achieve during this period has now become lost.\n\nNext up, “The Nineties.”\n\n–Scott D. Smith CAS\n\nWith sincere appreciation to Jeff Wexler CAS for invaluable contributions in style and content.\n\nby James Delhauer\n\nOn a set, the job of the person who is tasked with acquiring the content that is shot throughout the day is incredibly stressful. Whether we’re discussing the tape operators of days gone by or the most modern media recordists, there are challenges that have stood the test of time. Somewhere between hundreds of thousands and hundreds of millions of dollars are spent assembling the production. Countless man-hours contribute to making it the very best that it can be. Literal blood, sweat, and tears are spilled to create what we all hope will be a veritable work of art. Then, after all of that, it falls on the shoulders of the one person who is tasked with handling the media. They are simply given very delicate assets that have been created throughout the day and which represent the sum total of the production as a whole. Just about anything can go wrong. Data can be corrupted. Hard drives can be damaged. Video tape can tear. Fortunately, these risks are being minimized by the advent of a new method of media acquisition: server-based recording.\n\nThough different productions utilize a vast array of workflows, every single one since the Roundhay Garden Scene was first filmed in 1888, has come down to the media. And every single production needs someone to manage it. In today’s digital era, the most common workflow goes a little something like this. Cameras or external recorders capture video and audio data to an internal storage device some sort. When that unit is full, it is ejected and turned over to a media manager. The production continues with another memory card while the media manager takes the first one and offloads, backs up, and verifies the files on it. This is usually done with an intermediate program such as Pomfort’s Silverstack or Imagine’s Shotput Pro—programs that can do file comparisons to ensure that what was on the source media is identical to what ends up on the target media. When all of that content is secured on multiple external hard drives, the original memory card is returned to the production so that it can be wiped and reused. Rinse and repeat. At the end of each day, the media manager turns over at least one set of drives containing the day’s work to someone who will bring it to a post-production facility.\n\nThere, the work is moved from these temporary storage drives onto work servers, where assistant editors can begin their work.\n\nWhile prominent, this workflow does come with a few inherent drawbacks. Most notably, the process is both fragile and time-consuming. Digital storage, no matter how sophisticated, is vulnerable to failure, damage, or theft. When the media manager receives a card with part of the day’s work on it, that card is often the only raw copy of the work in existence. Careers could end in a heartbeat if anything were to happen to it. So it becomes his or her job to create multiple copies. Unfortunately, the time during which data transfers from one storage system to another is the time at which it is most vulnerable. An accidentally yanked cable or sudden power surge is all it takes to corrupt the open files as they are transferring over. This vulnerability is compounded by the fact that transferring files is time-consuming and becoming ever more so. As our industry continues to push the boundaries of resolution, color science, and bit depths, video files are getting bigger and bigger. As such, they require more time to offload, duplicate, and verify, meaning that the period of vulnerability is growing longer.\n\nBut emerging technologies are creating new workflows that circumvent these drawbacks. Among the most promising is server-based recording.\n\nRather than relying on disparate components that must be passed back and forth between different individuals on a set, server-based recording allows productions to streamline their workflows and unify everything through one interconnected system. All of the components can be plugged into a single network switch and communicate with one another directly. Cameras and audio devices send uncompressed media directly into the switch. The network feeds them into a digital recording server (such as a Pronology’s mRes or Sony’s PWS 4500), which takes the uncompressed data and encodes the signals into ready to edit files. These files are then sent back into the network, which in turn sends them to any desired network-attached storage devices (such as SmallTree’s TZ5 or Avid’s ISIS & NEXIS platforms). The moment the recordist hits the Stop button, he or she can open the files on a computer and bring the newly created clips into a nonlinear editing application in order to assess their viability. This method eliminates the intermediate process of utilizing memory cards, transfer stations, and shuttle drives in favor of writing directly to external storage and thus removes both the time and risk associated with manual offloading. It also offers instant piece of mind to both the person handling the media and the production as a whole that the work that has been done throughout the day is, in fact, intact and ready for post-production.\n\nAnd this is only the most basic of network-based workflows.\n\nBy utilizing advanced encoder systems, such as the aforementioned mRes platform, multiple tiers of files can be distributed across multiple pieces of network-attached storage. This gives the recordist the ability to simultaneously create both high-quality and proxy-grade video files and to make multiple copies of each in real time as a scene is being shot. This eliminates the potential need for time-consuming transcodes after the fact and, more importantly, this instant redundancy removes the key period of danger in which only a single fragile copy of the production’s work exists. As a result, recordists can now unmount network drives mere minutes after productions wrap and turn them over for delivery to post with one hundred percent certainty that there are multiple functioning copies of their work from the day. There is no need to spend several hours after wrap each day offloading cards and making backups.\n\nOr, to take things a step further, productions can take advantage of the inherent beauty that is the internet to skip the need for the shuttle process altogether. It is possible to create files in a manner that sends them directly to a post-production edit bay. With low bitrate files or a high-capacity upload pipeline, recordists can set up their workstations using transfer clients (such as Signiant Agent or File Catalyst) to take files that are created in a particular folder on their network-attached storage and automatically upload them to a cloud-based server, where post-production teams can download them for use. This process has the distinct advantage of sending editors new files throughout the day in order to accommodate a tight turnaround.\n\nConversely, for productions where the post-production team may be located on site, a hard line can be run from the recording network directly to the edit bays. By assigning the post team’s ISIS server (or comparable network attached server) as a recording destination, editors gain access to files while they are recording. In cases such as this, the production may opt to use “growing” Avid DNxHD files. This format takes advantage of Avid’s Advanced Authoring Format in order to routinely “close” and “reopen” files, allowing editors to work with them while they are still being recorded. For productions with incredibly tight turnarounds, this is the single fastest production to post-production workflow possible.\n\nAll of this makes server-based recording an incredibly versatile tool. However, it is not without its limitations. At this time, network-based encoders are limited to encoding widely available intermediate or delivery codecs, such as Apple ProRes or Avid DNxHD. Without direct support from companies with their own proprietary formats, they cannot output in formats such as REDCODE or ARRIRAW. Furthermore, setting up a network of this nature requires persistent power and space. It is also worth considering that, like most new technologies, server-based recording often comes with a hefty price tag. These limitations make the process unsuited for productions hoping to take advantage of the full range of Red and Arri cameras, productions in remote or isolated locations, and low-budget productions.\n\nSo when is it most appropriate or necessary to take advantage of this emerging technology? While it can be of use in a single-camera environment, this method of recording truly shines in live or archaically termed “live to tape” multi-cam environments, where anywhere from three to several dozen cameras are in use. After all, if a show records twelve cameras for one hour, the media manager suddenly has to juggle twelve hours’ worth of content. It is much easier to write all twelve to a network-attached storage unit than to offload all twelve cards one by one. Also, due to the fact that network-attached storage drives can be configured to store hundreds of terabytes, the process is ideally suited for live events or sports broadcasts where stopping and starting the records risks missing key one time only moments. But above all, it is best used when time is critical. The ability to bring files into a nonlinear editing system as they are being recorded and work in real time is a game changer for media managers, producers, and editors alike.\n\nThis technology is already revolutionizing the way television productions approach on-set media capture and it is still in its infancy. It will continue to grow and evolve. Given time, it is my sincere hope that it will find its way into the feature film market and become more practical for smaller productions to adopt. For the time being, Local 695 Video Engineers should begin to take note of what is available and familiarize themselves with the technology so that they are prepared to take advantage of the technology in the future.\n\nTruth and Action\n\nSound mixes a moving palette for Spike Lee’s new joint\n\nby Daron James\n\nThe Civil Rights movement in 1950s and 1960s America was a tinderbox ready to explode, then in the ’70s, it continued with the emergence of the Black Power movement. The latter is the historic setting for BlacKkKlansman, a taught sociopolitical film from director Spike Lee.\n\nBased on the book Black Klansman by Ron Stallworth, the first African-American detective in the Colorado Springs Police Department, the adapted screenplay follows the true story of Stallworth’s (John David Washington) infiltration of the Ku Klux Klan and his eventual take down of an extremist hate group.\n\nProduction Sound Mixer Drew Kunin, Boom Operator Mark Goodermote, and Utility Marsha Smith took on the project, a paced schedule lasting from October to December 2017 that included a glut of filming locations in New York and a jaunt VFX stop in Colorado to blend Centennial State exteriors into the Big Apple.\n\nThe movie opens up in black-and-white, featuring Alec Baldwin as a bigot-spewing racist; a 16mm projector beams images of his message overlapping his face and onto the wall behind him—the noise of the machine pierces through the soundscape. On set, everything was done live with no visual effects. It meant the clacking of the projector would compete with Baldwin’s dialog. “I didn’t know how loud Alec was going to be, which turned out to be very,” says Kunin. “I was a bit surprised when he launched into it—it was a little bit of wild ride to keep his level from overloading, but we were lucky he had enough volume that overrode the projector.” Sound ran a Schoeps CMIT-5U as boom and placed a lav for dialog.\n\nKunin uses a mix of DPA, Sanken, and Countryman lavs with Lectrosonics wireless on projects. His general mantra being to only wire when necessary, however, on BlacKkKlansman, they went ahead and wired everyone to be safe. Multiple roaming cameras shooting wide and tight coverage were an acting catalyst, but also being the team’s first project with Lee, they didn’t want to interrupt the rhythm with the need of adding a lav after the fact.\n\nWe first meet Stallworth working in the file room of the police department but he soon receives his first undercover assignment to attend a lecture delivered by Black Panther Party philosopher Kwame Ture aka Stokely Carmichael (Corey Hawkins). It’s here he meets Patrice (Laura Harrier), a fiery activist Stallworth warms to. Inside the assembly hall, hundreds of extras look on, shouting with praised remarks during Ture’s moving speech.\n\nSound couldn’t place a microphone overhead Ture because of wide camera shots. Instead, the period microphone at the lectern was made live and an additional plant mic was hidden. Another boom captured reactions of the crowd.\n\nFor that scene in post, Re-recording Mixer Tom Fleischman, a longtime collaborator of Lee’s, augmented the speech with a bit a reverb to add to the size of the room and layered dozens of responses from the crowd. “Drew turned in production tracks that were really well recorded. When you have a good track, it makes my job easier,” says Fleischman.\n\nThe challenge of the Ture speech scene in post was making sure the audience callouts felt like there were actually in the room and not ADR. Mixing in 7.1 made it easier for Fleischman, then it became a matter of going through it to make it sound natural.\n\n“It could have easily become quite a noisy scene, but my philosophy at any given moment in any film is that there is one sound that needs to be in the forefront,” says Fleischman. “We needed to balance the scene in a way that made sure every line was intelligible so that the underlying track sounded real to the audience and not like they were hearing something in a vacuum. The whole idea for me is story and keeping the audience involved and not letting them be distracted by any sound element.”\n\nIf you listen closely to the scene, you will hear a “boom shakalaka” from the crowd. That’s actually Lee’s voice. During the mix, the director asked Fleischman to grab a mic so he could record the audio and found spots for it in the scene.\n\nAfter that initial assignment, Stallworth starts his own undercover operation after stumbling across a newspaper ad from the Ku Klux Klan seeking new members. Stallworth calls the number on the notice and David Duke (Topher Grace), the leader of the hate group, actually picks up the line. Disguising his voice, Duke thinks Stallworth is a racist white man and invites him in the inner circle.\n\nTo shoot these scenes, Production Designer Curt Beech built two sets outfitted with period-appropriate props on NY stages so production could shoot Stallworth and Duke simultaneously. Sound recorded both actors’ dialog simultaneously as well. “We placed mics overhead on both ends, plus we tapped the telephone line on a separate track for editorial to play with,” says Kunin, whose cart setup is based around a Sonosax mixer and an Aaton Cantar X-3 recorder. All Fleischman had to do was “filter down the track a little” to make it sound more like it was coming from a phone.\n\nStallworth, now a member of the KKK, has one problem: he’s black. To be the face of the operation, he asks fellow detective Flip (Adam Driver) to pose as Ron, where he meets the leader of the local chapter, Walter (Ryan Eggold). Stallworth asks Flip to wear a wire and Kunin was able to find a few in the style of the old BCM 30 to put on camera, which added to the complexity of lav placement.\n\nCostumes from designer Marci Rodgers posed a different challenge as they lauded the fashion of the time. Stallworth wore lush colors, jazzy prints, and mixed textures of denim, velvet shirts, silky button-downs, suede vests, and leather jackets. To lav Washington, center chest became the default position to avoid material movement leaking into the track.\n\nFor Patrice, she was dressed in long leather jackets, dark turtlenecks, mini-dresses, and knee-high boots, among others. “Laura was a little tricky to mic,” admits Kunin. “Not because of the material she was wearing but because it was hard to hide a mic. Marsha worked with wardrobe to sew in special compartment to place in the bodypacks.”\n\nSound had to pay close attention during the nightclub scenes Ron and Patrice go to hang, talk, and dance. For dialog to be mixed cleanly, Kunin dropped the song to capture the dialog and used a thumper to aid the beat. Music is a big part of Lee’s storytelling. Besides the musical soundtrack that includes “Too Late to Turn Back Now” by Eddie Cornelius and “Oh Happy Day” by Edwin Hawkins, the director tapped Terence Blanchard (Malcom X, 25th Hour) for its score.\n\n“When it comes to writing music, I let the film tell the story,” says Blanchard. “The first thing I thought about when I saw a cut was Jimi Hendrix playing the National Anthem on guitar. Being an African-American, you’re constantly bombarded with issue of bigotry every day. This story is a reaffirmation of what we’re going though. Jimi was a primal scream for all of us, so it’s why the electric guitar plays a prominent role in the sound of the score.”\n\nFlip, now deep in the local Klan chapter, attends meetings at the home of Felix (Jasper Pääkkönen), a follower who wants to put words into action. It’s here the undercover detectives learn Felix is planning a plot to spoil another activists meeting; one that involves a character played by Harry Belafonte, an icon of the Civil Rights movement.\n\nFelix’s residence, a practical location found upstate in Ossining, New York, was small and scenes were filled with multiple actors and multiple cameras. At times, squeezing in a boom operator was not possible, especially when Felix puts Flip through a lie detector test in a broom closet of a room. It meant sound had to rely on plant mics and lavs to cover the dialog. In other tight locations, gaps in the wall allowed to place the boom in the room but not Goodermote.\n\nLeading up to the climax of the movie, picture intercuts two different story plots. On one side, you have Flip being initiated into the Ku Klux Klan, where a crowd gleefully cheers during a screening of 1915’s The Birth of a Nation. On the other, a group from Patrice’s African-American student union peacefully sits around Belafonte as he delivers the most galvanizing moment in the movie; a recounting of the lynching of Jesse Washington he witnessed as a young man. “To see him was a very powerful moment,” says Kunin. “Working on that scene had so much gravity to it, we took extra precaution in our approach.” In recording Belafonte’s dialog, sound let the cameras set up its shots, then they strategically placed an extra in front of a plant mic for additional recording.\n\nThough backdropped in the mid-’70s, the film is not only about the past but about the state of how we’re living today. Nothing couldn’t be more evident than the film’s final sequence; a collection of uncensored videos from the Charlottesville protests. Lee left the material untouched. “What you hear is the sound straight out of the smartphones and online videos. There’s no foley or effects added,” says Fleischman. “We only mixed in the score and it plays against the raw sound really strongly.” Blanchard notes, “That’s classic Spike. He makes a statement about what’s going on in our country and leaves you there to think about it.”\n\nby Steve Morrow\n\nFrom my very first meeting, it was obvious that Bradley Cooper knew he wanted A Star Is Born to feel real and immersive. He certainly achieved that in this film, with handheld camera work and live vocals, he leads us into a world that feels simultaneously epic and intimately authentic.\n\nAs a director, Cooper cultivated a great atmosphere on set that was familial and inspiring to work in. Communication and collaboration were paramount for him, which fostered an environment where every member of the cast and crew could perform at their best.\n\nThere was never any doubt that Bradley Cooper and Lady Gaga would be singing live. Neither of them wanted the film to feel like a traditional musical and there would be no lip-syncing to playback. For me, this was a dream come true, recording a music-based film and capturing the performances live, with the production sound being the vocal track instead of a studio recording.\n\nFor the next few months, I ran through different concept setups to figure out exactly how to get the best vocals and tracks possible for the various scenarios we would be shooting in. To ensure that we had the best system in place, we set up a mini-concert during prep to run through and test the concepts. We ultimately landed on having the band perform to playback with just the vocals being recorded live. Jason Ruder, Music Editor, and one of the re-recording mixers, did a quick mix of the prep mini-concert and with Warner Bros., Cooper, and Gaga happy it was clear that we had landed on the right method.\n\nThe movie opens on Jackson Maine’s performance at Stagecoach, the entire scene was filmed live at Stagecoach in only eight minutes. We shot between two concert acts, Jamie Johnson and Willie Nelson. Moving only between their sets meant we got our gear up in the few minutes allotted before Johnson’s set and then filmed and broke down in the minutes before Nelson’s set. One of the biggest concerns to everyone was the potential of music being leaked while filming at these venues with live crowds. To counter that, we came up with an earwig playback setup with no amplification. The performers could hear the music and the singing was recorded live, but the crowd couldn’t hear the vocals or music beyond the first couple of rows. We modified this system for Glastonbury where we had to be super mobile, we were a skeleton crew with only four minutes to set up and shoot. We had the festival’s monitor mixer put the instrument playback into Cooper’s wedge at a low level, and he sang live (unamplified) in front of one hundred thousand festival goers. The crowds were always so fantastic and excited even though they couldn’t hear much of anything. There were some fun headlines at the time about technical difficulties causing the lack of amplification, but it was all a part of the plan to keep the music as secret as possible.\n\nWe shot performances all over, from Coachella and Stagecoach, to Glastonbury, the Shrine Auditorium, the Greek Theatre, the Orpheum, to the Palm Springs Convention Center, a few small nightclubs and a drag bar. We had to be prepared to record absolutely everything live, which meant up to sixty-one tracks of audio at any given time. I used two Midas M32R mixers with a digital stage snake, each mixer has thirty-two inputs and by combining them via Dante into the Sound Devices 970, I could record all the tracks needed. We muted the musicians’ instruments through the amps but still recorded the feeds for post to use. To help capture the atmosphere, we used a DPA 5100 surround sound mic in the crowd and two shotgun mics, one at stage left and right, aimed at the crowd for their reactions. We created a room mapping whenever possible by recording an impulse response for post to be able to take the original studio instrument recordings and balance them to sound like they were recorded live with the vocals in each venue.\n\nFor the track “Always Remember Us This Way” (one of my favorites in this film), Gaga requested a digital grand piano so we could record the piano isolated from the vocals. We were able to take a stereo feed from the digital piano to track her playing, record the vocals cleanly, and keep the music from being heard by the crowd.\n\nFor regular production days, we were a three-man crew: myself, Craig Dollinger (Boom Operator), and Michael Kaleta (Sound Utility.) On music days, we had Nick Baxter on our team as our Pro Tools music editor and monitor mixer Antoine Arvizu. Each music day was like throwing a concert, and the whole team was needed to set up and break down the mics, cabling, stage snakes, in ears and earwigs. These days, we generally had a three-hour pre-call, to set up the wedges, mic all the instruments, and get everything patched to a Midas digital stage box. We used the stage box to split the signal of all the feeds, one to me at the cart and the other to a monitor mixer at the side of stage. The monitor mixer would control the reverb that was heard by Gaga and Cooper via Phonak Earwigs. We would also route the singing and music to the band through earwigs for the concerts with live crowds since the music and singing were not amplified. At the cart, I worked off of the two Midas M32R’s recording to two Sound Devices 970’s. I prepared for sixty-four tracks, our highest track count was sixty-one. We were also joined on set by Jason Ruder, there to observe our recording process during music performances to make handling all the tracks in post as easy as possible.\n\nMusic interacting with the script was so important to Cooper’s vision of the film. He wanted the songs to be a character in the film and each word of the music there to propel and reflect the story. To help achieve this, multiple songs would often be performed in one setup so that he could have the options available when he went into editing. On our end, we built each Pro Tools session to include nearly all of the film’s music so we could easily and quickly switch from song to song at a moment’s notice, with some songs being added the day of shooting.\n\nThere’s always a sense of fun on set when you do a music-driven film. In between setups, the band would jam out and occasionally, Matthew Libatique would grab a camera and start rolling on the action. I was often glued to my seat staring at the monitors as we could just start rolling at any minute. This film is one of the most rewarding I’ve worked on to date, filled with plenty of technical challenges and lots of fun. Bradley Cooper’s directorial leadership always let you know you were an integral part of something special. It was an honor to work with both he and Lady Gaga, two incredibly talented and passionate artists.\n\nMission: Impossible – Fallout marks my second outing in the series and third film with Tom Cruise. I had previously worked on Mission: Impossible – Rogue Nation, but collaborated with a new team for this latest installment. My longstanding colleagues, Steve Finn and Anthony Ortiz, who worked on Mission: Impossible – Rogue Nation, both decided to make the break to mixing. I’m pleased to say both were successful, and hopefully, they learned as much from me as I learned from them in the years we spent together.\n\nby Chris Munro\n\nPrevious experience has taught me to expect the unexpected; after all, anything can happen. So, upon starting pre-production, it came as no surprise when one of my first meetings with Tom Cruise was at the London Heliport in Battersea. It took place as Cruise piloted the helicopter that took us to an airfield close to the studios. He explained there was a plan for a helicopter-chase sequence in the film where he needed to be able to pilot the helicopter with no visible headset or helmet. At this stage there was no script, and for some weeks, we worked with Writer/Director/Producer Christopher McQuarrie, verbally explaining the storyline. The Mission films are all about practical stunts and FX so everything has to work in real-life situations.\n\nFortunately, I have worked on a number of films featuring helicopters and used them as an essential means of reaching challenging locations. The most notable project, Black Hawk Down, garnered me an Academy Award for Best Sound.\n\nI had previously considered using bone conduction technology, most recently on Mission: Impossible – Rogue Nation for the sequence where Tom Cruise is on the outside of a giant Airbus A400M in flight. The technology has been around for years, but I learned that the military had adopted it, which greatly improved the audio quality. The challenge with older technology is that many of the sounds in speech are made in the mouth and not all transmitted through bone conduction. Without these sounds, speech can sometimes be less intelligible. The research process was quite extensive, as not all information was readily accessible. I eventually came upon a company that was developing bone conduction headsets for commercial use, but the caveat was headsets needed to be custom made. Thus, we needed to arrange for an audiologist to take impressions of Cruise’s ears and create concealed bone conduction headsets specifically for him.\n\nPhoto: Chiabella James.\n\nThe next stage was to test if and how they would work. I set up four large powered speakers in a studio office and played back helicopter sounds at a level in which you could not hear someone speak. We then invited Tom Cruise to sit in the room with the earpieces fitted and connected to a walkie-talkie. Incidentally, the earpieces also offered a high degree of hearing protection, which would be important for anyone spending hours in a helicopter without a headset. I went outside the room with another walkie-talkie, and we were able to communicate perfectly. With the first stage complete, I now had to work out how the system would function in a helicopter.\n\nAt this stage, the model of helicopter had not been determined, though we knew that it would be one made by Airbus Industries. Speaking with Airbus engineers, I established that different helicopters may use different avionics systems, and it was not possible to modify or interfere with these in any way given it may affect airworthiness of the aircraft.\n\nAs a result, I called upon long-term collaborator Jim McBride, who has been the technical wizard on many films that I have been involved with in the past such as Black Hawk Down, Gravity, and Captain Phillips. McBride has worked in varying technical capacities on films, as well as in music, and even in a nuclear power station. McBride and I decided we needed to build totally independent self-powered interfaces that were isolated from the helicopter avionics, yet could still use the same PTT (push to talk) button on the helicopter cyclic or control stick.\n\nOnce we had prototype interfaces made, we were ready to start testing with helicopters. With the assistance of aeronautical engineers, we went back to Denham Airfield and installed our equipment in a helicopter. Pleased with the results, we sought approval from Tom Cruise and asked him to give it a try. The first thing Tom did before take-off was to call the control tower for a radio check. ATC reported good quality but were suspicious. The controller remarked he didn’t believe we were in a helicopter because it was too clear, as he couldn’t hear any engine or rotor noise. This result was due to the fact we’d used the bone conduction units fitted in Cruise’s ears with minimal pickup of background noise.\n\nThe next step was sorting how we would connect to a recorder. The limited space within the helicopter prompted us to find something that could be easily hidden when cameras were fitted.\n\nWe originally thought about using a radio link to connect to a hidden multitrack recorder that would also be recording 5.1 FX but wanted to avoid radio transmission within the helicopter if possible. I decided to record to Lectrosonics PDRs which would have timecode sync with cameras and could be easily hidden. We made an output on the helicopter communication interfaces for them to connect to.\n\nEven with production not yet underway, I had put in a substantial contribution to the film. This level of prep was essential for sound efficiency and to ensure all ran smoothly. In some respects, there are similarities to sound design in the theatre, and perhaps a production sound designer would be a more appropriate title considering we no longer mix to a mono or stereo Nagra. The mixing component of our job has become less important; however, our responsibilities have increased proportionately with the advancements in technology.\n\nThe helicopter sequences were not at the start of the schedule so we still had a little time to perfect the systems. Shooting began in Paris in April of 2017 at the Grand Palais, with car and motorbike chases throughout central Paris. I was joined by UK assistants Lloyd Dudley and Jim Hok, as well as Paris-based assistant Gautier Isern, who had recently finished working with Mark Weingarten on Dunkirk.\n\nI needed a small multitrack capability at this stage and experimented with the Zoom F8 and the DPA 5100 surround mic which we could easily hide in the BMW M5 cars. Given we had several cars with different camera rigs, their relative low cost made it possible to hide one in every car. We used radio mics on the actors, primarily so that we could record a rushes track for editorial purposes. This also allowed McQ to monitor performances in a follow vehicle. Supervising Sound Editor James Mather, his dialog editors, along with Re-recording Mixer Mike Prestwood Smith, would later decide which of the mics worked best. We also mounted transmitters on various parts of the car exterior with DPA 4160 mics to get sound FX. We followed the chases in a specially adapted high-speed-chase vehicle with antennae mounted for sound and video and remote camera heads. The van was rigged to carry McQ, DP (who also operated a remote head camera), video assist, another camera remote, and 1st ACs. We would chase the cars or motorbikes whist shooting either from cameras mounted on the action vehicles, tracking vehicles, or very often, an electric bike. We always had a team pre-rigging the next car or bike to be used after a shot was complete. Our team rigged mics on every camera tracking vehicle whether it be the Russian Arm, an electric camera bike or ATV. The rear-mounted mics on the cars and bikes were rigged close to the exhausts, while others were mounted close to the engines. I was particularly looking for FX that sounded real, knowing that sound FX editors could use these later as a base to create a much bigger soundscape. I was not always looking for super-clean FX but usually something raw that sounded more documentary style rather than super-clean FX. That said, I did usually try to record clean interior ambiences in 5.1 with the DPA 5100 surround mic.\n\nThe Grand Palais sequences were set in a big music event with lasers, light shows, and projected graphics that all had to be in sync, so we worked closely with an AV company to provide audio playback linked to a timeline based on timecode from playback to ensure continuity from shot to shot of graphics, sound, and lighting. Several weeks were spent working in Paris shooting a major action sequence alongside the River Seine. The sequence is where Ethan Hunt (Tom Cruise) once again meets Soloman Lane (Sean Harris). Sean had previously established that the voice of his character was quiet yet menacing. It was not easy to record during the mayhem of the wild action sequence, but we managed to capture it while retaining his performance.\n\nThe following location was in Queenstown on New Zealand South Island where we started with sequences set in a medi-camp prior to shooting helicopter sequences. Steve Harris joined our crew here, as he had worked on several films in New Zealand, thus very familiar with the landscape. Many of our locations required travel by helicopter, so I needed an ultra-small shooting rig that could give me the facilities of my normal rig yet fit easily into a helicopter. I built a rig on one of the larger all-terrain Zuca carts fitted out with a Zoom F8, a custom-built aux output box with the ability to send feeds to video and comms, an IFB transmitter, a Lectrosonics VR field with six radio mic receiver channels powered by a lead acid block battery in the base.\n\nOn our first helicopter shooting day, the 1st AD told me with a grin, we would be starting with dialog on the first shot and had dialog to record from Henry Cavill hanging outside a helicopter. Fortunately, I had already taken the precaution of having bone conduction earpieces made for Henry. These were invaluable when connected to a Lectrosonics PDR to record his dialog in flight. I also needed to record sound FX inside the helicopter, and I used the smallest multitrack with timecode that I could find, the Zoom F8. We fixed a DPA 5100 5.1 microphone to the interior roof and hid the F8 under a seat. This was done so if it were caught in camera, the 5100 could look as if it were part of the helicopter. We were able to start/stop and adjust levels on the F8 without needing to access it by using the F8 controller app on an iPhone. Similar to the Paris chase sequences, I wanted the FX to sound real and not like enhanced library recordings. Thus, a little wind on Henry’s mic often added to the reality. Additionally, because the bone conduction units were largely unaffected by background noise, the 5.1 recordings could be useful even if only certain elements of the six tracks would be used.\n\nTom Cruise always piloted his own helicopter with cameras mounted on it. Director Christopher McQuarrie would fly in another and film from at least one other camera helicopter. Because the helicopters would take off and shoot for at least thirty minutes at a time, Editor Eddie Hamilton asked if I could record McQ’s helicopter comms in addition to what I was recording of Tom and Henry for the takes. This would allow him to align with exactly what McQ was intending for each shot. I used another PDR for this because of the timecode sync capability and small size. At the end of the day, we became data wranglers downloading the various SD cards and making sound reports.\n\nAfter New Zealand, we returned to the UK to shoot in studio and various London locations, eventually joined by Hosea Ntaborwa. I had mentored Hosea when he was at the National Film and TV School on behalf of BAFTA and Warner Bros. creative talent. This was one of his first jobs after graduating and since then, has become part of my regular team working with us on The Voyage of Dr. Dolittle and Spider-Man: Far from Home. We had some rather challenging locations in London and it was whilst shooting on one of these, Cruise injured his ankle.\n\nThe company went on hiatus for a few weeks, which gave me an opportunity to prepare for the HALO jump sequence. That is High Altitude Low Opening and involved Tom Cruise jumping from an aircraft at an altitude high enough to require oxygen. We had a huge wind tunnel built on the backlot in the studio so that the skydive team could plan and practice manoeuvres while\n\nexperimenting with camera angles. The wind tunnel was also useful for my first asssistant Lloyd Dudley and I to develop the system for recording Cruise during the jump. We were able to set up communications between the skydive team, McQ, camera operator, and ground safety.\n\nThe wind tunnel was incredibly noisy so we were appreciative of the fact that the bone conducting earpieces offered a high degree of hearing protection. The skydive team were amazed that we managed to achieve audible communication in the wind tunnel, which made it much easier to plan shots and make adjustments. The team warned that skydivers have been trying for years to get good recordings during the dives but that they were always battling against wind noise and that it would be impossible to record. I reminded them that this was Mission: Impossible!\n\nWhen we began shooting again, many of the locations were very inhospitable to sound, but nothing I had not come across before. The team also continued on studio sets at WB studios at Leavesden.\n\nDuring this time, I continued to prepare for the HALO jump sequence, which was to be shot in Abu Dhabi. We did a number of tests with Cruise and the skydive team jumping from a Cesna Caravan at various UK sites. What we couldn’t test was what would happen at the highest altitudes when oxygen was required, and the jump was from a giant C17 aircraft. I was concerned with safety and that the equipment we were using in both Tom and the skydiver’s helmets was intrinsically safe. The dive helmets contained lighting which could potentially ignite the oxygen, so we arranged for tests to be done in an RAF lab with all of the equipment used for the HALO jump. We also had dialog to record inside the C17 as the jump progressed from a dialog scene directly into the jump. We shot some exteriors of the C17 and interiors on the ground at RAF Brize Norton near Oxford in the UK. This at least gave us a chance to consider what we would be up against.\n\nEventually, the time came to travel to Abu Dhabi. My crew there was Lloyd Dudley and Hosea Ntaborwa. Lloyd concentrated mainly on looking after Tom Cruise’s bone conduction headsets and fitting Lectrosonics PDR recorders to actors. Hosea was in charge of comms for the skydive team and recording 5.1 FX in the aircraft. I was particularly interested in sounds of breathing and how that can add tension. Once we played some of these sounds for Tom, he immediately wanted it to be a major part of the soundtrack during the jump sequence. I was not looking for pristine recordings that sounded like they were shot in a studio, instead, I was interested in the raw sounds of the helmet mics and bone conducting units which could give a more realistic documentary-type sound. I was not opposed to some wind noise and realised this could add to the reality. Having original sound adds to the feeling of reality even the audience are only subconsciously aware of it. Post production was greatly contracted due to the hiatus in shooting. Supervising Sound Editor James Mather had time restraints, thus appreciated all of the raw sound FX we could give him as elements to enable his team to create the final audio to be mixed by Mike Prestwood Smith. Both have been collaborators of mine on several previous films.\n\nIn conclusion, this film involved huge leaps of faith from all parties involved. I greatly appreciate the support from Tom Cruise, Chris McQuarrie, the production team, and my team throughout. My previous experiences, intuition, along with an incredibly well-respected new team proved immensely valuable. Much of the sound was unmonitored being recorded on PDR and hidden recorders. Chris McQuarrie and the producers trusted us to deliver on Mission: Impossible – Fallout using never-before-used technology. Most importantly, Tom Cruise trusted we would develop technology that would allow him to perform stunts efficiently, safely, and wherever possible, avoid ADR in order to enhance the reality.\n\nHALO COMMS\n\nThe original intention was to use a radio mic TX on TC with a recording facility and have each of the additional divers and camera operator have a receiver connected to helmet-fixed earpieces. When we were safety testing, one of the requests was that we did not transmit anything inside the aircraft but that it was OK to transmit as soon as the divers had exited. I argued that radio mics would be fairly low power and on legal frequencies but then realised that it may have been a mistake to try to achieve recordings and comms with the same device. Additionally, we needed ground contact when the divers reached a lower altitude so that they could be given any safety information about wind or any other issues and radio mic TX may not have enough range.\n\nI decided to use Motorola walkie-talkies for comms mainly because they were reliable and we were familiar with them. We used finger-operated PTT (push to talk) with custom-made interfaces to connect to the Motorolas. The PTT was run inside the sleeve of each diver and operated with a finger and thumb.\n\nFor TC (shown as EH on diagram), we used a bone conduction headset in each ear. One ear was talk/listen connected to the Motorola via the PTT and the other ear was connected directly to a Lectrosonics PDR. Another PDR connected to a da-capo mic (Que audio in US?) mounted in the helmet. I chose the da-capo mic mainly because I just happened to have some and also because these are what I had successfully used in helmets on Gravity. I had to send mics for safety testing to be intrinsically safe when used in the helmet, which also had oxygen being pumped in. I immediately thought that the da-capo mics may be well sealed as they are waterproof. It was not a particularly scientific decision.\n\nTom Cruise’s Bone Conductive Earpieces & Microphone\n\nI had custom-moulded bone conduction units in each of TC’s ears. It was necessary to have one in each ear to give hearing protection but also allows use of the aircraft PTT as one ear is for talking and one for listening. There is a switch on the interface that allows for either, and this was connected via the pilot’s headset connectors in the helicopter on a US NATO plug or on some aircraft a Lemo connector. Trim pots on the interface require a miniature screwdriver to adjust talk and listen levels. It is powered by an internal battery and has transformer isolation on the connection to the helicopter to ensure isolation from the helicopter avionics.\n\nThe PDR records directly from the “talk” ear for a clean track of TC and is therefore pre PTT. That is to say that even if TC is not transmitting through the aircraft radio, his voice is still recorded.\n\nAn output from the co-pilot comms socket goes to track eight of a Zoom F8 which is primarily to record 5.1 ambience within the helicopter. Track eight records all comms: the director, any communication with other aircraft, ATC, and so on.\n\nThe DPA 5100 was fixed to the inside roof of the helicopter cabin toward the rear. If ever it were inadvertently caught on camera, it looks like part of the structure. The recorder was hidden and operated by the F8 control app on an iPhone.\n\nThe PDR was stopped and started using dweedle tones within the Letrosonics PDR remote app, also from an iPhone.\n\nOverview\n\nThe past ninety years of sound recording for motion picture production has seen a steady evolution in regards to the technologies used both on set and in studios for post production. Formats used for recording sound have changed markedly over the years (with the major transitions being the move away from optical soundtracks to analog magnetic recording, and from analog magnetic to digital magnetic formats, and finally, to file-based recording). Along with these changes, there has been a steady progression in the mixing equipment used both on set for production sound, as well as re-recording. Beginning with fairly crude two input mixers in the late 1920’s, up to current digital consoles boasting ninety-six or more inputs, mixing consoles have seen vast changes in both their capabilities and technology used within. In the article, we will take a look at the evolution of mixing equipment, and how it has impacted recording styles.\n\nIn The Beginning\n\nIf you were a production mixer in the early 1930’s, you didn’t have a lot of choices when it came to sound mixing equipment. For starters, there were only two manufacturers, Western Electric and RCA. Studios did not own the equipment. Instead, it was leased from the manufacturers, and the studio paid a licensing fee for the use of the equipment (readily evidenced by the inclusion of either “Western Electric Sound Recording” or “Recorded by RCA Photophone Sound System” in the end credits). Both the equipment, as well as the related operating manuals, were tightly controlled by the manufacturers. For example, Western Electric manuals had serial numbers assigned to them, corresponding to the equipment on lease by the studio. These were large multi-volume manuals, consisting of hundreds of pages of detailed operating instructions, schematics, and related drawings. If you didn’t work at a major studio, there is no way you would even be able to obtain the manuals (much less comprehend their contents).\n\nEarly on, both Western Electric and RCA established operations that were specifically dedicated to sound recording for film, with sales and support operations located in Hollywood and New York. Manufacturing for RCA was done at its Camden, NJ, facilities. Western Electric opted to do its initial manufacturing at both the huge Hawthorne Works facility on the south side of Chicago, as well as its Kearny, New Jersey, plant. These facilities employed thousands of people already engaged in the manufacturing of telephone and early sound reinforcement equipment, as well as related manufacturing of vacuum tubes and other components used in sound equipment.\n\nThe engineering design for early sound equipment was done by engineers who came out of sound reinforcement and telephony design and manufacturing, as these areas of endeavor already had shared technologies related to speech transmission equipment. (Optical sound recording was still in its infancy at this stage though, and required a completely different set of engineering skills.)\n\nWith the rapid adoption of sound by the major studios beginning in April 1928 (post The Jazz Singer), there was no time for manufacturers to develop equipment from the ground up. If they were to establish and maintain a foothold in the motion picture business, they had to move as quickly as possible. Due to the high cost and complexities of manufacturing, engineers were encouraged by management to adapt existing design approaches used for broadcast and speech reinforcement equipment, as well as disc recording, to the needs of the motion picture business. As such, it was not unusual to see mixing and amplifier equipment designed for broadcast and speech reinforcement show up in a modified form for film recording.\n\nExamples of these shared technologies are evident in nearly all of the equipment manufactured by both RCA and Western Electric (operating under their Electrical Research Products division). While equipment such as optical recorders and related technology had to be designed from the ground up, when it came to amplifiers, mixers’ microphones and speakers, manufacturers opted to adapt what they could from their current product lineup to the needs of the motion picture sound field. This is particularly evident in the equipment manufactured by RCA, which had shared manufacturing facilities for sound mixing equipment, microphones, loudspeakers and related technology used in the broadcast and sound reinforcement fields.\n\nIt was not unusual to see equipment originally designed for broadcast (and later, music recording) show up in the catalogs of equipment for film sound recording all the way through the early 1970’s.\n\nDesign Approaches\n\nWhile the amplifier technology used in early sound mixing equipment varied somewhat between manufacturers, much of the overall operational design philosophy for film sound mixers remained the same all the way up through the mid to late 1950’s, when the stranglehold that RCA and Western Electric had on motion picture business began to be eaten away by the development of magnetic recording. (The sole standout being Fox, who had developed its own Fox-Movietone system.) New magnetic technologies (first developed by AEG in Germany in 1935), began gaining a foothold after the end of WWII, and players such as Ampex, Nagra, Magnecord, Magnasync, Magna-Tech, Fairchild, Stancil-Hoffman, Rangertone, Bach-Auricon, and others began to enter the field. Unlike RCA and Western Electric, these manufacturers were willing to sell their equipment outright to studios, and didn’t demand the licensing fees that were associated with the leasing arrangements of RCA and Western Electric.\n\nDespite these advances, RCA and Western Electric were still the major suppliers for most film sound recording equipment for major studios well into the mid-sixties and early seventies, with upgraded versions of their optical recorders (which had been developed at significant cost) in the late 1940’s still being used to strike optical soundtracks for release prints. Both RCA and Western Electric developed “modification kits” for their existing dubbers and recorders, whereby mag heads and the associated electronics were added to film transports, thereby alleviating the cost of a wholesale replacement of all the film transport equipment. Much of this equipment remained in use at many facilities up until the 1970’s, when studios began taking advantage of high-speed dubbers with reverse capabilities.\n\nThe 1940’s\n\nAfter the initial rush to marry sound to motion pictures, the 1940’s saw a steady series of improvements in film sound recording, mostly related to optical sound recording systems and solving problems related to synchronous filming on set, such as camera noise, arc light noise, poor acoustics, and related issues. In 1935, Siemens in Germany had developed a directional microphone which provided a solution to sounds coming from off set.\n\nDisney also released the movie Fantasia, a groundbreaking achievement that featured the first commercial use of multi-channel surround sound in the theater. Using eight(!) channels of interlocked optical sound recorders for the recording of the music score and numerous equipment designs churned out by engineers at RCA, it can safely be said that the “Fantasound” system represented the most significant advance in film sound during the 1940’s. However, except for the development of the three-channel (L/C/R) panpot, the basic technology utilized for the mixing consoles remained mostly unchanged.\n\nLikewise, functionality of standard mixing equipment for production sound saw few advances, except for much-needed improvements to amplifier technology (primarily in relation to problems due to microphonics in early tube designs). Re-recording consoles, however, began to see some changes, mostly in regards to equalization. Some studios began increasing the count of dubbers as well, which required an increase in the number of inputs required. For the most part, though, the basic operations of film sound recording and re-recording remained as they were in the previous decade.\n\nThe 1950’s\n\nWhile manufacturers such as RCA and Western Electric attempted to extend the useful life of the optical sound equipment that they had sunk a significant amount of development money into, by the late 1950’s, the technology for production sound recording had already begun making the transition to ¼” tape with the introduction of the Nagra III recorder. Though other manufacturers such as Ampex, Rangertone, Scully, RCA, and Fairchild had also adapted some of their ¼” magnetic recorders for sync capability, all of these machines were essentially studio recorders that simply had sync heads fitted to them. While the introduction of magnetic recording significantly improved the quality of sound recording, it would remain for Stefan Kudelski to introduce the first truly lightweight battery-operated recorder capable of high-quality recording, based on the recorders that he originally designed for broadcast field recording. This was a complete game-changer, and eliminated the need for a sprocketed film recorder or studio tape machine to be located off-set somewhere (frequently in a truck), with the attendant need for and AC power or bulky battery boxes and inverters.\n\nLater, Uher and Stellevox would also introduce similar battery- operated ¼” recorders that could also record sync sound. Up until this point, standard production mixing equipment had changed little in terms of design philosophies from the equipment initially developed in the early 1930’s (with the exception being some of the mixing equipment developed for early stereo recording during the early 1950’s for movies such as The Robe). Despite the development of the Germanium transistor by Bell Laboratories in 1951, most (if not all) film sound recording equipment of the 1950’s was still of vacuum tube design. Not only did this equipment require a significant source of power for operation, they were, by nature, heavy and bulky as a result of the power transformers and audio transformers that were a standard feature of all vacuum tube audio designs. In addition, they produced a lot of heat!\n\nMost “portable” mixers of the 1950’s were still based largely on broadcast consoles manufactured by RCA, Western Electric (ERPI), Altec, and Collins. Again, all of vacuum tube design. The first commercial solid-state recording console wouldn’t come around until 1964, designed by Rupert Neve. A replacement for the venerated Altec 1567A tube mixer didn’t appear until the introduction of the Altec 1592A in the 1970’s.\n\nA common trait amongst all of these designs was that nearly all of them were four input mixers. The only EQ provided was a switchable LF rolloff or high-pass filter. There were typically, no pads or first stage mic preamp gain trim controls. The mic preamps typically had a significant amount of gain, required to compensate for the low output of most of the ribbon and dynamic mics utilized in film production at the time (while condenser mics existed, they also tended to have relatively low output as well).\n\nAll had rotary faders, usually made by Daven. And except for the three-channel mixers expressly designed for stereo recording in the 1950’s, all had a single mono output.\n\nRe-recording consoles were of course much larger, with more facilities for equalization and other signal processing, but even these consoles seldom had more than eight to twelve inputs per section.\n\nThe 1960’s\n\nWhile the 1950’s saw some significant advances in the technology of sound recording and reproduction, with the exception of the introduction of stereo sound (which was typically for CinemaScope roadshow releases), there had not been any really significant advances in recording methods since the transition from optical to magnetic recording. Power amplifiers and speaker systems had somewhat improved, boosting the performance of cinema reproduction. However, most mixing consoles relied on circuit topologies that were based on equipment from the 1940’s and 1950’s, with some minor improvements in dynamic range and signal-to-noise ratio.\n\nIt was during this period that technologies developed for music and broadcast began to seep into the area of film sound, and the approach to console designs began to change. The most notable shift was a move from the tube-based designs of the 1950’s to solid-state electronics, which significantly reduced the size and weight of portable consoles, and also for the first time, allowed for a design approach that could use batteries as a direct source of power, without the need for inverters or generators required to power traditional vacuum tube designs. This opened up a significant range of possibilities that had not existed before.\n\nWith the introduction of solid-state condenser mics, designers began to incorporate microphone powering as part their overall design approach to production mixers, which eliminated the need for cumbersome outboard power supplies.\n\nSome mixers also began to include mic preamp gain trims as part the overall design approach (also borrowed from music consoles of the era), which made it easier to optimize the gain and fader settings for a particular microphone, and the dynamics of a scene.\n\nThe 1960’s would also see the introduction of straight-line faders (largely attributed to music engineer Tom Dowd during his stint at Atlantic Records in New York). In the film world, straight-line faders showed up first in re-recording consoles, which could occupy a larger “footprint.” However, they were slow to be adopted for production recording equipment. This was due in part to some resistance on the part of sound mixers who had grown up on rotary faders (with some good-sized bakelite knobs on them!), but also due to the fact that early wire-wound straight-line faders (such as Altec and Langevin) fared rather poorly in harsh conditions, requiring frequent cleaning.\n\nStill, even by the end of the 1960’s, not much had changed in terms of the overall approach to production recording. Four input mixers were still the standard in most production sound setups, with little or no equalization. But the landscape was beginning to shift. While RCA and Westrex were still around, they had lost their dominance in the production world of film (although RCA still had a thriving business in theater sound service arena).\n\nThings were about to change however.\n\nPart 2 will continue in the next edition.\n\n–Scott D. Smith CAS\n\nby James Delhauer\n\nWhen looking at the history of the technology that defines our industry, the acceleration of progress that has occurred in the recent past is truly staggering. In 1953, the National Television System Committee (NTSC) introduced the color television broadcasting format, which is colloquially known today as NTSC Standard Definition. Though minor variations on the format were introduced over time, it remained mostly unchanged until the first high-definition television standards were officially adopted in the United States more than forty years later in 1994. But just twenty years after that, in 2014, that resolution was made obsolete when the first digital streaming services began to widely distribute content in a 4K Ultra-High Definition (UHD) television resolution. Though 4K UHD is the current highest standard of content distribution, current speculation suggests that mainstream adoption of even larger 8K displays will begin in the United States in 2023 and that distribution platforms will start officially supporting it shortly thereafter. If accurate, this would mean that the amount of time between home media standards has halved between each leap forward. And in the not-so-distant future, that could be a very real challenge.\n\nA digital image is made up of what we call pixels—tiny dots that come together in rows and columns to make up a single image. What we call resolution is a measurement of the number of pixels in a given image. NTSC Standard Definition format images are made up of six hundred and forty vertical lines and four hundred and eighty horizontal lines, commonly represented as 640×480. Though a variety of different high-definition formats do exist, the one referred to as True HD increased those figures to one thousand, nine hundred and twenty vertical lines by one thousand and eighty horizontal lines. This is referred to as 1920×1080 or simply 1080 for short. This increase in pixels results in substantially more dots being used to make up the same picture, allowing for more detail, precision, and color shading when replicating what a camera’s sensor captures, making for a more nuanced product.\n\nBut while we all love to be dazzled by the absolute clarity, color, and sharpness that high-resolution imagery can offer, there are very real logistical quandaries that filmmakers need to consider. For Local 695 data engineers in particular, whose responsibilities can include media playback, on-set chroma keying, off-camera recording, copying files from camera media to external storage devices, backup and redundancy creation, transcoding, and syncing, digital real estate is a growing concern. At four times the number of pixels as Full High Definition, 4K UHD means four times the amount of data is captured over the same amount of time. The impending move to 8K will multiply this amount by another factor of four, as 8K is double the number of both horizontal and vertical lines of 4K and not just double the number of pixels. In practical terms, productions will need to spend sixteen times as much money on media.\n\nBut drive space isn’t the only issue. Just because our data quantities are increasing does not mean that the films and series we make can accommodate turnaround times that are four and sixteen times longer. Therefore, we need faster drives and more powerful computers.\n\nIn simplest terms, a hard drive’s read and write speeds determine how quickly it can access the data stored within and add new data onto itself. For data engineers, this is of critical importance when transferring media from one location, such as a camera card, to another, such as a production shuttle. A standard spinning disk hard drive’s speed is determined based on how fast the disk inside of it spins. Typical work drives spin approximately seventy-two hundred times per minute when brand new. In theoretical terms, these drives are capable of transferring between eighty and one hundred and sixty megabytes of data per second. Unfortunately, even at this speed, these drives are not always suited for high-definition work—let alone the more intensive labors of 4K and beyond. A convenient way to get around the problem is through a process known as Redundant Array of Independent Disks, or RAID. Though RAIDing can be done in a variety of ways, the basic concept is that multiple drives are used to accomplish a single task. By using two hard drives (or more) instead of one, the task being performed can use the performance speed of each drive simultaneously. In live broadcast environments, it is common to use RAID configurations that are making use of anywhere from four to sixteen hard drives at once in order to ingest multiple cameras’ worth of media simultaneously.\n\nHowever, while those speeds are impressive and more up to the challenge of high-resolution production, they do come with serious drawbacks. Every hard drive introduced into the configuration represents a potential point of failure in the RAID array. In a simple RAID configuration, the loss of a single hard drive could mean the loss of all footage contained within the array. More complex configurations take this into account and create redundancies but these require additional hard drives, returning us to the issue of real estate. Newer solid state hard drives—media devices that have no moving parts and therefore, don’t rely on disk speed—may represent a possible solution to the RAID issue in time. Though they are currently significantly more expensive (a single terabyte 7200rpm hard disk drive can be bought for as little as $44, whereas a solid state drive of the same size and brand will run you $230), they are significantly faster at performing the same tasks. This theoretically means that fewer drives could go into a single RAID configuration, reducing the points of failure in the array. Moreover, with no mechanical parts to jam or degrade over time, they may be less prone to failure in the first place. Unfortunately, we will need to wait for production costs to bring retail prices on solid state media down before this becomes a viable alternative.\n\nThis is all assuming that a hard drive is not limited in any way by its connection speed to the computer with which it is communicating. The physical port that a hard drive uses to interface with a computer may have a speed limitation completely unrelated to the drive’s. The most common type of port, USB 3.0, has a theoretical limit of five gigabits per second, with one gigabit equaling approximately one-eighth of the more commonly measured gigabyte. A single spinning disk drive does not read or write faster than that and so there is no problem.\n\nHowever, an array of drives working together can easily exceed that limit, at which point, data is going through a choke point when passing through the wire connecting a computer to a hard drive. At the time of writing this article, the fastest connection port on the market is tied between Thunderbolt 3 and 40g Ethernet. These two port types both have theoretical maximums of 40gbps, though neither has seen widespread adoption within the industry at this time.\n\nAll of that being said, engineers don’t just need to be able to move data around faster if we are to keep up with the demands of higher resolution. It is of equal importance that we process it more quickly too. Since working with ultra-high definition and larger formats requires a prohibitive amount of computer processing power, our editor friends in Local 700 rarely do it. Instead, they make use of a process known as “Offline to Online Editing,” where they use lower resolution proxy file copies of the camera media when assembling their projects and then swap out those proxies for the original high-resolution camera media when preparing to color grade and deliver. Where do those proxy files come from?\n\nUs.\n\nLocal 695 data engineers can be tasked with creating these proxies on set, which necessitates being able to work with the raw, high-quality footage captured by the camera. This means more powerful and efficient computers are becoming necessary. There are several factors that determine how powerful a computer is for our purposes. Processor speed, memory, graphics memory, hard drive speed, and connection speed all need to be taken into account. For these reasons, and a few others, the majority of the industry has become reliant on Apple computers.\n\nUnfortunately, Apple’s professional line of computers tend to stagnate for long periods of time. The Mac Pro, Apple’s line of professional grade editing machines, has remained unchanged since 2013. The company has announced a replacement, tentatively to be released in 2019 and a stopgap solution was introduced in 2017 when the company unveiled the iMac Pro but these computers are not cheap. An introductory iMac Pro costs $4999 while a fully upgraded machine can be bought for as much as $13,200. And this assumes the use of only one machine at a time.\n\nWhile the world of major motion pictures has largely embraced the move from lower viewing resolutions to higher ones, with digital cinema cameras recording in 6K and 8K resolutions already, television has yet to catch up with the current Ultra-High Definition standards. In the United States, many series still record in high definition. It’s understandable when the sheer volume of footage is taken into consideration. While feature films record enough footage to assemble a presentation lasting between ninety minutes and three hours, television series spanning multiple seasons can last hundreds of episodes. The need to process and preserve all of that footage requires a staggering amount of resources. Doing all of that in 4K or 8K makes it an even more monumental challenge. But as the cost of 4K televisions, computer monitors, and even cellphone screens continue to plummet and as 8K displays are introduced into the market, our audience is going to demand that this challenge be met.\n\nThese concerns are not new. The jump from standard definition to high definition presented the same obstacles in the late 1990s. The new factor at play here is time. While we, as an industry, could no doubt rise to the growing needs of 4K production with time, the advent of the 8K world is already within eyesight. How far behind that is the realm of 16K? Another ten years? Or maybe just another five? It will be interesting to see at what point the innovation of one avenue of technology collides with the reality of another.\n\nMy Life as a Commercial Sound Mixer\n\nby Crew ChamberlainI\n\nMy name is Crew Chamberlain, yes, that is my real name, ever since the spring of 1952 when I was born in Fullerton, Calif., at the edge of what we now call the “Thirty Mile Zone,” a child of the baby boom, as well as a fourth-generation Californian, from a large family that settled in and around Orange County. Many of them worked the “Oil Patch” for the Standard Oil Company, others had their own businesses, but not one of them worked in Hollywood or even knew someone who did. Media back then was a black-and-white television (five stations), radio (lots of stations), and a Saturday-afternoon trip to the Fox Fullerton to see the latest double feature for thirty-five cents.\n\nAs much as I loved the films and TV that I grew up with, it never occurred to me that there was a system of people, places, and companies that made the content I consumed. We just loved the grand and silly movies like Journey to the Center of the Earth, The Nutty Professor, The Great Escape, or TV shows like The Twilight Zone and The Man From U.N.C.L.E.. I was totally oblivious to the process of film production. That started changing in high school where I was a proud C student. I only enjoyed having fun, playing drums in my garage band, body surfing, drawing, or painting.\n\nIn my sophomore year at Sunny Hills High School, an art teacher gave our class a unique assignment. She had us paint patterns on exposed 16mm film (a clear strip) which we then projected in class and played songs to. We all liked this so much we asked to do it again and again. She next suggested if we could get access to an 8mm camera, we could try and make a short film. We did and it was terrible. Not a Spielberg in the bunch, but we had a blast doing it. That little spark started a slow smoldering interest of film within me. With parental pressure to go to college, the Vietnam War spinning out of control, and a very motivational mandatory draft, I discovered that there was a thing called “Film School.” Not the ubiquitous film schools of today, at the time there were only three: NYU, UCLA, and USC. It took me a year at ASU to get my grades up, but with some hustle and a lot of luck, I got into the USC School of Cinema in 1971. I had found my calling.\n\nThe USC Film School was a nerdy endeavor in the eyes of the student body back then, until the release of American Graffiti by fellow Cinema School alum George Lucas during my second year. All of a sudden, everyone knew and thought highly of our little dilapidated barn and stable complex on the corner of campus. What a deep dive I took. We lived, breathed, and endlessly discussed cinema. I watched at least two films a day from every genre and era. A very talented group of professors like Mel Sloan, Ken Miura, and Drew Casper engaged and enlightened us. We had to make many films, write a screenplay, and learn the nuts and bolts of the gear. Film cameras! Sound recording on Nagras with microphones! History & criticism!\n\nEditing … sweet editing. A filmmaker’s last chance to make a story into something. That hands-on creativity is what I loved the most. That was going to be my choice of career path after college. I would become an editor and of course, some day a director….\n\nAfter USC, I landed a job at Wexler Films (not related to Haskell or Jeff) that made medical and health-related shorts for high school education departments. I was an ‘assistant editor,’ but really a PA. On shoot days, I did everything needed, including the sound recording for interviews on a Nagra 4L with a 415 or Sony ECM 50 lavs. I loved production shoot days and these interactions lead to other sound gigs. Soon I was sound recording on docs, interviews, and low-budget films. I also started booming for other mixers which seemed very natural to me as I had the coordination and enough sound knowledge to get it going. Looking back, the progression was a steady one. I went from working on drive-in-level Corman films, to a John Cassavetes movie (Opening Night), to Albert Brooks’ first film (Real Life).\n\nFor the first thirteen years of my career, I pursued films above all else and landed some good ones. I was fortunate to get better and better movies and experience with some of the best mixers of the day, Jeff Wexler, Jim Webb, Art Rochester, and Keith Wester. Between all the films that I did, I boomed for many good commercial mixers, but mostly Roger Daniel. As much as I appreciated the work, I wasn’t a fan of commercial shoots. No one whoever went to film school wanted to do commercials.\n\nFilms were the goal and I did about as well as possible thanks to Jeff Wexler and Don Coufal mentoring me. Thankfully, Roger continued to hire me between films and showed me many of the valuable aspects to commercials that I had overlooked. Like the time/money ratio.\n\nIt all came into focus for me in 1986. I was thirty-five years old with a wife and two kids under five years old that I loved and I wasn’t able to be a full-time father and husband for. That year, I was on location for nine of the prior twelve months. I took a good long look around, I felt good about where I’d been and all I had learned and decided that a change was needed and my path forward was going to be in the commercial realm. At least until the kids grew up, I consoled myself.\n\nCommercials. Yep, commercials. Still hard to believe. When I got in the IATSE in 1977, film was the undisputed king of the crop. Television was a different beast than today, closer to the old factory system of the studios that were cranking out a steady stream of cop shows or three-camera sitcoms, and commercials were, well, commercials. The stigma was understandable as commercials were formulaic and square at best. That all started changing in the mid-’80s as a new sensibility and atheistic took hold. Directors like Joe Pytka, Ridley and Tony Scott, Rick Levine, Bob Giraldi, Adrian Lyne and others brought a cinematic look and approach to the process. An entertaining thirty- or sixty-second movie/story was the new direction. These directors all did commercials, as well as the emerging music videos and feature films. For them, work was work. Another day to practice and perfect their craft was the sensibility. Money was a motivator too. The commercial stigma started to evaporate a bit.\n\nFor a Sound Man (that’s what we were called and mostly were back then), the commercial work left the two walled sets on quiet stages and more often ventured out to practical locations where the challenges were more difficult, exactly like film and TV. The work was still ninety percent one boom and a mono Nagra and luckily one camera, so our success rate was high. Roger and I did so well that I was working more days a year than I had prior, but the big difference being I was home every night with my family. I was happy and seldom looked back. On commercial shoots back then, many a day was short for the Sound Department. We’d have a later call, record our part, and go home early. A lot of six- to eight-hour days in the eighties and early nineties. The glory days as it were.\n\nRoger had a very loyal clientele of the top players in the commercial world, the LA, NYC, and London production companies and directors called him first. I nic"
    }
}
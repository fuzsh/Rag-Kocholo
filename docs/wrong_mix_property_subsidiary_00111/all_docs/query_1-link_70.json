{
    "id": "wrong_mix_property_subsidiary_00111_1",
    "rank": 70,
    "data": {
        "url": "https://www.forbes.com/sites/forbestechcouncil/2019/10/25/big-data-testing-for-devops-and-agile-are-your-installations-keeping-up/",
        "read_more_link": "",
        "language": "en",
        "title": "Big Data Testing For DevOps And Agile: Are Your Installations Keeping Up?",
        "top_image": "https://imageio.forbes.com/blogs-images/forbestechcouncil/files/2019/10/a-23-3.jpg?format=jpg&height=900&width=1600&fit=bounds",
        "meta_img": "https://imageio.forbes.com/blogs-images/forbestechcouncil/files/2019/10/a-23-3.jpg?format=jpg&height=900&width=1600&fit=bounds",
        "images": [
            "https://blogs-images.forbes.com/assets/images/avatars/blog-3949_400_6d18078e84d93600baaf18165a1394f8.jpg",
            "https://blogs-images.forbes.com/forbestechcouncil/files/2017/11/ruslandesyatnikov_avatar_1511546115.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Ruslan Desyatnikov"
        ],
        "publish_date": "2019-10-25T00:00:00",
        "summary": "",
        "meta_description": "How should DevOps and Agile teams reduce lengthy setup times that defer production?",
        "meta_lang": "en",
        "meta_favicon": "https://i.forbesimg.com/48X48-F.png",
        "meta_site_name": "Forbes",
        "canonical_link": "https://www.forbes.com/sites/forbestechcouncil/2019/10/25/big-data-testing-for-devops-and-agile-are-your-installations-keeping-up/",
        "text": "It’s a generally accepted maxim that the business community’s fascination with big data, which started in the mid-2000s, ran out of steam about five years ago.\n\nBut that’s only partly true.\n\nWhile the talking points have shifted to DevOps, Agile, AI and machine learning, big data never left the scene. On the contrary: It has become so widespread that no one feels the need to bring it up anymore. Today, it’s just data.\n\nTo give some context, big data and business analytics solutions are expected to reach $274.3 billion worldwide by 2022, from an estimated $189.1 billion in 2019, according to International Data Corporation.\n\nIt’s no different in the domain of software testing. Big data testing plays a massive role in helping companies identify defects in real time, reduce storage costs, make smarter decisions, manage client expectations and implement new strategies. And that’s just in the short term. Big data testing can lead to greater efficiencies and increased revenue in the long run because large volumes of data can reveal patterns and trends that would be unobtainable otherwise.\n\nI must also add that big data testing is, nevertheless, in an exploratory phase. As a fairly recent trend, QA teams around the world are still probing for best practices. Testers must consider all the indexes and abstractions in order to reveal patterns out of low-density, crude data. The sheer scalability of big data means that the application has to be continuously verified across its operative capacity — without processing the full load that comprises the enormously high data quantities.\n\nAll this is new territory for many testers. But with the snowballing effect of the number of big data applications, the demand for these skills is accelerating in parallel.\n\nBig Data Testing Constraints In DevOps And Agile Environments\n\nThe perceived drawback of big data testing is that testers end up spending the bulk of their time validating data instead of actually testing the system. Managers see it as an antithesis in today’s climate of booming Agile frameworks and DevOps practices that enable fast change and fast release.\n\nSimply put, testers are faced with so much data that traditional databases such as Oracle and PostgreSQL can’t handle the volume. Big data comes in a plethora of formats, comprising unstructured data and a huge range of different data (video, audio, images, etc.) which is challenging to store in the row and column setup we’re all familiar with. Plus, the velocity of large data volumes makes storing and retrieving data impossible with conventional software techniques. Just think that around 6,000 tweets are shared every second, on average, according to Internet Live Stats.\n\nAdding on, virtual machines required for virtualization testing can be slow and unresponsive and can cause bottlenecks when running big data testing in real time. When it comes to huge datasets, testers always need to verify more data at a quicker rate. Also, they must be able to test across different platforms, which can hinder progress because each platform is made up of its own unique properties.\n\nEven that’s not all. Automated tools aren’t capable of solving problems that randomly pop up during testing; this means that Agile and DevOps teams will need the expertise of someone with strong technical proficiency to keep testing running smoothly.\n\nLastly, with big data testing, testers don’t have the option of implementing a sampling strategy in order to manually test what they consider statistically justified cases. They also can’t perform exhaustive verification and test all possible data combinations present at the beginning of testing.\n\nTips And Tricks\n\nHow should DevOps and Agile teams reduce lengthy setup times that defer production? If you’re a manager, you can:\n\nInvest in training. Regular testing tools require a rudimentary working knowledge, while big data testing requires a specialized set of skills.\n\nOptimize your data life cycle management. Design testing installations so that data is restored at a continuous rate. Importing new data and cataloging old data should run without interruption, minus the requirement to create individual folders for every iteration.\n\nLocate defects, and disable blockers. This is where performance testing comes in. It quickly processes huge data volumes and estimates the speed, scalability and stability of structured and unstructured data.\n\nBroaden your toolkit. Big data testing doesn’t rely on any particular testing tool; the gamut is wide and keeps expanding.\n\nGo big or go home. Build a testing tool that lets you populate your installation with large amounts of data — any quick import or replication process you’re already using will do in this case.\n\nAmplify your research and development initiatives. Big data testing is more ambiguous than traditional testing and is less predictable and less accurate in estimating time intervals. Testers need to be armed with knowledge in order to handle problems that arise from uncertainty.\n\nAutomate as many facets for collecting valid data as possible to save time. The data will be analyzed at a later stage to reveal the big picture.\n\nReset existing installation to a known state. By using backup capability, you’ll support operational and regression tests.\n\nValidate outcomes at every big data aspect. Big data’s features — volume, velocity, variety and veracity — require functional testing for verifying each stage, removing defects and enhancing client satisfaction.\n\nCreate a special test environment. Big data testing relies on huge amounts of data and files and cannot be conducted just anywhere.\n\nThe Future Of Big Data Testing\n\nExtremely large datasets are now baked into a vast number of business processes and functions. From supply chain management and operations to marketing and staffing, big data has infiltrated virtually all industries, showing no signs of losing momentum.\n\nManagers of DevOps and Agile teams will need to address the need for computational analysis of data that is too big or moves too quickly for existing processing aptitudes. If they want to serve their clients’ appetite for patterns, trends and associations, they’ll also have to embrace big data testing."
    }
}
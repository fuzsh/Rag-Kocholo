{
    "id": "dbpedia_2449_0",
    "rank": 36,
    "data": {
        "url": "https://www.qmul.ac.uk/maths/research/seminars/complex-systems/",
        "read_more_link": "",
        "language": "en",
        "title": "School of Mathematical Sciences",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://www.qmul.ac.uk/maths/media/ow-assets/assets/icons/qm-logo-white.svg",
            "http://www.maths.qmul.ac.uk/modules/file/icons/application-pdf.png",
            "http://www.maths.qmul.ac.uk/modules/file/icons/application-pdf.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Here I present my ongoing work of estimating mutation rate per cell divisions by combining stochastic processes, Bayesian methods and genomic sequencing data.\n\nHuman cancers usually contain hundreds of billions of cells at diagnosis. During tumour growth these cells accumulate thousand of mutations, errors in the DNA, making each tumour cell unique. This heterogeneity is a major source for evolution within single tumours, subsequent progression and possible treatment resistance. Recent technological advances such as increasingly cheaper genome sequencing allows measuring some of the heterogeneity. However, the theoretical understanding and interpretation of the available data remains mostly unclear. For example, the most basic evolutionary properties of human tumours, such as mutation and cell survival rates or tumour ages are mostly unknown. Here I will present some mathematical modelling of the underlying stochastic processes. In more detail, I will construct the distribution of mutational distances in a tumour that can be measured from multi-region sequencing. I show that these distributions can be understood as random sums of independent random variables. In combination with appropriate sequencing data and Bayesian inference based on our theoretical results some of the evolutionary parameters can be recovered for tumours of single patients.\n\nSystems with delayed interactions play a prominent role in a variety of fields, ranging from traffic and population dynamics, gene regulatory and neural networks or encrypted communications. When subjecting a semiconductor laser to reflections of its own emission, a delay results from the propagation time of the light in the external cavity. Because of its experimental accessibility and multiple applications, semiconductor lasers with delayed feedback or coupling have become one of the most studied delay systems. One of the most experimentally accessible properties to characterise these chaotic dynamics is the autocorrelation function. However, the relationship between the autocorrelation function and other nonlinear properties of the system is generally unknown. Therefore, although the autocorrelation function is often one of the key characteristics measured, it is unclear which information can be extracted from it. Here, we present a linear stochastic model with delay, that allows to analytically derive the autocorrelation function. This linear model captures fundamental properties of the experimentally obtained autocorrelation function of laser with delayed feedback, such as the shift and asymmetric broadening of the different delay echoes. Fitting this analytical autocorrelation to its experimental counterpart, we find that the model reproduces, in most dynamical regimes of the laser, the experimental data surprisingly well. Moreover, it is possible to establish a relation between the set of parameters from the linear model and dynamical properties of the semiconductor lasers, as relaxation oscillation frequency and damping rate.\n\nThe Graph Minors Project of Robertson and Seymour is one of the highlights\n\nof twentieth-century mathematics. In a long series of mostly difficult papers\n\nthey prove theorems that give profound insight into the qualitative structure\n\nof members of proper minor-closed classes of graphs. This insight enables\n\nthem to prove some remarkable banner theorems, one of which is that in\n\nany finite set of graphs there is one that is a minor of the other; in other\n\nwords, graphs are well-quasi-ordered under the minor order.\n\nA canonical way to obtain a matroid is from a set of columns of a matrix over\n\na eld. If each column has at most two nonzero entries there is an obvious\n\ngraph associated with the matroid; thus it is not hard to see that matroids\n\ngeneralise graphs. Robertson and Seymour always believed that their results\n\nwere special cases of more general theorems for matroids obtained from\n\nmatrices over nite elds. For over a decade, Jim Geelen, Bert Gerards and\n\nI have been working towards achieving this generalisation. In this talk I will\n\ndiscuss our success in achieving the generalisation for binary matroids, that\n\nis, for matroids that can be obtained from matrices over the 2-element eld.\n\nIn this talk I will give a very general overview of my work with Geelen\n\nand Gerards. I will not assume familiarity with matroids nor will I assume\n\nfamiliarity with the results of the Graph Minors Project.\n\nKoopman operators globally linearise nonlinear dynamical systems, and their spectral information can be a powerful tool for analysing and decomposing nonlinear dynamical systems. However, Koopman operators are infinite-dimensional, and computing their spectral information is a considerable challenge. We introduce measure-preserving extended dynamic mode decomposition (mpEDMD), the first Galerkin method whose eigendecomposition converges to the spectral quantities of Koopman operators for general measure-preserving dynamical systems. mpEDMD is a data-driven and structure-preserving algorithm based on an orthogonal Procrustes problem that enforces measure-preserving truncations of Koopman operators using a general dictionary of observables. It is flexible and easy to use with any pre-existing DMD-type method and with different data types. We prove the convergence of mpEDMD for projection-valued and scalar-valued spectral measures, spectra, and Koopman mode decompositions. For the case of delay embedding (Krylov subspaces), our results include convergence rates of the approximation of spectral measures as the size of the dictionary increases. We demonstrate mpEDMD on a range of challenging examples, its increased robustness to noise compared with other DMD-type methods, and its ability to capture the energy conservation and cascade of a turbulent boundary layer flow with Reynolds number > 60000 and state-space dimension >100000. Finally, if time permits, we discuss how this algorithm forms part of a broader program on the foundations of infinite-dimensional spectral computations.\n\nRef: Colbrook, Matthew J. \"The mpEDMD algorithm for data-driven computations of measure-preserving dynamical systems.\" SIAM Journal on Numerical Analysis 61.3 (2023): 1585-1608.\n\nThe first application of dimension group to Cantor dynamical systems was in the work of I. Putnam in 1989 where he used dimension group to study interval exchange transformations (IET). Then in his works with T. Giordano, R. I. Herman, and C.F. Skau they developed those ideas which was based on creating Kakutani-Rokhlin (K-R) partitions for IET's to the general case of Cantor systems. They made a break through with this theory when they proved that every uniquely ergodic Cantor minimal system is orbit equivalent to either a Denjoy's or an odometer system; a topological analogues of the well-known Dye's theorem in ergodic theory. Having sequences of K-R partitions, which is the bridge to dimension group, has been established for every zero-dimensional systems (by the works of S. Bezuglyi, K. Medynets, T. Downarowicz, O. Karpel and T. Shimomura) and is a strong tool in studying continuous and measurable spectrum of Cantor systems as well as topological factoring between them. In this talk I will make an introduction to the notions of K-R towers and dimension group and then I will discuss some recent results about applications of them in studying spectrum and topological factoring of dynamical systems on Cantor sets.\n\nPrimality and factorisation play a key role in number theory, being key aspects on the study of the multiplicative structure of the integers and motivating similar constructions in more general number systems. It is thus not surprising that sets of algebraic numbers defined by imposing restrictions on their factorisation structure have deep and interesting properties. We discuss (mostly) two-dimensional shift spaces constructed using number-theoretically defined sets as a basis, and connect the shift space with the local structure and behaviour of the generating set; classic examples such as the shift of visible lattice points and the one-dimensional squarefree shift have natural generalisations in the family of k-free shift spaces, the latter being deeply intertwined with ideas from algebraic number theory; these also serve as first examples of the more general class of multidimensional B-free shift spaces. These shift spaces exhibit interesting (and, from certain perspectives, unusual) properties, including the combination of high complexity (positive entropy) with symmetry rigidity (the automorphism group, or centralizer, is “essentially” trivial, containing only shift maps). Our discussion will be focused on a geometrical interpretation of the notion of “symmetry” in these systems, for which the most appropriate tool is the extended symmetry group (or normalizer), a natural generalisation of the automorphism group which exhibits a wide variety of interesting and non-trivial behaviours in this context, in contrast to the standard automorphism group. This is joint work with Michael Baake, Christian Huck, Mariusz Lemanczyk and Andreas Nickel.\n\nAbstract: Given an action of a group by homeomorphisms on a compact metrisable space X, the enveloping semigroup of this action is its compactification in the semigroup of functions from X to X w.r.t. the topology of point wise convergence. It has been introduced by Robert Ellis in the 60’s. It has very interesting algebraic and topological properties which may serve to characterise the group action. One interesting topological property goes under the name of tameness, or its contrary. A group action is tame if its enveloping semigroup is the sequential compactification of the group action. Minimal tame group actions are almost determined by their spectrum. Non-tame group actions have the reputation of being difficult to manage, but, in joint work with Reem Yassawi we have recently been able to determine the enveloping semigroup of all Z-actions defined by bijective substitutions. We may therefore say that bijective substitutions define “easy” non-tame Z-actions. In this talk I propose simple algebraic concepts from semigroup theory which can be used to refine the notion of non-tameness and distinguish “easy” non-tameness from a more difficult one.\n\nA novel method is presented for stochastic interpolation of a sparsely sampled time signal based on a superstatistical random process generated from a Gaussian scale mixture. In comparison to other stochastic interpolation methods such as kriging, this method possesses strong non-Gaussian properties and is thus applicable to a broad range of real-world time series. A precise sampling algorithm is provided in terms of a mixing procedure that consists of generating a field u(x,t), where each component ux(t) is synthesized with identical underlying noise but covariance Cx(t,s) parameterized by a log-normally distributed parameter x. Due to the Gaussianity of each component ux(t), standard sampling algorithms and methods to constrain the process on the sparse measurement points can be exploited. The scale mixture u(t) is then obtained by assigning each point in time t a x(t) and therefore a specific value from u(x,t), where log x(t) is itself a realization of a Gaussian process with a correlation time large compared to the correlation time of u(x,t). Finally, a wavelet-based hierarchical representation of the interpolating paths is introduced, which is shown to provide an adequate method to locally interpolate large datasets.\n\nThe structure of complex networks can be characterized by counting and analyzing network motifs, which are small graph structures that occur repeatedly in a network, such as triangles or chains. Recent work has generalized motifs to temporal and dynamic network data. However, existing techniques do not generalize to sequential or trajectory data, which represents entities walking through the nodes of a network, such as passengers moving through transportation networks. The unit of observation in these data is fundamentally different, since we analyze observations of walks (e.g., a trip from airport A to airport C through airport B), rather than independent observations of edges or snapshots of graphs over time. In this talk, I will discuss our recent work defining sequential motifs in observed walk data, which are small, directed, and sequenced-ordered graphs corresponding to patterns in observed walks. We draw a connection between counting and analysis of sequential motifs and Higher-Order Network (HON) models. We show that by mapping edges of a HON, specifically a kth-order DeBruijn graph, to sequential motifs, we can count and evaluate their importance in observed data, and we test our proposed methodology with two datasets: (1) passengers navigating an airport network and (2) people navigating the Wikipedia article network. We find that the most prevalent and important sequential motifs correspond to intuitive patterns of traversal in the real systems, and show empirically that the heterogeneity of edge weights in an observed higher-order DeBruijn graph has implications for the distributions of sequential motifs we expect to see across our null models.\n\nData amount, variety, and heterogeneity have been increasing drastically for several years, offering a unique opportunity to better understand complex systems. Among the different modes of data representation, networks appear particularly successful. Indeed, a wide and powerful range of tools from graph theory are available for their exploration. However, the integrated exploration of large multidimensional datasets remains a major challenge in many scientific fields. For instance, in bioinformatics, the understanding of biological systems would require the integrated analysis of dozens of different datasets. In this context, multilayer networks emerged as key players in the analysis of such complex data. Moreover, recent years have witnessed the extension of network exploration approaches to capitalize on more complex and richer network frameworks. Random walks, for instance, have been extended to explore multilayer networks. These kinds of methods are currently used for exploring the whole topology of large-scale networks. Random walk with restart, a special case of random walk, allows to measure similarity between a given node and all the other nodes of a network. This strategy is known to outperform methods based on local distance measures for the prioritization of gene-disease associations. However, current random walk approaches are limited in the combination and heterogeneity of networks they can handle. New analytical and numerical random walk methods are needed to cope with the increasing diversity and complexity of multilayer networks. In the context of my thesis, I developed a new mathematical framework and its associated Python package, named MultiXrank, that allow the integration and exploration of any combinations of networks. The proposed formalism and algorithm are general and can handle heterogeneous and multiplex networks, both directed and weighted. As part of my Ph. D., I also applied this new method to several biological questions such as the prioritization of gene and drug candidates for being involved in different disorders, gene-disease association predictions, and the integration of 3D DNA conformation information with gene and disease networks. This last application offers new tracks to unveil disease comorbidities relationships. During my Ph.D., I was also interested in the extension of several other methods to multilayer networks. In particular, I generalized the Katz similarity measure to multilayer networks. I also developed a new method of community detection. This new community detection is based on random walks with restart and allows the identification of clusters from multilayer network nodes. Finally, I studied network embedding, especially in the case of shallow embedding methods. In this context, I did a literature review, which is quickly evolving. I also developed a network embedding method based on MultiXrank that opens the embedding to more complex multilayer networks.\n\nLattice models such as self-avoiding walk and polygon models have long proved useful for understanding the equilibrium and asymptotic properties of long polymers in solution. Interest in using lattice models to study knot and link statistics grew in the late 1980s when a lattice polygon model was used (by Sumners and Whittington and by Pippenger in 1988) to prove the 1960s Frisch and Wasserman and Delbruck (FWD) conjecture that long polymers should be knotted (self-entangled) with high probability. At the same time, since DNA entanglements were known to be obstructions for normal cellular processes, understanding the entanglement statistics of DNA drew the attention of polymer modellers. Despite much progress since then, many open questions remain for lattice polygon models regarding the details of the knot distribution and the typical \"size\" of the knotted or linked parts. After a general overview of these topics, I will discuss a recent breakthrough about the asymptotic scaling form for the number of n-edge embeddings of a link L in a simple cubic lattice tube with dimensions 2 x 1 x infinity. We prove using a combination of new knot theory results and new lattice polygon combinatorics that, as n goes to infinity, the ratio of the number of n-edge unknot polygons to the number of n-edge link-type L polygons goes to 0 like 1/n to a power, where the power is the number of prime link factors of L. This proves a 1990's conjectured scaling form that is expected to hold for any tube size and in the limit of infinite tube dimensions. The proof also allows us to establish results about the average size of the knotted and linked parts. Monte Carlo results indicate that the same scaling form holds for larger tube sizes and we connect our results to DNA in nanochannel/nanopore experiments. This is joint work with M Atapour, N Beaton, J Eng, K Ishihara, K Shimokawa and M Vazquez.\n\nTime permitting, I will also discuss recent results and open questions for the special case of two component links in which both components span a lattice tube (2SAPs). The latter is joint work with J Eng, P Puttipong, and R Scharein.\n\nThe aim of the following work is to model the maintenance of ecological networks in forest environments, built from bioreserves, patches and corridors, when these grids are subject to random processes such as extreme natural events. The management plan consists in providing both temporary and sustainable habitats to migratory species. It also aims at ensuring connectivity between the natural areas without interruption. After presenting the random graph-theoretic framework, we apply the stochastic optimal control to the graph dynamics. Our results show that the preservation of the network architecture cannot be achieved, under stochastic control, over the entire duration. It can only be accomplished, at the cost of sacrificing the links between the patches, by increasing usage of the control devices. This would have a negative effect on the species migration by causing congestion among the channels left at their disposal. The optimal scenario, in which the shadow price is at its lowest and all connections are well-preserved, occurs at half of the course, be it the only optimal stopping moment found on the stochastic optimal trajectories. The optimal forestry policy thus has to cut down the timing of the practices devoted to biodiversity protection by half.\n\nA few decades ago Baxter conjectured that the “standard” q-state (color) Potts model, where a ferromagnetic interaction takes place between nearest neighboring spins on the square lattice, undergoes a second order transition for q ≤ qc and a first order transition for q > qc with qc = 4 being the changeover integer. Renormalization group arguments suggest that Baxter’s conjecture should hold for other lattices or interaction content, provided that the interaction is local.\n\nThere are, however, counterexamples. An interesting one is the so-called Potts model with “invisible” colors (PMIC), where the standard model is equipped with additional r “invisible” colors that control the entropy of the system but do not affect the energy. It has been shown that for r sufficiently large, the PMIC undergoes a first order transition. Thus, it may occur that the changeover integer is smaller than four or even does not exist.\n\nWe introduce a hybrid Potts model (HPM) where qc can be manipulated in a different way. Consider a system where a random concentration p of the spins assume q0 colors and a random concentration 1 − p of the spins assume q > q0 colors. It is known that when the system is homogeneous, with an integer spin number q0 or q, it undergoes a second or a first order transition, respectively. It is argued that there is a concentration p* such that the transition behavior is changed at p* . This idea is demonstrated analytically and by simulations on the standard model.\n\nIndependently, a mean field type all-to-all interaction HPM is studied. It is shown analytically that p* exists for this model. Exact expressions for the second order critical line in concentration-temperature parameter space, together with some other related critical properties, are derived.\n\nVirtually all the emergent properties of a complex system are rooted in the non-homogeneous nature of the behaviours of its elements and of the interactions among them. However, the fact that heterogeneity and correlations can appear simultaneously at local, mesoscopic, and global scales, is a concrete challenge for any systematic approach to quantify them in systems of different types. We develop here a scalable and non-parametric framework to characterise the presence of heterogeneity and correlations in a complex system, based on the statistics of random walks over the underlying network of interactions among its units. In particular, we focus on normalised mean first passage times between meaningful pre-assigned classes of nodes, and we showcase a variety of their potential applications. We found that the proposed framework is able to characterise polarisation in voting systems such as the roll-call votes in the US Congress. Moreover, the distributions of class mean first passage times can help identifying the key players responsible for the spread of a disease in a social system, and also allow us to introduce the concept of dynamic segregation, that is the extent to which a given group of people, characterized by a given income or ethncity, is internally clustered or exposed to other groups as a result of mobility. By analysing census and mobility data on more than 120 major US cities, we found that the dynamic segregation of African American communities is significantly associated with the weekly excess COVID-19 incidence and mortality in those communities.\n\nSince its inception in the 19th century through the efforts of Poincaré and Lyapunov, the theory of dynamical systems addresses the qualitative behaviour of dynamical systems as understood from models. From this perspective, the modeling of dynamical processes in applications requires a detailed understanding of the processes to be analyzed. This deep understanding leads to a model, which is an approximation of the observed reality and is often expressed by a system of Ordinary/Partial, Underdetermined (Control), Deterministic/Stochastic differential or difference equations. While models are very precise for many processes, for some of the most challenging applications of dynamical systems (such as climate dynamics, brain dynamics, biological systems or the financial markets), the development of such models is notably difficult. On the other hand, the field of machine learning is concerned with algorithms designed to accomplish a certain task, whose performance improves with the input of more data. Applications for machine learning methods include computer vision, stock market analysis, speech recognition, recommender systems and sentiment analysis in social media. The machine learning approach is invaluable in settings where no explicit model is formulated, but measurement data is available. This is frequently the case in many systems of interest, and the development of data-driven technologies is becoming increasingly important in many applications.\n\nThe intersection of the fields of dynamical systems and machine learning is largely unexplored and the objective of this talk is to show that working in reproducing kernel Hilbert spaces offers tools for a data-based theory of nonlinear dynamical systems. In this talk, we introduce a data-based approach to estimating key quantities which arise in the study of nonlinear autonomous, control and random dynamical systems. Our approach hinges on the observation that much of the existing linear theory may be readily extended to nonlinear systems - with a reasonable expectation of success- once the nonlinear system has been mapped into a high or infinite dimensional Reproducing Kernel Hilbert Space. In particular, we develop computable, non-parametric estimators approximating controllability and observability energies for nonlinear systems. We apply this approach to the problem of model reduction of nonlinear control systems. It is also shown that the controllability energy estimator provides a key means for approximating the invariant measure of an ergodic, stochastically forced nonlinear system. We also show how kernel methods can be used to detect critical transitions for some multi scale dynamical systems. We also use the method of kernel flows to predict some chaotic dynamical systems. Finally, we show how kernel methods can be used to approximate center manifolds, propose a data-based version of the centre manifold theorem and construct Lyapunov functions for nonlinear ODEs. This is joint work with Jake Bouvrie (MIT, USA), Peter Giesl (University of Sussex, UK), Christian Kuehn (TUM, Munich/Germany), Romit Malik (ANNL), Sameh Mohamed (SUTD, Singapore), Houman Owhadi (Caltech), Martin Rasmussen (Imperial College London), Kevin Webster (Imperial College London), Bernard Hasasdonk, Gabriele Santin and Dominik Wittwar (University of Stuttgart).\n\nThe dispersal of individuals within an animal population will depend upon local properties intrinsic to the environment that differentiate superior from inferior regions as well as properties of the population. Competing concerns can either draw conspecifics together in aggregation, such as collective defence against predators, or promote dispersal that minimizes local densities, for instance to reduce competition for food. In this talk we consider a range of models of non-independent movement. These include established models, such as the ideal free distribution, but also novel models which we introduce, such as the wheel. We will also discuss several ways to combine different models to create a flexible model to address a variety of dispersal mechanisms. We further discuss novel measures of movement coordination and show how to generate a population movement that achieves appropriate values of the measure specified. The movement framework that we have developed is both of interest as a stand-alone process to explore movement, but also able to generate a variety of movement patterns that can be embedded into wider evolutionary models where movement is not the only consideration.\n\nThe duration of interaction events in a society is a fundamental measure of its collective nature and potentially reflects variability in individual behavior. Using automated monitoring of social interactions of individual honeybees in 5 honeybee colonies, we performed a high-throughput measurement of trophallaxis and face-to-face event durations experienced by honeybees over their entire lifetimes. We acquired a rich and detailed dataset consisting of more than 1.2 million interactions in five honeybee colonies. We find that bees, like humans, also interact in bursts but that spreading is significantly faster than in a randomized reference network and remains so even after an experimental demographic perturbation. Thus, while burstiness may be an intrinsic property of social interactions, it does not always inhibit spreading in real-world communication networks.The interaction time distribution is heavy-tailed, as previously reported for human face-to-face interactions. We developed a theory of pair interactions that takes into account individual variability and predicts the scaling behavior for both bee and extant human datasets. The individual variability of worker honeybees was non-zero, but less than that of humans, possibly reflecting their greater genetic relatedness. Our work shows how individual differences can lead to universal patterns of behavior that transcend species and specific mechanisms for social interactions.\n\nIn cancer, but also evolution in general, great effort is expended to find \"driver-mutations\", which are specific mutations in genes that significantly increase the fitness of an individual or a cell - and, in the case of cancer, cause the growth of tumour in the first place. But how can we distinguish them if we don't know what baseline to compare them to? This is where research into the dynamics of neutral random mutations becomes relevant. We find certain signals such as the measured frequency and burden distributions of mutations in a sample that can give us information about core population characteristics like the population size of stem cells, the mutation rate, or the percentage of symmetric cell divisions. Analysing the patterns of random mutations thus provides a theoretical tool to interpret genomic data of healthy tissues, for the purpose of both improving detection of true driver mutations as well as learning more about the underlying dynamics of the population which are often hard to measure directly.\n\nHyperbolic dynamic systems, by definition, have two complementary directions, one with uniform (stable) contraction and the other with uniform (unstable) expansion. A famous and important example of these systems is the so-called Smale horseshoe. However, there are many systems where hyperbolicity is not satisfied. We are interested in dynamics where there is a (central) direction where the effects of contraction and expansion superpose and the resulting action of the dynamics is neutral. In this context, I will present dynamical systems that have a simple description as a skew product of a horseshoe and two diffeomorphisms of the circle. Important examples of these diffeomorphisms are the projective actions of SL(2, R) matrices on the circle. The most interesting case occurs when one matrix is hyperbolic (eigenvalues different from one) and the other one is elliptic (eigenvalues of modulus one). In this model, two related systems come together: a non-hyperbolic system (with stable, unstable and central directions) and a matrix product (called a cocycle). The Lyapunov exponent (expansion rate) associated to the central direction corresponds to the exponential growth of the norms of the product of these matrices. We want to describe the “spectrum” of such “Lyapunov exponents”. For this, it is necessary to understand the underlying “thermodynamic formalism” and the \"structure of the space ergodic measures”, where the appearance of so-called non-hyperbolic measures is a key difficulty. Our goal is to discuss this scenario, presenting key concepts and ideas and some results.\n\nTemporal graphs (in which edges are active only at specified time\n\nsteps) are an increasingly important and popular model for a wide variety\n\nof natural and social phenomena. I'll talk a bit about what's been going on\n\nin the world of temporal graphs, and then go on to the idea of graph\n\nmodification in a temporal setting.\n\nMotivated by a particular agricultural example, I’ll talk about the\n\ntemporal nature of livestock networks, with a quick diversion into\n\nrecognising the periodic nature of some cattle trading systems. With\n\nbovine epidemiology in mind, I'll talk about a particular modification\n\nproblem in which we assign times to edges so as to maximise or minimise\n\nreachability sets within a temporal graph. I'll mention an assortment of\n\ncomplexity results on these problems, showing that they are hard under a\n\ndisappointingly large variety of restrictions. In particular, if edges can\n\nbe grouped into classes that must be assigned the same time, then the\n\nproblem is hard even on directed acyclic graphs when both the reachability\n\ntarget and the classes of edges are of constant size, as well as on an\n\nextremely restrictive class of trees. The situation is slightly better if\n\neach edge is active at a unique timestep - in some very restricted cases\n\nthe problem is solvable in polynomial time. (Joint work with Kitty Meeks.)\n\nThe modern world can be best described as interlinked networks, of individuals, computing devices and social networks; where information and opinions propagate through their edges in a probabilistic or deterministic manner via interactions between individual constituents. These interactions can take the form of political discussions between friends, gossiping about movies, or the transmission of computer viruses. Winners are those who maximise the impact of scarce resource such as political activists or advertisements, or by applying resource to the most influential available nodes at the right time. We developed an analytical framework, motivated by and based on statistical physics tools, for impact maximisation in probabilistic information propagation on networks; to better understand the optimisation process macroscopically, its limitations and potential, and devise computationally efficient methods to maximise impact (an objective function) in specific instances.\n\nThe research questions we have addressed relate to the manner in which one could maximise the impact of information propagation by providing inputs at the right time to the most effective nodes in the particular network examined, where the impact is observed at some later time. It is based on a statistical physics inspired analysis, Dynamical Message Passing that calculates the probability of propagation to a node at a given time, combined with a variational optimisation process. We address the following questions: 1) Given a graph, a budget and a propagation/infection process, which nodes are best to infect to maximise the spreading? 2) Maximising the impact on a subset of particular nodes at given times, by accessing a limited number of given nodes. 3) Identify the most appropriate vaccination targets to isolate a spreading disease through containment of the epidemic. 4) Optimal deployment of resource in the presence of competitive/collaborative processes. We also point to potential applications.\n\nLokhov A.Y. and Saad D., Optimal Deployment of Resources for Maximizing Impact in Spreading Processes, PNAS 114 (39), E8138 (2017)\n\nFor every random process, all measurable quantities are described\n\ncomprehensively through their probability distributions. in the ideal but rare case\n\nthey can be obtained analytically, i.e., completely. most physical\n\nmodels are not accessible analytically thus one has to perform numerical\n\nsimulations. usually this means one does many independent runs and\n\nobtains estimates of the probability distributions by the measured\n\nhistograms. since the number of repetitions is limited, maybe 10\n\nmillion, correspondingly the distributions can be estimated in a range\n\ndown to probabilities like 10^-10. but what if one wants to obtain the\n\nfull distribution, in the spirit of obtaining all information?\n\nthis means one desires to get the distribution down to the rare\n\nevents, but without waiting forever by performing an almost infinite\n\nnumber of simulation runs.\n\nhere, we study rare events numerically using a very general black-box\n\nmethod. it is based on sampling vectors of random numbers within an\n\nartificial finite-temperature (boltzmann) ensemble to access rare\n\nevents and large deviations for almost arbitrary equilibrium and\n\nnon-equilibrium processes. in this way, we obtain probabilities as\n\nsmall as 10^-500 and smaller, hence (almost) the full distribution can\n\nbe obtained in a reasonable amount of time.\n\nhere, some applications are presented:\n\ndistribution of work performed for a critical (t=2.269)\n\ntwo-dimensional ising system of size lxl=128x128 upon rapidly changing\n\nthe external magnetic field (only by obtaining the distribution over hundreds\n\nof decades it allows to check the jarzynski and crooks\n\ntheorems which exactly relate the non-equilibrium work to the\n\nequilibrium free energy);\n\ndistribution of perimeters and area of convex hulls of\n\nfinite-dimensional single and multiple random walks;\n\ndistribution of the height fluctuations of the kardar-parisi-zhang (kpz)\n\nequation via a model of directed polymers in random media.\n\nThe explosion in digital music information has spurred the developing of mathematical models and computational algorithms for accurate, efficient, and scalable processing of music information. Total global recorded music revenue was US$17.3b in 2017, 41% of which was digital (2018 IFPI Report). Industrial scale applications like Shazam has over 150 million active users monthly and Spotify over 140 million. With such widespread access to large digital music collections, there is substantial interest in scalable models for music processing. Optimisation concepts and methods thus play an important role in machine models of music engagement, music experience, music analysis, and music generation. In the first part of the talk, I shall show how optimisation ideas and techniques have been integrated into computer models of music representation and expressivity, and into computational solutions to music generation and structure analysis.\n\nAdvances in medical and consumer devices for measuring and recording physiological data have given rise to parallel developments in computing in cardiology. While the information sources (music and cardiac signals) share many rhythmic and other temporal similarities, the techniques of mathematical representation and computational analysis have developed independently, as have the tools for data visualization and annotation. In the second part of the talk, I shall describe recent work applying music representation and analysis techniques to electrocardiographic sequences, with applications to personalised diagnostics, cardiac-brain interactions, and disease and risk stratification. These applications represent ongoing collaborations with Professors Pier Lambiase and Peter Taggart (UCL), and Dr. Ross Hunter at the Barts Heart Centre.\n\nAbout the speaker:\n\nElaine Chew is Professor of Digital Media in the School of Electronic Engineering and Computer Science at Queen Mary University of London. Before joining QMUL in Fall 2011, she was a tenured Associate Professor in the Viterbi School of Engineering and Thornton School of Music (joint) at the University of Southern California, where she founded the Music Computation and Cognition Laboratory and was the inaugural honoree of the Viterbi Early Career Chair. She has also held visiting appointments at Harvard (2008-2009) and Lehigh University (2000-2001), and was Affiliated Artist of Music and Theater Arts at MIT (1998-2000). She received PhD and SM degrees in Operations Research at MIT (in 2000 and 1998, respectively), a BAS in Mathematical and Computational Sciences (honors) and in Music (distinction) at Stanford (1992), and FTCL and LTCL diplomas in Piano Performance from Trinity College, London (in 1987 and 1985, respectively).\n\nShe was awarded an ERC ADG in 2018 for the project COSMOS: Computational Shaping and Modeling of Musical Structures, and is a past recipient of a 2005 Presidential Early Career Award in Science and Engineering (the highest honor conferred on young scientists/engineers by the US Government at the White House) and Faculty Early Career Development (CAREER) Award by the US National Science Foundation, and 2007/2017 Fellowships at Harvard’s Radcliffe Institute for Advanced Studies. She is an alum (Fellow) of the (US) National Academy of Science's Kavli Frontiers of Science Symposia and of the (US) National Academy of Engineering's Frontiers of Engineering Symposia for outstanding young scientists and engineers.\n\nHer research, centering on computational analysis of music structures in performed music, performed speech, and cardiac arrhythmias, has been supported by the ERC, EPSRC, AHRC, and NSF, and featured on BBC World Service/Radio 3, Smithsonian Magazine, Philadelphia Inquirer, Wired Blog, MIT Technology Review, The Telegraph, etc.\n\nSystems with delayed interactions play a prominent role in a variety of fields, ranging from traffc and population dynamics, gene regulatory and neural networks or encrypted communications. When subjecting a semiconductor laser to reflections of its own emission, a delay results from the propagation time of the light in the external cavity. Because of its experimental accessibility and multiple applications, semiconductor lasers with delayed feedback or coupling have become one of the most studied delay systems. One of the most experimentally accessible properties to characterise these chaotic dynamics is the autocorrelation function. However, the relationship between the autocorrelation function and other nonlinear properties of the system is generally unknown. Therefore, although the autocorrelation function is often one of the key characteristics measured, it is unclear which information can be extracted from it. Here, we present a linear stochastic model with delay, that allows to analytically derive the autocorrelation function. This linear model captures fundamental properties of the experimentally obtained autocorrelation function of laser with delayed feedback, such as the shift and asymmetric broadening of the different delay echoes. Fitting this analytical autocorrelation to its experimental counterpart, we find that the model reproduces, in most dynamical regimes of the laser, the experimental data surprisingly well. Moreover, it is possible to establish a relation between the set of parameters from the linear model and dynamical properties of the semiconductor lasers, as relaxation oscillation frequency and damping rate.\n\nElements composing complex systems usually interact in several different ways and as such the interaction architecture is well modelled by a network with multiple layers - a multiplex network–. However only in a few cases can such multi-layered architecture be empirically observed, as one usually only has experimental access to such structure from an aggregated projection. A fundamental challenge is thus to determine whether the hidden underlying architecture of complex systems is better modelled as a single interaction layer or results from the aggregation and interplay of multiple layers.\n\nAssuming a prior of intralayer Markovian diffusion, in this talk I will present a method [1] that, using local information provided by a random walker navigating the aggregated network, is able possible to determine in a robust manner whether these dynamics can be more accurately represented by a single layer or they are better explained by a (hidden) multiplex structure. In the latter case, I will also provide a Bayesian method to estimate the most probable number of hidden layers and the model parameters, thereby fully reconstructing its hidden architecture. The whole methodology enables to decipher the underlying multiplex architecture of complex systems by exploiting the non- Markovian signatures on the statistics of a single random walk on\n\nthe aggregated network.\n\nIn fact, the mathematical formalism presented here extends above and beyond detection of physical layers in networked complex systems, as it provides a principled solution for the optimal decomposition and projection of complex, non-Markovian dynamics into a Markov switching combination of diffusive modes.\n\nI will validate the proposed methodology with numerical simulations of both (i) random walks navigating hidden multiplex networks (thereby reconstructing the true hidden architecture) and (ii) Markovian and non-Markovian continuous stochastic processes (thereby reconstructing an effective multiplex decomposition where each layer accounts for a different diffusive mode). I will also state two existence theorems guaranteeing that an exact reconstruction of the dynamics in terms of these hidden jump-Markov models is always possible for arbitrary finite-order Markovian and fully non-Markovian processes. Finally, using experiments, I will apply the methodology to understand the dynamics of RNA polymerases at the single-molecule level.\n\n[1] L. Lacasa, I.P. Mariño, J. Miguez, V. Nicosia, E. Roldan, A. Lisica, S.W. Grill, J. Gómez-Gardeñes,\n\nMultiplex decomposition of non-Markovian dynamics and the hidden layer reconstruction\n\nproblem\n\nPhysical Review X 8, 031038 (2018)\n\nConsider equations of motion that generate dispersion of an ensemble of particles. For a given dynamical system an interesting problem is not only what type of diffusion is generated by its equations of motion but also whether the resulting diffusive dynamics can be reproduced by some known stochastic model. I will discuss three examples of dynamical systems generating different types of diffusive transport: The first model is fully deterministic but non-chaotic by displaying a whole range of normal and anomalous diffusion under variation of a single control parameter [1]. The second model is a dissipative version of the paradigmatic standard map. Weakly perturbing it by noise generates subdiffusion due to particles hopping between multiple attractors [2]. The third model randomly mixes in time chaotic dynamics generating normal diffusive spreading with non-chaotic motion where all particles localize. Varying a control parameter the mixed system exhibits a transition characterised by subdiffusion. In all three cases I will show successes, failures and pitfalls if one tries to reproduce the resulting diffusive dynamics by using simple stochastic models. Joint work with all authors on the references cited below.\n\n[1] L. Salari, L. Rondoni, C. Giberti, R. Klages, Chaos 25, 073113 (2015)\n\n[2] C.S. Rodrigues, A.V. Chechkin, A.P.S. de Moura, C. Grebogi and R. Klages, Europhys. Lett. 108, 40002 (2014)\n\n[3] Y.Sato, R.Klages, to be published.\n\nIt is widely believed that to perform cognition, it is essential for a system to \"have an architecture in the form of a neural network, i.e. to represent a collection of relatively simple units coupled to each other with adjustable couplings. The main, if not the only, reason for this conviction is that the single natural cognitive system known to us, the brain, has this property. With this, understanding how the brain works is one of the greatest challenges of modern science.\"\n\nThe traditional way to study the brain is to explore its separate parts and to search for correlations and emergent patterns in their behavior. This approach does not satisfactorily answer some fundamental questions, such as how memories are stored, or how the data from detailed neural measurements could be arranged in a single picture explaining what the brain does. It is well appreciated that the mind is an emergent property of the brain, and it is important to find the right level for its description.\n\nThere have been much research devoted to describing and understanding the brain from the viewpoint of the dynamical systems (DS) theory. However, the focus of this research has been on the behavior of the system and was largely limited to modelling of the brain, or of the phenomena occurring in the brain.\n\nWe propose to shift the focus from the brain behavior to the ruling force behind the behavior, which in a DS is the velocity vector field. We point out that this field is a mathematical representation of the device's architecture, the result of interaction between all of the device's components, and as such represents an emergent property of the device. With this, the brain's unique feature is its architectural plasticity, i.e. a continual formation, severance, strenghtening and weakening of its inter-neuron connections, which is firmly linked to its cognitive abilities. We propose that the self-organising architectural plasticity of the brain creates a plastic self-organising velocity field, which evolves spontaneously according to some deterministic laws under the influence of sensory stimuli. Velocity fields of this type have not been known in the theory of dynamical systems, and we needed to introduce them specially to describe cognition [1].\n\nWe hypothesize that the ability to perform cognition is linked to the ability to create a self-organising velocity field evolving according to some appropriate laws, rather than with the neural-network architecture per se. With this, the plastic neural network is the means to create the required velocity field, which might not be uniqe.\n\nTo verify our hypothesis, we construct a very simple dynamical system with plastic velocity field, which is arhictecturally not a neural network, and show how it is able to perform basic cognition expected of neural networks, such as memorisation, classification and pattern recognition.\n\nLooking at the brain through the prism of its velocity vector field offers answers to a range of questions about memory storage and pattern recognition in the brain, and delivers the sought-after link between the brain substance and the bodily behavior. At the same time, constructing various rules of self-organisation of a velocity vector field and implementing them in man-made devices could lead to artificial intelligent machines of novel types.\n\n[1] Janson, N.B. & Marsden, C.J. Dynamical system with plastic self-organized velocity field as an alternative conceptual model of a cognitive system. Scientific Reports 7, 17007 (2017).\n\nMuch of the progress that has been made in the field of complex networks is\n\nattributed to adopting dynamical processes as the means for studying these\n\nnetworks, as well as their structure and response to external factors. In this\n\ntalk, by taking a different lens, I view complex networks as combinatorial\n\nstructures and show that this — somewhat alternative — approach brings new\n\nopportunities for exploration. Namely, the focus is made on the sparse regime of\n\nthe configuration model, which is the maximum entropy network constrained by an\n\narbitrary degree distribution, and on the generalisations of this model to the\n\ncases of directed and coloured edges (also known as the configuration multiplex\n\nmodel). We study how the (multivariate) degree distribution in these networks\n\ndefines global emergent properties, such as the sizes and structure of connected\n\ncomponents. By applying Joyal's theory of combinatorial species, the questions\n\nof connectivity and structure are formalised in terms of formal power series,\n\nand unexpected link is made to stochastic processes. Then, by studying the\n\nlimiting behaviour of these processes, we derive asymptotic theory that is rich\n\non analytical expressions for various generalisations of the configuration\n\nmodel. Furthermore, interesting connections are made between configuration model\n\nand physical processes of different nature.\n\nThe mean-median map [4, 2, 1, 3] was originally introduced as a map over the space of nite multisets of real numbers. It extends such a multiset by adjoining to it a new number uniquely determined by the stipulation that the mean of the extended multiset be equal to the median of the original multiset. An open conjecture states that the new numbers produced by iterating this map form a sequence which stabilises, i.e., reaches a nite limit in nitely many iterations. We study the mean-median map as a dynamical system on the space of nite multisets of univariate piecewise-ane continuous functions with rational coecients. We determine the structure of the limit function in the neighbourhood of a distinctive family of rational points. Moreover, we construct a reduced version of the map which simplies the dynamics in such neighbourhoods and allows us to extend the results of [1] by over an order of magnitude.\n\nReferences\n\n[1] F. Cellarosi, S. Munday, On two conjectures for M&m sequences, J. Di. Equa-\n\ntions and Applications 2 (2017), 428-440.\n\n[2] M. Chamberland, M. Martelli, The mean-median map, J. Di. Equations and\n\nApplications, 13 (2007), 577-583.\n\n[3] J. Hoseana, The mean-median map, MSc thesis, Queen Mary, University of\n\nLondon, 2015.\n\n[4] H. Shultz, R. Shi\n\nett, M&m sequences, The College Mathematics Journal, 36\n\n(2005), 191-198.\n\nKinetic theory is a landmark of statistical physics and is applicable to reveal the physical Brownian motion from first principles. In this framework, the Boltzmann and Langevin equations are systematically derived from the Newtonian dynamics via the Bogoliubov-Born-Green-Kirkwood-Yvon (BBGKY) hierarchy [1,2]. In light of this success, it is natural to apply this methodology to social science beyond physics, such as to finance. In this presentation, we apply kinetic theory to financial Brownian motion [3,4] with the empirical support by detailed high-frequency data analysis of a foreign exchange (FX) market.\n\nWe first show our data analysis to identify the microscopic dynamics of high-frequency traders (HFTs). By tracking trajectories of all traders individually, we characterize the dynamics of HFTs from the viewpoint of trend-following. We then introduce a microscopic model of FX traders incorporating with the trend following law. We apply the mathematical formulation of kinetic theory to the microscopic model for coarse-graining; Boltzmann-like and Langevin-like equations are derived via a generalized BBGKY hierarchy. We perturbatively solve these equations to show the consistency between our microscopic model and real data. Our work highlights the potential power of statistical physics in understanding the financial market dynamics from their microscopic dynamics.\n\nReferences\n\n[1] S. Chapman, T. G. Cowling, The Mathematical Theory of Non-Uniform Gases, (Cambridge University Press, Cambridge, England, 1970).\n\n[2] N. G. van Kampen, Stochastic Processes in Physics and Chemistry, 3rd ed. (Elsevier, Amsterdam, 2007).\n\n[3] K. Kanazawa, T. Sueshige, H. Takayasu, M. Takayasu, Phys. Rev. Lett. 120, 138301 (2018).\n\n[4] K. Kanazawa, T. Sueshige, H. Takayasu, M. Takayasu, Phys. Rev. E (in press, arXiv:1802.05993).\n\nThis will be a joint seminar of Complex Systems with the Institute of Applied Data Sciences.\n\nTopology, one of the oldest branches of mathematics, provides an expressive and affordable language which is progressively pervading many areas of biology, computer science and physics.\n\nIn this context, topological data analysis (TDA) tools have emerged as able to provide insights into high-dimensional, noisy and non-linear datasets coming from very different subjects.\n\nHere I will introduce two TDA tools, persistent homology and Mapper, and illustrate what novel insights they are yielding, with particular attention to the study of the functional, structural and genetic connectomes.\n\nI will show how topological observables capture and distinguish variations in the mesoscopic functional organization in two case studies: i) between drug-induced altered brain states, and ii) between perceptual states and the corresponding mental images.\n\nMoving to the structural level, I will compare the homological features of structural and functional brain networks across a large age span and highlight the presence of dynamically coordinated compensation mechanisms, suggesting that functional topology is conserved over the depleting structural substrate.\n\nFinally, using brain gene expression data, I will briefly describe recent work on the construction of a topological genetic skeleton highlighting differences in structure and function of different genetic pathways within the brain.\n\nWe all need to rely on cooperation with other individuals in many aspects of everyday life, such as teamwork and economic exchange in anonymous markets. In this seminar I will present some empirical evidence from human experiments carried out in a controlled laboratory setting which focus on the impact of reputation in dynamic networked interactions. People are engaged in playing pairwise repeated Prisoner's Dilemma games with their neighbours, or partners, and they are paid with real money according to their performance during the experiment. We will see whether and how the ability to make or break links in social networks fosters cooperation, paying particular attention to whether information on an individual’s actions is freely available to potential partners. Studying the role of information is relevant as complete knowledge on other people’s actions is often not available for free. We will also focus on the role of individual reputation, an indispensable tool to guide decisions about social and economic interactions with individuals otherwise unknown, and in the way this reputation is obtained in a hierarchical structure. We will show how the presence of reputation can be fundamental for achieving higher levels of cooperation in human societies. These findings point to the importance of ensuring the truthfulness of reputation for a more cooperative and fair society.\n\nParkinson’s disease is a neurodegenerative condition characterised by loss\n\nof neurons producing dopamine in the brain. It affects 7 million people\n\nworldwide, making it the second most common neurodegenerative disease, and\n\nit currently has no cure. The difficulty of developing treatments and\n\ntherapies lies in the limited understanding of the mechanisms that induce\n\nneurodegeneration in the disease. Experimental evidence suggests that the\n\naggregation alpha synuclein monomers into toxic oligomeric forms can be the\n\ncause of dopaminergic cell death and that their detection in cerebrospinal\n\nfluid could be a potential biomarker for the disease. In addition, the study\n\nof these alpha synuclein aggregates and their aggregation pathways could\n\npotentially lead to early diagnostic of the disease. However, the small size\n\nof alpha synuclein monomers and the heterogeneity of the oligomers makes\n\ntheir detection under conventional bulk approaches extremely challenging,\n\noften requiring sample concentrations orders of magnitude higher than\n\nclinically relevant. Nanopore sensing techniques offer a powerful platform\n\nto perform such analysis, thanks to their ability to read the information of\n\na single molecule at a time while requiring very low sample volume (µl).\n\nThis project presents a novel nanopore configuration capable of addressing\n\nthese limitations: two nanopores separated by a 20nm gap joined together by\n\na zeptolitre nanobridge. The confinement slows molecules translocating\n\nthrough the nanobridge by up to two orders of magnitude compared to standard\n\nnanopore configurations, improving significantly the limits of detection.\n\nFurthermore, this new nanopore setting is size adaptable, and can be used to\n\ndetect a variety of analytes.\n\nIn this talk I will present a new modeling framework to describe co-existing physical and socio-economic components in interconnected smart-grids. The modeling paradigm builds on the theory of evolutionary game dynamics and bio-inspired collective decision making. In particular, for a large population of players we consider acollective decision making process with three possible options: option A or B or no option. The more popular option is more likely to be chosen by uncommitted players and cross-inhibitory signals can be sent to attract players committed to a different option. This model originates in the context of honeybees swarms, and we generalise it to accommodate other applications such as duopolistic competition and opinion dynamics as well as consumers' behavior in the grid. During the talk I will introduce a new game dynamics called expected gain pairwise comparison dynamics which explains the ways in which the strategic behaviour of the players may lead to deadlocks or consensus. I will then discuss equilibrium points and stability in the case of symmetric or asymmetric cross-inhibitory signals. I will discuss the extension of the results to the case of structured environment in which the players are modelled via a complex network with heterogeneous connectivity. Finally, I will illustrate the ways in which such modeling framework can be extended to energy systems.\n\nThe hydrodynamic approximation is an extremely powerful tool to describe the behavior of many-body systems such as gases. At the Euler scale (that is, when variations of densities and currents occur only on large space-time scales), the approximation is based on the idea of local thermodynamic equilibrium: locally, within fluid cells, the system is in a Galilean or relativistic boost of a Gibbs equilibrium state. This is expected to arise in conventional gases thanks to ergodicity and Gibbs thermalization, which in the quantum case is embodied by the eigenstate thermalization hypothesis. However, integrable systems are well known not to thermalize in the standard fashion. The presence of infinitely-many conservation laws preclude Gibbs thermalization, and instead generalized Gibbs ensembles emerge. In this talk I will introduce the associated theory of generalized hydrodynamics (GHD), which applies the hydrodynamic ideas to systems with infinitely-many conservation laws. It describes the dynamics from inhomogeneous states and in inhomogeneous force fields, and is valid both for quantum systems such as experimentally realized one-dimensional interacting Bose gases and quantum Heisenberg chains, and classical ones such as soliton gases and classical field theory. I will give an overview of what GHD is, how its main equations are derived and its relation to quantum and classical integrable systems. If time permits I will touch on the geometry that lies at its core, how it reproduces the effects seen in the famous quantum Newton cradle experiment, and how it leads to exact results in transport problems such as Drude weights and non-equilibrium currents.\n\nThis is based on various collaborations with Alvise Bastianello, Olalla Castro Alvaredo, Jean-Sébastien Caux, Jérôme Dubail, Robert Konik, Herbert Spohn, Gerard Watts and my student Takato Yoshimura, and strongly inspired by previous collaborations with Denis Bernard, M. Joe Bhaseen, Andrew Lucas and Koenraad Schalm.\n\nThe motion of a tracer particle in a complex medium typically exhibits anomalous diffusive patterns, characterised, e.g, by a non-liner mean-squared displacement and/or non-Gaussian statistics. Modeling such fluctuating dynamics is in general a challenging task, that provides, despite all, a fundamental tool to probe the rheological properties of the environment. A prominent example is the dynamics of a tracer in a suspension of swimming microorganisms, like bacteria, which is driven by the hydrodynamic fields generated by the active swimmers. For dilute systems, several experiments confirmed the existence of non-Gaussian fat tails in the displacement distribution of the probe particle, that has been recently shown to fit well a truncated Lévy distribution. This result was obtained by applying an argument first proposed by Holtsmark in the context of gravitation: the force acting on the tracer is the superposition of the hydrodynamic fields of spatially random distributed swimmers. This theory, however, does not clarify the stochastic dynamics of the tracer, nor it predicts the non monotonic behaviour of the non-Gaussian parameter of the displacement distribution. Here we derive the Langevin description of the stochastic motion of the tracer from microscopic dynamics using tools from kinetic theory. The random driving force in the equation of motion is a coloured Lévy Poisson process, that induces power-law distributed position displacements. This theory predicts a novel transition of their characteristic exponents at different timescales. For short ones, the Holtzmark-type scaling exponent is recovered; for intermediate ones, it is larger. Consistently with previous works, for even longer ones the truncation appears and the distribution converge to a Gaussian. Our approach allows to employ well established functional methods to characterize the displacement statistics and correlations of the tracer. In particular, it qualitatively reproduces the non monotonic behaviour of the non-Gaussian parameter measured in recent experiments.\n\nCharacterizing how we explore abstract spaces is key to understand our (ir)rational behaviour and decision making. While some light has been shed on the navigation of semantic networks, however, little is known about the mental exploration of metric spaces, such as the one dimensional line of numbers, prices, etc. Here we address this issue by investigating the behaviour of users exploring the “bid space” in online auctions. We find that they systematically perform Lévy flights, i.e., random walks whose step lengths follow a power-law distribution. Interestingly, this is the best strategy that can be adopted by a random searcher looking for a target in an unknown environment, and has been observed in the foraging patterns of many species. In the case of online auctions, we measure the power-law scaling over several decades, providing the neatest observation of Lévy ﬂights reported so far. We also show that the histogram describing single individual exponents is well peaked, pointing out the existence of an almost universal behaviour. Furthermore, a simple model reveals that the observed exponents are nearly optimal, and represent a Nash equilibrium. We rationalize these ﬁndings through a simple evolutionary process, showing that the observed behaviour is robust against invasion of alternative strategies. Our results show that humans share with the other animals universal patterns in general searching processes, and raise fundamental issues in cognitive, behavioural and evolutionary sciences.\n\nBiological invasion can be generically defined as the uncontrolled spread and proliferation of species to areas outside of their native range, hence called alien, usually following by their unintentional introduction by humans. A conventional view of the alien species spatial spread is that it occurs via the propagation of a travelling population front. In a realistic 2D system, such a front normally separates the invaded area behind the front from the uninvaded areas in front of the front. I will show that there is an alternative scenario called “patchy invasion” where the spread takes place via the spatial dynamics of separate patches of high population density with a very low density between them, and a continuous population front does not exist at any time. Patchy invasion has been studied theoretically in much detail using diffusion-reaction models, e.g. see Chapter 12 in [1]. However, diffusion-reaction models have many limitations; in particular, they almost completely ignore the so-called long distance dispersal (usually associated with stochastic processes known as Levy flights). Correspondingly, I will then present some recent results showing that patchy invasion can occur as well when long distance dispersal is taken into account [2]. In this case, the system is described by integral-difference equations with fat-tailed dispersal kernels. I will also show that apparently minor details of kernel parametrization may have a relatively strong effect on the rate of species spread.\n\n[1] Malchow H, Petrovskii SV, Venturino E (2008) Spatiotemporal Patterns in Ecology and Epidemiology: Theory, Models, Simulations. Chapman & Hall / CRC Press, 443p.\n\n[2] Rodrigues LAD, Mistro DC, Cara ER, Petrovskaya N, Petrovskii SV (2015) Bull. Math. Biol. 77, 1583-1619.\n\nThe title of my talk was the topic of an Advanced Study Group for which I was convenor last year [1]. In my talk I will give a brief outline about our respective research activities. It should be understandable to a rather general audience.\n\nA question that attracted a lot of attention in the past two decades is whether biologically relevant search strategies can be identified by statistical data analysis and mathematical modeling. A famous paradigm in this field is the Levy flight hypothesis. It states that under certain mathematical conditions Levy dynamics, which defines a key concept in the theory of anomalous stochastic processes, leads to an optimal search strategy for foraging organisms. This hypothesis is discussed very controversially in the current literature [2]. After briefly introducing the stochastic processes of Levy flights and Levy walks I will review examples and counterexamples of experimental data and their analyses confirming and refuting the Levy flight hypothesis. This debate motivated own work on deriving a fractional diffusion equation for an n-dimensional correlated Levy walk [3], studying search reliability and search efficiency of combined Levy-Brownian motion [4], and investigating stochastic first passage and first arrival problems [5].\n\n[1] www.mpipks-dresden.mpg.de/~asg_2015(link is external)\n\n[2] R.Klages, Search for food of birds, fish and insects, invited book chapter in: A.Bunde, J.Caro, J.Kaerger, G.Vogl (Eds.), Diffusive Spreading in Nature, Technology and Society. (Springer, Berlin, 2017).\n\n[3] J.P.Taylor-King, R.Klages, S.Fedotov, R.A.Van Gorder, Phys.Rev.E 94, 012104 (2016).\n\n[4] V.V.Palyulin, A.Chechkin, R.Klages, R.Metzler, J.Phys.A: Math.Theor. 49, 394002 (2016).\n\n[5] G.Blackburn, A.V.Chechkin, V.V.Palyulin, N.W.Watkins, R.Klages, tbp.\n\nWe all need to rely on cooperation with other individuals in many aspects of everyday life, such as teamwork and economic exchange in anonymous markets. In this seminar I will present two laboratory experiments which focus on the impact of information and reputation on human behavior when people engage cooperative interactions on dynamic networks. In the first study, we investigate whether and how the ability to make or break links in social networks fosters cooperation, paying particular attention to whether information on an individual’s actions is freely available to potential partners. Studying the role of information is relevant as complete knowledge on other people’s actions is often not available for free. In the second work, we focus our attention on the role of individual reputation, an indispensable tool to guide decisions about social and economic interactions with individuals otherwise unknown. Usually, information about prospective counterparts is incomplete, often being limited to an average success rate. Uncertainty on reputation is further increased by fraud, which is increasingly becoming a cause of concern. To address these issues, we have designed an experiment where participants could spend money to have their observable cooperativeness increased. Our findings point to the importance of ensuring the truthfulness of reputation for a more cooperative and fair society.\n\nZeros of vibrational modes have been fascinating physicists for\n\nseveral centuries. Mathematical study of zeros of eigenfunctions goes\n\nback at least to Sturm, who showed that, in dimension d=1, the n-th\n\neigenfunction has n-1 zeros. Courant showed that in higher dimensions\n\nonly half of this is true, namely zero curves of the n-th eigenfunction of\n\nthe Laplace operator on a compact domain partition the domain into at\n\nmost n parts (which are called \"nodal domains\").\n\nIt recently transpired that the difference between this upper bound\n\nand the actual value can be interpreted as an index of instability of\n\na certain energy functional with respect to suitably chosen\n\nperturbations. We will discuss two examples of this phenomenon: (1)\n\nstability of the nodal partitions of a domain in R^d with respect to a\n\nperturbation of the partition boundaries and (2) stability of a graph\n\neigenvalue with respect to a perturbation by magnetic field. In both\n\ncases, the \"nodal defect\" of the eigenfunction coincides with the\n\nMorse index of the energy functional at the corresponding critical\n\npoint. We will also discuss some applications of the above results.\n\nBased on arXiv:1103.1423, CMP'12 (with R.Band, H.Raz, U.Smilansky),\n\narXiv:1107.3489, GAFA'12 (with P.Kuchment, U.Smilansky),\n\narXiv:1110.5373, APDE'13\n\narXiv:1212.4475, PTRSA'13 to appear (with T.Weyand),\n\narXiv:1503.07245, JMP'15 to appear (with R.Band and T.Weyand)\n\nMany physical systems can be described by particle models. The interaction between these particles is often modeled by forces, which typ- ically depend on the inter-particle distance, e.g., gravitational attraction in celestial me- chanics, Coulomb forces between charged par- ticles or swarming models of self-propelled par- ticles. In most physical systems Newtons third law of actio-reactio is valid. However, when considering a larger class of interacting par- ticle models, it might be crucial to introduce an asymmetry into the interaction terms, such that the forces not only depend on the dis- tance, but also on direction. Examples are found in pedestrian models, where pedestrians typically pay more attention to people in front than behind, or in traffic dynamics, where dri- vers on highways are assumed to adjust their speed according to the distance to the preced- ing car. Motivated by traffic and pedestrian models, it seems valuable to study particle sys- tems with asymmetric interaction where New- tons third law is invalid. Here general parti- cle models with symmetric and asymmetric re- pulsion are studied and investigated for finite- range and exponential interaction in straight corridors and annulus. In the symmetric case transitions from one-to multi-lane (zig-zag) be- havior including multi-stability are observed for varying particle density and for a varying curvature with fixed density. When the asym- metry of the interaction is taken into account a new “bubble”-like pattern arises when the dis- tance between lanes becomes spatially mod- ulated and changes periodically in time, i.e. peristaltic motion emerges. We find the tran- sition from the zig-zag state to the peristaltic state to be characterized by a Hopf bifurcation.\n\nAssessing systemic risk in financial markets and identifying systemically important financial institutions and assets is of great importance. In this talk I will consider two channels of propagation of financial systemic risk, (i) the common exposure to similar portfolios and fire sale spillovers and (ii) the liquidity cascades in the interbank networks. For each of them I will show how the use of statistical models of networks might be useful in systemic risk studies. In the first case, by applying the Maximum Entropy principle to the bipartite network of banks and assets, we propose a method to assess aggregated and single bank’s systemicness and vulnerability and to statistically test for a change in these variables when only the information on the size of each bank and the capitalization of the investment assets are available. In the second case, by inferring a stochastic block model from the e-MID interbank network, we show that the extraordinary ECB intervention during the sovereign debt crisis changed completely the large scale organization of such market and we identify the banks that, changing their strategy in response to the intervention, contributed most to the architectural network mutation.\n\nIn a good solvent, a polymer chain assumes an extended configuration. As the solvent quality (or the temperature) is lowered, the configuration changes to globular, which is more compact. This collapse transition is also called coil-globule transition in the literature. Since the pioneering work by de Gennes, it is known that it corresponds to a tricritical point in a grand-canonical parameter space. In the most used lattice model to study it, the chain is represented by a self-avoiding walk on and the solvent is effectively taken into account by including attractive interactions between monomers on first neighbor sites which are not consecutive along a chain (SASAW's: self-attracting self-avoiding walks). We will review the model and show that small changes in it may lead to different phase diagrams, where the collapse transition is no longer a tricritial point. In particular, if the polymer is represented by a trail, which allows for multiple visits of sites but mantains the constraint of single visits of edges, we find two distinct polymerized phases besides the non-polymerized phase and the collapse transition becomes a bicritical point.\n\nStructural and dynamical similarities of different real networks suggest that some universal laws might accurately describe the dynamics of these networks, albeit the nature and common origin of such laws remain elusive. Here we show that causal network representing the large-scale structure of spacetime in our accelerating universe is a power-law graph with strong clustering, similar to many complex networks such as the Internet, social or biological networks. We prove that this strcutural similarity is a consequence of the asymptotic equivalence between the large-scale growth dynamics of complex networks and causal networks. This equivalence suggests that unexpectedly similar laws govern the dynamics of complex networks and spacetime in the universe, with implications to network\n\nscience and cosmology. However, our simple frameworks is unable to explain the emergence of community structure, a property that, along with scale-free degree distributions and strong clustering, is commonly found in real complex networks. Here we show how latent network geometry coupled with preferential attachment of the nodes to this geometry fills this gap. We call this mechanism geometric preferential attachment (GPA) and validate it against the Internet. GPA gives rise to soft communities that provide a different perspective on the community structure in networks. The connections between GPA and cosmological models, including inflation, are also discussed.\n\nThe binary-state voter model describes a system of agents who adopt the opinions of their neighbours. The coevolving voter model (CVM, [1]) extends its scope by giving the agents the option to sever the link instead of adopting a contrarian opinion. The resulting simultaneous evolution of the network and the configuration leads to a fragmentation transition typical of such adaptive systems. The CVM was our starting point for investigating coevolution in the context of multilayer networks, work that IFISC was tasked with under the scope of the LASAGNE Initiative. In this talk I will briefly review some of the outcomes and follow-up works. First we will see how coupling together of two CVM networks modifies the transitions and results in a new type of fragmentation [2]. I will then identify the latter with the behaviour of the single-network CVM with select nodes constantly under the stress of noise [3]. Finally, I will relate our attempts to reproduce the effect of multiplexing on the voter model by studying behaviour of the standard aggregates; the negative outcome of which gives validity to considering the multiplex as a fundamentally novel, non-reducible structure [4].\n\n[1] F. Vazquez, M. San Miguel and V. M. Eguiluz, Generic Absorbing Transition in Coevolution Dynamics, Physical Review Letters, 100, 108702 (2008)\n\n[2] MD, M. San Miguel and V. E. Eguiluz, Absorbing and Shattered Fragmentation Transitions in Multilayer Coevolution, Physical Review E, 89, 062818, (2014)\n\n[3] MD, V. M. Eguiluz and M. San Miguel, Noise in Coevolving Networks, Physical Review E, 92, 032803, (2015)\n\n[4] MD, V. Nicosia, V. Latora and M. San Miguel, Irreducibility of Multilayer Network Dynamics: the Case of the Voter Model,arXiv:1507.08940 (2015)\n\nRectification of work in non-equilibrium conditions has been one of the important topic of non-equilibrium statistical mechanics. Within the framework of equilibrium thermodynamics, it is well known that the works can be rectified from two thermal equilibrium baths. We address the question that how can we rectify work from Brownian object (piston) attached to multiple environments, including non-equilibrium baths? We focus on adiabatic piston problem under nonlinear friction, where the piston with sliding friction separates two gases of the same pressure, but different temperatures. Without sliding friction, the direction of piston motion is known to be determined from the difference of temperature of two gases [1,2]. However, if sliding friction exists, we report that the direction of motion depends on the amplitude of the friction, and nonlinearity of the friction [3]. If time allows, we also report the possibility of application to the problem of fluctuating heat engine, where the temperature of gas is changed, in a cyclic manner [4].\n\n[1] E. H. Lieb, Physica A 263 491 (1999).\n\n[2] Ch. Gruber and J. Piasecki, Physica A 268 412 (1999). A. Fruleux, R. Kawai and K. Sekimoto, Phys. Rev. Lett. 108 160601 (2012).\n\n[3] T. G. Sano and H. Hayakawa, Phys. Rev. E 89 032104 (2014).\n\n[4] T. G. Sano and H. Hayakawa, arXiv:1412.4468 (2014).\n\nFluctuation in small systems has attracted wide interest because of the recent experimental development in biological, colloidal, and electrical systems. As accurate data on fluctuation have become accessible, the importance of mathematical modeling of fluctuation’s dynamics has been increasing. One of the minimal models for such systems is the Langevin equation, which is a simple model composed of the viscous friction and the white Gaussian noise. The validity of the Langevin model has been shown in terms of some microscopic theories [1], and this model has been used not only theoretically but also experimentally in describing thermal fluctuation.\n\nOn the other hand, non-Gaussian properties of fluctuation are reported to emerge in athermal systems, such as biological, granular, and electrical systems. A natural question then would arises: When and how does the non-Gaussian fluctuation emerge for athermal systems? In this seminar, we present a systematic method to derive a Langevin-like equation driven by non-Gaussian noise for a wide class of stochastic athermal systems, starting from master equations and developing an asymptotic expansion [2, 3]. We found an explicit condition whereby the non-Gaussian properties of the athermal noise become dominant for tracer particles associated with both thermal and athermal environments. We also derive an inverse formula to infer microscopic properties of the athermal bath from the statistics of the tracer particle. Furthermore, we obtain the full-order asymptotic formula of the steady distribution function for an arbitrary strong non-linear friction, and show that the first-order approximation corresponds to the independent kick model [4]. We apply our formulation to a granular motor under viscous and Coulombic frictions, and analytically obtain the angular velocity distribution functions. Our theory demonstrates that the non-Gaussian Langevin equation is a minimal model of athermal systems.\n\n[1] N.G. van Kampen, Stochastic Processes in Physics and Chemistry, North-Holland (2007).\n\n[2] K. Kanazawa, T.G. Sano, T. Sagawa, and H. Hayakawa, Phys. Rev. Lett. 114, 090601 (2015).\n\n[3] K. Kanazawa, T.G. Sano, T. Sagawa, and H. Hayakawa, J. Stat. Phys. 160, 1294 (2015).\n\n[4] J. Talbot, R.D. Wildman, and P. Viot, Phys. Rev. Lett. 107, 138001 (2011).\n\nThe synaptic inputs arriving in the cortex are under many circumstances\n\nhighly variable. As a consequence, the spiking activity of cortical\n\nneurons is strongly irregular such that the coefficient of variation of\n\nthe inter-spike interval distribution of individual neurons is\n\napproximately Poisson-like. To model this activity, balanced networks\n\nhave been put forward where a coordination between excitatory and strong\n\ninhibitory input currents, which nearly cancel in individual neurons,\n\ngives rise to this irregular spiking activity. However, balanced\n\nnetworks of excitatory and inhibitory neurons are characterized by a\n\nstrictly linear relation between stimulus strength and network firing\n\nrate. This linearity makes it hard to perform more complex computational\n\ntasks like the generation of receptive fields, multiple stable activity\n\nstates or normalization, which have been measured in many sensory\n\ncortices. Synapses displaying activity dependent short-term plasticity\n\n(STP) have been previously reported to give rise to a non-linear network\n\nresponse with potentially multiple stable states for a given stimulus.\n\nIn this seminar, I will discuss our recent analytical and numerical\n\nanalysis of computational properties of balanced networks which\n\nincorporate short-term plasticity. We demonstrate stimuli are normalized\n\nby the network and that increasing the stimulus to one sub-network,\n\nsuppresses the activity in the neighboring population. Thereby,\n\nnormalization and suppression are linear in stimulus strength when STP\n\nis disabled and become non-linear with activity dependent synapses.\n\nThe densest way to pack objects in space, also known as the packing problem, has intrigued scientists and philosophers for millenia. Today, packing comes up in various systems over many length scales from batteries and catalysts to the self-assembly of nanoparticles, colloids and biomolecules. Despite the fact that so many systems' properties depend on the packing of differently-shaped components, we still have no general understanding of how packing varies as a function of particle shape. Here, we carry out an exhaustive study of how packing depends on shape by investigating the packings of over 55,000 polyhedra. By combining simulations and analytic calculations, we study families of polyhedra interpolating between Platonic and Archimedean solids such as the tetrahedron, the cube, and the octahedron. Our resulting density surface plots can be used to guide experiments that utilize shape and packing in the same way that phase diagrams are essential to do chemistry. The properties of particle shape indeed are revealing why we can assemble certain crystals, transition between different ones, or get stuck in kinetic traps.\n\nLinks: http://journals.aps.org/prx/abstract/10.1103/PhysRevX.4.011024(link is external),\n\nhttp://www.newscientist.com/article/dn25163-angry-alien-in-packing-puzzl...(link is external),\n\nhttp://physicsworld.com/cws/article/news/2014/mar/03/finding-better-ways...(link is external),\n\nhttp://physics.aps.org/synopsis-for/10.1103/PhysRevX.4.011024(link is external)\n\nMy website: http://www-personal.umich.edu/~dklotsa/Daphne_Klotsas_Homepage/Home.html\n\nI will discuss methods for spatio-temporal modelling in molecular,\n\ncell and population biology. Three classes of models will be considered:\n\n(i) microscopic (individual-based) models (molecular dynamics,\n\nBrownian dynamics) which are based on the simulation of\n\ntrajectories of molecules (or individuals) and their localized\n\ninteractions (for example, reactions);\n\n(ii) mesoscopic (lattice-based) models which divide the computational\n\ndomain into a finite number of compartments and simulate the time\n\nevolution of the numbers of molecules (numbers of individuals)\n\nin each compartment; and\n\n(iii) macroscopic (deterministic) models which are written in terms\n\nof mean-field reaction-diffusion-advection partial differential\n\nequations (PDEs) for spatially varying concentrations.\n\nIn the first part of my talk, I will discuss connections between the\n\nmodelling frameworks (i)-(iii). I will consider chemical reactions both at\n\na surface and in the bulk. In the second part of my talk, I will present\n\nhybrid (multiscale) algorithms which use models with a different level\n\nof detail in different parts of the computational domain.\n\nThe main goal of this multiscale methodology is to use a detailed\n\nmodelling approach in localized regions of particular interest\n\n(in which accuracy and microscopic detail is important) and a less\n\ndetailed model in other regions in which accuracy may be traded\n\nfor simulation efficiency. I will also discuss hybrid modelling\n\nof chemotaxis where an individual-based model of cells is coupled\n\nwith PDEs for extracellular chemical signals.\n\nEmpirical evidence suggesting that living systems might operate in the vicinity of critical points, at the borderline between order and disorder, has proliferated in recent years, with examples ranging from spontaneous brain activity, to the dynamic of gene expression or to flock dynamics. However, a well-founded theory for understanding how and why living systems tune themselves to be poised in the vicinity of a critical point is lacking. In this talk I will review the concept of criticality with its associated scale invariance and power-law distributions. I will discuss mechanisms by which inanimate systems may self-tune to critical points and compare such phenomenology with what observed in living systems. I will also introduce the concept of Griffiths phase --an old acquaintance from the physics of disordered systems-- and show how it can be very naturally related to criticality in living structures such as the brain. In particular, taking into account the complex hierarchical-modular architecture of cortical networks, the usual singular critical pointin the dynamics of neural activity propagation is replaced by an extended critical-like region with a fascinating dynamics which might justify the trade-off between segregation and integration, needed to achieve complex cognitive functions.\n\nContemporary finance is characterized by a complex pattern of relations between financial institutions that can be conveniently modeled in terms of networks.\n\nIn stable market conditions, connections allow banks to diversify their investments and reduce their individual risk. The same networked structure may, however, become a source of contagion and stress amplification when some banks go bankrupt.\n\nWe consider a network model of financial contagion due to the combination of overlapping portfolios and market-impact, and we show how it can be understood in terms of a generalized branching process. We estimate the circumstances under which systemic instabilities are likely to occur as a function of parameters such as leverage, market crowding and diversification.\n\nThe analysis shows that the probability of observing global cascades of bankruptcies is a non-monotonic function of both the average diversification of financial institutions, and that there is a critical threshold for leverage below which the system is stable. Moreover the system exhibits \"robust yet fragile'' behavior, with regions of the parameter space where contagion is rare but catastrophic whenever it occurs.\n\nI will discuss the mean field kinetics of irreversible coagulation in\n\nthe presence of a source of monomers and a sink at large cluster sizes\n\nwhich removes large particles from the system. These kinetics are\n\ndescribed by the Smoluchowski coagulation equation supplemented with\n\nsource and sink terms. In common with many driven dissipative systems with\n\nconservative interactions, one expects this system to reach a stationary\n\nstate at large times characterised by a constant flux of mass in the\n\nspace of cluster sizes from the small-scale source to the large scale sink.\n\nWhile this is indeed the case for many systems, I will present here a\n\nclass of systems in which this stationary state is dynamically unstable.\n\nThe consequence of this instability is that the long-time kinetics are\n\noscillatory in time. This oscillatory behaviour is caused by the fact that\n\nmass is transferred through the system in pulses rather than via a stationary\n\ncurrent in such a way that the mass flux is constant on average. The\n\nimplications of this unusual behaviour the non-equilibrium kinetics of\n\nother systems will be discussed.\n\nWhen driven out of equilibrium by a temperature gradient, fluids respond by developing a nontrivial, inhomogeneous structure according to the governing macroscopic laws. Here we show that such structure obeys strikingly simple universal scaling laws arbitrarily far from equilibrium, provided that both macroscopic local equilibrium (LE) and Fourier’s law hold. These results, that we prove for hard sphere fluids and more generally for systems with homogeneous potentials in arbitrary dimension, are likely to remain valid in the much broader family of strongly correlating fluids where excluded volume interactions are dominant. Extensive simulations of hard disk fluids confirm the universal scaling laws even under strong temperature gradients, suggesting that Fourier’s law remains valid in this highly nonlinear regime, with the expected corrections absorbed into a non-linear conductivity functional. Our results also show that macroscopic LE is a very strong property, allowing us to measure the hard disks equation of state in simulations far from equilibrium with a surprising accuracy comparable to the best equilibrium simulations. Subtle corrections to LE are found in the fluctuations of the total energy which strongly point out to the non-locality of the nonequilibrium potential governing the fluid’s macroscopic behavior out of equilibrium. Finally, our simulations show that both LE and the universal scaling laws are robust in the presence of strong finite-size effects, via a bulk-boundary decoupling mechanism by which all sorts of spurious finite-size and boundary corrections sum up to renormalize the effective boundary conditions imposed on the bulk fluid, which behaves macroscopically.\n\nIn the past few years, multilayer, interdependent and multiplex networks have quickly become a big avenue in mathematical modelling of networked complex systems, with applications in social sciences, large-scale infrastructures, information and communications technology, neuroscience, etc. In particular, it has been shown that such networks can describe the resilience of large coupled infrastructures (power grids, Internet, water systems, …) to failures, by studying percolation properties under random damage.\n\nPercolation is perhaps the simplest model of network resilience and can be defined or extended to multiplex networks (defined as a network with multiple edge types) in many different ways. In some cases, new analytical approaches must be introduced to include features that are intrinsic to multiplex networks. In other cases, extensions of classical models give origin to new critical phenomena and complex behaviours.\n\nRegarding the first case, I will illustrate a new theoretical approach to include edge overlap in a simple percolation\n\nmodel. Edge overlap, i.e. node pairs connected on different layers, is a feature common to many empirical cases,\n\nsuch as in transportation networks, social networks and epidemiology. Our findings illustrate properties of\n\nmultiplex resilience to random damage and may give assistance in the design of large-scale infrastructure.\n\nRegarding the second aspect, I will present models of pruning and bootstrap percolation in multiplex networks. Bootstrap may be seen as a simple activation process and has applications in many areas of science. Our extension to multiplex networks can be solved analytically, has potential applications in network security, and provides a step in dealing with dynamical processes occurring on the network.\n\nIn April 2010 I gave a seminar at the Santa Fe Institute where I demonstrated that certain classic problems in economics can be resolved by re-visiting basic tenets of the formalism of decision theory. Specifically, I noted that simple mathematical models of economic processes, such as the random walk or geometric Brownian motion, are non-ergodic. Because of the non-stationarity of the processes, observables cannot be assumed to be ergodic, and this leads to a difference in important cases between time averages and ensemble averages. In the context of decision theory, the former tend to indicate how an individual will fare over time, while the latter may apply to collectives but are a priori meaningless for individuals. The effects of replacing expectation values by time averages are staggering -- realistic predictions for risk aversion, market stability, and economic inequality follow directly. This observation led to a discourse with Murray Gell-Mann and Kenneth Arrow about the history and development of decision theory, where the first studies of stochastic systems were carried out in the 17th century, and its relation to the development of statistical mechanics where refined concepts were introduced in the 19th century. I will summarize this discourse and present my current understanding of the problems.\n\nCultural change is often quantified by changes in frequency of cultural traits over time. Based on those (observable) frequency patterns researchers aim to infer the nature of the underlying evolutionary processes and therefore to identify the (unobservable) causes of cultural change. Especially in archaeological and anthropological applications this inverse problem gains particular importance as occurrence or usage frequencies are often the only available information about past cultural traits or traditions and the forces affecting them. In this talk we start analyzing the described inference problem and discuss it in the context of the question of which learning strategies human populations should deploy to be well-adapted to changing environmental conditions. To do so we develop a mathematical framework which establishes a causal relationship between changes in frequency of different cultural traits and the considered underlying evolutionary processes (in our case learning strategies). Besides gaining theoretical insights into the question of which learning strategies lead to efficient adaptation processes in changing environments we focus on ‘reverse engineering’ conclusions about the learning strategies deployed in current or past population, given knowledge of the frequency change dynamic over space and time. Using appropriate statistical techniques we investigate under which conditions population-level characteristics such as frequency distributions of cultural variants carry a signature of the underlying evolutionary processes and if this is the case how much information can be inferred from it. Importantly, we do not expect the existence of a unique relationship between observed frequency data and underlying evolutionary processes; to the contrary, we suspect that different processes can produce similar frequency pattern. However, our approach might help narrow down the range of possible processes that could have produced those observed frequency patterns, and thus still be instructive in the face of uncertainty. Rather than identifying a single evolutionary process that explains the data, we focus on excluding processes that cannot have produced the observed changes in frequencies. In the last part of the talk, we demonstrate the applicability of the developed framework to anthropological case studies.\n\nAn analytical solution for a network growth model of intrinsic vertex fitness is presented, along\n\nwith a proposal to a new paradigm in fitness based network growth models. This class of models\n\nis classically characterised by a fitness linking mechanism that governs the attachment rate of new\n\nlinks to existing nodes and a distribution of node fitness, that measures the attractiveness of a node.\n\nIt is argued in the present paper, that this distinction is unnecessary, instead linking prope"
    }
}
{
    "id": "dbpedia_2449_1",
    "rank": 85,
    "data": {
        "url": "https://blog.computationalcomplexity.org/2019/",
        "read_more_link": "",
        "language": "en",
        "title": "Computational Complexity",
        "top_image": "https://blog.computationalcomplexity.org/favicon.ico",
        "meta_img": "https://blog.computationalcomplexity.org/favicon.ico",
        "images": [
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://pup-assets.imgix.net/onix/images/9780691175782.jpg?w=640",
            "https://i.creativecommons.org/l/by-nc/4.0/88x31.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Lance Fortnow"
        ],
        "publish_date": null,
        "summary": "",
        "meta_description": "Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch",
        "meta_lang": "en",
        "meta_favicon": "https://blog.computationalcomplexity.org/favicon.ico",
        "meta_site_name": "",
        "canonical_link": "https://blog.computationalcomplexity.org/2019/",
        "text": "Why is there no all-encompassing term for a course on Models of Computation?\n\nIn my last blog post I asked my readers to leave comments saying what the name of the course that has some of Regular Languages, Context Free Languages Decideability, P, NP (any maybe other stuff) in it. I suspected there would be many different names and their were. I was able to put all but 6 into 4 equivalence classes. So that's 10 names. Thats a lot especially compared to\n\n(Introduction to) Algorithms\n\nand\n\n(Introduction to) Cryptography\n\nwhich I suspect have far fewer names. One commenter pointed out that the reason for the many different names is that there are many versions of the course. That's not quite an explanation since there are also many different versions of Cryptography---at UMCP crypto is cross listed in THREE departments (CS, Math, EE) and its taught by 6 or so different people who don't talk to each other (I am one of them). I think Algorithms is more uniform across colleges.\n\nI think that terms Algorithms and Cryptography are both rather broad and can accommodate many versions of the courses, whereas no term seems to be agreed upon to encompass the DFA etc course.\n\nEven saying DFA etc is not quite right since some of the courses spend little or even no time on DFA's.\n\nBelow is a list of all the names I got and some comments. Note that some schools appear twice since they have two courses along these lines.\n\n-----------------------------------------\n\nTITLE: (Introduction to) Theory of Computation:\n\nSwarthmore: Theory of Computation\n\nUCSD: Theory of Computation\n\nSaint Michaels: Theory of Computation\n\nUniv of Washington: Introduction to the Theory of Computation\n\nWaterloo: Introduction to the Theory of Computing\n\nCOMMENT: Theory of Computation could have been the term that encompasses all of these courses. I speculate that it didn't catch on since it sounds too much like computability theory which is only one part of the course.\n\n------------------------\n\nTITLE: Formal Languages and XXX\n\nCMU: Formal Languages, Automata, and Computability\n\nFlorida Tech: Formal Languages and Automata Theory\n\nUC-Irvine: Formal Languages and Automata Theory\n\nUniv of Chicago: Introduction to Formal Languages\n\nUniversity of Bucharest: Formal Language and Automata\n\nTU Darmstadt: Formal Foundations of CS I: Automata, Formal Languages, and Decidability\n\nTUK Germany: Formal Languages and Computability\n\nCOMMENT: The title makes it sound like they don't cover P and NP. I do not know if thats true; however, I speculate that, it could never be the encompassing term.\n\nSpell Check things Automata and Computability are not words, but I've googled them and they seem to be words.\n\n--------------------------\n\nTITLE: Computability/Decidability and Complexity/Intractability\n\nReed College: Computability and Complexity\n\nCaltech: Decidability and Intractability\n\nCOMMENT: The title makes it sound like they don't cover regular or context free languages. I do not know if that's true; however, I speculate that, since the terms sound that way, they never caught on as the general term.\n\nSpellecheck thinks that neither Decidability nor Decideability is a word. Google seems to say that I should leave out the e, so I will.\n\n------------------------------\n\nTITLE: Blah MODELS Blah\n\nTel-Aviv (a long time ago) Computational Models\n\nUIUC: Algorithms and Models of Computation (also has some algorithms in it)\n\nWaterloo: Models of Computation (enriched version)\n\nCOMMENT: Models of Computation sounds like a good name for the course! Too bad it didn't catch on. It would also be able to withstand changes in the content like more on parallelism or more on communication complexity.\n\n------------------------------\n\nTITLE: MISC\n\nCMU: Great Ideas in Theoretical Computer Science\n\nUCLouvain (Belgium) Calculabilite (Computability)\n\nMoscow Inst. of Phy. and Tech.: Mathematical logic and Theory of Algorithms\n\nPortland State University: Computational Structures\n\nGermany: Informatik III (Not all of Germany)\n\nUniv of Chicago: Introduction to Complexity\n\nCOMMENT: All of these terms are to narrow to have served as a general term.\n\nA non-moral dilemma about cheating, but it brings up some points\n\nI often give two versions of an exam and TELL THE STUDENTS I am doing this so that they don't even try to cheat. I've even had two different classes take the midterm at the same time, same room, every other seat, so the person next to you is in a different course. And I TELL THE STUDENTS that I am doing this. A colleague of mine says I shouldn't TELL THE STUDENTS. Here are our arguments\n\n1) Don't tell: students cheat a lot and this is a way to catch them.\n\n2) Tell: Dealing with cheating distracts from our mission of teaching so best to be preventative so it does not happen. Less noble- tell them so that you don't have to deal with the cheating issue.\n\nI have heard of the following case at a diff school some years ago and want your take on it:\n\nthere was one question on the midterm that was different on the two exams- the prof changed the key number, but they were the same question really. The prof was in a hurry for some reason and FORGOT TO TELL THE STUDENTS. You can probably guess what happened next, but not what happened after that\n\nOne of the students exams had the solution to THE OTHER PROBLEM on it. Clearly cheating. When called in the student said:\n\nSince you didn't tell us that they were different exams the cheating claim is unfair!\n\nThey DID admit their guilt, but they DID NOT have any contrition.\n\nOptions for what penalty to go for:\n\n1) A 0 on the exam itself\n\n2) An F in the course\n\n3) A notation on the transcript indicating Failed-because-cheated. I don't know what that notation was at the schol the story took place, but at UMCP its XF. (Side Note- not clear if someone outside of UMCP looks at a transcript and sees an XF they'll know what the means. But the F part makes it look bad.)\n\n4) Expulsion from school. (This might not be the profs call- this may depend on if its a first offense.)\n\nThe lack of contrition bothers me, though the prof who told me the story said that the student may have said it out of shock- the first thing that came into their mind. I asked the prof how the student was doing in the class and the prof said, CORRECTLY, that that is irrelevant.\n\nSO- what penalty would you go for?\n\nThe professor went for XF. The student, at the hearing, once again said\n\nSince you didn't tell us that they were different exams the cheating claim is unfair!\n\nThe professor told me that he thinks the student was trying to claim it was entrapment, though he had a hard time expressing this coherently. If the student had been a coherent thinker, he probably wouldn't have needed to cheat.\n\nHe got the equivalent of an XF.\n\nBut here is my real question: Should we TELL THE STUDENTS that they are different exams (I think yes) or\n\nshould we NOT tell them so can catch them?\n\nRandom non-partisan thoughts on the Prez Election\n\nThis post is non-partisan, but in the interest of full disclosure I disclose that I will almost surely be voting for the Democratic Nominee. And I say almost surely because very weird things could happen.I can imagine a republican saying, in 2015 I will almost surely be voting for the Republican Nominee and then later deciding to not vote for Trump.\n\nMy Past Predictions: Early on in 2007 I predicted it would be Obama vs McCain and that Obama would win. Was I smart or lucky? Early in 2011 I predicted Paul Ryan would be the Rep. Candidate. Early in 2015 and even into 2016 I predicted that Trump would not get the nomination. After he got the nomination I predicted he would not become president. So, in answer to my first question, I was lucky not smart. Having said all of this I predict that the Dem. candidate will be Warren. Note- this is an honest prediction, not one fueled by what I want to see happen. I predict Warren since she seems to be someone who can bridge the so-called establishment and the so-called left (I dislike the terms LEFT and RIGHT since issues and views change over time). Given my past record I would not take me too seriously. Also, since this prediction is not particularly unusual, if I am right this would NOT be impressive (My Obama prediction was impressive, and my Paul Ryan prediction would have been very impressive had I been right.)\n\nElectability: My spell checker doesn't think its a word. Actually it shouldn't be a word. It's a stupid concept. Recall\n\nJFK was unelectable since he was Catholic.\n\nRonald Reagan was unelectable because he was too conservative.\n\nA draft dodging adulterer named Bill Clinton could not possible beat a sitting president who just won a popular war.\n\nNobody named Barack Hussein Obama, who is half-black, could possibly get the nomination, never mind the presidency. And Hillary had the nomination locked up in 2008--- she had no any serious challengers.\n\n(An article in The New Republic in 2007 predicted a brokered convention for the Republicans where Fred Thompson, Mitt Romney, and Rudy Guilliani would split the vote, and at the same time a cake walk for Hillary Clinton with\n\nBarak Obama winning Illinois in the primaries but not much else. Recall that 2008 was McCain vs Obama.)\n\nDonald Trump will surely be stopped from getting the nomination because, in the end, The Party Decides.\n\nRepublican voters in 2016 will prefer Rubio to Trump since Marco is more electable AND more conservative. Hence, in the space of Rep. Candidates, Rubio dominates Trump. So, by simple game theory, Trump can't get the nomination. The more electable Rubio, in the 2016 primaries, won Minnesota, Wash DC, and Puerto Rico (Puerto Rico has a primary. Really!) One of my friends thought he also won Guam (Guam?) but I could not find evidence of that on the web. Okay, so why did Trump win? Because voters are not game theorists.\n\nANYWAY, my point is that how can anyone take the notion of electability seriously when unelectable people have gotten elected?\n\nPrimaries: Dem primary voters are torn between who they want to be president and who can beat Trump. Since its so hard to tell who can beat who, I would recommend voting for who you like and not say stupid things like\n\nAmerican would never elect a 76 year old socialist whose recently had a heart attack.\n\nor\n\nTrump beat a women in 2016 so we can't nominate a women\n\nor\n\nAmerica is not ready to elect a gay president yet. (America is never ready to do X until after it does X and then the pundits ret-con their opinions.For example, of course America is ready for Gay-Marriage. Duh.)\n\nWho won the debate?\n\nWhoever didn't bother watching it :-). I think the question is stupid and has become who got out a clever sound bite. We need sound policy, not sound bites!\n\nThe Sheldon Conjecture (too late for Problems with a Point)\n\nChapter 5 of Problems with a Point (by Gasarch and Kruskal) is about how mathematical objects get their names. If it was an e-book that I could edit and add to (is this a good idea or not? later on that) then I would have added the following.\n\nThe Sheldon Conjecture\n\nBackground: Nobel Laureate Sheldon Cooper has said that 73 is the best number because\n\na) 73 is prime.\n\nb) 73 is the 21st prime and note that 7*3=21.\n\nc) The mirror of 73, namely 37, is prime.\n\nd) 37 is the 12th prime, and 12 is the mirror of 21.\n\nSheldon never quite said its the only such number; that was conjectured by Jessie Byrnes, Chris Spicer, and Alyssa Turnquist here. They called it Sheldon's Conjecture probably since Sheldon Cooper should have conjectured it\n\nWhy didn't Sheldon make Sheldon's conjecture? This kind of question has been asked before:\n\nCould Euler have conjectured the prime number theorem\n\nWhy didn't Hilbert (or others) pursue Ramsey Theory?\n\n(readers are encouraged to give other examples)\n\nI doubt we'll be asking this about Sheldon Cooper since he is a fictional character.\n\nI am delighted that\n\na) There is a Sheldon's Conjecture.\n\nb) It has been solved by Pomerance and Spicer, see here\n\nActually (b) might not be so delightful--- once a conjecture is proven its stops being called by the name of the conjecturer. If you don't believe me just ask Vazsonyi or Baudet. If you don't know who they are then (1) see here and (2) that proves my point. So perhaps I wish it had not been solved so The Sheldon Conjecture would live on as a name.\n\nAnother issue this brings up: Lets say that Problems with a Point was an online book that I was able to edit easily. Then I might add material on The Sheldon Conjecture. And while I am at it, I would add The Monty Hall Paradox to the chapter on how theorems get there names. Plus, I would fix some typos and references. Perhaps update some reference. Now lets say that all books were online and the authors could modify them. Would this be good or bad?\n\n1) Good- The book would get better and better as errors got removed.\n\n2) Good- The book would get to include material that is appropriate but came out after it was published.\n\n3) Good- The book would get to include material that is appropriate but the authors forgot to include the first time around.\n\n4) Bad- For referencing the book or for book reviews of the book, you are looking at different objects. The current system has First Edition, Second Edition, etc, so you can point to which one you are looking at. The easily-edited books would have more of a continuous update process so harder to point to things.\n\n5) Bad- When Clyde and I emailed the final version to the publisher we were almost done. When we got the galleys and commented on them we were DONE DONE! For typos and errors maybe I want to fix them online, but entire new sections--- when we're done we are DONE.\n\n6) Bad- at what point is it (i) a corrected version of the old book, (ii) a new edition of the old book, (iii) an entirely new book? Life is complicated enough.\n\nI would prob like a system where you can fix errors but can't add new material. Not sure if that's really a clean distinction.\n\nWilliam Kruskal's 100th birthday\n\nToday, Oct 10, 2019 is William Kruskal's 100th birthday (he's dead, so no cake. Oh well.) William Kruskal was a great statistician. To honor him we have a guest post by his nephew Clyde Kruskal. We also note that the Kruskal Family is one of the top two math families of all time (see here). William is a reason why the other two Kruskal brothers went into mathematics: As a much older sibling (6 years older than Martin and 8 years older than Joseph), he encouraged their early mathematical development.\n\nHere are some pictures of William Kruskal and of the Kruskal Family: here\n\nGuest Post by Clyde Kruskal\n\nI was asked to blog about my uncle, the statistician, William H. Kruskal, on the centennial of his birth. We called him Uncle Bill. He is best known for co-inventing the Kruskal-Wallis test.\n\nThere are two stories that I know about Bill's childhood, which must have been family lore:\n\n(1) As a young child, Bill was a prolific reader. His reading comprehension outstripped his conversational English. One morning, having just read the word ``schedule'' in a book, and obviously having never heard it pronounced, he sat down to breakfast and asked:\n\n\"What is the ske·DU·le for today?\"\n\n(2) My grandparents once had Bill take an occupational assessment test. The tester said that Bill was a very bright child, and should become a traffic engineer to solve the problems with traffic congestion. (This would have been the 1920s!) As you probably know, Uncle Bill did not succeed in ending traffic congestion. Oh well.\n\nRecently there has been a controversy over whether to ask about citizenship in the 2020 census. In the late 1900s there was a different controversy: whether to adjust the known undercount statistically. In general, Democrats wanted to adjust the count and Republicans did not (presumably because Democratic states tended to have a larger undercount). A national committee was set up to study the issue, with four statisticians in favor and four against. I was surprised to learn that Uncle Bill was on the commission as one of those against adjustment, since, I thought his political views were more closely aligned with those of the Democrats. He was very principled, basing his views only on statistical arguments. I saw him give a talk on the subject, which seemed quite convincing (but, then again, I did not see the other side). They ended up not adjusting.\n\nFor more on William Kruskal, in general, and his view on adjusting the census, in particular, see the pointers at the end of this post.\n\nI have more to say. I just hope that I am on the ske·DU·le to blog about Uncle Bill at the bicentennial of his birth.\n\nThe William Kruskal Legacy: 1919-2005 by Fienberg, Stigler, and Tanur\n\nA short biography of William Kruskal by J.J. O'Connor and E.F. Robertson\n\nWilliam Kruskal: Mentor and Friend by Judith Tanur\n\nWilliam Kruskal: My Scholarly and Scientific Model by Stephen Fienberg\n\nA conversation with William Kruskal by Sandy Zabell\n\nTestimony for house subcommittee on census and population for 1990 (see page 140)\n\nWhat comes first theory or practice? Its Complicated!\n\nHaving majored in pure math I had the impression that usually the theory comes first and then someone works out something to work in practice. While this is true sometimes it is often NOT true and this will not surprise any of my blog readers. Even so, I want to tell you about some times it surprised me. This says more about my ignorance than about math or applications or whatnot.\n\n1) Quantum\n\na) Factoring was proven to be in BQP way before actual quantum computers could do this in reasonable time (we're still waiting).\n\nb) Quantum Crypto- This really is out there. I do not know what came first, the theory or the practice. Or if they were in tandem.\n\nc) (this one is the inspiration for the post) When I first heard the term Quantum Supremacy I thought it meant the desire for a THEOREM that problem A is in BQP but is provably not in P. For example, if someone proved factoring is not in P (unlikely this will be proven, and hey- maybe factoring is in P). Perhaps some contrived problem like those constructed by diagonalization (my spell checker thinks that's not a word. Having worked in computability theory, I think it is. Darn- my spellchecker thinks computability is not word.) Hence when I heard that Google had a paper proving Quantum Supremacy (I do not recall if I actually heard the word proven) I assumed that there was some theoretical breakthrough. I was surprised and not in the slightest disappointed to find out it involved actual quantum computers.\n\nQuestion: When the term Quantum Supremacy was first coined, did they mean theoretical, or IRL, or both?\n\n2) Ramsey Theory\n\na) For Ramsey's Theorem and Van Der waerden's theorem and Rado's theorem and others I could name, first a theorem showed a upper bound on a number, then later computers and perhaps some math got better bounds on that number.\n\nb) Consider the following statement:\n\nFor all c there exists P such that for all c-colorings of {1,...,P} there exists x,y,z the same color such that x2 +y2 = z2.\n\nRonald Graham conjectured the c=2 case and offered $100 in the 1980's. (I do not know if he had any comment on the general case.) I assumed that it would be proven with ginormous bounds on the P(c) function, and then perhaps some reasonable bound would be found by clever programming and some math. (see here for the Wikipedia Entry about the problem, which also has pointers to other material).\n\nInstead the c=2 case was proven with an exact bound, P(2)=7825, by a computer program, in 2016. The proof is 200 terabytes. So my prediction was incorrect.\n\nAs for the result\n\nPRO: We know the result is true for c=2 and we even know the exact bound. Wow! and for Ramsey Theory its unusual to have exact bounds!\n\nCON: It would be good to have a human-readable proof. This is NOT an anti-technology statement. For one thing, a human-readable proof might help us get the result for c=3 and beyond.\n\n3) This item is a cheat in that I knew the empirical results first. However, I will tell you what I am sure I would have thought (and been wrong) had I not know them.\n\nGiven k, does the equation\n\nx3 +y3 +z3 = k\n\nhave a solution in Z? I would have thought that some hard number theory would determine\n\nfor which k it has a solution (with a proof that does not give the actual solutions) and for then a computer programs would try to find the solutions. Instead (1) some values of k are ruled out by simple mod considerations, and (2) as for the rest, computers have found solutions for some of them. Lipton-Regan (here) and Gasarch (here) have blogged about the k=33 case. Lipton-Regan also comment on the more recent k=42 case.\n\nQuantum Supremacy: A Guest Post by Abhinav Deshpande\n\nI am delighted to introduce you to Abhinav Deshpande, who is a graduate student at the University of Maryland, studying Quantum Computing. This will be a guest post on the rumors of the recent Google breakthrough on Quantum Supremacy. For other blog posts on this exciting rumor, see Scott Aaronson's post, Scott Aaronson's second post on it, John Preskill's quanta article, Fortnow's post,\n\nand there may be others.\n\nGuest post by Abhinav:\n\nI (Abhinav) thank Bill Fefferman for help with this post, and Bill Gasarch for inviting me to do a guest post.\n\nThe quest towards quantum computational supremacy\n\nSeptember saw some huge news in the area of quantum computing, with rumours that the Google AI Lab has achieved a milestone known as 'quantum computational supremacy', also termed 'quantum supremacy' or 'quantum advantage' by some authors. Today, we examine what this term means, the most promising approach towards achieving this milestone, and the best complexity-theoretic evidence we have so far against classical simulability of quantum mechanics. We will not be commenting on details of the purported paper since there is no official announcement or claim from the authors so far.\n\nWhat it means\n\nFirst off, the field of quantum computational supremacy arose from trying to formally understand the differences in the power of classical and quantum computers. A complexity theorist would view this goal as trying to give evidence to separate the complexity classes BPP and BQP. However, it turns out that one can gain more traction from considering the sampling analogues of these classes, SampBPP and SampBQP. These are classes of distributions that can be efficiently sampled on classical and quantum computers, respectively. Given a quantum circuit U on n qubits, one may define an associated probability distribution over 2^n outcomes as follows: apply U to the fiducial initial state |000...0> and measure the resulting state in the computational basis. This produces a distribution D_U.\n\nA suitable way to define the task of simulating the quantum circuit is as follows:\n\nInput: Description of a quantum circuit U acting on n qubits.\n\nOutput: A sample from the probability distribution D_U obtained by measuring U|000...0> in the computational basis.\n\nOne of the early works in this field was that of Terhal and DiVincenzo, which first considered the complexity of sampling from a distribution (weak simulation) as opposed to that of calculating the exact probability of a certain outcome (strong simulation). Weak simulation is arguably the more natural notion of simulating a quantum system, since in general, we cannot feasibly compute the probability of a certain outcome even if we can simulate the quantum circuit. Subsequent works by Aaronson and Arkhipov, and by Bremner, Jozsa, and Shepherd established that if there is a classically efficient weak simulator for different classes of quantum circuits, the polynomial hierarchy collapses to the third level.\n\nSo far, we have only considered the question of exactly sampling from the distribution D_U. However, any realistic experiment is necessarily noisy, and a more natural problem is to sample from a distribution that is not exactly D_U but from any distribution D_O that is ε-close in a suitable distance measure, say the variation distance.\n\nThe aforementioned work by Aaronson and Arkhipov was the first to consider this problem, and they made progress towards showing that a special class of quantum circuits (linear optical circuits) is classically hard to approximately simulate in the sense above. The task of sampling from the output of linear optical circuits is known as boson sampling. At the time, it was the best available way to show that quantum computers may solve some problems that are far beyond the reach of classical computers.\n\nEven granting that the PH doesn't collapse, one still needs to make an additional conjecture to establish that boson sampling is not classically simulable. The conjecture is that additively approximating the output probabilities of a random linear optical quantum circuit is #P-hard. The reason this may be true is that output probabilities of random linear optical quantum circuits are Permanents of a Gaussian random matrix, and the Permanent is as hard to compute on a random matrix as it is on a worst-case matrix. Therefore, the only missing link is to go from average-case hardness of exact computation to average-case hardness of an additive estimation. In addition, if we make a second conjecture known as the \"anti-concentration\" conjecture, we can show that this additive estimation is non-trivial: it suffices to give us a good multiplicative estimation with high probability.\n\nSo that's what quantum computational supremacy is about: we have a computational task that is efficiently solvable with quantum computers, but which would collapse the polynomial hierarchy if done by a classical computer (assuming certain other conjectures are true). One may substitute \"collapse of the polynomial hierarchy\" with stronger conjectures and incur a corresponding tradeoff in the likelihood of the conjecture being true.\n\nRandom circuit sampling\n\nIn 2016, Boixo et al. proposed to replace the class of quantum circuits for which some hardness results were known (commuting circuits and boson sampling) by random circuits of sufficient depth on a 2D grid of qubits having nearest-neighbour interactions. Concretely, the proposed experiment would be to apply random unitaries from a specified set on n qubits arranged on a 2D grid for sufficient depth, and then sample from the resulting distribution. The two-qubit unitaries in the set are restricted to act between nearest neighbours, respecting the geometric This task is called random circuit sampling (RCS).\n\nAt the time, the level of evidence for the hardness of this scheme was not yet the same as the linear optical scheme. However, given the theoretical and experimental interest in the idea of demonstrating a quantum speedup over classical computers, subsequent works by Bouland, Fefferman, Nirkhe and Vazirani, and Harrow and Mehraban bridged this gap (the relevant work by Aaronson and Chen will be discussed in the following section). Harrow and Mehraban proved anticoncentration for random circuits. In particular, they showed that a 2-dimensional grid of n qubits achieve anticoncentration in depth O(\\sqrt{n}), improving upon earlier results with higher depth due to Brandao, Harrow and Horodecki. Bouland et al. proved the same supporting evidence for RCS as that for boson sampling, namely a worst-to-average-case reduction for exactly computing most output probabilities, even without the permanent structure possessed by linear optical quantum circuits.\n\nVerification\n\nSo far, we have not discussed the elephant in the room: of verifying that the output distribution supported on 2^n outcomes. It turns out that there are concrete lower bounds such as those due to Valiant and Valiant, showing that verifying whether an empirical distribution is close to a target distribution is impossible if one has few samples.\n\nBoixo et al. proposed a way of certifying the fidelity of the purported simulation. Their key observation was to note that if their experimental system is well modelled by a noise model called global depolarising noise, estimating the output fidelity is possible with relatively few outcomes. Under global depolarising noise with fidelity f, the noisy distribution takes the form D_N = f D_U + (1-f) I, where I is the uniform distribution over the 2^n outcomes. Together with another empirical observation about the statistics of output probabilities of the ideal distribution D_U, they argued that computing the following cross-entropy score would serve as a good estimator of the fidelity:\n\nf ~ H(I, D_U) - H(D_exp, D_U), where H(D_A,D_B) is the cross-entropy between the two distributions: H(D_A, D_B) = -\\sum_i p_A log (p_B).\n\nThe proposal here was to experimentally collect several samples from D_exp, classically compute using brute-force the probabilities of these outcomes in the distribution D_U, and estimate the cross-entropy using this information. If the test outputs a high score for a computation on sufficiently many qubits and depth, the claim is that quantum supremacy has been achieved.\n\nAaronson and Chen gave alternative form of evidence for the hardness of scoring well on a test that aims to certify quantum supremacy similar to the manner above. This sidesteps the issue of whether a test similar to the one above does indeed certify the fidelity. The specific problem considered was \"Heavy Output Generation\" (HOG), the problem of outputting strings that have higher than median probability in the output distribution. Aaronson and Chen linked the hardness of HOG to a closely related problem called \"QUATH\", and conjectured that QUATH is hard for classical computers.\n\nOpen questions\n\nAssuming the Google team has performed the impressive feat of both running the experiment outlined before and classically computing the probabilities of the relevant outcomes to see a high score on their cross-entropy test, I discuss the remaining positions a skeptic might take regarding the claim about quantum supremacy.\n\n\"The current evidence of classical hardness of random circuit sampling is not sufficient to conclude that the task is hard\". Assuming that the skeptic believes that the polynomial hierarchy does not collapse, a remaining possibility is that there is no worst-to-average-case reduction for the problem of *approximating* most output probabilities, which kills the proof technique of Aaronson and Arkhipov to show hardness of approximate sampling.\n\n\"The cross-entropy proposal does not certify the fidelity.\" Boixo et al. gave numerical evidence and other arguments for this statement, based on the observation that the noise is of the global depolarising form. A skeptic may argue that the assumption of global depolarising noise is a strong one.\n\n\"The QUATH problem is not classically hard.\" In order to give evidence for the hardness of QUATH, Aaronson and Chen examined the best existing algorithms for this problem and also gave a new algorithm that nevertheless do not solve QUATH with the required parameters.\n\nIt would be great if the community could work towards strengthening the evidence we already have for this task to be hard, either phrased as a sampling experiment or together with the verification test.\n\nFinally, I think this is an exciting time for quantum computing and to witness this landmark event. It may not be the first probe of an experiment that is \"hard\" to classically simulate, since there are many quantum experiments that are beyond the reach of current classical simulations, but the inherent programmability and control present in the experimental system is what enables the tools of complexity theory to be applied to the problem. A thought that fascinates me is the idea that we may be exploring quantum mechanics in a regime never probed this carefully before, the \"high complexity regime\" of quantum mechanics. One imagines there are important lessons in physics here.\n\nApplicants to Grad School are too good. Here is why this might be a problem.\n\nSitting around with three faculty we had the following conversation\n\nALICE: When I applied to grad school in 1980 they saw a strong math major (that is, I had good grades in hard math courses) but very little programming or any sort of computer science. That kind of person would NOT be admitted today since there are plenty of strong math majors who ALSO have the Comp Sci chops.\n\nBOB: When I applied to grad school I was a comp sci major but my grades were not that good- A's in system courses, B's and even some C's in math. But I did a Security project that lead to a paper that got into a (minor) systems workshop. Two of my letters bragged a lot about that. (How do I know that? Don't ask.) So I got into grad school in 1989. That kind of person would NOT be admitted today since there are plenty of people who have papers in minor conferences whose grades ARE good in stuff other than their area.\n\nCAROL: In 1975 I was an English major at Harvard. The summer between my junior and senior year I took a programming course over the summer and did very well and liked it. I then took some math. Then I worked in industry at a computer scientist for 5 years. Then I applied to grad school and they liked my unusual background. Plus I did well on the GRE's. Letters from my boss at work helped me, I don't think they would count letters from industry now. They took a chance on me, and it paid off (I got a PhD) but I don't think they would let someone like me in now since they don't have to take a chance. They can admit people who have done research, have solid backgrounds, and hence are not taking a chance. The irony is that some of those don't finish anyway.\n\n1) Are Alice, Bob, and Carol right that they wouldn't be admitted to grad school now? I think they are with a caveat- they might end up in a lower tier grad school then they did end up in. Also, Alice and Bob I am more certain would not end up in the top tier grad schools they did since they can be compared DIRECTLY to other applicants,\n\nwhere as Carol might be more orthogonal.\n\n2) I have a sense (backed up my no data) that we are accepting fewer unusual cases than we used to (not just UMCP but across the country) because too many of the applicants are the standard very-good-on-paper applicants. Even the on-paper is not quite fair- they ARE very good for real.\n\n3) Assume we are taking less unusual cases. Is that bad? I think so as people with different backgrounds (Carol especially) add to the diversity of trains of thought in a program, and that is surely a good thing. If EVERY students is a strong comp sci major who has done some research, there is a blandness to that.\n\n4) What to do about this? First off, determine if this is really a problem. If it is then perhaps when looking at grad school applicants have some sensitivity to this issue.\n\n5) For grad school admissions I am speculating. For REU admissions (I have run an REU program for the last 7 years and do all of the admissions myself) I can speak with more experience. The students who apply have gotten better over time and this IS cause for celebration; however, it has made taking unusual cases harder.\n\nAre there any natural problems complete for NP INTER TALLY? NP INTER SPARSE?\n\nRecall:\n\nA is a tally set if A ⊆ 1*.\n\nA is a sparse set if there is a polynomial p such that the number of strings of length n is ≤ p(n).\n\nIf there exists a sparse set A that is NP-hard under m-reductions (even btt-reductions) then P=NP. (See this post.)\n\nIf there exists a sparse set A that is NP-hard under T-reductions then PH collapses. (See this post.)\n\nOkay then!\n\nI have sometimes had a tally set or a sparse set that is in NP and I think that its not in P. I would like to prove, or at least conjecture, that it's NP-complete. But alas, I cannot since then P=NP. (Clarification: If my set is NP-complete then P=NP. I do not mean that the very act of conjecturing it would make P=NP. That would be an awesome superpower.)\n\nSo what to do?\n\nA is NPSPARSE-complete if A is in NP, A is sparse, and for all B that are in NP and sparse, B ≤m A.\n\nSimilar for NPTALLY and one can also look at other types of reductions.\n\nSo, can I show that my set is NPSPARSE-complete? Are there any NPSPARSE-complete sets? Are there NATURAL ones? (Natural is a slippery notion- see this post by Lance.)\n\nHere is what I was able to find out (if more is known then please leave comments with pointers.)\n\n1) It was observed by Bhurman, Fenner, Fortnow, van Velkebeek that the following set is NPTALLY-complete:\n\nLet M1, M2, ... be a standard list of NP-machines. Let\n\nA = { 1(i,n,t) : Mi(1n) accepts on some path within t steps }'\n\nThe set involves Turing Machines so its not quite what I want.\n\n2) Messner and Toran show that, under an unlikely assumption about proof systems there exists an NPSPARSE-complete set. The set involves Turing Machines. Plus it uses an unlikely assumption. Interesting, but not quite what I want.\n\n3) Buhrman, Fenner, Fortnow, van Melkebeek also showed that there are relativized worlds where there are no NPSPARSE sets (this was their main result). Interesting but not quite what I want.\n\n4) If A is NE-complete then the tally version: { 1x : x is in A } is likely NPTALLY-complete. This may help me get what I want.\n\nOkay then!\n\nAre there any other sets that are NPTALLY-complete. NPSPARSE-complete? The obnoxious answer is to take finite variants of A. What I really want a set of such problems so that we can proof other problems NPTALLY-complete or NPSPARSE-complete with the ease we now prove problems NP-complete.\n\nObstacles to improving Classical Factoring Algorithms\n\nIn Samuel Wagstaff's excellent book The Joy of Factoring (see here for a review) there is a discussion towards the end about why factoring algorithms have not made much progress recently. I\n\nparaphrase it:\n\n--------------------------------------------------------\n\nThe time complexities of the fastest known algorithms can be expressed as a formula of the following form (where N is the number to be factored):\n\n(*) exp(c(ln N)^t (ln(ln N))^{1-t})\n\nfor some constants c and for 0 < t < 1. For the Quadratic Sieve (QS) and Elliptic Curve Method (ECM) t=1/2. For the Number Field Sieve (NFS) t=1/3. The reason for this shape for the time complexity is the requirement of finding one or more smooth numbers (numbers that have only small primes as factors).\n\n----------------------------------------------------------\n\nThis got me thinking: Okay, there may not be a drastic improvement anytime soon but what about just improving t? Is there a mathematical reason\n\nwhy an algorithm with (say) t=1/4 has not been discovered? In an earlier era I would have had to write a letter to Dr. Wagstaff to ask him. Buy an envelope, buy a stamp, find his address, the whole nine yards (my younger readers should ask their grandparents what envelopes and stamps were). In the current era I emailed him. And got a response.\n\nSamuel Wagstaff:\n\nThe fastest known factoring algorithms find smooth numbers subject to parameter choice(s). In all these algorithms, one parameter choice is the smoothness bound B: a number is smooth if all its prime factors are < B. The NFS has the degree of a polynomial as an additional parameter.\n\nOne analyzes the complexity of these algorithms by estimating the total work required (to find enough smooth numbers) for an arbitrary parameter choice using Dickman's function to predict the density of smooth numbers. Then one uses calculus to find the parameter choice(s) that minimize the total work function. Calculus also yields the optimal values for the parameter(s).\n\nIf you have k parameters to choose, you will get the time complexity (*) with t = 1/(1+k). If you have no parameters (k = 0),you get (*) with t = 1, basically exponential time N^c. With one parameter to optimize, as in CFRAC (continued fractions algorithm) and QS, you get t = 1/2. NFS has two parameters, so t = 1/3. ECM also has t = 1/2 because it uses only one parameter, the smoothness bound B. If you want to get t = 1/4, you have to find a third parameter to optimize. No one has found one yet. That is the answer to your question.\n\nNote that some choices made in some factoring algorithms don't count as parameters. For example, the number of polynomials used in the multiple-polynomial quadratic sieve, and the upper bound on large primes kept, don't affect t. They affect the running time only in making c smaller. So you have to find a third parameter that matters in order to get (*) with t = 1/4. Or find three completely different new parameters.\n\nTuring to be on the Bank of England 50 pound note, giving me an excuse to talk about Turing\n\nBILL: Darling, guess who is soon going to be on the Bank of England 50 pound note?\n\nDARLING: Alan Turing.\n\nBILL: How did you deduce that? (She is right, see here.)\n\nDARLING: Since you asked it, it couldn't be a member of the Royal Family (you don't care about that) or some British Politician (you don't care about that either). It had to be a mathematician or computer scientist.\n\nBILL: It could have been Hardy. I wonder if Ramanujan could qualify---do they need to be British? At this website it says\n\nOf course, banknotes need to be universally accepted. We therefore look for UK characters who have made an important contribution to our society and culture through their innovation, leadership or values. We do not include fictional characters, or people who are still living (except the monarch on the front of the note). Finally, we need to have a suitable portrait of the person which will be easy to recognise.\n\n(They spell recognise with an s instead of a z, so spellcheck flagged it, but I won't change it.)\n\nNote that people on the banknotes have to be UK characters. I honestly don't know if that means they must be citizens.\n\nOKAY, so here are a few thoughts on Turing.\n\n1) When I visited Bletchley Park there was a booklet that bragged about the fact that Bletchley Park was much better at cracking codes than Germany because they allowed people to work there based only on ability (unlike Germany) - women worked there, Turing who was Gay worked there. I think this is simplistic. Did any Jews work there (anti-semitism was widespread in England, and the world, at the time)? I doubt any blacks worked there since if they did that would be well known by now (if I am wrong let me know). Women DID work there but was their work respected and used? (I honestly don't know). Did Germany also use women at their codebreaking centers? Was Turing known to be gay (if not then Bletchley gets no points for tolerating him). Was JUST having Turing the reason they could crack codes. Plus I am sure there were other factors aside from merit-only.\n\n2) Turing was given a Pardon for his ``crimes'' in August 2014. When I see things like this I wonder who was against it and why and if they were an obstacle.\n\na) Human Rights Advocate Peter Tatchell noted that its wrong to just single out Turing. Other people prosecuted under that law who did not help beat the German's in WW II should also be pardoned. The government later DID such a pardon in 2017.\n\nb) Judge Minister Lord McNally objected to the pardon:\n\nA posthumous pardon was not considered appropriate as Alan Turing was properly convicted of what at the time was a criminal offence. He would have known that his offence was against the law and that he would be prosecuted. It is tragic that Alan Turing was convicted of an offence that now seems both cruel and absurd—particularly poignant given his outstanding contribution to the war effort. However, the law at the time required a prosecution and, as such, long-standing policy has been to accept that such convictions took place and, rather than trying to alter the historical context and to put right what cannot be put right, ensure instead that we never again return to those times.\n\nWhile I disagree with him, I do note that, based on what he wrote and his general record, I think he is not saying this from being anti-gay. There is a hard general question here: how does a society right past wrongs? I think pardoning and apologizing is certainly fine, but frankly it seems to weak. What else could a society due? Financial renumeration to living relatives? I don't think giving Inagh Payne (Turing's niece, who I think is still alive) would really help here.\n\nc) At the bill's second reading in the House of Commons on 29 November 2013, Conservative MP Christopher Chope objected to the bill, delaying its passage\n\nI couldn't find Chope's reasons. On the one hand, they may be similar to McNally's. On the other hand he is against same sex marriage so its possible (though I do not know this) that he anti-gay and that is why he is against the pardon. If someone can find what his explanation for blocking the Turing bill is, or other evidence that he is anti-gay, please leave it in the comments.\n\n3) Did the delay matter? I was surprised to find out---Yes. Here is the full passage from Wikipedia:\n\nAt the bill's second reading in the House of Commons on 29 November 2013, Conservative MP Christopher Chope objected to the bill, delaying its passage. The bill was due to return to the House of Commons on 28 February 2014,[175] but before the bill could be debated in the House of Commons,[176] the government elected to proceed under the royal prerogative of mercy. On 24 December 2013, Queen Elizabeth II signed a pardon for Turing's conviction for \"gross indecency\", with immediate effect.[17] Announcing the pardon, Lord Chancellor Chris Grayling said Turing deserved to be \"remembered and recognised for his fantastic contribution to the war effort\" and not for his later criminal conviction.[16][18] The Queen officially pronounced Turing pardoned in August 2014.[177] The Queen's action is only the fourth royal pardon granted since the conclusion of the Second World War.[178] Pardons are normally granted only when the person is technically innocent, and a request has been made by the family or other interested party; neither condition was met in regard to Turing's conviction.[179]\n\nThis amazed me! I thought the Queen had NO power (too bad--- I wish she could just say NO BREXIT). Or that she formally has power but if she ever used it, it might be blocked somehow and taken away. So I am surprised she has a power she can use at all.\n\n4) I wonder if the Pardon had to happen before they put him on the Banknote. I have been told that this is a very American Question--- England has no Constitution and operates more on Custom and Tradition than on written rules.\n\n5) I had always assumed that Turing committed suicide. Without going into detail, the Wikipedia site on Turing does give intelligent counterarguments to this. See here"
    }
}
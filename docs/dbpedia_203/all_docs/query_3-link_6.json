{
    "id": "dbpedia_203_3",
    "rank": 6,
    "data": {
        "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6001464/",
        "read_more_link": "",
        "language": "en",
        "title": "Rating the quality of a body of evidence on the effectiveness of health and social interventions: A systematic review and mapping of evidence domains",
        "top_image": "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/pmc-card-share.jpg?_=0",
        "meta_img": "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/pmc-card-share.jpg?_=0",
        "images": [
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/favicons/favicon-57.png",
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/icon-dot-gov.svg",
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/icon-https.svg",
            "https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/logos/AgencyLogo.svg",
            "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/logo-blackwellopen.png",
            "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/corrauth.gif",
            "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/corrauth.gif",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6001464/bin/JRSM-9-224-g001.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6001464/bin/JRSM-9-224-g002.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6001464/bin/JRSM-9-224-g003.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Movsisyan",
            "Jane Dennis",
            "Eva Rehfuess",
            "Sean Grant",
            "Paul Montgomery"
        ],
        "publish_date": "2018-06-09T00:00:00",
        "summary": "",
        "meta_description": "Rating the quality of a body of evidence is an increasingly common component of research syntheses on intervention effectiveness. This study sought to identify and examine existing systems for rating the quality of a body of evidence on the effectiveness ...",
        "meta_lang": "en",
        "meta_favicon": "https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon.ico",
        "meta_site_name": "PubMed Central (PMC)",
        "canonical_link": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6001464/",
        "text": "3.4. Mapping of evidence domains\n\nThe evidence domains used to rate the quality of a body of evidence were often similar in concept across systems yet different in how they were described and operationalized. We encourage readers to use Table and Figure as 2 complementary sources of information on the identified evidence rating systems to examine the discrepancies in labeling and describing evidence domains. Table provides an overview of the domains of evidence as they are reported in the original studies, while Figure maps the 13 discrete domains we identified in included systems and presents how they are reported in each of the included systems. More information on how the specific evidence domains were defined and operationalized in each system is presented in Supporting Information (Online Supplement). In the sections below, we briefly summarize the identified discrete set of domains of evidence (see Figure ), as well as the reported activities underpinning the development and dissemination of these systems (see Figure ).\n\n3.4.1. Study design\n\nTwelve systems included an evidence domain related to the design of the individual studies constituting the body of evidence. All but 4 of these systems35, 36, 45, 50 described an “evidence hierarchy” approach that influenced how overall quality of a body of evidence was assessed. Procedurally, this entailed initially privileging a body of evidence from certain study designs (namely RCTs) as providing a higher quality (compared with other study designs) before assessing other evidence domains. While all systems with an evidence hierarchy approach placed evidence from RCTs at the top of this hierarchy, many further privileged specific nonrandomized study designs over others. For example, the system used by the Joanna Briggs Institute39 suggested initial ratings of quality depending on whether a body of evidence consists of experimental (Level 1), quasi‐experimental (Level 2), or observational studies (Level 3). Similarly, the GRADE‐modified GEPHI system for public health interventions recommends that a body of evidence consisting of nonrandomized studies with controls or before and after [uncontrolled] studies have an initial rating of “moderate” quality if these studies used methods to minimize selection bias and confounding.41\n\n3.4.2. Study execution\n\nFifteen systems included an evidence domain related to assessing how well studies constituting the body of evidence were executed to minimize threats to internal and external validities (also labeled as quality of study execution, risk of bias, study limitations, and study quality). In most instances, however, systems mainly included criteria to assess risks of bias or threats to the internal validity for assessing study execution. A few systems, however, also included specific criteria for assessing the generalizability of the study results, that is, criteria related to the external validity of the individual studies in the body of evidence.\n\nSystems varied in how they operationalized assessment of study execution. Some systems used design‐specific criteria, such as checklists or signaling questions for appraising RCTs36, 38, 40, 43 or longitudinal studies.43 Most systems, however, described more generic criteria to assess study execution across various study designs included in the body of evidence.37, 45, 46, 48, 50\n\n3.4.3. Consistency\n\nFourteen systems included an evidence domain related to the consistency of evidence. Generally, systems defined consistency as “the extent to which findings are similar across included studies” in a body of evidence,48 usually in reference to the degree of similarity in the magnitude and/or direction of effect estimates. Most systems, however, did not report any specific criteria on how to rate consistency in the body of evidence. Only a few systems discussed specific procedures, such as statistical testing for heterogeneity to rate consistency in the body of evidence. The GRADE‐modified GEPHI approach distinguished between 2 types of consistency ratings41: The first type was identical to the domain of the GRADE approach termed as inconsistency and defined as “assessment of statistical heterogeneity in the estimates of the effect.”51 The second type of consistency rating was specified in the system as “consistency” assessment and was defined as presence of “consistent evidence across a large number of settings, geographical locations and diverse epidemiological study designs.” The system argued that the fact that an intervention effect is reproducible under highly variable conditions suggests reduced likelihood that the observed effect is attributable to confounding or bias.41 This can increase a reviewer's confidence in the body of evidence regarding the overall effectiveness of an intervention.\n\n3.4.4. Measures of precision\n\nEleven systems included an evidence domain that we have classified as relating to measures of precision of the body of evidence: ie, considerations of the impact that random error may have on effect estimates. Systems differed widely in the level of specification and sophistication they required for assessing precision of the body of evidence. For instance, many systems recommend only considering the number of studies in the body of evidence as a measure of precision37, 43, 45, 46, 52; however, only 1 of these systems specifies a threshold for the minimum number of studies to be included in the body of evidence.52 Furthermore, only the GRADE approach and its variants described specific criteria for assessing precision regarding the sufficiency of the sample size of the body of evidence.38, 39, 40, 41 These systems assessed sufficiency of the sample size relative to an “optimal information size”: ie, “number of patients (for continuous outcomes) and events (for dichotomous outcomes) that would be needed to regard a body of evidence as having adequate power.”53 In addition, these systems also considered the boundaries of confidence intervals for an effect estimate in relation to a null effect and a clinically important effect threshold to make an overall judgment about the precision of a body of evidence. The estimate of the effect of an intervention is judged to be less precise if the confidence interval is wide to include a null effect or a threshold, which is considered as clinically unimportant.53\n\n3.4.5. Directness\n\nIn general, the systems used concepts of directness, applicability, and generalizability of evidence interchangeably and inconsistently—often without providing clear definitions or specific criteria to guide the assessment.35, 37, 47, 48, 50 In addition, these terms were not necessarily used as synonyms across the systems. For example, the system endorsed by the National Health and Medical Research Council of Australia used the term “applicability” to address whether the body of evidence was relevant to the local context (including the organizational and cultural contexts), while the term generalizability was used to refer to how precisely a body of evidence answered a review or a guideline question in populations and settings of interest.48 To disentangle the discrepancies in the terminology, we have used the terminology of the GRADE approach, namely, “directness” of evidence, to describe the domains of evidence from the included systems related to the notion of comparability of the evidence to the original research question. We have identified 6 systems that used this domain of evidence to assess how directly the available evidence answers a review or a guideline question regarding Population, Intervention, Comparison, and Outcomes elements of the question.35, 39, 40, 41, 48, 54\n\n3.4.6. Publication bias\n\nFive systems included publication bias as a domain for rating the quality of a body of evidence.36, 39, 40, 41, 55 All but 1 of these systems followed a definition of publication bias as used within the GRADE approach, that is, “a failure to identify studies as a result of studies remaining unpublished or obscurely published.”55 The system used by AHRQ, on the other hand, considered publication bias as only 1 type of potential bias within a broader domain of reporting biases, which was itself defined as a decision by authors or journals to report research findings based on their direction and magnitude of effect.40 Selective outcome reporting and selective analysis reporting were the other types of reporting biases described in this system.\n\n3.4.7. Magnitude of effect\n\nWe identified 7 systems, which included magnitude of effect as a distinct domain to rate the quality of a body of evidence on the effectiveness of health or social interventions.36, 39, 40, 41, 43, 46, 56, 57 However, only 4 of these systems specified the thresholds for what they considered to be a “large” magnitude of effect.39, 41, 56, 57 This predominantly included a relative risk greaten than 2, or less than 0.2, as suggested in the GRADE approach.56\n\n3.4.8. Dose‐response\n\nOverall, 5 systems considered dose‐response as a distinct domain of evidence when rating the quality of a body of evidence on the effectiveness of health or social interventions.39, 40, 41, 47, 56, 57 The systems commonly defined dose‐response as a “pattern of a larger effect with greater exposure to an intervention.”40\n\n3.4.9. Plausible residuals\n\nAll systems that followed the structure of the GRADE approach (overall 4 systems, including GRADE itself) considered counteracting confounding, as a domain to upgrade the quality of a body of evidence, when a body of evidence is mainly composed of observational studies.39, 40, 41, 56 Two possibilities were commonly applied: “if all plausible residual biases would diminish the observed effect, or if all plausible residual biases would suggest a spurious effect when no effect is observed.”56\n\n3.4.10. Analogy\n\nOnly 1 system—the GEPHI system—included an evidence domain related to analogous evidence. The GEPHI system operationalized analogous evidence as supporting evidence from similar or “analogous” interventions that are known to operate through the same or similar mechanisms, which, if present, could lead to a higher quality of a body of evidence rating.41 In the context of WHO guidelines on indoor air quality, the system discusses the example of how certainty in the effects of household air pollution from solid fuel can be enhanced by strong empirical evidence about the effects of second‐hand or active smoking. In this example, both household air pollution and second‐hand or active smoking expose individuals to similar combustion mixtures and therefore are viewed as analogous pieces of evidence.41\n\n3.4.11. Robustness\n\nRobustness of evidence was described as a domain to rate the quality of a body of evidence by one system.52 The system suggests that reviewers measure robustness of evidence through sensitivity analysis with a priori defined thresholds. For example, a reviewer may decide a priori that a threshold for robustness assessment is one in which “confidence intervals of the last three cumulative, random‐effects meta‐analyses remain fully on the same side of zero after removing of the study with the smallest weight.”52\n\n3.4.12. Applicability\n\nFour systems described applicability as a domain of evidence measuring the extent to which evidence may be applicable in a specific context.37, 47, 48, 50 It is worth highlighting that we identified 3 additional systems,40, 45, 46 which considered applicability of evidence as a separate judgment when making recommendations for practice. In these systems, discussion of applicability was held separately from other domains of evidence, and largely within a context of guideline development. For example, the GRADE‐based system endorsed by AHRQ clearly separates judgments of directness of evidence from that of applicability assessment. In this system, directness of evidence is defined to express “how closely the available measures an outcome of interest” and relies on 2 judgments40: the directness of the employed outcomes (ie, whether the available evidence is in fact only a proxy for an ultimate outcomes of interest) and directness of comparisons (ie, whether evidence derives from head‐to‐head comparisons). Meanwhile, the system defines applicability as the external validity of the evidence base regarding different populations and is considered explicitly but separately from the overall rating of the quality of a body of evidence.40\n\n3.4.13. Coherence\n\nOnly 3 systems included an evidence domain related to assessing the coherence of the causal pathway of an intervention41, 47, 57: that is, related to the assessment of a theory of change or a mechanism whereby an intervention is expected to operate. The GEPHI system recommends assessing confidence in the overall causal pathway between an intervention and distal outcomes (referred to as rating of coherence of evidence) regarding the evidence informing each individual link in the causal pathway.41 It describes this domain specifically in the context of interventions that involve complex causal pathways, where evidence directly linking the intervention with the distal outcomes is frequently unavailable. Similarly, by using analytic frameworks, the USPTSF system rates certainty of evidence in the overall chain of evidence for a specific preventive service.47 The system described by Tang and colleagues (2008) included assessment of the known mechanisms of action as a domain of evidence for rating of the quality of a body of evidence: “if the theoretical basis is not known, the strength of evidence will be less convincing.”57\n\n4.1. “State of the field” map of evidence rating systems for health and social interventions\n\nThis systematic review set out to describe the content, development, and dissemination of the systems for rating the quality of a body of evidence on intervention effectiveness across health and social policies. The review identified 17 systems that have made useful contributions to rating the quality of a body of evidence in health and social research synthesis. While this review identified domains of evidence that were commonly reported across the systems, there was significant variation in the specifications for these domains. The systems used different terminology to denote similar constructs of evidence when rating the quality of a body of evidence. The systems also varied in how they operationalized the domains of evidence, that is, in whether they described specific criteria and provided guidance for assessing each domain in an operationalizable manner. This review also identified domains of evidence that were found only in a few systems (see Figure ). In general, the discrete set of domains identified in our review can be viewed to largely follow the “viewpoints for causation” proposed by Sir Austin Bradford Hill,59 although the relative coverage of these criteria across the included systems varies. For example, domains of evidence that will correspond to the Hill's criteria of experiment (study design and study execution), strength of association (magnitude of effect), consistency, and dose‐response gradient have been reported more extensively in evidence rating systems. Meanwhile, our review found only 3 systems, which considered domains corresponding to the Bradford Hill viewpoints of plausibility and coherence of evidence, and only 1 system included a domain on the analogous evidence. This can partly be explained by the challenges of developing an operational framework in research synthesis to assess the evidence against these criteria, including the need to search and integrate different sources of evidence.60\n\nAs this systematic review aimed to consider evidence rating systems across health and social policies, the identified variation in the terminology and description of evidence domains may partly reflect how research synthesis and its practice differs across policy areas and types of interventions. One of the most contested topics in the discussions of the quality of a body of evidence relates to the hierarchy of evidence initially described in the paradigm of evidence‐based medicine as an approach to differentiate between weak and strong study designs for assessing intervention effectiveness.61 While different versions of the evidence hierarchy have been described in clinical medicine, all of them place study designs such as case series (considered relatively weaker in protecting against threats to internal validity) in the bottom of the hierarchy, followed by case‐control and cohort studies in the middle and RCTs at the top.62 As our findings demonstrate, this evidence hierarchy approach is still used in many evidence rating systems, and particularly those developed and employed in clinical medicine. The widely adopted GRADE approach also follows this approach by way of describing 2 broad categories of study designs as a starting point for the body‐of‐evidence rating process (RCT evidence is initially rated as “high” quality and non‐RCT evidence as “low” quality). By contrast, our findings show that systems which are used in broader policy areas, such as public health, tend to allow more flexibility for differentiating between the many types of non‐RCT designs within their constructions of evidence hierarchies (see section 3.4.1 and Table ). This practice is commensurate with a view that quasi‐experimental approaches should be given appropriate provisions in evidence rating systems as valuable methods for making causal inferences for public health interventions.63\n\nConsistency of the body of evidence was another frequently reported domain of evidence in the included systems. Our findings demonstrate that evidence rating systems currently conceptualize consistency as similarity in the magnitude and direction of effect estimates across studies (of same or similar design) included in the body of evidence. There are, however, concerns that this approach only partly reflects the central tenet of scientific method, specifically that findings are replicable across “a variety of situations and techniques.”59 From this perspective, there are suggestions for a broader interpretation of the consistency of evidence to also consider “triangulation of evidence” across different methodological approaches when arriving at overall conclusions about intervention effectiveness.64 Triangulation has been defined as integration of evidence from several different methodological approaches (different study designs and analytical approaches), which address the same underlying causal question, but which vary in key sources of potential bias (for example, multivariable regression, instrumental variables, and RCTs).65 The importance of evidence triangulation has been cogently argued in the context of public health interventions involving longer causal pathways and multiple targets and behaviors, such as smoking or alcohol consumption, which are difficult (or impossible) to evaluate with RCTs alone. When the results from different methodological approaches are consistent in that they all point to the same conclusion, this is argued to strengthen the confidence in the overall findings (see Lawlor et al., 2016).65 Our review identified only 1 system which extended the domain of consistency to consider evidence from different study designs.41 Its broad interpretation, which looks at evidence from different methodological approaches to inform the rating of the quality of a body of evidence, was unique within our findings (see section 3.4.3).\n\nOur review identified very few instances where the systems provided a definition for the construct of the quality of the body of evidence (see section 3.3). The few reported definitions mainly focus on the confidence in a direct estimate of the effect of an intervention—a definition initially suggested by GRADE. It is worth noting here that the most recent publication of the GRADE Working Group clarifies this definition of the quality of a body of evidence based on a priori defined threshold and the context of the review.66The quality of a body of evidence is currently conceptualized to reflect the extent to which reviewers can be confident that “the true effect for a specific outcome lies on one side of a specified threshold or within a chosen range.”66 The revised guidance suggests 3 types of ratings: noncontextualized, partly contextualized, and fully contextualized (see Table for more details). In this new conceptualization, the quality of a body of evidence ratings is explicitly acknowledged to be contingent upon a priori defined thresholds of what may be considered as meaningful effects in different contexts. These thresholds and the resultant ratings may therefore vary depending on the context and purpose of the review.\n\nTable 2\n\nSettingDegree of ContextualizationThreshold or RangeHow to SetWhat Certainty Rating RepresentsPrimarily for systematic reviews and health technology assessmentNoncontextualizedRange: 95% CIUsing existing limits of the 95% CICertainty that the effect lies within the confidence intervalOR ≠ 1; RR ≠ 1; HR ≠ 1; RD ≠ 0Using the threshold of null effectCertainty that the effect of one treatment differs from anotherPrimarily for systematic reviews and health technology assessmentPartly contextualizedSpecified magnitude of effecteg, small effect is the effect small enough to not use the intervention if adverse effects/costs are appreciableCertainty in a specified magnitude of effect for 1 outcome (eg, trivial, small, moderate, or large)Primarily for practice guidelinesFully contextualizedThreshold determined with consideration of all critical outcomesConsidering the range of effects on all critical outcomes and the values and preferencesConfidence that the direction of the net effect will not differ from 1 end of the certainty range to the other\n\nRegarding the activities underpinning the development and dissemination of the included systems, our review found that most systems did not report a comprehensive literature review or a consensus‐based procedure for developing the system (see Figure ). In a similar vein, we found little reporting of how these systems were written up and further disseminated. It therefore remains difficult to assess how the described domains of evidence have been conceptualized and the degree to which they are, or are not, the product of scientific consensus. In the meantime, if not properly developed and disseminated, these systems may have limited value and use in research synthesis.26 In this regard, our review shows that the GRADE approach is 1 of the most comprehensive and transparent evidence rating systems in its guidance as well as its development and dissemination.7"
    }
}
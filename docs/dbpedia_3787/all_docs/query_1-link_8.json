{
    "id": "dbpedia_3787_1",
    "rank": 8,
    "data": {
        "url": "https://encord.com/blog/what-is-out-of-distribution-ood-detection/",
        "read_more_link": "",
        "language": "en",
        "title": "What is Out-of-Distribution (OOD) Detection?",
        "top_image": "https://images.prismic.io/encord/2a41f7b2-7943-4629-b71a-7c03791f521a_OOD.png?auto=compress%2Cformat&fit=max",
        "meta_img": "https://images.prismic.io/encord/2a41f7b2-7943-4629-b71a-7c03791f521a_OOD.png?auto=compress%2Cformat&fit=max",
        "images": [
            "https://images.prismic.io/encord/2a41f7b2-7943-4629-b71a-7c03791f521a_OOD.png?auto=compress%2Cformat&fit=max&w=906&h=638",
            "https://images.prismic.io/encord/2a41f7b2-7943-4629-b71a-7c03791f521a_OOD.png?auto=compress%2Cformat&fit=max&w=906&h=638",
            "https://encord.com/static/VectorDesktop-d6a994f2c668a0332ba39898992e598f.png",
            "https://encord.com/static/VectorTablet-5246b4eeb12ce3a011a59f9a65313af7.png",
            "https://encord.com/static/VectorDesktop-d6a994f2c668a0332ba39898992e598f.png",
            "https://encord.cdn.prismic.io/encord/ZmrVVZm069VX1tfd_Union.svg",
            "https://images.prismic.io/encord/7db4b634-db89-4461-bea0-82c52fcb21e4_1646655026017.jpeg?auto=compress%2Cformat&fit=max&w=80&h=80",
            "https://images.prismic.io/encord/7db4b634-db89-4461-bea0-82c52fcb21e4_1646655026017.jpeg?auto=compress%2Cformat&fit=max&w=80&h=80",
            "https://images.prismic.io/encord/7d2c1e97-9b89-4c8a-aa68-7b95aacc97e4_image4.png?auto=compress,format",
            "https://encord.cdn.prismic.io/encord/Zk3PGCol0Zci9WSy_information.svg",
            "https://images.prismic.io/encord/400c35fb-0e12-4958-b46f-9c9ce7054134_cta-img.png?auto=compress,format",
            "https://images.prismic.io/encord/92e9ab65-2be4-449a-8abf-1c35966dda84_image3.png?auto=compress,format",
            "https://images.prismic.io/encord/26ee49a9-5d02-4500-bb59-ea1827648c59_image2.png?auto=compress,format",
            "https://images.prismic.io/encord/400c35fb-0e12-4958-b46f-9c9ce7054134_cta-img.png?auto=compress,format",
            "https://encord.com/static/VectorDesktop-d6a994f2c668a0332ba39898992e598f.png",
            "https://encord.com/static/VectorTablet-5246b4eeb12ce3a011a59f9a65313af7.png",
            "https://encord.com/static/VectorDesktop-d6a994f2c668a0332ba39898992e598f.png",
            "https://encord.cdn.prismic.io/encord/ZmrVVZm069VX1tfd_Union.svg",
            "https://images.prismic.io/encord/7db4b634-db89-4461-bea0-82c52fcb21e4_1646655026017.jpeg?auto=compress%2Cformat&fit=max&w=80&h=80",
            "https://images.prismic.io/encord/7db4b634-db89-4461-bea0-82c52fcb21e4_1646655026017.jpeg?auto=compress%2Cformat&fit=max&w=80&h=80",
            "https://images.prismic.io/encord/065b7cde-b140-43c5-94ee-033d29bc3c34_image2.jpg?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/065b7cde-b140-43c5-94ee-033d29bc3c34_image2.jpg?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/991bfbf1-90c6-45fe-b1f1-900c7a41d547_Data+Curation.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/991bfbf1-90c6-45fe-b1f1-900c7a41d547_Data+Curation.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/d7ed7c81-e1a5-4665-b5e8-889d6ebf8f17_ML+modeling+vs+ML+observability.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/d7ed7c81-e1a5-4665-b5e8-889d6ebf8f17_ML+modeling+vs+ML+observability.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/ZgXBBMt2UUcvBQ2P_VisualizationsinDatabricks.png?auto=format%2Ccompress&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/ZgXBBMt2UUcvBQ2P_VisualizationsinDatabricks.png?auto=format%2Ccompress&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/ZgKwrccYqOFdyFc1_Mora-Banner.png?auto=format%2Ccompress&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/ZgKwrccYqOFdyFc1_Mora-Banner.png?auto=format%2Ccompress&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/65e096f89c42d04f7d96a37a_image-3-.png?auto=format%2Ccompress&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/65e096f89c42d04f7d96a37a_image-3-.png?auto=format%2Ccompress&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/65e0b7bd27237c2bb829aa4d_image-44-.png?auto=format%2Ccompress&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/65e0b7bd27237c2bb829aa4d_image-44-.png?auto=format%2Ccompress&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/32022a01-5deb-4084-9ab2-608bdf44d18f_Gemini+1.5.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/32022a01-5deb-4084-9ab2-608bdf44d18f_Gemini+1.5.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/af42bef8-9530-4d95-806d-2440ad79b04c_image4.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/af42bef8-9530-4d95-806d-2440ad79b04c_image4.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/a2da04e9-4214-403a-a463-0ebc7b2ef929_image+%2875%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/a2da04e9-4214-403a-a463-0ebc7b2ef929_image+%2875%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/2bd1cb87-8b2b-473f-85f5-11d97e1420e3_What+is+Model+Observability+-+Encord.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/2bd1cb87-8b2b-473f-85f5-11d97e1420e3_What+is+Model+Observability+-+Encord.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/7afd5c49-7289-4edf-8b97-87a1973c0d51_image+%2831%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/7afd5c49-7289-4edf-8b97-87a1973c0d51_image+%2831%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/0cab0829-77a1-4589-ae0a-6e6ffa83b73d_image+%2828%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/0cab0829-77a1-4589-ae0a-6e6ffa83b73d_image+%2828%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/613edaae-6914-403d-bf6f-39cdf80c1786_image5.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/613edaae-6914-403d-bf6f-39cdf80c1786_image5.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/424f6050-6d3f-49ab-b807-8465b455622c_Cross+Entropy+Loss+Functions.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/424f6050-6d3f-49ab-b807-8465b455622c_Cross+Entropy+Loss+Functions.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/6155fefd-96d8-476c-8b9b-f579671f51b5_Fine-tuning+vs+training.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/6155fefd-96d8-476c-8b9b-f579671f51b5_Fine-tuning+vs+training.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/1e3870ac-9b13-4749-b84a-e809733f6f5a_Mean+Average+Precision+in+Object+Detection-+Encord.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/1e3870ac-9b13-4749-b84a-e809733f6f5a_Mean+Average+Precision+in+Object+Detection-+Encord.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/760fff56-285d-43d6-8612-d410688a6c71_Vision+Language+Models.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/760fff56-285d-43d6-8612-d410688a6c71_Vision+Language+Models.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/acf3e8ac-56e3-4c2a-a64e-3c866a5a5043_image+%2821%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/acf3e8ac-56e3-4c2a-a64e-3c866a5a5043_image+%2821%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/692f0d83-ef27-4932-b131-7fcb9b40b530_ChatGPT.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/692f0d83-ef27-4932-b131-7fcb9b40b530_ChatGPT.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/f9a534e0-7fd1-4c97-894d-ffa02549447e_Scale+Alternatives.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/f9a534e0-7fd1-4c97-894d-ffa02549447e_Scale+Alternatives.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/bae5445b-042c-4876-a96e-00356a8851b7_Sudoku.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/bae5445b-042c-4876-a96e-00356a8851b7_Sudoku.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/76851426-6533-4e6f-a7b3-ef67b07f16ef_DALL-E+3+Blog+Post+Image.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/76851426-6533-4e6f-a7b3-ef67b07f16ef_DALL-E+3+Blog+Post+Image.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/cff8838f-a3f4-4d1e-aead-942047579fdb_image+%2820%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/cff8838f-a3f4-4d1e-aead-942047579fdb_image+%2820%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/8a55a37e-6c18-454e-a366-e7b8aff5daf5_image3.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/8a55a37e-6c18-454e-a366-e7b8aff5daf5_image3.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/f3cd75d4-c3ba-4d3e-9fe6-fc49d0b547c6_image9.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/f3cd75d4-c3ba-4d3e-9fe6-fc49d0b547c6_image9.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/0ee8d42b-fe26-4ad6-8a06-be74eff3565b_image2.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/0ee8d42b-fe26-4ad6-8a06-be74eff3565b_image2.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/c6ec5dd2-e45d-4b6a-9c8b-5cdef30aef63_Recent+papers.jpg?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/c6ec5dd2-e45d-4b6a-9c8b-5cdef30aef63_Recent+papers.jpg?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/d9a505d2-9fb8-425d-bd8f-cd4ffa0001b7_image9.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/d9a505d2-9fb8-425d-bd8f-cd4ffa0001b7_image9.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/f1f5d3a3-06b3-4ea6-a450-bc900661a077_Barlow-Twins.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/f1f5d3a3-06b3-4ea6-a450-bc900661a077_Barlow-Twins.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/cab9b6c5-bf6d-4575-9de3-4f4d90238e2f_Vision-Transformers-%28ViT%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/cab9b6c5-bf6d-4575-9de3-4f4d90238e2f_Vision-Transformers-%28ViT%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/d9880bf1-9792-481b-89af-cf8c34d1af54_image2.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/d9880bf1-9792-481b-89af-cf8c34d1af54_image2.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/4d5afeba-9695-4704-9fd8-13e34abe37f0_Transfer+Learning.jpg?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/4d5afeba-9695-4704-9fd8-13e34abe37f0_Transfer+Learning.jpg?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/065b7cde-b140-43c5-94ee-033d29bc3c34_image2.jpg?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/065b7cde-b140-43c5-94ee-033d29bc3c34_image2.jpg?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/cfa0df6b-b85b-4bed-93be-1b8895e06da8_image2.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/cfa0df6b-b85b-4bed-93be-1b8895e06da8_image2.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/991bfbf1-90c6-45fe-b1f1-900c7a41d547_Data+Curation.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/991bfbf1-90c6-45fe-b1f1-900c7a41d547_Data+Curation.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/3268a966-e19e-484e-bf64-646e64387956_Hyperparameter+Tuning.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/3268a966-e19e-484e-bf64-646e64387956_Hyperparameter+Tuning.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/402bca8e-ea61-4668-956e-f69eade05ff1_Part+2.jpg?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/402bca8e-ea61-4668-956e-f69eade05ff1_Part+2.jpg?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/f04dedd4-e636-41ca-a868-72b2c74969d6_Part+1.jpg?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/f04dedd4-e636-41ca-a868-72b2c74969d6_Part+1.jpg?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/9eb3fbc1-3768-4a7b-a624-759fcd327ad2_Deep+Learning.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/9eb3fbc1-3768-4a7b-a624-759fcd327ad2_Deep+Learning.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/93d426cc-ec5e-4c66-88b3-54ef05c098d7_Time+series+w+RNN.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/93d426cc-ec5e-4c66-88b3-54ef05c098d7_Time+series+w+RNN.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/384870c1-ada4-4d59-8450-919dbaf86b04_image+%2844%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/384870c1-ada4-4d59-8450-919dbaf86b04_image+%2844%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/3fcce73d-4116-4e1c-9a27-d99ca637a27a_image1.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/3fcce73d-4116-4e1c-9a27-d99ca637a27a_image1.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/80e2444d-26ec-4504-bde1-b5a15092e9ab_PUG+Blog+Banner.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/80e2444d-26ec-4504-bde1-b5a15092e9ab_PUG+Blog+Banner.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/d7ed7c81-e1a5-4665-b5e8-889d6ebf8f17_ML+modeling+vs+ML+observability.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/d7ed7c81-e1a5-4665-b5e8-889d6ebf8f17_ML+modeling+vs+ML+observability.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/583430bd-6888-4ad7-a91c-4a8e10d8096d_Model+Inference.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/583430bd-6888-4ad7-a91c-4a8e10d8096d_Model+Inference.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/b23e44bf-aea5-404a-ba3e-5c4d74116982_Data+factory.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/b23e44bf-aea5-404a-ba3e-5c4d74116982_Data+factory.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/044b6a04-3077-4069-9dc6-80d1bca6b52a_image+%2840%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/044b6a04-3077-4069-9dc6-80d1bca6b52a_image+%2840%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/fbdaee27-be5a-48c6-a2c6-eeb16967a103_image+%2838%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/fbdaee27-be5a-48c6-a2c6-eeb16967a103_image+%2838%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/31000dbf-703e-4d39-a761-572e25e38adb_BIas+in+machine+learning+%281%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/31000dbf-703e-4d39-a761-572e25e38adb_BIas+in+machine+learning+%281%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/af0365e1-0625-4343-b39f-63e077deabce_Per-SAM+vs+Mask+RCNN.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/af0365e1-0625-4343-b39f-63e077deabce_Per-SAM+vs+Mask+RCNN.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/7d53a172-8e30-423a-924f-abfcc64c8c0e_image6.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/7d53a172-8e30-423a-924f-abfcc64c8c0e_image6.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/c53a4e69-7209-48b6-9190-cc578af755f0_image9.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/c53a4e69-7209-48b6-9190-cc578af755f0_image9.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/5accaa76-6fad-437b-8fbb-94b9544c3789_image7.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/5accaa76-6fad-437b-8fbb-94b9544c3789_image7.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/e0cf64c6-71b5-4c3f-abc3-4f3769cde785_KL+Divergence.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/e0cf64c6-71b5-4c3f-abc3-4f3769cde785_KL+Divergence.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/746ef267-ecdf-405c-9393-90bceb1f9a36_Synthetic+data.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/746ef267-ecdf-405c-9393-90bceb1f9a36_Synthetic+data.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/11b9026c-edc4-4d23-b6f3-09bd0ede3e28_image+%2835%29+2.jpg?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/11b9026c-edc4-4d23-b6f3-09bd0ede3e28_image+%2835%29+2.jpg?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/4586fc5c-4788-47b5-a443-8ef3bb5155c2_Llama+v2.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/4586fc5c-4788-47b5-a443-8ef3bb5155c2_Llama+v2.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/959d564b-c80a-499f-86b4-b248e804d4eb_image9.gif?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/959d564b-c80a-499f-86b4-b248e804d4eb_image9.gif?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/703be18a-dd25-4306-a7b5-e3873d4a25ce_F1+Blog+Banner.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/703be18a-dd25-4306-a7b5-e3873d4a25ce_F1+Blog+Banner.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/f5e1a224-fa73-4e58-984d-deb579885fc6_contrastive+learning+banner.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/f5e1a224-fa73-4e58-984d-deb579885fc6_contrastive+learning+banner.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/c0657d28-5216-4170-9b91-3ca0fe9ec2c1_Semantic+Seg.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/c0657d28-5216-4170-9b91-3ca0fe9ec2c1_Semantic+Seg.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/29257b2c-bcdf-454d-8d32-4b3221eabeaa_Guide+to+RLHF.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/29257b2c-bcdf-454d-8d32-4b3221eabeaa_Guide+to+RLHF.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/8dd324c7-4f12-4c52-95fa-511499a88cb9_image+%2812%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/8dd324c7-4f12-4c52-95fa-511499a88cb9_image+%2812%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/1f526e45-d0ed-495d-b19f-c7b97725f6f0_image+%2813%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/1f526e45-d0ed-495d-b19f-c7b97725f6f0_image+%2813%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/ba91424d-edd4-4cfe-84b7-94e6e8e1f131_image+%2811%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/ba91424d-edd4-4cfe-84b7-94e6e8e1f131_image+%2811%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/2f442132-bc37-48dd-af91-302b083d04bb_image3.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/2f442132-bc37-48dd-af91-302b083d04bb_image3.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/3147e4cb-22e3-4d8d-a84c-3407186b5e4b_Frame+1000001737.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/3147e4cb-22e3-4d8d-a84c-3407186b5e4b_Frame+1000001737.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/94455529-c0f1-4f5f-a790-ec363800c7fb_image+%289%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/94455529-c0f1-4f5f-a790-ec363800c7fb_image+%289%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/a50a93cc-595a-411f-85a7-0ae8418f379c_image%284%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/a50a93cc-595a-411f-85a7-0ae8418f379c_image%284%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/88cd5fa0-e8e9-49e1-a6e1-a89aa4c4f7ca_vector+similarity+search.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/88cd5fa0-e8e9-49e1-a6e1-a89aa4c4f7ca_vector+similarity+search.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/54de7c45-c97d-4c87-8f1d-118e509dfa97_1.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/54de7c45-c97d-4c87-8f1d-118e509dfa97_1.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/c66851e9-9b1c-4137-874c-9a9e06a8cd84_How+to+Measure+Model+Performance+in+Computer+Vision+A+Comprehensive+Guide.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/c66851e9-9b1c-4137-874c-9a9e06a8cd84_How+to+Measure+Model+Performance+in+Computer+Vision+A+Comprehensive+Guide.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/b2fc8322-f612-4d59-b261-cc1749d7001f_image2.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/b2fc8322-f612-4d59-b261-cc1749d7001f_image2.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/a82e415e-a411-453c-8309-250c0c9a2011_Data+refinement+strategies.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/a82e415e-a411-453c-8309-250c0c9a2011_Data+refinement+strategies.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/6a1e49ec-5e0f-42ae-b930-9289b01a1471_Best+Video+Annotation+tools.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/6a1e49ec-5e0f-42ae-b930-9289b01a1471_Best+Video+Annotation+tools.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/df774eb1-d9df-4767-b152-18f3f56f9d57_Thumbnail+ImageBind+Explained.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/df774eb1-d9df-4767-b152-18f3f56f9d57_Thumbnail+ImageBind+Explained.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/2b3bbc92-bb9e-4adb-8cec-0c46b59be923_self-supervised+learning+explained.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/2b3bbc92-bb9e-4adb-8cec-0c46b59be923_self-supervised+learning+explained.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/00c9db11-7475-4752-8077-aef48eeac950_VFMs+explained+%281%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/00c9db11-7475-4752-8077-aef48eeac950_VFMs+explained+%281%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/362b5e92-1eb2-46a5-91f7-77541e148fbd_DINOv2+self-supervised+learning_+img1.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/362b5e92-1eb2-46a5-91f7-77541e148fbd_DINOv2+self-supervised+learning_+img1.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/7d9206e9-0d93-4737-99e3-b873faf9a062_full+guide+to+foundation+models.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/7d9206e9-0d93-4737-99e3-b873faf9a062_full+guide+to+foundation+models.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/0d886cbe-db86-4006-b509-554359d6b2c0_Object+Detection_.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/0d886cbe-db86-4006-b509-554359d6b2c0_Object+Detection_.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/41884f66-9209-48a8-96e0-c3b754ecf446_SegGPT-thumbnail.+%282%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/41884f66-9209-48a8-96e0-c3b754ecf446_SegGPT-thumbnail.+%282%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/c906cc2b-0bdf-4722-9b59-18af3e952c97_Segment-all-models-thumbnail..png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/c906cc2b-0bdf-4722-9b59-18af3e952c97_Segment-all-models-thumbnail..png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/7df045db-935f-443c-8693-70988a033754_344167390_A_dream_of_a_giant_photorealistic_ginger_cat_is__playing_in_the_road_stopping_traffic_photographed_by_David_LaChapelle_.webp?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/7df045db-935f-443c-8693-70988a033754_344167390_A_dream_of_a_giant_photorealistic_ginger_cat_is__playing_in_the_road_stopping_traffic_photographed_by_David_LaChapelle_.webp?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/e3dfc705-deaf-46a6-96e1-d86c184fee32_One+Shot+Learning+Blog+Header.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/e3dfc705-deaf-46a6-96e1-d86c184fee32_One+Shot+Learning+Blog+Header.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/b1d66c7a-52be-467f-b6e6-c354a927f83a_HAL_closeup.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/b1d66c7a-52be-467f-b6e6-c354a927f83a_HAL_closeup.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/f7c346f4-7ef4-4262-8071-dca0765b5468_image%284%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/f7c346f4-7ef4-4262-8071-dca0765b5468_image%284%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/fc12e910-7c35-4181-90d5-a8f6d6544eab_Data_curation_thumbnail.webp?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/fc12e910-7c35-4181-90d5-a8f6d6544eab_Data_curation_thumbnail.webp?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/1fac39a2-d52d-40d6-a5e5-7e54ddf0fed0_5+Bias.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/1fac39a2-d52d-40d6-a5e5-7e54ddf0fed0_5+Bias.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/a04dd173-3f60-4301-ab9a-86f66145ebb5_piret-ilver-98MbUldcDJY-unsplash.jpg?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/a04dd173-3f60-4301-ab9a-86f66145ebb5_piret-ilver-98MbUldcDJY-unsplash.jpg?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/a93e02b6-9a7d-4d76-bb74-9726b2a07661_Car.webp?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/a93e02b6-9a7d-4d76-bb74-9726b2a07661_Car.webp?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/2bc7fa42-e2e9-4128-a624-e1a4f6134b00_Frame+487.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/2bc7fa42-e2e9-4128-a624-e1a4f6134b00_Frame+487.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/d2c5c481-9603-4046-94ae-1d061b5d1aeb_4+things+to+do.webp?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/d2c5c481-9603-4046-94ae-1d061b5d1aeb_4+things+to+do.webp?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/ee5778d3-a7de-418f-955a-be59ce078f04_Data+Approximation.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/ee5778d3-a7de-418f-955a-be59ce078f04_Data+Approximation.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/5a676cee-9d13-480d-96fb-ac788de33034_data+augmentation.webp?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/5a676cee-9d13-480d-96fb-ac788de33034_data+augmentation.webp?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/1385e64d-ff53-4f15-b46c-2d4b434413c2_Object+Tracking.webp?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/1385e64d-ff53-4f15-b46c-2d4b434413c2_Object+Tracking.webp?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/e7c86d33-b673-4ace-b870-15b7cbd55a55_Startup.webp?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/e7c86d33-b673-4ace-b870-15b7cbd55a55_Startup.webp?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/5068d72b-3862-43e4-9cbb-042fba220ff6_6+steps+to+models.webp?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/5068d72b-3862-43e4-9cbb-042fba220ff6_6+steps+to+models.webp?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/e581a6d9-82eb-4c07-967d-3b63f8878eba_Introduction+to+synthetic+training+data.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/e581a6d9-82eb-4c07-967d-3b63f8878eba_Introduction+to+synthetic+training+data.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/900afe57-9cc1-4662-a27b-fdd8fa4d6ca6_Sythetic+Training+Data.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/900afe57-9cc1-4662-a27b-fdd8fa4d6ca6_Sythetic+Training+Data.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/b974b751-f42d-40cc-9c48-15851bc2577b_2700.jpg?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/b974b751-f42d-40cc-9c48-15851bc2577b_2700.jpg?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/59553dbd-c28f-45f3-b374-427840480fc5_1000.jpg?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/59553dbd-c28f-45f3-b374-427840480fc5_1000.jpg?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/818c6d29-a479-4fed-8f1c-30d101fa00b1_3100.webp?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/818c6d29-a479-4fed-8f1c-30d101fa00b1_3100.webp?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/201d1f7d-b74f-4804-bfc4-5c5731fb61f1_Mirco-models.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/201d1f7d-b74f-4804-bfc4-5c5731fb61f1_Mirco-models.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/2ae70f11-966d-4c1e-ac11-ce842ecfeeac_6368ddfad96697ba16a6b234_Best+Datasets-p-800.webp?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/2ae70f11-966d-4c1e-ac11-ce842ecfeeac_6368ddfad96697ba16a6b234_Best+Datasets-p-800.webp?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/57bd343a-7e54-4653-a716-f8fbd88d1afc_image+%284%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://images.prismic.io/encord/57bd343a-7e54-4653-a716-f8fbd88d1afc_image+%284%29.png?auto=compress%2Cformat&fit=max&w=368&h=270",
            "https://cdn.drata.com/badge/soc2-dark.png",
            "https://images.prismic.io/encord/d5a5f02e-d8df-49c2-9413-5633a8e75e7d_soc2-certificate.png?auto=compress,format",
            "https://encord.cdn.prismic.io/encord/ZoZ1tR5LeNNTwyYw_g22024.svg",
            "https://dc.ads.linkedin.com/collect/?pid=4241362&fmt=gif"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Alexandre Bonnet"
        ],
        "publish_date": "2023-09-15T09:10:59+00:00",
        "summary": "",
        "meta_description": "Explore Out-of-Distribution (OOD) detection in AI: its significance, challenges, and the latest research trends. Dive deep into why modern AI models face OOD issues and the cutting-edge techniques being developed to tackle them. A must-read for AI enthusiasts and professionals.",
        "meta_lang": "en",
        "meta_favicon": "/apple-touch-icon.png",
        "meta_site_name": "",
        "canonical_link": "https://encord.com/blog/what-is-out-of-distribution-ood-detection/",
        "text": "Inter-rater Reliability: Definition, Examples, Calculation\n\nInter-rater reliability measures the agreement between two or more raters or observers when assessing subjects. This metric ensures that the data collected is consistent and reliable, regardless of who is collects or analyzes it. The significance of inter-rater reliability cannot be overstated, especially when the consistency between observers, raters, or coders is paramount to the validity of the study or assessment. Inter-rater reliability refers to the extent to which different raters or observers give consistent estimates of the same phenomenon. It is a measure of consistency or agreement between two or more raters. On the other hand, intra-rater reliability measures the consistency of ratings given by a single rater over different instances or over time. In research, inter-rater reliability is pivotal in ensuring the validity and reliability of study results. In qualitative research, where subjective judgments are often required, having a high degree of inter-rater reliability ensures that the findings are not merely the result of one individual's perspective or bias. Instead, it confirms that multiple experts view the data or results similarly, adding credibility to the findings.1 Moreover, in studies where multiple observers are involved, inter-rater reliability helps standardize the observations, ensuring that the study's outcomes are not skewed due to the variability in observations. Methods to Measure Inter-rater Reliability Inter-rater reliability, often called IRR, is a crucial statistical measure in research, especially when multiple raters or observers are involved. It assesses the degree of agreement among raters, ensuring consistency and reliability in the data collected. Various statistical methods have been developed to measure it, each with unique advantages and applications.1 Cohen's Kappa Cohen's Kappa is a widely recognized statistical method used to measure the agreement between two raters. It considers the possibility of the agreement occurring by chance, providing a more accurate measure than a simple percentage agreement. The Kappa statistic ranges from -1 to 1, where 1 indicates perfect agreement, 0 suggests no better agreement than chance, and -1 indicates complete disagreement.2 The formula for calculating Cohen's Kappa is: Where: \\( p_o \\) is the observed proportion of agreement \\( p_e \\) is the expected proportion of agreement Using Cohen's Kappa is essential when the data is categorical, and raters may agree by chance. It provides a more nuanced understanding of the reliability of raters. Intraclass Correlation Coefficient (ICC) The Intraclass Correlation Coefficient, commonly known as ICC, is another method used to measure the reliability of measurements made by different raters. It's beneficial when the measurements are continuous rather than categorical. ICC values range between 0 and 1, with values closer to 1 indicating higher reliability. One of the main differences between ICC and Cohen's Kappa is their application. While Cohen's Kappa is best suited for categorical data, ICC is ideal for continuous data. Additionally, ICC can be used for more than two raters, making it versatile in various research settings. Percentage Agreement Percentage agreement is the simplest method to measure inter-rater reliability. It calculates the proportion of times the raters agree without considering the possibility of chance agreement. While it's straightforward to compute, it doesn't provide as nuanced a picture as methods like Cohen's Kappa or ICC. For instance, if two raters agree 85% of the time, the percentage agreement is 85%. However, this method doesn't account for agreements that might have occurred by chance, making it less robust than other methods. Despite its simplicity, it is essential to be cautious when using percentage agreement, especially when the stakes are high, as it might provide an inflated sense of reliability. Factors Affecting Inter-rater Reliability Inter-rater reliability (IRR) is a crucial metric in research methodologies, especially when data collection involves multiple raters. It quantifies the degree of agreement among raters, ensuring that the data set remains consistent across different individuals. However, achieving a high IRR, such as a perfect agreement, is difficult. Several factors can influence the consistency between raters, and comprehending these can aid in enhancing the reliability measures of the data. Rater Training One of the most important factors affecting IRR is the training of raters. Proper training can significantly reduce variability and increase the coefficient of inter-rater agreement. For instance, in Krippendorff's study (2011) study, raters trained using a specific methodology exhibited a Cohen’s Kappa value of 0.85, indicating a high level of agreement, compared to untrained raters with a kappa value of just 0.5.4 Training ensures that all raters understand the rating scale and the criteria they are evaluating against. For example, in clinical diagnoses, raters can be trained using mock sessions where they are presented with sample patient data. Feedback sessions after these mock ratings can pinpoint areas of disagreement, offering a chance to elucidate and refine the methodology. Training and clear guidelines are not just best practices; they're essential. They bridge the gap between subjective judgments and objective evaluations, ensuring research remains unbiased and true to its purpose. Clarity of Definitions The clarity of definitions in the rating process is pivotal. Providing raters with unambiguous definitions, such as elucidating the difference between intra-rater and inter-rater reliability or explaining terms like \"percent agreement\" versus \"chance agreement,\" ensures consistency. For example, in a research method involving the assessment of academic papers, if \"originality\" isn't clearly defined, raters might have divergent interpretations. A clear definition of terms in a study involving Krippendorff’s alpha as a reliability measure increased the alpha value from 0.6 to 0.9, indicating a higher degree of agreement.5 Defining the time frame between tests can lead to more consistent results in test-retest reliability assessments. Subjectivity in Ratings Subjectivity, especially in ordinal data, can significantly impede achieving a high IRR. For instance, in a data collection process involving movie reviews, two raters might have different thresholds for what constitutes a \"good\" film, leading to varied ratings. A Pearson correlation study found that when raters were given a clear guideline, the coefficient increased by 20%.6 To curtail subjectivity, it's imperative to have explicit guidelines. Tools like Excel for data analysis can help visualize areas of high variability. Moreover, employing reliability estimates like Fleiss Kappa or Cronbach's alpha can provide a clearer picture of the degree of agreement. For instance, a Fleiss Kappa value closer to 1 indicates high inter-rater reliability. While tools like the kappa statistic, intra-class correlation coefficient, and observed agreement offer quantifiable metrics, the foundation of high IRR lies in rigorous training, precise definitions, and minimizing subjectivity. Practical Applications and Examples of Inter-rater Reliability Inter-rater reliability (IRR) is used in various research methods to ensure that multiple raters or observers maintain consistency in their assessments. This measure often quantified using metrics such as Cohen’s Kappa or the intra-class correlation coefficient, is paramount when subjective judgments are involved. Let's explore the tangible applications of inter-rater reliability across diverse domains. Clinical Settings In clinical research, IRR is indispensable. Consider a scenario where a large-scale clinical trial is underway. Multiple clinicians collect data, assessing patient responses to a new drug. Here, the level of agreement among raters becomes critical. The trial's integrity is compromised if one clinician records a side effect while another overlooks it. In such settings, metrics like Fleiss Kappa or Pearson's correlation can quantify the degree of agreement among raters, ensuring that the data set remains consistent.7 Furthermore, in diagnoses, the stakes are even higher. A study revealed that when two radiologists interpreted the same X-rays without a standardized rating scale, their diagnoses had a variability of 15%. However, clear guidelines and training reduced the variability to just 3%, showcasing the power of high inter-rater reliability in clinical settings. Social Sciences Social sciences, with their inherent subjectivity, lean heavily on IRR. Multiple researchers conducted observational studies in a study exploring workplace dynamics in English corporate culture. Using tools like Excel for data analysis, the researchers found that the observed agreement among raters was a mere 60% without established guidelines. However, post-training and with clear definitions, the agreement soared to 90%, as measured by Krippendorff’s alpha.9 Education Education, a sector shaping future generations, cannot afford inconsistencies. Consider grading, a process fraught with subjectivity. In a study involving multiple teachers grading the same set of papers, the initial score variability was 20%. However, after a rigorous training session and with a standardized rating scale, the variability plummeted to just 5%.10 Standardized tests are the gateways to numerous opportunities, especially relying on IRR. A disparity in grading can alter a student's future. For instance, a test-retest reliability study found that scores varied by as much as 15 points on a 100-point scale without ensuring inter-rater agreement. Such inconsistencies can differentiate between a student getting their dream opportunity or missing out.10 Inter-rater reliability, quantified using metrics like the kappa statistic, Cronbach's alpha, or the intra-rater reliability measure, is non-negotiable across domains. Whether it's clinical trials, anthropological studies, or educational assessments, ensuring consistency among raters is not just a statistical necessity; it's an ethical one. Inter-rater Reliability: Key Takeaways Inter-rater reliability (IRR) is a cornerstone in various research domains, ensuring that evaluations, whether from clinical diagnoses, academic assessments, or qualitative studies, are consistent across different raters. Its significance cannot be overstated, as it safeguards the integrity of research findings and ensures that subjective judgments don't skew results. IRR is a litmus test for data reliability, especially when multiple observers or raters are involved. The call to action for researchers is clear: rigorous training and comprehensive guidelines for raters are non-negotiable. Ensuring that raters are well-equipped, both in terms of knowledge and tools, is paramount. It's not just about achieving consistent results; it's about upholding the sanctity of the research process and ensuring that findings are valid and reliable. Future Directions As we look ahead, the landscape of inter-rater reliability is poised for evolution. With technological advancements, there's potential for more sophisticated methods to measure and ensure IRR. Software solutions equipped with artificial intelligence and machine learning capabilities might soon offer tools that can assist in training raters, providing real-time feedback, and even predicting areas of potential disagreement. Moreover, as research methodologies become more intricate, the role of technology in aiding the process of ensuring IRR will undoubtedly grow. The future holds promise, from virtual reality-based training modules for raters to advanced statistical tools that can analyze inter-rater discrepancies in real time. For researchers and professionals alike, staying abreast of these advancements will ensure their work remains at the forefront of reliability and validity. In conclusion, while the principles of inter-rater reliability remain steadfast, the tools and methods to achieve it are ever-evolving, promising a future where consistency in evaluations is not just hoped for but assured.\n\nSep 01 2023\n\n5 M\n\nData Curation in Computer Vision\n\nIn recent years, the explosion in data volume, which soared to around 97 zettabytes globally in 2022 and is projected to reach over 181 zettabytes by 2025, has been a boon for the fields of artificial intelligence (AI) and machine learning (ML). These fields thrive on large datasets to generate more accurate results. Extracting value from such vast amounts of data, however, is often challenging. High-quality data is necessary to produce good results, particularly for AI systems powered by sophisticated computer vision (CV) algorithms and foundation models. Since CV models typically process unstructured data consisting of thousands of images, managing datasets effectively becomes crucial. One aspect of data management that’s essential is data curation. When you incorporate data curation as part of your workflow, you can check for data quality issues, missing values, data distribution drift, and inconsistencies across your datasets that could impact the performance of downstream computer vision and machine learning models. Data curation also helps you efficiently select edge cases, scenarios that are unlikely but could have significant implications if overlooked. For instance, a model for self-driving cars trained on images of roads and alleys under normal weather conditions may fail to recognize critical objects under extreme weather. Therefore, curating a dataset to include images across a spectrum of weather conditions is essential to account for such edge cases. In this article, you will learn about data curation in detail, explore its challenges, and understand how it helps improve CV model performance. We’ll also specifically discuss the role of data annotation in curating computer vision datasets and how you can use the Encord platform for data curation. Ready? Let’s dive right in! 🏊 What is Data Curation? When you “curate data,” it simply means collecting, cleaning, selecting, and organizing data for your ML models so downstream consumers can get complete, accurate, relevant, and unbiased data for training and validating models. It’s an iterative process that you must follow even after model deployment to ensure the incoming data matches the data in the production environment. Data curation differs from data management—management is a much broader concept involving the development of policies and standards to maintain data integrity throughout the data lifecycle. What are the steps involved in data curation? Let’s explore them below! Data Collection The data curation phase starts with collecting data from disparate sources, public or proprietary databases, data warehouses, or web scraping. Data Validation After collection, you can validate your data using automated pipelines to check for accuracy, completeness, relevance, and consistency. Data Cleaning Then, clean the data to remove corrupted data points, outliers, incorrect formats, duplicates, and other redundancies to maintain data quality. Want to learn more about data cleaning? Read our comprehensive guide on Mastering Data Cleaning and Data Preprocessing. Normalization Next is normalization, which involves re-scaling data values to a standard range and distribution that aid algorithms that are sensitive to input scales, thus preventing skews in learned weights and coefficients. De-identification It is a standard method of removing personally identifiable information from datasets, such as names, social security numbers (SSNs), and contact information. Data Transformation You can build automated pipelines to transform data into meaningful features for better model training. Feature engineering is a crucial element in this process. It allows data teams to find relevant relationships between different columns and turn them into features that help explain the target variable. Data Augmentation Data augmentation introduces slight dataset variations to expand data volume and scenario coverage. You can use image operations like crop, flip, zoom, rotate, pan, and scale to enhance computer vision datasets. Data augmentation example Note that augmented data differs from synthetic data. Synthetic data is computer-generated fake data that resembles real-world data. Typically, it is generated using state-of-the-art generative algorithms. On the other hand, augmented data refers to variations in training data, regardless of how it is generated. Data Sampling Data sampling refers to using a subset of data to train AI models. However, this may introduce bias during model training since we select only a specific part of the dataset. Such issues can be avoided through probabilistic sampling techniques like random, stratified, weighted, and importance sampling. You can read our complete guide on How to Mitigate Bias in Machine Learning Models to learn how to reduce bias in models efficiently. Data Partitioning The final step in data curation is data partitioning. This involves dividing data into training, validation, and test sets. The model uses the training datasets to learn patterns and compute coefficients or weights. During training, the model’s performance is tested on a validation set. If the model performs poorly during validation, it can be adjusted by fine-tuning its hyper-parameters. Once you have satisfactory performance on the validation set, the test set is used to assess critical performance metrics, such as accuracy, precision, F1 score, etc., to see if the model is ready for deployment. While there’s no one fixed way of splitting data into train, test, and validation sets, you can use the sampling methods described in the previous section to ensure that each dataset represents the population in a balanced manner. Doing so ensures your model doesn’t suffer from underfitting or overfitting. Get a deeper understanding of training, validation, and test sets by reading the article on Training, Validation, Test Split for Machine Learning Datasets. Data Curation in Computer Vision While the above data curation steps generally apply to machine learning, the curation process involves more complexity when preparing data for computer vision tasks. First, let’s list the common types of computer vision tasks and then discuss annotation, a critical data curation step. Common Types of Computer Vision Tasks Object Detection This task is for when you want to identify specific objects within your images. Below is an example of a butterfly detected with bounding boxes around the object, including a classification of the species of the butterfly “Ringlet.” Object detection example in Encord Annotate. Interested in reading more about object detection? Head to our blog to read Object Detection: Models, Use Cases, and Examples. Image Classification Image classification models predict whether an object exists in a given image based on the patterns they learn from the training data. For instance, an animal classifier would label the below image as “Crab” if the classifier had been trained on a good sample of crab images. “Walking Crab” classification in Encord Active. Face Recognition Facial recognition tasks involve complex convolutional neural nets (CNNs) to learn intricate facial patterns and recognize faces in images. Semantic Segmentation You can identify each pixel of a given object within an image through semantic segmentation. For instance, the image below illustrates how semantic segmentation distinguishes between several elements in a given image on a pixel level. Semantic segmentation example Text-to-Image Generative Models Generating images from text is a new development in the generative AI space that involves writing text-based input prompts to describe the type of image you want. The generative model processes the prompt and produces suitable images that match the textual description. Several proprietary and open-source models, such as Midjourney, Stable Diffusion, Craiyon, DeepFloyd, etc., are recent examples that can create realistic photos and artwork in seconds. Role of Data Annotation In Curating Computer Vision Data Computer vision tasks require careful data annotation as part of the data curation process to ensure that models work as expected. Data annotation refers to labeling images (typically in the training data) so the model knows the ground truth for accurate predictions. Let’s explore a few annotation techniques below. Bounding Box: The technique annotates a bounding box around the object of interest for image classification and object detection tasks. An example of bounding box annotation within Encord Annotate. Landmarking: In landmarking, the objective is to annotate individual features within an image. It’s suitable for facial recognition tasks. An example of landmarking to label different facial features Tracking: Tracking is useful for annotating moving objects across multiple images. An example of tracking a moving car label within Encord. General Considerations for Annotating Image Data Data annotation can be time-consuming as it requires considerable effort to label each image or object within an image. It’s advisable to clearly define standard naming conventions for labeling to ensure consistency across all images. You can use labeled data from large datasets, such as ImageNet, which contains over a million training images across 1,000 object classes. It is ideal for building a general-purpose image classification model. Also, it’s important to develop a robust review process to identify annotation errors before feeding the data to a CV model. Leveraging automation in the annotation workflow reduces the time to identify those errors, as manual review processes are often error-prone and costly. Moreover, your team can employ innovative methods like active learning and image embeddings to improve data annotation accuracy. Let’s look at them briefly below. Active Learning Instead of labeling all the images in a dataset, an active learning workflow allows you to annotate only a few valuable images and use them for training. It uses an informativeness score that helps decide which image will be most beneficial to improving performance. For example, in a given dataset containing 1,500 images, the active learning method identifies the most valuable data samples (let’s say 100 images) for annotation, allowing you to train your ML model on a subset of labeled images and validate it on the remaining unlabeled 1,400 images. Metric performance explorer in Encord Active. You can use a data and model evaluation tool like Encord Active to assign confidence scores to the 1,400 images and send them upstream for data annotators on Encord Annotate to cross-check images with the lowest scores and re-label them manually. Through this, active learning reduces data annotation time and can significantly improve the performance of your computer vision models. Interested in learning more about active learning? Read our detailed Practical Guide to Active Learning for Computer Vision Image Embeddings Image embeddings are vectorized versions of image data where similar images have similar numerical vector representations. Data embeddings plot in Encord Active. Typically, image embeddings are helpful for semantic segmentation tasks as they break down an image into relevant vectors, allowing computer vision models to classify pixels more accurately. They also help with facial recognition tasks by representing each facial feature as a number in the vector. The model can better use the vectorized form to distinguish between several facial structures. Embeddings make it easier for algorithms to compute how similar two or more images are numerically. It helps practitioners annotate images more accurately. Lastly, image embeddings are the backbone of generative text-to-image models, where practitioners can convert text-image pairs into embeddings. For example, you can have the text “image of a dog” and an actual dog’s image paired together and converted into an embedding. You can pass such embeddings as input to a generative model so it learns to create a dog’s image when it identifies the word “Dog” in a textual prompt. Challenges in Data Curation Data provides the foundation for building high-quality machine learning models. However, collecting relevant data comes with several challenges. Evolving Data Landscape: With the rapid rise of big data, maintaining consistent and accurate data across time and platforms is challenging. Data distributions can change quickly as more data comes in, making data curation more difficult. Data Security Concerns: Edge computing is giving rise to security issues as organizations must ensure data collection from several sources is secure. It calls for robust encryption and de-identification strategies to protect private information and maintain data integrity throughout curation. Data Infrastructure and Scalability: It’s difficult for organizations to develop infrastructure for handling the ever-increasing scale of data and ML applications. The exponential rise in data volume is causing experts to shift from code-based strategies to data-centric AI, primarily focusing on building models that help with data exploration and analysis. Data Scarcity: Mission-critical domains like healthcare often need more high-quality data sources. This makes it difficult for you to curate data and build accurate models. Models built using low-quality data can more likely give false positives, which is why expert human supervision is required to monitor the outcomes of such models. Using Encord Index for Data Curation Encord’s end-to-end training data platform enables you to curate and manage data. The platform you can quickly identify and rectify data quality issues, manage metadata, and leverage automated workflows to streamline the curation process. Features such as vector embeddings, AI-assisted metrics, and model predictions help in finding labeling errors quickly, ensuring high-quality datasets for training. In this section, you will see how the different stages of the data curation workflow work in Encord. Data Annotation and Validation After collecting your dataset, you need to annotate it and validate the quality of your annotations and images. For this stage, Encord Annotate supports all key annotation types, such as bounding boxes, polygons, polylines, image segmentation, and more, across various visual formats. Polygon and bounding box annotations in Encord Annotate. It includes auto-annotation features such as Meta’s Segment Anything Model and other AI-assisted labeling techniques that can aid your annotation process and reduce the chances of annotation errors occurring. Annotate provides data integrations into popular storage services and warehouses, so you do not have to worry about moving your data. Annotate’s quality assessment toolkit helps scale your data validation processes by spotting hidden errors in your training dataset. You can also use Annotate to automatically find classification and geometric errors in your training data, ensuring that your labels are of the highest possible quality before they go into production. The illustration below shows what you can expect the annotation and validation workflows to look like with Encord: Annotation and validation workflow with Encord. Data Cleaning With Encord Active, you can refine the dataset by efficiently identifying and removing duplicate entries, for example. How does it do this? Active computes the image embeddings and uses algorithms to evaluate the dataset based on objective quality metrics like “Uniqueness,” “Area,” “Contrast,” and so on. Try Below In this example, you will identify outlier images using the embeddings view, use the \"similarity search\" to find similar objects, select multiple images and then add to a collection. Normalization While curating your data, you might want to adjust your values on a consistent scale. In color images, each pixel has three values (one for each of the red, green, and blue channels), usually ranging from 0 to 255. Normalization rescales these pixel values to a new range. Exploring your data distribution provides a clear lens to understand the images you want to normalize to a standard range, often 0 to 1 or -1 to 1. In the workflow below, you can see the distribution of Red, Blue, and Green pixel values across an entire image set. Metric distribution in Encord Active. De-identification Safeguarding sensitive information is fundamental to building trust and ensuring the ethical use of data in machine learning and computer vision applications. Active can aid the de-identification process by allowing you to identify images through textual prompts that likely contain Personally Identifiable Information (PII). Annotate can help you anonymize or de-identify PII programmable from the SDK. Finding human faces in Encord Active. Data curation is a critical determinant of the success of computer vision projects as businesses increasingly depend on AI for better user applications and efficient business operations. However, the complexity and challenges of data curation, especially in computer vision, call for the right tools to streamline this process. The Encord platform provides the tools to curate and manage your data pipelines. Data Curation: Key Takeaways As companies gravitate more toward AI to solve business problems using complex data, the importance of data curation will increase significantly. The points below are critical considerations organizations must make to build a successful data curation workflow. Data curation is a part of data management. As such, data curation in isolation may only solve a part of the problem. Encord Index offers robust tools for holistic data curation within AI workflows. The curation workflow must suit specific requirements. A workflow that works for a particular task may fail to produce results for another. Encord allows you to customize and automate your data curation workflow for any vision use case. Encord Index enhances data management by offering powerful tools for organizing, visualizing, and ensuring data The right combination of data curation tools can accelerate the development of high-quality training data. Encord Annotate provides features to label visual data and manage large-scale annotation teams using customizable workflows and quality control tools. With Encord Active, you can find failure modes, surface poor-quality data, and evaluate your model’s performance. Data curation is an ongoing process, and each organization must commit to robust data curation practices throughout the model building, deployment, and monitoring stages while continuing to improve the curation workflow as data evolves.\n\nNov 02 2023\n\n6 M\n\nML Monitoring vs. ML Observability\n\nPicture this: you've developed an ML system that excels in domain X, performing task Y. It's all smooth sailing as you launch it into production, confident in its abilities. But suddenly, customer complaints start pouring in, signaling your once-stellar model is now off its game. Not only does this shake your company's reputation, but it also demands quick fixes. The catch? Sorting out these issues at the production level is a major headache. What could've saved the day? Setting up solid monitoring and observability right from the get-go to catch anomalies and outliers before they spiral out of control. Fast forward to today, AI and ML are everywhere, revolutionizing industries by extracting valuable insights, optimizing operations, and guiding data-driven decisions. However, these advancements necessitate a proactive approach to ensure timely anomaly and outlier detection for building reliable, efficient, and transparent models. That's where ML monitoring and observability come to the rescue, playing vital roles in developing trustworthy AI systems. In this article, we'll uncover the crucial roles of ML monitoring and ML observability in crafting dependable AI-powered systems. We'll explore their key distinctions and how they work together to ensure reliability in AI deployments. What is ML Monitoring? Machine learning monitoring refers to the continuous and systematic process of tracking a machine learning model’s performance, behavior, and health from development to production. It encompasses collecting, analyzing, and interpreting various metrics, logs, and data generated by ML systems to ensure optimal functionality and prompt detection of potential issues or anomalies. ML Monitoring Framework ML monitoring detects and tracks: Metrics like accuracy, precision, recall, F1 score, etc. Changes in input data distributions, known as data drift Instances when a model's performance declines or degrades Anomalies and outliers in model behavior or input data Model latency Utilization of computational resources, memory, and other system resources Data quality problems in input datasets, such as missing values or incorrect labels, that can negatively impact model performance Bias and fairness of models Model versions Breaches and unauthorized access attempts Interested in learning more about bias in machine learning models? Read our comprehensive guide on Mitigating Bias In Machine Learning Models Objectives A Machine Learning Model Monitoring Checklist ML monitoring involves the continuous observation, analysis, and management of various aspects of ML systems to ensure they are functioning as intended and delivering accurate outcomes. It primarily focuses on the following: Model Performance Tracking: Helps machine learning practitioners and stakeholders understand how well a model fares with new data and whether the predictive accuracy aligns with the intended goals. Early Anomaly Detection: Involves continuously analyzing model behavior to promptly detect any deviations from expected performance. This early warning system helps identify potential issues, such as model degradation, data drift, or outliers, which, if left unchecked, could lead to significant business and operational consequences. Root-Cause Analysis: Identifying the fundamental reason behind issues in the model or ML pipeline. This enables data scientists and ML engineers to pinpoint the root causes of problems, leading to more efficient debugging and issue resolution. Diagnosis: Examines the identified issues to understand their nature and intricacies, which assists in devising targeted solutions for smoother debugging and resolution. Model Governance: Establishes guidelines and protocols to oversee model development, deployment, and maintenance. These guidelines ensure ML models are aligned with organizational standards and objectives. Compliance: Entails adhering to legal and ethical regulations. ML monitoring ensures that the deployed models operate within defined boundaries and uphold the necessary ethical and legal standards. Check out our curated list of Top Tools for Outlier Detection in Computer Vision. Importance of Monitoring in Machine Learning ML monitoring is significant for several reasons, including: Proactive Anomaly Resolution: Early detection of anomalies through ML monitoring enables data science teams to take timely actions and address issues before they escalate. This proactive approach helps prevent significant business disruptions and customer dissatisfaction, especially in mission-critical industries. Data Drift Detection: ML monitoring helps identify data drift, where the input data distribution shifts over time. If a drift is detected, developers can take prompt action to update and recalibrate the model, ensuring its accuracy and relevance to the changing data patterns. Continuous Improvement: ML Monitoring ensures iterative model improvement by providing feedback on model behavior. This feedback loop supports refining ML algorithms and strategies, leading to enhanced model performance over time. Risk Mitigation: ML monitoring helps mitigate risks associated with incorrect predictions or erroneous decisions, which is especially important in industries such as healthcare and finance, where model accuracy is critical. Performance Validation: Monitoring provides valuable insights into model performance in production environments, ensuring that they continue to deliver reliable results in real-word applications. To achieve this, monitoring employs various techniques, such as cross-validation and A/B testing, which help assess model generalization and competence in dynamic settings. What is ML Observability? ML Observability Machine learning observability is an important practice that provides insights into the inner workings of ML data pipelines and system well-being. It involves understanding decision-making, data flow, and interactions within the ML pipeline. As ML systems become more complex, so does observability due to multiple interacting components such as data pipelines, model notebooks, cloud setups, containers, distributed systems, and microservices. According to Gartner, by 2026, 70% of organizations effectively implementing observability will attain quicker decision-making, granting a competitive edge in business or IT processes. ML observability detects: Model behavior during training, inference, and decision-making processes Data flow through the ML pipeline, including preprocessing steps and data transformations Feature importance and their contributions to model predictions Model profiling Model performance metrics, such as accuracy, precision, recall, and F1 score Utilization of computational resources, memory, and processing power by ML models Bias in ML models Anomalies and outliers in model behavior or data. Model drift, data drift, and concept drift occur when model behavior or input data changes over time Overall performance of the ML system, including response times, latency, and throughput Model error analysis Model explainability and interpretability Model versions and their performance using production data Objectives The primary objectives of ML observability are: Transparency and Understandability: ML observability aims to provide transparency into the black-box nature of ML models. By gaining a deeper understanding of model behavior, data scientists can interpret model decisions and build trust in the model's predictions. Root Cause Analysis: ML observability enables thorough root cause analysis when issues arise in the ML pipeline. By tracing back the sequence of events and system interactions, ML engineers can pinpoint the root causes of problems and facilitate effective troubleshooting. Data Quality Assessment: ML observability seeks to monitor data inputs and transformations to identify and rectify data quality issues that may adversely affect model performance. Performance Optimization: With a holistic view of the system's internal dynamics, ML observability aims to facilitate the optimization of model performance and resource allocation to achieve better results. Importance of Observability in Machine Learning ML observability plays a pivotal role in AI and ML, offering crucial benefits such as: Continuous Improvement: ML observability offers insights into model behavior to help refine algorithms, update models, and continuously enhance their predictive capabilities. Proactive Problem Detection: ML observability continuously observes model behavior and system performance to address potential problems before they escalate. Real-time Decision Support: ML observability offers real-time insights into model performance, enabling data-driven decision-making in dynamic and rapidly changing environments. Building Trust in AI Systems: ML observability fosters trust in AI systems. Understanding how models arrive at decisions provides confidence in the reliability and ethics of AI-driven outcomes. Compliance and Accountability: In regulated industries, ML observability helps maintain compliance with ethical and legal standards. Understanding model decisions and data usage ensure models remain accountable and within regulatory bounds. ML Monitoring vs. ML Observability: Overlapping Elements Both monitoring and observability are integral components of ML OPs that work in tandem to ensure the seamless functioning and optimization of ML models. Although they have distinct purposes, there are essential overlaps where their functions converge, boosting the overall effectiveness of the ML ecosystem. Some of their similar elements include: Anomaly Detection Anomaly detection is a shared objective in both ML monitoring and observability. Monitoring systems and observability tools are designed to identify deviations in model behavior and performance that may indicate potential issues or anomalies. Data Quality Control Ensuring data quality is essential for robust ML operations, and both monitoring and observability contribute to this aspect. ML monitoring systems continuously assess the quality and integrity of input data, monitoring for data drift or changes in data distribution that could impact model performance. Similarly, observability tools enable data scientists to gain insights into the characteristics of input data and assess its suitability for training and inference. Real-time Alerts Real-time alerts are a shared feature of both ML monitoring and observability. When critical issues or anomalies are detected, these systems promptly trigger alerts, notifying relevant stakeholders for immediate action to minimize outages. Continuous ML Improvement ML monitoring and observability foster a culture of ongoing improvement in machine learning. Monitoring identifies issues like performance drops, prompting iterative model refinement. Observability offers insights into system behavior, aiding data-driven optimization and enhanced decisions. Model Performance Assessment Evaluating model performance is a fundamental aspect shared by both monitoring and observability. Monitoring systems track model metrics over time, allowing practitioners to assess performance trends and benchmark against predefined thresholds. Observability complements this by offering a comprehensive view of the ML pipeline, aiding in the identification of potential bottlenecks or areas of improvement that may affect overall model performance. ML Monitoring vs. ML Observability: Key Differences ML Monitoring vs. ML Observability While ML monitoring and ML observability share common goals in ensuring effective machine learning operationalization, they differ significantly in their approaches, objectives, and the scope of insights they provide. Area of Focus The primary distinction lies in their focus. ML monitoring primarily answers \"what\" is happening within the ML system, tracking key metrics and indicators to identify deviations and issues. On the other hand, ML observability delves into the \"why\" and \"how\" aspects, providing in-depth insights into the internal workings of the ML pipeline. Its goal is to provide a deeper understanding of the model's behavior and decision-making process. Objectives ML monitoring's main objective is to track model performance, identify problem areas, and ensure operational stability. It aims to validate that the model is functioning as expected and provides real-time alerts to address immediate concerns. In contrast, ML observability primarily aims to offer holistic insights into the ML system's health. This involves identifying systemic issues, data quality problems, and shedding light on the broader implications of model decisions. Approach ML monitoring is a failure-centric practice, designed to detect and mitigate failures in the ML model. It concentrates on specific critical issues that could lead to incorrect predictions or system downtime. In contrast, ML observability pursues a system-centric approach, analyzing the overall system health, including data flows, dependencies, and external factors that denote the system's behavior and performance. Perspective ML Monitoring typically offers an external, high-level view of the ML model, focusing on metrics and performance indicators visible from the outside. ML Observability, on the other hand, offers a holistic view of the ML system inside and out. It provides insights into internal states, algorithmic behavior, and the interactions between various components, leading to an in-depth awareness of the system's dynamics. Performance Analytics ML monitoring relies on historical metrics data to analyze model performance and identify trends over time. ML observability, in contrast, emphasizes real-time analysis, allowing data scientists and engineers to explore the model's behavior in the moment, thereby facilitating quicker and more responsive decision-making. Use Case ML monitoring is particularly valuable in scenarios where immediate detection of critical issues is essential, such as in high-stakes applications like healthcare and finance. ML observability, on the other hand, shines in complex, large-scale ML systems where understanding the intricate interactions between various components and identifying systemic issues are crucial. To exemplify this difference, consider a medical AI company analyzing chest x-rays. Monitoring might signal a performance metric decline over the past month. Meanwhile, ML observability can detect that a new hospital joined the system, introducing different image sources and affecting features, underscoring the significance of systemic insights in intricate, large-scale ML systems. Encord Active: Empowering Robust ML Development With Monitoring & Observability Encord Active is an open-source ML platform that is built to revolutionize the process of building robust ML models. With its comprehensive suite of end-to-end monitoring and observability features, Encord Active equips practitioners with the essential tools to elevate their ML development journey. Encord Active Prominent features include intuitive dashboards for performance assessment. These dashboards facilitate the monitoring of performance metrics and visualization of feature distributions. It also offers automated robustness tests for mitigation, detects biases for ethical outcomes, and enables comprehensive evaluations for effective comparisons. Additionally, auto-identification of labeling errors ensures reliable results. ML Observability vs. ML Monitoring: Key Takeaways Effective ML monitoring and ML observability are crucial for developing and deploying successful machine learning models. ML monitoring components, such as real-time alerts and metrics collection, ensure continuous tracking of model performance and prompt issue identification. ML observability components, such as root cause analysis and model fairness assessment, provide detailed insights into the ML system's behavior and enable proactive improvements for long-term system reliability. The combination of ML monitoring and ML observability enables proactive issue detection and continuous improvement, leading to optimized ML systems. Together, ML monitoring and ML observability play a pivotal role in achieving system health, mitigating bias, and supporting real-time decision-making. Organizations can rely on both practices to build robust and trustworthy AI-driven solutions and drive innovation.\n\nAug 15 2023\n\n4 M\n\nVisualizations in Databricks\n\nWith data becoming a pillar stone of a company’s growth strategy, the market for visualization tools is growing rapidly, with a projected compound annual growth rate (CAGR) of 10.07% between 2023 and 2028. The primary driver of these trends is the need for data-driven decision-making, which involves understanding complex data patterns and extracting actionable insights to improve operational efficiency. PowerBI and Tableau are traditional tools with interactive workspaces for creating intuitive dashboards and exploring large datasets. However, other platforms are emerging to address the ever-changing nature of the modern data ecosystem. In this article, we will discuss the visualizations offered by Databricks - a modern enterprise-scale platform for building data, analytics, and artificial intelligence (AI) solutions. Databricks Databricks is an end-to-end data management and model development solution built on Apache Spark. It lets you create and deploy the latest generative AI (Gen AI) and large language models (LLMs). The platform uses a proprietary Mosaic AI framework to streamline the model development process. It provides tools to fine-tune LLMs seamlessly through enterprise data and offers a unified service for experimentation through foundation models. In addition, it features Databricks SQL, a state-of-the-art lakehouse for cost-effective data storage and retrieval. It lets you centrally store all your data assets in an open format, Delta Lake, for effective governance and discoverability. Further, Databricks SQL has built-in support for data visualization, which lets you extract insights from datasets directly from query results in the SQL editor. Users also benefit from the visualization tools featured in Databricks Notebooks, which help you build interactive charts by using the Plotly library in Python. Through these visualizations, Databricks offers robust data analysis for monitoring data assets critical to your AI models. So, let’s discuss in more detail the types of chart visualizations, graphs, diagrams, and maps available on Databricks to help you choose the most suitable visualization type for your use case. Effective visualization can help with effortless data curation. Learn more about how you can use data curation for computer vision Visualizations in Databricks As mentioned earlier, Databricks provides visualizations through Databricks SQL and Databricks Notebooks. The platform lets you run multiple SQL queries to perform relevant aggregations and apply filters to visualize datasets according to your needs. Databricks also allows you to configure settings related to the X and Y axes, legends, missing values, colors, and labels. Users can also download visualizations in PNG format for documentation purposes. The following sections provide an overview of the various visualization types available in these two frameworks, helping you select the most suitable option for your project. Bar Chart Bar charts are helpful when you want to compare the frequency of occurrence of different categories in your dataset. For instance, you can draw a bar chart to compare the frequency of various age groups, genders, ethnicities, etc. Additionally, bar charts can be used to view the sum of the prices of all orders placed in a particular month and group them by priority. Bar chart The result will show the months on the X-axis and the sum of all the orders categorized by priority on the Y-axis. Line Line charts connect different data points through straight lines. They are helpful when users want to analyze trends over some time. The charts usually show time on the X-axis and some metrics whose trajectory you want to explore on the Y-axis. Line chart For instance, you can view changes in the average price of orders over the years grouped by priority. The trends can help you predict the most likely future values, which can help you with financial projections and budget planning. Pie Chart Pie charts display the proportion of different categories in a dataset. They divide a circle into multiple segments, each showing the proportion of a particular category, with the segment size proportional to the category’s percentage of the total. Pie chart For instance, you can visualize the proportion of orders for each priority. The visualization is helpful when you want a quick overview of data distribution across different segments. It can help you analyze demographic patterns, market share of other products, budget allocation, etc. Scatter Plot A scatter plot displays each data point as a dot representing a relationship between two variables. Users can also control the color of each dot to reflect the relationship across different groups. Scatter Plot For instance, you can plot the relationship between quantity and price for different color-coded item categories. The visualization helps in understanding the correlation between two variables. However, users must interpret the relationship cautiously, as correlation does not always imply causation. Deeper statistical analysis is necessary to uncover causal factors. Area Charts Area charts combine line and bar charts by displaying lines and filling the area underneath with colors representing particular categories. They show how the contribution of a specific category changes relative to others over time. Area Charts For instance, you can visualize which type of order priority contributed the most to revenue by plotting the total price of different order priorities across time. The visualization helps you analyze the composition of a specific metric and how that composition varies over time. It is particularly beneficial in analyzing sales growth patterns for different products, as you can see which product contributed the most to growth across time. Box Chart Box charts concisely represent data distributions of numerical values for different categories. They show the distribution’s median, skewness, interquartile, and value ranges. Box Chart For instance, the box can display the median price value through a line inside the box and the interquartile range through the top and bottom box enclosures. The extended lines represent minimum and maximum price values to compute the price range. The chart helps determine the differences in distribution across multiple categories and lets you detect outliers. You can also see the variability in values across different categories and examine which category was the most stable. Bubble Chart Bubble charts enhance scatter plots by allowing you to visualize the relationship of three variables in a two-dimensional grid. The bubble position represents how the variable on the X-axis relates to the variable on the Y-axis. The bubble size represents the magnitude of a third variable, showing how it changes as the values of the first two variables change. Bubble chart The visualization is helpful for multi-dimensional datasets and provides greater insight when analyzing demographic data. However, like scatter plots, users must not mistake correlation for causation. Combo Chart Combo charts combine line and bar charts to represent key trends in continuous and categorical variables. The categorical variable is on the X-axis, while the continuous variable is on the Y-axis. Combo Chart For instance, you can analyze how the average price varies with the average quantity according to shipping date. The visualization helps summarize complex information involving relationships between three variables on a two-dimensional graph. However, unambiguous interpretation requires careful configuration of labels, colors, and legends. Heatmap Chart Heatmap charts represent data in a matrix format, with each cell having a different color according to the numerical value of a specific variable. The colors change according to the value intensity, with lower values typically having darker and higher values having lighter colors. Heatmap chart For instance, you can visualize how the average price varies according to order priority and order status. Heatmaps are particularly useful in analyzing correlation intensity between two variables. They also help detect outliers by representing unusual values through separate colors. However, interpreting the chart requires proper scaling to ensure colors do not misrepresent intensities. Histogram Histograms display the frequency of particular value ranges to show data distribution patterns. The X-axis contains the value ranges organized as bins, and the Y-axis shows the frequency of each bin. Histogram For instance, you can visualize the frequency of different price ranges to understand price distribution for your orders. The visualization lets you analyze data spread and skewness. It is beneficial in deeper statistical analysis, where you want to derive probabilities and build predictive models. Pivot Tables Pivot tables can help you manipulate tabular displays through drag-and-drop options by changing aggregation records. The option is an alternative to SQL filters for viewing aggregate values according to different conditions. Pivot Tables For instance, you can group total orders by shipping mode and order category. The visualization helps prepare ad-hoc reports and provides important summary information for decision-making. Interactive pivot tables also let users try different arrangements to reveal new insights. Choropleth Map Visualization Choropleth map visualization represents color-coded aggregations categorized according to different geographic locations. Regions with higher value intensities have darker colors, while those with lower intensities have lighter shades. Choropleth map visualization For instance, you can visualize the total revenue coming from different countries. This visualization helps determine global presence and highlight disparities across borders. The insights will allow you to develop marketing strategies tailored to regional tastes and behavior. Funnel Visualization Funnel visualization depicts data aggregations categorized according to specific steps in a pipeline. It represents each step from top to bottom with a bar and the associated value as a label overlay on each bar. It also displays cumulative percentage values showing the proportion of the aggregated value resulting from each stage. Funnel Visualization For instance, you can determine the incoming revenue streams at each stage of the ordering process. This visualization is particularly helpful in analyzing marketing pipelines for e-commerce sites. The tool shows the proportion of customers who view a product ad, click on it, add it to the cart, and proceed to check out. Cohort Analysis Cohort analysis offers an intuitive visualization to track the trajectory of a particular metric across different categories or cohorts. Cohort Analysis For instance, you can analyze the number of active users on an app that signed up in different months of the year. The rows will depict the months, and the columns will represent the proportion of active users in a particular cohort as they move along each month. The visualization helps in retention analysis as you can determine the proportion of retained customers across the user lifecycle. Counter Display Databricks allows you to configure a counter display that explicitly shows how the current value of a particular metric compares with the metric’s target value. Counter display For instance, you can check how the average total revenue compares against the target value. In Databricks, the first row represents the current value, and the second is the target. The visualization helps give a quick snapshot of trending performance and allows you to quantify goals for better strategizing. Sankey Diagrams Sankey diagrams show how data flows between different entities or categories. It represents flows through connected links representing the direction, with entities displayed as nodes on either side of a two-dimensional grid. The width of the connected links represents the magnitude of a particular value flowing from one entity to the other. Sankey Diagram For instance, you can analyze traffic flows from one location to the other. Sankey diagrams can help data engineering teams analyze data flows from different platforms or servers. The analysis can help identify bottlenecks, redundancies, and resource constraints for optimization planning. Sunburst Sequence The sunburst sequence visualizes hierarchical data through concentric circles. Each circle represents a level in the hierarchy and has multiple segments. Each segment represents the proportion of data in the hierarchy. Furthermore, it color codes segments to distinguish between categories within a particular hierarchy. Sunburst Sequence For instance, you can visualize the population of different world regions through a sunburst sequence. The innermost circle represents a continent, the middle one shows a particular region, and the outermost circle displays the country within that region. The visualization helps data science teams analyze relationships between nested data structures. The information will allow you to define clear data labels needed for model training. Table A table represents data in a structured format with rows and columns. Databricks offers additional functionality to hide, reformat, and reorder data. Tables help summarize information in structured datasets. You can use them for further analysis through SQL queries. Word Cloud Word cloud visualizations display words in different sizes according to their frequency in textual data. For instance, you can analyze customer comments or feedback and determine overall sentiment based on the highest-occurring words. Word Cloud While word clouds help identify key themes in unstructured textual datasets, they can suffer from oversimplification. Users must use word clouds only as a quick overview and augment textual analysis with advanced natural language processing techniques. Visualization is critical to efficient data management. Find out the top tools for data management for computer vision Visualizations in Databricks: Key Takeaways With an ever-increasing data volume and variety, visualization is becoming critical for quickly communicating data-based insights in a simplified manner. Databricks is a powerful tool with robust visualization types for analyzing complex datasets. Below are a few key points to remember regarding visualization in Databricks. Databricks SQL and Databricks Notebooks: Databricks offers advanced visualizations through Databricks SQL and Databricks Notebooks as a built-in functionality. Visualization configurations: Users can configure multiple visualization settings to produce charts, graphs, maps, and diagrams per their requirements. Visualization types: Databricks offers multiple visualizations, including bar charts, line graphs, pie charts, scatter plots, area graphs, box plots, bubble charts, combo charts, heatmaps, histograms, pivot tables, choropleth maps, funnels, cohort tables, counter display, Sankey diagrams, sunburst sequences, tables, and word clouds.\n\nMar 28 2024\n\n10 M\n\nMicrosoft MORA: Multi-Agent Video Generation Framework\n\nWhat is Mora? Mora is a multi-agent framework designed for generalist video generation. Based on OpenAI's Sora, it aims to replicate and expand the range of generalist video generation tasks. Sora, famous for making very realistic and creative scenes from written instructions, set a new standard for creating videos that are up to a minute long and closely match the text descriptions given. Mora distinguishes itself by incorporating several advanced visual AI agents into a cohesive system. This lets it undertake various video generation tasks, including text-to-video generation, text-conditional image-to-video generation, extending generated videos, video-to-video editing, connecting videos, and simulating digital worlds. Mora can mimic Sora’s capabilities using multiple visual agents, significantly contributing to video generation. In this article, you will learn: Mora's innovative multi-agent framework for video generation. The importance of open-source collaboration that Mora enables. Mora's approach to complex video generation tasks and instruction fidelity. About the challenges in video dataset curation and quality enhancement. TL; DR Mora's novel approach uses multiple specialized AI agents, each handling different aspects of the video generation process. This innovation allows various video generation tasks, showcasing adaptability in creating detailed and dynamic video content from textual descriptions. Mora aims to fix the problems with current models like Sora, which is closed-source and does not let anyone else use it or do more research in the field, even though it has amazing text-to-video conversion abilities 📝🎬. Unfortunately, Mora still has problems with dataset quality, video fidelity, and ensuring that outputs align with complicated instructions and people's preferences. These problems show where more work needs to be done in the future. OpenAI Sora’s Closed-Source Nature The closed-source nature of OpenAI's Sora presents a significant challenge to the academic and research communities interested in video generation technologies. Sora's impressive capabilities in generating realistic and detailed videos from text descriptions have set a new standard in the field. Related: New to Sora? Check out our detailed explainer on the architecture, relevance, limitations, and applications of Sora. However, the inability to access its source code or detailed architecture hinders external efforts to replicate or extend its functionalities. This limits researchers from fully understanding or replicating its state-of-the-art performance in video generation. Here are the key challenges highlighted due to Sora's closed-source nature: Inaccessibility to Reverse-Engineer Without access to Sora's source code, algorithms, and detailed methodology, the research community faces substantial obstacles in dissecting and understanding the underlying mechanisms that drive its exceptional performance. This lack of transparency makes it difficult for other researchers to learn from and build upon Sora's advancements, potentially slowing down the pace of innovation in video generation. Extensive Training Datasets Sora's performance is not just the result of sophisticated modeling and algorithms; it also benefits from training on extensive and diverse datasets. But the fact that researchers cannot get their hands on similar datasets makes it very hard to copy or improve Sora's work. High-quality, large-scale video datasets are crucial for training generative models, especially those capable of creating detailed, realistic videos from text descriptions. However, these datasets are often difficult to compile due to copyright issues, the sheer volume of data required, and the need for diverse, representative samples of the real world. Creating, curating, and maintaining high-quality video datasets requires significant resources, including copyright permissions, data storage, and management capabilities. Sora's closed nature worsens these challenges by not providing insights into compiling the datasets, leaving researchers to navigate these obstacles independently. Computational Power Creating and training models like Sora require significant computational resources, often involving large clusters of high-end GPUs or TPUs running for extended periods. Many researchers and institutions cannot afford this much computing power, which makes the gap between open-source projects like Mora and proprietary models like Sora even bigger. Without comparable computational resources, it becomes challenging to undertake the necessary experimentation—with different architectures and hyperparameters—and training regimes required to achieve similar breakthroughs in video generation technology. Learn more about these limitations in the technical paper. Evolution: Text-to-Video Generation Over the years, significant advancements in text-to-video generation technology have occurred, with each approach and architecture uniquely contributing to the field's growth. Here's a summary of these evolutionary stages, as highlighted in the discussion about text-to-video generation in the Mora paper: GANs (Generative Adversarial Networks) Early attempts at video generation leveraged GANs, which consist of two competing networks: a generator that creates images or videos that aim to be indistinguishable from real ones, and a discriminator that tries to differentiate between the real and generated outputs. Despite their success in image generation, GANs faced challenges in video generation due to the added complexity of temporal coherence and higher-dimensional data. Generative Video Models Moving beyond GANs, the field saw the development of generative video models designed to produce dynamic sequences. Generating realistic videos frame-by-frame and maintaining temporal consistency is a challenge, unlike in static image generation. Auto-Regressive Transformers Auto-regressive transformers were a big step forward because they could generate video sequences frame-by-frame. These models predicted each new frame based on the previously generated frames, introducing a sequential element that mirrors the temporal progression of videos. But this approach often struggled with long-term coherence over longer sequences. Large-Scale Diffusion Models Diffusion models, known for their capacity to generate high-quality images, were extended to video generation. These models gradually refine a random noise distribution toward a coherent output. They apply this iterative denoising process to the temporal domain of videos. Related: Read our guide on HuggingFace’s Dual-Stream Diffusion Net for Text-to-Video Generation. Image Diffusion U-Net Adapting the U-Net architecture for image diffusion models to video content was critical. This approach extended the principles of image generation to videos, using a U-Net that operates over sequences of frames to maintain spatial and temporal coherence. 3D U-Net Structure The change to a 3D U-Net structure allowed for more nuance in handling video data, considering the extra temporal dimension. This change also made it easier to model time-dependent changes, improving how we generate coherent and dynamic video content. Latent Diffusion Models (LDMs) LDMs generate content in a latent space rather than directly in pixel space. This approach reduces computational costs and allows for more efficient handling of high-dimensional video data. LDMs have shown that they can better capture the complex dynamics of video content. Diffusion Transformers Diffusion transformers (DiT) combine the strengths of transformers in handling sequential data with the generative capabilities of diffusion models. This results in high-quality video outputs that are visually compelling and temporally consistent. Useful: Stable Diffusion 3 is an example of a multimodal diffusion transformer model that generates high-quality images and videos from text. Check out our explainer on how it works. AI Agents: Advanced Collaborative Multi-agent Structures The paper highlights the critical role of collaborative, multi-agent structures in developing Mora. It emphasizes their efficacy in handling multimodal tasks and improving video generation capabilities. Here's a concise overview based on the paper's discussion on AI Agents and their collaborative frameworks: Multimodal Tasks Advanced collaborative multi-agent structures address multimodal tasks involving processing and generating complex data across different modes, such as text, images, and videos. These structures help integrate various AI agents, each specialized in handling specific aspects of the video generation process, from understanding textual prompts to creating visually coherent sequences. Cooperative Agent Framework (Role-Playing) The cooperative agent framework, characterized by role-playing, is central to the operation of these multi-agent structures. Each agent is assigned a unique role or function in this framework, such as prompt enhancement, image generation, or video editing. By defining these roles, the framework ensures that an agent with the best skills for each task is in charge of that step in the video generation process, increasing overall efficiency and output quality. Multi-Agent Collaboration Strategy The multi-agent collaboration strategy emphasizes the orchestrated interaction between agents to achieve a common goal. In Mora, this strategy involves the sequential and sometimes parallel processing of tasks by various agents. For instance, one agent might enhance an initial text prompt, convert it into another image, and finally transform it into a video sequence by yet another. This collaborative approach allows for the flexible and dynamic generation of video content that aligns with user prompts. AutoGen (Generic Programming Framework) A notable example of multi-agent collaboration in practice is AutoGen. This generic programming framework is designed to automate the assembly and coordination of multiple AI agents for a wide range of applications. Within the context of video generation, AutoGen can streamline the configuration of agents according to the specific requirements of each video generation task to generate complex video content from textual or image-based prompts. Mora drone to butterfly flythrough shot. | Image Source. Role of an AI Agent The paper outlines the architecture involving multiple AI agents, each serving a specific role in the video generation process. Here's a closer look at the role of each AI agent within the framework: Illustration of how to use Mora to conduct video-related tasks Prompt Selection and Generation Agent This agent is tasked with processing and optimizing textual prompts for other agents to process them further. Here are the key techniques used for Mora: GPT-4: This agent uses the generative capabilities of GPT-4 to generate high-quality prompts that are detailed and rich in context. Prompt Selection: This involves selecting or enhancing textual prompts to ensure they are optimally prepared for the subsequent video generation process. This step is crucial for setting the stage for generating images and videos that closely align with the user's intent. Good Read: Interested in GPT-4 Vision alternatives? Check out our blog post. Text-to-Image Generation Agent This agent uses a retrained large text-to-image model to convert the prompts into initial images. The retraining process ensures the model is finely tuned to produce high-quality images, laying a strong foundation for the video generation process. Image-to-Image Generation Agent This agent specializes in image-to-image generation, taking initial images and editing them based on new prompts or instructions. This ability allows for a high degree of customization and improvement in video creation. Image-to-Video Generation Agent This agent transforms static images into dynamic video sequences, extending the visual narrative by generating coherent frames. Here are the core techniques and models: Core Components: It incorporates two pre-trained models: GPT-3 for understanding and generating text-based instructions, and Stable Diffusion for translating these instructions into visual content. Prompt-to-Prompt Technique: The prompt-to-prompt technique guides the transformation from an initial image to a series of images that form a video sequence. Classifier-Free Guidance: Classifier-free guidance is used to improve the fidelity of generated videos to the textual prompts so that the videos remain true to the users' vision. Text-to-Video Generation Agent: This role is pivotal in transforming static images into dynamic videos that capture the essence of the provided descriptions. Stable Video Diffusion (SVD) and Hierarchical Training Strategy: A model specifically trained to understand and generate video content, using a hierarchical training strategy to improve the quality and coherence of the generated videos. Video Connection Agent This agent creates seamless transitions between two distinct video sequences for a coherent narrative flow. Here are the key techniques used: Pre-Trained Diffusion-Based T2V Model: This model uses a pre-trained diffusion-based model specialized in text-to-video (T2V) tasks to connect separate video clips into a cohesive narrative. Text-Based Control: This method uses textual descriptions to guide the generation of transition videos that seamlessly connect disparate video clips, ensuring logical progression and thematic consistency. Image-to-Video Animation and Autoregressive Video Prediction: These capabilities allow the agent to animate still images into video sequences, predict and generate future video frames based on previous sequences, and create extended and coherent video narratives. Mora’s Video Generation Process Mora's video-generation method is a complex, multi-step process that uses the unique capabilities of specialized AI agents within its framework. This process allows Mora to tackle various video generation tasks, from creating videos from text descriptions to editing and connecting existing videos. Here's an overview of how Mora handles each task: Mora’s video generation process. Text-to-Video Generation This task begins with a detailed textual prompt from the user. Then, the Text-to-Image Generation Agent converts the prompts into initial static images. These images serve as the basis for the Image-to-Video Generation Agent, which creates dynamic sequences that encapsulate the essence of the original text and produce a coherent video narrative. Text-Conditional Image-to-Video Generation This task combines textual prompts with a specific starting image. Mora first improves the input with the Prompt Selection and Generation Agent, ensuring that the text and image are optimally prepared for video generation. Then, the Image-to-Video Generation Agent takes over, generating a video that evolves from the initial image and aligns with the textual description. Extend Generated Videos To extend an existing video, Mora uses the final frame of the input video as a launchpad. The Image-to-Video Generation Agent crafts additional sequences that logically continue the narrative from the last frame, extending the video while maintaining narrative and visual continuity. Video-to-Video Editing In this task, Mora edits existing videos based on new textual prompts. The Image-to-Image Generation Agent first edits the video's initial frame according to the new instructions. Then, the Image-to-Video Generation Agent generates a new video sequence from the edited frame, adding the desired changes to the video content. Connect Videos Connecting two videos involves creating a transition between them. Mora uses the Video Connection Agent, which analyzes the first video's final frame and the second's initial frame. It then generates a transition video that smoothly links the two segments into a cohesive narrative flow. Simulating Digital Worlds Mora generates video sequences in this task that simulate digital or virtual environments. The process involves appending specific style cues (e.g., \"in digital world style\") to the textual prompt, guiding the Image-to-Video Generation Agent to create a sequence reflecting the aesthetics of a digital realm. This can involve stylistically transforming real-world images into digital representations or generating new content within the specified digital style. See Also: Read our explainer on Google’s Video Gaming Companion: Scalable Instructable Multiworld Agent [SIMA]. Mora: Experimental Setup As detailed in the paper, the experimental setup for evaluating Mora is comprehensive and methodically designed to assess the framework's performance across various dimensions of video generation. Here's a breakdown of the setup: Baseline The baseline for comparison includes existing open-sourced models that showcase competitive performance in video generation tasks. These models include Videocrafter, Show-1, Pika, Gen-2, ModelScope, LaVie-Interpolation, LaVie, and CogVideo. These models are a reference point for evaluating Mora's advancements and position relative to the current state-of-the-art video generation. Basic Metrics The evaluation framework comprises several metrics to quantify Mora's performance across different dimensions of video quality and condition consistency: Video Quality Measurement Object Consistency: Measures the stability of object appearances across video frames. Background Consistency: Assesses the uniformity of the background throughout the video. Motion Smoothness: Evaluates the fluidity of motion within the video. Aesthetic Score: Gauges the artistic and visual appeal of the video. Dynamic Degree: Quantifies the video's dynamic action or movement level. Imaging Quality: Assesses the overall visual quality of the video, including clarity and resolution. Video Condition Consistency Metric Temporal Style: Measures how consistently the video reflects the temporal aspects (e.g., pacing, progression) described in the textual prompt. Appearance Style: Evaluates the adherence of the video's visual style to the descriptions provided in the prompt, ensuring that the generated content matches the intended appearance. Self-Defined Metrics Video-Text Integration (VideoTI): Measures the model’s fidelity to textual instructions by comparing text representations of input images and generated videos. Temporal Consistency (TCON): Evaluates the coherence between an original video and its extended version, providing a metric for assessing the integrity of extended video content. Temporal Coherence (Tmean): Quantifies the correlation between the intermediate generated and input videos, measuring overall temporal coherence. Video Length: This parameter quantifies the duration of the generated video content, indicating the model's capacity for producing videos of varying lengths. Implementation Details The experiments use high-performance hardware, specifically TESLA A100 GPUs with substantial VRAM. This setup ensures that Mora and the baseline models are evaluated under conditions allowing them to fully express their video generation capabilities. The choice of hardware reflects the computational intensity of training and evaluating state-of-the-art video generation models. Mora video generation - Fish underwater flythrough Limitations of Mora The paper outlines several limitations of the Mora framework. Here's a summary of these key points: Curating High-Quality Video Datasets Access to high-quality video datasets is a major challenge for training advanced video generation models like Mora. Copyright restrictions and the sheer volume of data required make it difficult to curate diverse and representative datasets that can train models capable of generating realistic and varied video content. Read Also: The Full Guide to Video Annotation for Computer Vision. Quality and Length Gaps While Mora demonstrates impressive capabilities, it has a noticeable gap in quality and maximum video length compared to state-of-the-art models like Sora. This limitation is particularly evident in tasks requiring the generation of longer videos, where maintaining visual quality and coherence becomes increasingly challenging. Simulating videos in Mora vs in Sora. Instruction Following Capability Mora sometimes struggles to precisely follow complex or detailed instructions, especially when generating videos that require specific actions, movements, or directionality. This limitation suggests that further improvement in understanding and interpreting textual prompts is needed. Human Visual Preference Alignment The experimental results may not always align with human visual preferences, particularly in scenarios requiring the generation of realistic human movements or the seamless connection of video segments. This misalignment highlights the need to incorporate a more nuanced understanding of physical laws and human dynamics into the video-generation process. Mora Vs. Sora: Feature Comparisons The paper compares Mora and OpenAI's Sora across various video generation tasks. Here's a detailed feature comparison based on their capabilities in different aspects of video generation: Check out the project repository on GitHub. Mora Multi-Agent Framework: Key Takeaways The paper \"Mora: Enabling Generalist Video Generation via a Multi-Agent Framework\" describes Mora, a new framework that advances video technology. Using a multi-agent approach, Mora is flexible and adaptable across various video generation tasks, from creating detailed scenes to simulating complex digital worlds. Because it is open source, it encourages collaboration, which leads to new ideas, and lets the wider research community add to and improve its features. Even though Mora has some good qualities, it needs high-quality video datasets, video quality, length gaps, trouble following complicated instructions correctly, and trouble matching outputs to how people like to see things. Finding solutions to these problems is necessary to make Mora work better and be used in more situations. Continuing to improve and develop Mora could change how we make video content so it is easier for creators and viewers to access and have an impact.\n\nMar 26 2024\n\n8 M\n\nQwen-VL and Qwen-VL-Chat: Introduction to Alibaba’s AI Models\n\nQwen-VL is a series of open-source large vision-language models (LVLMs), offering a potent combination of advanced capabilities and accessibility. As an open-source project, Qwen-VL not only democratizes access to cutting-edge AI technology but also positions itself as a formidable competitor to established models from tech giants like OpenAI’s GPT-4V and Google’s Gemini. In the competitive landscape of LVLMs, Qwen-VL has quickly risen to the forefront, securing its place as a leader on the OpenVLM leaderboard. This leaderboard, which encompasses 38 different VLMs including GPT-4V, Gemini, QwenVLPlus, LLaVA, and others, serves as a comprehensive benchmark for evaluating model performance across 13 distinct multimodal tasks. OpenVLM Leaderboard Qwen-VL's performance across these benchmarks underscores its versatility and robustness in handling various vision-language tasks with unparalleled accuracy and efficiency. By leading the charge on the OpenVLM leaderboard, Qwen-VL sets a new standard for excellence in the field, pushing the boundaries of what is possible with LVLMs and paving the way for future advancements in multimodal AI research. Introduction to Large-scale Vision Language Models (LVLMs) Large Language Models (LLMs) have attracted attention in recent years for their remarkable text generation and comprehension capabilities in the field of generative AI. However, their limitation to processing text alone has constrained their utility in various applications. In response to this limitation, a new class of models known as Large Vision Language Models (LVLMs) has come up, aiming to integrate visual data with textual information to address vision-centric tasks. LVLMs improve conventional LLMs by integrating vision language learning, thus extending their applicability to include image datasets. However, despite their promising potential, open-source LVLM implementations encounter hurdles such as inadequate training and optimization when compared to proprietary models. Also, understanding visual content still remains a significant challenge for existing LVLM frameworks. Overview of Qwen-VL The Qwen-VL series represents a significant advancement in Large Vision Language Models (LVLMs), designed to overcome the limitations of existing models and equip LLMs with visual processing capabilities. Built upon the Alibaba Cloud’s 7 billion parameter model, Qwen-7B language model, the Qwen-VL series introduces a visual receptor architecture comprising a language-aligned visual encoder and a position-aware adapter. This architecture enables Qwen-VL models to effectively process visual inputs, generate responses based on prompts, and perform various vision-language tasks such as image recognition, image captioning, visual question answering, and visual grounding. Qwen-VL models demonstrate leading performance on vision-centric benchmarks and support multiple languages, including English and Chinese. For more information on VLMs, read the blog Guide to Vision-Language Models (VLMs) Key Features of Qwen-VL Qwen-VL models demonstrate good accuracy on a wide range of vision-centric understanding benchmarks, surpassing other SOTA models of similar scales. They excel not only in conventional benchmarks such as captioning and question-answering but also in recently introduced dialogue benchmarks. Here are the key features of Qwen-VL: Multi-lingual Support: Similar to Qwen-LM, Qwen-VLs are trained on multilingual image-text data, with a substantial corpus in English and Chinese. This enables Qwen-VLs to naturally support English, Chinese, and other multilingual instructions. Multi-image Capability: During training, Qwen-VLs can handle arbitrary interleaved image-text data as inputs, allowing them to compare, understand, and analyze context when multiple images are provided. Fine-grained Visual Understanding: Qwen-VLs exhibit highly competitive fine-grained visual understanding abilities, thanks to their higher-resolution input size and fine-grained corpus used during training. Compared to existing vision-language generalists, Qwen-VLs demonstrate superior performance in tasks such as grounding, text-reading, text-oriented question answering, and fine-grained dialogue comprehension. Vision-centric Understanding: This allows the model to comprehensively interpret and process visual information. With advanced architecture integrating a language-aligned visual encoder and position-aware adapter, Qwen-VL excels in tasks like image captioning, question answering, and visual grounding. Its fine-grained analysis ensures precise interpretation of visual content, making Qwen-VL highly effective in vision-language tasks and real-world applications. Design Structure of Qwen-VL Beginning with the foundation of Qwen-LM, the model is enhanced with visual capacity through several key components: Visual Receptor: Qwen-VL incorporates a carefully designed visual receptor, which includes a visual encoder and adapter. This component is responsible for processing image inputs and extracting fixed-length sequences of image features. Input-Output Interface: The model's input-output interface is optimized to differentiate between image and text feature inputs. Special tokens are utilized to delineate image feature input, ensuring seamless integration of both modalities. 3-stage Training Pipeline: Qwen-VL employs a sophisticated 3-stage training pipeline to optimize model performance. This pipeline encompasses comprehensive training stages aimed at fine-tuning the model's parameters and enhancing its ability to comprehend and generate responses for both text and image inputs. Multilingual Multimodal Cleaned Corpus: Qwen-VL is trained on a diverse multilingual multimodal corpus, which includes cleaned data encompassing both textual and visual information. This corpus facilitates the model's ability to understand and generate responses in multiple languages while effectively processing various types of visual content. Model Architecture of Qwen-VL The architecture of Qwen-VL comprises three key components, each contributing to the model's robustness in processing both text and visual inputs. Large Language Model Qwen-VL leverages a large language model as its foundational component. This machine learning model is initialized with pre-trained weights obtained from Qwen-7B, ensuring a strong linguistic foundation for the model's language processing capabilities. Visual Encoder Qwen-VL employs the Vision Transformer (ViT) architecture, utilizing pre-trained weights from Openclip's ViT-bigG. During both training and inference, input images are resized to a specific resolution. The visual encoder processes these images by dividing them into patches with a stride of 14, thereby generating a set of image features that encapsulate visual information. Position-aware Vision-Language Adapter To address efficiency concerns arising from long sequences of image features, Qwen-VL introduces a vision-language adapter. This adapter is designed to compress the image features, enhancing computational efficiency. It consists of a single-layer cross-attention module initialized randomly. This module utilizes a group of trainable embeddings as query vectors and the image features from the visual encoder as keys for cross-attention operations. By employing this mechanism, the visual feature sequence is compressed to a fixed length of 256. To preserve positional information crucial for fine-grained image comprehension, 2D absolute positional encodings are incorporated into the query-key pairs of the cross-attention mechanism. This ensures that positional details are retained during the compression process. The compressed image feature sequence of length 256 is then fed into the large language model, enabling Qwen-VL to effectively process both textual and visual inputs and perform a wide range of vision-language tasks with high accuracy and efficiency. Training Pipeline of Qwen-VL series For more information, read the official paper released on Arxiv: Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond. Performance of Qwen-VL against State-of-The-Art LVLMs The performance of Qwen-VL models, particularly Qwen-VL-Max, surpasses SOTA models such as Gemini Ultra and GPT-4V in various text-image multimodal tasks. Compared to the open-source version of Qwen-VL, these models achieve comparable results to Gemini Ultra and GPT-4V, while significantly outperforming previous best results from open-source models. Performance of Qwen-VL-Plus and Qwen-VL-Max against other LVLM In particular, Qwen-VL-Max demonstrates superior performance over GPT-4V from OpenAI and Gemini from Google in tasks related to Chinese question answering and Chinese text comprehension. This achievement highlights the advanced capabilities of Qwen-VL-Max and its potential to establish new benchmarks in multimodal AI research and application. It should also be noted that most SOTA models are not trained on chinese language. Capabilities of Qwen-VL Qwen-VL exhibits a diverse range of capabilities that enable it to effectively comprehend and interact with visual and textual information, as well as reason and learn from its environment. These capabilities include: Basic Recognition Capabilities Qwen-VL demonstrates strong basic recognition capabilities, accurately identifying and describing various elements within images, including common objects, celebrities, landmarks, and intricate details. Recognition capabilities of Qwen-VL Visual Agent Capability As a visual agent, Qwen-VL is capable of providing detailed background information, answering questions, and analyzing complex visual content. It can also compose poetry in multiple languages inspired by visual stimuli and analyze everyday screenshots. Visual Agent Capabilities of Qwen-VL Visual Reasoning Capability Qwen-VL possesses advanced visual reasoning capabilities, extending beyond content description to comprehend and interpret intricate representations such as flowcharts, diagrams, and other symbolic systems. It excels in problem-solving and reasoning tasks, including mathematical problem-solving and profound interpretations of charts and graphs. Qwen-VL has advanced visual reasoning capabilities Text Information Recognition and Processing Qwen-VL exhibits enhanced text information recognition and processing abilities, efficiently extracting information from tables and documents, reformatting it to meet customized output requirements, and effectively identifying and converting dense text. It also supports images with extreme aspect ratios, ensuring flexibility in processing diverse visual content. Advanced text information recognition and processing abilities of Qwen-VL Few-shot Learning on Vision-Language Tasks Qwen-VL demonstrates satisfactory in-context learning (few-shot learning) ability, achieving superior performance on vision-language tasks such as question answering and image captioning compared to models with similar numbers of parameters. Its performance rivals even larger models, showcasing its adaptability and efficiency in learning from limited data. For more information on few-shot learning, read the blog Few Shot Learning in Computer Vision: Approaches & Uses Qwen-VL Availability Qwen-VL, including Qwen-VL-Plus and Qwen-VL-Max, is now readily accessible through various platforms, offering researchers and developers convenient access to its powerful capabilities: HuggingFace: Users can access Qwen-VL-Plus and Qwen-VL-Max through the Huggingface Spaces and Qwen website, enabling seamless integration into their projects and workflows. Dashscope APIs: The APIs of Qwen-VL-Plus and Qwen-VL-Max are available through the Dashscope platform, providing developers with the flexibility to leverage its capabilities for their AI applications. Detailed documentation and quick-start guides are available on the Dashscope platform for easy integration. QianWen Web Portal: By logging into the Tongyi QianWen web portal and switching to \"Image Understanding\" mode, users can harness the latest Qwen-VL-Max capabilities for image understanding tasks. This mode offers additional functionalities tailored specifically for image processing and understanding. ModelScope: The Qwen-VL-Chat demo is available on modelscope. GitHub Repository: The code and model weights of both Qwen-VL and Qwen-VL-Chat are openly available to download on GitHub, allowing researchers and developers to explore, modify, and utilize them freely. The commercial use of these resources is permitted, enabling their integration into commercial projects and applications. Qwen-VL-Chat Qwen-VL-Chat, as a generalist multimodal LLM-based AI assistant, supports complex interactions, including multiple image inputs, multi-round question answering, and creative capabilities. Unlike traditional vision-language chatbots, Qwen-VL-Chat's alignment techniques enable it to comprehend and respond to complex visual and textual inputs with superior accuracy and flexibility. Here's how Qwen-VL-Chat stands out in real-world dialog benchmarks and compares with existing models: Qwen-VL-Chat Vs. Vision-Language Chat Performance of Qwen-VL against other generalist models across various tasks Qwen-VL-Chat's advanced capabilities are evaluated using the TouchStone benchmark, which assesses overall text-image dialogue capability and alignment with humans. Unlike conventional models like chatGPT or Bard, Qwen-VL-Chat excels in handling direct image input, thanks to fine-grained image annotations provided by human labeling. With a comprehensive coverage of 300+ images, 800+ questions, and 27 categories, including attribute-based Q&A, celebrity recognition, writing poetry, summarizing multiple images, product comparison, and math problem solving, Qwen-VL-Chat achieves superior performance in understanding and responding to complex visual and textual inputs. You can find the official tutorial to implement Qwen-VL-Chat on your own on Github. Real-world Dialog Benchmark Qwen-VL-Chat's outstanding results in other multimodal benchmarks, such the MME Benchmark and Seed-Bench, demonstrate that its performance evaluation extends beyond the TouchStone benchmark. In both the perceptual and cognition tracks, Qwen-VL-Chat obtains state-of-the-art scores in the MME Benchmark, an extensive evaluation of multimodal large language models. The Qwen series, which includes Qwen-VL-Chat, achieves state-of-the-art performance in Seed-Bench, a benchmark consisting of 19K multiple-choice questions with precise human annotations. Qwen-VL: What’s Next? The release of the Qwen-VL series represents a significant stride forward in large-scale multilingual vision-language models, with the goal of advancing multimodal research. Qwen-VL has demonstrated its superiority over comparable artificial intelligence models across various benchmarks, facilitating multi"
    }
}
{
    "id": "dbpedia_7753_3",
    "rank": 2,
    "data": {
        "url": "https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-29/issue-1",
        "read_more_link": "",
        "language": "en",
        "title": "The Annals of Mathematical Statistics",
        "top_image": "https://projecteuclid.org/favicon.png",
        "meta_img": "https://projecteuclid.org/favicon.png",
        "images": [
            "https://projecteuclid.org/Content/themes/SPIEImages/Loading.gif",
            "https://projecteuclid.org/Content/themes/SPIEImages/InformationQuestionMark.png",
            "https://projecteuclid.org/Content/themes/SPIEImages/Loading.gif",
            "https://projecteuclid.org/images/Project%20Euclid%20Images/Header_ProjectEuclid_Logo.png",
            "https://projecteuclid.org/Content/themes/SPIEImages/Loading.gif",
            "https://projecteuclid.org/images/journals/cover_aoms.jpg",
            "https://projecteuclid.org/Content/themes/SPIEImages/OpenAccessIcon.svg",
            "https://projecteuclid.org/Content/themes/SPIEImages/OpenAccessIcon.svg",
            "https://projecteuclid.org/Content/themes/SPIEImages/OpenAccessIcon.svg",
            "https://projecteuclid.org/Content/themes/SPIEImages/OpenAccessIcon.svg",
            "https://projecteuclid.org/Content/themes/SPIEImages/OpenAccessIcon.svg",
            "https://projecteuclid.org/Content/themes/SPIEImages/OpenAccessIcon.svg",
            "https://projecteuclid.org/Content/themes/SPIEImages/OpenAccessIcon.svg",
            "https://projecteuclid.org/Content/themes/SPIEImages/OpenAccessIcon.svg",
            "https://projecteuclid.org/Content/themes/SPIEImages/OpenAccessIcon.svg",
            "https://projecteuclid.org/Content/themes/SPIEImages/OpenAccessIcon.svg",
            "https://projecteuclid.org/Content/themes/SPIEImages/OpenAccessIcon.svg",
            "https://projecteuclid.org/Content/themes/SPIEImages/OpenAccessIcon.svg",
            "https://projecteuclid.org/Content/themes/SPIEImages/OpenAccessIcon.svg",
            "https://projecteuclid.org/Content/themes/SPIEImages/OpenAccessIcon.svg",
            "https://projecteuclid.org/Content/themes/SPIEImages/OpenAccessIcon.svg",
            "https://projecteuclid.org/Content/themes/SPIEImages/OpenAccessIcon.svg",
            "https://projecteuclid.org/Content/themes/SPIEImages/OpenAccessIcon.svg",
            "https://projecteuclid.org/Content/themes/SPIEImages/OpenAccessIcon.svg",
            "https://projecteuclid.org/Content/themes/SPIEImages/OpenAccessIcon.svg",
            "https://projecteuclid.org/Content/themes/SPIEImages/OpenAccessIcon.svg",
            "https://projecteuclid.org/Content/themes/SPIEImages/OpenAccessIcon.svg",
            "https://projecteuclid.org/Content/themes/SPIEImages/OpenAccessIcon.svg",
            "https://projecteuclid.org/Content/themes/SPIEImages/OpenAccessIcon.svg",
            "https://projecteuclid.org/Content/themes/SPIEImages/OpenAccessIcon.svg",
            "https://projecteuclid.org/Content/themes/SPIEImages/OpenAccessIcon.svg",
            "https://projecteuclid.org/Content/themes/SPIEImages/OpenAccessIcon.svg",
            "https://projecteuclid.org/Content/themes/SPIEImages/OpenAccessIcon.svg",
            "https://projecteuclid.org/Content/themes/SPIEImages/OpenAccessIcon.svg",
            "https://projecteuclid.org/Content/themes/SPIEImages/OpenAccessIcon.svg",
            "https://projecteuclid.org/Content/themes/SPIEImages/OpenAccessIcon.svg",
            "https://projecteuclid.org/Content/themes/SPIEImages/OpenAccessIcon.svg",
            "https://projecteuclid.org/Content/themes/SPIEImages/OpenAccessIcon.svg",
            "https://projecteuclid.org/Content/themes/SPIEImages/OpenAccessIcon.svg",
            "https://projecteuclid.org/Content/themes/SPIEImages/OpenAccessIcon.svg",
            "https://projecteuclid.org/Content/themes/SPIEImages/OpenAccessIcon.svg",
            "https://projecteuclid.org/Content/themes/SPIEImages/OpenAccessIcon.svg",
            "https://projecteuclid.org/Content/themes/SPIEImages/OpenAccessIcon.svg",
            "https://projecteuclid.org/Content/themes/SPIEImages/OpenAccessIcon.svg",
            "https://projecteuclid.org/Content/themes/SPIEImages/OpenAccessIcon.svg",
            "https://projecteuclid.org/Content/themes/SPIEImages/OpenAccessIcon.svg",
            "https://projecteuclid.org/Content/themes/SPIEImages/OpenAccessIcon.svg",
            "https://projecteuclid.org/Images/Project Euclid/Back-Top_Icon.png",
            "https://projecteuclid.org/images/Global/X-logo-black.png",
            "https://projecteuclid.org/images/Project%20Euclid%20Images/PEDL_logo_footer.png",
            "https://projecteuclid.org/images/Project%20Euclid%20Images/Header_ProjectEuclid_Logo.png",
            "https://projecteuclid.org/Content/themes/SPIEImages/Close_Icon.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "The Annals of Mathematical Statistics",
        "meta_lang": "",
        "meta_favicon": "/favicon.png",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "A factorial experiment involving $m$ factors such that the $i$th factor has $m_i$ levels is termed an asymmetrical factorial design. If the number of levels is equal to one another the experiment is termed a symmetric factorial experiment. When the block size of the experiment permits only a sub-set of the factorial combinations to be assigned to the experimental units within a block, resort is made to the theory of confounding. With respect to symmetric factorial designs, the theory of confounding has been highly developed by Bose [1], Bose and Kishen [4], and Fisher [11], [12]. An excellent summary of the results of this research appears in Kempthorne [13]. However, these researches are closely related to Galois field theory resulting in (i) only symmetric factorial designs being incorporated into the current theory of confounding; (ii) the common level must be a prime (or power of a prime) number; and (iii) the block size must be a multiple of this prime number. The theory of confounding for asymmetric designs has not been developed to any great degree. Examples of asymmetric designs can be found in Yates [19], Cochran and Cox [9], Li [15], and Kempthorne [13]. Nair and Rao [16] have given the statistical analysis of a class of asymmetrical two-factor designs in considerable detail. Kramer and Bradley [14] discuss the application of group divisible designs to asymmetrical factorial experiments, however their paper is mainly confined to the two-factor case and its intra-block analysis. It is the purpose of this paper, which was done independently of their work, to outline the general theory for using the group divisible incomplete block designs for asymmetrical factorial experiments. The use of incomplete block designs for asymmetric factorial experiments results in (i) no restriction that the levels must be a prime (or power of a prime) number, (ii) no restriction with respect to the dependence of the block size on the type of level, and (iii) unlike the previous referenced works on asymmetric factorial designs, the resulting analysis is simple, does not increase in difficulty with an increasing number of factors, and \"automatically adjusts\" for the effects of partial confounding. Section 2 states three useful lemmas, Section 3 contains the main results of this paper, and Section 4 outlines the recovery of inter-block information.\n\nUsing a stochastic approximation procedure $\\{X_n\\}, n = 1, 2, \\cdots$, for a value $\\theta$, it seems likely that frequent fluctuations in the sign of $(X_n - \\theta) - (X_{n - 1} - \\theta) = X_n - X_{n - 1}$ indicate that $|X_n - \\theta|$ is small, whereas few fluctuations in the sign of $X_n - X_{n - 1}$ indicate that $X_n$ is still far away from $\\theta$. In view of this, certain approximation procedures are considered, for which the magnitude of the $n$th step (i.e., $X_{n + 1} - X_n$) depends on the number of changes in sign in $(X_i - X_{i - 1})$ for $i = 2, \\cdots, n$. In theorems 2 and 3, $$X_{n + 1} - X_n$$ is of the form $b_nZ_n$, where $Z_n$ is a random variable whose conditional expectation, given $X_1, \\cdots, X_n$, has the opposite sign of $X_n - \\theta$ and $b_n$ is a positive real number. $b_n$ depends in our processes on the changes in sign of $$X_i - X_{i - 1}(i \\leqq n)$$ in such a way that more changes in sign give a smaller $b_n$. Thus the smaller the number of changes in sign before the $n$th step, the larger we make the correction on $X_n$ at the $n$th step. These procedures may accelerate the convergence of $X_n$ to $\\theta$, when compared to the usual procedures ([3] and [5]). The result that the considered procedures converge with probability one may be useful for finding optimal procedures. Application to the Robbins-Monro procedure (Theorem 2) seems more interesting than application to the Kiefer-Wolfowitz procedure (Theorem 3).\n\nPlackett [1] has discussed the history and generalizations of the Gaussian theorem which states that least squares estimates are linear unbiased estimates with minimum variance. General forms of the theorem are due to Aitken [2], [3] and Rao [4], [5]. The essence of the proof for Aitken's general case consists in minimizing, simultaneously, certain quadratic forms involving linear combinations of the parameters. Plackett derived Aitken's result by using a matrix relation. The proof of the theorem follows quickly once the relation is established. A somewhat similar but simpler matrix relation is used by Rao ([4], page 10). Aitken [2] and Rao [4], [5] obtain minimum variance with the use of Lagrange multipliers. Unless one has a method of working with matrices of derivatives it seems necessary to differentiate with respect to the many scalars constituting the matrices and to assemble the results in desired matrix form. Authors frequently give only the assembled results ([4], page 10, [5], page 17, [6], page 83). The question arises as to whether it is possible to use the logically preferable matrix derivative methods of minimization. It is shown below that the use of matrices of partial derivatives [7] leads logically to the solution without the necessity of changing to and from scalar notation, or without the necessity of establishing some relation which implicitly contains the solution. Matrix derivative methods seem to be preferable methods for undertaking solutions of problems of simultaneous matrix minimization with side conditions for the same reason that derivative methods are preferable to the use of some (unknown) relation in solving problems of minimization involving scalars. They may also be used in establishing the relation which may then be verified without their use. The paper includes generalizations of the results of Aitken [2], [3], Rao [4], [5], and David and Neyman [8]. It gives a general formula for simultaneous unbiased estimators of linear functions of parameters when the parameters are subject to linear restrictions and shows how the results are applicable to special cases. It provides formulas for the variance matrix of these estimators. It generalizes a matrix relation used by Plackett [1]. It used the matrix square root transformation in establishing the general result for the variance of (weighted) residuals when there may be linear restrictions on the parameters. It provides a generalization of a formula of David and Neyman [8] in estimating the variance matrix of the unbiased linear estimators.\n\nLet $X_1 < X_2 < \\cdots < X_n$ be a sample of size $n$, ordered increasingly, of a one-dimensional random variable $X$ which has the continuous cumulative distribution function $F$. It is well known, [1], that the statistic \\begin{equation*}\\tag{1}D^+_n = \\sup_{-\\infty < x < + \\infty} \\{F_n(x) - F(x)\\},\\end{equation*} where $F_n(x)$ is the empirical distribution function determined by $X_1, X_2, \\cdots, X_n$, has a probability distribution independent of $F$. One may, therefore, assume that $X$ has the uniform distribution in (0, 1) and, observing that the supremum in (1) must be attained at one of the sample points, write without loss of generality \\begin{equation*}\\tag{2}D^+_n = \\max_{1 \\leqq i \\leqq n} (i/n - U_i),\\end{equation*} where $U_1 < U_2 < \\cdots < U_n$ is an ordered sample of a random variable with uniform distribution in (0, 1). For a given $n > 0$ define the random variable $i^{\\ast}$ as that value of $i$, determined uniquely with probability 1, for which the maximum in (2) is reached, i.e., such that \\begin{equation*}\\tag{3}D^+_n = i^{\\ast}/n - U_{i^{\\ast}},\\end{equation*} and write \\begin{equation*}\\tag{3.1} U_{i^{\\ast}} = U^{\\ast}.\\end{equation*} The main object of this paper is to obtain the distribution functions of $(i^{\\ast}, U^{\\ast})$, of $i^{\\ast}$ and of $U^{\\ast}$. The asymptotic distribution of $\\alpha_n = i^{\\ast}/n$ is also investigated, and bounds are obtained on the difference between the exact and the asymptotic distribution. A number of general identities, which are not commonly known, have been verified and used in proving the above-mentioned results. Since these identities may be helpful in other problems of this type, they are separated from the main proofs and appear in the next section.\n\nThe problem is that of estimating the trend of a normal process when the trend function is known up to a finite number of coefficients. That is, $$y_t = x_t + f(t),\\qquad 0 \\leqq t \\leqq T,$$ where $x_t$ is a normal process with mean zero and covariance function $$E\\lbrack X_u, X_v\\rbrack = C(u, v)$$ and $$f(t) = k_{1\\phi_1}(t) + \\cdots + k_s\\phi_s(t).$$ The $\\phi_i(t)$ are known functions and the $k_i$ are to be estimated. The standard procedure in such a case is to derive the estimates by the maximum likelihood method. However, if the covariance function $C(u, v)$ is not completely known, this is usually impossible, and it is essential to find an alternative procedure. The method of least squares has been proposed by Mann [1]. The estimates obtained by this method are independent of $C(u, v)$ and have the additional advantage of being easily computed. Mann and Moranda [2] showed that for the Ornstein Uhlenbeck process the asymptotic efficiency of the least square estimate relative to the maximum likelihood estimate is one, in the special case that the $\\phi_i(t)$ are polynomials or trigonometric polynomials. Mann defines the efficiency $\\bar e(T)$ of an estimate $\\bar f(t)$ $$\\bar e(T) = \\frac{E\\Big\\lbrack\\int^T_0 \\lbrack\\hat f(t) - f(t)\\rbrack^2 dt\\Big\\rbrack}{E\\Big\\lbrack\\int^T_0 \\lbrack\\hat f(t) - f(t)\\rbrack^2 dt\\Big\\rbrack},$$ where $\\hat f(t)$ is the maximum likelihood estimate. For the cases that shall be of particular interest--the Ornstein Uhlenbeck process with $\\hat f(t)$ a linear unbiased estimate--Mann and Moranda [2] have shown that $\\bar e(T) \\leqq 1$. In the present paper the asymptotic efficiency of the least square estimates will be computed for a wider class of functions $\\phi_i(t)$. It will be shown that except for a special case just slightly broader than the one treated by Mann and Moranda, the asymptotic efficiency is actually less than one. Thus except for this special case, the least square estimates could be improved upon. An alternative estimate $\\bar k_i(\\alpha)$ is proposed. It will be shown that for $a \\geqq \\beta$, where $\\beta$ is the true correlation parameter in the Ornstein Uhlenbeck process, the estimates $\\bar k_i(\\alpha)$ are asymptotically more efficient than the least square estimates, and in fact as $\\alpha \\rightarrow \\beta$ from above the efficiency increases (strictly) to one.\n\nThis paper deals with the unbiased estimation of the correlation of two variates having a bivariate normal distribution (Sec. 2), and of the intraclass correlation, i.e., the common correlation coefficient of a $p$-variate normal distribution with equal variances and equal covariances (Sec. 3). In both cases, the estimator has the following properties. It is a function of a complete sufficient statistic and is therefore the unique (except for sets of probability zero) minimum variance unbiased estimator. Its range is the region of possible values of the estimated quantity. It is a strictly increasing function of the usual estimator differing from it only by terms of order $1/n$ and consequently having the same asymptotic distribution. Since the unbiased estimators are cumbersome in form in that they are expressed as series or integrals, tables are included giving the unbiased estimators as functions of the usual estimators. In Sec. 4 we give an unbiased estimator of the squared multiple correlation. It has the properties mentioned in the second paragraph except that it may be negative, which the squared multiple correlation cannot. In each case the estimator is obtained by inverting a Laplace transform. We are grateful to W. H. Kruskal and L. J. Savage for very helpful comments and suggestions, and to R. R. Blough for his able computations.\n\nA point executes Brownian motion in a bounded, connected, and open three dimensional region $D$. When it reaches the boundary $\\Gamma$, at point $\\alpha$, it is instantaneously returned to $D$ according to probability measure $\\mu(\\alpha)$ (we write $\\mu(\\alpha, A)$ for the measure of set $A$), and the Brownian motion is resumed. This is a Markov process and, subject to certain regularity conditions on $\\Gamma$ and $\\mu(\\alpha)$, we derive the limiting distribution of the process. Processes of this sort have been considered by Feller [1]; he has obtained the transition probabilities of such processes. He is concerned more generally with Markov processes with continuous sample functions on a linear interval; the return may be instantaneous or after a random period of time. Let $p^0(t, \\xi, A)$ be the probability that the point is in set $A$ of $D$ at time $t$ when it is initially at point $\\xi$ of $D$, with the additional restriction that no boundary contacts have been made. It is known that \\begin{equation*}\\tag{1}p^0(t, \\xi, A) = \\int_A u(t, \\xi, x)dx,\\end{equation*} where $dx$ is the volume element about $x$ and $u$ is the solution of the equation $$\\frac{1}{2}\\Delta u = u_t,$$ subject to the conditions $$u(t, \\xi, \\alpha) = 0,\\quad\\alpha\\varepsilon\\Gamma,\\quad\\lim_{t \\rightarrow 0} \\int_C u(t, \\xi, x)dx = 1,$$ where $C$ is any sphere of non-zero radius with center $\\xi$ which is entirely within $D$. We may write explicitly $$u(t,\\xi,x) = \\sum^\\infty_{k = 1} v_k(\\xi)v_k(x)e^{-\\lambda_kt},$$ where $\\lambda_k$ is the $k$th eigenvalue and $v_k(x)$ the corresponding eigenfunction of the equation $\\Delta u + 2\\lambda u = 0$ subject to the boundary condition $u = 0$ on $\\Gamma$. If $K(\\xi, x)$ is the Green's function of $\\Delta u = 0$ in $D$, then ([2], and [3], page 273) \\begin{equation*}\\tag{2}K(\\xi, x) = \\frac{1}{2} \\int^\\infty_0 u(t, \\xi, x) dt.\\end{equation*} Let $\\phi(t, \\xi, \\alpha)dt d\\alpha$ be the probability the point is absorbed at surface element $d\\alpha$ of $\\Gamma$ between $t$ and $t + dt$ when it is initially at point $\\xi$ of $D$. Then $\\phi$ is half the interior normal derivative of $u$ at point $\\alpha$ of $\\Gamma$ ([3], page 273). When the point is initially at $\\xi$ the probability of ultimate absorption in set $S$ of $\\Gamma$ is given by \\begin{equation*}\\tag{3}\\pi(\\xi,S) = \\int_S \\int^\\infty_0 \\phi(t, \\xi, \\alpha) dt d\\alpha.\\end{equation*} We may define a discrete parameter Markov process with $\\Gamma$ as state space by taking as transition probability \\begin{equation*}\\tag{4}\\pi(\\alpha, S) = \\int_D \\pi(\\xi, S)\\mu(\\alpha, d\\xi)\\end{equation*} This Markov process has a limiting distribution $\\pi$ which satisfies the equation \\begin{equation*}\\tag{5}\\pi(S) = \\int_\\Gamma \\pi(\\alpha, S)\\pi(d\\alpha).\\end{equation*} We define a measure of sets of $D$ by $$\\lambda(A) = \\int_\\Gamma \\mu(\\alpha, A)\\pi(d\\alpha).$$ We may now write the density function for the limiting distribution. If $M(\\xi)$ is the mean time of reaching the boundary when the point is initially at $\\xi$, $$M(\\xi) = \\int_\\Gamma \\int^\\infty_0 t_\\phi(t, \\xi, \\alpha) dt d\\alpha.$$ then the density function of the limiting distribution is \\begin{equation*}\\tag{6}\\frac{2 \\int_D K(\\xi, x)\\lambda(d\\xi)}{\\int_D M(\\xi)\\lambda(d\\xi)}.\\end{equation*} If we are given a probability measure $\\lambda$ in $D$ and the return is always according to $\\lambda$, then it is clear that the limiting density of this process is also given by (6). If $\\lambda$ concentrates at a single point $\\xi$ we may drop the integrals in (6), and in particular we get $$M(\\xi) = 2 \\int_D K(\\xi, x)dx.$$ We note that (6) is essentially the steady distribution of temperature in the following problem: $D$ is a homogeneous heat conducting body whose boundary is kept at temperature 0 and in which there is a constant source of heat distributed according to $\\lambda$. Regarding the regularity conditions, we shall assume that $\\Gamma$ is made up of finitely many surfaces, each with a continuously turning tangent plane and that $D$ has a Green's function ([4], page 262). We will assume there is a closed set $B$ in $D$ such that $$\\inf_{\\alpha \\varepsilon \\Gamma} \\mu(\\alpha, B) = \\gamma > 0.$$\n\nSuppose $X(1, 1), X(1, 2), \\cdots, X(1, n_1), X(2, 1), \\cdots, X(2, n_2), \\cdots, X(k, 1), \\cdots, X(k, n_k)$ are independent chance variables, $X(i, j)$ having the probability density function $f_i(x)$, for $j = 1, \\cdots, n_i, i = 1, \\cdots, k$. We assume that for each $i, f_i(x)$ is bounded and has at most a finite number of discontinuities. We denote $n_1 + n_2 + \\cdots + n_k$ by $N$, and we assume that $n_i/N$ is equal to $r_i$, where $r_i$ is a given positive number. Let $Y_1 \\leqq Y_2 \\leqq \\cdots \\leqq Y_N$ denote the ordered values of the $N$ observations $$X(1, 1), \\cdots, X(k, n_k).$$ Define $W_i$ as $Y_{i + 1} - Y_i$ for $i = 1, \\cdots, N - 1$. For any given nonnegative $t$, let $R_N(t)$ denote the proportion of the values $W_1, \\cdots, W_{N - 1}$ which are greater than $t/N$. Let $S(t)$ denote $$\\int^\\infty_{-\\infty} (r_1f_1(x) + r_2f_2(x) + \\cdots + r_kf_k(x)) \\exp \\{-t\\lbrack r_1f_1(x) + \\cdots + r_kf_k(x)\\rbrack\\} dx$$ and $V(N)$ denote $\\sup_{t \\geqq 0}|R_N(t) - S(t)|$. Then it is shown that $V(N)$ converges stochastically to zero as $N$ increases. This is a generalization of [1], where $k$ was equal to unity. The result is applied to find the asymptotic behavior of ranks in a $k$-sample problem."
    }
}
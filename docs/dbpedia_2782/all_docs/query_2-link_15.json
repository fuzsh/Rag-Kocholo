{
    "id": "dbpedia_2782_2",
    "rank": 15,
    "data": {
        "url": "https://learn.microsoft.com/en-us/azure/azure-video-indexer/video-indexer-overview",
        "read_more_link": "",
        "language": "en",
        "title": "What is Azure AI Video Indexer?",
        "top_image": "https://learn.microsoft.com/en-us/media/open-graph-image.png",
        "meta_img": "https://learn.microsoft.com/en-us/media/open-graph-image.png",
        "images": [
            "https://learn.microsoft.com/en-us/azure/azure-video-indexer/media/video-indexer-overview/model-chart.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2024-08-22T08:00:00+00:00",
        "summary": "",
        "meta_description": "This article gives an overview of the Azure AI Video Indexer service.",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": "https://learn.microsoft.com/en-us/azure/azure-video-indexer/video-indexer-overview",
        "text": "Azure AI Video Indexer overview\n\nAzure AI Video Indexer is a cloud application, part of Azure AI services, built on Azure AI services (such as the Face, Translator, Azure AI Vision, and Speech). It enables you to extract the insights from your videos using Azure AI Video Indexer video and audio models.\n\nAzure AI Video Indexer analyzes the video and audio content by running 30+ AI models, generating rich insights. Here is an illustration of the audio and video analysis performed by Azure AI Video Indexer in the background:\n\nTo start extracting insights with Azure AI Video Indexer, see the how can I get started section.\n\nWhat can I do with Azure AI Video Indexer?\n\nAzure AI Video Indexer's insights can be applied to many scenarios:\n\nDeep search: Use the insights extracted from the video to enhance the search experience across a video library. For example, indexing spoken words and faces can enable the search experience of finding moments in a video where a person spoke certain words or when two people were seen together. Search based on such insights from videos is applicable to news agencies, educational institutes, broadcasters, entertainment content owners, enterprise LOB apps, and in general to any industry that has a video library that users need to search against.\n\nContent creation: Create trailers, highlight reels, social media content, or news clips based on the insights Azure AI Video Indexer extracts from your content. Keyframes, scenes markers, and timestamps of the people and label appearances make the creation process smoother and easier, enabling you to easily get to the parts of the video you need when creating content.\n\nAccessibility: Whether you want to make your content available for people with disabilities or if you want your content to be distributed to different regions using different languages, you can use the transcription and translation provided by Azure AI Video Indexer in multiple languages.\n\nMonetization: Azure AI Video Indexer can help increase the value of videos. For example, industries that rely on ad revenue (news media, social media, and so on) can deliver relevant ads by using the extracted insights as additional signals to the ad server.\n\nContent moderation: Use textual and visual content moderation models to keep your users safe from inappropriate content and validate that the content you publish matches your organization's values. You can automatically block certain videos or alert your users about the content.\n\nRecommendations: Video insights can be used to improve user engagement by highlighting the relevant video moments to users. By tagging each video with additional metadata, you can recommend to users the most relevant videos and highlight the parts of the video that matches their needs.\n\nVideo/audio AI features\n\nThe following list shows the insights you can retrieve from your video/audio files using Azure AI Video Indexer video and audio AI features (models).\n\nNote\n\nGiven privacy and regulatory requirements, some of these features have restricted use and/or require authorization to full utilize.\n\nUnless specified otherwise, a model is generally available.\n\nVideo models\n\nFace detection: Detects and groups faces appearing in the video.\n\nCelebrity identification: Identifies over 1 million celebritiesâlike world leaders, actors, artists, athletes, researchers, business, and tech leaders across the globe. The data about these celebrities can also be found on various websites (IMDB, Wikipedia, and so on).\n\nAccount-based face identification: Trains a model for a specific account. It then recognizes faces in the video based on the trained model. For more information, see Customize a Person model from the Azure AI Video Indexer website and Customize a Person model with the Azure AI Video Indexer API.\n\nThumbnail extraction for faces: Identifies the best captured face in each group of faces (based on quality, size, and frontal position) and extracts it as an image asset.\n\nOptical character recognition (OCR):â¯Extracts text from images like pictures, street signs and products in media files to create insights.\n\nVisual content moderation: Detects adult and/or racy visuals.\n\nLabels identification: Identifies visual objects and actions displayed.\n\nScene segmentation: Determines when a scene changes in video based on visual cues. A scene depicts a single event and it's composed by a series of consecutive shots, which are semantically related.\n\nShot detection: Determines when a shot changes in video based on visual cues. A shot is a series of frames taken from the same motion-picture camera. For more information, see Scenes, shots, and keyframes.\n\nBlack frame detection: Identifies black frames presented in the video.\n\nKeyframe extraction: Detects stable keyframes in a video.\n\nRolling credits: Identifies the beginning and end of the rolling credits in the end of TV shows and movies.\n\nEditorial shot type detection: Tags shots based on their type (like wide shot, medium shot, close up, extreme close up, two shot, multiple people, outdoor and indoor, and so on). For more information, see Editorial shot type detection.\n\nObserved people detection: Detects observed people in videos and provides information such as the location of the person in the video frame (using bounding boxes) and the exact timestamp (start, end) and confidence when a person appears. For more information, see Trace observed people in a video.\n\nMatched person: Matches people that were observed in the video with the corresponding faces detected. The matching between the observed people and the faces contain a confidence level.\n\nDetected clothing: Detects the clothing types of people appearing in the video and provides information such as long or short sleeves, long or short pants and skirt or dress. The detected clothing is associated with the people wearing it and the exact timestamp (start, end) along with a confidence level for the detection are provided.\n\nFeatured clothing: Captures featured clothing images appearing in a video. You can improve your targeted ads by using the featured clothing insight. For information on how the featured clothing images are ranked and how to get the insights, see featured clothing.\n\nObject detection Detects unique objects that are also tracked so that if they return to the frame they are recognized. See Azure AI Video Indexer object detection\n\nSlate detection: Identifies the following movie post-production insights when indexing a video using the advanced indexing option:\n\nClapperboard detection with metadata extraction.\n\nDigital patterns detection, including color bars.\n\nTextless slate detection, including scene matching.\n\nFor details, see Slate detection.\n\nTextual logo detection: Matches a specific predefined text using Azure AI Video Indexer OCR. For example, if a user created a textual logo: \"Microsoft\", different appearances of the word Microsoft will be detected as the \"Microsoft\" logo. For more information, see Detect textual logo.\n\nAudio models\n\nAudio transcription: Converts speech to text over 50 languages and allows extensions. For more information, see Azure AI Video Indexer language support.\n\nAutomatic language detection: Identifies the dominant spoken language. For more information, see Azure AI Video Indexer language support. If the language can't be identified with confidence, Azure AI Video Indexer assumes the spoken language is English.\n\nMulti-language speech identification and transcription: Identifies the spoken language in different segments from audio. It sends each segment of the media file to be transcribed and then combines the transcription back to one unified transcription. For more information about transcription see Transcription\n\nClosed captioning: Creates closed captioning in three formats: VTT, TTML, SRT.\n\nTwo channel processing: Auto detects separate transcript and merges to single timeline.\n\nNoise reduction: Clears up telephony audio or noisy recordings (based on Skype filters).\n\nTranscript customization (CRIS): Trains custom speech to text models to create industry-specific transcripts. For more information, see Customize a Language model.\n\nSpeaker enumeration: Maps and understands which speaker spoke which words and when. Sixteen speakers can be detected in a single audio-file.\n\nSpeaker statistics: Provides statistics for speakers' speech ratios.\n\nTextual content moderation: Detects explicit text in the audio transcript.\n\nText-based emotion detection: Emotions such as joy, sadness, anger, and fear that were detected via transcript analysis.\n\nTranslation: Creates translations of the audio transcript to many different languages. For more information, see Azure AI Video Indexer language support.\n\nAudio effects detection: Detects the following audio effects in the non-speech segments of the content: alarm or siren, dog barking, crowd reactions (cheering, clapping, and booing), gunshot or explosion, laughter, breaking glass, and silence.\n\nThe detected acoustic events are in the closed captions file. The file can be downloaded from the Azure AI Video Indexer website. For more information, see Audio effects detection.\n\nNote\n\nThe full set of events is available only when you choose Advanced Audio Analysis when uploading a file, in upload preset. By default, only silence is detected.\n\nAudio and video models (multi-channels)\n\nWhen indexing by one channel, partial results for those models are available.\n\nKeywords extraction: Extracts keywords from speech and visual text.\n\nNamed entities extraction: Extracts brands, locations, and people from speech and visual text via natural language processing (NLP).\n\nTopic inference: Extracts topics based on various keywords (that is, keywords 'Stock Exchange', 'Wall Street' produces the topic 'Economics'). The model uses three different ontologies (IPTC, Wikipedia and the Video Indexer hierarchical topic ontology). The model uses transcription (spoken words), OCR content (visual text), and celebrities recognized in the video using the Video Indexer facial recognition model.\n\nArtifacts: Extracts rich set of \"next level of details\" artifacts for each of the models.\n\nSentiment analysis: Identifies positive, negative, and neutral sentiments from speech and visual text.\n\nHow can I get started with Azure AI Video Indexer?\n\nLearn how to get started with Azure AI Video Indexer.\n\nOnce you set up, start using insights and check out other How to guides.\n\nCompliance, privacy and security\n\nNote\n\nOn June 11, 2020, Microsoft announced that it will not sell facial recognition technology to police departments in the United States until strong regulation, grounded in human rights, has been enacted. As such, customers may not use facial recognition features or functionality included in Azure AI services, such as Face or Video Indexer, if a customer is, or is allowing use of such services by or for, a police department in the United States.\n\nYou must comply with all applicable laws in your use of Azure AI Video Indexer, and you may not use Azure AI Video Indexer or any Azure service in a manner that violates the rights of others, or that may be harmful to others.\n\nBefore uploading any video/image to Azure AI Video Indexer, You must have all the appropriate and legal rights to use the video/image, including, where required by law, all the necessary consents from individuals (if any) in the video/image, for the use, processing, and storage of their data in Azure AI Video Indexer and Azure. Some jurisdictions may impose special legal requirements for the collection, online processing and storage of certain categories of data, such as biometric data. Before using Azure AI Video Indexer and Azure for the processing and storage of any data subject to special legal requirements, you must ensure your use complies with all such legal requirements that may apply to You and your intended use."
    }
}
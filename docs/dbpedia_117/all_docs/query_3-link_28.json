{
    "id": "dbpedia_117_3",
    "rank": 28,
    "data": {
        "url": "https://www.quantstart.com/articles/Bayesian-Statistics-A-Beginners-Guide/",
        "read_more_link": "",
        "language": "en",
        "title": "Bayesian Statistics: A Beginner's Guide",
        "top_image": "https://www.quantstart.com/static/images/favicon.png",
        "meta_img": "https://www.quantstart.com/static/images/favicon.png",
        "images": [
            "https://quantstartmedia.s3.amazonaws.com/images/article-images/articles/bayesian-statistics-a-beginners-guide/qs-bayes-bernoulli.png",
            "https://www.quantstart.com/static/images/qsalpha-sidebar-advert-small.png",
            "https://www.quantstart.com/static/images/quantcademy-sidebar-advert-small.png",
            "https://www.quantstart.com/static/images/sat-sidebar-advert-small.png",
            "https://www.quantstart.com/static/images/aat-sidebar-advert-small.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "Bayesian Statistics: A Beginner's Guide",
        "meta_lang": "en",
        "meta_favicon": "/static/images/favicon.png",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Article updated April 2022 for Python 3.8\n\nOver the last few years we have spent a good deal of time on QuantStart considering option price models, time series analysis and quantitative trading. It has become clear to me that many of you are interested in learning about the modern mathematical techniques that underpin not only quantitative finance and algorithmic trading, but also the newly emerging fields of data science and statistical machine learning.\n\nQuantitative skills are now in high demand not only in the financial sector but also at consumer technology startups, as well as larger data-driven firms. Hence we are going to expand the topics discussed on QuantStart to include not only modern financial techniques, but also statistical learning as applied to other areas, in order to broaden your career prospects if you are quantitatively focused.\n\nIn order to begin discussing the modern techniques, we must first gain a solid understanding in the underlying mathematics and statistics that underpins these models. One of the key modern areas is that of Bayesian Statistics. We have not yet discussed Bayesian methods in any great detail on the site. This article has been written to help you understand the \"philosophy\" of the Bayesian approach, how it compares to the traditional/classical frequentist approach to statistics and the potential applications in both quantitative finance and data science.\n\nIn the article we will:\n\nDefine Bayesian statistics (or Bayesian inference)\n\nCompare Classical (\"Frequentist\") statistics and Bayesian statistics\n\nDerive the famous Bayes' rule, an essential tool for Bayesian inference\n\nInterpret and apply Bayes' rule for carrying out Bayesian inference\n\nCarry out a concrete probability coin-flip example of Bayesian inference\n\nWhat is Bayesian Statistics?\n\nBayesian statistics is a particular approach to applying probability to statistical problems. It provides us with mathematical tools to update our beliefs about random events in light of seeing new data or evidence about those events.\n\nIn particular Bayesian inference interprets probability as a measure of believability or confidence that an individual may possess about the occurance of a particular event.\n\nWe may have a prior belief about an event, but our beliefs are likely to change when new evidence is brought to light. Bayesian statistics gives us a solid mathematical means of incorporating our prior beliefs, and evidence, to produce new posterior beliefs.\n\nBayesian statistics provides us with mathematical tools to rationally update our subjective beliefs in light of new data or evidence.\n\nThis is in contrast to another form of statistical inference, known as classical or frequentist statistics, which assumes that probabilities are the frequency of particular random events occuring in a long run of repeated trials.\n\nFor example, as we roll a fair (i.e. unweighted) six-sided die repeatedly, we would see that each number on the die tends to come up 1/6 of the time.\n\nFrequentist statistics assumes that probabilities are the long-run frequency of random events in repeated trials.\n\nWhen carrying out statistical inference, that is, inferring statistical information from probabilistic systems, the two approaches - frequentist and Bayesian - have very different philosophies.\n\nFrequentist statistics tries to eliminate uncertainty by providing estimates. Bayesian statistics tries to preserve and refine uncertainty by adjusting individual beliefs in light of new evidence.\n\nFrequentist vs Bayesian Examples\n\nIn order to make clear the distinction between the two differing statistical philosophies, we will consider two examples of probabilistic systems:\n\nCoin flips - What is the probability of an unfair coin coming up heads?\n\nElection of a particular candidate for UK Prime Minister - What is the probability of seeing an individual candidate winning, who has not stood before?\n\nThe following table describes the alternative philosophies of the frequentist and Bayesian approaches:\n\nExample Frequentist Interpretation Bayesian Interpretation Unfair Coin Flip The probability of seeing a head when the unfair coin is flipped is the long-run relative frequency of seeing a head when repeated flips of the coin are carried out. That is, as we carry out more coin flips the number of heads obtained as a proportion of the total flips tends to the \"true\" or \"physical\" probability of the coin coming up as heads. In particular the individual running the experiment does not incorporate their own beliefs about the fairness of other coins. Prior to any flips of the coin an individual may believe that the coin is fair. After a few flips the coin continually comes up heads. Thus the prior belief about fairness of the coin is modified to account for the fact that three heads have come up in a row and thus the coin might not be fair. After 500 flips, with 400 heads, the individual believes that the coin is very unlikely to be fair. The posterior belief is heavily modified from the prior belief of a fair coin. Election of Candidate The candidate only ever stands once for this particular election and so we cannot perform \"repeated trials\". In a frequentist setting we construct \"virtual\" trials of the election process. The probability of the candidate winning is defined as the relative frequency of the candidate winning in the \"virtual\" trials as a fraction of all trials. An individual has a prior belief of a candidate's chances of winning an election and their confidence can be quantified as a probability. However another individual could also have a separate differing prior belief about the same candidate's chances. As new data arrives, both beliefs are (rationally) updated by the Bayesian procedure.\n\nThus in the Bayesian interpretation a probability is a summary of an individual's opinion. A key point is that different (intelligent) individuals can have different opinions (and thus different prior beliefs), since they have differing access to data and ways of interpreting it. However, as both of these individuals come across new data that they both have access to their (potentially differing) prior beliefs will lead to posterior beliefs that will begin converging towards each other under the rational updating procedure of Bayesian inference.\n\nIn the Bayesian framework an individual would apply a probability of 0 when they have no confidence in an event occuring, while they would apply a probability of 1 when they are absolutely certain of an event occuring. A probability assigned between 0 and 1 allows weighted confidence in other potential outcomes.\n\nIn order to carry out Bayesian inference, we need to utilise a famous theorem in probability known as Bayes' rule and interpret it in the correct fashion. In the following box, we derive Bayes' rule using the definition of conditional probability. However, it isn't essential to follow the derivation in order to use Bayesian methods, so feel free to skip the box if you wish to jump straight into learning how to use Bayes' rule.\n\nApplying Bayes' Rule for Bayesian Inference\n\nAs we stated at the start of this article the basic idea of Bayesian inference is to continually update our prior beliefs about events as new evidence is presented. This is a very natural way to think about probabilistic events. As more and more evidence is accumulated our prior beliefs are steadily \"washed out\" by any new data.\n\nConsider a (rather nonsensical) prior belief that the Moon is going to collide with the Earth. For every night that passes, the application of Bayesian inference will tend to correct our prior belief to a posterior belief that the Moon is less and less likely to collide with the Earth, since it remains in orbit.\n\nIn order to demonstrate a concrete numerical example of Bayesian inference it is necessary to introduce some new notation.\n\nFirstly, we need to consider the concept of parameters and models. A parameter could be the weighting of an unfair coin, which we could label as $\\theta$. Thus $\\theta = P(H)$ would describe the probability distribution of our beliefs that the coin will come up as heads when flipped. The model is the actual means of encoding this flip mathematically. In this instance, the coin flip can be modelled as a Bernoulli trial.\n\nBernoulli Trial\n\nA Bernoulli trial is a random experiment with only two outcomes, usually labelled as \"success\" or \"failure\", in which the probability of the success is exactly the same every time the trial is carried out. The probability of the success is given by $\\theta$, which is a number between 0 and 1. Thus $\\theta \\in [0,1]$.\n\nOver the course of carrying out some coin flip experiments (repeated Bernoulli trials) we will generate some data, $D$, about heads or tails.\n\nA natural example question to ask is \"What is the probability of seeing 3 heads in 8 flips (8 Bernoulli trials), given a fair coin ($\\theta=0.5$)?\".\n\nA model helps us to ascertain the probability of seeing this data, $D$, given a value of the parameter $\\theta$. The probability of seeing data $D$ under a particular value of $\\theta$ is given by the following notation: $P(D|\\theta)$.\n\nHowever, if you consider it for a moment, we are actually interested in the alternative question - \"What is the probability that the coin is fair (or unfair), given that I have seen a particular sequence of heads and tails?\".\n\nThus we are interested in the probability distribution which reflects our belief about different possible values of $\\theta$, given that we have observed some data $D$. This is denoted by $P(\\theta|D)$. Notice that this is the converse of $P(D|\\theta)$. So how do we get between these two probabilities? It turns out that Bayes' rule is the link that allows us to go between the two situations.\n\nThe entire goal of Bayesian inference is to provide us with a rational and mathematically sound procedure for incorporating our prior beliefs, with any evidence at hand, in order to produce an updated posterior belief. What makes it such a valuable technique is that posterior beliefs can themselves be used as prior beliefs under the generation of new data. Hence Bayesian inference allows us to continually adjust our beliefs under new data by repeatedly applying Bayes' rule.\n\nThere was a lot of theory to take in within the previous two sections, so I'm now going to provide a concrete example using the age-old tool of statisticians: the coin-flip.\n\nCoin-Flipping Example\n\nIn this example we are going to consider multiple coin-flips of a coin with unknown fairness. We will use Bayesian inference to update our beliefs on the fairness of the coin as more data (i.e. more coin flips) becomes available. The coin will actually be fair, but we won't learn this until the trials are carried out. At the start we have no prior belief on the fairness of the coin, that is, we can say that any level of fairness is equally likely.\n\nIn statistical language we are going to perform $N$ repeated Bernoulli trials with $\\theta = 0.5$. We will use a uniform distribution as a means of characterising our prior belief that we are unsure about the fairness. This states that we consider each level of fairness (or each value of $\\theta$) to be equally likely.\n\nWe are going to use a Bayesian updating procedure to go from our prior beliefs to posterior beliefs as we observe new coin flips. This is carried out using a particularly mathematically succinct procedure using conjugate priors. We won't go into any detail on conjugate priors within this article, as it will form the basis of the next article on Bayesian inference. It will however provide us with the means of explaining how the coin flip example is carried out in practice.\n\nThe uniform distribution is actually a more specific case of another probability distribution, known as a Beta distribution. Conveniently, under the binomial model, if we use a Beta distribution for our prior beliefs it leads to a Beta distribution for our posterior beliefs. This is an extremely useful mathematical result, as Beta distributions are quite flexible in modelling beliefs. However, I don't want to dwell on the details of this too much here, since we will discuss it in the next article. At this stage, it just allows us to easily create some visualisations below that emphasises the Bayesian procedure!\n\nIn the following figure we can see 6 particular points at which we have carried out a number of Bernoulli trials (coin flips). In the first sub-plot we have carried out no trials and hence our probability density function (in this case our prior density) is the uniform distribution. It states that we have equal belief in all values of $\\theta$ representing the fairness of the coin.\n\nThe next panel shows 2 trials carried out and they both come up heads. Our Bayesian procedure using the conjugate Beta distributions now allows us to update to a posterior density. Notice how the weight of the density is now shifted to the right hand side of the chart. This indicates that our prior belief of equal likelihood of fairness of the coin, coupled with 2 new data points, leads us to believe that the coin is more likely to be unfair (biased towards heads) than it is tails.\n\nThe following two panels show 10 and 20 trials respectively. Notice that even though we have seen 2 tails in 10 trials we are still of the belief that the coin is likely to be unfair and biased towards heads. After 20 trials, we have seen a few more tails appear. The density of the probability has now shifted closer to $\\theta=P(H)=0.5$. Hence we are now starting to believe that the coin is possibly fair.\n\nAfter 50 and 500 trials respectively, we are now beginning to believe that the fairness of the coin is very likely to be around $\\theta=0.5$. This is indicated by the shrinking width of the probability density, which is now clustered tightly around $\\theta=0.46$ in the final panel. Were we to carry out another 500 trials (since the coin is actually fair) we would see this probability density become even tighter and centred closer to $\\theta=0.5$.\n\nThus it can be seen that Bayesian inference gives us a rational procedure to go from an uncertain situation with limited information to a more certain situation with significant amounts of data. In the next article we will discuss the notion of conjugate priors in more depth, which heavily simplify the mathematics of carrying out Bayesian inference in this example.\n\nFor completeness, I've provided the Python code (heavily commented) for producing this plot. It makes use of SciPy's statistics model, in particular, the Beta distribution:\n\nimport numpy as np from scipy import stats from matplotlib import pyplot as plt if __name__ == \"__main__\": # Create a list of the number of coin tosses (\"Bernoulli trials\") number_of_trials = [0, 2, 10, 20, 50, 500] # Conduct 500 coin tosses and output into a list of 0s and 1s # where 0 represents a tail and 1 represents a head data = stats.bernoulli.rvs(0.5, size=number_of_trials[-1]) # Discretise the x-axis into 100 separate plotting points x = np.linspace(0, 1, 100) # Loops over the number_of_trials list to continually add # more coin toss data. For each new set of data, we update # our (current) prior belief to be a new posterior. This is # carried out using what is known as the Beta-Binomial model. # For the time being, we won't worry about this too much. It # will be the subject of a later article! for i, N in enumerate(number_of_trials): # Accumulate the total number of heads for this # particular Bayesian update heads = data[:N].sum() # Create an axes subplot for each update ax = plt.subplot(int(len(number_of_trials) / 2), 2, i + 1) ax.set_title(\"%s trials, %s heads\" % (N, heads)) # Add labels to both axes and hide labels on y-axis plt.xlabel(\"$P(H)$, Probability of Heads\") plt.ylabel(\"Density\") if i == 0: plt.ylim([0.0, 2.0]) plt.setp(ax.get_yticklabels(), visible=False) # Create and plot a Beta distribution to represent the # posterior belief in fairness of the coin. y = stats.beta.pdf(x, 1 + heads, 1 + N - heads) plt.plot(x, y, label=\"observe %d tosses,\\n %d heads\" % (N, heads)) plt.fill_between(x, 0, y, color=\"#aaaadd\", alpha=0.5) # Expand plot to cover full width/height and show it plt.tight_layout() plt.show()\n\nI'd like to give special thanks to my good friend Jonathan Bartlett, who runs TheStatsGeek.com, for reading drafts of this article and for providing helpful advice on interpretation and corrections. Thanks Jon!\n\nRelated Articles"
    }
}
Abstract Musical genres are categorical labels created by humans to characterize pieces of music. A musical genre is characterized by the common characteristics shared by its members. These characteristics typically are related to the instrumentation, rhythmic structure, and harmonic content of the music. Genre hierarchies are commonly used to structure the large collections of music available on the Web. Currently musical genre annotation is performed manually.

Existing audio tools handle the increasing amount of computer audio data inadequately. The typical tape-recorder paradigm for audio interfaces is inflexible and time consuming, especially for large data sets. On the other hand, completely automatic audio analysis and annotation is impossible using current techniques. Alternative solutions are semi-automatic user interfaces that let users interact with sound in flexible ways based on content.

Since the 1950's, Sound and Music Computing (SMC) research has been producing a profound impact on the development of culture and technology in our post-industrial society. SMC research approaches the whole sound and music communication chain from a multidisciplinary point of view. By combining scientific, technological and artistic methodologies it aims at understanding, modelling, representing and producing sound and music using computational approaches. This book, by describing the state of the art in SMC research, gives hints of future developments, whose general purpose will be to bridge the semantic gap, the hiatus that currently separates sound from sense and sense from soun

In order to represent musical content, pitch and timing information is utilized in the majority of existing work in Symbolic Music Information Retrieval (MIR). Symbolic representations such as MIDI allow the easy calculation of such information and its manipulation. In contrast, most of the existing work in Audio MIR uses timbral and beat information, which can be calculated using automatic computer audition techniques.

Machines have the power and potential to make expressive music on their own. This thesis aims to computationally model the process of creating music using experience from listening to examples. Our unbiased signal-based solution models the life cycle of listening, composing, and performing, turning the machine into an active musician, instead of simply an instrument. We accomplish this through an analysis-synthesis technique by combined perceptual and structural modeling of the musical surface, which leads to a minimal data representation. We introduce a music cognition framework that results from the interaction of psychoacoustically grounded causal listening, a time-lag embedded feature representation, and perceptual similarity clustering. Our bottom-up analysis intends to be generic and uniform by recursively revealing metrical hierarchies and structures of pitch, rhythm, and timbre. Training is suggested for top-down unbiased supervision, and is demonstrated with the prediction ...

State-of-the-art MIR issues are presented and discussed both from the symbolic and audio points of view. As for the symbolic aspects, different approaches are presented in order to provide an overview of the different available solutions for particular MIR tasks. This section ends with an overview of MX, the IEEE standard XML language specifically designed to support interchange between musical notation, performance, analysis, and retrieval applications. As for the audio level, first we focus on blind tasks like beat and tempo tracking, pitch tracking and automatic recognition of musical instruments. Then we present algorithms that work both on compressed and uncompressed data. We analyze the relationships between MIR and feature extraction presenting examples of possible applications. Finally we focus on automatic music synchronization and we introduce a new audio player that supports the MX logic layer and allows to play both score and audio coherently.

Abstract Existing audio tools handle the increasing amount of computer audio data inadequately. The typical tape-recorder paradigm for audio interfaces is inflexible and time consuming, especially for large data sets. On the other hand, completely automatic audio analysis and annotation is impossible using current techniques. Alternative solutions are semi-automatic user interfaces that let users interact with sound in flexible ways based on content.
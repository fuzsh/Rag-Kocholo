{
    "id": "dbpedia_7620_1",
    "rank": 59,
    "data": {
        "url": "https://arxiv.org/html/2307.15337v3",
        "read_more_link": "",
        "language": "en",
        "title": "Skeleton-of-Thought: Prompting LLMs for Efficient Parallel Generation",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://arxiv.org/html/x1.png",
            "https://arxiv.org/html/x2.png",
            "https://arxiv.org/html/x3.png",
            "https://arxiv.org/html/x4.png",
            "https://arxiv.org/html/x5.png",
            "https://arxiv.org/html/x6.png",
            "https://arxiv.org/html/x7.png",
            "https://arxiv.org/html/x8.png",
            "https://arxiv.org/html/x9.png",
            "https://arxiv.org/html/x10.png",
            "https://arxiv.org/html/x11.png",
            "https://arxiv.org/html/x12.png",
            "https://arxiv.org/html/x13.png",
            "https://arxiv.org/html/x14.png",
            "https://arxiv.org/html/x15.png",
            "https://arxiv.org/html/x16.png",
            "https://arxiv.org/html/x17.png",
            "https://arxiv.org/html/x18.png",
            "https://arxiv.org/html/x19.png",
            "https://arxiv.org/html/x20.png",
            "https://arxiv.org/html/x21.png",
            "https://arxiv.org/html/x22.png",
            "https://arxiv.org/html/x23.png",
            "https://arxiv.org/html/x24.png",
            "https://arxiv.org/html/x25.png",
            "https://arxiv.org/html/x26.png",
            "https://arxiv.org/html/x27.png",
            "https://arxiv.org/html/x28.png",
            "https://arxiv.org/html/x29.png",
            "https://arxiv.org/html/x30.png",
            "https://arxiv.org/html/x31.png",
            "https://arxiv.org/html/x32.png",
            "https://arxiv.org/html/x33.png",
            "https://arxiv.org/html/x34.png",
            "https://arxiv.org/html/x35.png",
            "https://arxiv.org/html/x36.png",
            "https://arxiv.org/html/x37.png",
            "https://arxiv.org/html/x38.png",
            "https://arxiv.org/html/x39.png",
            "https://arxiv.org/html/x40.png",
            "https://arxiv.org/html/x41.png",
            "https://arxiv.org/html/x42.png",
            "https://arxiv.org/html/x43.png",
            "https://arxiv.org/html/x44.png",
            "https://arxiv.org/html/x45.png",
            "https://arxiv.org/html/x46.png",
            "https://arxiv.org/html/x47.png",
            "https://arxiv.org/html/x48.png",
            "https://arxiv.org/html/x49.png",
            "https://arxiv.org/html/x50.png",
            "https://arxiv.org/html/x51.png",
            "https://arxiv.org/html/x52.png",
            "https://arxiv.org/html/x53.png",
            "https://arxiv.org/html/x54.png",
            "https://arxiv.org/html/x55.png",
            "https://arxiv.org/html/x56.png",
            "https://arxiv.org/html/x57.png",
            "https://arxiv.org/html/x58.png",
            "https://arxiv.org/html/x59.png",
            "https://arxiv.org/html/x60.png",
            "https://arxiv.org/html/x61.png",
            "https://arxiv.org/html/x62.png",
            "https://arxiv.org/html/x63.png",
            "https://arxiv.org/html/x64.png",
            "https://arxiv.org/html/x65.png",
            "https://arxiv.org/html/x66.png",
            "https://arxiv.org/html/x67.png",
            "https://arxiv.org/html/x68.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "HTML conversions sometimes display errors due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.\n\nfailed: scrextend\n\nfailed: parselines\n\nfailed: scrextend\n\nfailed: xstring\n\nfailed: minitoc\n\nAuthors: achieve the best HTML results from your LaTeX submissions by following these best practices.\n\nLicense: arXiv.org perpetual non-exclusive license\n\narXiv:2307.15337v3 [cs.CL] 02 Mar 2024\n\n\\doparttoc\\faketableofcontents\n\nSkeleton-of-Thought: Prompting LLMs for Efficient Parallel Generation\n\nXuefei Ning11{}^{1}start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT\n\nfoxdoraame@gmail.com &Zinan Lin2â£*2{}^{2*}start_FLOATSUPERSCRIPT 2 * end_FLOATSUPERSCRIPT\n\nlinzinan1995@gmail.com &Zixuan Zhou14â£*14{}^{14*}start_FLOATSUPERSCRIPT 14 * end_FLOATSUPERSCRIPT\n\nzhouzx21@mails.tsinghua.edu.cn &Zifu Wang33{}^{3}start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT\n\nzifu.wang@kuleuven.be &Huazhong Yang11{}^{1}start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT\n\nyanghz@tsinghua.edu.cn &Yu Wang11{}^{1}start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT\n\nyu-wang@tsinghua.edu.cn Equal contribution.\n\nAbstract\n\nThis work aims at decreasing the end-to-end generation latency of large language models (LLMs). One of the major causes of the high generation latency is the sequential decoding approach adopted by almost all state-of-the-art LLMs. In this work, motivated by the thinking and writing process of humans, we propose Skeleton-of-Thought (SoT), which first guides LLMs to generate the skeleton of the answer, and then conducts parallel API calls or batched decoding to complete the contents of each skeleton point in parallel. Not only does SoT provide considerable speed-ups across 12 LLMs, but it can also potentially improve the answer quality on several question categories. SoT is an initial attempt at data-centric optimization for inference efficiency, and showcases the potential of eliciting high-quality answers by explicitly planning the answer structure in language.\n\n11{}^{1}start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT Department of Electronic Engineering, Tsinghua University, Beijing, China\n\n22{}^{2}start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT Microsoft Research, Redmond, Washington, USA\n\n33{}^{3}start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT ESAT-PSI, KU Leuven, Leuven, Belgium\n\n44{}^{4}start_FLOATSUPERSCRIPT 4 end_FLOATSUPERSCRIPT Infinigence-AI\n\nWebsite: https://sites.google.com/view/sot-llm\n\nCode: https://github.com/imagination-research/sot\n\n1 Introduction\n\nLarge language models (LLMs) (Brown et al., 2020; Touvron et al., 2023a; Du et al., 2022; OpenAI, 2023; Zheng et al., 2023) have shown exceptional performance in natural language processing and chatbot systems. However, the inference process of the state-of-the-art LLMs is slow, hindering their interactive use. For example, it takes 22 seconds for Claude (Anthropic, 2023) (accessed through Slack API) and 43 seconds for Vicuna-33B V1.3 (a 33B LLaMA-based model, running locally on one NVIDIA A100 GPU) to answer the question in Fig. 1.\n\nWe conclude three major causes of LLMsâ€™ slow inference: (1) A large model size requires a large amount of memory, memory access, and computation. For example, the FP16 weights of 175B GPT-3 take 350GB memory, which means at least 5Ã—\\timesÃ—80GB A100 GPUs are needed to keep the model in GPU memory. Even with enough GPUs, the heavy memory access and computation slow down the inference. (2) The attention operation in the prevailing transformer architecture is I/O bounded and has a quadratic memory and computation complexity in sequence length. (3) The sequential decoding approach in inference generates tokens one by one. This approach introduces a significant inference latency since the generation of tokens cannot be parallelized. There is a bunch of literature addressing the first two axes: large model size (Xiao et al., 2022; Frantar et al., 2022; Lin et al., 2023; Sheng et al., 2023; Wang et al., 2021) and attention operation (Kitaev et al., 2020; Wang et al., 2020; Dao et al., 2022; Zaheer et al., 2020; Chen et al., 2023b). These works either compress/redesign the model (Xiao et al., 2022; Frantar et al., 2022; Lin et al., 2023; Kitaev et al., 2020; Wang et al., 2020; Dao et al., 2022; Zaheer et al., 2020) or redesign the serving system (Sheng et al., 2023; Chen et al., 2023b) and hardware (Wang et al., 2021).\n\nIn contrast to prior work, we tackle the third axis and question the common assumption that LLMs have to do fully sequential decoding. We show the feasibility of parallel decoding of off-the-shelf LLMs without any changes to their model, system, or hardware. For instance, for the question in Fig. 1, we can reduce the latency from 22 seconds to 12 seconds (1.83Ã—\\timesÃ— speed-up) with Claude, and from 43 seconds to 16 seconds (2.69Ã—\\timesÃ— speed-up) with Vicuna-33B V1.3 on an NVIDIA A100.\n\nThe idea stems from reflecting on how humans ourselves answer questions. Humans do not always think about questions and write answers in a sequential fashion. In contrast, for many question types, we first derive the skeleton according to some protocols and strategies, and then add evidence and details to explain each point. This is especially the case on occasions like offering consultancy, taking tests, writing papers, and so on. This intuition has our back to question the necessity of fully sequential decoding. In this paper, we propose Skeleton-of-Thought (SoT). Specifically, as shown in Fig. 1, we guide the LLM to derive a skeleton first by itself. Based on the skeleton, the LLMs can complete each point in parallel so that we get a speed-up. SoT can be utilized to accelerate both open-source models with batched decoding and API-based models with parallel API calls.\n\nThe current SoT is suitable for questions that require a long answer whose structure can be planned ahead, while not suitable for questions that require step-by-step reasoning or only need a short answer. Therefore, to make the overall solution more practical, we design an extension, SoT with router (SoT-R), which employs a router to only trigger SoT for suitable questions.\n\nWe test SoT on 12 recently released LLMs. Not only does SoT provide considerable speed-ups (up to 2.39Ã—\\timesÃ—), but it can also improve the answer quality in many cases (Fig. 1).\n\nNote that in contrast to existing model- and system-level efforts for inference efficiency, SoT takes a novel â€œdata-levelâ€ pathway by letting the LLM organize its output content. This novel perspective is becoming feasible and is expected to grow in importance, owing to the evolving capabilities of state-of-the-art LLMs. We hope this work can stimulate more research in the realm of data-centric optimization (Zha et al., 2023; HazyResearch, 2023) for efficiency.\n\nThe rest of the paper is organized as follows. We first introduce SoT in Â§ 2 and show its results in Â§ 3. Then, we expand on the SoT-R extension in Â§ 4. Â§ 5 positions SoT in the research ecosystem (expanded in App. D). Finally, we analyze the limitations and share outlooks of SoT in Â§ 6.\n\n2 Skeleton-of-Thought (SoT)\n\n2.1 Method\n\nOverview. Based on the intuition that humans usually think about and answer a question in an organized way, the core idea of this work is to guide the LLM itself to give a skeleton first and then write the overall answer parallelly instead of sequentially. Fig. 1 illustrates how SoT produces the final answer to a user question qğ‘qitalic_q.\n\n(1) Skeleton stage. SoT first assembles a skeleton request, Tsâ¢(question=q)superscriptğ‘‡ğ‘ questionğ‘T^{s}(\\mbox{question}=q)italic_T start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT ( question = italic_q ), using the skeleton prompt template Tssuperscriptğ‘‡ğ‘ T^{s}italic_T start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT (Â§ 2.1, and Â§ B.1 in Â§ B.1) with the question qğ‘qitalic_q as the parameter. The skeleton prompt template is written to guide the LLM to output a concise skeleton of the answer. Then, we extract the BğµBitalic_B points from the skeleton response Rssuperscriptğ‘…ğ‘ R^{s}italic_R start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT of the LLM.\n\n(2) Point-expanding stage. Based on the skeleton, we let the LLM expand on each point in parallel. Specifically, for the point with index bğ‘bitalic_b and skeleton Rbssubscriptsuperscriptğ‘…ğ‘ ğ‘R^{s}_{b}italic_R start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT, SoT uses Tpâ¢eâ¢(question=q,skeleton=Rs,point index=b,point skeleton=Rbs)superscriptğ‘‡ğ‘ğ‘’formulae-sequencequestionğ‘formulae-sequenceskeletonsuperscriptğ‘…ğ‘ formulae-sequencepoint indexğ‘point skeletonsuperscriptsubscriptğ‘…ğ‘ğ‘ T^{pe}(\\mbox{question}=q,\\mbox{skeleton}=R^{s},\\mbox{point index}=b,\\mbox{% point skeleton}=R_{b}^{s})italic_T start_POSTSUPERSCRIPT italic_p italic_e end_POSTSUPERSCRIPT ( question = italic_q , skeleton = italic_R start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT , point index = italic_b , point skeleton = italic_R start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT ) as the point-expanding request for the LLM, where Tpâ¢esuperscriptğ‘‡ğ‘ğ‘’T^{pe}italic_T start_POSTSUPERSCRIPT italic_p italic_e end_POSTSUPERSCRIPT is the point-expanding prompt template (Â§ 2.1). Finally, after completing all points, we concatenate the point-expanding responses {Rbpâ¢e}b=1,â‹¯,Bsubscriptsubscriptsuperscriptğ‘…ğ‘ğ‘’ğ‘ğ‘1â‹¯ğµ\\{R^{pe}_{b}\\}_{b=1,\\cdots,B}{ italic_R start_POSTSUPERSCRIPT italic_p italic_e end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_b = 1 , â‹¯ , italic_B end_POSTSUBSCRIPT to get the final answer.\n\nParallel point expanding. We conduct parallel point-expanding so that SoT is able to achieve a speed-up than normal decoding.\n\n(1) For proprietary models with only API access, we can issue multiple parallel API calls to get an end-to-end latency gain at the cost of an increased number of API requests and tokens.\n\n(2) For open-source models that we can run locally, we let them process the point-expanding requests as a batch (paddings are added to the left of the point-expanding requests). We explain below why this could achieve speed-ups. A typical LLM generative process consists of two phases: (a) the prefilling phase in which the prompt is parsed to generate the key-value cache for further use, and (b) the decoding phase in which tokens are generated one by one in a sequential manner. The decoding phase accounts for the majority of the end-to-end latency, especially when generating a long response. Note that the decoding phase is bottlenecked by weight loading instead of activation loading or computation. Consequently, running LLM inference with increased batch sizes does not increase the per-token latency much. Therefore, SoT allows us to decode roughly BÃ—B\\timesitalic_B Ã— more tokens within the same amount of time if we parallelly decode BğµBitalic_B points. See App. E for the expanded discussions and the supporting experiments. Please refer to App. B for more implementation details.\n\n3 SoT Evaluation\n\nDatasets. We evaluate SoT on two recent assistant-style datasets: (1) Vicuna-80 (Chiang et al., 2023), which contains 80 questions spanning nine categories, such as coding, math, writing, roleplay, and so on, and (2) WizardLM (Xu et al., 2023), which contains 218 questions spanning more categories and diverse difficulties. Due to space constraints, we only report Vicuna-80 results in the main paper, and defer WizardLM results to the Apps. G and I.\n\nModels. We test SoT on 12 models, including 9 open-source models and 3 API-based models. We obtain the weights of all the open-source models from Hugging Face. See App. A for more details.\n\n3.1 Evaluation of Efficiency\n\nAPI-based models. We record the latency of every API call with start = time.time(); ...; elapsed_time = time.time() - start, and add the latency of the skeleton API call and the slowest point-expanding API call as the SoT latency.\n\nOpen-source models. All open-source models we currently evaluate are based on the LLaMA 7B, 13B, or 33B architectures. Thus, to enable fast analysis, we first make a latency profiling table for each LLaMA architecture on NVIDIA A100. The table contains the architectureâ€™s (1) latency for prefilling sequences of length 1 to 700 with different batch sizes (from 1 to 16), and (2) decoding one token with a context of length 1 to 1024 with different batch sizes (from 1 to 16). With these three latency profiling tables, given the number of points BğµBitalic_B, the token lengths of the requests and responses in the skeleton and point-expanding stages, we can quickly estimate the SoT latency by simply looking up entries in the tables and adding them up. See App. F for a more detailed description of how we conduct the profiling and estimate the latency.\n\nIn addition to the above approach, we also compare the actual latency of SoT and normal sequential generation (abbreviated as â€œnormalâ€ in the following discussion) in Â§ G.1.4.\n\nThe rest of this section shows the speed-ups of SoT on different models (Â§ 3.1.1) and question categories (Â§ 3.1.2). In addition, we also report the latency breakdown of SoT stages in Â§ G.1.2 and the SoT speed-ups on an RTX 3090 GPU in Â§ G.1.3.\n\n3.1.1 Speed-up Breakdown: Models\n\nWe investigate how SoT reduces the end-to-end latency on different models. Fig. 1(a) shows the average speed-up for each model across all question categories. We can see that SoT obtains a >>>2Ã—\\timesÃ— speed-up (up to 2.39Ã—\\timesÃ—) on 8 out of 12 models.\n\nWe report the detailed statistics about token lengths and numbers of points in Fig. 11. (1) In terms of the point number BğµBitalic_B (Fig. 10(a)), LLaMA2, Vicuna-7B V1.1, Vicuna-7B V1.3, and ChatGPT-3.5 yield relatively fewer points (<<<6), while GPT-4 and StableVicuna-13B generates the largest number of points on average (â‰ˆ\\approxâ‰ˆ9). (2) Regarding the point-expanding response length, Figs. 10(b), 10(c) and 10(d) show that the API-based models, ChatGPT-3.5, Claude, and GPT-4, follow the point-expanding request better and generate shorter point-expanding responses than the open-source models. One can also notice that StableVicuna-13Bâ€™s longest point-expanding responses for many question categories can be as lengthy as the overall normal answer, since it fails to adhere to the â€œWrite it **very shortly**â€ instruction in the point-expanding request. Consequently, SoT cannot accelerate StableVicuna-13B well. (3) Regarding the length balance degree between point responses, Fig. 10(e) shows that LLaMA2 and the API-based models generate more balanced point-expanding responses. (4) As for the overall length of the final aggregated answer (Fig. 10(f)), employing SoT on most models results in answers that are, on average, 1âˆ¼similar-to\\simâˆ¼2Ã—\\timesÃ— longer than the normal answer.\n\n3.1.2 Speed-up Breakdown: Question Categories\n\nHere we investigate how SoT reduces the end-to-end latency for different question categories. Fig. 1(b) shows the average speed-up for each question category across all models. The question categories for which SoT can provide high-quality answers are marked in green, and other categories are marked in red (see Â§ 3.2.3 for the answer quality evaluation). We can see that SoT can obtain speed-ups for all question categories. For the five question categories that SoT can provide high-quality answers (i.e., knowledge, generic, common-sense, roleplay, counterfactual), SoT can speed up the overall answer generation process by 1.89Ã—\\timesÃ— to 2.33Ã—\\timesÃ— in the meantime.\n\n3.2 Evaluation of Answer Quality\n\nIn order to compare the answer quality of the normal sequential generation (abbreviated as â€œnormalâ€ in the following discussion) and SoT generation, we adopt two LLM-based evaluation frameworks: FastChat (Zheng et al., 2023) and LLMZoo (Chen et al., 2023c). The evaluation process is to present a question and a pair of answers (from normal or SoT generation) to an LLM judge (GPT-4 in the main paper; see Â§ I.4 for the results evaluated using ChatGPT-3.5) and ask for its preference.\n\nHere are more details about the evaluation of the answer quality:\n\n(1) Detailed metrics. FastChat provides one metric for the general answer quality. In addition to a general metric, LLMZoo provides five detailed metrics on the answersâ€™ coherence, diversity, immersion, integrity, and relevance.\n\n(2) Question categories. FastChat provides two special evaluation prompts for coding and math questions for more accurate evaluation, whereas LLMZoo does not. Following the implementation in LLMZoo, we exclude math and coding questions in all LLMZoo evaluation results.\n\n(3) Extentions to avoid evaluation bias. To avoid the potential bias from the order of the two answers presented to the LLM judge, we extend FastChat and LLMZoo evaluation frameworks by running the evaluation twice with either ordering of the two answers. In either evaluation, a score of 1, 0, and -1 is assigned when SoT wins, ties, or loses, respectively. The final evaluation is that SoT wins/ties/loses when the sum of the two scores is positive/zero/negative. For example, if SoT wins in one evaluation and loses in the other evaluation, the result is â€œtieâ€. If SoT wins (loses) in one evaluation and ties in the other, the result is â€œwinâ€ (â€œloseâ€).\n\n(4) Net win rates. We further define net win rates to give a summarized view of the answer quality. Given the number of questions that SoT wins (#win) and loses (#lose), we define net win rates as #winâˆ’#lose/total number of questions#win#losetotal number of questions\\nicefrac{{\\text{\\#win}-\\text{\\#lose}}}{{\\text{total number of questions}}}/ start_ARG #win - #lose end_ARG start_ARG total number of questions end_ARG. 0% means that SoT performs competitively to the normal baseline (wins and loses in the same number of questions). Higher values mean that SoT performs better.\n\nIn the following sections, we first present the overall quality of SoT answers (Â§ 3.2.1), and then go into the details across different question categories (Â§ 3.2.3), models (Â§ 3.2.2), and metrics (Â§ 3.2.4).\n\n3.2.1 Overall Quality\n\nIn Fig. 3, we show the win/tie/lose rates (the percentage of the cases when SoT wins/ties/loses compared to normal generation) across all models and questions using the two metrics from FastChat and LLMZoo that capture the general quality of the answers. We notice a discrepancy between the two metrics on when SoT is strictly better than the baseline (45.8% v.s. 29.5%). Despite that, the two metrics agree that SoT is not worse than the baseline in around 60% of the cases, and the win rates are close to the lose rates. This result suggests that the answers of SoT maintain good quality of that of the normal generation.\n\n3.2.2 Quality Breakdown: Models\n\nWe compute net win rates on all models in Fig. 4. Again, we see that the two general metrics from FastChat and LLMZoo have different absolute values but similar rankings. In particular, both metrics agree that OpenChat-13B, Vicuna-7B V1.1, Claude, LLaMA2-Chat-13B have low net win rates, whereas Vicuna-13B V1.3, StableVicuna-13B, and UltraLM-13B have high net win rates.\n\nWe investigate the answers in Â§ I.1.1, and summarize the key takeaways as follows. Some models have low SoT net win rates as they cannot understand the skeleton and point-expanding prompts well. Some other models have low SoT net win rates as their normal answers already have good quality, making it hard for SoT to beat them (e.g., Claude). For models that are able to understand the SoT prompts and the normal answers are not good enough, SoT can improve the answer quality. We expect that further improving SoT prompts or fine-tuning the models can make it easier for LLMs to understand the skeleton and point-expanding prompts and ultimately result in better answer quality.\n\n3.2.3 Quality Breakdown: Question Categories\n\nWe compute net win rates on all question categories in Fig. 5. Similar to Fig. 3, we see that LLMZoo tends to be more optimistic about the quality of SoT than FastChat. Nevertheless, the conclusions are consistent: SoT performs relatively well on generic, common-sense, knowledge, roleplay, and counterfactual, and relatively poorly on writing, fermi, math, and coding.\n\nWe investigate the answers in Â§ I.1.2, and summarize the key takeaways as follows. SoT performs well when the question can be answered in several points whose details can be expanded independently. This includes a wide range of real-world questions. On the other hand, it is fundamentally challenging to apply SoT on questions that require step-by-step thinking, in which the latter steps require the details from the earlier steps, such as math questions. To make SoT general across broader question categories, one promising pathway is to enable SoT to adaptively fall back to normal generation, which we explore in Â§ 4. Interestingly, our results suggest that some LLMs are already able to do that occasionally without special prompting or tuning (see Â§ I.1.2).\n\n3.2.4 Quality Breakdown: Metrics\n\nIn Fig. 6, we show more detailed metrics from LLMZoo to reveal in which aspects SoT can improve or hurt the answer quality. On average, we can see that SoT improves the diversity and relevance while hurting the immersion and coherence.\n\nThrough answer investigation (Â§ I.1.3), we summarize the key takeaways as follows. The skeleton stage of SoT explicitly require LLMs to discuss the answers from multiple aspects without filler words. This improves the diversity and relevance of the answers. As for coherence and immersion, SoT is not worse than the normal generation around 60% of the time. One future direction is to improve the SoT prompts or pipeline so that the answers can be better in more metrics.\n\n4 SoT with Router (SoT-R): Adapatively Triggering SoT\n\nIn Â§ 3, we see that SoT provides considerable speed-ups while maintaining (or even improving) answer quality for many question types. However, the biggest limitation is that SoT is not suitable for questions that require step-by-step reasoning (Â§ 3.2.3). Towards pushing the practical adoption of SoT, we explore the possibility of adaptively triggering SoT only when it is suitable. To achieve that, we propose a router module that decides if SoT should be applied for the user request, and then call either SoT or normal decoding accordingly. This paradigm aligns with the recent trends of composing multiple models to solve complicated tasks (Chase, 2022; Shen et al., 2023). To implement the router, we explore two options: LLM prompting as the router (no model training is needed) (Â§ 4.1), and trained RoBERTa as the router (Â§ 4.2). The evaluation is provided in Â§ 4.3.\n\n4.1 Prompting Router\n\nWe directly ask an LLM if the question is suitable for SoT. More specifically, we ask the LLM if the desired answer is in a list of independent points (see Â§ C.1 for the prompt). If the answer is yes, we will use SoT; otherwise, we will use normal generation (i.e., directly feeding the question to the LLM). We employ GPT-4 as the LLM router given its strong capability.\n\n4.2 Trained Router\n\nWhile leveraging GPT-4 as the router obviates the need for model training, its performance remains sensitive to prompt design. Therefore, we approach the problem as a sequence classification task by fine-tuning a small language model as the router. Specifically, we annotate the LIMA dataset (Zhou et al., 2023) as the training set to train a RoBERTa model (Liu et al., 2019), which has only 120M parameters. Details about the annotation and training can be found in Â§ C.2.1 and C.2.2.\n\n5 SoT In the Context of Literature\n\nThis section positions SoT in related work to reveal how SoT (1) is connected to, (2) is different from, and (3) can harness the power of other methods. See App. D for the expanded discussion.\n\nEfficient LLM methods at model and system levels. At the model level, prior work proposes efficient architectures, including dynamic mixture-of-experts (Lepikhin et al., 2021), low-complexity attention (Kitaev et al., 2020), and multi-query attention (Shazeer, 2019). However, they usually require a significant re-training cost. In contrast, compression methods require a smaller amount of fine-tuning cost by reducing the complexity of pre-trained LLMs, such as quantization (Frantar et al., 2022) and weight or activation sparsification (Mishra et al., 2021; Zaheer et al., 2020).\n\nAt the system level, prior work (1) optimizes the computational graph (Dao et al., 2022), (2) optimizes the assignment and scheduling of computational graph on devices (Sheng et al., 2023), or (3) designs batching or caching mechanisms for serving multiple users (Fang et al., 2021). These techniques address the large memory access and footprint posed by the vast model scale and attention mechanism, and mainly aim at enhancing the throughput rather than the end-to-end latency. As SoT trades off throughput for end-to-end latency, SoT can make these throughput-oriented techniques help with end-to-end latency. This interesting synergy offers opportunities for achieving better trade-offs between latency and throughput in future serving systems.\n\nIn contrast to model- and system-level techniques, SoT is a data-level technique in a new â€œcontent co-organization for efficiencyâ€ paradigm. See Â§ 6 for more discussions.\n\nEfficient LLM methods through parallel generation. Some prior work also addresses the sequential decoding issues. Speculative decoding (SD) methods (Stern et al., 2018) employ smaller models to generate some consecutive tokens sequentially and apply the target LLMs to verify them parallelly. Non-autoregressive generation (NAG) methods (Gu et al., 2018; Xiao et al., 2023) sample and refine consecutive tokens parallelly, often with the support of a modified and tuned model.\n\nRelying on either assisting models or special models and sampling schemes, SD and NAG methods conduct parallel verification or sampling and refinement of consecutive tokens. In contrast, SoT prompts the LLM itself to plan the contents in a way that permits the parallel generation of tokens in different segments, by exploiting the emerging instruction-following and planning ability of LLMs.\n\nPrompting methods for LLMs. Recent years have witnessed the emergence of the â€œpre-train, prompt, and predictâ€ paradigm, which has shown promise in enhancing LLMsâ€™ quality in math and commonsense reasoning (Wei et al., 2022; Kojima et al., 2022; Wang et al., 2022; Chen et al., 2022) and planning for multi-modality tasks (Shen et al., 2023; Zhu et al., 2023). Instead of focusing on answer quality, SoT is a first attempt at exploiting the power of prompting to improve efficiency.\n\n6 Limitations, Future Work, and Open Questions\n\nAnswer quality evaluation. Our answer quality evaluation is far from perfect due to the limited prompt set, the potential bias of GPT-4 judges, and the inherent difficulty of evaluating LLM generations. Currently, we did not conduct human evaluation since it is easy for a human to tell whether an answer is generated with SoT due to its distinctive pattern, which might cause evaluation bias.\n\nEliciting or improving LLMsâ€™ ability. Â§ 3.2.4 demonstrates SoTâ€™s potential of enhancing answer quality. It is part of a broader trend in recent research, exemplified by work including CoT (Kojima et al., 2022; Wei et al., 2022), ToT (Yao et al., 2023), and ReAct (Yao et al., 2022), which collectively affirm the notion that explicitly articulating the thought process in language can elicit high-quality answers from LLMs. These findings resemble human thinking: rather than relying solely on the first intuition or purely sequential thinking, we often document step-by-step reasoning or thought organization to attain high-quality answers. This intriguing parallel prompts us to explore further how we can draw from the human thinking process to facilitate more effective and efficient AI.\n\nFor instance, SoT currently ignores the dependencies between points. A conceptually better way is to organize the points as Graph-of-Thoughts, where the edges represent the dependencies, and each point is decoded conditioned on the contents of its ancestor points. In addition, instead of complying with a static graph, we expect the need of having dynamic Graph-of-Thoughts, where the high-level thought structure is adjusted dynamically by LLMs themselves. This could potentially combine the efficiency and global thinking advantages of SoT with the logical reasoning and impromptu thinking strengths of methods like CoT (Kojima et al., 2022; Wei et al., 2022). Notably, a contemporary work (Besta et al., 2023) has attempted to design Graph-of-Thoughts to elicit reasoning. Furthermore, it is interesting to explore how the SoT answers can be used to fine-tune LLMs to generate more structured answers in a self-improving way (Zelikman et al., 2022; Huang et al., 2022).\n\nEfficiency and overhead of SoT in different scenarios. Serving systems commonly adopt batch processing to handle concurrent queries. This raises a concern of whether SoT may hurt serving throughput due to parallel requests. (1) When there is an unsaturated number of concurrent queries, SoT can effectively reduce latency and enhance GPU utilization. Example scenarios include (a) Edge-side applications with a single user; (b) Centralized services during periods with unsaturated user requests and underutilized computing capacity. It is interesting to study the appropriate SoT triggering conditions based on system workloads. (2) When there is a saturated number of concurrent queries, SoT is still useful for improving answer quality. However, in this case, it is important to consider the computation overhead from SoT. We delve into this concern in App. H.\n\nFor API-based models, a notable concern arises regarding the increased number of prefilling tokens (App. H). Given that many APIs charge token usage, SoT may lead to higher costs. To address this, one can use prompt tuning to design shorter SoT prompts (Jiang et al., 2023).\n\nData-centric efficiency optimization. While data-centric engineering for improving answer quality (Zha et al., 2023; HazyResearch, 2023) is gaining popularity, its potential for inference efficiency is not explored yet. SoT is the first attempt. As LLM capabilities and the amount of LLM-generated data are growing rapidly, data-centric techniques could become more useful in the future. To pave the way towards that, there are a lot to explore. For example, the acceleration ratio of SoT depends on the SoT prompt, the model, and the question, and thus not as predictable and controllable as model- or system-level techniques, which might hinder the practical adoption. We look forward to future work to unlock the full potential of data-centric efficiency optimization.\n\nAcknowledgements\n\nWe thank Sergey Yekhanin (Microsoft Research), and Tianji Wu (Infinigence AI) for their support and suggestions on the work. We thank Tianyu Fu for many initial discussions on the idea. We thank Ke Hong and Genghan Zhang for their discussions about profiling. We thank Yue Wu for the help on the Claude scripts. We thank Da Yu, Chulin Xie, and Saiqian Zhang for their suggestions on revising the first version of the paper. We thank Rui Hu, Cheng Cheng, Jack Jin, Zhoutong Ye, Mingze Sun, Jun Yan, Zhi Zhang, Yuxuan Tong, Nianhui Guo, and Andrea Santilli for their suggestions on revising the second version of the paper. We thank Chris Stetkiewicz, Amanda Melfi, and Amber Tingle from Microsoft for their suggestions and help on writing. We thank the anonymous reviewers for their insightful questions and suggestions.\n\nReferences\n\nAnthropic (2023) Anthropic. Introducing claude, May 2023. URL https://www.anthropic.com/index/introducing-claude.\n\nBesta et al. (2023) Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: Solving elaborate problems with large language models. arXiv preprint arXiv:2308.09687, 2023.\n\nBrown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877â€“1901, 2020.\n\nCai et al. (2019) Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han. Once-for-all: Train one network and specialize it for efficient deployment. arXiv preprint arXiv:1908.09791, 2019.\n\nChase (2022) Harrison Chase. LangChain, October 2022. URL https://github.com/hwchase17/langchain.\n\nChen et al. (2023a) Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023a.\n\nChen et al. (2022) Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588, 2022.\n\nChen et al. (2023b) Zhaodong Chen, Zheng Qu, Yuying Quan, Liu Liu, Yufei Ding, and Yuan Xie. Dynamic n: M fine-grained structured sparse attention mechanism. In Proceedings of the 28th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming, pp. 369â€“379, 2023b.\n\nChen et al. (2023c) Zhihong Chen, Junying Chen, Hongbo Zhang, Feng Jiang, Guiming Chen, Fei Yu, Tiannan Wang, Juhao Liang, Chen Zhang, Zhiyi Zhang, Jianquan Li, Xiang Wan, Haizhou Li, and Benyou Wang. Llm zoo: democratizing chatgpt. https://github.com/FreedomIntelligence/LLMZoo, 2023c.\n\nChiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/.\n\nChung et al. (2022) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.\n\nDao et al. (2022) Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher RÃ©. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344â€“16359, 2022.\n\nDenton et al. (2014) Emily L Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus. Exploiting linear structure within convolutional networks for efficient evaluation. Advances in neural information processing systems, 27, 2014.\n\nDing et al. (2023) Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. arXiv preprint arXiv:2305.14233, 2023.\n\nDu et al. (2022) Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 320â€“335, 2022.\n\nElsken et al. (2019) Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey. The Journal of Machine Learning Research, 20(1):1997â€“2017, 2019.\n\nFan et al. (2018) Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. arXiv preprint arXiv:1805.04833, 2018.\n\nFang et al. (2021) Jiarui Fang, Yang Yu, Chengduo Zhao, and Jie Zhou. Turbotransformers: an efficient gpu serving system for transformer models. In Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, pp. 389â€“402, 2021.\n\nFedus et al. (2022) William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. The Journal of Machine Learning Research, 23(1):5232â€“5270, 2022.\n\nFrantar et al. (2022) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.\n\nGanesh et al. (2021) Prakhar Ganesh, Yao Chen, Xin Lou, Mohammad Ali Khan, Yin Yang, Hassan Sajjad, Preslav Nakov, Deming Chen, and Marianne Winslett. Compressing large-scale transformer-based models: A case study on bert. Transactions of the Association for Computational Linguistics, 9:1061â€“1080, 2021.\n\nGante (2023) Joao Gante. Assisted generation: a new direction toward low-latency text generation. https://huggingface.co/blog/assisted-generation, 2023. Accessed: 2023-06-23.\n\nGoogle (2021) Google. Tensorflow serving, 2021. URL https://github.com/tensorflow/serving.\n\nGu et al. (2018) Jiatao Gu, James Bradbury, Caiming Xiong, Victor O.K. Li, and Richard Socher. Non-autoregressive neural machine translation. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=B1l8BtlCb.\n\nHan et al. (2015) Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015.\n\nHazyResearch (2023) HazyResearch. Data-centric ai. https://github.com/HazyResearch/data-centric-ai, 2023. Accessed: 2023-07-04.\n\nHuang et al. (2022) Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large language models can self-improve. arXiv preprint arXiv:2210.11610, 2022.\n\nHuang et al. (2019) Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al. Gpipe: Efficient training of giant neural networks using pipeline parallelism. Advances in neural information processing systems, 32, 2019.\n\nIvanov et al. (2021) Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers. Proceedings of Machine Learning and Systems, 3:711â€“732, 2021.\n\nJiang et al. (2023) Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Llmlingua: Compressing prompts for accelerated inference of large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, December 2023. URL https://arxiv.org/abs/2310.05736.\n\nKitaev et al. (2020) Nikita Kitaev, Åukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451, 2020.\n\nKojima et al. (2022) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199â€“22213, 2022.\n\nKrishnamoorthi (2018) Raghuraman Krishnamoorthi. Quantizing deep convolutional networks for efficient inference: A whitepaper. arXiv preprint arXiv:1806.08342, 2018.\n\nKrizhevsky (2014) Alex Krizhevsky. One weird trick for parallelizing convolutional neural networks. arXiv preprint arXiv:1404.5997, 2014.\n\nKwon et al. (2023) Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. arXiv preprint arXiv:2309.06180, 2023.\n\nLepikhin et al. (2021) Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. {GS}hard: Scaling giant models with conditional computation and automatic sharding. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=qrwe7XHTmYb.\n\nLester et al. (2021) Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021.\n\nLeviathan et al. (2022) Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. arXiv preprint arXiv:2211.17192, 2022.\n\nLi et al. (2023a) Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for â€mindâ€ exploration of large scale language model society, 2023a.\n\nLi et al. (2015) Jiwei Li, Minh-Thang Luong, and Dan Jurafsky. A hierarchical neural autoencoder for paragraphs and documents. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 1106â€“1115, 2015.\n\nLi & Liang (2021) Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190, 2021.\n\nLi et al. (2023b) Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval, 2023b.\n\nLi et al. (2023c) Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. Making language models better reasoners with step-aware verifier. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 5315â€“5333, 2023c.\n\nLi et al. (2021) Zhuohan Li, Siyuan Zhuang, Shiyuan Guo, Danyang Zhuo, Hao Zhang, Dawn Song, and Ion Stoica. Terapipe: Token-level pipeline parallelism for training large-scale language models. In International Conference on Machine Learning, pp. 6543â€“6552. PMLR, 2021.\n\nLin et al. (2023) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023.\n\nLiu et al. (2023) Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):1â€“35, 2023.\n\nLiu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach, 2019.\n\nLoshchilov & Hutter (2019) Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019.\n\nLu et al. (2017) Wenyan Lu, Guihai Yan, Jiajun Li, Shijun Gong, Yinhe Han, and Xiaowei Li. Flexflow: A flexible dataflow accelerator architecture for convolutional neural networks. In 2017 IEEE International Symposium on High Performance Computer Architecture (HPCA), pp. 553â€“564. IEEE, 2017.\n\nMiao et al. (2023) Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Rae Ying Yee Wong, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, and Zhihao Jia. Specinfer: Accelerating generative llm serving with speculative inference and token tree verification. arXiv preprint arXiv:2305.09781, 2023.\n\nMishra et al. (2021) Asit Mishra, Jorge Albericio Latorre, Jeff Pool, Darko Stosic, Dusan Stosic, Ganesh Venkatesh, Chong Yu, and Paulius Micikevicius. Accelerating sparse deep neural networks. arXiv preprint arXiv:2104.08378, 2021.\n\nNarayanan et al. (2019) Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R Devanur, Gregory R Ganger, Phillip B Gibbons, and Matei Zaharia. Pipedream: Generalized pipeline parallelism for dnn training. In Proceedings of the 27th ACM Symposium on Operating Systems Principles, pp. 1â€“15, 2019.\n\nNarayanan et al. (2021) Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, and Matei Zaharia. Memory-efficient pipeline-parallel dnn training. In International Conference on Machine Learning, pp. 7937â€“7947. PMLR, 2021.\n\nNVIDIA (2019) NVIDIA. Fastertransformer, 2019. URL https://github.com/NVIDIA/FasterTransformer.\n\nNVIDIA (2021) NVIDIA. Triton inference server, 2021. URL https://developer.nvidia.com/triton-inference-server.\n\nOpenAI (2023) OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\n\nOuyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730â€“27744, 2022.\n\nPhung (2023) Duy Phung. Stablevicuna-13b, May 2023. URL https://huggingface.co/CarperAI/stable-vicuna-13b-delta.\n\nPress et al. (2022) Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350, 2022.\n\nPuduppully et al. (2019) Ratish Puduppully, Li Dong, and Mirella Lapata. Data-to-text generation with content selection and planning. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pp. 6908â€“6915, 2019.\n\nRajbhandari et al. (2020) Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 1â€“16. IEEE, 2020.\n\nRen et al. (2021) Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He. {{\\{{ZeRO-Offload}}\\}}: Democratizing {{\\{{Billion-Scale}}\\}} model training. In 2021 USENIX Annual Technical Conference (USENIX ATC 21), pp. 551â€“564, 2021.\n\nSantilli et al. (2023) Andrea Santilli, Silvio Severino, Emilian Postolache, Valentino Maiorca, Michele Mancusi, Riccardo Marin, and Emanuele RodolÃ . Accelerating transformer inference for translation via parallel decoding. In acl, 2023.\n\nSchick et al. (2023) Timo Schick, Jane Dwivedi-Yu, Roberto DessÃ¬, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023.\n\nSenseTime (2023a) SenseTime. Lightllm. https://github.com/ModelTC/lightllm, 2023a. Accessed: 2023-09-26.\n\nSenseTime (2023b) SenseTime. Openppl. https://github.com/openppl-public/ppl.nn, 2023b. Accessed: 2023-09-26.\n\nShao et al. (2019) Zhihong Shao, Minlie Huang, Jiangtao Wen, Wenfei Xu, and Xiaoyan Zhu. Long and diverse text generation with planning-based hierarchical variational model. arXiv preprint arXiv:1908.06605, 2019.\n\nShazeer (2019) Noam Shazeer. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150, 2019.\n\nShen et al. (2023) Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580, 2023.\n\nSheng et al. (2023) Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E Gonzalez, et al. High-throughput generative inference of large language models with a single gpu. arXiv preprint arXiv:2303.06865, 2023.\n\nShin et al. (2020) Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 4222â€“4235, 2020.\n\nStern et al. (2018) Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit. Blockwise parallel decoding for deep autoregressive models. Advances in Neural Information Processing Systems, 31, 2018.\n\nSun et al. (2023) Ziteng Sun, Ananda Theertha Suresh, Jae Hun Ro, Ahmad Beirami, Himanshu Jain, Felix Yu, Michael Riley, and Sanjiv Kumar. Spectr: Fast speculative decoding via optimal transport. In Workshop on Efficient Systems for Foundation Models @ ICML2023, 2023. URL https://openreview.net/forum?id=d0mGsaheuT.\n\nSzegedy et al. (2016) Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2818â€“2826, 2016.\n\nTaori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. Alpaca: A strong, replicable instruction-following model. https://crfm.stanford.edu/2023/03/13/alpaca.html, 2023. Accessed: 2023-06-23.\n\nTouvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.\n\nTouvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023b.\n\nWang et al. (2023a) Guan Wang, Sijie Cheng, Qiying Yu, and Changling Liu. Openllms: Less is more for open-source models, July 2023a. URL https://github.com/imoneoi/openchat.\n\nWang et al. (2021) Hanrui Wang, Zhekai Zhang, and Song Han. Spatten: Efficient sparse attention architecture with cascade token and head pruning. In 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA), pp. 97â€“110. IEEE, 2021.\n\nWang et al. (2020) Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020.\n\nWang et al. (2022) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.\n\nWang et al. (2023b) Zifu Wang, Teodora Popordanoska, Jeroen Bertels, Robin Lemmens, and Matthew B Blaschko. Dice semimetric losses: Optimizing the dice score with soft labels. In Medical Image Computing and Computer Assisted Intervention, 2023b.\n\nWei et al. (2021) Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.\n\nWei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824â€“24837, 2022.\n\nWen et al. (2016) Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in deep neural networks. Advances in neural information processing systems, 29, 2016.\n\nXiao et al. (2022) Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. arXiv preprint arXiv:2211.10438, 2022.\n\nXiao et al. (2023) Yisheng Xiao, Lijun Wu, Junliang Guo, Juntao Li, Min Zhang, Tao Qin, and Tie-yan Liu. A survey on non-autoregressive generation for neural machine translation and beyond. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.\n\nXu et al. (2023) Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023.\n\nXu et al. (2021) Yuanzhong Xu, HyoukJoong Lee, Dehao Chen, Blake Hechtman, Yanping Huang, Rahul Joshi, Maxim Krikun, Dmitry Lepikhin, Andy Ly, Marcello Maggioni, et al. Gspmd: general and scalable parallelization for ml computation graphs. arXiv preprint arXiv:2105.04663, 2021.\n\nYao et al. (2022) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022.\n\nYao et al. (2023) Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601, 2023.\n\nYu et al. (2022) Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. Orca: A distributed serving system for {{\\{{Transformer-Based}}\\}} generative models. In 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22), pp. 521â€“538, 2022.\n\nZaheer et al. (2020) Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. Advances in neural information processing systems, 33:17283â€“17297, 2020.\n\nZelikman et al. (2022) Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:15476â€“15488, 2022.\n\nZha et al. (2023) Daochen Zha, Zaid Pervaiz Bhat, Kwei-Herng Lai, Fan Yang, Zhimeng Jiang, Shaochen Zhong, and Xia Hu. Data-centric artificial intelligence: A survey. arXiv preprint arXiv:2303.10158, 2023.\n\nZhai et al. (2022) Yujia Zhai, Chengquan Jiang, Leyuan Wang, Xiaoying Jia, Shang Zhang, Zizhong Chen, Xin Liu, and Yibo Zhu. Bytetransformer: A high-performance transformer boosted for variable-length inputs. arXiv preprint arXiv:2210.03052, 2022.\n\nZhang et al. (2023) Yifan Zhang, Jingqin Yang, Yang Yuan, and Andrew Chi-Chih Yao. Cumulative reasoning with large language models. arXiv preprint arXiv:2308.04371, 2023.\n\nZheng et al. (2022) Lianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng Chen, Yanping Huang, Yida Wang, Yuanzhong Xu, Danyang Zhuo, Eric P Xing, et al. Alpa: Automating inter-and {{\\{{Intra-Operator}}\\}} parallelism for distributed deep learning. In 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22), pp. 559â€“578, 2022.\n\nZheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.\n\nZhou et al. (2023) Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment, 2023.\n\nZhou et al. (2022) Zhe Zhou, Xuechao Wei, Jiejing Zhang, and Guangyu Sun. {{\\{{PetS}}\\}}: A unified framework for {{\\{{Parameter-Efficient}}\\}} transformers serving. In 2022 USENIX Annual Technical Conference (USENIX ATC 22), pp. 489â€“504, 2022.\n\nZhu et al. (2023) Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, et al. Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory. arXiv preprint arXiv:2305.17144, 2023.\n\nZoph & Le (2017) Barret Zoph and Quoc V. Le. Neural architecture search with reinforcement learning. In International Conference on Learning Representations (ICLR), 2017.\n\nAppendix\n\n\\parttoc\n\nAppendix B Implementation Details of Skeleton-of-Thought\n\nB.1 Prompt\n\nThe skeleton prompt is shown in Â§ 2.1 and B.1 and the point-expanding prompt is shown in Â§ 2.1.\n\nSkeleton prompt template. In order to make the output skeleton short and in a consistent format for the good of efficiency and ease of point extraction, the skeleton prompt template (1) describes the task precisely, and (2) provides a partial answer â€œ1.â€ for the LLM to continue writing. The skeleton responses are in the desired format in most cases. Therefore, we can use a simple regular expression (\\d+)\\.\\s?([\\s\\S]+?)(?=\\n|\\n*$) to extract point indexes and point skeletons from the skeleton response.\n\nWe find that GPT-4 can work well without the two demonstrations in the skeleton prompt. Therefore, we do not include the two demonstrations for GPT-4 (Â§ 2.1). For all other models, the two demonstrations are included, as shown in Â§ B.1.\n\nPoint-expanding prompt template. It describes the point-expanding task and provides a partial answer. We also provide instructions â€œWrite it **very shortly** in 1âˆ¼similar-to\\simâˆ¼2 sentenceâ€ so that the LLMs keep the answers concise. Unlike the skeleton prompt template, we find that demonstrations are not necessary to get reasonable results.\n\nWe find that Claude and GPT-4 follows the instruction â€œWrite it **very shortly** in 1âˆ¼similar-to\\simâˆ¼2 sentence and do not continue with other points!â€ in Â§ 2.1 very well, so that the answers are very short. Therefore, we delete â€œ**very shortly**â€ from the prompt template in Claude and GPT-4.\n\nPartial answer.\n\nIn the Â§ 2.1 and 2.1, we provide partial answers so that LLMs can follow the desired response format better.\n\nWe can put the partial answer at the end of the prompt for the open-source models to continue writing. An implementation detail is that different open-source models have different conversation templates (i.e., different ways to combine user and assistant messages into one string). For example, Vicuna (Chiang et al., 2023) uses the string â€œUSER:â€ and â€œ ASSISTANT:â€ for the placeholder â€œ[User:]â€ and â€œ[Role]â€ in the Â§ 2.1 and 2.1, respectively, while UltraLM (Ding et al., 2023) uses â€œUser:â€ and â€œâŸ¨/sâŸ©Assistant:â€. We build our open-source model experiments with the help of the FastChat codebase (Zheng et al., 2023), in which the conversation templates of many models are already handled correctly. We implement the conversation templates of OpenChat-13B, StableVicuna-13B, and UltraLM-13B according to their official guides and codes.\n\nFor ChatGPT-3.5, we provide partial answers as a last message in the chat history from the assistant. Note that it is not a documented approach. We find it works well in most cases, in that ChatGPT-3.5 continues the texts from the provided partial answer. However, in some rare cases, ChatGPT-3.5 repeats the provided partial answers.\n\nFor Claude over Slack, there is no obvious way to give the API a partial answer. We resort to modifying the prompt template slightly by adding\n\nPlease start your answer from â€œ{partial answer}â€ and do not output other things before that\n\nat the end. We find that Claude understands and obeys it well. For GPT-4, we also take this approach.\n\nSystem Message.\n\nWe do not include the system message in the prompts for open-source models except LLaMA2.\n\nThe partial answer, â€œ**very shortly**â€, and the 2-shot demonstrations discussed above are the only differences between the prompts we used across all models and all evaluations.\n\nB.2 Supporting Multi-Round Conversation\n\nTo use SoT in a multi-round conversation, we can just put the question and the final aggregated answer in the history, removing all the SoT prompts. In this way, using SoT in one conversation round will not introduce additional prefill cost in future rounds.\n\nAppendix C Implementation Details of Skeleton-of-Thought with Router\n\nC.1 Prompting Router\n\nWe use Â§ C.1 for querying GPT-4 as the router. If the answer is â€œAâ€ (i.e., the question can be answered in a list of independent points), we will use SoT. Otherwise, if the answer is â€œBâ€ (i.e., the answer is in a list of points but they depend on each other) or â€œCâ€ (i.e., the answer should not be in a list of points), SoT is not suitable and we will fall back to normal decoding.\n\nC.2 Trained Router\n\nWe tackle the routing problem as a sequence classification task. We first annotate the LIMA training set (Zhou et al., 2023), and then fine-tune a RoBERTa model (Liu et al., 2019) using the labeled data. Finally, we apply the tuned RoBERTa as the router on Vicuna-80 and WizardLM. We detail the steps in the following.\n\nC.2.1 Annotation Process\n\nIn the classification task, a label of 1 (positive) indicates that this question can be answered with SoT, while a label of 0 (negative) suggests that using the normal generation mode is more suitable. We annotate the LIMA training set, which consists of 1,030 Q&As sourced from three community webpages: Stack Exchange, wikiHow, and the Pushshift Reddit. We also annotate the Vicuna-80 and WizardLM datasets for evaluation.\n\nWe use GPT-4 to assist the annotation process. Specifically, we present each question to GPT-4 and analyze its answer to determine whether SoT can be triggered for this question. We assign a positive label to a question if GPT-4â€™s response meets two criteria: (1) it contains a list of points that can be expanded in parallel, (2) each point provides sufficient details (i.e., the point-expanding response is not too short), which will enable SoT to achieve a speed-up. Two of the paperâ€™s authors conduct the annotation process independently, and discuss the inconsistent annotations to decide the final label.\n\nC.2.2 Training Details\n\nWe use roberta-base with 120M parameters as the router model. The finetuning is conducted using the AdamW optimizer (Loshchilov & Hutter, 2019) with a weight decay of 0.01. The learning rate undergoes a warm-up phase during the first 1% of iterations to 5e-5 and then decays linearly. We train the model for 2 epochs using a batch size of 32. Input sequences are either padded or truncated to achieve a consistent length of 512 tokens.\n\nIn the application of SoT, false positives (SoT is incorrectly triggered when it should not be, resulting in degraded answer quality) are of more significant concern than false negatives (the router misses a potential SoT trigger, resulting in a reduced speed-up). Thus, to mitigate false positives, we employ the Tversky loss (Wang et al., 2023b) with parameters Î±=0.7ğ›¼0.7\\alpha=0.7italic_Î± = 0.7 and Î²=0.3ğ›½0.3\\beta=0.3italic_Î² = 0.3, which penalizes false positives more heavily than false negatives. We also incorporate label smoothing (Szegedy et al., 2016) with a factor of Ïµ=0.2italic-Ïµ0.2\\epsilon=0.2italic_Ïµ = 0.2. Overall, the entire fine-tuning process is efficient, completing in 2 minutes on an NVIDIA A100 GPU.\n\nC.3 Router Consistency\n\nWe present the confusion matrices for the three routers to illustrate their consistency. The results on Vicuna-80 and WizardLM are shown in Tables 4 and 4, respectively.\n\nOn Vicuna-80, we can observe a notable level of agreement among the three routers. Compared with the GPT-4-prompting router, the trained router exhibits a slightly higher number of false negatives w.r.t. the human annotations. Conversely, on WizardLM, given the intricate answer structure and the presence of many ambiguous cases, the routers show significant discrepancies. Specifically, the GPT-4 router produces many false positives, which pose adverse affects on the answer quality (see Â§ I.2). The RoBERTa router aligns more closely with the human annotations.\n\nC.4 Concurrent execution for SoT-R\n\nIn SoT-R, the router serves as an additional stage that extends the two-stage SoT pipeline, as illustrated in Fig. 9. To push the limit of latency optimization, we can run the router, normal generation, and SoT generation concurrently. Once the router makes a decision, one of the normal and SoT generation processes can be aborted. However, this approach will increase the token overhead. Therefore, we did not employ this approach in this work and leave it to future work.\n\nAppendix D SoT In the Context of Literature (Expanded)\n\nD.1 Efficient LLMs\n\nExtensive research has been dedicated to enhancing the throughput and latency of LLM inference. We first discuss model-level architecture design or compression techniques. These techniques change the model and can benefit both the latency and throughput but require finetuning to retain the model quality. Then, we discuss system-level efforts that optimize the computational graph or the assignment and scheduling of the computational graph on computation and storage devices. Most system-level efforts accelerate the prefilling phase or focus on improving the throughput. Finally, we discuss some research efforts that share a similar motivation to ours, namely, addressing the efficiency issue of sequential decoding.\n\nModel-level optimization.\n\nConsiderable architectural design efforts have emerged to (1) improve the scalability w.r.t. model size by introducing mixture-of-expert inference (Lepikhin et al., 2021; Fedus et al., 2022), (2) address the quadratic complexity w.r.t. input size of attention by designing new attention mechanisms (Kitaev et al., 2020; Wang et al., 2020), (3) reduce the memory access and footprint of attention by using multi-query attention (Shazeer, 2019), and so on. However, these methods usually require a substantial re-training cost. The model compression techniques require a smaller amount of fine-tuning by reducing the model complexity of a pre-trained LLM from certain aspects (Ganesh et al., 2021). Representative techniques include quantization (Xiao et al., 2022; Frantar et al., 2022; Lin et al., 2023), the static or dynamic pruning of weights, activation, and attention (Mishra et al., 2021; Zaheer et al., 2020; Wang et al., 2021; Chen et al., 2023b), and so on.\n\nZooming out from LLM compression to the whole field of model compression, we can see that model co-design or compression for efficiency has received tremendous attention in the past few years and has grown into large research fields, such as pruning (Han et al., 2015; Wen et al., 2016), quantization (Krishnamoorthi, 2018), factorization (Denton et al., 2014), and neural architecture search (Zoph & Le, 2017; Elsken et al., 2019; Cai et al., 2019). Different from the model co-design paradigm, SoT is in a â€œcontent co-organization for efficiencyâ€ paradigm for improving the LLM efficiency. Along with the growth in the LLM capabilities and amount of LLM-generated data, data-level techniques could become important tools in the efficient LLM toolbox.\n\nSystem-level optimization.\n\nIn the realm of lossless acceleration, considerable efforts have been devoted to addressing the I/O-bound nature of LLMs on modern hardware platforms (Dao et al., 2022). Numerous studies (Dao et al., 2022; Zhai et al., 2022; Ivanov et al., 2021; NVIDIA, 2019) have focused on adjusting the computational graph by fusing and implementing operations in an I/O-friendly way. As a representative method, FlashAttention (Dao et al., 2022) fuses all operations of one attention into one GPU kernel with spatially tiled computation to reduce the off-chip I/O of the attention map. While FlashAttention can effectively accelerate training and the prefilling phase of inference, it cannot accelerate the decoding phase much (when the batch size is small), as it is the I/O of weights rather than activation or attention map that bottlenecks the decoding phase. For example, when the context length is 64, decoding one token using LLaMA-7B needs to load each of the 7B parameters from the off-chip HBM onto the GPU chip at least once, but only transferring about 20M (0.02B) activation values between the off-chip HBM and GPU chip.\n\nIn order to satisfy Service Level Objectives, serving systems focus on improving the serving throughput under latency constraints. To this end, serving systems (Fang et al., 2021; NVIDIA, 2021; Google, 2021) pack multiple queries together into a batch to improve the hardware utilization. The batching technique has proven highly effective in enhancing throughput, leading to the development of various variants. For example, some work designs methods to decide which queries to batch together (Fang et al., 2021; Zhou et al., 2022), while others selectively batch parts of the model to enable fine-grained iteration-level batching (Yu et al., 2022) or multi-task batching (Zhou et al., 2022). Various model parallelism (Lu et al., 2017; Huang et al., 2019; Narayanan et al., 2019; Rajbhandari et al., 2020; Narayanan et al., 2021; Li et al., 2021; Zheng et al., 2022) and offloading (Ren et al., 2021; Sheng et al., 2023) techniques have been proposed to maximize the throughput of LLM training or inference. In a nutshell, given the computational graph and device configurations, these techniques optimize the split, assignment, and scheduling of computations, storage, and communications on devices. In addition to the model parallelism and batching techniques, an efficient memory management mechanism for LLM workloads is also an essential feature in the serving systems (Kwon et al., 2023; SenseTime, 2023a; b).\n\nTo sum up, these system-level techniques mainly help with the throughput in training and batched inference. They can be used by SoT to improve the throughput of the batched decoding of multiple segments. This means that SoT can harness the power of these throughput-oriented techniques and make them help with the end-to-end latency, offering a new dimension for better trading off latency and throughput in future serving systems.\n\nAnother parallelism perspective to position SoT is that SoT guides the LLM to adjust the sequential workload to become â€œinter-contentâ€ parallelizable, which differs from the parallelism levels in existing serving systems, including inter-instance (Krizhevsky, 2014; Rajbhandari et al., 2020), inter-operation (Huang et al., 2019; Narayanan et al., 2019; 2021), intra-operation (Xu et al., 2021), and inter-token (Li et al., 2021). It may be worthwhile to explore the integration of SoT into serving systems to maximize the hardware utilization.\n\nDecoding optimization.\n\nOne bottleneck for the end-to-end latency lies in the autoregressive decoding phase, where tokens must be generated one by one. Due to the dependency between tokens, the computation of different tokens cannot be parallelized, causing severe under-utilization of GPU. In order to improve the end-to-end decoding latency of a given LLM, speculative decoding methods (Stern et al., 2018; Leviathan et al., 2022; Chen et al., 2023a; Gante, 2023; Sun et al., 2023; Miao et al., 2023) propose to use cheaper approaches to generate short candidate token sequences, for example, by sequentially decoding with an assisting model much smaller than the given LLM. Then, they use the LLM to parallelly verify the candidates and keep the prefix sequence that matches the LLMâ€™s verification results.\n\nAnother line of work that shares the motivation of addressing the autoregressive efficiency issue is non-autoregressive generation (NAG) methods (Gu et al., 2018; Xiao et al., 2023). NAG methods sample consecutive tokens parallelly, often with the aid of a modified and tuned model. To maintain the answer quality, instead of sampling for one iteration, many NAG methods refine the output parallelly for multiple iterations (Xiao et al., 2023; Santilli et al., 2023).\n\nTo summarize, the speculative decoding methods use assisting models for letting the LLM conduct parallel verification of consecutive tokens, and the NAG methods rely on specially designed models, training schemes, or sampling schemes for the parallel sampling and refinement of consecutive tokens. In contrast, SoT prompts the LLM itself to plan the contents in a way that permits the parallel generation of multiple tokens in different segments. SoT exploits the emerging instruction-following and planning ability of SoTA LLMs rather than relying on specially designed modeling, sampling, and training schemes. This is different from all existing work that targets the autoregressive efficiency issue.\n\nD.2 Prompting Methods for LLMs\n\nIn recent years, the â€œpre-train, prompt, and predictâ€ paradigm has emerged (Liu et al., 2023), which designs prompts comprising task descriptions and (optionally) a few demonstrations to guide pre-trained LLMs in generating answers for a wide range of downstream tasks. Researchers found that instruction-tuned LLMs (Brown et al., 2020; Wei et al., 2021; Ouyang et al., 2022; Chung et al., 2022; Taori et al., 2023) possess a strong ability to (1) generalize to new tasks thanks to the diverse natural language descriptions encountered during instruction tuning, and (2) learn in-context using a few demonstrations without weight tuning.\n\nIn virtue of these abilities, the field has been manually engineering (Brown et al., 2020; Kojima et al., 2022; Shen et al., 2023; Li et al., 2023a), automatic searching (Shin et al., 2020), or continuously tuning (Li & Liang, 2021; Lester et al., 2021) the prompts for uncovering the capabilities of LLMs on downstream tasks. There are a bunch of prompting methods that improves the reasoning performance of LLMs by designing thinking flows mimicking human reasoning: (1) mimicking the step-by-step or compositional thinking structure (Wei et al., 2022; Kojima et al., 2022; Press et al., 2022; Yao et al., 2023; Besta et al., 2023; Zhang et al., 2023), (2) designing multiple reasoning paths and their aggregation (Wang et al., 2022; Yao et al., 2023; Li et al., 2023c), and (3) using tools for calculation and information retrieval (Chen et al., 2022; Yao et al., 2022; Schick et al., 2023). As a representative example, the Chain-of-Thought prompts largely improve the performance on tasks that require logical reasoning by simply providing a â€œLetâ€™s think step by stepâ€ (Kojima et al., 2022) instruction or a few demonstrations (Wei et al., 2022). Another topic that arises quite a surge of interests is to prompt LLMs to help finish complex multi-modality task (Shen et al., 2023; Zhu et al., 2023). For example, HuggingGPT (Shen et al., 2023) design prompts to guide the LLM to generate structural JSON for the orchestration of multi-model execution to finish complex tasks.\n\nTo summarize, the large literature on prompting methods has been aiming at uncovering different capabilities of LLM and improving the answer quality on different downstream tasks. In contrast, SoT is a first attempt at exploiting the power of prompting to improve efficiency.\n\nD.3 Hierarchical Text Generation\n\nSoT can be regarded as being â€œhierarchicalâ€ since it has high-level answer structure planning. Prior studies in hierarchical text generation (Li et al., 2015; Shao et al., 2019; Puduppully et al., 2019; Fan et al., 2018) all focus on enhancing the answer quality, including improving the long-range coherence, relevance to the topic, or reducing redundancy. These methods craft hierarchical neural architectures that contain different modules to model high-level (sentence-level or document-level) and low-level (word-level) dependencies (Li et al., 2015; Shao et al., 2019; Fan et al., 2018). They still employ sequential word-by-word generation without parallelization between sentences.\n\nNote that the sentence-level representations in previous work (Li et al., 2015; Shao et al., 2019) are â€œimplicitâ€ latent variables instead of â€œexplicitâ€ language descriptions. Some previous studies (Shao et al., 2019; Puduppully et al., 2019) train a dedicated planning module to execute explicit content planning in advance. Nevertheless, these methods all conduct â€œclosed-formâ€ planning that only reorders and groups the input keywords, rather than producing â€œfree-formâ€ plans on â€œwhat to sayâ€ and â€œhow to sayâ€. All the hierarchical architectures and planning modules require training or even special data processing (Puduppully et al., 2019).\n\nTo summarize, in terms of the objective, the primary focus of SoT â€“ efficient generation â€“ is different from previous hierarchical text generation literature. In terms of the methodology, instead of designing new hierarchical architectures or planning modules, SoT exploits the emerging planning and instruction-following abilities of LLMs to do explicit (which means the plan is described by interpretable language) and free-form planning. This allows SoT to be applied to off-the-shelf LLMs for producing structured answers.\n\nAs the hierarchical text generation literature focuses on enhancing answer quality, they could provide inspiration for future expansions of SoT to generate high-quality answers for broader types of questions.\n\nAppendix E Efficiency Analysis\n\nThis section gives a detailed explanation on why SoT can reduce the overall decoding latency with the same computational resource for local models.\n\nThe vanilla approach processes only one question and decodes the answers sequentially, whereas SoT processes multiple point-expanding requests and the answers in a batch. We focus on the following question: â€œCompared to processing only one sequence, how much peak memory overhead and latency increase will be brought by processing a batch of sequences?â€\n\nA typical LLM generative process consists of two phases: (1) the prefilling phase in which the prompt is parsed to generate the key-value cache for further use, and (2) the decoding phase in which tokens are generated one by one in a sequential manner. The decoding phase accounts for the majority of the end-to-end latency, especially when generating a long response. As shown in Table 5, when running Vicuna-7B on NVIDIA A100-80G, the actual computing performance is only 0.31 TFLOPS (0.1% utilization) in the decoding phase, compared to 43 TFLOPS (13.8% utilization) during prefilling. The utilization is calculated with respect to the FP16 tensor core peak performance â€“ 312 TFLOPS for NVIDIA-A100. As a result, the latency of decoding only one token is comparable to that of prefilling 128 tokens (40ms). This huge gap in actual computing performance and thereby the latency arises from the fact that all LLM weights need to be loaded onto the GPU chip at least once only for decoding one token, so the decoding is heavily bottlenecked by the I/O of weights and the GPU computation units cannot be well utilized.\n\nWhen conducting batched decoding, as the sequence batch size BğµBitalic_B increases, the latency of decoding one token for each sequence stays roughly the same (Fig. 9(a)), as the amount of LLM weights that needs to be loaded onto the chip does not change. As a result, the GPU computation utilization (Actual GPU PerformancePeak GPU PerformanceActual GPU PerformancePeak GPU Performance\\frac{\\text{Actual GPU Performance}}{\\text{Peak GPU Performance}}divide start_ARG Actual GPU Performance end_ARG start_ARG Peak GPU Performance end_ARG) increases almost linearly as BğµBitalic_B increases (Fig. 9(b)). In other words, for generating a final answer of length Nğ‘Nitalic_N, if we cut the answer into BğµBitalic_B segments of length N/Bğ‘ğµN/Bitalic_N / italic_B and decode them as a batch, we can get a BÃ—B\\timesitalic_B Ã— decoding speed-up compared to sequential decoding. Nevertheless, in practice, as prefilling longer requests brings some overhead, and the lengths of the BğµBitalic_B segments could be imbalanced, the actual speed-up of the batched point-expanding stage compared with the original prefilling and sequential decoding process is smaller than BğµBitalic_B.\n\nAs for the peak memory overhead, the amount of LLM weights can be one to two orders of magnitude larger than that of all the intermediate activations as long as the prefilling token length is not too large, not to mention that most activations do not need to be saved for back-propagation during inference. Therefore, the LLM weights account for the majority of the memory footprint in our test cases. Consequently, as shown in Fig. 9(c), the peak memory overhead due to the increasing size of the KV cache and activation grows at a slow pace as the batch size BğµBitalic_B increases. Thanks to the small peak memory overhead, in all of our experiments, we managed to use one GPU to run SoT without seeking help from other peak memory optimization techniques (e.g., quantization (Frantar et al., 2022; Lin et al., 2023), offloading (Sheng et al., 2023)).\n\nAppendix F Efficiency Profiling\n\nWe run the profiling on the target GPU (NVIDIA A100-80G and NVIDIA RTX 3090) with CUDA 11.7, using the Hugging Face transformer library 4.28.1 and PyTorch 2.0.1. The host of A100-80G has an Intel Xeon Platinum 8358P CPU and 1T memory. The host of RTX 3090 has an Intel Xeon Gold 6246R CPU and 512G memory.\n\nLatency profiling and estimation.\n\nFor the decoding phase, we denote tBDâ¢(k)superscriptsubscriptğ‘¡ğµğ·ğ‘˜t_{B}^{D}(k)italic_t start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT ( italic_k ) as the latency of batched decoding the k+1ğ‘˜1k+1italic_k + 1-th token with batch size BğµBitalic_B, where the superscript Dğ·Ditalic_D stands for â€œdecodeâ€. For each batch size B=1,â‹¯,16ğµ1â‹¯16B=1,\\cdots,16italic_B = 1 , â‹¯ , 16 and each context length k=1,â‹¯,1024ğ‘˜1â‹¯1024k=1,\\cdots,1024italic_k = 1 , â‹¯ , 1024, we use torch.cuda.Event to record the latency of decoding one token. We run each decoding three times continuously and take their geometric mean as {tBDâ¢(k)}k=1,â‹¯,1024;B=1,â‹¯,16subscriptsuperscriptsubscriptğ‘¡ğµğ·ğ‘˜formulae-sequenceğ‘˜1â‹¯1024ğµ1â‹¯16\\{t_{B}^{D}(k)\\}_{k=1,\\cdots,1024;B=1,\\cdots,16}{ italic_t start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT ( italic_k ) } start_POSTSUBSCRIPT italic_k = 1 , â‹¯ , 1024 ; italic_B = 1 , â‹¯ , 16 end_POSTSUBSCRIPT. For the prefilling phase, we profile the latency of batched prefilling the inputs with token length kğ‘˜kitalic_k in rangeâ¢(1,700,10)range170010\\mbox{range}(1,700,10)range ( 1 , 700 , 10 ) and batch size B=1,â‹¯,16ğµ1â‹¯16B=1,\\cdots,16italic_B = 1 , â‹¯ , 16, and denote it as tBPâ¢(k)superscriptsubscriptğ‘¡ğµğ‘ƒğ‘˜t_{B}^{P}(k)italic_t start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_P end_POSTSUPERSCRIPT ( italic_k ), where the superscript Pğ‘ƒPitalic_P stands for â€œprefillâ€. We run each test seven times continuously, regard the first two times as the warmup tests, and take the geometric mean of the last five times as {tBPâ¢(k)}k=1,11,â‹¯,691;B=1,â‹¯,16subscriptsuperscriptsubscriptğ‘¡ğµğ‘ƒğ‘˜formulae-sequenceğ‘˜111â‹¯691ğµ1â‹¯16\\{t_{B}^{P}(k)\\}_{k=1,11,\\cdots,691;B=1,\\cdots,16}{ italic_t start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_P end_POSTSUPERSCRIPT ( italic_k ) } start_POSTSUBSCRIPT italic_k = 1 , 11 , â‹¯ , 691 ; italic_B = 1 , â‹¯ , 16 end_POSTSUBSCRIPT. Once we get the latency profiling table, given a request with lisubscriptğ‘™ğ‘–l_{i}italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT tokens and the decoding batch size BğµBitalic_B, the latency of generating losubscriptğ‘™ğ‘œl_{o}italic_l start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT tokens can be estimated as:\n\nTâ¢(li,lo,B)=t~BPâ¢(li)+âˆ‘k=lili+loâˆ’1tBDâ¢(k),ğ‘‡subscriptğ‘™ğ‘–subscriptğ‘™ğ‘œğµsuperscriptsubscript~ğ‘¡ğµğ‘ƒsubscriptğ‘™ğ‘–superscriptsubscriptğ‘˜subscriptğ‘™ğ‘–subscriptğ‘™ğ‘–subscriptğ‘™ğ‘œ1superscriptsubscriptğ‘¡ğµğ·ğ‘˜T(l_{i},l_{o},B)=\\tilde{t}_{B}^{P}(l_{i})+\\sum_{k=l_{i}}^{l_{i}+l_{o}-1}t_{B}^% {D}(k),italic_T ( italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_l start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT , italic_B ) = over~ start_ARG italic_t end_ARG start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_P end_POSTSUPERSCRIPT ( italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) + âˆ‘ start_POSTSUBSCRIPT italic_k = italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + italic_l start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT - 1 end_POSTSUPERSCRIPT italic_t start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT ( italic_k ) , (1)\n\nwhere the subscripts iğ‘–iitalic_i and oğ‘œoitalic_o stand for â€œinputâ€ and â€œoutputâ€. Note that we only test the prefilling latency every ten token lengths (i.e., 1,11,21,â‹¯11121â‹¯1,11,21,\\cdots1 , 11 , 21 , â‹¯) for fast profiling and estimate t~BPâ¢(li)superscriptsubscript~ğ‘¡ğµğ‘ƒsubscriptğ‘™ğ‘–\\tilde{t}_{B}^{P}(l_{i})over~ start_ARG italic_t end_ARG start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_P end_POSTSUPERSCRIPT ( italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) by tBPâ¢(âŒŠli10âŒ‹Ã—10+1)superscriptsubscriptğ‘¡ğµğ‘ƒsubscriptğ‘™ğ‘–10101t_{B}^{P}(\\lfloor\\frac{l_{i}}{10}\\rfloor\\times 10+1)italic_t start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_P end_POSTSUPERSCRIPT ( âŒŠ divide start_ARG italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG start_ARG 10 end_ARG âŒ‹ Ã— 10 + 1 ).\n\nThe SoT decoding process consists of two stages: the skeleton stage and the point-expanding stage. Denoting the token length of the skeleton request and skeleton response as lissuperscriptsubscriptğ‘™ğ‘–ğ‘ l_{i}^{s}italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT and lossuperscriptsubscriptğ‘™ğ‘œğ‘ l_{o}^{s}italic_l start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT, the token length of the longest point-expanding request and the longest point-expanding response as lipâ¢esuperscriptsubscriptğ‘™ğ‘–ğ‘ğ‘’l_{i}^{pe}italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p italic_e end_POSTSUPERSCRIPT and lopâ¢esuperscriptsubscriptğ‘™ğ‘œğ‘ğ‘’l_{o}^{pe}italic_l start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p italic_e end_POSTSUPERSCRIPT, the number of the points as BğµBitalic_B, we can compute the latency of the skeleton and point-expanding stages as:\n\nLsâ¢(lis,los)superscriptğ¿ğ‘ superscriptsubscriptğ‘™ğ‘–ğ‘ superscriptsubscriptğ‘™ğ‘œğ‘ \\displaystyle L^{s}(l_{i}^{s},l_{o}^{s})italic_L start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT ( italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT , italic_l start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT ) =Tâ¢(lis,los,1),absentğ‘‡superscriptsubscriptğ‘™ğ‘–ğ‘ superscriptsubscriptğ‘™ğ‘œğ‘ 1\\displaystyle=T(l_{i}^{s},l_{o}^{s},1),= italic_T ( italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT , italic_l start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT , 1 ) , (2) Lpâ¢eâ¢(lipâ¢e,lopâ¢e,B)superscriptğ¿ğ‘ğ‘’superscriptsubscriptğ‘™ğ‘–ğ‘ğ‘’superscriptsubscriptğ‘™ğ‘œğ‘ğ‘’ğµ\\displaystyle L^{pe}(l_{i}^{pe},l_{o}^{pe},B)italic_L start_POSTSUPERSCRIPT italic_p italic_e end_POSTSUPERSCRIPT ( italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p italic_e end_POSTSUPERSCRIPT , italic_l start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p italic_e end_POSTSUPERSCRIPT , italic_B ) =Tâ¢(lipâ¢e,lopâ¢e,B).absentğ‘‡superscriptsubscriptğ‘™ğ‘–ğ‘ğ‘’superscriptsubscriptğ‘™ğ‘œğ‘ğ‘’ğµ\\displaystyle=T(l_{i}^{pe},l_{o}^{pe},B).= italic_T ( italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p italic_e end_POSTSUPERSCRIPT , italic_l start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p italic_e end_POSTSUPERSCRIPT , italic_B ) . (3)\n\nUsing the latency profiling table, we can further estimate the average GPU computing performance in FLOPS (i.e., FLOPs per second) of decoding losubscriptğ‘™ğ‘œl_{o}italic_l start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT tokens with prefilling length lisubscriptğ‘™ğ‘–l_{i}italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT as\n\nPDâ¢(li,lo,B)=âˆ‘k=lili+loâˆ’1fBDâ¢(k)âˆ‘k=lili+loâˆ’1tBDâ¢(k),superscriptğ‘ƒğ·subscriptğ‘™ğ‘–subscriptğ‘™ğ‘œğµsuperscriptsubscriptğ‘˜subscriptğ‘™ğ‘–subscriptğ‘™ğ‘–subscriptğ‘™ğ‘œ1superscriptsubscriptğ‘“ğµğ·ğ‘˜superscriptsubscriptğ‘˜subscriptğ‘™ğ‘–subscriptğ‘™ğ‘–subscriptğ‘™ğ‘œ1superscriptsubscriptğ‘¡ğµğ·ğ‘˜P^{D}(l_{i},l_{o},B)=\\frac{\\sum_{k=l_{i}}^{l_{i}+l_{o}-1}f_{B}^{D}(k)}{\\sum_{k% =l_{i}}^{l_{i}+l_{o}-1}t_{B}^{D}(k)},italic_P start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT ( italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_l start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT , italic_B ) = divide start_ARG âˆ‘ start_POSTSUBSCRIPT italic_k = italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + italic_l start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT - 1 end_POSTSUPERSCRIPT italic_f start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT ( italic_k ) end_ARG start_ARG âˆ‘ start_POSTSUBSCRIPT italic_k = italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + italic_l start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT - 1 end_POSTSUPERSCRIPT italic_t start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT ( italic_k ) end_ARG , (4)\n\nwhere fBDâ¢(k)superscriptsubscriptğ‘“ğµğ·ğ‘˜f_{B}^{D}(k)italic_f start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT ( italic_k ) denotes the FLOPs of decoding one token with context length kğ‘˜kitalic_k, which is calculated by DeepSpeedâ€™s FLOPs profiler . Fig. 9(b) reports the average GPU computing performance during the process of decoding 64 tokens (prefilling length=128), i.e., PDâ¢(128,64,B)superscriptğ‘ƒğ·12864ğµP^{D}(128,64,B)italic_P start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT ( 128 , 64 , italic_B ).\n\nMemory profiling and evaluation.\n\nTo evaluate the peak memory, we use torch.cuda.max_memory_allocated to record the memory consumption of prefilling sequences of different lengths and decoding with different context lengths and a batch size ranging from 1 to 16. Then, we calculate the peak memory of each stage as the maximum value of the prefilling and decoding phases, and calculate the overall peak memory of SoT as the maximum value of the skeleton and point-expanding stages.\n\nAppendix G Efficiency Evaluation\n\nG.1 Skeleton-of-Thought\n\nG.1.1 Detailed Statistics of Token Lengths and Point Numbers\n\nG.1.2 Latency Breakdown: SoT Stages and Phases\n\nFig. 12 presents the absolute latencies of normal and SoT generations on Vicuna-80. Again, the speed-ups of SoT compared with normal generation is evident. We can see that the decoding phases predominantly account for the end-to-end latency. Consequently, although SoT has higher prefilling latency in the skeleton stage than the normal generation and introduces additional point-expanding prefilling latency â€“ which is expected â€“ this has negligible impact on the overall latency and thereby the overall speed-up.\n\nG.1.3 Efficiency Evaluation on NVIDIA RTX 3090\n\nWe present the SoT speed-ups and latency breakdown on RTX 3090 in Fig. 13. We test the three 7B models, as their FP16-precision version can be run on an RTX 3090 GPU without further peak memory optimization techniques such as weight quantization (Frantar et al., 2022; Lin et al., 2023) or offloading (Sheng et al., 2023). On these three models, SoT can obtain 1.94Ã—\\timesÃ— to 2.40Ã—\\timesÃ— speed-up on average on Vicuna-80.\n\nFor the five question categories that SoT can provide high-quality answers (i.e., knowledge, common-sense, generic, roleplay, counterfactual), SoT can speed-up the overall answer generation process by 1.96Ã—\\timesÃ— to 2.52Ã—\\timesÃ— in the meantime. Note that for the math category, despite the average speed-up being 1.20Ã—\\timesÃ— by calculating the speed-up across the three math questions, SoT does not reduce the absolute latency of processing the three questions.\n\nG.1.4 Actual Latency Testing\n\nThis section reports the actual SoT speed-up on the Vicuna-80 with batch testing (instead of analyzing with pre-made profiling tables), using a single NVIDIA A100 GPU. We test the actual end-to-end latency of the SoT and normal decoding with the 9 open-source models. For each model, we run the speed-up test for five times and plot the box in Fig. 14.\n\nAs shown in Fig. 13(a), the current SoT solution obtains a >2Ã—>2\\times> 2 Ã— speed-up on 6 out of the 9 open-source models (i.e., Vicuna-7B V1.1, Vicuna-7B V1.3, UltraLM-13B, LLaMA2-Chat-7B, Vicuna-13B V1.3, and LLaMA2-Chat-13B), and a >1.7absent1.7>1.7> 1.7 speed-up on OpenChat-13B and Vicuna-33B V1.3. SoT achieves no speed-up on StableVicuna-13B. As shown in Fig. 13(b), for the five question categories that SoT can provide high-quality answers (i.e., knowledge, common-sense, generic, roleplay, counterfactual), SoT can speed-up the overall answer generation process by 2.15Ã—\\timesÃ— to 2.50Ã—\\timesÃ— in the meantime.\n\nG.2 Skeleton-of-Thought with Router\n\nThe overhead brought by the router inference is relatively small: On the Vicuna-80 dataset, the prompting and trained router have an average latency of 0.65s (0.39sâˆ¼similar-to\\simâˆ¼1.37s) and 0.04s (0.008sâˆ¼similar-to\\simâˆ¼1.55s), respectively. On the WizardLM dataset, the average latency of the prompting and trained router is 0.80s (0.36sâˆ¼similar-to\\simâˆ¼2.22s) and 0.03s (0.009sâˆ¼similar-to\\simâˆ¼2.52s), respectively.\n\nG.2.1 Speed-up breakdown: models\n\nFig. 15 shows the speed-ups of SoT-R on different models on the Vicuna-80 dataset. Fig. 16 and Fig. 17 show the speed-ups of SoT-R on different models on the WizardLM dataset. We can observe that on Vicuna-80, the two methods yield similar speed-ups, whereas on WizardLM, GPT-4 prompting router usually obtains higher speed-ups than the trained router, especially on GPT-4 itself.\n\nG.2.2 Speed-up breakdown: categories\n\nFig. 18 and Fig. 19 show the speed-ups of SoT-R on different question categories of Vicuna-80 dataset. The trained router achieves slightly higher speed-up on most of the categories (except for knowledge, writing, and fermi). Fig. 20 and Fig. 21 show the speed-ups of SoT-R on different question categories of WizardLM dataset. We can observe that on 19 out of 29 categories, using the prompting router achieves higher speed-ups than using the trained router.\n\nAppendix H Overhead of SoT in Different Scenarios\n\nDespite the optimizations made to the decoding phase, SoT brings overhead to the prefilling phase as the model needs to handle additional SoT prompts. Table 6 reports SoTâ€™s prefilling overhead for the API-based models. These statistics are averaged across the Vicuna-80 questions that are suitable for SoT (according to our manual annotation). We can see that SoT significantly increases the number of prefilling tokens. This is because that SoT issues an independent point-expanding request for each point, with the average number of points being 6.8 on Vicuna-80 dataset across all evaluated models. Consequently, the APIs need to prefill the point-expanding request multiple times.\n\nWhen using SoT to serve the open-source models, a simple and small trick is to prefill the common prefix of point-expanding requests with a batch size of 1 during Stage 2 (i.e., the point-expanding stage). Table 7 shows the prefilling overhead after applying the trick. Although the ratio is considerably smaller compared to that of the API-based models, this computational overhead remains a concern, especially during periods of high system workload.\n\nThere are some possibilities to further reduce the token and computational overhead that are worth exploring in future work. To name a few: (1) When using SoT in serving systems, we can simply reuse the key-value cache containing the question and skeleton from Stage 1 during Stage 2, rather than re-prefilling them as in a multi-round conversation. (2) Generally, as LLM capabilities continue to evolve and prompt tuning techniques advance (Shin et al., 2020; Li & Liang, 2021; Lester et al., 2021; Jiang et al., 2023), the possibility of using much shorter prompts to activate the SoT mode in the future holds promise, which would significantly mitigate the token or computational overhead.\n\nAppendix I Answer Quality Evaluation\n\nI.1 Skeleton-of-Thought\n\nI.1.1 Answer Analysis: Different Models\n\nModels on which SoT has low net win rates. We observe two types of reasons for the low net win rates.\n\n(1) OpenChat-13B, Vicuna-7B V1.1, and LLaMA2-Chat-13B. For the weak models such as OpenChat-13B and Vicuna-7B V1.1, they are not able to follow the SoT prompts precisely. For OpenChat-13B and LLaMA2-Chat-13B, sometimes the skeleton contains undesired contents (e.g., completing the skeleton points as shown below).\n\nQuestion: How many lightning strikes occur on Earth each day? Try to explain your answer. Your explanation should take the reader through your reasoning step-by-step.\n\n\\IfEqCase correctâœ“incorrectâœ— SoTâ€™s answer (OpenChat-13B) (Orange texts are the skeleton.) {addmargin}[1em]2em(Only showing the skeleton) 1.Introduction.\n\n2. The Earthâ€™s atmosphere.\n\n3. Lightning formation.\n\n4. Electrical discharge.\n\n5. Lightning strikes per second.\n\n6. Calculating the total.\n\nSkeleton:\n\n1. Introduction.\n\nLightning is a beautiful and awe-inspiring natural phenomenon, but it can also be dangerous. It occurs when positively charged particles in a thundercloud discharge to the negatively charged Earth. The process is still not fully understood, but scientists have been able to estimate how many lightning strikes occur on Earth each day.\n\nSkeleton:\n\n1. The Earthâ€™s atmosphere.\n\nThe Earthâ€™s atmosphere plays a crucial role in the formation of lightning. It consists of different layers, with the troposphere being the lowest and most active. The troposphere is where most of the weather occurs, and it is where lightning primarily forms. The atmosphereâ€™s temperature, humidity, and wind patterns all contribute to the likelihood of a lightning strike.\n\nSkeleton:\n\n2. Lightning formation.\n\nWhen warm, moist air rises, it cools and forms water <<<more texts are omittedâ€¦>>>\n\nFor both OpenChat-13B and Vicuna-7B V1.1, sometimes they do not write anything in the point-expanding stage when details are needed.\n\nQuestion: As a superhero, how would you explain your origin story to a curious child?\n\n\\IfEqCase correctâœ“incorrectâœ— SoTâ€™s answer (Vicuna-7B V1.1) (Orange texts are the skeleton.) {addmargin}[1em]2em1. Born with extraordinary abilities.\n\n2. Trained to use powers for the greater good.\n\n3. Protecting the innocent and fighting villains.I use my powers to defend the defenseless and bring justice to those who have been wronged.\n\n4. Inspired by courage, justice, and selflessness.\n\nThese two issues result in bad net win rates.\n\n(2) Claude. For strong models such as Claude, we do not see major issues with SoTâ€™s answers except the common issues across all models discussed in Â§ 3.2.3. We hypothesize that the low net win rates are majorly due to the fact that their normal sequential generation is already very good.\n\nModels on which SoT has high net win rates. The models with high net win rates (Vicuna-13B V1.3, StableVicuna-13B, and UltraLM-13B) are between the above two extremes. On one hand, they are good enough to understand SoT prompts. While we observe that they sometimes have the same issues as OpenChat-13B and Vicuna-7B V1.1 discussed before, these issues happen much less frequently. On the other hand, their normal sequential generation performance has a larger room for improvement than Claude and ChatGPT-3.5 so that the benefits from SoT are more visible on these models.\n\nI.1.2 Answer Analysis: Different Question Categories\n\nCategories on which SoT has low net win rates"
    }
}
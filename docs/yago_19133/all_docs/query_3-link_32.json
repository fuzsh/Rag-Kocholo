{
    "id": "yago_19133_3",
    "rank": 32,
    "data": {
        "url": "https://dcase.community/challenge2024/task-few-shot-bioacoustic-event-detection",
        "read_more_link": "",
        "language": "en",
        "title": "shot Bioacoustic Event Detection",
        "top_image": "https://dcase.community/favicon.ico",
        "meta_img": "https://dcase.community/favicon.ico",
        "images": [
            "https://dcase.community/images/logos/dcase/dcase2024_logo.png",
            "https://dcase.community/images/logos/dcase/dcase2024_logo.png",
            "https://dcase.community/images/logos/dcase/workshop.png",
            "https://dcase.community/images/logos/dcase/challenge.png",
            "https://dcase.community/images/logos/dcase/dcase2024_logo.png",
            "https://dcase.community/images/person/burooj_ghani.jpg",
            "https://dcase.community/images/person/ines_nolasco.jpg",
            "https://dcase.community/images/person/jinhua_liang.png",
            "https://dcase.community/images/person/shubhr_singh.jpg",
            "https://dcase.community/images/person/vincent_lostanlen.jpg",
            "https://dcase.community/images/person/ariana_peshkin.jpg",
            "https://dcase.community/images/person/emily_grout.png",
            "https://dcase.community/images/person/hanna_pamula.jpg",
            "https://dcase.community/images/person/Helen_Whitehead.jpg",
            "https://dcase.community/images/person/Joe_Morford.jpg",
            "https://dcase.community/images/person/Michael Emmerson.jpg",
            "https://dcase.community/images/person/Frants_Jensen.jpg",
            "https://dcase.community/images/person/Ester_Villa.jpg",
            "https://dcase.community/images/person/dan_stowell.jpg",
            "https://dcase.community/images/tasks/challenge2021/few_shot_bioacoustics.png",
            "https://zenodo.org/badge/DOI/10.5281/zenodo.10829604.svg",
            "https://zenodo.org/badge/DOI/10.5281/zenodo.7879692.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "This task focuses on sound event detection in a few-shot learning setting for animal (mammal and bird) vocalisations. Participants will be expected to create a method that can extract information from five exemplar vocalisations (shots) of mammals or birds and detect and classify sounds in field recordings. Challenge has ended …",
        "meta_lang": "en",
        "meta_favicon": "/favicon.ico",
        "meta_site_name": "",
        "canonical_link": "https://dcase.community/challenge2024/task-few-shot-bioacoustic-event-detection",
        "text": "This task focuses on sound event detection in a few-shot learning setting for animal (mammal and bird) vocalisations. Participants will be expected to create a method that can extract information from five exemplar vocalisations (shots) of mammals or birds and detect and classify sounds in field recordings.\n\nChallenge has ended. Full results for this task can be found in the Results page.\n\nDescription\n\nFew-shot learning is a highly promising paradigm for sound event detection. It is also an extremely good fit to the needs of users in bioacoustics, in which increasingly large acoustic datasets commonly need to be labelled for events of an identified category (e.g. species or call-type), even though this category might not be known in other datasets or have any yet-known label. While satisfying user needs, this will also benchmark few-shot learning for the wider domain of sound event detection (SED).\n\nFew-shot learning describes tasks in which an algorithm must make predictions given only a few instances of each class, contrary to standard supervised learning paradigm. The main objective is to find reliable algorithms that are capable of dealing with data sparsity, class imbalance and noisy/busy environments. Few-shot learning is usually studied using N-way-K-shot classification, where N denotes the number of classes and K the number of examples for each class.\n\nSome reasons why few-shot learning has been of increasing interest:\n\nScarcity of supervised data can lead to unreliable generalisations of machine learning models.\n\nExplicitly labeling a huge dataset can be costly both in time and resources.\n\nFixed ontologies or class labels used in SED and other DCASE tasks are often a poor fit to a given user’s goal.\n\nDevelopment Set\n\nThe development set is pre-split into training and validation sets. The training set consists of five sub-folders deriving from a different source each. Along with the audio files multi-class annotations are provided for each. The validation set consists of two sub-folders deriving from a different source each, with a single-class (class of interest) annotation file provided for each audio file.\n\nTraining Set\n\nThe training set contains five different sub-folders (BV, HV, JD, MT,WMW). Statistics are given overall and specific for each sub-folder.\n\nOverall\n\nStatistics Values Number of audio recordings 174 Total duration 21 hours Total classes (excl. UNK) 47 Total events (excl. UNK) 14229\n\nBV\n\nThe BirdVox-DCASE-10h (BV for short) contains five audio files from four different autonomous recording units, each lasting two hours. These autonomous recording units are all located in Tompkins County, New York, United States. Furthermore, they follow the same hardware specification: the Recording and Observing Bird Identification Node (ROBIN) developed by the Cornell Lab of Ornithology. Andrew Farnsworth, an expert ornithologist, has annotated these recordings for the presence of flight calls from migratory passerines, namely: American sparrows, cardinals, thrushes, and warblers. In total, the annotator found 2,662 from 11 different species. We estimate these flight calls to have a duration of 150 milliseconds and a fundamental frequency between 2 kHz and 10 kHz.\n\nStatistics Values Number of audio recordings 5 Total duration 10 hours Total classes (excl. UNK) 11 Total events (excl. UNK) 9026 Ratio event/duration 0.04 Sampling rate 24,000 Hz\n\nHT\n\nSpotted hyenas are a highly social species that live in \"fission-fusion\" groups where group members range alone or in smaller subgroups that split and merge over time. Hyenas use a variety of types of vocalizations to coordinate with one another over both short and long distances. Spotted hyena vocalization data were recorded on custom-developed audio tags designed by Mark Johnson and integrated into combined GPS / acoustic collars (Followit Sweden AB) by Frants Jensen and Mark Johnson. Collars were deployed on female hyenas of the Talek West hyena clan at the MSU-Mara Hyena Project (directed by Kay Holekamp) in the Masai Mara, Kenya as part of a multi-species study on communication and collective behavior. Field work was carried out by Kay Holekamp, Andrew Gersick, Frants Jensen, Ariana Strandburg-Peshkin, and Benson Pion; labeling was done by Kenna Lehmann and colleagues.\n\nStatistics Values Number of audio recordings 5 Total duration 5 hours Total classes (excl. UNK) 3 Total events (excl. UNK) 611 Ratio events/duration 0.05 Sampling rate 6000 Hz\n\nJD\n\nJackdaws are corvid songbirds which usually breed, forage and sleep in large groups, but form a pair bond with the same partner for life. They produce thousands of vocalisations per day, but many aspects of their vocal behaviour remained unexplored due to the difficulty in recording and assigning vocalisations to specific individuals, especially in natural settings. In a multi-year field study (Max-Planck-Institute for Ornithology, Seewiesen, Germany), wild jackdaws were equipped with small backpacks containing miniature voice recorders (Edic Mini Tiny A31, TS-Market Ltd., Russia) to investigate the vocal behaviour of individuals interacting normally with their group, and behaving freely in their natural environment. The jackdaw training dataset contains a 10-minute on-bird sound recording of one male jackdaw during the breeding season 2015. Field work was conducted by Lisa Gill, Magdalena Pelayo van Buuren and Magdalena Maier. Sound files were annotated by Lisa Gill, based on a previously established video-validation in a captive setting.\n\nStatistics Values Number of audio recordings 1 Total duration 10 minutes Total classes (excl. UNK) 1 Total events (excl. UNK) 357 Ratio event/duration 0.06 Sampling rate 22,050 Hz\n\nMT\n\nMeerkats are a highly social mongoose species that live in stable social groups and use a variety of distinct vocalizations to communicate and coordinate with one another. Meerkat vocalization data were recorded at the Kalahari Meerkat Project (Kuruman River Reserve, South Africa; directed by Marta Manser and Tim Clutton-Brock), as part of a multi-species study on communication and collective behavior. Data in the training set were recorded on small audio devices (TS Market, Edic Mini Tiny+ A77, 8 kHz) integrated into combined GPS/audio collars which were deployed on multiple members of meerkat groups to monitor their movements and vocalizations simultaneously. Recordings were carried out during daytime hours while meerkats were primarily foraging (digging in the ground for small prey items). Field work was carried out by Ariana Strandburg-Peshkin, Baptiste Averly, Vlad Demartsev, Gabriella Gall, Rebecca Schaefer and Marta Manser. Audio recordings were labeled by Baptiste Averly, Vlad Demartsev, Ariana Strandburg-Peshkin, and colleagues.\n\nStatistics Values Number of audio recordings 2 Total duration 1 hour and 10 mins Total classes (excl. UNK) 4 Total events (excl. UNK) 1294 Ratio event/duration 0.04 Sampling rate 8,000 Hz\n\nWMW\n\nWMW consist on a selection of recordings from the Western Mediterranean Wetlands Bird dataset. The recordings are taken from the Xeno-Canto portal. The present selection consists in 161 audio recordings of different lengths that have at least 10 positive events. These have been annotated for 26 different classes of 20 species of birds.\n\nStatistics Values Number of audio recordings 161 Total duration 4 hour and 40 mins Total classes (excl. UNK) 26 Total events (excl. UNK) 2941 Ratio event/duration 0.24 Sampling rate various\n\nTraining annotation format\n\nAnnotation files have the same name as their corresponding audiofiles with extension *.csv. For the training set multi-class annotations are provided, with positive (POS), negative (NEG) and unknown (UNK) values for each class. UNK indicates uncertainty about a class and participants can choose to ignore it. Example of an annotation file for audio.wav:\n\nValidation Set\n\nThe validation set comprises of six sub-folders (HB, PB, ME, RD, PB24, and PW). Specific information about the source of the recordings are not provided for the participants for the duration of the challenge, as to make information available for the validation set as similar to the evaluation set (once that is also published). More information about both will be made available after the end of the challenge.\n\nThere is no overlap between the training set and validation set classes.\n\nOverall\n\nStatistics Values Number of audio recordings 43 Total duration 49 hours and 57 minutes Total classes (excl. UNK) 7 Total events (excl. UNK) 3504\n\nHB\n\nStatistics Values Number of audio recordings 10 Total duration 2 hours and 38 minutes Total classes (excl. UNK) 1 Total events (excl. UNK) 712 Sampling rate 44100 Hz\n\nPB\n\nStatistics Values Number of audio recordings 6 Total duration 3 hours Total classes (excl. UNK) 2 Total events (excl. UNK) 292 Sampling rate 44,100 Hz\n\nME\n\nStatistics Values Number of audio recordings 2 Total duration 20 minutes Total classes (excl. UNK) 2 Total events (excl. UNK) 73 Sampling rate 48000 Hz\n\nRD\n\nStatistics Values Number of audio recordings 6 Total duration 18 hours Total classes (excl. UNK) 1 Total events (excl. UNK) 1372 Sampling rate 48000 Hz\n\nPB24\n\nStatistics Values Number of audio recordings 4 Total duration 2 hours Total classes (excl. UNK) 2 Total events (excl. UNK) 350 Sampling rate 44,100 Hz\n\nPW\n\nStatistics Values Number of audio recordings 15 Total duration 24 hours Total classes (excl. UNK) 1 Total events (excl. UNK) 705 Sampling rate 96000 Hz\n\nValidation annotation format\n\nAnnotation files have the same name as their corresponding audiofiles with extension *.csv. For the validation set single-class (class of interest) annotations are provided, with positive (POS), unkwown (UNK) values. UNK indicates uncertainty about a class and participants can choose to ignore it. Each audio file should be treated separately of the rest, as there is possible overlap between the classes of the evaluation set across different audio files.\n\nParticipants must treat the task as a 5-shot setting and only use the first five POS annotations for the class of interest for each file, when trying to predict the rest.\n\nExample of an annotation file for audio_val.wav:\n\nDownload\n\nEvaluation Set\n\nThe Evaluation set is identical to the one utilized in the 2023 challenge and consists of 66 audio files acquired from different bioacoustic sources organized by 8 subsets (CHE, CHE23, CT, CW, DC, MGE, MS, QU) At this time only the first 5 annotations are provided for each file, with events marked as positive (POS) for the class of interest. The annotation files follow the same format as for the validation set. This dataset is to be used for evaluation purposes during the task.\n\nTask setup\n\nThis few-shot task will run as a 5-shot task. Hence, five annotated calls from each recording in the evaluation set will be provided to the participants. Each recording of the evaluation set will have a single class of interest which the participants will then need to detect through the recording. Each recording can have multiple types of calls or species present in it, as well as background noise, however only the label of interest needs to be detected.\n\nDuring the development period the participants are required to treat the validation set in the same way as the evaluation set by using the first five positive (POS) events for their models. Participants should keep in mind that our evaluation metric ignores anything before the end time of the fifth positive event, hence using randomly selected events from the validation set may lead to incorrect performance values.\n\nTask rules\n\nUse of external data (e.g. audio files, annotations) is allowed only after approval from the task coordinators (contact: burooj.ghani@naturalis.nl or i.dealmeidanolasco@qmul.ac.uk ). Typically these external datasets should be public, open datasets.\n\nUse of pre-trained models is allowed only after approval from the task coordinators (contact: burooj.ghani@naturalis.nl or i.dealmeidanolasco@qmul.ac.uk).\n\nList of external datasets and models that are allowed (note that this list is not exhaustive, please ask the coordinators for an approval for other models/datasets):\n\nDataset name Type Added Link AudioSet audio, video 01.03.2023 https://research.google.com/audioset/ OpenL3 model 01.03.2023 https://openl3.readthedocs.io/ ECAPA-TDNN model 01.03.2023 https://github.com/speechbrain/speechbrain PANN model 01.03.2023 https://github.com/qiuqiangkong/audioset_tagging_cnn BEATs model 01.03.2023 https://github.com/microsoft/unilm/tree/master/beats ESC50 audio dataset 01.03.2023 http://www.cs.cmu.edu/~alnu/tlwled/esc50.htm ImageNet image dataset 14.05.2021 http://www.image-net.org/ VoxCeleb audio,visual dataset 01.03.2023 https://www.robots.ox.ac.uk/~vgg/data/voxceleb/ Xeno-Canto audio data repository 01.04.2024 https://xeno-canto.org BIRB audio dataset 01.04.2024 https://arxiv.org/pdf/2312.07439.pdf TUT Acoustic scenes 2016 audio dataset 01.03.2023 https://zenodo.org/record/45739#.YJ76v5NKidY BirdNET model 01.04.2024 https://github.com/kahst/BirdNET-Analyzer Google Perch model 01.04.2024 https://github.com/google-research/perch TweetyNet model 01.04.2024 https://github.com/yardencsGitHub/tweetynet ANIMAL-SPOT model 01.04.2024 https://github.com/ChristianBergler/ANIMAL-SPOT wav2vec2 model 01.04.2024 https://github.com/facebookresearch/fairseq/tree/main/examples/wav2vec\n\nThe development dataset (i.e. training and validation) can be augmented without the use of external data.\n\nParticipants are not allowed to use VGG Sound dataset, DCASE2018 Bird Audio Detection task dataset and AudioSet Strong\n\nParticipants are not allowed to make subjective judgments of the evaluation data, nor to annotate it.\n\nParticipants are not allowed to use extra annotations for the provided data.\n\nParticipants are only allowed to use the first five positive (POS) annotations from each validation set annotation file and use the rest for evaluation of their method.\n\nParticipants must treat each file in the validation set independently of the others (e.g. for prototypical networks do not save prototypes between audio files). This is due to the fact that the classes of the validation set are hidden and there is possible overlap between them inside the validation set.\n\nEnsemble models are not allowed.\n\nSubmission\n\nOfficial challenge submission consists of:\n\nSystem output file (*.csv)\n\nMetadata file (*.yaml)\n\nTechnical report explaining in sufficient detail the method (*.pdf)\n\nSystem output should be presented as a single text-file (in CSV format, with a header row as shown in the example output below).\n\nFor each system, meta information should be provided in a separate file, containing the task-specific information. This meta information enables fast processing of the submissions and analysis of submitted systems. Participants are advised to fill the meta information carefully while making sure all information is correctly provided.\n\nWe allow up to 4 system output submissions per participant/team. For each system, metadata should be provided in a separate file, containing the task specific information. All files should be packaged into a zip file for submission. Please make a clear connection between the system name in the submitted metadata (the *.yaml file), submitted system output (the *.csv file), and the technical report. The detailed information regarding the challenge information can be found in the Submission page. Finally, for supporting reproducible research, we kindly ask from each participant/team to consider making available the code of their method (e.g. in GitHub) and pre-trained models, after the challenge is over.\n\nPlease note: automated procedures will be used for the evaluation of the submitted results. Therefore, the column names should be exactly as indicated in the example output below. Events in each file should be in order of start time.\n\nExample output:\n\nAudiofilename,Starttime,Endtime BUK5_20161101_002104a.wav,356.0134694,356.1384127 BUK5_20161101_002104a.wav,356.18839,356.488254 BUK5_20161101_002104a.wav,356.5882086,356.7131519\n\nMetadata file\n\nExample meta information file for task 5 baseline system task5/Morfi_QMUL_task5_1/Morfi_QMUL_task5_1.meta.yaml:\n\nEvaluation Metric\n\nWe implemented an event-based F-measure, macro-averaged evaluation metric. We use IoU followed by bipartite graph matching. The evaluation metric ignores the part of the file that contains the first five positive (POS) events and measure are estimated after the end time of the fifth positive event for each file. Furthermore, real-world datasets contain a small number of ambiguous or unknown labels marked as UNK in the annotation files provided. This evaluation metrics treats these separately during evaluation, so as not to penalise algorithms that can perform better than a human annotator. Final ranking of methods will be based on the overall F-measure for the whole of the evaluation set.\n\nResults\n\nRank Submission Information Code Author Affiliation Event-based\n\nF-score\n\nwith 95% confidence interval\n\n(Evaluation dataset) Baseline_TempMatch_task5_1 Ines Nolasco Centre for Digital Music, EECS, Queen Mary University of London, UK 14.9 (14.0 - 15.3) Baseline_PROTO_task5_1 Jinhua Liang Centre for Digital Music, EECS, Queen Mary University of London, UK 41.6 (41.0 - 42.1) KAO_NTHU_task5_2 SHENG-LUN KAO NTHUEE 30.27 ( 29.42 - 30.81 ) KAO_NTHU_task5_3 SHENG-LUN KAO NTHUEE 40.1 (39.2 - 40.7) KAO_NTHU_task5_1 SHENG-LUN KAO NTHUEE 46.9 (45.6 - 47.9) Lu_AILab_task5_1 Pengyuan Zhao CUMT 56.7 (56.2 - 57.1) Lu_AILab_task5_2 Pengyuan Zhao CUMT 31.4 (31.0 - 31.7) Lu_AILab_task5_3 Pengyuan Zhao CUMT 37.7 (37.3 - 38.1) Lu_AILab_task5_4 Pengyuan Zhao CUMT 43.0 (42.3 - 43.4) XF_NUDT_task5_1 Liu Wei iFLYTEK 64.1 (63.8 - 64.5) XF_NUDT_task5_4 Deng Xuyao NUDT 28.4 (27.4 - 29.0) XF_NUDT_task5_3 Liu Hongyan iFLYTEK 61.5 (61.1 - 61.9) XF_NUDT_task5_2 Lin Fuliang iFLYTEK 65.2 (64.9 - 65.5) Latifi_IDMT_task5_1 Amir Latifi Bidarouni IDMT 0.7 (0.6 - 0.7) QianHu_BHEBIT_task5_1 Mengkai Sun (1) BHIEI (2) BIT-SMT 21.7 (21.1 - 22.2) QianHu_BIT_task5_3 Mengkai Sun (1) BHIEI (2) BIT-SMT 21.7 (21.2 - 22.1) QianHu_BHEBIT_task5_4 Mengkai Sun (1) BHIEI (2) BIT-SMT 42.5 (41.7 - 43.0) QianHu_BHEBIT_task5_2 Mengkai Sun (1) BHIEI (2) BIT-SMT 34.1 (33.6 - 34.5) Hoffman_ESP_task5_4 Benjamin Hoffman ESP 8.1 (7.6 - 8.4) Hoffman_ESP_task5_2 Benjamin Hoffman ESP 3.3 (3.0 - 3.5) Hoffman_ESP_task5_3 Benjamin Hoffman ESP 1.2 (0.9 - 1.3) Hoffman_ESP_task5_1 Benjamin Hoffman ESP 6.5 (6.1 - 6.9) Bordoux_WUR_task5_1 Valentin Bordoux WUR 42.0 (41.3 - 42.5)\n\nComplete results and technical reports can be found in the results page\n\nComplete results and technical reports can be found at results page\n\nBaseline system\n\nThe following baseline is provided:\n\nDeep learning prototypical network (based on a 3-top ranked systems from task5 at the DCASE challenge 2022)\n\nBaseline Results\n\nSystem F-measure Precision Recall Prototypical Network with negative sampling 52.14% 56.18% 48.64%\n\nCitation"
    }
}
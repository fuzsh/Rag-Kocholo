{
    "id": "wrong_mix_property_starring_00149_1",
    "rank": 19,
    "data": {
        "url": "https://arxiv.org/html/2401.05459v2",
        "read_more_link": "",
        "language": "en",
        "title": "Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Security",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://arxiv.org/html/extracted/2401.05459v2/figs/software_paradigm.png",
            "https://arxiv.org/html/extracted/2401.05459v2/figs/architecture.png",
            "https://arxiv.org/html/extracted/2401.05459v2/figs/foundamental_capabilities.png",
            "https://arxiv.org/html/extracted/2401.05459v2/figs/SecurityAndPrivacy/security.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Personal LLM Agents:\n\nInsights and Survey about the Capability, Efficiency and Security\n\nYuanchun Li1†, Hao Wen1‡, Weijun Wang1‡, Xiangyu Li1‡, Yizhen Yuan1‡, Guohong Liu1‡,\n\nJiacheng Liu1, Wenxing Xu1, Xiang Wang1, Yi Sun1, Rui Kong1, Yile Wang1, Hanfei Geng1,\n\nJian Luan2, Xuefeng Jin3, Zilong Ye4, Guanjing Xiong5, Fan Zhang6, Xiang Li7,\n\nMengwei Xu8, Zhijun Li9, Peng Li1, Yang Liu1, Ya-Qin Zhang1, Yunxin Liu1\n\n1 Institute for AI Industry Research (AIR), Tsinghua University\n\n2 Xiaomi AI Lab 3 Huawei Technologies Co., Ltd. 4 Shenzhen Heytap Technology Co., Ltd.\n\n5 vivo AI Lab 6 Viomi Technology Co., Ltd. 7 Li Auto Inc.\n\n8 Beijing University of Posts and Telecommunications 9 Soochow University\n\n† Project Lead ‡ Section Lead\n\nContact: liyuanchun@air.tsinghua.edu.cn\n\nWebsite: https://github.com/MobileLLM/Personal_LLM_Agents_Survey\n\nAbstract\n\nSince the advent of personal computing devices, intelligent personal assistants (IPAs) have been one of the key technologies that researchers and engineers have focused on, aiming to help users efficiently obtain information and execute tasks, and provide users with more intelligent, convenient, and rich interaction experiences. With the development of the smartphone and Internet of Things, computing and sensing devices have become ubiquitous, greatly expanding the functional boundaries of IPAs. However, due to the lack of capabilities such as user intent understanding, task planning, tool using, and personal data management etc., existing IPAs still have limited practicality and scalability.\n\nRecently, the emergence of foundation models, represented by large language models (LLMs), brings new opportunities for the development of IPAs. With the powerful semantic understanding and reasoning capabilities, LLM can enable intelligent agents to solve complex problems autonomously. In this paper, we focus on Personal LLM Agents, which are LLM-based agents that are deeply integrated with personal data and personal devices and used for personal assistance. We envision that Personal LLM Agents will become a major software paradigm for end-users in the upcoming era. To realize this vision, we take the first step to discuss several important questions about Personal LLM Agents, including their architecture, capability, efficiency and security. We start by summarizing the key components and design choices in the architecture of Personal LLM Agents, followed by an in-depth analysis of the opinions collected from domain experts. Next, we discuss several key challenges to achieve intelligent, efficient and secure Personal LLM Agents, followed by a comprehensive survey of representative solutions to address these challenges.\n\nKeywords Intelligent personal assistant ⋅⋅\\cdot⋅ Large language model ⋅⋅\\cdot⋅ LLM agent ⋅⋅\\cdot⋅ Mobile devices ⋅⋅\\cdot⋅ Intelligence levels ⋅⋅\\cdot⋅ Task automation ⋅⋅\\cdot⋅ Sensing ⋅⋅\\cdot⋅ Memory ⋅⋅\\cdot⋅ Efficiency ⋅⋅\\cdot⋅ Security and privacy\n\n1 Introduction\n\nScience fiction has portrayed numerous striking characters of Intelligent Personal Assistants (IPAs), which are software agents that can augment individuals’ abilities, complete complicated tasks, and even satisfy emotional needs. These intelligent agents represent most people’s fantasies regarding artificial intelligence (AI). With the widespread adoption of personal devices (e.g., smartphones, smart home equipment, electric vehicles, etc.) and the advancement of machine learning technology, this fantasy is gradually becoming the reality. Today, many mobile devices embeds IPA software, such as Siri [1], Google Assistant [2], Alexa [3], etc. These intelligent agents are deeply entwined with users, capable of accessing user data and sensors, controlling various personal devices, and accessing personalized services associated with private accounts.\n\nHowever, today’s intelligent personal assistants still suffer from the limitations of flexibility and scalability. Their level of intelligence is far from adequate, particularly evident in their understanding of user intent, reasoning, and task execution. Most of today’s intelligent personal assistants are limited to performing tasks within a restricted domain (e.g., simple functions in built-in apps). Once a user requests for tasks beyond these boundaries, the agent fails to comprehend and execute the actions accurately. Altering this circumstance necessitates a significant expansion of the agent’s capability to support a broader and more flexible scope of tasks. However, it is difficult for current IPA products to support tasks at scale. Most of the today’s IPAs require to follow specific predefined rules to complete tasks, such as developer-defined or user-demonstrated steps. Therefore, developers or users must explicitly specify which functions they wish to support, in addition to defining the triggers and steps for task execution. This approach inherently restricts the scalability to wider range of tasks, since supporting more tasks demands extensive time and labor cost. Some approaches have attempted to automatically learn to support tasks through supervised learning or reinforcement learning [4, 5, 6]. However, these methods also rely on a substantial amount of manual demonstrations and/or the definition of reward functions.\n\nThe emergence of Large Language Models (LLMs) [7] in recent years has brought brand new opportunities for the development of IPAs, demonstrating the potential to address the scalability issues of intelligent personal assistants. In comparison to traditional methods, large language models such as ChatGPT, Claude, and others have exhibited unique capabilities such as instruction following, commonsense reasoning, and zero-shot generalization. These abilities have been achieved through unsupervised learning on massive corpora (exceeding 1.4 trillion words) and subsequently fine-tuned with human feedback. Leveraging these capabilities, researchers have successfully adopted large language models to empower autonomous agents (aka. LLM agents), which aims to solve complex problems by automatically making plans and using tools such as search engines, code interpreters, and third-party APIs.\n\nAs a unique type of intelligent agents, IPAs also have the potential to be revolutionized by LLMs with significantly enhanced scalability, capability, and usefulness. We call such LLM-powered intelligent personal assistants as Personal LLM Agents. As compared with normal LLM agents, Personal LLM Agents are more deeply engaged with personal data and mobile devices, and are more explicitly designed for assisting people rather than replacing people. Specifically, the primary way to assist users is by reducing repetitive, tedious, and low-value labor in their daily routine, letting the users focus on more interesting and valuable things, thereby enhancing the efficiency and quality of their work and life. Personal LLM Agents can be built upon existing software stacks (e.g., mobile apps, websites, etc.), while bringing refreshing user experience with ubiquitous intelligent automation abilities. Therefore, we expect Personal LLM Agents to become a major software paradigm for personal computing devices in the AI era, as shown in Figure 1.\n\nDespite the promising future of Personal LLM Agents, related research is still in its nascent stage, presenting numerous intricacies and challenges. This paper takes the first step to discuss the route map, design choices, main challenges and possible solutions in implementing Personal LLM Agents. Specifically, we focus primarily on the aspects related to “personal” parts within Personal LLM Agents, encompassing the analysis and utilization of users’ personal data, the use of personal resources, deployment on personal devices, and the provision of personalized services. The straightforward integration of the general language capabilities of LLMs into IPAs is not within the scope of this paper.\n\nWe started by taking a survey with domain experts of Personal LLM Agents. We invited 25 chief architects, managing directors, and/or senior engineers/researchers from leading companies who are working on IPAs and/or LLMs on personal devices. We asked the experts’ opinions about the opportunities and challenges of integrating LLMs in their consumer-facing products. Based on our understanding and analyses of experts’ insights, we summarized a simple and generic architecture of Personal LLM Agents, in which the intelligent management and utilization of personal data (user context, environment status, activity history, personalities, etc.) and personal resources (mobile apps, sensors, smart-home devices, etc.) play the most vital role. The ability to manage and utilize these personal objects differentiates the intelligence of Personal LLM Agents. Inspired by the L1-L5 intelligence levels of autonomous driving, we also give an taxonomy of five intelligent levels of Personal LLM Agents.\n\nOur findings also highlight several major technical challenges to implement such Personal LLM Agents, which can be categorized into three aspects including the fundamental capabilities, efficiency, and security & privacy. We further dive deeper into these aspects with detailed explanations of the challenges and comprehensive survey of possible solutions. Specifically, for each technical aspect, we briefly explain its relevance and importance to personal LLM agents, then break it down to several main research problems. For example, the foundamental capabilities for personal LLM agents include task execution, context sensing, and memorization. The efficiency of agents is primarily determined by the LLM inference efficiency, customization efficiency, and memory retrieval efficiency. The security and privacy concerns of personal LLM agents can be categorized as data confidentiality, decision reliability, and system integrity. For each research problem, we summarize the main techniques involved with the problem, followed by a brief introduction of the related work. Due to the wide scope of the techniques in personal LLM agents, we only include the most relevant or recent works, rather than attempting to cover all related approaches.\n\nThe main content and contributions of this paper can be summarized as follows:\n\n1.\n\nWe summarize the status quo of existing intelligent personal assistants in both industry and academia, while analyzing their primary limitations and future trends in the LLM era.\n\n2.\n\nWe collect insights from senior domain experts in the area of LLM and personal agents, proposing a generic system architecture and a definition of intelligence levels for personal LLM agents.\n\n3.\n\nWe review the literature on three important technical aspects of personal LLM agents, including foundamental capabilities, efficiency, and security & privacy.\n\n2 A Brief History of Intelligent Personal Assistants\n\n2.1 Timeline View of the Intelligent Personal Assistants History\n\nIntelligent Personal Assistants (IPAs) have a long history of development. We depict the rough timeline of the IPA history in Figure 2. The development progress can be divided into four stages, each marked with a unique color in the figure.\n\nThe 1st stage spans from the 1950s to the late 1980s, which is mainly about the development of speech recognition techniques. The early stage of speech recognition started from basic digits and words. Bell Laboratories developed “Audrey”, which could recognize numbers 0-9 with about 90% accuracy. In 1962, the “shoebox” [8] system came out from Advanced Systems Development Division Laboratory at IBM, which was capable to recognize for up to 16 words. From 1971 to 1976, the Speech Understanding Research (SUR) project, funded by the US Department of Defense, significantly advanced speech recognition technology. The Harpy system [9] was particularly representative, as it could understand sentences composed of 1011 words, equivalent to the proficiency of a three-year-old child. In 1986, IBM developed the Tangora speech recognition typing system [10], capable of recognizing 20,000 words and offering predictive and error-correction capabilities. The Tangora system utilized Hidden Markov Models [11], requiring individual speaker training for voice recognition, with pauses between each word.\n\nThe 2nd stage covers the period from the 1990s to the late 2000s, since speech recognition started to be integrated into software for certain advanced functions. In 1990, the “Dragon Dictate” software [12] was released, which was the first speech recognition product for consumers. It was originally designed to work on Microsoft Windows, supporting discrete speech recognition. “Speakable items” [13] was introduced by Apple in 1993, enabling users to control their computer with natural speaking. In 1996, IBM launched “MedSpeak” [14] for radiologists, which is also the first commercial product supporting continuous speech recognition. Microsoft integrated speech recognition into Office applications in 2002 [15], and Google added voice search to Google Mobile App on iPhone in 2008 [16].\n\nThe 3rd stage extends from the early 2010s. In this period, always-on virtual assistant services began to appear on mobile devices such as smartphones and personal computers. Siri [1], widely considered as the first intelligent personal assistant installed on modern smartphones, was integrated into Apple’s iPhone 4S in 2011. Since its launch, Siri has remained a key built-in software for Apple devices, including iPhones, iPad, Apple Watch, HomePod and Mac, continuously undergoing updates and iterations to incorporate new features. Similar to Siri, many other virtual intelligent assistant started to appear in the period. In 2014, Microsoft released Cortana [17], and gradually integrated it into desktop computers and other platforms. Amazon released Alexa [3] in the same year, which could complete tasks such as voice interaction, music playing, setting alarms, etc. Beyond voice search, Google Assistant [2] was unveiled in 2016, supporting users to interact with both speaking and keyboard input.\n\nThe 4th stage started recently when LLMs start to draw attention from all over the world. Based on LLMs, there emerged many intelligent chatbots (e.g., ChatGPT [18]), as well as some LLM-powered IPA software installed on personal devices (e.g., Copilot [19]). The details of this stage will be covered in Section 2.2.4.\n\n2.2 Technical View of the Intelligent Personal Assistants History\n\nSince there are many aspects that can reflect the intelligence of personal assistants, we select one of the most important ability of Intelligent Personal Assistants, namely the task automation ability (following instructions and completing tasks), to be mainly focused on. In the following subsections, we will introduce four main types of techniques to enable intelligent task automation in IPA. Note that these types of solutions have been developing concurrently, and there is no strict chronological order between them.\n\n2.2.1 Template-based Programming\n\nMost of the commercial IPA products support task automation through template-based approaches. In these approaches, the functions that can be automated are predefined as templates, each of which usually contains the task description, related actions, example queries to match, supported parameters to fullfil, etc. Given a user command, the agent first map the command to the most relevant template, then follow the predefined steps to complete the task. The workflow is illustrated in Figure 3.\n\nWhen using this method to automate tasks, app developers are required to follow the document of certain APIs (e.g., the Google Assistant API [2], SiriKit [20], etc.) to create the template for each function they want to automate. Besides, some approaches are proposed to enable end-users to create their own templates of tasks, such as the “Shortcuts” [21] feature on iPhone devices, enabling the automation of repetitive operation sequences. Similar functions are also implemented in many products and academic research for the Android system, such as Tasker [22], Anywhere [23], Epidosite [24] and Microsoft’s uLink [25] system, etc.\n\nThe advantages of such template-based task automation method lie in its reliability and accuracy, since the steps in the template are deterministic and carefully programmed. However, its scalability is pretty limited, because of the relatively complex mechanism for supporting new tasks. As a result, most apps, including the popular apps from large companies, do not support any automated task or only support some elementary ones, leading to very unflexible user experience. End-users can easilly give up the idea to use IPAs after several unsuccessful attempts [26, 27, 28, 29]. This limitation poses a major obstacle to the further development of template-based intelligent personal assistants.\n\n2.2.2 Supervised Learning Methods\n\nTo address the constraints of template-based IPA methods, researchers are actively investigating automated approaches for enhanced UI understanding and automation. Supervised learning offers a direct method for task automation by training models that predicts subsequent actions and states based on task inputs and current states. The main research questions include how to learn a representation of software GUI and how to train the interaction model.\n\nThe idea of learning an interaction model from human interaction traces is introduced in Humanoid [30], which aims to generate human-like test inputs based on the GUI layout information. Seq2act [4] firstly focused on the mobile UI task automation domain, where the natural language instructions need to be mapped to a sequence of actions that can be directly executed. The framework decomposed the problem into an action phrase-extraction part and a grounding part, both using the Transformer [31] network. Inspired by the success of pretraining in NLP, ActionBert [32] uses self-supervised pretraining to enhance the model’s understanding of UIs. Specifically, to capture the semantics information of the UI switching actions, the model is designed to take a pair of UIs as input, and output embeddings of both UIs and individual components. Fu et al. [33] extended the concept of Words/Sentences from NLP to Pixel-Words/Screen-Sentences. By pre-training with visual atomic components (Pixel-Words), the PW2SS framework (Sentence Transformer) could accomplish various downstream GUI understanding tasks. Aimed at better compatibility with the restricted resource on mobile devices, Versatile UI Transformer (VUT) [34] was proposed to learn different UI grounding tasks within a single small model. It handles images, structures, and text-based types of data, using 3 task heads to support performing 5 distinct tasks simultaneously, including UI object detection, natural language command grounding, widget captioning, screen summarization and UI tappability prediction. Based on the self-aligned characteristics between components of different modalities, UIBert [35] presented a well-designed joint image-text model to utilize the correspondence, learning contextual UI embeddings from unlabeled data. To address the problem of lacking UI metadata, such as DOM tree and view hierarchy, SpotLight [36] introduced a vision-only approach for mobile UI understanding by taking screenshots and a region of interest (the “focus”) as input. Composed of a vision encoder and a language decoder, it can complete tasks according to the provided screenshot and prompt. Besides, Lexi [37] was proposed to leverage text-based instruction manuals and user guides to curate a multimodal dataset. By fusing text and visual features as input to the co-attention transformer layers, the model is pre-trained to form connections between text-based instructions and UI screenshots. UINav [38] utilized a referee model to evaluate the performance of the agent, immediately inform the users of the feedback. It also adopted demonstration augmentation to increase the data diversity.\n\nAs compared with template-based methods, supervised learning approaches have the potential to generalize to unseen tasks after sufficient training. However, training the model typically requires a lot of high-quality human-annotated data. Given the diversity of tasks and apps in the real world, obtaining the training data that covers diverse use cases is challenging.\n\n2.2.3 Reinforcement Learning Methods\n\nUnlike supervised learning-based task automation approaches that require a large amount of training samples, reinforcement learning (RL)-based approaches allows the agent to acquire the capability of task automation by continuously interacting with the target interfaces. During the interaction, the agent gets feedback of rewards that indicate the progress of task completion, and it gradually learns how to automate the tasks by maximizing the reward payoff.\n\nTo train RL-based task automation agents, a reward function that indicates the progress towards task completion is required. World of Bits (WoB) [39] was proposed as a general platform for agents to complete tasks on the Web using keyboard and mouse. The platform came with a benchmark called “MiniWoB”, containing tasks on a set of self-created toy websites with predefined rewards. Glider [5] defines the reward function for real-world websites based on the semantic similarity between the task description and the UI action sequence, as well as the locality and directionality of the action sequence.\n\nAnother challenge of RL-based task automation is the huge action space and the sparse reward. A typical GUI-grounded task usually involves 5555-10101010 steps, each of which contains 10101010-100100100100 candidate actions, leading to a search space size of 105superscript10510^{5}10 start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT-10010superscript10010100^{10}100 start_POSTSUPERSCRIPT 10 end_POSTSUPERSCRIPT. The task is completed only if the correct sequence of actions is taken. In order to tackle such challenge, many frameworks have been proposed. Liu et al. [6] introduced the method to use high-level “workflows” to constrain the allowable actions at each time step. The workflows can prune out bad exploration directions, accelerating the agent’s ability to discover rewards. Gur et al. [40] decomposed the complicated instruction into multiple smaller ones, and schedule a curriculum for the agents to gradually manage to follow an increasing number of sub-instructions. Besides, a meta-learning framework is also proposed to generate instruction-following tasks. Jia et al. [41] framed the actions of agent on the web into three distince categories, namely, DOM selection, token selection, and mode selection. What’s more, a factorized Q-value function is designed, assuming the independence of DOM selection and token selection. Glider [5] achieves its goal of reducing action space with a hierachical policy, which contains a master policy to handle the overall navigation and sub-policies to deal with specific widgets. Humphreys et al. [42] proposed the framework to directly use mouse and keyboard to complete tasks instead of depending on the specialized action spaces, which simplifies the use of behavioural priors informed by actual human-computer interactions.\n\nSimilar to supervised learning methods, the RL-based methods also suffer from poor generalization ability. To achieve flexible and robust task automation, the RL agent needs to train on a large amount of tasks, each requires a well-designed reward function. Defining the reward functions for massive diverse tasks can be difficult.\n\n2.2.4 Early Adoption of Foundation Models\n\nIn recent years, pretrained large fundation models, represented by large language models (LLMs), have seen rapid development and brought new opportunities for personal assistants.\n\nThe scaling law [43] for language models reveals the importance of increasing model parameters for improving model performance, followed by a bunch of models with billions of parameters. The LLMs are typically trained with large-scale open-domain text data in an unsupervised manner, followed by instruction fine-tuning [44] and reinforcement learning with human feedback (RLHF) [45, 44] to improve performance and alignment. ChatGPT [18] unveiled by OpenAI at the end of 2022 is a milestone of LLM that demonstrated astounding question-answering capabilities. By feeding simple task descriptions into the LLM as input prompts, the tasks and responses of LLMs can be easily customized. Besides, these models have also demonstrated robust generalization abilities across various language understanding and reasoning tasks. ChatGPT itself can be viewed as an intelligent personal assistant that assist users by returning information in text responses.\n\nInspired by the capabilities of LLMs, researchers have attempted to let LLMs use tools [46] autonomously to accomplish complex tasks. For instance, such as controlling browsers [47, 48] for information retrieval and summarization, invoking robot programming interfaces for robot behavior control [49, 50, 51], and calling code interpreters for complex data processing [52, 53, 54, 55], among others. It is a natural idea to integrate these capabilities into intelligent personal assistants, enabling more intelligent ways to manipulate personal data, personal devices and personalized services.\n\nThere are already some commercial products that have attempted to integrate LLM with IPA. For instance, Microsoft’s Copilot system [19] has integrated the capabilities of GPT-4 [56], assisting users of Windows in automatically drafting documents, creating presentations, summarizing emails, and thereby enhancing user work efficiency. New Bing [57] also improves the experience of surfing the internet, providing a powerful efficient search engine which better understands what users want. Similarly, Google has integrated LLMs (Bard [58], Gemini [59]) into the search engine to enable more convenient web search experience. Smartphone companies including Huawei, Xiaomi, Oppo, Vivo have also integrated large models (PanGu [60], MiLM [61], etc.) into their on-device IPA products. It is worth noting that some of them adopt solutions based on locally-deployed lightweight LLMs. So far, most of these commercial products are just simple integration of the chat interfaces of LLMs into the personal assistants. Research about deeper functional integration will be discussed in Section 4.1.\n\nDespite exhibiting vast potential, this research direction is currently in an early exploration stage. There is still a substantial distance away from the ultimate goal of truly understanding and assisting users with intelligent agents. What’s more, many issues related to efficiency, security and privacy have not been adequately addressed yet. The subsequent parts of this paper will systematically summarize and discuss the key issues in this direction.\n\n3 Personal LLM Agents: Definition & Insights\n\nWitnessing the great potential of LLM-based intelligent personal assistants and wide interests in both academia and industry, we take the first step to systematically discuss the opportunities, challenges and techniques related to this direction.\n\nWe define Personal LLM Agents as a special type of LLM-based agent that is deeply integrated with personal data, personal devices, and personal services. The main purpose of personal LLM agents is to assist end-users, helping them to reduce repetitive and cumbersome work and focus more on interesting and important affairs. Following this definition, the generic automation methods (prompting, planning, self-reflection, etc.) are similar to normal LLM-based agents. We focus on the aspects that are related to the “personal” parts, such as the management of personal data, the use of smartphone apps, deployment to resource-constrained personal devices, etc.\n\nWe envision that Personal LLM Agents will become a major software paradigm for personal devices in the LLM era. However, the software stack and ecosystem of Personal LLM Agents are still at a very early stage. Many important questions related to the system design and implementation are unclear yet.\n\nTherefore, we attempted to address some of the questions based on insights collected from domain experts. Specifically, we invited 25 experts who are chief architects, managing directors, or senior engineers/researchers from 8 leading companies that are working on IPA-related products, including smartphone personal assistants, smart-home solutions, and intelligent cockpit systems. We talked with them casually on the topics of Personal LLM Agents and asked them several common questions, ranging from the application scenarios to the deployment challenges. Based on our discussion and collected answers, we summarize the insights into three subsections, including the key components of Personal LLM Agents, a taxonomy of intelligence levels, and expert opinions about common problems.\n\n3.1 Key Components\n\nBased on our discussions about the desired features of Personal LLM Agents, we first summarize the main components to support such features, as shown in Figure 4.\n\nUndoubtedly, the core of Personal LLM Agents is a foundation model (large language model or other variants, we call it LLM for simplicity), which connects all other components. Firstly, the LLM is the basis to support different skills for serving the users, including responsive skills that directly execute tasks as users requested (such as question answering, weather checking, event scheduling, etc.) and proactive skills that offer services without explicit user commands (such as life logging, managing user attention, activity recommendation, etc.).\n\nSecondly, to support these skills, the LLM manages various local resources, including mobile applications, sensors, and IoT devices. For example, the agent may complete weather checking by interacting with a smartphone weather app. Meanwhile, many people have mentioned the importance of Personal LLM Agents to provide personalized and context-aware services. Therefore, the LLM should maintain the information about the user, including the current user context (status, activity, location, etc.) and historic user memory (profile, logs, personality, etc.). To manipulate these resources, contexts and memories, it is also desired to use dedicated management systems like vector databases in combination with the LLM.\n\nThe combination of these key components is analogous to an operating system [62], wherein:\n\n1.\n\nThe foundation model is like the kernel in traditional operating systems. It is employed for systematic management and scheduling of various resources, thereby facilitating the functions of the agents.\n\n2.\n\nThe local resource layer is similar to the driver programs in traditional operating systems. In traditional OS, each driver manages a specialized set of hardware. While in Personal LLM Agents, each local resource component manages a type of tool and provides APIs for the LLM to use.\n\n3.\n\nUser context and user memory correspond to the program contexts and system logs maintained during system operations. These components form the basis for the agent to support personalized services.\n\n4.\n\nThe skills at the top layer are analogous to the software applications in traditional OS. Similar to the installation and removal of applications, the skills of agents should also be allowed to be flexibly enabled or disabled.\n\n3.2 Intelligence Levels of Personal LLM Agents\n\nThe desired features of Personal LLM Agents require different kinds of capabilities. Inspired by the six levels of autonomous driving, we categorize the intelligence levels of Personal LLM Agents into five levels, denoted as L1 to L5, as shown in Figure 5. The key characteristics and representative use cases of each level are listed in Table 1.\n\nAt each level, the user and agent are responsible for different duties. At Level 1 (Simple Step Following), agents only take charge of step execution, and the other duties are in charge of the user. For example, when users give the command, agents follow explicit steps defined by the developer or given by the user to complete the task. The L1 agents do not have any ability of sensing or planning. Most template-based IPA products belong to this category.\n\nAs the intelligence level increases, the agents gradually take on more duties. At level 2, the supported tasks are still deterministic (i.e., involving a fixed sequence of actions to complete), but the detailed steps to execute each task are no longer given explicitly. The agents have to auto-complete the necessary steps based on the user’s task description. For instance, given a user query “How is the weather of Beijing today”, the agent calls the weather API with Beijing” as a parameter and retrieves weather information from the response. Unlike the deterministic tasks at level 2, agents at level 3 can complete more complicated tasks that require strategic planning and self-reflection. For instance, the command “Tell Alice about my schedule for tomorrow” needs the agent to determine how to gather the schedule information (e.g., using the user’s calendar and chat history) and how to inform Alice about the information (e.g., summarizing the calendar events and sending via the messenger app). In these tasks, agents autonomously and iteratively generate and perform the execution plan based on intermediate feedback until completing the tasks.\n\nThe agents in L1-L3 work passively driven by the users’ commands, while agents at level 4 can understand users’ historical data, sense the current situation, and proactively offer personalized services at appropriate times.\n\nWith ultra intelligence at level 5, agents play the role of an Autonomous Avatar that can fully represent the user in completing complex affairs, thus users only need to focus on creativity and emotion. Agents not only sense the current status, but also predict the users’ future activities and take actions to facilitate them. Beyond directly serving users, an Autonomous Avatar can also collaborate with other agents to alleviate the burden of their users’ communication. Moreover, the level-5 agents should be able to continuously improve themselves through self-evolution.\n\n3.3 Opinions on Common Problems\n\nOpinion 1 (where to deploy the LLM): Edge-cloud (local-remote) collaborated deployment of LLM is preferred, while existing cloud-only (remote-only) (e.g., ChatGPT) is not a widely acceptable solution. As shown in Figure 7, 88% of participants prefer an edge-cloud collaborated architecture, 58.33% of them support local deployment, and 81.82% of them are not satisfied with the existing cloud-only solutions. Their main concerns are 1) the high latency of remote LLM service, 2) the privacy issue of transmitting personal data to the cloud, and 3) the huge cost of cloud-based LLM services.\n\nOpinion 2 (how to customize the agents): Combining fine-tuning and in-context learning is the most acceptable way to achieve customization. In Personal LLM Agents, customizing the agent for different users and scenarios is considered necessary. Figure 7 shows that 66.67% of participants support combining the advantages of both fine-tuning and in-context learning to reach personalization (L4 intelligence). 43.75% of them do not believe L4 can be achieved by in-context learning; one possible reason is our participants are from the industry, thus they are more focused on the LLM for specific vertical domains where in-context learning hasn’t received much attention.\n\nIn questions 3-5, we ask participants to rank the options and the following tables (Table 3-5) summarize their ranks. Rank 1st-4th denotes the rankness of these options voted by the participants; for example, 72% in Table 3 means that 72% participants rank Text as their first preferred modality. The “score” in each table is calculated based on the Borda Count [63], where each candidate receives points equal to the average of the number of candidates they outrank in each ballot, with the lowest-ranked getting 2222 and the highest n+1𝑛1n+1italic_n + 1 points, where n is the total number of candidates. For instance, 4.564.564.564.56 in Table 3 equals to 5×72%+4×20%+3×0+2×8%5percent724percent20302percent85\\times 72\\%+4\\times 20\\%+3\\times 0+2\\times 8\\%5 × 72 % + 4 × 20 % + 3 × 0 + 2 × 8 %.\n\nOpinion 3 (what modalities to use): The multi-modal LLM, especially Textual and Visual modalities, is desired for Personal LLM Agents. In our statistical result, Text is the most preferred modality just as the most popular LLMs used (e.g., GPT series and LLaMA series). The second-ranked Image option and the Video modality which is specifically mentioned by 20% of the participants show that the visual modality plays a promising role in the future of personal LLM agents.\n\nOpinion 4 (which LLM ability is the most crucial for IPA products): Language understanding is considered the most important capability of LLMs, whereas the ability to handle long contexts is regarded as the most unimportant one. On the contrary, in academia, the capability to handle long context is regarded as very important and is extensively studied. This different opinion originates from the specific vertical-domain LLMs our participants supposed and the general-purpose LLMs of academic researchers. In vertical-domain LLMs, the queries and tasks from users are not very diverse, hence the capacity of long context is not that critical.\n\nOpinion 5 (how to interact with the agents): Voice-based interaction is the most popular way. Unsurprisingly, just like the existing virtual assistant Siri, mimicking the human communication method – voice interaction is the most common and efficient choice. Text-based chatbots and GUI rank second and third since most of the participating experts focus on mobile devices, e.g., smartphones. Virtual reality only obtains a 1.521.521.521.52 score which is the lowest across all questions; this may stem from the high price of VR devices and the unsatisfied user experience of current VR techniques.\n\nOpinion 6 (which agent ability is needed to develop): In the future development of Personal LLM Agents, “more intelligent and autonomous decision-making capability” is considered the most critical feature among our participants; almost half of the participants (47.83%) rank it at first place. The options “Continuous improvement of user experience and interaction methods” and “Secure handling of personal data” also received much attention, with 36.36% and 33.33% respectively, tying for the second place. Although \"Integration with IoT devices\" ranks last, 47.63% of participants still believe it is important as an infrastructure for Personal LLM Agents.\n\nOpinion 7 (what features are desired for an ideal IPA): Based on the responses from the participants, we summarize the following six key features of an ideal agent:\n\n•\n\nEfficient Data Management and Search: The agent acts as an external brain to remember the user’s data by efficient data storage. It provides users with fast retrieval and precise search capabilities.\n\n•\n\nWork and Life Assistance: The agent serves as a copilot in work when users ask for technical details. It can also perform repetitive and heavy tasks and provide document and content generation for users.\n\n•\n\nPersonalized Services and Recommendations: According to user habits, the agent can discover the potential needs of users and then proactively provide services for users. It can serve as a personal and family health manager, medical server, shopping comparison assistance, travel assistance, etc.\n\n•\n\nAutonomous Task Planning and Completion: The agent can understand the user’s intention, decompose the tasks proposed by the user and automatically perform them step by step (further in autonomous chain-of-thought functions), and help the user complete the steps that need manual with explicit instructions.\n\n•\n\nEmotional Support and Social Interaction: The agent can understand and help the user adjust their emotions by chatting. It can also understand users’ relationships with different people, and help them write the response draft in users’ voices.\n\n•\n\nDigital Representative and Beyond: The agent can represent the user to attend meetings, drive the car, go to work, and do any authorized tasks. It can truly understand the user and communicate and socialize with others in the present users themselves.\n\nOpinion 8 (what are the most urgent technical challenges): According to the responses from the participants, the most urgent challenges and technical issues are categorized as follows:\n\n•\n\nIntelligence. 1) Multimodal Support: LLMs need to understand and process different data types (e.g., text, images, and videos), thus it should possess advanced data alignment and interpretation capabilities. 2) Context Understanding and Context-aware Actions: In various application scenarios, LLMs must accurately understand user requirements and generate corresponding control instructions. This needs LLMs’ context understanding ability and the ability to convert the context to effective actions. 3) Enhancing Domain-specific Abilities of Lightweight LLM: LLMs on resource-limited personal devices might underperform in complex tasks or understanding deep contextual meanings due to their size and complexity constraints. Therefore, how to boost the lightweight models’ capabilities and handle complex tasks in specific domains is widely concerned.\n\n•\n\nPerformance. 1) Effective LLM Compression or Compact Architecture: Running LLMs on resource-limited mobile devices needs to balance the performance and quality of task completion. Efficient model compression techniques that concern the characteristics of LLMs to keep high quality of task completion are desirable. 2) Practical Local-Remote Collaborative Architecture: Local-remote collaborative architecture of LLM is considered promising, which is desired to inherit both the fast/low-cost response ability of local model and the high-quality generation ability of the cloud model. However, how to achieve accurate and efficient collaboration is widely considered as an important challenge.\n\n•\n\nSecurity & Privacy. 1) Data Security and Privacy Protection: Ensuring the security of personal data and the protection of user privacy is critical when using personal data to train and execute LLMs. This proposes an urgent requirement to develop new data anonymization techniques and privacy protection protocols. 2) Inference Accuracy and Harmlessness: Ensure that the model outputs are precise and harmless for users, especially when used for decision-making or in sensitive scenarios.\n\n•\n\nPersonalization & Storage. Personalization requires efficient data storage solutions to manage and leverage user-related data, including their preferences, historical behaviors, and interactions.\n\n•\n\nTraditional OS Support. For mobile-based LLM agents, a critical requirement is LLM-friendly interfaces and support of traditional operating systems like Android. This may involve updates at the operating system level and the development of application programming interfaces (APIs) for better integration and utilization of LLM’s functionalities.\n\nMotivated by the valuable opinions of domain experts, the following sections will discuss the desired capabilities and potential challenges in more detail.\n\n4 Fundamental Capabilities\n\nWe first discuss the capabilities required by Personal LLM Agents to support diverse features. Excluding the general capabilities of normal LLM agents, we focus on three fundamental capabilities for personal assistants, including task execution, context sensing, and memorization. Task execution (§4.1) is to translate the users’ commands or the proactively perceived tasks into actions on personal resources. The purpose of context sensing (§4.2) is to perceive the current state of the user and the environment, providing comprehensive information for task execution. Memorization (§4.3) is to record the user data, enabling the agent to recall past events, summarize knowledge and self-evolve. While context sensing and memorization are abilities associated with querying information from users, task execution refers to the ability of providing services to users. Figure 8 depicts the relation of these fundamental capabilities. The following sections discuss these capabilities in details.\n\n4.1 Task Execution\n\nTask execution is a fundamental capability of a Personal LLM Agent, enabling it to respond to user requests and carry out specified tasks. In our scenario, the agent is designed to interact with and control various personal devices such as smartphones, computers and IoT devices to automatically execute users’ commands.\n\nA fundamental requirement for task execution is the agent’s ability to accurately interpret tasks as communicated by users. Typically, tasks may originate from users’ verbal or written instructions, from which the intelligent agent discerns the user’s intent. With the maturation of voice recognition technology, converting voice information into text has become highly convenient [64, 65].\n\nPersonal LLM Agents should make plans and take actions automatically after converting the users’ commands into text. While planning poses a challenge for traditional DNNs, LLM-based agents exhibit greater proficiency in this regard. The planning and reasoning abilities of LLM agents have been discussed in the former surveys [66, 67, 68]. Our paper primarily focuses on the manipulation of personal data and interaction with personal devices. A significant consideration is that Personal LLM Agents might need to interact with applications or systems that may lack comprehensive API support. Consequently, we also explore the user interface (UI) as an important tool for personal agents, enabling effective interaction in scenarios where API limitations exist.\n\n4.1.1 Task Automation Methods\n\nBased on the types of interaction mode, the methods of task execution can be categorized into code-based and UI-based approaches. In the code-based scenario, agents primarily complete tasks by automatically generating code to call APIs. Under UI-based scenarios, agents interact with personal devices by automatically simulating human interactions with the UI interface.\n\nCode-based Task Automation often involves generating appropriate code to interact with APIs, databases, and DNN models. Traditional code-based personal assistants are often based on slot-filling-based task-oriented dialogue (TOD) frameworks. In the era of LLM, more researchers are attempting to directly use LLMs to directly generate code that calls APIs in order to accomplish more complex tasks.\n\n•\n\nSlot-filling method is often used in task-oriented dialogue systems (TOD) or chatbots, which is conversational AI designed to assist users in completing specific tasks through dialogue [69, 70]. In a task-oriented dialogue system, “slots” are predefined categories of information necessary to complete a task. For example, in a travel booking application, slots might include destination, travel dates, number of passengers, etc. During a conversation, the system prompts the user for this information, and calls corresponding APIs to complete the tasks. For mobile devices, many approaches focus on facilitating task automation by allowing users to demonstrate the desired tasks, which can be executed via a conversational interface [71, 72, 24, 25]. These methods often assume that the user’s tasks can be defined as a collection of slot-value pairs. This assumption allows for precise management of the conversation with the controllable units, and to execute the task is to keep prompting users for the values of slots that have not been identified. However, these methods do not consider complex cases where there are multiple values for a slot or relationships between slots [73]. Besides, they heavily rely on well-defined APIs and lack adaptability to unseen domains. Recent research papers utilize the understanding and reasoning ability of LLMs to complete more complex and multi-turn TOD tasks [74, 75, 76, 77], and improve the efficiency of Slot-filling methods.\n\n•\n\nProgram synthesis method is to utilize the code generation ability of LLMs to interact with APIs. One way is to fine-tune LLMs to use specific APIs. WebGPT [47] fine-tunes a GPT-3 [78] to answer long-form questions by calling Microsoft Bing Web Search API [79]. Some recent works [46, 80, 81, 82] fine-tune LLMs to retrieve and call APIs, enhancing their performance in various tasks like mathematical reasoning and program synthesis. Octopus V2 [83] introduces a 2B parameter on-device LLM to call Android APIs for task automation. Another way is to utilize the chain reasoning [84, 85, 68] and in-context learning ability [78] of LLMs. They show descriptions and demonstrations of the tools (e.g. APIs, other DNNs, etc.) in context and ask LLMs how to use them to complete tasks [86, 87, 88, 52, 89]. However, fine-tuning LLMs can be costly and restricted to the predefined set of tools, and in-context learning may fail when the number of APIs go large. Thus, authors of ToolkenGPT [90] attempt to solve this problem by representing each tool (API) as a token.\n\nCode-based methods can complete thousands of tasks from web searching to image generating. However, not all the needed APIs are available for agent developers in real-life apps out of security concerns or business interests. Besides, there are tasks that can be executed easily for human users but are difficult for calling system APIs [73]. Depending solely on publicly available APIs may not fully meet the highly diverse requirements for mobile task automation.\n\nUI-based Task Automation. Autonomous UI agents attempt to translate users’ tasks into UI actions on smartphones or other personal devices, automating these tasks through direct UI interaction. Compared to code-based task execution, autonomous UI agents do not rely on publicly available APIs, potentially allowing for more versatile automation capabilities. However, executing users’ tasks by UI actions is not easy for traditional DNN models because of the implicit relations between tasks and UI elements. Recently, researchers utilize the comprehension and reasoning abilities of LLMs to improve the performance of autonomous UI agents.\n\nThe input of the UI agent is a task described in natural language, and a representation of the current UI, and the output is the UI action to be executed on the UI. Depending on how they represent the UI, we can categorize the autonomous UI agents into text-based GUI representation and multimodal GUI representation.\n\n•\n\nText-based GUI representation is to convert the UIs into pure text. Seq2act [4] trains a transformer-based model [31] to ground users’ instruction to UI actions described in <operation, object, argument> tuples. Researchers also investigate prompting with mobile UIs to complete tasks of UI instruction mapping [91]. The authors convert mobile UI into HTML code, which is easy for LLMs to understand because an important part of their training data is scraped from Github. DroidBot-GPT [92] is an LLM-based system to complete users’ tasks in a sequence of UI actions. Mind2Web [93] filters the raw HTML of webpages with a smaller LM and uses the LLM to select the target element and action. AutoDroid [94] uses app analysis tools to acquire app domain-specific knowledge and uses it to augment the LLMs for task automation. In AXNav [95], authors build a system using LLMs and pixel-based UI Understanding to execute manual accessibility tests. MemoDroid [96] introduces an LLM-based mobile task automator that can break tasks into smaller sub-tasks and complete them by recalling former actions.\n\n•\n\nMultimodal representation is to use the image (and text) description of UI as the input of the Personal LLM Agents. Early research work is focused on training multimodal transformers to ground user commands to UI elements [97, 98, 38]. In the era of LLMs, some approaches attempted to combine visual encoders with LLMs to handle GUI images [99, 100, 101]. With the advent of Large Multimodal Models (LMMs), a growing number of projects employed visual language agents for UI action grounding and navigation [102, 103]. One trend involves leveraging powerful LMMs such as GPT-4V to comprehend GUIs and select UI elements [104, 105, 106, 107]. Another line of research is to customize open-sourced LMMs by fine-tuning on large-scale datasets for GUI-related tasks [108, 109, 110].\n\nWhile UI-based task automation has the potential to achieve a more flexible personal agent framework compared to API-based automation, its research is still in the early stages. It remains challenging to accomplish more complex user commands. Besides, the privacy and security issues have not been fully addressed [94, 99]. It also remains controversial about the UI representation. While multimodal representation can handle elements that cannot be parsed through accessibility services, it is plagued by the heavy demands of screen recording and the limited reasoning abilities of current vision language models [111].\n\n4.1.2 Autonomous Agent Frameworks\n\nAn LLM-powered autonomous agent is composed of an LLM brain to make plans and self-reflection, a memory to store past information and knowledge, and a tool usage module to interact with tools (e.g. APIs, UIs, programming languages) [112, 67]. There are a lot of popular projects that provide frameworks for users to create LLM-powered agents [113, 114, 115, 116, 117, 118, 119, 120, 121]. They attempt to enhance the ability of LLMs by interacting with other external tools and retrieving long/short-term memory. Auto-GPT [113] is one of the most famous frameworks, which can execute users’ commands by generating prompts for GPT and using external tools. LangChain [114] is another popular framework that helps developers to create more sophisticated and context-aware applications using LLMs. Due to the ability to understand and produce natural language, LLM-powered agents can also engage with one another effortlessly, fostering an environment where collaboration and competition among multiple agents can thrive [122, 123, 118, 124]. These autonomous agent frameworks make significant engineering contributions, providing a more user-friendly framework for the LLM-powered applications.\n\nFor mobile devices, AutoDroid [94] provides an effective framework for developing mobile agents. Developers can easily create an automator for mobile tasks by either exploring apps using a test input generator or through manual demonstration. AutoDroid then automatically analyzes these records and utilizes them to improve Language Learning Models (LLMs) for more efficient task automation. Huang et al. [125] develop a new method to effectively extract macros (basic units of user activity in apps such as “login”, or “call a contact”) from user-smartphone interaction traces. These macros can help agents to automatically complete tasks.\n\n4.1.3 Evaluation\n\nEvaluating the performance of task execution is a challenging issue. For API-based task execution, former surveys have provided a comprehensive summary on how to evaluate them [66, 68]. Our paper mainly focuses on the evaluation of UI-based task automation.\n\nMetrics: The metrics of UI-based task execution are completion rate [4, 97, 94] and manually designed reward [126, 127]. The completion rate is the probability that all actions predicted by the model are entirely consistent with the ground truth. However, since there may be different methods to complete a task, and the ground truth typically represents only one of these methods, the accuracy evaluated by this approach is not entirely correct [94]. Manually designing rewards based on the crucial steps can be more precise [127], but they are less scalable because of the complex annotating process.\n\nBenchmarks: Table 6 lists the benchmarks of UI-based task automation. One group of benchmarks is static datasets, which often include a set of human-annotated tasks, structured UI data (and screenshots), and actions to complete the tasks. Some of the tasks are synthetically generated [4, 126, 127]. The early works mainly focus on low-level tasks with clear instructions [128, 35], for example, click the ‘settings’ button, and then click ‘Font size’. Later works introduce high-level tasks that could be completed in multiple steps [4, 129, 97, 130, 93, 131, 132, 133, 134, 135], for example, delete all the events in my calendar. Another group of benchmarks are platforms that enable the agent to interact with. MiniWoB++ [39, 6], WebShop [136], and WebArena [137] provide web environments where agents can navigate and operate on the web by clicking, typing, closing page, and so on. AgentStudio [138] provides a comprehensive platform that supports interactions with versatile real-world computers. AndroidEnv [126] and MobileEnv [127] provide a dynamic environment where agents can engage with any Android-based application and the core operating system. This framework allows for a wide scope of interaction and task-solving capabilities within the diverse Android platform.\n\nRemark. Existing approaches have demonstrated the remarkable ability of LLM agents in task reasoning and planning. However, there are several important problems to solve to realize practical Personal LLM Agents. 1. How to accurately and efficiently assess the performance of agents in real-world scenarios. Because there are usually various ways to accomplish the same task, it is inaccurate to use a static dataset to measure the accuracy of task execution. Meanwhile, dynamically testing the tasks in a simulated environment may be inefficient and hard to reproduce. 2. How to robustly determine if a task has been completed. LLMs often experience hallucinations during task execution, making it difficult to determine whether the current task has been completed. 3. Regarding UI agents, what is the best way to represent the software UI? The vision-based representation (e.g. screenshot) is generally available, while the text-based representation is usually more lightweight and friendly for LLM agents to operate.\n\n4.2 Context Sensing\n\nContext Sensing refers to the process that the agent senses the status of the user or the environment, in order to provide more customized services. In this work, we adopt a broad definition of context sensing, by considering generic information gathering process as a form of sensing. Hardware-based sensing aligns with the conventional notion of sensing, primarily involving data acquisition through various sensors, wearable devices, edge devices, and other data sources. On the other hand, software-based sensing emphasizes diverse means of data acquisition. For example, analyzing user typing habits and common phrases constitutes a form of software-base sensing.\n\nIn Personal LLM Agents, context sensing capability serves various purposes. 1. Enabling Sensing Tasks: Some tasks inherently require the agent to do sensing. For instance, when a user requires the agent to detect snoring during sleep, the agent must possess the ability to actively acquire, process, and analyze audio data. 2. Supplementing Contextual Information: The sensed information can facilitate the execution of ambiguous or complex tasks. For example, when the user wants to listen some music, it’s good to know the current activity of the user to recommend appropriate music. 3. Triggering Context-aware Services: The sensing capability is also the basis to provide proactive services. For example, the agent may notice the users to keep focus upon detecting dangerous driving behaviors. 4. Augmenting Agent Memory: Some information perceived through sensing can become a part of the agent memory, which can be used by the agent for further customization and self-evolution.\n\nWe introduce the techniques of context sensing from two perspectives, including sensing sources and sensing targets.\n\n4.2.1 Sensing Sources\n\nHardware Sensor. Modern personal devices are equipped with a wide range of built-in hardware sensors, including accelerometers, gyroscopes, magnetic field sensors, light sensors, thermometers [139], microphones [140], GPS modules, cameras [141], etc. Some other modules such as bluetooth and Wi-Fi [142] can also be used for sensing purposes. With the growing prevalence of wearable and IoT devices such as smart watches, bluetooth headphones [143], and smart home devices [144], the sensing scope and sensing modalities are greatly expanded.\n\nRecently, there has been a proliferation of research exploring the deep integration of LLMs with raw sensor data. For instance, several studies directly embed raw IMU data into prompts for LLM, enabling Human Activity Recognition (HAR) [145] or trajectory prediction [146]. Zhang et al. [147] provides LLM with a bird’s-eye view of a 3D scene and allows it to iteratively select viewpoints to understand 3D point cloud scenes. Additionally, Zheng et al. [148] employs a trainable dual-channel audio frontend and fine-tuned LLM to enable LLM to comprehend spatial sound. Similar frontend and fine-tuning approaches are prevalent in various domains such as LiDAR [149] and autonomous driving [150, 151].\n\nSoftware Sensor. Unlike hardware sensing that obtains data from real sensor devices, software sensing focuses on obtaining information from existing data, such as app usage [152], call records [153], typing habits [154], video game [155], etc. The scope of software sensing is incredibly broad. For instance, in the field of natural language processing or audio, there exists a plethora of sensing research based on text or speech. Furthermore, recommendation systems such as e-commerce or short video platforms, the process typically involves first sensing certain user information and subsequently recommending specific products or content. These sensors let agents better understand the users, enabling them to provide with more intelligent and personalized services.\n\nCombination of Multiple Sensors. Multi-sensor collaborative sensing stands out as an effective method for enhancing perceptual capabilities. Previous endeavors have demonstrated the assessment of user emotions, stress levels, and emotional states based on touchscreen and inertial sensors [156], identification of time spent through screen capture and sensor data [157], breath detection through headphone microphones [158], and nuanced motion detection through sensors and audio [159].\n\nThe significance of multi-sensor collaboration extends to the proliferation of intelligent wearables and smart homes. For instance, automatic recognition of when a user is working or resting using data collected from personal devices [160] (smartwatches, laptops, and smartphones), or action detection through the combination of headphones and smartphone microphones [143]. Furthermore, technologies involving the fusion of household appliances, such as user action perception based on existing wired devices [161], motion recognition in smart home environments [144], Wi-Fi-based motion detection [162], multiperson detection [142], and sleep monitoring [163].\n\nThere are three different approaches to enable LLM to understand and utilize sensor data.\n\n•\n\nOption 1: Sensor Data as Prompt. This method directly inputs sensor data into LLM as text prompts. Such an approach can be applied to various sensing sources such as IMU [146] and bluetooth [164]. The mappings between the raw sensor data and the prompts can be created through rules, such as mapping tactile sensations on object surfaces to descriptors like “soft” or “hard” [165]. This method is simple and effective as demonstrated many existing studies. However, it also has important limitations, such as the significant computational cost of processing large volumes of raw data and the limited ability of LLM to understand the complex sensor data in plain text.\n\n•\n\nOption 2: Sensor Data Encoding + Fine-tuning. This approach enables LLM to understand sensor data with a data encoder. The encoder generates token embeddings from the raw sensor data with a learned neural network, and the embeddings are usually integrated into LLM through fine-tuning. This method yields significant results for complex sensor data, such as LiDAR [149] and dual-channel audio [148]. This approach allows LLM to efficiently understand sensor modalities, which is used to construct complex end-to-end systems like autonomous driving [151, 150]. Its drawback lies in the high training difficulty.\n\n•\n\nOption 3: Redirecting Sensor Data to Domain-Specific Models. This approach doesn’t process sensor data directly with LLMs, while it uses LLMs to invoke other specialized small models to deal with the raw sensor data. For example, Darvish et al. [166] leveraging techniques like object detection or pose estimation to assist chemical experiment robots in improving perception and understanding, additional information is added to the raw data stream and transformed into a form that LLMs can understand.\n\nMulti-sensor and multi-device scenarios necessitate intricate considerations in data source selection, data fusion, and data analysis methods. Existing methodologies include LLM-driven strategies for generating multi-sensor policies in human behavior understanding [167], emotion-agnostic multi-sensor data multitask learning frameworks [168], cross-modal fusion of sensing data [169], wearable device motion recognition with a focus on multi-sensor fusion [170], and predictive anxiety in sensor data under conditions of data absence [171]. Furthermore, there are studies that analyze the importance of data features in fall detection [172].\n\nWith the evolution of sensing technologies, multi-sensor and multi-device collaborative sensing has become a staple approach for perceiving complex scenarios. Effectively integrating diverse data sources to maximize accuracy and determining methods to eliminate less crucial data from a multitude of sources to conserve resources are vital research areas.\n\n4.2.2 Sensing Targets\n\nThe objectives of context sensing can be categorized into environment sensing and user sensing. Environment sensing encompasses factors such as location, occasion, religious and cultural backgrounds, national and societal contexts, and more. Meanwhile, user sensing incorporates elements such as user activities, states, personal information, personality traits, emotions, goals, physical conditions, and other related aspects.\n\nSensing the Environment. We further categorize environment sensing into two dimensions: scene sensing and occasion sensing. Scene sensing predominantly involves more tangible environmental factors, such as locations and places. Occasion sensing delves into deeper environmental information, including religious and cultural backgrounds, national differences, and social relationships.\n\n•\n\nScene sensing is often readily perceptible but hold significant importance, leading to variations both in behavior and emphasis. For behavior instance, detecting a user in a library prompts the agent to adjust the phone to silent mode, while in a bar increasing the volume and activating vibration may be necessary. Similarly to emphasis, when a user is in a meeting room, the agent should focus more on tasks related to meeting content recording and work organization, whereas in a gym, emphasis should shift towards fitness plans and heart rate analysis. Previous work in scene awareness has employed various techniques [173], such as location-based approaches [174], audio or video analysis [175, 176], and sensor capabilities analyzing aspects like airflow through smartphone microphones to assess ventilation [140], or scene recognition achieved by analyzing macro photographs taken with the smartphone camera when placed near a surface [141]. Zhang et al. [147] let LLM understand 3D scenes through LLM-guided multiple viewpoint selection.\n\n•\n\nOccasion perception is more elusive in perception, and their impacts are relatively discreet. Earlier studies have identified differences in behavior and emotion recognition tasks across countries [177] and regions [178]. The national, ethnic, religious, and cultural backgrounds implied by the current user and setting are crucial. Perceiving others and objects in the current environment is equally vital. For example, previous work detected social scenarios based on sensor data, analyzing the behavior of socially anxious individuals in different social settings [179]. Other research delved into analyzing drinking-related social scenes using multiple sensors, even predicting the size and gender composition of drinking groups [180]. Additionally, studies explored the relations between sensor data, dietary habits, and social settings, revealing a strong association between binge eating and social environments, making it predictable [181]. Liang et al. [182] use LLM forecasting pedestrian flow through the analysis of public events.\n\nEnvironment sensing is crucial context information for a personal agent. Different environments lead to distinct behaviors and focal points, extending beyond mere locations to encompass social occasions, cultural backgrounds, and deeper conceptual elements, all environment individuals and relationships, interactions, and anticipating the impacts on both the environment and the user. These considerations directly influence the level of intelligence exhibited by the personal agent.\n\nSensing the User. User awareness is one of the primary features of Personal LLM Agents. A deeper understanding of the user can better reflects the value and significance of the Personal LLM Agents. We categorize user sensing into two temporal dimensions, including short-term and long-term. Short-term sensing exhibits higher temporal variability and increased randomness. On the other hand, long-term sensing necessitates extended maintenance and correction, making it relatively more stable and reliable.\n\n•\n\nShort-term user sensing encompasses various aspects, including users’ routine actions [183], or specialized activities such as tooth brushing effectiveness [184], Ji et al. [145] found that even directly feeding IMU data to LLM can perform Human Activity Recognition (HAR) tasks. User states such as working or resting [160, 157], user health conditions [185, 139, 186], as well as user emotions [187, 156] and stress levels [188]. Recently, numerous studies have attempted to explore the applications of LLMs in the field of health monitoring [189, 190, 191]. Short-term sensing typically involve rapidly changing and shallow-level state information. Efficiently capturing such information can significantly enhance the context awareness of Personal LLM Agents.\n\n•\n\nLong-term user sensing mainly focus on the analysis of users’ profile and personality. Various approaches have been proposed to understand users’ work, study, and daily life. For instance, a study utilized sensor data from new smartphones to detect the prolonged psychological states of freshmen [192]. Another study demonstrated the capability to predict learning performance and social activities based on perception data [193]. Gao et al. [194] delve into the techniques to predict personality based on the intensity of physical activities. There is also research examining the relationship between sensor data and user career advancement [195], as well as a study that predicts user life satisfaction [196]. Furthermore, specific states of users have been a focus, including studies on the perception of mental illnesses [197, 198], such as one that predicts and analyzes schizophrenia [199], depression [190], and another that detects habits like smoking [200]. Lifelo et al. [191] utilized LLM to conduct psychological disorder analysis for a highly rare African language. Additionally, Ouyang and Srivastava [201] attempt to extract higher-level perceptual information from simple data. Long-term sensing involve deep and abstract information, containing the profound logic behind user behavior. These pieces of information are often more subtle, making perception and maintenance challenging. However, they constitute an essential aspect for advanced personal agents.\n\nIn terms of user sensing, there are also several LLM-based initiatives, such as employing LLM for recommendation tasks [202, 203], sentiment analysis with LLM [204], and the development of a personal doctor equipped with inquiry and perception capabilities [205].\n\nRemark. Existing methods often confine themselves to specific sensors, individual apps, or particular domains. In Personal LLM Agents, a possible opportunity is to unify all sensing results concerning the environment and the user to originate from diverse sources. However, to achieve this goal involves several important research challenges. 1. What is a unified format or ontology of the sensed information? The agents should be able to convert diverse sensing data into this format and conveniently use the data for various downstream tasks. 2. Given the broad scope of sensing, how can the agents decide when and what to sense, in order to provide context-aware services with minimal overhead?\n\n4.3 Memorizing\n\nMemorizing denotes the capability to record, manage and utilize historical data in Personal LLM Agents. This capability enables the agents to keep track of the user, learn from past experiences, extract useful knowledge, and apply this acquired knowledge to further enhance the service quality. The related work is mainly aimed to answer two questions, including how to obtain the memory and how to utilize the memory.\n\n4.3.1 Obtaining Memory\n\nThe agent memory can be in various formats. For example, the basic user profiles (e.g., birthdate, addresses, personalities, preferences) are often stored in key-value pairs, allowing for easy key-based retrieval. Historical records are usually represented as sequences indexed by timestamps, which archive user service access, activities, system events and so on over the time. The user’s documents, photos, videos, etc. are stored as files, which are often produced by other applications. There are mainly two ways to obtain the memory: directly logging the raw data or indirectly inferring knowledge from raw data.\n\nLogging. The most straightforward way to obtain memory is through logging, such as recording user input, system events, and sensed contexts. Logging data is often relatively simple. Life logging is a commonly-discussed topic that focuses on tracking and recording user data created through the activities and behaviors of users, contributing to a comprehensive understanding of individuals’ lifestyles and preferences [206, 207]. Data recorded at specific moments using video cameras provide deeper overview of daily activities [208]. Moreover, recording data over long periods of time can provide valuable insights into behavior patterns, which will support the personalization of intelligent agents [209].\n\nInferring. Another way of Personal LLM Agents to obtain memory is to extract knowledge from the raw data. With the advancements in machine learning and data analytics, it has become possible to infer user behavior, patterns, and interactions to gain insights into their psychology, preferences, and other high-level information. For example, user personality can be extracted from texts [210, 211], emotions can be read from image and text data [212, 213], preferences can be modeled from historical interaction information [214], and knowledge graphs can be extracted from smartphone push notifications [215]. These extracted high-level information will also be stored as memories of the agent and utilized in services.\n\n4.3.2 Managing and Utilizing Memory\n\nAfter obtaining the memory, the next question is how to manage and utilize the memory to provide better services in Personal LLM Agents. Based on the purposes of utilizing memory, we divide the relevant techniques into following three parts, including raw data management, memory-augmented LLM inference, and agent self-evolution.\n\nRaw Data Management and Processing. A basic ability of Personal LLM Agents is to access and process the raw memory data (e.g., selecting, filtering, transforming to other formats, etc.), in order to facilitate other advanced functions. This line of work primarily focus on enabling more natural and human-comprehensible access, manipulation, and modification of data. Since the input-output and reasoning processes of LLMs are based on natural language, such interfaces are more easily integrated with other capabilities of large models. In this research area, numerous endeavors have explored the use of machine learning models or template-based methods to map user data requests to database SQL statements [216, 217]. There are also framework-level works examining how to unify and simplify data interfaces. For instance, PrivacyStreams [218] unifies all personal data access and processing interfaces into a stream-based framework, which is more conducive for large language models to comprehend and manage.\n\nMemory-augmented LLM Inference. To enable the Personal LLM Agents to provide customized services based on the user-related memory, it is usually desired to make use of the memory data in the LLM inference process. Recent research in LLM agents has explored leveraging memory to enhance decision-making and reasoning [85, 219, 220, 221, 222], which provides inspiration for a solution where Personal LLM Agents can offer personalized services to users through memories. The techniques can be different based on the types of the memory.\n\n•\n\nShort-term memory preserves and retains pertinent information in the form of symbolic variables, ensuring its accessibility and applicability during the current decision cycle. This includes perceptual inputs, active knowledge (generated by reasoning or retrieved from memory data), and other core information carried over from the previous decision cycle (e.g.,., agent’s active goals). CoT [84], Scratchpads [223] encourage the LLM to generate intermediate reasoning, using the LLM’s own context as a form of working memory. CoALA [224] proposes that working memory should be a persistent data structure during long-term memory (LLM) calls. Each call generates its input from a subset of working memory (e.g., a prompt template and relevant variables), and the output is subsequently parsed into other variables (e.g., an action name and arguments) which are stored back in working memory and used to execute the corresponding action. In addition, short-term memory has the capability to interact with long-term memory and other data interfaces, serving as the central hub connecting different components of a language agent [225, 226].\n\n•\n\nLong-term memory stores experiences from earlier decision cycles. This can consist of history event flows [219], game trajectories from previous episodes [227, 228], interaction information between the user and the agent or other representations of the agent’s experiences. During the planning stage of a decision cycle, these episodes may be retrieved into working memory to support reasoning. An agent can also write new experiences from working to episodic memory as a form of learning. Secondly, long-term memory stores an agent’s knowledge about the world and itself. Traditional approaches leverage retrieval for reasoning or decision-making initialize memory from an external database for knowledge support (e.g., retrieval-augmented methods in NLP [229, 230], “reading to learn” approaches in RL [231, 232]). Agents may also write new knowledge obtained from LLM reasoning and user into long-term memory as a form of learning to incrementally build up world knowledge from experience.\n\nAgent Self-evolution. To better accommodate users, Personal LLM Agents may also need to dynamically update themselves based on the memory data. We refer to this as “self-evolution”. The foundational functionality of intelligent agents is predominantly reliant on LLM. Therefore, the key to the self-evolution of intelligent agents lies in how to leverage LLM for the discovery and exploration of new skills, as well as in the continuous update of the LLM itself.\n\n•\n\nLearning Skills. Currently, numerous efforts are underway to enable LLM-based agents to engage in continuous skill learning and acquisition [233, 234]. These methods draw inspiration from the generality and interpretability of programs [235], considering skills as executable code, and optimize skill acquisition by leveraging the in-context learning ability of LLM through the strategic use of prompts. They also manage a skill repository, integrating new skills as APIs, enabling intelligent agents to continually learn and reuse these skills in subsequent tasks. Prior work has demonstrated that modern LLMs can capture relevant information about meaningful skill chains [51, 49]. Hence, intelligent agents have the capability to acquire novel skills by strategically linking skills within a foundational skill set [236]. In this process of skill chaining, the intelligent agent makes purposeful selections of subsequent meaningful skills, leveraging the a priori knowledge embedded in LLM and utilizing execution feedback to allow the language model to adjust its selections. This targeted approach enables the agent to efficiently assimilate complex skills.\n\n•\n\nFinetuning LLM. To achieve the self-evolution of intelligent agents, continuous fine-tuning of the LLM is also required. There are several reasons: 1. Current LLMs were not specifically designed for agent-specific use cases, such as generating actions or self-evaluations, where limited learning support is provided by few-shot prompting. 2. Due to performance constraints on mobile devices, the capabilities of the LLM component of the intelligent agent are limited. This limitation makes it difficult for the model to acquire new skills through prior knowledge and in-context learning abilities. 3. During the operational phases of intelligent agents, the consistent emergence of materials such as the latest corpus [237], new knowledge [238], and tools [239] can frequently change the task schemas. This necessitates continual adaptation of LLMs. In such cases, fine-tuning the model becomes necessary to enhance its capacity for handling new tasks and generating appropriate actions. Research indicates that fine-tuned smaller LLMs could outperform prompted larger LLMs for specific reasoning [240, 241] and acting [225] needs, while enjoying reduced inference time and expense. Parameter efficient fine-tuning (PEFT) [242] presents a promising approach for efficiently fine-tuning LLMs. It only requires fine-tuning a small subset of external parameters [243], making it friendly for edge devices, and it can effectively alleviate the issue of catastrophic forgetting [244]. There have also been some preliminary attempts to conduct the study of LLM fine-tuning for agents [245] with trajectories from multiple tasks and prompting methods, inspiring future endeavors aimed at developing more capable and useful Personal LLM Agents.\n\nRemark. The ability to generate and leverage the memory about the user is the basis of personalization in Personal LLM Agents. We highlight following three open problems surrounding the memory mechanism of Personal LLM Agents. 1. The agent memory can potentially be huge, heterogeneous and dynamic. What is the most effective and efficient way for the agents to organize and retrieve the memory? 2. Human has the ability to forget. Since inappropriate data in the memory can be harmful for the agents’ service quality and efficiency, how can the agents determine what information to memorize? 3. What is the best way for the agents to self-evolve with the memory? Specifically, what data to use, when to evolve, and how (fine-tuning or else)? How can the personalized models accept updates of the base foundation model?\n\n5 Efficiency\n\nDue to the limited hardware resource and power supply on many personal devices, it is important to improve the efficiency of Personal LLM Agents in the deployment stage. We’ve discussed in Section 4 the fundamental capabilities of Personal LLM Agents, including task execution, context sensing, and memorizing. These capabilities, as shown in Figure 9, are backed by more elementary processes, mainly including the inference, customization and memory retrieval of the LLM agent. Each of these processes desires careful optimization of efficiency, as described below.\n\nInference of LLMs is the basis of an agent’s various capabilities. For example, the agent may first decompose a complex task into several steps with the help of the LLM, then solve each step through either LLM inference or invoking personal tools (e.g., schedule a meeting). Sensing the context or generating the memory may also rely on the reasoning abilities of LLMs. While the cost of using the tools or sensors is usually hard to estimate due to the diversity, LLM inference is a common procedure that demands a lot of both computation and memory resources. Therefore, the LLM inference becomes the performance bottleneck for the Personal LLM Agents, requiring careful optimizations on its efficiency.\n\nCustomization is another important process of Personal LLM Agents for accommodating different user requirements. Customization is needed when the agents are installed to different users or used in different scenarios. The self-evolution of Personal LLM Agents is also a process of customization. To offer customized services, an agent can either feed the LLM with different context tokens or tune the LLM with domain-specific data. Due to the frequent needs of customization, the processes may impose considerable pressure on the system’s computational and storage resources.\n\nMemory manipulation is another costly process. To provide better services, the agents may require access to longer contexts or external memories, such as environment perceptions, user profiles, interaction histories, data files, etc. Consequently, this gives rise to two considerations. The first pertains to necessitating LLMs to handle longer inputs. The second issue centers around the management and acquisition of information from an external memory bank.\n\nWe’ll dive into the efficiency of each component in the following subsections, as is shown in Figure 10.\n\n5.1 Efficient Inference\n\nSince the runtime cost of Personal LLM Agents is dominated by LLM inference, it is important to improve the inference efficiency to enhance the overall efficiency of the agent. Although the total inference cost can be significantly influenced by the design of agents, including how the agents send requests to LLMs, what prompts to use, etc., we will be focused on model and system-level approaches only. The reason is that the designs of agents may vary based on the actual applications and don’t directly contribute to the efficiency of LLM inference itself.\n\nMany model and system-level approaches have been proposed to improve the efficiency of LLM inference. While some of them are generic for the overall performance and efficiency (e.g., model compression), there are also techniques targeting the efficiency of specific perspectives, such as model size, inference latency, memory consumption, energy consumption, etc. We will discuss these aspects separately in the following parts of this subsection.\n\n5.1.1 Model Compression\n\nModel compression techniques, which directly reduce the model size and computations, are generic optimizations to enhance the inference efficiency of LLMs, including computation, memory, energy and etc. The model compression techniques are further categorized into various approaches, including quantization, pruning (sparsity), distillation and low-rank factorization.\n\nQuantization is one of the most important compression approaches for LLMs. It reduces the model size by using fewer bits to represent the model parameters, and also reduces computations with system-level support for quantized kernels. Quantization methods can be further divided into post-training quantization (PTQ) and quantization-aware training (QAT), based on whether additional training is required after quantization. Unlike QAT (e.g., LLM-QAT [248]) which requires non-negligible additional training effort, PTQ is more available and flexible for on-device deployment under different hardware constraints.\n\nRecent works have revealed that the difficulty of LLM quantization mainly lies in activations, where the outliers are hard to quantize [305, 306]. Existing works have proposed various approaches to tackle this challenge. A typical line of work adopts the weight only quantization (WOQ) paradigm, which conduct integer quantization (e.g., INT4 and INT8) on weights only, while preserving activations in float formats (e.g., FP16 and FP32). WOQ achieves a trade-off between the compression ratio and model perplexity. A straightforward way of WOQ is the group-wise uniform quantization implemented in current mobile deployment frameworks (e.g., llama.cpp [307] and MLC-LLM [308]). Recent works also proposed different quantization algorithms to enhance model capability, such as GPTQ [246] and AWQ [247].\n\nDespite the WOQ techniques, another line of work quantizes both weights and activations. For example, ZeroQuant [249] performs INT8 quantization for both weights and activations, using group-wise quantization for model weights and token-wise quantization for activations. However, the activations, including key-value (KV) pairs, are usually more difficult to quantize compared to model weights because of outliers. There have been extensive works to tackle this challenge. SmoothQuant [250] migrates the quantization difficulty of activations to weights through additional scaling operations that “smooth” the outliers in activations, and thereby achieve negligible accuracy degradation in W8A8 quantization. Subsequent works further attempt to lower the usable quantization bitwidth down to 4-bit through various techniques including channel re-ordering (RPTQ [309]), channel-wise shifting and scaling (Outlier Suppression+ [310]), and adaptive channel reassembling (QLLM [311]). Notably, RPTQ addresses the KV storage issue by developing a new quantization scheme that focuses solely on KV cache when quantizing activations, which is the major memory consumer in long-context inference.\n\nWhile integer quantization methods such as INT4 and INT8 remain mainstream solutions in current deployment practice, there has been a new trend of low-bit floating point quantization, such as FP4 and FP8. One reason is that floating point quantization can achieve comparable or even higher accuracy than integer quantization [312, 313, 314]. Besides, floating point quantization is possible to achieve higher computational performance on both cloud GPUs like NVIDIA H100 with dedicated computing support, and mobile GPUs [315].\n\nPruning reduces the model size and computations by removing less important connections in the network. Pruning is categorized into structured pruning and unstructured pruning. Structure pruning usually removes weights in regular patterns, such as a rectangle block in the matrix or an entire channel, while unstructured pruning doesn’t impose such constraints. Consequently, structured pruning (e.g., LLM-Pruner [251]) is more hardware-friendly but more difficult to maintain model accuracy. While traditional pruning approaches require costly retaining process to preserve model capability, recent works like SparseGPT [252] and Wanda [253] have explored to perform unstructured or semi-structured pruning in one-shot.\n\nKnowledge Distillation (KD) involves using a well-performing teacher model (usually with a large number of parameters and high precision) to guide the training of a lightweight student model (usually with fewer parameters and lower precision). Through distillation, the student model is well-aligned to the teacher model with relative smaller training dataset, and has the chance to perform even better on downstream tasks [256]. Based on whether the teacher model’s parameters are required in the training process, distillation methods can be further categorized into white-box (e.g., BabyLlama [254] and MiniLLM [255]) and black-box ones (e.g., Distilling Step-by-Step [256] and SCoTD [257]). Since the student model are often lightweight quantized or pruned model, KD is also adopted in QAT and pruning techniques to enhance the training performance. For example, LLM-QAT [248] proposes a data-free distillation method to preserve the original output distribution in the quantized model.\n\nLow-rank Factorization refers to approximating the original weight matrix by the product of two low-rank matrices, thereby reducing the model’s parameter size and computational load. Specifically, a weight matrix W𝑊Witalic_W of shape m×n𝑚𝑛m\\times nitalic_m × italic_n is factorized into the product of Um×rsuperscript𝑈𝑚𝑟U^{m\\times r}italic_U start_POSTSUPERSCRIPT italic_m × italic_r end_POSTSUPERSCRIPT and Vn×rsuperscript𝑉𝑛𝑟V^{n\\times r}italic_V start_POSTSUPERSCRIPT italic_n × italic_r end_POSTSUPERSCRIPT, such that W≈U⁢VT𝑊𝑈superscript𝑉𝑇W\\approx UV^{T}italic_W ≈ italic_U italic_V start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT and r≪m,nmuch-less-than𝑟𝑚𝑛r\\ll m,nitalic_r ≪ italic_m , italic_n. Low-rank Factorization can be combined with quantization (e.g., ZeroQuant-V2 [258]) and pruning (e.g., LoSparse [259]) methods to enhance the compression ratio. Besides, low-rank adapters effectively reduce the customization overhead of LLMs, which we leave to 5.2.\n\n5.1.2 Inference Acceleration\n\nExcept for making the models more compact as discussed in Section 5.1.3, there are various other techniques to accelerate the LLM inference process.\n\nA major characteristic that sets the LLM apart from the traditional non-Transformer models is the attention mechanism [31]. Since the computational cost of attention increases near quadratically with the context length, it is particularly important to enhance the computational efficiency of long-context inference. Existing works have explored to reduce context length and optimize attention kernels to better support long-context inference. We’ll dive into these techniques separately.\n\nKV Cache is a widely adopted technique in both mobile (e.g., llama.cpp [307] and mlc-llm [308]) and cloud LLM serving frameworks (e.g., DeepSpeed [316] and vLLM [317]), to avoid redundant computation in LLM inference. Specifically, KV Cache involves storing (i.e., “caching”) and incrementally updating the Key-Value (KV) pairs, which are intermediate results in the attention calculation, in each token’s generation. Therefore, the repeated part in the KV computation is avoided to reduce the computational cost. However, in long-context inference, the computational cost of attention is still a system bottleneck despite the skipped KV calculations, making it crucial to compress the context length in such scenarios.\n\nContext Compression methods enhance the inference efficiency by reducing the length of the context, especially the KV cache. Co-quantization of weights and activations, including KV cache, is an intuitive approach to compress the KV cache, which has been discussed in Section 5.1.1. Besides quantization, context pruning removes less important tokens in the context to reduce the computational cost. The effectiveness of this method is based on the observation that tokens have different impacts on the final output, and removing less important tokens won’t cause significant degradation of the model’s capability [263, 318, 264, 265]. A typical line of work is to compress the context at the prefill stage based on different importance of tokens [260, 261, 262]. However, these methods are one-shot and cannot prune the KV cache when the context length continuously grows during token generation. To address this, Dynamic Context Pruning [263] uses a learnable mechanism to continuously determine and drop uninformative tokens. While the learnable mechanism introduces a fine-tuning overhead, Zhang et al. [264], proposes a token eviction strategy that can be applied without fine-tuning.\n\nInspired by the same observation that tokens are not equally important, other works also explored to reduce computations of less important tokens instead of directly removing them. COLT5 [319] employs a conditional computation mechanism, which devotes more resources to important tokens in both FFN and attention. SkipDecode [320] designs a token-level early exit method that works seamlessly with batched inference and KV cache, to skip some operators in the computational graph when a token is less important.\n\nKernel Optimization is another approach towards LLM inference acceleration. Optimization for small-batch or single-batch inference is especially important for edge scenarios including the locally-deployed Personal LLM Agents. Existing works have revealed that the attention calculation becomes a bottleneck when the sequence length is long, since the complexity of attention scales quadratically with the sequence length, while that of the FFN scales linearly. Therefore, efficient attention kernels including FlashAttention [266, 267] and FlashDecoding++ [268] have been proposed to improve the speed of long-text inference. Some works also reduce the computational complexity of attention from the algorithm aspect. For example, Linformer [321] achieves linear complexity for self-attention in the prefill phase. Besides, reducing dequantization overhead also provides significant performance improvement as demonstrated by LUT-GEMM [322].\n\nSpeculative Decoding [270, 269] is an effective approach in small-batch inference to improve the latency. The batch size of LLM inference at the edge is smaller than on the cloud, and is usually 1 (i.e., single query), which makes the inference workload extremely memory-bound. Speculative decoding mitigates this challenge by “guessing” several subsequent tokens through a lightweight “draft model”, and then validating the draft tokens in batches using the large “oracle model”. Miao et al. [323] and Spector and Re [324] further enhance speculative decoding with a tree-based verification instead of sequential ones to reuse intermediate results shared across these sequences. While these methods ensure zero bias in the generated results, BiLD [325] proposes to only fallback or rollback to the oracle model occasionally when the draft model is not capable to generate high quality contents.\n\n5.1.3 Memory Reduction\n\nLLM inference is not only computationally-intensive, but also memory-consuming, which causes challenges in the deployment of Personal LLM Agents. Therefore, it is necessary to perform optimizations on the memory efficiency of LLM inference. KV cache and model weights are two major causes of this memory overhead. In a short-context scenario where the KV storage requires much less memory than the model weights, the model compression techniques in Section 5.1.1 are very effective to reduce the memory requirement to store the weights. However, in the long-context scenario, the KV cache, whose size grows linearly with the context length, will dominate the total memory consumption.\n\nAn effective approach to address this issue is to compress the KV cache using quantization and pruning techniques mentioned in Section 5.1.1 and Section 5.1.2. While the quantization methods are generic to reduce the memory footprint of KV cache, not all the pruning-based methods directly contribute to the memory efficiency. Only those methods that prune the corresponding rows/columns in the KV cache when continuously removing input tokens in the context can prevent the KV cache size from exceeding the memory limit. For example, Anagnostidis et al. [263] and Zhang et al. [264] proposed to identify and evict uninformative tokens during generation. However, the one-sho"
    }
}
{
    "id": "wrong_mix_property_starring_00149_1",
    "rank": 20,
    "data": {
        "url": "https://www.paperdigest.org/2023/10/nips-2023-highlights/",
        "read_more_link": "",
        "language": "en",
        "title": "Paper Digest: NeurIPS 2023 Highlights",
        "top_image": "https://www.paperdigest.org/wp-content/uploads/2023/03/paperdigest-min.jpg",
        "meta_img": "https://www.paperdigest.org/wp-content/uploads/2023/03/paperdigest-min.jpg",
        "images": [
            "https://www.paperdigest.org/wp-content/uploads/2023/03/pd-circle-min.png",
            "https://www.paperdigest.org/wp-content/uploads/2023/03/pd-circle-min.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2023-10-24T00:00:00-04:00",
        "summary": "",
        "meta_description": "The Conference on Neural Information Processing Systems (NeurIPS) is one of the top machine learning conferences. In 2023, it is to be held in New Orleans. To help the community quickly catch up on the work presented in this conference, Paper Digest Team processed all accepted papers, and generated",
        "meta_lang": "en",
        "meta_favicon": "https://www.paperdigest.org/wp-content/uploads/2019/05/pd_circle.ico",
        "meta_site_name": "Paper Digest",
        "canonical_link": "https://www.paperdigest.org/2023/10/nips-2023-highlights/",
        "text": "Paper Author(s) 1 Toolformer: Language Models Can Teach Themselves to Use Tools\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: In this paper, we show that LMs can teach themselves to *use external tools* via simple APIs and achieve the best of both worlds. Timo Schick; Jane Dwivedi-Yu; Roberto Dessi; Roberta Raileanu; Maria Lomeli; Eric Hambro; Luke Zettlemoyer; Nicola Cancedda; Thomas Scialom; 2 The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data Only\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: However, as larger models requiring pretraining on trillions of tokens are considered, it is unclear how scalable is curation, and whether we will run out of unique high-quality data soon. At variance with previous beliefs, we show that properly filtered and deduplicated web data alone can lead to powerful models; even significantly outperforming models trained on The Pile. Guilherme Penedo; Quentin Malartic; Daniel Hesslow; Ruxandra Cojocaru; Hamza Alobeidli; Alessandro Cappelli; Baptiste Pannier; Ebtesam Almazrouei; Julien Launay; 3 InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: In this paper, we conduct a systematic and comprehensive study on vision-language instruction tuning based on the pretrained BLIP-2 models. Wenliang Dai; Junnan Li; DONGXU LI; Anthony Meng Huat Tiong; Junqi Zhao; Weisheng Wang; Boyang Li; Pascale N Fung; Steven Hoi; 4 QLoRA: Efficient Finetuning of Quantized LLMs\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. Tim Dettmers; Artidoro Pagnoni; Ari Holtzman; Luke Zettlemoyer; 5 Direct Preference Optimization: Your Language Model Is Secretly A Reward Model\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper, we leverage a mapping between reward functions and optimal policies to show that this constrained reward maximization problem can be optimized exactly with a single stage of policy training, essentially solving a classification problem on the human preference data. Rafael Rafailov; Archit Sharma; Eric Mitchell; Christopher D Manning; Stefano Ermon; Chelsea Finn; 6 Reflexion: Language Agents with Verbal Reinforcement Learning\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: We propose Reflexion, a novel framework to reinforce language agents not by updating weights, but instead through linguistic feedback. Noah Shinn; Federico Cassano; Ashwin Gopinath; Karthik Narasimhan; Shunyu Yao; 7 LIMA: Less Is More for Alignment\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling Chunting Zhou; Pengfei Liu; Puxin Xu; Srinivasan Iyer; Jiao Sun; Yuning Mao; Xuezhe Ma; Avia Efrat; Ping Yu; LILI YU; Susan Zhang; Gargi Ghosh; Mike Lewis; Luke Zettlemoyer; Omer Levy; 8 Self-Refine: Iterative Refinement with Self-Feedback\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. Aman Madaan; Niket Tandon; Prakhar Gupta; Skyler Hallinan; Luyu Gao; Sarah Wiegreffe; Uri Alon; Nouha Dziri; Shrimai Prabhumoye; Yiming Yang; Shashank Gupta; Bodhisattwa Prasad Majumder; Katherine Hermann; Sean Welleck; Amir Yazdanbakhsh; Peter Clark; 9 Vicuna Evaluation: Exploring LLM-as-a-Judge and Chatbot Arena\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. Lianmin Zheng; Wei-Lin Chiang; Ying Sheng; Siyuan Zhuang; Zhanghao Wu; Yonghao Zhuang; Zi Lin; Zhuohan Li; Dacheng Li; Eric Xing; Hao Zhang; Joseph Gonzalez; Ion Stoica; 10 Language Is Not All You Need: Aligning Perception with Language Models\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: In this work, we introduce KOSMOS-1, a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context (i.e., few-shot), and follow instructions (i.e., zero-shot). Shaohan Huang; Li Dong; Wenhui Wang; Yaru Hao; Saksham Singhal; Shuming Ma; Tengchao Lv; Lei Cui; Owais Khan Mohammed; Barun Patra; Qiang Liu; Kriti Aggarwal; Zewen Chi; Nils Bjorck; Vishrav Chaudhary; Subhojit Som; XIA SONG; Furu Wei; 11 Pick-a-Pic: An Open Dataset of User Preferences for Text-to-Image Generation\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: The ability to collect a large dataset of human preferences from text-to-image users is usually limited to companies, making such datasets inaccessible to the public. To address this issue, we create a web app that enables text-to-image users to generate images and specify their preferences. Yuval Kirstain; Adam Polyak; Uriel Singer; Shahbuland Matiana; Joe Penna; Omer Levy; 12 Mathematical Capabilities of ChatGPT\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: We investigate the mathematical capabilities of two iterations of ChatGPT (released 9-January-2023 and 30-January-2023) and of GPT-4 by testing them on publicly available datasets, as well as hand-crafted ones, using a novel methodology. Simon Frieder; Luca Pinchetti; Chevalier; Ryan-Rhys Griffiths; Tommaso Salvatori; Thomas Lukasiewicz; Philipp Petersen; Julius Berner; 13 Segment Everything Everywhere All at Once\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: In this work, we present SEEM, a promotable and interactive model for segmenting everything everywhere all at once in an image. Xueyan Zou; Jianwei Yang; Hao Zhang; Feng Li; Linjie Li; Jianfeng Wang; Lijuan Wang; Jianfeng Gao; Yong Jae Lee; 14 Language Models Don’t Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: We demonstrate that CoT explanations can be heavily influenced by adding biasing features to model inputs�e.g., by reordering the multiple-choice options in a few-shot prompt to make the answer always (A)�which models systematically fail to mention in their explanations. Miles Turpin; Julian Michael; Ethan Perez; Samuel Bowman; 15 AlpacaFarm: A Simulation Framework for Methods That Learn from Human Feedback\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: Replicating and understanding this instruction-following process faces three major challenges: the high cost of data collection, the lack of trustworthy evaluation, and the absence of reference method implementations. We address these bottlenecks with AlpacaFarm, a simulator that enables research and development for learning from feedback at a low cost. Yann Dubois; Xuechen Li; Rohan Taori; Tianyi Zhang; Ishaan Gulrajani; Jimmy Ba; Carlos Guestrin; Percy Liang; Tatsunori Hashimoto; 16 Fine-Grained Human Feedback Gives Better Rewards for Language Model Training\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: In this paper, we use fine-grained human feedback (e.g., which sentence is false, which sub-sentence is irrelevant) as an explicit training signal. Zeqiu Wu; Yushi Hu; Weijia Shi; Nouha Dziri; Alane Suhr; Prithviraj (Raj) Ammanabrolu; Noah Smith; Mari Ostendorf; Hannaneh Hajishirzi; 17 ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: We present a comprehensive solution to learn and improve text-to-image models from human preference feedback. Jiazheng Xu; Xiao Liu; Yuchen Wu; Yuxuan Tong; Qinkai Li; Ming Ding; Jie Tang; Yuxiao Dong; 18 Visual Instruction Tuning\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: We present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and an LLM for general-purpose visual and language understanding. Haotian Liu; Chunyuan Li; Qingyang Wu; Yong Jae Lee; 19 Perfect Linear Concept Erasure in Closed Form\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: We prove that a rank $k – 1$ orthogonal projection is sufficient to perfectly guard a $k$-class concept from all linear adversaries with convex loss functions, and provide the formula in closed form. Nora Belrose; David Schneider-Joseph; Shauli Ravfogel; Ryan Cotterell; Edward Raff; Stella Biderman; 20 Scaling Data-Constrained Language Models\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Niklas Muennighoff; Alexander Rush; Boaz Barak; Teven Le Scao; Nouamane Tazi; Aleksandra Piktus; Thomas Wolf; Colin Raffel; Sampo Pyysalo; 21 Faith and Fate: Limits of Transformers on Compositionality\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: As a measure of compositional complexity, we introduce computation graphs to systematically quantify the level of complexity, and break down reasoning steps into intermediate sub-procedures. Nouha Dziri; Ximing Lu; Melanie Sclar; Xiang (Lorraine) Li; Liwei Jiang; Bill Yuchen Lin; Sean Welleck; Peter West; Chandra Bhagavatula; Ronan Le Bras; Jena Hwang; Soumya Sanyal; Xiang Ren; Allyson Ettinger; Zaid Harchaoui; Yejin Choi; 22 StableRep: Synthetic Images from Text-to-Image Models Make Strong Visual Representation Learners\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: We show that (1) when the generative model is properly configured, training self-supervised methods on synthetic images can match or beat the real image counterpart;(2) by treating the multiple images generated from the same text prompt as positives for each other, we develop a multi-positive contrastive learning method, which we call StableRep. Yonglong Tian; Lijie Fan; Phillip Isola; Huiwen Chang; Dilip Krishnan; 23 Dissecting Knowledge Distillation: An Exploration of Its Inner Workings and Applications\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: Does its data invariance properties become similar? Our work presents a comprehensive study to try to answer these questions. Utkarsh Ojha; Yuheng Li; Anirudh Sundara Rajan; Yingyu Liang; Yong Jae Lee; 24 Data Selection for Language Models Via Importance Resampling\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: Instead, we extend the classic importance resampling approach used in low-dimensions for LM data selection. We propose Data Selection with Importance Resampling (DSIR), an efficient and scalable framework that estimates importance weights in a reduced feature space for tractability and selects data with importance resampling according to these weights. Sang Michael Xie; Shibani Santurkar; Tengyu Ma; Percy Liang; 25 Visual Instruction Inversion: Image Editing Via Image Prompting\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: We present a method for image editing via visual prompting. Thao Nguyen; Yuheng Li; Utkarsh Ojha; Yong Jae Lee; 26 SceneScape: Text-Driven Consistent Scene Generation\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: We present a method for text-driven perpetual view generation — synthesizing long-term videos of various scenes solely, given an input text prompt describing the scene and camera poses. Rafail Fridman; Amit Abecasis; Yoni Kasten; Tali Dekel; 27 Tree of Thoughts: Deliberate Problem Solving with Large Language Models\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. Shunyu Yao; Dian Yu; Jeffrey Zhao; Izhak Shafran; Tom Griffiths; Yuan Cao; Karthik Narasimhan; 28 Paraphrasing Evades Detectors of AI-generated Text, But Retrieval Is An Effective Defense\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: To increase the robustness of AI-generated text detection to paraphrase attacks, we introduce a simple defense that relies on retrieving semantically-similar generations and must be maintained by a language model API provider. Kalpesh Krishna; Yixiao Song; Marzena Karpinska; John Wieting; Mohit Iyyer; 29 DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: In this paper, we propose Domain Reweighting with Minimax Optimization (DoReMi), which first trains a small proxy model using group distributionally robust optimization (Group DRO) over domains to produce domain weights (mixture proportions) without knowledge of downstream tasks. We then resample a dataset with these domain weights and train a larger, full-sized model. Sang Michael Xie; Hieu Pham; Xuanyi Dong; Nan Du; Hanxiao Liu; Yifeng Lu; Percy Liang; Quoc V Le; Tengyu Ma; Adams Wei Yu; 30 Scalable 3D Captioning with Pretrained Models\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: We introduce Cap3D, an automatic approach for generating descriptive text for 3D objects. Tiange Luo; Chris Rockwell; Honglak Lee; Justin Johnson; 31 Emergent and Predictable Memorization in Large Language Models\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: The prevalence of such undesirable memorization can pose issues for model trainers, and may even require discarding an otherwise functional model. We therefore seek to predict which sequences will be memorized before a large model’s full train-time by extrapolating the memorization behavior of lower-compute trial runs. Stella Biderman; USVSN PRASHANTH; Lintang Sutawika; Hailey Schoelkopf; Quentin Anthony; Shivanshu Purohit; Edward Raff; 32 HuggingGPT: Solving AI Tasks with ChatGPT and Its Friends in Hugging Face\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: Considering large language models (LLMs) have exhibited exceptional ability in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks and language could be a generic interface to empower this. Based on this philosophy, we present HuggingGPT, a framework that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Yongliang Shen; Kaitao Song; Xu Tan; Dongsheng Li; Weiming Lu; Yueting Zhuang; 33 Self-Supervised Learning with Lie Symmetries for Partial Differential Equations\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: In this work, we learn general-purpose representations of PDEs from heterogeneous data by implementing joint embedding methods for self-supervised learning (SSL), a framework for unsupervised representation learning that has had notable success in computer vision. Grégoire Mialon; Quentin Garrido; Hannah Lawrence; Danyal Rehman; Bobak Kiani; Yann LeCun; 34 OpenProteinSet: Training Data for Structural Biology at Scale\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: Generation of MSAs is highly computationally intensive, however, and no datasets comparable to those used to train AlphaFold2 have been made available to the research community, hindering progress in machine learning for proteins. To remedy this problem, we introduce OpenProteinSet, an open-source corpus of more than 16 million MSAs, associated structural homologs from the Protein Data Bank, and AlphaFold2 protein structure predictions. Gustaf Ahdritz; Nazim Bouatta; Sachin Kadyan; Lukas Jarosch; Dan Berenberg; Ian Fisk; Andrew Watkins; Stephen Ra; Richard Bonneau; Mohammed AlQuraishi; 35 Towards Automated Circuit Discovery for Mechanistic Interpretability\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: This work proposes a novel algorithm, Automatic Circuit DisCovery (ACDC), to automate the identification of the important units in the network. Arthur Conmy; Augustine Mavor-Parker; Aengus Lynch; Stefan Heimersheim; Adrià Garriga-Alonso; 36 Does Localization Inform Editing? Surprising Differences in Causality-Based Localization Vs. Knowledge Editing in Language Models\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: In this paper, we find that we can change how a fact is stored in a model by editing weights that are in a different location than where existing methods suggest that the fact is stored. Peter Hase; Mohit Bansal; Been Kim; Asma Ghandeharioun; 37 Diffusion Self-Guidance for Controllable Image Generation\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: We introduce self-guidance, a method that provides precise control over properties of the generated image by guiding the internal representations of diffusion models. Dave Epstein; Allan Jabri; Ben Poole; Alexei Efros; Aleksander Holynski; 38 ToolkenGPT: Augmenting Frozen Language Models with Massive Tools Via Tool Embeddings\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: Although the latter method offers adaptability to new tools, it struggles with the inherent context length constraint of LLMs when many new tools are presented, and mastering a new set of tools with few-shot examples remains challenging, resulting in suboptimal performance. To address these limitations, we propose a novel solution, named **ToolkenGPT**, wherein LLMs effectively learn to master tools as predicting tokens through **tool embeddings** for solving complex tasks. Shibo Hao; Tianyang Liu; Zhen Wang; Zhiting Hu; 39 Inference-Time Intervention: Eliciting Truthful Answers from A Language Model\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: We introduce Inference-Time Intervention (ITI), a technique designed to enhance the truthfulness of large language models (LLMs). Kenneth Li; Oam Patel; Fernanda Viégas; Hanspeter Pfister; Martin Wattenberg; 40 Objaverse-XL: A Colossal Universe of 3D Objects\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: In this work, we present Objaverse-XL, a dataset of over 10 million 3D objects. Matt Deitke; Ruoshi Liu; Matthew Wallingford; Huong Ngo; Oscar Michel; Aditya Kusupati; Alan Fan; Christian Laforte; Vikram Voleti; Samir Yitzhak Gadre; Eli VanderBilt; Aniruddha Kembhavi; Carl Vondrick; Georgia Gkioxari; Kiana Ehsani; Ludwig Schmidt; Ali Farhadi; 41 Stable Bias: Evaluating Societal Representations in Diffusion Models\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: This evaluation, however, is made more difficult by the synthetic nature of these systems’ outputs: common definitions of diversity are grounded in social categories of people living in the world, whereas the artificial depictions of fictive humans created by these systems have no inherent gender or ethnicity. To address this need, we propose a new method for exploring the social biases in TTI systems. Sasha Alexandra Luccioni; Christopher Akiki; Margaret Mitchell; Yacine Jernite; 42 MVDiffusion: Enabling Holistic Multi-view Image Generation with Correspondence-Aware Diffusion\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: This paper introduces MVDiffusion, a simple yet effective multi-view image generation method for scenarios where pixel-to-pixel correspondences are available, such as perspective crops from panorama or multi-view images given depth/pose. Shitao Tang; Fuyang Zhang; Jiacheng Chen; Peng Wang; Yasutaka Furukawa; 43 Language Models Can Solve Computer Tasks\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: However, previous approaches to this problem require large amounts of expert demonstrations and task-specific reward functions, both of which are impractical for new tasks. In this work, we show that a pre-trained large language model (LLM) agent can execute computer tasks guided by natural language using a simple prompting scheme where the agent recursively criticizes and improves its output (RCI). Geunwoo Kim; Pierre Baldi; Stephen McAleer; 44 Learning Universal Policies Via Text-Guided Video Generation\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: Recent progress in text-guided image synthesis has yielded models with an impressive ability to generate complex novel images, exhibiting combinatorial generalization across domains. Motivated by this success, we investigate whether such tools can be used to construct more general-purpose agents. Yilun Du; Mengjiao (Sherry) Yang; Bo Dai; Hanjun Dai; Ofir Nachum; Josh Tenenbaum; Dale Schuurmans; Pieter Abbeel; 45 Holistic Evaluation of Text-to-Image Models\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: However, existing evaluations primarily focus on image-text alignment and quality. To address this limitation, we introduce a new benchmark, Holistic Evaluation of Text-to-Image Models (HEIM). Tony Lee; Michihiro Yasunaga; Chenlin Meng; Yifan Mai; Joon Sung Park; Agrim Gupta; Yunzhi Zhang; Deepak Narayanan; Hannah Teufel; Marco Bellagente; Minguk Kang; Taesung Park; Jure Leskovec; Jun-Yan Zhu; Fei-Fei Li; Jiajun Wu; Stefano Ermon; Percy Liang; 46 Generating Images with Multimodal Language Models\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Jing Yu Koh; Daniel Fried; Russ Salakhutdinov; 47 Simple and Controllable Music Generation\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: We introduce MusicGen, a single Language Model (LM) that operates over several streams of compressed discrete music representation, i.e., tokens. Jade Copet; Felix Kreuk; Itai Gat; Tal Remez; Gabriel Synnaeve; Yossi Adi; Alexandre Defossez; 48 Structural Pruning for Diffusion Models\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: The impressive capability of these models, however, often entails significant computational overhead during both training and inference. To tackle this challenge, we present Diff-Pruning, an efficient compression method tailored for learning lightweight diffusion models from pre-existing ones, without the need for extensive re-training. Gongfan Fang; Xinyin Ma; Xinchao Wang; 49 Where Are We in The Search for An Artificial Visual Cortex for Embodied Intelligence?\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: We present the largest and most comprehensive empirical study of pre-trained visual representations (PVRs) or visual ‘foundation models’ for Embodied AI. Arjun Majumdar; Karmesh Yadav; Sergio Arnaud; Jason Yecheng Ma; Claire Chen; Sneha Silwal; Aryan Jain; Vincent-Pierre Berges; Tingfan Wu; Jay Vakil; Pieter Abbeel; Jitendra Malik; Dhruv Batra; Yixin Lin; Oleksandr Maksymets; Aravind Rajeswaran; Franziska Meier; 50 Repetition In Repetition Out: Towards Understanding Neural Text Degeneration from The Data Perspective\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: In this work, we aim to advance our understanding by presenting a straightforward and unified explanation from the data perspective. Huayang Li; Tian Lan; Zihao Fu; Deng Cai; Lemao Liu; Nigel Collier; Taro Watanabe; Yixuan Su; 51 Are Aligned Neural Networks Adversarially Aligned?\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: They respond helpfully to user questions, but when asked to perform some behavior that would cause harm, will politely refuse. We study to what extent these models are aligned even when interacting with an adversarial user who constructs worst-case adversarial example inputs. Nicholas Carlini; Florian Tramer; Daphne Ippolito; Ludwig Schmidt; Milad Nasr; Matthew Jagielski; Pang Wei Koh; Irena Gao; Christopher A. Choquette-Choo; 52 Patch N’ Pack: NaViT, A Vision Transformer for Any Aspect Ratio and Resolution\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: However, models such as the Vision Transformer (ViT) offer flexible sequence-based modeling, and hence varying input sequence lengths. We take advantage of this with NaViT (Native Resolution ViT) which uses sequence packing during training to process inputs of arbitrary resolutions and aspect ratios. Mostafa Dehghani; Basil Mustafa; Josip Djolonga; Jonathan Heek; Matthias Minderer; Mathilde Caron; Andreas Steiner; Joan Puigcerver; Robert Geirhos; Ibrahim Alabdulmohsin; Avital Oliver; Piotr Padlewski; Alexey Gritsenko; Mario Lucic; Neil Houlsby; 53 Counterfactual Memorization in Neural Language Models\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: We formulate a notion of counterfactual memorization which characterizes how a model’s predictions change if a particular document is omitted during training. Chiyuan Zhang; Daphne Ippolito; Katherine Lee; Matthew Jagielski; Florian Tramer; Nicholas Carlini; 54 Ecosystem-level Analysis of Deployed Machine Learning Reveals Homogeneous Outcomes\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: In practice, the societal impact of machine learning is determined by the surrounding context of machine learning deployments. To capture this, we introduce *ecosystem-level analysis*: rather than analyzing a single model, we consider the collection of models that are deployed in a given context. Connor Toups; Rishi Bommasani; Kathleen Creel; Sarah Bana; Dan Jurafsky; Percy Liang; 55 Characterizing Graph Datasets for Node Classification: Homophily-Heterophily Dichotomy and Beyond\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: In this work, we show that commonly used homophily measures have critical drawbacks preventing the comparison of homophily levels across different datasets. Oleg Platonov; Denis Kuznedelev; Artem Babenko; Liudmila Prokhorenkova; 56 Cheaply Evaluating Inference Efficiency Metrics for Autoregressive Transformer APIs\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: Unfortunately, access to LLMs today is largely restricted to black-box text generation APIs; raw runtimes measured through this interface do not satisfy these desiderata: model providers can implement software and hardware optimizations orthogonal to the model, and shared infrastructure introduces performance contention. We propose a new metric for inference efficiency that puts models on equal footing as though they were served on uniform hardware and software and without performance contention. Deepak Narayanan; Keshav Santhanam; Peter Henderson; Rishi Bommasani; Tony Lee; Percy Liang; 57 Lexinvariant Language Models\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: First, we prove that we can construct a lexinvariant LM to converge to the true language model at a uniform rate that is polynomial in terms of the context length, with a constant factor that is sublinear in the vocabulary size. Second, to build a lexinvariant LM, we simply encode tokens using random Gaussian vectors, such that each token maps to the same representation within each sequence but different representations across sequences. Qian Huang; Eric Zelikman; Sarah Chen; Yuhuai Wu; Gregory Valiant; Percy Liang; 58 Distributed Inference and Fine-tuning of Large Language Models Over The Internet\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: In this work, we investigate methods for cost-efficient inference and fine-tuning of LLMs, comparing local and distributed strategies. Alexander Borzunov; Dmitry Baranchuk; Tim Dettmers; Max Ryabinin; Younes Belkada; Artem Chumachenko; Pavel Samygin; Colin Raffel; 59 Red Teaming Deep Neural Networks with Feature Synthesis Tools\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: Our key insight is that we can train models that respond to specific triggers (e.g., a specific patch inserted into an image) with specific outputs (i.e. a label) and then evaluate interpretability tools based on whether they help humans identify these triggers. Stephen Casper; Tong Bu; Yuxiao Li; Jiawei Li; Kevin Zhang; Kaivalya Hariharan; Dylan Hadfield-Menell; 60 InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: While LLMs have recently exhibited promising coding capabilities, current coding benchmarks mostly consider a static instruction-to-code sequence transduction process, which has the potential for error propagation and a disconnect between the generated code and its final execution environment. To address this gap, we introduce InterCode, a lightweight, flexible, and easy-to-use framework for constructing interactive code environments with multiple types of feedback signals. John Yang; Akshara Prabhakar; Karthik Narasimhan; Shunyu Yao; 61 LLaVA-Med: Training A Large Language-and-Vision Assistant for Biomedicine in One Day\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: In this paper, we propose a cost-efficient approach for training a vision-language conversational assistant that can answer open-ended research questions of biomedical images. Chunyuan Li; Cliff Wong; Sheng Zhang; Naoto Usuyama; Haotian Liu; Jianwei Yang; Tristan Naumann; Hoifung Poon; Jianfeng Gao; 62 Statistical Knowledge Assessment for Generative Language Models\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: Given varying prompts, does a GLM consistently generate factually correct answers? In this paper, we introduce a statistical knowledge assessment framework guided by latent variables and the KaRR metric, which quantifies a model’s knowledge by computing its continuous probability across diverse text forms. Qingxiu Dong; Jingjing Xu; Lingpeng Kong; Zhifang Sui; Lei Li; 63 Jailbroken: How Does LLM Safety Training Fail?\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: Large language models trained for safety and harmlessness remain susceptible to adversarial misuse, as evidenced by the prevalence of �jailbreak� attacks on early releases of ChatGPT that elicit undesired behavior. Going beyond recognition of the issue, we investigate why such attacks succeed and how they can be created. Alexander Wei; Nika Haghtalab; Jacob Steinhardt; 64 Likelihood-Based Diffusion Language Models\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: In this work, we take the first steps towards closing the perplexity gap between autoregressive and diffusion-based language models, with the goal of building and releasing a diffusion model which outperforms the smallest widely-adopted autoregressive model (GPT-2 124M). Ishaan Gulrajani; Tatsunori Hashimoto; 65 OpenMask3D: Open-Vocabulary 3D Instance Segmentation\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: While such a representation can be directly employed to perform semantic segmentation, existing methods have limitations in their ability to handle object instances. In this work, we address this limitation, and propose OpenMask3D, which is a zero-shot approach for open-vocabulary 3D instance segmentation. Ayca Takmaz; Elisabetta Fedele; Robert Sumner; Marc Pollefeys; Federico Tombari; Francis Engelmann; 66 VisIT-Bench: A Dynamic Benchmark for Evaluating Instruction-Following Vision-and-Language Models\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: We introduce VisIT-Bench, a robust benchmark for diverse real-life vision-language instructions across 70 tasks, from recognition to reasoning. Yonatan Bitton; Hritik Bansal; Jack Hessel; Rulin Shao; Wanrong Zhu; Anas Awadalla; Josh Gardner; Rohan Taori; Ludwig Schmidt; 67 How Far Can Camels Go? Exploring The State of Instruction Tuning on Open Resources\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: In this work we explore recent advances in instruction-tuning language models on a range of open instruction-following datasets. Yizhong Wang; Hamish Ivison; Pradeep Dasigi; Jack Hessel; Tushar Khot; Khyathi Chandu; David Wadden; Kelsey MacMillan; Noah Smith; Iz Beltagy; Hannaneh Hajishirzi; 68 Why Diffusion Models Memorize and How to Mitigate Copying\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: Images generated by diffusion models like Stable Diffusion are increasingly widespread. Recent works and even lawsuits have shown that these models are prone to replicating their training data, unbeknownst to the user. In this paper, we first analyze this memorization problem in text-to-image diffusion models. Gowthami Somepalli; Vasu Singla; Micah Goldblum; Jonas Geiping; Tom Goldstein; 69 3D-LLM: Injecting The 3D World Into Large Language Models\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: In this work, we propose to inject the 3D world into large language models, and introduce a whole new family of 3D-LLMs. Yining Hong; Haoyu Zhen; Peihao Chen; Shuhong Zheng; Yilun Du; Zhenfang Chen; Chuang Gan; 70 Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: In this paper, we devise an approach for learning an effective initialization from offline data that also enables fast online fine-tuning capabilities. Mitsuhiko Nakamoto; Yuexiang Zhai; Anikait Singh; Max Sobol Mark; Yi Ma; Chelsea Finn; Aviral Kumar; Sergey Levine; 71 Emergent Correspondence from Image Diffusion\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: In this paper, we find that correspondence emerges in diffusion models without any explicit supervision. Luming Tang; Menglin Jia; Qianqian Wang; Cheng Perng Phoo; Bharath Hariharan; 72 Fine-Tuning Language Models with Just Forward Passes\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: In this work, we propose a memory-efficient zeroth-order optimizer (MeZO), adapting the classical ZO-SGD method to operate in-place, thereby fine-tuning LMs with the same memory footprint as inference. Sadhika Malladi; Tianyu Gao; Eshaan Nichani; Alex Damian; Jason Lee; Danqi Chen; Sanjeev Arora; 73 LayoutGPT: Compositional Visual Planning and Generation with Large Language Models\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: We propose LayoutGPT, a method to compose in-context visual demonstrations in style sheet language to enhance visual planning skills of LLMs. Weixi Feng; Wanrong Zhu; Tsu-Jui Fu; Varun Jampani; Arjun Akula; Xuehai He; S Basu; Xin Eric Wang; William Yang Wang; 74 What Makes Good Examples for Visual In-Context Learning?\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: To demystify in-context learning in computer vision, we conduct an extensive research and identify a critical problem: downstream performance is highly sensitivie to the choice of visual in-context examples. To address this problem, we propose a prompt retrieval framework specifically for large vision models, allowing the selection of in-context examples to be fully automated. Yuanhan Zhang; Kaiyang Zhou; Ziwei Liu; 75 Stable and Low-precision Training for Large-scale Vision-language Models\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: We introduce new methods for 1) accelerating and 2) stabilizing training for large language-vision models. Mitchell Wortsman; Tim Dettmers; Luke Zettlemoyer; Ari Morcos; Ali Farhadi; Ludwig Schmidt; 76 Battle of The Backbones: A Large-Scale Comparison of Pretrained Models Across Computer Vision Tasks\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: Battle of the Backbones (BoB) makes this choice easier by benchmarking a diverse suite of pretrained models, including vision-language models, those trained via self-supervised learning, and the Stable Diffusion backbone, across a diverse set of computer vision tasks ranging from classification to object detection to OOD generalization and more. Micah Goldblum; Hossein Souri; Renkun Ni; Manli Shu; Viraj Prabhu; Gowthami Somepalli; Prithvijit Chattopadhyay; Adrien Bardes; Mark Ibrahim; Judy Hoffman; Rama Chellappa; Andrew Wilson; Tom Goldstein; 77 LLM-Pruner: On The Structural Pruning of Large Language Models\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM. Xinyin Ma; Gongfan Fang; Xinchao Wang; 78 RAPHAEL: Text-to-Image Generation Via Large Mixture of Diffusion Paths\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: We introduce a text-conditional image diffusion model, termed RAPHAEL, to generate highly artistic images, which accurately portray the text prompts, encompassing multiple nouns, adjectives, and verbs. Zeyue Xue; Guanglu Song; Qiushan Guo; Boxiao Liu; Zhuofan Zong; Yu Liu; Ping Luo; 79 Guide Your Agent with Adaptive Multimodal Rewards\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: In this paper, we instead propose to utilize the knowledge captured within large vision-language models for improving the generalization capability of control agents. Changyeon Kim; Younggyo Seo; Hao Liu; Lisa Lee; Jinwoo Shin; Honglak Lee; Kimin Lee; 80 Does Progress on ImageNet Transfer to Real-world Datasets?\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: In particular, we study datasets collected with the goal of solving real-world tasks (e.g., classifying images from camera traps or satellites), as opposed to web-scraped benchmarks collected for comparing models. Alex Fang; Simon Kornblith; Ludwig Schmidt; 81 The Impact of Positional Encoding on Length Generalization in Transformers\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: In this paper, we conduct a systematic empirical study comparing the length generalization performance of decoder-only Transformers with five different position encoding approaches including Absolute Position Embedding (APE), T5’s Relative PE, ALiBi, and Rotary, in addition to Transformers without positional encoding (NoPE). Amirhossein Kazemnejad; Inkit Padhi; Karthikeyan Natesan Ramamurthy; Payel Das; Siva Reddy; 82 TextDiffuser: Diffusion Models As Text Painters\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: Diffusion models have gained increasing attention for their impressive generation abilities but currently struggle with rendering accurate and coherent text. To address this issue, we introduce TextDiffuser, focusing on generating images with visually appealing text that is coherent with backgrounds. Jingye Chen; Yupan Huang; Tengchao Lv; Lei Cui; Qifeng Chen; Furu Wei; 83 Paxion: Patching Action Knowledge in Video-Language Foundation Models\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: Despite recent video-language models’ (VidLM) impressive performance on various benchmark tasks, our diagnostic tasks reveal their surprising deficiency (near-random performance) in action knowledge, suggesting that current models rely on object recognition abilities as a shortcut for action understanding. To remedy this, we propose a novel framework, **Paxion**, along with a new **Discriminative Video Dynamics Modeling (DVDM)** objective. Zhenhailong Wang; Ansel Blume; Sha Li; Genglin Liu; Jaemin Cho; Zineng Tang; Mohit Bansal; Heng Ji; 84 C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: We present C-Eval, the first comprehensive Chinese evaluation suite designed to assess advanced knowledge and reasoning abilities of foundation models in a Chinese context. Yuzhen Huang; Yuzhuo Bai; Zhihao Zhu; Junlei Zhang; Jinghan Zhang; Tangjun Su; Junteng Liu; Chuancheng Lv; Yikai Zhang; jiayi lei; Yao Fu; Maosong Sun; Junxian He; 85 Neural Functional Transformers\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: Nevertheless, constructing expressive and efficient neural functional architectures that can handle high-dimensional weight-space objects remains challenging. This paper uses the attention mechanism to define a novel set of permutation equivariant weight-space layers and composes them into deep equivariant models called neural functional Transformers (NFTs). Allan Zhou; Kaien Yang; Yiding Jiang; Kaylee Burns; Winnie Xu; Samuel Sokota; J. Zico Kolter; Chelsea Finn; 86 Permutation Equivariant Neural Functionals\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: We approach the design of neural functionals through the lens of symmetry, in particular by focusing on the permutation symmetries that arise in the weights of deep feedforward networks because hidden layer neurons have no inherent order. We introduce a framework for building *permutation equivariant* neural functionals, whose architectures encode these symmetries as an inductive bias. Allan Zhou; Kaien Yang; Kaylee Burns; Adriano Cardace; Yiding Jiang; Samuel Sokota; J. Zico Kolter; Chelsea Finn; 87 Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: However, this dependence can significantly constrain the true potential of AI-assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self-consistency, and undesirable biases. To address these challenges, we propose a novel approach called SELF-ALIGN, which combines principle-driven reasoning and the generative power of LLMs for the self-alignment of AI agents with minimal human supervision. Zhiqing Sun; Yikang Shen; Qinhong Zhou; Hongxin Zhang; Zhenfang Chen; David Cox; Yiming Yang; Chuang Gan; 88 Imagine That! Abstract-to-Intricate Text-to-Image Synthesis with Scene Graph Hallucination Diffusion\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: In this work, we investigate the task of text-to-image (T2I) synthesis under the abstract-to-intricate setting, i.e., generating intricate visual content from simple abstract text prompts. Shengqiong Wu; Hao Fei; Hanwang Zhang; Tat-Seng Chua; 89 Focused Transformer: Contrastive Training for Context Scaling\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: We pinpoint a key challenge, referred to as the distraction issue, where keys associated with distinct semantic values may overlap, making them challenging to differentiate. To address this issue, we propose the Focused Transformer (FoT), a method that utilizes a training process inspired by contrastive learning. Szymon Tworkowski; Konrad Staniszewski; Mikołaj Pacek; Yuhuai Wu; Henryk Michalewski; Piotr Miłoś; 90 Language Models Augmented with Decoupled Memory\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: Existing large language models (LLMs) can only afford fix-sized inputs due to the input length limit, preventing them from utilizing rich long-context information from past inputs. To address this, we propose a framework, Decoupled-Memory-Augmented LLMs (DeMA), which enables LLMs to memorize long history. Weizhi Wang; Li Dong; Hao Cheng; Xiaodong Liu; Xifeng Yan; Jianfeng Gao; Furu Wei; 91 Extensible Prompts for Language Models on Zero-shot Language Style Customization\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: We propose eXtensible Prompt (X-Prompt) for prompting a large language model (LLM) beyond natural language (NL). Tao Ge; Hu Jing; Li Dong; Shaoguang Mao; Yan Xia; Xun Wang; Si-Qing Chen; Furu Wei; 92 PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning About Change\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: There is a strong need for systematic and extensible planning benchmarks with sufficient diversity to evaluate whether LLMs have innate planning capabilities. Motivated by this, we propose PlanBench, an extensible benchmark suite based on the kinds of domains used in the automated planning community, especially in the International Planning Competition, to test the capabilities of LLMs in planning or reasoning about actions and change. Karthik Valmeekam; Matthew Marquez; Alberto Olmo; Sarath Sreedharan; Subbarao Kambhampati; 93 On The Planning Abilities of Large Language Models – A Critical Investigation\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: Intrigued by the claims of emergent reasoning capabilities in LLMs trained on general web corpora, in this paper, we set out to investigate their planning capabilities. Karthik Valmeekam; Matthew Marquez; Sarath Sreedharan; Subbarao Kambhampati; 94 Scaling in Depth: Unlocking Robustness Certification on ImageNet\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: This paper investigates strategies for expanding certifiably robust training to larger, deeper models. Kai Hu; Andy Zou; Zifan Wang; Klas Leino; Matt Fredrikson; 95 Grounding Neural Inference with Satisfiability Modulo Theories\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: In this paper we present a set of techniques for integrating Satisfiability Modulo Theories (SMT) solvers into the forward and backward passes of a deep network layer, called SMTLayer. Matt Fredrikson; Kaiji Lu; Somesh Jha; Saranya Vijayakumar; Vijay Ganesh; Zifan Wang; 96 Benchmarking Distribution Shift in Tabular Data with TableShift\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: As a consequence, the robustness of tabular models to distribution shift is poorly understood. To address this issue, we introduce TableShift, a distribution shift benchmark for tabular data. Josh Gardner; Zoran Popovic; Ludwig Schmidt; 97 Improving Multimodal Datasets with Image Captioning\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: Our work focuses on caption quality as one major source of noise, and studies the effectiveness of generated captions in increasing the utility of web-scraped datapoints with nondescript text. Thao Nguyen; Samir Yitzhak Gadre; Gabriel Ilharco; Sewoong Oh; Ludwig Schmidt; 98 Improving CLIP Training with Language Rewrites\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: In this paper, we introduce Language augmented CLIP (LaCLIP), a simple yet highly effective approach to enhance CLIP training through language rewrites. Lijie Fan; Dilip Krishnan; Phillip Isola; Dina Katabi; Yonglong Tian; 99 Embroid: Unsupervised Prediction Smoothing Can Improve Few-Shot Classification\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: We propose Embroid, a method which computes multiple representations of a dataset under different embedding functions, and uses the consistency between the LM predictions for neighboring samples to identify mispredictions. Neel Guha; Mayee Chen; Kush Bhatia; Azalia Mirhoseini; Frederic Sala; Christopher Ré; 100 OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: We introduce the OBELICS dataset, an open web-scale filtered dataset of interleaved image-text documents comprising 141 million web pages extracted from Common Crawl, 353 million associated images, and 115 billion text tokens. Hugo Laurençon; Lucile Saulnier; Leo Tronchon; Stas Bekman; Amanpreet Singh; Anton Lozhkov; Thomas Wang; Siddharth Karamcheti; Alexander Rush; Douwe Kiela; Matthieu Cord; Victor Sanh; 101 Optimizing Prompts for Text-to-Image Generation\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: Instead of laborious human engineering, we propose prompt adaptation, a general framework that automatically adapts original user input to model-preferred prompts. Yaru Hao; Zewen Chi; Li Dong; Furu Wei; 102 RealTime QA: What’s The Answer Right Now?\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: We introduce RealTime QA, a dynamic question answering (QA) platform that announces questions and evaluates systems on a regular basis (weekly in this version). Jungo Kasai; Keisuke Sakaguchi; yoichi takahashi; Ronan Le Bras; Akari Asai; Xinyan Yu; Dragomir Radev; Noah Smith; Yejin Choi; Kentaro Inui; 103 Tracr: Compiled Transformers As A Laboratory for Interpretability\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: We show how to compile human-readable programs into standard decoder-only transformer models. David Lindner; Janos Kramar; Sebastian Farquhar; Matthew Rahtz; Tom McGrath; Vladimir Mikulik; 104 VisionLLM: Large Language Model Is Also An Open-Ended Decoder for Vision-Centric Tasks\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: In this work, we present an LLM-based framework for vision-centric tasks, termed VisionLLM. Wenhai Wang; Zhe Chen; Xiaokang Chen; Jiannan Wu; Xizhou Zhu; Gang Zeng; Ping Luo; Tong Lu; Jie Zhou; Yu Qiao; Jifeng Dai; 105 GenEval: An Object-focused Framework for Evaluating Text-to-image Alignment\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: In this paper, we introduce GenEval, an object-focused framework to evaluate compositional image properties such as object co-occurrence, position, count, and color. Dhruba Ghosh; Hannaneh Hajishirzi; Ludwig Schmidt; 106 What Is The Inductive Bias of Flatness Regularization? A Study of Deep Matrix Factorization Models\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: We show that with the standard Restricted Isometry Property (RIP) on the measurements, minimizing the trace of Hessian is approximately equivalent to minimizing the Schatten 1-norm of the corresponding end-to-end matrix parameters (i.e., the product of all layer matrices), which in turn leads to better generalization. Khashayar Gatmiry; Zhiyuan Li; Tengyu Ma; Sashank Reddi; Stefanie Jegelka; Ching-Yao Chuang; 107 Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: However, LLMs have inherent limitations as they are incapable of accessing up-to-date information (stored on the Web or in task-specific knowledge bases), using external tools, and performing precise mathematical and logical reasoning. In this paper, we present Chameleon, an AI system that mitigates these limitations by augmenting LLMs with plug-and-play modules for compositional reasoning. Pan Lu; Baolin Peng; Hao Cheng; Michel Galley; Kai-Wei Chang; Ying Nian Wu; Song-Chun Zhu; Jianfeng Gao; 108 Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved with Text\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: We use a linear assignment algorithm to place images into longer bodies of text using CLIP features, a process that we show outperforms alternatives. Wanrong Zhu; Jack Hessel; Anas Awadalla; Samir Yitzhak Gadre; Jesse Dodge; Alex Fang; Youngjae Yu; Ludwig Schmidt; William Yang Wang; Yejin Choi; 109 Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: In this work, we propose using reinforcement learning (RL) to fine-tune text-to-image models. Ying Fan; Olivia Watkins; Yuqing Du; Hao Liu; Moonkyung Ryu; Craig Boutilier; Pieter Abbeel; Mohammad Ghavamzadeh; Kangwook Lee; Kimin Lee; 110 Provably Bounding Neural Network Preimages\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: In this work, we present the INVPROP algorithm for verifying properties over the preimage of a linearly constrained output set of a neural network, which can be combined with branch-and-bound to increase precision. Christopher Brix; Suhas Kotha; Huan Zhang; J. Zico Kolter; Krishnamurthy Dvijotham; 111 DataComp: In Search of The Next Generation of Multimodal Datasets\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: Multimodal datasets are a critical component in recent breakthroughs such as CLIP, Stable Diffusion and GPT-4, yet their design does not receive the same research attention as model architectures or training algorithms. To address this shortcoming in the machine learning ecosystem, we introduce DataComp, a testbed for dataset experiments centered around a new candidate pool of 12.8 billion image-text pairs from Common Crawl. Samir Yitzhak Gadre; Gabriel Ilharco; Alex Fang; Jonathan Hayase; Georgios Smyrnis; Thao Nguyen; Ryan Marten; Mitchell Wortsman; Dhruba Ghosh; Jieyu Zhang; Eyal Orgad; Rahim Entezari; Giannis Daras; Sarah Pratt; Vivek Ramanujan; Yonatan Bitton; Kalyani Marathe; Stephen Mussmann; Richard Vencu; Mehdi Cherti; Ranjay Krishna; Pang Wei Koh; Olga Saukh; Alexander Ratner; Shuran Song; Hannaneh Hajishirzi; Ali Farhadi; Romain Beaumont; Sewoong Oh; Alex Dimakis; Jenia Jitsev; Yair Carmon; Vaishaal Shankar; Ludwig Schmidt; 112 On The Connection Between Pre-training Data Diversity and Fine-tuning Robustness\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: Pre-training has been widely adopted in deep learning to improve model performance, especially when the training data for a target task is limited. In our work, we seek to understand the implications of this training strategy on the generalization properties of downstream models. Vivek Ramanujan; Thao Nguyen; Sewoong Oh; Ali Farhadi; Ludwig Schmidt; 113 Ordering-based Conditions for Global Convergence of Policy Gradient Methods\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: We prove that, for finite-arm bandits with linear function approximation, the global convergence of policy gradient (PG) methods depends on inter-related properties between the policy update and the representation. Jincheng Mei; Bo Dai; Alekh Agarwal; Mohammad Ghavamzadeh; Csaba Szepesvari; Dale Schuurmans; 114 Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: We observe that the generative behavior of diffusion models is not strongly dependent on the choice of image degradation, and in fact, an entire family of generative models can be constructed by varying this choice. Arpit Bansal; Eitan Borgnia; Hong-Min Chu; Jie Li; Hamid Kazemi; Furong Huang; Micah Goldblum; Jonas Geiping; Tom Goldstein; 115 Collaborative Development of NLP Models\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: Moreover, the exhaustive delineation of a concept is challenging, and an improper approach can create shortcuts or interfere with original data or other concepts. To address these challenges, we introduce CoDev, a framework that enables multi-user interaction with the model, thereby mitigating individual limitations. Fereshte Khani; Marco Tulio Ribeiro; 116 Text Alignment Is An Efficient Unified Model for Massive NLP Tasks\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: In this paper, we propose text alignment as an efficient unified model for a wide range of crucial tasks involving text entailment, similarity, question answering (and answerability), factual consistency, and so forth. Yuheng Zha; Yichi Yang; Ruichen Li; Zhiting Hu; 117 Proximity-Informed Calibration for Deep Neural Networks\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: Motivated by the empirical findings, we propose ProCal, a plug-and-play algorithm with a theoretical guarantee to adjust sample confidence based on proximity. Miao Xiong; Ailin Deng; Pang Wei Koh; Jiaying Wu; Shen Li; Jianqing Xu; Bryan Hooi; 118 LinkerNet: Fragment Poses and Linker Co-Design with 3D Equivariant Diffusion\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: In this work, we address a more general problem where the poses of the fragments are *unknown* in 3D space. Jiaqi Guan; Xingang Peng; PeiQi Jiang; Yunan Luo; Jian Peng; Jianzhu Ma; 119 Language Models Meet World Models: Embodied Experiences Enhance Language Models\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: The limitation arises from the fact that LMs are trained only on written text and miss essential embodied knowledge and skills. In this paper, we propose a new paradigm of enhancing LMs by finetuning them with world models, to gain diverse embodied knowledge while retaining their general language capabilities. Jiannan Xiang; Tianhua Tao; Yi Gu; Tianmin Shu; Zirui Wang; Zichao Yang; Zhiting Hu; 120 The Learnability of In-Context Learning\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: In this paper, we propose a first-of-its-kind PAC based framework for in-context learnability, and use it to provide the first finite sample complexity results for the in-context learning setup. Noam Wies; Yoav Levine; Amnon Shashua; 121 Isotropic Loss Design for Non-contrastive SSL\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: Here, we analytically study learning dynamics under cosine similarity in the eigenspace of the predictor network and show that collapse is avoided through implicit variance regularization similar to Euclidean loss but with fundamentally different dynamics. Manu Srinath Halvagal; Axel Laborieux; Friedemann Zenke; 122 DreamHuman: Animatable 3D Avatars from Text\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: We present \\emph{DreamHuman}, a method to generate realistic animatable 3D human avatar models entirely from textual descriptions. Nikos Kolotouros; Thiemo Alldieck; Andrei Zanfir; Eduard Bazavan; Mihai Fieraru; Cristian Sminchisescu; 123 Grounded Decoding: Guiding Text Generation with Grounded Models for Robot Control\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: Thus, if we want to make use of the semantic knowledge in a language model while still situating it in an embodied setting, we must construct an action sequence that is both likely according to the language model and also realizable according to grounded models of the environment. We frame this as a problem similar to probabilistic filtering: decode a sequence that both has high probability under the language model and high probability under a set of grounded model objectives. Wenlong Huang; Fei Xia; Dhruv Shah; Danny Driess; Andy Zeng; Yao Lu; Pete Florence; Igor Mordatch; Sergey Levine; Karol Hausman; brian ichter; 124 Rethinking Bias Mitigation: Fairer Architectures Make for Fairer Face Recognition\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: Motivated by the belief that the inductive bias of a model architecture is more important than the bias mitigation strategy, we take a different approach to bias mitigation. Samuel Dooley; Rhea Sukthanker; John Dickerson; Colin White; Frank Hutter; Micah Goldblum; 125 Setting The Trap: Capturing and Defeating Backdoor Threats in PLMs Through Honeypots\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: In this study, our objective is to develop a backdoor-resistant tuning procedure that yields a backdoor-free model, no matter whether the fine-tuning dataset contains poisoned samples. Ruixiang Tang; Jiayi Yuan; Yiming Li; Zirui Liu; Rui Chen; Xia Hu; 126 Managing Temporal Resolution in Continuous Value Estimation: A Fundamental Trade-off\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: The impact of time discretization on RL methods has not been fully characterized in existing theory, but a more detailed analysis of its effect could reveal opportunities for improving data-efficiency. We address this gap by analyzing Monte-Carlo policy evaluation for LQR systems and uncover a fundamental trade-off between approximation and statistical error in value estimation. Zichen Zhang; Johannes Kirschner; Junxi Zhang; Francesco Zanini; Alex Ayoub; Masood Dehghan; Dale Schuurmans; 127 SugarCrepe: Fixing Hackable Benchmarks for Vision-Language Compositionality\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: This hackability is so dire that blind models with no access to the image outperform state-of-the-art vision-language models. To remedy this rampant vulnerability, we introduce $\\textit{SugarCrepe}$, a new benchmark for vision-language compositionality evaluation. Cheng-Yu Hsieh; Jieyu Zhang; Zixian Ma; Aniruddha Kembhavi; Ranjay Krishna; 128 Self-Chained Image-Language Model for Video Localization and Question Answering\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: Although humans often find a video moment to focus on and rewind the moment to answer questions, training a query-aware video moment localizer often requires expensive annotations and high computational costs. To address this issue, we propose Self-Chained Video Localization-Answering (SeViLA), a novel framework that leverages a single image-language model (BLIP-2) to tackle both temporal keyframe localization and question answering on videos. Shoubin Yu; Jaemin Cho; Prateek Yadav; Mohit Bansal; 129 Dissecting Chain-of-Thought: A Study on Compositional In-Context Learning of MLPs\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: Chain-of-thought (CoT) is a method that enables language models to handle complex reasoning tasks by decomposing them into simpler steps. Despite its success, the underlying mechanics of CoT are not yet fully understood. In an attempt to shed light on this, our study investigates the impact of CoT on the ability of transformers to in-context learn a simple to study, yet general family of compositional functions: multi-layer perceptrons (MLPs). Yingcong Li; Kartik Sreenivasan; Angeliki Giannou; Dimitris Papailiopoulos; Samet Oymak; 130 A Performance-Driven Benchmark for Feature Selection in Tabular Deep Learning\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: We construct a challenging feature selection benchmark evaluated on downstream neural networks including transformers, using real datasets and multiple methods for generating extraneous features. Valeriia Cherepanova; Gowthami Somepalli; Jonas Geiping; C. Bayan Bruss; Andrew Wilson; Tom Goldstein; Micah Goldblum; 131 What You See Is What You Read? Improving Text-Image Alignment Evaluation\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: In this work, we study methods for automatic text-image alignment evaluation. Michal Yarom; Yonatan Bitton; Soravit Changpinyo; Roee Aharoni; Jonathan Herzig; Oran Lang; Eran Ofek; Idan Szpektor; 132 Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: Scaling laws have been recently employed to derive compute-optimal model size (number of parameters) for a given compute duration. We advance and refine such methods to infer compute-optimal model shapes, such as width and depth, and successfully implement this in vision transformers. Ibrahim Alabdulmohsin; Lucas Beyer; Alexander Kolesnikov; Xiaohua Zhai; 133 Scaling Open-Vocabulary Object Detection\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: Major challenges in scaling self-training are the choice of label space, pseudo-annotation filtering, and training efficiency. We present the OWLv2 model and OWL-ST self-training recipe, which address these challenges. Matthias Minderer; Alexey Gritsenko; Neil Houlsby; 134 Any-to-Any Generation Via Composable Diffusion\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: We present Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Zineng Tang; Ziyi Yang; Chenguang Zhu; Michael Zeng; Mohit Bansal; 135 DIFUSCO: Graph-based Diffusion Solvers for Combinatorial Optimization\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: This paper broadens the current scope of neural solvers for NPC problems by introducing a new graph-based diffusion framework, namely DIFUSCO. Zhiqing Sun; Yiming Yang; 136 Meta-in-context Learning in Large Language Models\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: In the present paper, we demonstrate that the in-context learning abilities of large language models can be recursively improved via in-context learning itself. Julian Coda-Forno; Marcel Binz; Zeynep Akata; Matt Botvinick; Jane Wang; Eric Schulz; 137 Revisiting Out-of-distribution Robustness in NLP: Benchmarks, Analysis, and LLMs Evaluations\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: We identify that the distribution shift settings in previous studies commonly lack adequate challenges, hindering the accurate evaluation of OOD robustness. To address these issues, we propose a benchmark construction protocol that ensures clear differentiation and challenging distribution shifts. Lifan Yuan; Yangyi Chen; Ganqu Cui; Hongcheng Gao; FangYuan Zou; Xingyi Cheng; Heng Ji; Zhiyuan Liu; Maosong Sun; 138 Segment Anything in 3D with NeRFs\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: This paper aims to generalize SAM to segment 3D objects. Jiazhong Cen; Zanwei Zhou; Jiemin Fang; chen yang; Wei Shen; Lingxi Xie; Dongsheng Jiang; XIAOPENG ZHANG; Qi Tian; 139 Diffusion Hyperfeatures: Searching Through Time and Space for Semantic Correspondence\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: We propose Diffusion Hyperfeatures, a framework for consolidating multi-scale and multi-timestep feature maps into per-pixel feature descriptors that can be used for downstream tasks. Grace Luo; Lisa Dunlap; Dong Huk Park; Aleksander Holynski; Trevor Darrell; 140 EmbodiedGPT: Vision-Language Pre-Training Via Embodied Chain of Thought\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: In this work, we introduce EmbodiedGPT, an end-to-end multi-modal foundation model for embodied AI, empowering embodied agents with multi-modal understanding and execution capabilities. Yao Mu; Qinglong Zhang; Mengkang Hu; Wenhai Wang; Mingyu Ding; Jun Jin; Bin Wang; Jifeng Dai; Yu Qiao; Ping Luo; 141 Is Your Code Generated By ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: Such limitation in the existing benchmarks begs the following question: In the era of LLMs, is the code generated really correct? To answer this, we propose EvalPlus – a code synthesis benchmarking framework to rigorously evaluate the functional correctness of LLM-synthesized code. Jiawei Liu; Chunqiu Steven Xia; Yuyao Wang; LINGMING ZHANG; 142 ResShift: Efficient Diffusion Model for Image Super-resolution By Residual Shifting\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: Existing acceleration sampling techniques inevitably sacrifice performance to some extent, leading to over-blurry SR results. To address this issue, we propose a novel and efficient diffusion model for SR that significantly reduces the number of diffusion steps, thereby eliminating the need for post-acceleration during inference and its associated performance deterioration. Zongsheng Yue; Jianyi Wang; Chen Change Loy; 143 Evaluating and Improving Tool-Augmented Computation-Intensive Math Reasoning\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: In CARP, we test four LLMs with CoT prompting, and find that they are all prone to make mistakes at the early steps of the solution, leading to incorrect answers. Based on this finding, we propose a new approach that can deliberate the reasoning steps with tool interfaces, namely \\textbf{DELI}. Beichen Zhang; Kun Zhou; Xilin Wei; Xin Zhao; Jing Sha; Shijin Wang; Ji-Rong Wen; 144 Simplifying and Empowering Transformers for Large-Graph Representations\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: In this paper, we critically demonstrate that even using a one-layer attention can bring up surprisingly competitive performance across node property prediction benchmarks where node numbers range from thousand-level to billion-level. Qitian Wu; Wentao Zhao; Chenxiao Yang; Hengrui Zhang; Fan Nie; Haitian Jiang; Yatao Bian; Junchi Yan; 145 Are Diffusion Models Vision-And-Language Reasoners?\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: However, unlike discriminative vision-and-language models, it is a non-trivial task to subject these diffusion-based generative models to automatic fine-grained quantitative evaluation of high-level phenomena such as compositionality. Towards this goal, we perform two innovations. First, we transform diffusion-based models (in our case, Stable Diffusion) for any image-text matching (ITM) task using a novel method called DiffusionITM. Second, we introduce the Generative-Discriminative Evaluation Benchmark (GDBench) benchmark with 7 complex vision-and-language tasks, bias evaluation and detailed analysis. Benno Krojer; Elinor Poole-Dayan; Vikram Voleti; Chris Pal; Siva Reddy; 146 Autodecoding Latent 3D Diffusion Models\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: Such data is scarce for 3D generation, prohibiting the learning of large-scale diffusion models for 3D synthesis. We present a novel approach to the generation of static and articulated 3D assets that has a 3D autodecoder at its core. Evangelos Ntavelis; Aliaksandr Siarohin; Kyle Olszewski; Chaoyang Wang; Luc V Gool; Sergey Tulyakov; 147 Textually Pretrained Speech Language Models\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: In this work, we propose TWIST, a method for training SpeechLMs using a warm-start from a pretrained textual language models. Michael Hassid; Tal Remez; Tu Anh Nguyen; Itai Gat; Alexis CONNEAU; Felix Kreuk; Jade Copet; Alexandre Defossez; Gabriel Synnaeve; Emmanuel Dupoux; Roy Schwartz; Yossi Adi; 148 Learning New Dimensions of Human Visual Similarity Using Synthetic Data\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: In this paper, we develop a perceptual metric that assesses images holistically. Stephanie Fu; Netanel Tamir; Shobhita Sundaram; Lucy Chai; Richard Zhang; Tali Dekel; Phillip Isola; 149 MADLAD-400: Monolingual And Document-Level Large Audited Dataset\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: We introduce MADLAD-400, a manually audited, general domain 3T token monolingual dataset based on CommonCrawl, spanning 419 languages. Sneha Kudugunta; Isaac Caswell; Biao Zhang; Xavier Garcia; Derrick Xin; Aditya Kusupati; Romi Stella; Ankur Bapna; Orhan Firat; 150 Symbolic Discovery of Optimization Algorithms\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: We present a method to formulate algorithm discovery as program search, and apply it to discover optimization algorithms for deep neural network training. Xiangning Chen; Chen Liang; Da Huang; Esteban Real; Kaiyuan Wang; Hieu Pham; Xuanyi Dong; Thang Luong; Cho-Jui Hsieh; Yifeng Lu; Quoc V Le; 151 Towards Revealing The Mystery Behind Chain of Thought: A Theoretical Perspective\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: Despite the enormous empirical success, the underlying mechanisms behind CoT and how it unlocks the potential of LLMs remain elusive. In this paper, we take a first step towards theoretically answering these questions. Guhao Feng; Yuntian Gu; Haotian Ye; Bohang Zhang; Di He; Liwei Wang; 152 Timewarp: Transferable Acceleration of Molecular Dynamics By Learning Time-Coarsened Dynamics\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: We present *Timewarp*, an enhanced sampling method which uses a normalising flow as a proposal distribution in a Markov chain Monte Carlo method targeting the Boltzmann distribution. Leon Klein; Andrew Foong; Tor Fjelde; Bruno Mlodozeniec; Marc Brockschmidt; Sebastian Nowozin; Frank Noe; Ryota Tomioka; 153 On Efficient Training Algorithms For Transformer Language Models\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: In this work, we revisit three algorithms: layer stacking, layer dropping, and selective backpropagation. Jean Kaddour; Oscar Key; Piotr Nawrot; Pasquale Minervini; Matt Kusner; 154 Real-World Image Variation By Aligning Diffusion Inversion Chain\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: Our investigation uncovers that this domain gap originates from a latents’ distribution gap in different diffusion processes. To address this issue, we propose a novel inference pipeline called Real-world Image Variation by ALignment (RIVAL) that utilizes diffusion models to generate image variations from a single image exemplar. Yuechen Zhang; Jinbo Xing; Eric Lo; Jiaya Jia; 155 Tree-Rings Watermarks: Invisible Fingerprints for Diffusion Images\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: In this paper, we introduce a novel technique called Tree-Ring Watermarking that robustly fingerprints diffusion model outputs. Yuxin Wen; John Kirchenbauer; Jonas Geiping; Tom Goldstein; 156 Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: We describe an easy-to-use approach to automatically optimize hard text prompts through efficient gradient-based optimization. Yuxin Wen; Neel Jain; John Kirchenbauer; Micah Goldblum; Jonas Geiping; Tom Goldstein; 157 Reward Imputation with Sketching for Contextual Batched Bandits\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: In this paper, we propose an efficient approach called Sketched Policy Updating with Imputed Rewards (SPUIR) that completes the unobserved rewards using sketching, which approximates the full-information feedbacks. Xiao Zhang; Ninglu Shao; Zihua Si; Jun Xu; Wenhan Wang; Hanjing Su; Ji-Rong Wen; 158 REASONER: An Explainable Recommendation Dataset with Comprehensive Labeling Ground Truths\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: In the past few years, while a lot of promising explainable recommender models have been proposed, the datasets used to evaluate them still suffer from several limitations, for example, the explanation ground truths are not labeled by the real users, the explanations are mostly single-modal and around only one aspect. To bridge these gaps, in this paper, we build a new explainable recommendation dataset, which, to our knowledge, is the first contribution that provides a large amount of real user labeled multi-modal and multi-aspect explaination ground truths. Xu Chen; Jingsen Zhang; Lei Wang; Quanyu Dai; Zhenhua Dong; Ruiming Tang; Rui Zhang; Li Chen; Xin Zhao; Ji-Rong Wen; 159 Pengi: An Audio Language Model for Audio Tasks\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: We introduce Pengi, a novel Audio Language Model that leverages Transfer Learning by framing all audio tasks as text-generation tasks. Soham Deshmukh; Benjamin Elizalde; Rita Singh; Huaming Wang; 160 Solving Inverse Problems Provably Via Posterior Sampling with Latent Diffusion Models\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: We present the first framework to solve general inverse problems leveraging pre-trained *latent* diffusion models. Litu Rout; Negin Raoof; Giannis Daras; Constantine Caramanis; Alex Dimakis; Sanjay Shakkottai; 161 On The Exploitability of Instruction Tuning\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: In this work, we investigate how an adversary can exploit instruction tuning by injecting specific instruction-following examples into the training data that intentionally changes the model’s behavior. Manli Shu; Jiongxiao Wang; Jonas Geiping; Chaowei Xiao; Tom Goldstein; 162 VisoGender: A Dataset for Benchmarking Gender Bias in Image-text Pronoun Resolution\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: We introduce VisoGender, a novel dataset for benchmarking gender bias in vision-language models. Siobhan Mackenzie Hall; Fernanda Gonçalves Abrantes; Hanwen Zhu; Grace Sodunke; Aleksandar Shtedritski; Hannah Rose Kirk; 163 Nonparametric Identifiability of Causal Representations from Unknown Interventions\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: Our goal is to identify both the ground truth latents and their causalgraph up to a set of ambiguities which we show to be irresolvable from interventional data. Julius von Kügelgen; Michel Besserve; Liang Wendong; Luigi Gresele; Armin Kekić; Elias Bareinboim; David Blei; Bernhard Schölkopf; 164 StyleDrop: Text-to-Image Synthesis of Any Style\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: In this paper, we introduce *StyleDrop*, a method that enables the synthesis of images that faithfully follow a specific style using a text-to-image model. Kihyuk Sohn; Lu Jiang; Jarred Barber; Kimin Lee; Nataniel Ruiz; Dilip Krishnan; Huiwen Chang; Yuanzhen Li; Irfan Essa; Michael Rubinstein; Yuan Hao; Glenn Entis; Irina Blok; Daniel Castro Chin; 165 Multi-Objective Agency Requires Non-Markovian Rewards\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: To this end, we propose a practical non-Markovian aggregation scheme that overcomes the impossibility with only one additional parameter for each objective. Silviu Pitis; 166 Free-Bloom: Zero-Shot Text-to-Video Generator with LLM Director and LDM Animator\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: To generate a semantic-coherent video, exhibiting a rich portrayal of temporal semantics such as the whole process of flower blooming rather than a set of “moving images”, we propose a novel Free-Bloom pipeline that harnesses large language models (LLMs) as the director to generate a semantic-coherence prompt sequence, while pre-trained latent diffusion models (LDMs) as the animator to generate the high fidelity frames. Hanzhuo Huang; Yufan Feng; Cheng Shi; Lan Xu; Jingyi Yu; Sibei Yang; 167 VPP: Efficient Universal 3D Generation Via Voxel-Point Progressive Representation\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: Motivated by the characteristics of different representations, we propose VPP, a voxel-point progressive representation for both efficient and universal 3D generation. Zekun Qi; Muzhou Yu; Runpei Dong; Kaisheng Ma; 168 Bridging Discrete and Backpropagation: Straight-Through and Beyond\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: This limitation poses challenges for problems involving discrete latent variables. To address this issue, we propose a novel approach to approximate the gradient of parameters involved in generating discrete latent variables. Liyuan Liu; Chengyu Dong; Xiaodong Liu; Bin Yu; Jianfeng Gao; 169 Self-Supervised Learning of Representations for Space Generates Multi-Modular Grid Cells\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: We define a novel self-supervised learning (SSL) framework for properly arranging representations in an abstract coding space, and show that it can produce grid codes when constrained to perform high-efficiency representation of space with recurrent neural networks. Rylan Schaeffer; Mikail Khona; Tzuhsuan Ma; Cristobal Eyzaguirre; Sanmi Koyejo; Ila Fiete; 170 Are Emergent Abilities of Large Language Models A Mirage?\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: We present our alternative explanation in a simple mathematical model, then test it in three complementary ways: we (1) make, test and confirm three predictions on the effect of metric choice using the InstructGPT/GPT-3 family on tasks with claimed emergent abilities, (2) make, test and confirm two predictions about metric choices in a meta-analysis of emergent abilities on BIG-Bench; and (3) show how to choose metrics to produce never-before-seen seemingly emergent abilities in multiple vision tasks across diverse deep networks. Rylan Schaeffer; Brando Miranda; Sanmi Koyejo; 171 Unlimiformer: Long-Range Transformers with Unlimited Length Input\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: In this work, we propose Unlimiformer: a general approach that wraps any existing pretrained encoder-decoder transformer, and offloads the cross-attention computation to a single $k$-nearest-neighbor ($k$NN) index, while the returned $k$NN distances are the attention dot-product scores. Amanda Bertsch; Uri Alon; Graham Neubig; Matthew Gormley; 172 OpenAGI: When LLM Meets Domain Experts\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: In this work, we introduce \\textbf{OpenAGI}, an open-source AGI research platform designed for multi-step, real-world tasks. Yingqiang Ge; Wenyue Hua; Kai Mei; jianchao ji; Juntao Tan; Shuyuan Xu; Zelong Li; Yongfeng Zhang; 173 A Case for Reframing Automated Medical Image Classification As Segmentation\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: However, recent work has drastically reduced the cost of training segmentation networks. In light of this recent work, we reexamine the choice of training classification vs. segmentation models. Sarah Hooper; Mayee Chen; Khaled Saab; Kush Bhatia; Curtis Langlotz; Christopher Ré; 174 Controlling Text-to-Image Diffusion By Orthogonal Finetuning\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: How to effectively guide or control these powerful models to perform different downstream tasks becomes an important open problem. To tackle this challenge, we introduce a principled finetuning method — Orthogonal Finetuning (OFT), for adapting text-to-image diffusion models to downstream tasks. Zeju Qiu; Weiyang Liu; Haiwen Feng; Yuxuan Xue; Yao Feng; Zhen Liu; Dan Zhang; Adrian Weller; Bernhard Schölkopf; 175 H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: Often, a large amount of transient state information, referred to as the $\\mathsf{KV}$ $\\mathsf{cache}$, is stored in GPU memory in addition to model parameters, scaling linearly with the sequence length and batch size. In this paper, we introduce a novel approach for implementing the $\\mathsf{KV}$ $\\mathsf{cache}$ which significantly reduces its memory footprint. Zhenyu Zhang; Ying Sheng; Tianyi Zhou; Tianlong Chen; Lianmin Zheng; Ruisi Cai; Zhao Song; Yuandong Tian; Christopher Ré; Clark Barrett; Zhangyang Atlas Wang; Beidi Chen; 176 High-Fidelity Audio Compression with Improved RVQGAN\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: To that end, we introduce a high-fidelity universal neural audio compression algorithm that achieves ~90x compression of 44.1 KHz audio into tokens at just 8kbps bandwidth. Rithesh Kumar; Prem Seetharaman; Alejandro Luebs; Ishaan Kumar; Kundan Kumar; 177 Weakly-Supervised Concealed Object Segmentation with SAM-based Pseudo Labeling and Multi-scale Feature Grouping\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: It remains a challenging task since (1) it is hard to distinguish concealed objects from the background due to the intrinsic similarity and (2) the sparsely-annotated training data only provide weak supervision for model learning. In this paper, we propose a new WSCOS method to address these two challenges. Chunming He; Kai Li; Yachao Zhang; Guoxia Xu; Longxiang Tang; Yulun Zhang; Zhenhua Guo; Xiu Li; 178 On Evaluating Adversarial Robustness of Large Vision-Language Models\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: To this end, we propose evaluating the robustness of open-source large VLMs in the most realistic and high-risk setting, where adversaries have only black-box system access and seek to deceive the model into returning the targeted responses. Yunqing Zhao; Tianyu Pang; Chao Du; Xiao Yang; Chongxuan LI; Ngai-Man (Man) Cheung; Min Lin; 179 NAVI: Category-Agnostic Image Collections with High-Quality 3D Shape and Pose Annotations\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: To enable systematic research progress on 3D reconstruction from casual image captures, we propose `NAVI’: a new dataset of category-agnostic image collections of objects with high-quality 3D scans along with per-image 2D-3D alignments providing near-perfect GT camera parameters. Varun Jampani; Kevis-kokitsi Maninis; Andreas Engelhardt; Arjun Karpur; Karen Truong; Kyle Sargent; Stefan Popov; Andre Araujo; Ricardo Martin Brualla; Kaushal Patel; Daniel Vlasic; Vittorio Ferrari; Ameesh Makadia; Ce Liu; Yuanzhen Li; Howard Zhou; 180 SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: We introduce SwiftSage, a novel agent framework inspired by the dual-process theory of human cognition, designed to excel in action planning for complex interactive reasoning tasks. Bill Yuchen Lin; Yicheng Fu; Karina Yang; Prithviraj (Raj) Ammanabrolu; Faeze Brahman; Shiyu Huang; Chandra Bhagavatula; Yejin Choi; Xiang Ren; 181 Bypass Exponential Time Preprocessing: Fast Neural Network Training Via Weight-Data Correlation Preprocessing\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: In this work, we present a new preprocessing method that simply stores the weight-data correlation in a tree data structure in order to quickly, dynamically detect which neurons fire at each iteration. Josh Alman; 杰昊 梁; Zhao Song; Ruizhe Zhang; Danyang Zhuo; 182 TART: A Plug-and-play Transformer Module for Task-agnostic Reasoning\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: This raises an intriguing question: Are LLMs actually capable of learning how to reason in a task-agnostic manner? We answer this in the affirmative and, as a proof of concept, propose TART which generically improves an LLM’s reasoning abilities using a synthetically trained reasoning module. Kush Bhatia; Avanika Narayan; Christopher De Sa; Christopher Ré; 183 Skill-it! A Data-driven Skills Framework for Understanding and Training Language Models\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: Using our proposed framework, we introduce an online data sampling algorithm, Skill-It, over mixtures of skills for learning skills more quickly for both continual pre-training and fine-tuning regimes, where we aim to learn multiple skills in the former and an individual skill in the latter. Mayee Chen; Nicholas Roberts; Kush Bhatia; Jue WANG; Ce Zhang; Frederic Sala; Christopher Ré; 184 Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: Here, we explore Monarch Mixer (M2), a new architecture that uses the same sub-quadratic primitive along both sequence length and model dimension. Dan Fu; Jessica Grogan; Isys Johnson; Simran Arora; Evan Sabri Eyuboglu; Armin Thomas; Benjamin Spector; Michael Poli; Atri Rudra; Christopher Ré; 185 VidChapters-7M: Video Chapters at Scale\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: This important topic has been understudied due to the lack of publicly released datasets. To address this issue, we present VidChapters-7M, a dataset of 817K user-chaptered videos including 7M chapters in total. Antoine Yang; Arsha Nagrani; Ivan Laptev; Josef Sivic; Cordelia Schmid; 186 How Does GPT-2 Compute Greater-than?: Interpreting Mathematical Abilities in A Pre-trained Language Model\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: In this paper, we investigate the basic mathematical abilities often acquired by pre-trained language models. Michael Hanna; Ollie Liu; Alexandre Variengien; 187 Multi-scale Diffusion Denoised Smoothing\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: In this paper, we investigate the trade-off between accuracy and certified robustness of denoised smoothing: for example, we question on which representation of diffusion model would maximize the certified robustness of denoised smoothing. Jongheon Jeong; Jinwoo Shin; 188 PyNeRF: Pyramidal Neural Radiance Fields\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: We propose a simple modification to grid-based models by training model heads at different spatial grid resolutions. Haithem Turki; Michael Zollhöfer; Christian Richardt; Deva Ramanan; 189 UniPC: A Unified Predictor-Corrector Framework for Fast Sampling of Diffusion Models\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: In this paper, we develop a unified corrector (UniC) that can be applied after any existing DPM sampler to increase the order of accuracy without extra model evaluations, and derive a unified predictor (UniP) that supports arbitrary order as a byproduct. Wenliang Zhao; Lujia Bai; Yongming Rao; Jie Zhou; Jiwen Lu; 190 PIXIU: A Comprehensive Benchmark, Instruction Dataset and Large Language Model for Finance\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: This paper introduces PIXIU, a comprehensive framework including the first financial LLM based on fine-tuning LLaMA with instruction data, the first instruction data with 128K data samples to support the fine-tuning, and an evaluation benchmark with 8 tasks and 15 datasets. Qianqian Xie; Weiguang Han; Xiao Zhang; Yanzhao Lai; Min Peng; Alejandro Lopez-Lira; Jimin Huang; 191 AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: To account for the inherent sequential characteristic of natural language, we introduce Auto-Regressive Diffusion (AR-Diffusion). Tong Wu; Zhihao Fan; Xiao Liu; Yeyun Gong; yelong shen; Jian Jiao; Hai-Tao Zheng; Juntao Li; zhongyu wei; Jian Guo; Nan Duan; Weizhu Chen; 192 BIRD: Generalizable Backdoor Detection and Removal for Deep Reinforcement Learning\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: Despite initial defenses proposed in recent studies, these methods have very limited generalizability and scalability. To address this issue, we propose BIRD, a technique to detect and remove backdoors from a pretrained DRL policy in a clean environment without requiring any knowledge about the attack specifications and accessing its training process. Xuan Chen; Wenbo Guo; Guanhong Tao; Xiangyu Zhang; Dawn Song; 193 Diffusion Models and Semi-Supervised Learners Benefit Mutually with Few Labels\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts Related Code View\n\nHighlight: In an effort to further advance semi-supervised generative and classification tasks, we propose a simple yet effective training strategy called *dual pseudo training* (DPT), built upon strong semi-supervised learners and diffusion models. Zebin You; Yong Zhong; Fan Bao; Jiacheng Sun; Chongxuan LI; Jun Zhu; 194 Synthetic Pretraining for Few-shot Black-Box Optimization\n\nRelated Papers Related Patents Related Grants Related Venues Related Experts View\n\nHighlight: In this work, we address the more challenging yet realistic setting of few-shot black-box"
    }
}
{
    "id": "dbpedia_1328_2",
    "rank": 60,
    "data": {
        "url": "https://arxiv.org/html/2405.14002v1",
        "read_more_link": "",
        "language": "en",
        "title": "Animal Behavior Analysis Methods Using Deep Learning: A Survey",
        "top_image": "",
        "meta_img": "",
        "images": [],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            "Animal Behavior",
            "Deep Learning",
            "Pose Estimation",
            "Object Detection",
            "Bio-acoustics",
            "Machine Learning"
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Edoardo Fazzari , Donato Romano , Fabrizio Falchi and Cesare Stefanini\n\nAbstract.\n\nAnimal behavior serves as a reliable indicator of the adaptation of organisms to their environment and their overall well-being. Through rigorous observation of animal actions and interactions, researchers and observers can glean valuable insights into diverse facets of their lives, encompassing health, social dynamics, ecological relationships, and neuroethological dimensions. Although state-of-the-art deep learning models have demonstrated remarkable accuracy in classifying various forms of animal data, their adoption in animal behavior studies remains limited. This survey article endeavors to comprehensively explore deep learning architectures and strategies applied to the identification of animal behavior, spanning auditory, visual, and audiovisual methodologies. Furthermore, the manuscript scrutinizes extant animal behavior datasets, offering a detailed examination of the principal challenges confronting this research domain. The article culminates in a comprehensive discussion of key research directions within deep learning that hold potential for advancing the field of animal behavior studies.\n\nAnimal Behavior, Deep Learning, Pose Estimation, Object Detection, Bio-acoustics, Machine Learning\n\n‚Ä†‚Ä†copyright: acmcopyright‚Ä†‚Ä†journalyear: 2023‚Ä†‚Ä†doi: XXXXXXX.XXXXXXX‚Ä†‚Ä†ccs: Computing methodologies Machine learning‚Ä†‚Ä†ccs: Applied computing Bioinformatics\n\n1. Introduction\n\nAnimal behavior encompasses a spectrum of actions, reactions, and activity patterns demonstrated by animals in response to their environment, fellow organisms, and internal stimuli (Brown and de Bivort, 2017). This expansive field covers a diverse range of behaviors, spanning from innate instincts and simple reflexes to intricate social interactions and learned conduct. The study of animal behavior involves the observation, description, and comprehension of how animals engage with one another and their surroundings. Presently, the landscape of animal behavior research is undergoing rapid evolution, propelled by the continual introduction of innovative experimental methodologies and the advancement of sophisticated behavior detection systems (Wang et al., 2021a). This progression holds particular significance in advancing our understanding of neuroethological aspects, exemplified by the utilization of mice in exploring diseases like Alzheimer‚Äôs (Pedersen et al., 2006), and in refining animal welfare practices within agriculture (Mishra and Sharma, 2023). The impetus behind this surge in progress is the integration of cutting-edge technologies, with deep learning standing out as a transformative force that reshapes the approaches researchers employ to investigate and interpret animal behaviors(Brown and de Bivort, 2017).\n\nDeep learning has emerged as a pivotal tool in the exploration of animal behavior. This advanced branch of artificial intelligence empowers computers to autonomously discern patterns and features from extensive datasets. As researchers amass increasingly intricate datasets through state-of-the-art monitoring technologies, including high-resolution cameras, GPS tracking devices, and sensors, the capability of deep learning algorithms to extract meaningful insights becomes indispensable (Benaissa et al., 2023; Koger et al., 2023). This not only expedites the analysis process but also reveals nuanced aspects of animal behavior that were previously challenging to decipher. Furthermore, deep learning plays a crucial role in the development of sophisticated behavior detection systems. These systems can automatically recognize and classify various behaviors, allowing researchers to redirect their focus from laborious manual data annotation to the interpretation of results (Arablouei et al., 2023b). This acceleration in data processing and behavior recognition enhances the scalability and efficiency of animal behavior studies, ushering in a new era of discovery and understanding in this dynamic field.\n\n1.1. Survey structure\n\nThe structure of our survey is systematically delineated as follows: In the initial section, we expound upon the underlying motivation propelling the use of deep learning for examining animal behaviors, explaining the inherent limitations therein. Concurrently, we articulate the research questions that our survey endeavors to address. Subsequently, a comprehensive exposition of the methodological framework employed for conducting the survey is presented, which includes a discerning analysis of the gathered data to elucidate discernible trends within the research domain. The subsequent segmentation of the study into two distinct components is pivotal to the overarching architecture of this article. These segments, namely pose estimation and non-pose estimation-based methods, constitute the primary constituents of our investigation, elucidating the extraction of salient information pertinent to animal behavioral analysis and its subsequent application in behavior identification. Following this delineation, we furnish a compendium of publicly available datasets. In conclusion, we revisit the initially posited research questions, providing responses in light of the findings expounded within the two principal segments of the article.\n\n1.2. Contributions\n\nThis survey article makes a threefold contribution to the current understanding of the study of animal behavior through deep learning:\n\n‚Ä¢\n\nWe provide a thorough examination of existing technologies and algorithms employed in the analysis of animal behavior. This entails a detailed exploration of methodologies and approaches currently prevalent in this domain, providing readers with a nuanced understanding of the technological landscape.\n\n‚Ä¢\n\nWe compile and present a comprehensive list of publicly available datasets relevant to the research field. This compilation serves as a valuable resource for researchers and practitioners, facilitating access to essential data for furthering investigations into animal behaviors through data-driven methodologies.\n\n‚Ä¢\n\nWe engage in a substantive discussion regarding potential directions for the evolution of the field. Emphasizing the integration of deep learning techniques, our discourse aims to enhance the quality of existing technologies, thereby advancing the understanding of animal behaviors. This forward-looking analysis provides insights into potential avenues for improvement and innovation.\n\nTo the best of our knowledge, this survey is the only examination of the topic to date. The comprehensive overview, dataset compilation, and forward-looking discussions collectively contribute to a nuanced understanding of current advancements and lay the groundwork for future developments in the application of deep learning to the study of animal behavior.\n\n2. Motivation and problem statement\n\nIn this section, we expound upon the foundational motivations that underscore the study of animal behavior through deep learning, articulating the diverse advantages and objectives inherent to this specialized research domain. Concurrently, we meticulously scrutinize the principal limitations that characterize this field, recognizing the nuanced challenges that arise from variations in research setups. Our intention is to furnish a comprehensive guide for prospective researchers, endowing them with a thorough understanding of potential impediments prior to initiating investigations within this domain. Concurrently, we underscore the myriad opportunities inherent in the study of animal behavior, fostering an appreciation for the intricate dimensions of this research frontier.\n\n2.1. Limitations in studying animal behaviors\n\nThe exploration of animal behavior is confronted by a multitude of challenges that intricately shape the effectiveness and practicality of its applications. These challenges permeate the methodologies employed for data acquisition, the intricacies of data analysis (both in terms of location and computational demands), and the nuanced process of data annotation.\n\nAn integral aspect of animal behavior studies is the utilization of sensors for data collection. However, the attachment of sensors to animals introduces a unique set of challenges. Notably, there is a risk of inducing stress responses and altering normal behaviors (Zhang et al., 2020). This necessitates a profound reflection on the authenticity of the collected data, urging researchers to question whether stress-induced behaviors accurately mirror natural patterns. Moreover, the task of differentiating genuine behavioral signals from background noise in sensor data adds complexity to interpretation, emphasizing the requirement for advanced algorithms capable of discerning meaningful patterns amidst the noise (Kavlak et al., 2023). Beyond stress responses and noise challenges, sensor equipment grapples with limitations in battery life (Mekruksavanich et al., 2022), impacting the duration and scope of behavioral studies, especially in scenarios requiring continuous data collection over extended periods. Researchers are challenged to strike a balance between the need for comprehensive, continuous monitoring and the practical constraints imposed by limited battery capacities.\n\nTransitioning into the domain of mobile devices introduces another layer of complexity to the challenges encountered in deep learning applications. The implementation of models on these devices confronts the persistent issue of storage limitations (Cao et al., 2020). Balancing robust object detection with efficient data compression becomes a paramount concern, with techniques like Quantized-CNN (Wu et al., 2016) attempting to address this challenge. However, the ongoing quest is to achieve this balance without compromising precision, a crucial consideration for the reliability of behavioral analyses.\n\nA pivotal challenge arises in the realm of labeling and annotation, where economic and practical constraints hinder the tagging of large datasets for each animal (Bhattacharya and Shahnawaz, 2021). This bottleneck impedes the scalability of deep learning models, heavily reliant on labeled datasets for effective training. The impracticality of manual labeling raises fundamental concerns about the breadth and accuracy of behavioral datasets, impacting the reliability of subsequent analyses. Subjectivity compounds these challenges, influencing the accuracy and consistency of behavioral annotations. Visual inspection, often subjective, is limited in providing objective insights into complex animal behaviors (Bernardes et al., 2021). Manual annotation, while traditional, is labor-intensive and susceptible to inter-annotator disagreements (Zhou et al., 2022; Segalin et al., 2021). The inherent subjectivity introduces variability, raising questions about the replicability and reliability of experiments (Hou et al., 2020; Dell et al., 2014).\n\nMoreover, these challenges extend to innovative techniques, such as multi-view recordings, which hold promise for providing richer insights into animal behaviors (Jiang et al., 2021). However, challenges arise in correlating social behaviors from different perspectives due to the lack of correspondence across data sources. Effectively coordinating information from multiple viewpoints demands inventive approaches to ensure accuracy and reliability, representing a frontier where deep learning methodologies can contribute significantly.\n\nA major challenge surfaces when comparing laboratory studies, where challenges often revolve around the subjectivity introduced by the controlled environment, with ethological studies conducted in the wild presenting a distinct set of challenges, notably in-field tracking (Marshall et al., 2022). The diverse and unpredictable environments encountered in the wild introduce complexities not found in controlled laboratory settings. Bridging the gap between these two disciplines necessitates adaptable detection and tracking algorithms that seamlessly operate in both environments. Robust algorithms capable of handling varying animal sizes, changing appearances, clutter, occlusions, and unpredictable environments are vital for extracting meaningful insights (Haalck et al., 2020; Hou et al., 2020; Lauer et al., 2022). These challenges underscore the critical need for technological innovation that aligns with the demands of both controlled and wild settings.\n\n2.2. Objectives in studying animal behaviors\n\nStudying animal behavior offers manifold advantages, enriching our comprehension of the natural world and presenting practical applications across diverse domains, including neuroscience, pharmacology, medicine, agriculture, ecology, and robotics. Six key advantages of studying animal behavior are identified:\n\n(1)\n\nBiodiversity Conservation: Understanding animal behavior is crucial for the conservation of biodiversity (Hou et al., 2020; Nilsson et al., 2020; Wijeyakulasuriya et al., 2020; Ditria et al., 2020; Pillai et al., 2023). Knowledge of behaviors such as migration patterns, feeding habits, and reproductive strategies is essential for designing effective conservation strategies and protecting endangered species.\n\n(2)\n\nEcological Understanding: Animal behavior provides insights into the ecological dynamics of ecosystems (Ak√ßay et al., 2020; Gotanda et al., 2019). Behavioral studies help researchers understand how animals interact with their environment, including their roles in nutrient cycling, seed dispersal, and predator-prey relationships (Nasiri et al., 2023; Chen et al., 2020; Yamada et al., 2020).\n\n(3)\n\nHuman Health and Medicine: Studying animal behavior can have implications for human health and medicine (Shaw and Lahrman, 2023; Hart, 2011; Gnanasekar et al., 2022; Manduca et al., 2023). For example, research on animal models helps in understanding certain diseases and developing potential treatments. Behavioral studies on animals also contribute to our understanding of the neurobiology and psychology that underlie human behavior (Mathis and Mathis, 2020; Coria-Avila et al., 2022; Saleh et al., 2023).\n\n(4)\n\nAnimal Welfare and Husbandry: Knowledge of animal behavior is essential for promoting the welfare of domesticated animals and optimizing their husbandry practices (Jiang et al., 2021; Bao and Xie, 2022). Understanding how animals express natural behaviors can inform the design of environments that support their physical and psychological well-being (Tassinari et al., 2021; Manoharan, 2020; Jiang et al., 2020).\n\n(5)\n\nPest Control and Agriculture: Although right now it is very limited to pest identification, understanding the behavior of pest species can aid in the development of effective pest control strategies in agriculture (Coulibaly et al., 2022; J√∫nior and Rieder, 2020; Mendoza et al., 2023). This knowledge helps farmers manage crop damage and reduce the need for harmful pesticides (Teixeira et al., 2023; Mankin et al., 2021).\n\n(6)\n\nUnderstanding Social Dynamics: Observing social behaviors in animals can provide insights into the principles governing social structures and interactions (Xiao et al., 2023; Papaspyros et al., 2023). This knowledge can have applications in fields such as sociology and psychology, contributing to our understanding of social dynamics in general (Alameer et al., 2022; Perez and Toler-Franklin, 2023; Landgraf et al., 2021).\n\nIn this context, deep learning plays a pivotal role and emerges as a major technology in advancing the field, opening new opportunities. Addressing the limitations discussed in the previous section, we will now elaborate on the advantages that deep learning and computational technologies bring to the study of animal behavior.\n\nWe previously emphasized the integral aspect of employing sensors for data collection in animal behavior studies, which may induce stress and high noise levels. The use of multiple and diverse sensors for data acquisition, coupled with advanced architectures incorporating fusion layers, has been shown to mitigate noise and enhance the precision of analysis (Mahmud et al., 2021). However, attaching sensors directly to animals may introduce bias, prompting researchers in livestock health assessment and neuroscience to adopt computer vision. The ability of computer vision to provide real-time, non-invasive, and accurate animal-level information through the use of cameras has gained popularity (Oliveira et al., 2021). Nevertheless, this approach is limited to setups within the camera frame, except for innovations like Haalck et al.‚Äôs (Haalck et al., 2020) moving camera that tracks animals, creating a dynamic map of their environment. In larger scenarios, such as meadows where cows graze, sensors remain preferable. Nevertheless, collecting and analyzing sensor data from mobile devices on animals proves challenging and time-consuming. To address this, Dang et al. (Dang et al., 2022) introduced Long Range Area Network (LoRaWAN), where sensors attached to cows connect to gateways transmitting information to the cloud. This not only overcomes the limitations of computational power associated with mobile devices but also ensures continuous, real-time data analysis.\n\nThe efficacy of deep learning is contingent on annotated data, especially for supervised approaches. While manual annotation remains unavoidable, in tasks such as pose estimation and classification, labeling can be iterative. This involves annotating a small portion of the dataset, training a network, predicting on new images, correcting labels, and repeating this process multiple times. This iterative approach accelerates the labeling process, as demonstrated by Pereira et al. (Pereira et al., 2019). Another approach is to generate artificial labels (Li and Lee, 2023).\n\nTable 1 succinctly encapsulates a consolidated overview of both primary limitations and advantages, providing a discerning reference for researchers navigating the sophisticated landscape of animal behavior studies.\n\n2.3. Research questions\n\nThe survey aims to address the following research questions:\n\nRQ1 Which animal species are less considered and why?\n\nThis research question seeks to investigate overlooked or less studied animal species in the context of behavior analysis. To address this question, the survey will explore existing literature and research to identify trends and biases in the selection of animal subjects. The objective is to understand why certain species receive less attention and to gain insights into potential gaps in knowledge, aiding the development of more comprehensive and inclusive research strategies.\n\nRQ2 What deep learning methods have been used in the literature for animal behavior analysis?\n\nThis research question focuses on summarizing and categorizing the existing deep learning methods employed in the literature for animal behavior analysis. The survey will review a wide range of studies to identify and classify the various deep learning techniques applied to analyze animal behavior. The objective is to analyze existing methodologies to identify trends, strengths, and limitations of current approaches in the field.\n\nRQ3 What are the differences between human and animal behavior analysis?\n\nThis research question aims to highlight the distinctions between the analysis of human behavior and that of other animal species. The survey will compare methodologies, ethical considerations, and challenges specific to studying human and animal behavior.\n\nRQ4 What are the deep learning strategies that are suitable and could enhance this task, but are not yet exploited?\n\nThis research question looks forward, aiming to identify untapped potential in the application of deep learning to animal behavior analysis. The survey will involve a comprehensive review of the current literature to identify gaps or areas where deep learning strategies have not been extensively explored. This involves proposing novel applications of existing techniques or suggesting modifications to adapt deep learning methods for more effective analysis of animal behavior.\n\n3. Method for literature survey\n\nIn this section, we clarify the methodology applied in our survey. Our approach involved a thorough systematic review to carefully select the pertinent studies considered in this article. Following this, we accurately analyzed the gathered information, employing statistical methods to derive meaningful insights.\n\n3.1. Search and selection strategies\n\nThis section delineates the methodologies employed for data collection and synthesis. Initially, data acquisition was conducted through systematic searches on academic repositories, including Google Scholar, IEEE Xplore, and the Springer Database. The formulated search queries were as follows:\n\nanimal behavior AND deep learning\n\n(insect OR wild) AND behavior AND deep learning\n\nThe decision to employ distinct queries for insects and wild animals was necessitated by the observed paucity of literature in these categories relative to studies involving farm animals and neuroethology, commonly focused on mice. Thus, the formulation of specific queries was imperative to encompass a broader spectrum of animal species.\n\nSubsequently, the acquired data underwent systematic tabulation based on features explicated in Table 2. These features were derived from discerned patterns identified during a comprehensive analysis of the extant literature. Synthesizing the outcomes of this analysis, Sections 4 and 5 encapsulate the aggregated findings, summarizing the respective papers that expound upon solution methodologies grounded in pose estimation and those that do not.\n\nFinally, a judicious filtering operation was executed to extract only those articles germane to the objectives of this survey, resulting in 151 articles. Each article within this subset underwent thorough examination, and pertinent references therein were scrutinized and subsequently incorporated into our survey to enrich its content.\n\n3.2. Comprehensive science mapping analysis\n\n3.2.1. Annual scientific production\n\nIn the process of retrieving articles, our attention was exclusively directed towards research publications spanning the temporal spectrum from 2020 to 2023. Figure 1a elucidates this distribution through a histogram, illustrating the quantitative representation of papers across each respective year.\n\n3.2.2. Scientific production based on animal considered\n\nFigure 1b illustrates the distribution of percentages pertaining to the various animal species under consideration in the selected articles. Evidently, a predominant emphasis is placed on research concerning livestock, notably focusing on cows and pigs, as well as studies involving mice, related mostly to neuroscience.\n\n3.2.3. Research field of authors\n\nGiven that animal behavior analysis is an interdisciplinary and multidisciplinary field, it becomes imperative to comprehend the research background of individuals engaged in this domain. Despite the predominant focus on articles related to deep learning technologies, it is noteworthy that a considerable number of non-artificial intelligence practitioners are actively entering this field, as illustrated in Figure 1c. Interestingly, when combining ‚Äùcomputer science‚Äù (encompassing computer engineering) and ‚Äùartificial intelligence,‚Äù they constitute only 18% of the scholarly contributions. In contrast, bio-related fields, including biology, animal science, agriculture, veterinary, and ecology, collectively contribute 30% to the research landscape. Noteworthy is the active participation of various engineering fields, even those with a mechanical-electrical background, in the exploration of animal behavior. Additionally, a compelling correlation is observed in the fields of neuroscience and psychology, where the majority of articles are dedicated to the study of mice.\n\n4. Pose estimation-based methods\n\nPose estimation, the process of identifying and locating the position and orientation of objects, is a fundamental technique widely used in the examination of animal behaviors alongside object detection, as discussed in subsection 5.3. Originating from Human Pose Estimation (HPE), the evolution into Animal Pose Estimation (APE) was spearheaded by Mathis et al. through DeepLabCut (Mathis et al., 2018) and Pereira et al. via LEAP (Pereira et al., 2019), subsequently evolving into SLEAP (Pereira et al., 2022). This section delves into an in-depth analysis of these two methodologies juxtaposed with emerging trends within the field of research. Given the primary focus of our survey on animal behavior analysis, subsequent to the introduction of these predominant approaches, we elucidate the utilization of pose estimation outputs for behavior analysis and classification. For a more comprehensive understanding of animal pose estimation, we recommend perusing the survey conducted by Jiang et al. (Jiang et al., 2022a), published in 2022.\n\nLEAP (Pereira et al., 2019) is a single-animal pose estimation model employing convolutional layers culminating in confidence maps that delineate the probability distribution for each distinct body keypoint. This architectural design, depicted in Figure 2, is characterized by its simplicity, featuring three sets of convolutional layers. The initial two sets are terminated by max pooling to alleviate computational complexity. Subsequently, transposed convolution is applied to restore the original dimensions of the images, yet with a depth corresponding to the number of keypoints, thereby generating a confidence map for each. Despite its simplicity, the LEAP model encounters challenges in non-laboratory settings due to issues such as occlusion, prompting the introduction of T-LEAP (Russello et al., 2022). T-LEAP preserves the architecture of LEAP but diverges in its use of 3D convolution instead of 2D convolution. The input to T-LEAP comprises four consecutive frames extracted from videos, enhancing the model‚Äôs robustness. Notably, T-LEAP maintains a focus on single-animal pose estimation, as elucidated in Figure 2. Subsequently, the author of LEAP introduced a refined version known as Social LEAP (SLEAP) (Pereira et al., 2022), designed to proficiently address the challenges associated with multi-animal pose estimation through the integration of both bottom-up (Papandreou et al., 2018) and top-down strategies (Nguyen and Kresovic, 2022). In the top-down strategy, SLEAP first identifies individuals and subsequently detects their respective body parts. Unlike LEAP, SLEAP seamlessly incorporates this approach without the need for an additional object detection architecture. On the other hand, the bottom-up strategy in SLEAP involves detecting individual body parts and subsequently grouping them into individuals based on their connectivity. A key advantage of this dual-strategy framework is its efficiency, requiring only a single pass through the neural network. The output of this strategy produces multi-part confidence maps and part affinity fields (PAFs) (Cao et al., 2017), constituting vector fields that intricately represent spatial relationships between pairs of body parts. Additionally, SLEAP undergoes a structural enhancement by transitioning from LEAP‚Äôs backbone to a more intricate U-Net architecture (Ronneberger et al., 2015), thereby significantly improving accuracy in the realm of multi-animal pose estimation scenarios.\n\nSimilarly, DeepLabCut (DLC) (Mathis et al., 2018) has evolved significantly over time. Initially designed as a single-animal pose estimation method, it utilizes a pretrained ResNet-50 (He et al., 2016) backbone with subsequent deconvolutional layers to generate confidence maps for keypoints. This approach, taking advantage of Imagenet pretrained weights, allowed DLC to effectively estimate skeletons with minimal data. The model‚Äôs capabilities were later expanded to include 3D pose estimation through the use of multiple cameras (Nath et al., 2019). Each camera view was trained independently, and sophisticated camera calibration techniques were employed to derive 3D locations. A subsequent milestone in DLC‚Äôs development involved addressing the challenges of multi-animal pose estimation (Lauer et al., 2022). This evolution introduced DLCRNet, a structural modification that replaced the ResNet backbone. DLCRNet employs a bottom-up multi-animal pose estimation approach, featuring a multi-fusion architecture and a multi-stage decoder. The decoder utilizes multiple stages of score maps and PAFs (Cao et al., 2017) to predict keypoints for each animal. Further innovation is exemplified by SuperAnimal (Ye et al., 2022), which introduced transformer layers into the model architecture.\n\nWhile DLC (Mathis et al., 2018) and SLEAP (Pereira et al., 2022) currently stand as the predominant pose estimation methodologies in behavior analysis for animal behavior classification, it is imperative to acknowledge recent advancements in animal pose estimation architectures. Several notable methodologies have been introduced:\n\n‚Ä¢\n\nOptiFlex (Liu et al., 2020) is a video-based animal pose estimation method that, given a skip ratio sùë†sitalic_s and a frame range fùëìfitalic_f, assembles a sequence of 2‚Å¢f+12ùëì12f+12 italic_f + 1 images with indices ranging from t‚àís√ófùë°ùë†ùëìt-s\\times fitalic_t - italic_s √ó italic_f to t+s√ófùë°ùë†ùëìt+s\\times fitalic_t + italic_s √ó italic_f. This sequence is input to a model based on residual blocks with intermediate supervision, generating predictions for each image and producing a sequence of heatmap tensors. These tensors are then fed into an OpticalFlow model, ultimately yielding the final heatmap prediction for index tùë°titalic_t. OptiFlex has demonstrated superior accuracy compared to DeepLabCut (Mathis et al., 2018), LEAP (Pereira et al., 2019), and DeepPoseKit (Graving et al., 2019).\n\n‚Ä¢\n\nSemiMultiPose (Blau et al., 2022) introduces a semi-supervised multi-animal pose estimation approach, building upon DeepGraphPose (Wu et al., 2020) and DirectPose (Tian et al., 2019). Taking both labeled and unlabeled frames as input, the method processes them using a ResNet (He et al., 2016) backbone, generating a compact representation fed into three branches: one for detecting keypoint heatmaps (B1), one for bounding box heatmaps (B2), and a third for keypoint detection (B3). SemiMultiPose aims to generate pseudo keypoint coordinates from B2 and B3 for the self-supervised branch, contributing to B1. The network has shown improved accuracy compared to SLEAP (Pereira et al., 2022). However, the authors note that in cases of abundant labeled data, their method may not significantly outperform others, and for single-animal pose estimation with unlabeled frames from a sequential video, DeepGraphPose (Wu et al., 2020) might outperform SemiMultiPose (Blau et al., 2022), benefiting from the consideration of spatial and temporal information.\n\n‚Ä¢\n\nLightning Pose (Biderman et al., 2023) exploits spatiotemporal statistics of unlabeled videos in two ways. Firstly, it introduces unsupervised training objectives penalizing the network for predictions violating the smoothness of physical motion, multiple-view geometry, or departing from a low-dimensional subspace of plausible body configurations. Secondly, it proposes a novel network architecture predicting poses for a given frame using temporal context from surrounding unlabeled frames. The resulting pose estimation networks exhibit superior performance with fewer labels, generalize effectively to unseen videos, and provide smoother and more reliable pose trajectories for downstream analysis (e.g., neural decoding analyses) compared to previously mentioned approaches.\n\n‚Ä¢\n\nBhattacharya et al. (Bhattacharya and Shahnawaz, 2021) introduced a novel model for recognizing the pose of multiple animals from unlabeled data. The approach involves the removal of background information from each image and the application of an edge detection algorithm to the body of the animal. Subsequently, the motion of the edge pixels is tracked, and agglomerative clustering is performed to segment body parts. In a departure from previous methods, the end result is not specific keypoints but rather the segmentation of body parts. To achieve this, the authors utilized contrastive learning to discourage the grouping of distant body parts together.\n\nAfter obtaining the skeletal representation of each animal in every frame, whether from videos or images, the subsequent step involves processing the data to discern specific behaviors. The trajectories derived from pose estimation can be effectively analyzed through statistical methods. Weber et al. (Weber et al., 2022) utilized DeepLabCut predictions (Mathis et al., 2018) and ANOVA (Kaufmann and Schering, 2014) to conduct behavioral profiling of rodents, with a focus on studying stroke recovery. In a similar vein, Lee et al. (Lee et al., 2021), employing DLC (Mathis et al., 2018), investigated the behavior of non-tethered fruit flies. Their study involved predicting locomotion patterns and identifying the centroid of the animals‚Äô legs.\n\nMachine learning for analyzing pose estimation trajectories becomes crucial when classifying postures and relating them to specific behaviors. One of the simplest approaches is to use a Nearest-Neighbor classifier. Saleh et al. (Saleh et al., 2023) tested this method to classify mouse behaviors such as crossing and rearing in an open-field experiment, achieving 97% accuracy. Other machine learning approaches were employed by Fang et al. (Fang et al., 2021) and Nilsson et al. (Nilsson et al., 2020). The former used a naive Bayesian classifier to identify eating, preening, resting, walking, standing, and running behaviors for poultry analysis, providing a disease warning system. The latter introduced the SimBa toolkit, importing DeepLabCut (Mathis et al., 2018) or DeepPoseKit (Graving et al., 2019) projects to create classifiers using RandomForest (Breiman, 2001) and extracting features like velocities and total movements. McKenzie-Smith et al. (McKenzie-Smith et al., 2023) used trajectories obtained with SLEAP (Pereira et al., 2022) to identify stereotyped behaviors such as grooming, proboscis extension, and locomotion in Drosophila melanogaster, using resulting ethograms provided by MotionMapper (Berman et al., 2014) to explore how flies‚Äô behavior varies across time of day and days, finding distinct circadian patterns in all stereotyped behaviors.\n\nOther authors opted for recurrent and convolutional neural networks, with simple approaches such as using Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and 1D convolutional neural networks to process trajectories for drawing behavioral conclusions. Examples include detecting lameness in horses (Alagele and Yildirim, 2022) and determining chemical interactions experienced by crickets (Fazzari et al., 2023). More complex approaches include Wittek et al. (Wittek et al., 2023)‚Äôs use of InceptionTime (Ismail Fawaz et al., 2020), an ensemble of deep convolutional neural network models, to classify seven distinct behaviors in birds. Some authors simplified the classification process by introducing a non-linear clustering phase to improve the feature space, followed by classification using Multilayer Perceptrons (MLP), demonstrating advantages in classification (Ye et al., 2022; Schneider et al., 2023).\n\nA recent emerging trend involves the utilization of unsupervised learning techniques in the analysis of animal behavior. Luxer et al. (Luxem et al., 2022) have innovatively proposed a methodology for processing trajectories derived from DeepLabCut (Mathis et al., 2018) by employing a Variational Auto-Encoder (VAE) (Kingma and Welling, 2013). Subsequently, they apply a Hidden Markov Model (HMM) (Rabiner and Juang, 1986) to the new representation of trajectories to discern underlying motifs. Following a comprehensive analysis of motif usage, the authors iteratively employ HMM, limiting the number of motifs to those surpassing a 1% usage threshold in the previous analysis. The refined motifs were attributed to specific behavior exhibited by the mice, such as exploration, rearing, grooming, pausing, or walking. Notably, this methodological approach outperforms conventional techniques, such as Auto-Regressive HMM (AR-HMM) or MotionMapper (Berman et al., 2014), when applied directly to the motion sequences.\n\nMotion trajectories extend their utility beyond predicting the behavior of individual animals; in multi-animal scenarios, they can also be applied to unravel the intricate web of social interactions among them. Segalin et al. (Segalin et al., 2021) introduced the Mouse Action Recognition System (MARS), a sophisticated automated pipeline tailored for pose estimation and behavior quantification in pairs of freely interacting mice. MARS adeptly discerns three specific social behaviors: close investigation, mounting, and attack. On a different note, Zhou et al. (Zhou et al., 2022) proposed the Cross-Skeleton Interaction Graph Aggregation Network (CS-IGANet), a groundbreaking framework designed to capture the diverse dynamics of freely interacting mice. CS-IGANet successfully identifies a spectrum of behaviors, including approaching, attacking, chasing, copulation, walking away from another mouse, sniffing, and many others.\n\nTrajectories not only serve as a means to identify specific behaviors but are also instrumental in anomaly detection. For instance, Fujimori et al. (Fujimori et al., 2020) employed OneClassSVM (Boser et al., 1992) and IsolationForest (Liu et al., 2008) to detect outlier behaviors in domestic cats. Similarly, Gnanasekar et al. (Gnanasekar et al., 2022) utilized pose estimation data to predict abnormal behavior in mice undergoing opioid withdrawal, employing pretrained convolutional neural networks for the classification of shaking behaviors.\n\n5. Non pose estimation-based methods\n\nIn this section, we expound upon methodologies employed in the investigation of animal behaviors without recourse to pose estimation techniques. To enhance clarity and systematic presentation, we have delineated subsections corresponding to each methodology.\n\n5.1. Sensor based approaches\n\nSensor-generated data, typically originating from accelerometers or gyroscopes, has been extensively explored in the literature, as comprehensively in (Kleanthous et al., 2022a; Neethirajan, 2020) surveys. These surveys delve into the application of classical machine learning methods in modern animal farming and the study of animal behavior. More recently, a shift towards leveraging deep learning approaches has been observed. Arablouei et al. (Arablouei et al., 2023b) utilized a wearable collar tag equipped with an accelerometer to collect data from grazing beef cattle. They applied a Multi-Layer Perceptron to classify behaviors such as grazing, walking, ruminating, resting, and drinking. Similarly, Eerdekens et al. (Eerdekens et al., 2020) employed tri-axial accelerometers on horses, strategically positioned at the two front legs‚Äô lateral side. They proposed a Convolutional Neural Network to detect behaviors like standing, walking, trotting, cantering, rolling, pawing, and flank-watching based on the acquired data. Mekruksavanich et al. (Mekruksavanich et al., 2022), instead, segmented accelerometer data into 2-second windows and exploited a pre-trained ResNet model (He et al., 2016) to perform sheep activity recognition. Dang et al. (Dang et al., 2022) introduced the integration of multiple sensors, collecting environmental data (e.g., temperature, humidity) alongside cow behavior information obtained from accelerometers and gyroscopes. They preprocessed this information using a 1D-convolutional neural network and LSTM networks for classifying walking, feeding, lying, and standing. In a recent study, Pan et al. (Pan et al., 2023) introduced four novel Convolutional Neural Network architectures tailored for Animal Action Recognition (AAR). These architectures, namely one-channel temporal (OCT), one-channel spatial (OCS), OCT and spatial (OCTS), and two-channel temporal and spatial (TCTS) networks, leverage data from 3D accelerometers and 3D gyroscopes. The core objective of their research was to mscrupulously identify behaviors such as movement, drinking, eating, nursing, sleeping, and lying in lactating sows.\n\nIn addition to accelerometer and gyroscope data, GNSS (Global Navigation Satellite System) data emerges as a valuable tool for understanding animal behavior. Arablouei et al. (Arablouei et al., 2023a) explored this avenue by employing GNSS to extract pertinent information about cattle behavior, including metrics like distance from water points, median speed, and median estimated horizontal position error. Integrating this GNSS data with accelerometry information, the researchers pursued two distinct approaches. The first involved concatenating features from both sensor datasets into a comprehensive feature vector, subsequently fed into a MLP classifier. Alternatively, the second approach centered on fusing the posterior probabilities predicted by two separate MLP classifiers. These methodologies enabled the accurate detection of behaviors such as grazing, walking, resting, and drinking.\n\n5.2. Bioacoustics\n\nWhile bioacoustics offers a captivating glimpse into animal behavior (Stowell, 2022), given the integral role of sound in animal activities such as communication, mating, navigation, and territorial defense (Chalmers et al., 2021), the current landscape of published articles predominantly emphasizes animal identification (Xu et al., 2020; Bravo Sanchez et al., 2021; Varma et al., 2021) and sound event detection (Nolasco et al., 2023; Moummad et al., 2023). Notably, the existing literature reveals a scarcity of research endeavors combining acoustics and deep learning for the identification of animal behaviors. Wang et al. (Wang et al., 2021b) stand out as pioneers in this domain, as they endeavored to classify sheep behaviors, including chewing, biting, chewing-biting, and ruminating sounds. This was accomplished using a recording device positioned proximal to the animal‚Äôs face, with a placebo class designated as noise. The acquired wavelet data were leveraged for classification tasks through both a feed-forward neural network and a recurrent neural network. Additionally, the information was further processed by transforming it into a log-scaled Mel-spectrogram, serving as input for a convolutional neural network. The findings underscore that while the recurrent neural network exhibited superior performance, the convolutional neural network outperformed the feed-forward approach, attributing its success to the enhanced signal representation offered by the Mel-spectrogram.\n\n5.3. Object Detection\n\nIn conjunction with pose estimation techniques, object detection stands out as a widely employed deep learning methodology for analyzing animal behavior. Its prevalence may be attributed to its established utility in animal recognition and detection (Chen et al., 2021; Teixeira et al., 2023; Banerjee et al., 2023), prompting researchers to redirect their focus toward studying animal welfare and activity.\n\nAmong the leading architectures for animal behavior identification, Faster R-CNN (Ren et al., 2015) and particularly YOLO (Redmon et al., 2016) are frequently employed. Nonetheless, alternative architectures have been proposed. For instance, Samsudin et al. (Samsudin et al., 2022) utilized SSD MobileNetv2 (Sandler et al., 2018) to detect abnormal and normal zebrafish larvae behaviors for examining the effects of neurotoxins. McIntosh et al. (McIntosh et al., 2020) introduced TempNet, incorporating an encoder bridge and residual blocks with a two-staged spatial-temporal encoder, to detect startle event in fish.\n\nObject detection serves a dual role, encompassing instantaneous behavior detection through image or video frame analysis, as well as the quantification and tracking of specific behaviors. The accurate analysis of single frames, counting, and frame-by-frame examination enable researchers to quantify both the duration and frequency of distinct actions. For instance, the application of YOLO in the study by Alameer et al. (Alameer et al., 2022) facilitated the quantification of contact frequency among pigs, allowing the identification of peculiar behaviors such as rear snorting and tail-biting. In the context of cows and pigs, a crucial aspect involves quantifying movement and aggressive behavior (Alameer et al., 2022; Odo et al., 2023). Furthermore, efforts to discern rank relationships based on fighting behavior in animals like cows are of crucial importance (Uchino and Ohwada, 2021). Importantly, for tasks demanding prolonged animal identification, tracking is conventionally executed using DeepSort (Wojke et al., 2017; Evangelista et al., 2022).\n\nEfficient instant detection can be accomplished by conducting a single analysis on the animal and directly classifying its behavior through a single image. In this context, deep learning object detection models prove instrumental in directly identifying behaviors such as positional activities (e.g., mating, standing, feeding, spreading, fighting, drinking) for the comprehensive analysis of animal health and stress behaviors (Riekert et al., 2020; Manoharan, 2020; Wang et al., 2020). These models also find application in disease identification, such as the detection of wryneck (Elbarrany et al., 2023), and in studying behavioral adaptations to new environments (Li et al., 2019b). Furthermore, object detection models can be extended to operate with thermal and infrared images. For example, Xudong et al. (Xudong et al., 2020) utilized thermal images for the automatic recognition of dairy cow mastitis, introducing the EFMYOLOv3 model. Similarly, Lei et al. employed infrared images to discern feeding, resting, moving, and socializing behaviors in slow animals (Lei et al., 2022).\n\nBeyond these applications, notable approaches utilizing object detection include Fuentes et al. (Fuentes et al., 2020), who integrated YOLO and Optical Flow to detect actions in cows. Additionally, some researchers employ object detection solely for localizing the animal within the image or video. They subsequently crop that region and use it in other models, leveraging 2D pretrained networks or introducing 3D convolutional neural networks for video analysis (Feighelstein et al., 2023; Thanh and Netramai, 2022).\n\n5.4. Others\n\nSeveral research endeavors have employed unique deep learning methodologies, distinct from those discussed in the preceding section. Due to the relative scarcity of deep learning strategies for evaluating animal behavior in the existing literature using the aforementioned approaches, we endeavored to compile a comprehensive assortment of ideas. To achieve this, we have identified and categorized five distinct approaches:\n\n‚Ä¢\n\nConvolutional Classification on Raw Data. Alameer et al. (Alameer et al., 2020) employed a GoogLeNet-like architecture(Szegedy et al., 2015) to discern between feeding and non-nutritive visits to a manger in pig recordings. In a similar vein, Ayadi et al. (Ayadi et al., 2020) utilized VGG19 (Simonyan and Zisserman, 2014) to determine whether cows were ruminating or not. This network architecture was also applied to identify various behaviors in mice, such as grooming, licking the abdomen, squatting, resting, circling, wandering, climbing, and searching (Wang et al., 2021a). Similarly leveraging pre-trained networks, Andresen et al. (Andresen et al., 2020) developed a fully automated system for surveilling post-surgical and post-anesthetic effects in mice facial expressions, employing InceptionV3 (Szegedy et al., 2016) for pain identification. Notably, Bohnslav et al. (Bohnslav et al., 2021) introduced DeepEthogram, a software tested for predicting mice and flies behaviors. The approach involves using a sequence of 11 frames, where the last frame is the target for prediction. Optical flow frames are generated using MotionNet (Zhu et al., 2019). These frames, along with the target frame, are fed into a feature extractor (ResNet architectures (He et al., 2016; Hara et al., 2018)) to extract both flow and spatial features. Subsequently, the features are concatenated, and Temporal Guassian Mixture (TGM) model (Piergiovanni and Ryoo, 2019) is applied for classification. Han et al. (Han et al., 2020) employed a simpler approach, superimposing the frame to be predicted with computationally generated optical flow from the subsequent frame. The resulting image is then classified using a convolutional neural network to categorize behaviors in fish shoals, including normal state, group stimulated, individual disturbed, feeding, anoxic, and starvation state.\n\n‚Ä¢\n\nSegmentation. Xiao et al.(Xiao et al., 2023) employed Mask R-CNN (He et al., 2017) to segment birds within a 3D space, facilitating the analysis of their interactions based on distinct social actions: approach, stay, leave, and sing to. In contrast, other researchers have devised innovative pipelines to investigate animal behavior. EthoFlow(Bernardes et al., 2021) is a software grounded in segmentation, enabling the tracking and behavioral analysis of organisms (validated on bee datasets). On a different note, SIPEC (Marks et al., 2022) constitutes a pipeline leveraging an Xception network (Chollet, 2017) to extract features from frames. These features are subsequently processed over time using a Temporal Convolutional Network (TCN) (Lea et al., 2016) to classify the animal‚Äôs behavior in each frame. While SIPEC abstains from segmentation in the case of single animal classification, it seamlessly incorporates segmentation for multi-animal behavior classification.\n\n‚Ä¢\n\nSelf-Supervised Learning. Jia et al. (Jia et al., 2022) proposed an innovative self-supervised learning approach known as Selfee, designed for extracting comprehensive and discriminative features directly from raw video recordings of animal behaviors. Selfee utilizes a pair of Siamese convolutional neural networks(Koch et al., 2015), trained explicitly to generate discriminative representations for live frames.\n\n‚Ä¢\n\nExplainability. To the best of our knowledge, Choi et al. (Choi et al., 2022) stands as the sole contributor employing Explainable Artificial Intelligence (XAI). In their study, they harnessed Grad-CAM (Selvaraju et al., 2016) to delve into the decision-making process of a neural network designed to distinguish between unstable and stable ant swarms. The investigation aimed to ascertain the network‚Äôs capacity to comprehend intricate behaviors such as dueling and dominance biting, shedding light on the explainability of its predictions.\n\n‚Ä¢\n\nBehavior identification in clips. Li et al. (Li et al., 2020) undertook the task of categorizing significant pig behaviors, such as feeding, lying, motoring, scratching, and mounting. Their approach involved the development of Pig‚Äôs Multi-Behavior recognition (PMB-SCN), a sophisticated architecture built upon the SlowFast framework (Feichtenhofer et al., 2019) and leveraging spatio-temporal convolution. PMB-SCN comprised two distinct SlowFast pathways with varying temporal speeds. The slow pathway utilized a larger temporal stride when processing input frames (e.g., 8, considering a clip with a length of 64 frames), while the fast pathway employed a smaller temporal stride (e.g., 2). The features extracted by these pathways were interconnected through lateral connections (Lin et al., 2017), enhancing the model‚Äôs ability to capture complex spatio-temporal patterns. The final phase of the methodology involved classification, where the fused features were utilized to discern and categorize various pig behaviors effectively.\n\n6. Publicly available datasets\n\nThis section meticulously enumerates publicly accessible datasets featured or referenced in the articles identified through our systematic search. Table 3 presents details on each dataset, including the article of introduction, authorship, targeted species, data type (e.g., images, videos, audio signals, sensor data), the specific tasks for which they were utilized and the content of the datasets. Noteworthy is the incorporation of references indicating the dual usage of datasets‚Äîinitially introduced for a specific task and subsequently repurposed, signified by citations in the Application column. Regrettably, several articles utilized private datasets, although some authors may offer dataset-sharing options. We recommend consulting the corresponding articles to explore potential data access avenues.\n\nA number of key considerations can be gleaned from Table 3, as follows:\n\n‚Ä¢\n\nIn the context of datasets tailored for pose estimation, a salient observation is the standardization of skeletal structures when deployed across diverse animal species (Cao et al., 2019; Yu et al., 2021; Ng et al., 2022). This standardization facilitates the training of a singular network, avoiding the need for species-specific networks. Conversely, datasets exclusive to individual animal species exhibit intricate skeletal configurations tailored to the anatomical nuances of that species. For instance, precision in detecting the distal and proximal ends of crickets‚Äô antennae may be achievable (Fazzari et al., 2022), but such granularity may not translate to not insect species like horses.\n\n‚Ä¢\n\nThe inclination of animals like goats and birds to move in open fields compels researchers to rely on sensor data or alternative methods, such as audio recordings, for event and action detection. This approach is significantly more manageable than tracking the animals with cameras. However, the utilization of sensors is constrained by the availability and affordability of these devices, directly impacting the number of individuals involved and the quantity of sequences that can be compiled for the dataset.\n\n‚Ä¢\n\nFor identifying static positions, such as whether an animal is lying down or standing, still images suffice. However, capturing and analyzing videos, or more precisely, short video clips, is crucial for recognizing dynamic actions and behaviors. These clips are intentionally brief to focus solely on the relevant action event, ensuring accurate classification using deep learning techniques. This approach effectively eliminates extraneous or unrelated behaviors that may interfere with the identification of the specific behavioral instance. This is the rationale behind Yang et al.‚Äôs decision to utilize 15 frames for each video clip (Yang et al., 2022).\n\n‚Ä¢\n\nUnfortunately, publicly available collective and social behavior prediction and analysis datasets are currently limited to mice and fish, even though we discussed a study in this survey that utilized explainable artificial intelligence to analyze ant behavior. This innovative research methodology relies heavily on video data, presenting a computational challenge that demands substantial processing efforts. The intricate nature of this approach necessitates a considerable investment of time for meticulous frame and event labeling, thereby slightly diminishing its overall research appeal and popularity.\n\n‚Ä¢\n\nAn essential consideration pertains to the primary focus of many databases, which primarily aim at identification, detection, pose estimation, and tracking. Despite this orientation, it is crucial to acknowledge that several datasets have been instrumental in behavioral analysis, even if not explicitly designed for such purposes. Researchers are strongly encouraged not to overlook animal datasets merely because they do not pertain to specific behaviors. Valuable insights can be gleaned from these datasets, and their broader applicability should be explored beyond their initially intended scope.\n\n7. Discussion about research questions\n\nIn concluding our extensive survey, we undertake the task of responding to the research questions outlined in subsection 2.3, drawing upon the insights gleaned from the studies expounded upon earlier. These responses aim to offer readers practical guidance in navigating the dynamic trends discerned from the comprehensive examination of the state-of-the-art literature. Their purpose is to serve as a compass for readers, facilitating a deeper comprehension of emerging patterns and fostering the implementation of advancements in the field of animal behavior.\n\nRQ1 (Which animal species are less considered and why?): In the context of animal behavior analysis employing deep learning, farm animals take center stage, as illustrated in Figure 1. Pigs and cows emerge as prominent subjects, while mice claim a noteworthy position owing to their significance in neuroscience research (Bryda, 2013). Despite chickens being the most widely farmed animals globally, surpassing even cows, sheep, and goats (Robinson et al., 2014), their consideration in behavior analysis appears relatively subdued. This discrepancy may be attributed to the limited nature of chicken behavior, encompassing mostly activities such as sleeping, moving, and eating. They are predominantly analyzed for welfare-related assessments (Mohialdin et al., 2023; Joo et al., 2022).\n\nGoats and sheep, less scrutinized in behavioral studies, likely owe their lower visibility to their outdoor grazing habits, which hinder the feasibility of employing image processing techniques. Unlike pigs, which are often studied within confined spaces, the vast open areas in which goats and sheep are typically raised limit the practicality of utilizing image processing, even with occasional drone deployment (Al-Thani et al., 2020). Birds face a similar challenge, requiring continuous tracking or data collection from sensors for comprehensive analysis (Bergen et al., 2022).\n\nFor aquatic creatures like fishes, a distinct hurdle arises from the difficulty in training neural networks on underwater images. These images often suffer from poor quality due to distortion and color/contrast loss in water, necessitating an image enhancement phase for meaningful analysis (Saleh et al., 2022).\n\nA notable observation is the limited consideration given to domestic animals in this research context (Choi et al., 2021; Lecomte et al., 2021; Chambers et al., 2021; Kasnesis et al., 2022). This may stem from the scarcity of veterinary professionals engaged in this evolving field or ethical concerns surrounding the study of domestic animals.\n\nRQ2 (What deep learning methods have been used in the literature for animal behavior analysis?): Throughout this survey, we delineate two distinct methodologies employed for the analysis of animal behaviors: pose estimation and non-pose estimation methods. Pose estimation-based approaches hinge on the analysis of trajectories traced by keypoints, with subsequent utilization of various machine and deep learning techniques to scrutinize behavior. Predominantly, recurrent neural networks and 1D convolutional neural networks are the favored deep learning methods, though recent applications have also embraced variational auto-encoders for unsupervised motif identification (Kingma and Welling, 2013; Luxem et al., 2022) and convolutional graph networks for interaction analysis (Zhou et al., 2022). However, a prevalent trend emerges wherein data is often processed through statistical analysis or traditional machine learning methods (Saleh et al., 2023; Fang et al., 2021; Nilsson et al., 2020; McKenzie-Smith et al., 2023). This inclination may stem from the fact that many researchers are not inherently engaged in artificial intelligence or data science, as mentioned earlier (see subsection 3.2), or it could be influenced by the volume of available data, given the heightened data requirements of deep learning (√ñzda≈ü et al., 2023). Despite the migration of classification tasks and behavior discovery to deep learning, certain aspects, such as behavior outlier detection, persist in employing classical machine learning approaches like OneClassSVM (Boser et al., 1992) and IsolationForest (Liu et al., 2008). On the other hand, non-pose estimation encompasses diverse applications, categorized based on the type of data: sensors, audio and video, and image data. Sensor data commonly undergoes processing through (1D)-convolutional or recurrent neural networks (Eerdekens et al., 2020; Mekruksavanich et al., 2022; Pan et al., 2023; Dang et al., 2022), and in some cases, multi-layer perceptron classifiers (Arablouei et al., 2023b), particularly when sensor data is transformed into features like velocity, angles, humidity, location, among others (Arablouei et al., 2023a). In the domain of bioacoustics, recurrent neural networks or pre-trained convolutional neural networks on spectrogram images are frequently applied (Wang et al., 2021b). For images and videos, processing methods vary according to the task at hand. They may be employed for object detection, where identification extends beyond the animal to encompass specific behaviors, typically achieved through frameworks such as YOLO (Redmon et al., 2016) and Faster R-CNN (Ren et al., 2015). Alternatively, pre-trained neural networks or segmentation techniques, with subsequent analysis of the segmentation mask, are utilized (Xiao et al., 2023). Figure 3 and Figure 4 encapsulate and illustrate the summarized pose and non-pose estimation methods for behavior analysis expounded in this survey.\n\nRQ3 (What are the differences between human and animal behavior analysis?): Deep learning for animal behavior analysis draws inspiration from and translates methodologies used in human applications. While similarities exist in predicting final classification outputs, notable differences emerge in data handling, particularly with sensor data. Moreover, a distinct gap is evident in pose estimation methods. In the following list, we highlight these divergences that present opportunities for leveraging deep learning in animal behavior analysis:\n\n‚Ä¢\n\nPose estimation (regression). In animal pose estimation, regression-based methodologies have not garnered widespread acclaim as observed in human applications. This divergence in popularity can likely be attributed to their utilization predating the ascendancy of heatmap-based techniques (Zheng et al., 2023). Conversely, within the human context, these regression approaches have demonstrated noteworthy success, particularly when seamlessly integrated with multi-task learning techniques (Ruder, 2017). By facilitating the exchange of information between interconnected tasks, such as pose estimation and pose-based action recognition, models can enhance their generalization capabilities. For example, Fan et al. (Fan et al., 2015) introduced an innovative dual-source convolutional neural network, employing both image patches and complete images for two distinct tasks: joint detection, responsible for discerning whether a patch contains a body joint, and joint localization, aimed at pinpointing the precise location of the joint within the patch. Each task is associated with its dedicated loss function, and the synergistic combination of these tasks yields notable improvements in overall performance.\n\n‚Ä¢\n\nPose estimation (heatmap). While these approaches find application in the realm of animal behavior, we believe that leveraging the following methods employed in human pose estimation could significantly augment this field:\n\nUsing pre-trained an general models based on transformers. Xu et al. (Xu et al., 2023) introduced VitPose++, an extension of the VitPose model (Xu et al., 2022), specifically designed for generic body pose estimation. This innovative framework utilizes vision transformers and distinguishes itself by not only focusing on human subjects but also extending its application to animals. The training process incorporates the AP-10K (yu2021ap) and APT36K (Yang et al., 2022) datasets, ensuring a comprehensive understanding of both human and animal poses. We believe that fine-tuning pre-trained models of this kind may improve animal pose estimation results.\n\nUsing Generative Adversarial Networks (GANs) (Goodfellow et al., 2014). The benefit can be twofold. GANs are explored in human pose estimation to generate biologically plausible pose configurations and to discriminate the predictions with high confidence from those with low confidence, which could infer the poses of the occluded body parts (Chen et al., 2017; Chou et al., 2018). A valuable aspect that could be used also in our task, solving possible occlusion issue in open-field research. Another usage is to perform adversarial data augmentation treating the pose estimation network as a discriminator and using augmentation networks as a generator to perform adversarial augmentations (Peng et al., 2018).\n\n‚Ä¢\n\nSmart handling of sensor data. The significance of detecting human behaviors through sensor technologies has grown significantly in recent years (Helmi et al., 2023; Park et al., 2023a; Islam et al., 2023). This has prompted researchers to critically evaluate the relevance of various sensors employed in behavior classification through deep learning techniques. The central question revolves around the necessity of each sensor in contributing to accurate behavior identification and whether a contribution significance analysis can be effectively employed in this context. Addressing this concern, Li et al. (Li et al., 2023) introduced a novel method aimed at optimizing sensor selection. Their approach involves assessing the self-information brought by the jth sensor concerning the occurrence of a specific activity Aisubscriptùê¥ùëñA_{i}italic_A start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and multiplying it by the universality of the same sensor j during instances of the same activity Aisubscriptùê¥ùëñA_{i}italic_A start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. This innovative strategy proved to be highly effective, resulting in improved recognition rates and reduced time consumption, thanks to the elimination of redundant and noisy data.\n\n‚Ä¢\n\nQuantifying data scarcity. As highlighted in the limitations section (refer to subsection 2.1), the collection of behavior data is frequently a laborious and resource-intensive process. Consequently, estimating dataset size has garnered attention in the field of Human Action Recognition (HAR) as an integral component of data collection planning. The objective is to mitigate the time and effort invested in behavior data collection while ensuring precise estimates of model parameters. For example, Hossain et al. (Hossain et al., 2023) introduced a method grounded in Uncertainty Quantification (UQ) (Abdar et al., 2021) to determine the optimal amount of behavior data required for obtaining accurate estimates of model parameters when modeling human behaviors as a Markov Decision Process (Bellman, 1957). Technologies of this nature have the potential to expedite research endeavors by facilitating more efficient workflows.\n\nRQ4 (What are the deep learning strategies that are suitable and could enhance this task, but are not yet exploited?): Exploring animal behavior through deep learning is just one facet of the multifaceted study in ethology and neuroscience. Beyond mere identification, understanding the decision-making processes and the emergence of new behaviors in animals is of paramount importance (Laboratory et al., 2021). This survey focuses on methodologies primarily centered around detecting behavioral classes or, in the case of unsupervised learning, identifying behavioral motifs. These motifs are then grouped into behavioral classes based on similarity, often with the assistance of human experts. However, animals, much like humans, exhibit dynamic changes in their behavior over time There is a growing need for methods that can efficiently capture these trial-to-trial changes, breaking them down into a learning component and a noise component (Ashwood et al., 2020). This is where reinforcement learning plays a pivotal role. Not only does it allow the perception of shifts in animal behavior and provide examples of biological learning algorithms, but it also enables the emulation of animal movements. This emulation has given rise to digital twins (Liu et al., 2022), providing a valuable tool for a more comprehensive study of animal behavior thanks to the ease of data acquisition. The convergence of deep learning strategies and reinforcement learning opens up exciting possibilities, paving the way for the development of interactive simulacra of animal behavior, akin to advancements already achieved in human behavior studies (Park et al., 2023b). This innovative approach empowers researchers to simulate how animals might adapt their behavior in diverse environments while interacting with various agents, be they other animals, humans, or even robots (Yamaguchi et al., 2018; Mori et al., 2022; Romano and Stefanini, 2021). Ultimately, the fusion of deep learning and reinforcement learning holds the promise of creating dynamic, interactive simulations that significantly deepen our understanding of the nuances of animal behavior across a spectrum of contexts.\n\n8. Conclusions\n\nIn summary, this survey rigorously examined the manifold benefits associated with the application of deep learning methodologies in the identification of animal behavior. Commencing with a detailed elucidation of the underlying motivations, limitations, objectives, and pertinent research inquiries steering the integration of deep learning within this domain, we established a robust contextual framework. Subsequently, our scrutiny extended to a thorough review of contemporary techniques, systematically categorized into pose and non-pose estimation methodologies. Within these delineations, we expounded upon a spectrum of methodologies, encompassing sequence processing in pose estimation and biocoustics, direct classification utilizing convolutional neural networks, outliers detection, convolutional graph neural networks, object detection, segmentation strategies, self-supervised learning, explainability, and unsupervised learning. Moreover, we curated a comprehensive table of publicly available datasets relevant to animal behavior, thereby augmenting the practical utility of deep learning applications. Our discourse on the subject and prospective considerations has pinpointed extant challenges within the literature, proffering a roadmap for potential research trajectories conducive to the advancement of the field. In essence, this survey serves as an invaluable compendium for researchers spanning diverse domains, with particular relevance to ethologists and neuroscientists. We believe that this survey will function as a guiding beacon, steering forthcoming research initiatives and fostering advancements in the intricate domain of animal behavior studies using deep learning.\n\nReferences\n\n(1)\n\nAbdar et al. (2021) Moloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan, Li Liu, Mohammad Ghavamzadeh, Paul Fieguth, Xiaochun Cao, Abbas Khosravi, U Rajendra Acharya, et al. 2021. A review of uncertainty quantification in deep learning: Techniques, applications and challenges. Information fusion 76 (2021), 243‚Äì297.\n\nAk√ßay et al. (2020) H√ºseyin G√∂khan Ak√ßay, Bekir Kabasakal, Duygug√ºl Aksu, Nusret Demir, Melih √ñz, and Ali Erdoƒüan. 2020. Automated bird counting with deep learning for regional bird distribution mapping. Animals 10, 7 (2020), 1207.\n\nAl-Thani et al. (2020) Najla Al-Thani, Alreem Albuainain, Fatima Alnaimi, and Nizar Zorba. 2020. Drones for sheep livestock monitoring. In 2020 IEEE 20th Mediterranean Electrotechnical Conference (MELECON). IEEE, 672‚Äì676.\n\nAlagele and Yildirim (2022) Mohammed Alagele and Remzi Yildirim. 2022. ANIMAL GAIT IDENTIFICATION USING A DEEP LEARNING METHOD. In 2022 International Symposium on Multidisciplinary Studies and Innovative Technologies (ISMSIT). IEEE, 540‚Äì542.\n\nAlameer et al. (2022) Ali Alameer, Stephanie Buijs, Niamh O‚ÄôConnell, Luke Dalton, Mona Larsen, Lene Pedersen, and Ilias Kyriazakis. 2022. Automated detection and quantification of contact behaviour in pigs using deep learning. biosystems engineering 224 (2022), 118‚Äì130.\n\nAlameer et al. (2020) Ali Alameer, Ilias Kyriazakis, Hillary A Dalton, Amy L Miller, and Jaume Bacardit. 2020. Automatic recognition of feeding and foraging behaviour in pigs using deep learning. Biosystems engineering 197 (2020), 91‚Äì104.\n\nAndresen et al. (2020) Niek Andresen, Manuel W√∂llhaf, Katharina Hohlbaum, Lars Lewejohann, Olaf Hellwich, Christa Th√∂ne-Reineke, and Vitaly Belik. 2020. Towards a fully automated surveillance of well-being status in laboratory mice using deep learning: Starting with facial expression analysis. PLoS One 15, 4 (2020), e0228059.\n\nArablouei et al. (2023b) Reza Arablouei, Liang Wang, Lachlan Currie, Jodan Yates, Flavio AP Alvarenga, and Greg J Bishop-Hurley. 2023b. Animal behavior classification via deep learning on embedded systems. Computers and Electronics in Agriculture 207 (2023), 107707.\n\nArablouei et al. (2023a) Reza Arablouei, Ziwei Wang, Greg J Bishop-Hurley, and Jiajun Liu. 2023a. Multimodal sensor data fusion for in-situ classification of animal behavior using accelerometry and GNSS data. Smart Agricultural Technology 4 (2023), 100163.\n\nAshwood et al. (2020) Zoe Ashwood, Nicholas A Roy, Ji Hyun Bak, and Jonathan W Pillow. 2020. Inferring learning rules from animal decision-making. Advances in Neural Information Processing Systems 33 (2020), 3442‚Äì3453.\n\nAyadi et al. (2020) Safa Ayadi, Ahmed Ben Said, Rateb Jabbar, Chafik Aloulou, Achraf Chabbouh, and Ahmed Ben Achballah. 2020. Dairy cow rumination detection: A deep learning approach. In Distributed Computing for Emerging Smart Networks: Second International Workshop, DiCES-N 2020, Bizerte, Tunisia, December 18, 2020, Proceedings 2. Springer, 123‚Äì139.\n\nBanerjee et al. (2023) Shoubhik Chandan Banerjee, Khursheed Ahmad Khan, and Rati Sharma. 2023. Deep-worm-tracker: Deep learning methods for accurate detection and tracking for behavioral studies in C. elegans. Applied Animal Behaviour Science 266 (2023), 106024.\n\nBao and Xie (2022) Jun Bao and Qiuju Xie. 2022. Artificial intelligence in animal farming: A systematic literature review. Journal of Cleaner Production 331 (2022), 129956.\n\nBarnard et al. (2016) Shanis Barnard, Simone Calderara, Simone Pistocchi, Rita Cucchiara, Michele Podaliri-Vulpiani, Stefano Messori, and Nicola Ferri. 2016. Quick, accurate, smart: 3D computer vision technology helps assessing confined animals‚Äô behaviour. PloS one 11, 7 (2016), e0158748.\n\nBellman (1957) Richard Bellman. 1957. A Markovian Decision Process. Journal of Mathematics and Mechanics 6, 5 (1957), 679‚Äì684. http://www.jstor.org/stable/24900506\n\nBenaissa et al. (2023) S Benaissa, FAM Tuyttens, D Plets, L Martens, L Vandaele, W Joseph, and B Sonck. 2023. Improved cattle behaviour monitoring by combining Ultra-Wideband location and accelerometer data. animal 17, 4 (2023), 100730.\n\nBergen et al. (2022) Silas Bergen, Manuela M Huso, Adam E Duerr, Melissa A Braham, Todd E Katzner, Sara Schmuecker, and Tricia A Miller. 2022. Classifying behavior from short-interval biologging data: An example with GPS tracking of birds. Ecology and Evolution 12, 2 (2022), e08395.\n\nBerman et al. (2014) Gordon J Berman, Daniel M Choi, William Bialek, and Joshua W Shaevitz. 2014. Mapping the stereotyped behaviour of freely moving fruit flies. Journal of The Royal Society Interface 11, 99 (2014), 20140672.\n\nBernardes et al. (2021) Rodrigo Cupertino Bernardes, Maria Augusta Pereira Lima, Raul Narciso Carvalho Guedes, Cl√≠ssia Barboza da Silva, and Gustavo Ferreira Martins. 2021. Ethoflow: computer vision and artificial intelligence-based software for automatic behavior analysis. Sensors 21, 9 (2021), 3237.\n\nBhattacharya and Shahnawaz (2021) Samayan Bhattacharya and Sk Shahnawaz. 2021. Pose recognition in the wild: Animal pose estimation using agglomerative clustering and contrastive learning. arXiv preprint arXiv:2111.08259 (2021).\n\nBiderman et al. (2023) Dan Biderman, Matthew R Whiteway, Cole Hurwitz, Nicholas Greenspan, Robert S Lee, Ankit Vishnubhotla, Richard Warren, Federico Pedraja, Dillon Noone, Michael Schartner, et al. 2023. Lightning Pose: improved animal pose estimation via semi-supervised learning, Bayesian ensembling, and cloud-native open-source tools. bioRxiv (2023).\n\nBjerge et al. (2023) Kim Bjerge, Jamie Alison, Mads Dyrmann, Carsten Eie Frigaard, Hjalte MR Mann, and Toke Thomas H√∏ye. 2023. Accurate detection and identification of insects from camera trap images with deep learning. PLOS Sustainability and Transformation 2, 3 (2023), e0000051.\n\nBlau et al. (2022) Ari Blau, Christoph Gebhardt, Andres Bendesky, Liam Paninski, and Anqi Wu. 2022. SemiMultiPose: A Semi-supervised Multi-animal Pose Estimation Framework. arXiv preprint arXiv:2204.07072 (2022).\n\nBocaj et al. (2020) Enkeleda Bocaj, Dimitris Uzunidis, Panagiotis Kasnesis, and Charalampos Z Patrikakis. 2020. On the benefits of deep convolutional neural networks on animal activity recognition. In 2020 International Conference on Smart Systems and Technologies (SST). IEEE, 83‚Äì88.\n\nBohnslav et al. (2021) James P Bohnslav, Nivanthika K Wimalasena, Kelsey J Clausing, Yu Y Dai, David A Yarmolinsky, Tom√°s Cruz, Adam D Kashlan, M Eugenia Chiappe, Lauren L Orefice, Clifford J Woolf, et al. 2021. DeepEthogram, a machine learning pipeline for supervised behavior classification from raw pixels. Elife 10 (2021), e63377.\n\nBoser et al. (1992) Bernhard E Boser, Isabelle M Guyon, and Vladimir N Vapnik. 1992. A training algorithm for optimal margin classifiers. In Proceedings of the fifth annual workshop on Computational learning theory. 144‚Äì152.\n\nBravo Sanchez et al. (2021) Francisco J Bravo Sanchez, Md Rahat Hossain, Nathan B English, and Steven T Moore. 2021. Bioacoustic classification of avian calls from raw sound waveforms with an open-source deep learning architecture. Scientific Reports 11, 1 (2021), 15733.\n\nBreiman (2001) Leo Breiman. 2001. Random forests. Machine learning 45 (2001), 5‚Äì32.\n\nBrown and de Bivort (2017) Andr√© EX Brown and Benjamin de Bivort. 2017. The study of animal behaviour as a physical science. bioRxiv (2017). https://doi.org/10.1101/220855 arXiv:https://www.biorxiv.org/content/early/2017/11/17/220855.full.pdf\n\nBryda (2013) Elizabeth C Bryda. 2013. The Mighty Mouse: the impact of rodents on advances in biomedical research. Missouri medicine 110, 3 (2013), 207.\n\nBurgos-Artizzu et al. (2012) Xavier P Burgos-Artizzu, Piotr Doll√°r, Dayu Lin, David J Anderson, and Pietro Perona. 2012. Social behavior recognition in continuous video. In 2012 IEEE conference on computer vision and pattern recognition. IEEE, 1322‚Äì1329.\n\nCao et al. (2019) Jinkun Cao, Hongyang Tang, Hao-Shu Fang, Xiaoyong Shen, Cewu Lu, and Yu-Wing Tai. 2019. Cross-domain adaptation for animal pose estimation. In Proceedings of the IEEE/CVF international conference on computer vision. 9498‚Äì9507.\n\nCao et al. (2020) Shuo Cao, Dean Zhao, Xiaoyang Liu, and Yueping Sun. 2020. Real-time robust detector for underwater live crabs based on deep learning. Computers and Electronics in Agriculture 172 (2020), 105339.\n\nCao et al. (2017) Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh. 2017. Realtime multi-person 2d pose estimation using part affinity fields. In Proceedings of the IEEE conference on computer vision and pattern recognition. 7291‚Äì7299.\n\nChalmers et al. (2021) Carl Chalmers, Paul Fergus, S Wich, and SN Longmore. 2021. Modelling Animal Biodiversity Using Acoustic Monitoring and Deep Learning. In 2021 International Joint Conference on Neural Networks (IJCNN). IEEE, 1‚Äì7.\n\nChambers et al. (2021) Robert D Chambers, Nathanael C Yoder, Aletha B Carson, Christian Junge, David E Allen, Laura M Prescott, Sophie Bradley, Garrett Wymore, Kevin Lloyd, and Scott Lyle. 2021. Deep learning classification of canine behavior using a single collar-mounted accelerometer: Real-world validation. Animals 11, 6 (2021), 1549.\n\nChen et al. (2021) Chen Chen, Weixing Zhu, and Tomas Norton. 2021. Behaviour recognition of pigs and cattle: Journey from computer vision to deep learning. Computers and Electronics in Agriculture 187 (2021), 106255.\n\nChen et al. (2020) Chen Chen, Weixing Zhu, Juan Steibel, Janice Siegford, Junjie Han, and Tomas Norton. 2020. Recognition of feeding behaviour of pigs and determination of feeding time of each pig by a video-based deep learning method. Computers and Electronics in Agriculture 176 (2020), 105642.\n\nChen et al. (2017) Yu Chen, Chunhua Shen, Xiu-Shen Wei, Lingqiao Liu, and Jian Yang. 2017. Adversarial posenet: A structure-aware convolutional network for human pose estimation. In Proceedings of the IEEE international conference on computer vision. 1212‚Äì1221.\n\nChoi et al. (2022) Taeyeong Choi, Benjamin Pyenson, Juergen Liebig, and Theodore P Pavlic. 2022. Beyond tracking: using deep learning to discover novel interactions in biological swarms. Artificial Life and Robotics 27, 2 (2022), 393‚Äì400.\n\nChoi et al. (2021) Yoona Choi, Heechan Chae, Jonguk Lee, Daihee Park, and Yongwha Chung. 2021. Cat Monitoring and Disease Diagnosis System based on Deep Learning. Journal of Korea Multimedia Society 24, 2 (2021), 233‚Äì244.\n\nChollet (2017) Fran√ßois Chollet. 2017. Xception: Deep learning with depthwise separable convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition. 1251‚Äì1258.\n\nChou et al. (2018) Chia-Jung Chou, Jui-Ting Chien, and Hwann-Tzong Chen. 2018. Self adversarial training for human pose estimation. In 2018 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC). IEEE, 17‚Äì30.\n\nCoria-Avila et al. (2022) Genaro A Coria-Avila, James G Pfaus, Agust√≠n Orihuela, Adriana Dom√≠nguez-Oliva, Nancy Jos√©-P√©rez, Laura Astrid Hern√°ndez, and Daniel Mota-Rojas. 2022. The neurobiology of behavior and its applicability for animal welfare: A review. Animals 12, 7 (2022), 928.\n\nCoulibaly et al. (2022) Solemane Coulibaly, Bernard Kamsu-Foguem, Dantouma Kamissoko, and Daouda Traore. 2022. Explainable deep convolutional neural networks for insect pest recognition. Journal of Cleaner Production 371 (2022), 133638.\n\nDang et al. (2022) Thai-Ha Dang, Ngoc-Hai Dang, Viet-Thang Tran, and Wan-Young Chung. 2022. A LoRaWAN-Based Smart Sensor Tag for Cow Behavior Monitoring. In 2022 IEEE Sensors. IEEE, 1‚Äì4.\n\nDell et al. (2014) Anthony I Dell, John A Bender, Kristin Branson, Iain D Couzin, Gonzalo G de Polavieja, Lucas PJJ Noldus, Alfonso P√©rez-Escudero, Pietro Perona, Andrew D Straw, Martin Wikelski, et al. 2014. Automated image-based tracking and its application in ecology. Trends in ecology & evolution 29, 7 (2014), 417‚Äì428.\n\nDitria et al. (2020) Ellen M Ditria, Sebastian Lopez-Marcano, Michael Sievers, Eric L Jinks, Christopher J Brown, and Rod M Connolly. 2020. Automating the analysis of fish abundance using object detection: optimizing animal ecology with deep learning. Frontiers in Marine Science (2020), 429.\n\nEerdekens et al. (2020) Anniek Eerdekens, Margot Deruyck, Jaron Fontaine, Luc Martens, Eli De Poorter, David Plets, and Wout Joseph. 2020. Resampling and data augmentation for equines‚Äô behaviour classification based on wearable sensor accelerometer data using a convolutional neural network. In 2020 International Conference on Omni-layer Intelligent Systems (COINS). IEEE, 1‚Äì6.\n\nElbarrany et al. (2023) Abdullah Magdy Elbarrany, Abdallah Mohialdin, and Ayman Atia. 2023. The Use of Pose Estimation for Abnormal Behavior Analysis in Poultry Farms. In 2023 5th Novel Intelligent and Leading Emerging Sciences Conference (NILES). IEEE, 33‚Äì36.\n\nEvangelista et al. (2022) Ivan Roy S Evangelista, Ronnie Concepcion, Maria Gemel B Palconit, Argel A Bandala, and Elmer P Dadios. 2022. YOLOv7 and DeepSORT for Intelligent Quail Behavioral Activities Monitoring. In 2022 IEEE 14th International Conference on Humanoid, Nanotechnology, Information Technology, Communication and Control, Environment, and Management (HNICEM). IEEE, 1‚Äì5.\n\nFan et al. (2015) Xiaochuan Fan, Kang Zheng, Yuewei Lin, and Song Wang. 2015. Combining local appearance and holistic view: Dual-source deep neural networks for human pose estimation. In Proceedings of the IEEE conference on computer vision and pattern recognition. 1347‚Äì1355.\n\nFang et al. (2021) Cheng Fang, Tiemin Zhang, Haikun Zheng, Junduan Huang, and Kaixuan Cuan. 2021. Pose estimation and behavior classification of broiler chickens based on deep neural networks. Computers and Electronics in Agriculture 180 (2021), 105863.\n\nFazzari et al. (2023) Edoardo Fazzari, Fabio Carrara, Fabrizio Falchi, Cesare Stefanini, and Donato Romano. 2023. Using AI to decode the behavioral responses of an insect to chemical stimuli: towards machine-animal computational technologies. International Journal of Machine Learning and Cybernetics (2023). https://doi.org/10.1007/s13042-023-02009-y\n\nFazzari et al. (2022) Edoardo Fazzari, Fabio Carrara, Fabrizio Falchi, Cesare Stefanini, Donato Romano, et al. 2022. A Workflow for Developing Biohybrid Intelligent Sensing Systems. (2022).\n\nFeichtenhofer et al. (2019) Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. 2019. Slowfast networks for video recognition. In Proceedings of the IEEE/CVF international conference on computer vision. 6202‚Äì6211.\n\nFeighelstein et al. (2023) Marcelo Feighelstein, Yamit Ehrlich, Li Naftaly, Miriam Alpin, Shenhav Nadir, Ilan Shimshoni, Renata H Pinho, Stelio PL Luna, and Anna Zamansky. 2023. Deep learning for video-based automated pain recognition in rabbits. Scientific Reports 13, 1 (2023), 14679.\n\nFuentes et al. (2020) Alvaro Fuentes, Sook Yoon, Jongbin Park, and Dong Sun Park. 2020. Deep learning-based hierarchical cattle behavior recognition with spatio-temporal information. Computers and Electronics in Agriculture 177 (2020), 105627.\n\nFujimori et al. (2020) Shiori Fujimori, Takaaki Ishikawa, and Hiroshi Watanabe. 2020. Animal behavior classification using DeepLabCut. In 2020 IEEE 9th Global Conference on Consumer Electronics (GCCE). IEEE, 254‚Äì257.\n\nGnanasekar et al. (2022) Sudarsini Tekkam Gnanasekar, Svetlana Yanushkevich, Nynke J Van den Hoogen, and Tuan Trang. 2022. Rodent Tracking and Abnormal Behavior Classification in Live Video using Deep Neural Networks. In 2022 IEEE Symposium Series on Computational Intelligence (SSCI). IEEE, 830‚Äì837.\n\nGoodfellow et al. (2014) Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. Advances in neural information processing systems 27 (2014).\n\nGore et al. (2023) Sayali V Gore, Rohit Kakodkar, Tha√≠s Del Rosario Hern√°ndez, Sara Tucker Edmister, and Robbert Creton. 2023. Zebrafish Larvae Position Tracker (Z-LaP Tracker): a high-throughput deep-learning behavioral approach for the identification of calcineurin pathway-modulating drugs using zebrafish larvae. Scientific Reports 13, 1 (2023), 3174.\n\nGotanda et al. (2019) Kiyoko M Gotanda, Damien R Farine, Claudius F Kratochwil, Kate L Laskowski, and Pierre-Olivier Montiglio. 2019. Animal behavior facilitates eco-evolutionary dynamics. arXiv preprint arXiv:1912.09505 (2019).\n\nGraving et al. (2019) Jacob M Graving, Daniel Chae, Hemal Naik, Liang Li, Benjamin Koger, Blair R Costelloe, and Iain D Couzin. 2019. DeepPoseKit, a software toolkit for fast and robust animal pose estimation using deep learning. Elife 8 (2019), e47994.\n\nHaalck et al. (2020) Lars Haalck, Michael Mangan, Barbara Webb, and Benjamin Risse. 2020. Towards image-based animal tracking in natural environments using a freely moving camera. Journal of neuroscience methods 330 (2020), 108455.\n\nHan et al. (2020) Fangfang Han, Junchao Zhu, Bin Liu, Baofeng Zhang, and Fuhua Xie. 2020. Fish shoals behavior detection based on convolutional neural network and spatiotemporal information. IEEE Access 8 (2020), 126907‚Äì126926.\n\nHara et al. (2018) Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh. 2018. Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition. 6546‚Äì6555.\n\nHart (2011) Benjamin L Hart. 2011. Behavioural defences in animals against pathogens and parasites: parallels with the pillars of medicine in humans. Philosophical Transactions of the Royal Society B: Biological Sciences 366, 1583 (2011), 3406‚Äì3417.\n\nHe et al. (2017) Kaiming He, Georgia Gkioxari, Piotr Doll√°r, and Ross Girshick. 2017. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision. 2961‚Äì2969.\n\nHe et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Identity mappings in deep residual networks. In Computer Vision‚ÄìECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11‚Äì14, 2016, Proceedings, Part IV 14. Springer, 630‚Äì645.\n\nHelmi et al. (2023) Ahmed M Helmi, Mohammed AA Al-qaness, Abdelghani Dahou, and Mohamed Abd Elaziz. 2023. Human activity recognition using marine predators algorithm with deep learning. Future Generation Computer Systems 142 (2023), 340‚Äì350.\n\nHochreiter and Schmidhuber (1997) Sepp Hochreiter and J√ºrgen Schmidhuber. 1997. Long short-term memory. Neural computation 9, 8 (1997), 1735‚Äì1780.\n\nHossain et al. (2023) Tahera Hossain, Wanggang Shen, Anindya Antar, Snehal Prabhudesai, Sozo Inoue, Xun Huan, and Nikola Banovic. 2023. A Bayesian approach for quantifying data scarcity when modeling human behavior via inverse reinforcement learning. ACM Transactions on Computer-Human Interaction 30, 1 (2023), 1‚Äì27.\n\nHou et al. (2020) Jin Hou, Yuxin He, Hongbo Yang, Thomas Connor, Jie Gao, Yujun Wang, Yichao Zeng, Jindong Zhang, Jinyan Huang, Bochuan Zheng, et al. 2020. Identification of animal individuals using deep learning: A case study of giant panda. Biological Conservation 242 (2020), 108414.\n\nIslam et al. (2023) Md Milon Islam, Sheikh Nooruddin, Fakhri Karray, and Ghulam Muhammad. 2023. Multi-level feature fusion for multimodal human activity recognition in Internet of Healthcare Things. Information Fusion 94 (2023), 17‚Äì31.\n\nIsmail Fawaz et al. (2020) Hassan Ismail Fawaz, Benjamin Lucas, Germain Forestier, Charlotte Pelletier, Daniel F Schmidt, Jonathan Weber, Geoffrey I Webb, Lhassane Idoumghar, Pierre-Alain Muller, and Fran√ßois Petitjean. 2020. Inceptiontime: Finding alexnet for time series classification. Data Mining and Knowledge Discovery 34, 6 (2020), 1936‚Äì1962.\n\nJia et al. (2022) Yinjun Jia, Shuaishuai Li, Xuan Guo, Bo Lei, Junqiang Hu, Xiao-Hong Xu, and Wei Zhang. 2022. Selfee, self-supervised features extraction of animal behaviors. Elife 11 (2022), e76218.\n\nJiang et al. (2022a) Le Jiang, Caleb Lee, Divyang Teotia, and Sarah Ostadabbas. 2022a. Animal pose estimation: A closer look at the state-of-the-art, existing gaps and opportunities. Computer Vision and Image Understanding (2022), 103483.\n\nJiang et al. (2020) Min Jiang, Yuan Rao, Jingyao Zhang, and Yiming Shen. 2020. Automatic behavior recognition of group-housed goats using deep learning. Computers and Electronics in Agriculture 177 (2020), 105706.\n\nJiang et al. (2022b) Zheheng Jiang, Zhihua Liu, Long Chen, Lei Tong, Xiangrong Zhang, Xiangyuan Lan, Danny Crookes, Ming-Hsuan Yang, and Huiyu Zhou. 2022b. Detecting and tracking of multiple mice using part proposal networks. IEEE Transactions on Neural Networks and Learning Systems (2022).\n\nJiang et al. (2021) Zheheng Jiang, Feixiang Zhou, Aite Zhao, Xin Li, Ling Li, Dacheng Tao, Xuelong Li, and Huiyu Zhou. 2021. Multi-view mouse social behaviour recognition with deep graphic model. IEEE Transactions on Image Processing 30 (2021), 5490‚Äì5504.\n\nJoo et al. (2022) Kevin Hyekang Joo, Shiyuan Duan, Shawna L Weimer, and Mohammad Nayeem Teli. 2022. Birds‚Äô Eye View: Measuring Behavior and Posture of Chickens as a Metric for Their Well-Being. arXiv preprint arXiv:2205.00069 (2022).\n\nJ√∫nior and Rieder (2020) Telmo De Cesaro J√∫nior and Rafael Rieder. 2020. Automatic identification of insects from digital images: A survey. Computers and Electronics in Agriculture 178 (2020), 105784.\n\nKamminga et al. (2018) Jacob W Kamminga, Duc V Le, Jan Pieter Meijers, Helena Bisby, Nirvana Meratnia, and Paul JM Havinga. 2018. Robust sensor-orientation-independent feature selection for animal activity recognition on collar tags. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 2, 1 (2018), 1‚Äì27.\n\nKamminga et al. (2019) Jacob W Kamminga, Nirvana Meratnia, and Paul JM Havinga. 2019. Dataset: Horse movement data and analysis of its potential for activity recognition. In Proceedings of the 2nd Workshop on Data Acquisition To Analysis. 22‚Äì25.\n\nKasnesis et al. (2022) Panagiotis Kasnesis, Vasileios Doulgerakis, Dimitris Uzunidis, Dimitris G Kogias, Susana I Funcia, Marta B Gonz√°lez, Christos Giannousis, and Charalampos Z Patrikakis. 2022. Deep learning empowered wearable-based behavior recognition for search and rescue dogs. Sensors 22, 3 (2022), 993.\n\nKaufmann and Schering (2014) J√∂rg Kaufmann and AG Schering. 2014. Analysis of Variance ANOVA. John Wiley & Sons, Ltd. https://doi.org/10.1002/9781118445112.stat06938 arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118445112.stat06938\n\nKavlak et al. (2023) AT Kavlak, M Pastell, and P Uimari. 2023. Disease detection in pigs based on feeding behaviour traits using machine learning. biosystems engineering 226 (2023), 132‚Äì143.\n\nKingma and Welling (2013) Diederik P Kingma and Max Welling. 2013. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 (2013).\n\nKleanthous et al. (2022b) Natasa Kleanthous, Abir Hussain, Wasiq Khan, Jennifer Sneddon, and Panos Liatsis. 2022b. Deep transfer learning in sheep activity recognition using accelerometer data. Expert Systems with Applications 207 (2022), 117925.\n\nKleanthous et al. (2022a) Natasa Kleanthous, Abir Jaafar Hussain, Wasiq Khan, Jennifer Sneddon, Ahmed Al-Shamma‚Äôa, and Panos Liatsis. 2022a. A survey of machine learning approaches in animal behaviour. Neurocomputing 491 (2022), 442‚Äì463.\n\nKnight and Bayne (2019) Elly C Knight and Erin M Bayne. 2019. Classification threshold and training data affect the quality and utility of focal species data processed with automated audio-recognition software. Bioacoustics 28, 6 (2019), 539‚Äì554.\n\nKoch et al. (2015) Gregory Koch, Richard Zemel, Ruslan Salakhutdinov, et al. 2015. Siamese neural networks for one-shot image recognition. In ICML deep learning workshop, Vol. 2. Lille.\n\nKoger et al. (2023) Benjamin Koger, Adwait Deshpande, Jeffrey T Kerby, Jacob M Graving, Blair R Costelloe, and Iain D Couzin. 2023. Quantifying the movement, behaviour and environmental context of group-living animals using drones and computer vision. Journal of Animal Ecology (2023).\n\nLaboratory et al. (2021) International Brain Laboratory, Valeria Aguillon-Rodriguez, Dora Angelaki, Hannah Bayer, Niccol√≤ Bonacchi, Matteo Carandini, Fanny Cazettes, Gaelle Chapuis, Anne K Churchland, Yang Dan, et al. 2021. Standardized and reproducible measurement of decision-making in mice. Elife 10 (2021), e63711.\n\nLabuguen et al. (2021) Rollyn Labuguen, Jumpei Matsumoto, Salvador Blanco Negrete, Hiroshi Nishimaru, Hisao Nishijo, Masahiko Takada, Yasuhiro Go, Ken-ichi Inoue, and Tomohiro Shibata. 2021. MacaquePose: a novel ‚Äúin the wild‚Äù macaque monkey pose dataset for markerless motion capture. Frontiers in behavioral neuroscience 14 (2021), 581154.\n\nLandgraf et al. (2021) Tim Landgraf, Gregor HW Gebhardt, David Bierbach, Pawel Romanczuk, Lea Musiolek, Verena V Hafner, and Jens Krause. 2021. Animal-in-the-loop: using interactive robotic conspecifics to study social behavior in animal groups. Annual Review of Control, Robotics, and Autonomous Systems 4 (2021), 487‚Äì507.\n\nLauer et al. (2022) Jessy Lauer, Mu Zhou, Shaokai Ye, William Menegas, Steffen Schneider, Tanmay Nath, Mohammed Mostafizur Rahman, Valentina Di Santo, Daniel Soberanes, Guoping Feng, et al. 2022. Multi-animal pose estimation, identification and tracking with DeepLabCut. Nature Methods 19, 4 (2022), 496‚Äì504.\n\nLea et al. (2016) Colin Lea, Rene Vidal, Austin Reiter, and Gregory D Hager. 2016. Temporal convolutional networks: A unified approach to action segmentation. In Computer Vision‚ÄìECCV 2016 Workshops: Amsterdam, The Netherlands, October 8-10 and 15-16, 2016, Proceedings, Part III 14. Springer, 47‚Äì54.\n\nLecomte et al. (2021) Charly G Lecomte, Johannie Audet, Jonathan Harnie, and Alain Frigon. 2021. A validation of supervised deep learning for gait analysis in the cat. Frontiers in Neuroinformatics 15 (2021), 712623.\n\nLee et al. (2021) Sanghoon Lee, Brayden Waugh, Garret O‚ÄôDell, Xiji Zhao, Wook-Sung Yoo, and Dal Hyung Kim. 2021. Predicting Fruit Fly Behaviour using TOLC device and DeepLabCut. In 2021 IEEE 21st International Conference on Bioinformatics and Bioengineering (BIBE). IEEE, 1‚Äì6.\n\nLei et al. (2022) Yujie Lei, Pengmei Dong, Yan Guan, Ying Xiang, Meng Xie, Jiong Mu, Yongzhao Wang, and Qingyong Ni. 2022. Postural behavior recognition of captive nocturnal animals based on deep learning: a case study of Bengal slow loris. Scientific Reports 12, 1 (2022), 7738.\n\nLi and Lee (2023) Chen Li and Gim Hee Lee. 2023. ScarceNet: Animal Pose Estimation with Scarce Annotations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 17174‚Äì17183.\n\nLi et al. (2020) Dan Li, Kaifeng Zhang, Zhenbo Li, and Yifei Chen. 2020. A spatiotemporal convolutional network for multi-behavior recognition of pigs. Sensors 20, 8 (2020), 2381.\n\nLi et al. (2019b) Juan Li, Chen Xu, Lingxu Jiang, Ying Xiao, Limiao Deng, and Zhongzhi Han. 2019b. Detection and analysis of behavior trajectory for sea cucumbers based on deep learning. Ieee Access 8 (2019), 18832‚Äì18840.\n\nLi et al. (2019a) Shuyuan Li, Jianguo Li, Hanlin Tang, Rui Qian, and Weiyao Lin. 2019a. ATRW: a benchmark for Amur tiger re-identification in the wild. arXiv preprint arXiv:1906.05586 (2019).\n\nLi et al. (2023) Yang Li, Guanci Yang, Zhidong Su, Shaobo Li, and Yang Wang. 2023. Human activity recognition based on multienvironment sensor data. Information Fusion 91 (2023), 47‚Äì63.\n\nLin et al. (2017) Tsung-Yi Lin, Piotr Doll√°r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. 2017. Feature pyramid networks for object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition. 2117‚Äì2125.\n\nLiu et al. (2008) Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. 2008. Isolation forest. In 2008 eighth ieee international conference on data mining. IEEE, 413‚Äì422.\n\nLiu et al. (2020) XiaoLe Liu, Si-yang Yu, Nico Flierman, Sebastian Loyola, Maarten Kamermans, Tycho M Hoogland, and Chris I De Zeeuw. 2020. OptiFlex: video-based animal pose estimation using deep learning enhanced by optical flow. BioRxiv (2020), 2020‚Äì04.\n\nLiu et al. (2022) Yongkui Liu, He Xu, Ding Liu, and Lihui Wang. 2022. A digital twin-based sim-to-real transfer for deep reinforcement learning-enabled industrial robot grasping. Robotics and Computer-Integrated Manufacturing 78 (2022), 102365.\n\nLostanlen et al. (2019) Vincent Lostanlen, Kaitlin Palmer, Elly Knight, Christopher Clark, Holger Klinck, Andrew Farnsworth, Tina Wong, Jason Cramer, and Juan Pablo Bello. 2019. Long-distance detection of bioacoustic events with per-channel energy normalization. arXiv preprint arXiv:1911.00417 (2019).\n\nLuxem et al. (2022) Kevin Luxem, Petra Mocellin, Falko Fuhrmann, Johannes K√ºrsch, Stephanie R Miller, Jorge J Palop, Stefan Remy, and Pavol Bauer. 2022. Identifying behavioral structure from deep variational embeddings of animal motion. Communications Biology 5, 1 (2022), 1267.\n\nMahmud et al. (2021) Md Sultan Mahmud, Azlan Zahid, Anup Kumar Das, Muhammad Muzammil, and Muhammad Usman Khan. 2021. A systematic literature review on deep learning applications for precision cattle farming. Computers and Electronics in Ag"
    }
}
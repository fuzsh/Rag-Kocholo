{
    "id": "dbpedia_8305_1",
    "rank": 38,
    "data": {
        "url": "https://medium.com/%40rksachin/practice-thesequestions-for-google-cloud-certified-professional-cloud-architect-exam-6a66c86d5825",
        "read_more_link": "",
        "language": "en",
        "title": "Practice these Questions for Google Cloud Certified Professional Cloud Architect Exam",
        "top_image": "https://miro.medium.com/v2/resize:fit:400/1*ULFVnMc-Ni1UUQ577KhboA.png",
        "meta_img": "https://miro.medium.com/v2/resize:fit:400/1*ULFVnMc-Ni1UUQ577KhboA.png",
        "images": [
            "https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png",
            "https://miro.medium.com/v2/resize:fill:88:88/2*UC3MTqhD5j7Bfc9LWwGfBQ.jpeg",
            "https://miro.medium.com/v2/resize:fill:144:144/2*UC3MTqhD5j7Bfc9LWwGfBQ.jpeg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Sachin Sharma | Senior DevOps Engineer & Lead",
            "medium.com",
            "Sachin Sharma",
            "Senior DevOps Engineer & Lead"
        ],
        "publish_date": "2020-09-01T12:51:17.603000+00:00",
        "summary": "",
        "meta_description": "Google Cloud is becoming great choice for many infrastructure requirements and needs, since it is very important to get to know this one as well. Google Cloud Certified Professional Cloud Architect…",
        "meta_lang": "en",
        "meta_favicon": "https://miro.medium.com/v2/5d8de952517e8160e40ef9841c781cdc14a5db313057fa3c3de41c6f5b494b19",
        "meta_site_name": "Medium",
        "canonical_link": "https://medium.com/@rksachin/practice-thesequestions-for-google-cloud-certified-professional-cloud-architect-exam-6a66c86d5825",
        "text": "Google Cloud is becoming great choice for many infrastructure requirements and needs, since it is very important to get to know this one as well. Google Cloud Certified Professional Cloud Architect Exam helps you to take important decision around GCP services as per your different use cases.\n\nHere we won’t talk about any concepts or methods, just bunch of questions with best possible answer.\n\n1. Your company’s test suite is a custom C++ application that runs tests throughout each day on Linux virtual machines. The full test suite takes several hours to complete, running on a limited number of on-premises servers reserved for testing. Your company wants to move the testing infrastructure to the cloud, to reduce the amount of time it takes to fully test a change to the system, while changing the tests as little as possible.\n\nWhich cloud infrastructure should you recommend?\n\nGoogle Compute Engine managed instance groups with auto-scaling.\n\n2. A lead software engineer tells you that his new application design uses websockets and HTTP sessions that are not distributed across the web servers. You want to help him ensure his application will run properly on Google Cloud Platform.\n\nWhat should you do?\n\nMeet with the cloud operations team and the engineer to discuss load balancer options.\n\n3. The application reliability team at your company this added a debug feature to their backend service to send all server events to Google Cloud Storage for eventual analysis. The event records are at least 50 KB and at most 15 MB and are expected to peak at 3,000 events per second. You want to minimise data loss.\n\nWhich process should you implement?\n\n1. Append metadata to file body 2. Compress individual files 3. Name files with a random prefix pattern 4. Save files to one bucket\n\n4. A recent audit revealed that a new network was created in your GCP project. In this network, a GCE instance has an SSH port open to the world. You want to discover this network’s origin.\n\nWhat should you do?\n\nIn the Logging section of the console, specify GCE Network as the logging section. Search for the Create Insert entry.\n\n5. You want to make a copy of a production Linux virtual machine in the US-Central region. You want to manage and replace the copy easily if there are changes on the production virtual machine. You will deploy the copy as a new instance in a different project in the US-East region.\n\nWhat steps must you take?\n\nCreate a snapshot of the root disk, create an image file in Google Cloud Storage from the snapshot, and create a new virtual machine instance in the US-East region using the image file the root disk.\n\n6. You want to enable your running Google Kubernetes Engine cluster to scale as demand for your application changes.\n\nWhat should you do?\n\nUpdate the existing Kubernetes Engine cluster with the following command: gcloud alpha container clusters update mycluster - -enable- autoscaling - -min-nodes=1 - -max-nodes=10\n\n7. Your company places a high value on being responsive and meeting customer needs quickly. Their primary business objectives are release speed and agility. You want to reduce the chance of security errors being accidentally introduced.\n\nWhich two actions can you take? Choose 2 answers.\n\n- Use source code security analyzers as part of the CI/CD pipeline\n\n- Run a vulnerability security scanner as part of your continuous-integration /continuous-delivery (CI/CD) pipeline\n\n8. Your customer is moving their corporate applications to Google Cloud Platform. The security team wants detailed visibility of all projects in the organisation. You provision the Google Cloud Resource Manager and set up yourself as the org admin.\n\nWhat Google Cloud Identity and Access Management (Cloud IAM) roles should you give to the security team?\n\nOrg viewer, project viewer\n\n9. You are helping the QA team to roll out a new load-testing tool to test the scalability of your primary cloud services that run on Google Compute Engine with Cloud\n\nBigtable.\n\nWhich three requirements should they include? Choose 3 answers.\n\n- Create a separate Google Cloud project to use for the load-testing environment\n\n- Instrument the production services to record every transaction for replay by the load-testing tool\n\n- Instrument the load-testing tool and the target services with detailed logging and metrics collection\n\n10. Your company runs several databases on a single MySQL instance. They need to take backups of a specific database at regular intervals. The backup activity needs to complete as quickly as possible and cannot be allowed to impact disk performance.\n\nHow should you configure the storage?\n\nUse gcsfuse to mount a Google Cloud Storage bucket as a volume directly on the instance and write backups to the mounted location using mysqldump.\n\n11. Your BigQuery project has several users. For audit purposes, you need to see how many queries each user ran in the last month. What should you do?\n\nUse \"˜bq show' to list all jobs. Per job, use \"˜bq Is' to list job information and get the required information.\n\n12. You want to automate the creation of a managed instance group. The VMs have many OS package dependencies. You want to minimize the startup time for VMs in the instance group.\n\nWhat should you do?\n\nCreate a custom VM image with all OS package dependencies. Use Deployment Manager to create the managed instance group with the VM image.\n\n13. Your company captures all web traffic data in Google Analytics 360 and stores it in BigQuery. Each country has its own dataset. Each dataset has multiple tables.\n\nYou want analysts from each country\n\nto be able to see and query only the data for their respective countries.\n\nHow should you configure the access rights?\n\nCreate a group per country. Add analysts to their respective country-groups. Create a single group \"˜all_analysts', and add all country-groups as members. Grant the \"˜all-analysts' group the IAM role of BigQuery dataViewer. Share the appropriate dataset with view access with each respective analyst country-group.\n\n14. You have been engaged by your client to lead the migration of their application infrastructure to GCP. One of their current problems is that the on-premises high performance SAN is requiring frequent and expensive upgrades to keep up with the variety of workloads that are identified as follows: 20TB of log archives retained for legal reasons; 500 GB of VM boot/data volumes and templates; 500 GB of image thumbnails; 200 GB of customer session state data that allows customers to restart sessions even if off-line for several days.\n\nWhich of the following best reflects your recommendations for a cost-effective storage allocation?\n\nMemcache backed by Cloud Datastore for the customer session state data. Lifecycle- managed Cloud Storage for log archives, thumbnails, and VM boot/data volumes.\n\n15. Your web application uses Google Kubernetes Engine to manage several workloads. One workload requires a consistent set of hostnames even after pod scaling and relaunches.\n\nWhich feature of Kubernetes should you use to accomplish this?\n\nPersistent Volumes\n\n16. Your applications will be writing their logs to BigQuery for analysis. Each application should have its own table. Any logs older than 45 days should be removed.\n\nYou want to optimise storage and follow Google-recommended practices. What should you do?\n\nMake the tables time-partitioned, and configure the partition expiration at 45 days\n\n17. You want your Google Kubernetes Engine cluster to automatically add or remove nodes based on CPU load.\n\nWhat should you do?\n\nConfigure a HorizontalPodAutoscaler with a target CPU usage. Enable the Cluster Autoscaler from the GCP Console.\n\n18. You need to develop procedures to verify resilience of disaster recovery for remote recovery using GCP. Your production environment is hosted on-premises. You need to establish a secure, redundant connection between your on-premises network and the GCP network.\n\nWhat should you do?\n\nVerify that Dedicated Interconnect can replicate files to GCP. Verify that Cloud VPN can establish a secure connection between your networks if Dedicated Interconnect fails.\n\n19. Your company operates nationally and plans to use GCP for multiple batch workloads, including some that are not time-critical. You also need to use GCP services that are HIPAA-certified and manage service costs.\n\nHow should you design to meet Google best practices?\n\nProvisioning preemptible VMs to reduce cost. Disable and then discontinue use of all GCP and APIs that are not HIPAA-compliant.\n\n20. Your customer wants to do resilience testing of their authentication layer. This consists of a regional managed instance group serving a public REST API that reads from and writes to a Cloud SQL instance.\n\nWhat should you do?\n\nSchedule a disaster simulation exercise during which you can shut off all VMs in a zone to see how your application behaves.\n\n21. You are using Cloud SQL as the database backend for a large CRM deployment. You want to scale as usage increases and ensure that you don’t run out of storage, maintain 75% CPU usage cores, and keep replication lag below 60 seconds. What are the correct steps to meet your requirements?\n\n1. Enable automatic storage increase for the instance. 2. Create a Stackdriver alert when CPU usage exceeds 75%, and change the instance type to reduce CPU usage. 3. Create a Stackdriver alert for replication lag, and shard the database to reduce replication time.\n\n22. You are tasked with building an online analytical processing (OLAP) marketing analytics and reporting tool. This requires a relational database that can operate on hundreds of terabytes of data. What is the Google-recommended tool for such applications?\n\nBigQuery, because it is designed for large-scale processing of tabular data\n\n23. You have deployed an application to Kubernetes Engine, and are using the Cloud SQL proxy container to make the Cloud SQL database available to the services running on Kubernetes. You are notified that the application is reporting database connection issues. Your company policies require a post-mortem. What should you do?\n\nIn the GCP Console, navigate to Stackdriver Logging. Consult logs for Kubernetes Engine and Cloud SQL.\n\n24. Your company pushes batches of sensitive transaction data from its application server VMs to Cloud Pub/Sub for processing and storage. What is the Google- recommended way for your application to authenticate to the required Google Cloud services?\n\nEnsure that VM service accounts are granted the appropriate Cloud Pub/Sub IAM roles.\n\n25. You want to establish a Compute Engine application in a single VPC across two regions. The application must communicate over VPN to an on-premises network.\n\nHow should you deploy the VPN?\n\nDeploy Cloud VPN Gateway in each region. Ensure that each region has at least one VPN tunnel to the on-premises peer gateway.\n\n26. Your company wants to start using Google Cloud resources but wants to retain their on-premises Active Directory domain controller for identity management.\n\nWhat should you do?\n\nUse Google Cloud Directory Sync to synchronize Active Directory usernames with cloud identities and configure SAML SSO.\n\n27. You are running a cluster on Kubernetes Engine (GKE) to serve a web application. Users are reporting that a specific part of the application is not responding anymore. You notice that all pods of your deployment keep restarting after 2 seconds. The application writes logs to standard output. You want to inspect the logs to find the cause of the issue. Which approach can you take?\n\nReview the Stackdriver logs for the specific GKE container that is serving the unresponsive part of the application.\n\n28. You are using a single Cloud SQL instance to serve your application from a specific zone. You want to introduce high availability. What should you do?\n\nCreate a failover replica instance in the same region, but in a different zone\n\n29. Your company is running a stateless application on a Compute Engine instance. The application is used heavily during regular business hours and lightly outside of business hours. Users are reporting that the application is slow during peak hours. You need to optimise the application’s performance. What should you do?\n\nCreate a custom image from the existing disk. Create an instance template from the custom image. Create an autoscaled managed instance group from the instance template.\n\n30. Your web application has several VM instances running within a VPC. You want to restrict communications between instances to only the paths and ports you authorise, but you don’t want to rely on static IP addresses or subnets because the app can autoscale. How should you restrict communications?\n\nUse firewall rules based on network tags attached to the compute instances\n\n31. You are designing an application for use only during business hours. For the minimum viable product release, you’d like to use a managed product that automatically “scales to zero” so you don’t incur costs when there is no activity.\n\nWhich primary compute resource should you choose?\n\nCloud Functions\n\n32. You are creating an App Engine application that uses Cloud Datastore as its persistence layer. You need to retrieve several root entities for which you have the identifiers. You want to minimize the overhead in operations performed by Cloud Datastore. What should you do?\n\nCreate the Key object for each Entity and run a batch get operation\n\n33. You need to upload files from your on-premises environment to Cloud Storage. You want the files to be encrypted on Cloud Storage using customer-supplied encryption keys. What should you do?\n\nSupply the encryption key in a .boto configuration file. Use gsutil to upload the files.\n\n34. Your customer wants to capture multiple GBs of aggregate real-time key performance indicators (KPIs) from their game servers running on Google Cloud Platform and monitor the KPIs with low latency. How should they capture the KPIs?\n\nOutput custom metrics to Stackdriver from the game servers, and create a Dashboard in Stackdriver Monitoring Console to view them.\n\n35. You have a Python web application with many dependencies that requires 0.1 CPU cores and 128 MB of memory to operate in production. You want to monitor and maximise machine utilisation. You also want to reliably deploy new versions of the application. Which set of steps should you take?\n\nPerform the following: 1. Create a Kubernetes Engine cluster with n1-standard-1 type machines. 2. Build a Docker image from the production branch with all of the dependencies, and tag it with the version number. 3. Create a Kubernetes Deployment with the imagePullPolicy set to \"IfNotPresent\" in the staging namespace, and then promote it to the production namespace after testing.\n\n36. You are creating an App Engine application that uses Cloud Datastore as its persistence layer. You need to retrieve several root entities for which you have the identifiers. You want to minimize the overhead in operations performed by Cloud Datastore. What should you do?\n\nCreate the Key object for each Entity and run a batch get operation\n\n37. You have developed an application using Cloud ML Engine that recognizes famous paintings from uploaded images. You want to test the application and allow specific people to upload images for the next 24 hours. Not all users have a Google Account. How should you have users upload images?\n\nHave users upload the images to Cloud Storage using a signed URL that expires after 24 hours.\n\n38. Your web application must comply with the requirements of the European Union’s General Data Protection Regulation (GDPR). You are responsible for the technical architecture of your web application. What should you do?\n\nDefine a design for the security of data in your web application that meets GDPR requirements.\n\n39. You need to set up Microsoft SQL Server on GCP. Management requires that there’s no downtime in case of a data center outage in any of the zones within a\n\nGCP region. What should you do?\n\nConfigure a Cloud SQL instance with high availability enabled.\n\n40. The development team has provided you with a Kubernetes Deployment file. You have no infrastructure yet and need to deploy the application. What should you do?\n\nUse gcloud to create a Kubernetes cluster. Use kubectl to create the deployment.\n\n41. You need to evaluate your team readiness for a new GCP project. You must perform the evaluation and create a skills gap plan incorporates the business goal of cost optimization. Your team has deployed two GCP projects successfully to date. What should you do?\n\nAllocate budget for team training. Create a roadmap for your team to achieve Google Cloud certification based on job role.\n\n42. For this question, refer to the TerramEarth case study. https://cloud.google.com/certification/guides/cloud-architect/casestudy-terramearth-rev2\n\nBecause you do not know every possible future use for the data TerramEarth collects, you have decided to build a system that captures and stores all raw data in case you need it later. How can you most cost-effectively accomplish this goal?\n\nHave the vehicles in the field continue to dump data via FTP, and adjust the existing Linux machines to immediately upload it to Cloud Storage with gsutil.\n\n43. For this question, refer to the TerramEarth case study. https://cloud.google.com/certification/guides/cloud-architect/casestudy-terramearth-rev2\n\nToday, TerramEarth maintenance workers receive interactive performance graphs for the last 24 hours (86,400 events) by plugging their maintenance tablets into the vehicle. The support group wants support technicians to view this data remotely to help troubleshoot problems. You want to minimize the latency of graph loads. How should you provide this functionality?\n\nExecute queries against data indexed by vehicle_id.timestamp in Cloud Bigtable.\n\n44. For this question, refer to the TerramEarth case study. https://cloud.google.com/certification/guides/cloud-architect/casestudy-terramearth-rev2\n\nYour agricultural division is experimenting with fully autonomous vehicles. You want your architecture to promote strong security during vehicle operation. Which two architecture characteristics should you consider? (choose two)\n\n- Treat every microservice call between modules on the vehicle as untrusted.\n\n- Use a Trusted Platform Module (TPM) and verify firmware and binaries on boot.\n\n45. For this question, refer to the TerramEarth case study. https://cloud.google.com/certification/guides/cloud-architect/casestudy-terramearth-rev2\n\nWhich of TerramEarth’s legacy enterprise processes will experience significant change as a result of increased Google Cloud Platform adoption?\n\nCapacity planning, TCO calculations, OpEx/CapEx allocation\n\n46. For this question, refer to the TerramEarth case study. https://cloud.google.com/certification/guides/cloud-architect/casestudy-terramearth-rev2\n\nYou analyzed TerramEarth’s business requirement to reduce downtime and found that they can achieve a majority of time saving by reducing customers’ wait time for parts. You decided to focus on reduction of the 3 weeks’ aggregate reporting time. Which modifications to the company’s processes should you recommend?\n\nIncrease fleet cellular connectivity to 80%, migrate from FTP to streaming transport, and develop machine learning analysis of metrics.\n\n47. Your company wants to deploy several microservices to help their system handle elastic loads. Each microservice uses a different version of software libraries. You want to enable their developers to keep their development environment in sync with the various production services. Which technology should you choose?\n\nContainers\n\n48. Your company wants to track whether someone is present in a meeting room reserved for a scheduled meeting. There are 1000 meeting rooms across 5 offices on 3 continents. Each room is equipped with a motion sensor that reports its status every second. You want to support the data ingestion needs of this sensor network. The receiving infrastructure needs to account for the possibility that the devices may have inconsistent connectivity. Which solution should you design?\n\nHave devices poll for connectivity to Cloud Pub/Sub and publish the latest messages on a regular interval to a shared topic for all devices.\n\n49. Your company wants to try out the cloud with low risk. They want to archive approximately 100 TB of their log data to the cloud and test the serverless analytics features available to them there, while also retaining that data as a long-term disaster recovery backup. Which two steps should they take? (choose two)\n\n- Load logs into BigQuery\n\n- Upload log files into Cloud Storage\n\n50. You set up an autoscaling managed instance group to serve web traffic for an upcoming launch. After configuring the instance group as a backend service to an HTTP(S) load balancer, you notice that virtual machine (VM) instances are being terminated and re-launched every minute. The instances do not have a public IP address. You have verified that the appropriate web response is coming from each instance using the curl command. You want to ensure that the backend is configured correctly. What should you do?\n\nEnsure that a firewall rule exists to allow load balancer health checks to reach the instances in the instance group.\n\n51. Your organization has a 3-tier web application deployed in the same Google Cloud Virtual Private Cloud (VPC). Each tier (web, API, and database) scales independently of the others. Network traffic should flow through the web to the API tier, and then on to the database tier. Traffic should not flow between the web and the database tier. How should you configure the network with minimal steps?\n\nAdd tags to each tier and set up firewall rules to allow the desired traffic flow.\n\n52. You are designing a large distributed application with 30 microservices. Each of your distributed microservices needs to connect to a database backend. You want to store the credentials securely. Where should you store the credentials?\n\nIn a key management system\n\n53. For this question, refer to the Mountkirk Games case study. https://cloud.google.com/certification/guides/cloud-architect/casestudy-mountkirkgames-rev2\n\nMountkirk Games wants to set up a real-time analytics platform for their new game. The new platform must meet their technical requirements. Which combination of Google technologies will meet all of their requirements?\n\nCloud Dataflow, Cloud Storage, Cloud Pub/Sub, and BigQuery\n\n54. For this question, refer to the Mountkirk Games case study. https://cloud.google.com/certification/guides/cloud-architect/casestudy-mountkirkgames-rev2\n\nMountkirk Games has deployed their new backend on Google Cloud Platform (GCP). You want to create a thorough testing process for new versions of the backend before they are released to the public. You want the testing environment to scale in an economical way. How should you design the process?\n\nCreate a scalable environment in Google Cloud for simulating production load.\n\n55. For this question, refer to the Mountkirk Games case study. https://cloud.google.com/certification/guides/cloud-architect/casestudy-mountkirkgames-rev2\n\nMountkirk Games wants to set up a continuous delivery pipeline. Their architecture includes many small services that they want to be able to update and roll back quickly. Mountkirk Games has the following requirements: (1) Services are deployed redundantly across multiple regions in the US and Europe, (2) Only frontend services are exposed on the public internet, (3) They can reserve a single frontend IP for their fleet of services, and (4) Deployment artifacts are immutable. Which set of products should they use?\n\nContainer Registry, Google Kubernetes Engine, Cloud Load Balancing\n\n56. To reduce costs, the Director of Engineering has required all developers to move their development infrastructure resources from on-premises virtual machines (VMs) to Google Cloud. These resources go through multiple start/stop events during the day and require state to persist. You have been asked to design the process of running a development environment in Google Cloud while providing cost visibility to the finance department. Which two steps should you take? (choose two)\n\n- Use persistent disks to store the state. Start and stop the VM as needed.\n\n- use BigQuery billing export and labels to relate cost to groups.\n\n57. The database administration team has asked you to help them improve the performance of their new database server running on Compute Engine. The database is used for importing and normalizing the company’s performance statistics. It is built with MySQL running on Debian Linux. They have an n1-standard-8 virtual machine with 80 GB of SSD zonal persistent disk which they can’t restart until the next maintenance event. What should they change to get better performance from this system as soon as possible and in a cost-effective manner?\n\nDynamically resize the SSD persistent disk to 500 GB.\n\n58. You are deploying an application on App Engine that needs to integrate with an on-premises database. For security purposes, your on-premises database must not be accessible through the public Internet. What should you do?\n\nDeploy your application on App Engine flexible environment and use Cloud VPN to limit access to the on-premises database.\n\n59. You are working in a highly secured environment where public Internet access from the Compute Engine VMs is not allowed. You do not yet have a VPN connection to access an on-premises file server. You need to install specific software on a Compute Engine instance. How should you install the software?\n\nUpload the required installation files to Cloud Storage. Configure the VM on a subnet with a Private Google Access subnet. Assign only an internal IP address to the VM. Download the installation files to the VM using gsutil.\n\n60. Your company is moving 75 TB of data into Google Cloud. You want to use Cloud Storage and follow Google-recommended practices. What should you do?\n\nMove your data onto a Transfer Appliance. Use a Transfer Appliance Rehydrator to decrypt the data into Cloud Storage.\n\n61. You have an application deployed on Kubernetes Engine using a Deployment named echo-deployment. The deployment is exposed using a Service called echo- service. You need to perform an update to the application with minimal downtime to the application. What should you do?\n\nUse kubectl set image deployment/echo-deployment <new-image>\n\n62. Your company is using BigQuery as its enterprise data warehouse. Data is distributed over several Google Cloud projects. All queries on BigQuery need to be billed on a single project. You want to make sure that no query costs are incurred on the projects that contain the data. Users should be able to query the datasets, but not edit them.\n\nHow should you configure users’ access roles?\n\nAdd all users to a group. Grant the group the roles of BigQuery jobUser on the billing project and BigQuery dataViewer on the projects that contain the data.\n\n63. Your company has multiple on-premises systems that serve as sources for reporting. The data has not been maintained well and has become degraded over time.\n\nYou want to use Google-recommended practices to detect anomalies in your company data. What should you do?\n\nUpload your files into Cloud Storage. Use Cloud Dataprep to explore and clean your data.\n\n64. Google Cloud Platform resources are managed hierarchically using organisation, folders, and projects. When Cloud Identity and Access Management (IAM) policies exist at these different levels, what is the effective policy at a particular node of the hierarchy?\n\nThe effective policy is the union of the policy set at the node and policies inherited from its ancestors\n\n65. You are migrating your on-premises solution to Google Cloud in several phases. You will use Cloud VPN to maintain a connection between your on-premises systems and Google Cloud until the migration is completed. You want to make sure all your on-premise systems remain reachable during this period. How should you organize your networking in Google Cloud?\n\nUse an IP range on Google Cloud that does not overlap with the range you use on-premises\n\n66. You have found an error in your App Engine application caused by missing Cloud Datastore indexes. You have created a YAML file with the required indexes and want to deploy these new indexes to Cloud Datastore. What should you do?\n\nPoint gcloud datastore create-indexes to your configuration file\n\n67. You have an application that will run on Compute Engine. You need to design an architecture that takes into account a disaster recovery plan that requires your application to fail over to another region in case of a regional outage. What should you do?\n\nDeploy the application on two Compute Engine instance groups, each in the same project but in a different region. Use the first instance group to serve traffic, and use the HTTP load balancing service to fail over to the standby instance group in case of a disaster.\n\n68. You want to create a private connection between your instances on Compute Engine and your on-premises data center. You require a connection of at least 20\n\nGbps. You want to follow Google-recommended practices. How should you set up the connection?\n\nCreate a VPC and connect it to your on-premises data center using Dedicated Interconnect.\n\n69. You are analyzing and defining business processes to support your startup’s trial usage of GCP, and you don’t yet know what consumer demand for your product will be. Your manager requires you to minimize GCP service costs and adhere to Google best practices. What should you do?\n\nUtilize free tier and sustained use discounts. Provide training to the team about service cost management.\n\n70. You are building a continuous deployment pipeline for a project stored in a Git source repository and want to ensure that code changes can be verified deploying to production. What should you do?\n\nUse Jenkins to build the staging branches and the master branch. Build and deploy changes to production for 10% of users before doing a complete rollout.\n\n71. You have an outage in your Compute Engine managed instance group: all instance keep restarting after 5 seconds. You have a health check configured, but autoscaling is disabled. Your colleague, who is a Linux expert, offered to look into the issue. You need to make sure that he can access the VMs. What should you do?\n\nDisable the health check for the instance group. Add his SSH key to the project-wide SSH keys\n\n72. Your company is migrating its on-premises data center into the cloud. As part of the migration, you want to integrate Kubernetes Engine for workload orchestration. Parts of your architecture must also be PCI DSS-compliant. Which of the following is most accurate?\n\nKubernetes Engine and GCP provide the tools you need to build a PCI DSS-compliant environment.\n\n73. You are designing a mobile chat application. You want to ensure people cannot spoof chat messages, by providing a message were sent by a specific user.\n\nWhat should you do?\n\nUse public key infrastructure (PKI) to encrypt the message client side using the originating user's private key.\n\n74. As part of implementing their disaster recovery plan, your company is trying to replicate their production MySQL database from their private data centre to their GCP project using a Google Cloud VPN connection. They are experiencing latency issues and a small amount of packet loss that is disrupting the replication.\n\nWhat should they do?\n\nConfigure a Google Cloud Dedicated Interconnect.\n\n75. Your customer support tool logs all email and chat conversations to Cloud Bigtable for retention and analysis. What is the recommended approach for sanitizing this data of personally identifiable information or payment card information before initial storage?\n\nDe-identify the data with the Cloud Data Loss Prevention API\n\n76. You are using Cloud Shell and need to install a custom utility for use in a few weeks. Where can you store the file so it is in the default execution path and persists across sessions?\n\n~/bin\n\n77. Your organization has a 3-tier web application deployed in the same network on Google Cloud Platform. Each tier (web, API, and database) scales independently of the others. Network traffic should flow through the web to the API tier and then on to the database tier. Traffic should not flow between the web and the database tier.\n\nHow should you configure the network?\n\nAdd tags to each tier and set up firewall rules to allow the desired traffic flow\n\n78. Your development team has installed a new Linux kernel module on the batch servers in Google Compute Engine (GCE) virtual machines (VMs) to speed up the nightly batch process. Two days after the installation, 50% of the batch servers failed the nightly batch run. You want to collect details on the failure to pass back to the development team.\n\nWhich three actions should you take? Choose 3 answers.\n\n- Use stackdriver Logging to search for the module log entries.\n\n- Use gcloud or Cloud Console to connect to the serial console and observe the logs\n\n- Adjust the Google Stackdriver timeline to match the failure time, and observe the batch server metrics.\n\n79. Your company wants to try out the cloud with low risk. They want to archive approximately 100 TB of their log data to the cloud and test the analytics features available to them there, while also retaining that data as a long-term disaster recovery backup.\n\nWhich two steps should you take? Choose 2 answers.\n\n- Load logs into Google BigQuery\n\n- Upload log files into Google Cloud Storage\n\n80. You created a pipeline that can deploy your source code changes to your infrastructure in instance groups for self-healing. One of the changes negatively affects your key performance indicator. You are not sure how to fix it, and investigation could take up to a week.\n\nWhat should you do?\n\nRevert the source code change, and rerun the deployment pipeline\n\n81. Your organization wants to control IAM policies for different departments independently, but centrally.\n\nWhich approach should you take?\n\nA single Organization with Folders for each department\n\n82. Auditors visit your teams every 12 months and ask to review all the Google Cloud Identity and Access Management (Cloud IAM) policy changes in the previous 12 months. You want to streamline and expedite the analysis and audit process.\n\nWhat should you do?\n\nEnable Google Cloud Storage (GCS) log export to audit logs into a GCS bucket and delegate access to the bucket\n\n83. You are designing a large distributed application with 30 microservices. Each of your distributed microservices needs to connect to a database back-end. You want to store the credentials securely.\n\nWhere should you store the credentials?\n\nIn a secret management system\n\n84. A lead engineer wrote a custom tool that deploys virtual machines in the legacy data center. He wants to migrate the custom tool to the new cloud environment.\n\nYou want to advocate for the adoption of Google Cloud Deployment Manager.\n\nWhat are two business risks of migrating to Cloud Deployment Manager? Choose 2 answers.\n\n- Cloud Deployment Manager can be used to permanently delete cloud resources\n\n- Cloud Deployment Manager only supports automation of Google Cloud resources\n\n85. A development manager is building a new application. He asks you to review his requirements and identify what cloud technologies he can use to meet them. The application must:\n\n1. Be based on open-source technology for cloud portability\n\n2. Dynamically scale compute capacity based on demand\n\n3. Support continuous software delivery\n\n4. Run multiple segregated copies of the same application stack\n\n5. Deploy application bundles using dynamic templates\n\n6. Route network traffic to specific services based on URL\n\nWhich combination of technologies will meet all of his requirements?\n\nGoogle Kubernetes Engine, Jenkins, and Cloud Load Balancing\n\n86. You have created several preemptible Linux virtual machine instances using Google Compute Engine. You want to properly shut down your application before the virtual machines are preempted.\n\nWhat should you do?\n\nCreate a shutdown script and use it as the value for a new metadata entry with the key shutdown-script in the Cloud Platform Console when you create the new virtual machine instance\n\n87. Your solution is producing performance bugs in production that you did not see in staging and test environments. You want to adjust your test and deployment procedures to avoid this problem in the future.\n\nWhat should you do?\n\nDeploy changes to a small subset of users before rolling out to production\n\n88. A small number of API requests to your microservices-based application take a very long time. You know that each request to the API can traverse many services.\n\nYou want to know which service takes the longest in those cases.\n\nWhat should you do?\n\nInstrument your application with Stackdriver Trace in order to break down the request latencies at each microservice\n\n89. During a high traffic portion of the day, one of your relational databases crashes, but the replica is never promoted to a master. You want to avoid this in the future.\n\nWhat should you do?\n\nImplement routinely scheduled failovers of your databases\n\n90. Your organization requires that metrics from all applications be retained for 5 years for future analysis in possible legal proceedings.\n\nWhich approach should you use?\n\nConfigure Stackdriver Monitoring for all Projects, and export to Google Cloud Storage\n\n91. Your company has decided to build a backup replica of their on-premises user authentication PostgreSQL database on Google Cloud Platform. The database is 4\n\nTB, and large updates are frequent. Replication requires private address space communication.\n\nWhich networking approach should you use?\n\nGoogle Cloud Dedicated Interconnect\n\n92. Your company is forecasting a sharp increase in the number and size of Apache Spark and Hadoop jobs being run on your local datacenter. You want to utilise the cloud to help you scale this upcoming demand with the least amount of operations work and code change.\n\nWhich product should you use?\n\nGoogle Cloud Dataproc\n\n93. You want to optimize the performance of an accurate, real-time, weather-charting application. The data comes from 50,000 sensors sending 10 readings a second, in the format of a timestamp and sensor reading.\n\nWhere should you store the data?\n\nGoogle Cloud Bigtable\n\n94. Your company’s user-feedback portal comprises a standard LAMP stack replicated across two zones. It is deployed in the us-central1 region and uses autoscaled managed instance groups on all layers, except the database. Currently, only a small group of select customers have access to the portal. The portal meets a 99,99% availability SLA under these conditions. However next quarter, your company will be making the portal available to all users, including unauthenticated users. You need to develop a resiliency testing strategy to ensure the system maintains the SLA once they introduce additional user load.\n\nWhat should you do?\n\nCreate synthetic random user input, replay synthetic load until autoscale logic is triggered on at least one layer, and introduce \"chaos\" to the system by terminating random resources on both zones\n\n95. One of the developer on your team deployed their application in Google Container Engine with the Dockerfile below. They report that their application deployments are taking too long.\n\nFROM ubuntu:16.04\n\nCOPY . /src\n\nRUN apt-get update && apt-get install -y python python-pip\n\nRUN pip install -r requirement.txt\n\nYou want to optimise this Dockerfile for faster deployment times without adversely affecting the app’s functionality. Which two actions should you take? Choose 2 answers.\n\n- Use a slimmed down base image like Alpine Linux.\n\n- Copy the source after he package dependencies (Python and pip) are installed.\n\n96. Your customer is receiving reports that their recently updated Google App Engine application is taking approximately 30 seconds to load for some of their users.\n\nThis behavior was not reported before the update.\n\nWhat strategy should you take?\n\nRoll back to an earlier known good release initially, then use Stackdriver Trace and Logging to diagnose the problem in a development/test/staging environment\n\n97. A production database virtual machine on Google Compute Engine has an ext4-formatted persistent disk for data files. The database is about to run out of storage space.\n\nHow can you remediate the problem with the least amount of downtime?\n\nIn the Cloud Platform Console, increase the size of the persistent disk and use the resize2fs command in Linux.\n\n98. You are creating a solution to remove backup files older than 90 days from your backup Cloud Storage bucket. You want to optimize ongoing Cloud Storage spend.\n\nWhat should you do?\n\nWrite a lifecycle management rule in JSON and push it to the bucket with gsutil\n\n99. You have been asked to select the storage system for the click-data of your company’s large portfolio of websites. This data is streamed in from a custom website analytics package at a typical rate of 6,000 clicks per minute. With bursts of up to 8,500 clicks per second. It must have been stored for future analysis by your data science and user experience teams.\n\nWhich storage infrastructure should you choose?\n\nGoogle Cloud Bigtable\n\n100. Your company wants to track whether someone is present in a meeting room reserved for a scheduled meeting. There are 1000 meeting rooms across 5 offices on 3 continents. Each room is equipped with a motion sensor that reports its status every second. The data from the motion detector includes only a sensor ID and several different discrete items of information. Analysts will use this data, together with information about account owners and office locations.\n\nWhich database type should you use?\n\nNoSQL\n\n101. You set up an autoscaling instance group to serve web traffic for an upcoming launch. After configuring the instance group as a backend service to an HTTP(S) load balancer, you notice that virtual machine (VM) instances are being terminated and re-launched every minute. The instances do not have a public IP address.\n\nYou have verified the appropriate web response is coming from each instance using the curl command. You want to ensure the backend is configured correctly.\n\nWhat should you do?\n\nEnsure that a firewall rule exists to allow load balancer health checks to reach the instances in the instance group.\n\n102. You write a Python script to connect to Google BigQuery from a Google Compute Engine virtual machine. The script is printing errors that it cannot connect to\n\nBigQuery.\n\nWhat should you do to fix the script?\n\nRun your script on a new virtual machine with the BigQuery access scope enabled\n\n103. Your customer is moving an existing corporate application to Google Cloud Platform from an on-premises data center. The business owners require minimal user disruption. There are strict security team requirements for storing passwords.\n\nWhat authentication strategy should they use?\n\nProvision users in Google using the Google Cloud Directory Sync tool\n\n104. Your company has successfully migrated to the cloud and wants to analyze their data stream to optimize operations. They do not have any existing code for this analysis, so they are exploring all their options. These options include a mix of batch and stream processing, as they are running some hourly jobs and live- processing some data as it comes in.\n\nWhich technology should they use for this?\n\nGoogle Cloud Dataflow\n\n105. An application development team believes their current logging tool will not meet their needs for their new cloud-based product. They want a better tool to capture errors and help them analyze their historical log data. You want to help them find a solution that meets their needs.\n\nWhat should you do?\n\nHelp them define their requirements and assess viable logging tools\n\n106. You need to reduce the number of unplanned rollbacks of erroneous production deployments in your company’s web hosting platform. Improvement to the QA/Test processes accomplished an 80% reduction.\n\nWhich additional two approaches can you take to further reduce the rollbacks? Choose 2 answers.\n\n- Introduce a green-blue deployment model\n\n- Fragment the monolithic platform into microservices\n\n107. To reduce costs, the Director of Engineering has required all developers to move their development infrastructure resources from on-premises virtual machines\n\n(VMs) to Google Cloud Platform. These resources go through multiple start/stop events during the day and require state to persist. You have been asked to design the process of running a development environment in Google Cloud while providing cost visibility to the finance department.\n\nWhich two steps should you take? Choose 2 answers.\n\n- Use the - -no-auto-delete flag on all persistent disks and stop the VM\n\n- Use Google BigQuery billing export and labels to associate cost to groups\n\n108. One of your primary business objectives is being able to trust the data stored in your application. You want to log all changes to the application data.\n\nHow can you design your logging system to verify authenticity of your logs?\n\nDigitally sign each timestamp and log entry and store the signature\n\n109. Your company has decided to make a major revision of their API in order to create better experiences for their developers. They need to keep the old version of the API available and deployable, while allowing new customers and testers to try out the new API. They want to keep the same SSL and DNS records in place to serve both APIs.\n\nWhat should they do?\n\nUse separate backend pools for each API path behind the load balancer\n\n110. Your company plans to migrate a multi-petabyte data set to the cloud. The data set must be available 24hrs a day. Your business analysts have experience only with using a SQL interface.\n\nHow should you store the data to optimise it for ease of analysis?\n\nLoad data into Google BigQuery\n\nI have collected these questions from different sources while practicing for my certification exam and thought it would be great to share these with you all. Please let me know your views on this and what improvements I can do here to make it better.\n\nI am preparing a separate article with the Case study for the specific exam since that will be very vast as per conceptual understanding as well. I will link that in this article itself asap.\n\nIf you like this, please give clap/s for me, it really motivates me to write more useful things.\n\nIf you have anything in your mind and want me to write about that then also you can suggest me. My LinkedIn profile in the bio, you can connect with me from there.\n\nThank you very much for reading this.\n\nHappy Learning !!! 🙂 📚"
    }
}
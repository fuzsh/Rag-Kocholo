{
    "id": "dbpedia_8305_1",
    "rank": 14,
    "data": {
        "url": "https://arxiv.org/html/2406.05487v1",
        "read_more_link": "",
        "language": "en",
        "title": "SyDRA: An Approach to Understand Game Engine Architecture",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://arxiv.org/html/x1.png",
            "https://arxiv.org/html/extracted/5653515/images/approach/indegree.png",
            "https://arxiv.org/html/extracted/5653515/images/approach/outdegree.png",
            "https://arxiv.org/html/extracted/5653515/images/approach/centrality.png",
            "https://arxiv.org/html/x2.png",
            "https://arxiv.org/html/x3.png",
            "https://arxiv.org/html/x4.png",
            "https://arxiv.org/html/extracted/5653515/images/results/flaxengine_thirdparty.png",
            "https://arxiv.org/html/x5.png",
            "https://arxiv.org/html/x6.png",
            "https://arxiv.org/html/x7.png",
            "https://arxiv.org/html/x8.png",
            "https://arxiv.org/html/x9.png",
            "https://arxiv.org/html/extracted/5653515/images/tools/moose-exp.png",
            "https://arxiv.org/html/extracted/5653515/images/tools/vscode-exp.png",
            "https://arxiv.org/html/x10.png",
            "https://arxiv.org/html/x11.png",
            "https://arxiv.org/html/x12.png",
            "https://arxiv.org/html/x13.png",
            "https://arxiv.org/html/x14.png",
            "https://arxiv.org/html/x15.png",
            "https://arxiv.org/html/x16.png",
            "https://arxiv.org/html/x17.png",
            "https://arxiv.org/html/x18.png",
            "https://arxiv.org/html/x19.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            "game engines",
            "coupling",
            "impact analysis",
            "controlled experiment"
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "11institutetext: Concordia University, Montreal, Canada\n\n11email: {gabriel.cavalheiroullmann,yann-gael.gueheneuc}@concordia.ca 22institutetext: École de Technologie Supérieure, Montreal, Canada\n\n22email: fabio.petrillo@etsmtl.ca 33institutetext: Univ. Lille, CNRS, Inria, Centrale Lille, UMR 9189 - CRIStAL\n\n33email: nicolas.anquetil@inria.fr 44institutetext: Université de Montréal, Montreal, Canada\n\n44email: cristiano.politowski@umontreal.ca\n\nSyDRA: An Approach to Understand Game Engine Architecture\n\nGabriel C. Ullmann 11 Yann-Gaël Guéhéneuc 11 Fabio Petrillo 22 Nicolas Anquetil 33 Cristiano Politowski 44\n\nAbstract\n\nGame engines are tools to facilitate video game development. They provide graphics, sound, and physics simulation features, which would have to be otherwise implemented by developers. Even though essential for modern commercial video game development, game engines are complex and developers often struggle to understand their architecture, leading to maintainability and evolution issues that negatively affect video game productions. In this paper, we present the Subsystem-Dependency Recovery Approach (SyDRA), which helps game engine developers understand game engine architecture and therefore make informed game engine development choices. By applying this approach to 10 open-source game engines, we obtain architectural models that can be used to compare game engine architectures and identify and solve issues of excessive coupling and folder nesting. Through a controlled experiment, we show that the inspection of the architectural models derived from SyDRA enables developers to complete tasks related to architectural understanding and impact analysis in less time and with higher correctness than without these models.\n\nKeywords:\n\ngame engines, coupling, impact analysis, controlled experiment\n\n1 Introduction\n\nGame engines are tools to facilitate and accelerate video game development. They provide out-of-the-box features that are broad enough to be used to create a variety of video games, for example, graphics rendering, sound management, and physics simulation. Developing a game engine from scratch is expensive and time-consuming, an endeavour only large video game companies can afford. While risky, such endeavour brings benefits such as the possibility of optimizing the tool for a specific kind of game or game genre. Moreover, by creating their in-house solution, companies avoid paying licensing fees while being free to license their technology to others, such as CryEngine and IdTech have done recently.\n\nHowever, the advantages of developing a game engine only last as long as the understanding of its structure remains. Over time, development teams change due to high turnover (Cadin, 2006, p. 292), and the game engine code base changes, which causes it to drift away from its original structure, in a process known as architectural drift or erosion (Baabad et al., 2020, p. 2). Moreover, due to the need to innovate and experiment with different video games, companies sometimes decide to re-purpose their game engines by developing new features on top of a legacy architecture. In this scenario, the lack of understanding of the original architecture of the core game engine code causes maintenance and evolution problems. An example of this kind of problem was reported by developers of the Frostbite game engine, used by Bioware to develop Anthem in 2019 :\n\n“Frostbite is like an in-house engine with all the problems that entails—it’s poorly documented, hacked together, and so on—with all the problems of an externally sourced engine,” said one former BioWare employee. “Nobody you actually work with designed it, so you don’t know why this thing works the way it does, why this is named the way it is.”\n\nThe lack of understanding of the architecture of a game engine hinders a developer’s capacity to maintain and evolve it, considerably affecting the schedule of a game development project. For example, the re-purposing of Frostbite to Dragon Age: Inquisition “took up about a third of the project’s development time” . Similarly, developers from Bethesda added multiplayer support to Creation Engine, originally made for single-player games only. During the development of Fallout 76, this feature caused several bugs and “put additional time pressure on the schedule” . Therefore, we can claim architectural understanding impacts both the technical and managerial sides of video game development.\n\nWhile the problems reported by Frostbite and Creation Engine developers reflect the reality of closed-source game engine development, similar problems can be observed in open-source game engines. Even though game engines such as Unreal and Godot maintain official documentation and support forums, these data sources mostly focus on providing video game developers with a way to “get started” by explaining how to use the game engine’s features, while hiding low-level aspects of their implementation. Therefore, game engine developers wishing to re-purpose a game engine or choose which best fits their needs cannot rely solely on this documentation. They must plunge into the code, study its structures, and finally compare them with those of other game engines to make informed game engine development choices.\n\nIn this paper, we use the Subsystem Dependency Recovery Approach (SyDRA), described in the work of Ullmann et al. (2023), to help game engine developers understand game engine architecture and therefore make informed game engine development choices. By applying SyDRA to 10 open-source game engines, we obtain architectural models that can be used to compare game engine architectures and identify and solve problems such as high coupling and low cohesion. Through a controlled experiment with 16 participants, we show that the inspection of the architectural models derived from SyDRA enables developers to complete tasks related to architectural understanding and impact analysis in less time and with higher correctness than without these models.\n\nThe implementation and evaluation of SyDRA enable us to answer the following research questions:\n\n•\n\nRQ1: Do architectural models help developers understand game engines more effectively than by simply inspecting the code?\n\n•\n\nRQ2: Do architectural models help game engine developers perform game engine impact analysis more effectively than by simply inspecting the code?\n\nWe answer positively to these two questions and conclude that by using SyDRA, developers can better understand game engine architecture without increasing their perceived workload. Our experiments also provide insights into how developers perceive workload in software analysis tools and how it correlates with their professional experience.\n\nThe paper is organised as follows. Section 2 presents related work on software architecture and game engines. Section 3 provides a high-level description of SyDRA, our software architecture recovery approach, and how it was used to generate an architectural model of 10 open-source game engines. Section 4 describes how the architectural models resulting from SyDRA can help game engine developers solve folder organization and coupling issues, as well as make game engine development choices. Section 5 describes the design and execution of a user study and its results. Section 6 presents internal and external threats to validity, and Section 7 presents the conclusion and future work.\n\n2 Related Work\n\nThe structure and purpose of game engines are often not well-documented, and for this reason, several researchers have attempted to use software architecture recovery techniques to search for architectural structures in the source code and help developers assign them meaning. For example, Munro et al. (2009, p. 247) used Doxygen , a popular documentation generation tool, to extract dependency information from an open-source version of the IdTech game engine. This data was then used to create dependency graphs, which aided “in the process of identifying suitable improvements and enhancements to a specific engine and have supported implementing these in an appropriate manner”.\n\nResearchers conduct experiments to evaluate the usefulness of extracted architectural models, for example, Heijstek et al. (2011) and Briand et al. (2001). In these studies, they present a set of architectural understanding tasks to developers. By measuring how swiftly and correctly developers can perform tasks with and without a supporting architectural model or documentation, they assess the benefit this model brings to system understanding. In Section 5, we describe how we designed and conducted a controlled experiment based on the work of Briand et al. (2001).\n\nAlternatively, researchers may conduct field studies, observing how developers and architects use them in real scenarios (Abi-Antoun and Aldrich, 2008, p. 1). However, this kind of study requires collaboration with companies and long-term observation. Considering the closed-source nature of video game and game engine development, this would not be a viable option for our study. As we explain in Section 3, SyDRA relies on the analysis of source code and documentation, both of which must be openly available.\n\n3 Approach\n\nThe Subsystem Dependency Recovery Approach (SyDRA) comprises six steps, shown in Figure 1, and it helps game engine developers to analyse one or more game engines. SyDRA’s steps describe how a developer can consistently select game engines to analyse, cluster their files and folders and later cross-referencing this information with an include graph. As a result, they obtain architectural models that can be used to understand a game engine’s architectural structure, identify and reduce excessive coupling, and increase cohesion.\n\nOur implementation of SyDRA is available on Zenodo , along with the results from our analysis of 10 open-source game engines, which we also show and discuss. We obtain these results by implementing SyDRA’s steps as follows:\n\n1. System Selection: We select 10 open-source game engines from GitHub, searching for the “game engine” keyword, filtering by the language C++, and then ordering the results by their popularity. We consider popularity to be the number of GitHub stars in a repository. We chose this metric because it is “partial evidence for the repository containing an engineered software project” (Munaiah et al., 2017, p.7). This way, we avoid selecting “toy” projects which do not properly represent the scale and complexity of industry-grade game engines.\n\n2. Subsystem Selection: For each selected game engine, we define how to cluster each file and folder into functional groups, which are subsystems. We consider 16 subsystems described in the “Runtime Game Engine Architecture” by (Gregory, 2018, p. 33), each corresponding to common features needed to create most video games. For example, graphics, audio, physics simulation and input device processing. We summarize this list of subsystems as shown in LABEL:tab:gregory-summarized.\n\n3. Subsystem Detection: For each selected game engine, we manually cluster all files and folders into the selected subsystems. To ensure files implementing the same features are clustered into the same group, we consider their naming, folder hierarchy, mentions in the documentation and the comments found in their source code to determine their functionality.\n\n4. Include Graph Generation: For each selected game engine, we generate an include graph which represents dependencies between files. This step is done automatically with the cinclude2dot tool .\n\n5. Architectural Model Generation: For each selected game engine, we used the data obtained from Steps 3 and 4 to generate an architectural model. These models were then loaded into Moose, a software analysis platform which we describe in more detail in Section 5.3. This step is done semi-automatically.\n\n6. Architectural Model Visualisation: For each selected game engine, we use the “Architectural map” visualisation from Moose to generate a visual representation of the include graph, files, folders and subsystems. This step is done semi-automatically.\n\nAdditionally, based on the number of nodes and edges on each include graph, we computed the following metrics, illustrated in Figure 2:\n\n•\n\nIn-degree: The count of incoming edges of a node (e.g., the Physics subsystem is included by three other subsystems).\n\n•\n\nOut-degree: The count of outgoing edges of a node (e.g., the Physics subsystem includes one other subsystem).\n\n•\n\nBetweenness centrality: The extent to which a node lies in the path of others (Badar et al., 2013, p. 758) (e.g., most subsystems include the Physics subsystem, which in turn includes others).\n\nIn Section 4, we describe the structure of the architectural models obtained by applying SyDRA to 10 open-source game engines. By observing the subsystems and relationships among subsystems, we can explain the game engines they represent, and how game engine developers can use this information to increase cohesion through folder reorganization and decreasing coupling. Furthermore, in Section 5, we show examples of how the use of architectural models can inform developers performing architecture understanding and impact analysis tasks.\n\n4 Application\n\nAs a result of applying the SyDRA, we obtained 10 architectural models, one for each of the selected game engines. These models are graphs where each node is a file, and each edge is an include relationship between files. For example, using Moose “Architectural map”, these files can be viewed clustered by subsystem, or individually, as shown in Figure 3.\n\nIn this section, we show some examples of visual and numerical information extracted from these 10 architectural models and how they help developers understand three aspects of game engines: subsystem coupling, subsystem cohesion and coupling between files and subsystems.\n\n4.1 Subsystem Coupling\n\nThe models produced by the SyDRA approach enable us to visualize the high-level architecture of the game engine, meaning it shows us what features are available and how they depend on each other. In Unreal Engine and Godot, we detected all 16 subsystems described in the reference architecture (see Figure 4). In the remaining game engines, we detected 12 or more subsystems. The only exception was OlcPixelGameEngine, in which we detected only five subsystems. Therefore, 90% of the game engines we analysed contain at least 75% of the subsystems described in the reference architecture, which shows there are many similarities between the reference architecture and the actual architecture of open-source game engines.\n\nThe Core (COR), Low-Level Renderer (LLR) and Resources (RES) are the most frequently coupled subsystems, as shown in Table 2. As shown in LABEL:tab:gregory-summarized, these subsystems have several responsibilities, and we believe this is the reason behind their high coupling. For example, the Low-Level Renderer (LLR) subsystem goes beyond simply drawing on the screen and also provides abstractions that, while visual, also relates to other subsystems. For example, while camera functionality is part of Low-Level Renderer (LLR), to know what is within the view of the camera and therefore, what needs to be drawn, it depends on information from the Scene Graph / Culling Optimizations (SGC) subsystem, which provides culling and occlusion computation.\n\nCore (COR), Low-Level Renderer (LLR) and Resources (RES) have high in-degree and centrality, along with Platform Independence Layer (PLA), which likewise centralizes utilities and cross-platform compatibility code. While this type of analysis enables us to detect coupling patterns, it cannot explain why this coupling exists and whether it could be reduced. For this reason, in Section 4.3, we show examples that explain why certain subsystems are more coupled in certain game engines, whether these coupling patterns repeat in different game engines and what they show about the architecture.\n\nWhile architectural models can give us insights into groups of subsystems, or even groups of game engines, we can also focus on a particular subsystem of a particular game engine to understand its features. For example, on Figure 5, we show that, upon inspection of the Third-Party SDKs (SDK) subsystem of FlaxEngine, we can view the include relationship between its libraries and several subsystems, and from this relationships, we can infer the functionality of the libraries, even if we do not know their functionality. For example, we can infer DirectXMesh is a graphics-related library, due to it being included by with the Low-Level Renderer (LLR) subsystem. Similarly, detex, a texture decompression library, is included by the Resources (RES) subsystem, which is responsible for file loading and management.\n\nIn addition to their functional features, understanding the availability of specific libraries within the Third-Party SDKs (SDK) can offer valuable insights into both architectural considerations and functional limitations. For example, Unreal Engine uses PhysX, a physics library optimized to leverage GPU acceleration typically found in NVidia video cards. Conversely, Godot uses Bullet, and cannot benefit from this hardware performance boost. Consequently, when game engine developers are faced with a decision between two platforms based on their physics capabilities, this insight can serve to inform their choice. While present in the code and folder structures, the inspection of the architectural model helps make this information more evident to the game engine developer.\n\n4.2 Subsystem Cohesion\n\nEspecially on game engines that have been in development for decades, such as Unreal, Godot and O3DE, the folder structure tends to grow large and sometimes become excessively nested. Also, a small number of folders contains the majority of files, which is an indicator of low file cohesion. Architectural models based on files and folders, such as those produced by SyDRA, highlight where the bulk of the source code is located within the directory structure, and therefore points of low file cohesion in this structure. For example, in O3DE, most of the files implementing subsystem features are located in ./Code/Framework. By analysing the subfolder organisation of each folder we observe that the folders AzCore, AzFramework, AzNetworking and AzToolsFramework share the same folder organisation pattern, comprised as follows:\n\n•\n\nFeatures: Contain files that implement features. This folder always has the same name as its parent (e.g., AzCore has a folder also named AzCore).\n\n•\n\nPlatform: Contains files that implement a part of Platform Independence Layer (PLA) subsystem related to the “features” folder.\n\n•\n\nTests: Contains unit tests to features in the “features” folder.\n\nWe demonstrate this folder organisation pattern in Figure 6. While its rationale is not explained in O3DE’s documentation, we believe it was created to break down the Platform Independence Layer (PLA), a large subsystem that encompasses many features, into folders with a lower file count. However, while highlighting the separation between O3DE’s core and other subsystems, this organisation does not use its folder hierarchy to cohesively group all subsystem code under one folder. For example,AzQtComponents, which is a part of the Front-End (FES) subsystem, is in the same hierarchy level as AzFramework, a folder which contains files for several subsystems.\n\nIn Figure 7, we show an alternative organisation that has two folders in its top level, “Core” and “Features”, which are then subdivided by purpose: containing code from O3DE’s core only or from other subsystems. Each of these top-level folders keeps its own Platform and Test folders. Thus, we avoid naming repetition and create a more semantic folder hierarchy, which separates the subsystems and their features from the higher-level concept of “Core” vs. “Features”.\n\nWhile at first glance such folder-level refactoring may appear merely aesthetic, it can help developers re-purpose game engines. By eliminating unnecessary nesting and clustering files by subsystem, developers can focus on going forward with the development new features without being encumbered by legacy architectural structures and naming that no longer serve a purpose.\n\n4.3 File and Subsystem Coupling\n\nThe inspection of the include relationships between files and the cross-referencing of this information with game engine documentation, when it exists, may give us detailed insights into how a particular part of a subsystem works.\n\nFor example, if we look into Panda3D’s Low-Level Renderer (LLR) subsystem, we observe it has a high in-degree because of its display folder, which contains files implementing graphics-related functionality used by the Scene Graph / Culling Optimizations (SGC) and Visual Effects (VFX) subsystems.\n\nOne of these files, graphicsStateGuardian.h, implements a graphics state guardian (GSG), which receives high-level rendering instructions (e.g., drawing a character present in the scene graph) and then handles low-level rendering instructions in a format the operating system and graphics hardware can understand. As explained by Goslin and Mine (2004, p. 112):\n\n“All code specific to rendering on a particular platform is contained within a well-defined class called a graphics state guardian. After the system transforms and culls the scene graph, it hands off the graphics entities to the GSG for rendering. A game or application only needs to interact with the scene graph, which means the only part of the code that the system must port and optimize for a particular hardware platform is the local version of the GSG class itself.”\n\nIn GamePlay3d, the Profiling & Debugging (DEB) subsystem has a high in-degree because its DebugNew.h file is included by several subsystems to replace global new and delete C++ operators for “memory tracking” . We observe a similar implementation in Urho3d. We observe the Logger.h file is also frequently included for debugging purposes. Even though debugging code would normally be removed upon pushing to the master branch, we observe four files in GamePlay3d’s repository still include either DebugNew.h or Logger.h.\n\nThese are just two examples of how detailed information about subsystem functionality can help game engine developers understand the subsystems they are working with. This information is essential during the planning of game engine re-purposing, so game developers can be aware of the existing architectural structures and how the change or removal of one impacts others. In Section 5, we show other examples of how the use of architectural models can support architectural understanding and inform developers on impact analysis.\n\n4.4 Discussion\n\n\"And you see every time I made a further division, up came more boxes based on these divisions until I had a huge pyramid of boxes. Finally you see that while I was splitting the cycle up into finer and finer pieces, I was also building a structure. (…) The overall name of these interrelated structures, the genus of which the hierarchy of containment and structure of causation are just species, is a system.\" Robert M. Pirsig, Zen and the Art of Motorcycle Maintenance\n\nAs presented in Section 4, developers can identify architectural problems and reflect on solutions by visualising and analysing game engine architectural models. While useful for analysing game engines individually, we can also combine models from different game engines to observe which architectural patterns emerge from this combination, and which are, therefore, shared among all the game engines in the analysed set.\n\nFor example, in Figure 9, we use a box-and-line diagram to represent the most frequent relationships between subsystems on the 10 game engines we analysed. In the centre of the diagram, we placed the subsystems with the highest betweenness centrality, forming an inner core (dark red). Next, we placed other subsystems which appear in Table 2 in the outer core (light red). Finally, we placed the subsystems which do not appear in Table 2 in the outer core’s periphery (white). All relationships shown in the diagram are among the most frequent, as shown in Table 2. When there was a tie (e.g., two pairs had the same frequency), we chose the coupling pair with the highest sum of betweenness centrality.\n\nIn this emergent architecture, we observe that Low-Level Renderer (LLR) often inter-depends on Core (COR), which it uses to access functionality in the Platform Compatibility Layer (PLA) and Resources (RES). It is often included by the World Editor (EDI) and Gameplay Foundations (GMP), which are both visual interfaces between the user and the game engine. Because it manages UI elements that emit events and trigger actions throughout the system, Front End (FES) often depends on the event/messaging system in Core (COR).\n\nBy providing a high-level view of subsystem includes, an emergent game engine architecture such as we show here can provide an architectural reference to game engine developers wishing to build a new game engine that structurally resembles the set of game engines we analysed. It may also serve as a guide to impact analysis of an existing game engine, showing which subsystems might be affected by a change in any other number of subsystems.\n\n5 Controlled Experiment\n\nWe conduct a controlled experiment with 16 developers to determine the qualitative success of SyDRA in supporting developers’ understanding and maintenance of game engines. We base our study design on another similar study by Briand et al. (2001, p. 518), henceforth called “original experiment”. In this section, we describe the design and execution of our controlled experiment, presenting and discussing the obtained results.\n\nWe employed a between-group 2 x 1 design, as described in Table 3. The independent variables are the experimental runs (X) and the tools used to analyse Godot (Y). The assignment of participants to the control (A) and treatment (B) groups was done randomly to control learning and fatigue effects. Tasks were shown randomly to participants, except for Tasks 3 and 4, which depended on each other and could not be understood if shown in reverse order. We provide more details about our task choices in Section 5.4.\n\n5.1 Hypotheses\n\nThe null hypothesis is stated as:\n\n•\n\nH0: Using SyDRA provides no significant difference in the understandability and maintainability of game engine architecture.\n\nThe alternative hypotheses, i.e., what is expected to occur, are stated as:\n\n•\n\nH1: It is significantly easier to understand game engine architecture\n\nby using SyDRA.\n\n•\n\nH2: It is significantly easier to perform impact analysis (locate changes)\n\nin game engines by using SyDRA.\n\n5.2 Participants\n\nWe recruited 16 participants, all over 18 years of age and with prior experience in object-oriented programming. We recruited them via email or by asking them in person. Most participants are men under 30 based in Brazil or Canada. They are mostly students, researchers or software developers outside the video game industry. They have mostly 2 to 5 years of software development experience and have used Unity for student or hobby projects.\n\nParticipants’ familiarity with game engine usage and development varied greatly. For example, while 57% of the participants reported no experience with game engines, two participants reported developing their own game engines. This diversity of levels of experience is important because it allows us to observe how the tools we selected for the study were used differently by each kind of developer and their challenges. A more detailed breakdown of demographics can be found in the replication package we published on Zenodo .\n\n5.3 Materials\n\nIn this study, participants analysed Godot , a cross-platform, free and open-source game engine released by Juan Linietsky and Ariel Manzur in 2014. We chose Godot due to its relevance to the open-source developer community on GitHub, as explained in Section 3. While control group participants used exclusively Visual Studio Code to analyse Godot’s source code, treatment group participants used both Moose + Visual Studio Code, and therefore had access to the “Architectural map” visualisation of Godot produced with SyDRA.\n\nWe created instructional documents to teach participants to use the given tools. These documents provide step-by-step instructions accompanied by screenshots, illustrating how to use tool features. Additionally, at the end of each document, there were three optional exercises to motivate participants to put into practice the instructions they just read. As we explain in Section 5.5, we asked participants to read the document related to the tool they were about to use before performing the tasks.\n\nIn this section, we provide an overview of the main features of Moose and Visual Studio Code and our rationale for choosing these tools for the study. We also describe the features participants used during the experiment to aid them in the completion of the tasks and show examples of the screenshots included in the instructional documents. Moose: A platform for software analysis composed of several tools built on top of the Pharo programming language. It enables users to define metamodels and create models based on them. It also allows users to visualise these models as “Architectural Maps”, as explained in Section 3. Model entities can be written (or “propagated” in Moose’s jargon) to a bus, which is a channel of communication between tools (Anquetil et al., 2020, p.130). Moose tools, such as the “Architectural Map”, can then read entities from the bus and do something with them (e.g., draw a visualisation).\n\nDuring the study, as shown in Figure 10, treatment group participants located files and folders using Moose “Architectural Map” (top right) and propagated them to a built-in source code browser to inspect the source code (bottom). Participants could launch Visual Studio Code from Moose built-in editor to use features such as code folding, syntax highlighting, and search, not available on Moose. They could also use Moose tag browser to see the list of files clustered into each subsystem, represented as coloured tags (top left). Visual Studio Code: A source code editor released by Microsoft in 2015. Also commonly referred to as VS Code. During the study, control group participants used it to locate files and folders inside the Godot repository, read source code and search for words as directed by the task statements, as we show in Figure 11. We chose VS Code due to its broad popularity among software developers. According to the Stack Overflow Developer Survey 2023, 74.09% of professional developers and 78.39% of developers learning to code use VS Code .\n\n5.4 Tasks\n\nWe asked participants to perform nine tasks during the study, which were of two kinds: architectural understanding and impact analysis. In architectural understanding tasks, we asked participants to explain game engine subsystems and dependencies between files. In impact analysis tasks, we asked participants to point out which files should be changed due to a change/removal of functionality in another part of the system. Participants performed seven architectural understanding tasks and two impact analysis tasks.\n\nWe wrote our task statements based on those provided in Appendix A of the original experiment (Briand et al., 2001, p. 527). We changed the statements slightly to make them easier for novice developers to understand. We also adapted task statements to reflect the steps participants in different groups had to perform to find files in the tools they were using, as demonstrated in Table 4. For example, while in Task 2, we asked treatment group participants to “Expand the Audio subsystem” and then “propagate” a given file, we asked control group participants to search the file by name and then open it . This way, we ensured both participants were directed to the same file, even though they followed different steps to find and open it.\n\n5.5 Procedures\n\nThe study session was divided into three parts. First, we asked participants to read and follow the instructional document described in Section 5.3. Then, we asked them to perform the tasks. Finally, participants completed a debriefing questionnaire, where we asked them for background information and also for a workload assessment of the tasks they performed. For the second and third parts, participants submitted their answers via an online form, which also computed the time elapsed between answers.\n\nThe debriefing questionnaire was divided into two parts . In questions 1 to 7, we asked participants about their professional backgrounds and demographics. In questions 8 to 13, we asked participants to make a workload assessment of the tasks they performed based on the NASA TLX (Task Load Index) questionnaire. We chose NASA TLX because it has been used for over 20 years by several studies that evaluate software development (Al Madi et al., 2022, p.668), interface design and decision-making activities (Hart, 2006, p.906).\n\nWe used the information provided by participants in the debriefing questionnaire to qualitatively measure their perception of effort and stress concerning the tools they used and the tasks they performed. We also correlated their performance with their years of development experience and familiarity with game engines to understand how each variable influences performance. We discuss these comparisons in more detail in Section 4.\n\nWe remained available throughout the study session to support participants but kept physically distanced from them. Our support was limited to clarifying task descriptions when asked and resolving technical issues with the computer and tools related to the study.\n\n5.6 Measurements\n\nWe measured the level of game engine architectural understanding by the participants by measuring the time they spent on tasks and how correctly they completed tasks. We derived six dependent variables from this data, as in the original experiment (Briand et al., 2001, p. 518):\n\n•\n\nUndTime: Time spent on architectural understanding tasks in minutes.\n\n•\n\nUndCorr: Correctness of architectural understanding tasks (e.g., the number of tasks correctly answered).\n\n•\n\nModTime: Time spent on impact analysis tasks in minutes.\n\n•\n\nModComp: Completeness of the impact analysis, obtained by dividing the number of correct files informed by the participant by the actual number of correct files.\n\n•\n\nModCorr: Correctness of the impact analysis, obtained by dividing the number of correct files informed by the participant by the total number of files informed by the participant.\n\n•\n\nModRate: Modification rate, obtained by dividing the number of correct files informed by the participant by ModTime.\n\nThe original study did not define architectural understanding correctness, so we defined it as binary: an answer is either correct (1) or incorrect (0). Therefore, UndCorr ranges from zero to seven, given there were seven architectural understanding tasks in total. For Impact Analysis tasks, correctness is defined as the ratio between the number of files the participant informed correctly, and the total number of files they informed. Therefore, ModCorr ranges from zero to two, given there were two impact analysis tasks in total.\n\n5.7 Data Analysis Procedures\n\nWe collected data from all participants during a single experimental run, which lasted about one month. Therefore, eight data points were available for the control group and eight for the treatment group. All participants answered all tasks and debriefing questions. In this section, we summarise the statistical techniques we used to determine whether the data collected during the study was statistically significant:\n\n1.\n\nNumber of participants: We used a two-sample T-test to determine the number of participants we would need to detect statistically significant differences. We considered α=0.05𝛼0.05\\alpha=0.05italic_α = 0.05, a statistical power of 0.9 and standard deviations based on the original experiment. For task completion time, we defined a minimum difference of 1 minute. For task correctness, we defined a minimum 30% difference. The minimum number of participants would be 14 in total. We exceeded this number by conducting the study with 16 participants to ensure statistical significance.\n\n2.\n\nNormality: We used both the Kolmogorov-Smirnov and the Shapiro-Wilks’ W normality tests. In both cases, we observed all dependent variables had non-normal distributions.\n\n3.\n\nStatistical Significance: We used a non-parametric significance test that is adequate for non-normal data, the Wilcoxon Matched Pairs test. To be significant, the result of the test, called the Z value, must exceed the critical Z value for α=0.05𝛼0.05\\alpha=0.05italic_α = 0.05, one-tailed, as provided by the Wilcoxon Signed-Ranks Table . We observed UndTime, UndCorr, ModCorr and ModRate exceeded the critical Z value, while ModTime and ModComp did not.\n\n4.\n\nEffect Size: We computed the effect size for each variable by calculating the difference between the control group and treatment group arithmetic means, then dividing the result by the geometric mean of the control group and treatment group standard deviations .\n\n5.8 Results\n\nIn Table 5 we show a summary of the dependent variables collected from the 16 participants of the study. The columns represent the mean (X¯¯𝑋\\overline{X}over¯ start_ARG italic_X end_ARG), the median (m~~𝑚\\tilde{m}over~ start_ARG italic_m end_ARG), minimum and maximum values and standard deviation (s). On average, participants took 62 minutes to complete understanding tasks and 31 minutes to complete impact analysis tasks, totalling 1 hour and 33 minutes. From the standard deviation, we observe a high variability in both completion time and correctness, reflecting the participants’ diverse levels of experience.\n\nBy observing variables related to time (UndTime) and correctness (UndCorr) together in Figure 12(a) and Figure 12(c), we can understand how the tool used by each group influenced the performance of participants in the architectural understanding tasks. While the treatment group completed the tasks faster, both groups completed them with the same level of correctness. Therefore, using SyDRA decreases task completion time but has no effect on task correctness.\n\nIn impact analysis tasks, the control group completed tasks faster but also less correctly than the treatment group, as we show in Figure 12(b) and Figure 12(d). This happened because, by using exclusively VS Code, participants had more difficulty finding all outgoing and incoming include relationships between files and ended their analysis prematurely. In contrast, participants using SyDRA could succinctly visualize relationships as arrows in the “Architectural Map”, which helped them complete impact analysis more correctly.\n\nHowever, we must also consider statistical significance and effect size when interpreting these results. For example, while UndTime, UndCorr and ModCorr are statistically significant, ModTime is not, which means the average impact analysis time observed in this study cannot be generalised. Moreover, we did not identify a large effect size for any statistically significant variables, as shown in Table 6. Considering the scale defined by Kampenes et al. (2007) for software engineering, the largest effect size for a statistically significant variable was “medium” for ModCorr.\n\nConclusion of the Controlled Experiment:\n\nBased on our observations, we accept both H1 and H2. The results show that using SyDRA enables a statistically significant decrease in task completion time while not affecting task correctness. Regarding impact analysis, using SyDRA results in slightly higher task correctness but no statistically significant difference in task completion time.\n\n5.9 Discussion\n\nIn Figure 13, we compare participant answers about six aspects of task load described in the NASA TLX questionnaire: mental, physical and temporal demand, perception of success, effort and frustration. As for the perception of success and temporal demand, there was no difference between groups, which is evidence that the tools the participants used did not make them feel overwhelmed.\n\nWe also observe that the participant’s perception of success correlates with their experience level. For example, video game developers reported a higher perception of success, lower mental demand and lower frustration than non-video game developers and students/researchers. We observe the same pattern when comparing novice (less than five years of professional experience) and experienced (five years of professional experience or more) developers of all backgrounds.\n\nThe treatment group reported lower mental demand, perception of effort and frustration when compared to the control group, which is evidence that participants felt more comfortable using VS Code with the SyDRA-generated architectural model instead of VS Code only. In contrast, the treatment group reported higher physical demand than the control group. However, physical demand is not a major contributor to workload in software development (Al Madi et al., 2022, p.671). Therefore, we believe these reports of high physical demand are not evidence that using SyDRA necessarily demands more body movement or effort than VS Code only.\n\nOverall, the data we collected through the NASA TLX questionnaire shows that participants perceive a lower task load when using SyDRA, even though this perception correlates to each participant’s professional experience and familiarity with the video game domain. The largest difference we observed between the groups was in the perception of task mental demand and frustration.\n\n6 Threats to validity\n\nIn this section, we discuss threats to the validity of SyDRA and the controlled experiment.\n\n6.1 External Validity\n\nWe acknowledge that the game engines we selected for analysis with SyDRA may not be entirely representative of all open-source game engines or the entire video game industry. We mitigated this issue by selecting game engines based on their popularity, as described in Section 3. We confined our analysis to C++ game engines, which may have led to the exclusion of pertinent game engines developed in other programming languages.\n\nMoreover, we acknowledge using “Runtime Game Engine Architecture” (Gregory, 2018, p. 33) in subsystem detection across all game engines, potentially introducing a bias. As a mitigation strategy, in future work, we intend to encompass a wider spectrum of subsystems, both obtained via SyDRA and game engine development literature.\n\nIn the controlled experiment, we used a 2x1 design, not within-subject, which differs from the original 2x2 within-subject design. We chose this design due to limited participant availability, which made it hard or sometimes impossible to ensure all participants could participate twice in the study. As explained in the original experiment, by using the 2x2 within-subject design “the error variance due to differences among subjects is reduced” (Briand et al., 2001, p.518). We are aware that by choosing the 2x1 design we risked obtaining higher error variance for all dependent variables. In future work, we intend to run another study with more participants and also use a 2x2 within-subject design.\n\nMoreover, a confounding effect may result from our selection of game engines, analysis tools and participants for the controlled experiment. For example, Godot may not be representative of all open-source game engines in size and complexity. Also, most participants did not have prior experience with video game development and therefore do not accurately represent developers in these domains. Finally, we did not detect a large effect size for any of the statistically significant dependent variables, which is evidence that our results may not generalize to other game engines or more diverse participants.\n\n6.2 Internal Validity\n\nThe subsystem detection step of SyDRA was performed manually by the first author, which may have introduced a bias in the process. To mitigate this issue, we intend to assign multiple people to work in this step and later combine their results by consensus. We also intend to explore quasi-automated approaches for subsystem detection to determine the most suitable method for game engines and other types of software.\n\nMoreover, we are aware that our analysis of SyDRA’s results is dependent on Moose behaviour, and also on the graph metric (e.g., in-degree and centrality) we extracted from the architectural models. Changing them could also change the results and therefore our perception of these game engine architectures. In future work, we intend to experiment with different software analysis and visualisation tools and measure to what extent they can help developers perform architectural understanding, impact analysis and testing activities.\n\nIn the context of the controlled experiment, while we measured how quickly and correctly participants completed understanding and impact analysis tasks, we did not measure whether they would be able to implement the changes in the source code quickly and correctly as well. Also, while tasks allowed for several possible solutions, we did not verify whether the solution provided by the participant was the best or most optimized in practice, only whether it was architecturally sound.\n\nWe are aware that most task statements and debriefing questions allowed for multiple interpretations and that may have been the reason for the large variation in terms of task completion time and task correctness we observed. An observer effect may have also contributed to this variation, considering participants may have felt less stressed or behaved differently if they were not being observed. As we explained in Section 5.5, we tried to mitigate this effect by physically distancing from the participants and interacting with them only when necessary.\n\nA maturation effect may have occurred in the study due to participants learning how the tools work as the study proceeded. Some participants also had prior experience with the tools used in the study, which might have helped them complete tasks faster and more correctly than others. As stated in Section 5.3, we mitigated this effect by choosing a tool that is known by most developers (Visual Studio Code), as well as providing instructional documents for both tools.\n\nAn instrumentation effect may also have occurred due to differences between control and treatment task descriptions. As explained in Section 5.4, we did our best to ensure the tasks could be completed with a similar amount of effort in both tools and that the task descriptions were clear and stated in the same way to both groups. However, it is possible that these differences also influenced the participants’ abilities to understand and complete tasks more correctly.\n\n7 Conclusion\n\nIn this paper, we present the Subsystem-Dependency Recovery Approach (SyDRA). By applying this approach to 10 open-source game engines, we obtain architectural models which can be used to compare game engine architectures and identify and solve problems such as high coupling and low cohesion. Additionally, we showed and discussed ways in which the metrics and visualisations derived from architectural models can aid game engine development, such as:\n\n•\n\nArchitectural Understanding: Architectural model visualisations provide a friendly way for novice game engine developers to understand this kind of system and start developing their own subsystems or plugins. Moreover, we show how the use of architectural models can aid architectural understanding in Section 5.8.\n\n•\n\nImpact Analysis: Game engine developers can refactor their code more safely by visualising how changes to a subsystem could impact the whole game engine.\n\n•\n\nReference Extraction: Game engine architects seeking to design a new engine can extract architectural models from similar systems and use them as references. They can either extract and visualize data for a single system or join data from multiple systems within the same family, as we show in Figure 9. This is useful both for large companies and small indie developers who develop tailor-made solutions, e.g., for performance.\n\nThrough a controlled experiment with 16 software developers, we evaluated the extent to which SyDRA helps developers perform architecture understanding and impact analysis tasks. We asked developers to analyse Godot, an open-source game engine. While control group participants used exclusively VS Code to analyse Godot’s source code, treatment group participants used both Moose + Visual Studio Code, and therefore had access to the “Architectural map” visualisation of Godot produced with SyDRA. Our experiment’s most important contribution is showing a small yet statistically significant decrease in completion time for architectural understanding tasks when using VS Code in conjunction with the SyDRA-generated architectural model.\n\nFinally, our analysis of the answers in the experiment’s debriefing questionnaires shows that by using SyDRA developers can better understand game engine architecture while not increasing their perceived workload. In future work, we intend to ask game engine developers to evaluate our suggestions for subsystem coupling reduction (Section 4.1) and subsystem cohesion increase (as shown in Section 4.2), as well as the emergent game engine architecture we propose in Section 4.4. We aim to understand whether these suggestions are useful in a professional video game development setting. Moreover, we are aware SyDRA is flexible enough to be used in the analysis of other software families which are fundamental to video game production, such as image and sound editors and 3D modelling software. Therefore, we intend to apply SyDRA to these software families, to identify common architectural structures that can guide their creation, maintenance and evolution.\n\nAcknowledgements\n\nThe authors were partially supported by the NSERC Discovery Grant and Canada Research Chairs programs.\n\nReferences\n\n(1)\n\nAbi-Antoun and Aldrich (2008) Marwan Abi-Antoun and Jonathan Aldrich. 2008. A field study in static extraction of runtime architectures. In Proceedings of the 8th ACM SIGPLAN-SIGSOFT workshop on Program analysis for software tools and engineering. ACM, Atlanta Georgia, 22–28. https://doi.org/10.1145/1512475.1512481\n\nAgrahari and Chimalakonda (2021) Vartika Agrahari and Sridhar Chimalakonda. 2021. What’s Inside Unreal Engine? - A Curious Gaze!. In 14th Innovations in Software Engineering Conference (formerly known as India Software Engineering Conference). ACM, Bhubaneswar, Odisha India, 1–5. https://doi.org/10.1145/3452383.3452404\n\nAl Madi et al. (2022) Naser Al Madi, Siyuan Peng, and Tamsin Rogers. 2022. Assessing Workload Perception in Introductory Computer Science Projects using NASA-TLX. In Proceedings of the 53rd ACM Technical Symposium on Computer Science Education. ACM, Providence RI USA, 668–674. https://doi.org/10.1145/3478431.3499406\n\nAnquetil et al. (2020) Nicolas Anquetil, Anne Etien, Mahugnon H. Houekpetodji, Benoit Verhaeghe, Stéphane Ducasse, Clotilde Toullec, Fatiha Djareddir, Jerôme Sudich, and Moustapha Derras. 2020. Modular Moose: A New Generation of Software Reverse Engineering Platform. In Reuse in Emerging Software Engineering Practices, Sihem Ben Sassi, Stéphane Ducasse, and Hafedh Mili (Eds.). Vol. 12541. Springer International Publishing, Cham, 119–134. https://doi.org/10.1007/978-3-030-64694-3_8 Series Title: Lecture Notes in Computer Science.\n\nBaabad et al. (2020) Ahmed Baabad, Hazura Binti Zulzalil, Sa’adah Hassan, and Salmi Binti Baharom. 2020. Software Architecture Degradation in Open Source Software: A Systematic Literature Review. IEEE Access 8 (2020), 173681–173709. https://doi.org/10.1109/ACCESS.2020.3024671\n\nBadar et al. (2013) Kamal Badar, Julie M. Hite, and Yuosre F. Badir. 2013. Examining the relationship of co-authorship network centrality and gender on academic research performance: the case of chemistry researchers in Pakistan. Scientometrics 94, 2 (Feb. 2013), 755–775. https://doi.org/10.1007/s11192-012-0764-z\n\nBriand et al. (2001) L.C. Briand, C. Bunse, and J.W. Daly. 2001. A controlled experiment for evaluating quality guidelines on the maintainability of object-oriented designs. IEEE Transactions on Software Engineering 27, 6 (June 2001), 513–530. https://doi.org/10.1109/32.926174\n\nCadin (2006) Loïc Cadin. 2006. HRM Practices in the Video Games Industry : Industry or Country Contingent ? (avec Defillippi R. et F. Guérin). European Management Journal 24, 4 (Aug. 2006), 288–298. https://shs.hal.science/halshs-00271887\n\nGoslin and Mine (2004) M. Goslin and M.R. Mine. 2004. The Panda3D graphics engine. Computer 37, 10 (Oct. 2004), 112–114. https://doi.org/10.1109/MC.2004.180\n\nGregory (2018) Jason Gregory. 2018. Game engine architecture (3 ed.). Taylor & Francis, CRC Press, Boca Raton.\n\nHart (2006) Sandra G. Hart. 2006. Nasa-Task Load Index (NASA-TLX); 20 Years Later. Proceedings of the Human Factors and Ergonomics Society Annual Meeting 50, 9 (Oct. 2006), 904–908. https://doi.org/10.1177/154193120605000909\n\nHeijstek et al. (2011) Werner Heijstek, Thomas Kuhne, and Michel R.V. Chaudron. 2011. Experimental Analysis of Textual and Graphical Representations for Software Architecture Design. In 2011 International Symposium on Empirical Software Engineering and Measurement. IEEE, Banff, AB, Canada, 167–176. https://doi.org/10.1109/ESEM.2011.25\n\nKampenes et al. (2007) Vigdis By Kampenes, Tore Dybå, Jo E. Hannay, and Dag I.K. Sjøberg. 2007. A systematic review of effect size in software engineering experiments. Information and Software Technology 49, 11-12 (Nov. 2007), 1073–1086. https://doi.org/10.1016/j.infsof.2007.02.015\n\nMunaiah et al. (2017) Nuthan Munaiah, Steven Kroh, Craig Cabrey, and Meiyappan Nagappan. 2017. Curating GitHub for engineered software projects. Empirical Software Engineering 22, 6 (Dec. 2017), 3219–3253. https://doi.org/10.1007/s10664-017-9512-6\n\nMunro et al. (2009) James Munro, Cornelia Boldyreff, and Andrea Capiluppi. 2009. Architectural studies of games engines &#x2014; The quake series. In 2009 International IEEE Consumer Electronics Society’s Games Innovations Conference. IEEE, London, UK, 246–255. https://doi.org/10.1109/ICEGIC.2009.5293600\n\nUllmann et al. (2023) Gabriel C. Ullmann, Yann-Gaël Guéhéneuc, Fabio Petrillo, Nicolas Anquetil, and Cristiano Politowski. 2023. Visualising Game Engine Subsystem Coupling Patterns. In Entertainment Computing – ICEC 2023, Paolo Ciancarini, Angelo Di Iorio, Helmut Hlavacs, and Francesco Poggi (Eds.). Vol. 14455. Springer Nature Singapore, Singapore, 263–274. https://doi.org/10.1007/978-981-99-8248-6_22 Series Title: Lecture Notes in Computer Science."
    }
}
{
    "id": "dbpedia_8305_3",
    "rank": 47,
    "data": {
        "url": "https://stats.stackexchange.com/questions/3713/choosing-a-clustering-method",
        "read_more_link": "",
        "language": "en",
        "title": "Choosing a clustering method",
        "top_image": "https://cdn.sstatic.net/Sites/stats/Img/apple-touch-icon@2.png?v=344f57aa10cc",
        "meta_img": "https://cdn.sstatic.net/Sites/stats/Img/apple-touch-icon@2.png?v=344f57aa10cc",
        "images": [
            "https://cdn.sstatic.net/Sites/stats/Img/logo.svg?v=60d6be2c448d",
            "https://i.sstatic.net/TWeCs.png?s=64",
            "https://www.gravatar.com/avatar/8f168d2bb191fcc5e8429f5b94e332ee?s=64&d=identicon&r=PG",
            "https://www.gravatar.com/avatar/a007be5a61f6aa8f3e85ae2fc18dd66e?s=64&d=identicon&r=PG",
            "https://www.gravatar.com/avatar/8fde274037596ee56ec97ce209c4ecdf?s=64&d=identicon&r=PG",
            "https://www.gravatar.com/avatar/d4164d26264f27e45e05a8db2c53f7fe?s=64&d=identicon&r=PG",
            "https://www.gravatar.com/avatar/9ebcb424936c7a6d6eb19bc6607f0f96?s=64&d=identicon&r=PG",
            "https://www.gravatar.com/avatar/d5f2ff5386a74effff73168d50d55c82?s=64&d=identicon&r=PG",
            "https://i.sstatic.net/rJb36.jpg?s=64",
            "https://stats.stackexchange.com/posts/3713/ivc/0f69?prg=b1b3f6ad-8470-425e-9cc2-601cd8fa2246"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2010-10-18T15:58:40",
        "summary": "",
        "meta_description": "When using cluster analysis on a data set to group similar cases, one needs to choose among a large number of clustering methods and measures of distance.  Sometimes, one choice might influence the...",
        "meta_lang": "en",
        "meta_favicon": "https://cdn.sstatic.net/Sites/stats/Img/favicon.ico?v=8f7a5a991257",
        "meta_site_name": "Cross Validated",
        "canonical_link": "https://stats.stackexchange.com/questions/3713/choosing-a-clustering-method",
        "text": "There is no definitive answer to your question, as even within the same method the choice of the distance to represent individuals (dis)similarity may yield different result, e.g. when using euclidean vs. squared euclidean in hierarchical clustering. As an other example, for binary data, you can choose the Jaccard index as a measure of similarity and proceed with classical hierarchical clustering; but there are alternative approaches, like the Mona (Monothetic Analysis) algorithm which only considers one variable at a time, while other hierarchical approaches (e.g. classical HC, Agnes, Diana) use all variables at each step. The k-means approach has been extended in various way, including partitioning around medoids (PAM) or representative objects rather than centroids (Kaufman and Rousseuw, 1990), or fuzzy clustering (Chung and Lee, 1992). For instance, the main difference between the k-means and PAM is that PAM minimizes a sum of dissimilarities rather than a sum of squared euclidean distances; fuzzy clustering allows to consider \"partial membership\" (we associate to each observation a weight reflecting class membership). And for methods relying on a probabilistic framework, or so-called model-based clustering (or latent profile analysis for the psychometricians), there is a great package: Mclust. So definitively, you need to consider how to define the resemblance of individuals as well as the method for linking individuals together (recursive or iterative clustering, strict or fuzzy class membership, unsupervised or semi-supervised approach, etc.).\n\nUsually, to assess cluster stability, it is interesting to compare several algorithm which basically \"share\" some similarity (e.g. k-means and hierarchical clustering, because euclidean distance work for both). For assessing the concordance between two cluster solutions, some pointers were suggested in response to this question, Where to cut a dendrogram? (see also the cross-references for other link on this website). If you are using R, you will see that several packages are already available in Task View on Cluster Analysis, and several packages include vignettes that explain specific methods or provide case studies.\n\nCluster Analysis: Basic Concepts and Algorithms provides a good overview of several techniques used in Cluster Analysis. As for a good recent book with R illustrations, I would recommend chapter 12 of Izenman, Modern Multivariate Statistical Techniques (Springer, 2008). A couple of other standard references is given below:\n\nCormack, R., 1971. A review of classification. Journal of the Royal Statistical Society, A 134, 321–367.\n\nEveritt, B., 1974. Cluster analysis. London: Heinemann Educ. Books.\n\nGordon, A., 1987. A review of hierarchical classification. Journal of the Royal Statistical Society, A 150, 119–137.\n\nGordon, A., 1999. Classification, 2nd Edition. Chapman and Hall.\n\nKaufman, L., Rousseuw, P., 1990. Finding Groups in Data: An Introduction to Cluster Analysis. New York, Wiley.\n\nYou can't know in advance which clustering algorithm would be better, but there are some clues, for example if you want to cluster images there are certain algorithms you should try first like Fuzzy Art, or if you want to group faces you should start with (GGCI) global geometric clustering for image.\n\nAnyway this does not guarantee the best result, so what I would do is use a program that allows you to methodically run different cluster algorithms, such as weka, RapidMiner or even R (which is non visual), There I will set the program to launch all the different clustering algorithms I can, with all the possible different distances, and if they need parameters, experiment each one with a variety of different parameter values (besides if I do not know the amount of clusters, run each one with a variety of numbers of it). Once you settled the experiment, leave it running, but remember to store somewhere the results of each clustering run.\n\nThen compare the results in order to obtain the best resulting clustering. This is tricky because there are several metrics you can compare and not all are provided by every algorithm. For example fuzzy clustering algorithms have different metrics than non-fuzzy, but they can still be compared by considering the fuzzy resulting groups as non-fuzzy, I will stick for the comparison to the classic metrics such as:\n\n• SSE: sum of the square error from the items of each cluster.\n\n• Inter cluster distance: sum of the square distance between each cluster centroid.\n\n• Intra cluster distance for each cluster: sum of the square distance from the items of each cluster to its centroid.\n\n• Maximum Radius: largest distance from an instance to its cluster centroid.\n\n• Average Radius: sum of the largest distance from an instance to its cluster centroid divided by the number of clusters.\n\nChoosing the right distance is not an elementary task. When we want to make a cluster analysis on a data set, different results could appear using different distances, so it's very important to be careful in which distance to choose because we can make a false good artefact that capture well the variability, but actually without sense in our problem.\n\nThe Euclidean distance is appropriate when I have continuous numerical variables and I want to reflect absolute distances. This distance takes into account every variable and doesn’t remove redundancies, so if I had three variables that explain the same (are correlated), I would weight this effect by three. Moreover, this distance is not scale invariant, so generally I have to scale previously to use the distance.\n\nExample ecology: We have different observations from many localities, of which the experts have taken samples of some microbiological, physical and chemical factors. We want to find patterns in ecosystems. These factors have a high correlation, but we know everyone is relevant, so we don’t want to remove these redundancies. We use the Euclidean distance with scaled data to avoid the effect of units.\n\nThe Mahalanobis distance is appropriate when I have continuous numerical variables and I want to reflect absolute distances, but we want to remove redundancies. If we have repeated variables, their repetitious effect will disappear.\n\nThe family Hellinger, Species Profile and Chord distance are appropriate when we want to make emphasis on differences between variables, when we want to differentiate profiles. These distances weights by total quantities of each observation, in such a way that the distances are small when variable by variable the individuals are more similar, although in absolute magnitudes was very different. Watch out! These distances reflect very well the difference between profiles, but lost the magnitude effect. They could be very useful when we have different sample sizes. Example ecology: We want to study the fauna of many lands and we have a data matrix of an inventory of the gastropod (sampling locations in rows and species names in columns). The matrix is characterized by having many zeros and different magnitudes because some localities have some species and others have other species. We could use Hellinger distance.\n\nBray-Curtis is quite similar, but it’s more appropriate when we want to differentiate profiles and also take relative magnitudes into account.\n\nHere is a summary of several clustering algorithms that can help to answer the question\n\n\"which clustering technique i should use?\"\n\nThere is no objectively \"correct\" clustering algorithm Ref\n\nClustering algorithms can be categorized based on their \"cluster model\". An algorithm designed for a particular kind of model will generally fail on a different kind of model. For eg, k-means cannot find non-convex clusters, it can find only circular shaped clusters.\n\nTherefore, understanding these \"cluster models\" becomes the key to understanding how to choose among the various clustering algorithms / methods. Typical cluster models include:\n\n[1] Connectivity models: Builds models based on distance connectivity. Eg hierarchical clustering. Used when we need different partitioning based on tree cut height. R function: hclust in stats package.\n\n[2] Centroid models: Builds models by representing each cluster by a single mean vector. Used when we need crisp partitioning (as opposed to fuzzy clustering described later). R function: kmeans in stats package.\n\n[3] Distribution models: Builds models based on statistical distributions such as multivariate normal distributions used by the expectation-maximization algorithm. Used when cluster shapes can be arbitrary unlike k-means which assumes circular clusters. R function: emcluster in the emcluster package.\n\n[4] Density models: Builds models based on clusters as connected dense regions in the data space. Eg DBSCAN and OPTICS. Used when cluster shapes can be arbitrary unlike k-means which assumes circular clusters.. R function dbscan in package dbscan.\n\n[5] Subspace models: Builds models based on both cluster members and relevant attributes. Eg biclustering (also known as co-clustering or two-mode-clustering). Used when simultaneous row and column clustering is needed. R function biclust in biclust package.\n\n[6] Group models: Builds models based on the grouping information. Eg collaborative filtering (recommender algorithm). R function Recommender in recommenderlab package.\n\n[7] Graph-based models: Builds models based on clique. Community structure detection algorithms try to find dense subgraphs in directed or undirected graphs. Eg R function cluster_walktrap in igraph package.\n\n[8] Kohonen Self-Organizing Feature Map: Builds models based on neural network. R function som in the kohonen package.\n\n[9] Spectral Clustering: Builds models based on non-convex cluster structure, or when a measure of the center is not a suitable description of the complete cluster. R function specc in the kernlab package.\n\n[10] subspace clustering : For high-dimensional data, distance functions could be problematic. cluster models include the relevant attributes for the cluster. Eg, hddc function in the R package HDclassif.\n\n[11] Sequence clustering: Group sequences that are related. rBlast package.\n\n[12] Affinity propagation: Builds models based on message passing between data points. It does not require the number of clusters to be determined before running the algorithm. It is better for certain computer vision and computational biology tasks, e.g. clustering of pictures of human faces and identifying regulated transcripts, than k-means, Ref Rpackage APCluster.\n\n[13] Stream clustering: Builds models based on data that arrive continuously such as telephone records, financial transactions etc. Eg R package BIRCH [https://cran.r-project.org/src/contrib/Archive/birch/]\n\n[14] Document clustering (or text clustering): Builds models based on SVD. It has used in topic extraction. Eg Carrot [http://search.carrot2.org] is an open source search results clustering engine which can cluster documents into thematic categories.\n\n[15] Latent class model: It relates a set of observed multivariate variables to a set of latent variables. LCA may be used in collaborative filtering. R function Recommender in recommenderlab package has collaborative filtering functionality.\n\n[16] Biclustering: Used to simultaneously cluster rows and columns of two-mode data. Eg R function biclust in package biclust.\n\n[17] Soft clustering (fuzzy clustering): Each object belongs to each cluster to a certain degree. Eg R Fclust function in the fclust package."
    }
}
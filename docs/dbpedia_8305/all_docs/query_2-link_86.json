{
    "id": "dbpedia_8305_2",
    "rank": 86,
    "data": {
        "url": "http://www.cs.sun.ac.za/courses/cs771/",
        "read_more_link": "",
        "language": "en",
        "title": "CS 771: Honours Project",
        "top_image": "http://www.cs.sun.ac.za/courses/cs771/favicon.png",
        "meta_img": "http://www.cs.sun.ac.za/courses/cs771/favicon.png",
        "images": [],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "Computer Science 771: Honours Project",
        "meta_lang": "en",
        "meta_favicon": "favicon.ico",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "The year project\n\nThe Honours Project in Computer Science is a 32-credit year module (RW771, 63444-771), and is a critical part of our honours degree. The project consists of a substantial piece of independent research or software engineering (programming). The student must demonstrate skills appropriate to the sort of project, including literature survey, the software engineering process, testing and evaluation, and documenting and presenting results. There is the expectation of nontrivial or a significant amount of code to be written.\n\nEach project will be supervised by at least one CS academic staff member. Each project proposal below has an alphanumeric identifier that starts with a sequence of letters indicating the supervisor.\n\nIdentifier Supervisor AE Andries Engelbrecht BF Bernd Fischer BT Bill Tucker BVDM Brink van der Merwe CPI Cornelia Inggs FY Fabian Yamaguchi GR Gavin Rens LVZ Lynette van Zijl MD Marcel Dunaiski MN Mkhuseli Ngxande SK Steve Kroon TG Trienko Grobler WHK Willem Bester\n\nNote that the project list for 2024 will be available by Monday 5 Feb\n\nAE1: Multi-Guide Differential Evolution for Multi-Objective Optimization\n\nAE2: Knowledge-Biased Decision Tree Induction using Genetic Programming\n\nAE3: Active Learning in Neural Network Ensembles\n\nAE4: Automated Identification of Leg Implants from X-Ray Images\n\nAE5: Bone Fracture Prediction from Limited X-Ray Images\n\nAE7: A Music Score and Tutor App\n\nAE8: Python Computational Intelligence library\n\nAE9: Constrained Optimization Component of Computational Intelligence library\n\nAE10: Multi-/Many-Objective Optimization Component of Computational Intelligence library\n\nAE11: Research Manager\n\nBF1: Plagiarism detection for Python\n\nBF2: AI-based Software Fault Localization\n\nBF3: Stress-testing compilers by automated innocuous changes\n\nBF4: Metamorphic library fuzzing\n\nBF5: Normalizing, analyzing, and transforming user stories with NLP technologie\n\nBF6: Mining dictionaries for fuzz testing\n\nBF7: Generating SMT-LIB tests\n\nBT1: Multi-tiered end-user support for wireless community networks\n\nBT2: Community network management interface\n\nBT5: Educational resource infrastructure and delivery for remote SAN communities\n\nBT6: Metadata as annotation and re-narration\n\nBT7: Annotation of school content with sign language videos for Deaf students\n\nBvdM1: Studying Transformers through the lens of Formal Language Theory\n\nBvdM2: Reasoning with transformer based models\n\nBvdM3: LangChain (NO LONGER AVAILABLE)\n\nBvdM4: Man is to Computer Programmer as Woman is to Homemaker\n\nBvdM5: Do I need a PhD in Number Theory to use Zero-Knowledge Proofs in Software Development?\n\nBvdM6: What would you use to write a parser in 2024?\n\nBvdM7: Breaking regex matchers by using counters\n\nCPI2: Postgraduate management solution\n\nCPI3: Tournament Engine\n\nCPI4: Full-stack web application for managing tournaments\n\nCPI5: Static analysis and LLMs for feedback generation\n\nFY1: Disk-exhaustion vulnerabilities\n\nFY2: Implementation of a modern open-source HTTPS Interception Proxy\n\nGR1: Safe Reinforcement Learning via a Violation Measure and Safe Policy Space\n\nGR2: Investigate Monte Carlo Tree Search Strategies\n\nGR3: Prioritized Multi-Objective Reinforcement Learning\n\nGR4: Enhancing the Hybrid POMDP-BDI Agent Architecture with Contemporary POMDP Planning\n\nGR5: Analyzing and Optimizing a Probabilistic Logic Programming Language for Solving MDPs\n\nGR6: Safe Reinforcement Learning via Behavior Tree Look-ahead\n\nGR7: Comparison of Open- and Closed-loop MCTS for POMDP Planning\n\nGR8: Extend Utility Behvior Trees with Reward Machines\n\nLvZ1: Symbol detection in hierarchical nonsequential diagrams\n\nLvZ2: A web application to translate Braille to text\n\nLvZ3: Gamified grammar\n\nLvZ4: Random generation of nondeterministic finite automata\n\nLvZ5: LEGO stop motion movie clips\n\nLvZ6: JFlap for XNFA\n\nLvZ7: Clustered cellular automata for three-dimensional optimisation\n\nLvZ8: Braille software prototype deployments\n\nMD1: Hierarchical Document Classification using Large Language Models (LLMs)\n\nMD2: Mining Wikipedia for NLP benchmark datasets for low-resource languages\n\nMD3: Community detection algorithm allowing for overlapping, hierarchical cluster extractions\n\nMD4: Wildfire prediction models for South African using image processing\n\nMD5: Supervised author disambiguation algorithms\n\nMD6: Evaluating bug suspiciousness formulas\n\nMD7: Getting a teaching Q&A platform production ready\n\nMN2: Object detection and classification for early vehicle warning.\n\nMN3 Edu Chatbot - Making Learning Personal with AI\n\nMN4: Predictive maintenance - Road degradation as a case study.\n\nMN5: Real-Time 3D Object Tracking and Motion Prediction with a Multi-Spectral Camera System.\n\nMN6: Natural Language Interface to Data Visualisation\n\nMN7: Deep Learning-Based Denoising of Histogram Spectral Data with Mixed Noise Models\n\nSK1: An Ingenious MCTS-minimax hybrid for Lines of Action\n\nSK2: Gymnastics competition software\n\nSK3: Ingenious Opening Books and Retrograde Analysis\n\nSK4: Prospective configuration for learning in neural networks\n\nSK5: Best practice software engineering for the Ingenious Framework\n\nSK6: Improved testing for the Ingenious Framework\n\nTG1: Extending a remote sensing web application\n\nTG2: Deep Learning and the morphological classification of radio galaxies in the visibility domain\n\nTG3: Detecting RFI in interferometric data using machine learning\n\nTG4: Characterizing the symmetrical properties of radio galaxies\n\nTG5: Effect of Ionosphere on low-frequency uGMRT observations\n\nTG6: Calibration artefacts\n\nTG7: Boxes\n\nTG8: Extending a web application of the Cavalieri integral\n\nTG9: Empirical investigation of overfitting within neural networks\n\nWHK1: R4 (Regex Repo wRangler Redux)\n\nWHK2: Parallel regular expression matching using hardware\n\nWHK3: C “style” checker via code property graph\n\nWHK4: Question by question marking system for paper papers\n\nWHK5: An automaton layout engine for LaTeX\n\nWHK6: Type inference for Python functions\n\nAE1: Multi-Guide Differential Evolution for Multi-Objective Optimization\n\nWe have developed the multi-guide particle swarm optimization (MGPSO) as a multi-swarm approach to solve multi-objective optimization problems (MOPs) and many-objective optimization problems (MaOPs). The MGPSO assigns a sub-swarm for each objective function, and all candidate solutions in that sub-swarm is updated by making using of only the corresponding objective function and an archive guide. Archive guides are selected from the archive, which is a memory of non-dominated solutions to the MOP or MaOP being optimized. The archive guide is a stochastically weighted difference vector between a candidate solution and a non-dominated solution in the archive.\n\nThe purpose of this project is to apply the same concepts on differential evolution. The resulting multi-guide differential evolution (MGDE) algorithm will be a multi-population approach, where each objective function is optimized by one sub-population. The fitness of individuals in the sub-population is calculated using the corresponding objective function, and the selection operator is applied on these fitness values. Mutation and crossover operators are applied on the individuals of the sub-population, but with the following variations to be explored:\n\nThe mutation operator will add an archive guide, which is a stochastically weighted difference vector between the target vector and a non-dominated solution in the archive. The latter can be selected randomly, or via tournament selection, with the winner of the tournament selected as the solution with the smallest crowding distance.\n\nApply crossover between the parent, the trial vector and a non-dominated solution selected from the archive.\n\nAdditional approaches to combine the archive guide will be developed.\n\nThe resulting MGDE variations will be compared with the MGPSO and other state-of-the-art multi-objective particle swarm optimizations (MOPSOs) and multi-objective evolutionary algorithms (MOEAs) on MOPs and MaOP.\n\nThis is a research project, with the potential to write a journal paper. Note that it will be required for the algorithms to be developed in python, preferably as part of our currently library. It is required that you register for the AIP791 module in order to do this project.\n\nAE2: Knowledge-Biased Decision Tree Induction using Genetic Programming\n\nGenetic programming (GP) has been efficiently used to evolve decision trees (DTs), given a suitable DT grammar and a training set of supervised samples. GP has been used to evolve classification trees, regresion trees and model trees. The existing work on GP for decision tree induction exclude any available human knowledge about the problem domain to bias the induction of the DTs. Inclusion of human expert knowledge about the problem to bias the evolutionary process may help to produce DTs that are less complex, and with better generalization performance. This research will investigate approaches to include human expertise in the induction process, by including tree representatins of expert knowledge as initial chromosomes in the population. The project will also explore evolutionary operators that bias towards human expert knowledge. The resulting knowledge-based decision tree induction algorithm will then be compared with GP decision tree induction algorithms that do not make use of prior domain knowledge.\n\nThis is a research project, with the potential to write a journal paper. Note that it will be required for the algorithms to be developed in python. It is required that you register for the AIP791 module where genetic programming is covered as well as RW741 where decision trees are covered.\n\nAE3: Active Learning in Neural Network Ensembles\n\nEnsemble learning is an approach in which two or more models are fitted to the same data, and the predictions of each model are combined. Ensemble learning aims to achieve better performance with the ensemble of models than with any individual model. There are three main categories of ensemble approaches, i.e. bagging, boosting, and stacking. A bagging approach to ensemble learning results in each ensemble member to randomly sample a subset of instances from the main dataset (with or without replacement) to construct a predictive model on. All of the selected training instances are used to develop the develop the individual predictive model. In machine learning, training on all of the training instances is referred to as passive learning, or fixed-set learning. As an alternative, active learning refers to an approach to learning where the predictive model is not constructed on all of the instances, but only on instances considered to be most informative by the predictive model. Two main approaches to active learning exists, namely\n\nselective learning, where the training algorithm selects from the original training set only a subset of the most informative instances at each iteration, or at triggerd selection intervals, making use the current knowledge learnt by the predictive model; and\n\nincremental learning, where training starts on an initial small subset of the most informative instances, and more informative instances are added during training, again using the current knowlegde embedded in the predictive model.\n\nActive learning approaches help to reduce the computation cost of training a predictive model and also results in better generalization performance.\n\nThis procjet will explore the benefit of active learning within neural network (NN) ensembles ensembles. Two approaches will be considered:\n\nactive learning based selection from the original training set by each member of the ensemble, and\n\na bagging approach where each NN selects a bag from the original data set, and active learning is applied to this bag.\n\nThe active learning based NN ensemble approaches will be compared with a standard baggging approach and to active learning on individual NNs.\n\nThis is a research project, with the potential to write a journal paper. Note that it will be required for the algorithms to be developed in python. It is required that you register for the RW741 module where NNs, active learning, and ensemble learning are covered.\n\nAE4: Automated Identification of Leg Implants from X-Ray Images\n\nThe main goal of this project is to develop predictive models to detect orthopaedic implants and devices from X-rays, and to classify the implants. This include the detection of different types of plates, screws, nails or rods, wires or pins. In addition to the identification of the implant/device type, there is a need to also identify the manufacturer of the implant/device. The project will start with an in-depth and critical review of implant detection approaches to result in a categorization of approaches. These will range from object detection, template matching, to optimization-based approaches. The project will focus on object detection and template mathcing approaches. Limited availability of images for certain implant types needs to be taken into consideration. The project will propose an approach to implant detection and classification under the prescence of lack of sufficient data and imbalanced class distributions.\n\nThis is a research project, with the potential to write a journal paper. A dataset is available.\n\nAE5: Bone Fracture Prediction from Limited X-Ray Images\n\nWe have recently proposed a pipeline for bone fracture detection and classification from X-rays. The pipeline consists of an object detector and a convolutional neural network (CNN)-based predictive model. The problem experienced in this work is the very limited number of X-rays, and specifically under-representation of some fracture classes. Additionally, the problem has a skew class distribution. This projct will continue with thisEngineeringabilitation after limb-lengthening surgery\n\nLimb lengthening is a surgical treatment that can reduce or correct limb-length discrepancies. The process consists of three main phases: (1) distraction during which time the bones grow slowly longer over time, (2) consolidation during which time the bones begin to heal and calcify, (3) recovery when rehabilitation begins and more weight barring is placed on the bones. A challenge after limb-lengthening surgery is to decide on the optimal time to begin with the recovery phase. If started too early, the bone may not be strong enough to bear weight, and may consequently result in fractures. The purpose of this research is to develop a deep learning approach towards prediction of bone strength from X-ray images, and thereby to predict if the recovery phase may be initiated.\n\nThis is a research project, with the potential to write a journal paper. A dataset will be made available.\n\nAE7: A Music Score and Tutor App\n\nThe purpose of this project is to design a music score app, and to implement the main skeleton of the app and some of its functionality. This project will form the basis for future expansion of the music score app with various functionalities. The ultimate goal is to develop an app that has the following functionalities:\n\nEditing of scores for individual music instruments\n\nEditing of scores for multiple instruments\n\nInclusion of music syntax and semantic checks\n\nPlayback of edit music\n\nConvertion of edited music to pdf, or into a format such as music markup langauge\n\nAutomatic transposing of music scores\n\nRead in music scores provided in music markup language, and allow further editing\n\nScanning of music and conversion of the scanned music in the chosen internal representation\n\nRecognition of a music instrument\n\nHelp with tuning of a music instrument\n\nDetection of errors while playing an instrument\n\nPaging of a score while playing an instrument\n\nAnd many others.\n\nFor the purposes of this project, the above will be taking into consideration for the software engineering component of the project. Functionalities to be implemented should include at least the eiditing of scores, reading of scores from music markup language files, syntax and semantic checks, conversion to pdf and music markup language. Future projects will add additional fanctionalities.\n\nNote that you need to have backkround in music theory in order to do this project.\n\nThis is a software engineering project, and the deliverable should be a functional app.\n\nAE8: Python Computational Intelligence library\n\nThere exists a variety of computational intelligence libraries. Some are extremely popular, such as TensorFlow and PyTorch. However, the ecosystem for nature-inspired optimization algorithms is fragmented and underdeveloped. For example, a library may support only multi-objective optimization but not constrained optimization, while another library supports the opposite. Some libraries only support a specific computational intelligence paradigm, such as genetic algorithms or particle swarm optimization. Some libraries are not easily extendable which makes integration with other code difficult. Libraries may also lack documentation, rigorous testing, or have not received updates in months or years. For these reasons, using these libraries for research can be difficult.\n\nThe objective of this Honours project is to develop a new computational intelligence library guided by high-level but practical software design principles. The library will be developed in Python with a focus on nature-inspired optimization algorithms and the tools related to those algorithms. The library will be very generic to allow for single-solution problems, multi-solution problems, single objectives, multi- and many-objectives, static objective functions, dynamically changing search landscapes, boundary constrained and constrained problems, single population and multi-population algorithms, and hyper-heuristic algorithms.\n\nA very important consideration in the design of the library will be interaction with future planned empirical analysis libraries, results repositories, and fitness landscape analysis libraries.\n\nThis work will also include an analysis of existing libraries and identify areas that the proposed library aims to solve. The success of this project will be determined by the usability of the proposed library and feedback from fellow students and supervisors.\n\nThis is a software engineering project. It is required that you register for the AIP791 module.\n\nAE9: Constrained Optimization Component of Computational Intelligence library\n\nThe objective of this project is to design and implement the constrained optimization component of the Python Computational Intelligence Library to be designed as part of project AE8. Therefore, the students doing projects AE8 and AE9/AE10 will work closely together.\n\nConstrained optimization problems are problems where feasible values that can be assigned to decision variables are restricted by box-constraints, and equality and inequality constraints. Various approaches have been developed to ensure that meta-heuristics produce feasible solutions that satisfy these constraints. Some of the constraint-handling approaches are very general and can be applied to any meta-heuristic, while some are algorithm specific. Some of the approaches ensure feasibility of solutions throughout the search process, while others allow infeasible solutions to which a repair mechanism is applied. Other approaches convert the constrained optimization problem into a box-constrained optimization problem through the use of penalty methods, or by formulating the dual Lagrangian which results in the formulation of a minimax optimization problem solved using a coevolutionary subpopulation approach. Another alternative is to convert the constrained optimization problem into a box-constrained multi-objective or many-objective optimization problem and then to use a multi-/many-objective optimization problem to find feasible solutions.\n\nThe objective is to develop a constrained optimization framework that allows for implementations of the various constriant-handling approaches, and to ensure that those that are algorithm agnostic can be applied to any meta-heuristic in the Computational Intelligence library. In addition to the design of the framework, a number of constraint-handling approaches will be implemented as proof of concept.\n\nWhile this is a software engineering project, the first task will be to complete a review of constraint-handling mechanisms. Fortunately, a recent MSc thesis has a very complete review that can be used as inputs.\n\nIt is required that you register for the AIP791 module in order to do this project.\n\nAE10: Multi-/Many-Objective Optimization Component of Computational Intelligence library\n\nThe objective of this project is to design and implement the multi-/many-objective optimization component of the Python Computational Intelligence Library to be designed as part of project AE8. Therefore, the students doing projects AE8 and AE9/AE10 will work closely together.\n\nMulti-objective optimization problems (MOPs) are problems where two or three conflicting objectives have to be simultaneously optimized. Many-objective optimization problems (MaOPs) are problems where more than three conflicting objectives have to be simultaneously optimized. MOPs and MaOPs can be uncosntrained, box-constrained or constrained. Various approaches have been developed to solve MOPs and MaOPs, from weighted aggregation approaches to Pareto dominance-based approaches. The weighted aggregation approaches converts the problem into a single-objective optimization problem as a weighted sum over the various objectives. Some of the Pareto-based approaches solve the problem making use of one pupolation, while others are multi-population approaches. All of the multi-/many-objective optimization algorithms maintain an archive of solutions which balances the objectives to be optimized. Note that solutions to MOPs and MaOPs are almost always not a single solution, but a set of non-dominated solutions. These solutions are stored in the archive. Different approaches exist to find candidate non-dominated solutions and to maintain the archive. Some of these approaches are algorithm agnostic, while others are algorithm dependent.\n\nThe objective is to develop a multi-/many-objective optimization framework that allows for implementations of the various multi-/many-objective optimization and archive management approaches, and to ensure that those that are algorithm agnostic can be applied to any meta-heuristic in the Computational Intelligence library. In addition to the design of the framework, a number of these approaches will be implemented as proof of concept.\n\nWhile designing the framework, consideration has to be given to the fact that MOPs and MaOPs can also be constrained.\n\nWhile this is a software engineering project, the first task will be to complete a review of multi-/many-objective optimization algoroithms. Fortunately, good review articles and theses are available to use as input.\n\nIt is required that you register for the AIP791 module in order to do this project.\n\nAE11: Research Manager\n\nWhen doing research on a specific topic, for example particle swarm optimization (PSO), one of the first tasks that the researcher has to do is to find as many as possible research papers on PSO, to organize these in some way, and then to read and possibly annotated the papers with keywords/key phrases or to summarize the papers. This project will develop a web-based tool to help the researcher in optimizing these processes. The research manager must include the following functionalities:\n\nProvided a topic, crawl the web to find as many as possible research papers and theses on that topic.\n\nPapers have to be tagged uniquely, and it should be possible to view papers in different orders/categories, for example,\n\nbased on year of publication\n\nalphabetically based on first author\n\nbased on country of authors, or author institutions\n\nbased on sub-topics, e.g. multi-modal optimization, constrained optimization, multi-objective optimization, etc.\n\nbased on article type, i.e. journal article, conference article, thesis\n\nbased on whether the paper is theoretical or application-based\n\nbased on application\n\nBibliometric information should be extracted, for example,\n\nnumber of publications on the topic or sub-topic per year\n\nlist of authors and there details for a specific topic or sub-topic\n\nnumber of publications per author, country, institution on a topic or sub-topic\n\nAutomate the process to extract sub-topic maps from a collection of papers, where the collection can be specified on different granularity levels, for example,\n\nover all documents\n\nover all documents per year\n\nover all documents for specific author\n\nAutomated keyword extraction from the collected papers.\n\nFacility for the reader to tag each paper with user-defined keywords or phrases\n\nA search facility to find all papers in a specified collection of papers that matches a given set of keywords or key phrases\n\nA process to auto-generate Bibtex files for the papers, and to provide ordered bibliograhies for papers in different formats suchs as html or markdown.\n\nAllowance for different main topics, e.g. particle swarm optimization as one topic and genetic algorithms as another topic. Note that some papers may address more than one main topic.\n\nAn approach to cluster a collection of papers, and then to analyze these clusters.\n\nA timeline of papers published on different sub-topics within a given main topic.\n\nThe developed research manager can therefore be used to support the researcher, to provide bibliometric information, and to obtain an overview of the broad research topic.\n\nThis is a software engineering project. The use case will be on particle swarm optimization, and therefore it will be of benefit if you do Artificial Intelligence 791.\n\nBF1: Plagiarism detection for Python\n\nMost plagiarism detection tools (Moss [1] or JPlag [2]) abstract the program into a high-level representation (“token stream”) and then look for long shared subsequences. These tools are susceptible to spoofing attacks that try to hide plagiarism [3,4], and cannot handle many of the high-level programming structures typically used in python (e.g., comprehension, simultaneous assignments, …). The goal of this project is to build a better plagiarism detection tool that is robust against such spoofing attacks and the use of idiosyncratic python programming patterns. This can be implemented as a set of source-to-source transformations that normalize the python code, and used as a front-end to Moss.\n\nThis is a research-oriented software development project which requires good python skills, and an interest in program analysis and transformation (but no prior experience with these).\n\n[1] https://theory.stanford.edu/~aiken/moss/\n\n[2] https://github.com/jplag/JPlag\n\n[3] https://vivek-kaushal.medium.com/subtle-art-of-de-moss-ing-58ad4ea32c68\n\n[4] Mossad: defeating software plagiarism detection, Breanna Devore-McDonald, Emery D. Berger, https://dl.acm.org/doi/10.1145/3428206\n\n[5] Detecting Automatic Software Plagiarism via Token Sequence Normalization, Timur Sağlam, Moritz Brödel, Larissa Schmid, Sebastian Hahner, https://conf.researchr.org/track/icse-2024/icse-2024-research-track\n\nBF2: AI-based Software Fault Localization\n\nSoftware fault localization (SFL) tries to predict which parts (typically individual methods or statements) of a software system contain a fault, using static program analysis techniques (e.g., bounded model checking) or coverage data collected during the execution of a test suite (e.g., spectrum-based fault localization [1]). Recently, various AI methods such as deep learning [2], transformer models, or large language models (LLMs) [3,4].\n\nThe goal of this project is to survey the landscape of AI-based fault localization methods, to compare the different approaches experimentally, and to explore new alternatives. This is a research-oriented project that can have multiple outcomes, including a detailed literature survey, and the development of a new LLM-based fault localization method.\n\n[1] Dylan Callaghan, Bernd Fischer: Improving Spectrum-Based Localization of Multiple Faults by Iterative Test Suite Reduction. ISSTA 2023: 1445-1457\n\n[2] Yiling Lou, Qihao Zhu, Jinhao Dong, Xia Li, Zeyu Sun, Dan Hao, Lu Zhang, and Lingming Zhang. 2021. Boosting coverage-based fault localization via graph-based representation learning. In ESEC/FSE ’21: 29th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, Athens, Greece, August 23-28, 2021, Diomidis Spinellis, Georgios Gousios, Marsha Chechik, and Massimiliano Di Penta (Eds.). ACM, 664–676.\n\n[3] S. Kang, G. An, and S. Yoo, “A preliminary evaluation of llm-based fault localization,” arXiv preprint arXiv:2308.05487, 2023.\n\n[4] Y. Wu, Z. Li, J. M. Zhang, M. Papadakis, M. Harman, and Y. Liu, “Large language models in fault localisation,” 2023, arXiv:2308.15276.\n\nBF3: Stress-testing compilers by automated innocuous changes\n\nThe goal of this project is to design, implement, and evaluate a tool that applies behaviour-preserving transformations to test programs, resulting in modified programs that should behave as the original. The mutants can be generated by tree transformations. The project should find a grammar for a real language, implement some transformations, and evaluate the approach using a real compiler.\n\nThis is a research-oriented software development project which could feed into a related MSc project, and yield a publication. Candidates should be interested in programming languages.\n\nBF4: Metamorphic library fuzzing\n\nMetamorphic testing is a differential testing method that exploits equivalences and other properties of the test input domain (e.g., a+b = b+a) to construct new test inputs and, more importantly, to derive the expected test outputs (and so circumvent the oracle problem). Metamorphic fuzzing uses fuzzing methods to construct a large number of test inputs, and so helps scaling metamorphic testing.\n\nThe goal of this project is to build a metamorphic fuzzing framework for Java classes. It can build on a previously developed grammar-based fuzzing framework for Java classes.\n\nThis is a research-oriented software development project which could feed into a related MSc project, and yield a publication. Candidates should be interested in programming languages and software testing.\n\nBF5: Normalizing, analyzing, and transforming user stories with NLP technologie\n\nUser stories are short, high-level requirements descriptions that are the main system specification in agile software development approaches. They often follow a stylized format - “As an , I want because \". Unfortunately, users often don't follow this format, which makes it more difficult to mechanically analyze the requirements. The goal of this project is to use various natural-language technologies (such as large language models) to convert into the stylized format, and then to analyze the requirements embodied in the user stories, e.g., find out the different user classes (and their relations), group the user stories into use cases, or extract a logic-based ontology that can be used for further analyses.\n\nThis is a research-oriented project. Initial experiments with ChatGPT have shown some success, but a large part of the project involves prompt engineering and automating the interaction with LLMs.\n\nBF6: Mining dictionaries for fuzz testing\n\nFuzz testing incrementally constructs test cases by randomly changing or adding input bytes. This simple approach is surprisingly effective in many cases, but becomes ineffective when the system under test expects more verbose and structured inputs, such as web pages, programs, or JSON or XML object descriptions. Fuzz testing tools therefore rely on keyword dictionaries to increase the granularity at which the inputs are constructed, but these dictionaries must be specified manually. The goal of this project is to develop and evaluate a system that automatically extracts keyword dictionaries for a black-box system under test from an existing input corpus; more specifically, the system should learn the sets of keywords and operators, the lexeme classes (e.g., identifiers and numbers) and the formatting and commenting rules with minimal feedback (i.e., input is syntactically correct or not) from the system under test.\n\nThis is a research-oriented software development project which could feed into a related MSc project, and yield a publication. Candidates should be interested in programming languages.\n\nBF7: Generating SMT-LIB tests\n\nSMT-LIB [1] is a standard notation for satisfiability solvers “modulo theory”, i.e., automated solvers that can decided the validity of a logical formula over a number of different logical and mathematical theories such as integers, arrays, or strings.\n\nThe goal of this project is to develop a tool that can generate tests for SMT-solvers using the SMT-LIB format. This includes ensuring that the tests follow the visibility and typing rules of the SMT-LIB format, but also constructing test oracles (or, conversely, constructing tests that are by construction satisfiable).\n\n[1] https://smtlib.cs.uiowa.edu/\n\nBT1: Multi-tiered end-user support for wireless community networks\n\nCustomer support systems are often multi-tiered, where the first line of support is akin to the IT Crowd’s “is the power switched on?” (this is not a joke). Next tier support is more equipped to handle technical issues; and a higher tier can handle systemic and/or architectural issues. For a community network like Zenzeleni, or a remote network based on LoRa, there is another level of support already on the ground: the people who live there who helped build and maintain the network and its power supply. Thus, we envision a level 0 before first line support. This support could have a curated crowd-sourced how-to repository with text, videos, voice and video; and an inquiry/escalation portal to get interactive access to successive levels of support. This project entails learning how customer support systems are architected, producing a prototype for this use-case, and designing an interface for content population, access and interaction which could use a WhatsApp or Signal-like API that consumes low mobile data should a WiFi segment fail, to get help from fellow community members first, before escalating up the chain. We aim to work together with a community network to establish requirements and involve in a co-design process; all the while taking local language and culture(s) into consideration. We strongly recommend you take CS771 Computing & Society in order to prepare you for doing such a human-centred project.\n\nBT2: Community network management interface\n\nProprietary network management tools like UniFi have user interfaces for skilled network managers. For a rural community network like Zenzeleni, such network managers are sitting in East London or Umthatha, and are far from the community network itself. While such tools could be used by community network members on the ground, users will likely be overwhelmed by them. This project explores the feasibility of a network management tool meant for people on the ground. For example, network discovery, fault detection and overall management could be crowd-sourced from community network members. For example, someone could define a node at a given GPS position, or declare that a particular node is down for whatever reason. That information can be triangulated to SNMP-collected data, too. A user interface should be presented in a way that people on the ground can understand; which could be simplified versions of conventional network management graphs and maps; or could be something completely different! Thus, this project is more an exploration of a different kind of network management tailored for a rural community network whose members are not formally trained network managers. We aim to work together with a community network to establish requirements and involve in a co-design process; all the while taking local language and culture(s) into consideration. We strongly recommend you take CS771 Computing & Society in order to prepare you for doing such a human-centred project.\n\nA rich media contact centre uses text, voice and video in synchronous and/or asynchronous modes. Last year, we explored a rich media contact centre for Deaf people that included a video interpretation service where a South African Sign Language (SASL) interpreter is a third party doing the translation. That prototype explored making the communication as web-based as possible. That project report and git repository are starting points for this project. We’d like to rework and improve upon that prototype, perhaps with Jitsi; and add a Customer Relationship Management (CRM) tool on the back-end, something like Salesforce or an open source alternative like HubSpot, to keep track of client interactions, enable history for call centre operators, and set up remote interpretation services. When designed in a generalised fashion, a rich media contact centre could cater for all of our official languages. We choose to emphasise SASL because it includes all communication modalities, and we’ve been working with Deaf communities for quite a long time. This project will entail surveying the web for tools like Jitsi, HubSpot, etc., making informed choices, architecting a prototype that integrates these tools while providing usable user interfaces. We will work with at least one Deaf community; most likely the National Institute for the Deaf (NID) in Worcester. We strongly recommend you take CS771 Computing & Society in order to prepare you for doing such a human-centred project.\n\nLast year we started working with NGOs Abalimi/Harvest of Hope in Khayalitsha that help community farmers get produce to market. We learned that the logistics involved are complex, and their existing manual and mostly paper-based system is difficult to manage; there is some WhatsApp involved. We built an initial prototype for an Honours project and its report will be the starting point for this project. We are looking for someone with strong logistics skills to map out stakeholder interactions, processes and dependencies; and strong user interface skills to provide ease of use for NGO staff and farmers. It is envisioned that interfaces will be both browser and WhatsApp-based. There is an MSc student working on this project, too, who is a native isiXhosa speaker and can help integrate your work into a more long term engagement with these people. We strongly recommend you take CS771 Computing & Society in order to prepare you for doing such a human-centred project.\n\nBT5: Educational resource infrastructure and delivery for remote SAN communities\n\nLast year, an Honours project produced a prototype to provide curated content on a solar-powered Raspberry Pi with WiFi for a remote SAN community. We are working with people at !KhwaTtu, Oxford University and a remote community in Tsumkwe, Namibia to explore crowd-sourced content and annotation. An example of annotation for this use-case would be to provide a local language audio translation, perhaps aimed at children, for a YouTube video in English showing how to weave a basket. Our prototype is based on the SolarSPELL system. The main differences between our approach and SolarSPELL are 1) crowd-sourced content and 2) annotation, both of which are all meant to be curated. That prototype was a good start to present content and simple curation, and an MSc student is going to continue working on it to improve content authoring and curation. There are several angles to explore. One of them is annotation. We envision annotation via a browser plug-in, although we are open to alternatives. Another issue is a data mule-based way to share content between communities. This is sometimes referred to as delay tolerant networking. The idea is to either use an SD card or physically bring the entire device to another location to sync up content and annotations for communities who are isolated and may not even have or afford connectivity. Note this project may require some travel yet offers the opportunity to co-design, and possibly co-produce, some digital artefacts with living descendants of one of the oldest human societies on our planet. If we cannot arrange long-range travel due to cost and/or logistics and time constraints, we can always utilise our contacts at !KhwaTtu to facilitate the execution of this project. We strongly recommend you take CS771 Computing & Society in order to prepare you for doing such a human-centred project.\n\nBT6: Metadata as annotation and re-narration\n\nIt’s quite natural to want to empower all people to create, access and utilise content in their own preferred ways. A problem is that most online content on the Web is dominated by certain languages and cultural metaphors. Thus, the majority of information consumers, and potentially producers, too, especially those from ‘developing’ regions, are largely dealing with content that they may not fully be able to comprehend or produce. In other words, content consumption and production is inherently biased. Of course, there are automated translation tools available; however, ‘humans in the loop’ can translate between languages, cultures and different modes of understanding, e.g. between an adult and a child. We can call this re-narration or annotation, where an original source document (of any kind) can be annotated in some way. One way think of annotation is metadata like size of file, date of creation/modification, likes/dislikes, and keywords. Metadata could also include these types of annotation. For example, a video of how to handle diabetes in children in English could be annotated with a South African Sign Language video for Deaf people to understand. The project aims to explore the practicalities how metadata can be generalised to consist of any type of document. It will start with work done by Janastu’s Alipi project and the Networked Interactive Digital Books project; and look at issues like browser-based extensions and localised and customisable interfaces to edit and view annotations. We will work together with a marginalised community (to be determined) to provide a use-case for this project. We strongly recommend you take CS771 Computing & Society in order to prepare you for doing such a human-centred project.\n\nBT7: Annotation of school content with sign language videos for Deaf students\n\nNote this project has already been assigned to Zander Vermeulen\n\nDeaf children in South Africa have language-based hurdles that are difficult to overcome. The main obstacle is a lack of content in their ‘mother tongue’, South African Sign Language (SASL). Now that SASL has become the 12th official South African language, it is incumbent on the education sector to provide 1st language material to Deaf scholars. CAPS is the basis of primary and secondary education in South Africa. CAPS is generally poorly suited to non-English languages, especially SASL, of course, which means that we will have to rely on stakeholders much more than we will refer to official documents. The Linguistics department at Stellenbosch University has been collecting videos in SASL of CAPS and CAPS-related content that can help Deaf children approach this material in their own language. This project looks at how we might annotate an existing CAPS content site with these videos and/or provide a platform for doing something like this. The project entails working with stakeholders in Linguistics and with at least one local Deaf community to figure out a) how to provide such a system and b) work out human computer interfaces that are appealing and natural for Deaf end-users. We strongly recommend you take CS771 Computing & Society in order to prepare you for doing such a human-centred project.\n\nBvdM1: Studying Transformers through the lens of Formal Language Theory\n\nThe transformers architecture is an attention based language model architecture which has supplanted recurrent neural networks as the state-of-the-art in a range of natural and programming language processing tasks. In this project, the following questions should be investigated from an experimental point of view: Which regular languages can transformers recognise, how do transformers recognise these languages and how might this ability be improved by changing the tokenizer or attention mechanism. The framework developed should make use of the HuggingFace SequenceClassifierOutput API. When evaluating the performance of transformer models, for negative examples, the edit distance between the test input and regular language under consideration, should be taken into account. For example, when negative examples are on the border of the regular language the transformer is attempting to recognize (i.e. by changing only one symbol in the input string, we end up with a word in the regular language under consideration), the transformer might struggle to provide a correct classification, but further away, the performance of the transformer might be significantly better. For positive (respectively negative) examples, the edit distance between the test string and all positive (repsectively all negative) training examples, should be considered. It might for example be the case, that a test example is much longer than any of the training examples, and that the transformer struggle to generalize to test examples which are much longer than any of the strings in the training set. In cases where possible, equivalent RASP (see [3]) programs should be written\n\nResources\n\n[1] Hugging Face\n\n[2] MLRegTest: A Benchmark for the Machine Learning of Regular Languages\n\n[3] Paper: Transformers Learn Shortcuts to Automata; On YouTube\n\n[4] Paper: Thinking Like Transformers; On YouTube\n\n[5] How do you find out the edit distance between a given string and a RegExp?\n\nBvdM2: Reasoning with transformer based models\n\nIn this project you should investigate the limitations of transformer reasoning, by considering the correlation between the reasoning width of the computational graph (as introduced in [1]), and weather the transformer can produce either the code solving an algorithmic coding problem, or the solution to an Einstein puzzle. Other reasoning benchmarks beyond coding problems and Einstein puzzles, may also be considered, such as solving Sudoku puzzles or performing arithmetic operations. If considering arithmetic, the influence of changing the sequence of digits representing a number, to an explicitly representation indicating the ones, tens, hundreds, etc, may be considered.\n\nResources\n\n[1] Paper: Faith and Fate: Limits of Transformers on Compositionality; On YouTube\n\n[2] Einstein Puzzles\n\n[3] Teaching Arithmetic to Small Transformers\n\n[4] Reasoning with Transformer-based Models: Deep Learning, but Shallow Reasoning\n\nBvdM3: LangChain (NO LONGER AVAILABLE)\n\nLangChain is an open source orchestration framework for the development of applications using large language models (LLMs). LLMs excel at responding to prompts in a general context, but struggle in a specific domain they were never trained on. With LangChain, organizations can repurpose LLMs for domain-specific applications without retraining or fine-tuning. Development teams can build complex applications referencing proprietary information to augment model responses. LangChain simplifies artificial intelligence (AI) development by abstracting the complexity of data source integrations and prompt refining. LangChain provides AI developers with tools to connect language models with external data sources. Various websites, such as [2], lists ideas for LangChain projects. In this software engineering project, you should pick your own project, the only restriction is that you have to make use of LangChain.\n\nResources\n\n[1] LangChain documentation\n\n[2] LangChain Projects to Enhance your Portfolio in 2024\n\nBvdM4: Man is to Computer Programmer as Woman is to Homemaker\n\nWord embedding models have become a fundamental component in a wide range of Natural Language Processing (NLP) applications. However, embeddings trained on human-generated corpora have been demonstrated to inherit strong gender stereotypes that reflect social constructs. This project should investigate and compare the various word embedding algorithms, consider strategies to measure the gender bias in word embeddings, and come up with ways to produce gender neutral word embeddings.\n\nResources\n\n[1] Learning Gender-Neutral Word Embedding\n\n[2] Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings\n\nBvdM5: Do I need a PhD in Number Theory to use Zero-Knowledge Proofs in Software Development?\n\nThe ever-evolving quest for enhancing privacy of user data in software systems, has led to the exploration of how to use innovative cryptographic techniques in software systems. Among these, Zero-Knowledge Proofs (ZKPs), introduced in the 1980’s by Goldwasser, have emerged as a viable solution to many privacy concerns. ZKPs are cryptographic protocols that enable one party to prove to another that a given statement is true without revealing any information about the statement itself. This general idea can be used to ensure privacy of user data in various software applications. Grasping the intricacies of Zero-Knowledge Proofs requires a profound comprehension of sophisticated mathematical concepts – the required math is often referred to as Moon Math (but perhaps ZKPs can be used in software without understanding all the Moon Math). With the recent increase of interest in ZKPs, a set of ZKPs tools have been created, providing various levels of abstraction. This project should either mine GitHub repositories in order to provide insights into the dynamic and swiftly changing landscape of ZKP tooling systems, or should investigate benchmarking of ZKP tools (or make use of a combination of the two). Mining Software Repositories (MSR) is a research field that involves analysing data from software repositories to gain insights into various aspects of software development. This field encompasses a range of techniques to extract useful information from source code, commits, contributors, stars, forks and lifespan. Researchers in MSR often aim to understand patterns and trends in software development, identify factors influencing software quality, predict software defects and improve the overall software development process. In the context of ZKPs, MSR emerges as an important tool to comprehend the landscape of ZKP development, identify trends and recognize commonly used tools within this domain. By employing MSR techniques, researchers gain access to a wealth of information, such as commit histories and developer interactions. In the pursuit of understanding ZKP development, MSR provides a lens through which one can discern patterns of tool adoption, collaboration dynamics, and the evolution of ZKP-related projects over time. By either following a MSR approach, or benchmarking ZKP tools (or a combination of the two approaches) this projects should produce a more technical motivated version of the post “A Developer’s Guide to the zkGalaxy”.\n\nResources\n\n[1] Zero-Knowledge Proofs: The Magic Key to Identity Privacy\n\n[2] A Developer’s Guide to the zkGalaxy\n\n[3] zk-Bench: A Toolset for Comparative Evaluation and Performance Benchmarking of SNARKs\n\n[4] The promises and perils of mining github\n\nBvdM6: What would you use to write a parser in 2024?\n\nIn this project, the following parser generator tools should be compared on toy programming languages: Treesitter, Spoofax, Antlr, Yacc. It should be considered (in part) how difficult it is to write a grammar (for toy programming languages) using these four tools, and how much effort is involved in converting a grammar for one of these tools, to another. The focus should thus be on writing the frontend of a compiler. Special attention should be given to the various ways in which these tools facilitate disambiguation when having grammar conflicts. It should be taken into account, that these tools (apart from Treesitter and Spoofax) support different classes of grammars. It is not required, but it can also be considered to include a user study as part of this project.\n\nResources\n\n[1] A Brief History of the Spoofax Language Workbench\n\n[2] What would you use to write a parser in 2021?\n\n[3] Treesitter\n\n[4] Language Workbenches: The Killer-App for Domain Specific Languages?\n\n[5] Comparison of parser generators\n\n[6] Evaluating and comparing language workbenches: Existing results and benchmarks for the future\n\nBvdM7: Breaking regex matchers by using counters\n\nRegular pattern matching, where the patterns are expressed with finite-state automata or regular expressions (regexes), has numerous applications in text search and analysis, network security and bioinformatics. But, matching with regexes can lead to denial of service attacks. Such denial of service attacks, is often caused by the fact that regex libraries in Java and Python implements a backtracking matching algorithm, in contrast to the regex libraries in Go and Rust. Most backtracking regex matchers recently modified their implementations, by adding memoization, in order to reduce the prevalence of bad matching time. This project should focus on identifying regexes having bad matching time (on some input), which is caused mainly by counters being present in the regex (i.e. subexpressions of the form r{m,n}). Algorithms should be implemented to identify regexes (with counters) which are safe to use (with current regex libraries), or the focus should be on implementing new matchers which are safe to use (in terms of matching time).\n\nResources\n\n[1] Stack Overflow Outage Postmortem\n\n[2] On YouTube: On the Impact and Defeat of Regex Denial of Service – James Davis’s PhD Defense Talk\n\n[3] Paper: Software-Hardware Codesign for Efficient In-Memory Regular Pattern Matching; On YouTube\n\n[4] Counting in Regexes Considered Harmful: Exposing ReDoS Vulnerabilty of Nonbacktracking matchers\n\nTLDR: Develop a tool for checking thread-safety of non-blocking multi-threaded applications efficiently\n\nThe performance of multi-threaded code can be improved by using only atomic hardware operations instead of locks to synchronise updates to shared data structures. Code using this approach is hard to debug and various techniques have been developed for testing whether these non-blocking updates are thread-safe. Most of these techniques require some form of instrumentation, but in previous work we have implemented a solution for testing thread-safety that do not require instrumentation. The implementation uses a verification tool, called JPF, to generate all the possible execution sequences of the code under test and then uses a trace comparison technique to automatically check each sequence for thread-safety.\n\nWe would like to investigate the effectiveness of a more scalable version of this technique. The proof-of-concept implementation is single-threaded and therefore lacks scalability, especially when the symbolic execution capabilities of JPF is used. However, the benefits of using JPF’s symbolic execution capabilities would be a tool that is much easier to use, because even the input values can be generated automatically.\n\nObjectives\n\nDevelop a parallel implementation that uses JPFs symbolic execution capabilities and that scales, so that it can check thread-safety for larger numbers of method calls and threads.\n\nInvestigate further automation techniques or extensions or a decoupled design that can be easily integrated with other tools; there are many possibilities here and your choice will depend on what you find the most interesting.\n\nBroad / Deep: Deep\n\nPublication: Possible\n\nScope for continued study: Yes\n\nCPI2: Postgraduate management solution\n\nTLDR: Develop a full-stack solution for managing and analysing all the content related to postgraduate students and postgraduate modules.\n\nWe have more than 200 Honours applications yearly from both South African as well as international Universities, with different curricula and result divisions. Currently we keep track of applications, equivalent modules at other Universities, waiting lists, module choices, etc. in a collection of spreadsheets and directories. We would like a web application that can help us monitor postgraduate applications, determine whether applicants meet the admission criteria, and manage accepted/waiting lists.\n\nObjectives\n\nYour task would be to develop such a system with a database back end, for storing anonymous data that can be used in trend analysis, and a web front end, for interacting with the system and viewing results.\n\nThe system should integrate with SunStudent to download the necessary records and documents; we have already obtained the necessary IDs for interacting with SunStudent from IT.\n\nThe transcript-OCR tool, which was developed last year should be integrated to automatically extract data from the PDF transcripts of external institutions and then the system should make it easy for the user to compare the extracted info with the PDF document and edit it where necessary. There is an interesting avenue to explore with this feature for someone interested in machine learning, because the manual edits of captured transcript-data can be used to improve the extraction process via reinforcement learning.\n\nThe system must be able to display statistics and trends, such as anonymous historic data on the number of students that applied, was accepted, enrolled, passed, …\n\nThe system must be able to export results such as class lists, waiting lists, admitted lists, … for download in CSV format.\n\nFurther requirements:\n\nMust support Office 365 login, which provides MFA (Multi Factor Authentication)\n\nMust support differentiated access via user permissions\n\nBroad / Deep: Broad\n\nPublication: Unlikely\n\nScope for continued study: Not directly\n\nCPI3: Tournament Engine\n\nTLDR: Develop a tournament engine for running tournaments using a modern scalable architecture\n\nThe third-year projects for Concurrent Programming often involve the development of an AI game client, such as Othello. Currently a Python script is used to run matches and requires the full Ingenious Framework jar with a server and lobby. Actually all that should be required to run a single match is a referee and two clients.\n\nYour task would be to develop a new tournament engine for running tournaments. You basically need to split the Ingenious Framework into separate components (at least the referees and clients) and then write a tournament engine that will coordinate the running of the components in matches and tournaments and emit the results as a stream of events that can be recorded in a database. The recommended approach for building, deploying, and coordinating components is to use container-based technologies, such as Kubernetes\n\nRequirements\n\nThe following components are required: a Tournament-Engine that can schedule matches, a referee and clients for at least one supported domain.\n\nThe components should be deployed in containers\n\nThe Tournament-Engine must have a well defined API for creating, starting, and stopping tournaments and matches.\n\nThe Tournament-Engine must use the Kubernetes API to deploy the referee and clients for matches.\n\nIt should be possible to run single matches, locally, by simply using docker compose, since the referee and clients are each in their own container; this will help with debugging when developing new clients.\n\nA database might be needed to persist information about running matches, so that when the Tournament-Engine crashes and comes back up again it can stop hanging matches, for example.\n\nYou should use gRPC for communication between all the components. Other technologies to consider would be something like Thrift. However, gRPC is a very popular technology for communication in distributed systems.\n\nYou could use the Swagger UI to test your Tournament-Engine during development.\n\nTo log the Tournament-Engine’s events for debugging, you could write a listener that registers with the Tournament-Engine and prints all the events it receives.\n\nBroad / Deep: Broad\n\nPublication: Unlikely\n\nScope for continued study: Not directly, but in related work\n\nCPI4: Full-stack web application for managing tournaments\n\nTLDR: Develop a full-stack web application, including a database, Tournament-BFF (back end for front end), and front end for managing tournaments.\n\nThe third-year projects for Concurrent Programming often involve the development of an AI game client, such as Othello. Currently, a Python script is used to create games and run tournaments.\n\nObjectives Your task would be to develop a web application with a database back end for storing historic tournament data, a React front end for the user-interface, and a Tournament-BFF with an API to support a good user experience for creating, running, and viewing tournaments. The Tournemant-BFF will store data in the Database and run tournaments via the Tournament-Engine’s API (see CPI3). All of these components should be run in Docker containers, so that they can be managed by Kubernetes.\n\nRequirements\n\nThe following components are required: a React front end, a database back end, and a Tournament-BFF (backend for frontend).\n\nThe Tournament-Engine itself is a separate project, but should have a well-defined API that can be used by the Tournament-BFF.\n\nThe Tournament-BFF must also have a well-defined API to support a good user experience for the creating, running, and viewing of tournaments.\n\nYou could use Swagger to test your Tournament-BFF during development.\n\nYou could start with an in-memory database or use SQLite and then when your whole pipeline is working replace it with a more sofisticated Database, such as CockgroachDB or PostgreSQL\n\nYou should use gRPC, instead of a restAPI for communication between the back end and front end; the advantage would be that you can stream events (such as match moves and results) from the back end so that you could have a more engaging user experience. Other technologies to consider would be something like Thrift. However, gRPC is a very popular technology for communication in distributed systems.\n\nBroad / Deep: Broad\n\nPublication: Unlikely\n\nScope for continued study: Not directly, but in related work\n\nCPI5: Static analysis and LLMs for feedback generation\n\nTLDR: Investigate the feasibility and efficacy of combining static analysis techniques and LLMs for analysing student code submissions and providing feedback\n\nOur research group has an ongoing project, developing tools and techniques for providing feedback to students while they are completing tutorials as well as mark final submissions. Our research to date has focussed on static analysis techniques. More specifically building, comparing, and analysing Code Property Graphs (CPGs) for submissions.\n\nLarge language models (LLMs) have the potential to improve the feedback we are able to provide to students. However, LLMs on their own would not be ideal, because we do not want to provide the student with a solution and nor guide them down the wrong path. Thus we would like to investigate the feasibility of combining static analysis techniques and LLMs in a toolchain.\n\nObjectives\n\nYour task would be to help with this investigation\n\nYou should investigate the applicability of various technologies – e.g., the T5 LLM for code generation, Jeorn for generating Code Property Graphs and LangChain for building a toolchain – and implement various toolchain combinations.\n\nBroad / Deep: Deep\n\nPublication: Possible Scope for continued study: Yes\n\nFY1: Disk-exhaustion vulnerabilities\n\nIn this research project, we ask under which conditions vulnerabilities that allow attackers to cause the disk space to run out can be leveraged to execute arbitrary codes with the rights of the vulnerable application.\n\nThe project is executed in multiple phases: we begin by identifying operations under which failure to create new content on disk has security implications, e.g., by disclosing previously written data or by associating a session with the data of a previous user. In this phase, sample code is created, in which failure to write to disk has serious consequences.\n\nIn the second phase of the project, we attempt to identify previous vulnerability reports in which attackers were able to exhaust disk space and attempt to create exploits for these vulnerabilities that have an impact larger than the resource exhaustion itself.\n\nFinally, we create proof-of-concept tooling to identify such vulnerabilities automatically and attempt to uncover previously unknown vulnerabilities in open-source projects.\n\nFY2: Implementation of a modern open-source HTTPS Interception Proxy\n\nAn interception proxy is a component commonly used in security assessments in order to test HTTP/HTTPS-based Application Programming Interfaces (APIs). To this end, the tester places herself between the browser and server and (automatically) modifies requests in order to trigger bugs in the server.\n\nThis project focused on the design and implementation of a modern interception proxy that itself offers an HTTP-based API, making it programmable. In effect the interception proxy can be used as a building block of larger automated vulnerability assessment systems. Key concerns will be the definition of a suitable API, various problems that arise when client/server use the encrypted HTTPS protocol, and the proper implementation of an HTTP(S)-based proxy server in the first place.\n\nGR1: Safe Reinforcement Learning via a Violation Measure and Safe Policy Space\n\nSafe Reinforcement Learning (RL) is becoming a distinct sub-topic of RL. It is concerned with minimizing or completely avoiding the agent getting into unsafe/dangerous situations. The avoidance of dangerous situations is usually expected after the final policy has been computed, but we might also demand that dangerous situations be avoided during exploration/learning. One relatively simple approach to achieve safe behavior (after learning) is to define safety constraints, then define a violation measure based on those constraints, and finally, find a policy which minimizes the violation measure while maximizing rewards (see [1]). Working with safety constraints could leverage off the theory of constrained Markov decision processes (CMDPs). For some problems, it might be acceptable or even desirable to learn or somehow determine the space of safe policies while ignoring rewards, and then only learn rewarding behavior ‘within’ the safe policy space. In this project, we’ll first try to define the safe policy space analytically, given a set of constraints. If this is impossible or too difficult given the student’s time and expertise, we’ll try to learn the safe policy space. This approach to safe RL will then be evaluated on various safety-critical environments with various parameter and meta-parameter choices. In order to do this project, the student is required to register for the CS794 module (Search and Planning - Cognitive Robotics).\n\nReferences\n\n[1] Reinforcement Learning by Minimizing Constraint Violation - Dissertation\n\nResearch Type (more s/w engineering, balance bwix s/w engineering and theory, more theory)\n\nmore theory\n\nGR2: Investigate Monte Carlo Tree Search Strategies\n\nMonte Carlo Tree Search (MCTS) is a class of algorithms that return the ‘best’ move or action in a particular game/system state. This project will focus on applying MCTS to Markov decision processes (MDPs). When the action is performed and the system moves to the next state, MCTS is called again to provide the next action, and so on. The algorithm searches thru a tree of nodes, where a node is a child of another node if there is a move or action from the parent to the child. Nodes represent agent states. Monte Carlo methods are used to choose which moves/actions to generate or explore at every node. After some (typically, one) new node is generated, a random playout/rollout is performed until a win or loose (for games) or until an ‘adequate’ depth is reached (for MDPs). In this project we shall investigate MCTS for partially observable MDPs. We shall investigate different rollout stategies (to estimate the value of a node) and expansion strategies (to decide which action to explore or exploit next). Various strategies in various problem domains might have different effects on the quality of an agent’s behavior and on the time it takes the agent to make decisions. In order to do this project, the student is required to register for the CS794 module (Search and Planning - Cognitive Robotics).\n\nReferences\n\n[1] A Survey of Monte Carlo Tree Search Methods\n\nResearch Type (more s/w engineering, balance bwix s/w engineering and theory, more theory)\n\nmore s/w engineering\n\nGR3: Prioritized Multi-Objective Reinforcement Learning\n\nOften, an agent has more than one objective. Traditionally, in Markov decision processes (MDP) planning and Reinforcement Learning (RL), the rewards for multiple objectives are all encoded into the reward function by relatively simple linear combinations of rewards or case-based reasoning. However, when some objectives have higher priority than others (e.g., firstly, never collide with large objects, secondly, get to destination), it becomes difficult or impossible to design a reward function such that the rewards of the individual objectives are not in conflict. A simple linear trade-off is not always effective. Recent work on multi-objective RL has tackled this issue with some success. In this project, we shall follow a similar approach but in a more simplified way, appropriate for the scope of an Honours project. The questrion we would like to answer is, Which methods for reward design are more effective than linear combinations for multiple objectives (say two or three) where at least one objective should dominate in importance? We should use ideas from the preliminary work of Rietz et al. [1], especially the work referring to lexicographic priorities on the subtasks (Skalse et al., 2022; G´abor et al., 1998; Zhang et al., 2022) and the concept of Q-decomposition (Russell & Zimdars, 2003; also mentioned in [1]), which is an ``algorithm for MORL problems where the complex task’s reward function is the sum of subtask rewards.’’ In order to do this project, the student is required to register for the CS794 module (Search and Planning - Cognitive Robotics).\n\nReferences\n\n[1] PRIORITIZED SOFT Q-DECOMPOSITION FOR LEXICOGRAPHIC REINFORCEMENT LEARNING\n\nResearch Type (more s/w engineering, balance bwix s/w engineering and theory, more theory)\n\nbalance bwix s/w engineering and theory\n\nGR4: Enhancing the Hybrid POMDP-BDI Agent Architecture with Contemporary POMDP Planning\n\nThe Hybrid POMDP-BDI agent (HPBA) architecture was introducedin 2017 for agents to pursue multiple goals and to do so under uncertainty. An HBPA maintains a measure of how satisfied it is with each goal currently and selects its actions so as to pursue goals which are least satisfied. The HPBA architecture employs the partially observable Markov decision process (POMDP) framework to model the environment. The original HPBA architecture uses exact belief update during planning, however, this is impractical for non-trivial applications. In this project, we want to replace the optimal/exact planner with an online planner that can deal with much larger domains in a time-appropriate way. Plan chaching must still be done but could be improved over the original naive method, for instance, by learning a Behavior Tree. Some contemporary POMDP planners that might be considered are listed in the References section. In order to do this project, the student is required to register for the CS794 module (Search and Planning - Cognitive Robotics).\n\nReferences\n\n[1] HPBA\n\n[2] POMCP\n\n[3] DESPOT\n\n[4] POMCPOW\n\n[5] ABT\n\nResearch Type (more s/w engineering, balance bwix s/w engineering and theory, more theory)\n\nmore s/w engineering\n\nGR5: Analyzing and Optimizing a Probabilistic Logic Programming Language for Solving MDPs\n\nProbLog is a probabilistic logic programming language, that is, it is a programming language based on logic, which includes notions of probability. In 2010, Van Den Broek et al. proposed an extension to ProbLog, called DTProbLog, for one-shot decision making. In 2016 Bueno et al., extended (in a sense) DTProbLog to model sequencial decision problems - called MDP-ProbLog - because it solves Markov Decision Processes (MDPs). The implementation of MDP-ProbLog is available at https://github.com/thiagopbueno/mdp-problog. There are several aims in this project, involving MDP-ProbLog: 1) analyze its capabilities on various problem domains, 2) optimize it, given new insights into ProbLog and Python that occurred in the last seven years, 3) extend the utility predicate to allow utilities of state-action pairs, not only actions and states independently, as is currently the case and 4) report on the the new version of MDP-ProbLog for future projects/research to build on. We might find that the source code of the current version of MDP-ProbLog is not organized in the most efficient or clear way; in this case, a complete rewrite of the source code would be necessary. In order to do this project, the student is required to register for the CS794 module (Search and Planning - Cognitive Robotics). In order to do this project, the student is required to register for the CS794 module (Search and Planning - Cognitive Robotics).\n\nReferences\n\n[1] Key publications re. ProbLog\n\n[2] DTProbLog\n\n[3] MDP-ProbLog\n\nResearch Type (more s/w engineering, balance bwix s/w engineering and theory, more theory)\n\nmore s/w engineering\n\nGR6: Safe Reinforcement Learning via Behavior Tree Look-ahead\n\nSafe Reinforcement Learning (RL) has become a distinct sub-topic of RL [1]. It is concerned with minimizing or completely avoiding the agent getting into unsafe/dangerous situations. The avoidance of dangerous situations is usually expected after the final policy has been computed, but we might also demand that dangerous situations be avoided during exploration/learning. Behavior Trees (BTs) were developed relatively recently by the computer-gaming industry to specify what actions a non-player character should take at any time [2]. BTs have several benefits, including reactivity, modularity, and transparency (human-readability). This project is about using human-defined BTs to simulate (reason about) sequences of actions to predict danger, and then use this prediction to modify the reward to influence an agent to learn a RL policy that is safe. That is, we want to do RL to learn a policy that does not violate any safety constrants encoded by a BT. This idea is very speculative and the details might need to be changed to end up with a meaningful contribution. In order to do this project, the student is required to register for the CS794 module (Search and Planning - Cognitive Robotics).\n\nReferences\n\n[1] SafeRL\n\n[2] BTs\n\nResearch Type (more s/w engineering, balance bwix s/w engineering and theory, more theory)\n\nbalance bwix s/w engineering and theory\n\nGR7: Comparison of Open- and Closed-loop MCTS for POMDP Planning\n\nThere are two approaches for applying Monte Carlo Tree Search (MCTS) to a Markov Decision Processes (MDPs) with stochastic transitions: open-loop planning and closed-loop planning. In open-loop planning, instead of representing a single state, each node represents the collection of possible states an action can result in. Thus, statistics are averaged over the set of possible successor states for a given action. In closed-loop planning, we modify the tree to additionally contain alternating layers of chance nodes. The children of a chance node are the possible successor states. Open-loop planning does not store any state information, reducing memory usage, but requires it to access the model to simulate a transition every time it traverses between visited nodes. Closed-loop planning can use stored state information, but builds a significantly wider tree. Closed-loop planning can model different strategies for each possible successor state, while open-loop planning assumes the same strategy must be used regardless of which possible state is reached. For this project, the student will investicated the strengths and weakneses of the two approaches in various partially observable MDP (POMDP) environments. In order to do this project, the student is required to register for the CS794 module (Search and Planning - Cognitive Robotics).\n\nReferences\n\n[1] A Survey of Monte Carlo Tree Search Methods\n\n[2] Open Loop Execution of Tree-Search Algorithms\n\nResearch Type (more s/w engineering, balance bwix s/w engineering and theory, more theory)\n\nmore s/w engineering\n\nGR8: Extend Utility Behvior Trees with Reward Machines\n\nBehavior Trees (BTs) were developed relatively recently by the computer-gaming industry to specify what actions a non-player character should take at any time. BTs have several benefits, including reactivity, modularity, and transparency (human-readability). A Utility BT is a BT that has utilities (rewards) attached to different path choices in the tree (see [1]). A Reward Machine is a Mealy machine (a finite-state machine whose output values are determined both by its current state and the current inputs) specialized for specifying a reward model where the rewards have temporal dependencies (see [2]). An example of a temporally dependant reward is “Get 10 if agent visits location A, then location C, then location D; get -5 if agent visits location C, then location A, then location D; get 0 otherwise.” This kind of reward behavior cannot be captures using regular (non-temporal) reward functions; an agent visiting location D would always have to get the same reward. This project will attempt to answer this question: Is there an effective way to encode/capture temporally dependent rewards in a Utility BT? The answer will be via a proposed formal model and evaluation on at least one relevant environment. In order to do this project, the student is required to register for the CS794 module (Search and Planning - Cognitive Robotics).\n\nReferences\n\n[1] Video about Utility BTs\n\n[2] Reward Machines\n\nResearch Type (more s/w engineering, balance bwix s/w engineering and theory, more theory)\n\nmore theory\n\nLvZ1: Symbol detection in hierarchical nonsequential diagrams\n\nMachine learning is particulary useful to recognize objects on images or diagrams. In this project, we consider the recognition of individual abstract symbols in hierarchical nonsequential diagrams. It is required that the object be recognized as the correct type, and that its coordinates and orientation be retrieved. Although there are many practical examples of such diagrams, we will use crochet diagrams. Once the symbols are recognized, you should attempt a translation of the hierarchical nonsequential representation, into a sequential linear representation. In the case of crochet diagrams, this means that the diagram must be translated into text.\n\nResearch/software engineering project.\n\nDifficulty meter: 8/10\n\nLvZ2: A web application to translate Braille to text\n\nA web application must be developed, where scans of Braille pages must be uploaded and translated into Braille codes. The work rests on an existing machine learning model that can recognize scanned Braille characters. You will therefore have to interface with the system, but you are not expected to work on the machine learning model. Rather, the aim is to produce a user-friendly foolproof web page where uploaded scans can be recognized. It will be necessary to implement automatic error corrections of the recognized Braille codes, based on a given word list/dictionary. Then, the Braille codes must be translated to text, based on an existing translation system.\n\nThis is a pure software engineering project.\n\nDifficulty meter: 7.5/10\n\nLvZ3: Gamified grammar\n\nSupervisor: Dr Laurette Marais, CSIR\n\nMost of the South African languages have complex morphosyntax, making them challenging for students to learn as additional languages. In this software development project, you will utilise a computational grammar framework to gamify grammar and apply it to isiZulu.\n\nGrammatical Framework GF is a modern programming language and framework for engineering and applying computational grammars in natural language programming applications. Each grammar consists of a central abstract syntax, which defines categories and functions, and one or more concrete syntaxes, each of which implement the categories and functions for a specific natural language. The abstract syntax determines how trees can be constructed, and a concrete syntax determines how the tree is linearised into a natural language string. The combination of an abstract and a language specific concrete syntax defines a so-called Controlled Natural Language (CNL).\n\nGF grammars are compiled into Portable Grammar Format (PGF) and can be loaded and interacted with via the GF C runtime. The runtime has bindings to Python, Java and C#. For this project, a web application must be developed that presents the user with a beautiful, neatly laid out dynamic graphical user interface for building and modifying trees. This will involve creating and positioning editable graphical elements as nodes of a tree, and adapting the layout as the user interacts with the elements, creating and removing nodes as necessary. It would be necessary to keep a history of user actions, in order to enable undo and redo operations.\n\nGamification is achieved by generating example strings from the CNL and requiring the user to construct a correct tree (ambiguity is an inherent feature of natural language). Initially, a correct tree may be presented with some missing leaf nodes, and difficulty is increased by presenting an increasingly sparsely populated tree for the user to modify.\n\nAn isiZulu PGF will be supplied towards which the gamification can be tailored, but in principle the app should work with any PGF.\n\nThis is a software engineering project, but publication within the CALL field would be possible if a user study is included in a subsequent year.\n\nDifficulty meter: 6/10\n\nLvZ4: Random generation of nondeterministic finite automata\n\nThe random generation of unary and binary deterministic finite automata over the domain of the regular languages had been investigated in [1]. Using normal methods to randomly create start states, final states and the transition table, does not work; the distribution is not over languages but over the domain of numbers. So, the question is really how to quantify the domain of the regular languages, so that distributions can be found over this domain. However, the problem is open for the random generation of nondeterministic automata. Your task is to investigate and implement a solution to this problem.\n\nPrerequisite: CS345, and a keen interest in automata theory.\n\n[1] Raitt, Lesley Anne. “Random generation of finite automata over the domain of the regular languages.” MSc thesis, Stellenbosch: Stellenbosch University, 2006. [2] Héam, P.C. and Joly, J.L., 2015. On the uniform random generation of non deterministic automata up to isomorphism. In Implementation and Application of Automata: 20th International Conference, CIAA 2015, Umeå, Sweden, August 18-21, 2015, Proceedings 20 (pp. 140-152). Springer International Publishing. [3] Bassino, F. and Nicaud, C., 2007. Enumeration and random generation of accessible automata. Theoretical Computer Science, 381(1-3), pp.86-104.\n\nThis is a pure research project. Difficulty meter: 8.5/10\n\nLvZ5: LEGO stop motion movie clips\n\nThe aim of the project is to make LEGO stop motion movie clips. You will be required to use Unity to produce the movie clips. Note that you will build on an existing project that can generate true stop motion clips for a LEGO minifigure. You must now produce a system that will allow the user to add intricate backgrounds, and add scenes to the stop motion movie. In particular, you must handle collision detection and lighting. You must develop a user-friendly system that can incorporate the existing figure animations, and that provides an easy to use system to produce the rest of the stop motion movie. Note that the idea is to have a LEGO feel to the background.\n\nPrerequisite: A working knowledge of Unity will be advantageous.\n\nThis is a software engineering project.\n\nDifficulty meter: 7.5/10, and it is a substantial amount of graphics coding\n\nLvZ6: JFlap for XNFA\n\nA package needs to be developed to implement symmetric difference nondeterministic finite automata (XNFA), with the functionality similar to that of NFA in JFlap. The important issue here is the interface, but a basic implementation for XNFA is required for teaching purposes. Hence, the system must allow an XNFA to be input from both a text file and also interactively via a GUI. Full editing of the machine on the display is required. You should compare different layout algorithms to display the automaton on the screen. You would need to implement comparative interfaces, with for example the input XNFA on the left of the screen and the corresponding DFA on the right of the screen. You should visually show the execution of an XNFA on a given input word by means of an execution tree. You must provide the facility to print the output to a picture file (like jpg or png).\n\nPrerequisite: CS345\n\nThis is a pure software engineering project.\n\nDifficulty meter: 6/10, but it is a lot of coding.\n\nLvZ7: Clustered cellular automata for three-dimensional optimisation\n\nCellular automata with clustering (CCA) allow for varying cell sizes during cellular evolutions. It has been shown that such clustering improves the efficiency of cellular layout algorithms for fixed shapes in the two-dimensional case.\n\nThe aim of this project is to investigate the extension of CCA in the 3D case, and to develop a layout algorithm to arrange a library of known patterns in this case.\n\nPrerequisite: A mathematical inclination and an interest in patterns.\n\nThis is a pure research project, and can be used as a stepping stone towards an MSc.\n\n[1] E. Smal, Construction of LEGO sculpture instructions from 3D graphics models.\n\nDifficulty meter: 9/10\n\nLvZ8: Braille software prototype deployments\n\nThos project will require you to work closely with the client, and will give a real world experience of extending existing software, deployment of software, and hacking an Android-base hardware device. You should be interested in getting technical challenges solved (networking, hardware integration, app deployment).\n\nWe have a suite of prototype software developed for the Pioneer School for the Blind in Worcester. These prototypes must be installed, and the liaison at the school must be taught how to use the software. Extensions to the software must be implemented to make the prototypes usable in their environment. The suite of software includes: A translation system from text to Braille and vice versa; Existing apps must be deployed on the Google Playstore, and made accessible for download and use. An electronic classroom system must be extended to work with the wifi installed in the school. A translation system from sheet music to Braille music notation must be installed.\n\nThe student who chooses this project must be wiling to spend time at the school during recess periods, and specifically in June/July. There will be a fair amount of coding involved.\n\nPure software engineering project.\n\nDifficulty meter: 6.5/10\n\nMD1: Hierarchical Document Classification using Large Language Models (LLMs)\n\nFine-tuning pre-trained Language Models (PLMs) has been an established approach for document classification for a while now. However, categorising text documents into a hierarchical class structure instead of a simple class set yields interesting research avenues yet to be explored. The overarching research question for this project is: “What are effective ways to incorporate the information from the structured class hierarchy in the learning tasks of PLMs?”\n\nFor this project you will analyse various prompt-tuning approaches for PLMs to categorise all Masters and PhD dissertations published at South African universities into appropriate disciplines, fields, and topics.\n\nFor this project you will work towards two main objectives:\n\nAnalyse various prompt-tuning approaches for PLMs to categorise a large collection of documents.\n\nDevelop a well-designed web-based dashboard appropriate for searching and navigating this type of data.\n\nThe project is research heavy but has a fun software development component.\n\nPublishable: Unikely, however, this research can be extended and lead into a MSc degree.\n\nSkills you will acquire:\n\nWork with LLMs and prompt-tuning of PLMs.\n\nMachine learning frameworks/platforms such as PyTorch, PyTorch Lightning, and Hugging Face.\n\nScientific methodology.\n\nDashboard design, effective visualisation strategies, and web development.\n\nMD2: Mining Wikipedia for NLP benchmark datasets for low-resource languages\n\nThe goal for this project is to create benchmark datasets for various NLP tasks by mining Wikipedia. Your methods will be tested on the English version of Wikipedia, however, the objective of this project is to create datasets that can be used for the evaluation of NLP models for low-resource languages such as most languages spoken in sub-saharan Africa. You will analyse the consistency and quality of each dataset and execute preliminary experiments to showcase the viability of your benchmark datasets.\n\nThis project is research heavy.\n\nPublishable: Possible.\n\nSkills you will acquire:\n\nData mining techniques.\n\nNLP techniques such as named-entity classification, text classification, keyword extraction, coreference resolution, word-sense disambiguation.\n\nThe transformer model architecture.\n\nExperimental design scientific methodology.\n\nMD3: Community detection algorithm allowing for overlapping, hierarchical cluster extractions\n\nThe Louvain method for community detection is an algorithm to extract non-overlapping clusters from complex networks. It is a greedy optimisation heuristic for the modularity maximisation problem, designed for efficiency and has a run-time of O(n * log n) where n is the number of nodes in a network.\n\nThe Leiden method improved on the Louvain method by enforcing that all extracted clusters are well-connected without a significant speed reduction. However, both methods do not allow for clusters to overlap and the extracted clusters are not hierarchical. These two properties may be of value in certain applications. For example, it may be beneficial to organise academic papers into categories at varying granularities.\n\nFor this project you will research, develop, and analyse methods to adapt community detection algorithms to incorporate the additional cluster requirements.\n\nThis project is research heavy.\n\nPublishable: Potentially, it highly depends on what you come up with.\n\nSkills you will acquire:\n\nWorking with large complex networks.\n\nResearch methodology.\n\nScientific methodology.\n\nMD4: Wildfire prediction models for South African using image processing\n\nSouth Africa’s biodiverse landscapes are, unfortunately, becoming increasingly susceptible to wildfires. Effective wildfire risk modelling has therefore become an essential aspect of managing the country’s environmental affairs. The Fire Danger Index (FDI) currently used, evaluates fire risk based solely on weather features, ignoring critical factors such as fuel load, veld age, and topography.\n\nFor this project, you will model South African wildfire risk by assessing an assortment of Geographic Information System (GIS) analysis tools and techniques, including image processing. The project will comprise of both research into optimal risk modelling strategies, as well as a web-based dashboard designed as a decision support tool for policymakers and local land owners. The aim is that your project will aid effective wildfire management and prevention.\n\nThis project is research heavy but contains software development components.\n\nPublishable: Perhaps.\n\nSkills you will acquire:\n\nImage processing techniques.\n\nSupervised learning.\n\nWorking with GIS data.\n\nInteractive dashboard development.\n\nExperimental design and scientific methodology.\n\nMD5: Supervised author disambiguation algorithms\n\nThe ability to automatically map journal papers to the researchers that wrote them and vice versa map researchers to their collections of published papers is a fundamental issue. It would seem to be a simple and straightforward problem to solve yet it remains a major, unsolved problem in information science.\n\nReliable author name disambiguation is difficult because:\n\nA single person may publish under different names\n\nMany different persons have identical names\n\nSparse information (e.g., only initials and the surname are available)\n\nIn this project, you will research current state-of-the-art supervised author name disambiguation methods. You will implement the most promising methods for the South African context where the performance of your implementation will be compared to state-of-the-art unsupervised methods.\n\nThis project is research heavy.\n\nPublishable: Likely. This research can be extended and lead into a M.Sc. degree.\n\nSkills you will acquire:\n\nWorking with graph data (big data).\n\nSupervised learning.\n\nScientific methodology.\n\nMD6: Evaluating bug suspiciousness formulas\n\nSpectrum-based fault localization uses coverage information obtained by executing a test suite to find program elements (e.g., methods or statements) that are faulty and ones that are not. Specifically, it uses a given suspiciousness formula (e.g., Ochiai, Tarantula, Jaccard, and Naish) to compute a suspiciousness score for each element, and ranks the elements in score order. Given that there are a large number of different formulas the obvious question is which formula produces the best ranked lists. However, it is unclear how such formulas should be compared, or indeed what constitutes a better ranking.\n\nFor this project you will research analytical methods on how to best determine which suspiciousness formulas are best under various scenarios. In other words, given different ranked lists of suspected bugs, which list gives the best priority to the critical bugs that should be fixed first.\n\nThis project is research heavy.\n\nPublishable: Likely. This research can be extended and lead into a M.Sc. degree.\n\nSkills you will acquire:\n\nSpectrum-based fault localization methods.\n\nRanking metrics (Ochiai, Tarantula, etc…).\n\nExperimental setups and scientific methodology.\n\nMD7: Getting a teaching Q&A platform production ready\n\nThe aim of this project is to improve an online lecturing Q&A tool. The current version of this tool allows students to ask questions, upvote their peers’ questions, and start discussions on topics. Questions are ranked based on the number of upvotes or similarity. This allows lecturers or moderators to focus on the important questions and answer them. This project is inspired by Google Moderator, a service that was discontinued by Google in 2015.\n\nThis is a software engineering project and you will learn how to take an existing software codebase from prototype to production while also adding new features.\n\nPublishable: No.\n\nSkills you will acquire:\n\nSoftware lifecycle software development.\n\nProgram analysis (we will look at static and dynamic analysis techniques).\n\nProgram testing.\n\nImagine a world where surveillance cameras are more than passive observers, but intelligent guardians that actively identify and report unusual events. Current surveillance systems often rely on manual monitoring, leading to fatigue, missed incidents, and underutilized camera networks. You are required to automate anomaly detection on video footage using deep learning techniques.\n\nMN2: Object detection and classification for early vehicle warning.\n\nRoad accidents remain a significant concern in South Africa, often stemming from the late detection of obstacles by drivers. This kind of technology is currently available in high-end cars. For this project, you are required to develop an early vehicle warning system utilizing obstacle detection and distance estimation to minimize collision risks on South African roads. The system should be implemented in a mobile phone setting. TinyML techniques will be used to build this system. You will leverage the YOLO model for object detection and identification.\n\nMN3 Edu Chatbot - Making Learning Personal with AI\n\nTraditional learning often feels rigid and impersonal, failing to cater to individual learning styles and paces. Edu Chatbot seeks to bridge this gap by:\n\nPromoting active learning: Engaging through dialogue and open-ended questions fosters deeper understanding and critical thinking.\n\nProviding personalized support: Tailoring explanations and feedback to individual needs can boost confidence and accelerate progress.\n\nMaking learning accessible: Edu Chatbot can offer assistance anytime, anywhere, breaking down barriers to education.\n\nThis project challenges you to delve into the world of ChatGPT and create a version specifically designed for high school learners. The significance of Edu Chatbot is to revolutionize the way we learn, making education more engaging, personalized, and accessible for all.\n\nMN4: Predictive maintenance - Road degradation as a case study.\n\nPredictive maintenance offers a proactive approach, leveraging data analysis to anticipate equipment failures before they occur. This enables timely interventions, preventing costly downtime and extending equipment lifespan. Machine learning (ML) has emerged as a powerful tool for predictive maintenance, analyzing complex data patterns, and predicting potential issues with remarkable accuracy. You are required to develop a machine learning-based predictive maintenance model for predicting when a particular road needs to be maintained.\n\nMN5: Real-Time 3D Object Tracking and Motion Prediction with a Multi-Spectral Camera System.\n\nSupervisors: MN and Prof. Shaun Wyngaardt, Department of Physics\n\nThis project investigates a novel approach for real-time object tracking and motion prediction in 3D space. The system utilizes a stereoscopic multi-spectral camera system, capturing visual data simultaneously in both the visible and infrared spectrums. This rich data stream allows for robust object detection and tracking even in challenging lighting conditions.\n\nMN6: Natural Language Interface to Data Visualisation\n\nSupervisors: MN and Dr Ndivhuwo Makondo, IBM Research – Africa\n\nThe use of data visualization serves as a potent instrument that facilitates academics and decision-makers in comprehending intricate datasets, as well as identifying patterns, trends, and linkages. However, individuals lacking knowledge in coding or data analysis may struggle to comprehend data visualisation technologies. Users must have basic skills in programming languages such as Python or R, as well as have a solid understanding of data manipulation and analysis. These technical requirements may prove to be a challenge for individuals who do not have a background in computer science or data analytics. Consequently, there is a need for more user-friendly tools and platforms that can empower a greater number of users to effectively create, modify, and engage with data visualisations without extensive technical expertise. Natural Language Processing (NLP)-based chatbots have the potential to facilitate user interactions with data visualization tools using natural language. This project will investigate NLP techniques for interacting with data and creating visualisations using various Python plotting libraries.\n\nPublishable? Yes.\n"
    }
}
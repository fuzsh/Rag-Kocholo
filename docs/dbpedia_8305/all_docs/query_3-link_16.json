{
    "id": "dbpedia_8305_3",
    "rank": 16,
    "data": {
        "url": "https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-k-means-clustering/",
        "read_more_link": "",
        "language": "en",
        "title": "Introduction to K-Means Clustering",
        "top_image": "https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/K-means-Clustering-scaled.jpg",
        "meta_img": "https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/K-means-Clustering-scaled.jpg",
        "images": [
            "https://av-public-assets.s3.ap-south-1.amazonaws.com/logos/av-logo-svg.svg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/navbar.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/navbar.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/navbar.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/navbar.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/navbar.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/default_avatar.svg",
            "https://av-identity.s3.amazonaws.com/users/user/nocNbboBQPy1IoXTP7JInw.jpg",
            "https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-08-07-15-19-27.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-08-07-16-25-31.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-08-07-16-29-45.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-08-08-14-46-17.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-08-08-14-47-20.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-08-08-14-49-11.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-08-08-14-51-31.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-08-08-14-52-26.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-08-08-14-52-58.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-08-08-15-02-44.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-08-08-15-03-22.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-08-08-15-05-03.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-08-08-14-51-3122589.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-08-08-15-32-17.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-08-08-15-37-02.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-08-08-15-37-22.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-08-08-15-39-19.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-08-08-15-39-34.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-08-09-12-21-43.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/Screenshot-from-2019-08-09-12-23-55-1.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/Screenshot-from-2019-08-09-12-24-35-1.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/Screenshot-from-2019-08-09-12-26-59-1.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/Screenshot-from-2019-08-09-12-28-14-1.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-08-09-12-54-12.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-08-09-12-56-38.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-08-09-12-58-58.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-08-09-13-05-36.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-08-09-13-09-47.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-08-09-13-14-37.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-08-09-13-15-26.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-08-09-13-16-01.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-08-09-13-19-05.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-08-09-12-21-431.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-08-09-15-21-32.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-08-09-15-23-39.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-08-09-15-26-07.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-08-09-15-32-20.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-08-09-15-33-17.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-08-09-15-51-38.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-08-09-15-53-27.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-08-09-15-54-09.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-08-09-15-54-58.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-08-09-15-56-31.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-08-09-15-57-51.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-08-09-15-59-05.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-08-09-16-01-34.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-08-09-16-34-04.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-08-09-16-36-28.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-08-09-16-39-31.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-08-09-16-46-52.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-08-09-16-55-02.png",
            "https://av-identity.s3.amazonaws.com/users/user/nocNbboBQPy1IoXTP7JInw.jpg",
            "https://secure.gravatar.com/avatar/2ef119b6f8f6306f5966450e125d2c26?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/eed74e736d0a3795af0cc24f161a8a24?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/ae57225a1690fe3dd920c0a352616fd1?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/afe8e3e1a7ea21c6eba0addd04837d0c?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/22f2880c92cbd51a0ddb5f6b216547c7?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/4ee9eb59c050f7b90fe10cee99737bab?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/eed74e736d0a3795af0cc24f161a8a24?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/3dcf74017593c47768c22d1408bd3819?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/eed74e736d0a3795af0cc24f161a8a24?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/b60b1424d0a7450568831b1fe948736a?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/eed74e736d0a3795af0cc24f161a8a24?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/f7dc5b7aa1157b3ae1676f2265aea2e9?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/01716e84d3b2efd87eac16024b25d8a1?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/eed74e736d0a3795af0cc24f161a8a24?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/4c16576d4dfd20f730e01f22f22ea4f8?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/eed74e736d0a3795af0cc24f161a8a24?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/307c64383c2fb766a2b192a4ec24dc0c?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/eed74e736d0a3795af0cc24f161a8a24?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/0155bbba5609f1346b8faf3bd629dd3f?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/eed74e736d0a3795af0cc24f161a8a24?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/f8878b9d07307cc5e43030d0d3cfa1b6?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/eed74e736d0a3795af0cc24f161a8a24?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/bd2e2ea350f45097296915cb5c1c5e15?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/eed74e736d0a3795af0cc24f161a8a24?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/7d0ee8db1c0adb38227d7042fb1539df?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/eed74e736d0a3795af0cc24f161a8a24?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/bc12aa8715c5300eaff03a7acf394fec?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/60961819f22e001812eddf1d50b651f4?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/eed74e736d0a3795af0cc24f161a8a24?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/f020a3c6af0b1812bbdb08a28365e689?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/eed74e736d0a3795af0cc24f161a8a24?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/197cb929e73324444faf5132979db0d8?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/eed74e736d0a3795af0cc24f161a8a24?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/46d5cec891a66db374beabf270c8f252?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/eed74e736d0a3795af0cc24f161a8a24?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/a013384c2e47a52c34608bbdaab1bc23?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/eed74e736d0a3795af0cc24f161a8a24?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/21ef4e00e8d8e4c38ba06a20bd3903e2?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/5ad7f45c40d797f0aac064cd2fce816d?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/29df61f631ddb29dff286bc7eda68272?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/1e2ccb23491d0a270d37d2d61b66d6dd?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/46170a4356122aa282d2a86edc1bac09?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/7f1f258daebf23897c173b429692c9ae?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/972b089b0600a2356da0ea4938264141?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/800f4cfc73aceb28c9b8e5324974b9fd?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/d84147bdb8e3f9b7f301bab49b5f738f?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/aab037209b0e1a3017535b4bcab771cc?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/aab037209b0e1a3017535b4bcab771cc?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/cbfc73f7f899476c16f4226e51846a5d?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/1047cb04791bdba86d19fa0775b29b0e?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/d40cd3795efdf788cd449c9fd7e723b1?s=74&d=mm&r=g",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/path-digital.png",
            "https://av-identity.s3.amazonaws.com/users/user/bGnsep7nT0GMWuLpkDl15Q.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/in.png",
            "https://av-identity.s3.amazonaws.com/users/user/R7HrsWl1QrGRiw_e9m4fDA.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/in.png",
            "https://av-identity.s3.amazonaws.com/users/user/ZcU4ALTFT96MVCzfiGuhsQ.jpeg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/in.png",
            "https://av-identity.s3.amazonaws.com/users/user/aM3WrxdNSTGLg7LoqX-q0w.png",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/in.png",
            "https://av-identity.s3.amazonaws.com/users/user/zy4FL_yyQlG4PkWcyGYvhw.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/in.png",
            "https://av-identity.s3.amazonaws.com/users/user/a4ByfUyoQRmdGzLpBzHVLw.jpeg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/in.png",
            "https://av-identity.s3.amazonaws.com/users/user/ZTsmKl-1Qvqn07FUzgaBNw.png",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/in.png",
            "https://d2cd20fxv8fgim.cloudfront.net/homepage/images/Play_Store.svg",
            "https://d2cd20fxv8fgim.cloudfront.net/homepage/images/App_Store.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Pulkit Sharma"
        ],
        "publish_date": "2019-08-19T03:15:55+00:00",
        "summary": "",
        "meta_description": "The ultimate guide to K-means clustering algorithm - definition, concepts, methods, applications, and challenges, along with Python code.",
        "meta_lang": "en",
        "meta_favicon": "https://imgcdn.analyticsvidhya.com/favicon/av-fav.ico",
        "meta_site_name": "Analytics Vidhya",
        "canonical_link": "https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-k-means-clustering/",
        "text": "K-means clustering, originating from signal processing, is a technique in vector quantization. Its objective is to divide a set of n observations into k clusters, with each observation assigned to the cluster whose mean (cluster center or centroid) is closest, thereby acting as a representative of that cluster.\n\nIn this article, we will cover all about k-means clustering one of the most commonly used clustering methods) and its components comprehensively. We’ll look at clustering, why it matters, and its applications before diving deep into k-means clustering.\n\nLearning Objectives\n\nGet introduced to K-Means Clustering.\n\nUnderstand the properties of clusters and the various evaluation metrics for clustering.\n\nGet acquainted with some of the many real-world applications of K-Means Clustering.\n\nImplement K-Means Clustering in Python on a real-world dataset.\n\nWhat is K-Means Clustering?\n\nK-means clustering is a popular unsupervised machine learning algorithm used for partitioning a dataset into a pre-defined number of clusters. The goal is to group similar data points together and discover underlying patterns or structures within the data.\n\nRecall the first property of clusters – it states that the points within a cluster should be similar to each other. So, our aim here is to minimize the distance between the points within a cluster.\n\nThere is an algorithm that tries to minimize the distance of the points in a cluster with their centroid – the k-means clustering technique.\n\nK-means is a centroid-based algorithm or a distance-based algorithm, where we calculate the distances to assign a point to a cluster. In K-Means, each cluster is associated with a centroid.\n\nThe main objective of the K-Means algorithm is to minimize the sum of distances between the points and their respective cluster centroid.\n\nOptimization plays a crucial role in the k-means clustering algorithm. The goal of the optimization process is to find the best set of centroids that minimizes the sum of squared distances between each data point and its closest centroid.\n\nTo learn more about clustering and other machine learning algorithms (both supervised and unsupervised) check out the following courses-\n\nMachine Learning Certification Course for Beginners\n\nApplied Machine Learning Course\n\nCertified AI & ML Blackbelt+ Program\n\nHow K-Means Clustering Works?\n\nHere’s how it works:\n\nInitialization: Start by randomly selecting K points from the dataset. These points will act as the initial cluster centroids.\n\nAssignment: For each data point in the dataset, calculate the distance between that point and each of the K centroids. Assign the data point to the cluster whose centroid is closest to it. This step effectively forms K clusters.\n\nUpdate centroids: Once all data points have been assigned to clusters, recalculate the centroids of the clusters by taking the mean of all data points assigned to each cluster.\n\nRepeat: Repeat steps 2 and 3 until convergence. Convergence occurs when the centroids no longer change significantly or when a specified number of iterations is reached.\n\nFinal Result: Once convergence is achieved, the algorithm outputs the final cluster centroids and the assignment of each data point to a cluster.\n\nObjective of k means Clustering\n\nThe main objective of k-means clustering is to partition your data into a specific number (k) of groups, where data points within each group are similar and dissimilar to points in other groups. It achieves this by minimizing the distance between data points and their assigned cluster’s center, called the centroid.\n\nHere’s an objective:\n\nGrouping similar data points: K-means aims to identify patterns in your data by grouping data points that share similar characteristics together. This allows you to discover underlying structures within the data.\n\nMinimizing within-cluster distance: The algorithm strives to make sure data points within a cluster are as close as possible to each other, as measured by a distance metric (usually Euclidean distance). This ensures tight-knit clusters with high cohesiveness.\n\nMaximizing between-cluster distance: Conversely, k-means also tries to maximize the separation between clusters. Ideally, data points from different clusters should be far apart, making the clusters distinct from each other.\n\nWhat is Clustering?\n\nCluster analysis is a technique used in data mining and machine learning to group similar objects into clusters. K-means clustering is a widely used method for cluster analysis where the aim is to partition a set of objects into K clusters in such a way that the sum of the squared distances between the objects and their assigned cluster mean is minimized.\n\nHierarchical clustering and k-means clustering are two popular techniques in the field of unsupervised learning used for clustering data points into distinct groups. While k-means clustering divides data into a predefined number of clusters, hierarchical clustering creates a hierarchical tree-like structure to represent the relationships between the clusters.\n\nExample\n\nLet’s try understanding this with a simple example. A bank wants to give credit card offers to its customers. Currently, they look at the details of each customer and, based on this information, decide which offer should be given to which customer.\n\nNow, the bank can potentially have millions of customers. Does it make sense to look at the details of each customer separately and then make a decision? Certainly not! It is a manual process and will take a huge amount of time.\n\nSo what can the bank do? One option is to segment its customers into different groups. For instance, the bank can group the customers based on their income:\n\nCan you see where I’m going with this? The bank can now make three different strategies or offers, one for each group. Here, instead of creating different strategies for individual customers, they only have to make 3 strategies. This will reduce the effort as well as the time.\n\nThe groups I have shown above are known as clusters, and the process of creating these groups is known as clustering. Formally, we can say that:\n\nClustering is the process of dividing the entire data into groups (also known as clusters) based on the patterns in the data.\n\nCan you guess which type of learning problem clustering is? Is it a supervised or unsupervised learning problem?\n\nThink about it for a moment and use the example we just saw. Got it? Clustering is an unsupervised learning problem!\n\nHow is Clustering an Unsupervised Learning Problem?\n\nLet’s say you are working on a project where you need to predict the sales of a big mart:\n\nOr, a project where your task is to predict whether a loan will be approved or not:\n\nWe have a fixed target to predict in both of these situations. In the sales prediction problem, we have to predict the Item_Outlet_Sales based on outlet_size, outlet_location_type, etc., and in the loan approval problem, we have to predict the Loan_Status depending on the Gender, marital status, the income of the customers, etc.\n\nSo, when we have a target variable to predict based on a given set of predictors or independent variables, such problems are called supervised learning problems.\n\nNow, there might be situations where we do not have any target variable to predict.\n\nSuch problems, without any fixed target variable, are known as unsupervised learning problems. In these problems, we only have the independent variables and no target/dependent variable.\n\nIn clustering, we do not have a target to predict. We look at the data, try to club similar observations, and form different groups. Hence it is an unsupervised learning problem.\n\nWe now know what clusters are and the concept of clustering. Next, let’s look at the properties of these clusters, which we must consider while forming the clusters.\n\nProperties of K means Clustering\n\nHow about another example of k-means clustering algorithm? We’ll take the same bank as before, which wants to segment its customers. For simplicity purposes, let’s say the bank only wants to use the income and debt to make the segmentation. They collected the customer data and used a scatter plot to visualize it:\n\nOn the X-axis, we have the income of the customer, and the y-axis represents the amount of debt. Here, we can clearly visualize that these customers can be segmented into 4 different clusters, as shown below:\n\nThis is how clustering helps to create segments (clusters) from the data. The bank can further use these clusters to make strategies and offer discounts to its customers. So let’s look at the properties of these clusters.\n\nFirst Property of K-Means Clustering Algorithm\n\nAll the data points in a cluster should be similar to each other. Let me illustrate it using the above example:\n\nIf the customers in a particular cluster are not similar to each other, then their requirements might vary, right? If the bank gives them the same offer, they might not like it, and their interest in the bank might reduce. Not ideal.\n\nHaving similar data points within the same cluster helps the bank to use targeted marketing. You can think of similar examples from your everyday life and consider how clustering will (or already does) impact the business strategy.\n\nSecond Property of K-Means Clustering Algorithm\n\nThe data points from different clusters should be as different as possible. This will intuitively make sense if you’ve grasped the above property. Let’s again take the same example to understand this property:\n\nWhich of these cases do you think will give us the better clusters? If you look at case I:\n\nCustomers in the red and blue clusters are quite similar to each other. The top four points in the red cluster share similar properties to those of the blue cluster’s top two customers. They have high incomes and high debt values. Here, we have clustered them differently. Whereas, if you look at case II:\n\nPoints in the red cluster completely differ from the customers in the blue cluster. All the customers in the red cluster have high income and high debt, while the customers in the blue cluster have high income and low debt value. Clearly, we have a better clustering of customers in this case.\n\nHence, data points from different clusters should be as different from each other as possible to have more meaningful clusters. The k-means algorithm uses an iterative approach to find the optimal cluster assignments by minimizing the sum of squared distances between data points and their assigned cluster centroid.\n\nSo far, we have understood what clustering is and the different properties of clusters. But why do we even need clustering? Let’s clear this doubt in the next section and look at some applications of clustering.\n\nApplications of Clustering in Real-World Scenarios\n\nClustering is a widely used technique in the industry. It is being used in almost every domain, from banking and recommendation engines to document clustering and image segmentation.\n\nCustomer Segmentation\n\nWe covered this earlier – one of the most common applications of clustering is customer segmentation. And it isn’t just limited to banking. This strategy is across functions, including telecom, e-commerce, sports, advertising, sales, etc.\n\nDocument Clustering\n\nThis is another common application of clustering. Let’s say you have multiple documents and you need to cluster similar documents together. Clustering helps us group these documents such that similar documents are in the same clusters.\n\nImage Segmentation\n\nWe can also use clustering to perform image segmentation. Here, we try to club similar pixels in the image together. We can apply clustering to create clusters having similar pixels in the same group.\n\nYou can refer to this article to see how we can use clustering for image segmentation tasks.\n\nRecommendation Engines\n\nClustering can also be used in recommendation engines. Let’s say you want to recommend songs to your friends. You can look at the songs liked by that person and then use clustering to find similar songs and finally recommend the most similar songs.\n\nThere are many more applications that I’m sure you have already thought of. You can share these applications in the comments section below. Next, let’s look at how we can evaluate our clusters.\n\nUnderstanding the Different Evaluation Metrics for Clustering\n\nThe primary aim of clustering is not just to make clusters but to make good and meaningful ones. We saw this in the below example:\n\nHere, we used only two features, and hence it was easy for us to visualize and decide which of these clusters was better.\n\nUnfortunately, that’s not how real-world scenarios work. We will have a ton of features to work with. Let’s take the customer segmentation example again – we will have features like customers’ income, occupation, gender, age, and many more. We would not be able to visualize all these features together and decide on better and more meaningful clusters.\n\nThis is where we can make use of evaluation metrics. Let’s discuss a few of them and understand how we can use them to evaluate the quality of our clusters.\n\nInertia\n\nRecall the first property of clusters we covered above. This is what inertia evaluates. It tells us how far the points within a cluster are. So, inertia actually calculates the sum of distances of all the points within a cluster from the centroid of that cluster. Normally, we use Euclidean distance as the distance metric, as long as most of the features are numeric; otherwise, Manhattan distance in case most of the features are categorical.\n\nWe calculate this for all the clusters; the final inertial value is the sum of all these distances. This distance within the clusters is known as intracluster distance. So, inertia gives us the sum of intracluster distances:\n\nNow, what do you think should be the value of inertia for a good cluster? Is a small inertial value good, or do we need a larger value? We want the points within the same cluster to be similar to each other, right? Hence, the distance between them should be as low as possible.\n\nKeeping this in mind, we can say that the lesser the inertia value, the better our clusters are.\n\nDunn Index\n\nWe now know that inertia tries to minimize the intracluster distance. It is trying to make more compact clusters.\n\nLet me put it this way – if the distance between the centroid of a cluster and the points in that cluster is small, it means that the points are closer to each other. So, inertia makes sure that the first property of clusters is satisfied. But it does not care about the second property – that different clusters should be as different from each other as possible.\n\nThis is where the Dunn index comes into action.\n\nAlong with the distance between the centroid and points, the Dunn index also takes into account the distance between two clusters. This distance between the centroids of two different clusters is known asinter-cluster distance. Let’s look at the formula of the Dunn index:\n\nDunn index is the ratio of the minimum of inter-cluster distances and maximum of intracluster distances.\n\nWe want to maximize the Dunn index. The more the value of the Dunn index, the better the clusters will be. Let’s understand the intuition behind the Dunn index:\n\nIn order to maximize the value of the Dunn index, the numerator should be maximum. Here, we are taking the minimum of the inter-cluster distances. So, the distance between even the closest clusters should be more which will eventually make sure that the clusters are far away from each other.\n\nAlso, the denominator should be minimum to maximize the Dunn index. Here, we are taking the maximum of all intracluster distances. Again, the intuition is the same here. The maximum distance between the cluster centroids and the points should be minimum, eventually ensuring that the clusters are compact.\n\nSilhouette Score\n\nThe silhouette score and plot are used to evaluate the quality of a clustering solution produced by the k-means algorithm. The silhouette score measures the similarity of each point to its own cluster compared to other clusters, and the silhouette plot visualizes these scores for each sample. A high silhouette score indicates that the clusters are well separated, and each sample is more similar to the samples in its own cluster than to samples in other clusters. A silhouette score close to 0 suggests overlapping clusters, and a negative score suggests poor clustering solutions.\n\nHow to Apply K-Means Clustering Algorithm?\n\nLet’s now take an example to understand how K-Means actually works:\n\nStopping Criteria for K-Means Clustering\n\nThere are essentially three stopping criteria that can be adopted to stop the K-means algorithm:\n\nCentroids of newly formed clusters do not change\n\nPoints remain in the same cluster\n\nMaximum number of iterations is reached\n\nWe can stop the algorithm if the centroids of newly formed clusters are not changing. Even after multiple iterations, if we are getting the same centroids for all the clusters, we can say that the algorithm is not learning any new pattern, and it is a sign to stop the training.\n\nAnother clear sign that we should stop the training process is if the points remain in the same cluster even after training the algorithm for multiple iterations.\n\nFinally, we can stop the training if the maximum number of iterations is reached. Suppose we have set the number of iterations as 100. The process will repeat for 100 iterations before stopping.\n\nImplementing K-Means Clustering in Python From Scratch\n\nTime to fire up our Jupyter notebooks (or whichever IDE you use) and get our hands dirty in Python!\n\nWe will be working on the loan prediction dataset that you can download here. I encourage you to read more about the dataset and the problem statement here. This will help you visualize what we are working on (and why we are doing this). Two pretty important questions in any data science project.\n\nFirst, import all the required libraries:\n\nNow, we will read the CSV file and look at the first five rows of the data:\n\nFor this article, we will be taking only two variables from the data – “LoanAmount” and “ApplicantIncome.” This will make it easy to visualize the steps as well. Let’s pick these two variables and visualize the data points:\n\nPython Code:\n\nSteps 1 and 2 of K-Means were about choosing the number of clusters (k) and selecting random centroids for each cluster. We will pick 3 clusters and then select random observations from the data as the centroids:\n\nHere, the red dots represent the 3 centroids for each cluster. Note that we have chosen these points randomly, and hence every time you run this code, you might get different centroids.\n\nNext, we will define some conditions to implement the K-Means Clustering algorithm. Let’s first look at the code:\n\nThese values might vary every time we run this. Here, we are stopping the training when the centroids are not changing after two iterations. This is the most common convergence criteria used for the K-Means clustering. We have initially defined the diff as 1, and inside the whole loop, we are calculating this diff as the difference between the centroids in the previous iteration and the current iteration.\n\nWhen this difference is 0, we stop the training. Let’s now visualize the clusters we have got:\n\nAwesome! Here, we can clearly visualize three clusters. The red dots represent the centroid of each cluster. I hope you now have a clear understanding of how K-Means work.\n\nHowever, there are certain situations where this algorithm might not perform as well. Let’s look at some challenges you can face while working with k-means.\n\nChallenges With the K-Means Clustering Algorithm And\n\nThe k value in k-means clustering is a crucial parameter that determines the number of clusters to be formed in the dataset. Finding the optimal k value in the k-means clustering can be very challenging, especially for noisy data. The appropriate value of k depends on the data structure and the problem being solved. It is important to choose the right value of k, as a small value can result in under-clustered data, and a large value can cause over-clustering.\n\nChallenges\n\nAlso, one of the common challenges we face while working with K-Means is that the size of clusters is different. Let’s say we have the following points:\n\nThe leftmost and the rightmost clusters are of smaller size compared to the central cluster. Now, if we apply k-means clustering on these points, the results will be something like this:\n\nAnother challenge with k-means is when the densities of the original points are different. Let’s say these are the original points:\n\nHere, the points in the red cluster are spread out, whereas the points in the remaining clusters are closely packed together. Now, if we apply k-means on these points, we will get clusters like this:\n\nWe can see that the compact points have been assigned to a single cluster. Whereas the points that are spread loosely but were in the same cluster have been assigned to different clusters. Not ideal, so what can we do about this?\n\nSolutions\n\nOne of the solutions is to use a higher number of clusters.\n\nSo, in all the above scenarios, instead of using 3 clusters, we can have a bigger number. Perhaps setting k=10 might lead to more meaningful clusters.\n\nRemember how we randomly initialize the centroids in k-means clustering? Well, this is also potentially problematic because we might get different clusters every time. So, to solve this problem of random initialization, there is an algorithm called K-Means++ that can be used to choose the initial values, or the initial cluster centroids, for K-Means.\n\nDetermining the optimal number of clusters for k-means clustering can be another challenge.\n\nOptimal number heavily relies on subjective interpretations and the underlying structure of the data. One commonly used method to find the optimal number of clusters is the elbow method, which plots the sum of squared Euclidean distances between data points and their cluster center and chooses the number of clusters where the change in the sum of squared distances begins to level off.\n\nOutliers can have a significant impact on the results of k-means clustering, as the algorithm is sensitive to extreme values.\n\nThis makes it important to identify and handle outliers before applying k-means clustering to ensure that the results are meaningful and not skewed by the presence of outliers. There are various methods to identify and handle outliers, such as removing them, transforming them, or using a robust variant of k-means clustering that is less sensitive to the presence of outliers.\n\nThe algorithm can handle millions of data points and produce results in a matter of seconds or minutes, making it a popular choice for analyzing big data. However, as the size of the data set increases, the computational cost of k-means clustering can also increase. Hence, it is important to consider alternative algorithms when working with extremely large data sets.\n\nK-Means++ to Choose Initial Cluster Centroids for K-Means Clustering\n\nIn some cases, if the initialization of clusters is not appropriate, K-Means can result in arbitrarily bad clusters. This is where K-Means++ helps. It specifies a procedure to initialize the cluster centers before moving forward with the standard k-means clustering algorithm.\n\nUsing the K-Means++ algorithm, we optimize the step where we randomly pick the cluster centroid. We are more likely to find a solution that is competitive with the optimal K-Means solution while using the K-Means++ initialization.\n\nSteps to Initialize the Centroids Using K-Means++\n\nThe first cluster is chosen uniformly at random from the data points we want to cluster. This is similar to what we do in K-Means, but instead of randomly picking all the centroids, we just pick one centroid here\n\nNext, we compute the distance (D(x)) of each data point (x) from the cluster center that has already been chosen\n\nThen, choose the new cluster center from the data points with the probability of x being proportional to (D(x))2\n\nWe then repeat steps 2 and 3 until k clusters have been chosen\n\nLet’s take an example to understand this more clearly. Let’s say we have the following points, and we want to make 3 clusters here:\n\nNow, the first step is to randomly pick a data point as a cluster centroid:\n\nLet’s say we pick the green point as the initial centroid. Now, we will calculate the distance (D(x)) of each data point with this centroid:\n\nThe next centroid will be the one whose squared distance (D(x)2) is the farthest from the current centroid:\n\nIn this case, the red point will be selected as the next centroid. Now, to select the last centroid, we will take the distance of each point from its closest centroid, and the point having the largest squared distance will be selected as the next centroid:\n\nWe will select the last centroid as:\n\nWe can continue with the K-Means algorithm after initializing the centroids. Using K-Means++ to initialize the centroids tends to improve the clusters. Although it is computationally costly relative to random initialization, subsequent K-Means often converge more rapidly.\n\nI’m sure there’s one question that you’ve been wondering since the start of this article – how many clusters should we make? In other words, what should be the optimum number of clusters to have while performing K-Means?\n\nHow to Choose the Right Number of Clusters in K-Means Clustering?\n\nOne of the most common doubts everyone has while working with K-Means is selecting the right number of clusters.\n\nSo, let’s look at a technique that will help us choose the right value of clusters for the K-Means algorithm. Let’s take the customer segmentation example that we saw earlier. To recap, the bank wants to segment its customers based on their income and amount of debt:\n\nHere, we can have two clusters which will separate the customers as shown below:\n\nAll the customers with low income are in one cluster, whereas the customers with high income are in the second cluster. We can also have 4 clusters:\n\nHere, one cluster might represent customers who have low income and low debt; another cluster is where customers have high income and high debt, and so on. There can be 8 clusters as well:\n\nHonestly, we can have any number of clusters. Can you guess what would be the maximum number of possible clusters? One thing we can do is assign each point to a separate cluster. Hence, in this case, the number of clusters will equal the number of points or observations.\n\nThe maximum possible number of clusters will be equal to the number of observations in the dataset.\n\nBut then, how can we decide the optimum number of clusters? One thing we can do is plot a graph, also known as an elbow curve, where the x-axis will represent the number of clusters and the y-axis will be an evaluation metric. Let’s say inertia for now.\n\nYou can choose any other evaluation metric like the Dunn index as well:\n\nNext, we will start with a small cluster value, say 2. Train the model using 2 clusters, calculate the inertia for that model, and finally plot it in the above graph. Let’s say we got an inertia value of around 1000:\n\nNow, we will increase the number of clusters, train the model again, and plot the inertia value. This is the plot we get:\n\nWhen we changed the cluster value from 2 to 4, the inertia value reduced sharply. This decrease in the inertia value reduces and eventually becomes constant as we increase the number of clusters further.\n\nSo,the cluster value where this decrease in inertia value becomes constant can be chosen as the right cluster value for our data.\n\nHere, we can choose any number of clusters between 6 and 10. We can have 7, 8, or even 9 clusters. You must also look at the computation cost while deciding the number of clusters. If we increase the number of clusters, the computation cost will also increase. So, if you do not have high computational resources, my advice is to choose a lesser number of clusters.\n\nLet’s now implement the K-Means Clustering algorithm in Python. We will also see how to use K-Means++ to initialize the centroids and will also plot this elbow curve to decide what should be the right number of clusters for our dataset.\n\nImplementing K-Means Clustering in Python\n\nWe will be working on a wholesale customer segmentation problem. You can download the dataset using thislink. The data is hosted on the UCI Machine Learning repository.\n\nThe aim of this problem is to segment the clients of a wholesale distributor based on their annual spending on diverse product categories, like milk, grocery, region, etc. So, let’s start coding!\n\nWe will first import the required libraries:\n\nNext, let’s read the data and look at the first five rows:\n\nWe have the spending details of customers on different products like Milk, Grocery, Frozen, Detergents, etc. Now, we have to segment the customers based on the provided details.\n\nLet’s pull out some statistics related to the data:\n\nHere, we see that there is a lot of variation in the magnitude of the data. Variables like Channel and Region have low magnitude, whereas variables like Fresh, Milk, Grocery, etc., have a higher magnitude.\n\nSince K-Means is a distance-based algorithm, this difference in magnitude can create a problem.\n\nBring all the variables to the same magnitude\n\nThe magnitude looks similar now.\n\nCreate a kmeans function and fit it on the data\n\nWe have initialized two clusters and pay attention – the initialization is not random here. We have used the k-means++ initialization which generally produces better results as we have discussed in the previous section as well.\n\nLet’s evaluate how well the formed clusters are. To do that, we will calculate the inertia of the clusters:\n\nOutput: 2599.38555935614\n\nWe got an inertia value of almost 2600. Now, let’s see how we can use the elbow method to determine the optimum number of clusters in Python.\n\nWe will first fit multiple k-means models, and in each successive model, we will increase the number of clusters.\n\nWe will store the inertia value of each model and then plot it to visualize the result\n\nCan you tell the optimum cluster value from this plot? Looking at the above elbow curve, we can choose any number of clusters between 5 to 8.\n\nSet the number of clusters as 6 and fit the model\n\nValue count of points in each of the above-formed clusters\n\nSo, there are 234 data points belonging to cluster 4 (index 3), 125 points in cluster 2 (index 1), and so on. This is how we can implement K-Means Clustering in Python.\n\nConclusion\n\nIn this article, we discussed one of the most famous clustering algorithms – K-Means Clustering. We implemented it from scratch and looked at its step-by-step implementation. We looked at the challenges we might face while working with K-Means and also saw how K-Means++ can be helpful when initializing the cluster centroids.\n\nFinally, we implemented k-means and looked at the elbow method, which helps to find the optimum number of clusters in the K-Means algorithm.\n\nIf you have any doubts or feedback, feel free to share them in the comments section below. And make sure you check out the comprehensive ‘Applied Machine Learning‘ course that takes you from the basics of machine learning to advanced algorithms (including an entire module on deploying your machine learning models!).\n\nKey Takeaways\n\nK-Means Clustering is a simple yet powerful algorithm in data science.\n\nThe algorithm works to group together or form clusters of data points from an unlabeled dataset.\n\nIt is used in a plethora of real-world situations across various domains, such as banking and image segmentation, to document clustering and business decision-making.\n\nFrequently Asked Questions"
    }
}
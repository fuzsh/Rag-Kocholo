{
    "id": "dbpedia_8305_2",
    "rank": 77,
    "data": {
        "url": "https://cran.r-project.org/web/packages/dendextend/vignettes/dendextend.html",
        "read_more_link": "",
        "language": "en",
        "title": "Introduction to dendextend",
        "top_image": "",
        "meta_img": "",
        "images": [],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Tal Galili"
        ],
        "publish_date": "2023-03-24T00:00:00",
        "summary": "",
        "meta_description": "",
        "meta_lang": "",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Chaining\n\nFunction calls in dendextend often get a dendrogram and returns a (modified) dendrogram. This doesnât lead to particularly elegant code if you want to do many operations at once. The same is true even in the first stage of creating a dendrogram.\n\nIn order to construct a dendrogram, you will (often) need to go through several steps. You can either do so while keeping the intermediate results:\n\nd1 <- c(1:5) # some data d2 <- dist(d1) d3 <- hclust(d2, method = \"average\") dend <- as.dendrogram(d3)\n\nOr, you can also wrap the function calls inside each other:\n\ndend <- as.dendrogram(hclust(dist(c(1:5)), method = \"average\"))\n\nHowever, both solutions are not ideal: the first solution includes redundant intermediate objects, while the second is difficult to read (since the order of the operations is from inside to out, while the arguments are a long way away from the function).\n\nTo get around this problem, dendextend encourages the use of the %>% (âpipeâ or âchainingâ) operator (imported from the magrittr package). This turns x %>% f(y) into f(x, y) so you can use it to rewrite (âchainâ) multiple operations such that they can be read from left-to-right, top-to-bottom.\n\nFor example, the following will be written as it would be explained:\n\ndend <- c(1:5) %>% # take the a vector from 1 to 5 dist %>% # calculate a distance matrix, hclust(method = \"average\") %>% # on it compute hierarchical clustering using the \"average\" method, as.dendrogram # and lastly, turn that object into a dendrogram.\n\nFor more details, you may look at:\n\nmagrittr on CRAN\n\nIntroduction to the magrittr package\n\nSimpler R coding with pipes > the present and future of the magrittr package\n\nGetting nodes attributes in a depth-first search\n\nWhen extracting (or inserting) attributes from a dendrogramâs nodes, it is often in a âdepth-first searchâ. Depth-first search is when an algorithm for traversing or searching tree or graph data structures. One starts at the root and explores as far as possible along each branch before backtracking.\n\nHere is a plot of a tree, illustrating the order in which you should read the ânodes attributesâ:\n\nWe can get several nodes attributes using get_nodes_attr (notice the order corresponds with what is shown in the above figure):\n\n# Create a dend: dend <- 1:5 %>% dist %>% hclust %>% as.dendrogram # Get various attributes dend %>% get_nodes_attr(\"height\") # node's height\n\n#> [1] 4 1 0 0 2 0 1 0 0\n\ndend %>% hang.dendrogram %>% get_nodes_attr(\"height\") # node's height (after raising the leaves)\n\n#> [1] 4.0 1.0 0.6 0.6 2.0 1.6 1.0 0.6 0.6\n\ndend %>% get_nodes_attr(\"members\") # number of members (leaves) under that node\n\n#> [1] 5 2 1 1 3 1 2 1 1\n\ndend %>% get_nodes_attr(\"members\", id = c(2,5)) # number of members for nodes 2 and 5\n\n#> [1] 2 3\n\ndend %>% get_nodes_attr(\"midpoint\") # how much \"left\" is this node from its left-most child's location\n\n#> [1] 1.625 0.500 NA NA 0.750 NA 0.500 NA NA\n\ndend %>% get_nodes_attr(\"leaf\") # is this node a leaf\n\n#> [1] NA NA TRUE TRUE NA TRUE NA TRUE TRUE\n\ndend %>% get_nodes_attr(\"label\") # what is the label on this node\n\n#> [1] NA NA 1 2 NA 5 NA 3 4\n\ndend %>% get_nodes_attr(\"nodePar\") # empty (for now...)\n\n#> [1] NA NA NA NA NA NA NA NA NA\n\ndend %>% get_nodes_attr(\"edgePar\") # empty (for now...)\n\n#> [1] NA NA NA NA NA NA NA NA NA\n\nA similar function for leaves only is get_leaves_attr\n\nSetting a dendrogramâs labels\n\nWe can get a vector with the treeâs labels:\n\n# get the labels: dend15 %>% labels\n\n#> [1] 1 2 5 3 4\n\n# this is just like labels(dend)\n\nNotice how the treeâs labels are not 1 to 5 by order, since the tree happened to place them in a different order. We can change the names of the labels:\n\n# change the labels, and then print them: dend15 %>% set(\"labels\", c(111:115)) %>% labels\n\n#> [1] 111 112 113 114 115\n\n# could also be done using: # labels(dend) <- c(111:115)\n\nWe can change the type of labels to be characters. Not doing so may be a source of various bugs and problems in many functions.\n\ndend15 %>% labels\n\n#> [1] 1 2 5 3 4\n\ndend15 %>% set(\"labels_to_char\") %>% labels\n\n#> [1] \"1\" \"2\" \"5\" \"3\" \"4\"\n\nWe may also change their color and size:\n\npar(mfrow = c(1,2)) dend15 %>% set(\"labels_col\", \"blue\") %>% plot(main = \"Change label's color\") # change color dend15 %>% set(\"labels_cex\", 2) %>% plot(main = \"Change label's size\") # change color\n\nThe function recycles, from left to right, the vector of values we give it. We can use this to create more complex patterns:\n\n# Produce a more complex dendrogram: dend15_2 <- dend15 %>% set(\"labels\", c(111:115)) %>% # change labels set(\"labels_col\", c(1,2,3)) %>% # change color set(\"labels_cex\", c(2,1)) # change size par(mfrow = c(1,2)) dend15 %>% plot(main = \"Before\") dend15_2 %>% plot(main = \"After\")\n\nNotice how these âlabels parametersâ are nested within the nodePar attribute:\n\n# looking at only the left-most node of the \"after tree\": dend15_2[[1]][[1]] %>% unclass %>% str\n\n#> int 1 #> - attr(*, \"label\")= int 111 #> - attr(*, \"members\")= int 1 #> - attr(*, \"height\")= num 0 #> - attr(*, \"leaf\")= logi TRUE #> - attr(*, \"nodePar\")=List of 3 #> ..$ lab.col: num 1 #> ..$ pch : logi NA #> ..$ lab.cex: num 2\n\n# looking at only the nodePar attributes in this sub-tree: dend15_2[[1]][[1]] %>% get_nodes_attr(\"nodePar\")\n\n#> [,1] #> lab.col 1 #> pch NA #> lab.cex 2\n\nWhen it comes to color, we can also set the parameter âkâ, which will cut the tree into k clusters, and assign a different color to each label (based on its cluster):\n\npar(mfrow = c(1,2)) dend15 %>% set(\"labels_cex\", 2) %>% set(\"labels_col\", value = c(3,4)) %>% plot(main = \"Recycles color \\nfrom left to right\") dend15 %>% set(\"labels_cex\", 2) %>% set(\"labels_col\", value = c(3,4), k=2) %>% plot(main = \"Color labels \\nper cluster\") abline(h = 2, lty = 2)\n\nSetting a dendrogramâs nodes/leaves (points)\n\nEach node in a tree can be represented and controlled using the assign_values_to_nodes_nodePar, and for the special case of the nodes of leaves, the assign_values_to_leaves_nodePar function is more appropriate (and faster) to use. We can control the following properties: pch (point type), cex (point size), and col (point color). For pch we can additionally set bg (âbackgroundâ, although itâs really a fill for the shape). When bg is set, the outline of the point is defined by col and the internal fill is determined by bg. For example:\n\npar(mfrow = c(2,3)) dend13 %>% set(\"nodes_pch\", 19) %>% plot(main = \"(1) Show the\\n nodes (as a dot)\") #1 dend13 %>% set(\"nodes_pch\", 19) %>% set(\"nodes_cex\", 2) %>% plot(main = \"(2) Show (larger)\\n nodes\") #2 dend13 %>% set(\"nodes_pch\", 19) %>% set(\"nodes_cex\", 2) %>% set(\"nodes_col\", 3) %>% plot(main = \"(3) Show (larger+colored)\\n nodes\") #3 dend13 %>% set(\"leaves_pch\", 21) %>% plot(main = \"(4) Show the leaves\\n (as empty circles)\") #4 dend13 %>% set(\"leaves_pch\", 21) %>% set(\"leaves_cex\", 2) %>% plot(main = \"(5) Show (larger)\\n leaf circles\") #5 dend13 %>% set(\"leaves_pch\", 21) %>% set(\"leaves_bg\", \"gold\") %>% set(\"leaves_cex\", 2) %>% set(\"leaves_col\", \"darkred\") %>% plot(main = \"(6) Show (larger+colored+filled)\\n leaves\") #6\n\nAnd with recycling we can produce more complex outputs:\n\npar(mfrow = c(1,2)) dend15 %>% set(\"nodes_pch\", c(19,1,4)) %>% set(\"nodes_cex\", c(2,1,2)) %>% set(\"nodes_col\", c(3,4)) %>% plot(main = \"Adjust nodes\") dend15 %>% set(\"leaves_pch\", c(19,1,4)) %>% set(\"leaves_cex\", c(2,1,2)) %>% set(\"leaves_col\", c(3,4)) %>% plot(main = \"Adjust nodes\\n(but only for leaves)\")\n\nNotice how recycling works in a depth-first order (which is just left to right, when we only adjust the leaves). Here are the nodeâs parameters after adjustment:\n\ndend15 %>% set(\"nodes_pch\", c(19,1,4)) %>% set(\"nodes_cex\", c(2,1,2)) %>% set(\"nodes_col\", c(3,4)) %>% get_nodes_attr(\"nodePar\")\n\n#> [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] #> pch 19 1 4 19 1 4 19 1 4 #> cex 2 1 2 2 1 2 2 1 2 #> col 3 4 3 4 3 4 3 4 3\n\nWe can also change the height of of the leaves by using the hang.dendrogram function:\n\npar(mfrow = c(1,3)) dend13 %>% set(\"leaves_pch\", 19) %>% set(\"leaves_cex\", 2) %>% set(\"leaves_col\", 2) %>% # adjust the leaves hang.dendrogram %>% # hang the leaves plot(main = \"Hanging a tree\") dend13 %>% set(\"leaves_pch\", 19) %>% set(\"leaves_cex\", 2) %>% set(\"leaves_col\", 2) %>% # adjust the leaves hang.dendrogram(hang_height = .6) %>% # hang the leaves (at some height) plot(main = \"Hanging a tree (but lower)\") dend13 %>% set(\"leaves_pch\", 19) %>% set(\"leaves_cex\", 2) %>% set(\"leaves_col\", 2) %>% # adjust the leaves hang.dendrogram %>% # hang the leaves hang.dendrogram(hang = -1) %>% # un-hanging the leaves plot(main = \"Not hanging a tree\")\n\nAn example of what this function does to the leaves heights:\n\ndend13 %>% get_leaves_attr(\"height\")\n\n#> [1] 0 0 0\n\ndend13 %>% hang.dendrogram %>% get_leaves_attr(\"height\")\n\n#> [1] 1.35 0.85 0.85\n\nWe can also control the general heights of nodes using raise.dendrogram:\n\npar(mfrow = c(1,3)) dend13 %>% plot(main = \"First tree\", ylim = c(0,3)) dend13 %>% raise.dendrogram (-1) %>% plot(main = \"One point lower\", ylim = c(0,3)) dend13 %>% raise.dendrogram (1) %>% plot(main = \"One point higher\", ylim = c(0,3))\n\nIf you wish to make the branches under the root have the same height, you can use the flatten.dendrogram function.\n\ntanglegram\n\nA tanglegram plot gives two dendrogram (with the same set of labels), one facing the other, and having their labels connected by lines. Tanglegram can be used for visually comparing two methods of Hierarchical clustering, and are sometimes used in biology when comparing two phylogenetic trees.\n\nHere is an example of creating a tanglegram using dendextend:\n\ntanglegram(dends_15_51)\n\n# Same as using: # plot(dends_15_51) # since there is a plot method for dendlist # and also: # tanglegram(dend15, dend51)\n\nNotice how âuniqueâ nodes are highlighted with dashed lines (i.e.: nodes which contains a combination of labels/items, which are not present in the other tree). This can be turned off using highlight_distinct_edges = FALSE. Also notice how the connecting lines are colored to highlight two sub-trees which are present in both dendrograms. This can be turned off by setting common_subtrees_color_lines = FALSE. We can also color the branches of the trees to show the two common sub-trees using common_subtrees_color_branches = TRUE:\n\ntanglegram(dends_15_51, common_subtrees_color_branches = TRUE)\n\nWe may wish to improve the layout of the trees. For this we have the entanglement, to measure the quality of the alignment of the two trees in the tanglegram layout, and the untangle function, for improving it.\n\ndends_15_51 %>% entanglement # lower is better\n\n#> [1] 0.9167078\n\n# dends_15_51 %>% untangle(method = \"DendSer\") %>% entanglement # lower is better dends_15_51 %>% untangle(method = \"step1side\") %>% entanglement # lower is better\n\n#> [1] 0\n\nNotice that just because we can get two trees to have horizontal connecting lines, it doesnât mean these trees are identical (or even very similar topologically):\n\ndends_15_51 %>% untangle(method = \"step1side\") %>% tanglegram(common_subtrees_color_branches = TRUE)\n\nEntanglement is measured by giving the left treeâs labels the values of 1 till tree size, and than match these numbers with the right tree. Now, entanglement is the L norm distance between these two vectors. That is, we take the sum of the absolute difference (each one in the power of L). e.g: sum(abs(x-y)**L). And this is divided by the âworst caseâ entanglement level (e.g: when the right tree is the complete reverse of the left tree).\n\nL tells us which penalty level we are at (L0, L1, L2, partial Lâs etc). L>1 means that we give a big penalty for sharp angles. While L->0 means that any time something is not a straight horizontal line, it gets a large penalty If L=0.1 it means that we much prefer straight lines over non straight lines\n\nFinding an optimal rotation for the tanglegram of two dendrogram is a hard problem. This problem is also harder for larger trees.\n\nLetâs see how well some untangle methods can do.\n\nWithout doing anything:\n\nx <- dends_15_51 x %>% plot(main = paste(\"entanglement =\", round(entanglement(x), 2)))\n\nUsing DendSer:\n\n# x <- dends_15_51 %>% untangle(method = \"DendSer\") x <- dends_15_51 %>% untangle(method = \"ladderize\") x %>% plot(main = paste(\"entanglement =\", round(entanglement(x), 2)))\n\nOne solution for improving the tanglegram would be to randomly search the rotated tree space for a better solution. Here is how to use a random search:\n\nset.seed(3958) x <- dends_15_51 %>% untangle(method = \"random\", R = 10) x %>% plot(main = paste(\"entanglement =\", round(entanglement(x), 2)))\n\nWe can see we already got something better. An advantage of the random search is the ability to create many many trees and compare them to find the best pair.\n\nLetâs use a greedy forward step wise rotation of the two trees (first the left, then the right, and so on), to see if we can find a better solution for comparing the two trees. Notice that this may take some time to run (the larger the tree, the longer it would take), but we can limit the search for smaller kâs, and see what improvement that can bring us using step2side (slowest):\n\nx <- dends_15_51 %>% untangle(method = \"step2side\") x %>% plot(main = paste(\"entanglement =\", round(entanglement(x), 2)))\n\nWe got perfect entanglement (0).\n\nCorrelation measures\n\nWe shall use the following for the upcoming examples:\n\nset.seed(23235) ss <- sample(1:150, 10 ) dend1 <- iris[ss,-5] %>% dist %>% hclust(\"com\") %>% as.dendrogram dend2 <- iris[ss,-5] %>% dist %>% hclust(\"single\") %>% as.dendrogram dend3 <- iris[ss,-5] %>% dist %>% hclust(\"ave\") %>% as.dendrogram dend4 <- iris[ss,-5] %>% dist %>% hclust(\"centroid\") %>% as.dendrogram dend1234 <- dendlist(\"Complete\" = dend1, \"Single\" = dend2, \"Average\" = dend3, \"Centroid\" = dend4) par(mfrow = c(2,2)) plot(dend1, main = \"Complete\") plot(dend2, main = \"Single\") plot(dend3, main = \"Average\") plot(dend4, main = \"Centroid\")\n\nGlobal Comparison of two (or more) dendrograms\n\nThe all.equal.dendrogram function makes a global comparison of two or more dendrograms trees.\n\nall.equal(dend1, dend1)\n\n#> [1] TRUE\n\nall.equal(dend1, dend2)\n\n#> [1] \"Difference in branch heights - Mean relative difference: 0.4932164\"\n\nall.equal(dend1, dend2, use.edge.length = FALSE)\n\n#> [1] \"Dendrograms contain diffreent edges (i.e.: topology). Unique edges in target: | 2, 7, 13 | Unique edges in current: 7, 9, 11\"\n\nall.equal(dend1, dend2, use.edge.length = FALSE, use.topology = FALSE)\n\n#> [1] TRUE\n\nall.equal(dend2, dend4, use.edge.length = TRUE)\n\n#> [1] \"Difference in branch heights - Mean relative difference: 0.1969642\"\n\nall.equal(dend2, dend4, use.edge.length = FALSE)\n\n#> [1] \"Dendrograms contain diffreent edges (i.e.: topology). Unique edges in target: | 11 | Unique edges in current: 13\"\n\nall.equal(dendlist(dend1, dend1, dend1))\n\n#> [1] TRUE\n\nall.equal(dend1234)\n\n#> 1==2 #> \"Difference in branch heights - Mean relative difference: 0.4932164\" #> 1==3 #> \"Difference in branch heights - Mean relative difference: 0.2767035\" #> 1==4 #> \"Difference in branch heights - Mean relative difference: 0.4081231\" #> 2==3 #> \"Difference in branch heights - Mean relative difference: 0.4545673\" #> 2==4 #> \"Difference in branch heights - Mean relative difference: 0.1969642\" #> 3==4 #> \"Difference in branch heights - Mean relative difference: 0.1970749\"\n\nall.equal(dend1234, use.edge.length = FALSE)\n\n#> 1==2 #> \"Dendrograms contain diffreent edges (i.e.: topology). Unique edges in target: | 2, 7, 13 | Unique edges in current: 7, 9, 11\" #> 1==3 #> \"Dendrograms contain diffreent edges (i.e.: topology). Unique edges in target: | 7 | Unique edges in current: 7\" #> 1==4 #> \"Dendrograms contain diffreent edges (i.e.: topology). Unique edges in target: | 2, 7 | Unique edges in current: 7, 9\" #> 2==3 #> \"Dendrograms contain diffreent edges (i.e.: topology). Unique edges in target: | 9, 11 | Unique edges in current: 8, 15\" #> 2==4 #> \"Dendrograms contain diffreent edges (i.e.: topology). Unique edges in target: | 11 | Unique edges in current: 13\" #> 3==4 #> \"Dendrograms contain diffreent edges (i.e.: topology). Unique edges in target: | 15 | Unique edges in current: 9\"\n\nDistance matrix using dist.dendlist\n\nThe dist.dendlist function computes the Robinson-Foulds distance (also known as symmetric difference) between two dendrograms. This is the sum of edges in both trees with labels that exist in only one of the two trees (i.e.: the length of distinct_edges).\n\nx <- 1:5 %>% dist %>% hclust %>% as.dendrogram y <- set(x, \"labels\", 5:1) dist.dendlist(dendlist(x1 = x,x2 = x,y1 = y))\n\n#> x1 x2 #> x2 0 #> y1 4 4\n\ndend_diff(x,y)\n\ndist.dendlist(dend1234)\n\n#> Complete Single Average #> Single 6 #> Average 2 4 #> Centroid 4 2 2\n\nThis function might implement other topological distances in the future.\n\nCorrelation matrix using cor.dendlist\n\nBoth Bakerâs Gamma and cophenetic correlation (Which will be introduced shortly), can be calculated to create a correlation matrix using the cor.dendlist function (the default method is cophenetic correlation):\n\ncor.dendlist(dend1234)\n\n#> Complete Single Average Centroid #> Complete 1.0000000 0.4272001 0.5635291 0.4466374 #> Single 0.4272001 1.0000000 0.9508998 0.9910913 #> Average 0.5635291 0.9508998 1.0000000 0.9556376 #> Centroid 0.4466374 0.9910913 0.9556376 1.0000000\n\nThe corrplot library offers a nice visualization:\n\nlibrary(corrplot) corrplot(cor.dendlist(dend1234), \"pie\", \"lower\")\n\nWhich easily tells us that single, average and centroid give similar results, while complete is somewhat different.\n\n# same subtrees, so there is no need to color the branches dend1234 %>% tanglegram(which = c(2,3))\n\n# Here the branches colors are very helpful: dend1234 %>% tanglegram(which = c(1,2), common_subtrees_color_branches = TRUE)\n\nBakerâs Gamma Index\n\nBakerâs Gamma Index (see bakerâs paper from 1974) is a measure of association (similarity) between two trees of Hierarchical clustering (dendrograms). It is defined as the rank correlation between the stages at which pairs of objects combine in each of the two trees.\n\nOr more detailed: It is calculated by taking two items, and see what is the highest possible level of k (number of cluster groups created when cutting the tree) for which the two item still belongs to the same tree. That k is returned, and the same is done for these two items for the second tree. There are n over 2 combinations of such pairs of items from the items in the tree, and all of these numbers are calculated for each of the two trees. Then, these two sets of numbers (a set for the items in each tree) are paired according to the pairs of items compared, and a Spearman correlation is calculated.\n\nThe value can range between -1 to 1. With near 0 values meaning that the two trees are not statistically similar. For exact p-value one should use a permutation test. One such option will be to permute over the labels of one tree many times, calculating the distribution under the null hypothesis (keeping the trees topologies constant).\n\nNotice that this measure is not affected by the height of a branch but only of its relative position compared with other branches.\n\ncor_bakers_gamma(dend15, dend51)\n\n#> [1] 0.2751938\n\nEven that we can reach perfect entanglement, Bakerâs gamma shows us that the treeâs topology is not identical. As opposed with the correlation of a tree with itself:\n\ncor_bakers_gamma(dend15, dend15)\n\n#> [1] 1\n\nSince the observations creating the Bakerâs Gamma Index of such a measure are correlated, we need to perform a permutation test for the calculation of the statistical significance of the index. Letâs look at the distribution of Bakerâs Gamma Index under the null hypothesis (assuming fixed tree topologies). This will be different for different tree structures and sizes. Here are the results when the compared tree is itself (after shuffling its own labels), and when comparing tree 1 to the shuffled tree 2:\n\nset.seed(23235) the_cor <- cor_bakers_gamma(dend15, dend15) the_cor2 <- cor_bakers_gamma(dend15, dend51) the_cor\n\n#> [1] 1\n\nthe_cor2\n\n#> [1] 0.2751938\n\nR <- 100 cor_bakers_gamma_results <- numeric(R) dend_mixed <- dend15 for(i in 1:R) { dend_mixed <- sample.dendrogram(dend_mixed, replace = FALSE) cor_bakers_gamma_results[i] <- cor_bakers_gamma(dend15, dend_mixed) } plot(density(cor_bakers_gamma_results), main = \"Baker's gamma distribution under H0\", xlim = c(-1,1)) abline(v = 0, lty = 2) abline(v = the_cor, lty = 2, col = 2) abline(v = the_cor2, lty = 2, col = 4) legend(\"topleft\", legend = c(\"cor\", \"cor2\"), fill = c(2,4)) round(sum(the_cor2 < cor_bakers_gamma_results)/ R, 4)\n\n#> [1] 0.17\n\ntitle(sub = paste(\"One sided p-value:\", \"cor =\", round(sum(the_cor < cor_bakers_gamma_results)/ R, 4), \" ; cor2 =\", round(sum(the_cor2 < cor_bakers_gamma_results)/ R, 4) ))\n\nWe can see that we do not have enough evidence that dend15 and dend51 are significantly âsimilarâ (i.e.: with a correlation larger than 0).\n\nWe can also build a bootstrap confidence interval, using sample.dendrogram, for the correlation. This function can be very slow for larger trees, so make sure you use if carefully:\n\ndend1 <- dend15 dend2 <- dend51 set.seed(23801) R <- 100 dend1_labels <- labels(dend1) dend2_labels <- labels(dend2) cor_bakers_gamma_results <- numeric(R) for(i in 1:R) { sampled_labels <- sample(dend1_labels, replace = TRUE) # members needs to be fixed since it will be later used in nleaves dend_mixed1 <- sample.dendrogram(dend1, dend_labels=dend1_labels, fix_members=TRUE,fix_order=TRUE,fix_midpoint=FALSE, replace = TRUE, sampled_labels=sampled_labels ) dend_mixed2 <- sample.dendrogram(dend2, dend_labels=dend2_labels, fix_members=TRUE,fix_order=TRUE,fix_midpoint=FALSE, replace = TRUE, sampled_labels=sampled_labels ) cor_bakers_gamma_results[i] <- cor_bakers_gamma(dend_mixed1, dend_mixed2, warn = FALSE) } # here is the tanglegram tanglegram(dend1, dend2)\n\n# And here is the tanglegram for one sample of our trees: dend_mixed1 <- rank_order.dendrogram(dend_mixed1) dend_mixed2 <- rank_order.dendrogram(dend_mixed2) dend_mixed1 <- fix_members_attr.dendrogram(dend_mixed1) dend_mixed2 <- fix_members_attr.dendrogram(dend_mixed2) tanglegram(dend_mixed1, dend_mixed2)\n\ncor_bakers_gamma(dend_mixed1, dend_mixed2, warn = FALSE)\n\n#> [1] 1\n\nCI95 <- quantile(cor_bakers_gamma_results, probs=c(.025,.975)) CI95\n\n#> 2.5% 97.5% #> 0.2751938 1.0000000\n\npar(mfrow = c(1,1)) plot(density(cor_bakers_gamma_results), main = \"Baker's gamma bootstrap distribution\", xlim = c(-1,1)) abline(v = CI95, lty = 2, col = 3) abline(v = cor_bakers_gamma(dend1, dend2), lty = 2, col = 2) legend(\"topleft\", legend =c(\"95% CI\", \"Baker's Gamma Index\"), fill = c(3,2))\n\nThe bootstrap sampling can do weird things with small trees. In this case we had many times that the two trees got perfect correlation. The usage and interpretation should be done carefully!\n\nCophenetic correlation\n\nThe cophenetic distance between two observations that have been clustered is defined to be the inter-group dissimilarity at which the two observations are first combined into a single cluster. This distance has many ties and restrictions. The cophenetic correlation (see sokal 1962) is the correlation between two cophenetic distance matrices of two trees.\n\nThe value can range between -1 to 1. With near 0 values meaning that the two trees are not statistically similar. For exact p-value one should result to a permutation test. One such option will be to permute over the labels of one tree many times, and calculating the distribution under the null hypothesis (keeping the trees topologies constant).\n\ncor_cophenetic(dend15, dend51)\n\n#> [1] 0.3125\n\nThe function cor_cophenetic is faster than cor_bakers_gamma, and might be preferred for that reason.\n\nThe Fowlkes-Mallows Index and the Bk plot\n\nThe Fowlkes-Mallows Index\n\nThe Fowlkes-Mallows Index (see fowlkes 1983) (FM Index, or Bk) is a measure of similarity between two clusterings. The FM index ranges from 0 to 1, a higher value indicates a greater similarity between the two clusters.\n\nThe dendextend package allows the calculation of FM-Index, its expectancy and variance under the null hypothesis, and a creation of permutations of the FM-Index under H0. Thanks to the profdpm package, we have another example of calculating the FM (though it does not offer the expectancy and variance under H0):\n\nhc1 <- hclust(dist(iris[,-5]), \"com\") hc2 <- hclust(dist(iris[,-5]), \"single\") # FM index of a cluster with himself is 1: FM_index(cutree(hc1, k=3), cutree(hc1, k=3))\n\n#> [1] 1 #> attr(,\"E_FM\") #> [1] 0.37217 #> attr(,\"V_FM\") #> [1] 5.985372e-05\n\n# FM index of two clusterings: FM_index(cutree(hc1, k=3), cutree(hc2, k=3))\n\n#> [1] 0.8059522 #> attr(,\"E_FM\") #> [1] 0.4462325 #> attr(,\"V_FM\") #> [1] 6.464092e-05\n\n# we got a value far above the expected under H0 # Using the R code: FM_index_R(cutree(hc1, k=3), cutree(hc2, k=3))\n\n#> [1] 0.8059522 #> attr(,\"E_FM\") #> [1] 0.4462325 #> attr(,\"V_FM\") #> [1] 6.464092e-05\n\nThe E_FM and V_FM are the values expected under the null hypothesis that the two trees have the same topology but one is a random shuffle of the labels of the other (i.e.: âno connectionâ between the trees).\n\nSo for the values:\n\nFM_index(cutree(hc1, k=3), cutree(hc2, k=3))\n\n#> [1] 0.8059522 #> attr(,\"E_FM\") #> [1] 0.4462325 #> attr(,\"V_FM\") #> [1] 6.464092e-05\n\nWe can take (under a normal asymptotic distribution)\n\n0.4462 + 1.645 * sqrt(6.464092e-05)\n\n#> [1] 0.4594257\n\nAnd since 0.8059 (our value) > 0.4594 (the critical value under H0, with alpha=5% for a one sided test) - then we can say that we significantly reject the hypothesis that the two trees are ânot-similarâ.\n\nThe Bk plot\n\nIn the Bk method we calculate the FM Index (Bk) for each k (k=2,3,â¦,n-1) number of clusters, giving the association between the two trees when each is cut to have k groups. The similarity between two hierarchical clustering dendrograms, can be investigated, using the (k,Bk) plot: For every level of splitting of the two dendrograms which produces k clusters in each tree, the plot shows the number Bk, and therefore enables the investigation of potential nuances in the structure of similarity. The Bk measures the number of pairs of items which are in the same cluster in both dendrograms, one of the clusters in one of the trees and one of the clusters in the other tree, divided by the geometric mean of the number of pairs of items which are in the same cluster in each tree. Namely, \\({a_{uv}} = 1\\left( {or{\\rm{ }}{{\\rm{b}}_{uv}} = 1} \\right)\\) if the items u and v are in the same cluster in the first tree (second tree), when it is cut so to give k clusters, and otherwise 0:\n\n\\[{FM_k} = {B_k} = \\frac{{\\sum\\limits_{}^{} {{a_{uv}}{b_{uv}}} }}{{\\sqrt {\\sum\\limits_{}^{} {{a_{uv}}} \\sum\\limits_{}^{} {{b_{uv}}} } }}\\]\n\nThe Bk measure can be plotted for every value of k (except k=n) in order to create the â(k,Bk) plotâ. The plot compares the similarity of the two trees for different cuts. The mean and variance of Bk, under the null hypothesis (that the two trees are not âsimilarâ), and under the assumption that the margins of the matching matrix are fixed, are given in Fowlkes and Mallows (see fowlkes 1983). They allow making inference on whether the results obtained are different from what would have been expected under the null hypothesis (of now particular order of the treesâ labels).\n\nThe Bk and the Bk_plot functions allow the calculation of the FM-Index for a range of k values on two trees. Here are examples:\n\nset.seed(23235) ss <- TRUE # sample(1:150, 30 ) # TRUE # hc1 <- hclust(dist(iris[ss,-5]), \"com\") hc2 <- hclust(dist(iris[ss,-5]), \"single\") dend1 <- as.dendrogram(hc1) dend2 <- as.dendrogram(hc2) # cutree(tree1) # It works the same for hclust and dendrograms: Bk(hc1, hc2, k = 3)\n\n#> $`3` #> [1] 0.8059522 #> attr(,\"E_FM\") #> [1] 0.4462325 #> attr(,\"V_FM\") #> [1] 6.464092e-05\n\nBk(dend1, dend2, k = 3)\n\n#> $`3` #> [1] 0.8059522 #> attr(,\"E_FM\") #> [1] 0.4462325 #> attr(,\"V_FM\") #> [1] 6.464092e-05\n\nThe Bk plot:\n\nBk_plot(hc1, hc2, main = \"WRONG Bk plot \\n(due to the way cutree works with ties in hclust)\", warn = FALSE)\n\nBk_plot(dend1, dend2, main = \"CORRECT Bk plot \\n(based on dendrograms)\")"
    }
}
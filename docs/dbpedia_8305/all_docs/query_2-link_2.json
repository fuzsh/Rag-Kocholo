{
    "id": "dbpedia_8305_2",
    "rank": 2,
    "data": {
        "url": "https://www.analyticsvidhya.com/blog/2019/05/beginners-guide-hierarchical-clustering/",
        "read_more_link": "",
        "language": "en",
        "title": "What is Hierarchical Clustering in Python?",
        "top_image": "https://cdn.analyticsvidhya.com/wp-content/uploads/2019/05/hierarchical_clustering.jpg",
        "meta_img": "https://cdn.analyticsvidhya.com/wp-content/uploads/2019/05/hierarchical_clustering.jpg",
        "images": [
            "https://av-public-assets.s3.ap-south-1.amazonaws.com/logos/av-logo-svg.svg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/navbar.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/navbar.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/navbar.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/navbar.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/navbar.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/default_avatar.svg",
            "https://av-identity.s3.amazonaws.com/users/user/nocNbboBQPy1IoXTP7JInw.jpg",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2019/05/Screenshot-from-2019-05-15-13-10-32.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2019/05/Screenshot-from-2019-05-15-13-11-28.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2019/05/Screenshot-from-2019-05-15-13-12-35.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2019/05/Screenshot-from-2019-05-15-13-11-28.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2019/05/Screenshot-from-2019-05-15-13-31-06.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2019/05/Screenshot-from-2019-05-15-13-12-35.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2019/05/Screenshot-from-2019-05-15-13-11-28.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2019/05/Screenshot-from-2019-05-15-12-10-09.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2019/05/Screenshot-from-2019-05-17-15-57-13.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2019/05/Screenshot-from-2019-05-15-12-22-27.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2019/05/Screenshot-from-2019-05-15-12-32-18.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2019/05/Screenshot-from-2019-05-15-12-34-48.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2019/05/teacher-student.jpg",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2019/05/Screenshot-from-2019-05-17-16-12-33.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2019/05/Screenshot-from-2019-05-15-14-46-36.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2019/05/Screenshot-from-2019-05-15-14-54-20.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2019/05/Screenshot-from-2019-05-15-14-57-30.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2019/05/Screenshot-from-2019-05-15-14-58-11.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2019/05/Screenshot-from-2019-05-15-14-59-19.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2019/05/Screenshot-from-2019-05-15-15-01-11.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2019/05/Screenshot-from-2019-05-15-15-03-17.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2019/05/Screenshot-from-2019-05-16-17-11-39.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2019/05/Screenshot-from-2019-05-15-15-03-17.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2019/05/Screenshot-from-2019-05-16-18-06-37.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2019/05/Screenshot-from-2019-05-16-18-10-06.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2019/05/Screenshot-from-2019-05-16-18-12-55.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2019/05/Screenshot-from-2019-05-17-12-28-15.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2019/05/Screenshot-from-2019-05-17-12-34-15.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2019/05/Screenshot-from-2019-05-17-12-36-20.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2019/05/Screenshot-from-2019-05-17-12-40-02.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2019/05/Screenshot-from-2019-05-17-12-41-16.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2019/05/Screenshot-from-2019-05-17-12-42-44.png",
            "https://av-identity.s3.amazonaws.com/users/user/nocNbboBQPy1IoXTP7JInw.jpg",
            "https://secure.gravatar.com/avatar/9948f11609e1213e2a55f0e26c48cfb4?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/c49b1d13ae0e3ad57e58fde2b3840729?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/493d077b4b51a8f82c56daccfc970403?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/224ac38bcbf1a33a26c1106008fb5ce3?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/eed74e736d0a3795af0cc24f161a8a24?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/a1b8d1d53aa217111f2edb2f08b089e4?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/49533e24e02d26878d37e97072215f37?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/eed74e736d0a3795af0cc24f161a8a24?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/944549b6a7469bc381c01c49d9924b1c?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/eed74e736d0a3795af0cc24f161a8a24?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/ceab7ddec11c59139e38aaac5b474513?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/eed74e736d0a3795af0cc24f161a8a24?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/a95d7f690367c07ea2e9c8e3254f7a1c?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/8ece5540ab7b625f58a8b556e7134630?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/eed74e736d0a3795af0cc24f161a8a24?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/d153df3e4af8fc96ad9aea0b6d4567bb?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/eed74e736d0a3795af0cc24f161a8a24?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/e30b8e72f6d7868fd90a8e1225e717c7?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/eed74e736d0a3795af0cc24f161a8a24?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/899ebf26121df18ac7c00e3e50339da9?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/eed74e736d0a3795af0cc24f161a8a24?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/dff64537b93fe65a897033a9e2dd3878?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/eed74e736d0a3795af0cc24f161a8a24?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/dff64537b93fe65a897033a9e2dd3878?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/eed74e736d0a3795af0cc24f161a8a24?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/dff64537b93fe65a897033a9e2dd3878?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/eed74e736d0a3795af0cc24f161a8a24?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/c0145da5fec8ea13b44143e717d5fc89?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/eed74e736d0a3795af0cc24f161a8a24?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/c49b37dad2db77301de0b5ad95b18229?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/eed74e736d0a3795af0cc24f161a8a24?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/39ad4c4fcf851f062e6d13a8e1b01b96?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/eed74e736d0a3795af0cc24f161a8a24?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/98a086c9a8852851cbbba625844d08de?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/f1840fe142d5777ad262bdfd65555e3e?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/cd41fa05233de1723cb91a736bb50284?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/838e664ec201cc3de846d72e697ac609?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/b819a94da5928d8775c3bf45b8fd8a13?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/e35d9a24eaf5f5b9bbd12379e5b11446?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/8b409dbe1d5f70079f46006b9911ac04?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/e62c7dc58d34fd12098b6c53af7bc1df?s=74&d=mm&r=g",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/path-digital.png",
            "https://av-identity.s3.amazonaws.com/users/user/bGnsep7nT0GMWuLpkDl15Q.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/in.png",
            "https://av-identity.s3.amazonaws.com/users/user/R7HrsWl1QrGRiw_e9m4fDA.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/in.png",
            "https://av-identity.s3.amazonaws.com/users/user/ZcU4ALTFT96MVCzfiGuhsQ.jpeg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/in.png",
            "https://av-identity.s3.amazonaws.com/users/user/aM3WrxdNSTGLg7LoqX-q0w.png",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/in.png",
            "https://av-identity.s3.amazonaws.com/users/user/zy4FL_yyQlG4PkWcyGYvhw.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/in.png",
            "https://av-identity.s3.amazonaws.com/users/user/a4ByfUyoQRmdGzLpBzHVLw.jpeg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/in.png",
            "https://av-identity.s3.amazonaws.com/users/user/ZTsmKl-1Qvqn07FUzgaBNw.png",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/in.png",
            "https://d2cd20fxv8fgim.cloudfront.net/homepage/images/Play_Store.svg",
            "https://d2cd20fxv8fgim.cloudfront.net/homepage/images/App_Store.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Pulkit Sharma"
        ],
        "publish_date": "2019-05-27T03:59:48+00:00",
        "summary": "",
        "meta_description": "Explore Hierarchical Clustering: Types & Applications. Learn Agglomerative & Divisive methods. Understand it in customer segmentation.",
        "meta_lang": "en",
        "meta_favicon": "https://imgcdn.analyticsvidhya.com/favicon/av-fav.ico",
        "meta_site_name": "Analytics Vidhya",
        "canonical_link": "https://www.analyticsvidhya.com/blog/2019/05/beginners-guide-hierarchical-clustering/",
        "text": "Introduction\n\nIn the vast landscape of data exploration, where datasets sprawl like forests, hierarchical clustering acts as a guiding light, leading us through the dense thicket of information. Imagine a dendrogram, a visual representation of data relationships, branching out like a tree, revealing clusters and connections within the data. This is where machine learning meets the art of clustering, where Python serves as the wizard‚Äôs wand, casting spells of insight into the heart of datasets.\n\nIn this journey through the Python kingdom, we will unravel the mysteries of hierarchical clustering, exploring its intricacies and applications in data science. From dendrograms to distance matrices, from agglomerative to divisive clustering, we will delve deep into the techniques and methods that make hierarchical clustering a cornerstone of data analysis.\n\nJoin us as we embark on this adventure, where data points become nodes in a vast knowledge network, and clusters emerge like constellations in the night sky, guiding us toward the insights hidden within the data. Welcome to the world of hierarchical clustering in Python, where every cluster tells a story, and every dendrogram holds the key to unlocking the secrets of data science.\n\nStudy Material\n\nThere are multiple ways to perform clustering. I encourage you to check out our awesome guide to the different types of clustering: An Introduction to Clustering and different methods of clustering\n\nTo learn more about clustering and other machine learning algorithms (both supervised and unsupervised) check out the following comprehensive program- Certified AI & ML Blackbelt+ Program\n\nWhat is Hierarchical Clustering?\n\nHierarchical clustering is an unsupervised learning technique for grouping similar objects into clusters. It creates a hierarchy of clusters by merging or splitting them based on similarity measures. It uses a bottom-up approach or top-down approach to construct a hierarchical data clustering schema.\n\nClustering Hierarchical groups similar objects into a dendrogram. It merges similar clusters iteratively, starting with each data point as a separate cluster. This creates a tree-like structure that shows the relationships between clusters and their hierarchy.\n\nThe dendrogram from hierarchical clustering reveals the hierarchy of clusters at different levels, highlighting natural groupings in the data. It provides a visual representation of the relationships between clusters, helping to identify patterns and outliers, making it a valuable tool for exploratory data analysis. For example, let‚Äôs say we have the below points, and we want to cluster them into groups:\n\nWe can assign each of these points to a separate cluster:\n\nNow, based on the similarity of these clusters, we can combine the most similar clusters together and repeat this process until only a single cluster is left:\n\nWe are essentially building a hierarchy of clusters. That‚Äôs why this algorithm is called hierarchical clustering. I will discuss how to decide the number of clusters later. For now, let‚Äôs look at the different types of hierarchical clustering.\n\nAlso Read: Python Interview Questions to Ace Your Next Job Interview in 2024\n\nTypes of Hierarchical Clustering\n\nThere are mainly two types of hierarchical clustering:\n\nAgglomerative hierarchical clustering\n\nDivisive Hierarchical clustering\n\nLet‚Äôs understand each type in detail.\n\nAgglomerative Clustering Hierarchical\n\nWe assign each point to an individual cluster in this technique. Suppose there are 4 data points. We will assign each of these points to a cluster and hence will have 4 clusters in the beginning:\n\nThen, at each iteration, we merge the closest pair of clusters and repeat this step until only a single cluster is left:\n\nWe are merging (or adding) the clusters at each step, right? Hence, this type of clustering is also known as additive hierarchical clustering.\n\nDivisive Hierarchical Clustering\n\nDivisive Clustering Hierarchical works in the opposite way. Instead of starting with n clusters (in case of n observations), we start with a single cluster and assign all the points to that cluster.\n\nSo, it doesn‚Äôt matter if we have 10 or 1000 data points. All these points will belong to the same cluster at the beginning:\n\nNow, at each iteration, we split the farthest point in the cluster and repeat this process until each cluster only contains a single point:\n\nWe are splitting (or dividing) the clusters at each step, hence the name divisive hierarchical clustering.\n\nAgglomerative Clustering is widely used in the industry and will be the article‚Äôs focus. Divisive hierarchical clustering will be a piece of cake once we have a handle on the agglomerative type\n\nAlso Read: Python Tutorial to Learn Data Science from Scratch\n\nApplications of Hierarchical Clustering\n\nHere are some common applications of hierarchical clustering:\n\nBiological Taxonomy: Hierarchical clustering is extensively used in biology to classify organisms into hierarchical taxonomies based on similarities in genetic or phenotypic characteristics. It helps understand evolutionary relationships and biodiversity.\n\nDocument Clustering: In natural language processing, hierarchical clustering groups similar documents or texts. It aids in topic modeling, document organization, and information retrieval systems.\n\nImage Segmentation: Hierarchical clustering segments images by grouping similar pixels or regions based on color, texture, or other visual features. It finds applications in medical imaging, remote sensing, and computer vision.\n\nCustomer Segmentation: Businesses use hierarchical clustering to group customers into groups based on their purchasing behaviors, demographics, or preferences. This helps with targeted marketing, personalized recommendations, and customer relationship management.\n\nAnomaly Detection: Hierarchical clustering can identify outliers or anomalies in datasets by isolating data points that do not fit well into any cluster. It is useful in fraud detection, network security, and quality control.\n\nSocial Network Analysis: Hierarchical clustering helps uncover community structures or hierarchical relationships in social networks by clustering users based on their interactions, interests, or affiliations. It aids in understanding network dynamics and identifying influential users.\n\nMarket Basket Analysis: Retailers use hierarchical clustering to analyze transaction data and identify associations between products frequently purchased together. It enables them to optimize product placements, promotions, and cross-selling strategies.\n\nAdvantages and Disadvantages of Hierarchical Clustering\n\nHere are some advantages and disadvantages of hierarchical clustering:\n\nAdvantages of hierarchical clustering:\n\nEasy to interpret: Hierarchical clustering produces a dendrogram, a tree-like structure that shows the order in which clusters are merged. This dendrogram provides a clear visualization of the relationships between clusters, making it easy to interpret the results.\n\nNo need to specify the number of clusters: Unlike other clustering algorithms, such as k-means, hierarchical clustering does not require you to specify the number of clusters beforehand. The algorithm determines the number of clusters based on the data and the chosen linkage method.\n\nCaptures nested clusters: Hierarchical clustering captures the hierarchical structure in the data, meaning it can identify clusters within clusters (nested clusters). This can be useful when the data naturally forms a hierarchy.\n\nRobust to noise: Hierarchical clustering is robust to noise and outliers because it considers the entire dataset when forming clusters. Outliers may not significantly affect the clustering process, especially if a suitable distance metric and linkage method are chosen.\n\nDisadvantages of hierarchical clustering:\n\nComputational complexity: Hierarchical clustering can be computationally expensive, especially for large datasets. The time complexity of hierarchical clustering algorithms is typically ùëÇ(ùëõ2log‚Å°ùëõ)O(n2logn) or ùëÇ(ùëõ3)O(n3), where ùëõn is the number of data points.\n\nMemory usage: Besides computational complexity, hierarchical clustering algorithms can consume a lot of memory, particularly when dealing with large datasets. Storing the entire distance matrix between data points can require substantial memory.\n\nDifficulty with large datasets: Due to its computational complexity and memory requirements, hierarchical clustering may not be suitable for large datasets. In such cases, alternative clustering methods, such as k-means or DBSCAN, may be more appropriate.\n\nSensitive to noise and outliers: While hierarchical clustering is generally robust to noise and outliers, extreme outliers or noise points can still affect the clustering results, especially if they are not handled properly beforehand.\n\nDifficulty in merging clusters: Once clusters are formed in hierarchical clustering, merging or splitting them can be difficult, especially if the clustering uses a divisive method. This lack of flexibility can be a limitation in certain scenarios where cluster adjustments are needed.\n\nApplication of Hierarchical Clustering with Python\n\nIn Python, the scipy and scikit-learn libraries are often used to perform hierarchical clustering. Here‚Äôs how you can apply hierarchical clustering using Python:\n\nImport Necessary Libraries: First, you‚Äôll need to import the necessary libraries: numpy for numerical operations, matplotlib for plotting, and scipy.cluster.hierarchy for hierarchical clustering.\n\nGenerate or Load Data: You can either generate a synthetic dataset or load your dataset.\n\nCompute the Distance Matrix: Compute the distance matrix which will be used to form clusters.\n\nPerform Hierarchical Clustering: Use the linkage method to perform hierarchical clustering.\n\nPlot the Dendrogram: Visualize the clusters using a dendrogram.\n\nHere‚Äôs an example of hierarchical clustering using Python:\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\nfrom scipy.cluster.hierarchy import fcluster\n\nfrom sklearn.datasets import make_blobs\n\n# Generate sample data\n\nX, y = make_blobs(n_samples=100, centers=3, cluster_std=0.60, random_state=0)\n\n# Compute the linkage matrix\n\nZ = linkage(X, 'ward')\n\n# Plot the dendrogram\n\nplt.figure(figsize=(10, 7))\n\nplt.title(\"Dendrogram\")\n\nplt.xlabel(\"Sample index\")\n\nplt.ylabel(\"Distance\")\n\ndendrogram(Z)\n\nplt.show()\n\n# Determine the clusters\n\nmax_d = 7.0 # this can be adjusted based on the dendrogram\n\nclusters = fcluster(Z, max_d, criterion='distance')\n\n# Plot the clusters\n\nplt.figure(figsize=(10, 7))\n\nplt.scatter(X[:, 0], X[:, 1], c=clusters, cmap='prism')\n\nplt.title(\"Hierarchical Clustering\")\n\nplt.xlabel(\"Feature 1\")\n\nplt.ylabel(\"Feature 2\")\n\nplt.show()\n\nSupervised vs Unsupervised Learning\n\nUnderstanding the difference between supervised and unsupervised learning is important before we dive into the Clustering hierarchy. Let me explain this difference using a simple example.\n\nSuppose we want to estimate the count of bikes that will be rented in a city every day:\n\nOr, let‚Äôs say we want to predict whether a person on board the Titanic survived or not:\n\nExamples\n\nIn the first example, we have to predict the number of bikes based on features like the season, holiday, working day, weather, temperature, etc.\n\nIn the second example, we are predicting whether a passenger survived. In th‚Äô ‚ÄòSurviv‚Äôd‚Äô variable, 0 represents that the person did not survive, and 1 means the person did make it out alive. The independent variables here include Pclass, Sex, Age, Fare, etc.\n\nLet‚Äôs look at the figure below to understand this visually:\n\nHere, y is our dependent or target variable, and X represents the independent variables. The target variable is dependent on X, also called a dependent variable. We train our model using the independent variables to supervise the target variable. Hence, the name supervised learning.\n\nWhen training the model, we aim to generate a function that maps the independent variables to the desired target. Once the model is trained, we can pass new sets of observations, and the model will predict their target. This, in a nutshell, is supervised learning.\n\nIn these cases, we try to divide the entire data into a set of groups. These groups are known as clusters, and the process of making them is known as clustering.\n\nThis technique is generally used for clustering a population into different groups. A few common examples include segmenting customers, clustering similar documents, recommending similar songs or movies, etc.\n\nThere are many more applications of unsupervised learning. If you come across any interesting ones, feel free to share them in the comments section below!\n\nVarious algorithms help us make these clusters. The most commonly used clustering algorithms are K-means and Hierarchical clustering\n\nWhy Hierarchical Clustering?\n\nWe should first know how K-means works before we dive into hierarchical clustering. Trust me, it will make the concept of hierarchical clustering much easier.\n\nHere‚Äôs a brief overview of how K-means works:\n\nDecide the number of clusters (k)\n\nSelect k random points from the data as centroids\n\nAssign all the points to the nearest cluster centroid\n\nCalculate the centroid of newly formed clusters\n\nRepeat steps 3 and 4\n\nIt is an iterative process. It will keep on running until the centroids of newly formed clusters do not change or the maximum number of iterations are reached.\n\nBut there are certain challenges with K-means. It always tries to make clusters of the same size. Also, we have to decide the number of clusters at the beginning of the algorithm. Ideally, we would not know how many clusters should we have, in the beginning of the algorithm and hence it a challenge with K-means.\n\nThis is a gap hierarchical clustering bridge with aplomb. It takes away the problem of having to pre-define the number of clusters. Sounds like a dream! So, let‚Äôs see what hierarchical clustering is and how it improves on K-means.\n\nHow Does Hierarchical Clustering Improve on K-means?\n\nHierarchical clustering and K-means are popular clustering algorithms but have different strengths and weaknesses. Here are some ways in which hierarchical clustering can improve on K-means:\n\n1. No Need to Pre-specify Number of Clusters\n\nHierarchical Clustering:\n\nDoes not require the number of clusters (k) to be specified in advance.\n\nThe dendrogram provides a visual representation of the hierarchy of clusters, and the number of clusters can be determined by cutting the dendrogram at a desired level.\n\nK-means:\n\nRequires the number of clusters (k) to be specified beforehand, which can be difficult if the optimal number of clusters is unknown.\n\n2. Captures Nested Clusters\n\nHierarchical Clustering:\n\nIt can identify nested clusters, meaning it can find clusters within them.\n\nThis is useful for datasets with a natural hierarchical structure (e.g., taxonomy of biological species).\n\nK-means:\n\nAssumes clusters are flat and do not capture hierarchical relationships.\n\n3. Flexibility with Cluster Shapes\n\nHierarchical Clustering:\n\nCan find clusters of arbitrary shapes.\n\nThe algorithm is not restricted to spherical clusters and can capture more complex cluster structures.\n\nK-means:\n\nAssumes clusters are spherical and of similar size, which may not be suitable for datasets with irregularly shaped clusters.\n\n4. Distance Metrics and Linkage Criteria\n\nHierarchical Clustering:\n\nOffers flexibility in distance metrics (e.g., Euclidean, Manhattan) and linkage criteria (e.g., single, complete, average).\n\nThis flexibility can improve clustering performance on different types of data.\n\nK-means:\n\nTypically, it uses the Euclidean distance, which may not be suitable for all data types.\n\n5. Handling Outliers\n\nHierarchical Clustering:\n\nOutliers can be identified as singleton clusters at the bottom of the dendrogram.\n\nThis makes it easier to detect and potentially remove outliers.\n\nK-means:\n\nSensitive to outliers, as they can significantly affect the position of cluster centroids.\n\n6. Robustness to Initialization\n\nHierarchical Clustering:\n\nDoes not require random initialization of cluster centroids.\n\nThe clustering result is deterministic and does not depend on initial conditions.\n\nK-means:\n\nRequires random initialization of centroids, leading to different clustering results in different runs.\n\nThe algorithm may converge to local minima, depending on the initial placement of centroids.\n\n7. Visual Interpretation\n\nHierarchical Clustering:\n\nThe dendrogram provides a visual and interpretable representation of the clustering process.\n\nIt helps in understanding the relationships between clusters and the data structure.\n\nK-means:\n\nProvides cluster labels and centroids, but does not visually represent the clustering process.\n\nPractical Example\n\nLet‚Äôs consider a practical example using hierarchical clustering and K-means on a simple dataset:\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\nfrom sklearn.datasets import make_blobs\n\nfrom sklearn.cluster import KMeans\n\n# Generate sample data\n\nX, y = make_blobs(n_samples=100, centers=3, cluster_std=0.60, random_state=0)\n\n# Hierarchical Clustering\n\nZ = linkage(X, 'ward')\n\nplt.figure(figsize=(10, 7))\n\nplt.title(\"Hierarchical Clustering Dendrogram\")\n\ndendrogram(Z)\n\nplt.show()\n\n# K-means Clustering\n\nkmeans = KMeans(n_clusters=3, random_state=0).fit(X)\n\nlabels = kmeans.labels_\n\nplt.figure(figsize=(10, 7))\n\nplt.scatter(X[:, 0], X[:, 1], c=labels, cmap='prism')\n\nplt.title(\"K-means Clustering\")\n\nplt.show()\n\nSteps to Perform Hierarchical Clustering\n\nWe merge the most similar points or clusters in hierarchical clustering ‚Äì we know this. Now the question is ‚Äì how do we decide which points are similar and which are not? It‚Äôs one of the most important questions in clustering!\n\nHere‚Äôs one way to calculate similarity ‚Äì Take the distance between the centroids of these clusters. The points having the least distance are referred to as similar points and we can merge them. We can refer to this as a distance-based algorithm as well (since we are calculating the distances between the clusters).\n\nIn hierarchical clustering, we have a concept called a proximity matrix. This stores the distances between each point. Let‚Äôs take an example to understand this matrix and the steps to perform hierarchical clustering.\n\nSetting up the Example\n\nSuppose a teacher wants to divide her students into different groups. She has the marks scored by each student in an assignment and based on these marks, she wants to segment them into groups. There‚Äôs no fixed target here as to how many groups to have. Since the teacher does not know what type of students should be assigned to which group, it cannot be solved as a supervised learning problem. So, we will try to apply hierarchical clustering here and segment the students into different groups.\n\nLet‚Äôs take a sample of 5 students:\n\nCreating a Proximity Matrix\n\nFirst, we will create a proximity matrix which will tell us the distance between each of these points. Since we are calculating the distance of each point from each of the other points, we will get a square matrix of shape n X n (where n is the number of observations).\n\nLet‚Äôs make the 5 x 5 proximity matrix for our example:\n\nThe diagonal elements of this matrix will always be 0 as the distance of a point with itself is always 0. We will use the Euclidean distance formula to calculate the rest of the distances. So, let‚Äôs say we want to calculate the distance between point 1 and 2:\n\n‚àö(10-7)^2 = ‚àö9 = 3\n\nSimilarly, we can calculate all the distances and fill the proximity matrix.\n\nSteps to Perform Hierarchical Clustering\n\nStep 1: First, we assign all the points to an individual cluster:\n\nDifferent colors here represent different clusters. You can see that we have 5 different clusters for the 5 points in our data.\n\nStep 2: Next, we will look at the smallest distance in the proximity matrix and merge the points with the smallest distance. We then update the proximity matrix:\n\nHere, the smallest distance is 3 and hence we will merge point 1 and 2:\n\nLet‚Äôs look at the updated clusters and accordingly update the proximity matrix:\n\nHere, we have taken the maximum of the two marks (7, 10) to replace the marks for this cluster. Instead of the maximum, we can also take the minimum value or the average values as well. Now, we will again calculate the proximity matrix for these clusters:\n\nStep 3: We will repeat step 2 until only a single cluster is left.\n\nSo, we will first look at the minimum distance in the proximity matrix and then merge the closest pair of clusters. We will get the merged clusters as shown below after repeating these steps:\n\nWe started with 5 clusters and finally had a single cluster. This is how agglomerative hierarchical clustering works. But the burning question remains‚Äîhow do we decide the number of clusters? Let‚Äôs understand that in the next section.\n\nHow to Choose the Number of Clusters in Hierarchical Clustering?\n\nAre you ready to finally answer this question that‚Äôs been hanging around since we started learning? To get the number of clusters for hierarchical clustering, we use an awesome concept called a Dendrogram.\n\nA dendrogram is a tree-like diagram that records the sequences of merges or splits.\n\nExample\n\nLet‚Äôs get back to the teacher-student example. Whenever we merge two clusters, a dendrogram will record the distance between them and represent it in graph form. Let‚Äôs see how a dendrogram looks:\n\nWe have the samples of the dataset on the x-axis and the distance on the y-axis. Whenever two clusters are merged, we will join them in this dendrogram, and the height of the join will be the distance between these points. Let‚Äôs build the dendrogram for our example:\n\nTake a moment to process the above image. We started by merging sample 1 and 2 and the distance between these two samples was 3 (refer to the first proximity matrix in the previous section). Let‚Äôs plot this in the dendrogram:\n\nHere, we can see that we have merged samples 1 and 2. The vertical line represents the distance between these samples. Similarly, we plot all the steps where we merged the clusters, and finally, we get a dendrogram like this:\n\nWe can visualize the steps of hierarchical clustering. The more the distance of the vertical lines in the dendrogram, the more the distance between those clusters.\n\nNow, we can set a threshold distance and draw a horizontal line (Generally, we try to set the threshold so that it cuts the tallest vertical line). Let‚Äôs set this threshold as 12 and draw a horizontal line:\n\nThe number of clusters will be the number of vertical lines intersected by the line drawn using the threshold. In the above example, since the red line intersects 2 vertical lines, we will have 2 clusters. One cluster will have a sample (1,2,4) and the other will have a sample (3,5).\n\nSolving the Wholesale Customer Segmentation Problem\n\nTime to get our hands dirty in Python!\n\nWe will be working on a wholesale customer segmentation problem. You can download the dataset using this link. The data is hosted on the UCI Machine Learning repository. This problem aims to segment the clients of a wholesale distributor based on their annual spending on diverse product categories, like milk, grocery, region, etc.\n\nLet‚Äôs explore the data first and then apply Hierarchical Clustering to segment the clients.\n\nRequired Libraries\n\nLoad the data and look at the first few rows:\n\nPython Code\n\nThere are multiple product categories ‚Äì Fresh, Milk, Grocery, etc. The values represent the number of units each client purchases for each product. We aim to make clusters from this data to segment similar clients. We will, of course, use Hierarchical Clustering for this problem.\n\nBut before applying, we have to normalize the data so that the scale of each variable is the same. Why is this important? If the scale of the variables is not the same, the model might become biased towards the variables with a higher magnitude, such as fresh or milk (refer to the above table).\n\nSo, let‚Äôs first normalize the data and bring all the variables to the same scale:\n\nHere, we can see that the scale of all the variables is almost similar. Now, we are good to go. Let‚Äôs first draw the dendrogram to help us decide the number of clusters for this particular problem:\n\nThe x-axis contains the samples and y-axis represents the distance between these samples. The vertical line with maximum distance is the blue line and hence we can decide a threshold of 6 and cut the dendrogram:\n\nWe have two clusters as this line cuts the dendrogram at two points. Let‚Äôs now apply hierarchical clustering for 2 clusters:\n\nWe can see the values of 0s and 1s in the output since we defined 2 clusters. 0 represents the points that belong to the first cluster and 1 represents points in the second cluster. Let‚Äôs now visualize the two clusters:\n\nAwesome! We can visualize the two clusters here. This is how we can implement hierarchical clustering in Python.\n\nConclusion\n\nIn our journey, we‚Äôve uncovered a powerful tool for unraveling the complexities of data relationships. From the conceptual elegance of dendrograms to their practical applications in diverse fields like biology, document analysis, cluster analysis, and customer segmentation, hierarchical cluster analysis emerges as a guiding light in the labyrinth of data exploration.\n\nAs we conclude this expedition, we stand at the threshold of possibility, where every cluster tells a story, and every dendrogram holds the key to unlocking the secrets of data science. In the ever-expanding landscape of Python and machine learning, hierarchical clustering stands as a stalwart companion, guiding us toward new horizons of discovery and understanding.\n\nIf you are still relatively new to data science, I highly recommend taking the Applied Machine Learning course. It is one of the most comprehensive end-to-end machine learning courses you will find anywhere. Hierarchical clustering is just one of the diverse topics we cover in the course.\n\nWhat are your thoughts on hierarchical clustering? Do you feel there‚Äôs a better way to create clusters using less computational resources? Connect with me in the comments section below, and let‚Äôs discuss!\n\nFrequently Asked Questions?"
    }
}
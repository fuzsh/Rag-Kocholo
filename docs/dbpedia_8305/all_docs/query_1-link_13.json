{
    "id": "dbpedia_8305_1",
    "rank": 13,
    "data": {
        "url": "https://www.analyticsvidhya.com/blog/2021/06/single-link-hierarchical-clustering-clearly-explained/",
        "read_more_link": "",
        "language": "en",
        "title": "Single-Link Hierarchical Clustering Clearly Explained!",
        "top_image": "https://cdn.analyticsvidhya.com/wp-content/uploads/2019/12/k_means_clustering.jpg",
        "meta_img": "https://cdn.analyticsvidhya.com/wp-content/uploads/2019/12/k_means_clustering.jpg",
        "images": [
            "https://av-public-assets.s3.ap-south-1.amazonaws.com/logos/av-logo-svg.svg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/navbar.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/navbar.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/navbar.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/navbar.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/navbar.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/default_avatar.svg",
            "https://av-identity.s3.amazonaws.com/users/user/G0CPQHKUTQqtE1QGhqQkmA.JPG",
            "https://editor.analyticsvidhya.com/uploads/45830agg_fig.gif",
            "https://editor.analyticsvidhya.com/uploads/40351linkages.PNG",
            "https://editor.analyticsvidhya.com/uploads/63179dataframe.PNG",
            "https://editor.analyticsvidhya.com/uploads/25734scatterplot.png",
            "https://editor.analyticsvidhya.com/uploads/57992dist.PNG",
            "https://editor.analyticsvidhya.com/uploads/71315step1.PNG",
            "https://editor.analyticsvidhya.com/uploads/61173step2Updated.PNG",
            "https://editor.analyticsvidhya.com/uploads/98460step3Updated.PNG",
            "https://editor.analyticsvidhya.com/uploads/89498step4 - Copy.PNG",
            "https://editor.analyticsvidhya.com/uploads/846491_5rw_O4XuWkNjQf1nxxSyLw.png",
            "https://editor.analyticsvidhya.com/uploads/20905step5 - Copy.PNG",
            "https://editor.analyticsvidhya.com/uploads/480541_gCxnomipKDKk6zw8UbL1nw.png",
            "https://av-identity.s3.amazonaws.com/users/user/G0CPQHKUTQqtE1QGhqQkmA.JPG",
            "https://secure.gravatar.com/avatar/8670fab03cdb7847f302600e6ff8ec23?s=74&d=mm&r=g",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/path-digital.png",
            "https://av-identity.s3.amazonaws.com/users/user/bGnsep7nT0GMWuLpkDl15Q.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/in.png",
            "https://av-identity.s3.amazonaws.com/users/user/R7HrsWl1QrGRiw_e9m4fDA.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/in.png",
            "https://av-identity.s3.amazonaws.com/users/user/ZcU4ALTFT96MVCzfiGuhsQ.jpeg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/in.png",
            "https://av-identity.s3.amazonaws.com/users/user/aM3WrxdNSTGLg7LoqX-q0w.png",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/in.png",
            "https://av-identity.s3.amazonaws.com/users/user/zy4FL_yyQlG4PkWcyGYvhw.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/in.png",
            "https://av-identity.s3.amazonaws.com/users/user/a4ByfUyoQRmdGzLpBzHVLw.jpeg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/in.png",
            "https://av-identity.s3.amazonaws.com/users/user/ZTsmKl-1Qvqn07FUzgaBNw.png",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/in.png",
            "https://d2cd20fxv8fgim.cloudfront.net/homepage/images/Play_Store.svg",
            "https://d2cd20fxv8fgim.cloudfront.net/homepage/images/App_Store.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Harika Bonthu"
        ],
        "publish_date": "2021-06-12T12:16:02+00:00",
        "summary": "",
        "meta_description": "In Single-link hierarchical clustering, the distance between two clusters is the minimum distance between members of the two clusters",
        "meta_lang": "en",
        "meta_favicon": "https://imgcdn.analyticsvidhya.com/favicon/av-fav.ico",
        "meta_site_name": "Analytics Vidhya",
        "canonical_link": "https://www.analyticsvidhya.com/blog/2021/06/single-link-hierarchical-clustering-clearly-explained/",
        "text": "Introduction\n\nAs we all know, single-link Hierarchical clustering begins by treating each observation as an individual cluster and then iteratively merges clusters until all the data points form a single cluster. Transitioning to the use of dendrograms to represent hierarchical clustering results, clusters merge based on the distance between them.\n\nVarious types of linkages, such as single linkage, complete linkage, and average linkage, utilize different methods to calculate the distance between clusters.\n\nLearning Objectives:\n\nUnderstand the concept and purpose of Linear Discriminant Analysis (LDA)\n\nLearn how LDA performs dimensionality reduction and classification\n\nGrasp the mathematical principles behind Fisher’s Linear Discriminant\n\nExplore LDA implementation and applications using Python\n\nThis article was published as a part of the Data Science Blogathon\n\nLinkage Criteria\n\nIt determines the distance between sets of observations as a function of the pairwise distance between observations.\n\nFurthermore, in Single Linkage Clustering, the distance between two clusters is the minimum distance between members of the two clusters.\n\nAdditionally, in Complete Linkage, the distance between two clusters is the maximum distance between members of the two clusters.\n\nMoreover, in Average Linkage, the distance between two clusters is the average of all distances between members of the two clusters.\n\nFinally, in Centroid Linkage, the distance between two clusters is the distance between their centroids.\n\nSimilarly, in this article, we aim to understand the Clustering process using the Single Linkage Clustering Method.\n\nClustering Using Single Linkage\n\nBegin with importing necessary libraries\n\nimport numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline import scipy.cluster.hierarchy as shc from scipy.spatial.distance import squareform, pdist\n\nLet us create toy data using numpy.random.random_sample\n\na = np.random.random_sample(size = 5) b = np.random.random_sample(size = 5)\n\nOnce we generate the random data points, we will create a pandas data frame.\n\npoint = ['P1','P2','P3','P4','P5'] data = pd.DataFrame({'Point':point, 'a':np.round(a,2), 'b':np.round(b,2)}) data = data.set_index('Point') data\n\nA glance at our toy data. Looks clean. Let us jump into the clustering steps.\n\nStep1: Visualize the data using a Scatter Plot\n\nplt.figure(figsize=(8,5)) plt.scatter(data['a'], data['b'], c='r', marker='*') plt.xlabel('Column a') plt.ylabel('column b') plt.title('Scatter Plot of x and y')for j in data.itertuples(): plt.annotate(j.Index, (j.a, j.b), fontsize=15)\n\nStep2: Calculating the distance matrix in Euclidean method using pdist\n\ndist = pd.DataFrame(squareform(pdist(data[[‘a’, ‘b’]]), ‘euclidean’), columns=data.index.values, index=data.index.values)\n\nFor our convenience, we will be considering only the lower bound values of the matrix as shown below. Specifically, the lower bound values represent the minimum distance between any two points in the dataset.\n\nStep 3: Look for the least distance and merge those into a cluster\n\nWe see the points P3, P4 has the least distance “0.30232”. So we will first merge those into a cluster.\n\nStep 4: Re-compute the distance matrix after forming a cluster\n\nUpdate the Distance Between\n\nCluster (P3,P4) to P1\n\n= Min(dist(P3,P4), P1)) -> Min(dist(P3,P1),dist(P4,P1)) = Min(0.59304, 0.46098) = 0.46098\n\nCluster (P3,P4) to P2\n\n= Min(dist(P3,P4), P2) -> Min(dist(P3,P2),dist(P4,P2)) = Min(0.77369, 0.61612) = 0.61612\n\nAnd Cluster (P3,P4) to P5\n\n= Min(dist(P3,P4), P5) -> Min(dist(P3,P5),dist(P4,P5)) = Min(0.45222, 0.35847) = 0.35847\n\nRepeat steps 3 and 4 until you are left with one single cluster.\n\nAfter re-computing the distance matrix, we need to again look for the least distance to make a cluster.\n\nSubsequently, we repeat this process until we have clustered all observations.\n\nWe see the points P2, P5 has the least distance “0.32388”. So we will group those into a cluster and recompute the distance matrix.\n\nUpdate the distance between the cluster (P2,P5) to P1\n\n= Min(dist((P2,P5),P1)) -> Min(dist(P2,P1), dist(P5, P1)) = Min(1.04139, 0.81841) = 0.81841\n\nUpdate the distance between the cluster (P2,P5) to (P3,P4)\n\n= Min(dist((P2,P5), (P3,P4))) -> = Min(dist(P2,(P3,P4)), dist(P5,(P3,P4))) = Min(dist(0.61612, 0.35847)) = 0.35847\n\nAfter recomputing the distance matrix, we need to again look for the least distance.\n\nThe cluster (P2,P5) has the least distance with the cluster (P3, P4) “0.35847”. So we will cluster them together.\n\nUpdate the distance between the cluster (P3,P4, P2,P5) to P1\n\n= Min(dist(((P3,P4),(P2,P5)), P1)) = Min(0.46098, 0.81841) = 0.46098\n\nWe have completed obtaining a single cluster.\n\nTheoretically, the clustering steps proceed as follows:\n\nTransitioning to active voice: P3 and P4 points merge due to their least distance.\n\nNext, P2 and P5 points merge as they exhibit the least distance.\n\nSubsequently, we cluster the pairs (P3, P4) and (P2, P5).\n\nFinally, we merge the cluster (P3, P4, P2, P5) with the datapoint P1.\n\nWe can visualize the same using a dendrogram\n\nplt.figure(figsize=(12,5)) plt.title(\"Dendrogram with Single inkage\") dend = shc.dendrogram(shc.linkage(data[['a', 'b']], method='single'), labels=data.index)\n\nThe length of the vertical lines in the dendrogram shows the distance. For example, the distance between the points P2, P5 is 0.32388.\n\nThe step-by-step clustering that we did is the same as the dendrogram\n\nConclusion\n\nSingle linkage clustering involves several key steps. Initially, after visualizing the data and calculating a distance matrix, clusters are formed based on the shortest distances. Once each cluster is formed, the algorithm updates the distance matrix to incorporate new distances. Throughout this iterative process, all data points are clustered until revealing patterns in the data. Therefore, it’s a simple, intuitive method that can uncover hidden structures in the data.\n\nKey Takeaways:\n\nFirstly, LDA maximizes between-class scatter while minimizing within-class scatter\n\nMoreover, it assumes Gaussian distribution and identical covariance matrices for classes\n\nAdditionally, LDA can be extended for multi-class problems and addresses some limitations of logistic regression\n\nFinally, regularization techniques help overcome LDA’s small sample size problem\n\nThe media shown in this article are not owned by Analytics Vidhya and are used at the Author’s discretion.\n\nFrequently Asked Questions"
    }
}
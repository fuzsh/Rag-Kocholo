{
    "id": "dbpedia_5266_3",
    "rank": 10,
    "data": {
        "url": "https://meta.stackoverflow.com/questions/425525/conclusions-from-title-drafting-and-question-content-assistance-experiments-cond",
        "read_more_link": "",
        "language": "en",
        "title": "Conclusions from title-drafting and question-content assistance experiments conducted by Stack Exchange",
        "top_image": "https://meta.stackoverflow.com/Content/Sites/stackoverflowmeta/Img/apple-touch-icon@2.png?v=6de7587d1583",
        "meta_img": "https://meta.stackoverflow.com/Content/Sites/stackoverflowmeta/Img/apple-touch-icon@2.png?v=6de7587d1583",
        "images": [
            "https://i.sstatic.net/WyDMc.png?s=64",
            "https://i.sstatic.net/O3tnc.jpg?s=64",
            "https://i.sstatic.net/BGA1x.jpg?s=64",
            "https://www.gravatar.com/avatar/edad7f3ab41c3bdfdf851759dce5c33b?s=64&d=identicon&r=PG",
            "https://i.sstatic.net/L9NB3.png?s=64",
            "https://i.sstatic.net/mYIzP.png?s=64",
            "https://www.gravatar.com/avatar/f63373f9fc9e1d07ca312c4aef722834?s=64&d=identicon&r=PG&f=y&so-version=2",
            "https://www.gravatar.com/avatar/cc2372591e00f2ab504b59b4f7cbed43?s=64&d=identicon&r=PG",
            "https://meta.stackoverflow.com/posts/425525/ivc/5ca9?prg=2fa5786c-afd6-4489-a33c-5cbdfd098c42"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2023-07-11T16:33:16",
        "summary": "",
        "meta_description": "We’d like to share some results from our experimentation with two earlier releases: the first, a tool to generate titles for your post, and the second, a tool to help users format their questions’ ...",
        "meta_lang": "en",
        "meta_favicon": "https://meta.stackoverflow.com/Content/Sites/stackoverflowmeta/Img/favicon.ico?v=59758cd6555b",
        "meta_site_name": "Meta Stack Overflow",
        "canonical_link": "https://meta.stackoverflow.com/questions/425525/conclusions-from-title-drafting-and-question-content-assistance-experiments-cond",
        "text": "We’d like to share some results from our experimentation with two earlier releases: the first, a tool to generate titles for your post, and the second, a tool to help users format their questions’ bodies. We’re sharing both sets of results at the same time because we think the insights they offer are similar to each other, and taken together they can better communicate the way we’re thinking about proceeding from here.\n\nNot to bury the lede: We won’t be proceeding to release these features on the platform in their current iteration. By the numbers, these features did seem to improve the experience of the people who used them; however, they didn’t present a sufficient benefit to on-site outcomes to justify releasing them in their current forms. The content formatting assistant also experienced some issues during its launch, such as hallucinations, and inadvertently fixing the bugs with users’ code while editing their questions, which were not desired behaviors. We may still look to test incorporating features like these into the site at a later point since the users who used these tools did appreciate them. However, before we do so, we’ll have to take more time to develop and test appropriate safeguards, and flesh out exactly how we hope users will use these tools on the platform.\n\nWith that said, let’s start with the title-drafting assistant’s results. We tested four variants over the course of this experiment:\n\nThe ask page as it currently is, with no changes.\n\nThe ask page as it currently is, except we move the ‘title’ field to the bottom of the question page, after a user writes their question body.\n\nThe ask page with title-drafting assistance.\n\nThe ask page with title-drafting assistance, except the ‘title’ field is moved to a step after the question body is written, similar to variant #2.\n\nFor more information about the variants, you can check out the original post on the topic.\n\nWe measured a few key values from this experiment, which I’ll summarize briefly.\n\nThe ‘question completion rate’ represents the percent of the time a user begins to write a question, and then actually posts one. While it’s hard to tell what the maximum reasonable value here is, we hoped that the question completion rate would see a slight boost, or at least be neutrally affected, by AI assistance.\n\nThe ‘time to question completion’ represents the amount of time a user spends drafting a question before posting it. It takes users a considerable amount of time to write a question on Stack Overflow – an average of around 11 minutes. (The distribution is long-tailed, with some users taking significant amounts of time to write questions, but the vast majority of questions come in close to 11 minutes.)\n\nThe ‘question success rate’ represents the percent of the time a question receives one of the following: 2+ question score, 2+ total answer score, an accepted answer, or at least two non-deleted answers. While there are other ways to define question success, we generally accept this definition as representing the improvement or degradation in the question asking experience on the site.\n\nEdits to the question title – this one’s fairly straightforward.\n\nThe ‘satisfaction rate’ is the percentage of people who gave positive feedback for the suggested title on the ask page. (Users had the option of giving ‘thumbs up’ or ‘thumbs down’ feedback.)\n\nWithout further ado…\n\nBaseline (#1) Variant #2 Variant #3 Variant #4 Question completion rate 48% 47% 46% 48% Time to complete question 10 mins 36 secs 10 mins 30 secs 10 mins 12 secs 10 mins 36 secs Question success rate 22% 23% 23% 23% Edits to the question title 10% 10% 8% 7% Satisfaction rate n/a n/a 88% 94%\n\nSo, here’s our subjective evaluation. Moving the title to the bottom of the question page by itself doesn’t seem to have an impact on the questions’ success rates. However, generating AI-assisted titles one step after the user writes their question body is better than leaving the title in the position it’s currently in. Otherwise, we do see a small reduction in the amount of time it takes users to write their questions, but it’s not by an overwhelming amount. Question success has an analogous story: title-drafting assistance didn’t seem to harm the user experience surrounding question asking, but neither was the improvement significant enough for us to pursue further as-is in this iteration.\n\nWe ran essentially the same measures for the content formatting experiment. However, the test variants were naturally different. We tested both on the Ask Wizard, and on our normal ask page (“AskV2”).\n\nThe Ask Wizard page as it currently is, with no changes.\n\nThe Ask Wizard page with AI support, and with the question review step removed.\n\nThe Ask Wizard page without AI support, but with the question review step removed.\n\nThe AskV2 page as it currently is, with no changes.\n\nThe AskV2 page with AI draft-editing assistance.\n\nBefore you read too much into this table, please keep in mind – due to issues with the experiment, it was only on for a short time. Normally we’d want to leave an experiment like this running for at least a week or two, in order to capture a better understanding of what normal user activity should look like. So I’d recommend tempering expectations a bit for how strongly these results can be interpreted.\n\nAsk Wizard - Baseline Ask Wizard Var. 2 Ask Wizard Var. 3 AskV2 - Baseline AskV2 - AI Question completion rate 46% 51% 49% 70% 69% Question success rate 15% 16% 18% 28% 29% Edits to the question body 46% 36% 46% 30% 36% AI suggestion acceptance 83% 83% 69%\n\nThere was a noticeable improvement in question success rate for newer users who were using the Ask Wizard, though not an overwhelming one. For users who are more experienced, we saw no meaningful change in question completion rate. Newer users also saw the most benefit from the AI content formatter, though again the margin here is very small. The most significant impact seems to be on the question body edit rate, which - for new users only - seems to have fallen around 10%. Though, the degree to which this is due to user engagement differences on these questions, or a genuine reduction in need for edits, was not directly tested.\n\nOverall, newer users liked the content formatter’s suggestions significantly more than established ones. This isn’t too surprising, as new users are often the ones who need the most help massaging their questions to form.\n\nSo, where are we going from here? Well, both experiments have concluded, and we’re not planning to release these as features to the site as-is.\n\nOn the positive side, the results we see here show that these features are at least safe for us to play around with, which is a good baseline result – we don’t expect further experimentation with these features to have immediate, catastrophic effects. This means that there’s room for us to play around with design, implementation, and workflows without risking severe effects in production. (Yes, it is clear to us that if we want to publish final features, we need better safeguards to proceed.)\n\nOn the downside, these results aren’t as good as we were hoping for. While there does seem to be a reduction in curator workload as a result of these features, we were hoping that they would have more of an impact on the questions’ overall success rates, and therefore give a greater proportion of the users who come to our sites a good experience. While folks did overwhelmingly appreciate having AI suggestions on hand (even experienced askers liked it on the balance), this alone doesn’t necessarily make the overall user experience better if it doesn’t lead to better on-site outcomes.\n\nThe net result is that we’re going to shelve the public release of these features for a bit while we work on better proposals for how to integrate it into user workflows. And, if we believe our ideas are promising, then as usual, we’ll come back here to share more details and ask for your feedback.\n\nI was really hoping to see this study produce some sort of metric regarding the accuracy of the title generator. Here's a suggestion for how to evaluate title quality for future experiments.\n\nPresent the user with the standard \"Ask Question\" UI that requires the user to manually input a title. Have your title generator create a set of titles. Do not display these anywhere on the page, but attach them to the question in a hidden field. After the experiment is over, compare the user-supplied title against the bot-generated titles. This doesn't alter the UI in any visible or functional way, so you could collect this data for as long as you want without any impact on the user experience.\n\nYou can measure success in several ways. I imagined something like the current review queue, where experienced users can click through a sequence of questions. The page could display the bot-generated titles and ask the user to rate how similar each one is to the human-supplied title. Or, the page could display the question plus the bot- and human-generated titles and ask the user which one fits the question best. Those are unfortunately manual processes but since the expected consumer of the output will be a human, I don't see any way to measure quality without a human in the loop.\n\nFor best results, limit this test to existing users whose site history suggests that they are likely to write an accurate, high-quality title. You could even run this test against existing questions, which might give you the best apples-to-apples comparison. Find questions whose title was edited by someone other than the author, then run the title generator on them. Compare the original title against both the bot-generated titles and the human-edited title. Were the bot suggestions as good as the human editor?\n\nThe ‘question completion rate’ represents the percent of the time a user begins to write a question, and then actually posts one.\n\nI'll caution you against paying too much attention to this particular metric. I can't tell you how many times I've solved a problem simply by trying to explain it to somebody. It forces you to think about the problem from a different angle, and is often enough to clear up the misunderstanding that was causing the problem. People sometimes call this rubber duck debugging.\n\nWhen a user starts writing a question but doesn't post it, a lot of times it's because they answered their own question in the process. That should be counted as a success, since what they came here for was an answer. My own personal question completion rate on SO is probably under 15% for this very reason.\n\nAn automatic question drafting assistant would mean that the question asker did less of the work. That means less opportunity for the rubber-duck effect to solve the problem. For that reason, I'd expect the bot-assisted ask wizard to see an overall increase in the number of questions that end up surviving and getting posted.\n\nThis metric also appears to ignore cases where (for example) the asker viewed one of the \"similar questions\" that the wizard displays, finds their answer there, and subsequently abandons the question. Those questions should be filtered out of the data set since the goal of that feature is to eliminate the need to post a question in the first place.\n\nWhat you're attempting to measure here is a lot more complicated than your current model appears to take into account, and there are a number of aspects which can't really be measured (e.g., number of people who answered their own question prior to posting). For those reasons, I suggest not using this metric in the future.\n\nanswered Jul 13, 2023 at 1:21\n\nbtabta\n\n44.8k1818 silver badges1818 bronze badges"
    }
}
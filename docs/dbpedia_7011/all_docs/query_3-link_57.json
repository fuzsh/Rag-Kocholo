{
    "id": "dbpedia_7011_3",
    "rank": 57,
    "data": {
        "url": "https://www.linkedin.com/pulse/capturing-biases-age-ai-interview-founder-wikipedia-zhavoronkov",
        "read_more_link": "",
        "language": "en",
        "title": "Capturing Biases In the Age Of AI - The Interview With The Founder Of Wikipedia Founder Jimmy Wales",
        "top_image": "https://media.licdn.com/dms/image/D5612AQEm5qAdPxxbWQ/article-cover_image-shrink_720_1280/0/1671613285936?e=2147483647&v=beta&t=72h79hTDfgJOXNqjfO2P9SjXCh3HLYwvt5x7GTHFN68",
        "meta_img": "https://media.licdn.com/dms/image/D5612AQEm5qAdPxxbWQ/article-cover_image-shrink_720_1280/0/1671613285936?e=2147483647&v=beta&t=72h79hTDfgJOXNqjfO2P9SjXCh3HLYwvt5x7GTHFN68",
        "images": [
            "https://media.licdn.com/dms/image/D5612AQEm5qAdPxxbWQ/article-cover_image-shrink_720_1280/0/1671613285936?e=2147483647&v=beta&t=72h79hTDfgJOXNqjfO2P9SjXCh3HLYwvt5x7GTHFN68"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Alex Zhavoronkov"
        ],
        "publish_date": "2022-12-21T09:19:30+00:00",
        "summary": "",
        "meta_description": "Since its birth in 2001 Wikipedia has become an integral part of peoples’ lives with pretty much every Google query drawing from this massive volunteer-built and run online encyclopedia. In 2020 Wikimedia Foundation, the non-profit organization behind the platform employs around 300 people with the",
        "meta_lang": "en",
        "meta_favicon": "https://static.licdn.com/aero-v1/sc/h/al2o9zrvru7aqj8e1x2rzsrca",
        "meta_site_name": "",
        "canonical_link": "https://www.linkedin.com/pulse/capturing-biases-age-ai-interview-founder-wikipedia-zhavoronkov",
        "text": "Since its birth in 2001 Wikipedia has become an integral part of peoples’ lives with pretty much every Google query drawing from this massive volunteer-built and run online encyclopedia. In 2020 Wikimedia Foundation, the non-profit organization behind the platform employs around 300 people with the annual revenues predominantly coming from voluntary donations. Meanwhile the content is provided by over 41 million Wikipedia accounts of which only about 143 thousand or about 0.34% are actively contributing. While pretty much everyone who goes online in the geographies where it is not banned knows of this platform, few people realize the growing importance of its engine being due to artificial intelligence (AI).\n\nPretty much every AI company, including the organizations I manage or advise, is using Wikipedia in one way or another. It is used in full or in part by most of the major AI systems employing natural language processing (NLP) techniques and advanced transformer neural networks.\n\nWhen an AI is being trained on human input, it inherently captures human biases as we exposed in AI beauty contests - where AIs trained on databases of actors or models discriminated by race, age, and other features. Some AI’s though can capture misinformation and sentiment to the extent where it can really hurt as in the example of the Microsoft’s early chatbot “Tay”.\n\nSo essentially, if there are biases or inaccuracies in the world’s largest encyclopedia, these will be captured AIs. To find out what Wikipedia is doing to stay balanced, neutral, and accurate and what it can be doing better I decided to interview one of the most impactful humans alive who created and made Wikipedia free, the founder of Wikipedia, Jimmy Wales.\n\nWe first corresponded with “Jimbo”, Jimmy’s online sobriquet, when I was developing the International Aging Research Portfolio (IARP), and then connected in person at the very tightly-curated event for technology influencers called the Founder’s Forum at The Grove. Keeping a low-profile but shaking hands with the royals and power brokers, Jimbo gave a powerful talk on the need for free speech, balanced views, and the need for more diversity, and less biases.\n\nJimmy Wales at the Wikimania conference\n\nAlex: There are many stories about how you started Wikipedia during the dotcom revolution and how you turned that into the largest nonprofit educational resource on the planet. But it would be great to learn more about what you are doing today, how big of a role you're playing in Wikipedia, and what you are doing to take it to the next level?\n\nJimmy Wales: Yeah, sure. So, I'm on the board of directors and I play an active role there, and I'm active in the community. My role in the community is really mainly to talk to people about our core principles and values. I stay at a very high level I don't get involved in day-to-day editing matters very much at all. I'm not an employee of the Wikimedia Foundation, and I've never been an employee of the Wikimedia Foundation. I like to sort of maintain my certain position of independence within things. I consider myself a representative of that community.\n\nAlex: I’m happy to hear that you are still playing an active role in the community. Nowadays, one of the most pressing topics that is interesting to everybody is, in this interconnected world is the issue with the validity and quality of information. Both Facebook and Google and many other social networks and Twitter have been scrutinized for this. Of course, Wikipedia as a major source of all news in the world on all information is probably subject to that too. I'm just wondering if there is any kind of policy that the government, or the regulators are passing in that context to ensure that it's more regulated.\n\nJimmy Wales: I haven't seen much movement towards regulation in that context. I think regulation of something like neutrality, lack of bias or quality is very, very difficult. It's probably dangerous. Because of course, for many governments around the world their definition of quality information is information which praises them, and bad information is information which criticizes them. And so that is not really an approach that I would be excited about.\n\nFor us within Wikipedia we've always had what we call the “neutral point of view” policy, which is that Wikipedia shouldn't take a stand on any controversial issue but should present all the sides fairly. And that works reasonably well, but there are some interesting quirks and interesting side-effects. So, one of the things that we look for at Wikipedia is, high quality, reliable sources. But of course, if we have a bias within society, within the broad range of society then it's very difficult to avoid that seeping into Wikipedia, even if we're struggling against it, simply because we need reliable sources. And so, if the reliable sources are problematic, then that passes through to us. So that's an interesting question one which I think in certain areas, the areas where people are probably the most concerned about bias is probably not as bad as people think. This is because you can find very high-quality sources, debating the pros and cons of just about any major political issue. So the idea that even if you think for example, the liberal media, whatever, blah, blah, blah. The truth is that any particular potential policy you've got, the financial newspapers are different from the more socially liberal papers and so on.\n\nIt’s hard though, and it’s a complicated question. Historically there are things that women for example, may have been doing very good, solid, scientific work, but for old-fashioned sexist reasons, weren't awarded the top professorships, weren't given full credit for their work and so on. Which means they were not written into history. That's really problematic and that's a hard problem that we should be thinking a lot about. We can't fully solve that ourselves, we can't go back and fix injustices in the past. But we can be very mindful that, okay, the set of sources that we're using comes with a certain historical context and baggage, and we need to cope with that. The last thing that I want to say about this issue is of course, we have to examine ourselves and our community for our own biases, even though we do have this very strong ideal around neutrality, around fairness around inclusion. We have to realize that the Wikipedia community isn't as diverse as it probably could be and should be. Then we write about what we're interested in. So even if you put together 100 supercomputer geeks, who have every good intention in the world of being inclusive - the truth is they share a lot of common interests, they know a lot about the same things and yet they wouldn't necessarily recognize certain levels of bias. I mean, you can think of this simply as, young men in their twenties don't really know much, or think much, or care much about early childhood education, as an example. At least maybe some of them do, I don't mean to be too simplistic about it. But on average, they don't nearly as much as people who are older, as people who are mothers versus fathers and so on and so forth. And so, we have to recognize that. Say we need a more inclusive community; we need a broader community ourselves because all the good intentions in the world aren't going to make me know about things that other people in society know about. That's not really what it's about.\n\nAlex: That's very important because it actually relates to my next question. Nowadays, we are entering the age of AI, and many AI companies out there that are using Wikipedia for training sophisticated AI systems. These systems do acquire human biases that are very difficult to see, especially in very niche specialist areas like, biology, chemistry, and medicine. In some of those areas where it's impossible to have a generalist properly assess the situation. I'm just wondering what are you doing in this area? and whether currently Wikipedia management and maybe whether you are working with third parties who are developing any kind of transformer-based tools that can help you actually validate some of Wikipedia content to see if it leads to those biases or not.\n\nJimmy Wales: Yes. I think we have a generalized academic interest in the question but compared to the organizations who are pursuing AI research at scale, we're a tiny organization and so we don't imagine that we're going to be able to work effectively in that area - other than as a sort of a side research interest and things like that. In terms of us using AI, like right now it's very minimal. We have a tool that through machine learning looks at incoming edits to try to identify problematic edits. But that's in a very, very crude and simple fashion. It's pretty good at identifying things like, someone has replaced the entire article with one-word, deep learning tells you that's going to get reverted very quickly.\n\nIt's not really that amazing and I think not just us, but I think the world is still quite far (away from achieving this), other than a fairly crude kind of indicative type of technique. To read a text about Marie Curie, for example, one of the few famous female scientists of her era and give us feedback on bias there. Because the bias in an entry like that is so subtle, if it exists at all that it would be very, very difficult to say how to assess it. Or for example, if you say, take a crude look at the number of Wikipedia entries about women scientists in the 1700s, it's very tiny. But that's not a problem of bias in Wikipedia, it is that women were excluded from those professions at that time. We can't go back and create scientists in the past. So having AI to help with those kinds of issues I think is incredibly difficult. Because to really understand a particular situation or to look at a situation, and to actually understand is this an example of a bias or not involves a huge contextual judgment with a context of knowledge that's far broader than a GPT-type of very interesting stuff that generates quite impressive superficial texts, for example. But in terms of having a really deep understanding of issues is quite hard at the same time - so that's my skepticism.\n\nAt the same time, I have often thought that it would be very useful, even if it was an imperfect tool, one that could only find the worst cases as a starting point or that could only give you a score that's often wrong, but it's more right than wrong. That could be kind of interesting to say if you're interested in working on issues relating to bias, we've got this machine tool which is run over the entire Corpus of Wikipedia and has identified these scores and these things. It doesn't have to be perfect. It isn't like the AI can now fix Wikipedia. It's more like, okay, help us with the part that’s hard for humans. And the part that is hard for humans is not understanding the bias; it's not thinking through the implications, that's what only humans can do - really understanding that deep contextual knowledge. But what's hard for humans is reading all of Wikipedia, it's enormous. Where do you begin? I could pick and choose, but I'm going to pick and choose as well in a way of potential bias.\n\nAlex: You mentioned that Wikipedia is a tiny organization, and it's very difficult to invest in AI yourself. You know that Google is providing Wikipedia output in its search, in a very structured matter and is also utilizing AI techniques. You probably remember the kind of cusp of the internet era together with Yahoo and Google and the open directory project, DMOZ, and where Google was doing pretty much the same thing. So, they were putting out links in the context of, DMOZ. There were hierarchies of editors within the DMOZ ecosystem. It was a very interesting functional project. It probably would be quite useful nowadays as well. But then it just disappeared and now it's part of Google.\n\nSo, two questions here, first of all, what went wrong with DMOZ? Second, do you foresee anything like that happening with Wikipedia? Because Google has such a powerful engine and without it, it would be very difficult for Wikipedia to exist. So, if they were to integrate similar tools the traffic would not be there.\n\nJimmy Wales: Yeah. So, there are two questions. One is about, DMOZ. DMOZ, I believe was actually bought by AOL.\n\nAlex: I'm just saying that after Google integrated it, it became not as relevant as it was before.\n\nJimmy Wales: DMOZ had a lot of problems. So one of the problems that they had was, well when they were bought by AOL, as you know, many, many, interesting things were bought by AOL and then died. But even before and after that they had an incentive structure problem. So, designing an open community that can defend itself against outside influences is something that nobody quite knew how to do back then, and even today is quite difficult. And there are many, many ways of getting it wrong. One of the problems that the open directory project had was a problem of fiefdoms, people controlling certain areas and you couldn't get permission to join. There were accusations that it was because there was an enormous amount of money to be made in impacting where traffic flowed on the Internet.\n\nSo to have an open volunteer community that actually has that much power over the flow of traffic, when you're going to be besieged by people who don’t have the best interest of the world and the internet at heart, but just getting traffic for themselves, it's quite a hard problem. I'm not saying it's impossible, but I think it's quite a hard problem, which DMOZ was never able to solve, and which Google was able to solve in a way. If nothing else, they had a strong commercial interest in being seen to be a quality search engine where you couldn't simply pay them to get to the top of the rankings and so on and so forth. And obviously there's a lot to discuss and debate about the history of that. But in general, I would say that was what worked there.\n\nFor Wikipedia, we're not afraid of Google. I remember when Google came out with KNOL, there was this sort of thinking that it's the Wikipedia killer. But if you looked at the product for just a few minutes you realized very quickly that this was quite an interesting and cool and nicely done collaborative blog project. It didn't have the community design, I would say, to drive towards neutrality. In other words, it had, again, the problem of fiefdoms, it had the problem of no democratic control. So, it was quite clearly going to be something where one person would write, start their little KNOL, on some pseudo-scientific topic and invite others who believe the same thing and they would control it and that would be that, and it wasn't going to turn into a quality source of information.\n\nThe other thing is, I always joke that media is a terrible business, and what interest does anybody having in competing in that space? Everything is completely free, there are no ads, we only make money if people love us enough to give us money. We've got a strong community of volunteers, it’s kind of hard. Now who knows, if we're talking about AI and we're in sort of speculating about the future. One of the interesting questions is, will either Google or some upstart create something that can write something very much like a Wikipedia entry and do it very, very cheaply and therefore compete with us in a sense with a huge website, with tons of useful information?\n\nI'm a little bit skeptical, but I'm not saying that's never going to happen. I mean, the main reason I would say that I'm skeptical right now is of what we have seen today, although it's improved dramatically, it is still quite superficial. And again, to get to quality, one of the things that's really interesting about being a Wikipedian and talking to the Wikipedia editors is, that there is an incredible context of knowledge that's necessary to be able to explain a topic well, you have to understand your readers and what are they likely to know, you have to understand a lot of cultural nuances and baggage and so on. There's more than people realize and in fact, more than most people who are directly editing realized, because we are humans and so it's completely natural to us.\n\nThis is a different example, but a year or so ago, some researchers tricked a Tesla into speeding by putting a bit of tape on a sign and changed the 30. They didn't change the 30 to an 80, but they changed the 30 to something that looked closer to an eight. It was just enough to fool the Tesla. But you know, it's like, if I'm driving here in my neighborhood and I suddenly see a sign in my neighborhood that says speed limit 80 I wouldn't suddenly speed up. I'm like, that's weird I'm in a neighborhood, there's no way you could go 80 on this road. As a human you don't even think about things like that. AI will be able to, but at least for right now it cant. There are many, many sentences that you might put into a Wikipedia entry that a human would say, that's an odd thing to say and have a hard time articulating what's odd about it but humans could. So, I don't think where we're under any immediate danger from being replaced by AI or Google or anything like that.\n\nAlex: Thank you for that insight. I also don't think Wikipedia would be as popular without Google…\n\nJimmy Wales: Yes, one of the things you said, it would be hard for Wikipedia to exist without the traffic from Google. It will be hard for Google to exist if they didn't link to Wikipedia.\n\nAlex: If that is so, should they be contributing to the foundation quite a bit…Do they contribute to the foundation a lot?\n\nJimmy Wales: Some. They have donated in the past. It’s not something that we are super focused on. And one of the reasons is that we do value our independence. And so, we want to be very careful about our sources of money. And although I like Google and I know everybody at Google, and we've had a long-term relationship that's very healthy. I don't think it would be healthy if for example, 80% of our budget came from Google. Because then it becomes very, very difficult for us to act as an independent party. Whereas support from the general public means we can maintain our intellectual independence.\n\nAlex: For sure. And what motivates people to contribute to the Wikipedia? And of course, aside from altruistic motives, are there any motives that are not altruistic? And if you were to rank those into, altruistic, commercial or some others how would you do that?\n\nJimmy Wales: Yeah, so, I mean, I think for most people who edit Wikipedia, they do it because it's fun, they enjoy it. I mean, for me, the word altruism is quite a difficult word because it means you're doing something against your own interests. And I don't think many Wikipedians feel that they're suffering under a duty to help the world. They're just saying, this is something cool to do I enjoy it. By the way, it does improve the world and that's great too. It makes me feel happy. So, it's mostly about having fun. It's about meeting other interesting geeky people. If you spend your Thursday night editing Wikipedia, and you make a few changes here and there and you look something up and you correct an error and so on and so forth, you can go to bed thinking, yeah, that's all right that was fun. I enjoyed my evening. I had an interesting discussion or two. If you spend the same evening playing, Grand Theft Auto, well, okay maybe you had fun, but at the end when you go to bed and you think, gosh, that was a wasted night, I could have done something useful with my time.\n\nAlex: Well, that kind of relates to my other question I kind of thought about it right now. So, like many societies, there is a hierarchy of editors and some of the senior editors contribute for a long time. So, they create a network of friends, who can help them dominate in any dispute, especially when it comes to smaller issues, books, biographies, scientific research topics that can be easily overlooked by the community. I actually once experienced such abuse by one of the Wikipedia editors, but I never fought it. And how do you fight those networks that actually may spend a considerable amount of time, like sleeper agents, that contribute for a long time, participate in the community, but the long-term goal is to create a dramatic impact. Do you see anything like that?\n\nJimmy Wales: No. I think in general the phenomenon you describe is what I think makes Wikipedia great, that we have experienced editors that know each other. They share a certain set of values. They share an ethical framework for decision making, which is about quality neutrality, thoughtfulness, kindness, and so on. And obviously, for me it's not necessarily that we need to prevent that network because it's a very benevolent thing, but we need to make sure that it's healthy. So, we want to make sure that we don't have within ourselves, within our community that we don't get too kind of skeptical of newcomers. Don't bite the newbies, are we friendly? Are we opening to everyone? And sometimes people do have to step back and say, well, we aren't being unfriendly, but we have a lot of jargon for example. We have a lot of ways of doing things that we all understand because we've been here forever, but that newcomers find confusing and hard to understand. We need to do better at communicating what we're doing, and why to people. So, there is that element of it of really sort of thinking through.\n\nAnd then, I don't know, sleeper agents that's quite interesting. I've not seen that people do mention it from time to time. I can't say it's never happened that somebody is a fantastic Wikipedian and edits hundreds of different topics, but they're really there for that one thing that they want to change in the world and be biased on. We have people in the community who I don't think are sleeper agents, but who just, they're very emotional around one particular issue and they probably shouldn't be editing in that area. I always give the example of a biologist who did a fantastic and wonderful work about animals, and I think he was very good in marine biology. But anything around Israel and Palestine, he just was very difficult. It was an emotional issue for him, and he couldn't stay neutral and it was a problem. But I don't think that was because he was an agent or anything. I just think he had a personal passion for a certain topic that made it hard for him to be a Wikipedian.\n\nAlex: Well, maybe it's very difficult to see actually even large-scale networks like that now. My company is working on a system that can identify some of these biases. But we are developing an engine which ranks scientists, for internal purposes to predict the outcomes of clinical trials and establish the confidence level in experimental academic papers. To see how much you can trust them, and how impactful they are going to be in, let's say, 15 years when it comes to clinical trials and how their research is translating into drugs. This is a very easy way to access information. You look at the scientific papers, you link them all together, you train on those papers, you train on the clinical trials that they are associated with, you train on their network of co-authors and colleagues. There are multiple structured data sets.\n\nSo in theory it can be extended to Wikipedia to help monitor editors, and automatically identify kind of the bad agents. Is there a database of “bad editors” or editors you've discarded?\n\nJimmy Wales: Yeah. I mean, if you look at the edit history of Wikipedia, you can see, for example, if someone inserts or remove certain words, you can ask yourself the question for this person, for their edits, how long did their edit survive? And so, if you see someone whose edits don't survive very long versus you see someone who's edit do survived for a long time, that's probably to some extent an indicator of quality. But obviously there are many, many caveats and many other factors. So, for example, if you go into some fairly obscure area of knowledge that doesn't have very many people in Wikipedia and you're doing kind of, okay, but mediocre work, probably your words will survive. Whereas if you go to an area that has a number of very talented, very good editors, and you do some mediocre work, the media who work probably won't survive.\n\nSo, you've got to really kind of think through a lot of the broader things. But for me that sort of thing is the kind of thing that I think, some of the deep learning techniques ought to be able to handle reasonably well. And I think that's an interesting area of research to sort of think about, are there characteristics, are there areas of Wikipedia where it's easier to get started, for example. Are there areas where it's harder to get started? And I think I have some, a priori, anecdotal thoughts about where those things might be, but it would be interesting to see if I'm right or I'm wrong."
    }
}
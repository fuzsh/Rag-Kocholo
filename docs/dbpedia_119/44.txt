2.1 Statistical econophysics

Statistical econophysics comes from statistical physics and it is often associated with what we call ‘stylised facts’ in the economics. These stylised facts mainly refer to ‘empirical facts that arose in statistical studies of financial (or economic) time series and that seem to be persistent across various time periods, places, markets, assets etc.’ (Charkraboti et al. ([15], p. 994). Because this kind of econophysics is mainly based on a time analysis of financial or economic phenomena, it requires a lot of past data on prices, volumes, or transactions from which models describe phenomena such as, the fat-tailed empirical distributions of returns, the absence of autocorrelation of returns or volatility clustering. For statistical econophysics, economic systems are composed of multiple components (no learning agents) interacting in such a way as to generate the macro-properties for systems Citation[11]. These macro-properties can be characterised in terms of statistical regularitiesFootnote 10 . In opposition to economics (or agent-based econophysics), statistical econophysics considers that only the macro-level of the system can be observed and analyzedFootnote 11 . There is no modelling of the rational or/and individual behaviour like in the economic mainstream or in agent-based econophysics.

When they refer to agents, statistical econophysicists rather use a ‘zero intelligent’ agent in a sense defined by Gode and Sunder ([18], p. 124), i.e. an agent who ‘has not intelligence, does not seek or maximise profits and does not observe, remember or learn’. The agents’ behaviour is random and the result is then mathematically analogous to a reaction or diffusion model in physics. In a sense, a strictly statistical approach in econophysics could be summarised through the following chart (Figure ).

Statistical econophysics: towards a transdisciplinary analysis.

Statistical physics and economics/finance develop their own disciplinary conceptualisation of phenomena they are supposed to describe. Although this conceptualisation is directly embedded in a disciplinary matrix, it generates a collection of data which must be statistically described. The identification of universal statistical patterns (such as power laws, scaling properties, etc.), then justifies the application of concepts coming from statistical mechanics to economics and financial phenomena. Statistical econophysics can therefore be looked on as a transdisciplinary analysis Citation[19,20] based on the identification of universal statistical patterns (i.e. patterns observed in multiple distinct disciplinary contexts). Financial systems, for example, consist of a large number of components whose interactions generate observable properties such as power laws. Knowledge developed by this kind of model mainly results from analysis of past data that authors try to explain through complex statistical processes for which statistical physics could be very useful.

The main objective of statistical econophysics is to describe the past financial and economic data through the identification of macro-statistical properties implying a phenomenon of emergence. In this perspective, models used in statistical econophysics are phenomenally adequateFootnote 12 since they capture the main characteristics of the phenomena studied. The last section of this paper will present an overview of the main papers related to this statistical econophysics.

3.1 New treatment of data or the emergence of new statistical tools

For more than five decades, leptokurticityFootnote 16 has generated a lot of debate in finance. In the 1960s, some theoreticians tried to integrate the leptokurtic nature of distributions into mathematical models used in finance. Mandelbrot Citation[30], Samuelson Citation[31] and Fama Citation[32,33], directly inspired by Lévy Citation[34] and Gnedenko and Kolmogorov Citation[35], proposed to describe the evolution of financial returns with a specific case of stable Lévy process (a Pareto’s lawFootnote 17 ).

In the 1920s, Lévy demonstrated that the invariance of the distribution form with respect to the addition of independent variables was not specific to the Gaussian distribution. Lévy’s work led to the identification of specific stable distributions, which appear as a generalisation of three well-known distributions: Gaussian, CauchyFootnote 18 and LévyFootnote 19 . They are the only three stable distributions with a closed formula for the probability density function. For a long time, this absence of a closed formula defining the probability density had been the main problem with the use of stable Lévy distributions. This problem is now solved since density functions are estimated with specific computer programs.

Although stable distributions are a specific type of Lévy process, they appear as fixed points of a convolution operationFootnote 20 . In other words, there are asymptotic attractors for a distribution convoluted with itself a large number of times since it converges towards a stable lawFootnote 21 .

Stable Lévy processes are also infinitely divisible random processes, and they have the property of scaling (self-similarity), meaning that financial variables (daily, weekly and monthly) can be studied through a stable distribution of exactly the same form for each level of scale. Since there is no closed form formula for densities, a stable Lévy distribution is often described by its characteristic function , whose most popular form is givenFootnote 22 by

(1)

Stable Lévy movements are processes whose accretions are independent and stationary and follow an α-stable law of the type for which it is possible to observe constancy of the parameter α. For the class of stable processes, this parameter α (called the characteristic exponent) must be between 0 and 2, as shown in Figure .

Probability density function of stable distribution. We can observe that Gaussian distribution is a specific stable distribution (with α = 2).

The value of this exponent refers to the shape of the distribution: the lower α is, the more often extreme events are observed. Financial economists tried, in the 1960s, to describe the evolution of financial returns with other stable distributions (0 α 2) because their scaling properties were considered as very useful in finance (since they imply a potential scale-free analysis of variables, see Mandelbrot Citation[30]). However, although stable Lévy processes provide a better description of the evolution of financial markets, the authors were faced with a specific theoretical problem. Indeed, stable Lévy processes (with a characteristic exponent < 2) generate an indeterminate second statistical moment meaning that the variance can vary considerably depending on the size of the sample and the observation scale. Consequently, the variance does not tend towards a limiting value and it is said, therefore, to be infinite Footnote 23 . Although Fama Citation[33] gave a mathematical reinterpretation (in Paretian terms) of existing financial theory, he was unable to provide an economic interpretation to his article because variance (usually associated with risk in finance) was infinite Citation[33]. Many researchers in economics considered the infinite-variance hypothesis unacceptable because it is meaningless in the financial economics framework (in which financial risk is directly associated with variance).

In addition to these difficulties, theoreticians, at that time, had to face the absence of computational definition for evaluating parameters of stable Lévy processes. Fama Citation[33] himself regretted this point by explaining that the next step in the acceptability of these processes in financial economics would be ‘to develop more adequate statistical tools for dealing with stable Paretian distributions’ ([36], p. 429). These difficulties explain why very few economists followed the path opened by Fama and Mandelbrot towards using stable Lévy processesFootnote 24 .

According to Hughes Citation[37], power laws and scaling properties appeared in physics at the same period as Kolmogorov’s research Citation[38] on turbulence and have progressively become widespread in the discipline. However, as Stanley Citation[39] explained it, there were no physically based justifications, at that time, for the existence of scaling laws. More precisely, Stanley ([39], p. 18) wrote, ‘the scaling hypothesis is at best unproved and indeed, to some workers, represents an ad hoc assumption entirely devoid of physical content’Footnote 25 . Hughes Citation[37] explained that throughout the 1970s and the 1980s, physicists found theoretical and experimental evidence supporting the scaling laws and the existence of power laws in a lot of physical phenomena. Such evidence has increasingly been observed and published starting from the 1990s (see Dubkov et al. Citation[40]): chaotic dynamics of complex systems Citation[41,42]; front dynamics in reaction-diffusion systems Citation[43], fractional diffusion Citation[44], thermodynamics of anomalous diffusion Citation[45], dynamical foundation on non-canonical equilibrium Citation[46], quantum fractional kinetics Citation[47], diffusion by flows in porous media Citation[48], noise amplifications Citation[49], kinetic Ising and spherical models Citation[50].

This high degree of interest for stable Lévy processes went beyond the frontiers of physics since we observed an increasing number of papers proposing stable Lévy processes as the most appropriate statistical description for a great number of distinct phenomena such as: seismic series and earthquakes Citation[51], time series statistical analysis of DNA Citation[52], primary sequences of protein like copolymers Citation[53], spreading of epidemic processes Citation[54], flights of an albatross Citation[55], human memory retrieval Citation[56] Footnote 26 . As Dubkov et al. ([40], p. 2650) mentioned,

‘it is astonishing how the same diffusion equation can describe the behaviour of neutrons in a nuclear reactor, the light in the atmosphere, the stock markets values rate on financial exchange, particles of flower dust suspended in a fluid and so on. The fact that completely different by nature phenomena are described by identical equations is a direct indication that the matter concerns not the concrete mechanism of the phenomenon but rather the same common quality of whole class of similar phenomena’.

Econophysics is totally in line with this assumption of a common quality which would allow one to extract a specific pattern from complex details in non-physical systems that are the financial/economic phenomena Citation[29,57].

Coupled with growing literature on stable Lévy processes, the old issue of the physically plausible dimension of stable processes re-emerged. Gupta and Campanha Citation[58,59] rejected the applicability in physics of these processes claiming that they have ‘mathematical properties that discourage a physical approach because they have infinite variance’ ([59], p. 32). This opposition between theoretical properties and the empirical applicability of these processes, is clarified by Mantegna and Stanley ([29], p. 4) as follows:

‘Stochastic processes with infinite variance, although well-defined mathematically, are extremely difficult to use and, moreover, raise fundamental questions when applied to real systems. For example, in physical systems, the second moment is often related to the system temperature, so infinite variance implies an infinite temperature’.

Consequently, in the 1990s, physicists seemed to be facing the same problematic contradictions as financial economists in the 1960s: on the one hand, there was growing empirical evidence supporting power laws and, on the other hand, a problematic theoretical feature of these processes (the infinite character of varianceFootnote 27 ). Some physicists Citation[57,60] resolved this contradiction by introducing a new statistical technique: the truncation of a stable distribution consisting in normalising it using a particular function so that its variance becomes finiteFootnote 28 . In this perspective, stable Lévy distributions are used with the specific condition that there is a cut-off length for the price variations, above which the distribution function behaves as a Gaussian distribution. Generally, this truncation operation can be rendered as follows: , where Pα (x) designates the probability distribution in its Lévy form and is a truncation function allowing finite variance to be obtained. This truncation function can take a number of formsFootnote 29 . These functions are chosen in order to obtain the best fit to the empirical data or they are derived from models such as percolation theory or the generalised Fokker–Plank equation Citation[29]. The idea is to combine a statistical distribution factor and a cut-off after a certain step size which may be due to the limited sample under study. Gupta and Campanha Citation[58] generalised this approach with a generic probability distribution where (l) is the cut off at which the distribution begins to deviate from Lévy distribution. This deviation is associated with a function depending on time and the cut-off parameter (l). For small value of x, P(x) takes a value very close to the one expected for a stable Lévy process. But for large values of n, P(x) will tend to the value predicted for a Gaussian process. In this framework, the probability of taking a step size (x) at any time is defined by

(2)

If we consider an abrupt truncation, the process will be truncated with a constant k meaning that the truncation process can then be characterised by

(3)

In other words, if x is not too large, a truncated Lévy distribution behaves very much like a stable Lévy distribution since most of the value expected for x falls in the Lévy-like region. When x is beyond the crossover value, the variable x abruptly converges towards a Gaussian distribution. This truncation results from the central limit theorem, according to which there is a competition between normal distribution and power law distribution for very large samples. In an abrupt truncation, the central limit theorem can be applied for the asymptotic case but a power law distribution is kept for finite samples (before the cross value where the distribution switches to a Gaussian convergence), where power laws never really disappear because even on a very large sample, extreme events can appear (maybe more rarely but with a higher amplitude).

The idea is to reduce the fat-tails of the stable Lévy distribution without deforming the central part of the distribution in order to decompose it into a stable part for the short and mid-term and a Gaussian part for the long termFootnote 30 , as shown in Figure .

Empirical probability density function for high-frequency price differences of the Xerox stock traded in the New York Stock Exchange during the two-year period 1994 and 1995. This is a semi-logarithmic scaled plot of the probability distributions of the progressive truncated Lévy Process characterised by a α = 1.5. We can observe that for low values of n (red diamonds) the central part of the distributions is better described by the Lévy stable regime in solid line. The larger the values of n are, the more Gaussian the process become (see intermediated values of n, circles). For very large values of n (triangles) the truncated process has already switched into a Gaussian regime (dotted line) (adapted from Citation[29]).

The key question is to know ‘when’ and ‘how’ the process will switch from a stable to a Gaussian regime. This switch is assumed to occur for a large number of data while the question of ‘how’ refers to autocorrelations which act as a friction causing the ultraslow convergence to the Gaussian regime Citation[61].

The first truncation of stable Lévy processes was introduced by Mantegna Citation[57] who developed an abrupt truncation technique setting the distribution function abruptly to zero. However, the physical meaning of this kind of truncation stayed a very important issue and it generated debates in physics. According to Matsushita et al. Citation[62] or Gupta and Campanha Citation[59] the truncation techniques must be physically plausible as the latter wrote,

‘The dynamical properties of a complex physical or social system depend on the dynamical evolution of a large number of non-linear coupled subsystems. Thus, it is expected that in general, the probability of taking a step [a variation] should decrease gradually and not abruptly, in a complex way with step size due to limited physical capacity’ ([59], p. 232).

Physicists therefore had a good reasonFootnote 31 to go beyond abrupt truncated stable Lévy processes and to develop more complex truncation techniques such as gradual truncation Citation[59], exponential truncation Citation[62], damped truncation Citation[63].

The operation of truncating stable Lévy processes allowed physicists to use these processes to characterise physical phenomena without having a problem of indeterminate variance. The development of these truncation techniques also contributed to the emergence of econophysics, since it was then possible to use stable Lévy processes in the statistical description of financial markets. Originally, econophysics can be seen as the statistical contribution of physicists to a very old issue observed in financial economics.

4.1 Empirical facts and statistical econophysics

Statistical econophysics mainly refers to empirical studies revealing the statistical properties of financial time series whose analysis ‘has been of great interest, not only to practitioners (an empirical discipline) but also to theoreticians for making inferences and predictions’ ([15], p. 994). This strictly statistical approach is often associated with what we call, in financial economics, ‘stylised facts’ which are unexpected statistical macro-patterns observed in so many different situations that they appear to have an empirical truth. In other words, statistical econophysics deals with empirical facts which cannot be explained in terms of the theoretical mainstream dominating financial economicsFootnote 37 . For the 1960s, we observe an accumulation of empirical contradictions with the financial mainstream and the emergence of econophysics opened the way to the clarification of some of them.

The fat-tailed dimension of financial distribution is the oldestFootnote 38 stylised fact and probably the one at the origin of econophysics. As mentioned earlier, econophysics emerged from the first attempts of some physicists Citation[29,69] to describe financial distribution with statistical frameworks usually used in physics. Since the beginning of the 1990s, a great number of econophysical studies developed specific statistical frameworks such as gradually truncation Citation[58], exponentially truncation Citation[62], and damped truncation Citation[63] in order to capture the leptokurticity observed in the empirical distributions. These statistical solutions developed by econophysicists have been presented in the previous section.

Another stylised fact studied by the econophysicists refers to the importance of correlations and autocorrelations between variables such as stock prices, returns, order signs, transaction volumes, etc. The Gaussian framework used in the financial mainstream implies an absence of autocorrelation for independent and identically distributed financial returns meaning that their statistical features do not depend on the time scale. However, empirical data show that statistical properties of distributions change with time. The most well-known changing characteristic is the clustering volatility which refers to the fact that ‘large changes tend to be followed by large changes of either sign and small changes tend to be followed by small changes’ ([15], p. 994]). More precisely, absolute returns and squared returns appear to have a long-range decaying autocorrelation function. Some physicists Citation[70,71] tried to describe this clustering phenomenon by using specific power laws and stable Lévy processes. Because the complex phenomena require an analysis of all statistical links between elements, the econophysics literature has contributed to this specific issue related to the estimation of the correlations and covariance matrix. Laloux et al. Citation[72] showed that only a few eigenvalues survive above the noise bands while several methods have been proposed for identification of the non-random elements of the correlation matrix Citation[73,74]. Of course, results coming from the original correlation matrix differ strongly from those generated from the usual correlation matrix used in finance developed by MarkowitzFootnote 39 in 1952 Citation[75]. As Cizeau et al. ([70], p. 443) explained it, ‘standard covariance matrix estimates might vastly overstate the chance of diversification so that better performance could be expected from using cleaned up matrices’.

Econophysicists also used different ways to measure time in their statistical analysis of financial distributions: calendar time, event time (unit of time increases each time a one order is recorded on the market) or tick time (unit of time increments once on each occasion an order is done) are used in order to revisit all existing stylised facts. While Plerou et al. Citation[76] provided a specific link between the variance and the number of orders in a trade time, Griffin and Oomen Citation[77] developed a variance estimator in trade time and tick time.

As mentioned in the previous section, the computerisation of financial places provided extensive data on order dynamics such as order size, time of arrival of orders, placement of orders, cancellation of orders, etc. Many econophysicists proposed a modelling of this dynamics of order books. While Politi and Scalas Citation[78] or Ivanov et al. Citation[79] tried to describe the distribution related to the time of arrival of orders, Gopikrishnan et al. Citation[80] or Maslov and Mills Citation[81] characterised the unconditional distribution of order size (volume of transactions) and Mike and Farmer Citation[82] rather modelled the distribution related to cancellation of orders.

4.2 Non-trivial interactions, transfer of wealth and agent-based econophysics

The second representative sub-field in econophysics concerns agent-based econophysics which has mainly studied the modelling of order-driven markets and wealth distribution. The first area of knowledge provides two kinds of studies depending on the analogy used: on the one hand, some articles applied the agent-based methodology by considering agents as particles, and, on the other hand, some econophysicists use rather the analogy ‘order = particle’ in their application of the agent-based approach.

Some econophysicists use the analogy ‘agent = particle’ in order to describe the non-trivial behaviour of economic actors. Herding behaviour, for example, generates several studies Citation[83–85] associating the information dissemination process with a percolation model among traders whose interactions randomly connected their demand through clusters. In line with what financial economists do Citation[86], some econophysicists Citation[87,88] developed models with two different groups of actors: the fundamentalists (trading a volume proportional to the variation of financial prices) and the chartists (trading a constant level of volume). The physical dimension of these papers refers to the statistical estimation of the reproduced data resulting from the computer simulations. More precisely, econophysicists mainly use physical concepts (Hurst exponent, percolation, etc.) and, a statistical framework used in physics (truncated stable Lévy processes for unconditional distribution) to describe the financial data generated by the agent-based simulations (while economists rather use ARCH-type models to characterise conditional distribution supposed to complete an unconditional Gaussian distribution).

The agent-based approach has also been applied by econophysicists to better understand the emergence of money and how wealth is shared among the population. Pickhardt and Seibold Citation[89], for example, explained that income tax evasion dynamics can be modelled through an ‘agent-based econophysics model’ based on the Ising model of ferromagnetism while Donangelo and Sneppen Citation[90], as well as Shinohara and Gunji Citation[91], approached the emergence of money through studying the dynamics of exchange in a system composed of many interacting and learning agents. Whereas some studies Citation[92,93] used Lotka–Volterra equations to describe the wealth distribution, others expressed wealth exchange by using of matrix theory Citation[94], Markov chains Citation[95] or the Boltzman equation approach Citation[96,97].

Another set of papers uses this analogy ‘agent-particle’ to develop what econophysicists call the kinetic wealth exchange models whose objective is ‘to predict the time evolution of the distribution of some main quantity, such as wealth, by studying the corresponding flow process among individuals’ ([16], p. 1026) by using the general theory of transport of energy and finite-time difference stochastic equations in order to generate a predictive power-law distribution related to the evolution of wealth in an economic system. Dragulescu and Yakovenko Citation[98], Ferrero Citation[99], Heinsalu et al. Citation[100] or Patriarca et al. Citation[101] provided models describing the transfer of wealth for homogeneous agents (i.e. with the same statistical properties) while Chakraborti and Chakrabarti Citation[102], Angle Citation[103], Chatterjee et al. Citation[104] or Chakraborti and Partriarca Citation[105] developed a more complex kinetic wealth exchange model in which agents are diversified (in terms of initial wealth and savings parameter for example).

Concerning the analogy ‘order = particle’, Bak et al. Citation[106] used a reaction diffusion model in order to describe the orders dynamics. In this model, orders were particles moving along a price line, and whose random collisions were seen as transactions (see also Farmer et al. Citation[107], for the same kind of model). Maslov Citation[108] tried to make the model developed by Bak et al. Citation[106] more realistic by adding specific features related to the microstructure (organisation) of the market. In the same vein, Challet and Stinchcombe Citation[109] improved the Maslov Citation[108] model by considering two particles (ask and bid) which can be characterised through three potential states: deposition (limit order), annihilation (market order) and evaporation (cancellation). Slanina Citation[110] also proposed a new version of the Maslov model in which individual position (order) is not taken into account but rather substituted by a mean-field approximation. Assuming the market is in a stable state, the orders dynamics can then be described with an ergodic Markov chain (Smith et al. Citation[111] used the same methodology to describe the cancellation dynamics).

This section overviewed the existing studies developed in econophysics which appears to be a fast growing area of knowledge. However, although we observe an increasing number of articles related to this fieldFootnote 40 , all of them keep a deep link with physical sciences since they mainly use concepts and models coming from physics (and, moreover, these articles are mainly published in physics journals).
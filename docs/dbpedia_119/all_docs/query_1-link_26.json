{
    "id": "dbpedia_119_1",
    "rank": 26,
    "data": {
        "url": "https://www.cambridge.org/core/journals/journal-of-the-history-of-economic-thought/article/econophysics-a-new-challenge-for-financial-economics/D40FEB5B117576E4C54D80DE5BAFB39B",
        "read_more_link": "",
        "language": "en",
        "title": "ECONOPHYSICS: A NEW CHALLENGE FOR FINANCIAL ECONOMICS?",
        "top_image": "https://static.cambridge.org/covers/HET_0_0_0/journal_of the history of economic thought.jpg?send-full-size-image=true",
        "meta_img": "https://static.cambridge.org/covers/HET_0_0_0/journal_of the history of economic thought.jpg?send-full-size-image=true",
        "images": [
            "https://www.cambridge.org/core/cambridge-core/public/images/icn_circle__btn_close_white.svg",
            "https://www.cambridge.org/core/cambridge-core/public/images/logo_core.png",
            "https://www.cambridge.org/core/cambridge-core/public/images/logo_core.svg",
            "https://www.cambridge.org/core/cambridge-core/public/images/logo_core.svg",
            "https://www.cambridge.org/core/cambridge-core/public/images/logo_core.svg",
            "https://static.cambridge.org/covers/HET_0_0_0/journal-of-the-history-of-economic-thought.jpg",
            "https://www.cambridge.org/core/page-component/img/save-pdf-icon.080470e.svg",
            "https://www.cambridge.org/core/page-component/img/pdf-download-icon.c7fb40c.svg",
            "https://www.cambridge.org/core/page-component/img/pdf-download-icon.c7fb40c.svg",
            "https://www.cambridge.org/core/page-component/img/dropbox-icon.3d57046.svg",
            "https://www.cambridge.org/core/page-component/img/google-drive-icon.a50193b.svg",
            "https://www.cambridge.org/core/page-component/img/close-icon.194b28a.svg",
            "https://www.cambridge.org/core/page-component/img/share-icon.cbcfad8.svg",
            "https://www.cambridge.org/core/page-component/img/close-icon.194b28a.svg",
            "https://www.cambridge.org/core/page-component/img/cite-icon.44eaaa4.svg",
            "https://www.cambridge.org/core/page-component/img/rights-icon.d4a677c.svg",
            "https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20160405045243122-0506:S1053837213000205_inline1.gif?pub-status=live",
            "https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20160405045243122-0506:S1053837213000205_equ1.gif?pub-status=live",
            "https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20160405045243122-0506:S1053837213000205_inline2.gif?pub-status=live",
            "https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20160405045243122-0506:S1053837213000205_inline3.gif?pub-status=live",
            "https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20160405045243122-0506:S1053837213000205_inline4.gif?pub-status=live",
            "https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20160405045243122-0506:S1053837213000205_inline5.gif?pub-status=live",
            "https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20160405045243122-0506:S1053837213000205_inline6.gif?pub-status=live",
            "https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20160405045243122-0506:S1053837213000205_inline7.gif?pub-status=live",
            "https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20160405045243122-0506:S1053837213000205_inline8.gif?pub-status=live",
            "https://assets.crossref.org/logo/crossref-logo-100.png",
            "https://upload.wikimedia.org/wikipedia/commons/a/a9/Google_Scholar_logo_2015.PNG",
            "https://assets.crossref.org/logo/crossref-logo-100.png",
            "https://s3-eu-west-1.amazonaws.com/static.idm.prod-a.aop.cambridge.org/identity_61963948cd08362e1640a852.gif",
            "https://s3-eu-west-1.amazonaws.com/static.idm.prod-a.aop.cambridge.org/identity_61963948cd08362e1640a852.gif",
            "https://s3-eu-west-1.amazonaws.com/static.idm.prod-a.aop.cambridge.org/identity_61963948cd08362e1640a852.gif",
            "https://s3-eu-west-1.amazonaws.com/static.idm.prod-a.aop.cambridge.org/identity_61963948cd08362e1640a852.gif",
            "https://s3-eu-west-1.amazonaws.com/static.idm.prod-a.aop.cambridge.org/identity_61963948cd08362e1640a852.gif",
            "https://s3-eu-west-1.amazonaws.com/static.idm.prod-a.aop.cambridge.org/identity_61963948cd08362e1640a852.gif",
            "https://s3-eu-west-1.amazonaws.com/static.idm.prod-a.aop.cambridge.org/identity_61963948cd08362e1640a852.gif",
            "https://s3-eu-west-1.amazonaws.com/static.idm.prod-a.aop.cambridge.org/identity_61963948cd08362e1640a852.gif",
            "https://s3-eu-west-1.amazonaws.com/static.idm.prod-a.aop.cambridge.org/identity_61963948cd08362e1640a852.gif",
            "https://s3-eu-west-1.amazonaws.com/static.idm.prod-a.aop.cambridge.org/identity_61963948cd08362e1640a852.gif",
            "https://s3-eu-west-1.amazonaws.com/static.idm.prod-a.aop.cambridge.org/identity_61963948cd08362e1640a852.gif",
            "https://s3-eu-west-1.amazonaws.com/static.idm.prod-a.aop.cambridge.org/identity_61963948cd08362e1640a852.gif",
            "https://s3-eu-west-1.amazonaws.com/static.idm.prod-a.aop.cambridge.org/identity_61963948cd08362e1640a852.gif",
            "https://s3-eu-west-1.amazonaws.com/static.idm.prod-a.aop.cambridge.org/identity_61963948cd08362e1640a852.gif",
            "https://s3-eu-west-1.amazonaws.com/static.idm.prod-a.aop.cambridge.org/identity_61963948cd08362e1640a852.gif",
            "https://s3-eu-west-1.amazonaws.com/static.idm.prod-a.aop.cambridge.org/identity_61963948cd08362e1640a852.gif",
            "https://s3-eu-west-1.amazonaws.com/static.idm.prod-a.aop.cambridge.org/identity_61963948cd08362e1640a852.gif",
            "https://s3-eu-west-1.amazonaws.com/static.idm.prod-a.aop.cambridge.org/identity_61963948cd08362e1640a852.gif",
            "https://s3-eu-west-1.amazonaws.com/static.idm.prod-a.aop.cambridge.org/identity_61963948cd08362e1640a852.gif",
            "https://s3-eu-west-1.amazonaws.com/static.idm.prod-a.aop.cambridge.org/identity_61963948cd08362e1640a852.gif",
            "https://www.cambridge.org/core/cambridge-core/public/images/cambridge_logo.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "FRANCK JOVANOVIC",
            "CHRISTOPHE SCHINCKUS"
        ],
        "publish_date": null,
        "summary": "",
        "meta_description": "ECONOPHYSICS: A NEW CHALLENGE FOR FINANCIAL ECONOMICS? - Volume 35 Issue 3",
        "meta_lang": "en",
        "meta_favicon": "/core/cambridge-core/public/images/favicon.ico",
        "meta_site_name": "Cambridge Core",
        "canonical_link": "https://www.cambridge.org/core/journals/journal-of-the-history-of-economic-thought/article/econophysics-a-new-challenge-for-financial-economics/D40FEB5B117576E4C54D80DE5BAFB39B",
        "text": "The Birth of Financial Economics\n\nAs this section reminds us, financial economics owes its institutional birth in the 1960s to three elements: access to the tools of modern probability theory; the creation of new empirical data; and the extension of the analysis framework of economics.Footnote 5\n\nOn the accessibility of the tools of modern probability theory\n\nFinancial economics is intimately bound up with modern probability theory, from which its emergence, main models, and results are inseparable. So close are the links that, further to the publications of Harrison and Kreps (Harrison and Kreps Reference Harrison and Kreps1979) and Harrison and Pliska (Reference Harrison and Pliska1981),Footnote 6 some authors have suggested that economics has been dispossessed of financial theory, which has since resembled an application of modern probability theory (MacKenzie Reference MacKenzie2006, pp. 140–141). Or, as posited by Davis and Etheridge, Harrison and Pliska’s article (1981) “has turned ‘financial economics’ into ‘mathematical finance’” (Davis and Etheridge Reference Davis and Etheridge2006, p. 114).\n\nModern probability theory—probability for continuous quantities in continuous time—emerged in the 1930s (Von Plato Reference Von Plato1994) out of a number of works aimed at renewing traditional probability theory. The development of the modern version of probability theory was directly based on measure theory (Shafer and Vovk Reference Shafer and Vovk2001). The connection was made by Kolmogorov, who proposed the main founding concepts of this new branch of mathematics.\n\nFrom these beginnings in the 1930s, modern probability theory developed and became increasingly influential. But it was not until after World War II that Kolmogorov’s axioms came to dominate this discipline (Shafer and Vovk Reference Shafer and Vovk2005, pp. 54–55). It was also after World War II that the American probability school was born, led by DoobFootnote 7 and by Feller,Footnote 8 who proved, on the basis of the framework laid down by Kolmogorov, all results obtained prior to the 1950s, thereby enabling them to be accepted and integrated into the discipline’s theoretical corpus. These 1950s works led to the creation of a stable corpus that was accessible to non-specialists. From then on, the models and results of modern probability theory were used in the study of financial markets in a more systematic manner, in particular by scholars educated in economics.\n\nThe first step in this development was the dissemination of mathematical tools enabling the properties of random variables to be used and uncertainty reasoning to be developed. The first two writers to use tools that came out of modern probability theory to study financial markets were Harry Markowitz and A. D. Roy. In 1952, each published an article on the theory of portfolio choice theory.Footnote 9 Both used mathematical properties of random variables to build their model.Footnote 10 Their work was to re-prove a result that had long been known (and that was as old as the adage “Don’t put all your eggs in one basket”), using a new mathematical language, that of modern probability theory. Their contribution lay not in the result of portfolio diversification, but in the use of this new mathematical language.\n\nFrom the 1960s on, a new stage was embarked upon: authors no longer limited themselves to proving past results using the mathematical formalisms of modern probability theory, but connected mathematical formalism with the main concepts of economics, particularly the concept of equilibrium, to create new theories.\n\nThe efficient markets theory,Footnote 11 which can be considered as the first theory built by financial economists, provides a good example. This theory was initially referred to as the “random walk theory.” This term stresses the importance of mathematical formalism in the way issues were tackled before the discipline was constituted. The theory, first formulated by Fama (Reference Fama1965b), made it possible to link the mathematical model of a stochastic process with one of the keystones of economics, the concept of economic equilibrium (Jovanovic Reference Jovanovic and Cont2010). In 1970, Fama based the efficient markets theory on another mathematical concept that came from modern probability theory: the martingale model.Footnote 12 For Fama’s purposes, the most important attraction of the martingale formalism was its explicit reference to a set of information.Footnote 13 As such, the martingale model could be used to test the implication of the efficient markets theory that, if all available information is used, the expected profit is nil. This idea led to the definition of an efficient market that is generally used nowadays: “a market in which prices always ‘fully reflect’ available information is called ‘efficient’” (1970, p. 383). The part played by economics in the mathematical definition of the martingale model underlines economics’ key role in the creation of the structure of modern financial theory.\n\nThe creation of new empirical data\n\nIn parallel with the adoption of tools, models, and concepts from modern probability theory for analyzing financial markets, another crucial advance occurred in the 1960s: the creation of databases containing long-term statistical data on the evolution of stock-market prices. These databases allowed spectacular development of empirical studies used to test models and theories in finance. The development of these studies was the result of the creation of new statistical data and the emergence of computers.\n\nBeginning in the 1950s, computers gradually found their way into financial institutions and universities (Sprowls Reference Sprowls1963, p. 91). However, owing to the costs of using them and their limited calculation capacity, “it was during the next two decades, starting in the early 1960s, as computers began to proliferate and programming languages and facilities became generally available, that economists more widely became users” (Renfro Reference Renfro2009, p. 60). The first econometric modeling languages began to be developed during the 1960s and the 1970s (Renfro Reference Renfro and Renfro2004, p. 147). From the 1960s on, computer programs began to appear in increasing numbers of undergraduate, master’s, and doctoral theses. As computers came into more widespread use, easily accessible databases were constituted, and stock-market data could be processed in an entirely new way, thanks to, among other things, financial econometrics (Louçã Reference Louçã2007). Financial econometrics marked the start of a renewal of investigative studies on empirical data and the development of econometric tests.\n\nA number of financial-econometrics studies were carried out in the 1960s, using computers. While the first empirical studies of this type date back to 1863 in France and the early 1930s in the US (Poitras Reference Poitras2006, Jovanovic Reference Jovanovic2009b), the results were very limited because all calculations had to be performed by hand. With computers, empirical study could become more systematic and be conducted on a larger scale. Attempts were made to test the random nature of stock-market variations in different ways (Jovanovic Reference Jovanovic2009b). Markowitz’s hypotheses were used to develop specific computer programs to assist in making investment decisions.Footnote 14\n\nOf the databases created during the 1960s, one of the most important was set up by the Graduate School of Business at the University of Chicago,Footnote 15 one of the key institutions in the development of financial economics. The first version of this database, which collected monthly prices from January 1926 through December 1960, greatly facilitated the emergence of empirical studies. Apart from its exhaustiveness, it provided a history of stock-market prices and systematic updates.\n\nThe institutionalization of financial economics and the challenge to the dominant school of thought of the time\n\nThe third element that contributed to the institutional birth of financial economics was the integration of the analysis framework of economics (hypotheses, concepts, method, etc.) into the analysis of financial markets. This integration was the result of the formation in the early 1960s of a community of economists dedicated to the analysis of financial markets.\n\nUntil the 1960s, finance in the United States was taught mainly in business schools. The textbooks used were very practical, and few of them touched on what became modern financial theory. The research work that formed the basis of modern financial theory was carried out by isolated writers who were trained in economics or were surrounded by economists, such as Working, Cowles, Kendal, Roy, and Markowitz. No university community devoted to the subject existed prior to the 1960s.Footnote 16 During the 1960s and 1970s, training in American business schools changed radically, becoming more “rigorous.”Footnote 17 They began to “academicize” themselves, recruiting increasing numbers of economics professors who taught in university economics departments, such as Miller (Fama Reference Fama2008). Similarly, prior to offering their own doctoral programs, business schools recruited doctorands who had been trained in university economics departments.\n\nThe recruitment of economists interested in questions of finance unsettled teaching and research as hitherto practiced in business schools and inside the American Finance Association. The new recruits brought with them their analysis frameworks, methods, hypotheses, and concepts, and also used the new mathematics that arose out of modern probability theory. These changes and their consequences were substantial enough for the American Finance Association to devote part of its annual meeting to them in two consecutive years, 1965 and 1966.\n\nAt the 1965 annual meeting of the American Finance Association, an entire session was devoted to the necessity to rethink courses in finance curricula. Paul Wendt discussed the development of finance and explained that “a modern concept of technical market analysis is emerging which emphasizes the application of newer analytical techniques and computer technology to test traditional and new theories of stock-price behaviour” (Wendt Reference Wendt1966, pp. 421–422). At the 1966 annual meeting, the new president of the American Finance Association presented a paper on “The State of the Finance Field,” in which he talked of the changes being brought about by “the creators of the New Finance [who] become impatient with the slowness with which traditional materials and teaching techniques move along” (Weston Reference Weston1967, p. 539).Footnote 18 Although these changes elicited many debates (Whitley Reference Whitley and Samuels1986a, Reference Whitley and Samuels1986b; MacKenzie Reference MacKenzie2006; Poitras et al. Reference Poitras and Jovanovic2007; Jovanovic Reference Jovanovic2008; Poitras et al. Reference Poitras2010), none succeeded in challenging the global movement.\n\nThe antecedents of these new actors were a determining factor in the institutionalization of modern financial theory. Their background in economics allowed them to add theoretical content to the empirical results that had been accumulated since the 1930s and to the mathematical formalisms that had arisen from modern probability theory. In other words, economics brought the theoretical content that was missing.\n\nThe creation of a new scientific community requires that its new members share common tools, references, and problems. This was precisely the role of textbooks, seminars, and scientific journals. Those in financial economics were developed from the beginning of the 1960s with the arrival of this new generation of professors and students. The two journals that had published articles in finance, the Journal of Finance and the Journal of Business, changed their editorial policy during the 1960s. Both started publishing articles based on modern probability theory and on modeling (Bernstein Reference Bernstein1992, pp. 41–44, 129). They also published several special issues to reinforce the new orientation and results. In 1966, the Journal of Business published a special issue on “recent quantitative and formal research on the stock market.” In addition to these two journals, other scientific journals specializing in financial economics were created, such as the Journal of Financial and Quantitative Analysis in 1965. In 1968, the last-mentioned journal published a special issue on the application of the random walk model to stock prices.\n\nIt was also during the 1960s that textbooks and collections of articles started to appear.Footnote 19 These publications also helped define and stabilize a culture shared by the members of the new community. Several seminars were also organized. The new seminars and the publications contributed to the creation of a truly homogenous community (which shared common problems, common tools, and a common language) and scientific journals and courses in universities. One of the main features of this common culture was the creation of a canonical history of financial economics during the 1960s. This history was created to support theoretical viewpoints—viewpoints that led the mainstream community of scientists to recognize financial economics as a science (Jovanovic Reference Jovanovic2008).\n\nThe Birth of Econophysics\n\nEconophysics studies emerged in the 1990s. Their origins—like those of financial economics—lay in modern probability theory, the emergence of new empirical data, and the use of models, hypotheses, and methods taken from a discipline outside the mainstream of the time. The birth of econophysics, therefore, closely resembles that of financial economics in the 1960s.\n\nThe role of modern probability theory\n\nFrom the 1980s on, modern probability theory evolved to some degree, particularly with regard to the definition of stochastic processes known as “Lévy processes” (and, more precisely, as “stable Lévy processes”). Lévy processes include many classes of stochastic processes, such as the Wiener process, jump-diffusion processes, and jump stable Lévy processes.Footnote 20 Jump stable Lévy processes are characterized by the existence of small jumps in alternation with big jumps; in general, they have infinite variance. Due to their stable Lévy character, and unlike jump-diffusion models, jump stable Lévy models have infinite activity (infinite number of jumps on each time interval) and infinite variation. A specific class of jump stable Lévy processes considered by econophysicists is composed of stable Lévy processes.\n\nStable Lévy processes are jump processes, characterized by the stable Lévy distribution, having an α-stable law type $P\\left( {X{\\rm{ }} &#x3e; {\\rm{ }}x} \\right){\\rm{ }} &#x003D; {\\rm{ }}x^{ - \\alpha }$ in which it is possible to observe constancy of the coefficient α. A stable Lévy distribution with α = 2 is a Gaussian distribution; with α = 1, it is a Cauchy distribution; and with α = 3/2, it is a Pareto distribution.Footnote 21 Stable Lévy processes have a distribution with infinite variance, which is considered the main obstacle to the use of these processes in finance. Indeed, an infinite variance means that risk can vary considerably, depending on the size of the sample and the observation scale.\n\nAt the beginning of the 1980s, stable Lévy processes were the subject of a theoretical debate in specialist literature on statistics. These processes were essentially seen as theoretical tools (Zolotarev Reference Zolotarev1986) or as “monsters” (MacKenzie Reference MacKenzie2006, p. 108) with no real practical applications, due to their infinite variance. This theoretical characteristic added considerably to the complexity of applying these processes to observed phenomena, since the notion of variance very often refers to a well-defined empirical parameter, which is finite (risk in finance, temperature in thermodynamics, for example). Considering this situation, physicists developed theoretical solutions whose objective was to define a class of processes that was compatible with the empirical observations they had. This theoretical literature led to the development of Lévy processes with finite variance, called “truncated Lévy processes” (Schinckus Reference Schinckus2013).\n\nFrom the 1980s onwards, stable Lévy processes were increasingly used in physics,Footnote 22 particularly in statistical physics. The latter discipline can be thought of as the continuation of thermodynamics, and the use of stable Lévy processes in this field allowed more accurate modeling of the phenomenon of turbulence. The first studies on Lévy processes applied to turbulence phenomena were those of Kolmogorov on the scale invariance of turbulence in the 1930s. This theme was subsequently addressed by many physicists and mathematicians, particularly by Mandelbrot in the 1960s when he defined fractal mathematicsFootnote 23 and applied it to the phenomenon of turbulence.\n\nDespite the extension of probability theory to thermodynamics, physicists did not seem disposed to integrate stable Lévy processes into physics (Gupta and Campanha Reference Gupta and Campanha2002, p. 382). The reason for this methodological position—which mirrors that taken in financial economics—is that processes with infinite variance were not physically plausible (i.e., compatible with assumptions of physics).Footnote 24 As Gupta and Campanha (Reference Gupta and Campanha1999, p. 232) point out, Lévy processes “have mathematical properties that discourage a physical approach because they have infinite variance.”Footnote 25 The use of Lévy processes in physics necessitated the development of truncated Lévy processes, which allowed physicists to use these processes to statistically characterize turbulence phenomena without the problem of infinite variance.Footnote 26 The first truncated stable Lévy process in physics was proposed by Zolotarev in Reference Zolotarev1986. This response of physicists to the indeterminate nature of variance paved the way for finance applications to describe the evolution of financial markets using stable Lévy processes that are not Gaussian. The first application of this statistical solution to finance was proposed by Mantegna in 1991.\n\nTruncated Lévy processes, then, provide an opportunity to solve the problem of infinite variance observed in the application of Lévy processes in thermodynamics and finance. However, econophysicists use statistical processes in an instrumentalist manner. They are uninterested in hypotheses, preferring to focus on prediction. Of course, this kind of methodology is known in finance, where econometrics is often said to be based on the instrumentalist Friedmanian methodology (Angrist and Pischke Reference Angrist and Pischke2008). However, unlike econometrics, econophysics is founded on a physically plausible analysis in which each statistical parameter must have a physical meaning. From this perspective, econophysicists have developed more sophisticated stable Lévy processes, giving them a statistical tool in line with a physically plausible framework (Schinckus Reference Schinckus2013).\n\nThe creation of new empirical data\n\nIn parallel with this application of stable Lévy processes to the study of financial markets, new statistical data were created from the 1990s as a result of the automation of financial centers.\n\nSince the 1970s, a number of financial markets have been automated—Toronto, Paris, Nasdaq, and Euronext, for example. Today, electronic markets flood the financial sphere with accurate data in real time. The automation of markets has made it possible to record “intraday” data. Previously, statistical data on financial markets were generally made up of a single value per day (the average price). Today, by recording “intraday data,” all prices quoted are conserved. The new data collected in this manner have stimulated research into distributions of stock-market variations. It is difficult to determine laws of financial data distribution with any certainty. Mitzenmacher (Reference Mitzenmacher2004) reminds us of how close these laws are to the so-called exponential laws, and that the two types of law can be distinguished only by means of a large volume of data. The advent of intraday data has made it possible to build sufficiently broad samples to provide firm proof of Mandelbrot’s idea that the evolution of financial markets could be characterized using stable Lévy processes such as those used by econophysicists (Kou Reference Kou2008).\n\nIn this context, computer technology is presented as a tool that makes it possible to confirm with certainty the hypothesis that the evolution of prices and returns on financial markets can be characterized by a stable Lévy distribution. This growing quantification of financial information takes the form of an accumulation of data stored as temporal series, thereby making market finance “a natural area for physicists” (Gallegati, Keen, et al. Reference Gallegati and Keen2006, p. 1).\n\nMantegna and Stanley (Reference Mantegna and Stanley1999, p. 6), McCauley (Reference McCauley2004, p. 7), and Burda, Jurkiewicz, and Nowak (Reference Burda, Jurkiewicz and Nowak2003, p. 3) also underline the role that computerization played in the emergence of econophysics, and above all the fact that it broadened the perspective of statistical market analysis. Now that financial markets are computerized, tens of thousands of transactions or posted quotes in a single day—time-stamped to the nearest second—can be observed (Engle and Russell Reference Engle and Russell2004). Analyzing these new data sets brings new challenges, and they require new statistical tools to characterize them. More precisely, several phenomena can be detected with intraday data that are not present with monthly or daily data. The latter are generally the last prices quoted during a month or a day, or a mean of the prices quoted during a period, and, therefore, jumps in data are generally smaller and less frequent. The computerization of financial markets has, therefore, contributed to the use of new frameworks such as stable Lévy processes, which are better suited to the modeling of jumps in stock-price variations.\n\nComputerization of financial markets and, more generally, of the entire financial sphere has had another consequence that has favored the use of stable Lévy processes with which physicists work. According to Barber and Odean (Reference Barber and Odean2001), the computerization of the financial sphere provides an “illusion of knowledge” to online investors, who become excessively self-confident and tend to underestimate risks. This overconfidence of investors leads them to invest more, and in a more speculative way, than they otherwise would.Footnote 27 Barber and Odean (Reference Barber and Odean2001) conclude that, in this way, online trading contributes to an increase in market volatility. This greater volatility has engendered more extreme variations in quotations (Jiang, Tang, et al. Reference Jiang and Tang2002), and, therefore, the tails of empirical distributions have become fatter. The normal law, used by almost all models in financial economics, does not allow extreme variations to be taken into account.Footnote 28 Such variations are, however, perfectly integrated into stable Lévy processes. The increase in the volatility of financial markets implying fatter tails of empirical distributions has, therefore, helped justify the use of statistical tools developed in physics that are suited to the analysis of extreme phenomena.\n\nWe can see, therefore, a double contribution of technology to the emergence of econophysics: one is direct, resulting from the computerization of financial markets (better analysis and storage of data); the other, more indirect, results from financial behavior that computerization has engendered.\n\nThe institutionalization of econophysics\n\nIn less than twenty years, econophysics has earned recognition as a scientific field from academics of the hard sciences.Footnote 29 To gain this recognition, econophysicists adopted a variety of strategies for spreading their knowledge. Symposia were organized, several specialized journals created, and specific academic courses offered by physics departments to promote the scientific recognition and institutionalization of this new approach. All these strategies played a part not only in disseminating econophysics but also in creating a shared scientific culture (Nadeau Reference Nadeau, Cometti and Tiercelin1995).\n\nThe first publications date from the 1990s. The founding article by Stanley et al., published in 1996, strongly influenced physicists and mathematicians who, suddenly, developed a non-Gaussian approach to the study of financial returns (Kutner and Grech Reference Kutner and Grech2008). Since 1996, sustained growth in the number of articles devoted to econophysics has been observed (Gingras et al. Reference Gingras and Schinckus2012). The increase in the number of articles published each year earned econophysics official recognition as a subdiscipline of physical sciences in 2003—less than ten years after its birth.\n\nThe first textbooks on econophysics were not far behind, the first being published in 1999 by Mantegna and Stanley (An Introduction to Econophysics). The process of institutionalization was reinforced in 2006 with the creation of the Society for Economic Science with Heterogeneous Interacting Agents (ESHIA), whose objective is to promote an interdisciplinary research among economics, physics, and computer science (essentially artificial intelligence). This interdisciplinary project, supported by the creation of new journals,Footnote 30 is, therefore, aimed at the area covered by econophysics.\n\nA further indicator of the emergence and the institutionalization of the new scientific community is the organization of symposia and workshops. The first conference devoted to econophysics was organized in 1997 by the physics department of the University of Budapest. Today, conferences and symposia dedicated to econophysics are very numerous, notable among them being the Nikkei Econophysics Research Workshop and Symposium and Econophysics Colloquium. In addition to the numerous publications about econophysics, all these regular events constitute institutional spaces that are helping to make econophysics a true scientific community.\n\nThe last major element in the institutionalization of econophysics is university education. Today, the physics departments of the Universities of Fribourg (Switzerland), Ulm (Sweden), Münster (Germany), and Dublin (Ireland) offer courses in econophysics. Since 2002, the Universities of Warsaw and Wrolcaw (both in Poland) have been offering a bachelor’s and a master’s degree in econophysics, respectively (Kutner et al. Reference Kutner and Grech2008). Finally, the University of Houston (Texas, USA) created the first doctoral program in econophysics in 2006,Footnote 31 followed in 2009 by the University of Melbourne (Australia).Footnote 32 All these programs are offered by physics departments, and courses are essentially oriented toward statistical physics and condensed-matter physics.\n\nThe Role of Probability Theory and Empirical Data\n\nAs we saw in the first part, two elements strongly contributed to the emergence of both approaches: the development of modern probability theory, on the one hand; and the evolution of financial markets, which are increasingly quantitative (or digitized), on the other. In each case, these two factors acted as triggers for the emergence of an alternative approach. Let us now look more closely at this point.\n\nIn the 1960s, as explained earlier, some economists took up random processes at a time when mathematical developments had become newly accessible to non-mathematicians. The use or non-use of these new tools—modern probability theory and work on statistical data—constituted the main element setting the “new approach” against the “traditional approach” of the time: “Mathematical models, careful statistical testing of hypotheses, decision theory, the techniques of operations research, and the new and powerful tool of programming began to be applied to the finance field” (Weston Reference Weston1967, p. 539).\n\nThis mathematical eolution went hand in hand with technological developments as the use of computers gradually became widespread. Computers made it possible to perform tests on empirical data (in this case, econometric tests) in order to assess the methods proposed for earning money on financial markets, particularly chartist analysis. In this respect, Rosenfeld (Reference Rosenfeld1957, p. 52) proved to be visionary when he suggested using computers for testing theories on a large sample.\n\nThe development of probability theory combined with finer quantification of financial markets (thanks to developments in computing) were triggering factors in the emergence of econophysics, also. As explained earlier, physicists refined Lévy laws in order to use them in physics. In particular, they developed what are known as truncated Lévy laws, which have finite variance. In this perspective, truncated Lévy laws are to econophysics what the Gaussian framework was to mainstream financial economicsFootnote 33: statistical justification of its approach and, hence, a justification of its emergence.\n\nOnce again, in the case of econophysics, computers—again in parallel with mathematical developments—contributed to the emergence of the new approach, because they made possible better quantification of financial operations. Today, electronic markets rule the financial sphere, and allow more accurate study of the evolution of the real-time data they provide (stored in the form of time series). While this type of data has been studied by economists for several decades, the automation of markets has enabled intraday data providing “three orders of magnitude more data” to be recorded (Stanley, Amaral, et al. Reference Stanley and Amaral2000, p. 339). The quantity of data is an important factor at a statistical level because the larger the sample, the more reliable the identification of statistical patterns.\n\nComputerization of financial centers has led to the recording of huge quantities of financial data, so much so that econophysicists see finance as truly an “empirical (rather than axiomatic) science” (Bouchaud Reference Bouchaud and Potters2002). The creation of empirical databases had played the same role in the 1960s: they stimulated the application of mathematical models taken from modern probability theory and research into stock-market variations.\n\nThus, both financial economics and econophysics owe their emergence to the creation of new mathematics combined with the creation of new statistical data. Morgan and Morrison (Reference Morgan and Morrison1999) underlined the importance of models in twentieth-century scientific disciplines, particularly economics. Models have shown themselves to be “mediators” between theory and reality. In other words, they are neither one nor the other: “It is precisely because models are partially independent of both theories and world that they have this autonomous component and so can be used as instruments of exploration in both domains” (1999, p. 10). Analyses by Cartwright (Reference Cartwright1983), and Barberousse and Ludwig (Reference Barberousse and Ludvig2000), have shown that, in twentieth-century scientific disciplines at least, scientific models must be interpreted as fictions. Although both financial economics and econophysics owe their emergence to mathematical modeling, they must be distinguished from one another as regards the place occupied by theory: in the case of econophysics, there is currently no theoretical explanation to give meaning to the models used.Footnote 34\n\nThe Same Institutionalization Strategy\n\nAs regards institutionalization, econophysics is once again following the pattern observed during the emergence of financial economics: in both cases, a recognized discipline expanded towards a new field of research whose study had been hitherto dominated by another discipline. In the 1960s, economics expanded to the study of financial markets, which, at the time, was dominated by so-called “traditional” financial theory; in the 1990s, statistical physics expanded to the study of financial markets, which, at the time, were dominated by financial economics. In both cases, the new community was made up of scientists trained outside the discipline, and, hence, outside the mainstream. A kind of colonization of finance has occurred.\n\nThis colonization can also be detected in the new arrivals’ publication strategy. They began by publishing in journals of their discipline of origin to make themselves known and disseminate their results—a sort of takeover of recognized scientific journals in the discipline of origin.\n\nIn the 1960s, the newcomers took control of the two main journals specializing in finance at the time, the Journal of Business and the Journal of Finance. The aim was to modify the content of published articles by imposing a more strongly mathematical content and by using a particular structure: presenting the mathematical model and then empirical tests. To reinforce the new orientation, these two journals also published several special issues. Once control over these journals had been established, the newcomers developed their own journals, such as the Journal of Financial and Quantitative Analysis created in 1965.\n\nSimilarly, econophysicists chose to publish and gain acceptance in journals devoted to an existing theoretical field in physics (statistical physics) rather than create new journals outside an existing scientific space and, hence, structure. These journals are among the most prestigious in physics. This editorial strategy is a result not only of the methodology used by econophysicists (deriving from statistical physics) but also of this new community’s hope to gain recognition from the existing scientific community quickly, on the one hand, and to reach a larger audience, on the other hand. Then they took control of editorial boards (as in the case of Physica A and The European Journal of Physics B).\n\nThe new approaches had no alternative to this “colonization strategy,” because partisans of the dominant approach (and, hence, of the so-called mainstream journals) rejected these new theoretical developments in which they were not yet proficient. Gradual recognition of the new discipline subsequently allowed new specialist journals to be created, such as the Journal of Financial and Quantitative Analysis (1965), Quantitative Finance (2001), and the Journal of Economic Interaction & Coordination (2006), which are officially indexed under human sciences, making it possible to reach a wider readership (especially in economics).\n\nSimilar Claims regarding the Discipline’s Scientificity\n\nA final similarity is the use of the same discourse to justify the scientificity of the new approach. The emergence of both financial economics and econophysics was accompanied by particularly virulent criticism of the scientificity of existing studies. An analysis of the discourse used reveals three similar justifications: the claim of a more scientific approach, breaking with the past; belittling of the existing approach; and the claim of greater empirical realism.\n\nThe claim of a more scientific approach\n\nIn each case, proponents of the new approach challenged the traditional approach by asking its adepts to prove that it was scientific. This “confrontational” attitude is founded upon the challengers’ contention that the empirical studies, the new mathematics, and methodology they use guarantee a scientificity (i.e., a way of doing science) absent from the traditional approach.Footnote 35 The challengers maintain that the scientificity of a theory or a model should determine whether it is adopted or rejected.\n\nThis confrontational approach was used by the early financial economists in their opposition to the chartists and to financial analysts.Footnote 36 As an example, James Lorie (Lorie Reference Lorie1965, p. 17) taxed the chartists with not taking into account the tools used in a scientific discipline such as economics. Similarly, Fama (Fama Reference Fama1965c, p. 59), Fisher and Lorie (Reference Fisher and Lorie1964, pp. 1–2), and Archer (Reference Archer1968, pp. 231–232) presented their results as a “challenge” to chartists and financial analysts. In this debate, financial economists argued that their approach was based on scientific criteria, while chartism was based on folklore and had no scientific foundation. Consequently, financial economics should supplant previous folkloric practices.\n\nAnother argument is based on the method used. Consider Fama’s three articles (Fama Reference Fama1965b, Reference Fama1965c, Reference Fama1970). All used the same structure: the first part dealt with theoretical implications of the random walk model and its links with the efficient market hypothesis, while the second part presented empirical results that validate the random walk model. This sequence—theory then empirical results—is today very familiar. It constitutes the hypothetico-deductive method, the scientific method defended in economics since the middle of the twentieth century.\n\nAs with financial economics in the 1960s, the main epistemological justification for the emergence of econophysics was the idea that the new approach was more scientific than the old. Econophysicists claim that their approach is more neutral (i.e., not based on an a priori model) with regard to the study of chance. They explicitly demonstrate a willingness to develop models that are, on the one hand, more coherent from a physics point of view,Footnote 37 and, on the other hand, based on “raw” observations of economic systems (Stanley, Gabaix, et al. Reference Stanley and Gabaix2008). This approach is deemed more robust and more scientific than the empirical studies carried out in financial economics (Stanley et al. Reference Stanley and Gabaix2008, p. 3),Footnote 38 and, in addition, “a claim often made by econophysicists is that their models are more realistic than those offered up by economists and econometricians” (Stanley et al. Reference Stanley and Gabaix2008, p. 3).Footnote 39 By “physically realistic models,” the authors mean that econophysicists need to be able to give a physical meaning to the statistical parameters they use.Footnote 40\n\nBouchaud (2002, p. 238)41 and McCauley (Reference McCauley2004, p. 7) further assert their position by implying that physicists are the class of scientists best placed to deal with economic and financial phenomena.\n\nBelittling the existing approach\n\nJustifying the emergence of a new approach involves systematically calling into question the methodology employed by the mainstream and using a specific vocabulary to denigrate its work.\n\nCootner’s book (1964) was one of the first publications used by the proponents of financial economics to define the discipline. In his introduction, Cootner asserted that:\n\nAcademic studies have proven to be more sceptical about the folklore of the market place than those of the professional practitioners. To several of the authors represented in this volume the ‘patterns’ described by some market analysis are mere superstitions. (Cootner Reference Cootner1964, p. 1)\n\nCootner (Reference Cootner1964) presented the first studies of the financial economists he discussed as the first scientific approach to stock-market variations, which would supplant previous practices, judged to be groundless. The method employed and the empirical test of hypotheses were also presented as a guarantee of the scientificity of the results.\n\nFama (Reference Fama1965c, p. 59) and Lorie (Reference Lorie1966, p. 110), other emblematic figures in financial economics, denigrated traditional approaches in a similar manner. Hoffland (Reference Hoffland1967, pp. 85–88) provided a good summary of the situation:\n\nFolklore is a body of knowledge incorporating the superstitions, beliefs and practices of the unsophisticated portion of a society…. Folklore is distinguished from scientific knowledge by its lack of rigor…. The Dow Theory is often used as an example of a crudely formulated stock market ‘theory’….\n\nEconophysicists have proceeded in like fashion. In their work, they belittle the methodological framework of financial economics using similar vocabulary. They describe the theoretical developments of financial economics as “inconsistent … and appalling” (Stanley et al. Reference Stanley and Amaral1999, p. 288). Despite his being an economist,Footnote 42 Keen (Reference Keen2003, p. 109) discredits financial economics by highlighting the “superficially appealing” character of its key concepts or by comparing it to any “tapestry of beliefs” (Keen Reference Keen2003, p. 108). Marsili and Zhang (Reference Marsili and Zhang1998, p. 51) describe financial economics as “anti-empirical,” while McCauley does not shrink from comparing the scientific value of the models of financial economics to that of cartoons: “The multitude of graphs presented without error bars in current economics texts are not better than cartoons, because they are not based on real empirical data, only on falsified neo-classical expectations” (2006, p. 17).\n\nThe vocabulary used is designed to leave the reader in no doubt: “scientific,” “folklore,” “deplorable,” “superficial,” “sceptical,” “superstition,” “mystic,” “challenge.”\n\nThe claim of greater empirical realism\n\nFinancial economists underlined the importance of the empirical dimension of their research from their very first publications (Lorie Reference Lorie1965, p. 3). They saw the testability of their models and theories as a guarantee of scientificity, and concluded that “The empirical evidence to date provides strong support for the random-walk model” (Fama Reference Fama1965c, p. 59).\n\nThe same has happened in the case of the econophysicists, some of whom have insisted that it is precisely because the fundamental concepts of financial economics are “empirically and logically” (Keen Reference Keen2003, p. 108) erroneous that a new, more “realistic” form of modeling needs to be developed. Here, the term “realistic” must be understood, according to econophysicists, to mean true relationship; that is, the ability to describe the true relationship governing changes in financial quotations.Footnote 43 This realism is, therefore, essentially a posteriori and in no way directed at the nature of hypotheses formulated ex ante (unlike economics, for example). This relationship to empiricism is very marked in econophysicists, who regularly point out that the empirical dimension is central in their work. Thus, although the “empirical data” are the same for financial economists and for physicists (financial quotations in the form of temporal series), physicists are quick to point to their “direct use of raw data,” thereby criticizing the use of statistical transformations performed by financial economists to “normalize” data. Here is Mandelbrot on this point:\n\nThe Gaussian framework being a statistician’s best friend, often, when he must process data that are obviously not normal, he begins by “normalizing” them … in the same way, it has been very seriously suggested to me that I normalize price changes. I believe, quite to the contrary, that the long tails of histograms of price changes contain considerable amounts of information, and that there are a number of cogent reasons for tackling them head-on. (1997, p. 142)\n\nMcCauley directly attacks this practice used by financial economists, explaining,\n\nWe [econophysicists] have no mathematical model in mind a priori. We do not ‘massage’ the data. Data massaging is both dangerous and misleading.... Economists assume a preconceived model with several unknown parameters and then try to force-fit the model to a nonstationary time series by a ‘best choice of parameters.’ (2006, p. 8)\n\nThis methodological position is widespread among econophysicists, who work in the spirit of experimental physics in contrast to standard methods in economics. This empirical perspective is also justified, in the view of econophysicists, by the evolution of financial reality. The computerization of financial markets has led to better quantification of financial reality, which should now be studied as an “empirical science” (Bouchaud Reference Bouchaud and Potters2002, McCauley Reference McCauley2004). This radical viewpoint espoused by some econophysicists has an element of naivety. Indeed, in a sense, any sampling method is a massaging of data. Nevertheless, this viewpoint has led econophysicists to a better consideration of extreme values, while such values are considered as errors by the majority of financial economists.Footnote 44 However, econophysicists seem to forget that all statistical data are embedded in a theory. By developing only physically plausible frameworks, econophysicists also appear to have some a priori beliefs about the world. Indeed, for econophysicists, there are no “abnormal data,” but only data about reality. From this point of view, whatever econophysicists write, their empiricist methodology can also be seen as an implicit a priori assumption about the world directly inspired by a neo-positivist epistemology (Schinckus Reference Schinckus2010b, Reference Schinckus2013). In a sense, no way of collecting data can ever be totally neutral, because all data are necessarily the result of a specific process (Schinckus Reference Schinckus2010a).\n\nBetter Prediction of Empirical Facts\n\nEconophysics’ first strong point is its ability to explain empirical facts for which financial economics fails to account. As the work of Kuhn (Reference Kuhn1962) showed, the emergence of a new approach can be justified by its ability to provide answers to the anomalies of the established approach.Footnote 45 In order for a new approach to establish itself as the new reference point for a discipline, it must not only generalize the existing results of the old approach but must also better explain observed empirical facts (including those not explained by the old approach).\n\nThis question is deeply embedded in the history of financial economics. Indeed, the emergence of new empirical data and new statistical and mathematical models has regularly led financial economists to transform discrepancies between predictions and observations into anomalies. As we have seen, before the 1960s, statistical records of stock prices were monthly data; the CRSP then began collecting daily data. This changed the manner in which observations were treated. In 1978, just a few years after mainstream financial economics created the efficient-market hypothesis, Jensen pointed out that\n\nin a manner remarkably similar to that described by Thomas Kuhn in his book, The Structure of Scientific Revolutions, we seem to be entering a stage where widely scattered and as yet incohesive evidence is arising which seems to be inconsistent with the theory. As better data become available (e.g., daily stock-price data) and as our econometric sophistication increases, we are beginning to find inconsistencies that our cruder data and techniques missed in the past. It is evidence which we will not be able to ignore. (Jensen Reference Jensen1978, p. 95).\n\nEconophysics emerged in the wake of similar developments: the creation of new data (intraday data) and the new mathematical tools (truncated Lévy processes in particular) allowed more accurate observation and then transformed several discrepancies into anomalies. Consequently, econophysicists highlight the existence of a number of empirical facts that are not explained by mainstream financial economics and constitute anomalies. In view of the fact that prices on financial markets change more frequently and in a more orderly manner than is supposed by the Gaussian framework on which financial economics was established, econophysicists use α-stable Lévy processes directly to describe the evolution of financial data. Because econophysicists had their own theoretical goals (making Lévy processes compatible with a physically plausible approach), they developed a statistical framework outside the traditional approach. Doing so allows them to address a number of empirical facts that the traditional approach of financial economists cannot explain because it uses a Gaussian framework.Footnote 46\n\nThe main empirical facts to which econophysics proposes answers where financial economics is unable to are “fat tails,” “volatility persistence,” and “volatility clustering.”\n\nIn the 1960s, during the creation of mainstream financial economics, Mandelbrot (Reference Mandelbrot1963, Reference Mandelbrot, Massarik and Ratoosh1965) and Fama (Reference Fama1965a) drew attention to the high number of extreme events in finance and, hence, to the leptokurticity of empirical distributions, which have fat tails. At the time, these authors proposed describing empirical distributions with stable Lévy processes. The distributions associated with stable Lévy processes are approximately bell-shaped but they assign greater (than Gaussian) probability to events in the center and at the ends of the tails. However, in 1965, Fama (Reference Fama1965a, p. 416) pointed out that the infinite variance of stable Lévy processes is meaningless in financial economics, since this statistical parameter is associated with the concept of risk, and deplored the fact that no computational definition yet existed for evaluating this parameter. Financial economists did not have the statistical tools that would allow them to describe empirical distributions with a stable Lévy framework. Another reason why fat tails had not been studied lies in the difficulty of clearly identifying statistical processes with weekly or daily data (Mitzenmacher Reference Mitzenmacher2004). In this respect, the availability of intraday data since the 1990s has favored the identification of stable Lévy processes and has subsequently stimulated research into these processes. Since the 1990s, new statistical tools have been developed with the aim of creating stable Lévy processes with finite variance. Econophysical works are directly in line with these new statistical tools.\n\nThe second main empirical fact to which econophysics proposes answers is volatility persistence.Footnote 47 In the 1970s, when financial economists created their theoretical framework, they did not have the econometrical tools that could help authors to identify a persistence of volatility. According to this framework, which is based on the Gaussian distribution, there is no memory between stock-market returns. Starting from the 1980s, econometrical tools have been proposed, with the development of ARCH family models (Engle Reference Engle1982) demonstrating that volatility has slowly decaying autocorrelations showing a dependency between stock-market returns. Later, Schwert (Reference Schwert1989) showed this persistence of volatility in a different statistical context.\n\nThe last main empirical fact is volatility clustering.Footnote 48 In a Gaussian framework, one could expect to see a very uniform time distribution of large and small fluctuations. In 1981, an original use of a martingale model led Shiller to observe several periods of large fluctuations and periods of small fluctuations that are not consistent with the Gaussian framework. In other words, periods of intense fluctuations and low fluctuations tend to cluster together (Shiller Reference Shiller1981b, Reference Shiller1981a). Shiller’s works generated much debate and favored the emergence of new fields calling neoclassical finance into question—behavioral finance, for example (Schinckus Reference Schinckus2009). Econophysicists’ tools make it possible to put forward answers to this empirical fact.\n\nTo answer these anomalies, econophysicists base their analysis on Lévy processes that are better adapted to the empirical data available today. Lévy’s α-stable regimes are processes whose accretions are independent and stationary,Footnote 49 and follow an α-stable law type Pr{X > x} = x−α in which it is possible to observe constancy of the parameter α. In their Paretian form, these regimes have α < 2, and in these cases, it can be shown that variance is infinite.Footnote 50 As Belkacem (Reference Belkacem1996, p. 40) emphasized, “from a practical point of view, stable distributions are able to explain the thick distribution tails observed in empirical distributions of asset profitability rates.” A similar argument is found in the writings of Tankov (Reference Tankov2004, p. 13), who added that these processes are particularly interesting in financial economics because they allow discontinuities in the evolution of asset returns to be taken into account. This means that these processes are candidates for explaining the leptokurticity of financial data.\n\nThe analysis of these empirical facts proposed by econophysicists is essentially empirical itself, and points to econophysics’ predictive capacity. Above all, the solutions allow a better statistical understanding of the phenomenon of financial data variability. As pointed out by Shiller (Reference Shiller1981b), this phenomenon has been underestimated by neoclassical finance’s theoretical framework.\n\nThe integration of empirical facts unexplained by financial economics into the (more generalized) theoretical framework developed by econophysicists is, thus, an argument in favor of a shift in mainstream financial theory.\n\nA More General and More Complete Probabilistic Framework\n\nEconophysics’ second strong point in its bid to become the dominant approach is the use of statistical models that generalize those used in financial economics.\n\nFinancial economists mainly use the Gaussian framework in order to characterize financial uncertainty. Four reasons can be cited to explain the success of the Gaussian framework: the historical development of financial economics (Jovanovic Reference Jovanovic2008, Jovanovic et al. Reference Jovanovic and Schinckus2013),Footnote 51 simplicity (only two parameters are needed to describe data), the notion of normality (which can refer to the key concept of economic equilibrium), and, above all, statistical justification, which refers to one of the most fundamental theoremsFootnote 52: the central-limit theorem (CLT), which states that the sum\n\n$${\\rm{Z}}_{\\rm{n}} \\equiv \\sum\\limits_{{\\rm{i}} &#x003D; {\\rm{1}}}^{\\rm{n}} {{\\rm{x}}_{\\rm{i}} } $$\n\nof n stochastic variables x that are statistically independent, identically distributed,Footnote 53 and with a finite variance converges when n → ∞ to a Gaussian stochastic process (Feller Reference Feller1971).\n\nFor econophysicists, the Gaussian framework is the first step towards describing uncertainty in science. This first step can be generalized to a savage uncertainty “without normality” (Mandelbrot Reference Mandelbrot1997, p. 66). This generalization is based, on one hand, on Lévy’s work (1924) on random processes, and, on the other hand, on the generalized central-limit theorem developed by Gnedenko and Kolmogorov (Reference Gnedenko, Vladimirovich and Kolmogorov1954). In accordance with this generalization, the sum of random variables following Lévy laws, distributed independently and identically, converge towards the stable Lévy law having the same parameters. This generalization of the central-limit theorem justifies and provides a statistical foundation for the use of Lévy laws to characterize complex phenomena.Footnote 54 Lévy’s work proposed a generalization of several known distribution laws in the form of a family of random variable distributions notated as ${\\rm{S}}_{\\alpha {\\rm{,}}\\beta } (\\mu ,\\gamma )$.Footnote 55 By basing their approach on Lévy processes, econophysicists propose models that encompass all stochastic processes, such as Gaussian processes and Poisson processes.\n\nBy making use of stable Lévy processes, econophysics offers a more general theoretical framework than financial economics, which uses Gaussian distribution.Footnote 56 From this perspective, the models of financial economics are merely particular cases,Footnote 57 and the statistical tools used by econophysicists make possible technical integration of the Gaussian statistical framework on which mainstream financial economics is based. As a generalization of the Gaussian framework, the α-stable framework retains the fundamental properties (fractality and auto-affinity) of the Gaussian framework. There is, thus, mathematical continuity in the models used by financial economists and those used by econophysicists. This continuity had been advanced in the 1960s by Mandelbrot and Fama at the very beginning of the creation of mainstream financial economics. This attempt was not developed because of the theoretical meaninglessness of the hypothesis of infinite variance. Several paths have been explored by financial economists since the 1970s but none dropped the Gaussian framework (Jovanovic et al. Reference Jovanovic and Schinckus2013).Footnote 58 Today, the evolution of mathematics developed by econophysicists allows the development of a Markowitz portfolio theory, a generalized CAPM in an α-stable framework (Belkacem Reference Belkacem1996, Tankov Reference Tankov2004), and the development of a Black and Scholes option-pricing model in an α-stable framework (Huang and Wu Reference Huang and Wu2004). Such a characteristic would also make it possible to preserve the concept of market efficiency, even if new risk-analysis parameters were to be introduced. As Fama pointed out (Fama Reference Fama1965c), the concept of efficiency did not necessarily imply the Gaussianity of accretions.\n\nThe generalization used today by econophysicists is, thus, a strong point. Moreover, it renders the first attempts of financial economists such as Fama (Reference Fama1963) and Mandelbrot (Reference Mandelbrot, Massarik and Ratoosh1965) viable. These first attempts have been enhanced by the purely mathematical framework developed by Harrison and Pliska (Reference Harrison and Pliska1981). Indeed, by proposing a non-economic model in finance, Harrison and Pliska provided an entry point for an evolution of modern finance less based on economics. As a result, from a mathematical viewpoint, the models developed by econophysicists continue from where those developed by financial economists left off. The challenge for the future of econophysics will be to show that the models proposed by econophysicists can be integrated into this mathematical framework developed by Harrison and Pliska.Footnote 59\n\nThe Virtual Absence of Discussion between Financial Economists and Econophysicists\n\nAlthough econophysicists explicitly position themselves in relation to financial economics, it has to be noted that the response of economists to their criticisms has been, for all intents and purposes, non-existent. Some economists have pointed out the limitations of the econophysics approach (Gallegati et al. Reference Gallegati and Keen2006),Footnote 61 provoking a virulent response from one of the leading lights of econophysics (McCauley Reference McCauley2006). Aside from this brief exchange of communication, real theoretical debate between econophysicists and economists has so far seemed difficult. The sometimes severe criticisms made by econophysicists of the models and hypotheses used in financial economics have apparently failed to convince economists of the need to engage in theoretical discussions. One feature in particular explains the problem: econophysics’ framework is not directly compatible with that of financial economics, and econophysicists do not take this point into account. Indeed, because econophysicists’ focus is on mathematical development, very few of them have attempted to make their models compatible with the framework and hypotheses of financial economics.Footnote 62 One who has made the attempt to connect econophysics with financial economics is Bouchaud—see, for instance, Bouchaud and Potters (Reference Bouchaud and Potters2003).\n\nAs a result, the leading economics and finance journals make very few references to the work of econophysicists. Gingras and Schinckus (Reference Gingras and Schinckus2012) looked at the ten leading authors in econophysics, between 2002 and 2008, and found 401 references to these authors in the main economics and finance journals as against 2506 in the main physics journals. This disparity highlights the fact that econophysics is not considered the central issue in economic journals.Footnote 63\n\nOne might assume that, rather than economists “rejecting” econophysicists, it is more a case of the latter’s having developed their models outside the financial economics’ framework, and, hence, considering that they had no need to publish in mainstream economic journals or adapt their methodology and viewpoints to those of financial economists. With the evolution of the two disciplines as analyzed in the first part of this paper, this is no longer the case. First, we have shown that econophysicists have created new journals (Quantitative Finance and JEIC) in economics (and not in physics) in order to reach financial economists and an audience outside econophysics. Second, since econophysicists work on the same phenomena as economists, we should expect them to attempt publication in economics journals. To test this hypothesis, we conducted an informal survey, sending a questionnaire to twenty-seven leading econophysicists (included as source authors in our analysis) about the degree of openness of economics journals to econophysicists. To the question “Have you submitted a paper to a ranked journal in economics?”, a large majority of authors replied “yes.” When authors were asked to give the main reasonsFootnote 64 why their paper was rejected, they replied that referees in economic journals often have difficulties with the topic or/and the method used in their paper. Although based on a small sample (but including the central figures of econophysics), these results strongly suggest that economic journals are indeed reluctant to publish papers dedicated to econophysics. The lack of discussion between the two approaches could explain this situation, because no bridges are built between them.\n\nNevertheless, in 2008, the Journal of Economic Dynamics and Control published a special issue entitled Applications of Statistical Physics in Economics and Finance. Doyne Farmer and Thomas LuxFootnote 65 were guest editors for this special issue, articles for which were written by economists and physicists. This special issue aimed to “overcome the lack of communication between economists and econophysicists” (Farmer and Lux Reference Farmer and Lux2008, p. 3). As these authors pointed out in their editorial, there is “a lack of communication between physicists and economists. Physicists are perhaps the only group of scientific professionals who are even more arrogant than economists, and in many cases the arrogance and emotions of both sides have been strongly on display” (Farmer and Lux Reference Farmer and Lux2008, p. 3). In order to overcome the gap between the two camps, this special issue published twelve articles dedicated to econophysics, written by authors from the fields of economics and physics. Despite this first attempt at dialogue, debate has still not yet really begun.\n\nAmong the few economics journals to publish articles about econophysics is Journal of Economic Behavior & Organization, which recently broadened its editorial line to encompass the problems of complexity in economics (and consequently published several studies dedicated to econophysics). Other journals officially indexed under economics that regularly publish articles on econophysics include Quantitative Finance, created in 2002, and Journal of Economic Interaction and Coordination, created in 2006. The creation of these two journals, both of which have an editorial team made up largely of econophysicists, appears to be a strategy for the dissemination of econophysics, the aim being to make the approach better known to an audience of economists (Gingras et al. Reference Gingras and Schinckus2012).\n\nThe lack of dialogue can also be connected with the closed nature of economics.Footnote 66 Whitley (Reference Whitley and Samuels1986b) portrayed economics as a “reputationally controlled work organization” characterized by a strong and monolithic standardization of research. He explained that economics is a “partitioned bureaucracy” whose segmentation into several subfields allows it to marginalize all anomalies or empirical contradictions of the mainstream. Economics appears as a conservative novelty-producing system since it rewards intellectual innovation only if it is directly in line with the dominant research. All new fields that are not in accordance with the scientific standards used by the mainstream are simply ignored. In this perspective, the conceptual basis of econophysics could come only from outside the field of economics and be promoted by physicists who saw that the kind of distribution behind economic and financial phenomena, which are a collective response of the interactions of a large number of agents, are analogous to the distributions observed in condensed-matter physics as the result of the collective interactions of a large number of atoms. Starting from that analogy, they applied the methods of statistical mechanics, which explain the emergence of these distributions, to the case of economic and financial behavior. Such a move radically transforms the understanding of economics, as the usual Gaussian framework is replaced by a Lévy framework whose statistical properties are very different and are not necessarily consistent with the conceptual foundation of mainstream economics, based on equilibrium. Moreover, while economic theory is based on an atomistic reductionism in which reality must be explained in terms of rational representative agents, econophysics focuses on the interactions that give rise to complex phenomena that can be described through interactions between their parts.Footnote 67 These conceptual differences, coupled with the difference in disciplinary training between economists and econophysicists, have contributed to the development of econophysics as a separate scientific culture whose roots stayed in physics instead of developing out of economics, like other new specialties such as behavioral finance and experimental economics.\n\nThere is, thus, a dynamic of repulsion between economics and econophysics: as Whitley observed, “economics has a strong hierarchy of journals” (Whitley Reference Whitley and Samuels1986b, p. 192), and researchers who do not conform to the dominant standards are bound to publish outside that core and, therefore, be seen as irrelevant to the core, and consequently forced to publish in new journals not recognized by the mainstream of the discipline. Whereas this tendency could have given rise to a new speciality among the social sciences, the fact that the tools of econophysics were imported from physics, and that econophysics was prepared to accept the modeling of social phenomena as legitimate—thus enlarging its scope and possible job market—brought econophysics under the wing of physics as it first grew by using existing physics journals for publication and physics departments as training ground for the new breed of “econophysicists” (Gingras et al. Reference Gingras and Schinckus2012). We are seeing the emergence of a veritable scientific community independent of financial economics with its own academic courses, its own symposiums, and journals that are recognized in the field of physics.\n\nIn Whitley’s view, the lack of openness displayed by economics is designed to preserve the dominant theoretical framework and to marginalize anomalies that are likely to challenge the discipline: “as long as the theoretical establishment is able to dismiss ‘anomalies’ and difficulties as peripheral and the province of ‘applied’ subfields and yet retain control of the assessment of research competence in all areas, fundamental change seems improbable” (Whitley Reference Whitley and Samuels1986a, p. 204). This closed attitude in economics considerably restricts the scope for econophysics to win over financial economists. Moreover, physics shares the same closed-mindedness: according to the Science and Engineering Indicators (2000, p. 103, table 6–54), economics is the most hermetic field of the social sciences, with more than 87% of intra-disciplinary references compared to 50% in sociology. This is even more self-contained than physics, which cite physics journals in about 80% of their references. These data are consistent with Whitley’s (Reference Whitley and Samuels1986b) characterization of economics as a “partitioned bureaucracy” with a strong control over its theoretical core. Consequently, econophysicists have developed a closed attitude about financial economics.\n\nThe Refusal of Econophysicists to Incorporate the Framework of Financial Economics\n\nAs we have explained, from a statistical point of view, econophysics makes it possible to generalize the models used by the proponents of financial economics. However, up to now, most econophysicists have rejected the framework of financial economics because they consider it “too axiomatic and formal to deal with complex systems” (Challet, Marsili, et al. Reference Challet and Marsili2005, p. 14). Even some economists (Keen Reference Keen2003, p. 110) claim that all the key concepts (utility, perfect rationality, perfect competition, etc.) used in financial economics are “nonsense.” They are unobservable terms without an empirical base. Indeed, despite the fact that experiments exist in economics (Holt and Davis Reference Holt and Davis2005), all key concepts of economics cannot be directly confirmed because, according to econophysicists, these concepts result from apriorism. In this perspective, the key notions of economics are considered as “empirically flawed” (Keen Reference Keen2003, p. 109). For instance, the existence of equilibrium, which is a keystone of financial economics, has no foundation for econophysicists. In econophysics, equilibrium is rather considered as a potential state of the system because “there is no empirical evidence for equilibrium” seen as a final state of the system (McCauley Reference McCauley2004, p. 6). For econophycisists, economic equilibrium appears as an a priori beliefFootnote 68 that provides a “standardized approach and a standardized language in which to explain each conclusion” (Farmer and Geanakoplos Reference Farmer and Geanakoplos2009, p. 17).\n\nFrom a purely econophysics perspective, the financial market, like physical bodies, moves through different “phases,” which likewise display chaotic or coherent states. The evolution of phases and phase changes are, therefore, represented by a trajectory in this space. In accordance with chaos theory, physicists observe that, if one waits long enough, systems of this type tend to move through states that are neighboring or comparable to those they have been through in the past. In phase-diagram terms, the system is said to tend towards what is known as a “strange attractor.” This strange attractor cannot be assimilated into an equilibrium in the strict meaning of the term; rather, it is a geometric zone through which the system passes regularly.Footnote 69 The concept of strange attractor, taken directly from chaos theory, neatly sums up the idea that econophysicists have of the concept of equilibrium—which, in general, is explicitly rejected. “There is no empirical proof of the existence of a stable equilibrium,” explains McCauley (Reference McCauley2004, p. 6). In his view, the importance placed on this notion is more the result of an ideology or a belief than of an observation of reality (McCauley Reference McCauley2004, p. 295). He goes so far as to use the idea of non-equilibrium to distinguish finance theory from economics:\n\nStandard economic theory and standard finance theory have entirely different origins and show very little, if any, theoretical overlap. The former, with no empirical basis for its postulates, is based on idea of equilibrium, whereas finance theory is motivated by, and deals from the start with, empirical data and modeling via nonequilibrium stochastic dynamics. (McCauley Reference McCauley2004, p. 6)\n\nMandelbrot (Reference Mandelbrot2005, p. 143) adds:\n\nLet us return to fluctuations in finance; where do we see an economic equilibrium that is the equivalent of normal thermodynamic equilibrium? I quickly developed the feeling that the notion of economic equilibrium is devoid of content and that, to describe price variation, it will not be sufficient to modify benign chance by incorporating new details.\n\nMcCauley (Reference McCauley2004, p. 78) explains that the concept of equilibrium used in economics is of Newtonian inspiration (classical mechanics), whereas the idea of non-equilibrium owes more to the logics of statistical mechanics. From this point of view, investors are particles with complex and heterogeneous behavior, operating in a system (the market) whose macroscopic state can be characterized statistically (by Lévy laws) by a set of values (asset prices) that are transitory (non-equilibrium). “This low-level complexity (which is in a way microscopic) [at the individual level] can, under certain conditions, cause surprising and not disorderly effects at the macroscopic (collective) level” (Brandouy Reference Brandouy, Bourghelle, Brandouy, Gillet and Orléan2005, p. 122). This macroscopic perception of non-equilibrium as the sole explanation of the system’s microscopic states is directly inspired by thermodynamic modelsFootnote 70 and not by economics. This difference between economics and physics is also highlighted by Ruelle (Reference Ruelle1991, p. 113), and suggests that the notion of non-equilibrium could prove highly useful for the study of certain economic phenomena. Ball (Reference Ball2006, p. 687) adds that “equilibrium is the heart of the dominant economics whereas most models taken from econophysics are explicitly based on the concept of nonequilibrium.” The rejection of the theory and hypotheses of economics does not facilitate theoretical exchanges, and contributes to the absence of debate alluded to in our previous section. The studies put forward by econophysicists are not presented as improvements but rather as new models to replace current economic models purely and simply. Economists, for their part, prefer to remain aloof from this “methodological imperialism” by publishing no (or few) articles on econophysics (Gingras et al. Reference Gingras and Schinckus2012).\n\nThis rejection of the theoretical framework of financial economics leads econophysicists to discard the discipline’s main concepts and theories, as McCauley points out: “Econophysicists are safer to ignore the lessons taught in standard economic texts (both micro-macro) than to learn the economists’ production ideas and take them seriously” (2006, p. 608)."
    }
}
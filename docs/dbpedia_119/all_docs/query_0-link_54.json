{
    "id": "dbpedia_119_0",
    "rank": 54,
    "data": {
        "url": "https://necsi.edu/iccs-2018-abstracts",
        "read_more_link": "",
        "language": "en",
        "title": "ICCS 2018 Abstracts — New England Complex Systems Institute",
        "top_image": "http://static1.squarespace.com/static/5b68a4e4a2772c2a206180a1/t/5dd6d73b295b8a5de3fda191/1716920068415/LOGOsocial.png?format=1500w",
        "meta_img": "http://static1.squarespace.com/static/5b68a4e4a2772c2a206180a1/t/5dd6d73b295b8a5de3fda191/1716920068415/LOGOsocial.png?format=1500w",
        "images": [
            "https://images.squarespace-cdn.com/content/v1/5b68a4e4a2772c2a206180a1/1549577638098-EHHENEFEY7EL5DBDR39Y/Screen+Shot+2019-02-07+at+3.57.17+PM.png",
            "https://images.squarespace-cdn.com/content/v1/5b68a4e4a2772c2a206180a1/1540579702114-27EC3EMEG2EB8PTPTRAN/icons_2-01.png",
            "https://images.squarespace-cdn.com/content/v1/5b68a4e4a2772c2a206180a1/1540579702114-27EC3EMEG2EB8PTPTRAN/icons_2-01.png",
            "https://images.squarespace-cdn.com/content/v1/5b68a4e4a2772c2a206180a1/1540579803720-9OC8NHE52D0BT1PLFJ9C/tablet.png",
            "https://images.squarespace-cdn.com/content/v1/5b68a4e4a2772c2a206180a1/1540579803720-9OC8NHE52D0BT1PLFJ9C/tablet.png",
            "https://images.squarespace-cdn.com/content/v1/5b68a4e4a2772c2a206180a1/1540579761788-M1J25DOAS1ODR6CPQLZT/engage.png",
            "https://images.squarespace-cdn.com/content/v1/5b68a4e4a2772c2a206180a1/1540579761788-M1J25DOAS1ODR6CPQLZT/engage.png",
            "https://images.squarespace-cdn.com/content/v1/5b68a4e4a2772c2a206180a1/1540579831495-MVP30Z1ML799532HAY9I/analytics.png",
            "https://images.squarespace-cdn.com/content/v1/5b68a4e4a2772c2a206180a1/1540579831495-MVP30Z1ML799532HAY9I/analytics.png",
            "https://images.squarespace-cdn.com/content/v1/5b68a4e4a2772c2a206180a1/1540579864257-TT2ZEOX5XD9AVD2NX1BW/icons_2-03.png",
            "https://images.squarespace-cdn.com/content/v1/5b68a4e4a2772c2a206180a1/1540579864257-TT2ZEOX5XD9AVD2NX1BW/icons_2-03.png",
            "https://images.squarespace-cdn.com/content/v1/5b68a4e4a2772c2a206180a1/1540579891926-XLW7G4HDJ4Y584BO0R1U/about.png",
            "https://images.squarespace-cdn.com/content/v1/5b68a4e4a2772c2a206180a1/1540579891926-XLW7G4HDJ4Y584BO0R1U/about.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "https://images.squarespace-cdn.com/content/v1/5b68a4e4a2772c2a206180a1/1538152872094-3G5GYNNRO3MKJLR2325F/favicon.ico",
        "meta_site_name": "New England Complex Systems Institute",
        "canonical_link": "https://necsi.edu/iccs-2018-abstracts",
        "text": "Recently, studies of active systems (e.g. bacteria) in confined geometries have attracted great interest, since they show extraordinary collective behavior [1,2,3,4]. While from a theoretical point of view, collective dynamics represents an interesting case of study, it can also serve as a solid infrastructure for developing new technologies [5,6,7,8].\n\nTherefore, developing computational methods capable of efficiently reproducing the dynamics of confined systems is fundamental.\n\nIn this context, we elaborate a discrete model to perform a parametric study of a confined active system, spanning a wide range of possible values of the characteristic quantities of the system, such as geometrical configurations or particle peculiarities. In this way, it is possible to associate these properties with the collective emergent dynamics.\n\nWe follow the dynamics of self-propelled active particles confined in a channel with single-file condition. The channel is represented by a 1D lattice, and active particles move within it with a constant velocity. We also account for run-and-tumble dynamics by considering a random reorientation of a particle with rate λ.\n\nIn particular, we consider a system consisting of two microchambers, containing a number of particles N, connected through a microchannel with a length L.\n\nThis geometrical configuration has been already studied in molecular dynamics simulations (MDSs) [1], showing an interesting self-sustained density oscillation of particles. However, since MDSs are computationally expensive, the above study refers only to a single geometrical configuration and a fixed set of values for N, λ and L. This limitation has been overcome in this work by using the discrete model approach, that is computationally advantageous, and allows the parametric study of the collective emergent behaviour for a wide range of values of N, λ and L, well reproducing all the results found in MDSs.\n\nWe take into account short-range interactions, namely the excluded volume effect and the relative pushes between adjacent particles, responsible for the formation of aggregated states. In fact, self-propelled particles generally interact through non-reflecting collisions, with a subsequent formation of clusters [9]. We also consider long-range interactions amongst particles in the same cluster, which leads to the rise of collective dynamics.\n\nWe find the density oscillations relying on the formation of long active clusters. These clusters must be long enough to allow the formation of long-lasting flows of particles in the channel.\n\nResults show that in channels with L < L* = 25l (where l is the size of the active particle), long-lasting fluxes of particles are not sustained and the oscillatory dynamics does not appear. We also find a threshold value of λ* = 7 x 10^-3, above which the random reorientation of the particles causes a rapid fragmentation of the cluster. Therefore, efficient tumbling prevents the emergence of periodicity.\n\nIt is also very interesting to note that the oscillatory behavior does not depend on the number of swimmers N. However, high values of N (N>300) increase the period of oscillation, due to the increasing number of swimmers in the chambers, which reduces their motility.\n\nThese results can be the starting point for designing microstructured-devices based on the collective dynamics of living organisms. These devices would be capable of, for instance, controlling bacteria diffusion or transport passive matter.\n\nReferences\n\n[1] Paoluzzi M., Di Leonardo R., Angelani L., Self-sustained density oscillations of swimming bacteria conned in microchambers, Phys. Rev. Lett. 115(18) 188303, 2015.\n\n[2] Wioland H., Woodhouse F. G., Dunkel J., Kessler J., Goldstein R., Confinement Stabilizes a Bacterial Suspension into a Spiral Vortex, Phys. Rev. Lett. 110, 268102, 2013.\n\n[3] Yaouen Fily Y., Baskaran ., Hagan M. F., Dynamics and density distribution of strongly conned noninteracting nonaligning self-propelled particles in a nonconvex boundary, Physical review E91 , 012125, 2015.\n\n[4] Paoluzzi M., Di Leonardo R., Cristina Marchetti M. C., Angelani L., Shape and displacement Fluctuations in Soft Vesicles Filled by Active Particles, Sci Rep. 2016; 6: 34146, 2016.\n\n[5] Costanzo A., Di Leonardo R., Ruocco G., Angelani L., Transport of self-propelling bacteria in micro-channel flow, J. Phys.: Condens. Matter 24 065101, 2012.\n\n[6] Di Leonardo R., Angelani L., Ruocco G., Iebba V., Conte M. P., Schippa S., De Angelis F Mecarini F., Di Fabrizio E., Bacterial ratchet motors, Proc. Natl Acad. Sci. USA 107 9541, 2010.\n\n[7] Sokolov A., Apodaca M. M., Grzybowski B. A., Aranson I. S., Swimming bacteria power microscopic gears, Proc. Natl Acad. Sci. USA 107 969, 2010.\n\n[8] Koumakis N., Lepore A., Maggi C., Di Leonardo R., Targeted delivery of colloids by swimming bacteria, Nature communications 4, 2588, 2013.\n\n[9] Locatelli M., Baldovin F., Orlandini E., Pierno M., Active Brownian particles escaping a channel in single le, Phys. Rev. E 91, 022109, 2015.\n\nWe present a method of endowing agents in an agent-based model with sophisticated cognitive capabilities and a naturally tunable level of intelligence. Often, agent-based models use random behavior or greedy algorithms for maximizing objectives (such as a predator always chasing after the closest prey). However, random behavior is too simplistic in many circumstances and greedy algorithms, as well as classic AI planning techniques, can be brittle in the context of the unpredictable and emergent situations in which agents may find themselves. Our method centers around representing agent cognition as an independently defined, but connected, agent-based model. To that end, we have implemented our method in the NetLogo agent-based modeling platform, using the recently released LevelSpace extension, which we developed to allow NetLogo models to interact with other NetLogo models.\n\nOur method works as follows: The modeler defines what actions an agent can take (e.g., turn left, turn right, go forward, eat, etc.), what the agent knows (e.g., what the agent can see), what the agent is trying to do (e.g., maximize food intake while staying alive), and how the agent thinks the world works via a cognitive model defined by a separate agent-based model. Similar to Monte Carlo tree search methods used in game AI, during each tick of the simulation each agent runs a settable number of micro-simulations using its cognitive model, with initial conditions based on their surroundings, tracking what actions they take, and how well they meet their objectives as a consequence of those actions. The agent then selects an action based on the results of these micro-simulations. A significant upshot of this method is that it gives researchers several tunable parameters that precisely control agents’ “intelligence”, such as the number of micro-simulations to run and the length of each micro-simulation. Having such control over agents’ intelligence allows modelers to, for instance, naturally adjust agents’ cognitive capabilities based on what is reasonable for those agents, or have an evolvable “intelligence” parameter that directly correlates to the agents’ cognitive capabilities.\n\nAs an illustrative example, and to begin to understand how this type of cognition interacts with complex systems, we present a modification of a classic predator-prey model, in which the animals have been equipped with the cognitive faculties described above. Based on the Wolf-Sheep Predation model included with NetLogo, the model contains wolves, sheep, and grass. In the classic model, wolves and sheep move randomly and reproduce when they have sufficient energy. Sheep eat grass and wolves eat sheep. Grass grows back at a set rate. In our extension, we define a simplified version of the model that represents how the wolves and sheep think the world works. The cognitive model has similar mechanics as the Wolf-Sheep Predation model, but with two key simplifications: 1) the subject agent’s field of vision defines what other agents are included in the cognitive model, and similarly 2) the mechanics that are reasonable for the agent to know define what mechanics are in effect (e.g., the internal states of other agents are unknown to the subject agent). We then use NetLogo LevelSpace to equip each of the wolves and sheep with this cognitive model, which they then use to perform short-term simulations of their surroundings and select actions that lead to the best outcomes. This cognitive model naturally allows sheep, for instance, to realize that if they move towards a particular patch of grass, a wolf might eat them or another sheep may arrive there first. However, the cognitive model also automatically adapts to special circumstances that the sheep may find itself in. For instance, if the sheep is about to starve to death, they will be more willing to risk being eaten by a wolf if it means getting food. The naturally adaptive capabilities and emergent decision making sets this agent cognition method apart from traditional agent AI.\n\nTo understand the impact of this cognitive model on the dynamics of the predator-prey model, we performed experiments in which the number and length of the micro-simulations that the sheep use is varied, while the wolves are left with their original random behavior. We find that only two short micro-simulations of the sheep’s cognitive model are needed to dramatically improve the performance of the sheep, as measured by their mean population when the system reaches periodicity. More broadly, increasing the number of simulations monotonically improves the sheep’s performance as a group, tending towards an asymptote at the system’s carrying capacity. Simulation length, however, achieves peak performance at a relatively small number of ticks; when the simulations are too long, sheep performance drops. Thus, we find that giving the agents even limited cognitive capabilities results in dramatic changes to the systems long-term behavior.\n\nThe impact sector is the sector that uses business to achieve environmental and social positive impact in a sustainable manner. There are several popular terms in this sector that you may have come across: impact investing, social enterprise, double-bottom-line, people-planet-profit (the 3 Ps), purpose-driven business, etc. These terms, and this sector, have been receiving a lot of air time in recent years. And, rightfully so.\n\nIn keeping with this conference’s theme, we will take a more first-principles approach where we will look at how “impact”creates impact.\n\nThe impact sector has two aspects to it: the attempt to understand impact, and the attempt to design for desired impact. These are two aspects of the same coin of course -- but quite distinct in terms of the skillsets needed. One is analysis, the other is synthesis. Science is analysis, design is synthesis. Unfortunately, traditional science and engineering education has not focused on synthesis.\n\nIn many ways, the impact sector is a leader in the application of complex systems theory. In fact, the impact sector is born from the realization that traditional models of analysis, business and implementation have not solved the larger problems as expected, and a more holistic and comprehensive approach was needed.\n\nOur (current) mechanistic and reductionist understanding of the universe has led us to linear thinking, simple-cause-and-effect paradigms, negation of context and to silo-ed solutions. We see this thinking applied everywhere: in medicine, in government, in corporations. But, climate change, poverty, illness -- none of these can be attributed to a single cause.\n\nThe nature of change (or, evolution) is of the individual system experiencing change as a result of its internal processes, and, as a result of its selective responses to external stimuli. Thus the individual system expresses itself, as itself, within its environment thereby effecting change.\n\nConsider what that means for “scaling”. Science has taught us to test a solution in a “lab”, if it works, then to scale it. Scaling not only assumes that the context is constant, but it also negates the role of the individual system (perhaps a person). Instead of scaling, we must connect. Instead of applying a tested solution across individual systems, we must have the individual systems apply the solution.\n\nThe story of complex systems theory is that it is unifying and universal. Patterns of behavior that appear in chemical reactions also appear in cognition. Or, those in financial systems, also appear in social systems. The underlying theme of the universe is process, not things.\n\nBut then, how do we intervene when everything is connected to everything else? And in so doing, will we break more than we create? How do we even create within a multi-causal structure?\n\nThose are the challenges the impact sector is innovating within and innovating for.\n\nThis submission will bring a series of examples on how those challenges are being met in various industries.\n\nArtificial Intelligence (AI) has grown dramatically and become more and more institutionalized in the 21st Century. The evolution in AI has advanced the development of human society in our own time, with dramatic revolutions shaped by both theories and techniques. However, the fast-moving, complexity and dynamic of the network make AI a difficult field to be well-understood. To fill this gap, relying on the ability of the complex network topologies, we study the evolution of AI in the 21st Century from the perspective of network modeling and analysis according to following dimensions:\n\nThe evolvement of AI based on the volume of publications over time;\n\nAnalysis of impact and citation pattern to characterize the referencing behavior dynamics;\n\nIdentifying impactful papers/researchers/institutions and exploring their characteristics to quantify the milestone and landmark;\n\nThe inner structure by investigating topics evolution and interaction.\n\nOur study is performed on a large-scale scholarly dataset which consists of 58,447 publications and 1,206,478 citations spanning from 2000 to 2015. The publication metadata is obtained from Microsoft Academic Graph, which contains six entity types of scholarly data including authors, papers, institutions, journals, conferences, and the field of study. To construct and analyze the citation network of AI, we select the articles published in the list of top journals and conferences of China Computer Federation recommended international academic publications and Computing Research and Education Association of Australasia under the category “Artificial Intelligence”. Based on the analysis, the main findings are:\n\nIn the context of AI's growth, we discover that the number of publications, citations as well as the length of the author list has been increasing over the past 16 years. It suggests that the collaboration in the field of AI is becoming more and more common.\n\nFrom the perspective of reference behavior, the decrease in self-references including author self-references and journal/conference self-references indicates the science of AI is becoming more open-minded and more widely sharing. The development of techniques and tools (evidenced by the citing behavior of new literature) in AI leads the area getting diverse.\n\nWe use the average number of citations per paper of each papers/authors/institutions as an indicator to evaluate their importance. Those influential entities are consistent with our intuitions.\n\nFinally, we explore the inner structure of AI in the 21st Century. We identify the hot keywords and topics from the perspective of how they change with time. Some topics have attained “immortality” in this period such as computer vision, pattern recognition, feature extraction, etc. Furthermore, based on the co-presence of different topics and the citation relationship among them, we find interconnections and unveil the trend of development in this complex disciplinary.\n\nOverall, our findings demonstrate that AI is becoming more and more collaborative, diverse, and challenging during the first sixteen years of the 21st Century. These results not only explain the development of AI overtime, but also identify the important changes, with the ultimate goal of advancing the evolution of AI.\n\nBeliefs – Attitudes – Behavior as a Complex System\n\nby K. Raman\n\nWest Hartford, Connecticut, USA\n\nThe human mind and its workings, in individuals and groups, have been discussed in different ways by philosophers, sociologists, psychologists, and other thinkers.\n\nThis paper analyzes the dynamics of the interacting system of Beliefs, Attitudes and Behavior and related entities in the framework of current thinking about complex systems.\n\nAlthough a large amount of work has been done by social psychologists in this field over the last several decades, there are unresolved questions that merit further analysis.\n\nAlso, in today’s society, new questions are raised by changes in existing political systems, changes in the commercial world, the new ways in which information is presented to people by the Media, and the advent of the Internet, which has radically changed the modes of communication.\n\nIn the political arena, questions have been raised about the fair working and efficacy of voting systems, which determine the results of major elections.\n\nIn the commercial world, advertising using new media techniques has a major effect on attitudes and beliefs which determine consumer decisions on a large scale.\n\nThe dependence on Mass Media for news can strongly influence decision-making of all types.\n\nA fresh Systems perspective on what governs beliefs, attitudes and behavior can help answer questions of importance. Examining this field using the viewpoints suggested by the body of work on Complex Systems may shed light on the mechanisms involved in Beliefs, Attitudes and Behavior, and how they can be influenced. This paper is an attempt toward this.\n\nWe point out the basic components of the Belief-Attitude-Behavior interacting system, the working of each subsystem, and the causal relations among the elements of the system, in particular the mutual influences of Attitudes, Behavior and Beliefs. We examine the feedback effects that occur in this system, and their role in determining the characteristics of the system.\n\nWe point out the different types of inputs to this open system, and their possible effects on the dynamics of the system.\n\nWe discuss the various factors involved in the formation of Beliefs, Attitudes, and Behavior in modern society.\n\nAnd the diverse mechanisms for changes in these entities, and to what extent they can be controlled or influenced.\n\nWe discuss the important roles of Affect and Emotion, and Information Processing, in the working of the system. And the role of Communication in persuasion and changing attitudes and beliefs.\n\nWe present the idea of Attitudes as an emergent phenomenon.\n\nWe outline summary examples of Political, Business, Religious and Daily-Life Belief and Attitude Systems, especially in the context of modern societies.\n\nWe discuss the effects of modern technology and communication, including the Internet, on the structure and evolution of these Systems.\n\n- Growing complexity in product engineering in large scale OEM engineering operations –\n\nThe car industry had become massive under public pressure, found guilty for polluting the environment, lying about the efficiency of their products and taking an active role for decreasing the wellbeing in inner cities. The irrational and harmful cry for safer, cleaner and more electric cars is matched with an unprecedented expectation of individualization and even more comfort in the cars, has massive impact on the industry.\n\nAdditional services and safety standards, like remote updating, autonomous driving and entertainment connectivity is taking its toll on the established engineering organizations, processes and the individual engineer. The number of hard ware variants to be offered to downstream productions processes, amount for a standard car, e.g. VW Golf easily in the range of 7 to 8 million. This number is exploding exponentially, if electronic and software variants are added to the equation.\n\nDevelopment processes are getting longer and longer, due to the need to implicitly buffer for time and cost overruns. The increase in costs cannot be absorbed by higher retail prices and are eroding the car company profits. Beyond an increase in overall cost, customers are having only limited patience to wait for their car or are expecting a competitive flow of new models. Car industry is a fashion industry.\n\nOne of the most prevailing symptoms of proliferating complexity is the baffling inability of the engineers to explain emerging behavior of the complex systems, being put together. The adoption of Systems Engineering and Model Based Systems Engineering had been identified as the silver bullet to tame internal complexity. Largely neglected had been the fact the complexity of the product need to be catered and absorbed through the complexity of an engineering organization. However to the hierarchical organization and a command-and-control management systems, despite being efficient in the past, render the whole undertaking ineffective and fragile.\n\nThe paper being presented will touch on this experiences, explain the challenges and the road to solution that are delivering significant gains in due date performance and superior engineering systems and products.\n\nAt the core of the program is the working principle of Ashby’s law of requisite variety. A massive takeaway for senior management was the recognition of the abyss between power of decision making and technical competence to do though.\n\nThe principle of recursion between independent levels of autonomous control reflect the basic building blocks of a central nervous systems. Whereas the lower level of recursion prevent the higher levels from complete overflow and the risk to follow the trap of human nature, ignoring the most obvious risks.\n\nIn order to create a language of control and to make dependencies on narrow local optimization of human nature less harmful, we went back to Stafford Beer and his viable systems model, which had been published in its first draft, after a country wide application in Chile under the presidency of Salvador Allende.\n\nThe institutionalization of an “Engineering Craftsmanship” are helping the leading engineers to reconnect to the wholeness of the engineering value chain, addressing redundancies under lifecycle criteria and erode information latency early in the process.\n\nSystems Engineering can now deliver on its hailed promises and human nature as factor of ingenuity and ignorance are not left to a Darwinian process.\n\nMicrosoft’s Satya Nadella recently spoke of “the world as a computer.” This discussion asks: What is the operating system of this computer? Who sets the rules? And how? These questions have taken on greater political and economic importance in international relations as global connectivity and reliance on information and communications technology increase. The discussion explores the various dimensions of the questions from a law and public policy perspective in order to invite contributions from across diverse fields to the understanding of the complex systems at play.\n\nThe world as a computer is conceivable because of our networked world, and the resulting information Big Bang. Moore’s Law on the doubling of processor power every two years may be reaching diminishing returns, but it is being compounded by (1) the multiplication of computers in billions of devices generating and using data, (2) increases in the capacity to store that data, (3) increased bandwidth enabling wider and faster data transmission, and (4) more powerful software to manage and analyze all that data, aided by machine learning.\n\nBecause information flows cross national borders, they strain established forms of nation-state control over the content of information and, to a lesser extent, over the means of transmission exercised within sovereign territories, and generate conflicts when states assert extraterritorial jurisdiction over these movements or act outside their territories. The national interests involved are various – national security, economic development, consumer protection, social control, geopolitical ambition, among others – and mechanisms and norms to address transnational concerns are evolving. This evolution is taking place horizontally on broad questions of “who governs the internet,” and vertically with regard to specific issues such as intellectual property or cyber-warfare.\n\nPrivacy/ data protection and cybersecurity are two notable areas of such conflicts. Privacy has been a source of conflict especially between the US and EU in particular. The EU regulates to what countries personal information can be transferred and, in the wake of the Snowden leaks on US intelligence surveillance, the Court of Justice of the European Union invalidated an agreement between the US and EU enabling data transfers. Negotiation of a new agreement involved in the EU review of US privacy laws and its laws and practices on government access to information. This process is repeating itself as the EU negotiates new bilateral agreements or reviews existing agreements with countries such as Japan, South Korea, Israel, and Canada (and, coming soon with Brexit, the UK). At bottom, such agreements are a sui generis form of trade agreement negotiated between governments, with unstructured consultation with a variety of stakeholders.\n\nIn cybersecurity, national responses are evolving, and international norms with them. In important respects, cybersecurity presents greater transnational challenges than privacy because of the threat actors that are able to operate outside their national borders and because of shared vulnerabilities across global networks. In turn, many countries and organizations (companies in particular) have common interests because they rely on the same systems, same software and hardware, and face the same vulnerabilities and threats.\n\nCybersecurity therefore presents a significant opportunity for collaboration and development of international norms across borders and across nongovernmental sectors. Such conversations are progressing episodically on various tracks – multilateral and bilateral government discussions and other discussions in a variety of public-private or nongovernmental forums. These operate in ways that are both complementary and conflicting.\n\nThe study of governance of transnational norms and policies has been largely the province of law and public policy specialists, with the exception of technical standards and protocols. My hypothesis is that understanding of the challenges in this arena can be helped by additional disciplines to understand the interaction of the numerous different systems involved – from mapping networks, to gauging the traffic that runs on these networks, to the social architecture of the various stakeholders in the decision making. This will not solve the difficult political problems presented, but may help to chart a path toward solutions.\n\nModeling and simulation is used in many fields to study interesting and complex problems, including in biology, sociology, psychology, computer science, and many more. For the results of a simulation model to be trusted, they must be validated. Validation in this case refers to the determination of whether the simulation model adequately represents the system being studied. For instance, a simulation of gossip propagation should correctly represent the social network among people, their tendency to share gossip, and how accurate that gossip tends to be over time. Only when the model accurately represents that real world system can “what if” questions be asked of the model, to answer questions that may be difficult to determine in the real world.\n\nMany techniques exist for validating simulation models, and have been discussed for a number of decades. Some techniques are readily available to modelers, such as in the animation abilities of NetLogo, whereas others require significant data such as is the case with `results validation’ where a simulation model’s output is compared with data from the real world. The maturity of a scientific field is often measured with the level of reuse achieved by the researchers and practitioners of that field. In the case of modeling and simulation, the reuse of a simulation model is directly dependent on how well the model matches the real world system or the system being studied. How well the model matches the studied system is closely tied to the simulation’s validation. However, we do not have a standard for discussing the level of validation of a simulation model. In fact, many papers on simulation models either do not discuss whether the model has been validated, or saves validation for later work.\n\nWe propose a framework for quantifying the amount of validation performed on a simulation model, such that it is possible to answer the question of how well the simulation matches the system being studied in a consistent way across all simulation models. Our framework provides guidelines on determining the structure, behavior, and data that should be validated, a mechanism for tracking successful validation, and a metric for calculating the level of confidence gained in the model via validation for agent-based and discrete-event simulation models. Additionally, we provide a web-based tool to aid in applying this framework to a model. This work provides a suggested template for discussing validation within simulation papers, which is currently lacking in the field. With this framework, we provide one aspect that is necessary to treat our published simulation models as trustworthy and re-usable.\n\nHate, extremist and supremacy groups come from different ideologies, but share a common theme of extreme rhetoric and behavior — and ultimately possible violent events. Such groups have been widely studied in the social sciences, but less so from the perspective of disciplines associated with Complex Systems. Here we attempt to address this gap by analyzing their behaviors on online platforms. Such online environments present a unique platform for the expression of radical beliefs, since the global nature of the Internet enables individuals to examine and interact with messages and beliefs that are in opposition to their daily lives. Yet it is unclear how far-right radical messaging gets transmitted to others via web forums. Inside online forum communities, the participation operates on a J-curve with the majority of respondents posting infrequently. Such patterns of use suggest there may be more noise produced in these communities, with frequent posters potentially generating content that blurs the perceived value of signals generated by infrequent posters. This has particular importance when examining radical messaging, as infrequent posters who espouse radical beliefs may be connected to the broader forum population via frequent posters who may amplify their messaging.\n\nIn this work, we carry out an analysis of the data from three online alt-right forums NSM, Tightrope and White News. We propose a time-directed network analysis in order to recognize the pathways of possible causality between postings and individuals, suing a classification of the posts depending on the numbers of post on days after and before. We identify the users according to their behavior and the post according to its ideology. We show that this identification helps to effectively track and even forecast events based on who posts, and/or on the ideology of the post.\n\nWe also compare the time-series of the web forums to the dataset of real-world terrorist events of Alt-Right nature. The behavior of the real-world terrorist attacks seems to have a short-term (up to two weeks) causality effect on the Internet, while behavior on the Internet seems to have an effect on the medium term (between two weeks and six months) on the terrorist attacks. Finally, in the long term (biyearly and yearly scale), the number of post on the Internet is highly correlated with the number of terrorist events perpetrated by Alt-Right groups. In short, this provides three important time scales where the dynamics in the extremist Internet forums seems to have an interplay with violent events in the real-world.\n\nFinally, we find common patterns between online violence promoters, particularly between Alt-Right Internet forums and pro-ISIS groups in the social network VK. We identify that in both situations the posting activity follows an approximate power-law with a scale parameter close to 2. They also exhibit similar boundaries in a Burstiness-Memory phase diagram.\n\nCollaborative reasoning - whether successful or unsuccessful - is a core facet of human society. In business, politics, and everyday life, individuals with varying opinions, experience, and information attempt to collaborate and make decisions. If the topic under consideration is factual, the reasoning process can be well-modeled under an explore/exploit framework where agents attempt to find a global optimum given local information [7, 8]. However, if the topic is value-laden, if agents hold their own subjective opinions of the solution space, existing models can neither explain nor predict the process of collaborative reasoning. Yet, a great deal of real-world reasoning relies upon agents’ normative beliefs. In the political realm, for example, a policy solution is only optimal if it results in outcomes an agent would qualify as “good.” Political polarization, in this sense, does not represent agents unable to map the solution space, but rather indicates agents who are unable to come to consensus regarding the contours of the solution space itself. Despite skepticism to the contrary, numerous empirical studies demonstrate that people are able to productively discuss value-laden matters [4, 6, 11], indicating a growing need to understand the conditions under which these conversations succeed.\n\nThis paper presents a novel framework for modeling the value-laden process of collaborative reasoning, drawing upon group problem-solving literature as well as work around cultural convergence [13, 2, 5]. We model human reasoning as a complex network where nodes represent beliefs and edges represent the logical connections between those beliefs. This model builds upon scholarship across numerous fields showing that cognitive processes as diverse as reasoning [1], arguing [14], remembering [3], and learning [12] are best modeled as ‘conceptual networks’ in which ideas are connected to other ideas. Through the process of collaborative reasoning, discussants move through their interconnected webs of belief, offering up related arguments and attempting to find disconnects in others’ reasoning. In this deliberative game of ‘giving and asking for reasons” [10], each player tries to map the others’ belief system, using their own conceptual network to translate the signals they receive from other players.\n\nTo model this process, we take the solution space to be an NK landscape, initiated as a weighted, signed network. This rough terrain represents the true connections and trade-offs related to a single policy question. We take the heaviest path through this network as the optimal policy solution; e.g. the best set of policies to efficiently reach a given outcome. Each agent is initiated with their own belief space, representing their personal interpretation of the policy space. Each time step t represents a single speech-act, as each agent either shares an opinion (weight of a single edge) or receives someone else’s opinion. When an agent receives an opinion, they must decide whether or not to incorporate this new information. Good faith discussants [9] should incorporate new information into their thinking if it seems reasonable given their existing knowledge. We model this by having an agent move towards a received opinion if there is a positive cosine similarity between the first eigenvector of the agent’s existing beliefs and the first eigenvector of the proposed beliefs.\n\nModeling this with small groups of deliberators, we find that if agents reason together in good faith, they can reach consensus and identify optimal solutions, even when their views are initially divergent. We measure the convergence of agents’ beliefs as the Frobenius distance between their respective belief networks, and we measure the success of the reasoning process as the difference between a group’s selected solution - the path which they believe is optimal - and the ‘true’ path which is optimal from the solution landscape. Interestingly, even when agents fail to converge to the ‘true’ solution space, they may still select good policy solutions – e.g. a deliberating group may ultimately select the right choice for the wrong reasons. Ultimately, this work helps us better understand the dynamic process of collaborative reasoning around value-laden topics.\n\nReferences\n\n[1] R. Axelrod. Structure of decision: The cognitive maps of political elites. Princeton university press, 1976.\n\n[2] R. Axelrod. The dissemination of culture: A model with local convergence and global polarization. Journal of conflict resolution, 41(2):203–226, 1997.\n\n[3] A. M. Collins and E. F. Loftus. A spreading-activation theory of semantic processing. Psychological review, 82(6):407, 1975.\n\n[4] J. Fishkin. Reviving deliberative democracy. In ”Democracy Gridlocked?” Colloquium. Royal Academy of Belgium, 2014.\n\n[5] N. E. Friedkin, A. V. Proskurnikov, R. Tempo, and S. E. Parsegov. Network science on belief system dynamics under logic constraints. Science, 354(6310):321–326, 2016.\n\n[6] K. R. Knobloch, J. Gastil, J. Reedy, and K. Cramer Walsh. Did they deliberate? applying an evaluative model of democratic deliberation to the oregon citizens’ initiative review. Journal of Applied Communication Research, 41(2):105–125, 2013. ISSN 0090-9882 1479-5752. doi: 10.1080/00909882.2012.760746.\n\n[7] D. Lazer and A. Friedman. The network structure of exploration and exploitation. Administrative Science Quarterly, 52(4):667–694, 2007.\n\n[8] W. Mason and D. J. Watts. Collaborative learning in networks. Proceedings of the National Academy of Sciences, 109(3):764–769, 2012.\n\n[9] H. Mercier and H. Landemore. Reasoning is for arguing: Understanding the successes and failures of deliberation. Political Psychology, 33(2):243–258, 2012.\n\n[10] M. A. Neblo. Deliberative Democracy Between Theory and Practice. Cambridge University Press, 2015.\n\n[11] M. A. Neblo, K. M. Esterling, R. P. Kennedy, D. M. Lazer, and A. E. Sokhey. Who wants to deliberateand why? American Political Science Review, 104(03):566–583, 2010.\n\n[12] R. J. Shavelson. Methods for examining representations of a subjectmatter structure in a student’s memory. Journal of Research in Science Teaching, 11(3):231–249, 1974.\n\n[13] E. H. Spicer. Persistent cultural systems. Science, 174(4011):795–800, 1971. ISSN 0036-8075\n\n[14] S. E. Toulmin. The uses of argument. Cambridge University Press, 1958.\n\nFake news – a new buzzword which has become recently popular in the language of politics can be perceived as a kind of symbol of a new phenomenon which is shaping sociopolitical reality of the modern world. Remaining for a while at that term, a question should be asked: what does it mean “fake news”? To what is it related – to the absolute truth, to the constructed truth of post-modern society? Remaining on the ground of moderate constructivism, it may be stated that the “fake news” is just another social construct. If so, the process of construction “fake news” should be studied – the actors, their interpretations and the processes of negotiating intersubjective meaning.\n\nThe most significant feature of the modern society is not the quantitative information overabundance understood as production and necessity of reception of measurable information (signals, impulses, etc.). but the need to assign meaning to that superfluous information (sensemaking). In this situation a conjecture is put before that it is only broadly defined complexity studies (as familiar with the field, I avoid I purposively avoid the term “complexity science”), which can be helpful in a better understanding of the modern society.\n\nThe following main conjecture will be presented and scrutinized. Under the impact of information overabundance and its consequences, it is not only knowledge but ignorance which has to be taken into account in studying modern social systems.\n\nThe paper is developed upon the following assumptions.\n\n1. All uses of the “utterance” complexity, should it be “hard” complexity based on mathematical modelling or various types of “soft” complexity built upon analogies and metaphors deriving from the “hard” complexity, or qualitative ideas of complexity, as those of Luhmann, reflect the ignorance of the observer (participant) (Mesjasz, 2010) (“It’s complex, so I am not able to comprehend it to a certain extent”). Therefore more attention should be paid to a deeper analysis of ignorance. A new interpretation of complexity of social systems relating to ignorance will be proposed.\n\n2. Two types of ignorance can be distinguished – negative – the lack of knowledge, whatever knowledge may mean, and positive ignorance which results from development of science. The more we know the better we are aware that we do not know and the better we are aware that we do not know that we do not know. Examples of that kind are known from the past and they are broadly discussed at present (Nicolaus of Cusa, 1985; Smithson, 1989; Ravetz, 1990, 1993; Proctor, 2008; Roberts, 2012).\n\n3. When the constructivist character of knowledge and ignorance is taken into account, the problem of “structure” and hierarchy of ignorance obtains a new sense. It is not sufficient to realize what is known and what is not known. It is also becoming important why something is not known – why I do not know that I do not know? It concerns the knowledge of characteristics upon which knowledge and ignorance are defined. This idea can be called the second-level ignorance and it is different from ignorance of ignorance (the unknown unknowns). The sense of the second-level ignorance can be described as follows. Reflections about knowledge lead to a multi-level hierarchy of recursion – I know that I know that I know....ad infinitum. It’s worthwhile to remind that ignorance occurs only at the first and second levels of reflexivity. It is not logical to state: I do not know that I do not know that I do not know (3 levels). Not knowing may thus have only two levels. At the first level of ignorance, I know some features of an object of my cognitive process and of the environment of that object which allow for declaring what I know and what I do not know about that object (ignorance arising from absence or incompleteness of knowledge). It is summarized by a sentence: I do not know that I do not know (first level). At the second level – I do not know precisely those characteristics of the object and methods of their identification, which allow for declaring that I know and I do not know about the object at the first level (ignorance about knowledge of sources and methods of gaining knowledge). Thus I try to answer to the question: Why I do not know that I do not know?\n\n4. A new approach to complexity of social systems should be based not on the question: “what do I (we) know?” but on the question: ”what and why I (we) do not know and I (we) must use the notion ‘complexity’?”\n\n5. A question is arising how many levels of reflexivity are necessary in studying knowledge and ignorance in social studies. In the second order cybernetics (Foerster, 1982), considering the role of observer and in modern sociology including the role of reflection (reflexivity), analysis usually ends at the second level – meta-level or double hermeneutics (Giddens, 1993). The higher levels of reflection about knowledge could be applicable in more advanced hermeneutical discourse. In the paper only two levels of reflexive knowledge and two levels of reflection upon ignorance are proposed.\n\nApplying the above assumptions about reinterpretation of complexity, a new universal concept, which can be called Complexity-and-Ignorance-Sensitive-Systems-Approach (incidentally it has a symbolic acronym CAISSA) is proposed. It may have multiple applications in studying complex social systems. When applied in modern management it can be called CAISM (Complexity-and-Ignorance-Sensitive-Management). Not Ignorance Management, as it was once proposed (Gray, 2003), since it is logically incorrect.\n\nThe proposed concept resulting from long lasting research on social complexity is not designed as another too far-reaching “theory of everything” in social studies. It is a modest attempt based upon cautiousness and self-criticism.\n\nThe main aim of the paper is to present an application of this new approach based upon complexity research in studying new security challenges in modern society affected by the information overabundance.\n\nThe following example concerning the role of social complexity and ignorance will be developed.\n\nFirst, the meaning of the terms “security of information society” and/or “security in information society” will be primarily explained. These concepts are not identical with security of IT systems or cyber-security, etc., but rather concern negative consequences of information overabundance. It will be shown how two-level self-reflexive knowledge and two-level ignorance lead to a situation, which are harmful for the specific social collectivities, e.g. ignorance about environmental threats, self-delusion before economic crises, etc. It will be shown that analysis of such situations and phenomena can be accomplished solely with the ideas drawn from complexity studies enhanced with a deeper understanding of the links between complexity and ignorance. Since the social systems are always affected by ignorance stemming from lack of information, cultural limitations – e.g. taboos, prohibitions of access to information purposive activities of limiting, distorting and creating false information, the model of two-level reflexive knowledge and two-level reflexive ignorance will show potential complexity of societal threats under the conditions of information overabundance. The case studies will embody patterns of intersubjective social construction of concepts of “real news” and “fake news” under the conditions of two-level knowledge and two-level ignorance. Distortions stemming from potential sources of ignorance will be described and scrutinized.\n\nPRELIMINARY BIBLIOGRAPHY\n\nFoerster, von H. 1982. Observing Systems. A Collection of Papers by Heinz von Foerster. Seaside, CA, 1982: Intersystems Publications.\n\nGiddens, A. 1993. New Rules of Sociological Method: A Positive Critique of Interpretative Sociologies, 2nd Edition. Stanford: Stanford University Press.\n\nGray, D. 2003. Wanted: Chief Ignorance Officer. Harvard Business Review, 81(11): 22-24.\n\nMesjasz, C., 2010. Complexity of Social Systems. Acta Physica Polonica A, 117(4): 706–715. http://przyrbwn.icm.edu.pl/APP/PDF/117/a117z468.pdf, 10 April 2012.\n\nNicolas of Cusa, 1985. On Learned Ignorance (De Docta Ignorantia). Books I, II, III. Minneapolis, MN: The Arthur J. Banning Press.\n\nProctor, R. N. 2008. Agnotology. A Missing Term to Describe the Cultural Production of Ignorance (and Its Study). In R. N. Proctor, & L. Schiebinger (Eds.). Agnotology: The Making and Unmaking of Ignorance: 1–35. Stanford, CA: Stanford University Press.\n\nRavetz, J. R. 1990. Usable knowledge, usable ignorance: Incomplete science with policy implications. In J. R. Ravetz (Ed.), The Merger of Knowledge with Power: 260-283. London: Mansell Publishing Limited.\n\nRavetz, J. R. 1993. The sin of science. Ignorance of ignorance. Science Communication, 15(2): 157–165.\n\nRoberts, J. 2012. Organizational ignorance: Towards a managerial perspective on the unknown, Management Learning, 44(3): 215–236.\n\nSmithson, M. 1989. Ignorance and Uncertainty: Emerging Paradigms. New York: Springer Verlag.\n\nDiffusion Limited Aggregation (DLA) is a kinetic model for cluster growth de- scribed by Witten and Sander (Phys. Rev. Let. 1981), which consists on an idealization of the way dendrites or dust particles form, where the rate-limiting step is the diffusion of matter to the cluster.\n\nThe DLA model consists in a series of particles that are thrown one by one from the top edge of a two (or more) dimensional grid. The sites in the grid can be occupied or empty. Initially all the sites in the grid are empty except to the bottom line which are occupied. Each particle follows a random walk in the grid, starting from a random position in the top edge, until it neighbors an occupied site, or the particle escapes from one of the lateral or top edges.\n\nWe study a restricted version of DLA, consisting in the limitation on the direc- tions a particle is allowed to move, as if they were affected by an external force such as wind. In two dimensions, we consider three scenarios: the particles can move in three directions (downwards, left and right sides); two directions (the particles can move downwards and only to the right side); and one direction (the particles can move only downwards).\n\nTheoretical approaches to the DLA model are usually in the realm of fractal analysis, renormalization techniques and conformal representations. In this research we consider a perhaps unusual approach to study the DLA model, related with its computational capabilities. Machta and Greenlaw (J. Stat. Phy. 1996) studied, from the point of view of the computational complexity, a prediction problem. This problem consists in to decide whether a given site on the grid becomes occupied after the dynamics have taken place, i.e. all the particles have been discarded or have stuck to the cluster. We call this problem DLA-Prediction.\n\nMachta and Greenlaw showed that the (unrestricted version of) DLA-Prediction is P-Complete in two (or more) dimensions. We show that, restricted to two or three directions the prediction problem is still P-Complete. Furthermore, the case restricted to one direction can be solved by a fast-parallel-algorithm, i.e. is not P-Complete unless a wide believed conjecture in computer science is false. Later, we discuss the possible shapes realizable by the restricted DLA model introduced in this presentation.\n\nThe Belousov−Zhabotinsky (BZ) reaction is the prototype oscillatory chemical reaction. In reaction mixtures in a Petri dish, with ferroin/ferriin as catalyst and bromide added initially for production of bromomalonic acid, one sees the “spontaneous” formation of target patterns of concentric waves of oxidation (blue, high-ferriin) in a red/reduced/low-ferriin reaction medium, following an initiation period of several minutes in a red steady state. In analogous manganese (Mn(ii)/Mn(iii))-catalyzed reactions one typically sees bulk oscillations. The transition between bulk oscillations in manganese catalyzed reactions and pattern formation in ferroin-catalyzed reactions can be understood by chemical interpolation using mixed catalysts. We shall describe here the use of the BZ reaction as an experimental testbed for exploring spatio-temporal dynamics near criticality in a variety of related biological systems (the brain and the cardiac electrical system).\n\nAs observed by Glass and Mackey (From Clocks to Chaos: The Rhythms of Life, 1988) and Winfree (The geometry of biological time, 2001), there are strong analogies among BZ dynamics (as described in the Oregonator (Field, Körös and Noyes, J. Am. Chem. Soc. 1972; Field and Noyes, J. Chem. Phys. 1974), neuronal dynamics (as described by the FitzHigh-Nagumo model) and cardiac dynamics (where the FitzHigh-Nagumo model has been used as highly simplified starting point). In fact, the generic Boissonade-DeKepper model (J. Phys. Chem. 1980, )for excitable chemical systems (including the BZ reaction) is essentially the FitzHugh-Nagumo neuronal model (c.f. Hastings et al., J. Phys. Chem. A, 2016). All of these systems display a wide range of patterns of excitability when operate close to criticality (the excitable/ oscillatory boundary).\n\nA variety of abnormal states from epilepsy through ventricular fibrillation have been characterized as dynamical disease (Glass, Nature 2001; Chaos 2015 and references therein), whose control and treatment requires control of global dynamics. See, e.g., Weiss et al. (Circulation. 1999) and Garfinkel, et al. (Proc. Nat. Acad. Sci. 2000) for the dynamics of fibrillation and Meisel, C. et al. (PLoS Comp. Bio. 2012), Hesse and Gross (Frontiers Systems Neuro. 2014) and Massobrio et al. (Frontiers Systems Neuro. 2015 for criticality in the normal and epileptic brain, and Gosak et al. (Frontiers Physiol. 2017) for the role of criticality in pancreatic beta cells.\n\nWe report here on an experimental and theoretical study of synchronization and more generally pattern formation in the Belousov-Zhabotinsky reaction. In particular at the generic level, BZ and related dynamics derive from the interplay between a fast activator species, here bromous acid, and a slow inhibitory species bromide generated by the slow catalytic oxidation of a brominated organic acid, here bromomalonic acid. The dynamics of pacemaker formation in the BZ reaction can be described by slow passage through a Hopf bifurcation from excitable to auto-oscillatory dynamics, in which an effective stochiometric factor including all bromide production serves as a bifurcation parameter. For the ferroin-catalyzed reaction, this Hopf bifurcation is subcritical, allowing a combination of heterogeneities and random fluctuations to initiate pacemakers. The transition between bulk oscillations in manganese catalyzed reactions (where the background substrate is oscillatory) and pattern formation in ferroin-catalyzed reactions(where the background substrate is excitable, thus supporting traveling waves of excitation) can be understood as a change in the rate of production of the inhibitory species bromide from chemical interpolation using mixed catalysts. A second bifurcation, the Showalter-Noyes criterion for oscillations, that the concentration ratio [H+][bromate]/[malonic acid] exceeds a critical value, arises because [H+][BrO3–] parametrizes the ratio of the time scale of fast bromous acid (activator) dynamics to that of the slow redox dynamics of the catalyst, and thus the effective time scale of production of the inhibitory species bromide. The existence of near-critical dynamics supporting such a bifurcation is demonstrated with small perturbations (removal of bromide); also adding one catalyst (e.g., ferroin) to a reaction parametrized by another catalyst (e,g., Mn). Related research and future research directions will also be described.\n\nWe demonstrate that attention flows manifest knowledge, and the distance (similarity) between crypto economies has predictive power to understand whether a fork or fierce competition within the same token space will be a destructive force or not. When dealing with hundreds of currencies and thousands of tokens investors have to face a very practical constraint: attention quickly becomes a scarce resource. To understand the role of attention in trustless markets we use Coase’s theorem. For the theorem to hold, the conditions that the crypto communities that will split should meet are: (i)Well defined property rights: the crypto investor owns his attention; (ii) Information symmetry: it is reasonable to assume that up to the moment of the hard fork market participants are at a level ground in terms of shared knowledge. Specialization (who becomes the expert on each new digital asset) will come later; (iii) Low transaction costs: Just before the chains split there is no significant cost in switching attention. Other factors (such as mining profitability) will play a role after the fact, and any previous conditions (e.g. options sold on the future new assets) are mainly speculative. The condition of symmetry refers to the “common knowledge” available at t-1 where all that people know is the existing asset. Information asymmetries do exist at the micro level -we cannot assume full efficiency because transaction costs are really never zero. Say’s Law states that at the macro level, aggregate production inevitably creates an equal aggregate demand. Since a fork is really an event at the macroeconomic level (in this case, the economy of bitcoin cash vs the economy of bitcoin), the aggregate demand for output is determined by the aggregate supply of output — there is a supply of attention before there was demand for attention. The Economic Complexity Index (ECI) introduced by Hidalgo and Hausmann allows to predicting future economic growth by looking at the production characteristics of the economy as a whole, rather than as the sum of its parts i.e. the present information content of the economy is a predictor of future growth. Say’s Law and the ECI approach are about aggregation of dispersed resources, and that’s what makes those relevant to the study of decentralized systems. While economic complexity is measured by the mix of products that countries are able to make, crypto economy complexity depends on the remixing of activities. Some services are complex because few crypto economies consume them, and the crypto economies that consume those tend to be more diversified. We should differentiate between the structure of output (off-chain events) vs aggregated output (on-chain, strictly transactional events). It can be demonstrated that crypto economies tend to converge to the level of economic output that can be supported by the know-how that is embedded in their economy — and is manifested by attention flows. Therefore, it is likely that a crypto economy complexity is a driver of prosperity when complexity is greater than what we would expect, at a given level of investment return. As members of the community specialize in different aspects of the economy, the structure of the network itself becomes an expression of the composition of attention output. We use genetic programming to find drivers — in other words, to learn the rankings. Such a ranking score function has the form, returns tokenA > returns tokenB = f (sources tokenA > sources tokenB). Ultimately, the degree of complexity is an issue of trust or lack thereof, and that is what the flow of attention and its conversion into transactional events reveal.\n\nCommercial and public organizations alike are increasingly facing wicked problems (Camillus, 2008). Such multi-faceted problems exceed common organizational understanding of the relationship between means and ends (Lyles, 2013; Wijen, 2014; Von Hippel & Von Krogh, 2015; Brook, Pedler, Abbott & Burgoyne, 2016). Examples are the global financial crisis that struck the world as of 2008, global warming, acts of terrorism, and refugee streams. This kind of problems typically require that a deep understanding and possible solutions have to be developed jointly (Conklin, 2006).\n\nWicked problems have, so far, been addressed by social scientists as an isolated, unique kind of phenomenon (Rittel & Webber, 1973). This is remarkable, since the notion of ‘wickedness’ strongly resembles the concept of complexity that since the 1950s has developed into an overarching perspective within the academic world. Scholars with different areas of expertise became increasingly convinced that the reductionist Newtonian linear paradigm of science was insufficient for dealing with key questions about the fundamentals of life and the functioning of societies (Anderson, Arrow & Pines, 1998; Gell-Mann, 1994; Holland, 1998; Kaufmann, 1993; Waldrop, 1992). In essence the complexity perspective refers to individual agents –be it particles, neurons, molecules, algorithms, species, or organizations- obeying to quite simple rules, whereas in populations, ecosystems and groups , where all these simple rules are combined and interact in many different ways, an intricate pattern emerges that is highly adaptable, but also very unpredictable and difficult to grasp.\n\nOrganization and management scientists that have adopted the complexity lens, argue that organizations need to become ‘complex adaptive systems’ to be able to deal with an increasingly hypercompetitive, volatile business environment (Anderson, 1999; Boisot and Child, 1999; Axelrod & Cohen, 2001; Palmberg, 2009). In this article, we draw on the growing body of knowledge on complex adaptive systems (CAS) to offer a novel perspective on how organizations deal with wicked problems. In so doing, our study also helps to avoid the risk of reification of the (highly ambiguous) concept of ‘wicked problem’ by approaching it from the perspective of complexity science (Lane, Koka & Pathak, 2006).\n\nWe concentrate on self-organization as a key tenet of the CAS perspective. More specifically the assumption that self-organization is associated with the system’s interpretative capacity facilitated by its structural characteristics will be closely investigated (Galbraith, 1973; Weick, 1977; Anderson, 1999; Ashmos, Duchon & McDaniel, 2000). To analyze this relationship we use Weick’s (1979) organizing model. Central to this model is the concept of enactment. According to Weick, enactment strongly influences an organization’s sense making ability, arguing that only by acting organizations can genuinely probe their environment and gain the necessary feedback for decision-making and further action. Charm of the skeleton….\n\nEmpirically, we focus on the context of complex emergency operations, in which frontline actors on a daily basis struggle to manage the symptoms of wicked problems. The United Nations’ Multidimensional Integrated Stabilization Mission in Mali (MINUSMA) serves as the empirical setting. As part of the overarching MINUSMA organization, a tailor-made intelligence unit was established by the Netherlands armed forces to gather, analyze and disseminate intelligence on societal issues, such as illegal trafficking and narcotics trade, ethnic dynamics and tribal tensions, and corruption and bad governance (Rietjens & De Waard, 2017). The causal ambiguity and interrelatedness of these problem areas provide a highly interesting setting for investigating the organizational reality of coping with wicked problems. Matters become even more wicked, taking into account the complex, ad hoc UN organizational constellations that is created to help the government of Mali with the endeavor of getting the country back on its feet.\n\nOne of our main findings indicates that Weick’s (1979) enactment philosophy of organizing being a matter of conscious ongoing experimenting behavior, only partially occurred within the Mali case. In short, the mental models, which the Netherlands armed forces had developed based on prior experience, were important frames of reference for designing a customized solution for the new “wicked problem” encountered by the organization in Mali. However, this solution evolved into an idée fixes, resulting in a situation where the confrontation between the paper, strategic-level solution and the real-life, operational-level outcome did not lead to a fertile reciprocal relationship, necessary for coping with the ambiguity and volatility of the security situation in Mali. Interestingly, the pitfall of slipping back into a blueprint-like approach appeared to partially happen tacitly, caused by unavoidable internal issues, including resource restrictions set by the political decision-makers, a fixed schedule of personnel rotating in and out, and the number and availability of technological assets.\n\nRené Descartes’ enduring contribution to philosophy, natural science and mathematics includes the unresolved residue of Cartesian dualism, as well as the persistence in modern science of Descartes’ four precepts defining a method for conducting natural science, which included the implementation of reductive logic. The continued difficulty resolving the Cartesian brain/mind split may be directly related to Descartes’ interpretation of reductive logic, which has been sustained within the modern structure of the reductive natural science paradigm.\n\nPractical application of reductive logic is limited in many contexts involving complicated and convoluted system dynamics but may also be limited in principle by fundamental attributes of reductive logic, as it was composed by Descartes and as it is implemented in the modern reductive science paradigm. If reductive logic is fundamentally limited, this may reveal one way in which reductive logic may not provide a close enough approximation of the natural ‘logic’ instantiated by evolution, self-organization, and the emergence of complexity. Consequently, it is important to identify reductive propositions that reveal fundamental limits on the use of reductive logic. Spelling-out the implications of the limits associated with particular propositions may alter the future application of reductive logic in natural science. This could also consequently reveal novel solutions to particular unsolved or anomalous problems in science.\n\nKurt Gödel’s two famous incompleteness theorems provide a logical platform and a set of predictable implications, from which it is possible to construct a parallel analogy within reductive natural science. The analogy demonstrates that certain reductive propositions define a hard limit on reductive logic in science, in the form of reductive incompleteness. Specifically, reductive epiphenomenalism of consciousness is an unresolved reductive proposition characterized by theoretical contradiction and conceptual paradox most obvious when strong reductive logic supporting the proposition is demonstrated to be true. Paradoxically, epiphenomenalism reduces consciousness to quantum mechanics and erases from the universe the participatory consciousness that intentionally composed the proposition in the first place. This paradox, and other contradictions and inconsistencies cannot be avoided or resolved from inside the modern reductive science paradigm employing the singular ‘bottom-up’ application of reductive logic.\n\nHowever, the paradox, contradiction and inconsistency generated by epiphenomenalism can be subtly sidestepped by declaring reductive epiphenomenalism of consciousness to be an undecidable reductive proposition. Taking this step serves to protect the logic of reductive science from paradox and inconsistency and allows the development of a theoretical position in which truth and proof in reductive science can be separated, just as in the case of abstract logic in formal mathematical systems of sufficient complexity. It can be shown that reductive logic employed in science creates a sufficiently complex logical system that it can exhibit fundamental reductive incompleteness. Epiphenomenalism, therefore, reveals the presence of a fundamental hard limit on the application of reductive logic in natural science. Reductive incompleteness and its implications are analogically similar to Gödel’s formal incompleteness in abstract logic and mathematical systems of sufficient complexity.\n\nThomas Kuhn’s conception of a scientific revolution, as well as modern explorations of scientific theory or paradigm adaptations intended to open exploration of new domains of research, provides a framework within which the implications of reductive incompleteness can be spelled-out. Among the implications of the limit on reductive logic set by reductive incompleteness, is the potential for an unresolvable and undecidable reductive proposition, stated in the paradigm and logic of reductive natural science, to become a resolvable and decidable reductive proposition, within a closely related meta-reductive paradigm, employing slightly different assumptions and premises. This opens the door to the creation of adjacent possible meta-reductive paradigms in which previously unresolvable or anomalous reductive scientific problems might find novel solutions.\n\nAn example of an adaptive adjacent possible meta-reductive paradigm is presented which preserves strong reductive logic up to the limit of formal reductive incompleteness and then encompasses the modern reductive paradigm as a special case within a meta-paradigm. Within the framework of the meta-paradigm, it is possible to resolve and decide epiphenomenalism in favor of consciousness and mind being causally efficacious emergent agents in the Universe. A number of predictions and hypotheses arising from the meta-reductive paradigm address further unresolved problems and anomalies within reductive science. The predictions and hypotheses make the meta-paradigm a falsifiable theoretical proposition, which can be tested against the success and limits of the modern reductive science paradigm.\n\nThis abstract is based on work related to the book chapter: “Dynamical systems therapy (DST): Complex Adaptive Systems in Psychiatry and Psychotherapy” published in the Handbook of Research Methods in Complexity Science: Theory and Application, Mitleton-Kelly, E et al (eds), London: Edgar Elgar Publ., 2018\n\nCourts constitute one of the three branches of power (the judiciary branch), and as such, they are fundamental to the functioning of our democracies. Supreme courts are distinguished as they interpret the basic laws at the highest level and their decisions are then referred to as precedents in courts at other levels. As the courts strive to remain self-consistent and able to adapt to new legal challenges, the network of references connecting verdicts and rulings continues to grow exponentially and exhibits an ever-increasing level of complexity. Due to the importance of references to previous cases within legal reasoning, knowledge regarding the underlying patterns of citations between rulings can help us understand the mechanisms shaping the legal system. Here we investigate the citation patterns of The Court of Justice of the European Union (CJEU) in order to understand the underlying factors that affect the decision making process.\n\nWe consider the network of citations in the period between 1955 and 2014, where the network consists of the individual cases, and directed links signify the citations between the cases. As the court evolves, the network grows and the structure of citations becomes increasingly complex. Our main question is whether---and to what extent---the observed structure of the citation network can account for the citation patterns seen in the court? We pose the question as a link prediction problem. More precisely, we define six contextual and structural quantities and use these as input variables to predict each link in the network separately. The prediction is implemented as a recommender system: for a single link, we assign a score to all possible links and calculate the position of the original link in the sorted predictions.\n\nThis process provides, not only a measure of predictability of the court itself, but by interpreting the importance and the predictive power of individual properties, we learn about the nature of the underlying mechanism of citations. We show that the court’s citations are predictable to a surprisingly high extent. Further, we investigate the temporal evolution of the performance and importance of single features and show that contextual properties---such as the similarity between the content of the cases or their age---have a decreasing significance in describing the observed citations, compared to the increasing predictive power of structural similarities such as common citations. Then, we study the heterogeneity of the court with respect to its communities defined solely on structural basis.\n\nOur content analysis shows these network communities are coherent sub-fields of the court.\n\nWe perform the link prediction procedure restricted to the communities and find that the court is highly heterogeneous with respect to the the significant properties that predict the citations.\n\nEach community is characterized by a particular set of feature preferences that are descriptive of the references inside the community. The implications of the results are two-fold: they allow us to better understand the complex structure of the court decisions, but also to build recommendation systems aiding the work of practitioners.\n\nEmpirical scaling and dynamical regimes for GDP: challenges and opportunities\n\nHarold M Hastings, Tai Young-Taft, and Thomas Wang\n\nScaling laws in economic distributions have been considered for some time, for example relative to cities [1], companies [1-2], asset prices [3], and wage income [3]. Scaling laws may be considered relative to network dynamics with respect to the division of labor [4], the growth of cities [1, 5-6], and competition relative to imperial urban center [5-6]. This paper considers scaling of GDP relative to rank, scaling of per capita GDP, as well as scaling of trade (the gravity law [7,8]). Initial analysis analysis of GDP data and per capita GDP data from 1980 and 2016 (and many years in between) finds three scaling regions. The GDP of the largest ~25 economies (nations, EU) follows a power law GDP ~ 1/rank (c.f. [9]); this is followed by a second scaling region in which GDP falls off exponentially with rank and finally a third scaling region in which the GDP falls off exponentially with the square of rank. The distribution of per capita GDP also displays these three scaling regions in 2016; but only the first two in 1980. The broad pattern holds despite significant changes in technology (enormous growth in computing power, “intelligent” automation, the Internet), the size of the world economy, emergence of new economic powers such as China, and world trade (almost free communication, containerized shipping yielding sharp declines in shipping costs, trade partnerships, growth of the EU, the effect of multinationals displacing the traditional economic role of the nations-state [10]).\n\nThus, empirically, these patterns may be universal [11-15], in which case one of the targets for growth of potentially less developed economies (those in the second and third scaling regions) may be to identify and target causative differences between these economies and those in the first (power law) scaling region. To such an extent, data analysis is undertaken to identify such salient features of such national economies, and their evolution in time. Finally, we comment on the relationship between efficiency and size, and the effect of other related variables on GDP.\n\nReferences\n\n[1] West, G. (2017). Scale: The Universal Laws of Growth, Innovation, Sustainability, and the Pace of Life in Organisms, Cities, Economies, and Companies. Penguin Press.\n\n[2] Duboff, R. (1989). Accumulation and Power: Economic History of the United States. Routledge.\n\n[3] Dos Santos, P. (2017). The Principal of Social Scaling, Complexity.\n\n[4] Smith, A. (1977). An Inquiry into the Nature and Causes of the Wealth of Nations. University of Chicago Press.\n\n[5] Braudel, F. (1984). Capitalism and Civilization. Harper & Row.\n\n[6] Arrighi, G. (2010). The Long Twentieth Century: Money, Power, and the Origins of our Times. Verso.\n\n[7] Tinbergen, J. (1962). Shaping the World Economy: Suggestions for an International Economic Policy. Twentieth Century Fund, New York.\n\n[8] Poyhonen, P. (1963). A Tentative Model for the Volume of Trade between Countries, Weltwirtschafriches Archiv 90, 93-99.\n\n[9] Garlaschelli, D., DiMatteo, T., Aste, T., Caldarelli, G., Loffredo, M.L. (2007). Interplay between topology and dynamics in the world trade web, European Physics Journal B 57, 159-164.\n\n[10] Weber, M. (2003). General Economic History. Dover.\n\n[11] Solomon, S. and Richmond, P., (2002). Stable power laws in variable economies; Lotka-Volterra implies Pareto-Zipf. The European Physical Journal B-Condensed Matter and Complex Systems, 27, 257-261.\n\n[12] Solomon, S. and Richmond, P., (2001). Power laws of wealth, market order volumes and market returns. Physica A: Statistical Mechanics and its Applications 299, 188-197.\n\n[13] Mitzenmacher, M., (2004). A brief history of generative models for power law and lognormal distributions. Internet Mathematics 1,226-251.\n\n[14] Yakovenko, V.M., (2009). Econophysics, statistical mechanics approach to. In Encyclopedia of Complexity and Systems Science (pp. 2800-2826). Springer New York. https://arxiv.org/pdf/0709.3662\n\n[15] Yakovenko, V.M. and Rosser Jr, J.B. (2009). Colloquium: Statistical mechanics of money, wealth, and income. Reviews of Modern Physics 81, 1703. https://arxiv.org/pdf/0905.1518\n\nThe evolution of multicellularity is one of the major transitions in evolution, and one that allowed the diversification and further evolution of biological complexity [1]. There are two modes to form a multicellular organism: by aggregation of independent cells (aggregative multicellularity), and through the cohesion of dividing cells (clonal multicellularity). These two modes exhibit qualitatively distinct properties and they have evolved independently across the major lineages of life. Notably, clonal multicellularity is the mode that exhibits pervasive complex traits, such as division of labor (cell differentiation) and intricate life cycles [2]. Despite decades of work on molecular and genomic comparative studies, we still lack the understanding of what are the underlying principles that drove the emergence, stabilization, and diversification of multicellular complexity.\n\nWe propose that a set of two simple constraints are sufficient to enable the emergence of complex multicellularity: a constraint on cell motility (through cell-cell cohesion/adhesion) and a constraint on information transfer among cells. We tested this hypothesis using a spatially explicit model, where cells act as information-processing agents forming permanent or transient multicellular collectives. In our model, cells process information through an internal threshold Boolean network, and they can reproduce or die. Cells communicate with adjacent cells through a subset of the nodes in their internal network working as input/output nodes. After updating the networks in each cell, we can calculate the Hamming distance between cellular network states as a proxy for cell differentiation. We modeled clonal and aggregative multicellular development as irreversible and reversible cell-cell adhesion mechanisms, respectively. If irreversible, cells always adhere to their offspring after division, and each of this links can brake irreversibly. If reversible, cells can associate or dissociate with any adjacent cell, regardless of its lineage. To summarize our modeling framework, we simulated multicellular collectives where we controlled two important aspects: the amount of communication between cells, and the persistence of cell-cell cohesion links.\n\nFrom these two simple constraints, we observe the emergence of complex multicellular traits similar to those observed in extant multicellular organisms [2]. When cell-cell cohesion is irreversible, cells are always part of multicellular entities, and therefore there is readily a transition to multicellular organismality. Furthermore, cell death and cell-cell link severance can become simple mechanisms by which these multicellular collectives can reproduce. In contrast, reversible cell adhesion implies that there is no reproduction at the collective level: only the lower-level entities reproduce. Furthermore, collectives in aggregative development are always variable in cell number and cell types, which would preclude natural selection to act on collective traits. Finally, we observe that moderate intercellular communication ensures cell differentiation at both spatial and temporal dimensions, but only clonally developing collectives can stabilize cell types (defined as distinct and stable internal network states).\n\nIn the present work, we argue that simple constraints on cell motility and cell communication enabled the evolution of multicellularity, and therefore they represent a template upon which adaptive processes could further increase and diversify complexity. Our framework and results contrast with the dominant idea that the evolution of biological complexity implies intricate generative mechanisms through adaptive tinkering, and they align with the view that evolutionary novelties represent the enablement of expanding phenotypic spaces [3]. Understanding how multicellularity emerged and evolved can help on the understanding of biological complexity in general and particularly it can extend our knowledge on the organizing principles that guided other major evolutionary transitions.\n\n[1] Szathmáry, Eörs (2015). Toward major evolutionary transitions theory 2.0., Proc. Natl. Acad. Sci. USA. 112 (33) 10104-10111\n\n[2] King, Nicole (2017). The Origin of Animal Multicellularity and Cell Differentiation. Developmental Cell. Volume 43, Issue 2, 124 - 140\n\n[3] Longo G, Montévil M, Kauffman SA (2012). No entailing laws, but enablement in the evolution of the biosphere. arXiv 1201.2069v1\n\nThis paper offers a general systems definition of the phrase \"evolutionary development\", and an introduction to its application to the universe as a system. Evolutionary development, evo devo or ED is a term that can be used by philosophers, scientists, historians, and others as a replacement for the more general term “evolution”, whenever a scholar thinks experimental, selectionist, contingent and stochastic or “evolutionary” processes, and also convergent, statistically deterministic (probabilistically predictable) or “developmental” processes, including replication, may be simultaneously contributing to selection and adaptation in any complex system, including the universe as a system.\n\nLike living systems, our universe broadly exhibits both stochastic and deterministic components, in all historical epochs and at all levels of scale. It has a definite birth and it is inevitably senescing toward heat death. The idea that we live in an “evo devo universe,” one that has self-organized over past replications both to generate multilocal evolutionary variation (experimental diversity), and to convergently develop and pass to future generations selected aspects of its accumulated complexity (\"intelligence\") is an obvious hypothesis. Living systems harness stochastic evolutionary processes to produce novel developments, especially under stress, in a variety of systems and scales. If our universe is an adaptive replicator, it makes sense that it would do the same. Today, only a few cosmologists or physicists, even in the community that theorizes universal replication and the multiverse, have entertained the hypothesis that our universe may be both evolving and developing (engaging in both unpredictable experimentation and goal-driven, teleological, directional change and a replicative life cycle), as in living systems. Our models of universal replication, like Lee Smolin's cosmological natural selection (CNS), do not yet use the concept of universal development, or refer to development literature.\n\nI will argue that some variety of evo devo universe models must emerge in coming years, including models of CNS with Intelligence (CNS-I), which explore the ways emergent intelligence can be expected to constrain and direct “natural” selection, as it does in living systems. Evo devo models are one of several early approaches to an Extended Evolutionary Synthesis (EES), one that explores adaptation in both living and nonliving replicators. They have much to offer as a general approach to adaptive complexity, and may be required to understand several important phenomena under current research, including galaxy formation, the origin of life, the fine-tuned universe hypothesis, possible Earthlike and life fecundity in astrobiology, convergent evolution, the future of artificial intelligence, and our own apparent history of unreasonably smooth and resilient acceleration of both total and “leading edge” adapted complexity and intelligence growth, even under frequent and occasionally extreme past catastrophic selection events. If they are to become better validated in living systems and in nonliving adaptive replicators, including stars, prebiotic chemistry, and the universe as a system, they will require both better simulation capacity and advances in a variety of theories, which I shall briefly review.\n\nThe study of how opinions, innovations, behaviors, and knowledge spread has long been a central topic in physical, social, and ecological sciences. In the past, these have been commonly studied through simple contagion processes, processes in which information flows through the contact of two individuals. The inability for simple contagion models to account for the plethora of dynamical patterns observed in the real world, such as the polarization of opinions, has led to a search for additional mechanisms. Recent empirical evidence suggests that different matters spread in different ways, namely, that some require a dependence on the whole neighborhood of an individual to propagate. In that sense, the process of information acquisition requires reinforcement from multiple contact sources. This phenomenon became known as Complex Contagion. Although widely investigated in the literature of cascading effects, complex contagion has only recently received some attention in the context of population dynamics, i.e., when multiple competing opinions co-evolve over time in a population.\n\nComplex Contagion has been commonly modeled by a process of fractional thresholds. This implies that there is a well-defined threshold fraction of neighbors needed for an opinion/idea/innovation to be adopted by an individual. Dynamically, this results in a deterministic process that either percolates through the system or that becomes contained to a few elements of the system. Under this context, it was found that complex contagion spreading is speeded up by clustering of individuals in populations (triangular closures) but that a modular structure of the population can halt the propagation of an opinion. Indeed, the study of opinion dynamics has examined under which conditions a consensus is formed. Typical questions involve the time to consensus and how likely is it for a new opinion to invade a population.\n\nHere, we introduce a new class of complex contagion processes inspired by recent empirical findings in the literature of innovation and knowledge diffusion. [1] We consider different opinions coevolving in a population with potentially asymmetric properties of contagion. We assume that the probability of an opinion to spread to a new individual grows as an arbitrary power of the density of neighbors that already share that same opinion. We explore analytically and computationally the properties of this model in well-mixed and structured populations. Namely, we test well-mixed, homogeneous random, random, scale-free, and modular networks of influence. We show these populations span a dynamical space that exhibits patterns of polarization, consensus, and dominance. We map these patterns to topologically equivalent ones found in the literature of evolutionary games of cooperation. We find that these dynamical properties are robust to different population structures. Finally, we show how modular topologies can create different dynamics and additional dependences on the initial configuration of opinions. Our results are general and of relevance not only for the study of opinions and ideas but also when considering propagation in more abstract networks derived from data, like that of product complexity and product adoption by countries as well as others. [2,3]\n\nThe evident similarities between chaotic maps and Pseudo Random Number Generators (PRNG) and consequently cryptography have been a strong motivation for the researchers in the past few decades and numerous PRNGs and cryptosystems base on various chaotic maps have been designed in this period. The most noticeable similarities between chaos and PRNGs and cryptosystems are their strong sensitivity to the initial conditions and control parameters, aperiodicity, random-like behavior, and ergodicity. Yet, there are also several drawbacks in the application of the chaotic maps in the design of secure random number generators and cryptosystems that could lead to disaster if not treated well. Particularly, dozens of cryptographic algorithms based on the Logistic map were cryptanalyzed since the emergence of chaos based cryptography and the adequacy of the Logistic map has been questioned by researchers in this area. Almost all the chaos-based cryptography algorithms use the chaotic maps as an easy and deterministic source of entropy. Thus, these algorithms, directly or indirectly, use the characteristic of sensitivity to control parameters and initial conditions to reflect the cryptographic requirement of sensitivity to the keys. Therefore, the low sensitivity of the Logistic map to a range of control parameters and the three windows in the bifurcation diagram of the Logistic map are two of the major issues in the application of this map for the purpose of cryptography. Surprisingly, with all these issues Logistic map is still one of the most favorite maps in the field of chaos-based cryptography and each year several algorithms based on it are proposed.\n\nIn addition to the low sensitivity, the limitation in the size of the variables in computer simulation of the chaotic maps is another issue that arises in the application of chaotic maps in cryptography. Many of the chaotic maps are taken from a natural phenomenon and are based on real numbers. Thus, their implementation requires simulation of differential equations with real numbers in their structure. The approximation of the real numbers in the computer results in rounding and approximation errors. Palmore and Herring (1990) have investigated the effect of computer realization of the chaotic maps and fractals in computer and concluded that with rounding, in 30 to 49 iterations could “destroy all accuracy” of the results. Reviewing hundreds of papers and proceedings published on chaos-based cryptographic algorithms from 1998 to 2015 shows that majority the proposed algorithms are implemented in the finite precision of $10^{-13}$ to $10^{-16}$. The reason lies in the difficulty and possible lower speed of higher precision implementation of the chaotic maps. Perhaps another reason could be Blackledge and Ptitsyn’s research in 2010 that states increase in precision does not guarantee a sufficiently long trajectory in chaotic maps. In this study, we investigate the trajectories generated by various chaotic maps including Logistic map using statistical methods and propose a new PRNG based on the Logistic map that passes all the statistical randomness tests including the Big Crush test. The results of the analysis demonstrate that the proposed algorithm with proper implementation not only passes all the tests but also is fast and can provide a huge key-space for the cryptographic random number generator.\n\nHomo Potens: A Species Most Complex and Powerful\n\nWe are a species whose astounding powers of creativity and innovation are matched by destructive powers so enormous we could easily subvert -- for ourselves and all other species -- the very conditions of life on Earth. We are, as Edgar Morin says, Homo Sapiens-Homo Demens, a whirling mix of the wise and the foolish, the rational and the irrational. In a word, we are “potens”, the Latin for powerful. As Homo Potens, we are a species whose extraordinary potential for better or worse is realized through the exercise of power. To avert the perils that lurk in our Demens and to nurture the immense promise of our Sapiens will depend to a large extent on how well we understand ourselves as Potens.\n\nThere is yet another essential respect in which humans far surpass other species that we would do well to try to understand: our complexity. As with power, we partake in complexity for better or worse. Failure to deal with complexity tends to transform small problems into larger ones. Complexity can overwhelm. But it also poses challenges that, once mastered, make it possible to explore complex problems in greater depth. Advances in recent decades in understanding the nature of complex dynamical systems raise hopes that, over the long term, novel approaches to science itself can help us navigate the promise and perils of complexity.\n\nAs a means to cast light on Homo Potens as a most complex and powerful species, this essay proposes a complexity theory of power, a combination of power theory and complexity theory. The proposed theory correlates the ability of one party to exercise power over another (A.Allen; R. Dahl; S. Lukes) with disorganized complexity (W. Weaver) and the power to collaborate (A. Allen; H. Arendt; T. Parsons) with self-organized complexity (I. Prigogine). In this view, power exercised by one party to dominate another is a disorganizing process and power exercised by different parties to collaborate with one another is a self-organizing process. These processes can occur across scale in human systems. Whether at the level of interpersonal, national or global politics, self-organizing is a democratizing process through which the disorganizing effects of domination and authoritarianism can be countered and overcome. While complexity perspectives teach us that there nothing is inexorable or guaranteed about the future generally and the advance of self-organization more specifically, they also offer the hope we can better diagnose the debilitating effects of power imposed and learn how to exercise power with not over others.\n\nWhat happens in our brains when we consciously see, hear, feel, and know something? The Hard Problem of Consciousness is the problem of explaining how this happens. A theory of how the Hard Problem is solved needs to link brain to mind by modeling how brain dynamics give rise to psychological experiences, notably how emergent properties of brain dynamics generate properties of individual experiences. This talk summarizes evidence that Adaptive Resonance Theory, or ART, is accomplishing this goal. ART is a cognitive and neural theory of how advanced brains autonomously learn to attend, recognize, and predict objects and events in a changing world. ART predicted that “all conscious states are resonant states” and specified mechanistic links between processes of consciousness, learning, expectation, attention, resonance, and synchrony. It hereby provides functional and mechanistic explanations of data ranging from individual spikes and their synchronization to the dynamics of conscious and unconscious perceptual, cognitive, and cognitive-emotional behaviors. ART proposes how and why evolution has created conscious states. In brief, sensory data are typically ambiguous and incomplete, and incapable of supporting effective action. Only after cortical processing streams that obey computationally complementary laws interact through hierarchical resolution of uncertainty do sufficiently complete and stable representations form whereby to control effective action. Consciousness is an “extra degree of freedom” in resonant states that are triggered by these representations, thereby enabling successful predictive actions to be based upon them. Different resonances support seeing, hearing, feeling, and knowing. The talk will describe where these resonances occur in our brains, and how they interact. Both normal and clinical psychological and neurobiological data will be explained and predicted that have not been explained by alternative theories. The talk will also mention why some resonances do not become conscious, and why not all brain dynamics are resonant, including brain dynamics that control action.\n\nStephen Grossberg is Wang Professor of Cognitive and Neural Systems; Professor of Mathematics & Statistics, Psychological & Brain Sciences, and Biomedical Engineering; and Director of the Center for Adaptive Systems at Boston University. He is a principal founder and current research leader in computational neuroscience, theoretical psychology and cognitive science, and neuromorphic technology and AI. In 1957-1958, he introduced the paradigm of using systems of nonlinear differential equations to develop models that link brain mechanisms to mental functions, including widely used equations for short-term memory (STM), or neuronal activation; medium-term memory (MTM), or activity-dependent habituation; and long-term memory (LTM), or neuronal learning. His work focuses upon how individuals, algorithms, or machines adapt autonomously in real-time to unexpected environmental challenges. These discoveries together provide a blueprint for designing autonomous adaptive intelligent agents. They includes models of vision and visual cognition; object, scene, and event learning and recognition; audition, speech, and language learning and recognition; development; cognitive information processing; reinforcement learning and cognitive-emotional interactions; consciousness; visual and path integration navigational learning and performance; social cognition and imitation learning; sensory-motor learning, control, and planning; mental disorders; mathematical analysis of neural networks; experimental design and collaborations; and applications to neuromorphic technology and AI. Grossberg founded key infrastructure of the field of neural networks, including the International Neural Network Society and the journal Neural Networks, and has served on the editorial boards of 30 journals. His lecture series at MIT Lincoln Lab led to the national DARPA Study of Neural Networks. He is a fellow of AERA, APA, APS, IEEE, INNS, MDRS, and SEP. He has published 17 books or journal special issues, over 550 research articles, and has 7 patents. He was most recently awarded the 2015 Norman Anderson Lifetime Achievement Award of the Society of Experimental Psychologists (SEP), and the 2017 Frank Rosenblatt computational neuroscience award of the Institute for Electrical and Electronics Engineers (IEEE). See the following web pages for further information:\n\nsites.bu.edu/steveg\n\nhttp://en.wikipedia.org/wiki/Stephen_Grossberg http://cns.bu.edu/~steve/GrossbergNNeditorial2010.pdf\n\nhttp://scholar.google.com/citations?user=3BIV70wAAAAJ&hl=en\n\nhttp://www.bu.edu/research/articles/steve-grossberg-psychologist-brain-research/\n\nhttp://www.bu.edu/research/articles/stephen-grossberg-ieee-frank-rosenblatt-award/\n\nDivision of labor allows people to reap the benefits of specialization. What is often underappreciated is that division of labor does not just mean that people specialize, it means that people specialize in different things. As a consequence, in larger firms, no single individual disposes of all the knowledge required to keep operations going. The knowledge base of such firms is typically spread out across many different workers. When know-how takes this shape of \"distributed expertise\", the value of human capital is dependent on the ecosystem of complementary knowledge in which it is embedded. For workers, this means that the benefits of acquiring a certain specialization depend on having access to coworkers who can complement their skills and know-how. In this paper, I quantify complementarities among coworkers’ skill sets and show how they affect careers, returns to schooling and the large-plant and urban wage premiums.\n\nI focus on the skills workers acquire through education. To be "
    }
}
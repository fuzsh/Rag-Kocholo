{
    "id": "correct_birth_00148_3",
    "rank": 19,
    "data": {
        "url": "https://link.springer.com/chapter/10.1007/978-981-99-1600-9_9",
        "read_more_link": "",
        "language": "en",
        "title": "Knowledge Representation Learning and Knowledge-Guided NLP",
        "top_image": "https://static-content.springer.com/cover/book/978-981-99-1600-9.jpg",
        "meta_img": "https://static-content.springer.com/cover/book/978-981-99-1600-9.jpg",
        "images": [
            "https://link.springer.com/oscar-static/images/darwin/header/img/logo-springerlink-39ee2a28d8.svg",
            "https://media.springernature.com/w72/springer-static/cover-hires/book/978-981-99-1600-9?as=webp",
            "https://media.springernature.com/w215h120/springer-static/image/art%3Aplaceholder%2Fimages/placeholder-figure-springernature.png",
            "https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-3-031-44216-2?as=webp",
            "https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-3-031-44207-0?as=webp",
            "https://media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-981-99-1600-9_9/MediaObjects/385018_2_En_9_Fig1_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-981-99-1600-9_9/MediaObjects/385018_2_En_9_Fig2_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-981-99-1600-9_9/MediaObjects/385018_2_En_9_Fig3_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-981-99-1600-9_9/MediaObjects/385018_2_En_9_Fig4_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-981-99-1600-9_9/MediaObjects/385018_2_En_9_Fig5_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-981-99-1600-9_9/MediaObjects/385018_2_En_9_Fig6_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-981-99-1600-9_9/MediaObjects/385018_2_En_9_Fig7_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-981-99-1600-9_9/MediaObjects/385018_2_En_9_Fig8_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-981-99-1600-9_9/MediaObjects/385018_2_En_9_Fig9_HTML.png",
            "https://media.springernature.com/lw10/springer-static/image/chp%3A10.1007%2F978-981-99-1600-9_9/385018_2_En_9_IEq169_HTML.gif",
            "https://media.springernature.com/lw4/springer-static/image/chp%3A10.1007%2F978-981-99-1600-9_9/MediaObjects/385018_2_En_9_Equ28_HTML.png",
            "https://media.springernature.com/lw10/springer-static/image/chp%3A10.1007%2F978-981-99-1600-9_9/385018_2_En_9_IEq182_HTML.gif",
            "https://media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-981-99-1600-9_9/MediaObjects/385018_2_En_9_Fig10_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-981-99-1600-9_9/MediaObjects/385018_2_En_9_Fig11_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-981-99-1600-9_9/MediaObjects/385018_2_En_9_Fig12_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-981-99-1600-9_9/MediaObjects/385018_2_En_9_Fig13_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-981-99-1600-9_9/MediaObjects/385018_2_En_9_Fig14_HTML.png",
            "https://media.springernature.com/lw8/springer-static/image/chp%3A10.1007%2F978-981-99-1600-9_9/385018_2_En_9_IEq337_HTML.gif",
            "https://media.springernature.com/lw250/springer-static/image/chp%3A10.1007%2F978-981-99-1600-9_9/MediaObjects/385018_2_En_9_Equ60_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-981-99-1600-9_9/MediaObjects/385018_2_En_9_Fig15_HTML.png",
            "https://media.springernature.com/lw4/springer-static/image/chp%3A10.1007%2F978-981-99-1600-9_9/MediaObjects/385018_2_En_9_Equ65_HTML.png",
            "https://media.springernature.com/lw4/springer-static/image/chp%3A10.1007%2F978-981-99-1600-9_9/MediaObjects/385018_2_En_9_Equ66_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-981-99-1600-9_9/MediaObjects/385018_2_En_9_Fig16_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-981-99-1600-9_9/MediaObjects/385018_2_En_9_Fig17_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-981-99-1600-9_9/MediaObjects/385018_2_En_9_Fig18_HTML.png",
            "https://media.springernature.com/lw4/springer-static/image/chp%3A10.1007%2F978-981-99-1600-9_9/MediaObjects/385018_2_En_9_Equ70_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-981-99-1600-9_9/MediaObjects/385018_2_En_9_Fig19_HTML.png",
            "https://media.springernature.com/lw4/springer-static/image/chp%3A10.1007%2F978-981-99-1600-9_9/MediaObjects/385018_2_En_9_Equ71_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-981-99-1600-9_9/MediaObjects/385018_2_En_9_Fig20_HTML.png",
            "https://media.springernature.com/lw4/springer-static/image/chp%3A10.1007%2F978-981-99-1600-9_9/MediaObjects/385018_2_En_9_Equ72_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-981-99-1600-9_9/MediaObjects/385018_2_En_9_Fig21_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-981-99-1600-9_9/MediaObjects/385018_2_En_9_Fig22_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-981-99-1600-9_9/MediaObjects/385018_2_En_9_Fig23_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-981-99-1600-9_9/MediaObjects/385018_2_En_9_Fig24_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-981-99-1600-9_9/MediaObjects/385018_2_En_9_Fig25_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-981-99-1600-9_9/MediaObjects/385018_2_En_9_Fig26_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-981-99-1600-9_9/MediaObjects/385018_2_En_9_Fig27_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-981-99-1600-9_9/MediaObjects/385018_2_En_9_Fig28_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-981-99-1600-9_9/MediaObjects/385018_2_En_9_Fig29_HTML.png",
            "https://link.springer.com/oscar-static/images/logo-springernature-white-19dd4ba190.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Xu Han",
            "Weize Chen",
            "Zhiyuan Liu",
            "Yankai Lin",
            "Maosong Sun"
        ],
        "publish_date": "2023-07-21T00:00:00",
        "summary": "",
        "meta_description": "Knowledge is an important characteristic of human intelligence and reflects the complexity of human languages. To this end, many efforts have been devoted to organizing various human knowledge to improve the ability of machines in language understanding, such as...",
        "meta_lang": "en",
        "meta_favicon": "/oscar-static/img/favicons/darwin/apple-touch-icon-92e819bf8a.png",
        "meta_site_name": "SpringerLink",
        "canonical_link": "https://link.springer.com/chapter/10.1007/978-981-99-1600-9_9",
        "text": "As we mentioned before, we can organize knowledge using symbolic systems. However, as the scale of knowledge increases, using these symbolic systems naturally faces two challenges: data sparsity and computational inefficiency. Despite the importance of symbolic knowledge for NLP, these challenges indicate that symbolic systems are not an inherently machine-friendly form of knowledge organization. Specifically, data sparsity is a common problem in many fields. For example, when we use KGs to describe general world knowledge, the number of entities (nodes) in KGs can be enormous, while the number of relations (edges) in KGs is typically few, i.e., there are often no relations between two randomly selected entities in the real world, resulting in the sparsity of KGs. Computational inefficiency is another challenge we have to overcome since computers are better suited to handle numerical data and less adept at handling symbolic knowledge in KGs. As the size of KGs continues to grow, this efficiency challenge may become more severe.\n\nTo solve the above problems, distributed knowledge representations are introduced, i.e., low-dimensional continuous embeddings are used to represent symbolic knowledge. The sparsity problem is alleviated owing to using these distributed representations, and the computational efficiency is also improved. In addition, using embeddings to represent knowledge makes it more feasible and convenient to integrate symbolic knowledge into neural NLP models, motivating the exploration of knowledge-guided NLP. Up to now, distributed knowledge representations have been widely used in many applications requiring the support of human knowledge. Moreover, distributed knowledge representations can also significantly improve the ability of knowledge completion, knowledge fusion, and knowledge reasoning.\n\nIn this section, we take KGs that organize rich world knowledge as an example to introduce how to obtain distributed knowledge representations. Hereafter, we use \\(\\mathcal {G} = (\\mathcal {E}, \\mathcal {R}, \\mathcal {T})\\) to denote a KG, in which \\(\\mathcal {E}=\\{e_1, e_2, \\dots \\}\\) is the entity set, \\(\\mathcal {R}=\\{r_1, r_2, \\dots \\}\\) is the relation set, and \\(\\mathcal {T}\\) is the fact set. We use \\(h, t \\in \\mathcal {E}\\) to represent the head and tail entities, and h, t to represent their entity embeddings. A triplet \\(\\langle h, r, t\\rangle \\in \\mathcal {T}\\) is a factual record, where h, t are entities and r is the relation between h and t.\n\nGiven a triplet 〈h, r, t〉, a score function f(h, r, t) is used by knowledge representation learning methods to measure whether 〈h, r, t〉 is a fact or a fallacy. Generally, the larger the value of f(h, r, t), the higher the probability that 〈h, r, t〉 is true.Footnote 2 Based on f(h, r, t), knowledge representations can be learned with\n\n$$\\displaystyle \\begin{aligned} {} \\arg\\min_{\\theta} \\sum_{\\langle h, r, t \\rangle \\in \\mathcal{T}} \\sum_{\\langle \\tilde{h}, \\tilde{r}, \\tilde{t} \\rangle \\in \\tilde{\\mathcal{T}}} \\max\\left\\{0, f(\\tilde{h},\\tilde{r},\\tilde{t}) + \\gamma - f(h,r,t) \\right\\}, \\end{aligned} $$\n\n(9.1)\n\nwhere θ is the learnable embeddings of entities and relations, 〈h, r, t〉 indicates positive facts (i.e., triplets in \\(\\mathcal {T}\\)), and \\(\\langle \\tilde {h}, \\tilde {r}, \\tilde {t} \\rangle \\) indicates negative facts (triplets that do not exist in KGs). γ > 0 is a hyper-parameter used as a margin. A bigger γ means to learn a wider gap between f(h, r, t) and \\(f(\\tilde {h},\\tilde {r},\\tilde {t})\\). Considering that there are no explicit negative triplets in KGs, \\(\\tilde {\\mathcal {T}}\\) is usually defined as\n\n$$\\displaystyle \\begin{aligned} \\begin{aligned} \\tilde{\\mathcal{T}} = \\{\\langle \\tilde{h},r,t \\rangle | \\tilde{h} \\in \\mathcal{E}, \\langle h,r,t\\rangle \\in \\mathcal{T}\\}\\cup\\{\\langle h, \\tilde{r}, t \\rangle | \\tilde{r} \\in \\mathcal{R}, \\langle h,r,t\\rangle \\in \\mathcal{T}\\} \\\\ \\cup \\{\\langle h,r,\\tilde{t} \\rangle | \\tilde{t} \\in \\mathcal{E}, \\langle h,r,t\\rangle \\in \\mathcal{T}\\} - \\mathcal{T}, \\end{aligned} \\end{aligned} $$\n\n(9.2)\n\nwhich means \\(\\tilde {\\mathcal {T}}\\) is built by corrupting the entities and relations of the triplets in \\(\\mathcal {T}\\). Different from the margin-based loss function in Eq. (9.1), some methods apply a likelihood-based loss function to learn knowledge representations as\n\n$$\\displaystyle \\begin{aligned} \\begin{array}{rcl} \\arg\\min_{\\theta} \\sum_{\\langle h, r, t \\rangle \\in \\mathcal{T}} \\log \\big[1+ \\exp(-f(h,r,t)) \\big] + \\sum_{\\langle \\tilde{h}, \\tilde{r}, \\tilde{t} \\rangle \\in \\tilde{\\mathcal{T}}} \\log \\big[1+ \\exp (f(\\tilde{h},\\tilde{r},\\tilde{t}))\\big].\\\\ \\end{array} \\end{aligned} $$\n\n(9.3)\n\nNext, we present some typical knowledge representation learning methods as well as their score functions, including (1) linear representation methods that formalize relations as linear transformations between entities, (2) translation representation methods that formalize relations as translation operations between entities, (3) neural representation methods that apply neural networks to represent entities and relations, and (4) manifold representation methods that use complex manifold spaces instead of simple Euclidean spaces to learn representations.\n\n9.3.1 Linear Representation\n\nLinear representation methods formalize relations as linear transformations between entities, which is a simple and basic way to learn knowledge representations.\n\nStructured Embeddings (SE)\n\nSE [13] is a typical linear method to represent KGs. In SE, all entities are embedded into a d-dimensional space. SE designs two relation-specific matrices Mr,1, \\({\\mathbf {M}}_{r,2} \\in \\mathbb {R}^{d\\times d}\\) for each relation r, and these two matrices are used to transform the embeddings of entities. The score function of SE is defined as\n\n$$\\displaystyle \\begin{aligned} f(h,r,t)= - \\Vert {\\mathbf{M}}_{r,1} \\mathbf{h} - {\\mathbf{M}}_{r,2} \\mathbf{t}\\Vert, \\end{aligned} $$\n\n(9.4)\n\nwhere \\(\\lVert \\cdot \\rVert \\) is the vector norm. The assumption of SE is that the head and tail embeddings should be as close as possible after being transformed into a relation-specific space. Therefore, SE uses the margin-based loss function to learn representations.\n\nSemantic Matching Energy (SME)\n\nSME [11] builds more complex linear transformations than SE. Given a triplet 〈h, r, t〉, h and r are combined using a projection function to get a new embedding lh,r. Similarly, given t and r, we can get lt,r. Then, a point-wise multiplication function is applied on lh,r and lt,r to get the score of this triplet. SME introduces two different projection functions to build f(h, r, t): one is in the linear form\n\n$$\\displaystyle \\begin{aligned} \\begin{aligned} f(h,r,t) = {\\mathbf{l}}_{h,r}^{\\top}{\\mathbf{l}}_{t,r}, \\quad {\\mathbf{l}}_{h,r}={\\mathbf{M}}_{1}\\mathbf{h}+{\\mathbf{M}}_{2}\\mathbf{r}+{\\mathbf{b}}_1, \\quad {\\mathbf{l}}_{t,r}={\\mathbf{M}}_{3}\\mathbf{t}+{\\mathbf{M}}_{4}\\mathbf{r}+{\\mathbf{b}}_2, \\end{aligned} \\end{aligned} $$\n\n(9.5)\n\nand the other is in the bilinear form\n\n$$\\displaystyle \\begin{aligned} \\begin{aligned} f(h,r,t)= {\\mathbf{l}}_{h,r}^{\\top}{\\mathbf{l}}_{t,r}, \\quad {\\mathbf{l}}_{h,r}=({\\mathbf{M}}_{1}\\mathbf{h}\\odot{\\mathbf{M}}_{2}\\mathbf{r})+{\\mathbf{b}}_1, \\quad {\\mathbf{l}}_{t,r}=({\\mathbf{M}}_{3}\\mathbf{t}\\odot{\\mathbf{M}}_{4}\\mathbf{r})+{\\mathbf{b}}_2, \\end{aligned} \\end{aligned} $$\n\n(9.6)\n\nwhere ⊙ is the element-wise (Hadamard) product. M1, M2, M3, and M4 are learnable transformation matrices, and b1 and b2 are learnable bias vectors. Empirically, the margin-based loss function is suitable for dealing with the score functions built with vector norm operations, while the likelihood-based loss function is more usually used to process the score functions built with inner product operations. Since SME uses the inner product operation to build its score function, the likelihood-based loss function is thus used to learn representations.\n\nLatent Factor Model (LFM)\n\nLFM [70] aims to model large KGs based on a bilinear structure. By modeling entities as embeddings and relations as matrices, the score function of LFM is defined as\n\n$$\\displaystyle \\begin{aligned} f(h,r,t)= {\\mathbf{h}}^{\\top}{\\mathbf{M}}_r\\mathbf{t}, \\end{aligned} $$\n\n(9.7)\n\nwhere the matrix Mr is the representation of the relation r. Similar to SME, LFM adopts the likelihood-based loss function to learn representations. Based on LFM, DistMult [186] further restricts Mr to be a diagonal matrix. As compared with LFM, DistMult not only reduces the parameter size but also reduces the computational complexity and achieves better performance.\n\nRESCAL\n\nRESCAL [118, 119] is a representation learning method based on matrix factorization. By modeling entities as embeddings and relations as matrices, RESCAL adopts a score function the same to LFM. However, RESCAL employs neither the margin-based nor the likelihood-based loss function to learn knowledge representations. Instead, in RESCAL, a three-way tensor \\(\\overrightarrow {\\mathbf {X}} \\in \\mathbb {R}^{|\\mathcal {E}|\\times |\\mathcal {E}|\\times |\\mathcal {R}|}\\) is adopted. In the tensor \\(\\overrightarrow {\\mathbf {X}}\\), two modes respectively stand for head and tail entities, while the third mode stands for relations. The entries of \\(\\overrightarrow {\\mathbf {X}}\\) are determined by the existence of the corresponding triplet facts. That is, \\(\\overrightarrow {\\mathbf {X}}_{ijk}=1\\) if the triplet 〈i-th entity, k-th relation, j-th entity〉 exists in the training set, and otherwise \\(\\overrightarrow {\\mathbf {X}}_{ijk}=0\\). To capture the inherent structure of all triplets, given \\(\\overrightarrow {\\mathbf {X}}=\\{{\\mathbf {X}}_1, \\cdots , {\\mathbf {X}}_{|\\mathcal {R}|}\\}\\), for each slice \\({\\mathbf {X}}_n=\\overrightarrow {\\mathbf {X}}_{[:, :, n]}\\), RESCAL assumes the following factorization for Xn holds\n\n$$\\displaystyle \\begin{aligned} {} {\\mathbf{X}}_n \\approx \\mathbf{E}{\\mathbf{M}}_{r_n}{\\mathbf{E}}^{\\top}, \\end{aligned} $$\n\n(9.8)\n\nwhere \\(\\mathbf {E} \\in \\mathbb {R}^{|\\mathcal {E}| \\times d}\\) stands for the d-dimensional entity representations of all entities and \\({\\mathbf {M}}_{r_n} \\in \\mathbb {R}^{d \\times d}\\) represents the interactions between entities specific to the n-th relation rn. Following this tensor factorization assumption, the learning objective of RESCAL is defined as\n\n$$\\displaystyle \\begin{aligned} \\arg\\min_{\\mathbf{E},\\mathbf{M}} \\frac{1}{2}\\left(\\sum_{n=1}^{|\\mathcal{R}|} \\Vert {\\mathbf{X}}_n-\\mathbf{E}{\\mathbf{M}}_{r_n}{\\mathbf{E}}^{\\top}\\Vert_{F}^{2}\\right)+ \\frac{1}{2}\\lambda\\left(\\Vert \\mathbf{E}\\Vert_{F}^{2}+\\sum_{n=1}^{|\\mathcal{R}|} \\Vert {\\mathbf{M}}_{r_n}\\Vert _{F}^{2}\\right), \\end{aligned} $$\n\n(9.9)\n\nwhere \\(\\mathbf {M} = \\{{\\mathbf {M}}_{r_1}, {\\mathbf {M}}_{r_2},\\cdots ,{\\mathbf {M}}_{r_{|\\mathcal {R}|}}\\}\\) is the collection of all relation matrices, \\(\\lVert \\cdot \\rVert _F\\) is the Frobenius vector norm, and λ is a hyper-parameter to control the second regularization term.\n\nHolographic Embeddings (HolE)\n\nHolE [117] is proposed as an enhanced version of RESCAL. RESCAL works well with multi-relational data but suffers from a high computational complexity. To achieve high effectiveness and efficiency at the same time, HolE employs an operation named circular correlation to generate representations. The circular correlation operation \\(\\star : \\mathbb {R}^d \\times \\mathbb {R}^d \\rightarrow \\mathbb {R}^d\\) between two entities h and t is\n\n$$\\displaystyle \\begin{aligned} [\\mathbf{h}\\star \\mathbf{t}]_k=\\sum_{i=1}^{d} [\\mathbf{h}]_{i}[\\mathbf{t}]_{(k+i) \\ \\text{mod} \\ d+1}, \\end{aligned} $$\n\n(9.10)\n\nwhere [⋅]i means the i-th vector element. The score function is defined as\n\n$$\\displaystyle \\begin{aligned} f(h,r,t) = -{\\mathbf{r}}^{\\top}(\\mathbf{h}\\star \\mathbf{t}). \\end{aligned} $$\n\n(9.11)\n\nHolE adopts the likelihood-based loss function to learn representations.\n\nThe circular correlation operation brings several advantages. First, the circular correlation operation is noncommutative (i.e., h ⋆ t ≠ t ⋆ h), which makes it capable of modeling asymmetric relations in KGs. Second, the circular correlation operation has a lower computational complexity compared to the tensor product operation in RESCAL. Moreover, the circular correlation operation could be further accelerated with the help of fast Fourier transform (FFT), which is formalized as\n\n$$\\displaystyle \\begin{aligned} \\mathbf{h}\\star \\mathbf{t}=\\mathcal{F}^{-1}(\\overline{\\mathcal{F}(\\mathbf{h})}\\odot \\mathcal{F}(\\mathbf{t})), \\end{aligned} $$\n\n(9.12)\n\nwhere \\(\\mathcal {F}(\\cdot )\\) and \\(\\mathcal {F}(\\cdot )^{-1}\\) represent the FFT operation and its inverse operation, respectively, \\(\\overline {\\mathcal {F}(\\cdot )}\\) denotes the complex conjugate of \\(\\mathcal {F}(\\cdot )\\), and ⊙ stands for the element-wise (Hadamard) product. Due to the FFT operation, the computational complexity of the circular correlation operation is \\(\\mathcal {O}(d \\log d)\\), which is much lower than that of the tensor product operation.\n\n9.3.2 Translation Representation\n\nTranslation methods are another effective way to obtain distributed representations of KGs. To help readers better understand different translation representation methods, we first introduce their motivations.\n\nThe primary motivation is that it is natural to consider relations between entities as translation operations. For distributed representations, entities are embedded into a low-dimensional space, and ideal representations should embed entities with similar semantics into the nearby regions, while entities with different meanings should belong to distinct clusters. For example, William Shakespeare and Jane Austen may be in the same cluster of writers, Romeo and Juliet and Pride and Prejudice may be in another cluster of books. In this case, they share the same relation Notable Work, and the translations from writers to their books in the embedding space are similar.\n\nThe secondary motivation of translation methods derives from the breakthrough in word representation learning. Word2vec [108] proposes two simple models, skip-gram and CBOW, to learn distributed word representations from large-scale corpora. The learned word embeddings perform well in measuring word similarities and analogies. And these word embeddings have some interesting phenomena: if the same semantic or syntactic relations are shared by two word pairs, the translations within the two word pairs are similar. For instance, we have\n\n$$\\displaystyle \\begin{aligned} \\mathbf{w}(\\mathit{king})-\\mathbf{w}(\\mathit{man}) \\approx \\mathbf{w}(\\mathit{queen})-\\mathbf{w}(\\mathit{woman}), \\end{aligned} $$\n\n(9.13)\n\nwhere w(⋅) represents the embedding of the word. We know that the semantic relation between king and man is similar to the relation between queen and woman, and the above case shows that this relational knowledge is successfully embedded into word representations. Apart from semantic relations, syntactic relations can also be well represented by word2vec, as shown in the following example:\n\n$$\\displaystyle \\begin{aligned} \\mathbf{w}(\\mathit{bigger})-\\mathbf{w}(\\mathit{big}) \\approx \\mathbf{w}(\\mathit{smaller})-\\mathbf{w}(\\mathit{small}). \\end{aligned} $$\n\n(9.14)\n\nSince word2vec implies that the implicit relations between words can be seen as translations, it is reasonable to assume that relations in KGs can also be modeled as translation embeddings. More intuitively, if we represent a word pair and its implicit relation using a triplet, e.g., 〈big, Comparative, bigger〉, we can obviously observe the similarity between word representation learning and knowledge representation learning.\n\nThe last motivation comes from the consideration of the computational complexity. On the one hand, the substantial increase in the model complexity will result in high computational costs and obscure model interpretability, and a complex model may lead to overfitting. On the other hand, the experimental results on the model complexity demonstrate that the simpler models perform almost as well as more expressive models in most knowledge-related applications [117, 186], in the condition that large-scale datasets and a relatively large amount of relations can be used for training models. As KG size increases, the computational complexity becomes the primary challenge for knowledge representation learning. The intuitive assumption of modeling relations as translations rather than matrices leads to a better trade-off between effectiveness and efficiency.\n\nSince the translation-based methods are all extended from TransE [12], we thus first introduce TransE in detail and then introduce its extensions.\n\nTransE\n\nAs illustrated in Fig. 9.6, TransE embeds entities as well as relations into the same space. In the embedding space, relations are considered as translations from head entities to tail entities. With this translation assumption, given a triplet 〈h, r, t〉 in \\(\\mathcal {T}\\), we want h + r to be the nearest neighbor of the tail embedding t. The score function of TransE is then defined as\n\n$$\\displaystyle \\begin{aligned} f(h,r,t)= -\\Vert\\mathbf{h}+\\mathbf{r}-\\mathbf{t}\\Vert. \\end{aligned} $$\n\n(9.15)\n\nTransE uses the margin-based loss function for training. Although TransE is effective and efficient, it still has several challenges to be further explored.\n\nFirst, considering that there may be multiple correct answers given two elements in a triplet, under the translation assumption in TransE, each entity has only one embedding in all triplets, which may lead to reducing the discrimination of entity embeddings. In TransE, according to the entity cardinalities of relations, all relations are classified into four categories, 1-to-1, 1-to-Many, Many-to-1, and Many-to-Many. A relation is considered as 1-to-1 if one head appears with only one tail and vice versa, 1-to-Many if a head can appear with many tails, Many-to-1 if a tail can appear with many heads, and Many-to-Many if multiple heads appear with multiple tails. Statistics demonstrate that the 1-to-Many, Many-to-1, and Many-to-Many relations occupy a large proportion. TransE performs well on 1-to-1 relations, but has problems when handling 1-to-Many, Many-to-1, and Many-to-Many relations. For instance, given the head entity William Shakespeare and the relation Notable Work, we can get a list of masterpieces, such as Hamlet, A Midsummer Night’s Dream, and Romeo and Juliet. These books share the same writer information while differing in many other fields such as theme, background, and famous roles in the book. Due to the entity William Shakespeare and the relation Notable Work, these books may be assigned similar embeddings and become indistinguishable.\n\nSecond, although the translation operation is intuitive and effective, only considering the simple one-step translation may limit the ability to model KGs. Taking entities and relations as nodes and edges, the nodes that are not directly connected may be linked by a path of more than one edge. However, TransE focuses on minimizing ∥h + r −t∥, which only utilizes the one-step relation information in KGs, regardless of the latent relationships located in long-distance paths. For example, if we know 〈The forbidden city, Located in, Beijing〉 and 〈Beijing, Capital of, China〉, we can infer that The forbidden city locates in China. TransE can be further enhanced with the favor of multistep information.\n\nThird, the representation and the score function in TransE are oversimplified for the consideration of efficiency. Therefore, TransE may not be capable enough of modeling those complex entities and relations in KGs. There are still some challenges in how to balance effectiveness and efficiency as well as avoid overfitting and underfitting.\n\nAfter TransE, there are lots of subsequent methods addressing the above challenges. Specifically, TransH [165], TransR [90], TransD [102], and TranSparse [71] are proposed to solve the challenges in modeling complex relations, PTransE is proposed to encode long-distance information located in multistep paths, and CTransR, TransG, and KG2E further extend the oversimplified model of TransE. Next, we will discuss these subsequent methods in detail.\n\nTransH\n\nTransH [165] enables an entity to have multiple relation-specific representations to address the issue that TransE cannot well model 1-to-Many, Many-to-1, and Many-to-Many relations. As we mentioned before, in TransE, entities are embedded to the same semantic embedding space and similar entities tend to be in the same cluster. However, it seems that William Shakespeare should be in the neighborhood of Isaac Newton when talking about Nationality, while it should be close to Mark Twain when talking about Occupation. To accomplish this, entities should have multiple representations in different triplets.\n\nAs illustrated in Fig. 9.7, TransH proposes a hyperplane wr for each relation, and computes the translation on the hyperplane wr. Given a triplet 〈h, r, t〉, TransH projects h and t to the corresponding hyperplane wr to get the projection h⊥ and t⊥, and r is used to connect h⊥ and t⊥:\n\n$$\\displaystyle \\begin{aligned} {} {\\mathbf{h}}_{\\perp}=\\mathbf{h}-{\\mathbf{w}}_{r}^{\\top}\\mathbf{h}{\\mathbf{w}}_{r}, \\quad {\\mathbf{t}}_{\\perp}=\\mathbf{t}-{\\mathbf{w}}_{r}^{\\top}\\mathbf{t}{\\mathbf{w}}_{r}, \\end{aligned} $$\n\n(9.16)\n\nwhere wr is a vector and ∥wr∥2 is restricted to 1. The score function is\n\n$$\\displaystyle \\begin{aligned} f(h,r,t)= - \\Vert {\\mathbf{h}}_{\\perp}+\\mathbf{r}-{\\mathbf{t}}_{\\perp}\\Vert. \\end{aligned} $$\n\n(9.17)\n\nAs for training, TransH also minimizes the margin-based loss function with negative sampling, which is similar to TransE.\n\nTransR\n\nTransR [90] takes full advantage of linear methods and translation methods. As in Eq. (9.16), TransH enables entities to have multiple relation-specific representations by projecting them to different hyperplanes, while entity embeddings and relation embeddings are still restricted in the same space, which may limit the ability for modeling entities and relations. TransR assumes that entity embeddings and relation embeddings should be in different spaces.\n\nAs illustrated in Fig. 9.8, For a triplet 〈h, r, t〉, TransR projects h and t to the relation space of r, and this projection is defined as\n\n$$\\displaystyle \\begin{aligned} {\\mathbf{h}}_r=\\mathbf{h}{\\mathbf{M}}_r, \\quad {\\mathbf{t}}_r=\\mathbf{t}{\\mathbf{M}}_r, \\end{aligned} $$\n\n(9.18)\n\nwhere Mr is the projection matrix. hr and tr stand for the relation-specific entity representations in the relation space of r, respectively. This means that each entity has a relation-specific representation for each relation, and all translation operations are processed in the relation-specific space. The score function of TransR is\n\n$$\\displaystyle \\begin{aligned} f(h,r,t) = - \\Vert {\\mathbf{h}}_{r} + \\mathbf{r} - {\\mathbf{t}}_{r}\\Vert. \\end{aligned} $$\n\n(9.19)\n\nTransR constrains the norms of the embeddings and has ∥h∥2 ≤ 1, ∥t∥2 ≤ 1, ∥r∥2 ≤ 1, ∥hr∥2 ≤ 1, ∥tr∥2 ≤ 1. As for training, TransR uses the same margin-based loss function as TransE.\n\nFurthermore, a relation should also have multiple representations since the meanings of a relation with different head and tail entities differ slightly. For example, the relation Contains the Location has head-tail patterns like city-street, country-city, and even country-university, each conveys different attribute information. To handle these subtle differences, entities for a same relation should also be projected differently.\n\nTo this end, cluster-based TransR (CTransR) is then proposed, which is an enhanced version of TransR by taking the nuance in meaning for a same relation with different entities into consideration. More specifically, for each relation, all entity pairs of the relation are first clustered into several groups. The clustering process depends on the result of t −h for each entity pair (h, t), and h and t are the embeddings learned by TransE. Then, we assign a distinct sub-relation embedding rc for each cluster of the relation r according to cluster-specific entity pairs, and the original score function of TransR is modified as\n\n$$\\displaystyle \\begin{aligned} f(h,r,t)= - \\Vert {\\mathbf{h}}_{r}+{\\mathbf{r}}_c-{\\mathbf{t}}_{r}\\Vert - \\lambda \\Vert {\\mathbf{r}}_c-\\mathbf{r}\\Vert, \\end{aligned} $$\n\n(9.20)\n\nwhere λ is a hyper-parameter to control the regularization term and ∥rc −r∥ is to make the sub-relation embedding rc and the unified relation embedding r not too distinct.\n\nTransD\n\nTransD [102] is an extension of TransR that uses dynamic mapping matrices to project entities into relation-specific spaces. TransR focuses on learning multiple relation-specific entity representations. However, TransR projects entities according to only relations, ignoring the entity diversity. Moreover, the projection operations based on matrix-vector multiplication lead to a higher computational complexity compared to TransE, which is time-consuming when applied on large-scale KGs.\n\nFor each entity and relation, TransD defines two vectors: one is used as the embedding, and the other is used to construct projection matrices to map entities to relation spaces. As illustrated in Fig. 9.9, We use h, t, r to denote the embeddings of entities and relations, and hp, tp, rp to represent the projection vectors. There are two projection matrices Mrh, Mrt used to project entities to relation spaces, and these projection matrices are dynamically constructed as\n\n$$\\displaystyle \\begin{aligned} {\\mathbf{M}}_{rh}={\\mathbf{r}}_p{\\mathbf{h}}_p^{\\top}+\\mathbf{I}, \\quad {\\mathbf{M}}_{rt}={\\mathbf{r}}_p{\\mathbf{t}}_p^{\\top}+\\mathbf{I}, \\end{aligned} $$\n\n(9.21)\n\nwhich means the projection vectors of both entities and relations are combined to determine dynamic projection matrices. The score function is\n\n$$\\displaystyle \\begin{aligned} f(h,r,t)= - \\Vert {\\mathbf{M}}_{rh}\\mathbf{h} + \\mathbf{r} - {\\mathbf{M}}_{rt}\\mathbf{t} \\Vert. \\end{aligned} $$\n\n(9.22)\n\nThese projection matrices are initialized with identity matrices by setting all the projection vectors to 0 at initialization, and the normalization constraints in TransR are also used for TransD.\n\nTransD proposes a dynamic method to construct projection matrices by considering the diversities of both entities and relations, achieving better performance compared to existing methods in knowledge completion. Moreover, TransD lowers both computational and spatial complexity compared to TransR.\n\nTranSparse\n\nTranSparse [71] is also a subsequent work of TransR. Although TransR has achieved promising results, there are still two challenges remained. One is the heterogeneity challenge. Relations in KGs differ in granularity. Some relations express complex semantics between entities, while some other relations are relatively simple. The other is the imbalance challenge. Some relations have more valid head entities and fewer valid tail entities, while some are the opposite. If we consider these challenges rather than merely treating all relations equally, we can obtain better knowledge representations.\n\nExisting methods such as TransR build projection matrices for each relation, and these projection matrices have the same parameter scale, regardless of the variety in the complexity of relations. TranSparse is proposed to address this issue. The underlying assumption of TranSparse is that complex relations should have more parameters to learn while simple relations should have fewer parameters, where the relation complexity is judged from the number of triplets or entities linked to the relation. To accomplish this, two models are proposed, including TranSparse-share and TranSparse-separate.\n\nInspired by TransR, given a relation r, TranSparse-share builds a relation-specific projection matrix Mr(θr) for the relation. Mr(θr) is sparse and the sparse degree θr mainly depends on the number of entity pairs linked to r. Suppose Nr is the number of linked entity pairs, \\(N_r^*\\) represents the maximum number of Nr, and θmin denotes the minimum sparse degree of projection matrices that 0 ≤ θmin ≤ 1. The sparse degree of relation r is defined as\n\n$$\\displaystyle \\begin{aligned} \\theta_r=1-(1-\\theta_{\\text{min}}) \\frac{N_r}{N_{r}^*}. \\end{aligned} $$\n\n(9.23)\n\nBoth head and tail entities share the same sparse projection matrix Mr(θr). The score function is\n\n$$\\displaystyle \\begin{aligned} f(h,r,t) = - \\Vert {\\mathbf{M}}_r(\\theta_r)\\mathbf{h}+\\mathbf{r}-{\\mathbf{M}}_r(\\theta_r)\\mathbf{t}\\Vert . \\end{aligned} $$\n\n(9.24)\n\nDifferent from TranSparse-share, TranSparse-separate builds two different sparse matrices Mrh(θrh) and Mrt(θrt) for head and tail entities, respectively. Then, the sparse degree θrh (or θrt) depends on the number of head (or tail) entities linked to r. We have Nrh (or Nrt) to represent the number of head (or tail) entities, as well as \\(N_{rh}^*\\) (or \\(N_{rt}^*\\)) to represent the maximum number of Nrh (or Nrt). And θmin will also be set as the minimum sparse degree of projection matrices that 0 ≤ θmin ≤ 1. We have\n\n$$\\displaystyle \\begin{aligned} \\theta_{rh}=1-(1-\\theta_{\\text{min}})N_{rh}/N_{rh}^*, \\quad \\theta_{rt}=1-(1-\\theta_{\\text{min}})N_{rt}/N_{rt}^*. \\end{aligned} $$\n\n(9.25)\n\nThe final score function of TranSparse-separate is\n\n$$\\displaystyle \\begin{aligned} \\begin{aligned} f(h,r,t) = - \\Vert {\\mathbf{M}}_{rh}(\\theta_{rh})\\mathbf{h}+\\mathbf{r}-{\\mathbf{M}}_{rt}(\\theta_{rt})\\mathbf{t}\\Vert . \\end{aligned} \\end{aligned} $$\n\n(9.26)\n\nThrough sparse projection matrices, TranSparse solves the heterogeneity challenge and the imbalance challenge simultaneously.\n\nPTransE\n\nPTransE [89] is an extension of TransE that considers multistep relational paths. All abovementioned translation methods only consider simple one-step paths (i.e., relation) to perform the translation operation, ignoring the rich global information located in the whole KGs. For instance, if we notice the multistep relational path that 〈The forbidden city, Located in, Beijing〉 \\(\\rightarrow \\) 〈Beijing, Capital of, China〉, we can inference with confidence that the triplet 〈The forbidden city, Located in, China〉 may exist. Relational paths provide us a powerful way to build better representations for KGs and even help us better understand knowledge reasoning.\n\nThere are two main challenges when encoding the information in multistep relational paths. First, how to select reliable and meaningful relational paths among enormous path candidates in KGs, since there are lots of paths that cannot indicate reasonable relations. Consider two triplet facts 〈The forbidden city, Located in, Beijing〉 \\(\\rightarrow \\) 〈Beijing, held, 2008 Summer Olympics〉, it is hard to describe the relation between The forbidden city and 2008 Summer Olympics. Second, how to model the meaningful relational paths? It is not easy to handle the problem of semantic composition in relational paths.\n\nTo select meaningful relational paths, PTransE uses a path-constraint resource allocation (PCRA) algorithm to judge the path reliability. Suppose there is information (or resource) in the head entity h which will flow to the tail entity t through some certain paths. The basic assumption of PCRA is that the reliability of the path ℓ depends on the amount of resource that eventually flows from head to tail. Formally, we denote a certain path between h and t as ℓ = (r1, …, rl). The resource that travels from h to t following the path could be represented as ). For an entity m ∈ Si, the amount of resource that belongs to m is defined as\n\n$$\\displaystyle \\begin{aligned} R_\\ell(m)=\\sum_{n \\in S_{i-1}(\\cdot ,m)}\\frac{1}{|S_i(n,\\cdot)|}R_\\ell(n), \\end{aligned} $$\n\n(9.27)\n\nwhere Si−1(⋅, m) indicates all direct predecessors of the entity m along with the relation ri in Si−1 and Si(n, ⋅) indicates all direct successors of n ∈ Si−1 with the relation ri. Finally, the amount of resource that flows to the tail Rℓ(t) is used to measure the reliability of ℓ, given the triplet 〈h, ℓ, t〉.\n\nOnce we finish selecting those meaningful relational path candidates, the next challenge is to model the semantic composition of these multistep paths. PTransE proposes three composition operations, namely, addition, multiplication, and recurrent neural networks, to get the path representation l based on the relations in ℓ = (r1, …, rl). The score function is\n\n(9.28)\n\nwhere r indicates the golden relation between h and t. Since PTransE also wants to meet the assumption in TransE that , PTransE directly utilizes r in training. The optimization objective of PTransE is\n\n$$\\displaystyle \\begin{aligned} \\arg\\min_{\\theta}\\sum_{(h,r,t)\\in \\mathcal{T}}[\\mathcal{L}(h,r,t)+\\frac{1}{Z}\\sum_{\\ell \\in P(h,t)}R(\\ell|h,t)\\mathcal{L}(\\ell,r)], \\end{aligned} $$\n\n(9.29)\n\nwhere \\(\\mathcal {L}(h,r,t)\\) is the margin-based loss function with f(h, r, t), \\(\\mathcal {L}(\\ell ,r)\\) is the margin-based score function with f(ℓ, r), and Z =∑ℓ ∈ P(h,t)R(ℓ|h, t) is a normalization factor. The reliability R(ℓ|h, t) of ℓ in (h, ℓ, t) is well considered in the overall loss function. For the path ℓ, the initial resource is set as Rℓ(h) = 1. By recursively performing PCRA from h to t through ℓ, the resource Rℓ(t) can indicate how much information can be well translated, and Rℓ(t) is thus used to measure the reliability of the path ℓ, i.e., R(ℓ|h, t) = Rℓ(t). Besides PTransE, similar ideas [47, 50] also consider multistep relational paths and demonstrate that there is plentiful information located in multistep relational paths which could significantly improve knowledge representation.\n\nKG2E\n\nKG2E [65] introduces multidimensional Gaussian distributions to represent KGs. Existing translation methods usually consider entities and relations as vectors embedded in low-dimensional spaces. However, as explained above, entities and relations in KGs are diverse at different granularities. Therefore, the margin in the margin-based loss function that is used to distinguish positive triplets from negative triplets should be more flexible due to the diversity, and the uncertainties of entities and relations should be taken into consideration.\n\nKG2E represents entities and relations with Gaussian distributions. Specifically, the mean vector denotes the central position of an entity or a relation, and the covariance matrix denotes its uncertainties. Following the score function proposed in TransE, for 〈h, r, t〉, the Gaussian distributions of entities and relations are defined as\n\n$$\\displaystyle \\begin{aligned} \\mathbf{h}\\sim \\mathcal{N}(\\boldsymbol\\mu_h,\\boldsymbol{\\varSigma}_h), \\quad \\mathbf{t} \\sim \\mathcal{N}(\\boldsymbol\\mu_t,\\boldsymbol{\\varSigma}_t), \\quad \\mathbf{r} \\sim \\mathcal{N}(\\boldsymbol\\mu_r,\\boldsymbol{\\varSigma}_r). \\end{aligned} $$\n\n(9.30)\n\nNote that the covariances are diagonal for efficient computation. KG2E hypothesizes that head and tail entities are independent with specific relations; then, the translation could be defined as\n\n$$\\displaystyle \\begin{aligned} \\mathbf{e}\\sim \\mathcal{N}(\\boldsymbol\\mu_h-\\boldsymbol\\mu_t,\\boldsymbol{\\varSigma}_h+\\boldsymbol{\\varSigma}_t). \\end{aligned} $$\n\n(9.31)\n\nTo measure the dissimilarity between e and r, KG2E considers both asymmetric similarity and symmetric similarity, and then proposes two methods.\n\nThe asymmetric similarity is based on the KL divergence between e and r, which is a typical method to measure the similarity between two distributions. The score function is\n\n$$\\displaystyle \\begin{aligned} \\begin{aligned} f(h,r,t)&= - D_{\\text{KL}}(\\mathbf{e} \\| \\mathbf{r} )\\\\ &=- \\int_{x\\in \\mathbb{R}^{d}}\\mathcal{N}(x;\\boldsymbol\\mu_r,\\boldsymbol{\\varSigma}_r)\\log\\frac{\\mathcal{N}(x;\\boldsymbol\\mu_e,\\boldsymbol{\\varSigma}_e)}{\\mathcal{N}(x;\\boldsymbol\\mu_r,\\boldsymbol{\\varSigma}_r)}dx\\\\ &=- \\frac{1}{2}\\{\\text{tr}(\\boldsymbol{\\varSigma}_r^{-1}\\boldsymbol{\\varSigma}_r)+(\\boldsymbol\\mu_r-\\boldsymbol\\mu_e)^{\\top}\\boldsymbol{\\varSigma}_r^{-1}(\\boldsymbol\\mu_r-\\boldsymbol\\mu_e)- \\log\\frac{\\text{det}(\\boldsymbol{\\varSigma}_e)}{\\text{det}(\\boldsymbol{\\varSigma}_r)}-d\\}, \\end{aligned} \\end{aligned} $$\n\n(9.32)\n\nwhere tr(Σ) indicates the trace of Σ and Σ−1 indicates the inverse of Σ.\n\nThe symmetric similarity is built on the expected likelihood and probability product kernel. KE2G takes the inner product between the probability density functions of e and r as the measurement of similarity. The logarithm of score function defined is\n\n$$\\displaystyle \\begin{aligned} \\begin{aligned} f(h,r,t)&= - \\int_{x\\in \\mathbb{R}^{d}}\\mathcal{N}(x;\\boldsymbol\\mu_e,\\boldsymbol{\\varSigma}_e)\\mathcal{N}(x;\\boldsymbol\\mu_r,\\boldsymbol{\\varSigma}_r)dx\\\\ &=- \\log \\mathcal{N}(0;\\boldsymbol\\mu_e-\\boldsymbol\\mu_r,\\boldsymbol{\\varSigma}_e+\\boldsymbol{\\varSigma}_r)\\\\ &=- \\frac{1}{2}\\{(\\boldsymbol\\mu_e-\\boldsymbol\\mu_r)^{\\top}(\\boldsymbol{\\varSigma}_e+\\boldsymbol{\\varSigma}_r)^{-1}(\\boldsymbol\\mu_e-\\boldsymbol\\mu_r)\\\\ &\\quad +\\log \\text{det}(\\boldsymbol{\\varSigma}_e+\\boldsymbol{\\varSigma}_r)+d\\log (2\\pi)\\}. \\end{aligned} \\end{aligned} $$\n\n(9.33)\n\nThe optimization objective of KG2E is also margin-based similar to TransE. Both asymmetric and symmetric similarities are constrained by some regularizations to avoid overfitting:\n\n$$\\displaystyle \\begin{aligned} \\forall l \\in \\mathcal{E} \\cup \\mathcal{R}, \\quad \\Vert \\boldsymbol\\mu_l\\Vert _2 \\leq 1, \\quad c_{\\text{min}}\\mathbf{I} \\leq \\boldsymbol{\\varSigma}_l \\leq c_{\\text{max}}\\mathbf{I}, \\quad c_{\\text{min}} > 0, \\end{aligned} $$\n\n(9.34)\n\nwhere cmin and cmax are the hyper-parameters as the restriction values for covariance.\n\nTransG\n\nTransG [174] discusses the problem that some relations in KGs such as Contains the Location or Part of may have multiple sub-meanings, which is also discussed in TransR. In fact, these complex relations could be divided into several more precise sub-relations. To address this issue, CTransR is proposed with a preprocessing that clusters sub-relation according to entity pairs.\n\nAs illustrated in Fig. 9.10, TransG assumes that the embeddings containing several semantic components should follow a Gaussian mixture model. The generative process is:\n\n1.\n\nFor each entity e ∈ E, TransG sets a standard normal distribution: \\(\\boldsymbol {\\mu }_e\\sim \\mathcal {N}(\\mathbf {0,I})\\).\n\n2.\n\nFor a triplet 〈h, r, t〉, TransG uses the Chinese restaurant process (CRP) to automatically detect semantic components (i.e., sub-meanings in a relation): \\(\\pi _{r,n}\\sim \\operatorname {CRP}(\\beta )\\). πr,n is the weight of the i-th component generated by the Chinese restaurant process from the data.\n\n3.\n\nDraw the head embedding from a standard normal distribution: \\(\\mathbf {h}\\sim \\mathcal {N}(\\boldsymbol {\\mu }_h, \\sigma _h^2\\mathbf {I})\\).\n\n4.\n\nDraw the tail embedding from a standard normal distribution: \\(\\mathbf {t}\\sim \\mathcal {N}(\\boldsymbol {\\mu }_t, \\sigma _t^2\\mathbf {I})\\).\n\n5.\n\nCalculate the relation embedding for this semantic component: μr,n = t −h.\n\nFinally, the score function is\n\n$$\\displaystyle \\begin{aligned} \\begin{aligned} f(h,r,t)\\propto \\sum_{n=1}^{N_r}\\pi_{r,n}\\mathcal{N}(\\boldsymbol{\\mu}_{r,n};\\boldsymbol{\\mu}_t-\\boldsymbol{\\mu}_h,(\\sigma_h^2+\\sigma_t^2)\\mathbf{I}), \\end{aligned} \\end{aligned} $$\n\n(9.35)\n\nin which Nr is the number of semantic components of the relation r.\n\n9.3.3 Neural Representation\n\nWith the development of neural networks, several efforts have been devoted to exploring neural networks for modeling KGs. Next, we will introduce how to represent KGs with neural networks.\n\nSingle Layer Model (SLM)\n\nInspired by the previous works in representing KGs, SLM represents both entities and relations in low-dimensional spaces, and uses relation-specific matrices to project entities to relation spaces. Similar to the linear method SE, the score function of SLM is\n\n$$\\displaystyle \\begin{aligned} \\begin{aligned} f(h,r,t)={\\mathbf{r}}^{\\top}\\operatorname{tanh}({\\mathbf{M}}_{r,1}\\mathbf{h}+{\\mathbf{M}}_{r,2}\\mathbf{t}), \\end{aligned} \\end{aligned} $$\n\n(9.36)\n\nwhere h, \\(\\mathbf {t} \\in \\mathbb {R}^{d_e}\\) represent head and tail embeddings, \\(\\mathbf {r} \\in \\mathbb {R}^{d_r}\\) represents relation embedding, and Mr,1, \\({\\mathbf {M}}_{r,2} \\in \\mathbb {R}^{d_e \\times d_r}\\) stand for the relation-specific matrices.\n\nNeural Tensor Network (NTN)\n\nAlthough SLM has introduced relation embeddings as well as a nonlinear neural layer to build the score function, the representation capability is still restricted. NTN [143] is then proposed by introducing tensors into the SLM framework, which can be seen as an enhanced version of SLM. Besides the original linear neural network layer that projects entities to the relation space, NTN adds another tensor-based neural layer which combines head and tail embeddings with a relation-specific tensor. The score function of NTN is\n\n$$\\displaystyle \\begin{aligned} f(h,r,t)={\\mathbf{r}}^{\\top}\\operatorname{tanh}({\\mathbf{h}}^{\\top}\\overrightarrow{\\mathbf{M}}_r\\mathbf{t}+{\\mathbf{M}}_{r,1}\\mathbf{h}+{\\mathbf{M}}_{r,2}\\mathbf{t}+{\\mathbf{b}}_r), \\end{aligned} $$\n\n(9.37)\n\nwhere \\(\\overrightarrow {\\mathbf {M}}_r \\in \\mathbb {R}^{d_e \\times d_e \\times d_r}\\) is a three-way relation-specific tensor, br is the bias, and Mr,1, \\({\\mathbf {M}}_{r,2} \\in \\mathbb {R}^{d_e \\times d_r}\\) are the relation-specific matrices. Note that SLM can be seen as a simplified version of NTN if the tensor and the bias are set to zero.\n\nBesides improving the score function, NTN also attempts to utilize the latent textual information located in entity names and successfully achieves significant improvements. Differing from previous methods that provide each entity with a vector, NTN represents each entity as the average of its entity name’s word embeddings. For example, the entity Bengal tiger will be represented as the average word embeddings of Bengal and tiger. It is apparent that the entity name will provide valuable information for understanding an entity, since Bengal tiger may come from Bengal and be related to other tigers.\n\nNTN utilizes tensor-based neural networks to model triplet facts and achieves excellent success. However, the overcomplicated method leads to high computational complexity compared to other methods, and the vast number of parameters limits the performance on sparse and large-scale KGs.\n\nNeural Association Model (NAM)\n\nNAM [94] adopts multilayer nonlinear activation to model relations. More specifically, two structures are used by NAM to represent KGs: deep neural network (DNN) and relation modulated neural network (RMNN).\n\nNAM-DNN adopts a MLP with L layers to operate knowledge embeddings:\n\n$$\\displaystyle \\begin{aligned} {\\mathbf{z}}^{k}=\\text{Sigmoid}({\\mathbf{M}}^{k}{\\mathbf{z}}^{k-1}+{\\mathbf{b}}^{k}),\\ \\ k = 1, \\cdots, L, \\end{aligned} $$\n\n(9.38)\n\nwhere z0 = [h;r] is the concatenation of h and r, Mk is the weight matrix of the k-th layer, and bk is the bias vector of the k-th layer. Finally, NAM-DNN defines the score function as\n\n$$\\displaystyle \\begin{aligned} f(h,r,t) = \\text{Sigmoid}({\\mathbf{t}}^\\top{\\mathbf{z}}^{L}). \\end{aligned} $$\n\n(9.39)\n\nAs compared with NAM-DNN, NAM-RMNN additionally feeds the relation embedding r into the model:\n\n$$\\displaystyle \\begin{aligned} {\\mathbf{z}}^{k}=\\text{Sigmoid}({\\mathbf{M}}^{k}{\\mathbf{z}}^{k-1}+{\\mathbf{B}}^{k}\\mathbf{r}),\\ \\ k = 1, \\cdots, L, \\end{aligned} $$\n\n(9.40)\n\nwhere Mk and Bk indicate the weight and bias matrices. Finally, NAM-RMNN defines the score function as\n\n$$\\displaystyle \\begin{aligned} f(h,r,t) = \\text{Sigmoid}({\\mathbf{t}}^\\top{\\mathbf{z}}^{L}+{\\mathbf{B}}^{L+1}\\mathbf{r}). \\end{aligned} $$\n\n(9.41)\n\nConvolutional 2D Embeddings (ConvE)\n\nConvE [32] uses 2D convolutional operations over embeddings to model KGs. Specifically, ConvE uses convolutional and fully connected layers to model interactions between entities and relations. After that, the obtained features are flattened and transformed by a fully connected layer, and the inner product between the final feature and the tail entity embeddings is used to build the score function:\n\n$$\\displaystyle \\begin{aligned} f(h,r,t) = N\\big(\\operatorname{vec}(N([\\bar{\\mathbf{h}};\\bar{\\mathbf{r}}] * \\omega)) \\mathbf{W}\\big) \\cdot \\mathbf{t}, \\end{aligned} $$\n\n(9.42)\n\nwhere \\([\\bar {\\mathbf {h}};\\bar {\\mathbf {r}}]\\) is the concatenation of \\(\\bar {\\mathbf {h}}\\) and \\(\\bar {\\mathbf {r}}\\), N(⋅) is a neural layer, ∗ denotes the convolution operator, and \\(\\operatorname {vec}(\\cdot )\\) means compressing a matrix into a vector. \\(\\bar {\\mathbf {h}}\\) and \\(\\bar {\\mathbf {r}}\\) denote the 2D-reshaping versions of h and r, respectively: if \\(\\mathbf {h}, \\mathbf {r} \\in \\mathbb {R}^{d}\\), then \\(\\bar {\\mathbf {h}}, \\bar {\\mathbf {r}} \\in \\mathbb {R}^{d_{a} \\times d_{b}}\\), where d = dadb.\n\nTo some extent, ConvE can be seen as an improvement model based on HolE. Compared with HolE, ConvE adopts multiple neural layers to learn nonlinear features and is thus more expressive than HolE.\n\nRelational Graph Convolutional Networks (RGCN)\n\nRGCN [136] is an extension of GCNs to model KGs. The core idea of RGCN is to formalize modeling KGs as message passing. Therefore, in RGCN, the representations of entities and relations are the results of information propagation and fusion at multiple layers. Specifically, given an entity h, its embedding at the (k + 1)-th layer is\n\n$$\\displaystyle \\begin{aligned} {\\mathbf{h}}^{k+1} = \\text{Sigmoid}\\left( \\sum_{r\\in\\mathcal{R}} \\sum_{t\\in\\mathcal{N}_h^r} \\frac{1}{c_h^r}{\\mathbf{W}}^{k}_r{\\mathbf{t}}^{k} + \\tilde{\\mathbf{W}}^{k}{\\mathbf{t}}^{k}\\right), \\end{aligned} $$\n\n(9.43)\n\nwhere \\(\\mathcal {N}_h^r\\) denotes the neighbor set of h under the relation r and \\(c_h^r\\) is the normalization factor. \\(c_h^r\\) can be either learned or preset, and normally \\(c_h^r = |\\mathcal {N}_h^r|\\).\n\nNote that RGCN only aims to obtain more expressive features for entities and relations. Therefore, based on the output features of RGCN, any score function mentioned above can be used here, such as combining the features of RGCN and the score function of TransE to learn knowledge representations.\n\n9.3.4 Manifold Representation\n\nSo far, we have introduced linear methods, translation methods, and neural methods for knowledge representation. All these methods project entities and relations into low-dimensional embedding spaces, and seek to improve the flexibility and variety of entity and relation representations. Although these methods have achieved promising results, they assume that the geometry of the embedding spaces for entities and relations are all Euclidean. However, the basic Euclidean geometry may not be the optimal geometry to model the complex structure of KGs. Next, we will introduce several typical manifold methods that aim to use more flexible and powerful geometric spaces to carry representations.\n\nManifoldE\n\nManifoldE [173] considers the possible positions of golden candidates for representations in spaces as a manifold rather than a point. The overall score function of ManifoldE is\n\n$$\\displaystyle \\begin{aligned} \\begin{aligned} f(h,r,t)=-\\Vert M(h,r,t)-D_r^2\\Vert, \\end{aligned} \\end{aligned} $$\n\n(9.44)\n\nin which \\(D_r^2\\) is a relation-specific manifold parameter. Two kinds of manifolds are then proposed in ManifoldE. ManifoldE-Sphere is a straightforward manifold that supposes t should be located in the sphere which has h + r to be the center and Dr to be the radius. We have:\n\n$$\\displaystyle \\begin{aligned} M(h,r,t)=\\Vert \\mathbf{h}+\\mathbf{r}-\\mathbf{t}\\Vert. \\end{aligned} $$\n\n(9.45)\n\nA tail may correspond to many different head-relation pairs, and the manifold assumption requires that the tail lays in all the manifolds of these head-relation pairs, i.e., lays in the intersection of these manifolds. However, two spheres can only intersect only under some strict conditions. Therefore, the hyperplane is utilized because it is easier for two hyperplanes to intersect. The function of ManifoldE-Hyperplane is\n\n$$\\displaystyle \\begin{aligned} M(h,r,t)=(\\mathbf{h}+{\\mathbf{r}}_h)^{\\top}(\\mathbf{t}+{\\mathbf{r}}_t), \\end{aligned} $$\n\n(9.46)\n\nin which rh and rt represent the two entity-specific embeddings of the relation r. This indicates that for a triplet 〈h, r, t〉, the tail entity t should locate in the hyperplane whose normal vector is h + rh and intercept is \\(D_r^2\\). Furthermore, ManifoldE-Hyperplane considers absolute values in M(h, r, t) as |h + rh|⊤|t + rt| to double the solution number of possible tail entities. For both manifolds, ManifoldE applies a kernel form on the reproducing kernel Hilbert space.\n\nComplEx\n\nComplEx [153] employs an eigenvalue decomposition model which makes use of complex-valued embeddings, i.e., \\(\\mathbf {h}, \\mathbf {r}, \\mathbf {t} \\in \\mathbb {C}^d\\). Complex embeddings can well handle binary relations, such as the symmetric and antisymmetric relations. The score function of ComplEx is\n\n$$\\displaystyle \\begin{aligned} \\begin{aligned} f(h,r,t)&= \\text{Re}(\\langle \\mathbf{r}, \\mathbf{h}, \\mathbf{t}\\rangle)\\\\ &= \\langle\\text{Re}(\\mathbf{r}), \\text{Re}(\\mathbf{h}), \\text{Re}(\\mathbf{t})\\rangle + \\langle\\text{Re}(\\mathbf{r}), \\text{Im}(\\mathbf{h}), \\text{Im}(\\mathbf{t})\\rangle\\\\ & \\quad - \\langle\\text{Im}(\\mathbf{r}), \\text{Re}(\\mathbf{h}), \\text{Im}(\\mathbf{t})\\rangle - \\langle\\text{Im}(\\mathbf{r}), \\text{Im}(\\mathbf{h}), \\text{Re}(\\mathbf{t})\\rangle, \\end{aligned} \\end{aligned}$$\n\nwhere 〈x, y, z〉 =∑ixiyizi denotes the trilinear dot product, Re(x) is the real part of x, and Im(x) is the imaginary part of x. In fact, ComplEx can be viewed as a generalization of RESCAL that uses complex embeddings to model KGs.\n\nRotatE\n\nSimilar to ComplEx, RotatE [149] also represents KGs with complex-valued embeddings. RotatE defines relations as rotations from head entities to tail entities, which makes it easier to learn various relation patterns such as symmetry, antisymmetry, inversion, and composition. The element-wise (Hadamard) product can naturally represent the rotation process in the complex-valued space. Therefore, the score function of RotatE is\n\n$$\\displaystyle \\begin{aligned} f(h,r,t) = -\\Vert \\mathbf{h} \\odot \\mathbf{r} - \\mathbf{t}\\Vert, \\end{aligned} $$\n\n(9.47)\n\nwhere \\(\\mathbf {h}, \\mathbf {r}, \\mathbf {t} \\in \\mathbb {C}^d\\) and ⊙ denotes the element-wise (Hadamard) product. RotatE is simple and achieves quite good performance. Compared with previous methods, it is the first model that is theoretically able to model all the above four patterns (symmetry, antisymmetry, inversion, and composition). On the basis of RotatE, Zhang et al. [203] further introduce hypercomplex spaces to represent entities and relations, and achieves better performance.\n\nMuRP\n\nMuRP [4] proposes to embed the entities in the hyperbolic space since hyperbolic space is shown to be more suitable to represent hierarchical data than Euclidean space. Specifically, they embed the entities to the Poincaré model [130] (a typical geometric model in hyperbolic space), and exploit the Mobiüs transformations in the Poincaré model as the alternatives to vector-matrix multiplication and vector addition in Euclidean space. The score function of MuRP is\n\n$$\\displaystyle \\begin{aligned} \\begin{aligned} f(h, r, t)&=d_{\\mathbb{P}}({\\mathbf{h}}^{(r)}, {\\mathbf{t}}^{(r)})^2-b_h-b_t\\\\ &=d_{\\mathbb{P}}(\\exp_{\\mathbf{0}}^c({\\mathbf{M}}_r\\log_{\\mathbf{0}}^c(\\mathbf{h})), \\mathbf{t}\\oplus\\mathbf{r})-b_h-b_t, \\end{aligned} \\end{aligned} $$\n\n(9.48)\n\nwhere \\(d_{\\mathbb {P}}(\\cdot , \\cdot )\\) calculates the distance between two points in the Poincaré model, Mr is the transform matrix for the relation r, r is the translation vector of the relation r, and bh and bt are biases for the head and tail entities respectively. \\(\\exp _{\\mathbf {0}}^c\\) is the exponential mapping at 0 in the Poincaré model of the curvature c, and it maps points in the tangent space at 0 (an Euclidean subspace) to the Poincaré model. \\(\\log _{\\mathbf {0}}^c\\) is the logarithmic mapping at 0 in the Poincaré model of the curvature c, and is the inverse mapping for \\(\\exp _{\\mathbf {0}}^c\\). MuRP with a dimension as low as 40 achieves comparable results to the Euclidean models with dimension greater than 100, showing the effectiveness of hyperbolic space in encoding relational knowledge.\n\nHyboNet\n\nHyboNet [23] argues that previous hyperbolic methods such as MuRP only introduce the hyperbolic geometric for embeddings, but still perform linear transformations in tangent spaces (Euclidean subspaces), significantly limiting the capability of hyperbolic models. Inspired by the Lorentz transformation in Physics, HyboNet proposes a linear transformation in the Lorentz model [130] (another typical geometric model to build hyperbolic spaces) to avoid the introduction of exponential mapping and logarithmic mapping when transforming embeddings, significantly speeding up the network and stabilizing the computation. The score function of HyboNet is\n\n$$\\displaystyle \\begin{aligned} f(h, r, t)=d_{\\mathbb{L}}^2(g_r(\\mathbf{h}), \\mathbf{t})-b_h-b_t-\\delta, \\end{aligned} $$\n\n(9.49)\n\nwhere \\(d_{\\mathbb {L}}^2\\) is the squared Lorentzian distance between two points in Lorentz model, gr is the relation-specific Lorentz linear transformation, bh, bt are the biases for the head and tail entities, respectively, and δ is a hyper-parameter used to make the training process more stable.\n\n9.3.5 Contextualized Representation\n\nWe live in a complicated pluralistic real world where we can get information from different senses. Due to this, we can learn knowledge not only from structured KGs but also from text, schemas, images, and rules. Despite the massive size of existing KGs, there is a large amount of knowledge in the real world that may not be included in the KGs. Integrating multisource information provides a novel approach for learning knowledge representations not only from the internal structured information of KGs but also from other external information. Moreover, exploring multisource information can help further understand human cognition with different senses in the real world. Next, we will introduce typical methods that utilize multisource information to enhance knowledge representations.\n\nKnowledge Representation with Text\n\nTextual information is one of the most common and widely used information for knowledge representation. Wang et al. [164] attempt to utilize textual information by jointly learning representations of entities, relations, and words within the same low-dimensional embedding space. The method contains three parts: the knowledge model, the text model, and the alignment model. The knowledge model is learned on the triplets of KGs using TransE, while the text model is learned on the text using skip-gram. As for the alignment model, two methods are proposed to align entity and word representations by utilizing Wikipedia anchors and entity names, respectively.\n\nModeling entities and words into the same embedding space has the merit of encoding the information in both KGs and plain text in a unified semantic space. However, Wang’s joint model mainly depends on the completeness of Wikipedia anchors and suffers from the ambiguities of many entity names. To address these issues, Zhong et al. [207] further improve the alignment model with entity descriptions, assuming that entities should have similar semantics to their corresponding descriptions.\n\nDifferent from the above joint models that merely consider the alignments between KGs and textual information, description-embodied knowledge representation learning (DKRL) [176] can directly build knowledge representations from entity descriptions. Specifically, DKRL provides two kinds of knowledge representations: for each entity h, the first is the structure-based representation hS, which can be learned based on the structure of KGs, and the second is the description-based representation hD that derives from its description. The score function of DKRL derives from translation methods, and we have:\n\n$$\\displaystyle \\begin{aligned} \\begin{aligned} f(h,r,t)\\kern-1pt=\\kern-1pt-(\\Vert {\\mathbf{h}}_S\\kern-1pt+\\kern-1pt \\mathbf{r}-{\\mathbf{t}}_S\\Vert \\kern-1pt+\\kern-1pt \\Vert {\\mathbf{h}}_S\\kern-1pt+\\kern-1pt \\mathbf{r}-{\\mathbf{t}}_D\\Vert\\kern-1pt +\\kern-1pt \\Vert {\\mathbf{h}}_D\\kern-1pt+\\kern-1pt \\mathbf{r}-{\\mathbf{t}}_S\\Vert \\kern-1pt+\\kern-1pt \\Vert {\\mathbf{h}}_D\\kern-1pt+\\kern-1pt \\mathbf{r}-{\\mathbf{t}}_D\\Vert). \\end{aligned} \\end{aligned} $$\n\n(9.50)\n\nAs shown in Fig. 9.11, the description-based representations are constructed via CBOW or CNNs that can encode rich textual information from plain text into representations.\n\nCompared with conventional non-contextualized methods, the representations learned by DKRL are built with both structured and textual information and thus could perform better. Besides, DKRL can represent an entity even if it is not in the training set as long as there are a few sentences to describe this entity. Therefore, with millions of new entities emerging every day, DKRL can handle these new entities based on the setting of zero-shot learning.\n\nKnowledge Representation with Types\n\nEntity types, as hierarchical schemas, can provide rich structured information to understand entities. Generally, there are two paths for using entity types for knowledge representations: type-constrained methods and type-augmented methods.\n\nType-Constrained Methods\n\nKrompaß et al. [78] take type information as constraints to improve existing methods like RESCAL and TransE via type constraints. It is intuitive that for a particular relation, the head and tail entities associated with this relation can only be of some specific types. For example, the head entity of the relation Writes Books should be a person (more precisely, an author), and the tail entity should be a book.\n\nWith type constraints, in RESCAL, the original factorization \\({\\mathbf {X}}_n \\approx \\mathbf {E}{\\mathbf {M}}_{r_n}{\\mathbf {E}}^{\\top }\\) in Eq.( 9.8) can be modified to\n\n$$\\displaystyle \\begin{aligned} \\mathbf{X'}_n \\approx {\\mathbf{E}}_{[\\mathcal{H}_{r_n},:]}{\\mathbf{M}}_{r_n}{\\mathbf{E}}_{[\\mathcal{T}_{r_n},:]}^{\\top}, \\end{aligned} $$\n\n(9.51)\n\nwhere \\(\\mathcal {H}_{r_n}\\), \\(\\mathcal {T}_{r_n}\\) are the entity sets fitting the type constraints of the n-th relation rn in \\(\\mathcal {R}\\), and X′n is a sparse adjacency matrix of the shape \\(|\\mathcal {H}_{r_n}| \\times |\\mathcal {T}_{r_n}|\\). Intuitively, only the entities that fit type constraints will be considered during the factorization process.\n\nWith type constraints, in TransE, negative samples with higher quality can be generated. Learning knowledge representations need negative samples, and negative samples are often generated by randomly replacing triplets’ head or tail entities. Given a triplet 〈h, r, t〉, with type constraints, its negative samples \\(\\langle \\tilde {h}, \\tilde {r}, \\tilde {t} \\rangle \\) need to satisfy\n\n$$\\displaystyle \\begin{aligned} \\tilde{h} \\in \\mathcal{H}_r \\subseteq \\mathcal{E}, \\quad \\tilde{t} \\in \\mathcal{T}_r \\subseteq \\mathcal{E}. \\end{aligned} $$\n\n(9.52)\n\nIntuitively, for an entity whose type does not match the relation r, it will not be used to construct negative samples. The negative samples constructed with type constraints are more confusing, which is beneficial for learning more robust and effective representations.\n\nType-Augmented Methods\n\nIn addition to the simplicity and effectiveness of using the type information as constraints, the representation can be further enhanced by using the type information directly as additional information in the learning. Instead of merely viewing type information as type constraints, type-embodied knowledge representation learning (TKRL) is proposed [177], utilizing hierarchical type structures to instruct the construction of projection matrices. Inspired by TransR that every entity should have multiple representations in different relation spaces, the score function of TKRL is\n\n$$\\displaystyle \\begin{aligned} f(h,r,t)= - \\Vert {\\mathbf{M}}_{rh}\\mathbf{h}+\\mathbf{r}-{\\mathbf{M}}_{rt}\\mathbf{t}\\Vert , \\end{aligned} $$\n\n(9.53)\n\nin which Mrh and Mrt are two projection matrices for h and t that depend on their corresponding hierarchical types in this triplet. Two hierarchical encoders are proposed to learn the above projection matrices, regarding all sub-types in the hierarchy as projection matrices, where the recursive hierarchical encoder (RHE) is based on the matrix multiplication operation, and the weighted hierarchical encoder (WHE) is based on the matrix summation operation.\n\nFigure 9.12 shows a simple illustration of TKRL. Taking a type hierarchy c with m layers for instance, c(i) is the i-th sub-type. Considering the sub-type at the first layer is the most precise and the sub-type at the last layer is the most general, TKRL can get type-specific entity representations at different granularities following the hierarchical structure, and the projection matrices can be formalized as\n\n$$\\displaystyle \\begin{aligned} \\begin{aligned} {\\mathbf{M}}_{RHE_c} &=\\prod_{i=1}^{m}{{\\mathbf{M}}_{c^{(i)}}}={\\mathbf{M}}_{c^{(1)}}{\\mathbf{M}}_{c^{(2)}}\\dots{\\mathbf{M}}_{c^{(m)}}, \\\\ {\\mathbf{M}}_{WHE_c} &=\\sum_{i=1}^{m}{\\beta_i{\\mathbf{M}}_{c^{(i)}}}=\\beta_{1}{\\mathbf{M}}_{c^{(1)}} +\\dots+\\beta_{m}{\\mathbf{M}}_{c^{(m)}}, \\end{aligned} \\end{aligned} $$\n\n(9.54)\n\nwhere \\({\\mathbf {M}}_{c^{(i)}}\\) stands for the projection matrix of the i-th sub-type of the hierarchical type c and βi is the corresponding weight of the sub-type. Taking RHE as an example, given the entity William Shakespeare, it is first projected to a general sub-type space like human and then sequentially projected to a more precise sub-type like author or English author.\n\nKnowledge Representation with Images\n\nHuman cognition is highly related to the visual information of objects in the real world. For entities in KGs, their corresponding images can provide intuitive visual information about their appearance, which may give important hints about some attributes of the entities. For instance, Fig. 9.13 shows the images of Suit of armour and Armet. From these images, we can easily infer the fact 〈Suit of armour, Has a Part, Armet〉 directly.\n\nImage-embodied knowledge representation learning (IKRL) [175] is proposed to consider visual information when learning knowledge representations. Inspired by the abovementioned DKRL, for each entity h, IKRL also proposes the image-based representation hI besides the original structure-based representation hS, and jointly learns these entity representations simultaneously within the translation-based framework:\n\n$$\\displaystyle \\begin{aligned} f(h,r,t)\\kern-0.5pt=\\kern-0.5pt -(\\Vert {\\mathbf{h}}_S\\kern-0.5pt+\\kern-0.5pt \\mathbf{r}-{\\mathbf{t}}_S\\Vert \\kern-0.5pt+\\kern-0.5pt\\Vert {\\mathbf{h}}_S+\\mathbf{r}-{\\mathbf{t}}_I\\Vert \\kern-0.5pt+\\kern-0.5pt \\Vert {\\mathbf{h}}_I+\\mathbf{r}-{\\mathbf{t}}_S\\Vert +\\Vert {\\mathbf{h}}_I+\\mathbf{r}-{\\mathbf{t}}_I\\Vert). \\end{aligned} $$\n\n(9.55)\n\nMore specifically, IKRL uses CNNs to obtain the representations of all entity images, and then uses a matrix to project image representations from the image embedding space to the entity embedding space. Since one entity may have multiple images, IKRL uses an attention-based method to highlight those most informative images. IKRL not only shows the importance of visual information for representing entities but also shows the possibility of finding a unified space to represent heterogeneous and multimodal information.\n\nKnowledge Representation with Logic Rules\n\nTypical KGs store knowledge in the form of triplets with one relation linking two entities. Most existing knowledge representation methods only consider the information of triplets independently, ignoring the possible interactions and relations between different triplets. Logic rules, which are certain kinds of summaries derived from human prior knowledge, could help us with knowledge reasoning. For instance, given the triplet 〈 Beijing, Capital of, China〉, we can easily infer the triplet 〈Beijing, Located in, China〉 with high confidence, since we know the logic rule Capital of ⇒ Located in. To this end, various efforts have been devoted to exploring logic rules for KGs [5, 127, 162]. Here we introduce a typical translation method that jointly learns knowledge representations and logic rules – KALE [52]. KALE can rank all possible logic rules based on the results pre-trained by TransE, and then manually filter useful rules to improve knowledge representations.\n\nThe joint learning of KALE consists of two parts: triplet modeling and rule modeling. For the triplet modeling, KALE defines its score function following the translation assumption as\n\n$$\\displaystyle \\begin{aligned} \\begin{aligned} f(h,r,t)=1-\\frac{1}{3\\sqrt{d}}\\Vert \\mathbf{h}+\\mathbf{r}-\\mathbf{t}\\Vert , \\end{aligned} \\end{aligned} $$\n\n(9.56)\n\nin which d stands for the dimension of knowledge embeddings. f(h, r, t) takes a value in [0, 1], aiming to map discrete Boolean values (false or true) into a continuous space ([0, 1]). For the rule modeling, KALE uses the t-norm fuzzy logics [55] that compute the truth value of a complex formula from the truth values of its constituents. Especially, KALE focuses on two typical types of logic rules. The first rule is ∀h, t : 〈h, r1, t〉⇒〈h, r2, t〉 (e.g., given 〈Beijing, Capital of, China〉, we can infer that 〈Beijing, Located in, China〉). KALE represents the score function of this logic rule l1 via specific t-norm logical connectives as\n\n$$\\displaystyle \\begin{aligned} \\begin{aligned} f(l_1)=f(h,r_1,t) f(h,r_2,t)- f(h,r_1,t)+1. \\end{aligned} \\end{aligned} $$\n\n(9.57)\n\nThe second rule is ∀h, e, t : 〈h, r1, e〉∧〈e, r2, t〉⇒〈h, r3, t〉 (e.g., given 〈Tsinghua, Located in, Beijing〉 and 〈Beijing, Located in, China〉, we can infer that 〈Tsinghua, Located in, China〉). And the second score function is defined as\n\n$$\\displaystyle \\begin{aligned} \\begin{aligned} f(l_2)= f(h,r_1,e) f(e,r_2,t) f(h,r_3,t)- f(h,r_1,e) f(e,r_2,t)+1. \\end{aligned} \\end{aligned} $$\n\n(9.58)\n\nThe joint training contains all positive formulas, including triplet facts and logic rules. Note that for the consideration of rule qualities, KALE ranks all possible logic rules by their truth values with pre-trained TransE and manually filters some rules.\n\n9.3.6 Summary\n\nKnowledge representation learning is the cornerstone of applying knowledge for NLP tasks. Knowledge can be incorporated into NLP tasks in a high-quality manner only with good knowledge representations. In this section, we introduce five directions of existing efforts to obtain distributed knowledge representations: (1) linear methods, where relations are represented as linear transformations between entities, (2) translation methods, where relations are represented as additive translations between entities, (3) neural methods, where neural networks parameterize the interactions between entities and relations, (4) manifold methods, where representations are learned in more flexible and powerful geometric spaces instead of the basic Euclidean geometry, and (5) contextualized methods, where representations are learned under complex contexts.\n\nIn summary, from simple methods like SE and TransE, to more sophisticated methods that use neural networks (e.g., ConvE), the hyperbolic geometry (e.g., HyboNet), and textual information (e.g., DKRL), all these methods can provide effective knowledge representations. These methods lay a solid foundation for further knowledge-guided NLP and knowledge acquisition, which will be introduced in later sections. Note that more sophisticated methods do not necessarily lead to a better application in NLP tasks. Researchers still need to choose the appropriate knowledge representation learning method according to the characteristics of specific tasks and the balance between computational efficiency and representation quality.\n\nAn effective NLP agent is expected to accurately and deeply understand user demands, and appropriately and flexibly give responses and solutions. Such kind of work can only be done supported by certain forms of knowledge. To this end, knowledge-guided NLP has been widely explored in recent years. Figure 9.14 shows a brief pipeline of utilizing knowledge for NLP tasks. In this pipeline, we first need to extract knowledge from heterogeneous data sources and store the extracted knowledge with knowledge systems (e.g., KGs). Next, we need to project knowledge systems into low-dimensional continuous spaces with knowledge representation learning methods to manipulate the knowledge in a machine-friendly way. Finally, informative knowledge representations can be applied to handle various NLP tasks. After introducing how to learn knowledge representations, we will detailedly show in this section how to use knowledge representations for specific NLP tasks.\n\nThe performance of NLP models (more generally, machine learning models) depends on four critical factors: input data, model architecture, learning objective, and hypothesis space. And the whole goal is to minimize the structural risk\n\n$$\\displaystyle \\begin{aligned} \\min_{f \\in \\mathcal{F}}\\frac{1}{N}\\sum_{i=1}^{N} \\mathcal{L}(y_i, f(x_i)) + \\lambda \\mathcal{J}(f), \\end{aligned} $$\n\n(9.59)\n\nwhere xi is the input data, f is the model function, \\(\\mathcal {L}\\) is the learning objective, \\(\\mathcal {F}\\) is the hypothesis space, and \\(\\mathcal {J}(f)\\) is the regularization term. By applying knowledge to each of these four factors, we can form four directions to perform knowledge-guided NLP: (1) knowledge augmentation, which aims to augment the input data xi with knowledge; (2) knowledge reformulation, which aims to reformulate the model function f with knowledge; (3) knowledge regularization, which aims to regularize or modify the learning objectives \\(\\mathcal {L}\\) with knowledge; (4) knowledge transfer, which aims to transfer the pre-trained parameters as prior knowledge to constrain the hypothesis space \\(\\mathcal {F}\\).\n\nSome works [60, 61] have briefly introduced this knowledge-guided NLP framework, while in this section, we will further present more details around the four knowledge-guided directions. In addition, to make this section clearer and more intuitive, we will also introduce some specific application cases of knowledge-guided NLP.\n\n9.4.1 Knowledge Augmentation\n\nKnowledge augmentation aims at using knowledge to augment the input features of models. Formally, after using knowledge ) to augment the input, the original risk function is changed to\n\n(9.60)\n\nIn order to achieve this kind of knowledge augmentation at the input level, existing efforts focus on adopting two mainstream approaches.\n\nAugmentation with Knowledge Context\n\nOne approach is to directly add knowledge to the input as additional context. Augmenting language modeling with retrieval is a representative method, such as REALM [53] and RAG [86]. These methods retrieve background knowledge from additional corpora and then use the retrieved knowledge to provide more information for language modeling. Since the retrieved knowledge can significantly improve the performance of language understanding and generation, this approach to achieving knowledge augmentation is widely applied by question answering [76, 111] and dialogue systems [139, 168]. Next, we will take RAG as an example to show how to perform knowledge augmentation with knowledge context.\n\nExample: Knowledge Augmentation for the Generation of PTMs\n\nIn recent years, PTMs have achieved state-of-the-art results on a variety of NLP tasks, but these PTMs still face challenges in precisely accessing and manipulating knowledge and cannot well handle various knowledge-intensive tasks, especially for various text generation tasks that require extensive knowledge. To help PTMs utilize more knowledge for text generation, retrieval-augmented generation (RAG) [86] has been proposed with the aim of using the retrieved external knowledge as additional context to generate text with higher quality.\n\nGiven the input sequence x to generate the output sequence y, the overall process of the typical autoregressive generation method can be formalized as \\(P(y|x) = \\prod _{i=1}^N P_{\\theta }(y_i|x, y_{1:i-1})\\), where θ is the parameters of the generator, N is the length of y, and yi is the i-th token of y. To use more knowledge to generate y, RAG first retrieves the external information z according to the input x and then generates the output sequence y based on both x and z. To ensure that the retrieved contents can cover the crucial knowledge required to generate y, the top-K contents retrieved by the retriever are all used to help generate the output sequence y, and thus the overall generation process is\n\n$$\\displaystyle \\begin{aligned} \\begin{aligned} P_{\\text{RAG-Sequence}}(y|x) & \\approx \\sum_{z \\in \\text{top}-K[P_{\\eta}(\\cdot|x)]}P_{\\eta}(z|x)P_{\\theta}(y|x,z) \\\\ &= \\sum_{z\\in\\text{top-}K[P_{\\eta}(\\cdot|x)]} P_{\\eta}(z|x) \\prod_{i=1}^{N} P_{\\theta}(y_i|x,z,y_{1:i-1}), \\end{aligned} \\end{aligned} $$\n\n(9.61)\n\nwhere η is the parameters of the retriever.\n\nIn addition to applying knowledge augmentation at the sequence level, token-level RAG is also introduced to provide finer-grained augmentation. Specifically, token-level RAG first retrieves the top K external information according to the input x, which is the same as RAG-Sequence. When generating text, token-level RAG considers all the retrieved information together to generate the distribution for the next output token, instead of sequence-level RAG which separately generates sequences based on the retrieved content and then merges the generated sequences. Formally, the token-level RAG is\n\n$$\\displaystyle \\begin{aligned} \\begin{aligned} P_{\\text{RAG-Token}}(y|x) &\\approx \\prod_{i=1}^N \\sum_{z\\in\\text{top-}K[P(\\cdot|x)]} P_{\\eta}(z|x)P_{\\theta}(y_i|x,z,y_{1:i-1}). \\end{aligned} \\end{aligned} $$\n\n(9.62)\n\nTo sum up, RAG adds the retrieved knowledge to the input as additional context, which is a typical example of knowledge augmentation with knowledge context.\n\nAugmentation with Knowledge Embeddings\n\nAnother approach is to design special modules to fuse the original input features and knowledge embeddings and then use the knowledgeable features as the input to solve NLP tasks. Since this approach can help to fully utilize heterogeneous knowledge from multiple sources, many works follow this approach to integrate unstructured text and structured symbolic knowledge in KGs, leading to knowledge-guided information retrieval [87, 100] and knowledge-guided PTMs [96, 124, 128, 163, 185, 205]. Next, we will first introduce word-entity duet, an effective information retrieval method, and then take a typical knowledge-guided information retrieval method EDRM as an example to show how to perform knowledge augmentation with knowledge embeddings.\n\nExample: Knowledge Augmentation for Information Retrieval\n\nInformation retrieval focuses on obtaining informative representations of queries and documents, and then designing effective metrics to compute the similarities between queries and documents. The emergence of large-scale KGs has motivated the development of entity-oriented information retrieval, which aims to leverage KGs to improve the retrieval process. Word-entity duet [179] is a typical method for entity-oriented information retrieval. Specifically, given a query q and a document d, word-entity duet first constructs bag-of-words qw and dw. By annotating the entities mentioned by the query q and the document d, word-entity duet then constructs bag-of-entities qe and de. Based on bag-of-words and bag-of-entities, word-entity duet utilizes the duet representations of bag-of-words and bag-of-entities to match the query q and the document d. The word-entity duet method consists of a four-way interaction: query words to document words (qw-dw), query words to document entities (qw-de), query entities to document words (qe-dw), and query entities to document entities (qe-de).\n\nOn the basis of the word-entity duet method, EDRM [100] further uses distributed representations instead of bag-of-words and bag-of-entities to represent queries and documents for ranking. As shown in Fig. 9.15, EDRM first learns the distributed representations of entities according to entity-related information in KGs, such as entity descriptions and entity types. Then, EDRM uses interaction-based neural models [28] to match the query and documents with word-entity duet distributed representations. More specifically, EDRM uses a translation layer that calculates the similarity between query-document terms: (\\({\\mathbf {v}}_{w^q}^{i}\\) or \\({\\mathbf {v}}_{e^q}^{i}\\)) and (\\({\\mathbf {v}}_{w^d}^{j}\\) or \\({\\mathbf {v}}_{e^d}^{j}\\)). It constructs the interaction matrix M = {Mww, Mwe, Mew, Mee}, by denoting Mww, Mwe, Mew, Mee as the interactions of qw-dw, qw-de, qe-dw, qe-de, respectively. And the elements in these matrices are the cosine similarities of corresponding terms:\n\n$$\\displaystyle \\begin{aligned} \\begin{aligned} {\\mathbf{M}}_{ww}^{ij} = \\cos ({\\mathbf{v}}_{w^q}^{i}, {\\mathbf{v}}_{w^d}^{j})&; {\\mathbf{M}}_{we}^{ij} = \\cos ({\\mathbf{v}}_{w^q}^{i}, {\\mathbf{v}}_{e^d}^{j}),\\\\ {\\mathbf{M}}_{ew}^{ij} = \\cos ({\\mathbf{v}}_{e^q}^{i}, {\\mathbf{v}}_{w^d}^{j})&; {\\mathbf{M}}_{ee}^{ij} = \\cos ({\\mathbf{v}}_{e^q}^{i}, {\\mathbf{v}}_{e^d}^{j}). \\end{aligned} \\end{aligned} $$\n\n(9.63)\n\nThe final ranking feature Φ(M) is a concatenation of four cross matches:\n\n$$\\displaystyle \\begin{aligned} \\varPhi(\\mathbf{M}) = [\\phi({\\mathbf{M}}_{ww}) ; \\phi({\\mathbf{M}}_{we}) ; \\phi({\\mathbf{M}}_{ew}) ; \\phi({\\mathbf{M}}_{ee})], \\end{aligned} $$\n\n(9.64)\n\nwhere ϕ(⋅) can be any function used in interaction-based neural ranking models, such as using Gaussian kernels to extract the matching feature from the matrix M and then pool into a feature vector ϕ(M). For more details of designing ϕ(⋅) and using Φ(M) to compute ranking scores, we suggest referring to some typical interaction-based information retrieval models [28, 180].\n\nTo sum up, EDRM introduces distributed knowledge representations to improve the representations of queries and documents for information retrieval, which is a typical example of knowledge augmentation with knowledge embeddings.\n\n9.4.2 Knowledge Reformulation\n\nKnowledge reformulation aims at using knowledge to enhance the model processing procedure. Formally, after using knowledge to reformulate the model function, the original risk function is changed to\n\n(9.65)\n\nwhere fk(⋅) is the model function reformulated by knowledge. Considering the complexity of the model function f(⋅), it is difficult for us to comprehensively discuss the construction process of fk. To introduce this section more clearly and give readers a more intuitive understanding of knowledge reformulation, we here focus on introducing two relatively simple knowledge reformulation scenarios: knowledgeable preprocessing and post-processing.\n\nKnowledgeable Preprocessing\n\nOn the one hand, we can use the underlying knowledge-guided model layer for preprocessing to make features more informative [160, 167, 194]. Formally, xi is first input to the function k and then input to the function f as\n\n(9.66)\n\nwhere k(⋅) is the knowledge-guided model function used for preprocessing and f(⋅) is the original model function. The knowledge-guided attention mechanism is a representative approach that usually leverages informative knowledge representations to enhance model feature processing. Next, we will take two typical knowledge-guided attention mechanisms [58, 178] as examples to show how to use knowledge for model preprocessing.\n\nExample: Knowledge Reformulation for Knowledge Acquisition\n\nKnowledge acquisition includes two main approaches. One is knowledge graph completion (KGC), which aims to perform link prediction on KGs. The other is relation extraction (RE) to predict relations between entity pairs based on the sentences containing entity pairs. Formally, given sentences s1, s2, ⋯ containing the entity pair h, t, RE aims to evaluate the likelihood that a relation r and h, t can form a triplet based on the semantics of these sentences. Different from RE, KGC only uses the representations of h, r, t learned by knowledge representation learning methods to compute the score function f(h, r, t), and the score function serves knowledge acquisition.\n\nGenerally, RE and KGC models are learned separately, and these models cannot fully integrate text and knowledge to acquire more knowledge. To this end, Han et al. [58] propose a joint learning framework for knowledge acquisition, which can jointly learn knowledge and text representations within a unified semantic space via KG-text alignments. Figure 9.16 shows the brief framework of the joint model. For the text part, the sentence with two entities (e.g., Mark Twain and Florida) is regarded as the input to the encoder, and the output is considered to potentially describe specific relations (e.g., Place of Birth). For the KG part, entity and relation representations are learned via a knowledge representation learning method such as TransE. The learned representations of the KG and text parts are aligned during the training process.\n\nGiven sentences {s1, s2, ⋯ } containing the same entity pair h, t, not all of these sentences can help predict the relation between h and t. For a given relation r, there are many triplets {(h1, r, t1), (h2, r, t2), ⋯ } containing the relation, but not all triplets are important enough for learning the representation of r. Therefore, as shown in Fig. 9.17, Han et al. further adopt mutual attention to reformulate the preprocessing of both the text and knowledge models, to select more useful sentences for RE and more important triplets for KGC. Specifically, we use knowledge representations to highlight the more valuable sentences for predicting the relation between h and t. This process can be formalized as\n\n$$\\displaystyle \\begin{aligned} \\alpha = \\text{Softmax}({\\mathbf{r}}_{ht}^{\\top} {\\mathbf{W}}_{\\text{KA}} \\mathbf{S}), \\quad \\hat{\\mathbf{s}}=\\mathbf{S} \\alpha^{\\top}, \\end{aligned} $$\n\n(9.67)\n\nwhere WKA is a bilinear matrix of the knowledge-guided attention, S = [s1, s2, ⋯ ] are the hidden states of the sentences s1, s2, ⋯. \\({\\mathbf {r}}_{ht}^{\\top }\\) is a representation that can indicate the latent relation between h and t, computed based on knowledge representations. \\(\\hat {\\mathbf {s}}\\) is the feature after synthesizing the information of all sentences, which is used to predict the relation between h and t finally.\n\nSimilar to using knowledge representations to select high-quality sentences, we can also use semantic information to select triples conducive to learning relations. This process can be formalized as\n\n$$\\displaystyle \\begin{aligned} \\alpha = \\text{Softmax}({\\mathbf{r}}_{\\text{Text}}^{\\top} {\\mathbf{W}}_{\\text{SA}} \\mathbf{R}), \\quad {\\mathbf{r}}_{\\text{KG}} = \\mathbf{R} \\alpha^{\\top}, \\end{aligned} $$\n\n(9.68)\n\nwhere WSA is a bilinear matrix of the semantics-guided attention, \\(\\mathbf {R} = [{\\mathbf {r}}_{h_1t_1}, {\\mathbf {r}}_{h_2h_2}, \\cdots ]\\) are the triplet-specific relation representations of the triplets {(h1, r, t1), (h2, r, t2), ⋯ }. rText is the semantic representation of the relation r used by the RE model. rKG is the final relation representation enhanced with semantic information.\n\nThis work is a typical attempt to apply knowledge representations of existing KGs to reformulate knowledge acquisition models. In Sect. 9.5, we will introduce knowledge acquisition in more detail.\n\nExample: Knowledge Reformulation for Entity Typing\n\nEntity typing is the task of detecting semantic types for a named entity (or entity mention) in plain text. For example, given a sentence Jordan played 15 seasons in the NBA, entity typing aims to infer that Jordan in this sentence is a person, an athlete, and even a basketball player. Entity typing is important for named entity disambiguation since it can narrow down the range of candidates for an entity mention [21]. Moreover, entity typing also benefits massive NLP tasks such as relation extraction [98], question answering [184], and knowledge base population [20].\n\nNeural models [36, 138] have achieved state-of-the-art performance for fine-grained entity typing. However, these methods only consider the textual information of named entity mentions for entity typing while ignoring the rich information that KGs can provide for determining entity types. For example, in the sentence In 1975, Gates … Microsoft … company, even though we have no type information of Microsoft in KGs, other entities similar to Microsoft (e.g., IBM) in KGs can also provide supplementary information to help us determine the type of Microsoft. To take advantage of KGs for entity typing, knowledge-guided attention for neural entity typing (KNET) has been proposed [178].\n\nAs illustrated in Fig. 9.18, KNET mainly consists of two parts. Firstly, KNET builds a neural network, including a bidirectional LSTM and a fully connected layer, to generate context and named entity mention representations. Secondly, KNET introduces a knowledge-guided attention mechanism to emphasize those critical words and improve the quality of context representations. Here, we introduce the knowledge-guided attention in detail. KNET employs the translation method TransE to obtain entity embedding e for each entity e in KGs. During the training process, given the context words c = {wi, ⋯ , wj}, a named entity mention m and its corresponding entity embedding e, KNET computes the knowledge-guided attention as\n\n$$\\displaystyle \\begin{aligned} \\alpha = \\text{Softmax}({\\mathbf{e}}^{\\top} {\\mathbf{W}}_{\\text{KA}} \\mathbf{H}), \\quad \\mathbf{c}=\\mathbf{H} \\alpha^{\\top}, \\end{aligned} $$\n\n(9.69)\n\nwhere WKA is a bilinear matrix of the knowledge-guided attention and H = [hi, ⋯ , hj] are the bidirectional LSTM states of {wi, ⋯ , wj}. The context representation c is used as an important feature for the subsequent process of type classification.\n\nThrough the above two examples of knowledge acquisition and entity typing, we introduce how to highlight important features based on knowledge in the model preprocessing stage, so as to output better features to help improve model performance.\n\nKnowledgeable Post-Processing\n\nApart from reformulating model functions for pre-processing, on the other hand, knowledge can be used as an expert at the end of models for post-processing, guiding models to obtain more accurate and effective results [1, 51, 124]. Formally, xi is first input to the function f and then input to the function k as\n\n(9.70)\n\nwhere k(⋅) is the knowledge-guided model function used for post-processing and f(⋅) is the original model function. Knowledgeable post-processing is widely used by knowledge-guided language modeling to improve the word prediction process [1, 51]. Next, we will take a typical knowledge-guided language modeling method NKLM [1] as an example to show how to use knowledge representations to improve model post-processing (Fig. 9.19).\n\nExample: Knowledge Post-Processing on Language Modeling\n\nNKLM [1] aims to perform language modeling by considering both semantics and knowledge to generate text. Specifically, NKLM designs two ways to generate each word in the text. The first is the same as conventional auto-regressive models that generate a vocabulary word according to the probabilities over the vocabulary. The second is to generate a knowledge word according to external KGs. Specifically, NKLM uses the LSTM architecture as the backbone to generate words. For external KGs, NKLM stores knowledge representations to build a knowledgeable module \\(\\mathcal {K}=\\{({\\mathbf {a}}_1, O_1), ({\\mathbf {a}}_2, O_2), \\cdots , ({\\mathbf {a}}_n, O_n)\\}\\), in which Oi denotes the description of the i-th fact, ai denotes the concatenation of the representations of the head entity, relation and tail entity of the i-th fact.\n\nGiven the context {w1, w2, ⋯ , wt−1}, NKLM takes both the vocabulary word representation \\({\\mathbf {w}}_{t-1}^{v}\\), the knowledge word representation \\({\\mathbf {w}}_{t-1}^{o}\\), and the knowledge-guided representation at−1 at the step t − 1 as LSTM’s input \\({\\mathbf {x}}_t=\\{{\\mathbf {w}}_{t-1}^v, {\\mathbf {w}}_{t-1}^o, {\\mathbf {a}}_{t-1}\\}\\). xt is then fed to LSTM together with the hidden state ht−1 to get the output state ht. Next, a two-layer multilayer perceptron f(⋅) is applied to the concatenation of ht and xt to get the fact key kt = f(ht, xt). kt is then used to extract the most relevant fact representation at from the knowledgeable module. Finally, the selected fact at is combined with the hidden state ht to output a vocabulary word \\(w_{t}^{v}\\) and knowledge word \\(w_{t}^{o}\\) (which is copied from the entity name in the t-th fact), and then determine which word to generate at the step t.\n\nOverall, by using KGs to enhance the post-processing of language modeling, NKLM can generate sentences that are highly related to world knowledge, which are often difficult to model without considering external knowledge.\n\n9.4.3 Knowledge Regularization\n\nKnowledge regularization aims to use knowledge to modify the objective functions of models:\n\n(9.71)\n\nwhere \\(\\mathcal {L}_k(k, f(x_i))\\) is the additional predictive targets and learning objectives constructed based on knowledge and λk is a hyper-parameter to control the knowledgeable loss term.\n\nDistant supervision [109] is a representative method that uses external knowledge to heuristically annotate corpora as additional supervision signals. For many vital information extraction tasks, such as RE [58, 72, 91, 196] and entity typing [36, 138, 178], distant supervision is widely applied for model training. As we will introduce distant supervision in Sect. 9.5 to show how to build additional supervision signals with knowledge, we do not introduce concrete examples here.\n\nKnowledge regularization is also widely used by knowledge-guided PTMs [124, 163, 205]. To fully integrate knowledge into language modeling, these knowledge-guided PTMs design knowledge-specific tasks as their pre-training objectives and use knowledge representations to build additional prediction objectives. Next, we will take the typical knowledge-guided PTM ERNIE [205] as an example to show how knowledge regularization can help the learning process of models.\n\nExample: Knowledge Regularization for PTMs\n\nPTMs like BERT [33] have great abilities to extract features from text. With informative language representations, PTMs obtain state-of-the-art results on various NLP tasks. However, the existing PTMs rarely consider incorporating external knowledge, which is essential in providing related background information for better language understanding. For example, given a sentence Bob Dylan wrote Blowin’ in the Wind and Chronicles: Volume One, without knowing Blowin’ in the Wind is a song and Chronicles: Volume One is a book, it is not easy to know the occupations of Bob Dylan, i.e., songwriter and writer.\n\nTo this end, an enhanced language representation model with informative entities (ERNIE) is proposed [205]. Figure 9.20 is the overall architecture of ERNIE. ERNIE first augments the input data using knowledge augmentation as we have mentioned in Sect. 9.4.1. Specifically, ERNIE recognizes named entity mentions and then aligns these mentions to their corresponding entities in KGs. Based on the alignments between text and KGs, ERNIE takes the informative entity representations as additional input features.\n\nSimilar to conventional PTMs, ERNIE adopts masked language modeling and next sentence prediction as the pre-training objectives. To better fuse textual and knowledge features, ERNIE proposes denoising entity auto-encoding (DAE) by randomly masking some mention-entity alignments in the text and requiring models to select appropriate entities to complete the alignments. Different from the existing PTMs that predict tokens with only using local context, DAE requires ERNIE to aggregate both text and knowledge to predict both tokens and entities, leading to knowledge-guided language modeling. DAE is clearly a knowledge-guided objective function.\n\nIn addition to ERNIE, there are other representative works on knowledge regularization. For example, KEPLER [163] incorporates structured knowledge into its pre-training. Specifically, KEPLER encodes the textual description of entities as entity representations and predicts the relation between entities based on these description-based representations. In this way, KEPLER can learn the structured information of entities and relations in KGs in a language-modeling manner. WKLM [181] proposes a pre-training objective type-constrained entity replacement. Specifically, WKLM randomly replaces the named entity mentions in the text with other entities of the same type and requires the model to identify whether an entity mention is replaced or not. Based on the new pre-training objective, WKLM can accurately learn text-related knowledge and capture the type information of entities.\n\nFrom Fig. 9.20, we can find that ERNIE also adopts knowledge reformulation by adding the new aggregator layers designed for knowledge integration to the original Transformer architecture. To a large extent, the success of knowledge-guided PTMs comes from the fact that these models use knowledge to enhance important factors of model learning. Up to now, we have introduced knowledge augmentation, knowledge reformulation, and knowledge regularization. Next, we will further introduce knowledge transfer.\n\n9.4.4 Knowledge Transfer\n\nKnowledge transfer aims to use knowledge to obtain a knowledgeable hypothesis space, reducing the cost of searching optimal parameters and making it easier to train an effective model. There are two typical approaches to transferring knowledge: (1) transfer learning [120] that focuses on transferring model knowledge learned from labeled data to downstream task-specific models and (2) self-supervised learning [97] that focuses on transferring model knowledge learned from unlabeled data to downstream task-specific models. More generally, the essence of knowledge transfer is to use prior knowledge to constrain the hypothesis space:\n\n(9.72)\n\nwhere \\(\\mathcal {F}_k\\) is the knowledge-guided hypothesis space.\n\nKnowledge transfer is widely used in NLP. The fine-tuning stage of PTMs is a typical scenario of knowledge transfer, which aims to transfer the versatile knowledge acquired in the pre-training stage to specific tasks. Intuitively, after pre-training a PTM, fine-tuning this PTM can be seen as narrowing down searching task-specific parameters to a local hypothesis space around the pre-trained parameters rather than the global hypothesis space.\n\nAs we mentioned in Chap. 5, in addition to fine-tuning PTMs, prompt learning has also been widely explored. Despite the success of fine-tuning PTMs, it still faces two challenges. On the one hand, there is a gap between the objectives of pre-training and fine-tuning, since most PTMs are learned with language modeling objectives, yet downstream tasks may have quite different objective forms such as classification, regression, and labeling. On the other hand, as the parameter size of PTMs increases rapidly, fine-tuning PTMs has become resource-intensive. In order to alleviate these issues, prompts have been introduced to utilize the knowledge of PTMs in an effective and efficient manner [93].\n\nAs shown in Fig. 9.21, prompt learning aims at converting downstream tasks into a cloze-style task similar to pre-training objectives so that we can better transfer the knowledge of PTMs to downstream tasks. Taking prompt learning for sentiment classification as an example, a typical prompt consists of a template (e.g., …It was [MASK].) and a label word set (e.g., great and terrible) as candidates for predicting [MASK]. By changing the input using the template to predict [MASK] and mapping the prediction to corresponding labels, we can apply masked language modeling for sentiment classification. For example, given the sentence I like eating apples., we first use the prompt template to get the new input sentence I like eating apples. It was [MASK]. According to PTMs predicting great or terrible at the masked position, we can determine whether this sentence is positive or negative.\n\nThe recently proposed large-scale PTM GPT-3 [17] shows the excellent performance of prompt learning in various language understanding and generation tasks. In prompt learning, all downstream tasks are transformed to be the same as the pre-training tasks. And since the parameters of PTMs are frozen during prompt learning, the size of hypothesis space is much smaller compared to fine-tuning, making more efficient knowledge transfer possible.\n\nOverall, PTMs play an important role in driving the use of model knowledge. And to some extent, PTMs also influence the paradigm of using symbolic knowledge in NLP. As shown in Fig. 9.21, many knowledge probing works [74, 125, 126] show that by designing prompt, PTMs can even complete structured knowledge information. These studies show that PTMs, as good carriers of symbolic knowledge, can memorize symbolic knowledge well. Moreover, these studies also indicate one factor that may contribute to the power of PTMs: knowledge can be spontaneously abstracted by PTMs from large-scale unstructured data and then used to solve concrete problems, and the abstracted knowledge matches well with the knowledge formed by human beings. Inspired by this, we can further delve into how PTMs abstract knowledge and how PTMs store knowledge in their parameters, which is very meaningful for further advancing the integration of symbolic knowledge and model knowledge. On the other hand, all these studies also show the importance of knowledge-guided NLP. Compared with letting models slowly abstract knowledge from large-scale data, directly injecting symbolic knowledge into models is a more effective solution.\n\nThe success of PTMs demonstrates the clear advantages of fully transferring existing model knowledge in terms of computing efficiency and effectiveness, as compared to learning a model from scratch. Since we have introduced the details of PTMs in Chap. 5, in this section, we mainly discuss the valuable properties of knowledge transfer owned by PTMs.\n\n9.4.5 Summary\n\nIn this section, we present several ways in which knowledge is used to guide NLP models. Depending on the location of model learning where knowledge steps in, we group the guidance from knowledge into four categories: (1) knowledge augmentation, where knowledge is introduced to augment the input data, (2) knowledge reformulation, where special model modules are designed to interact with knowledge, (3) knowledge regularization, where knowledge does not directly intervene the forward pass of the model but acts as a regularizer, and (4) knowledge transfer, where knowledge helps narrow down the hypothesis space to achieve more efficient and effective model learning.\n\nThese approaches enable effective integration of knowledge into deep models, allowing models to leverage sufficient knowledge (especially symbolic knowledge) to better perform NLP tasks. Since knowledge is essential for models to understand and complete the NLP tasks, knowledge-guided NLP is a worthwhile area for researchers to continue to explore.\n\nThe KBs used in early expert systems and the KGs built in recent years both have long relied on manual construction. Manually organizing knowledge ensures that knowledge systems are constructed with high quality but suffers from inefficiency, incompleteness, and inconsistency in the annotation process. As shown in Fig. 9.22, the number of entities in the popular open-source KG WikidataFootnote 3 grew at a rate of over 15 million per year from 2017 to 2022. At this rate of growth, it is unrealistic to rely solely on human annotation to organize large-scale human knowledge. Therefore, it is crucial to explore automatic knowled"
    }
}
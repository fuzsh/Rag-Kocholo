{
    "id": "dbpedia_1330_3",
    "rank": 86,
    "data": {
        "url": "https://arxiv.org/html/2404.18992v1",
        "read_more_link": "",
        "language": "en",
        "title": "Unifying Simulation and Inference with Normalizing Flows",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://arxiv.org/html/x1.png",
            "https://arxiv.org/html/x2.png",
            "https://arxiv.org/html/x3.png",
            "https://arxiv.org/html/x4.png",
            "https://arxiv.org/html/x5.png",
            "https://arxiv.org/html/x6.png",
            "https://arxiv.org/html/x7.png",
            "https://arxiv.org/html/x8.png",
            "https://arxiv.org/html/x9.png",
            "https://arxiv.org/html/x10.png",
            "https://arxiv.org/html/x11.png",
            "https://arxiv.org/html/x12.png",
            "https://arxiv.org/html/x13.png",
            "https://arxiv.org/html/x14.png",
            "https://arxiv.org/html/x15.png",
            "https://arxiv.org/html/x16.png",
            "https://arxiv.org/html/x17.png",
            "https://arxiv.org/html/x18.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Haoxing Du Claudius Krause Vinicius Mikuni Benjamin Nachman Ian Pang David Shih\n\nAbstract\n\nThere have been many applications of deep neural networks to detector calibrations and a growing number of studies that propose deep generative models as automated fast detector simulators. We show that these two tasks can be unified by using maximum likelihood estimation (MLE) from conditional generative models for energy regression. Unlike direct regression techniques, the MLE approach is prior-independent and non-Gaussian resolutions can be determined from the shape of the likelihood near the maximum. Using an ATLAS-like calorimeter simulation, we demonstrate this concept in the context of calorimeter energy calibration.\n\n‚Ä†‚Ä†preprint: HEPHY-ML-24-01\n\nI Introduction\n\nDetector calibrations are one of the most important and foundational tasks in experimental physics. In particle and nuclear physics, the largest calibration step is usually a simulation-based correction to ensure that the reported properties of the measured particles are unbiased. Detector simulations are complex and accurate, but can only run forward in time and the resolutions are non-trivial. Combined, these properties mean that we cannot simply invert the simulation to predict true quantities given measured ones.\n\nThis is particularly acute for highly-segmented detectors where many individual channels are activated for a single particle as the measured phase space can be high dimensional and complex. A common example of this setting is calorimeter reconstruction. Unlike tracking detectors that aim to minimally disrupt the trajectory of a particle, calorimeters are designed to stop particles and the resulting showers inside dense materials are challenging to parse and are highly stochastic. Traditional calibration methods are optimized using relatively low-dimensional summary statistics. Such calibrations have enabled many science results, but the ultimate precision cannot be reached until we utilize all of the available low-level information for event reconstruction.\n\nMachine learning provides a set of tools that can analyze hadronic final states holistically to achieve the best precision. Recent examples supporting this claim include the energy calibrations of single hadrons [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], jets [12, 13, 14, 15, 16, 17, 18, 19, 20, 21], and global event properties [22, 23, 24, 25] at colliders. These techniques can automatically make use of finer segmentation to improve the resolution of reconstructed particle energies [4, 10]. While machine learning has broader utility than reconstructing individual hadron showers within a calorimeter, we focus on this case because it is particularly challenging and a critical component of event reconstruction at the Large Hadron Collider and elsewhere.\n\nWhile existing machine learning methods are promising, most approaches have at least two undesirable features: they are only point estimates and are prior-dependent. Firstly, most approaches produce a single estimate without quantifying ‚Äòuncertainties‚Äô. In particle/ nuclear physics, the spread of the difference between the inferred estimates and the true value is usually called the resolution, even though it is part of Uncertainty Quantification (UQ) in the broader machine learning literature. Secondly, techniques that employ standard machine learning regression tools are not universal: their performance depends on the distribution of examples (the ‚Äòprior‚Äô, e.g. uniform in energy) used during training. In fact, this prior dependence often results in a large calibration bias.\n\nOver the last years, some partial solutions to these two central challenges have been proposed. Generalized numerical inversion [26, 27, 28] is a prior-independent calibration approach, although it does not scale well to many output dimensions. The Gaussian Ansatz [17, 18] is a local maximum likelihood estimator, and thus prior-independent (more in Sec. II). For UQ, loss functions can be modified to estimate quantiles in addition to just the mean, median or mode [13, 14] and deep generative models can estimate the exact resolution function [24, 25]. The Gaussian Ansatz produces a local estimate of the resolution and so is currently the only method that is both prior-independent and that goes beyond a point estimate. However, the Gaussian Ansatz only gives a Gaussian approximation of the resolution. We note that it is also possible to use Bayesian versions [29, 30, 31, 32, 33] of neural networks to go beyond point estimates by providing the uncertainty of the output. However, the uncertainty would still be prior-dependent.\n\nIn this paper, we propose a prior-independent method for detector calibrations that produces per-shower resolution estimates. The approach is based on deep generative models with access to the explicit likelihood. In particular, we use normalizing flows [36, 37, 38, 39, 40, 41]. These machine learning models are invertible functions with a tractable Jacobian so that one can be used for both sampling and density estimation. The core idea behind a normalizing flow is that one starts with a simple random variable with a known probability density (e.g. a Gaussian) and then applies a series of transformations with the tractable Jacobian. This results in a complex probability density that can also be computed via the change of variables formula. The probability density can be made conditional by letting each transformation depend on the conditional quantity.\n\nNormalizing flows are state-of-the-art as calorimeter simulation surrogate models [42, 43, 44, 45, 46, 47, 48, 49]. Additionally, normalizing flows have shown similar good performance in other tasks in high-energy physics [50, 51, 52, 53, 54, 55, 56, 33, 57, 58, 59, 60, 61, 62, 24, 63, 64, 65, 66, 24, 25, 67, 68, 69, 70, 71, 72]. In this paper, we unify simulation and inference by showing how models trained for generation can be reused for calibration. As a demonstration of this approach, we use CaloFlow [42, 43], a normalizing flow-based calorimeter surrogate model. This machine learning model is trained on single-pion showers from an extended version [73] of the CaloGAN dataset [74, 75, 76] that now includes both a sampling calorimeter setup [77] and a hadronic component. The calibration task is to predict the incident pion energy given the distribution of energies recorded in the cells of the calorimeter. Importantly, we demonstrate the following key advantages of our approach:\n\n1.\n\nZero-shot calibration:\n\nNormalizing flow-based models trained for surrogate modelling can be reused for calibration by repurposing the probability density as a likelihood without any additional model retrainings.\n\n2.\n\nAccess to per-shower resolution:\n\nWith access to the complete likelihood, we can compute the exact resolution function. This allows us to obtain per-shower resolution estimates which give us additional information about how close the predicted energy is to the true energy. Furthermore, it enables us to estimate asymmetries in the resolution function (not possible with the GA).\n\n3.\n\nLess biased calibration:\n\nWe achieve a smaller calibration bias compared to a typical direct regression approach for the calorimeter setup considered in this work.\n\nThis paper is organized as follows. Machine learning-based calibration methods are introduced in Sec. II. Here we also provide precise definitions for bias and resolution used in this paper. In Sec. III, we summarize the CaloFlow algorithm and discuss the results of the calorimeter example which demonstrate the key advantages listed above. The paper ends with conclusions and outlook in Sec. IV. We collect some details on the used network architectures in an appendix.\n\nII Methods\n\nGiven samples from a forward model (usually a physics-based simulation) X‚àºpX|Z‚Å¢(x|z)similar-toùëãsubscriptùëùconditionalùëãùëçconditionalùë•ùëßX\\sim p_{X|Z}(x|z)italic_X ‚àº italic_p start_POSTSUBSCRIPT italic_X | italic_Z end_POSTSUBSCRIPT ( italic_x | italic_z ) for measured values XùëãXitalic_X and true values ZùëçZitalic_Z, the goal of calibration is to estimate ZùëçZitalic_Z from XùëãXitalic_X. In our notation, capital letters represent random variables and lower case letters represent realizations of those random variables. Both the measured and true values can be multidimensional, although we will focus on the common case where ZùëçZitalic_Z is one dimensional and X‚àà‚ÑùNùëãsuperscript‚ÑùùëÅX\\in\\mathbb{R}^{N}italic_X ‚àà blackboard_R start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT.\n\nThe usual assumption when deriving a simulation-based calibration is that the detector response is correct and universal, i.e. pX|Ztrain‚Å¢(x|z)=pX|Ztest‚Å¢(x|z)superscriptsubscriptùëùconditionalùëãùëçtrainconditionalùë•ùëßsuperscriptsubscriptùëùconditionalùëãùëçtestconditionalùë•ùëßp_{X|Z}^{\\text{train}}(x|z)=p_{X|Z}^{\\text{test}}(x|z)italic_p start_POSTSUBSCRIPT italic_X | italic_Z end_POSTSUBSCRIPT start_POSTSUPERSCRIPT train end_POSTSUPERSCRIPT ( italic_x | italic_z ) = italic_p start_POSTSUBSCRIPT italic_X | italic_Z end_POSTSUBSCRIPT start_POSTSUPERSCRIPT test end_POSTSUPERSCRIPT ( italic_x | italic_z ) for different physics processes (i.e. different p‚Å¢(z)ùëùùëßp(z)italic_p ( italic_z )) or for actual data.\n\n1.\n\nThe bias of a calibration is the deviation between the central tendency of the inferred estimate z^^ùëß\\hat{z}over^ start_ARG italic_z end_ARG and the true reference value zùëßzitalic_z. Any measure of central tendency can be used to measure closure, such as the median or mode. In this paper, we will focus on the mode of the inferred estimates given a fixed zùëßzitalic_z value. To compute the mode in practice, for a given zùëßzitalic_z, we look at KùêæKitalic_K measured values x1,x2‚Å¢‚Ä¶‚Å¢xKsubscriptùë•1subscriptùë•2‚Ä¶subscriptùë•ùêæx_{1},x_{2}\\dots x_{K}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ‚Ä¶ italic_x start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT corresponding to that zùëßzitalic_z. Each measured value xisubscriptùë•ùëñx_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT undergoes calibration to derive an inferred estimate z^isubscript^ùëßùëñ\\hat{z}_{i}over^ start_ARG italic_z end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. The mode is subsequently determined based on the collection of inferred estimates {z^1,z^2,‚Ä¶,z^K}subscript^ùëß1subscript^ùëß2‚Ä¶subscript^ùëßùêæ\\{\\hat{z}_{1},\\hat{z}_{2},\\dots,\\hat{z}_{K}\\}{ over^ start_ARG italic_z end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , over^ start_ARG italic_z end_ARG start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ‚Ä¶ , over^ start_ARG italic_z end_ARG start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT }.\n\n2.\n\nThe resolution is the spread of the difference between the inferred estimates and the true value. In this work, we have two relevant definitions:\n\n‚Ä¢\n\nFull resolution: For fixed zùëßzitalic_z, it is defined as half of the 68% confidence interval about the mode of a collection of inferred estimates.\n\n‚Ä¢\n\nPer-event resolution: For each xùë•xitalic_x, it is defined as half of the 68% confidence interval of the corresponding maximum likelihood estimate (see Sec. II.2).\n\nThe full resolution is not to be confused with the per-event resolution. The full resolution is defined for a collection of inferred estimates, whereas the per-event resolution is defined for each inferred estimate.\n\nII.1 Direct Regression\n\nMost proposals for deep learning-based calibration directly regress ZùëçZitalic_Z from XùëãXitalic_X using a loss function like the mean-squared error (MSE):\n\nL‚Å¢[f]=‚àëi(fMSE‚Å¢(xi)‚àízi)2,ùêødelimited-[]ùëìsubscriptùëñsuperscriptsubscriptùëìMSEsubscriptùë•ùëñsubscriptùëßùëñ2\\displaystyle L[f]=\\sum_{i}(f_{\\rm MSE}(x_{i})-z_{i})^{2}\\,,italic_L [ italic_f ] = ‚àë start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT roman_MSE end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) - italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , (1)\n\nfor a neural network fMSEsubscriptùëìMSEf_{\\rm MSE}italic_f start_POSTSUBSCRIPT roman_MSE end_POSTSUBSCRIPT. Using the calculus of variations, one can show that with enough training data and sufficiently flexible neural network architecture and training protocol, the solution to Eq. 1 is the average value of ZùëçZitalic_Z given X=xùëãùë•X=xitalic_X = italic_x:\n\nfMSE‚Å¢(x)subscriptùëìMSEùë•\\displaystyle f_{\\rm MSE}(x)italic_f start_POSTSUBSCRIPT roman_MSE end_POSTSUBSCRIPT ( italic_x ) =‚ü®Z|X=x‚ü©absentinner-productùëçùëãùë•\\displaystyle=\\langle Z|X=x\\rangle= ‚ü® italic_Z | italic_X = italic_x ‚ü© (2) =‚à´ùëëz‚Å¢z‚Å¢pZ|Xtrain‚Å¢(z|x)absentdifferential-dùëßùëßsuperscriptsubscriptùëùconditionalùëçùëãtrainconditionalùëßùë•\\displaystyle=\\int dz\\,z\\,p_{Z|X}^{\\text{train}}(z|x)= ‚à´ italic_d italic_z italic_z italic_p start_POSTSUBSCRIPT italic_Z | italic_X end_POSTSUBSCRIPT start_POSTSUPERSCRIPT train end_POSTSUPERSCRIPT ( italic_z | italic_x ) (3) =‚à´ùëëz‚Å¢z‚Å¢pX|Ztrain‚Å¢(x|z)‚Å¢pZtrain‚Å¢(z)pXtrain‚Å¢(x).absentdifferential-dùëßùëßsuperscriptsubscriptùëùconditionalùëãùëçtrainconditionalùë•ùëßsuperscriptsubscriptùëùùëçtrainùëßsuperscriptsubscriptùëùùëãtrainùë•\\displaystyle=\\int dz\\,z\\,p_{X|Z}^{\\text{train}}(x|z)\\,\\frac{p_{Z}^{\\text{% train}}(z)}{p_{X}^{\\text{train}}(x)}\\,.= ‚à´ italic_d italic_z italic_z italic_p start_POSTSUBSCRIPT italic_X | italic_Z end_POSTSUBSCRIPT start_POSTSUPERSCRIPT train end_POSTSUPERSCRIPT ( italic_x | italic_z ) divide start_ARG italic_p start_POSTSUBSCRIPT italic_Z end_POSTSUBSCRIPT start_POSTSUPERSCRIPT train end_POSTSUPERSCRIPT ( italic_z ) end_ARG start_ARG italic_p start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT start_POSTSUPERSCRIPT train end_POSTSUPERSCRIPT ( italic_x ) end_ARG . (4)\n\nNote that fMSEsubscriptùëìMSEf_{\\rm MSE}italic_f start_POSTSUBSCRIPT roman_MSE end_POSTSUBSCRIPT only gives a point estimate of zùëßzitalic_z for each xùë•xitalic_x and does not have access to per-event resolutions. In other words, there is no calibration uncertainty quantified by fMSEsubscriptùëìMSEf_{\\rm MSE}italic_f start_POSTSUBSCRIPT roman_MSE end_POSTSUBSCRIPT.\n\nFor a given zùëßzitalic_z, the bias can be computed as the deviation between zùëßzitalic_z and the mode of a collection of fMSE‚Å¢(xi)subscriptùëìMSEsubscriptùë•ùëñf_{\\rm MSE}(x_{i})italic_f start_POSTSUBSCRIPT roman_MSE end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ), and the full resolution at zùëßzitalic_z is then computed based on the 68% confidence interval about the mode. Note that the bias is defined as a function of zùëßzitalic_z. Hence, we cannot correct for this bias as we do not have access to zùëßzitalic_z while performing the calibration.\n\nThe challenge with Eq. 4 is that it depends on pZtrainsuperscriptsubscriptùëùùëçtrainp_{Z}^{\\text{train}}italic_p start_POSTSUBSCRIPT italic_Z end_POSTSUBSCRIPT start_POSTSUPERSCRIPT train end_POSTSUPERSCRIPT even when we assume that the detector response is universal. Unsurprisingly then, the bias of direct regression also depends on the training dataset. In Ref. [18], the prior dependence of MSE-based calibration is shown to be the main source of large calibration bias.\n\nII.2 Maximum Likelihood Inference\n\nMaximum likelihood estimation (MLE) involves finding the ZùëçZitalic_Z that maximizes the likelihood of the data,\n\nZ^=argmaxùëß‚Å¢pX|Z‚Å¢(X|z).^ùëçùëßargmaxsubscriptùëùconditionalùëãùëçconditionalùëãùëß\\hat{Z}=\\underset{z}{\\text{argmax}}\\,p_{X|Z}(X|z)\\,.over^ start_ARG italic_Z end_ARG = underitalic_z start_ARG argmax end_ARG italic_p start_POSTSUBSCRIPT italic_X | italic_Z end_POSTSUBSCRIPT ( italic_X | italic_z ) . (5)\n\nSince we assume that pX|Zsubscriptùëùconditionalùëãùëçp_{X|Z}italic_p start_POSTSUBSCRIPT italic_X | italic_Z end_POSTSUBSCRIPT is universal and Eq. 5 only depends on this likelihood, then Z^^ùëç\\hat{Z}over^ start_ARG italic_Z end_ARG is universal. We can go beyond the point estimate and use the likelihood function pX|Zsubscriptùëùconditionalùëãùëçp_{X|Z}italic_p start_POSTSUBSCRIPT italic_X | italic_Z end_POSTSUBSCRIPT to obtain the per-event resolution for each maximum likelihood estimate z^^ùëß\\hat{z}over^ start_ARG italic_z end_ARG. The access to per-event resolutions is a major advantage of MLE over direct regression in calibration. Examples showcasing this advantage are discussed in Sec. III.4.\n\nSimilar to the direct regression case, the bias and full resolution at a fixed zùëßzitalic_z value can be defined based on the mode and 68% confidence interval about the mode.\n\nAlthough the MLE-based calibration is always prior-independent, the prior dependence does not guarantee that the method is always unbiased. Only in certain scenarios (e.g. 1D Gaussian noise model such that pX|Z‚Å¢(x|z)‚àºN‚Å¢(z,œÉ)similar-tosubscriptùëùconditionalùëãùëçconditionalùë•ùëßùëÅùëßùúép_{X|Z}(x|z)\\sim N(z,\\sigma)italic_p start_POSTSUBSCRIPT italic_X | italic_Z end_POSTSUBSCRIPT ( italic_x | italic_z ) ‚àº italic_N ( italic_z , italic_œÉ )) can the MLE calibration be shown to be unbiased. Certain detector responses pX|Zsubscriptùëùconditionalùëãùëçp_{X|Z}italic_p start_POSTSUBSCRIPT italic_X | italic_Z end_POSTSUBSCRIPT may result in biased calibration. In general, prior independence is a necessary but insufficient condition for unbiased calibration. Even so, we show in Sec. III.3 that MLE-based calibration results in a smaller bias compared to direct regression for an ATLAS-like calorimeter setup. This suggests that in our application, the dependence on training prior is a bigger contributing factor to the bias than the detector response.\n\nAlso, the challenge with MLE is that we usually do not know pX|Zsubscriptùëùconditionalùëãùëçp_{X|Z}italic_p start_POSTSUBSCRIPT italic_X | italic_Z end_POSTSUBSCRIPT explicitly. Nevertheless, we are able to sample from this conditional density by running a simulation. The Gaussian Ansatz [17, 18] approaches this problem by estimating the likelihood locally around the mode with a clever neural network parameterization and learning rate schedule. In this work, we instead aim to learn the entire likelihood using a neural network ‚Äî an approach sometimes referred to as neural likelihood estimation in the simulation-based inference literature [78, 79, 80, 81]. This is substantially more work, but we observe that the work may already be done: given a fast simulation based on neural networks with access to the likelihood, we can use it for calibration in addition to generation without requiring any additional retraining.\n\nOur tool of choice is the normalizing flow (NF). NFs are neural networks that are optimized using maximum likelihood estimation. With access to an estimate of the full likelihood, we can study both Gaussian and non-Gaussian aspects of the resolution.\n\nIV Conclusions\n\nWe compared the calibration performance of CaloFlow and a DNN direct regression model on œÄ+superscriptùúã\\pi^{+}italic_œÄ start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT calorimeter showers from our new sampling calorimeter dataset [73], and found that the direct regression method is clearly more biased relative to the flow-based calibration method. This highlights the advantage of utilizing prior-independent MLE-based calibration for tasks such as particle energy regression.\n\nWe demonstrate a second advantage of using a MLE-based calibration methods over direct regression by estimating per-shower resolutions. The average estimated per-shower resolution obtained from CaloFlow closely aligns with the full resolution. In contrast, the direct regression approach yields only point estimates and lacks the capacity to make per-shower resolution predictions. For CaloFlow, we found that smaller per-shower resolutions tend to coincide with more accurately calibrated showers. This is evidence that the per-shower resolutions from CaloFlow are reliable and have the potential to be leveraged for improved calibration. Notably, utilizing a NF for calibration grants access to the complete resolution function, a feature inaccessible in previous methods like the Gaussian Ansatz. This characteristic enables the NF to capture asymmetries in the resolution, thereby expected to confer an advantage over the Gaussian Ansatz in specific calibration tasks. In future work, it would be interesting to study how the per-shower resolution information obtained by the NF can be utilized to further improve the calibration.\n\nData and code availability\n\nThe datasets used in this study can be found at Ref. [73] and the software to generate these datasets are located at https://github.com/hep-lbdl/CaloGAN/tree/two_layer. The machine learning software is at https://github.com/Ian-Pang/regression_with_CF.\n\nAcknowledgements\n\nWe would like to thank Rikab Gambhir for the helpful discussions related to the Gaussian Ansatz model. CK would like to thank the Baden-W√ºrttemberg-Stiftung for financing through the program Internationale Spitzenforschung, project Uncertainties ‚Äì Teaching AI its Limits (BWST_IF2020-010). IP and DS are supported by the U.S. Department of Energy (DOE), Office of Science grant DOE-SC0010008 and HD, VM, and BN are supported by the DOE under contract DE-AC02-05CH11231.\n\nAppendix A Architecture and training\n\nHere we briefly describe the architecture and training procedure used for CaloFlow (see Refs. [42, 43] for more details). There are some differences compared to the implementation in the original CaloFlow papers [42, 43], but most of main algorithm remains the same. One main difference is that only Flow-I is used in this study.\n\nThe flows used in this work are Masked Autoregressive Flows (MAFs) [92] with compositions of Rational Quadratic Splines (RQS) [93] as transformations. The RQS transformations are parameterized using neural networks known as MADE blocks [94]. Identical flow architectures are used in each of the two cases with uniformly and log-uniformly distributed Eincsubscriptùê∏incE_{\\rm inc}italic_E start_POSTSUBSCRIPT roman_inc end_POSTSUBSCRIPT. Each flow consists of six MADE blocks, each with two hidden layers of 64 nodes. The RQS transformations are defined with 8 bins and a tail bound of 14.\n\nThe incident energy of the incoming photon is preprocessed as\n\nEinc‚Üílog10‚Å°(Einc/10‚Å¢GeV).‚Üísubscriptùê∏incsubscript10subscriptùê∏inc10GeVE_{\\text{inc}}\\to\\log_{10}(E_{\\text{inc}}/10\\ \\text{GeV})\\,.italic_E start_POSTSUBSCRIPT inc end_POSTSUBSCRIPT ‚Üí roman_log start_POSTSUBSCRIPT 10 end_POSTSUBSCRIPT ( italic_E start_POSTSUBSCRIPT inc end_POSTSUBSCRIPT / 10 GeV ) . (6)\n\nThe layer energies are preprocessed as\n\nEi‚Üí2‚Å¢(log10‚Å°(Ei+1‚Å¢keV)‚àí1).‚Üísubscriptùê∏ùëñ2subscript10subscriptùê∏ùëñ1keV1E_{i}\\to 2\\left(\\log_{10}(E_{i}+1\\ \\text{keV})-1\\right)\\,.italic_E start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ‚Üí 2 ( roman_log start_POSTSUBSCRIPT 10 end_POSTSUBSCRIPT ( italic_E start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + 1 keV ) - 1 ) . (7)\n\nThe index iùëñiitalic_i denotes the layer number. In the original CaloFlow, a different preprocessing was used for the layer energies Eisubscriptùê∏ùëñE_{i}italic_E start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT in Flow-I where Eisubscriptùê∏ùëñE_{i}italic_E start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT were transformed to unit-space (see [42]).\n\nUniform noise in the range [0,0.1] keV was applied to the voxel energies during training and evaluation. The same range was used for the pion dataset in [44]. The range of uniform noise used in the original CaloFlow [42, 43] was [0,1] keV. The addition of noise was found to prevent the flow from fitting unimportant features. The training of flows in this work is optimized using independent Adam optimizers [87]. The flows were trained by minimizing ‚àílog‚Å°p1‚Å¢(E‚Üí|Einc)subscriptùëù1conditional‚Üíùê∏subscriptùê∏inc-\\log p_{1}(\\vec{E}|E_{\\text{inc}})- roman_log italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( over‚Üí start_ARG italic_E end_ARG | italic_E start_POSTSUBSCRIPT inc end_POSTSUBSCRIPT ) for 150 epochs with a batch size of 200. The initial learning of 10‚àí4superscript10410^{-4}10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT was chosen and a multi-step learning schedule was used when training the flow which halves the learning rate after each selected epoch milestone during the training.\n\nReferences\n\n[1] L. de Oliveira, B. Nachman and M. Paganini, Electromagnetic Showers Beyond Shower Shapes, Nucl. Instrum. Meth. A 951 (2020) 162879 [1806.05667].\n\n[2] CMS collaboration, The Phase-2 Upgrade of the CMS Endcap Calorimeter, .\n\n[3] ATLAS Collaboration, Deep Learning for Pion Identification and Energy Calibration with the ATLAS Detector, ATL-PHYS-PUB-2020-018 (2020) .\n\n[4] C. Neub√ºser, J. Kieseler and P. Lujan, Optimising longitudinal and lateral calorimeter granularity for software compensation in hadronic showers using deep neural networks, Eur. Phys. J. C 82 (2022) 92 [2101.08150].\n\n[5] N. Akchurin, C. Cowden, J. Damgov, A. Hussain and S. Kunori, On the Use of Neural Networks for Energy Reconstruction in High-granularity Calorimeters, JINST 16 (2021) P12036 [2107.10207].\n\n[6] J. Kieseler, G.C. Strong, F. Chiandotto, T. Dorigo and L. Layer, Calorimetric Measurement of Multi-TeV Muons via Deep Regression, Eur.Phys.J.C 82 (2021) 79 [2107.02119].\n\n[7] N. Akchurin, C. Cowden, J. Damgov, A. Hussain and S. Kunori, Perspectives on the Calibration of CNN Energy Reconstruction in Highly Granular Calorimeters, 2108.10963.\n\n[8] ATLAS collaboration, Point Cloud Deep Learning Methods for Pion Reconstruction in the ATLAS Experiment, .\n\n[9] S.R. Qasim, N. Chernyavskaya, J. Kieseler, K. Long, O. Viazlo, M. Pierini et al., End-to-end multi-particle reconstruction in high occupancy imaging calorimeters with graph neural networks, Eur.Phys.J.C 82 (2022) 753 [2204.01681].\n\n[10] F.T. Acosta, B. Karki, P. Karande, A. Angerami, M. Arratia, K. Barish et al., The Optimal use of Segmentation for Sampling Calorimeters, 2310.04442.\n\n[11] ATLAS collaboration, The application of neural networks for the calibration of topological cell clusters in the ATLAS calorimeters, .\n\n[12] R. Haake and C. Loizides, Machine Learning based jet momentum reconstruction in heavy-ion collisions, Phys. Rev. C 99 (2019) 064904 [1810.06324].\n\n[13] CMS collaboration, A Deep Neural Network for Simultaneous Estimation of b Jet Energy and Resolution, Comput. Softw. Big Sci. 4 (2020) 10 [1912.06046].\n\n[14] S. Cheong, A. Cukierman, B. Nachman, M. Safdari and A. Schwartzman, Parametrizing the Detector Response with Neural Networks, JINST 15 (2020) P01030 [1910.03773].\n\n[15] ALICE collaboration, Machine Learning based jet momentum reconstruction in Pb-Pb collisions measured with the ALICE detector, PoS EPS-HEP2019 (2020) 312 [1909.01639].\n\n[16] CMS collaboration, Mass regression of highly-boosted jets using graph neural networks, .\n\n[17] R. Gambhir, B. Nachman and J. Thaler, Learning Uncertainties the Frequentist Way: Calibration and Correlation in High Energy Physics, Phys.Rev.Lett. 129 (2022) 082001 [2205.03413].\n\n[18] R. Gambhir, B. Nachman and J. Thaler, Bias and Priors in Machine Learning Calibrations for High Energy Physics, Phys.Rev.D 106 (2022) 036011 [2205.05084].\n\n[19] ATLAS Collaboration, New techniques for jet calibration with the ATLAS detector, Eur.Phys.J.C 83 (2023) 761 [2303.17312].\n\n[20] ATLAS collaboration, Simultaneous energy and mass calibration of large-radius jets with the ATLAS detector using a deep neural network, 2311.08885.\n\n[21] ALICE collaboration, Measurement of the radius dependence of charged-particle jet suppression in Pb-Pb collisions at sNNsubscriptùë†NN\\sqrt{s_{\\rm NN}}square-root start_ARG italic_s start_POSTSUBSCRIPT roman_NN end_POSTSUBSCRIPT end_ARG = 5.02 TeV, 2303.00592.\n\n[22] M. Diefenthaler, A. Farhat, A. Verbytskyi and Y. Xu, Deeply Learning Deep Inelastic Scattering Kinematics, Eur.Phys.J.C 82 (2021) 1064 [2108.11638].\n\n[23] M. Arratia, D. Britzger, O. Long and B. Nachman, Reconstructing the Kinematics of Deep Inelastic Scattering with Deep Learning, Nucl.Instrum.Meth.A 1025 (2021) 166164 [2110.05505].\n\n[24] M. Leigh, J.A. Raine and T. Golling, ŒΩùúà\\nuitalic_ŒΩ-Flows: conditional neutrino regression, SciPost Phys. 14 (2022) 159 [2207.00664].\n\n[25] J.A. Raine, M. Leigh, K. Zoch and T. Golling, ŒΩ2superscriptùúà2\\nu^{2}italic_ŒΩ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT-Flows: Fast and improved neutrino reconstruction in multi-neutrino final states with conditional normalizing flows, 2307.02405.\n\n[26] A. Cukierman and B. Nachman, Mathematical Properties of Numerical Inversion for Jet Calibrations, Nucl. Instrum. Meth. A 858 (2017) 1 [1609.05195].\n\n[27] ATLAS Collaboration, Simultaneous Jet Energy and Mass Calibrations with Neural Networks, Tech. Rep. ATL-PHYS-PUB-2020-001, CERN, Geneva (Jan, 2020).\n\n[28] ATLAS Collaboration, Generalized Numerical Inversion: A Neural Network Approach to Jet Calibration, Tech. Rep. ATL-PHYS-PUB-2018-013, CERN, Geneva (Jul, 2018).\n\n[29] D.J. MacKay, Probable networks and plausible predictions-a review of practical bayesian methods for supervised neural networks, Network: computation in neural systems 6 (1995) 469.\n\n[30] R.M. Neal, Bayesian learning for neural networks, vol. 118, Springer Science & Business Media (2012).\n\n[31] Y. Gal et al., Uncertainty in deep learning, .\n\n[32] G. Kasieczka, M. Luchmann, F. Otterpohl and T. Plehn, Per-Object Systematics using Deep-Learned Calibration, 2003.11099.\n\n[33] M. Bellagente, M. Hau√ümann, M. Luchmann and T. Plehn, Understanding Event-Generation Networks via Uncertainties, SciPost Phys. 13 (2021) 003 [2104.04543].\n\n[34] V. Mikuni and B. Nachman, High-dimensional and Permutation Invariant Anomaly Detection, 2306.03933.\n\n[35] T. Finke, M. Kr√§mer, A. M√ºck and J. T√∂nshoff, Learning the language of QCD jets with transformers, JHEP 06 (2023) 184 [2303.07364].\n\n[36] E.G. Tabak and C.V. Turner, A family of nonparametric density estimation algorithms, Communications on Pure and Applied Mathematics 66 (2013) 145.\n\n[37] L. Dinh, D. Krueger and Y. Bengio, Nice: Non-linear independent components estimation, arXiv preprint arXiv:1410.8516 (2014) .\n\n[38] D.J. Rezende and S. Mohamed, Variational inference with normalizing flows, 2015. 10.48550/ARXIV.1505.05770.\n\n[39] L. Dinh, J. Sohl-Dickstein and S. Bengio, Density estimation using real nvp, arXiv preprint arXiv:1605.08803 (2016) .\n\n[40] G. Papamakarios, E. Nalisnick, D.J. Rezende, S. Mohamed and B. Lakshminarayanan, Normalizing flows for probabilistic modeling and inference, .\n\n[41] C. Winkler, D.E. Worrall, E. Hoogeboom and M. Welling, Learning likelihoods with conditional normalizing flows, CoRR abs/1912.00042 (2019) [1912.00042].\n\n[42] C. Krause and D. Shih, CaloFlow: Fast and Accurate Generation of Calorimeter Showers with Normalizing Flows, Phys.Rev.D 107 (2021) 113003 [2106.05285].\n\n[43] C. Krause and D. Shih, CaloFlow II: Even Faster and Still Accurate Generation of Calorimeter Showers with Normalizing Flows, Phys.Rev.D 107 (2021) 113004 [2110.11377].\n\n[44] C. Krause, I. Pang and D. Shih, CaloFlow for CaloChallenge Dataset 1, 2210.14245.\n\n[45] M.R. Buckley, C. Krause, I. Pang and D. Shih, Inductive CaloFlow, 2305.11934.\n\n[46] S. Diefenbacher, E. Eren, F. Gaede, G. Kasieczka, C. Krause, I. Shekhzadeh et al., L2LFlows: Generating High-Fidelity 3D Calorimeter Images, JINST 18 (2023) P10017 [2302.11594].\n\n[47] A. Xu, S. Han, X. Ju and H. Wang, Generative Machine Learning for Detector Response Modeling with a Conditional Normalizing Flow, 2303.10148.\n\n[48] I. Pang, J.A. Raine and D. Shih, SuperCalo: Calorimeter shower super-resolution, 2308.11700.\n\n[49] F. Ernst, L. Favaro, C. Krause, T. Plehn and D. Shih, Normalizing Flows for High-Dimensional Detector Simulations, 2312.09290.\n\n[50] B. Nachman and D. Shih, Anomaly Detection with Density Estimation, Phys. Rev. D 101 (2020) 075042 [2001.04990].\n\n[51] C. Gao, J. Isaacson and C. Krause, i-flow: High-Dimensional Integration and Sampling with Normalizing Flows, 2001.05486.\n\n[52] E. Bothmann, T. Jan√üen, M. Knobbe, T. Schmale and S. Schumann, Exploring phase space with Neural Importance Sampling, 2001.05478.\n\n[53] C. Gao, S. H√∂che, J. Isaacson, C. Krause and H. Schulz, Event Generation with Normalizing Flows, Phys. Rev. D 101 (2020) 076002 [2001.10028].\n\n[54] M. Bellagente, A. Butter, G. Kasieczka, T. Plehn, A. Rousselot, R. Winterhalder et al., Invertible Networks or Partons to Detector and Back Again, SciPost Phys. 9 (2020) 074 [2006.06685].\n\n[55] B. Stienen and R. Verheyen, Phase space sampling and inference from weighted events with autoregressive flows, SciPost Phys. 10 (2021) 038 [2011.13445].\n\n[56] S. Bieringer, A. Butter, T. Heimel, S. H√∂che, U. K√∂the, T. Plehn et al., Measuring QCD Splittings with Invertible Networks, SciPost Phys. 10 (2020) 126 [2012.09873].\n\n[57] A. Hallin, J. Isaacson, G. Kasieczka, C. Krause, B. Nachman, T. Quadfasel et al., Classifying Anomalies THrough Outer Density Estimation (CATHODE), Phys.Rev.D 106 (2021) 055006 [2109.00546].\n\n[58] T. Bister, M. Erdmann, U. K√∂the and J. Schulte, Inference of cosmic-ray source properties by conditional invertible neural networks, Eur.Phys.J.C 82 (2021) 171 [2110.09493].\n\n[59] A. Butter, T. Heimel, S. Hummerich, T. Krebs, T. Plehn, A. Rousselot et al., Generative Networks for Precision Enthusiasts, SciPost Phys. 14 (2021) 078 [2110.13632].\n\n[60] R. Winterhalder, V. Magerya, E. Villa, S.P. Jones, M. Kerner, A. Butter et al., Targeting Multi-Loop Integrals with Neural Networks, SciPost Phys. 12 (2022) 129 [2112.09145].\n\n[61] A. Butter, S. Diefenbacher, G. Kasieczka, B. Nachman, T. Plehn, D. Shih et al., Ephemeral Learning ‚Äì Augmenting Triggers with Online-Trained Normalizing Flows, SciPost Phys. 13 (2022) 087 [2202.09375].\n\n[62] R. Verheyen, Event Generation and Density Estimation with Surjective Normalizing Flows, SciPost Phys. 13 (2022) 047 [2205.01697].\n\n[63] A. Butter, T. Heimel, T. Martini, S. Peitzsch and T. Plehn, Two Invertible Networks for the Matrix Element Method, SciPost Phys. 15 (2022) 094 [2210.00019].\n\n[64] A. Hallin, G. Kasieczka, T. Quadfasel, D. Shih and M. Sommerhalder, Resonant anomaly detection without background sculpting, Phys.Rev.D 107 (2022) 114012 [2210.14924].\n\n[65] T. Heimel, R. Winterhalder, A. Butter, J. Isaacson, C. Krause, F. Maltoni et al., MadNIS ‚Äì Neural Multi-Channel Importance Sampling, SciPost Phys. 15 (2022) 141 [2212.06172].\n\n[66] M. Backes, A. Butter, M. Dunford and B. Malaescu, An unfolding method based on conditional Invertible Neural Networks (cINN) using iterative training, 2212.08674.\n\n[67] D. Sengupta, S. Klein, J.A. Raine and T. Golling, CURTAINs Flows For Flows: Constructing Unobserved Regions with Maximum Likelihood Estimation, 2305.04646.\n\n[68] J. Ackerschott, R.K. Barman, D. Gon√ßalves, T. Heimel and T. Plehn, Returning CP-Observables to The Frames They Belong, 2308.00027.\n\n[69] T. Heimel, N. Huetsch, R. Winterhalder, T. Plehn and A. Butter, Precision-Machine Learning for the Matrix Element Method, 2310.07752.\n\n[70] T. Heimel, N. Huetsch, F. Maltoni, O. Mattelaer, T. Plehn and R. Winterhalder, The MadNIS Reloaded, 2311.01548.\n\n[71] C. Bierlich, P. Ilten, T. Menzo, S. Mrenna, M. Szewc, M.K. Wilkinson et al., Towards a data-driven model of hadronization using normalizing flows, 2311.09296.\n\n[72] R. Das, G. Kasieczka and D. Shih, Residual ANODE, 2312.11629.\n\n[73] H. Du, C. Krause, V. Mikuni, B. Nachman, I. Pang and D. Shih, Electromagnetic + Hadronic Sampling Calorimeter Shower Images, Apr., 2024. 10.5281/zenodo.11073232.\n\n[74] B. Nachman, L. de Oliveira and M. Paganini, Electromagnetic calorimeter shower images, Mendeley Data (2017) .\n\n[75] M. Paganini, L. de Oliveira and B. Nachman, Accelerating Science with Generative Adversarial Networks: An Application to 3D Particle Showers in Multilayer Calorimeters, Phys. Rev. Lett. 120 (2018) 042003 [1705.02355].\n\n[76] M. Paganini, L. de Oliveira and B. Nachman, CaloGAN : Simulating 3D high energy particle showers in multilayer electromagnetic calorimeters with generative adversarial networks, Phys. Rev. D97 (2018) 014021 [1712.10321].\n\n[77] C. Krause, B. Nachman, I. Pang, D. Shih and Y. Zhu, Anomaly detection with flow-based fast calorimeter simulators, 2312.11618.\n\n[78] G. Papamakarios, D. Sterratt and I. Murray, Sequential neural likelihood: Fast likelihood-free inference with autoregressive flows, in The 22nd international conference on artificial intelligence and statistics, pp. 837‚Äì848, PMLR, 2019.\n\n[79] G. Papamakarios, Neural density estimation and likelihood-free inference, arXiv preprint arXiv:1910.13233 (2019) .\n\n[80] K. Cranmer, J. Brehmer and G. Louppe, The frontier of simulation-based inference, Proceedings of the National Academy of Sciences 117 (2020) 30055.\n\n[81] S. Dirmeier, C. Albert and F. Perez-Cruz, Simulation-based inference using surjective sequential neural likelihood estimation, arXiv preprint arXiv:2308.01054 (2023) .\n\n[82] GEANT4 collaboration, GEANT4‚Äìa simulation toolkit, Nucl. Instrum. Meth. A 506 (2003) 250.\n\n[83] J. Allison, K. Amako, J. Apostolakis, H. Araujo, P. Arce Dubois, M. Asai et al., Geant4 developments and applications, IEEE Transactions on Nuclear Science 53 (2006) 270.\n\n[84] J. Allison, K. Amako, J. Apostolakis, P. Arce, M. Asai, T. Aso et al., Recent developments in geant4, Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment 835 (2016) 186.\n\n[85] ATLAS collaboration, ATLAS liquid-argon calorimeter: Technical Design Report, Technical design report. ATLAS, CERN, Geneva (1996), 10.17181/CERN.FWRW.FOOQ.\n\n[86] A. Butter, S. Diefenbacher, G. Kasieczka, B. Nachman and T. Plehn, Amplifying statistics with ensembles of generative models, in ICLR 2021 SimDL Workshop https://simdl. github. io/files/18. pdf, 2021.\n\n[87] D.P. Kingma and J. Ba, Adam: A method for stochastic optimization, 2014.\n\n[88] P. Virtanen, R. Gommers, T.E. Oliphant, M. Haberland, T. Reddy, D. Cournapeau et al., SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python, Nature Methods 17 (2020) 261.\n\n[89] B. Efron, Bootstrap Methods: Another Look at the Jackknife, The Annals of Statistics 7 (1979) 1 .\n\n[90] D. Scott, Multivariate Density Estimation: Theory, Practice, and Visualization, John Wiley & Sons, New York, Chicester (1992).\n\n[91] C.W. Fabjan and F. Gianotti, Calorimetry for particle physics, Rev. Mod. Phys. 75 (2003) 1243.\n\n[92] G. Papamakarios, T. Pavlakou and I. Murray, Masked autoregressive flow for density estimation, Advances in neural information processing systems 30 (2017) .\n\n[93] C. Durkan, A. Bekasov, I. Murray and G. Papamakarios, Neural spline flows, Advances in Neural Information Processing Systems 32 (2019) 7511 [1906.04032].\n\n[94] M. Germain, K. Gregor, I. Murray and H. Larochelle, Made: Masked autoencoder for distribution estimation, in International conference on machine learning, pp. 881‚Äì889, PMLR, 2015."
    }
}
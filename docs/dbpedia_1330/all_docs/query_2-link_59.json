{
    "id": "dbpedia_1330_2",
    "rank": 59,
    "data": {
        "url": "http://www.cl.uni-heidelberg.de/statnlpgroup/empirical_methods_tutorial/",
        "read_more_link": "",
        "language": "en",
        "title": "Tutorial: Reproducible Machine Learning",
        "top_image": "http://www.cl.uni-heidelberg.de/statnlpgroup/images/favicon.ico",
        "meta_img": "http://www.cl.uni-heidelberg.de/statnlpgroup/images/favicon.ico",
        "images": [
            "http://www.cl.uni-heidelberg.de/statnlpgroup/images/statnlp_logo.png",
            "http://www.cl.uni-heidelberg.de/statnlpgroup/images/empirical_methods/reproducibility_plot-web_version.png",
            "http://www.cl.uni-heidelberg.de/statnlpgroup/images/avatar/stefan.jpg",
            "http://www.cl.uni-heidelberg.de/statnlpgroup/images/avatar/michael_hagmann.jpg",
            "http://www.cl.uni-heidelberg.de/statnlpgroup/images/uni_logo.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2024-06-20T14:16:14+00:00",
        "summary": "",
        "meta_description": "Tutorial on Statistical Methods for Reproducible Machine Learning",
        "meta_lang": "en",
        "meta_favicon": "/statnlpgroup/images/favicon.ico",
        "meta_site_name": "StatNLP Heidelberg",
        "canonical_link": "http://www.cl.uni-heidelberg.de/statnlpgroup/empirical_methods_tutorial/",
        "text": "Overview\n\nScientific progress in machine learning is driven by empirical studies that evaluate the relative quality of models. The goal of such an evaluation is to compare machine learning methods themselves, not to reproduce single test-set evaluations of particular optimized instances of trained models. The practice of reporting performance scores of single best models is particularly inadequate for deep learning because of a strong dependence of their performance on various sources of randomness. Such an evaluation practice raises methodological questions of whether a model predicts what it purports to predict (validity), whether a modelâ€™s performance is consistent across replications of the training process (reliability), and whether a performance difference between two models is due to chance (significance). The goal of this tutorial is to provide answers to these questions by concrete statistical tests. The tutorial is hands-on and accompanied by a textbook (Riezler and Hagmann, 2024) and a webpage including R and Python code.\n\nContents\n\nIntroduction (slides)\n\nMathematical Background: Linear Mixed Effects Models (LMEMs) and Generalized Likelihood Ratio Test (GLRT) (slides)\n\nSignificance (slides)\n\nReliability (slides)\n\nRecap: A worked-through example (slides)\n\nMathematical background: Generalized Additive Models (GAMs) (slides)\n\nValidity (slides)\n\nDiscussion (slides)\n\nSlides\n\nAll slides & references in one pdf (download)\n\nCode & Data\n\nPython code to conduct an inferential analysis and example data (download)\n\nPresenters\n\nLiterature"
    }
}
{
    "id": "dbpedia_1330_2",
    "rank": 16,
    "data": {
        "url": "https://dl.acm.org/doi/fullHtml/10.1145/3630106.3658974",
        "read_more_link": "",
        "language": "en",
        "title": "One Model Many Scores: Using Multiverse Analysis to Prevent Fairness Hacking and Evaluate the Influence of Model Design Decisions",
        "top_image": "https://dl.acm.org/cms/attachment/html/10.1145/3630106.3658974/assets/html/images/facct24-89-fig1.jpg",
        "meta_img": "",
        "images": [
            "https://dl.acm.org/cms/attachment/html/10.1145/3630106.3658974/assets/html/images/facct24-89-fig1.jpg",
            "https://dl.acm.org/cms/attachment/html/10.1145/3630106.3658974/assets/html/images/facct24-89-fig2.jpg",
            "https://dl.acm.org/cms/attachment/html/10.1145/3630106.3658974/assets/html/images/facct24-89-fig3.jpg",
            "https://dl.acm.org/cms/attachment/html/10.1145/3630106.3658974/assets/html/images/facct24-89-fig4.jpg",
            "https://dl.acm.org/cms/attachment/html/10.1145/3630106.3658974/assets/html/images/facct24-89-fig5.jpg",
            "https://dl.acm.org/cms/attachment/html/10.1145/3630106.3658974/assets/html/images/facct24-89-fig6.jpg",
            "https://dl.acm.org/cms/attachment/html/10.1145/3630106.3658974/assets/html/images/facct24-89-fig7.jpg",
            "https://dl.acm.org/cms/attachment/html/10.1145/3630106.3658974/assets/html/images/facct24-89-fig8.jpg",
            "https://dl.acm.org/cms/attachment/html/10.1145/3630106.3658974/assets/html/images/facct24-89-fig9.jpg",
            "https://dl.acm.org/cms/attachment/html/10.1145/3630106.3658974/assets/html/images/facct24-89-fig10.jpg",
            "https://dl.acm.org/cms/attachment/html/10.1145/3630106.3658974/assets/html/images/facct24-89-fig11.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Jan Simson",
            "Institute of Statistics",
            "LMU Munich",
            "jan.simson@lmu.de",
            "Florian Pfisterer",
            "pfistererf@googlemail.com",
            "Christoph Kern",
            "christoph.kern@lmu.de",
            "Keywords: algorithmic fairness",
            "multiverse analysis"
        ],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "2.2.1 Study 1: Model Design Decisions. We consider 9 distinct and orthogonal design decisions. Each of these decisions has two to five unique choice options, leading to a total of N = 61440 combinations of decisions or universes. We consider decisions roughly in the order they would be made during a typical analysis.\n\nExcluding Variables as Predictors (Exclude Features). Selecting features to train a model on presents a critical design decision. In the ADM context, it can be required to exclude certain protected features (such as sex/gender, race, ethnicity) as predictors due to legal constraints when designing a machine learning system. However, as prominently shown in various studies this does not necessarily lead to increased fairness, as the protected attribute is often correlated with other (“legitimate”) features [63]. We implement the following options for this decision in our case study: (1) use all features as predictors (incl. protected ones), (2) exclude race, the protected attribute in the case study, (3) exclude sex, a sensitive attribute and (4) exclude both race and sex from modelling.\n\nExcluding Subgroups of the Protected Attribute (Exclude Subgroups). When working with variables with an uneven distribution or very rare categories one may focus only on the most common groups, dropping data for smaller ones. This can be done to preserve the privacy of small groups, due to unreliability in the data or out of convenience to allow for an easier model interpretation downstream. However, the exclusion of subgroups of the population can potentially be harmful, with discriminatory differences in downstream model predictions. While we decided to include this practice as a decision in our analysis to (1) raise awareness of the issue and (2) represent the effects of the practice in our analysis, this should not be taken as an endorsement of this practice. We try to capture the implications of this practice via the attribute race. We therefore chose to include a decision of dropping certain groups from the training data based on their prevalence. Groups were not dropped from the test data used for evaluation as part of this decision. We include six options for this decision, with the fraction of discarded data in brackets : (1) to keep all groups ($0.00\\%$), (2) to drop the smallest group ($0.01\\%$), (3) to drop the two smallest groups ($0.33\\%$), (4) to keep the two largest groups ($27.45\\%$) and (5) to drop the category “Some Other Race alone” specifically ($15.81\\%$).\n\nScaling of Continuous Variables (Scale). It is common to scale continuous variables during preprocessing, centering them on a mean of μ = 0 and standard deviation of σ = 1 (also referred to as z-scaling). Scaling may be particularly advisable if kernel-based learners are used as it typically leads to improved performance for such models. We include two options for this decision: (1) to keep continuous variables as they are and (2) to scale continuous variables.\n\nBinning of Continuous Variables (Preprocess Age, Preprocess Income). Another common practice is binning continuous variables, i.e., turning continuous variables into ordinal variables with discrete categories. The reasons to do this are plentiful: To deal with outliers, to address privacy concerns, or for a more tangible interpretation to name a few. We provide two distinct and orthogonal decisions here on whether or how to bin the variables age and income. We include four options for the variable age: (1) perform no binning, (2) bin into bins of size 10, (3) bin into three evenly sized quantiles, (4) bin into four evenly sized quantiles. Likewise, we include four options for the variable income: (1) perform no binning, (2) bin into bins of size 10, 000, (3) bin into three evenly sized quantiles, (4) bin into four evenly sized quantiles.\n\nEncoding of Categorical Variables (Encode Categorical). Another common preprocessing step includes transforming categorical variables into a numerical format. When doing this, one typically has two options: (1) One-hot (or dummy) coding each variable with K categories into K (or K − 1) new binary variables or (2) ordinally encoding each variable by assigning an integer value from 1 to K for each category. Ordinal encoding is only applicable, however, for variables with a natural ordering. For all ordinal variables (including continuous variables that have been binned), we include both options. Any variables without a natural ordering are always one-hot coded.\n\nModel Type (Model). A major choice when designing any statistical or machine learning system is which model type one decides to use. While there is a large number of potential models to explore here, we focused on the most commonly used ones in the context of ADM in the literature. We note that hyperparameter selection has shown to have an impact on fairness, but choose to focus on other choices, as HPO has already been studied elsewhere [47]. We therefore support the following model types as options for this decision: (1) logistic regression [18], (2) random forest [29], (3) gradient boosting machine [25], and (4) elastic net [65] trained with their default hyperparameters.\n\nStratification of Train-Test Split (Stratify Split). Training and test sets are often created by simple random splitting of the full dataset. It can be beneficial, however, to perform this split conditional on certain groupings to ensure equal representation of all labels within both the train and test sets. We include four options for this decision: (1) to not stratify at all, using a completely random split instead, (2) to stratify using the target variable (public coverage), (3) to stratify using the protected attribute (race) and (4) to stratify using a combination of both variables.\n\nCutoff for Final Classification (Cutoff). At the end of the ML pipeline, the prediction models’ (risk) scores can be used to classify new observations based on a pre-specified classification threshold. By default a threshold of 0.5 would be used with every score equal or above classified as 1 (having coverage) and everything below as 0 (not having coverage). Actual interventions, however, are often based on the ranked list of scores such that (costly) interventions are targeted at the top X percent with the highest risk. With real-world scenarios often coming with resource-bound restrictions, one may for example only be able to provide an intervention for, say, $10\\%$ or $25\\%$ of the most in-need in the population. These real-world restrictions are typically not taken into account in fairness evaluations, despite having potentially devastating implications. We therefore also consider different cutoff values for the final predictions of the system. We support the following options for this decision: (1) use the default raw cutoff value of 0.5, (2) only treat the lowest 0.1 quantile as not having coverage, (2) only treat the lowest 0.25 quantile as not having coverage.\n\n2.2.2 Study 2: Evaluation. We consider 3 distinct and orthogonal decisions, all focusing on evaluation only. Each decision has between 2 and 7 options each. Together these produce a total of N = 28 unique evaluation strategies for any given model, without modifying the model or its predictions.\n\nGrouping of Protected Attribute (Fairness Grouping). When working with a fairness metric, it is necessary to specify for which groups of the protected attribute it is calculated. The present case study uses race as the protected attribute. For protected attributes with more than two categories, however, multiple comparisons can be computed. Depending on the application context one may, e.g., simplify these groups into the largest group (majority) and all other groups (minority) . An important note regarding this decision is that it changes how the fairness metric is calculated: with two groups, the difference between those two groups is calculated, however, with more than two groups all possible differences between group-pairs are calculated and the largest difference between them is used (the default behaviour in Weerts et al. [62]). Naturally, this has a strong influence on the fairness metric. We include two options for this decision: (1) The fairness metric is computed between the majority group and minority group and (2) the fairness metric is computed as the maximum of the metric as computed between all groups of the protected attribute (race).\n\nExclusion of Subgroups during Evaluation (Eval Exclude Subgroups). Similarly to how subgroups of the protected attribute may be excluded from the training data, they may also be excluded from the test data used for evaluation, with potentially even greater adverse impact. We examine the exclusion of the same subgroups as in the decision Exclude Subgroups in Study 1 (Section 2.2.1) and vary whether or not subgroups are also excluded from the test dataset. The same warnings raised for that decision are even more relevant for this decision and we strongly discourage the exclusion of subgroups in any system.\n\nEvaluation using a Subset of the Data (Eval on Subset). When assessing the fairness of a system, the evaluation may happen on only a subset of the eventual target population, for example because some populations may be easier to reach or because the model deployment context changes over time. While this practice is obviously not desirable, it may be necessary in certain situations due to real-world limitations in resources. An example of this is the popular COMPAS dataset [5] which was constructed using only data from a single county (Broward County, Florida), as a larger-scale construction of such a dataset would not have been feasible. We examine the following options for this decision, to represent possible population subsets one may use for evaluation: (1) examining only the largest geographical region (in terms of sample size), (2) examining the geographical region with the largest fraction of the privileged group; examining only data from the counties of (3) Los Angeles or (4) San Francisco, (5) examining a subset of only non-military people (as former military status may affect healthcare status), (6) examining only U.S. citizens and (7) not examining any subset, but rather using the full test data for evaluation.\n\n3.1.1 Importance of Decisions. We conducted a FANOVA [30] as described in Hutter et al. [31] to assess the importance of decisions on the fairness metric. This analysis decomposes the overall variance of the fairness metric into the fractions which are explained by each decision. These variance decompositions are used to assess the relative importance of decisions. Moreover, the FANOVA also allows computing explained variance for interactions of decisions. This is highly useful, as the overall interaction space between decisions is quite large with 511 possible (interaction and main) effects.\n\nUsing the resulting importance values from the FANOVA, one can see which decisions are associated with a high variation in fairness scores, whether it be by themselves or in conjunction with others. This allows assessing the most consequential decisions on a one-by-one case. Table 2 contains a ranked list of the most important decisions and decision interactions in our case study alongside their respective importance.\n\nAs can be seen in Table 2, the most important decision is how the stratification of the train-test split is performed. Moreover, the interaction of the chosen cutoff value with the stratification strategy is highly important, accounting for more than $30\\%$ of the variance in the fairness metric. It also becomes apparent that especially the interactions of decisions are relevant here, with all decisions among the top 10 except the stratification and cutoff being interactions rather than sole decisions.\n\nWe analyzed the three most important decisions or decision-interactions to further illustrate the methodology and how one would explore the results of the analysis. The results also highlight why one should investigate the decisions in a detailed manner and not just pick the most-fair and highest-performing universe's model. The decisions Stratify Split, Cutoff and their interaction account for all three of the most important decisions. When examining the decision separately, it can be seen how stratifying by the target variable leads to noticeably lower fairness scores (Figure 4 A, most important) and how the raw cutoff value of 0.5 is suddenly not leading to the best fairness scores anymore (Figure 4 B, third most important). The effects of both variables become most clear, however, when examining their interaction, which was identified as explaining almost as much variance as the most important decision. While using a cutoff value corresponding to the top $10\\%$ quantile leads to the least fair model when stratifying by the target variable it surprisingly leads to the models with the best average fairness metric when using any other stratification strategy (Figure 4 C, second most important).\n\nAs variation in random train-test splits can affect fairness and performance of machine learning models [17, 24], we repeated the complete multiverse analysis five times with different random seeds, achieving highly similar results regarding both the overall variation of the fairness metric (Figure A7) and the relative importance of decisions (Figure A8)."
    }
}
{
    "id": "dbpedia_4885_1",
    "rank": 77,
    "data": {
        "url": "https://www.tabvizexplorer.com/category/sentiment-analysis/",
        "read_more_link": "",
        "language": "en",
        "title": "Sentiment Analysis Archives",
        "top_image": "https://i0.wp.com/tabvizexplorer.com/wp-content/uploads/2018/05/wordcloud_after.jpeg?resize=452%2C356",
        "meta_img": "",
        "images": [
            "https://i0.wp.com/tabvizexplorer.com/wp-content/uploads/2018/05/WordcloudBefore.jpeg?resize=452%2C356",
            "https://i0.wp.com/tabvizexplorer.com/wp-content/uploads/2018/05/wordcloud_after.jpeg?resize=452%2C356",
            "https://i0.wp.com/tabvizexplorer.com/wp-content/uploads/2018/05/Sentiment.jpeg?resize=512%2C403",
            "https://www.tabvizexplorer.com/wp-content/plugins/social-profiles-widget/assets/images/default/Feed_24x24.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2018-05-30T11:47:18+10:00",
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "Tabvizexplorer.com",
        "canonical_link": "https://www.tabvizexplorer.com/category/sentiment-analysis/",
        "text": "This post we will learn about developing an predictive model to predict deal or no deal using Shark Tank dataset (US based show).\n\nProblem Statement\n\nShark Tank is a US based show wherein entrepreneurs and founders pitch their businesses in front of investors (aka Sharks) who decides to invest or not in the businesses based on multiple parameters.\n\nHere, we have got an dataset containing Shark Tank episodes with 495 records where each entrepreneur making their pitch to investors (aka sharks). Using multiple algorithms, we will predict given the description of new pitch, how likely is the pitch will convert into success or not.\n\nImport Dataset and Representation along with data cleaning\n\nImport the shark tank dataset into R\n\n# Read in the data Sharktank = read.csv(\"Shark Tank Companies-1.csv\", stringsAsFactors=FALSE)\n\nLoad all the libraries required for text mining\n\n# Load Library library(tm) library(SnowballC)\n\nTo use tm pacakge we first need to transform dataset into a corpus with required variable i.e. description. Next we normalize the texts in the reviews:\n\n1. Switch to lower case\n\n2. Remove punctuation marks and stopwords\n\n3. Remove extra whitespaces\n\n4. Stem the documents\n\n# Create corpus corpus = Corpus(VectorSource(Sharktank$description)) # Convert to lower-case corpus = tm_map(corpus, tolower) # Remove punctuation corpus = tm_map(corpus, removePunctuation) # Word cloud before removing stopwords library(wordcloud) wordcloud(corpus,colors=rainbow(7),max.words=100)\n\n# Remove stopwords, the, and corpus = tm_map(corpus, removeWords, c(\"the\", \"and\", stopwords(\"english\"))) # Remove extra whitespaces if any corpus = tm_map(corpus, stripWhitespace) # Stem document corpus = tm_map(corpus, stemDocument) # Word cloud after removing stopwords and cleaning wordcloud(corpus,colors=rainbow(7),max.words=100)\n\nTo analyze the texts, we need to use DTM (Document-Term Matrix): basically converting all the documents as rows, terms/words as columns, frequency of the term in the document. This will help us identify unique words in the corpus used frequently.\n\n#Document term matrix frequencies = DocumentTermMatrix(corpus) frequencies\n\n## <<DocumentTermMatrix (documents: 495, terms: 3501)>> ## Non-/sparse entries: 9531/1723464 ## Sparsity : 99% ## Maximal term length: 21 ## Weighting : term frequency (tf)\n\nTo reduce the dimensions in DTM, we will remove less frequent words using removeSparseTerms and sparsity less than 0.995\n\n# Remove sparse terms sparse = removeSparseTerms(frequencies, 0.995)\n\nConvert this dataset into data.frame and add dependant variable deal into data frame as final step for data preparation\n\n# Convert to a data frame descSparse = as.data.frame(as.matrix(sparse)) # Make all variable names R-friendly colnames(descSparse) = make.names(colnames(descSparse)) # Add dependent variable descSparse$deal = Sharktank$deal #Get no of deals table(descSparse$deal)\n\n## ## FALSE TRUE ## 244 251\n\nPredictive modelling\n\nTo predict whether investors(aka shark) will invest in the businesses we will use deal as an output variable and use the CART, logistic regression and random forest models to measure the performance and accuracy of the model.\n\nCART Model\n\n# Build CART model library(rpart) library(rpart.plot) SharktankCart = rpart(deal ~ ., data=descSparse, method=\"class\") #CART Diagram prp(SharktankCart, extra=2)\n\n# Evaluate the performance of the CART model predictCART = predict(SharktankCart, data=descSparse, type=\"class\") CART_initial <- table(descSparse$deal, predictCART) # Baseline accuracy BaseAccuracyCart = sum(diag(CART_initial))/sum(CART_initial)\n\nRandom Forest Model\n\n# Random forest model library(randomForest) set.seed(123) SharktankRF = randomForest(deal ~ ., data=descSparse)\n\n## Warning in randomForest.default(m, y, ...): The response has five or fewer ## unique values. Are you sure you want to do regression?\n\n# Make predictions: predictRF = predict(SharktankRF, data=descSparse) # Evaluate the performance of the Random Forest RandomForestInitial <- table(descSparse$deal, predictRF>= 0.5) # Baseline accuracy BaseAccuracyRF = sum(diag(RandomForestInitial))/sum(RandomForestInitial) #variable importance as measured by a Random Forest varImpPlot(SharktankRF,main='Variable Importance Plot: Shark Tank',type=2)\n\nLogistic Regression Model\n\n# Logistic Regression model set.seed(123) Sharktanklogistic = glm(deal~., data = descSparse) # Make predictions: predictLogistic = predict(Sharktanklogistic, data=descSparse) # Evaluate the performance of the Random Forest LogisticInitial <- table(descSparse$deal, predictLogistic> 0.5) # Baseline accuracy BaseAccuracyLogistic = sum(diag(LogisticInitial))/sum(LogisticInitial)\n\nNow let’s add additional variable called as Ratio which will be derived using column askfor/valuation and then we will re-run the models to see if we can have improved accuracy in the models\n\n# Add ratio variable into descSparse descSparse$ratio = Sharktank$askedFor/Sharktank$valuation #re-run the models to see if any changes ########CART Model########### SharktankCartRatio = rpart(deal ~ ., data=descSparse, method=\"class\") #CART Diagram prp(SharktankCartRatio, extra=2)\n\n# Evaluate the performance of the CART model predictCARTRatio = predict(SharktankCartRatio, data=descSparse, type=\"class\") CART_ratio <- table(descSparse$deal, predictCARTRatio) # Baseline accuracy BaseAccuracyRatio = sum(diag(CART_ratio))/sum(CART_ratio) #########Random Forrest############# #Random Forrest Model SharktankRFRatio = randomForest(deal ~ ., data=descSparse)\n\n## Warning in randomForest.default(m, y, ...): The response has five or fewer ## unique values. Are you sure you want to do regression?\n\n#Make predictions: predictRFRatio = predict(SharktankRFRatio, data=descSparse) # Evaluate the performance of the Random Forest RandomForestRatio <- table(descSparse$deal, predictRFRatio>= 0.5) # Baseline accuracy BaseAccuracyRFRatio = sum(diag(RandomForestRatio))/sum(RandomForestRatio) #variable importance as measured by a Random Forest varImpPlot(SharktankRFRatio,main='Variable Importance Plot: Shark Tank with Ratio',type=2)\n\n#########Logistic Regression########## #Logistic Model SharktanklogisticRatio = glm(deal~., data = descSparse) # Make predictions: predictLogisticRatio = predict(SharktanklogisticRatio, data=descSparse) # Evaluate the performance of the Random Forest LogisticRatio <- table(descSparse$deal, predictLogisticRatio>= 0.5) # Baseline accuracy BaseAccuracyLogisticRatio = sum(diag(LogisticRatio))/sum(LogisticRatio)\n\nConclusion\n\nLets look at the accuracy of each model before ratio column and after ratio column added into dataset for text mining.\n\n####CART MODEL #Before Ratio Column BaseAccuracyCart\n\n## [1] 0.6565657\n\n#After Ratio Column BaseAccuracyRatio\n\n## [1] 0.6606061\n\n####Logistic Regression #Before Ratio Column BaseAccuracyLogistic\n\n## [1] 0.9979798\n\n#After Ratio Column BaseAccuracyLogisticRatio\n\n## [1] 1\n\n####RandomForest #Before Ratio Column BaseAccuracyRF\n\n## [1] 0.5535354\n\n#After Ratio Column BaseAccuracyRFRatio\n\n## [1] 0.5575758\n\nWith CART Model we were able to predict around 65.65% and 66.06% accurate results using only description and description+ratio respectively. Using Random Forest, we were able to predict 55.35% and 55.75% accurate results using only description and description+ratio respectively.\n\nWith Logistic regression, it gave us 100% accuracy with both parameters however, this requires further validation with significant variables and remove unnecessary variables to derive an measureable output.\n\nI would urge readers to implement and use the knowledge from this post in making their own analysis on text and solve various problems.\n\nThat’s all for now. Please do let me know your feedback and if any particular topic you would like me to write on.\n\nDo subscribe to Tabvizexplorer.com to keep receive regular updates.\n\nAfter writing previous article on Twitter Sentiment Analysis on #royalwedding, I thought why not do analysis on ABC news online website and see if we can uncover some interesting insights. This is some good practice to do some data scrapping, text mining and use few algorithms to practice.\n\nBelow is the step by step guide:\n\nTo start with we will scrap headlines from ABC news for the duration of entire 2018. To get historical headlines, we will scrap abc news homepage via Wayback Machine. Post few data crunching and manipulation we will get the clean headlines.\n\nStep 1:\n\nLoad all important libraries which we will be using in this practice\n\n#Importing Libraries library(stringr) library(jsonlite) library(httr) library(rvest) library(dplyr) library(V8) library(tweenr) library(syuzhet) library(tidyverse) #Path where files output will be stored path<-'/Mithun/R/WebScraping/'\n\nStep 2:\n\nUse wayback API call with abc.net.au/news and pass this information into json with the text content.\n\nHere we will also filter the time stamp to have dates from 1st Jan’18.\n\n#Internet WAYBACK API CALL http://www.abc.net.au/news/ AU_url<-'http://web.archive.org/cdx/search/cdx?url=abc.net.au/news/&output=json' #API request req <- httr::GET(AU_url, timeout(20)) #Get data json <- httr::content(req, as = \"text\") api_dat <- fromJSON(json) #Get timestamps which will be used to pass in API time_stamps<-api_dat[-1,2] #Reverse order (so recent first) time_stamps<-rev(time_stamps) #Scrap the each and every URL to get headlines from theaustralian.com.au website head(time_stamps,n=50) #Filter time_stamps to have dates after 2018 time_stamps<-time_stamps[as.numeric(substr(time_stamps,1,8))>=20180000]\n\nStep 3\n\nWe will create an loop where we will pass URL with necessary timestamp to get all headlines which were published from 1st to 21st May’18 on abc news website. Also, we will remove all duplicate headlines which we might have got while scraping the website.\n\n#Dataframe to store output and loop to get headlines abc_scrap_all <-NA for(s in 1:length(time_stamps)){ Sys.sleep(1) feedurl<-paste0('https://web.archive.org/web/',time_stamps[s],'/http://www.abc.net.au/news/') print(feedurl) if(!is.na(feedurl)){ print('Valid URL') #Scrap the data from URLs try(feed_dat<-read_html(feedurl),timeout(10),silent=TRUE) if(exists('feed_dat')){ #USE initial<-html_nodes(feed_dat,\"[href*='/news/2018']\") #Date Date <- substr(time_stamps[s],1,8) #Get headlines headlines<-initial %>% html_text() #Combine comb<-data.frame(Date,headlines,stringsAsFactors = FALSE) #Remove NA headlines comb<-comb[!(is.na(comb$headlines) | comb$headlines==\"\" | comb$headlines==\" \") ,] #As a df comb<-data.frame(comb) #Remove duplicates on daily level comb <- comb[!duplicated(comb$headlines),] if(length(comb$headlines)>0){ #Save with the rest abc_scrap_all<-rbind(abc_scrap_all,comb) } rm(comb) rm(feed_dat) } } } #Remove duplicates abc_scrap_all_final <- abc_scrap_all[!duplicated(abc_scrap_all$headlines),]\n\nStep 4:\n\nWe will use the headlines and do sentiment analysis on the headlines using Syuzhet package and see if we can make some conclusion\n\nlibrary('syuzhet') abc_scrap_all_final$headlines <- str_replace_all(abc_scrap_all_final$headlines,\"[^[:graph:]]\", \" \") Sentiment <-get_nrc_sentiment(abc_scrap_all_final$headlines) td<-data.frame(t(Sentiment)) td_Rowsum <- data.frame(rowSums(td[2:1781])) #Transformation and cleaning names(td_Rowsum)[1] <- \"count\" td_Rowsum <- cbind(\"sentiment\" = rownames(td_Rowsum), td_Rowsum) rownames(td_Rowsum) <- NULL td_Plot<-td_Rowsum[1:10,] #Vizualisation library(\"ggplot2\") qplot(sentiment, data=td_Plot, weight=count, geom=\"bar\",fill=sentiment)+ggtitle(\"Abc News headlines sentiment analysis\")\n\nConclusion on Sentiment Analysis:\n\nHuman brain tends to be more attentive to negative information. To grab attention of readers, most of the media houses focus on negative and fear related news. Thats what we see when we analyzed the abc news website headlines as well.\n\nLets analyze further on headlines:\n\nWordcloud for frequently used words in headlines\n\nlibrary(tm) library(wordcloud) corpus = Corpus(VectorSource(tolower(abc_scrap_all_final$headlines))) corpus = tm_map(corpus, removePunctuation) corpus = tm_map(corpus, removeWords, stopwords(\"english\")) frequencies = DocumentTermMatrix(corpus) word_frequencies = as.data.frame(as.matrix(frequencies)) words <- colnames(word_frequencies) freq <- colSums(word_frequencies) wordcloud(words, freq, min.freq=sort(freq, decreasing=TRUE)[[100]], colors=brewer.pal(10, \"Paired\"), random.color=TRUE)\n\nSurprisingly, being an australian news agency most frequently used word in headlines related Donald trump (American President) followed by police, commonwealth games, sport and australia\n\nFind word associations:\n\nIf you have any specific word which can useful for analysis and help us identify the highly correlate words with that term. If word always appears together then correlation=1.0 and in our example we will find correlated words with 30% correlation.\n\nfindAssocs(dtm, \"tony\", corlimit=0.3)\n\n## $tony ## abbott headbutting cooke 30th hansons benneworth ## 0.64 0.35 0.30 0.30 0.30 0.30 ## mocking astro labe ## 0.30 0.30 0.30\n\n#0.3 means 30% correlation with word \"tony\"\n\nThat’s all for now. In my next post we will look further into text analytics using udpipe and see if we can build more on text association and analytics\n\nPlease do let me know your feedback and if any particular topic you would like me to write on.\n\nDo subscribe to Tabvizexplorer.com to keep receive regular updates.\n\nAfter a long break of 5 weeks I am back to blogging, Today we will go through Twitter Sentiment Analysis using R on #RoyalWedding.\n\nLast few years has been interesting revolution in social media, it is not just platform where people talk to one another but it has become platform where people:\n\nExpress interests\n\nShare views\n\nShow dissent\n\nPraise or criticize companies or politicians\n\nSo in this article we will learn how to analyze what people are posting on Twitter to come up with an solution which helps us understand about the public sentiments\n\nHow to create Twitter app\n\nTwitter has developed an API which we can use to analyze tweets posted by users and their underlying metadata. This API helps us extract data in structured format which can easily be analyzed.\n\nTo create Twitter app, you need to have twitter account and once you have that account visit twitter app page and create an application to access data. Step by step process is available on following link:\n\nhttps://iag.me/socialmedia/how-to-create-a-twitter-app-in-8-easy-steps/\n\nonce you have created the app, you will get following 4 keys:\n\na. Consumer key (API key)\n\nb. Consumer secret (API Secret)\n\nc. Access Token\n\nd. Access Token Secret\n\nThese above keys we will use it to extract data from twitter to do analysis\n\nImplementing Sentiment Analysis in R\n\nNow, we will write step by step process in R to extract tweets from twitter and perform sentiment analysis on tweets. We will select #Royalwedding as our topic of analysis\n\nExtracting tweets using Twitter application\n\nInstall the necessary packages\n\n# Install packages install.packages(\"twitteR\", repos = \"http://cran.us.r-project.org\") install.packages(\"RCurl\", repos = \"http://cran.us.r-project.org\") install.packages(\"httr\", repos = \"http://cran.us.r-project.org\") install.packages(\"syuzhet\", repos = \"http://cran.us.r-project.org\") # Load the required Packages library(twitteR) library(RCurl) library(httr) library(tm) library(wordcloud) library(syuzhet)\n\nNext step is set the Twitter API using the app we created and use the key along with access tokens to get the data\n\n# authorisation keys consumer_key = \"ABCD12345690XXXXXXXXX\" #Consumer key from twitter app consumer_secret = \"ABCD12345690XXXXXXXXX\" #Consumer secret from twitter app access_token = \"ABCD12345690XXXXXXXXX\" #access token from twitter app access_secret =\"ABCD12345690XXXXXXXXX\" #access secret from twitter app # set up setup_twitter_oauth(consumer_key,consumer_secret,access_token, access_secret)\n\n## [1] \"Using direct authentication\"\n\n# search for tweets in english language tweets = searchTwitter(\"#RoyalWedding\", n = 10000, lang = \"en\")\n\n# store the tweets into dataframe tweets.df = twListToDF(tweets)\n\nAbove code will invoke twitter app and extract the data with tweets having “#Royalwedding”. Since, Royal wedding is the flavor of season and talk of the world with everyone expressing their views on twitter.\n\nData Cleaning tweets for further analysis\n\nWe will remove hashtags, junk characters, other twitter handles and URLs from the tags using gsub function so we have tweets for further analysis\n\n# CLEANING TWEETS tweets.df$text=gsub(\"&amp\", \"\", tweets.df$text) tweets.df$text = gsub(\"&amp\", \"\", tweets.df$text) tweets.df$text = gsub(\"(RT|via)((?:\\\\b\\\\W*@\\\\w+)+)\", \"\", tweets.df$text) tweets.df$text = gsub(\"@\\\\w+\", \"\", tweets.df$text) tweets.df$text = gsub(\"[[:punct:]]\", \"\", tweets.df$text) tweets.df$text = gsub(\"[[:digit:]]\", \"\", tweets.df$text) tweets.df$text = gsub(\"http\\\\w+\", \"\", tweets.df$text) tweets.df$text = gsub(\"[ \\t]{2,}\", \"\", tweets.df$text) tweets.df$text = gsub(\"^\\\\s+|\\\\s+$\", \"\", tweets.df$text) tweets.df$text <- iconv(tweets.df$text, \"UTF-8\", \"ASCII\", sub=\"\")\n\nNow we have only relevant part of tweets which can use for analysis\n\nGetting sentiments score for each tweet\n\nLets score the emotions on each tweet as syuzhet breaks emotion into 10 different categories.\n\n# Emotions for each tweet using NRC dictionary emotions <- get_nrc_sentiment(tweets.df$text) emo_bar = colSums(emotions) emo_sum = data.frame(count=emo_bar, emotion=names(emo_bar)) emo_sum$emotion = factor(emo_sum$emotion, levels=emo_sum$emotion[order(emo_sum$count, decreasing = TRUE)])\n\nPost above steps, we are ready to visualize results to what type of emotions are dominant in the tweets\n\n# Visualize the emotions from NRC sentiments library(plotly) p <- plot_ly(emo_sum, x=~emotion, y=~count, type=\"bar\", color=~emotion) %>% layout(xaxis=list(title=\"\"), showlegend=FALSE, title=\"Emotion Type for hashtag: #RoyalWedding\") api_create(p,filename=\"Sentimentanalysis\")\n\nHere we see majority of the people are discussing positive about Royal Wedding which is good indicator for analysis.\n\nLastly, lets see which word contributes which emotion:\n\n# Create comparison word cloud data wordcloud_tweet = c( paste(tweets.df$text[emotions$anger > 0], collapse=\" \"), paste(tweets.df$text[emotions$anticipation > 0], collapse=\" \"), paste(tweets.df$text[emotions$disgust > 0], collapse=\" \"), paste(tweets.df$text[emotions$fear > 0], collapse=\" \"), paste(tweets.df$text[emotions$joy > 0], collapse=\" \"), paste(tweets.df$text[emotions$sadness > 0], collapse=\" \"), paste(tweets.df$text[emotions$surprise > 0], collapse=\" \"), paste(tweets.df$text[emotions$trust > 0], collapse=\" \") ) # create corpus corpus = Corpus(VectorSource(wordcloud_tweet)) # remove punctuation, convert every word in lower case and remove stop words corpus = tm_map(corpus, tolower) corpus = tm_map(corpus, removePunctuation) corpus = tm_map(corpus, removeWords, c(stopwords(\"english\"))) corpus = tm_map(corpus, stemDocument) # create document term matrix tdm = TermDocumentMatrix(corpus) # convert as matrix tdm = as.matrix(tdm) tdmnew <- tdm[nchar(rownames(tdm)) < 11,] # column name binding colnames(tdm) = c('anger', 'anticipation', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'trust') colnames(tdmnew) <- colnames(tdm) comparison.cloud(tdmnew, random.order=FALSE, colors = c(\"#00B2FF\", \"red\", \"#FF0099\", \"#6600CC\", \"green\", \"orange\", \"blue\", \"brown\"), title.size=1, max.words=250, scale=c(2.5, 0.4),rot.per=0.4)\n\nThis is how word cloud on tweets with #Royalwedding looks like. Basically using R, we can analyse the sentiments on the social media and this can be extended to particular handle or product to see what people are saying in social media and whether is it negative or positive\n\nPlease feel free to ask any questions or want me to write on any specific topic\n\nDo subscribe to Tabvizexplorer.com to keep receiving regular updates."
    }
}
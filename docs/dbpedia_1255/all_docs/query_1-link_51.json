{
    "id": "dbpedia_1255_1",
    "rank": 51,
    "data": {
        "url": "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/mr-azure-306",
        "read_more_link": "",
        "language": "en",
        "title": "HoloLens (1st gen) and Azure 306 - Streaming video - Mixed Reality",
        "top_image": "https://learn.microsoft.com/en-us/media/open-graph-image.png",
        "meta_img": "https://learn.microsoft.com/en-us/media/open-graph-image.png",
        "images": [
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-00.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-01.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-02.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-03.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-04.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-05.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-06.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-07.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-08.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-09.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-10.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-11.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-12.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-13.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-14.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-15.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-16.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-17.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-18.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-19.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-20.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-21.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-22.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-23.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-24.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-25.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-26.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-27.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-28.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-29.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-30.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-31.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-32.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-33.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-34.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-35.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-36.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-37.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-38.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-39.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-40.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-41.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-42.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-43.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-44.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-45.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-46.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-47.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-48.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-49.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-50.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-51.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-52.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-53.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-54.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-55.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-56.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-57.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-58.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-59.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-60.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-61.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-62.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-63.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-00.png",
            "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/images/azurelabs-lab6-01.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            "azure",
            "mixed reality",
            "academy",
            "unity",
            "tutorial",
            "api",
            "media services",
            "streaming video",
            "360",
            "immersive",
            "vr",
            "Windows 10",
            "Visual Studio"
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2023-02-01T00:00:00+00:00",
        "summary": "",
        "meta_description": "Complete this course to learn how to implement Azure Media Services within a mixed reality application.",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": "https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/tutorials/mr-azure-306",
        "text": "HoloLens (1st gen) and Azure 306: Streaming video\n\nIn this course you will learn how connect your Azure Media Services to a Windows Mixed Reality VR experience to allow streaming 360 degree video playback on immersive headsets.\n\nAzure Media Services are a collection of services that gives you broadcast-quality video streaming services to reach larger audiences on todayâs most popular mobile devices. For more information, visit the Azure Media Services page.\n\nHaving completed this course, you will have a mixed reality immersive headset application, which will be able to do the following:\n\nRetrieve a 360 degree video from an Azure Storage, through the Azure Media Service.\n\nDisplay the retrieved 360 degree video within a Unity scene.\n\nNavigate between two scenes, with two different videos.\n\nIn your application, it is up to you as to how you will integrate the results with your design. This course is designed to teach you how to integrate an Azure Service with your Unity Project. It is your job to use the knowledge you gain from this course to enhance your mixed reality application.\n\nDevice support\n\nCourse HoloLens Immersive headsets MR and Azure 306: Streaming video âï¸\n\nPrerequisites\n\nWe recommend the following hardware and software for this course:\n\nA development PC, compatible with Windows Mixed Reality for immersive (VR) headset development\n\nWindows 10 Fall Creators Update (or later) with Developer mode enabled\n\nThe latest Windows 10 SDK\n\nUnity 2017.4\n\nVisual Studio 2017\n\nA Windows Mixed Reality immersive (VR) headset\n\nInternet access for Azure setup and data retrieval\n\nTwo 360-degree videos in mp4 format (you can find some royalty-free videos at this download page)\n\nBefore you start\n\nTo avoid encountering issues building this project, it is strongly suggested that you create the project mentioned in this tutorial in a root or near-root folder (long folder paths can cause issues at build-time).\n\nSet up and test your Mixed Reality Immersive Headset.\n\nNote\n\nYou will not require Motion Controllers for this course. If you need support setting up the Immersive Headset, please click link on how to set up Windows Mixed Reality.\n\nChapter 1 - The Azure Portal: creating the Azure Storage Account\n\nTo use the Azure Storage Service, you will need to create and configure a Storage Account in the Azure portal.\n\nLog in to the Azure Portal.\n\nNote\n\nIf you do not already have an Azure account, you will need to create one. If you are following this tutorial in a classroom or lab situation, ask your instructor or one of the proctors for help setting up your new account.\n\nOnce you are logged in, click on Storage accounts in the left menu.\n\nOn the Storage Accounts tab, click on Add.\n\nIn the Create storage account tab:\n\nInsert a Name for your account, be aware this field only accepts numbers, and lowercase letters.\n\nFor Deployment model, select Resource manager.\n\nFor Account kind, select Storage (general purpose v1).\n\nFor Performance, select Standard.*\n\nFor Replication select Locally-redundant storage (LRS).\n\nLeave Secure transfer required as Disabled.\n\nSelect a Subscription.\n\nChoose a Resource group or create a new one. A resource group provides a way to monitor, control access, provision and manage billing for a collection of Azure assets.\n\nDetermine the Location for your resource group (if you are creating a new Resource Group). The location would ideally be in the region where the application would run. Some Azure assets are only available in certain regions.\n\nYou will need to confirm that you have understood the Terms and Conditions applied to this Service.\n\nOnce you have clicked on Create, you will have to wait for the service to be created, this might take a minute.\n\nA notification will appear in the portal once the Service instance is created.\n\nAt this point you do not need to follow the resource, simply move to the next Chapter.\n\nChapter 2 - The Azure Portal: creating the Media Service\n\nTo use the Azure Media Service, you will need to configure an instance of the service to be made available to your application (wherein the account holder needs to be an Admin).\n\nIn the Azure Portal, click on Create a resource in the top left corner, and search for Media Service, press Enter. The resource you want currently has a pink icon; click this, to show a new page.\n\nThe new page will provide a description of the Media Service. At the bottom left of this prompt, click the Create button, to create an association with this service.\n\nOnce you have clicked on Create a panel will appear where you need to provide some details about your new Media Service:\n\nInsert your desired Account Name for this service instance.\n\nSelect a Subscription.\n\nChoose a Resource Group or create a new one. A resource group provides a way to monitor, control access, provision and manage billing for a collection of Azure assets. It is recommended to keep all the Azure services associated with a single project (e.g. such as these labs) under a common resource group).\n\nIf you wish to read more about Azure Resource Groups, please follow this link on how to manage Azure Resource Groups.\n\nDetermine the Location for your resource group (if you are creating a new Resource Group). The location would ideally be in the region where the application would run. Some Azure assets are only available in certain regions.\n\nFor the Storage Account section, click the Please select... section, then click the Storage Account you created in the last Chapter.\n\nYou will also need to confirm that you have understood the Terms and Conditions applied to this Service.\n\nClick Create.\n\nOnce you have clicked on Create, you will have to wait for the service to be created, this might take a minute.\n\nA notification will appear in the portal once the Service instance is created.\n\nClick on the notification to explore your new Service instance.\n\nClick the Go to resource button in the notification to explore your new Service instance.\n\nWithin the new Media service page, within the panel on the left, click on the Assets link, which is about halfway down.\n\nOn the next page, in the top-left corner of the page, click Upload.\n\nClick on the Folder icon to browse your files and select the first 360 Video that you would like to stream.\n\nYou can follow this link to download a sample video.\n\nWarning\n\nLong filenames may cause an issue with the encoder: so to ensure videos do not have issues, consider shortening the length of your video file names.\n\nThe progress bar will turn green when the video has finished uploading.\n\nClick on the text above (yourservicename - Assets) to return to the Assets page.\n\nYou will notice that your video has been successfully uploaded. Click on it.\n\nThe page you are redirected to will show you detailed information about your video. To be able to use your video you need to encode it, by clicking the Encode button at the top-left of the page.\n\nA new panel will appear to the right, where you will be able to set encoding options for your file. Set the following properties (some will be already set by default):\n\nMedia encoder name Media Encoder Standard\n\nEncoding preset Content Adaptive Multiple Bitrate MP4\n\nJob name Media Encoder Standard processing of Video1.mp4\n\nOutput media asset name Video1.mp4 -- Media Encoder Standard encoded\n\nClick the Create button.\n\nYou will notice a bar with Encoding job added, click on that bar and a panel will appear with the Encoding progress displayed in it.\n\nWait for the Job to be completed. Once it is done, feel free to close the panel with the 'X' at the top right of that panel.\n\nImportant\n\nThe time this takes, depends on the file size of your video. This process can take quite some time.\n\nNow that the encoded version of the video has been created, you can publish it to make it accessible. To do so, click the blue link Assets to go back to the assets page.\n\nYou will see your video along with another, which is of Asset Type Multi-Bitrate MP4.\n\nNote\n\nYou may notice that the new asset, alongside your initial video, is Unknown, and has '0' bytes for it's Size, just refresh your window for it to update.\n\nClick this new asset.\n\nYou will see a similar panel to the one you used before, just this is a different asset. Click the Publish button located at the top-center of the page.\n\nYou will be prompted to set a Locator, which is the entry point, to file/s in your Assets. For your scenario set the following properties:\n\nLocator type > Progressive.\n\nThe date and time will be set for you, from your current date, to a time in the future (one hundred years in this case). Leave as is or change it to suit.\n\nNote\n\nFor more information about Locators, and what you can choose, visit the Azure Media Services Documentation.\n\nAt the bottom of that panel, click on the Add button.\n\nYour video is now published and can be streamed by using its endpoint. Further down the page is a Files section. This is where the different encoded versions of your video will be. Select the highest possible resolution one (in the image below it is the 1920x960 file), and then a panel to the right will appear. There you will find a Download URL. Copy this Endpoint as you will use it later in your code.\n\nNote\n\nYou can also press the Play button to play your video and test it.\n\nYou now need to upload the second video that you will use in this Lab. Follow the steps above, repeating the same process for the second video. Ensure you copy the second Endpoint also. Use the following link to download a second video.\n\nOnce both videos have been published, you are ready to move to the next Chapter.\n\nChapter 3 - Setting up the Unity Project\n\nThe following is a typical set up for developing with the Mixed Reality, and as such, is a good template for other projects.\n\nOpen Unity and click New.\n\nYou will now need to provide a Unity Project name, insert MR_360VideoStreaming.. Make sure the project type is set to 3D. Set the Location to somewhere appropriate for you (remember, closer to root directories is better). Then, click Create project.\n\nWith Unity open, it is worth checking the default Script Editor is set to Visual Studio. Go to Edit Preferences and then from the new window, navigate to External Tools. Change External Script Editor to Visual Studio 2017. Close the Preferences window.\n\nNext, go to File Build Settings and switch the platform to Universal Windows Platform, by clicking on the Switch Platform button.\n\nAlso make sure that:\n\nTarget Device is set to Any Device.\n\nBuild Type is set to D3D.\n\nSDK is set to Latest installed.\n\nVisual Studio Version is set to Latest installed.\n\nBuild and Run is set to Local Machine.\n\nDo not worry about setting up Scenes right now, as you will set these up later.\n\nThe remaining settings should be left as default for now.\n\nIn the Build Settings window, click on the Player Settings button, this will open the related panel in the space where the Inspector is located.\n\nIn this panel, a few settings need to be verified:\n\nIn the Other Settings tab:\n\nScripting Runtime Version should be Stable (.NET 3.5 Equivalent).\n\nScripting Backend should be .NET.\n\nAPI Compatibility Level should be .NET 4.6.\n\nFurther down the panel, in XR Settings (found below Publish Settings), tick Virtual Reality Supported, make sure the Windows Mixed Reality SDK is added.\n\nWithin the Publishing Settings tab, under Capabilities, check:\n\nInternetClient\n\nOnce you have made those changes, close the Build Settings window.\n\nSave your Project File Save Project.\n\nChapter 4 - Importing the InsideOutSphere Unity package\n\nFor this course you will need to download a Unity Asset Package called InsideOutSphere.unitypackage.\n\nHow-to import the unitypackage:\n\nWith the Unity dashboard in front of you, click on Assets in the menu at the top of the screen, then click on Import Package > Custom Package.\n\nUse the file picker to select the InsideOutSphere.unitypackage package and click Open. A list of components for this asset will be displayed to you. Confirm the import by clicking Import.\n\nOnce it has finished importing, you will notice three new folders, Materials, Models, and Prefabs, have been added to your Assets folder. This kind of folder structure is typical for a Unity project.\n\nOpen the Models folder, and you will see that the InsideOutSphere model has been imported.\n\nWithin the Materials folder you will find the InsideOutSpheres material lambert1, along with a material called ButtonMaterial, which is used by the GazeButton, which you will see soon.\n\nThe Prefabs folder contains the InsideOutSphere prefab which contains both the InsideOutSphere model and the GazeButton.\n\nNo code is included, you will write the code by following this course.\n\nWithin the Hierarchy, select the Main Camera object, and update the following components:\n\nTransform\n\nPosition = X: 0, Y: 0, Z: 0.\n\nRotation = X: 0, Y: 0, Z: 0.\n\nScale X: 1, Y: 1, Z: 1.\n\nCamera\n\nClear Flags: Solid Color.\n\nClipping Planes: Near: 0.1, Far: 6.\n\nNavigate to the Prefab folder, and then drag the InsideOutSphere prefab into the Hierarchy Panel.\n\nExpand the InsideOutSphere object within the Hierarchy by clicking the little arrow next to it. You will see a child object beneath it called GazeButton. This will be used to change scenes and thus videos.\n\nIn the Inspector Window click on the InsideOutSphere's Transform component, ensure that the following properties are set:\n\nTransform - Position\n\nX Y Z 0 0 0\n\nTransform - Rotation\n\nX Y Z 0 -50 0\n\nTransform - Scale\n\nX Y Z 0 0 0\n\nClick on the GazeButton child object, and set its Transform as follows:\n\nTransform - Position\n\nX Y Z 3.6 1.3 0\n\nTransform - Rotation\n\nX Y Z 0 0 0\n\nTransform - Scale\n\nX Y Z 1 1 1\n\nChapter 5 - Create the VideoController class\n\nThe VideoController class hosts the two video endpoints that will be used to stream the content from the Azure Media Service.\n\nTo create this class:\n\nRight-click in the Asset Folder, located in the Project Panel, and click Create > Folder. Name the folder Scripts.\n\nDouble click on the Scripts folder to open it.\n\nRight-click inside the folder, then click Create > C# Script. Name the script VideoController.\n\nDouble click on the new VideoController script to open it with Visual Studio 2017.\n\nUpdate the namespaces at the top of the code file as follows:\n\nusing System.Collections; using UnityEngine; using UnityEngine.SceneManagement; using UnityEngine.Video;\n\nEnter the following variables in the VideoController class, along with the Awake() method:\n\n/// <summary> /// Provides Singleton-like behaviour to this class. /// </summary> public static VideoController instance; /// <summary> /// Reference to the Camera VideoPlayer Component. /// </summary> private VideoPlayer videoPlayer; /// <summary> /// Reference to the Camera AudioSource Component. /// </summary> private AudioSource audioSource; /// <summary> /// Reference to the texture used to project the video streaming /// </summary> private RenderTexture videoStreamRenderTexture; /// <summary> /// Insert here the first video endpoint /// </summary> private string video1endpoint = \"-- Insert video 1 Endpoint here --\"; /// <summary> /// Insert here the second video endpoint /// </summary> private string video2endpoint = \"-- Insert video 2 Endpoint here --\"; /// <summary> /// Reference to the Inside-Out Sphere. /// </summary> public GameObject sphere; void Awake() { instance = this; }\n\nNow is the time to enter the endpoints from your Azure Media Service videos:\n\nThe first into the video1endpoint variable.\n\nThe second into the video2endpoint variable.\n\nWarning\n\nThere is a known issue with using https within Unity, with version 2017.4.1f1. If the videos provide an error on play, try using 'http' instead.\n\nNext, the Start() method needs to be edited. This method will be triggered every time the user switches scene (consequently switching the video) by looking at the Gaze Button.\n\n// Use this for initialization void Start() { Application.runInBackground = true; StartCoroutine(PlayVideo()); }\n\nFollowing the Start() method, insert the PlayVideo() IEnumerator method, which will be used to start videos seamlessly (so no stutter is seen).\n\nprivate IEnumerator PlayVideo() { // create a new render texture to display the video videoStreamRenderTexture = new RenderTexture(2160, 1440, 32, RenderTextureFormat.ARGB32); videoStreamRenderTexture.Create(); // assign the render texture to the object material Material sphereMaterial = sphere.GetComponent<Renderer>().sharedMaterial; //create a VideoPlayer component videoPlayer = gameObject.AddComponent<VideoPlayer>(); // Set the video to loop. videoPlayer.isLooping = true; // Set the VideoPlayer component to play the video from the texture videoPlayer.renderMode = VideoRenderMode.RenderTexture; videoPlayer.targetTexture = videoStreamRenderTexture; // Add AudioSource audioSource = gameObject.AddComponent<AudioSource>(); // Pause Audio play on Awake audioSource.playOnAwake = true; audioSource.Pause(); // Set Audio Output to AudioSource videoPlayer.audioOutputMode = VideoAudioOutputMode.AudioSource; videoPlayer.source = VideoSource.Url; // Assign the Audio from Video to AudioSource to be played videoPlayer.EnableAudioTrack(0, true); videoPlayer.SetTargetAudioSource(0, audioSource); // Assign the video Url depending on the current scene switch (SceneManager.GetActiveScene().name) { case \"VideoScene1\": videoPlayer.url = video1endpoint; break; case \"VideoScene2\": videoPlayer.url = video2endpoint; break; default: break; } //Set video To Play then prepare Audio to prevent Buffering videoPlayer.Prepare(); while (!videoPlayer.isPrepared) { yield return null; } sphereMaterial.mainTexture = videoStreamRenderTexture; //Play Video videoPlayer.Play(); //Play Sound audioSource.Play(); while (videoPlayer.isPlaying) { yield return null; } }\n\nThe last method you need for this class is the ChangeScene() method, which will be used to swap between scenes.\n\npublic void ChangeScene() { SceneManager.LoadScene(SceneManager.GetActiveScene().name == \"VideoScene1\" ? \"VideoScene2\" : \"VideoScene1\"); }\n\nTip\n\nThe ChangeScene() method uses a handy C# feature called the Conditional Operator. This allows for conditions to be checked, and then values returned based on the outcome of the check, all within a single statement. Follow this link to learn more about Conditional Operator.\n\nSave your changes in Visual Studio before returning to Unity.\n\nBack in the Unity Editor, click and drag the VideoController class [from]{.underline} the Scripts folder to the Main Camera object in the Hierarchy Panel.\n\nClick on the Main Camera and look at the Inspector Panel. You will notice that within the newly added Script component, there is a field with an empty value. This is a reference field, which targets the public variables within your code.\n\nDrag the InsideOutSphere object from the Hierarchy Panel to the Sphere slot, as shown in the image below.\n\nChapter 6 - Create the Gaze class\n\nThis class is responsible for creating a Raycast that will be projected forward from the Main Camera, to detect which object the user is looking at. In this case, the Raycast will need to identify if the user is looking at the GazeButton object in the scene and trigger a behavior.\n\nTo create this Class:\n\nGo to the Scripts folder you created previously.\n\nRight-click in the Project Panel, Create C# Script. Name the script Gaze.\n\nDouble click on the new Gaze script to open it with Visual Studio 2017.\n\nEnsure the following namespace is at the top of the script, and remove any others:\n\nusing UnityEngine;\n\nThen add the following variables inside the Gaze class:\n\n/// <summary> /// Provides Singleton-like behaviour to this class. /// </summary> public static Gaze instance; /// <summary> /// Provides a reference to the object the user is currently looking at. /// </summary> public GameObject FocusedGameObject { get; private set; } /// <summary> /// Provides a reference to compare whether the user is still looking at /// the same object (and has not looked away). /// </summary> private GameObject oldFocusedObject = null; /// <summary> /// Max Ray Distance /// </summary> float gazeMaxDistance = 300; /// <summary> /// Provides whether an object has been successfully hit by the raycast. /// </summary> public bool Hit { get; private set; }\n\nCode for the Awake() and Start() methods now needs to be added.\n\nprivate void Awake() { // Set this class to behave similar to singleton instance = this; } void Start() { FocusedGameObject = null; }\n\nAdd the following code in the Update() method to project a Raycast and detect the target hit:\n\nvoid Update() { // Set the old focused gameobject. oldFocusedObject = FocusedGameObject; RaycastHit hitInfo; // Initialise Raycasting. Hit = Physics.Raycast(Camera.main.transform.position, Camera.main.transform.forward, out hitInfo, gazeMaxDistance); // Check whether raycast has hit. if (Hit == true) { // Check whether the hit has a collider. if (hitInfo.collider != null) { // Set the focused object with what the user just looked at. FocusedGameObject = hitInfo.collider.gameObject; } else { // Object looked on is not valid, set focused gameobject to null. FocusedGameObject = null; } } else { // No object looked upon, set focused gameobject to null. FocusedGameObject = null; } // Check whether the previous focused object is this same // object (so to stop spamming of function). if (FocusedGameObject != oldFocusedObject) { // Compare whether the new Focused Object has the desired tag we set previously. if (FocusedGameObject.CompareTag(\"GazeButton\")) { FocusedGameObject.SetActive(false); VideoController.instance.ChangeScene(); } } }\n\nSave your changes in Visual Studio before returning to Unity.\n\nClick and drag the Gaze class from the Scripts folder to the Main Camera object in the Hierarchy Panel.\n\nChapter 7 - Setup the two Unity Scenes\n\nThe purpose of this Chapter is to setup the two scenes, each hosting a video to stream. You will duplicate the scene you have already created, so that you do not need to set it up again, though you will then edit the new scene, so that the GazeButton object is in a different location and has a different appearance. This is to show how to change between scenes.\n\nDo this by going to File > Save Scene as.... A save window will appear. Click the New folder button.\n\nName the folder Scenes.\n\nThe Save Scene window will still be open. Open your newly created Scenes folder.\n\nIn the File name: text field, type VideoScene1, then press Save.\n\nBack in Unity, open your Scenes folder, and left-click your VideoScene1 file. Use your keyboard, and press Ctrl + D you will duplicate that scene\n\nTip\n\nThe Duplicate command can also be performed by navigating to Edit > Duplicate.\n\nUnity will automatically increment the scene names number, but check it anyway, to ensure it matches the previously inserted code.\n\nYou should have VideoScene1 and VideoScene2.\n\nWith your two scenes, go to File > Build Settings. With the Build Settings window open, drag your scenes to the Scenes in Build section.\n\nTip\n\nYou can select both of your scenes from your Scenes folder through holding the Ctrl button, and then left-clicking each scene, and finally drag both across.\n\nClose the Build Settings window, and double click on VideoScene2.\n\nWith the second scene open, click on the GazeButton child object of the InsideOutSphere, and set its Transform as follows:\n\nTransform - Position\n\nX Y Z 0 1.3 3.6\n\nTransform - Rotation\n\nX Y Z 0 0 0\n\nTransform - Scale\n\nX Y Z 1 1 1\n\nWith the GazeButton child still selected, look at the Inspector and at the Mesh Filter. Click the little target next to the Mesh reference field:\n\nA Select Mesh popup window will appear. Double click the Cube mesh from the list of Assets.\n\nThe Mesh Filter will update, and now be a Cube. Now, click the Gear icon next to Sphere Collider and click Remove Component, to delete the collider from this object.\n\nWith the GazeButton still selected, click the Add Component button at the bottom of the Inspector. In the search field, type box, and Box Collider will be an option -- click that, to add a Box Collider to your GazeButton object.\n\nThe GazeButton is now partially updated, to look different, however, you will now create a new Material, so that it looks completely different, and is easier to recognize as a different object, than the object in the first scene.\n\nNavigate to your Materials folder, within the Project Panel. Duplicate the ButtonMaterial Material (press Ctrl + D on the keyboard, or left-click the Material, then from the Edit file menu option, select Duplicate).\n\nSelect the new ButtonMaterial Material (here named ButtonMaterial 1), and within the Inspector, click the Albedo color window. A popup will appear, where you can select another color (choose whichever you like), then close the popup. The Material will be its own instance, and different to the original.\n\nDrag the new Material onto the GazeButton child, to now completely update its look, so that it is easily distinguishable from the first scenes button.\n\nAt this point you can test the project in the Editor before building the UWP project.\n\nPress the Play button in the Editor and wear your headset.\n\nLook at the two GazeButton objects to switch between the first and second video.\n\nChapter 8 - Build the UWP Solution\n\nOnce you have ensured that the editor has no errors, you are ready to Build.\n\nTo Build:\n\nCheck the box called Unity C# Projects (this is important because it will allow you to edit the classes after build is completed).\n\nClick your new folder and then click Select Folder, so to choose that folder, to begin the build at that location.\n\nOnce Unity has finished building (it might take some time), it will open a File Explorer window at the location of your build.\n\nChapter 9 - Deploy on Local Machine\n\nOnce the build has been completed, a File Explorer window will appear at the location of your build. Open the Folder you named and built to, then double click on the solution (.sln) file within that folder, to open your solution with Visual Studio 2017.\n\nThe only thing left to do is deploy your app to your computer (or Local Machine).\n\nTo deploy to Local Machine:\n\nIn Visual Studio 2017, open the solution file that has just been created.\n\nIn the Solution Platform, select x86, Local Machine.\n\nIn the Solution Configuration select Debug.\n\nYou will now need to restore any packages to your solution. Right-click on your Solution, and click Restore NuGet Packages for Solution...\n\nNote\n\nThis is done because the packages which Unity built need to be targeted to work with your local machines references.\n\nGo to Build menu and click on Deploy Solution to sideload the application to your machine. Visual Studio will first build and then deploy your application.\n\nYour App should now appear in the list of installed apps, ready to be launched.\n\nWhen you run the Mixed Reality application, you will you be within the InsideOutSphere model which you used within your app. This sphere will be where the video will be streamed to, providing a 360-degree view, of the incoming video (which was filmed for this kind of perspective). Do not be surprised if the video takes a couple of seconds to load, your app is subject to your available Internet speed, as the video needs to be fetched and then downloaded, so to stream into your app. When you are ready, change scenes and open your second video, by gazing at the red sphere! Then feel free to go back, using the blue cube in the second scene!\n\nYour finished Azure Media Service application\n\nCongratulations, you built a mixed reality app that leverages the Azure Media Service to stream 360 videos.\n\nBonus Exercises\n\nExercise 1\n\nIt is entirely possible to only use a single scene to change videos within this tutorial. Experiment with your application and make it into a single scene! Perhaps even add another video to the mix.\n\nExercise 2\n\nExperiment with Azure and Unity, and attempt to implement the ability for the app to automatically select a video with a different file size, depending on the strength of an Internet connection."
    }
}
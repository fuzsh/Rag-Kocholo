{
    "id": "dbpedia_1178_0",
    "rank": 15,
    "data": {
        "url": "https://profiles.stanford.edu/john-ioannidis",
        "read_more_link": "",
        "language": "en",
        "title": "John P.A. Ioannidis' Profile",
        "top_image": "https://profiles.stanford.edu/images/favicon.ico;jsessionid=CC871951BA75FF5BDF580CA36C1B033C.cap-su-capappprd97?r=10.8.0",
        "meta_img": "https://profiles.stanford.edu/images/favicon.ico;jsessionid=CC871951BA75FF5BDF580CA36C1B033C.cap-su-capappprd97?r=10.8.0",
        "images": [
            "https://profiles.stanford.edu/proxy/api/cap/profiles/18745/resources/profilephoto/350x350.1509507274099.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "John P.A. Ioannidis is part of Stanford Profiles, official site for faculty, postdocs, students and staff information (Expertise, Bio, Research, Publications, and more). The site facilitates research and collaboration in academic endeavors.",
        "meta_lang": "en",
        "meta_favicon": "/images/favicon.ico;jsessionid=CC871951BA75FF5BDF580CA36C1B033C.cap-su-capappprd97?r=10.8.0",
        "meta_site_name": "",
        "canonical_link": "https://profiles.stanford.edu/john-ioannidis;jsessionid=CC871951BA75FF5BDF580CA36C1B033C.cap-su-capappprd97",
        "text": "Abstract\n\nThe COVID-19 pandemic led to relocation and reconstruction of health care resources and systems, and to a decrease in healthcare utilization, and this may have affected the treatment, diagnosis, prognosis, and psychosocial well-being of patients with cancer. We aimed to summarize and quantify the evidence on the impact of the COVID-19 pandemic on the full spectrum of cancer care. An umbrella review was undertaken to summarize and quantify the findings from systematic reviews on impact of the COVID-19 pandemic on cancer treatment modification, delays, and cancellations; delays or cancellations in screening and diagnosis; psychosocial well-being, financial distress, and use of telemedicine as well as on other aspects of cancer care. PubMed and WHO COVID-19 Database was searched for relevant systematic reviews with or without meta-analysis published before November 29th, 2022. Abstract, full text screening and data extraction were performed by two independent reviewers. AMSTAR-2 was used for critical appraisal of included systematic reviews. 51 systematic reviews evaluating different aspects of cancer care were included in our analysis. Most reviews were based on observational studies judged to be at medium and high risk of bias. Only 2 of the included reviews had high or moderate scores based on AMSTAR-2. Findings suggest treatment modifications in cancer care during the pandemic versus the pre-pandemic period were based on low level of evidence. Different degrees of delays and cancellations in cancer treatment, screening and diagnosis were observed, with low-and-middle income countries and countries that implemented lockdowns being disproportionally affected. A shift from in-person appointments to telemedicine use was observed, but utility of telemedicine, challenges in implementation and cost-effectiveness in different areas of cancer care were little explored. Evidence was consistent in suggesting psychosocial well-being (e.g., depression, anxiety, and social activities) of patients with cancer deteriorated, and cancer patients experienced financial distress, albeit results were in general not compared to pre-pandemic levels. Impact of cancer care disruption during the pandemic on cancer prognosis was little explored. In conclusion, Substantial but heterogenous impact of COVID-19 pandemic on cancer care has been observed. Evidence gaps exist on this topic, with mid- and long-term impact on cancer care being most uncertain.\n\nView details for DOI 10.7554/eLife.85679\n\nView details for PubMedID 37014058\n\nAbstract\n\nMany technology companies, including Airbnb, Amazon, Booking.com, eBay, Facebook, Google, LinkedIn, Lyft, Microsoft, Netflix, Twitter, Uber, and Yahoo!/Oath, run online randomized controlled experiments at scale, namely hundreds of concurrent controlled experiments on millions of users each, commonly referred to as A/B tests. Originally derived from the same statistical roots, randomized controlled trials (RCTs) in medicine are now criticized for being expensive and difficult, while in technology, the marginal cost of such experiments is approaching zero and the value for data-driven decision-making is broadly recognized.This is an overview of key scaling lessons learned in the technology field. They include (1) a focus on metrics, an overall evaluation criterion and thousands of metrics for insights and debugging, automatically computed for every experiment; (2) quick release cycles with automated ramp-up and shut-down that afford agile and safe experimentation, leading to consistent incremental progress over time; and (3) a culture of 'test everything' because most ideas fail and tiny changes sometimes show surprising outcomes worth millions of dollars annually. Technological advances, online interactions, and the availability of large-scale data allowed technology companies to take the science of RCTs and use them as online randomized controlled experiments at large scale with hundreds of such concurrent experiments running on any given day on a wide range of software products, be they web sites, mobile applications, or desktop applications. Rather than hindering innovation, these experiments enabled accelerated innovation with clear improvements to key metrics, including user experience and revenue. As healthcare increases interactions with patients utilizing these modern channels of web sites and digital health applications, many of the lessons apply. The most innovative technological field has recognized that systematic series of randomized trials with numerous failures of the most promising ideas leads to sustainable improvement.While there are many differences between technology and medicine, it is worth considering whether and how similar designs can be applied via simple RCTs that focus on healthcare decision-making or service delivery. Changes - small and large - should undergo continuous and repeated evaluations in randomized trials and learning from their results will enable accelerated healthcare improvements.\n\nView details for DOI 10.1186/s13063-020-4084-y\n\nView details for PubMedID 32033614\n\nAbstract\n\nCurrently, there is a growing interest in ensuring the transparency and reproducibility of the published scientific literature. According to a previous evaluation of 441 biomedical journals articles published in 2000-2014, the biomedical literature largely lacked transparency in important dimensions. Here, we surveyed a random sample of 149 biomedical articles published between 2015 and 2017 and determined the proportion reporting sources of public and/or private funding and conflicts of interests, sharing protocols and raw data, and undergoing rigorous independent replication and reproducibility checks. We also investigated what can be learned about reproducibility and transparency indicators from open access data provided on PubMed. The majority of the 149 studies disclosed some information regarding funding (103, 69.1% [95% confidence interval, 61.0% to 76.3%]) or conflicts of interest (97, 65.1% [56.8% to 72.6%]). Among the 104 articles with empirical data in which protocols or data sharing would be pertinent, 19 (18.3% [11.6% to 27.3%]) discussed publicly available data; only one (1.0% [0.1% to 6.0%]) included a link to a full study protocol. Among the 97 articles in which replication in studies with different data would be pertinent, there were five replication efforts (5.2% [1.9% to 12.2%]). Although clinical trial identification numbers and funding details were often provided on PubMed, only two of the articles without a full text article in PubMed Central that discussed publicly available data at the full text level also contained information related to data sharing on PubMed; none had a conflicts of interest statement on PubMed. Our evaluation suggests that although there have been improvements over the last few years in certain key indicators of reproducibility and transparency, opportunities exist to improve reproducible research practices across the biomedical literature and to make features related to reproducibility more readily visible in PubMed.\n\nView details for PubMedID 30457984\n\nAbstract\n\nMany published randomized clinical trials (RCTs) make claims for subgroup differences.To evaluate how often subgroup claims reported in the abstracts of RCTs are actually supported by statistical evidence (P < .05 from an interaction test) and corroborated by subsequent RCTs and meta-analyses.This meta-epidemiological survey examines data sets of trials with at least 1 subgroup claim, including Subgroup Analysis of Trials Is Rarely Easy (SATIRE) articles and Discontinuation of Randomized Trials (DISCO) articles. We used Scopus (updated July 2016) to search for English-language articles citing each of the eligible index articles with at least 1 subgroup finding in the abstract.Articles with a subgroup claim in the abstract with or without evidence of statistical heterogeneity (P < .05 from an interaction test) in the text and articles attempting to corroborate the subgroup findings.Study characteristics of trials with at least 1 subgroup claim in the abstract were recorded. Two reviewers extracted the data necessary to calculate subgroup-level effect sizes, standard errors, and the P values for interaction. For individual RCTs and meta-analyses that attempted to corroborate the subgroup findings from the index articles, trial characteristics were extracted. Cochran Q test was used to reevaluate heterogeneity with the data from all available trials.The number of subgroup claims in the abstracts of RCTs, the number of subgroup claims in the abstracts of RCTs with statistical support (subgroup findings), and the number of subgroup findings corroborated by subsequent RCTs and meta-analyses.Sixty-four eligible RCTs made a total of 117 subgroup claims in their abstracts. Of these 117 claims, only 46 (39.3%) in 33 articles had evidence of statistically significant heterogeneity from a test for interaction. In addition, out of these 46 subgroup findings, only 16 (34.8%) ensured balance between randomization groups within the subgroups (eg, through stratified randomization), 13 (28.3%) entailed a prespecified subgroup analysis, and 1 (2.2%) was adjusted for multiple testing. Only 5 (10.9%) of the 46 subgroup findings had at least 1 subsequent pure corroboration attempt by a meta-analysis or an RCT. In all 5 cases, the corroboration attempts found no evidence of a statistically significant subgroup effect. In addition, all effect sizes from meta-analyses were attenuated toward the null.A minority of subgroup claims made in the abstracts of RCTs are supported by their own data (ie, a significant interaction effect). For those that have statistical support (P < .05 from an interaction test), most fail to meet other best practices for subgroup tests, including prespecification, stratified randomization, and adjustment for multiple testing. Attempts to corroborate statistically significant subgroup differences are rare; when done, the initially observed subgroup differences are not reproduced.\n\nView details for DOI 10.1001/jamainternmed.2016.9125\n\nView details for PubMedID 28192563\n\nAbstract\n\nThe use and misuse of P values has generated extensive debates.To evaluate in large scale the P values reported in the abstracts and full text of biomedical research articles over the past 25 years and determine how frequently statistical information is presented in ways other than P values.Automated text-mining analysis was performed to extract data on P values reported in 12,821,790 MEDLINE abstracts and in 843,884 abstracts and full-text articles in PubMed Central (PMC) from 1990 to 2015. Reporting of P values in 151 English-language core clinical journals and specific article types as classified by PubMed also was evaluated. A random sample of 1000 MEDLINE abstracts was manually assessed for reporting of P values and other types of statistical information; of those abstracts reporting empirical data, 100 articles were also assessed in full text.P values reported.Text mining identified 4,572,043 P values in 1,608,736 MEDLINE abstracts and 3,438,299 P values in 385,393 PMC full-text articles. Reporting of P values in abstracts increased from 7.3% in 1990 to 15.6% in 2014. In 2014, P values were reported in 33.0% of abstracts from the 151 core clinical journals (n = 29,725 abstracts), 35.7% of meta-analyses (n = 5620), 38.9% of clinical trials (n = 4624), 54.8% of randomized controlled trials (n = 13,544), and 2.4% of reviews (n = 71,529). The distribution of reported P values in abstracts and in full text showed strong clustering at P values of .05 and of .001 or smaller. Over time, the \"best\" (most statistically significant) reported P values were modestly smaller and the \"worst\" (least statistically significant) reported P values became modestly less significant. Among the MEDLINE abstracts and PMC full-text articles with P values, 96% reported at least 1 P value of .05 or lower, with the proportion remaining steady over time in PMC full-text articles. In 1000 abstracts that were manually reviewed, 796 were from articles reporting empirical data; P values were reported in 15.7% (125/796 [95% CI, 13.2%-18.4%]) of abstracts, confidence intervals in 2.3% (18/796 [95% CI, 1.3%-3.6%]), Bayes factors in 0% (0/796 [95% CI, 0%-0.5%]), effect sizes in 13.9% (111/796 [95% CI, 11.6%-16.5%]), other information that could lead to estimation of P values in 12.4% (99/796 [95% CI, 10.2%-14.9%]), and qualitative statements about significance in 18.1% (181/1000 [95% CI, 15.8%-20.6%]); only 1.8% (14/796 [95% CI, 1.0%-2.9%]) of abstracts reported at least 1 effect size and at least 1 confidence interval. Among 99 manually extracted full-text articles with data, 55 reported P values, 4 presented confidence intervals for all reported effect sizes, none used Bayesian methods, 1 used false-discovery rates, 3 used sample size/power calculations, and 5 specified the primary outcome.In this analysis of P values reported in MEDLINE abstracts and in PMC articles from 1990-2015, more MEDLINE abstracts and articles reported P values over time, almost all abstracts and articles with P values reported statistically significant results, and, in a subgroup analysis, few articles included confidence intervals, Bayes factors, or effect sizes. Rather than reporting isolated P values, articles should include effect sizes and uncertainty metrics.\n\nView details for DOI 10.1001/jama.2016.1952\n\nView details for Web of Science ID 000372159800019\n\nAbstract\n\nTo use individual participant data meta-analysis (IPDMA) to estimate the minimal detectable change (MDC) of the Geriatric Depression Scale-15 (GDS-15) and to examine whether MDC may differ based on participant characteristics and study-level variables.This was a secondary analysis of data from an IPDMA on the depression screening accuracy of the GDS. Datasets from studies published in any language were eligible for the present study if they included GDS-15 scores for participants aged 60 or older. MDC of the GDS-15 was estimated via random-effects meta-analysis using 2.77 (MDC95) and 1.41 (MDC67) standard errors of measurement (SEM). Subgroup analyses were used to evaluate differences in MDC by participant age and sex. Meta-regression was conducted to assess for differences based on study-level variables, including mean age, proportion male, proportion with major depression, and recruitment setting.5,876 participants (mean age 76 years, 40% male, 11% with major depression) from 21 studies were included. The MDC95 was 3.81 points (95% confidence interval [CI] 3.59, 4.04), and MDC67 was 1.95 (95% CI 1.83, 2.03). The difference in MDC95 was 0.26 points (95% CI 0.04, 0.48) between ≥ 80-year-olds and < 80-year-olds; MDC95 was similar for females and males (0.05, 95% CI -0.12, 0.22). The MDC95 increased by 0.29 points (95% CI 0.17, 0.41) per 10% increase in proportion of participants with major depression; mean age had a small association (0.04 points, 95% CI 0.00 to 0.09) with MDC95, but sex and recruitment setting were not significantly associated.The MDC95 was 3.81 points and MDC67 was 1.95 points. MDC95 increased with the proportion of participants with major depression. Results can be used to evaluate individual changes in depression symptoms and as a threshold for assessing minimal clinical important difference estimates.\n\nView details for DOI 10.1016/j.jclinepi.2024.111443\n\nView details for PubMedID 38942179\n\nAbstract\n\nObservational studies are fraught with several biases including reverse causation and residual confounding. Overview of reviews of observational studies (ie, umbrella reviews) synthesise systematic reviews with or without meta-analyses of cross-sectional, case-control and cohort studies, and may also aid in the grading of the credibility of reported associations. The number of published umbrella reviews has been increasing. Recently, a reporting guideline for overviews of reviews of healthcare interventions (Preferred Reporting Items for Overviews of Reviews (PRIOR)) was published, but the field lacks reporting guidelines for umbrella reviews of observational studies. Our aim is to develop a reporting guideline for umbrella reviews on cross-sectional, case-control and cohort studies assessing epidemiological associations.We will adhere to established guidance and prepare a PRIOR extension for systematic reviews of cross-sectional, case-control and cohort studies testing epidemiological associations between an exposure and an outcome, namely Preferred Reporting Items for Umbrella Reviews of Cross-sectional, Case-control and Cohort studies (PRIUR-CCC). Step 1 will be the project launch to identify stakeholders. Step 2 will be a literature review of available guidance to conduct umbrella reviews. Step 3 will be an online Delphi study sampling 100 participants among authors and editors of umbrella reviews. Step 4 will encompass the finalisation of PRIUR-CCC statement, including a checklist, a flow diagram, explanation and elaboration document. Deliverables will be (i) identifying stakeholders to involve according to relevant expertise and end-user groups, with an equity, diversity and inclusion lens; (ii) completing a narrative review of methodological guidance on how to conduct umbrella reviews, a narrative review of methodology and reporting in published umbrella reviews and preparing an initial PRIUR-CCC checklist for Delphi study round 1; (iii) preparing a PRIUR-CCC checklist with guidance after Delphi study; (iv) publishing and disseminating PRIUR-CCC statement.PRIUR-CCC has been approved by The Ottawa Health Science Network Research Ethics Board and has obtained consent (20220639-01H). Participants to step 3 will give informed consent. PRIUR-CCC steps will be published in a peer-reviewed journal and will guide reporting of umbrella reviews on epidemiological associations.\n\nView details for DOI 10.1136/bmjopen-2022-071136\n\nView details for PubMedID 38889936\n\nAbstract\n\nIn the behavioral sciences, conducting pilot and/or feasibility studies (PFS) is a key step that provides essential information used to inform the design, conduct, and implementation of a larger-scale trial. There are more than 160 published guidelines, reporting checklists, frameworks, and recommendations related to PFS. All of these publications offer some form of guidance on PFS, but many focus on one or a few topics. This makes it difficult for researchers wanting to gain a broader understanding of all the relevant and important aspects of PFS and requires them to seek out multiple sources of information, which increases the risk of missing key considerations to incorporate into their PFS. The purpose of this study was to develop a consolidated set of considerations for the design, conduct, implementation, and reporting of PFS for interventions conducted in the behavioral sciences.To develop this consolidation, we undertook a review of the published guidance on PFS in combination with expert consensus (via a Delphi study) from the authors who wrote such guidance to inform the identified considerations. A total of 161 PFS-related guidelines, checklists, frameworks, and recommendations were identified via a review of recently published behavioral intervention PFS and backward/forward citation tracking of a well-known PFS literature (e.g., CONSORT Ext. for PFS). Authors of all 161 PFS publications were invited to complete a three-round Delphi survey, which was used to guide the creation of a consolidated list of considerations to guide the design, conduct, and reporting of PFS conducted by researchers in the behavioral sciences.A total of 496 authors were invited to take part in the three-round Delphi survey (round 1, N = 46; round 2, N = 24; round 3, N = 22). A set of twenty considerations, broadly categorized into six themes (intervention design, study design, conduct of trial, implementation of intervention, statistical analysis, and reporting) were generated from a review of the 161 PFS-related publications as well as a synthesis of feedback from the three-round Delphi process. These 20 considerations are presented alongside a supporting narrative for each consideration as well as a crosswalk of all 161 publications aligned with each consideration for further reading.We leveraged expert opinion from researchers who have published PFS-related guidelines, checklists, frameworks, and recommendations on a wide range of topics and distilled this knowledge into a valuable and universal resource for researchers conducting PFS. Researchers may use these considerations alongside the previously published literature to guide decisions about all aspects of PFS, with the hope of creating and disseminating interventions with broad public health impact.\n\nView details for DOI 10.1186/s40814-024-01485-5\n\nView details for PubMedID 38582840\n\nView details for PubMedCentralID PMC10998328\n\nAbstract\n\nAIMS: We have previously described the European Medicines Agency's (EMA) and the US Food and Drug Administration's guidelines, each for a specific psychiatric indication, on how to design pivotal drug trials used in new drug applications. Here, we report on our efforts over 3years to retrieve conflicts of interest declarations from EMA. We wanted to assess potential internal industry influence judged as the proportion of guideline committee members with industry conflicts of interest.METHODS: We submitted Freedom of Information requests in February 2020 to access EMA's lists of committee members (and their declared conflicts of interest) involved in drafting the 13 'Clinical efficacy and safety' guidelines available on EMA's website pertaining to psychiatric indications. In our request, we did not specify the exact EMA committees. Here, we describe the received documents and report the proportion of members with industry interests (i.e.defined as any financial industry relationship). It is a follow-up paper to our first report (http://doi.org/10.1017/S2045796021000147).RESULTS: After 2years and 9months (November 2022), the EMA sent us member lists and corresponding conflicts of interest declarations from the Committee for Medicinal Products for Human use (CHMP) from 2012, 2013 and 2017. These member lists pertained to 3 of the 13 requested guidelines (schizophrenia, depression and autism spectrum disorder). The 10 remaining guidelines were published before 2011 and EMA stated that they needed to require permission from their expert members (with unknown retrieval rate) and foresaw excessive workload and long wait. Therefore, we withdrew our request. The CHMPs from 2012, 2013 and 2017 had from 34 to 36 members; 39%-44% declared any interests and we judged 14%-18% as having industry interests. For the schizophrenia guideline, we identified two members with industry interests to companies who submitted feedback on the guideline. We did not receive declarations from the Central Nervous System (CNS) Working Party, the CHMP appointed expert group responsible for drafting and incorporating feedback into the guidelines.CONCLUSIONS: After almost 3years, we received information, which only partly addressed our request. We recommend EMA to improve transparency by publishing the author names and their corresponding conflicts of interest declarations directly in the 'Clinical efficacy and safety' guidelines and to not remove conflicts of interest declarations after 1year from their website to reduce the risk of stealth corporate influence during the development of these influential guidelines.\n\nView details for DOI 10.1017/S2045796024000179\n\nView details for PubMedID 38529624\n\nAbstract\n\nJust like an army of ants caught in an ant mill, individuals, groups and even whole societies are sometimes caught up in a Death Spiral, a vicious cycle of self-reinforcing dysfunctional behavior characterized by continuous flawed decision making, myopic single-minded focus on one (set of) solution(s), denial, distrust, micromanagement, dogmatic thinking and learned helplessness. We propose the term Death Spiral Effect to describe this difficult-to-break downward spiral of societal decline. Specifically, in the current theory-building review we aim to: (a) more clearly define and describe the Death Spiral Effect; (b) model the downward spiral of societal decline as well as an upward spiral; (c) describe how and why individuals, groups and even society at large might be caught up in a Death Spiral; and (d) offer a positive way forward in terms of evidence-based solutions to escape the Death Spiral Effect. Management theory hints on the occurrence of this phenomenon and offers turn-around leadership as solution. On a societal level strengthening of democracy may be important. Prior research indicates that historically, two key factors trigger this type of societal decline: rising inequalities creating an upper layer of elites and a lower layer of masses; and dwindling (access to) resources. Historical key markers of societal decline are a steep increase in inequalities, government overreach, over-integration (interdependencies in networks) and a rapidly decreasing trust in institutions and resulting collapse of legitimacy. Important issues that we aim to shed light on are the behavioral underpinnings of decline, as well as the question if and how societal decline can be reversed. We explore the extension of these theories from the company/organization level to the society level, and make use of insights from both micro-, meso-, and macro-level theories (e.g., Complex Adaptive Systems and collapsology, the study of the risks of collapse of industrial civilization) to explain this process of societal demise. Our review furthermore draws on theories such as Social Safety Theory, Conservation of Resources Theory, and management theories that describe the decline and fall of groups, companies and societies, as well as offer ways to reverse this trend.\n\nView details for DOI 10.3389/fsoc.2024.1194597\n\nView details for PubMedID 38533441\n\nView details for PubMedCentralID PMC10964949\n\nAbstract\n\nPreliminary studies (e.g., pilot/feasibility studies) can result in misleading evidence that an intervention is ready to be evaluated in a large-scale trial when it is not. Risk of Generalizability Biases (RGBs, a set of external validity biases) represent study features that influence estimates of effectiveness, often inflating estimates in preliminary studies which are not replicated in larger-scale trials. While RGBs have been empirically established in interventions targeting obesity, the extent to which RGBs generalize to other health areas is unknown. Understanding the relevance of RGBs across health behavior intervention research can inform organized efforts to reduce their prevalence.The purpose of our study was to examine whether RGBs generalize outside of obesity-related interventions.A systematic review identified health behavior interventions across four behaviors unrelated to obesity that follow a similar intervention development framework of preliminary studies informing larger-scale trials (i.e., tobacco use disorder, alcohol use disorder, interpersonal violence, and behaviors related to increased sexually transmitted infections). To be included, published interventions had to be tested in a preliminary study followed by testing in a larger trial (the two studies thus comprising a study pair). We extracted health-related outcomes and coded the presence/absence of RGBs. We used meta-regression models to estimate the impact of RGBs on the change in standardized mean difference (ΔSMD) between the preliminary study and larger trial.We identified sixty-nine study pairs, of which forty-seven were eligible for inclusion in the analysis (k = 156 effects), with RGBs identified for each behavior. For pairs where the RGB was present in the preliminary study but removed in the larger trial the treatment effect decreased by an average of ΔSMD=-0.38 (range - 0.69 to -0.21). This provides evidence of larger drop in effectiveness for studies containing RGBs relative to study pairs with no RGBs present (treatment effect decreased by an average of ΔSMD =-0.24, range - 0.19 to -0.27).RGBs may be associated with higher effect estimates across diverse areas of health intervention research. These findings suggest commonalities shared across health behavior intervention fields may facilitate introduction of RGBs within preliminary studies, rather than RGBs being isolated to a single health behavior field.\n\nView details for DOI 10.21203/rs.3.rs-3897976/v1\n\nView details for PubMedID 38464006\n\nView details for PubMedCentralID PMC10925410\n\nAbstract\n\nBACKGROUND: Selective reporting of results from only well-performing cut-offs leads to biased estimates of accuracy in primary studies of questionnaire-based screening tools and in meta-analyses that synthesize results. Individual participant data meta-analysis (IPDMA) of sensitivity and specificity at each cut-off via bivariate random-effects models (BREMs) can overcome this problem. However, IPDMA is laborious and depends on the ability to successfully obtain primary datasets, and BREMs ignore the correlation between cut-offs within primary studies.METHODS: We compared the performance of three recent multiple cut-off models developed by Steinhauser et al., Jones et al., and Hoyer and Kuss, that account for missing cut-offs when meta-analyzing diagnostic accuracy studies with multiple cut-offs, to BREMs fitted at each cut-off. We used data from 22 studies of the accuracy of the Edinburgh Postnatal Depression Scale (EPDS; 4475 participants, 758 major depression cases). We fitted each of the three multiple cut-off models and BREMs to a dataset with results from only published cut-offs from each study (published data) and an IPD dataset with results for all cut-offs (full IPD data). We estimated pooled sensitivity and specificity with 95% confidence intervals (CIs) for each cut-off and the area under the curve.RESULTS: Compared to the BREMs fitted to the full IPD data, the Steinhauser et al., Jones et al., and Hoyer and Kuss models fitted to the published data produced similar receiver operating characteristic curves; though, the Hoyer and Kuss model had lower area under the curve, mainly due to estimating slightly lower sensitivity at lower cut-offs. When fitting the three multiple cut-off models to the full IPD data, a similar pattern of results was observed. Importantly, all models had similar 95% CIs for sensitivity and specificity, and the CI width increased with cut-off levels for sensitivity and decreased with an increasing cut-off for specificity, even the BREMs which treat each cut-off separately.CONCLUSIONS: Multiple cut-off models appear to be the favorable methods when only published data are available. While collecting IPD is expensive and time consuming, IPD can facilitate subgroup analyses that cannot be conducted with published data only.\n\nView details for DOI 10.1186/s12874-023-02134-w\n\nView details for PubMedID 38302928\n\nAbstract\n\nIn the behavioral sciences, conducting pilot and/or feasibility studies (PFS) is a key step that provides essential information used to inform the design, conduct, and implementation of a larger-scale trial. There are more than 160 published guidelines, reporting checklists, frameworks, and recommendations related to PFS. All of these publications offer some form of guidance on PFS, but many focus on one or a few topics. This makes it difficult for researchers wanting to gain a broader understanding of all the relevant and important aspects of PFS and requires them to seek out multiple sources of information, which increases the risk of missing key considerations to incorporate into their PFS. The purpose of this study was to develop a consolidated set of considerations for the design, conduct, implementation, and reporting of PFS for interventions conducted in the behavioral sciences.To develop this consolidation, we undertook a review of the published guidance on PFS in combination with expert consensus (via a Delphi study) from the authors who wrote such guidance to inform the identified considerations. A total of 161 PFS-related guidelines, checklists, frameworks, and recommendations were identified via a review of recently published behavioral intervention PFS and backward/forward citation tracking of well-know PFS literature (e.g., CONSORT Ext. for PFS). Authors of all 161 PFS publications were invited to complete a three-round Delphi survey, which was used to guide the creation of a consolidated list of considerations to guide the design, conduct, and reporting of PFS conducted by researchers in the behavioral sciences.A total of 496 authors were invited to take part in the Delphi survey, 50 (10.1%) of which completed all three rounds, representing 60 (37.3%) of the 161 identified PFS-related guidelines, checklists, frameworks, and recommendations. A set of twenty considerations, broadly categorized into six themes (Intervention Design, Study Design, Conduct of Trial, Implementation of Intervention, Statistical Analysis and Reporting) were generated from a review of the 161 PFS-related publications as well as a synthesis of feedback from the three-round Delphi process. These 20 considerations are presented alongside a supporting narrative for each consideration as well as a crosswalk of all 161 publications aligned with each consideration for further reading.We leveraged expert opinion from researchers who have published PFS-related guidelines, checklists, frameworks, and recommendations on a wide range of topics and distilled this knowledge into a valuable and universal resource for researchers conducting PFS. Researchers may use these considerations alongside the previously published literature to guide decisions about all aspects of PFS, with the hope of creating and disseminating interventions with broad public health impact.\n\nView details for DOI 10.21203/rs.3.rs-3370077/v1\n\nView details for PubMedID 38168263\n\nView details for PubMedCentralID PMC10760234\n\nAbstract\n\nBACKGROUND: Despite the increasing number of biomarker studies published in the transplant literature over the past 20 years, demonstrations of their clinical benefit and their implementation in routine clinical practice are lacking. We hypothesized that suboptimal design, data, methodology and reporting might contribute to this phenomenon.METHODS: A systematic literature search was performed in PubMed, Embase, Scopus, Web of Science, and Cochrane Library between 1 January 2005 and 12 November 2022 (PROSPERO ID: CRD42020154747). All English language, original studies investigating the association between a biomarker and kidney-allograft outcome were included. The final set of publications was assessed by expert reviewers. After data collection, two independent reviewers randomly evaluated the inconsistencies for 30% of the references for each reviewer. If more than 5% of inconsistencies were observed for one given reviewer, a re-evaluation was conducted for all the references of the reviewer. The biomarkers were categorized according to their type and the biological milieu from which they were measured. The study characteristics related to the design, methods, results, and their interpretation were assessed, as well as reproducible research practices and transparency indicators.RESULTS: A total of 7372 publications were screened and 804 studies met the inclusion criteria. A total of 1143 biomarkers were assessed among the included studies from blood (n=821, 71.8%), intragraft (n=169, 14.8%), or urine (n=81, 7.1%) compartments. The number of studies significantly increased, with a median, yearly number of 31.5 studies (IQR: 23.8-35.5) between 2005 and 2012, and 57.5 (IQR: 53.3-59.8) between 2013 and 2022 (p<0.001). A total of 655 studies (81.5%) were retrospective, while 595 (74.0%) used data from a single center. The median number of patients included was 232 (IQR: 96-629) with a median follow-up posttransplant of 4.8 years (IQR: 3.0-6.2). Only 4.7% of studies were externally validated. A total of 346 studies (43.0%) did not adjust their biomarker for key prognostic factors while only 3.1% of studies adjusted the biomarker for standard-of-care patient monitoring factors. Data sharing, code sharing, and registration occurred in 8.8%, 1.1%, and 4.6% of studies, respectively. A total of 158 studies (20.0%) emphasized the clinical relevance of the biomarker despite the reported nonsignificant association of the biomarker with the outcome measure. A total of 288 studies assessed rejection as an outcome. We showed that these rejection studies shared the same characteristics as other studies.CONCLUSIONS: and Relevance Biomarker studies in kidney transplantation lack validation, rigorous design, methods and interpretation, and transparency. Higher standards in biomarker research may improve the clinical utility and clinical use.\n\nView details for DOI 10.1681/ASN.0000000000000260\n\nView details for PubMedID 38053242\n\nAbstract\n\nWhat were the frequency and temporal trends of reporting P-values and effect measures in the abstracts of reproductive medicine studies in 1990-2022, how were reported P-values distributed, and what proportion of articles that present with statistical inference reported statistically significant results, i.e. 'positive' results?Around one in six abstracts reported P-values alone without effect measures, while the prevalence of effect measures, whether reported alone or accompanied by P-values, has been increasing, especially in meta-analyses and randomized controlled trials (RCTs); the reported P-values were frequently observed around certain cut-off values, notably at 0.001, 0.01, or 0.05, and among abstracts present with statistical inference (i.e. P-value, CIs, or significant terms), a large majority (77%) reported at least one statistically significant finding.Publishing or reporting only results that show a 'positive' finding causes bias in evaluating interventions and risk factors and may incur adverse health outcomes for patients.Despite efforts to minimize publication reporting bias in medical research, it remains unclear whether the magnitude and patterns of the bias have changed over time.We studied abstracts of reproductive medicine studies from 1990 to 2022. The reproductive medicine studies were published in 23 first-quartile journals under the category of Obstetrics and Gynaecology and Reproductive Biology in Journal Citation Reports and 5 high-impact general medical journals (The Journal of the American Medical Association, The Lancet, The BMJ, The New England Journal of Medicine, and PLoS Medicine). Articles without abstracts, animal studies, and non-research articles, such as case reports or guidelines, were excluded.Automated text-mining was used to extract three types of statistical significance reporting, including P-values, CIs, and text description. Meanwhile, abstracts were text-mined for the presence of effect size metrics and Bayes factors. Five hundred abstracts were randomly selected and manually checked for the accuracy of automatic text extraction. The extracted statistical significance information was then analysed for temporal trends and distribution in general as well as in subgroups of study designs and journals.A total of 24 907 eligible reproductive medicine articles were identified from 170 739 screened articles published in 28 journals. The proportion of abstracts not reporting any statistical significance inference halved from 81% (95% CI, 76-84%) in 1990 to 40% (95% CI, 38-44%) in 2021, while reporting P-values alone remained relatively stable, at 15% (95% CI, 12-18%) in 1990 and 19% (95% CI, 16-22%) in 2021. By contrast, the proportion of abstracts reporting effect measures alone increased considerably from 4.1% (95% CI, 2.6-6.3%) in 1990 to 26% (95% CI, 23-29%) in 2021. Similarly, the proportion of abstracts reporting effect measures together with P-values showed substantial growth from 0.8% (95% CI, 0.3-2.2%) to 14% (95% CI, 12-17%) during the same timeframe. Of 30 182 statistical significance inferences, 56% (n = 17 077) conveyed statistical inferences via P-values alone, 30% (n = 8945) via text description alone such as significant or non-significant, 9.3% (n = 2820) via CIs alone, and 4.7% (n = 1340) via both CI and P-values. The reported P-values (n = 18 417), including both a continuum of P-values and dichotomized P-values, were frequently observed around common cut-off values such as 0.001 (20%), 0.05 (16%), and 0.01 (10%). Of the 13 200 reproductive medicine abstracts containing at least one statistical inference, 77% of abstracts made at least one statistically significant statement. Among articles that reported statistical inference, a decline in the proportion of making at least one statistically significant inference was only seen in RCTs, dropping from 71% (95% CI, 48-88%) in 1990 to 59% (95% CI, 42-73%) in 2021, whereas the proportion in the rest of study types remained almost constant over the years. Of abstracts that reported P-value, 87% (95% CI, 86-88%) reported at least one statistically significant P-value; it was 92% (95% CI, 82-97%) in 1990 and reached its peak at 97% (95% CI, 93-99%) in 2001 before declining to 81% (95% CI, 76-85%) in 2021.First, our analysis focused solely on reporting patterns in abstracts but not full-text papers; however, in principle, abstracts should include condensed impartial information and avoid selective reporting. Second, while we attempted to identify all types of statistical significance reporting, our text mining was not flawless. However, the manual assessment showed that inaccuracies were not frequent.There is a welcome trend that effect measures are increasingly reported in the abstracts of reproductive medicine studies, specifically in RCTs and meta-analyses. Publication reporting bias remains a major concern. Inflated estimates of interventions and risk factors could harm decisions built upon biased evidence, including clinical recommendations and planning of future research.No funding was received for this study. B.W.M. is supported by an NHMRC Investigator grant (GNT1176437); B.W.M. reports research grants and travel support from Merck and consultancy from Merch and ObsEva. W.L. is supported by an NHMRC Investigator Grant (GNT2016729). Q.F. reports receiving a PhD scholarship from Merck. The other author has no conflict of interest to declare.N/A.\n\nView details for DOI 10.1093/humrep/dead248\n\nView details for PubMedID 38015794\n\nAbstract\n\nImportance: Industry involvement is prominent in influential clinical trials, and commitments to transparency of trials are highly variable.Objective: To evaluate the modes of industry involvement and the transparency features of the most cited recent clinical trials across medicine.Design, Setting, and Participants: This cross-sectional study was a meta-research assessment including randomized and nonrandomized clinical trials published in 2019 or later. The 600 trials of any type of disease or setting that attracted highest number of citations in Scopus as of December 2022 were selected for analysis. Data were analyzed from March to September 2023.Main Outcomes and Measures: Outcomes of interest were industry involvement (sponsor, author, and analyst) and transparency (protocols, statistical analysis plans, and data and code availability).Results: Among 600 trials with a median (IQR) sample size of 415 (124-1046) participants assessed, 409 (68.2%) had industry funding and 303 (50.5%) were exclusively industry-funded. A total of 354 trials (59.0%) had industry authors, with 280 trials (46.6%) involving industry analysts and 125 trials (20.8%) analyzed exclusively by industry analysts. Among industry-funded trials, 364 (89.0%) reached conclusions favoring the sponsor. Most trials (478 trials [79.7%]) provided a data availability statement, and most indicated intention to share the data, but only 16 trials (2.7%) had data already readily available to others. More than three-quarters of trials had full protocols (482 trials [82.0%]) or statistical analysis plans (446 trials [74.3%]) available, but only 27 trials (4.5%) explicitly mentioned sharing analysis code (8 readily available; 19 on request). Randomized trials were more likely than nonrandomized studies to involve only industry analysts (107 trials [22.9%] vs 18 trials [13.6%]; P=.02) and to have full protocols (405 studies [86.5%] vs 87 studies [65.9%]; P<.001) and statistical analysis plans (373 studies [79.7%] vs 73 studies [55.3%]; P<.001) available. Almost all nonrandomized industry-funded studies (90 of 92 studies [97.8%]) favored the sponsor. Among industry-funded trials, exclusive industry funding (odds ratio, 2.9; 95% CI, 1.5-5.4) and industry-affiliated authors (odds ratio, 2.9; 95% CI, 1.5-5.6) were associated with favorable conclusions for the sponsor.Conclusions and Relevance: This cross-sectional study illustrates how industry involvement in the most influential clinical trials was prominent not only for funding, but also authorship and provision of analysts and was associated with conclusions favoring the sponsor. While most influential trials reported that they planned to share data and make both protocols and statistical analysis plans available, raw data and code were rarely readily available.\n\nView details for DOI 10.1001/jamanetworkopen.2023.43425\n\nView details for PubMedID 37962883\n\nAbstract\n\nBackground. Although preregistration can reduce researcher bias and increase transparency in primary research settings, it is less applicable to secondary data analysis. An alternative method that affords additional protection from researcher bias, which cannot be gained from conventional forms of preregistration alone, is an Explore and Confirm Analysis Workflow (ECAW). In this workflow, a data management organization initially provides access to only a subset of their dataset to researchers who request it. The researchers then prepare an analysis script based on the subset of data, upload the analysis script to a registry, and then receive access to the full dataset. ECAWs aim to achieve similar goals to preregistration, but make access to the full dataset contingent on compliance. The present survey aimed to garner information from the research community where ECAWs could be applied-employing the Avon Longitudinal Study of Parents and Children (ALSPAC) as a case example. Methods. We emailed a Web-based survey to researchers who had previously applied for access to ALSPAC's transgenerational observational dataset. Results. We received 103 responses, for a 9% response rate. The results suggest that-at least among our sample of respondents-ECAWs hold the potential to serve their intended purpose and appear relatively acceptable. For example, only 10% of respondents disagreed that ALSPAC should run a study on ECAWs (versus 55% who agreed). However, as many as 26% of respondents agreed that they would be less willing to use ALSPAC data if they were required to use an ECAW (versus 45% who disagreed). Conclusion. Our data and findings provide information for organizations and individuals interested in implementing ECAWs and related interventions. Preregistration. https://osf.io/g2fw5 Deviations from the preregistration are outlined in electronic supplementary material A.\n\nView details for DOI 10.1098/rsos.230568\n\nView details for PubMedID 37830032\n\nView details for PubMedCentralID PMC10565389\n\nAbstract\n\nTo systematically assess credibility and certainty of associations between cannabis, cannabinoids, and cannabis based medicines and human health, from observational studies and randomised controlled trials (RCTs).Umbrella review.PubMed, PsychInfo, Embase, up to 9 February 2022.Systematic reviews with meta-analyses of observational studies and RCTs that have reported on the efficacy and safety of cannabis, cannabinoids, or cannabis based medicines were included. Credibility was graded according to convincing, highly suggestive, suggestive, weak, or not significant (observational evidence), and by GRADE (Grading of Recommendations, Assessment, Development and Evaluations) (RCTs). Quality was assessed with AMSTAR 2 (A Measurement Tool to Assess Systematic Reviews 2). Sensitivity analyses were conducted.101 meta-analyses were included (observational=50, RCTs=51) (AMSTAR 2 high 33, moderate 31, low 32, or critically low 5). From RCTs supported by high to moderate certainty, cannabis based medicines increased adverse events related to the central nervous system (equivalent odds ratio 2.84 (95% confidence interval 2.16 to 3.73)), psychological effects (3.07 (1.79 to 5.26)), and vision (3.00 (1.79 to 5.03)) in people with mixed conditions (GRADE=high), improved nausea/vomit, pain, spasticity, but increased psychiatric, gastrointestinal adverse events, and somnolence among others (GRADE=moderate). Cannabidiol improved 50% reduction of seizures (0.59 (0.38 to 0.92)) and seizure events (0.59 (0.36 to 0.96)) (GRADE=high), but increased pneumonia, gastrointestinal adverse events, and somnolence (GRADE=moderate). For chronic pain, cannabis based medicines or cannabinoids reduced pain by 30% (0.59 (0.37 to 0.93), GRADE=high), across different conditions (n=7), but increased psychological distress. For epilepsy, cannabidiol increased risk of diarrhoea (2.25 (1.33 to 3.81)), had no effect on sleep disruption (GRADE=high), reduced seizures across different populations and measures (n=7), improved global impression (n=2), quality of life, and increased risk of somnolence (GRADE=moderate). In the general population, cannabis worsened positive psychotic symptoms (5.21 (3.36 to 8.01)) and total psychiatric symptoms (7.49 (5.31 to 10.42)) (GRADE=high), negative psychotic symptoms, and cognition (n=11) (GRADE=moderate). In healthy people, cannabinoids improved pain threshold (0.74 (0.59 to 0.91)), unpleasantness (0.60 (0.41 to 0.88)) (GRADE=high). For inflammatory bowel disease, cannabinoids improved quality of life (0.34 (0.22 to 0.53) (GRADE=high). For multiple sclerosis, cannabinoids improved spasticity, pain, but increased risk of dizziness, dry mouth, nausea, somnolence (GRADE=moderate). For cancer, cannabinoids improved sleep disruption, but had gastrointestinal adverse events (n=2) (GRADE=moderate). Cannabis based medicines, cannabis, and cannabinoids resulted in poor tolerability across various conditions (GRADE=moderate). Evidence was convincing from observational studies (main and sensitivity analyses) in pregnant women, small for gestational age (1.61 (1.41 to 1.83)), low birth weight (1.43 (1.27 to 1.62)); in drivers, car crash (1.27 (1.21 to 1.34)); and in the general population, psychosis (1.71 (1.47 to 2.00)). Harmful effects were noted for additional neonatal outcomes, outcomes related to car crash, outcomes in the general population including psychotic symptoms, suicide attempt, depression, and mania, and impaired cognition in healthy cannabis users (all suggestive to highly suggestive).Convincing or converging evidence supports avoidance of cannabis during adolescence and early adulthood, in people prone to or with mental health disorders, in pregnancy and before and while driving. Cannabidiol is effective in people with epilepsy. Cannabis based medicines are effective in people with multiple sclerosis, chronic pain, inflammatory bowel disease, and in palliative medicine but not without adverse events.PROSPERO CRD42018093045.None.\n\nView details for DOI 10.1136/bmj-2022-072348\n\nView details for PubMedID 37648266\n\nAbstract\n\nAmino acids in variable positions of proteins may be correlated, with potential structural and functional implications. Here, we apply exact tests of independence in R * C contingency tables to examine noise-free associations between variable positions of the SARS-CoV-2 spike protein, using as a paradigm sequences from Greece deposited in GISAID (N = 6,683/1,078 full length) for the period 29 February 2020 to 26 April 2021 that essentially covers the first three pandemic waves. We examine the fate and complexity of these associations by network analysis, using associated positions (exact P ≤ 0.001 and Average Product Correction ≥ 2) as links and the corresponding positions as nodes. We found a temporal linear increase of positional differences and a gradual expansion of the number of position associations over time, represented by a temporally evolving intricate web, resulting in a non-random complex network of 69 nodes and 252 links. Overconnected nodes corresponded to the most adapted variant positions in the population, suggesting a direct relation between network degree and position functional importance. Modular analysis revealed 25 k-cliques comprising 3 to 11 nodes. At different k-clique resolutions, one to four communities were formed, capturing epistatic associations of circulating variants (Alpha, Beta, B.1.1.318), but also Delta, which dominated the evolutionary landscape later in the pandemic. Cliques of aminoacidic positional associations tended to occur in single sequences, enabling the recognition of epistatic positions in real-world virus populations. Our findings provide a novel way of understanding epistatic relationships in viral proteins with potential applications in the design of virus control procedures. IMPORTANCE Paired positional associations of adapted amino acids in virus proteins may provide new insights for understanding virus evolution and variant formation. We investigated potential intramolecular relationships between variable SARS-CoV-2 spike positions by exact tests of independence in R * C contingency tables, having applied Average Product Correction (APC) to eliminate background noise. Associated positions (exact P ≤ 0.001 and APC ≥ 2) formed a non-random, epistatic network of 25 cliques and 1-4 communities at different clique resolutions, revealing evolutionary ties between variable positions of circulating variants and a predictive potential of previously unknown network positions. Cliques of different sizes represented theoretical combinations of changing residues in sequence space, allowing the identification of significant aminoacidic combinations in single sequences of real-world populations. Our analytic approach that links network structural aspects to mutational aminoacidic combinations in the spike sequence population offers a novel way to understand virus epidemiology and evolution.\n\nView details for DOI 10.1128/msystems.00440-23\n\nView details for PubMedID 37432011\n\nAbstract\n\nIntroduction: Reproducibility is a central tenant of research. We aimed to synthesize the literature on reproducibility and describe its epidemiological characteristics, including how reproducibility is defined and assessed. We also aimed to determine and compare estimates for reproducibility across different fields. Methods: We conducted a scoping review to identify English language replication studies published between 2018-2019 in economics, education, psychology, health sciences and biomedicine. We searched Medline, Embase, PsycINFO, Cumulative Index of Nursing and Allied Health Literature - CINAHL, Education Source via EBSCOHost, ERIC, EconPapers, International Bibliography of the Social Sciences (IBSS), and EconLit. Documents retrieved were screened in duplicate against our inclusion criteria. We extracted year of publication, number of authors, country of affiliation of the corresponding author, and whether the study was funded. For the individual replication studies, we recorded whether a registered protocol for the replication study was used, whether there was contact between the reproducing team and the original authors, what study design was used, and what the primary outcome was. Finally, we recorded how reproducibilty was defined by the authors, and whether the assessed study(ies) successfully reproduced based on this definition. Extraction was done by a single reviewer and quality controlled by a second reviewer. Results: Our search identified 11,224 unique documents, of which 47 were included in this review. Most studies were related to either psychology (48.6%) or health sciences (23.7%). Among these 47 documents, 36 described a single reproducibility study while the remaining 11 reported at least two reproducibility studies in the same paper. Less than the half of the studies referred to a registered protocol. There was variability in the definitions of reproduciblity success. In total, across the 47 documents 177 studies were reported. Based on the definition used by the author of each study, 95 of 177 (53.7%) studies reproduced. Conclusion: This study gives an overview of research across five disciplines that explicitly set out to reproduce previous research. Such reproducibility studies are extremely scarce, the definition of a successfully reproduced study is ambiguous, and the reproducibility rate is overall modest. Funding: No external funding was received for this work.\n\nView details for DOI 10.7554/eLife.78518\n\nView details for PubMedID 37341380\n\nAbstract\n\nData continue to accumulate indicating that many systematic reviews are methodologically flawed, biased, redundant, or uninformative. Some improvements have occurred in recent years based on empirical methods research and standardization of appraisal tools; however, many authors do not routinely or consistently apply these updated methods. In addition, guideline developers, peer reviewers, and journal editors often disregard current methodological standards. Although extensively acknowledged and explored in the methodological literature, most clinicians seem unaware of these issues and may automatically accept evidence syntheses (and clinical practice guidelines based on their conclusions) as trustworthy.A plethora of methods and tools are recommended for the development and evaluation of evidence syntheses. It is important to understand what these are intended to do (and cannot do) and how they can be utilized. Our objective is to distill this sprawling information into a format that is understandable and readily accessible to authors, peer reviewers, and editors. In doing so, we aim to promote appreciation and understanding of the demanding science of evidence synthesis among stakeholders. We focus on well-documented deficiencies in key components of evidence syntheses to elucidate the rationale for current standards. The constructs underlying the tools developed to assess reporting, risk of bias, and methodological quality of evidence syntheses are distinguished from those involved in determining overall certainty of a body of evidence. Another important distinction is made between those tools used by authors to develop their syntheses as opposed to those used to ultimately judge their work.Exemplar methods and research practices are described, complemented by novel pragmatic strategies to improve evidence syntheses. The latter include preferred terminology and a scheme to characterize types of research evidence. We organize best practice resources in a Concise Guide that can be widely adopted and adapted for routine implementation by authors and journals. Appropriate, informed use of these is encouraged, but we caution against their superficial application and emphasize their endorsement does not substitute for in-depth methodological training. By highlighting best practices with their rationale, we hope this guidance will inspire further evolution of methods and tools that can advance the field.\n\nView details for DOI 10.1186/s13643-023-02255-9\n\nView details for PubMedID 37291658\n\nView details for PubMedCentralID PMC10248995\n\nAbstract\n\nData continue to accumulate indicating that many systematic reviews are methodologically flawed, biased, redundant, or uninformative. Some improvements have occurred in recent years based on empirical methods research and standardization of appraisal tools; however, many authors do not routinely or consistently apply these updated methods. In addition, guideline developers, peer reviewers, and journal editors often disregard current methodological standards. Although extensively acknowledged and explored in the methodological literature, most clinicians seem unaware of these issues and may automatically accept evidence syntheses (and clinical practice guidelines based on their conclusions) as trustworthy.A plethora of methods and tools are recommended for the development and evaluation of evidence syntheses. It is important to understand what these are intended to do (and cannot do) and how they can be utilized. Our objective is to distill this sprawling information into a format that is understandable and readily accessible to authors, peer reviewers, and editors. In doing so, we aim to promote appreciation and understanding of the demanding science of evidence synthesis among stakeholders. We focus on well-documented deficiencies in key components of evidence syntheses to elucidate the rationale for current standards. The constructs underlying the tools developed to assess reporting, risk of bias, and methodological quality of evidence syntheses are distinguished from those involved in determining overall certainty of a body of evidence. Another important distinction is made between those tools used by authors to develop their syntheses as opposed to those used to ultimately judge their work.Exemplar methods and research practices are described, complemented by novel pragmatic strategies to improve evidence syntheses. The latter include preferred terminology and a scheme to characterize types of research evidence. We organize best practice resources in a Concise Guide that can be widely adopted and adapted for routine implementation by authors and journals. Appropriate, informed use of these is encouraged, but we caution against their superficial application and emphasize their endorsement does not substitute for in-depth methodological training. By highlighting best practices with their rationale, we hope this guidance will inspire further evolution of methods and tools that can advance the field.\n\nView details for DOI 10.1186/s12879-023-08304-x\n\nView details for PubMedID 37286949\n\nView details for PubMedCentralID PMC10247272\n\nAbstract\n\nData continue to accumulate indicating that many systematic reviews are methodologically flawed, biased, redundant, or uninformative. Some improvements have occurred in recent years based on empirical methods research and standardization of appraisal tools; however, many authors do not routinely or consistently apply these updated methods. In addition, guideline developers, peer reviewers, and journal editors often disregard current methodological standards. Although extensively acknowledged and explored in the methodological literature, most clinicians seem unaware of these issues and may automatically accept evidence syntheses (and clinical practice guidelines based on their conclusions) as trustworthy. A plethora of methods and tools are recommended for the development and evaluation of evidence syntheses. It is important to understand what these are intended to do (and cannot do) and how they can be utilized. Our objective is to distill this sprawling information into a format that is understandable and readily accessible to authors, peer reviewers, and editors. In doing so, we aim to promote appreciation and understanding of the demanding science of evidence synthesis among stakeholders. We focus on well-documented deficiencies in key components of evidence syntheses to elucidate the rationale for current standards. The constructs underlying the tools developed to assess reporting, risk of bias, and methodological quality of evidence syntheses are distinguished from those involved in determining overall certainty of a body of evidence. Another important distinction is made between those tools used by authors to develop their syntheses as opposed to those used to ultimately judge their work. Exemplar methods and research practices are described, complemented by novel pragmatic strategies to improve evidence syntheses. The latter include preferred terminology and a scheme to characterize types of research evidence. We organize best practice resources in a Concise Guide that can be widely adopted and adapted for routine implementation by authors and journals. Appropriate, informed use of these is encouraged, but we caution against their superficial application and emphasize their endorsement does not substitute for in-depth methodological training. By highlighting best practices with their rationale, we hope this guidance will inspire further evolution of methods and tools that can advance the field.\n\nView details for DOI 10.1111/aas.14295\n\nView details for PubMedID 37288997\n\nAbstract\n\nData continue to accumulate indicating that many systematic reviews are methodologically flawed, biased, redundant, or uninformative. Some improvements have occurred in recent years based on empirical methods research and standardization of appraisal tools; however, many authors do not routinely or consistently apply these updated methods. In addition, guideline developers, peer reviewers, and journal editors often disregard current methodological standards. Although extensively acknowledged and explored in the methodological literature, most clinicians seem unaware of these issues and may automatically accept evidence syntheses (and clinical practice guidelines based on their conclusions) as trustworthy. A plethora of methods and tools are recommended for the development and evaluation of evidence syntheses. It is important to understand what these are intended to do (and cannot do) and how they can be utilized. Our objective is to distill this sprawling information into a format that is understandable and readily accessible to authors, peer reviewers, and editors. In doing so, we aim to promote appreciation and understanding of the demanding science of evidence synthesis among stakeholders. We focus on well-documented deficiencies in key components of evidence syntheses to elucidate the rationale for current standards. The constructs underlying the tools developed to assess reporting, risk of bias, and methodological quality of evidence syntheses are distinguished from those involved in determining overall certainty of a body of evidence. Another important distinction is made between those tools used by authors to develop their syntheses as opposed to those used to ultimately judge their work. Exemplar methods and research practices are described, complemented by novel pragmatic strategies to improve evidence syntheses. The latter include preferred terminology and a scheme to characterize types of research evidence. We organize best practice resources in a Concise Guide that can be widely adopted and adapted for routine implementation by authors and journals. Appropriate, informed use of these is encouraged, but we caution against their superficial application and emphasize their endorsement does not substitute for in-depth methodological training. By highlighting best practices with their rationale, we hope this guidance will inspire further evolution of methods and tools that can advance the field.\n\nView details for DOI 10.11124/JBIES-23-00139\n\nView details for PubMedID 37282594\n\nAbstract\n\nData continue to accumulate indicating that many systematic reviews are methodologically flawed, biased, redundant, or uninformative. Some improvements have occurred in recent years based on empirical methods research and standardization of appraisal tools; however, many authors do not routinely or consistently apply these updated methods. In addition, guideline developers, peer reviewers, and journal editors often disregard current methodological standards. Although extensively acknowledged and explored in the methodological literature, most clinicians seem unaware of these issues and may automatically accept evidence syntheses (and clinical practice guidelines based on their conclusions) as trustworthy. A plethora of methods and tools are recommended for the development and evaluation of evidence syntheses. It is important to understand what these are intended to do (and cannot do) and how they can be utilized. Our objective is to distill this sprawling information into a format that is understandable and readily accessible to authors, peer reviewers, and editors. In doing so, we aim to promote appreciation and understanding of the demanding science of evidence synthesis among stakeholders. We focus on well-documented deficiencies in key components of evidence syntheses to elucidate the rationale for current standards. The constructs underlying the tools developed to assess reporting, risk of bias, and methodological quality of evidence syntheses are distinguished from those involved in determining overall certainty of a body of evidence. Another important distinction is made between those tools used by authors to develop their syntheses as opposed to those used to ultimately judge their work. Exemplar methods and research practices are described, complemented by novel pragmatic strategies to improve evidence syntheses. The latter include preferred terminology and a scheme to characterize types of research evidence. We organize best practice resources in a Concise Guide that can be widely adopted and adapted for routine implementation by authors and journals. Appropriate, informed use of these is encouraged, but we caution against their superficial application and emphasize their endorsement does not substitute for in-depth methodological training. By highlighting best practices with their rationale, we hope this guidance will inspire further evolution of methods and tools that can advance the field.\n\nView details for DOI 10.1111/bph.16100\n\nView details for PubMedID 37282770\n\nAbstract\n\n» Data continue to accumulate indicating that many systematic reviews are methodologically flawed, biased, redundant, or uninformative. Some improvements have occurred in recent years based on empirical methods research and standardization of appraisal tools; however, many authors do not routinely or consistently apply these updated methods. In addition, guideline developers, peer reviewers, and journal editors often disregard current methodological standards. Although extensively acknowledged and explored in the methodological literature, most clinicians seem unaware of these issues and may automatically accept evidence syntheses (and clinical practice guidelines based on their conclusions) as trustworthy.» A plethora of methods and tools are recommended for the development and evaluation of evidence syntheses. It is important to understand what these are intended to do (and cannot do) and how they can be utilized. Our objective is to distill this sprawling information into a format that is understandable and readily accessible to authors, peer reviewers, and editors. In doing so, we aim to promote appreciation and understanding of the demanding science of evidence synthesis among stakeholders. We focus on well-documented deficiencies in key components of evidence syntheses to elucidate the rationale for current standards. The constructs underlying the tools developed to assess reporting, risk of bias, and methodological quality of evidence syntheses are distinguished from those involved in determining overall certainty of a body of evidence. Another important distinction is made between those tools used by authors to develop their syntheses as opposed to those used to ultimately judge their work.» Exemplar methods and research practices are described, complemented by novel pragmatic strategies to improve evidence syntheses. The latter include preferred terminology and a scheme to characterize types of research evidence. We organize best practice resources in a Concise Guide that can be widely adopted and adapted for routine implementation by authors and journals. Appropriate, informed use of these is encouraged, but we caution against their superficial application and emphasize their endorsement does not substitute for in-depth methodological training. By highlighting best practices with their rationale, we hope this guidance will inspire further evolution of methods and tools that can advance the field.\n\nView details for DOI 10.2106/JBJS.RVW.23.00077\n\nView details for PubMedID 37285444\n\nAbstract\n\nBACKGROUND: Behavioral interventions are often complex, operate at multiple levels, across settings, and employ a range of behavior change techniques. Collecting and reporting key indicators of initial trial and intervention feasibility is essential to decisions for progressing to larger-scale trials. The extent of reporting on feasibility indicators and how this may have changed over time is unknown. The aims of this study were to (1) conduct a historical scoping review of the reporting of feasibility indicators in behavioral pilot/feasibility studies related to obesity published through 2020, and (2) describe trends in the amount and type of feasibility indicators reported in studies published across three time periods: 1982-2006, 2011-2013, and 2018-2020.METHODS: A search of online databases (PubMed, Embase, EBSCOhost, Web of Science) for health behavior pilot/feasibility studies related to obesity published up to 12/31/2020 was conducted and a random sample of 600 studies, 200 from each of the three timepoints (1982-2006, 2011-2013, and 2018-2020), was included in this review. The presence/absence of feasibility indicators, including recruitment, retention, participant acceptability, attendance, compliance, and fidelity, were identified/coded for each study. Univariate logistic regression models were employed to assess changes in the reporting of feasibility indicators across time.RESULTS: A total of 16,365 unique articles were identified of which 6873 of these were reviewed to arrive at the final sample of 600 studies. For the total sample, 428 (71.3%) studies provided recruitment information, 595 (99.2%) provided retention information, 219 (36.5%) reported quantitative acceptability outcomes, 157 (26.2%) reported qualitative acceptability outcomes, 199 (33.2%) reported attendance, 187 (31.2%) reported participant compliance, 23 (3.8%) reported cost information, and 85 (14.2%) reported treatment fidelity outcomes. When compared to the Early Group (1982-2006), studies in the Late Group (2018-2020) were more likely to report recruitment information (OR=1.60, 95%CI 1.03-2.49), acceptability-related quantitative (OR=2.68, 95%CI 1.76-4.08) and qualitative (OR=2.32, 95%CI 1.48-3.65) outcomes, compliance outcomes (OR=2.29, 95%CI 1.49-3.52), and fidelity outcomes (OR=2.13, 95%CI 1.21, 3.77).CONCLUSION: The reporting of feasibility indicators within behavioral pilot/feasibility studies has improved across time, but key aspects of feasibility, such as fidelity, are still not reported in the majority of studies. Given the importance of behavioral intervention pilot/feasibility studies in the translational science spectrum, there is a need for improving the reporting of feasibility indicators.\n\nView details for DOI 10.1186/s40814-023-01270-w\n\nView details for PubMedID 36949541\n\nAbstract\n\nItem 10 of the Edinburgh Postnatal Depression Scale (EPDS) is intended to assess thoughts of intentional self-harm but may also elicit concerns about accidental self-harm. It does not specifically address suicide ideation but, nonetheless, is sometimes used as an indicator of suicidality. The 9-item version of the EPDS (EPDS-9), which omits item 10, is sometimes used in research due to concern about positive endorsements of item 10 and necessary follow-up. We assessed the equivalence of total score correlations and screening accuracy to detect major depression using the EPDS-9 versus full EPDS among pregnant and postpartum women. We searched Medline, Medline In-Process and Other Non-Indexed Citations, PsycINFO, and Web of Science from database inception to October 3, 2018 for studies that administered the EPDS and conducted diagnostic classification for major depression based on a validated semi-structured or fully structured interview among women aged 18 or older during pregnancy or within 12months of giving birth. We conducted an individual participant data meta-analysis. We calculated Pearson correlations with 95% prediction interval (PI) between EPDS-9 and full EPDS total scores using a random effects model. Bivariate random-effects models were fitted to assess screening accuracy. Equivalence tests were done by comparing the confidence intervals (CIs) around the pooled sensitivity and specificity differences to the equivalence margin of delta=0.05. Individual participant data were obtained from 41 eligible studies (10,906 participants, 1407 major depression cases). The correlation between EPDS-9 and full EPDS scores was 0.998 (95% PI 0.991, 0.999). For sensitivity, the EPDS-9 and full EPDS were equivalent for cut-offs 7-12 (difference range -0.02, 0.01) and the equivalence was indeterminate for cut-offs 13-15 (all differences -0.04). For specificity, the EPDS-9 and full EPDS were equivalent for all cut-offs (difference range 0.00, 0.01). The EPDS-9 performs similarly to the full EPDS and can be used when there are concerns about the implications of administering EPDS item 10.Trial registration: The original IPDMA was registered in PROSPERO (CRD42015024785).\n\nView details for DOI 10.1038/s41598-023-29114-w\n\nView details for PubMedID 36899016\n\nAbstract\n\nBACKGROUND: Pilot/feasibility or studies with small sample sizes may be associated with inflated effects. This study explores the vibration of effect sizes (VoE) in meta-analyses when considering different inclusion criteria based upon sample size or pilot/feasibility status.METHODS: Searches were to identify systematic reviews that conducted meta-analyses of behavioral interventions on topics related to the prevention/treatment of childhood obesity from January 2016 to October 2019. The computed summary effect sizes (ES) were extracted from each meta-analysis. Individual studies included in the meta-analyses were classified into one of the following four categories: self-identified pilot/feasibility studies or based upon sample size but not a pilot/feasibility study (N≤100, N>100, and N>370 the upper 75th of sample size). The VoE was defined as the absolute difference (ABS) between the re-estimations of summary ES restricted to study classifications compared to the originally reported summary ES. Concordance (kappa) of statistical significance of summary ES between the four categories of studies was assessed. Fixed and random effects models and meta-regressions were estimated. Three case studies are presented to illustrate the impact of including pilot/feasibility and N≤100 studies on the estimated summary ES.RESULTS: A total of 1602 effect sizes, representing 145 reported summary ES, were extracted from 48 meta-analyses containing 603 unique studies (avg. 22 studies per meta-analysis, range 2-108) and included 227,217 participants. Pilot/feasibility and N≤100 studies comprised 22% (0-58%) and 21% (0-83%) of studies included in the meta-analyses. Meta-regression indicated the ABS between the re-estimated and original summary ES where summary ES ranged from 0.20 to 0.46 depending on the proportion of studies comprising the original ES were either mostly small (e.g., N≤100) or mostly large (N>370). Concordance was low when removing both pilot/feasibility and N≤100 studies (kappa=0.53) and restricting analyses only to the largest studies (N>370, kappa=0.35), with 20% and 26% of the originally reported statistically significant ES rendered non-significant. Reanalysis of the three case study meta-analyses resulted in the re-estimated ES rendered either non-significant or half of the originally reported ES.CONCLUSIONS: When meta-analyses of behavioral interventions include a substantial proportion of both pilot/feasibility and N≤100 studies, summary ES can be affected markedly and should be interpreted with caution.\n\nView details for DOI 10.1186/s13643-023-02184-7\n\nView details for PubMedID 36803891\n\nAbstract\n\nDuring the COVID-19 pandemic, children and adolescents were massively infected worldwide. In 2022, reinfections became a main feature of the endemic phase of SARS-CoV-2, so it is important to understand the epidemiology and clinical impact of reinfections.To assess the incidence, risk, and severity of pediatric SARS-CoV-2 reinfection.This retrospective cohort study used epidemiologic data of documented SARS-CoV-2 infections from the surveillance database of the Institute for Public Health of Vojvodina. A total of 32 524 children and adolescents from Vojvodina, Serbia, with laboratory-confirmed SARS-CoV-2 infection between March 6, 2020, and April 30, 2022, were followed up for reinfection until July 31, 2022.Incidence rates of documented SARS-CoV-2 reinfection per 1000 person-months, estimated risk of documented reinfection 90 days or more after laboratory confirmation of primary infection, reinfection severity, hospitalizations, and deaths.The study cohort included 32 524 children and adolescents with COVID-19 (mean [SD] age, 11.2 [4.9] years; 15 953 [49.1%] male), including 964 children (3.0%) who experienced documented reinfection. The incidence rate of documented reinfections was 3.2 (95% CI, 3.0-3.4) cases per 1000 person-months and was highest in adolescents aged 12 to 17 years (3.4; 95% CI, 3.2-3.7). Most reinfections (905 [93.9%]) were recorded in 2022. The cumulative reinfection risk was 1.3% at 6 months, 1.9% at 9 months, 4.0% at 12 months, 6.7% at 15 months, 7.2% at 18 months, and 7.9% after 21 months. Pediatric COVID-19 cases were generally mild. The proportion of severe clinical forms decreased from 14 (1.4%) in initial episodes to 3 (0.3%) in reinfections. Reinfected children were approximately 5 times less likely to have severe disease during reinfection compared with initial infection (McNemar odds ratio, 0.2; 95% CI, 0.0-0.8). Pediatric reinfections rarely led to hospitalization (0.5% vs 1.3% during primary infections), and none resulted in death.This cohort study found that the SARS-CoV-2 reinfection risk remained substantially lower for children and adolescents compared with adults as of July 2022. Pediatric infections were mild, and reinfections were even milder than primary infections.\n\nView details for DOI 10.1001/jamanetworkopen.2022.55779\n\nView details for PubMedID 36780157\n\nAbstract\n\nData continue to accumulate indicating that many systematic reviews are methodologically flawed, biased, redundant, or uninformative. Some improvements have occurred in recent years based on empirical methods research and standardization of appraisal tools; however, many authors do not routinely or consistently apply these updated methods. In addition, guideline developers, peer reviewers, and journal editors often disregard current methodological standards. Although extensively acknowledged and explored in the methodological literature, most clinicians seem unaware of these issues and may automatically accept evidence syntheses (and clinical practice guidelines based on their conclusions) as trustworthy.A plethora of methods and tools are recommended for the development and evaluation of evidence syntheses. It is important to understand what these are intended to do (and cannot do) and how they can be utilized. Our objective is to distill this sprawling information into a format that is understandable and readily accessible to authors, peer reviewers, and editors. In doing so, we aim to promote appreciation and understanding of the demanding science of evidence synthesis among stakeholders. We focus on well-documented deficiencies in key components of evidence syntheses to elucidate the rationale for current standards. The constructs underlying the tools developed to assess reporting, risk of bias, and methodological quality of evidence syntheses are distinguished from those involved in determining overall certainty of a body of evidence. Another important distinction is made between those tools used by authors to develop their syntheses as opposed to those used to ultimately judge their work.Exemplar methods and research practices are described, complemented by novel pragmatic strategies to improve evidence syntheses. The latter include preferred terminology and a scheme to characterize types of research evidence. We organize best practice resources in a Concise Guide that can be widely adopted and adapted for routine implementation by authors and journals. Appropriate, informed use of these is encouraged, but we caution against their superficial application and emphasize their endorsement does not substitute for in-depth methodological training. By highlighting best practices with their rationale, we hope this guidance will inspire further evolution of methods and tools that can advance the field.\n\nView details for DOI 10.3233/PRM-230019\n\nView details for PubMedID 37302044\n\nAbstract\n\nBoth citation and funding metrics converge in shaping current perceptions of academic success.To evaluate what proportion of the most-cited US-based scientists are funded by biomedical federal agencies and whether funded scientists are more cited than nonfunded ones.This survey study used linkage of a Scopus-based database on top-cited US researchers (according to a composite citation metric) and the National Institutes of Health RePORTER database of federal funding (33 biomedical federal agencies). Matching was based on name and institution. US-based top-cited scientists who were allocated to any of 69 scientific subfields highly related to biomedicine were considered in the main analysis. Data were downloaded on June 11, 2022.Proportion of US-based top-cited biomedical scientists who had any (1996-2022), recent (2015-2022), and current (2021-2022) funding. Comparisons of funded and nonfunded scientists assessed total citations and a composite citation index.There were 204 603 records in RePORTER (1996-2022) and 75 316 US-based top-cited scientists in the career-long citation database; 40 887 scientists were included in the main analysis. The proportion of US-based top-cited biomedical scientists (according to career-long citation impact) who had received any federal funding from biomedical research agencies was 62.7% (25 650 of 40 887) for any funding (1996-2022), 23.1% (9427 of 40 887) for recent funding (2015-2022), and 14.1% (5778 of 40 887) for current funding (2021-2022). Respective proportions were 64.8%, 31.4%, and 20.9%, for top-cited scientists according to recent single-year citation impact. There was large variability across scientific subfields (eg, current funding: 31% of career-long impact top-cited scientists in geriatrics, 30% in bioinformatics and 29% in developmental biology, but 0% in legal and forensic medicine, general psychology and cognitive sciences, and gender studies). Funded top-cited researchers were overall more cited than nonfunded top-cited scientists (median [IQR], 9594 [5650-1703] vs 5352 [3057-9890] citations; P < .001) and substantial difference remained after adjusting for subfield and years since first publication. Differences were more prominent in some specific biomedical subfields.In this survey study, biomedical federal funding had offered support to approximately two-thirds of the top-cited biomedical scientists at some point during the last quarter century, but only a small minority of top-cited scientists had current federal biomedical funding. The large unevenness across subfields needs to be addressed with ways that improve equity, efficiency, excellence, and translational potential.\n\nView details for DOI 10.1001/jamanetworkopen.2022.45590\n\nView details for PubMedID 36477476\n\nAbstract\n\nThe largest burden of COVID-19 is carried by the elderly, and persons living in nursing homes are particularly vulnerable. However, 94% of the global population is younger than 70 years and 86% is younger than 60 years. The objective of this study was to accurately estimate the infection fatality rate (IFR) of COVID-19 among non-elderly people in the absence of vaccination or prior infection. In systematic searches in SeroTracker and PubMed (protocol: https://osf.io/xvupr), we identified 40 eligible national seroprevalence studies covering 38 countries with pre-vaccination seroprevalence data. For 29 countries (24 high-income, 5 others), publicly available age-stratified COVID-19 death data and age-stratified seroprevalence information were available and were included in the primary analysis. The IFRs had a median of 0.034% (interquartile range (IQR) 0.013-0.056%) for the 0-59 years old population, and 0.095% (IQR 0.036-0.119%) for the 0-69 years old. The median IFR was 0.0003% at 0-19 years, 0.002% at 20-29 years, 0.011% at 30-39 years, 0.035% at 40-49 years, 0.123% at 50-59 years, and 0.506% at 60-69 years. IFR increases approximately 4 times every 10 years. Including data from another 9 countries with imputed age distribution of COVID-19 deaths yielded median IFR of 0.025-0.032% for 0-59 years and 0.063-0.082% for 0-69 years. Meta-regression analyses also suggested global IFR of 0.03% and 0.07%, respectively in these age groups. The current analysis suggests a much lower pre-vaccination IFR in non-elderly populations than previously suggested. Large differences did exist between countries and may reflect differences in comorbidities and other factors. These estimates provide a baseline from which to fathom further IFR declines with the widespread use of vaccination, prior infections, and evolution of new variants.\n\nView details for DOI 10.1016/j.envres.2022.114655\n\nView details for PubMedID 36341800\n\nAbstract\n\nSARS-CoV-2 Omicron variants contain many mutations in its spike receptor-binding domain, the target of all authorized monoclonal antibodies (MAbs). Determining the extent to which Omicron variants reduced MAb susceptibility is critical to preventing and treating COVID-19. We systematically reviewed PubMed and three preprint servers, last updated 11 April 2022, for the in vitro activity of authorized MAbs against the Omicron variants. Fifty-one studies were eligible, including 50 containing Omicron BA.1 susceptibility data and 17 containing Omicron BA.2 susceptibility data. The first two authorized MAb combinations, bamlanivimab/etesevimab and casirivimab/imdevimab, were largely inactive against the Omicron BA.1 and BA.2 variants. In 34 studies, sotrovimab displayed a median 4.0-fold (interquartile range [IQR]: 2.6 to 6.9) reduction in activity against Omicron BA.1, and in 12 studies, it displayed a median 17-fold (IQR: 13 to 30) reduction in activity against Omicron BA.2. In 15 studies, the combination cilgavimab/tixagevimab displayed a median 86-fold (IQR: 27 to 151) reduction in activity against Omicron BA.1, and in six studies, it displayed a median 5.4-fold (IQR: 3.7 to 6.9) reduction in activity against Omicron BA.2. In eight studies against Omicron BA.1 and six studies against Omicron BA.2, bebtelovimab displayed no reduction in activity. Disparate results between assays were common. For authorized MAbs, 51/268 (19.0%) results for wild-type control variants and 78/348 (22.4%) results for Omicron BA.1 and BA.2 variants were more than 4-fold below or 4-fold above the median result for that MAb. Highly disparate results between published assays indicate a need for improved MAb susceptibility test standardization or interassay calibration. IMPORTANCE Monoclonal antibodies (MAbs) targeting the SARS-CoV-2 spike protein are among the most effective measures for preventing and treating COVID-19. However, SARS-CoV-2 Omicron variants contain many mutations in their spike receptor-binding domains, the target of all authorized MAbs. Therefore, determining the extent to which Omicron variants reduced MAb susceptibility is critical to preventing and treating COVID-19. We identified 51 studies that reported the in vitro susceptibility of the two main Omicron variants BA.1 and BA.2 to therapeutic MAbs in advanced clinical development, including eight authorized individual MAbs and three authorized MAb combinations. We estimated the degree to which different MAbs displayed reduced activity against Omicron variants. The marked loss of activity of many MAbs against Omicron variants underscores the importance of developing MAbs that target conserved regions of spike. Highly disparate results between assays indicate the need for improved MAb susceptibility test standardization.\n\nView details for DOI 10.1128/spectrum.00926-22\n\nView details for PubMedID 35700134\n\nAbstract\n\nMental disorders represent a worldwide public health concern. Psychotherapies and pharmacotherapies are recommended as first line treatments. However, evidence has emerged that their efficacy may be overestimated, due to a variety of shortcomings in clinical trials (e.g., publication bias, weak control conditions such as waiting list). We performed an umbrella review of recent meta-analyses of randomized controlled trials (RCTs) of psychotherapies and pharmacotherapies for the main mental disorders in adults. We selected meta-analyses that formally assessed risk of bias or quality of studies, excluded weak comparators, and used effect sizes for target symptoms as primary outcome. We searched PubMed and PsycINFO and individual records of the Cochrane Library for meta-analyses published between January 2014 and March 2021 comparing psychotherapies or pharmacotherapies with placebo or treatment-as-usual (TAU), or psychotherapies vs. pharmacotherapies head-to-head, or the combination of psychotherapy with pharmacotherapy to either monotherapy. One hundred and two meta-analyses, encompassing 3,782 RCTs and 650,514 patients, were included, covering depressive disorders, anxiety disorders, post-traumatic stress disorder, obsessive-compulsive disorder, somatoform disorders, eating disorders, attention-deficit/hyperactivity disorder, substance use disorders, insomnia, schizophrenia spectrum disorders, and bipolar disorder. Across disorders and treatments, the majority of effect sizes for target symptoms were small. A random effect meta-analytic evaluation of the effect sizes reported by the largest meta-analyses per disorder yielded a standardized mean difference (SMD) of 0.34 (95% CI: 0.26-0.42) for psychotherapies and 0.36 (95% CI: 0.32-0.41) for pharmacotherapies compared with placebo or TAU. The SMD for head-to-head comparisons of psychotherapies vs. pharmacotherapies was 0.11 (95% CI: -0.05 to 0.26). The SMD for the combined treatment compared with either monotherapy was 0.31 (95% CI: 0.19-0.44). Risk of bias was often high. After more than half a century of research, thousands of RCTs and millions of invested funds, the effect sizes of psychotherapies and pharmacotherapies for mental disorders are limited, suggesting a ceiling effect for treatment research as presently conducted. A paradigm shift in research seems to be required to achieve further progress.\n\nView details for DOI 10.1002/wps.20941\n\nView details for PubMedID 35015359\n\nAbstract\n\nObjectives: To summarise the range, strength, and validity of reported associations between environmental risk factors and non-Hodgkin's lymphoma, and to evaluate the concordance between associations reported in meta-analyses of summary level data and meta-analyses of individual participant data.Design: Umbrella review and comparison of meta-analyses of summary and individual participant level data.Data sources: Medline, Embase, Scopus, Web of Science Core Collection, Cochrane Library, and Epistemonikos, from inception to 23 July 2021.Eligibility criteria for selecting studies: English language meta-analyses of summary level data and of individual participant data evaluating associations between environmental risk factors and incident non-Hodgkin's lymphoma (overall and subtypes).Data extraction and synthesis: Summary effect estimates from meta-analyses of summary level data comparing ever versus never exposure that were adjusted for the largest number of potential confounders were re-estimated using a random effects model and classified as presenting evidence that was non-significant, weak (P<0.05), suggestive (P<0.001 and >1000 cases), highly suggestive (P<0.000001, >1000 cases, largest study reporting a significant association), or convincing (P<0.000001, >1000 cases, largest study reporting a significant association, I2 <50%, 95% prediction interval excluding the null value, and no evidence of small study effects and excess significance bias) evidence. When the same exposures, exposure contrast levels, and outcomes were evaluated in meta-analyses of summary level data and meta-analyses of individual participant data from the International Lymphoma Epidemiology (InterLymph) Consortium, concordance in terms of direction, level of significance, and overlap of 95% confidence intervals was examined. Methodological quality of the meta-analyses of summary level data was assessed by the AMSTAR 2 tool.Results: We identified 85 meta-analyses of summary level data reporting 257 associations for 134 unique environmental risk factors and 10 subtypes of non-Hodgkin's lymphoma nearly all (79, 93%) were classified as having critically low quality. Most associations (225, 88%) presented either non-significant or weak evidence. The 11 (4%) associations presenting highly suggestive evidence were primarily for autoimmune or infectious disease related risk factors. Only one association, between history of coeliac disease and risk of non-Hodgkin's lymphoma, presented convincing evidence. Of 40 associations reported in meta-analyses of summary level data that were also evaluated in InterLymph meta-analyses of individual participant data, 22 (55%) pairs were in the same direction, had the same level of statistical significance, and had overlapping 95% confidence intervals; 28 (70%) pairs had summary effect sizes from the meta-analyses of individual participant data that were more conservative.Conclusion: This umbrella review suggests evidence of many meta-analyses of summary level data reporting weak associations between environmental risk factors and non-Hodgkin's lymphoma. Improvements to primary studies as well as evidence synthesis in evaluations of evironmental risk factors and non-Hodgkin's lymphoma are needed.Review registration number: PROSPERO CRD42020178010.\n\nView details for DOI 10.1136/bmjmed-2022-000184\n\nView details for PubMedID 36936582\n\nAbstract\n\nA series of aggressive restrictive measures were adopted around the world in 2020-2022 to attempt to prevent SARS-CoV-2 from spreading. However, it has become increasingly clear the most aggressive (lockdown) response strategies may involve negative side-effects such as a steep increase in poverty, hunger, and inequalities. Several economic, educational, and health repercussions have fallen disproportionately on children, students, young workers, and especially on groups with pre-existing inequalities such as low-income families, ethnic minorities, and women. This has led to a vicious cycle of rising inequalities and health issues. For example, educational and financial security decreased along with rising unemployment and loss of life purpose. Domestic violence surged due to dysfunctional families being forced to spend more time with each other. In the current narrative and scoping review, we describe macro-dynamics that are taking place because of aggressive public health policies and psychological tactics to influence public behavior, such as mass formation and crowd behavior. Coupled with the effect of inequalities, we describe how these factors can interact toward aggravating ripple effects. In light of evidence regarding the health, economic and social "
    }
}
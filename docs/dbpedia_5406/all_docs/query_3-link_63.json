{
    "id": "dbpedia_5406_3",
    "rank": 63,
    "data": {
        "url": "https://www.science.gov/topicpages/c/calibration%2Bprocess%2Bexperimental",
        "read_more_link": "",
        "language": "en",
        "title": "calibration process experimental: Topics by Science.gov",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://www.science.gov/scigov/desktop/en/images/SciGov_logo.png",
            "https://www.science.gov/topicpages/c/images/arrow-up.gif",
            "https://www.science.gov/topicpages/c/images/arrow-down.gif"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Gaussian process based modeling and experimental design for sensor calibration in drifting environments\n\nPubMed Central\n\nGeng, Zongyu; Yang, Feng; Chen, Xi; Wu, Nianqiang\n\n2016-01-01\n\nIt remains a challenge to accurately calibrate a sensor subject to environmental drift. The calibration task for such a sensor is to quantify the relationship between the sensorâs response and its exposure condition, which is specified by not only the analyte concentration but also the environmental factors such as temperature and humidity. This work developed a Gaussian Process (GP)-based procedure for the efficient calibration of sensors in drifting environments. Adopted as the calibration model, GP is not only able to capture the possibly nonlinear relationship between the sensor responses and the various exposure-condition factors, but also able to provide valid statistical inference for uncertainty quantification of the target estimates (e.g., the estimated analyte concentration of an unknown environment). Built on GPâs inference ability, an experimental design method was developed to achieve efficient sampling of calibration data in a batch sequential manner. The resulting calibration procedure, which integrates the GP-based modeling and experimental design, was applied on a simulated chemiresistor sensor to demonstrate its effectiveness and its efficiency over the traditional method. PMID:26924894\n\nAn innovative method for coordinate measuring machine one-dimensional self-calibration with simplified experimental process.\n\nPubMed\n\nFang, Cheng; Butler, David Lee\n\n2013-05-01\n\nIn this paper, an innovative method for CMM (Coordinate Measuring Machine) self-calibration is proposed. In contrast to conventional CMM calibration that relies heavily on a high precision reference standard such as a laser interferometer, the proposed calibration method is based on a low-cost artefact which is fabricated with commercially available precision ball bearings. By optimizing the mathematical model and rearranging the data sampling positions, the experimental process and data analysis can be simplified. In mathematical expression, the samples can be minimized by eliminating the redundant equations among those configured by the experimental data array. The section lengths of the artefact are measured at arranged positions, with which an equation set can be configured to determine the measurement errors at the corresponding positions. With the proposed method, the equation set is short of one equation, which can be supplemented by either measuring the total length of the artefact with a higher-precision CMM or calibrating the single point error at the extreme position with a laser interferometer. In this paper, the latter is selected. With spline interpolation, the error compensation curve can be determined. To verify the proposed method, a simple calibration system was set up on a commercial CMM. Experimental results showed that with the error compensation curve uncertainty of the measurement can be reduced to 50%.\n\nExperimental calibration procedures for rotating Lorentz-force flowmeters\n\nDOE PAGES\n\nHvasta, M. G.; Slighton, N. T.; Kolemen, E.; ...\n\n2017-07-14\n\nRotating Lorentz-force flowmeters are a novel and useful technology with a range of applications in a variety of different industries. However, calibrating these flowmeters can be challenging, time-consuming, and expensive. In this paper, simple calibration procedures for rotating Lorentz-force flowmeters are presented. These procedures eliminate the need for expensive equipment, numerical modeling, redundant flowmeters, and system down-time. Finally, the calibration processes are explained in a step-by-step manner and compared to experimental results.\n\nExperimental calibration procedures for rotating Lorentz-force flowmeters\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nHvasta, M. G.; Slighton, N. T.; Kolemen, E.\n\nRotating Lorentz-force flowmeters are a novel and useful technology with a range of applications in a variety of different industries. However, calibrating these flowmeters can be challenging, time-consuming, and expensive. In this paper, simple calibration procedures for rotating Lorentz-force flowmeters are presented. These procedures eliminate the need for expensive equipment, numerical modeling, redundant flowmeters, and system down-time. Finally, the calibration processes are explained in a step-by-step manner and compared to experimental results.\n\nExperimental Investigations of Non-Stationary Properties In Radiometer Receivers Using Measurements of Multiple Calibration References\n\nNASA Technical Reports Server (NTRS)\n\nRacette, Paul; Lang, Roger; Zhang, Zhao-Nan; Zacharias, David; Krebs, Carolyn A. (Technical Monitor)\n\n2002-01-01\n\nRadiometers must be periodically calibrated because the receiver response fluctuates. Many techniques exist to correct for the time varying response of a radiometer receiver. An analytical technique has been developed that uses generalized least squares regression (LSR) to predict the performance of a wide variety of calibration algorithms. The total measurement uncertainty including the uncertainty of the calibration can be computed using LSR. The uncertainties of the calibration samples used in the regression are based upon treating the receiver fluctuations as non-stationary processes. Signals originating from the different sources of emission are treated as simultaneously existing random processes. Thus, the radiometer output is a series of samples obtained from these random processes. The samples are treated as random variables but because the underlying processes are non-stationary the statistics of the samples are treated as non-stationary. The statistics of the calibration samples depend upon the time for which the samples are to be applied. The statistics of the random variables are equated to the mean statistics of the non-stationary processes over the interval defined by the time of calibration sample and when it is applied. This analysis opens the opportunity for experimental investigation into the underlying properties of receiver non stationarity through the use of multiple calibration references. In this presentation we will discuss the application of LSR to the analysis of various calibration algorithms, requirements for experimental verification of the theory, and preliminary results from analyzing experiment measurements.\n\nExperimental Demonstration of In-Place Calibration for Time Domain Microwave Imaging System\n\nNASA Astrophysics Data System (ADS)\n\nKwon, S.; Son, S.; Lee, K.\n\n2018-04-01\n\nIn this study, the experimental demonstration of in-place calibration was conducted using the developed time domain measurement system. Experiments were conducted using three calibration methodsâin-place calibration and two existing calibrations, that is, array rotation and differential calibration. The in-place calibration uses dual receivers located at an equal distance from the transmitter. The received signals at the dual receivers contain similar unwanted signals, that is, the directly received signal and antenna coupling. In contrast to the simulations, the antennas are not perfectly matched and there might be unexpected environmental errors. Thus, we experimented with the developed experimental system to demonstrate the proposed method. The possible problems with low signal-to-noise ratio and clock jitter, which may exist in time domain systems, were rectified by averaging repeatedly measured signals. The tumor was successfully detected using the three calibration methods according to the experimental results. The cross correlation was calculated using the reconstructed image of the ideal differential calibration for a quantitative comparison between the existing rotation calibration and the proposed in-place calibration. The mean value of cross correlation between the in-place calibration and ideal differential calibration was 0.80, and the mean value of cross correlation of the rotation calibration was 0.55. Furthermore, the results of simulation were compared with the experimental results to verify the in-place calibration method. A quantitative analysis was also performed, and the experimental results show a tendency similar to the simulation.\n\nApplication of iterative robust model-based optimal experimental design for the calibration of biocatalytic models.\n\nPubMed\n\nVan Daele, Timothy; Gernaey, Krist V; Ringborg, Rolf H; BÃ¶rner, Tim; Heintz, SÃ¸ren; Van Hauwermeiren, Daan; Grey, Carl; KrÃ¼hne, Ulrich; Adlercreutz, Patrick; Nopens, Ingmar\n\n2017-09-01\n\nThe aim of model calibration is to estimate unique parameter values from available experimental data, here applied to a biocatalytic process. The traditional approach of first gathering data followed by performing a model calibration is inefficient, since the information gathered during experimentation is not actively used to optimize the experimental design. By applying an iterative robust model-based optimal experimental design, the limited amount of data collected is used to design additional informative experiments. The algorithm is used here to calibrate the initial reaction rate of an Ï-transaminase catalyzed reaction in a more accurate way. The parameter confidence region estimated from the Fisher Information Matrix is compared with the likelihood confidence region, which is not only more accurate but also a computationally more expensive method. As a result, an important deviation between both approaches is found, confirming that linearization methods should be applied with care for nonlinear models. Â© 2017 American Institute of Chemical Engineers Biotechnol. Prog., 33:1278-1293, 2017. Â© 2017 American Institute of Chemical Engineers.\n\nEvaluation of assigned-value uncertainty for complex calibrator value assignment processes: a prealbumin example.\n\nPubMed\n\nMiddleton, John; Vaks, Jeffrey E\n\n2007-04-01\n\nErrors of calibrator-assigned values lead to errors in the testing of patient samples. The ability to estimate the uncertainties of calibrator-assigned values and other variables minimizes errors in testing processes. International Organization of Standardization guidelines provide simple equations for the estimation of calibrator uncertainty with simple value-assignment processes, but other methods are needed to estimate uncertainty in complex processes. We estimated the assigned-value uncertainty with a Monte Carlo computer simulation of a complex value-assignment process, based on a formalized description of the process, with measurement parameters estimated experimentally. This method was applied to study uncertainty of a multilevel calibrator value assignment for a prealbumin immunoassay. The simulation results showed that the component of the uncertainty added by the process of value transfer from the reference material CRM470 to the calibrator is smaller than that of the reference material itself (<0.8% vs 3.7%). Varying the process parameters in the simulation model allowed for optimizing the process, while keeping the added uncertainty small. The patient result uncertainty caused by the calibrator uncertainty was also found to be small. This method of estimating uncertainty is a powerful tool that allows for estimation of calibrator uncertainty for optimization of various value assignment processes, with a reduced number of measurements and reagent costs, while satisfying the requirements to uncertainty. The new method expands and augments existing methods to allow estimation of uncertainty in complex processes.\n\nStatistical analysis on experimental calibration data for flowmeters in pressure pipes\n\nNASA Astrophysics Data System (ADS)\n\nLazzarin, Alessandro; Orsi, Enrico; Sanfilippo, Umberto\n\n2017-08-01\n\nThis paper shows a statistical analysis on experimental calibration data for flowmeters (i.e.: electromagnetic, ultrasonic, turbine flowmeters) in pressure pipes. The experimental calibration data set consists of the whole archive of the calibration tests carried out on 246 flowmeters from January 2001 to October 2015 at Settore Portate of Laboratorio di Idraulica âG. Fantoliâ of Politecnico di Milano, that is accredited as LAT 104 for a flow range between 3 l/s and 80 l/s, with a certified Calibration and Measurement Capability (CMC) - formerly known as Best Measurement Capability (BMC) - equal to 0.2%. The data set is split into three subsets, respectively consisting in: 94 electromagnetic, 83 ultrasonic and 69 turbine flowmeters; each subset is analysed separately from the others, but then a final comparison is carried out. In particular, the main focus of the statistical analysis is the correction C, that is the difference between the flow rate Q measured by the calibration facility (through the accredited procedures and the certified reference specimen) minus the flow rate QM contemporarily recorded by the flowmeter under calibration, expressed as a percentage of the same QM .\n\nCamera calibration based on the back projection process\n\nNASA Astrophysics Data System (ADS)\n\nGu, Feifei; Zhao, Hong; Ma, Yueyang; Bu, Penghui\n\n2015-12-01\n\nCamera calibration plays a crucial role in 3D measurement tasks of machine vision. In typical calibration processes, camera parameters are iteratively optimized in the forward imaging process (FIP). However, the results can only guarantee the minimum of 2D projection errors on the image plane, but not the minimum of 3D reconstruction errors. In this paper, we propose a universal method for camera calibration, which uses the back projection process (BPP). In our method, a forward projection model is used to obtain initial intrinsic and extrinsic parameters with a popular planar checkerboard pattern. Then, the extracted image points are projected back into 3D space and compared with the ideal point coordinates. Finally, the estimation of the camera parameters is refined by a non-linear function minimization process. The proposed method can obtain a more accurate calibration result, which is more physically useful. Simulation and practical data are given to demonstrate the accuracy of the proposed method.\n\nFermentation process tracking through enhanced spectral calibration modeling.\n\nPubMed\n\nTriadaphillou, Sophia; Martin, Elaine; Montague, Gary; Norden, Alison; Jeffkins, Paul; Stimpson, Sarah\n\n2007-06-15\n\nThe FDA process analytical technology (PAT) initiative will materialize in a significant increase in the number of installations of spectroscopic instrumentation. However, to attain the greatest benefit from the data generated, there is a need for calibration procedures that extract the maximum information content. For example, in fermentation processes, the interpretation of the resulting spectra is challenging as a consequence of the large number of wavelengths recorded, the underlying correlation structure that is evident between the wavelengths and the impact of the measurement environment. Approaches to the development of calibration models have been based on the application of partial least squares (PLS) either to the full spectral signature or to a subset of wavelengths. This paper presents a new approach to calibration modeling that combines a wavelength selection procedure, spectral window selection (SWS), where windows of wavelengths are automatically selected which are subsequently used as the basis of the calibration model. However, due to the non-uniqueness of the windows selected when the algorithm is executed repeatedly, multiple models are constructed and these are then combined using stacking thereby increasing the robustness of the final calibration model. The methodology is applied to data generated during the monitoring of broth concentrations in an industrial fermentation process from on-line near-infrared (NIR) and mid-infrared (MIR) spectrometers. It is shown that the proposed calibration modeling procedure outperforms traditional calibration procedures, as well as enabling the identification of the critical regions of the spectra with regard to the fermentation process.\n\nMetrology: Calibration and measurement processes guidelines\n\nNASA Technical Reports Server (NTRS)\n\nCastrup, Howard T.; Eicke, Woodward G.; Hayes, Jerry L.; Mark, Alexander; Martin, Robert E.; Taylor, James L.\n\n1994-01-01\n\nThe guide is intended as a resource to aid engineers and systems contracts in the design, implementation, and operation of metrology, calibration, and measurement systems, and to assist NASA personnel in the uniform evaluation of such systems supplied or operated by contractors. Methodologies and techniques acceptable in fulfilling metrology quality requirements for NASA programs are outlined. The measurement process is covered from a high level through more detailed discussions of key elements within the process, Emphasis is given to the flowdown of project requirements to measurement system requirements, then through the activities that will provide measurements with defined quality. In addition, innovations and techniques for error analysis, development of statistical measurement process control, optimization of calibration recall systems, and evaluation of measurement uncertainty are presented.\n\nEffects of experimental design on calibration curve precision in routine analysis\n\nPubMed Central\n\nPimentel, Maria Fernanda; Neto, BenÃ­cio de Barros; Saldanha, Teresa Cristina B.\n\n1998-01-01\n\nA computational program which compares the effciencies of different experimental designs with those of maximum precision (D-optimized designs) is described. The program produces confidence interval plots for a calibration curve and provides information about the number of standard solutions, concentration levels and suitable concentration ranges to achieve an optimum calibration. Some examples of the application of this novel computational program are given, using both simulated and real data. PMID:18924816\n\nA Focusing Method in the Calibration Process of Image Sensors Based on IOFBs\n\nPubMed Central\n\nFernÃ¡ndez, Pedro R.; LÃ¡zaro, JosÃ© L.; Gardel, Alfredo; Cano, Ãngel E.; Bravo, Ignacio\n\n2010-01-01\n\nA focusing procedure in the calibration process of image sensors based on Incoherent Optical Fiber Bundles (IOFBs) is described using the information extracted from fibers. These procedures differ from any other currently known focusing method due to the non spatial in-out correspondence between fibers, which produces a natural codification of the image to transmit. Focus measuring is essential prior to carrying out calibration in order to guarantee accurate processing and decoding. Four algorithms have been developed to estimate the focus measure; two methods based on mean grey level, and the other two based on variance. In this paper, a few simple focus measures are defined and compared. Some experimental results referred to the focus measure and the accuracy of the developed methods are discussed in order to demonstrate its effectiveness. PMID:22315526\n\nA Kinematic Calibration Process for Flight Robotic Arms\n\nNASA Technical Reports Server (NTRS)\n\nCollins, Curtis L.; Robinson, Matthew L.\n\n2013-01-01\n\nThe Mars Science Laboratory (MSL) robotic arm is ten times more massive than any Mars robotic arm before it, yet with similar accuracy and repeatability positioning requirements. In order to assess and validate these requirements, a higher-fidelity model and calibration processes were needed. Kinematic calibration of robotic arms is a common and necessary process to ensure good positioning performance. Most methodologies assume a rigid arm, high-accuracy data collection, and some kind of optimization of kinematic parameters. A new detailed kinematic and deflection model of the MSL robotic arm was formulated in the design phase and used to update the initial positioning and orientation accuracy and repeatability requirements. This model included a higher-fidelity link stiffness matrix representation, as well as a link level thermal expansion model. In addition, it included an actuator backlash model. Analytical results highlighted the sensitivity of the arm accuracy to its joint initialization methodology. Because of this, a new technique for initializing the arm joint encoders through hardstop calibration was developed. This involved selecting arm configurations to use in Earth-based hardstop calibration that had corresponding configurations on Mars with the same joint torque to ensure repeatability in the different gravity environment. The process used to collect calibration data for the arm included the use of multiple weight stand-in turrets with enough metrology targets to reconstruct the full six-degree-of-freedom location of the rover and tool frames. The follow-on data processing of the metrology data utilized a standard differential formulation and linear parameter optimization technique.\n\nProcess for producing laser-formed video calibration markers.\n\nPubMed\n\nFranck, J B; Keller, P N; Swing, R A; Silberberg, G G\n\n1983-08-15\n\nA process for producing calibration markers directly on the photoconductive surface of video camera tubes has been developed. This process includes the use of a Nd:YAG laser operating at 1.06 microm with a 9.5-nsec pulse width (full width at half-maximum). The laser was constrained to operate in the TEM(00) spatial mode by intracavity aperturing. The use of this technology has produced an increase of up to 50 times the accuracy of geometric measurement. This is accomplished by a decrease in geometric distortion and an increase in geometric scaling. The process by which these laser-formed video calibrations are made will be discussed.\n\nExperimental validation of a self-calibrating cryogenic mass flowmeter\n\nNASA Astrophysics Data System (ADS)\n\nJanzen, A.; Boersch, M.; Burger, B.; Drache, J.; Ebersoldt, A.; Erni, P.; Feldbusch, F.; Oertig, D.; Grohmann, S.\n\n2017-12-01\n\nThe Karlsruhe Institute of Technology (KIT) and the WEKA AG jointly develop a commercial flowmeter for application in helium cryostats. The flowmeter functions according to a new thermal measurement principle that eliminates all systematic uncertainties and enables self-calibration during real operation. Ideally, the resulting uncertainty of the measured flow rate is only dependent on signal noises, which are typically very small with regard to the measured value. Under real operating conditions, cryoplant-dependent flow rate fluctuations induce an additional uncertainty, which follows from the sensitivity of the method. This paper presents experimental results with helium at temperatures between 30 and 70 K and flow rates in the range of 4 to 12 g/s. The experiments were carried out in a control cryostat of the 2 kW helium refrigerator of the TOSKA test facility at KIT. Inside the cryostat, the new flowmeter was installed in series with a Venturi tube that was used for reference measurements. The measurement results demonstrate the self-calibration capability during real cryoplant operation. The influences of temperature and flow rate fluctuations on the self-calibration uncertainty are discussed.\n\nElectronic transport in VO 2 âExperimentally calibrated Boltzmann transport modeling\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nKinaci, Alper; Kado, Motohisa; Rosenmann, Daniel\n\n2015-12-28\n\nMaterials that undergo metal-insulator transitions (MITs) are under intense study because the transition is scientifically fascinating and technologically promising for various applications. Among these materials, VO2 has served as a prototype due to its favorable transition temperature. While the physical underpinnings of the transition have been heavily investigated experimentally and computationally, quantitative modeling of electronic transport in the two phases has yet to be undertaken. In this work, we establish a density-functional-theory (DFT)-based approach to model electronic transport properties in VO2 in the semiconducting and metallic regimes, focusing on band transport using the Boltzmann transport equations. We synthesized high qualitymoreÂ Â» VO2 films and measured the transport quantities across the transition, in order to calibrate the free parameters in the model. We find that the experimental calibration of the Hubbard correction term can efficiently and adequately model the metallic and semiconducting phases, allowing for further computational design of MIT materials for desirable transport properties.Â«Â less\n\nHigh heat flux measurements and experimental calibrations/characterizations\n\nNASA Technical Reports Server (NTRS)\n\nKidd, Carl T.\n\n1992-01-01\n\nRecent progress in techniques employed in the measurement of very high heat-transfer rates in reentry-type facilities at the Arnold Engineering Development Center (AEDC) is described. These advances include thermal analyses applied to transducer concepts used to make these measurements; improved heat-flux sensor fabrication methods, equipment, and procedures for determining the experimental time response of individual sensors; performance of absolute heat-flux calibrations at levels above 2,000 Btu/cu ft-sec (2.27 kW/cu cm); and innovative methods of performing in-situ run-to-run characterizations of heat-flux probes installed in the test facility. Graphical illustrations of the results of extensive thermal analyses of the null-point calorimeter and coaxial surface thermocouple concepts with application to measurements in aerothermal test environments are presented. Results of time response experiments and absolute calibrations of null-point calorimeters and coaxial thermocouples performed in the laboratory at intermediate to high heat-flux levels are shown. Typical AEDC high-enthalpy arc heater heat-flux data recently obtained with a Calspan-fabricated null-point probe model are included.\n\nIn-Flight Calibration Processes for the MMS Fluxgate Magnetometers\n\nNASA Astrophysics Data System (ADS)\n\nBromund, K. R.; Leinweber, H. K.; Plaschke, F.; Strangeway, R. J.; Magnes, W.; Fischer, D.; Nakamura, R.; Anderson, B. J.; Russell, C. T.; Baumjohann, W.; Chutter, M.; Torbert, R. B.; Le, G.; Slavin, J. A.; Kepko, L.\n\n2015-12-01\n\nThe calibration effort for the Magnetospheric Multiscale Mission (MMS) Analog Fluxgate (AFG) and Digital Fluxgate (DFG) magnetometers is a coordinated effort between three primary institutions: University of California, Los Angeles (UCLA); Space Research Institute, Graz, Austria (IWF); and Goddard Space Flight Center (GSFC). Since the successful deployment of all 8 magnetometers on 17 March 2015, the effort to confirm and update the ground calibrations has been underway during the MMS commissioning phase. The in-flight calibration processes evaluate twelve parameters that determine the alignment, orthogonalization, offsets, and gains for all 8 magnetometers using algorithms originally developed by UCLA and the Technical University of Braunschweig and tailored to MMS by IWF, UCLA, and GSFC. We focus on the processes run at GSFC to determine the eight parameters associated with spin tones and harmonics. We will also discuss the processing flow and interchange of parameters between GSFC, IWF, and UCLA. IWF determines the low range spin axis offsets using the Electron Drift Instrument (EDI). UCLA determines the absolute gains and sensor azimuth orientation using Earth field comparisons. We evaluate the performance achieved for MMS and give examples of the quality of the resulting calibrations.\n\nCalibration process of highly parameterized semi-distributed hydrological model\n\nNASA Astrophysics Data System (ADS)\n\nVidmar, Andrej; Brilly, Mitja\n\n2017-04-01\n\nHydrological phenomena take place in the hydrological system, which is governed by nature, and are essentially stochastic. These phenomena are unique, non-recurring, and changeable across space and time. Since any river basin with its own natural characteristics and any hydrological event therein, are unique, this is a complex process that is not researched enough. Calibration is a procedure of determining the parameters of a model that are not known well enough. Input and output variables and mathematical model expressions are known, while only some parameters are unknown, which are determined by calibrating the model. The software used for hydrological modelling nowadays is equipped with sophisticated algorithms for calibration purposes without possibility to manage process by modeler. The results are not the best. We develop procedure for expert driven process of calibration. We use HBV-light-CLI hydrological model which has command line interface and coupling it with PEST. PEST is parameter estimation tool which is used widely in ground water modeling and can be used also on surface waters. Process of calibration managed by expert directly, and proportionally to the expert knowledge, affects the outcome of the inversion procedure and achieves better results than if the procedure had been left to the selected optimization algorithm. First step is to properly define spatial characteristic and structural design of semi-distributed model including all morphological and hydrological phenomena, like karstic area, alluvial area and forest area. This step includes and requires geological, meteorological, hydraulic and hydrological knowledge of modeler. Second step is to set initial parameter values at their preferred values based on expert knowledge. In this step we also define all parameter and observation groups. Peak data are essential in process of calibration if we are mainly interested in flood events. Each Sub Catchment in the model has own observations group\n\nExperimental Results of Site Calibration and Sensitivity Measurements in OTR for UWB Systems\n\nNASA Astrophysics Data System (ADS)\n\nViswanadham, Chandana; Rao, P. Mallikrajuna\n\n2017-06-01\n\nSystem calibration and parameter accuracy measurement of electronic support measures (ESM) systems is a major activity, carried out by electronic warfare (EW) engineers. These activities are very critical and needs good understanding in the field of microwaves, antennas, wave propagation, digital and communication domains. EW systems are broad band, built with state-of-the art electronic hardware, installed on different varieties of military platforms to guard country's security from time to time. EW systems operate in wide frequency ranges, typically in the order of thousands of MHz, hence these are ultra wide band (UWB) systems. Few calibration activities are carried within the system and in the test sites, to meet the accuracies of final specifications. After calibration, parameters are measured for their accuracies either in feed mode by injecting the RF signals into the front end or in radiation mode by transmitting the RF signals on to system antenna. To carry out these activities in radiation mode, a calibrated open test range (OTR) is necessary in the frequency band of interest. Thus site calibration of OTR is necessary to be carried out before taking up system calibration and parameter measurements. This paper presents the experimental results of OTR site calibration and sensitivity measurements of UWB systems in radiation mode.\n\nData processing and in-flight calibration systems for OMI-EOS-Aura\n\nNASA Astrophysics Data System (ADS)\n\nvan den Oord, G. H. J.; Dobber, M.; van de Vegte, J.; van der Neut, I.; Som de Cerff, W.; Rozemeijer, N. C.; Schenkelaars, V.; ter Linden, M.\n\n2006-08-01\n\nThe OMI instrument that flies on the EOS Aura mission was launched in July 2004. OMI is a UV-VIS imaging spectrometer that measures in the 270 - 500 nm wavelength range. OMI provides daily global coverage with high spatial resolution. Every orbit of 100 minutes OMI generates about 0.5 GB of Level 0 data and 1.2 GB of Level 1 data. About half of the Level 1 data consists of in-flight calibration measurements. These data rates make it necessary to automate the process of in-flight calibration. For that purpose two facilities have been developed at KNMI in the Netherlands: the OMI Dutch Processing System (ODPS) and the Trend Monitoring and In-flight Calibration Facility (TMCF). A description of these systems is provided with emphasis on the use for radiometric, spectral and detector calibration and characterization. With the advance of detector technology and the need for higher spatial resolution, data rates will become even higher for future missions. To make effective use of automated systems like the TMCF, it is of paramount importance to integrate the instrument operations concept, the information contained in the Level 1 (meta-)data products and the inflight calibration software and system databases. In this way a robust but also flexible end-to-end system can be developed that serves the needs of the calibration staff, the scientific data users and the processing staff. The way this has been implemented for OMI may serve as an example of a cost-effective and user friendly solution for future missions. The basic system requirements for in-flight calibration are discussed and examples are given how these requirements have been implemented for OMI. Special attention is paid to the aspect of supporting the Level 0 - 1 processing with timely and accurate calibration constants.\n\nResearch on camera on orbit radial calibration based on black body and infrared calibration stars\n\nNASA Astrophysics Data System (ADS)\n\nWang, YuDu; Su, XiaoFeng; Zhang, WanYing; Chen, FanSheng\n\n2018-05-01\n\nAffected by launching process and space environment, the response capability of a space camera must be attenuated. So it is necessary for a space camera to have a spaceborne radiant calibration. In this paper, we propose a method of calibration based on accurate Infrared standard stars was proposed for increasing infrared radiation measurement precision. As stars can be considered as a point target, we use them as the radiometric calibration source and establish the Taylor expansion method and the energy extrapolation model based on WISE catalog and 2MASS catalog. Then we update the calibration results from black body. Finally, calibration mechanism is designed and the technology of design is verified by on orbit test. The experimental calibration result shows the irradiance extrapolation error is about 3% and the accuracy of calibration methods is about 10%, the results show that the methods could satisfy requirements of on orbit calibration.\n\nCalibration of streamflow gauging stations at the Tenderfoot Creek Experimental Forest\n\nTreesearch\n\nScott W. Woods\n\n2007-01-01\n\nWe used tracer based methods to calibrate eleven streamflow gauging stations at the Tenderfoot Creek Experimental Forest in western Montana. At six of the stations the measured flows were consistent with the existing rating curves. At Lower and Upper Stringer Creek, Upper Sun Creek and Upper Tenderfoot Creek the published flows, based on the existing rating curves,...\n\nExperimental verification of self-calibration radiometer based on spontaneous parametric downconversion\n\nNASA Astrophysics Data System (ADS)\n\nGao, Dongyang; Zheng, Xiaobing; Li, Jianjun; Hu, Youbo; Xia, Maopeng; Salam, Abdul; Zhang, Peng\n\n2018-03-01\n\nBased on spontaneous parametric downconversion process, we propose a novel self-calibration radiometer scheme which can self-calibrate the degradation of its own response and ultimately monitor the fluctuation of a target radiation. Monitor results were independent of its degradation and not linked to the primary standard detector scale. The principle and feasibility of the proposed scheme were verified by observing bromine-tungsten lamp. A relative standard deviation of 0.39 % was obtained for stable bromine-tungsten lamp. Results show that the proposed scheme is advanced of its principle. The proposed scheme could make a significant breakthrough in the self-calibration issue on the space platform.\n\nTitaniQ recrystallized: experimental confirmation of the original Ti-in-quartz calibrations\n\nNASA Astrophysics Data System (ADS)\n\nThomas, Jay B.; Watson, E. Bruce; Spear, Frank S.; Wark, D. A.\n\n2015-03-01\n\nSeveral studies have reported the P- T dependencies of Ti-in-quartz solubility, and there is close agreement among three of the four experimental calibrations. New experiments were conducted in the present study to identify potential experimental disequilibrium, and to determine which Ti-in-quartz solubility calibration is most accurate. Crystals of quartz, rutile and zircon were grown from SiO2-, TiO2-, and ZrSiO4-saturated aqueous fluids in an initial synthesis experiment at 925 Â°C and 10 kbar in a piston-cylinder apparatus. A range of quartz crystal sizes was produced in this experiment; both large and small examples were analyzed by electron microprobe to determine whether Ti concentrations are correlated with crystal size. Cathodoluminescence images and EPMA measurements show that intercrystalline and intracrystalline variations in Ti concentrations are remarkably small regardless of crystal size. The average Ti-in-quartz concentration from the synthesis experiment is 392 Â± 1 ppmw Ti, which is within 95 % confidence interval of data from the 10 kbar isobar of Wark and Watson (Contrib Mineral Petrol 152:743-754, 2006) and Thomas et al. (Contrib Mineral Petrol 160:743-759, 2010). As a cross-check on the Ti-in-quartz calibration, we also measured the concentration of Zr in rutile from the synthesis experiment. The average Zr-in-rutile concentration is 4337 Â± 32 ppmw Zr, which is also within the 95 % confidence interval of the Zr-in-rutile solubility calibration of Ferry and Watson (Contrib Mineral Petrol 154:429-437, 2007). The P- T dependencies of Ti solubility in quartz and Zr solubility in rutile were applied as a thermobarometer to the experimental sample. The average Ti-in-quartz isopleth calculated from the calibration of Thomas et al. (Contrib Mineral Petrol 160:743-759, 2010) and the average Zr-in-rutile isopleth calculated from the calibration of Tomkins et al. (J Metamorph Geol 25:703-713, 2007) cross at 9.5 kbar and 920 Â°C, which is in excellent\n\nBayesian Treed Calibration: An Application to Carbon Capture With AX Sorbent\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nKonomi, Bledar A.; Karagiannis, Georgios; Lai, Kevin\n\n2017-01-02\n\nIn cases where field or experimental measurements are not available, computer models can model real physical or engineering systems to reproduce their outcomes. They are usually calibrated in light of experimental data to create a better representation of the real system. Statistical methods, based on Gaussian processes, for calibration and prediction have been especially important when the computer models are expensive and experimental data limited. In this paper, we develop the Bayesian treed calibration (BTC) as an extension of standard Gaussian process calibration methods to deal with non-stationarity computer models and/or their discrepancy from the field (or experimental) data. OurmoreÂ Â» proposed method partitions both the calibration and observable input space, based on a binary tree partitioning, into sub-regions where existing model calibration methods can be applied to connect a computer model with the real system. The estimation of the parameters in the proposed model is carried out using Markov chain Monte Carlo (MCMC) computational techniques. Different strategies have been applied to improve mixing. We illustrate our method in two artificial examples and a real application that concerns the capture of carbon dioxide with AX amine based sorbents. The source code and the examples analyzed in this paper are available as part of the supplementary materials.Â«Â less\n\nCalibrator device for the extrusion of cable coatings\n\nNASA Astrophysics Data System (ADS)\n\nGarbacz, Tomasz; DulebovÃ¡, Ä½udmila; SpiÅ¡Ã¡k, Emil; DulebovÃ¡, Martina\n\n2016-05-01\n\nThis paper presents selected results of theoretical and experimental research works on a new calibration device (calibrators) used to produce coatings of electric cables. The aim of this study is to present design solution calibration equipment and present a new calibration machine, which is an important element of the modernized technology extrusion lines for coating cables. As a result of the extrusion process of PVC modified with blowing agents, an extrudate in the form of an electrical cable was obtained. The conditions of the extrusion process were properly selected, which made it possible to obtain a product with solid external surface and cellular core.\n\nExperimental Validation of ARFI Surveillance of Subcutaneous Hemorrhage (ASSH) Using Calibrated Infusions in a Tissue-Mimicking Model and Dogs.\n\nPubMed\n\nGeist, Rebecca E; DuBois, Chase H; Nichols, Timothy C; Caughey, Melissa C; Merricks, Elizabeth P; Raymer, Robin; Gallippi, Caterina M\n\n2016-09-01\n\nAcoustic radiation force impulse (ARFI) Surveillance of Subcutaneous Hemorrhage (ASSH) has been previously demonstrated to differentiate bleeding phenotype and responses to therapy in dogs and humans, but to date, the method has lacked experimental validation. This work explores experimental validation of ASSH in a poroelastic tissue-mimic and in vivo in dogs. The experimental design exploits calibrated flow rates and infusion durations of evaporated milk in tofu or heparinized autologous blood in dogs. The validation approach enables controlled comparisons of ASSH-derived bleeding rate (BR) and time to hemostasis (TTH) metrics. In tissue-mimicking experiments, halving the calibrated flow rate yielded ASSH-derived BRs that decreased by 44% to 48%. Furthermore, for calibrated flow durations of 5.0 minutes and 7.0 minutes, average ASSH-derived TTH was 5.2 minutes and 7.0 minutes, respectively, with ASSH predicting the correct TTH in 78% of trials. In dogs undergoing calibrated autologous blood infusion, ASSH measured a 3-minute increase in TTH, corresponding to the same increase in the calibrated flow duration. For a measured 5% decrease in autologous infusion flow rate, ASSH detected a 7% decrease in BR. These tissue-mimicking and in vivo preclinical experimental validation studies suggest the ASSH BR and TTH measures reflect bleeding dynamics. Â© The Author(s) 2015.\n\nUncertainty Analysis of Instrument Calibration and Application\n\nNASA Technical Reports Server (NTRS)\n\nTripp, John S.; Tcheng, Ping\n\n1999-01-01\n\nExperimental aerodynamic researchers require estimated precision and bias uncertainties of measured physical quantities, typically at 95 percent confidence levels. Uncertainties of final computed aerodynamic parameters are obtained by propagation of individual measurement uncertainties through the defining functional expressions. In this paper, rigorous mathematical techniques are extended to determine precision and bias uncertainties of any instrument-sensor system. Through this analysis, instrument uncertainties determined through calibration are now expressed as functions of the corresponding measurement for linear and nonlinear univariate and multivariate processes. Treatment of correlated measurement precision error is developed. During laboratory calibration, calibration standard uncertainties are assumed to be an order of magnitude less than those of the instrument being calibrated. Often calibration standards do not satisfy this assumption. This paper applies rigorous statistical methods for inclusion of calibration standard uncertainty and covariance due to the order of their application. The effects of mathematical modeling error on calibration bias uncertainty are quantified. The effects of experimental design on uncertainty are analyzed. The importance of replication is emphasized, techniques for estimation of both bias and precision uncertainties using replication are developed. Statistical tests for stationarity of calibration parameters over time are obtained.\n\nPrecision process calibration and CD predictions for low-k1 lithography\n\nNASA Astrophysics Data System (ADS)\n\nChen, Ting; Park, Sangbong; Berger, Gabriel; Coskun, Tamer H.; de Vocht, Joep; Chen, Fung; Yu, Linda; Hsu, Stephen; van den Broeke, Doug; Socha, Robert; Park, Jungchul; Gronlund, Keith; Davis, Todd; Plachecki, Vince; Harris, Tom; Hansen, Steve; Lambson, Chuck\n\n2005-06-01\n\nLeading resist calibration for sub-0.3 k1 lithography demands accuracy <2nm for CD through pitch. An accurately calibrated resist process is the prerequisite for establishing production-worthy manufacturing under extreme low k1. From an integrated imaging point of view, the following key components must be simultaneously considered during the calibration - high numerical aperture (NA>0.8) imaging characteristics, customized illuminations (measured vs. modeled pupil profiles), resolution enhancement technology (RET) mask with OPC, reticle metrology, and resist thin film substrate. For imaging at NA approaching unity, polarized illumination can impact significantly the contrast formation in the resist film stack, and therefore it is an important factor to consider in the CD-based resist calibration. For aggressive DRAM memory core designs at k1<0.3, pattern-specific illumination optimization has proven to be critical for achieving the required imaging performance. Various optimization techniques from source profile optimization with fixed mask design to the combined source and mask optimization have been considered for customer designs and available imaging capabilities. For successful low-k1 process development, verification of the optimization results can only be made with a sufficiently tunable resist model that can predicate the wafer printing accurately under various optimized process settings. We have developed, for resist patterning under aggressive low-k1 conditions, a novel 3D diffusion model equipped with double-Gaussian convolution in each dimension. Resist calibration with the new diffusion model has demonstrated a fitness and CD predication accuracy that rival or outperform the traditional 3D physical resist models. In this work, we describe our empirical approach to achieving the nm-scale precision for advanced lithography process calibrations, using either measured 1D CD through-pitch or 2D memory core patterns. We show that for ArF imaging, the\n\nIn-Flight Calibration Processes for the MMS Fluxgate Magnetometers\n\nNASA Technical Reports Server (NTRS)\n\nBromund, K. R.; Leinweber, H. K.; Plaschke, F.; Strangeway, R. J.; Magnes, W.; Fischer, D.; Nakamura, R.; Anderson, B. J.; Russell, C. T.; Baumjohann, W.;\n\n2015-01-01\n\nThe calibration effort for the Magnetospheric Multiscale Mission (MMS) Analog Fluxgate (AFG) and DigitalFluxgate (DFG) magnetometers is a coordinated effort between three primary institutions: University of California, LosAngeles (UCLA); Space Research Institute, Graz, Austria (IWF); and Goddard Space Flight Center (GSFC). Since thesuccessful deployment of all 8 magnetometers on 17 March 2015, the effort to confirm and update the groundcalibrations has been underway during the MMS commissioning phase. The in-flight calibration processes evaluatetwelve parameters that determine the alignment, orthogonalization, offsets, and gains for all 8 magnetometers usingalgorithms originally developed by UCLA and the Technical University of Braunschweig and tailored to MMS by IWF,UCLA, and GSFC. We focus on the processes run at GSFC to determine the eight parameters associated with spin tonesand harmonics. We will also discuss the processing flow and interchange of parameters between GSFC, IWF, and UCLA.IWF determines the low range spin axis offsets using the Electron Drift Instrument (EDI). UCLA determines the absolutegains and sensor azimuth orientation using Earth field comparisons. We evaluate the performance achieved for MMS andgive examples of the quality of the resulting calibrations.\n\nCALET Data Processing and On-Orbit Detector Calibration\n\nNASA Astrophysics Data System (ADS)\n\nAsaoka, Yoichi\n\n2016-07-01\n\nThe CALET (CALorimetric Electron Telescope), launched to the International Space Station (ISS) in August 2015 and accumulating scientific data since October 2015, aims at long duration observations of high-energy cosmic rays onboard the ISS. The CALET detector features the very thick calorimeter of 30 radiation-length which consists of imaging and total absorption calorimeters (IMC and TASC respectively). It will directly measure the cosmic-ray electron spectrum in the energy range of 1 GeV-20 TeV with 2% energy resolution. In addition, the instrument has capabilities to measure the spectra of gamma-rays, protons and nuclei well into the TeV range. Precise pointing direction is determined with an attached Advanced Stellar Camera (ASC). To operate the CALET onboard ISS, the CALET Ground Support Equipment (CALET-GSE) and Waseda CALET Operations Center (WCOC) have been established at JAXA and Waseda Univ., respectively. Scientific operations of CALET are planned in the WCOC taking into account the orbital variations of geomagnetic rigidity cutoff. Scheduled command sequence is utilized to control CALET observation mode on orbit. A calibration data trigger mode, such as recording pedestal and penetrating particle events, a low-energy electron trigger mode operating at high geomagnetic latitude, and other dedicated trigger modes are scheduled around the ISS orbit while maintaining the maximum exposure to high-energy electrons. Scientific raw data called CALET Level 0 data are generated from raw telemetry packets in the CALET-GSE on an hourly basis by correcting time-order and by completing the data set using stored data taken during loss of real-time telemetry downlink. Level 0 data are processed to CALET Level 1 data in the WCOC by interpreting all the raw packets and building cosmic-ray event data as well as house keeping data. Level 1 data are then distributed to the collaboration for scientific data analysis. Level 1 data analysis is focused on the detector\n\nWavelength calibration of x-ray imaging crystal spectrometer on Joint Texas Experimental Tokamak\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nYan, W.; Chen, Z. Y., E-mail: zychen@hust.edu.cn; Jin, W.\n\n2014-11-15\n\nThe wavelength calibration of x-ray imaging crystal spectrometer is a key issue for the measurements of plasma rotation. For the lack of available standard radiation source near 3.95 Ã and there is no other diagnostics to measure the core rotation for inter-calibration, an indirect method by using tokamak plasma itself has been applied on joint Texas experimental tokamak. It is found that the core toroidal rotation velocity is not zero during locked mode phase. This is consistent with the observation of small oscillations on soft x-ray signals and electron cyclotron emission during locked-mode phase.\n\nOn-Ground Processing of Yaogan-24 Remote Sensing Satellite Attitude Data and Verification Using Geometric Field Calibration\n\nPubMed Central\n\nWang, Mi; Fan, Chengcheng; Yang, Bo; Jin, Shuying; Pan, Jun\n\n2016-01-01\n\nSatellite attitude accuracy is an important factor affecting the geometric processing accuracy of high-resolution optical satellite imagery. To address the problem whereby the accuracy of the Yaogan-24 remote sensing satelliteâs on-board attitude data processing is not high enough and thus cannot meet its image geometry processing requirements, we developed an approach involving on-ground attitude data processing and digital orthophoto (DOM) and the digital elevation model (DEM) verification of a geometric calibration field. The approach focuses on three modules: on-ground processing based on bidirectional filter, overall weighted smoothing and fitting, and evaluation in the geometric calibration field. Our experimental results demonstrate that the proposed on-ground processing method is both robust and feasible, which ensures the reliability of the observation data quality, convergence and stability of the parameter estimation model. In addition, both the Euler angle and quaternion could be used to build a mathematical fitting model, while the orthogonal polynomial fitting model is more suitable for modeling the attitude parameter. Furthermore, compared to the image geometric processing results based on on-board attitude data, the image uncontrolled and relative geometric positioning result accuracy can be increased by about 50%. PMID:27483287\n\nThe aluminum-in-olivine thermometer for mantle peridotites - Experimental versus empirical calibration and potential applications\n\nNASA Astrophysics Data System (ADS)\n\nBussweiler, Y.; Brey, G. P.; Pearson, D. G.; Stachel, T.; Stern, R. A.; Hardman, M. F.; Kjarsgaard, B. A.; Jackson, S. E.\n\n2017-02-01\n\nThis study provides an experimental calibration of the empirical Al-in-olivine thermometer for mantle peridotites proposed by De Hoog et al. (2010). We report Al concentrations measured by secondary ion mass spectrometry (SIMS) in olivines produced in the original high-pressure, high-temperature, four-phase lherzolite experiments by Brey et al. (1990). These reversed experiments were used for the calibration of the two-pyroxene thermometer and Al-in-orthopyroxene barometer by Brey and KÃ¶hler (1990). The experimental conditions of the runs investigated here range from 28 to 60 kbar and 1000 to 1300 Â°C. Olivine compositions from this range of experiments have Al concentrations that are consistent, within analytical uncertainties, with those predicted by the empirical calibration of the Al-in-olivine thermometer for mantle peridotites. Fitting the experimental data to a thermometer equation, using the least squares method, results in the expression: This version of the Al-in-olivine thermometer appears to be applicable to garnet peridotites (lherzolites and harzburgites) well outside the range of experimental conditions investigated here. However, the thermometer is not applicable to spinel-bearing peridotites. We provide new trace element criteria to distinguish between olivine from garnet-, garnet-spinel-, and spinel-facies peridotites. The estimated accuracy of the thermometer is Â± 20 Â°C. Thus, the thermometer could serve as a useful tool in settings where two-pyroxene thermometry cannot be applied, such as garnet harzburgites and single inclusions in diamond.\n\nPolarimetric SAR calibration experiment using active radar calibrators\n\nNASA Astrophysics Data System (ADS)\n\nFreeman, Anthony; Shen, Yuhsyen; Werner, Charles L.\n\n1990-03-01\n\nActive radar calibrators are used to derive both the amplitude and phase characteristics of a multichannel polarimetric SAR from the complex image data. Results are presented from an experiment carried out using the NASA/JPL DC-8 aircraft SAR over a calibration site at Goldstone, California. As part of the experiment, polarimetric active radar calibrators (PARCs) with adjustable polarization signatures were deployed. Experimental results demonstrate that the PARCs can be used to calibrate polarimetric SAR images successfully. Restrictions on the application of the PARC calibration procedure are discussed.\n\nPolarimetric SAR calibration experiment using active radar calibrators\n\nNASA Technical Reports Server (NTRS)\n\nFreeman, Anthony; Shen, Yuhsyen; Werner, Charles L.\n\n1990-01-01\n\nActive radar calibrators are used to derive both the amplitude and phase characteristics of a multichannel polarimetric SAR from the complex image data. Results are presented from an experiment carried out using the NASA/JPL DC-8 aircraft SAR over a calibration site at Goldstone, California. As part of the experiment, polarimetric active radar calibrators (PARCs) with adjustable polarization signatures were deployed. Experimental results demonstrate that the PARCs can be used to calibrate polarimetric SAR images successfully. Restrictions on the application of the PARC calibration procedure are discussed.\n\nVolumetric calibration of a plenoptic camera\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nHall, Elise Munz; Fahringer, Timothy W.; Guildenbecher, Daniel Robert\n\nHere, the volumetric calibration of a plenoptic camera is explored to correct for inaccuracies due to real-world lens distortions and thin-lens assumptions in current processing methods. Two methods of volumetric calibration based on a polynomial mapping function that does not require knowledge of specific lens parameters are presented and compared to a calibration based on thin-lens assumptions. The first method, volumetric dewarping, is executed by creation of a volumetric representation of a scene using the thin-lens assumptions, which is then corrected in post-processing using a polynomial mapping function. The second method, direct light-field calibration, uses the polynomial mapping in creationmoreÂ Â» of the initial volumetric representation to relate locations in object space directly to image sensor locations. The accuracy and feasibility of these methods is examined experimentally by capturing images of a known dot card at a variety of depths. Results suggest that use of a 3D polynomial mapping function provides a significant increase in reconstruction accuracy and that the achievable accuracy is similar using either polynomial-mapping-based method. Additionally, direct light-field calibration provides significant computational benefits by eliminating some intermediate processing steps found in other methods. Finally, the flexibility of this method is shown for a nonplanar calibration.Â«Â less\n\nVolumetric calibration of a plenoptic camera.\n\nPubMed\n\nHall, Elise Munz; Fahringer, Timothy W; Guildenbecher, Daniel R; Thurow, Brian S\n\n2018-02-01\n\nThe volumetric calibration of a plenoptic camera is explored to correct for inaccuracies due to real-world lens distortions and thin-lens assumptions in current processing methods. Two methods of volumetric calibration based on a polynomial mapping function that does not require knowledge of specific lens parameters are presented and compared to a calibration based on thin-lens assumptions. The first method, volumetric dewarping, is executed by creation of a volumetric representation of a scene using the thin-lens assumptions, which is then corrected in post-processing using a polynomial mapping function. The second method, direct light-field calibration, uses the polynomial mapping in creation of the initial volumetric representation to relate locations in object space directly to image sensor locations. The accuracy and feasibility of these methods is examined experimentally by capturing images of a known dot card at a variety of depths. Results suggest that use of a 3D polynomial mapping function provides a significant increase in reconstruction accuracy and that the achievable accuracy is similar using either polynomial-mapping-based method. Additionally, direct light-field calibration provides significant computational benefits by eliminating some intermediate processing steps found in other methods. Finally, the flexibility of this method is shown for a nonplanar calibration.\n\nVolumetric calibration of a plenoptic camera\n\nDOE PAGES\n\nHall, Elise Munz; Fahringer, Timothy W.; Guildenbecher, Daniel Robert; ...\n\n2018-02-01\n\nHere, the volumetric calibration of a plenoptic camera is explored to correct for inaccuracies due to real-world lens distortions and thin-lens assumptions in current processing methods. Two methods of volumetric calibration based on a polynomial mapping function that does not require knowledge of specific lens parameters are presented and compared to a calibration based on thin-lens assumptions. The first method, volumetric dewarping, is executed by creation of a volumetric representation of a scene using the thin-lens assumptions, which is then corrected in post-processing using a polynomial mapping function. The second method, direct light-field calibration, uses the polynomial mapping in creationmoreÂ Â» of the initial volumetric representation to relate locations in object space directly to image sensor locations. The accuracy and feasibility of these methods is examined experimentally by capturing images of a known dot card at a variety of depths. Results suggest that use of a 3D polynomial mapping function provides a significant increase in reconstruction accuracy and that the achievable accuracy is similar using either polynomial-mapping-based method. Additionally, direct light-field calibration provides significant computational benefits by eliminating some intermediate processing steps found in other methods. Finally, the flexibility of this method is shown for a nonplanar calibration.Â«Â less\n\nEvaluating Statistical Process Control (SPC) techniques and computing the uncertainty of force calibrations\n\nNASA Technical Reports Server (NTRS)\n\nNavard, Sharon E.\n\n1989-01-01\n\nIn recent years there has been a push within NASA to use statistical techniques to improve the quality of production. Two areas where statistics are used are in establishing product and process quality control of flight hardware and in evaluating the uncertainty of calibration of instruments. The Flight Systems Quality Engineering branch is responsible for developing and assuring the quality of all flight hardware; the statistical process control methods employed are reviewed and evaluated. The Measurement Standards and Calibration Laboratory performs the calibration of all instruments used on-site at JSC as well as those used by all off-site contractors. These calibrations must be performed in such a way as to be traceable to national standards maintained by the National Institute of Standards and Technology, and they must meet a four-to-one ratio of the instrument specifications to calibrating standard uncertainty. In some instances this ratio is not met, and in these cases it is desirable to compute the exact uncertainty of the calibration and determine ways of reducing it. A particular example where this problem is encountered is with a machine which does automatic calibrations of force. The process of force calibration using the United Force Machine is described in detail. The sources of error are identified and quantified when possible. Suggestions for improvement are made.\n\nMethods for Calibration of Prout-Tompkins Kinetics Parameters Using EZM Iteration and GLO\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nWemhoff, A P; Burnham, A K; de Supinski, B\n\n2006-11-07\n\nThis document contains information regarding the standard procedures used to calibrate chemical kinetics parameters for the extended Prout-Tompkins model to match experimental data. Two methods for calibration are mentioned: EZM calibration and GLO calibration. EZM calibration matches kinetics parameters to three data points, while GLO calibration slightly adjusts kinetic parameters to match multiple points. Information is provided regarding the theoretical approach and application procedure for both of these calibration algorithms. It is recommended that for the calibration process, the user begin with EZM calibration to provide a good estimate, and then fine-tune the parameters using GLO. Two examples have beenmoreÂ Â» provided to guide the reader through a general calibrating process.Â«Â less\n\nCalibration and compensation method of three-axis geomagnetic sensor based on pre-processing total least square iteration\n\nNASA Astrophysics Data System (ADS)\n\nZhou, Y.; Zhang, X.; Xiao, W.\n\n2018-04-01\n\nAs the geomagnetic sensor is susceptible to interference, a pre-processing total least square iteration method is proposed for calibration compensation. Firstly, the error model of the geomagnetic sensor is analyzed and the correction model is proposed, then the characteristics of the model are analyzed and converted into nine parameters. The geomagnetic data is processed by Hilbert transform (HHT) to improve the signal-to-noise ratio, and the nine parameters are calculated by using the combination of Newton iteration method and the least squares estimation method. The sifter algorithm is used to filter the initial value of the iteration to ensure that the initial error is as small as possible. The experimental results show that this method does not need additional equipment and devices, can continuously update the calibration parameters, and better than the two-step estimation method, it can compensate geomagnetic sensor error well.\n\nUtilization of Expert Knowledge in a Multi-Objective Hydrologic Model Automatic Calibration Process\n\nNASA Astrophysics Data System (ADS)\n\nQuebbeman, J.; Park, G. H.; Carney, S.; Day, G. N.; Micheletty, P. D.\n\n2016-12-01\n\nSpatially distributed continuous simulation hydrologic models have a large number of parameters for potential adjustment during the calibration process. Traditional manual calibration approaches of such a modeling system is extremely laborious, which has historically motivated the use of automatic calibration procedures. With a large selection of model parameters, achieving high degrees of objective space fitness - measured with typical metrics such as Nash-Sutcliffe, Kling-Gupta, RMSE, etc. - can easily be achieved using a range of evolutionary algorithms. A concern with this approach is the high degree of compensatory calibration, with many similarly performing solutions, and yet grossly varying parameter set solutions. To help alleviate this concern, and mimic manual calibration processes, expert knowledge is proposed for inclusion within the multi-objective functions, which evaluates the parameter decision space. As a result, Pareto solutions are identified with high degrees of fitness, but also create parameter sets that maintain and utilize available expert knowledge resulting in more realistic and consistent solutions. This process was tested using the joint SNOW-17 and Sacramento Soil Moisture Accounting method (SAC-SMA) within the Animas River basin in Colorado. Three different elevation zones, each with a range of parameters, resulted in over 35 model parameters simultaneously calibrated. As a result, high degrees of fitness were achieved, in addition to the development of more realistic and consistent parameter sets such as those typically achieved during manual calibration procedures.\n\nCalibration of stereo rigs based on the backward projection process\n\nNASA Astrophysics Data System (ADS)\n\nGu, Feifei; Zhao, Hong; Ma, Yueyang; Bu, Penghui; Zhao, Zixin\n\n2016-08-01\n\nHigh-accuracy 3D measurement based on binocular vision system is heavily dependent on the accurate calibration of two rigidly-fixed cameras. In most traditional calibration methods, stereo parameters are iteratively optimized through the forward imaging process (FIP). However, the results can only guarantee the minimal 2D pixel errors, but not the minimal 3D reconstruction errors. To address this problem, a simple method to calibrate a stereo rig based on the backward projection process (BPP) is proposed. The position of a spatial point can be determined separately from each camera by planar constraints provided by the planar pattern target. Then combined with pre-defined spatial points, intrinsic and extrinsic parameters of the stereo-rig can be optimized by minimizing the total 3D errors of both left and right cameras. An extensive performance study for the method in the presence of image noise and lens distortions is implemented. Experiments conducted on synthetic and real data demonstrate the accuracy and robustness of the proposed method.\n\nDetermination of calibration constants for the hole-drilling residual stress measurement technique applied to orthotropic composites. II - Experimental evaluations\n\nNASA Technical Reports Server (NTRS)\n\nPrasad, C. B.; Prabhakaran, R.; Tompkins, S.\n\n1987-01-01\n\nThe first step in the extension of the semidestructive hole-drilling technique for residual stress measurement to orthotropic composite materials is the determination of the three calibration constants. Attention is presently given to an experimental determination of these calibration constants for a highly orthotropic, unidirectionally-reinforced graphite fiber-reinforced polyimide composite. A comparison of the measured values with theoretically obtained ones shows agreement to be good, in view of the many possible sources of experimental variation.\n\nA Dynamic Calibration Method for Experimental and Analytical Hub Load Comparison\n\nNASA Technical Reports Server (NTRS)\n\nKreshock, Andrew R.; Thornburgh, Robert P.; Wilbur, Matthew L.\n\n2017-01-01\n\nThis paper presents the results from an ongoing effort to produce improved correlation between analytical hub force and moment prediction and those measured during wind-tunnel testing on the Aeroelastic Rotor Experimental System (ARES), a conventional rotor testbed commonly used at the Langley Transonic Dynamics Tunnel (TDT). A frequency-dependent transformation between loads at the rotor hub and outputs of the testbed balance is produced from frequency response functions measured during vibration testing of the system. The resulting transformation is used as a dynamic calibration of the balance to transform hub loads predicted by comprehensive analysis into predicted balance outputs. In addition to detailing the transformation process, this paper also presents a set of wind-tunnel test cases, with comparisons between the measured balance outputs and transformed predictions from the comprehensive analysis code CAMRAD II. The modal response of the testbed is discussed and compared to a detailed finite-element model. Results reveal that the modal response of the testbed exhibits a number of characteristics that make accurate dynamic balance predictions challenging, even with the use of the balance transformation.\n\nParameter Calibration of GTN Damage Model and Formability Analysis of 22MnB5 in Hot Forming Process\n\nNASA Astrophysics Data System (ADS)\n\nYing, Liang; Liu, Wenquan; Wang, Dantong; Hu, Ping\n\n2017-11-01\n\nHot forming of high strength steel at elevated temperatures is an attractive technology to achieve the lightweight of vehicle body. The mechanical behavior of boron steel 22MnB5 strongly depends on the variation of temperature which makes the process design more difficult. In this paper, the Gurson-Tvergaard-Needleman (GTN) model is used to study the formability of 22MnB5 sheet at different temperatures. Firstly, the rheological behavior of 22MnB5 is analyzed through a series of hot tensile tests at a temperature range of 600-800 Â°C. Then, a detailed process to calibrate the damage parameters is given based on the response surface methodology and genetic algorithm method. The GTN model together with the damage parameters calibrated is then implemented to simulate the deformation and damage evolution of 22MnB5 in the process of high-temperature Nakazima test. The capability of the GTN model as a suitable tool to evaluate the sheet formability is confirmed by comparing experimental and calculated results. Finally, as a practical application, the forming limit diagram of 22MnB5 at 700 Â°C is constructed using the Nakazima simulation and Marciniak-Kuczynski (M-K) model, respectively. And the simulation integrated GTN model shows a higher reliability by comparing the predicted results of these two approaches with the experimental ones.\n\nSingle-Vector Calibration of Wind-Tunnel Force Balances\n\nNASA Technical Reports Server (NTRS)\n\nParker, P. A.; DeLoach, R.\n\n2003-01-01\n\nAn improved method of calibrating a wind-tunnel force balance involves the use of a unique load application system integrated with formal experimental design methodology. The Single-Vector Force Balance Calibration System (SVS) overcomes the productivity and accuracy limitations of prior calibration methods. A force balance is a complex structural spring element instrumented with strain gauges for measuring three orthogonal components of aerodynamic force (normal, axial, and side force) and three orthogonal components of aerodynamic torque (rolling, pitching, and yawing moments). Force balances remain as the state-of-the-art instrument that provide these measurements on a scale model of an aircraft during wind tunnel testing. Ideally, each electrical channel of the balance would respond only to its respective component of load, and it would have no response to other components of load. This is not entirely possible even though balance designs are optimized to minimize these undesirable interaction effects. Ultimately, a calibration experiment is performed to obtain the necessary data to generate a mathematical model and determine the force measurement accuracy. In order to set the independent variables of applied load for the calibration 24 NASA Tech Briefs, October 2003 experiment, a high-precision mechanical system is required. Manual deadweight systems have been in use at Langley Research Center (LaRC) since the 1940s. These simple methodologies produce high confidence results, but the process is mechanically complex and labor-intensive, requiring three to four weeks to complete. Over the past decade, automated balance calibration systems have been developed. In general, these systems were designed to automate the tedious manual calibration process resulting in an even more complex system which deteriorates load application quality. The current calibration approach relies on a one-factor-at-a-time (OFAT) methodology, where each independent variable is\n\nApplication of six sigma and AHP in analysis of variable lead time calibration process instrumentation\n\nNASA Astrophysics Data System (ADS)\n\nRimantho, Dino; Rahman, Tomy Abdul; Cahyadi, Bambang; Tina Hernawati, S.\n\n2017-02-01\n\nCalibration of instrumentation equipment in the pharmaceutical industry is an important activity to determine the true value of a measurement. Preliminary studies indicated that occur lead-time calibration resulted in disruption of production and laboratory activities. This study aimed to analyze the causes of lead-time calibration. Several methods used in this study such as, Six Sigma in order to determine the capability process of the calibration instrumentation of equipment. Furthermore, the method of brainstorming, Pareto diagrams, and Fishbone diagrams were used to identify and analyze the problems. Then, the method of Hierarchy Analytical Process (AHP) was used to create a hierarchical structure and prioritize problems. The results showed that the value of DPMO around 40769.23 which was equivalent to the level of sigma in calibration equipment approximately 3,24Ï. This indicated the need for improvements in the calibration process. Furthermore, the determination of problem-solving strategies Lead Time Calibration such as, shortens the schedule preventive maintenance, increase the number of instrument Calibrators, and train personnel. Test results on the consistency of the whole matrix of pairwise comparisons and consistency test showed the value of hierarchy the CR below 0.1.\n\nLaser's calibration of an AOTF-based spectral colorimeter\n\nNASA Astrophysics Data System (ADS)\n\nEmelianov, Sergey P.; Khrustalev, Vladimir N.; Kochin, Leonid B.; Polosin, Lev L.\n\n2003-06-01\n\nThe paper is devoted to expedients of AOTF spectral colorimeters calibration. The spectrometer method of color values measuring with reference to spectral colorimeters on AOTF surveyed. The theoretical exposition of spectrometer data processing expedients is offered. The justified source of radiation choice, suitable for calibration of spectral colorimeters is carried out. The experimental results for different acousto-optical mediums and modes of interaction are submitted.\n\nVariability in Students' Evaluating Processes in Peer Assessment with Calibrated Peer Review\n\nERIC Educational Resources Information Center\n\nRussell, J.; Van Horne, S.; Ward, A. S.; Bettis, E. A., III; Gikonyo, J.\n\n2017-01-01\n\nThis study investigated students' evaluating process and their perceptions of peer assessment when they engaged in peer assessment using Calibrated Peer Review. Calibrated Peer Review is a web-based application that facilitates peer assessment of writing. One hundred and thirty-two students in an introductory environmental science courseâ¦\n\nPrecise calibration of few-cycle laser pulses with atomic hydrogen\n\nNASA Astrophysics Data System (ADS)\n\nWallace, W. C.; Kielpinski, D.; Litvinyuk, I. V.; Sang, R. T.\n\n2017-12-01\n\nInteraction of atoms and molecules with strong electric fields is a fundamental process in many fields of research, particularly in the emerging field of attosecond science. Therefore, understanding the physics underpinning those interactions is of significant interest to the scientific community. One crucial step in this understanding is accurate knowledge of the few-cycle laser field driving the process. Atomic hydrogen (H), the simplest of all atomic species, plays a key role in benchmarking strong-field processes. Its wide-spread use as a testbed for theoretical calculations allows the comparison of approximate theoretical models against nearly-perfect numerical solutions of the three-dimensional time-dependent SchrÃ¶dinger equation. Until recently, relatively little experimental data in atomic H was available for comparison to these models, and was due mostly due to the difficulty in the construction and use of atomic H sources. Here, we review our most recent experimental results from atomic H interaction with few-cycle laser pulses and how they have been used to calibrate important laser pulse parameters such as peak intensity and the carrier-envelope phase (CEP). Quantitative agreement between experimental data and theoretical predictions for atomic H has been obtained at the 10% uncertainty level, allowing for accurate laser calibration intensity at the 1% level. Using this calibration in atomic H, both accurate CEP data and an intensity calibration standard have been obtained Ar, Kr, and Xe; such gases are in common use for strong-field experiments. This calibration standard can be used by any laboratory using few-cycle pulses in the 1014 W cm-2 intensity regime centered at 800 nm wavelength to accurately calibrate their peak laser intensity to within few-percent precision.\n\nCalibration of mass spectrometric peptide mass fingerprint data without specific external or internal calibrants\n\nPubMed Central\n\nWolski, Witold E; Lalowski, Maciej; Jungblut, Peter; Reinert, Knut\n\n2005-01-01\n\nBackground Peptide Mass Fingerprinting (PMF) is a widely used mass spectrometry (MS) method of analysis of proteins and peptides. It relies on the comparison between experimentally determined and theoretical mass spectra. The PMF process requires calibration, usually performed with external or internal calibrants of known molecular masses. Results We have introduced two novel MS calibration methods. The first method utilises the local similarity of peptide maps generated after separation of complex protein samples by two-dimensional gel electrophoresis. It computes a multiple peak-list alignment of the data set using a modified Minimum Spanning Tree (MST) algorithm. The second method exploits the idea that hundreds of MS samples are measured in parallel on one sample support. It improves the calibration coefficients by applying a two-dimensional Thin Plate Splines (TPS) smoothing algorithm. We studied the novel calibration methods utilising data generated by three different MALDI-TOF-MS instruments. We demonstrate that a PMF data set can be calibrated without resorting to external or relying on widely occurring internal calibrants. The methods developed here were implemented in R and are part of the BioConductor package mscalib available from . Conclusion The MST calibration algorithm is well suited to calibrate MS spectra of protein samples resulting from two-dimensional gel electrophoretic separation. The TPS based calibration algorithm might be used to correct systematic mass measurement errors observed for large MS sample supports. As compared to other methods, our combined MS spectra calibration strategy increases the peptide/protein identification rate by an additional 5 â 15%. PMID:16102175\n\nCalibration for single multi-mode fiber digital scanning microscopy imaging system\n\nNASA Astrophysics Data System (ADS)\n\nYin, Zhe; Liu, Guodong; Liu, Bingguo; Gan, Yu; Zhuang, Zhitao; Chen, Fengdong\n\n2015-11-01\n\nSingle multimode fiber (MMF) digital scanning imaging system is a development tendency of modern endoscope. We concentrate on the calibration method of the imaging system. Calibration method comprises two processes, forming scanning focused spots and calibrating the couple factors varied with positions. Adaptive parallel coordinate algorithm (APC) is adopted to form the focused spots at the multimode fiber (MMF) output. Compare with other algorithm, APC contains many merits, i.e. rapid speed, small amount calculations and no iterations. The ratio of the optics power captured by MMF to the intensity of the focused spots is called couple factor. We setup the calibration experimental system to form the scanning focused spots and calculate the couple factors for different object positions. The experimental result the couple factor is higher in the center than the edge.\n\nMultimodal Spatial Calibration for Accurately Registering EEG Sensor Positions\n\nPubMed Central\n\nChen, Shengyong; Xiao, Gang; Li, Xiaoli\n\n2014-01-01\n\nThis paper proposes a fast and accurate calibration method to calibrate multiple multimodal sensors using a novel photogrammetry system for fast localization of EEG sensors. The EEG sensors are placed on human head and multimodal sensors are installed around the head to simultaneously obtain all EEG sensor positions. A multiple views' calibration process is implemented to obtain the transformations of multiple views. We first develop an efficient local repair algorithm to improve the depth map, and then a special calibration body is designed. Based on them, accurate and robust calibration results can be achieved. We evaluate the proposed method by corners of a chessboard calibration plate. Experimental results demonstrate that the proposed method can achieve good performance, which can be further applied to EEG source localization applications on human brain. PMID:24803954\n\nAlignment of the measurement scale mark during immersion hydrometer calibration using an image processing system.\n\nPubMed\n\nPeÃ±a-Perez, Luis Manuel; Pedraza-Ortega, Jesus Carlos; Ramos-Arreguin, Juan Manuel; Arriaga, Saul Tovar; Fernandez, Marco Antonio Aceves; Becerra, Luis Omar; Hurtado, Efren Gorrostieta; Vargas-Soto, Jose Emilio\n\n2013-10-24\n\nThe present work presents an improved method to align the measurement scale mark in an immersion hydrometer calibration system of CENAM, the National Metrology Institute (NMI) of Mexico, The proposed method uses a vision system to align the scale mark of the hydrometer to the surface of the liquid where it is immersed by implementing image processing algorithms. This approach reduces the variability in the apparent mass determination during the hydrostatic weighing in the calibration process, therefore decreasing the relative uncertainty of calibration.\n\nHow does higher frequency monitoring data affect the calibration of a process-based water quality model?\n\nNASA Astrophysics Data System (ADS)\n\nJackson-Blake, Leah; Helliwell, Rachel\n\n2015-04-01\n\nProcess-based catchment water quality models are increasingly used as tools to inform land management. However, for such models to be reliable they need to be well calibrated and shown to reproduce key catchment processes. Calibration can be challenging for process-based models, which tend to be complex and highly parameterised. Calibrating a large number of parameters generally requires a large amount of monitoring data, spanning all hydrochemical conditions. However, regulatory agencies and research organisations generally only sample at a fortnightly or monthly frequency, even in well-studied catchments, often missing peak flow events. The primary aim of this study was therefore to investigate how the quality and uncertainty of model simulations produced by a process-based, semi-distributed catchment model, INCA-P (the INtegrated CAtchment model of Phosphorus dynamics), were improved by calibration to higher frequency water chemistry data. Two model calibrations were carried out for a small rural Scottish catchment: one using 18 months of daily total dissolved phosphorus (TDP) concentration data, another using a fortnightly dataset derived from the daily data. To aid comparability, calibrations were carried out automatically using the Markov Chain Monte Carlo - DiffeRential Evolution Adaptive Metropolis (MCMC-DREAM) algorithm. Calibration to daily data resulted in improved simulation of peak TDP concentrations and improved model performance statistics. Parameter-related uncertainty in simulated TDP was large when fortnightly data was used for calibration, with a 95% credible interval of 26 Î¼g/l. This uncertainty is comparable in size to the difference between Water Framework Directive (WFD) chemical status classes, and would therefore make it difficult to use this calibration to predict shifts in WFD status. The 95% credible interval reduced markedly with the higher frequency monitoring data, to 6 Î¼g/l. The number of parameters that could be reliably auto-calibrated\n\nSelf-Calibrated In-Process Photogrammetry for Large Raw Part Measurement and Alignment before Machining\n\nPubMed Central\n\nMendikute, Alberto; Zatarain, Mikel; Bertelsen, Ãlvaro; Leizea, Ibai\n\n2017-01-01\n\nPhotogrammetry methods are being used more and more as a 3D technique for large scale metrology applications in industry. Optical targets are placed on an object and images are taken around it, where measuring traceability is provided by precise off-process pre-calibrated digital cameras and scale bars. According to the 2D target image coordinates, target 3D coordinates and camera views are jointly computed. One of the applications of photogrammetry is the measurement of raw part surfaces prior to its machining. For this application, post-process bundle adjustment has usually been adopted for computing the 3D scene. With that approach, a high computation time is observed, leading in practice to time consuming and user dependent iterative review and re-processing procedures until an adequate set of images is taken, limiting its potential for fast, easy-to-use, and precise measurements. In this paper, a new efficient procedure is presented for solving the bundle adjustment problem in portable photogrammetry. In-process bundle computing capability is demonstrated on a consumer grade desktop PC, enabling quasi real time 2D image and 3D scene computing. Additionally, a method for the self-calibration of camera and lens distortion has been integrated into the in-process approach due to its potential for highest precision when using low cost non-specialized digital cameras. Measurement traceability is set only by scale bars available in the measuring scene, avoiding the uncertainty contribution of off-process camera calibration procedures or the use of special purpose calibration artifacts. The developed self-calibrated in-process photogrammetry has been evaluated both in a pilot case scenario and in industrial scenarios for raw part measurement, showing a total in-process computing time typically below 1 s per image up to a maximum of 2 s during the last stages of the computed industrial scenes, along with a relative precision of 1/10,000 (e.g., 0.1 mm error in 1 m) with\n\nSelf-Calibrated In-Process Photogrammetry for Large Raw Part Measurement and Alignment before Machining.\n\nPubMed\n\nMendikute, Alberto; YagÃ¼e-Fabra, JosÃ© A; Zatarain, Mikel; Bertelsen, Ãlvaro; Leizea, Ibai\n\n2017-09-09\n\nPhotogrammetry methods are being used more and more as a 3D technique for large scale metrology applications in industry. Optical targets are placed on an object and images are taken around it, where measuring traceability is provided by precise off-process pre-calibrated digital cameras and scale bars. According to the 2D target image coordinates, target 3D coordinates and camera views are jointly computed. One of the applications of photogrammetry is the measurement of raw part surfaces prior to its machining. For this application, post-process bundle adjustment has usually been adopted for computing the 3D scene. With that approach, a high computation time is observed, leading in practice to time consuming and user dependent iterative review and re-processing procedures until an adequate set of images is taken, limiting its potential for fast, easy-to-use, and precise measurements. In this paper, a new efficient procedure is presented for solving the bundle adjustment problem in portable photogrammetry. In-process bundle computing capability is demonstrated on a consumer grade desktop PC, enabling quasi real time 2D image and 3D scene computing. Additionally, a method for the self-calibration of camera and lens distortion has been integrated into the in-process approach due to its potential for highest precision when using low cost non-specialized digital cameras. Measurement traceability is set only by scale bars available in the measuring scene, avoiding the uncertainty contribution of off-process camera calibration procedures or the use of special purpose calibration artifacts. The developed self-calibrated in-process photogrammetry has been evaluated both in a pilot case scenario and in industrial scenarios for raw part measurement, showing a total in-process computing time typically below 1 s per image up to a maximum of 2 s during the last stages of the computed industrial scenes, along with a relative precision of 1/10,000 (e.g. 0.1 mm error in 1 m) with\n\nAutonomous calibration of single spin qubit operations\n\nNASA Astrophysics Data System (ADS)\n\nFrank, Florian; Unden, Thomas; Zoller, Jonathan; Said, Ressa S.; Calarco, Tommaso; Montangero, Simone; Naydenov, Boris; Jelezko, Fedor\n\n2017-12-01\n\nFully autonomous precise control of qubits is crucial for quantum information processing, quantum communication, and quantum sensing applications. It requires minimal human intervention on the ability to model, to predict, and to anticipate the quantum dynamics, as well as to precisely control and calibrate single qubit operations. Here, we demonstrate single qubit autonomous calibrations via closed-loop optimisations of electron spin quantum operations in diamond. The operations are examined by quantum state and process tomographic measurements at room temperature, and their performances against systematic errors are iteratively rectified by an optimal pulse engineering algorithm. We achieve an autonomous calibrated fidelity up to 1.00 on a time scale of minutes for a spin population inversion and up to 0.98 on a time scale of hours for a single qubit Ï/2 -rotation within the experimental error of 2%. These results manifest a full potential for versatile quantum technologies.\n\nSHORT COMMUNICATION: An image processing approach to calibration of hydrometers\n\nNASA Astrophysics Data System (ADS)\n\nLorefice, S.; Malengo, A.\n\n2004-06-01\n\nThe usual method adopted for multipoint calibration of glass hydrometers is based on the measurement of the buoyancy by hydrostatic weighing when the hydrometer is plunged in a reference liquid up to the scale mark to be calibrated. An image processing approach is proposed by the authors to align the relevant scale mark with the reference liquid surface level. The method uses image analysis with a data processing technique and takes into account the perspective error. For this purpose a CCD camera with a pixel matrix of 604H Ã 576V and a lens of 16 mm focal length were used. High accuracy in the hydrometer reading was obtained as the resulting reading uncertainty was lower than 0.02 mm, about a fifth of the usual figure with the visual reading made by an operator.\n\nAlignment of the Measurement Scale Mark during Immersion Hydrometer Calibration Using an Image Processing System\n\nPubMed Central\n\nPeÃ±a-Perez, Luis Manuel; Pedraza-Ortega, Jesus Carlos; Ramos-Arreguin, Juan Manuel; Arriaga, Saul Tovar; Fernandez, Marco Antonio Aceves; Becerra, Luis Omar; Hurtado, Efren Gorrostieta; Vargas-Soto, Jose Emilio\n\n2013-01-01\n\nThe present work presents an improved method to align the measurement scale mark in an immersion hydrometer calibration system of CENAM, the National Metrology Institute (NMI) of Mexico, The proposed method uses a vision system to align the scale mark of the hydrometer to the surface of the liquid where it is immersed by implementing image processing algorithms. This approach reduces the variability in the apparent mass determination during the hydrostatic weighing in the calibration process, therefore decreasing the relative uncertainty of calibration. PMID:24284770\n\nDevelopment of a new calibration procedure and its experimental validation applied to a human motion capture system.\n\nPubMed\n\nRoyo SÃ¡nchez, Ana Cristina; Aguilar MartÃ­n, Juan JosÃ©; Santolaria Mazo, Jorge\n\n2014-12-01\n\nMotion capture systems are often used for checking and analyzing human motion in biomechanical applications. It is important, in this context, that the systems provide the best possible accuracy. Among existing capture systems, optical systems are those with the highest accuracy. In this paper, the development of a new calibration procedure for optical human motion capture systems is presented. The performance and effectiveness of that new calibration procedure are also checked by experimental validation. The new calibration procedure consists of two stages. In the first stage, initial estimators of intrinsic and extrinsic parameters are sought. The camera calibration method used in this stage is the one proposed by Tsai. These parameters are determined from the camera characteristics, the spatial position of the camera, and the center of the capture volume. In the second stage, a simultaneous nonlinear optimization of all parameters is performed to identify the optimal values, which minimize the objective function. The objective function, in this case, minimizes two errors. The first error is the distance error between two markers placed in a wand. The second error is the error of position and orientation of the retroreflective markers of a static calibration object. The real co-ordinates of the two objects are calibrated in a co-ordinate measuring machine (CMM). The OrthoBio system is used to validate the new calibration procedure. Results are 90% lower than those from the previous calibration software and broadly comparable with results from a similarly configured Vicon system.\n\nMantle temperatures, and tests of experimentally calibrated olivine-melt equilibria\n\nNASA Astrophysics Data System (ADS)\n\nPutirka, K. D.\n\n2005-12-01\n\nBecause the ratio Mgol/Mgliq (Kd(Mg)) is sensitive to T, olivine-liquid Kd's have long been used as geothermometers, and more recently, maximum Fo contents from volcanic rocks have been used to estimate mantle potential temperatures. Such estimates by Putirka (2005, G3) indicate higher mantle equilibration temperatures at Hawaii, compared to temperatures derived from earlier calibrations. Several published models were thus tested for their ability to reproduce T for 862 experimental data. The Putirka (2005) models did not include P corrections, which are added here: lnKd(Mg)=-1.88 + 30.85P(GPa)/T(C) - 0.04[H2O]liq + 0.068[Na2O+K2O]liq + 3629.7/T(C) + 0.0087[SiO2]liq - 0.015[CaO]liq lnKd(Fe)= -2.92 - 0.05[H2O]liq + 0.0264[Na2O+K2O]liq + 2976.13/T(C) + 0.01847[SiO2]liq + 0.0171[Al2O3]liq - 0.039[CaO]liq + 33.17P(GPa)/T(C) In these expressions, Kd(Mg) and Kd(Fe) are the partition coefficients for Mg and Fe between olivine and liquid, expressed as cation fractions; compositional corrections are in weight percent. The models are calibrated from 785 experimental data (P = 0.0001-15.5 GPa; 1213-2353 K). In the tests, the expressions of Beattie (1993) performed exceptionally well for dry systems with MgOliq < 17 wt. %, with a standard error of estimate of 35 K, compared to an SEE of 59 K for Ford et al. (1983) and an SEE of 51 K for the inversion of the Kd(Mg) model above. Recalibration of the Beattie (1993) model over this composition range thus appears unnecessary. But Beattie (1993), Ford et al. (1983), and other models overestimate T for hydrous systems, and for compositions with MgOliq > 17 wt. %; new models are therefore needed. Over the greater compositional range, model 1 above can be inverted to yield T with a SEE of 56 K, and an average mean (systematic) error of +3 K for 856 experimental data; this compares to a systematic error of -26 K for Beattie (1993) and -36 K for Ford et al. (1983). For use in equation (1) of Putirka (2005), the models above are also more\n\nThe influence of the spectral emissivity of flat-plate calibrators on the calibration of IR thermometers\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nCÃ¡rdenas-GarcÃ­a, D.; MÃ©ndez-Lango, E.\n\nFlat Calibrators (FC) are an option for calibration of infrared thermometers (IT) with a fixed large target. FCs are neither blackbodies, nor gray-bodies; their spectral emissivity is lower than one and depends on wavelength. Nevertheless they are used as gray-bodies with a nominal emissivity value. FCs can be calibrated radiometrically using as reference a calibrated IR thermometer (RT). If an FC will be used to calibrate ITs that work in the same spectral range as the RT then its calibration is straightforward: the actual FC spectral emissivity is not required. This result is valid for any given fixed emissivity assessedmoreÂ Â» to the FC. On the other hand, when the RT working spectral range does not match with that of the ITs to be calibrated with the FC then it is required to know the FC spectral emissivity as part of the calibration process. For this purpose, at CENAM, we developed an experimental setup to measure spectral emissivity in the infrared spectral range, based on a Fourier transform infrared spectrometer. Not all laboratories have emissivity measurement capability in the appropriate wavelength and temperature ranges to obtain the spectral emissivity. Thus, we present an estimation of the error introduced when the spectral range of the RT used to calibrate an FC and the spectral ranges of the ITs to be calibrated with the FC do not match. Some examples are developed for the cases when RT and IT spectral ranges are [8,13] Î¼m and [8,14] Î¼m respectively.Â«Â less\n\nOptics-Only Calibration of a Neural-Net Based Optical NDE Method for Structural Health Monitoring\n\nNASA Technical Reports Server (NTRS)\n\nDecker, Arthur J.\n\n2004-01-01\n\nA calibration process is presented that uses optical measurements alone to calibrate a neural-net based NDE method. The method itself detects small changes in the vibration mode shapes of structures. The optics-only calibration process confirms previous work that the sensitivity to vibration-amplitude changes can be as small as 10 nanometers. A more practical value in an NDE service laboratory is shown to be 50 nanometers. Both model-generated and experimental calibrations are demonstrated using two implementations of the calibration technique. The implementations are based on previously published demonstrations of the NDE method and an alternative calibration procedure that depends on comparing neural-net and point sensor measurements. The optics-only calibration method, unlike the alternative method, does not require modifications of the structure being tested or the creation of calibration objects. The calibration process can be used to test improvements in the NDE process and to develop a vibration-mode-independence of damagedetection sensitivity. The calibration effort was intended to support NASA s objective to promote safety in the operations of ground test facilities or aviation safety, in general, by allowing the detection of the gradual onset of structural changes and damage.\n\nSAR c"
    }
}
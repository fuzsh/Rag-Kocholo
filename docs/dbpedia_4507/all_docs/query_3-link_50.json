{
    "id": "dbpedia_4507_3",
    "rank": 50,
    "data": {
        "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6308681/",
        "read_more_link": "",
        "language": "en",
        "title": "Device for Acoustic Support of Orientation in the Surroundings for Blind People †",
        "top_image": "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/pmc-card-share.jpg?_=0",
        "meta_img": "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/pmc-card-share.jpg?_=0",
        "images": [
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/favicons/favicon-57.png",
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/icon-dot-gov.svg",
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/icon-https.svg",
            "https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/logos/AgencyLogo.svg",
            "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/logo-sensors.png",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6308681/bin/sensors-18-04309-g001.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6308681/bin/sensors-18-04309-g002.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6308681/bin/sensors-18-04309-g003.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6308681/bin/sensors-18-04309-g004.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6308681/bin/sensors-18-04309-g005.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6308681/bin/sensors-18-04309-g006.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6308681/bin/sensors-18-04309-g007.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Mariusz Kubanek",
            "Janusz Bobulski"
        ],
        "publish_date": "2018-12-14T00:00:00",
        "summary": "",
        "meta_description": "The constant development of modern technologies allows the creation of new and, above all, mobile devices supporting people with disabilities. All work carried out to improve the lives of people with disabilities is an important element of the field of ...",
        "meta_lang": "en",
        "meta_favicon": "https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon.ico",
        "meta_site_name": "PubMed Central (PMC)",
        "canonical_link": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6308681/",
        "text": "2.2. Overview of Hardware and Software Supporting\n\nThere are several sorts of devices and software supportive the blind. These include braille printers, Braille watches or with the option of sound hour reading, ordinary white sticks, software overlays for the blind or independent programs—for instance speech synthesizers for text reading or specific versions of GPS software [7]. A particular set of assistive devices blind people are aids orientation in the environment. These are especially intended for the blind GPS device, but also obstacle and objects detectors or imaging devices.\n\nThe white stick was probably the first device used by blind people to study the surroundings. The very idea of a cane is much earlier than its white color. It was only later that the white color was used to make the man more visible to other persons. The color itself has also become a symbol informing that the person using the cane is blind [7].\n\nUltraCane device has the form of a white stick, but additionally its handle is armed with ultrasonic detectors of obstacles. The user is informed about the detection of an obstacle (in the direction that he points) through two vibrating elements of the button-shaped handle [8].\n\n‘K’ Sonar is a minor device that the user holds in his hand. The device can be additionally mounted on the handle of a normal white stick, which increases the stability and lets the simultaneous use of the white stick and the device. In this combination, the user should hold the set directly behind the device handle, not sticks. ‘K’ Sonar uses an ultrasonic detector; however, the method of communication with the user and the type of information provided about objects are different than those used in UltraCane [9].\n\nThe vOICe is a group of programs that allows anyone interested to submit blind people for imaging device. To make the device you need a smartphone or computer, headphones and a low resolution web camera [10].\n\nBrainPort® V100 is alternative device that “transmitting” the image to a blind person not by means of sounds or vibrations, but using a series of electrodes located on the tongue. Even though device testing has been going on for many years, it was released for use and sale in the United States on 19 June 2015. Currently available in the USA and the EU [11].\n\nThe Kinect sensor has also been used in a non-invasive apnea detection system and breath rate observation using real-time image sequence analysis recorded in any sleep position and in all lighting conditions (even in the dark). A Microsoft Kinect sensor was used to visualize changes in the chest and abdomen from the respiratory rhythm [12].\n\nIn paper [13], the authors propose an idea similar to ours for helping the visually impaired in recognizing things in the everyday environment. This conception is implemented as a portable device consisting of a Microsoft Kinect sensor, a white stick, a tactile feedback device, and other elements. Using the Kinect sensor, the system searches for an object that the user asks the system to find, and then returns the search result to the user using the tactile feedback device.\n\nIn addition to the aforementioned methods using the kinect sensor, there are a number of other systems that use other sensors such as a smartphone [14] and a laser [15] that use other obstacle detection methods.\n\nIn the project Espacio Acustico Virtual [16], a similar method of image depth analysis was used; however, two small cameras were used for image acquisition. From these devices a stereo image is obtained and then it creates a depth of space, and subsequently generates appropriate sounds.\n\nIn the last years, various research works have addressed such challenges in their attempt to gain a higher level of functioning in the environment and to simplify the navigation of the blind people. The state of the art offers a wide range of electronic wearable device and systems, designed to provide a sensorial substitution to the human vision [17,18,19,20,21,22].\n\n3.1. Time of Flight Sensors\n\nThe Time of Flight (ToF) sensors send a light pulse and measure the time from sending this pulse to returning its reflected rays to the points of the photosensitive array. Return times from different points give a depth map. This is a general idea of the operation of this type of sensors. To reduce interference from the environment and to make the measurements more discreet, the light pulse is emitted in the infrared. In practice, three measurement methods are distinguished: the direct measurement method, the phase shift measurement method and the gated measurement method [11,23].\n\nThe direct measurement method is closest to the general idea of ToF sensors. A light pulse (having a duration for instance 10 ns), is emitted, and since then the matrix is ready for return light. In digital circuits, waiting time is measured (separately for each pixel of the image). In points where the reflected light reaches the measurement is stopped [11].\n\nDistance calculations at each point of the matrix based on the flight time can be performed using a very easy equation:\n\nD=12Δt⋅c\n\n(1)\n\nwhere: D—measured distance [m]; Δt—trip time [s]; c—speed of light [m/s] in the measured center.\n\nIf two of the same type sensors are used in one area, at the same time, they can interfere with their operation. In practice, assuming that the duration of the pulse is very short, the pulse path is also relatively short, and the operating frequency is relatively low (e.g., 30 frames per second), there is little probability that the sensors will send impulses that will coincide with each other (unless they use a series of pulses per one frame—then the probability increases) [11].\n\nThe method of measuring the phase shift consists not so much in measuring the time in which the reflected light reaches the receiver but measuring the phase shift between the modulated signal emitted by the transmitter and the signal received by the detector [11,16]. Phase shift analysis allows you to determine the time of beam travel. The measuring range of such a sensor is related to the length of the modulated wave (the wavelength of the light is constant, close to the infrared, while the amplitude of this wave is modulated) [11].\n\nIn the case of the gated measurement method, a shutter is used which opens the sensor to receive a light pulse at a given time, and thus from a given distance [23]. This approach has an interesting feature—it allows you to capture the image from a selected distance, bypassing the interference that could degrade the quality of the image read. As an example, mist can be given here, which would cause an earlier reflection of the light pulse; however, the shutter is only open to more distant objects, so earlier reflections will not introduce such significant disturbances.\n\n3.2. Structured Light Sensors\n\nThe basic principle of operation of sensors using structural light is the projection of a known light pattern on the scanned space, reading a distorted pattern using a calibrated camera/camera and calculating the depth map based on distortions [11]. In practice, structural light sensors project the pattern using projectors or lasers. The projected structures are usually stripes, although there are also other patterns, for instance, the spot in the case of the Kinect sensor (which although it is not a typical structural light sensor can be classified in this group) [11,23]. Depending on the projection method (also hardware limitations) and needs, different types of light are used. In the case of “home” sets, often consisting of multimedia projectors, visible light is projected. This approach makes it difficult to apply the method when there is additional ambient light. Infrared light (e.g., 850 nm) is often used in sensors dedicated to such applications. Such light is not visible to human eyes and (in certain frequency ranges) it does not occur in such large intensities in the environment as visible light.\n\nIn the case of 3D scanners, which are used to scan stationary objects, the collection of information about the entire field of view can be carried out using multiple frames and a slowly moving laser line. Such a method is unacceptable in the case of a sensor that scans the depth map of the dynamic scene, which changes quickly, where the readings should be at least a few frames per second (for the needs of the prototype constructed and discussed in this work). The next step after the projection is to take a picture (frame) distorted by the spatial objects of the pattern. If the sensor uses infrared light, unnecessary components are removed by means of optical filters so that the ambient light does not introduce disturbances to useful information.\n\nThe captured frame is subjected to computer image analysis. The analysis deals with the detection of distorted shapes and the calculation of depth maps [11]. Due to the need for image analysis, structural light technology requires more computing power than ToF technology. A part of 3D scanners/sensors analyzes images and generates a depth map using a processor embedded in the device, which relieves the processor of the computer that uses the already prepared depth map, returned by the sensor. This type of mechanism was used in the Kinect sensor [16]. This allows the sensor to be used even by computers that do not have very high computing power.\n\nAn intercepted image (frame) is transferred to the computer image analysis. The analysis deals with the discovery of distorted shapes and calculating the depth map [13]. Since it is necessary to analyze images the technology of structural light requires more computing power than the ToF technology. Some 3D sensors analyze images and generate a depth map using the processor embedded in the device, which relieves the computer’s processor which uses the completed depth map, returned by the sensor. Such a mechanism was used in the Kinect sensor [23]. This allows the sensor to be used even by computers with low computational capability.\n\n4.1. Xbox Kinect v1\n\nAs mentioned before, Xbox Kinect is a 3D (but not only 3D) sensor classified as a structural light sensor because the basis of its operation is the projection of a known pattern (pattern of spots). Kinect is a Microsoft product, but it is based on PrimeSense sensor technology. It was created as a game controller. Due to mass production, it has become cheap and therefore available to many people. It quickly began to be used as a 3D scanner or a 3D sensor for robots. Details of the sensor’s operation are not publicly available, which is why most of the information about the sensor’s exact operation is speculation based mainly on patent documents. The device is equipped not only with a 3D sensor (i.e., a laser projector, infrared camera and optical system— ), it also has a standard RGB camera, accelerometer and microphone array. In addition, the base is equipped with a motor that drives the sensor inclining system [23].\n\nThe viewing angles of the sensor (depth camera and RGB camera) are 43° vertically and 57° horizontally. The vertical viewing angle was additionally “widened” by means of a mechanical sensor inclining system of 27° downwards and upwards. The system is not adapted to continuous operation, but only to periodically adjust the angle of inclination [23].\n\nThere is a special overlay—Nyko Zoom—that modifies the optics of the Kinect sensor, by means of which a wider horizontal (and probably vertical) viewing angle can be achieved.\n\nThe depth map returned by the sensor has dimensions of 640 by 480 pixels by default. The infrared sensor has a physical resolution of 1280 by 1024 pixels. The depth value (using the freenect library) is returned as an 11-bit number (it is possible to transmit 16-bit depth, but may cause bandwidth problems), where 2047 are distances beyond the sensor range or depth values that could not be read. The depth values of individual points of the field of vision are returned in distances measured not from the lens but from the plane perpendicular to the optical axis of the infrared camera lens ( ).\n\nDirect sunlight prevents the Kinect sensor from reading distances. Fragments of surfaces that are not directly exposed to sunlight can usually be correctly read (or read with defects). The Kinect sensor is therefore not the best device to be used outdoors, but for other reasons has been chosen as a prototype sensor. First of all, due to the low cost of the sensor, the available libraries allow you to use the sensor in a simple way in the Linux system and due to the relatively low power consumption (2.25 W). The device is powered by voltages 5 V (from the USB interface) and 12 V from an additional power supply.\n\n5.1. The Principle of Device Operation\n\nAs point out before, the device task is to provide a depth map read by the sensor. The map is transmitted to a user using stereo headphones and several sounds (tones with exact frequencies). Sound management is done with software installed and software written right on the Raspberry Pi. The computer links with the sensor via the USB port, and with the headphones using a built-in sound card and a 3.5 mm jack socket.\n\n5.1.1. Depth Values Returned by the Kinect Sensor\n\nThe depth value is given by the sensor as 11 bit non-negative number, which gives us values in the range 0–2047, but it is not quite so. In fact, the zero value does not occur. The sensor at the perpendicular position of the optical axis relative to the horizontal surface of the obstacle, “sees” it only at a shortest distance of 57 cm and returns the value 488. If the object surface is set at an angle it is likely to slightly reduce the distance. The supreme value (2047) occurs when the object was detected, but no reading correct distance. This means that the value of 2047 always occurs when the distance rate could not be retrieved. This take place when the object was too close, too far away, when the lighting conditions were unsatisfactory at some point, or when the light has been noised by ambient light.\n\nAs part of the prototype preparation, measurements of the values returned by the device were made and comparison to the real distance. The program based on the presentation from the OpenKinect website was written for the measurements. The program shows on the screen the distance value read by the sensor in the center of the area of view. For easier pointing, a view of the whole depth map captured was left. Using a measuring cup, the distance between the sensor and the obstacle was measured, which after reading and writing the value was moved away by a reasonable distance. Though the measurements were not very precise, but rather demonstrative, they were precise enough to create characteristics of the sensor. It later helped performing transforming functions that adjust the loudness of the sound to the returned distance (more detailed information on these functions is given below; the way of transmitting the depth map using sounds is given below). You can see that the characteristic is non-linear— blue line.\n\nYou can clearly see the high precision and large changes in raw data gather by the sensor with a small variations in distance if the object is nearby. At larger distances, changes in raw values are much minor. However, it is possible to practice such a wide measuring range for the reason that the sampling is stable. Weak precision at more distances is not an obstacle in the use of the sensor on which this article focuses.\n\nBecause the Mix_SetPosition (…) function of the SDL_mixer library as the volume/distance value takes the 8-bit parameter (so the max value is 255—silence). It was required, therefore, to process raw values into a functioning range and a slight more linear characteristic using appropriate functions.\n\npresents charts of functions converting raw distance values, 11-bit to 8-bit values. Different version of the transformation functions turn out possibility to adapt the device to a wide open space or a tight like rooms. Rather, changes in the transforming function (and use the most universal) should be avoided, due to the chance that the brain of the person using the device will get accustomed to one function and will be able to recognize correct distance (to make it easier to add a fixed reference sound) volume, described later in this chapter). The mechanism described is not certain but plausible. You can also try to make the selection of the transformation function dependent on some variable component, for instance, on whether the person is in the building or outside.\n\nThe chart labeled KinectPkt shows the raw values returned by the sensor. Orange lines limit the field of the graph in which the useful range of the actual distance is located and the distance between the range 0–255 is returned by the sensor.\n\nThe graph marked as KinectAdapted is a graph showing the values of the function relative to the actual distance values:\n\nd2(KinectPkt)=KinectPkt−4882.215\n\n(2)\n\nThe function scales only raw values so that they are within the desired range. It is suitable for very tight spaces (for which the Kinect sensor is not suitable).\n\nThe graph marked as GammaAdapted is a graph presenting the values of the function relative to the actual distance values:\n\nd3(KinectPkt)=(KinectPkt2048)3⋅36⋅256−1244.44\n\n(3)\n\nThis function is based on the function—Gamma, which is placed in the demonstrative source code of the libfreenect library. The function has been converted in such way that the returned values are within the preferred range.\n\nA graph marked Own1 shows the values of the function relative to the actual distance values:\n\nd4(KinectPkt)=2KinectPkt8036−1\n\n(4)\n\nIt is an empirical function. It is more appropriate for use in the open air than Function (2) or Function (3).\n\nThe chart labeled Own2 shows the values of the function relative to the actual distance values:\n\nd5(KinectPkt)=(KinectPkt2048)3⋅36⋅256−3004.1\n\n(5)\n\nIt is an empirical function. For the largest real distance to be read, it does not give you total silence. It also “skips” a part of the smallest distances detected by the sensor, in which the volume is maximum. Negative values must be set to zero (possible with conditional instructions).\n\nThe functions described above ought to be limited in the program from the top and the bottom by means of conditional instructions so that they do not exceed the range of 0–255. These functions are only proposals that do not close the pool of available functions, but only open it. The usability of the device depends on the transformation function. The key is to create a function that works well under different conditions and ensures the best possible hearing of even small differences in the distance in the entire available range.\n\n5.1.2. Transmission of the Depth Map with Sounds\n\nThe depth map is transmitted to the user using sounds—fixed-frequency tones. Higher volume means shorter distance to the obstacle. The loudness distribution depending on the distance provides the above-described transforming functions.\n\nThe entire depth map given back by the sensor (640 × 480 px) is split into smaller blocks, which in this work will be called small fields. The operation have to be made due to the fact that it would not be possible or very difficult to transmit such a large amount of information. An additional argument for the division of the image are quite often occurring areas in which the sensor could not be able read the distance and such situation would introduce a confusion in the signal and could build an incorrect map of depth in the user’s mind.\n\nThe number of small fields fixes the final resolution of the transmitted depth map. In the prototype made, the depth map forwarded is 5 columns in 6 rows (5 × 6 small fields). This gives 30 small fields ( ). In comparison with the resolution of 640 × 480 px, which gives 307,200 px is not much. There is a good chance to increase the resolution. High resolution has been used in The vOICe program, which by default transmits a map of brightness or depth in 320 × 64 dots resolution.\n\nIn the prototype made, the coordinates of the depth point conveyed point depend on the lateralization of the sound signal and the pitch. The depth map is transmitted in the form of a “scan” of five columns from left to right and from right to left (the information in these two “scans” is not the same—this is explained further). Each transmitted column lasts a certain length of time, 200 ms, allowing quite a good listening to the tones. There are 6 small fields in one column (6 rows of depth maps), but not all are transmitted in parallel.\n\nThe pitch of individual tones determines at what height there are small fields represented by them—the higher tone is a small field located higher, the lower tone is a small field below. The tones are played in parallel (3 tones, not 6), and the volume of each matches to the distance transmitted by the small field. The scan from left to right is responsible for passing the three upper lines (three higher notes). The scan from right to left is responsible for the transmission of the three bottom lines (three lower tones). A graphic explanation of the above descriptions is given in .\n\nEach deep depth map cycle starts with a short tone, always at the same volume, duration 300 ms and frequency 100 Hz, not included in the map. Its purpose is to provide reference volume when a person is once in a quieter, once louder environment.\n\nThe sound signal of the transmitted depth map is also appended with pauses for easier identification of particular phases of the message. In the transmission cycle, one can distinguish: phase 1 reference sound with a duration of 300 ms; phase 2 pause (silence) with a duration of 200 ms; phase 3 transmission of 5 columns from three upper rows with a duration of 200 ms (one column) times 5, or 1000 ms; phase 4 pause (silence) with a duration of 200 ms; phase 5 transmission of 5 columns from the three bottom rows with a duration of 200 ms (one column) times 5, or 1000 ms; phase 6 pause (silence) with a duration of 200 ms.\n\nFrom the described values it is easy to calculate a total cycle time of 2900 ms. This results in a low transmission frequency of around 0.345 Hz. However, it should be noted that before the tone from the next column is reproduced, the depth map values are updated. It can be said that there is a partial refresh rate of 5 Hz (excluding gaps between individual “scans”).\n\nThe above times are determined in such a way that the beginner can hear all the sounds and learn what they mean. The next steps in learning how to use the device can be a gradual shortening of the length of individual phases until the minimum times are reached. It is likely that hardware limitations—Raspberry Pi 1 Model B+—would not allow for very short times. This problem is solved with the newer versions of the Raspberry Pi computer, with more computing power. The number of rows (sounds) can be increased. A greater increase in the number of columns results in possibly large, favorable values.\n\nThe values of fields are determined as follows: 1—The value of each of the pixels in a small field is computed using a transition table, which stores converted (using one of the transformation functions) at the start of the program distance values. This method was used in the freenect library demo code. 2—The values of all correctly read pixels are added together and their number is calculated. Pixels with too large values are discarded. These data serve to average the value later. In addition, the smallest indicated distance value is found among the pixels. 3—If the minimum specified distance value among all pixels in the small area is larger than the defined limit (currently 1.2 m) and if there is at least one pixel with a correctly read value, then the small field is described by means of correctly read pixels. 4—If the above condition was not met (i.e., if a nearby obstacle was detected), the small field is assigned the value of the minimum read distance occurring in the small field area.\n\nYou can also use the alternative method of the above algorithm, but it is probably less beneficial to use it. In this method, part of point 2 and point 3 are omitted—the values are not means, and the small field is always assigned the minimum value appearing in its area.\n\n5.1.3. Learning Mode\n\nSince it is not natural to make the view of the height of the view from the height of the sound, it is necessary to give the user the chance to learn this skill. For this purpose, a special program (operating directly on the device) has been made, which should help to learn reflexively depending on the height of the view from the height of the sound. In addition to the program, the person can also learn to recognize the sounds, but this is most likely a much harder learning method.\n\nIn order to enable the learning mode, after starting the normal mode, disconnect and immediately connect the Kinect sensor power plug (plug marked with a strip of insulating tape). If the plug-in is connected later than after about seven seconds, the program will not start. The device should then be restarted.\n\nThe learning mode works in an analogous way to an ordinary obstacle indicator. Device detects an obstacle situated exactly in the middle area of the sensor’s field of view and signals the smallest distance read in this area from the obstacle using a single tone, which increases in intensity as you approach an obstacle (use the same conversion function, which in normal mode). The height of the tone signaling the obstacle is selected based on the inclination of the sensor. The inclination of the sensor is read using its built-in accelerometer.\n\nIf the person bends his head forward (down), then the distance from the obstacle is signaled lower and lower (depending on the angle of inclination). If it tilts the head up, the distance is signaled with ever higher tones. No sound is reproduced if the tilt angle exceeds the field of view angle of the sensor in normal mode, which is 43°, 21.5° down or 21.5° up. It is easy to calculate that for one row (and therefore the tone) it is about 7.17°. Accelerometer readings returned by the libfreenect library are returned with a resolution of 0.5°.\n\nThe distance refresh rate here is much higher than in normal mode and is 20 Hz (refreshing every 50 ms). The reference sound is played every 80 readings, so every 4 s. It lasts the same as in normal mode—300 ms, and pauses surrounding it after 200 ms.\n\n5.1.4. Selection and Generation of Sounds\n\nIn order for the depth map message not to be unpleasant, tones based on the C7 (divided into many octaves) tones shifted by 50 cents up (1/4 tone) were used.\n\nIn the selection of sounds, the limitations of human hearing were guided, and so they tried to keep the greatest distances between tones in one “scan”. In addition, two identical sounds could not be heard in a single “scan”.\n\nThe basic frequencies of the tones belonging to the C7 token were obtained from the website [26]. Then, the tone frequencies were multiplied by a further power of two to get the frequency of the tones in the higher octaves, and the low frequencies of the tones were divided by successive powers of two to get the frequency of the tones in the lower octaves. The tone frequencies thus obtained have been raised by 50 cents. Of the obtained sounds, 6 frequencies were selected. Selected sounds are presented in .\n\nTable 1\n\nTone MarkFrequency Values [Hz]C+67.32134.63269.27538.631077.162154.33E+84.81169.63339.26678.621357.132714.27G+100.92201.85403.49806.971613.953227.89Ais+119.97239.93479.86959.621919.243838.47\n\nThe reference tone is in no way associated with the above sounds and has a frequency of 100 Hz, which can be freely changed.\n\nThe tones were generated using the Audacity program. The amplitude of the generated sounds has been set to 0.2. The sounds have been matched to each other using the boost slider (slider on the left side of each track). The gain for f = 67.32 Hz was set to +8 dB. For f = 169.63 Hz + 5 dB. For f = 403.49 Hz − 4 dB. For f = 479.86 Hz − 4 dB. For f = 806.97 − 2 dB. For f = 2154.33 − 9 dB.\n\nThe tones have been exported to WAV files with a sampling frequency of 44100 Hz, although for the device it can be 22050 Hz, which should not cause any problems."
    }
}
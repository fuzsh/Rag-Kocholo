{
    "id": "dbpedia_5018_3",
    "rank": 19,
    "data": {
        "url": "https://meet-global.bnext.com.tw/articles/view/47594",
        "read_more_link": "",
        "language": "en",
        "title": "How Nvidia Won the Graphics Card Industry",
        "top_image": "https://image-cdn.learnin.tw/bnextmedia/image/album/2019-01/img-1546485248-27928.jpg?w=1600",
        "meta_img": "https://image-cdn.learnin.tw/bnextmedia/image/album/2019-01/img-1546485248-27928.jpg?w=1600",
        "images": [
            "https://image-cdn.learnin.tw/bnextmedia/image/album/2019-01/img-1546485248-27928.jpg?w=900&output=webp"
        ],
        "movies": [
            "https://www.youtube.com/embed/TRZqE6H-dww"
        ],
        "keywords": [],
        "meta_keywords": [
            "數位時代",
            "Bnext",
            "創業",
            "Meet",
            "小聚",
            "新創"
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "In 1995, there were over thirty different companies competing with one another to build the best graphics chips for the personal computer. Six years later, there would be only three. With one clearly in the lead: Nvidia.｜Meet Global",
        "meta_lang": "en",
        "meta_favicon": "https://cdn.bnextmedia.com.tw/meet/images/global-meet-57x57.png",
        "meta_site_name": "",
        "canonical_link": "https://meet-global.bnext.com.tw/articles/view/47594",
        "text": "In 1995, there were over thirty different companies competing with one another to build the best graphics chips for the personal computer.\n\nSix years later, there would be only three. With one clearly in the lead: Nvidia.\n\nAs of this writing, Nvidia Corporation is the 15th biggest company in the world, worth half a trillion dollars.\n\nTheir graphics cards sell out like gangbusters the second they come onto the market.\n\nAnd the company is seeking to buy ARM for $40 billion.\n\nIn this video, we are going to look back into the past and see how a little startup came up from behind everyone else to dominate the graphics card industry on route to being the world-leading tech juggernaut it is today.\n\nIntroducing Nvidia\n\nNvidia is very well covered in the Wikipedia article so if you want details, you can read that first. The company's founding story and journey is as charismatic as its founder. But here is an accelerated history.\n\nNvidia was founded in 1993 by Jensen Huang, Chris Malachowsky, and Curtis Priem. The former director of coreware at LSI Logic Corporation, Huang was the company’s President and CEO. Malachowsky and Priem served on the technical side.\n\nI cannot help but also add that Huang was born in the city of Tainan. My family hometown and where many of my loved ones still live.\n\nThe company was founded with the intention of creating microprocessors capable of rendering high quality 3D graphics. Their name came from the Latin word \"invidia\", which means envy.\n\nGraphics in the 70s and 80s\n\nThe idea of having separate hardware handle the complicated tasks of 3D graphics processing long existed before Nvidia came onto the scene. For instance, the first Atari video game consoles, shipped in the 1970s, had graphics chips inside.\n\nBut the first commercial video cards - where you have chips or boards plugged into the main computer - came about in the 1980s. The IBM Professional Graphics Controller or PGA used an on-board Intel 8088 microprocessor to do video tasks. This freed up the computer's CPU to do other things.\n\nFirst released in 1984, the PGA cost too much - sticker price of $4-5,000 - and only worked with IBM PCs at the time. It did not succeed in the market. But the idea of a video card began to stick.\n\nIn the late 1980s, Silicon Graphics Inc or SGI emerged as the dominant graphics player in the market.\n\nThey were the first to refine the concept of a graphics pipeline - the conceptual model illustrating the stages through which graphics data is processed. In other words, it translates coordinates to 2D pixel space on a screen.\n\nAnd then in 1992 SGI released OpenGL, a public 3D graphics standard or API. With 120 commands to draw points, lines, and polygons, OpenGL would be the first API to be widely adopted by the industry. OpenGL support became a critical factor in graphics hardware.\n\nSGI hardware dominated the profitable graphics workstation space, but never took the consumer market all that seriously. A slew of others would soon scratch that itch.\n\nThe Growing Consumer Market\n\nIn 1990, Microsoft released Windows 3.1. And it had a graphical user interface, a GUI. And it was glorious.\n\nThe idea of being able to see and interact with things visually caught on for whatever reason. Over 2 million units were sold in the first six months after release.\n\nBut consumer interest in graphics really took off in 1993 with the release of a 3D game called Doom by game developer Id Software.\n\nMost contemporary video games then were Nintendo-ish childish toys. Doom bucked the trend and offered a mature, action-packed first person shooter.\n\nThree years later in 1996, Id released their followup to Doom - Quake. Quake offered brand new technical breakthroughs that Doom did not have. It offered full real-time 3D rendering and online multiplayer. Quake would also be a massive hit.\n\n3dfx Takes the Lead\n\nQuake sold well, but PC owners found that their computers were often unable to run the game at its full specs. Without additional help, the game looked terrible. And it was slow, which matters a big deal when you are trying to beat your friends in an online deathmatch.\n\nTo ameliorate this, main programmer John Carmack worked alongside chip designer Rendition on a version of Quake customized for their Verite chips. But after this fell through, he rewrote the game to support OpenGL. This version was later dubbed GLQuake.\n\nThis spurred new innovation in the consumer video card space. To take advantage of this game - one of the first \"killer apps\" - a graphics chip startup called 3Dfx quickly wrote a wrapper program that made OpenGL compatible with their own proprietary Glide graphics API. They then exploited a drop in RAM prices to launch the Voodoo video card.\n\nVoodoo cards ran chips that were designed by 3dfx in America, manufactured in Taiwan, and sold to various board partners for resale. It was the first consumer video card capable of running GLQuake very well - 640x480 in 16 bit color at 30 frames a second. Best of all, its boards sold for about $200 at the time.\n\nCrucially, the Voodoo only offered 3D acceleration. Users who wanted 2D acceleration needed a separate card for that. Despite that, it sold very well. At its peak, 3dfx had 80% of the 3D acceleration graphics card market.\n\nNvidia's Early Struggle\n\n3Dfx now dominated the industry. Nvidia, on the other hand, was just trying to make it out of the gate. Silicon Valley in the mid-1990s was going through a downturn. To make things worse, they were just another one out of many graphics chip startups at the time.\n\nTwo VC firms eventually put in $1.25 million each for a $2.5 million round. With that, Jensen Huang then struck three crucial partnerships to give the company an early breath of air.\n\nThe first Nvidia partnership had to do with manufacturing. The startup could not afford to build their own chip manufacturing facilities.\n\nSo in 1994, a year after their founding, Nvidia partnered with SGS-Thomson Microelectronics - now Switzerland-based STMicroelectronics. Nvidia would be in charge of the design while SGS-Thomson handled the manufacturing.\n\nThe second deal was with Diamond Multimedia Systems, an American multimedia company. In this deal, Diamond agreed to buy Nvidia's chips and put them in their multimedia accelerator boards.\n\nThe third big partnership was with Sega. Sega at this time was a popular video game company, but their games were mostly for the arcade and console market. They wanted to port their games to PCs, and selected Nvidia to help with that. Critically, Sega allowed Nvidia to use their brand name to help with the launch.\n\nThus came Nvidia's first piece of working silicon in May 1995: the NV1. The NV1 was a multi-media PCI card with graphics, sound, and Sega game pad support capability. The goal was to converge all of these multimedia cards together into one. To be the \"Soundblaster of multimedia\" as Jeff Fisher, Nvidia's EVP of sales, recalled.\n\nThe NV1 was sold to a number of board OEMs. Diamond resold it as the Diamond Edge 3D. The product was technically impressive, but it tried to do too much - resulting in significant compromises. Just as importantly, the NV1 was not compatible with the leading graphics APIs at the time - OpenGL, 3Dfx's Glide, and such. They were not interested in working with Nvidia on it.\n\nAnd so the NV1 was a flop. Nvidia ceased selling it in late 1996. Sega had at first wanted to work with the startup to build a more advanced NV1 for its next generation game console, the NV2, but that eventually fell through. The startup thus pivoted to back a different horse.\n\nThe Microsoft Giant\n\nMicrosoft very quickly noticed the growing trend of 3D PC video games. Just a few years ago, why would anyone need to add a video card to their computer? Now, consumers were outfitting their computers with graphics tools rivaling midrange workstations.\n\nIt was more than just video games. 3D graphics and visualization was the machine learning of its day, the hot thing that everybody had to have. Application developers deeply integrated it as a feature within their own products. For instance, business tools like spreadsheets and word processors.\n\nThis trend was real. And the Redmond giant realized that they did not have a horse in the race. OpenGL was not under their control - it was handled by an independent review board after SGI - and so they resolved to get involved.\n\nIn 1996, Microsoft acquired a British graphics company called Rendermorphics and then reworked their drivers to create their own Microsoft-backed 3D graphics API: Direct3D.\n\nDirect3D was part of a larger bundle of drivers called DirectX. It was built into the Windows operating system, and Microsoft touted it to game developers as the best way to make their games. They repeatedly called OpenGL inherently slow, kicking off a flurry of nerd wars.\n\nAdvocates emerged out of both sides. John Carmack of Id software famously backed OpenGL. After porting Quake to OpenGL to create the aforementioned GLQuake, Carmack announced that he had tried to do the same with Direct3D but then released a letter saying:\n\n\"Well, I have learned enough about it. I'm not going to finish the port. I have better things to do with my time ... Direct-3D … is a horribly broken API. It inflicts great pain and suffering on the programmers using it, without returning any significant advantages ... There is no good technical reason for the existence of D3D.\"\n\nThus, Direct3D took some early hits. But Bill Gates himself replied to this position statement, advocating for Direct3D and promising future improvements and support over time.\n\nNvidia's Three Bets\n\nNvidia's first shipped product did not sell well, but it was not a total failure. Being able to actually ship something is definitely a promising milestone, and the chip itself like as I said was technically impressive. The NV1’s release allowed Jensen to raise another $6.5 million round from Sequoia and Sierra Ventures.\n\nYou never want to waste a crisis. In 1993, Lee Kun-hee, chairman of Samsung Group, judged that his company could not compete in the market.\n\nThus, Lee's famous command to his subordinates: \"Change everything except your wife and kids.\"\n\nAfter consulting with a variety of experts, Nvidia's leadership changed everything. Having abandoning the multi-media space, they will bet on three key rising trends. Two, I will talk about right now. The third, later.\n\nFirst, the company would bet on Microsoft. Direct3D at the time was not in vogue, but Bill Gates himself promised that it would get better.\n\nNvidia bet its future on this happening - and perhaps more importantly, that its tight integration with Windows would soon make it the dominant 3D graphics API in the industry.\n\nSecond, the company bet on vertical integration by taking control of its device drivers. Device drivers are a type of software that operates or controls a type of device attached to your computer. Back in university I used to build my own PCs, and I remember having to spend a lot of time installing device drivers for my accessories. That was a pain.\n\nAt the time, most device drivers were written by the board partners. The OEMs who then resold the product to their customers. This created a situation of mismatched incentives. Drivers are extremely complicated and them working is critical to the device experience. You want to spend a lot of time on them, and that cannot be the case when a third party OEM writes them.\n\nNvidia opted to take that in-house, writing their own drivers for their own hardware. This aligned the proper incentives, and allowed Nvidia enough time to do it right.\n\nFurthermore, Nvidia can now receive and respond to issues from across all of its board partners. Aggregating these experiences would eventually allow the company to produce a very high performing device driver for all of its hardware, a critical advantage. Drivers would soon turn out to be a cornerstone of the company’s future success.\n\nThe Riva\n\nHaving implemented this complete change in direction, Nvidia was ready to make its move. The company's second (or third, I guess) product, the Riva 128, launched in 1997. At the time of launch, Nvidia had less than six weeks of cash left in the bank.\n\nThe Riva had been specifically designed to accelerate Direct3D as much as possible. OpenGL support was provided through a wrapper program. Critically, it supported both 3D and 2D graphics, a functionality that the current market leader 3Dfx Voodoo could not do.\n\nThe product sold well. Diamond Multimedia was one of the bigger buyers, reselling it as the Diamond Viper 330. It retailed for about $140.\n\nThe Riva was a success, but Nvidia still had a lot to do in order to win the market. They wanted to move fast. And for that to happen, they needed a partner who can keep up with that relentless pace. Their current arrangement would not suffice.\n\nThus, Nvidia's third big bet on its future. The company bet on Moore's Law, the power of ever-improving manufacturing processes, and outsourced manufacturing. For that, the company would have to lean on a new rising power across the Pacific.\n\nGoing Fabless\n\nIn 1998, the company ended their partnership with SGS-Thomson and struck a new one with Taiwan Semiconductor Manufacturing Company, or TSMC.\n\nThe SGS-Thomson partnership worked fine but same as with the device drivers situation, Nvidia management was bothered that the two companies ultimately had opposing interests. Jensen Huang had been on the supplier/manufacturer side while working at LSI Logic, renting out excess fab capacity for chip equity from its customers. He knew what could happen.\n\nSGS-Thomson ultimately looked out for itself. TSMC advocated for its partners, pledging never to compete with them. What TSMC wanted was for its partners to sell more and more chips every year.\n\nSo when he found out about TSMC, Jensen Huang got in touch with TSMC founder and CEO Morris Chang the only way he knew how. In an interview, Jensen recalls:\n\n\"I wrote him letters—the only way I had of contacting him. And one day he called me. I loved that TSMC's intentions were pure, that their success only came with our success ... Nvidia had to move fast to keep up with the competition, and TSMC kept up with our needs\"\n\nNvidia would eventually become one of TMSC's biggest customers. And TSMC used its platform advantages to eventually chase down Intel as the best chip manufacturer in the world.\n\nThis meant that the company can focus entirely on design, rather than manufacturing. They did this in spades, optimizing for speed and rapid iteration. Nvidia organized its teams into three groups. One worked on the upcoming design, another on a refresh of last year's design, and the last on next year's design.\n\nThe company adopted the latest chip design processes, which included computer aided design verification and validation. This included extensive simulations of a chip's physical layout - where the circuits and wires between them went.\n\nIn my video about electronic design automation, I briefly mentioned the importance of such test tools. If there is a bug in a chip design when it goes to TSMC, you just wasted a whole lot of money and time. So getting this right is crucial before shipping the design.\n\nThe end result of this was a relentless six-month product cycle. Every six months like clockwork, Nvidia was able to deliver a new chip generation. This brutal march ground down the graphics card industry and companies started to drop out.\n\nExcept for one: ATI, one of the few companies able to ratchet up their own release cycle to keep up. Even so, the market situation would have probably remained extremely fluid had the market leader not made a critical error.\n\nThe Fall of 3dfx\n\nGoing into 1998, 3dfx held the market lead with their Voodoo Graphics set. But as I mentioned, the product's technical limitations meant that it could only do 3D acceleration. 2D was not supported - and that was how Nvidia's and ATI's early products managed to gain a market foothold.\n\nThe company struggled in coming up with a proper 3D/2D followup. Their first attempt, the Rush chipset, released in mid-1997 and did not perform well. It took over a year for them to finally come out with something competitive in the market - the Banshee in Sept 1998.\n\nThis struggle ended up delaying their entire product lineup. Their 3D-only Voodoo followup, the Voodoo2, came out over 17 months after the release of the first Voodoo. Regardless it sold extremely well and the company thus still held a third of the high-end market.\n\nThen the company made a massive misstep. Up until now, they designed the chips and sold them to board partners like Diamond. But over time, management started to worry about the company's brand image. Only a few enthusiasts knew about 3dfx. Most retail buyers only saw Diamond's - the board partner’s - name.\n\nSo 3dfx wanted to do its own vertical integration expansion. Which is not bad. The bad part was the direction they ended up choosing.\n\nIn December 1998, 3dfx acquired board-maker STB for $141 million and announced that their chips would only be available in their own name-branded boards. Now 3dfx was directly competing with their old customers.\n\nIt went as well as you might have expected. Old partners defected to Nvidia. Management had never done manufacturing before and STB's Mexico-based facilities could not compete with its competitors in Asia. The division became a cash burn.\n\nThey released a few more interesting products before dissolving and selling their assets to Nvidia in 2000.\n\nIntel Closes the Door\n\nIn 1996, Intel said that 80% of PCs would have 3D graphics by the end of the millennium and that they would dominate the space by 1998. The company was the industry's 800-pound gorilla at the time and their words were enough even to cause Microsoft to cancel their own line of graphics hardware.\n\nAs promised, Intel released the Intel 740 card to great fanfare in 1998. However the card did not sell well. It used a port format, AGP, that Intel was trying to popularize. Most everyone stuck with the current standard PCI. While the card performed well in speed tests, real world performance lagged the market. Largely for these two reasons, the product failed to take hold at launch.\n\nThe rest of the market then updated their products and Intel, more used to the 1-2 year CPU development cycle, could not keep up with the rest of the graphics market. The i740 as it was called was promptly discontinued.\n\nBut the company continued to tinker in the integrated graphics processors space - graphics chips that use a portion of system memory rather than their own dedicated pool. Because they are integrated with a CPU chipset on a motherboard, Intel was able to leverage its CPU monopoly to build out massive market share in this space.\n\nNobody would ever mistake these graphics products for something top of the line. And it is likely that enthusiasts will scoff at them. But Intel's entry wiped out the market for low-margin, low performance graphics needs. Once Intel created something that worked well enough for most low-end users, the rest of the industry found themselves caught in the middle. And so they promptly shriveled and died.\n\nConclusion\n\nSo by the early 2000s, the graphics card space had drastically consolidated from over 30 to just three: Nvidia and ATI, a clear duopoly, with Intel taking up the low end.\n\nNvidia and ATI would do battle over the next few years until AMD acquired the latter in 2006 for $5.6 billion. AMD soon thereafter replaced the ATI branding with their own. Today, AMD's graphics cards sell quite well and their chips are commonly used in game consoles. But the hierarchy is set.\n\nThere is a lot to digest here. But I think something that really resonated with me is the idea of vertical integration expansion. There are so many ways to do it, but not all of them make good sense. Nvidia chose an adjacent expansion - device drivers - that added enough business value to become a lasting competitive edge against ATI and the rest of the industry. 3dfx went in a direction - manufacturing - that destroyed value, burned cash, and alienated their big customers. It is a lesson to hold closely.\n\nNvidia has since started to look beyond just being a graphics chipset provider. In 1999, they coined the phrase General Processing Unit, or GPU. They began popularizing the concept of general purpose computing, envisioning new applications beyond that of video game graphics. Such a vision would soon come to pass in the years to follow."
    }
}
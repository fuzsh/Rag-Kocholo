{
    "id": "dbpedia_2453_2",
    "rank": 66,
    "data": {
        "url": "https://blog.computationalcomplexity.org/2003/",
        "read_more_link": "",
        "language": "en",
        "title": "Computational Complexity",
        "top_image": "https://blog.computationalcomplexity.org/favicon.ico",
        "meta_img": "https://blog.computationalcomplexity.org/favicon.ico",
        "images": [
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://lh3.googleusercontent.com/blogger_img_proxy/AEn0k_vYAurAXvH3jg6cWAheif9ud0fIJjf0NBGZtWGz3-odbYaLL0sjBjLrhW9ru1KWypzl5n5vQqX-iZT18SHyjGGFvEztn1QdJ8i-WNTAm0qj=s0-d",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://resources.blogblog.com/img/icon18_email.gif",
            "https://resources.blogblog.com/img/icon18_edit_allbkg.gif",
            "https://pup-assets.imgix.net/onix/images/9780691175782.jpg?w=640",
            "https://i.creativecommons.org/l/by-nc/4.0/88x31.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Lance Fortnow"
        ],
        "publish_date": null,
        "summary": "",
        "meta_description": "Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch",
        "meta_lang": "en",
        "meta_favicon": "https://blog.computationalcomplexity.org/favicon.ico",
        "meta_site_name": "",
        "canonical_link": "https://blog.computationalcomplexity.org/2003/",
        "text": "Today is the 100th anniversary of the birth of John von Neumann, one of the greatest mathematicians of the 20th century. Let me discuss two aspects of his work, one big, one small, that have greatly affected computational complexity.\n\nThe von Neumann min-max theorem showed that every finite zero-sum two-person game has optimal mixed strategies. More formally, let A be the payoff matrix of a game, then\n\nmaxx miny xTAy = miny maxx xTAy where x and y are probability vectors.\n\nAndrew Yao used the min-max theorem to prove what we now call the Yao Principle: The worst case expected runtime of a randomized algorithm for any input equals best case running time of a deterministic algorithm for a worst-case distribution of inputs. The Yao principle has proven invaluable for proving upper and lower bounds for deterministic and probabilistic algorithms.\n\nHow can you get a fair coin by flipping a coin of unknown bias? You use the von Neumann coin-flipping trick: Flip the biased coin twice. If you get heads then tails output HEADS. If you get tails then heads output TAILS. Otherwise repeat. This procedure will output HEADS or TAILS with equal probability and if the bias is not too close to zero or one the expected number of repetitions is relatively small.\n\nThe von Neumann coin flipping trick is the first in a long line of research in complexity extracting random bits from weak random sources.\n\nJohn von Neumann passed away February 8, 1957 in Washington, DC.\n\nOn Saturday I visited the Einstein Exhibit at Chicago's Field Museum. Some manuscripts and letters and a nice exhibit explaining why time must vary if the speed of light remains a constant made this an interesting but not a must-see exhibit. The biggest surprise for me came from seeing how Einstein's fame happened overnight instead of the more gradual fame I would have expected. In 1919 a solar eclipse showed that light from stars do bend from gravitational forces. Einstein's fame grew immediately and his name became synonymous with genius.\n\nThis superstardom for a scientist doesn't seem to happen today. When Andrew Wiles proved Fermat's last theorem he did get some deserved attention but he never became a true household name. When you realize Wiles has hit the upper limit of fame a mathematician can receive (ruling out people like Ted Kaczynski and John Nash) one can see the Einstein effect of science may never return.\n\nOn the other hand, University of Chicago paleontologist Paul Sereno headlines the social page of the Chicago Tribune at the \"Party With Giants.\" Perhaps scientists can still achieve more than fifteen minutes of fame after all.\n\nTime for another of my favorite open problems.\n\nDoes NP=UP imply the polynomial-time hierarchy collapses?\n\nUP is the class of languages accepted by nondeterministic polynomial-time Turing machines that have at most one accepting computation for all inputs.\n\nThis problem has loose connections to Valiant-Vazirani but Hemaspaandra, Naik, Ogiwara and Selman have the most closely related result. Consider the following proposition.\n\n(*) There is a set A in NP such that for all satisfiable formula φ there is a unique satisfying assignment a of φ such that (φ,a) is in A.\n\nHemaspaandra et. al. show that (*) implies the polynomial-time hierarchy collapses to the second level.\n\nFor all we know, (*) and NP=UP are incomparable. If (*) holds for some A in P then NP=UP by just guessing a. We don't know whether NP=UP implies (*) since the accepting computations of a UP machine accepting SAT need not reveal a satisfying assignment of a formula.\n\nThere exist an oracle relative to which UP=NP≠co-NP. A relativized world with UP=NP and Σ2p≠Π2p remains open.\n\nEvery scientific field has their own rules for the order of authors in a paper. In theoretical computer science, tradition dictates that we list the authors alphabetically by last name. I don't agree with this tradition; rarely do all the co-authors of a paper play an equal role. The decision whether to add someone as a co-author, and thus an equal, often becomes difficult.\n\nBut breaking with tradition can have its own problems. I have three papers that break the alphabetical rule though two were in biology which has its own rules. In the other back in 1990, Carsten Lund, a graduate student at the time, made the key step in developing an interactive proof system for the permanent. For that we made him first author in the Lund-Fortnow-Karloff-Nisan paper. In retrospect I regret this decision. It only added confusion to those who cited the paper. Also did Lund not play as important a role in other papers where we kept alphabetical order? Breaking with tradition, even with the best of intentions, can often cause more harm than good.\n\nWant an easy rule to greatly improve your writing? Just avoid the following words, particularly in the abstract and introduction of your papers. am is are was were be been\n\nAvoiding these seven forms of \"to be\" will force you to write in the active tense instead of the passive making your sentences less boring. For example, instead of \"It is known that all functions can be computed securely in the information theoretic setting\" use \"We can compute all functions securely in the information theoretic setting.\"\n\nTaking this rule to the extreme can lead to some very convoluted sentences but, I promise, forcing yourself to think actively about every statement you write will make a great difference in your prose. In almost all cases the right answer is \"not to be.\"\n\nOver the last 15 years, two very active research areas seem at odds. Derandomization results have shown us that we can often remove randomness from computation but interactive proof systems and PCPs exhibit incredible power from randomness. There is no contradiction here, just two very different ways we use randomness in complexity: for searching and for hiding.\n\nTypically we think of randomness for searching, for example finding disagreements with Fermat's little theorem to show a number is composite or taking random walks on graphs to show they are connected. Derandomization results have given us reasons to believe we can replace the randomness in these computations with pseudorandom number generators.\n\nRandomness can also play the role of hiding, since no one can predict future coin tosses. In interactive proofs we make the jump from NP to PSPACE because of randomness. For PCPs with O(log n) queries the jump goes from P to NP and with poly queries from NP to NEXP, in the later case classes which are provable different. In all these cases the prover cannot cheat because it cannot predict coin tosses not yet made by the verifier. A verifier using a pseudorandom generator will fail here, since the prover could then predict the verifier's actions.\n\nAM protocols have the verifier flip coins first so no hiding going on, rather searching for a statement Merlin can prove and we expect some derandomization for AM. The result that MA is in AM says that sometimes we can replace hiding randomness with searching randomness.\n\nPaz Carmi, Yinnon Haviv and Enav Weinreb from Ben-Gurion University have solved the regular language problem I posted last month.\n\nThe problem came from Janos Simon based on a homework question in Kozen's book. Let L(A)={x|x^m is in A for some m}. The homework question asked to show that L(A) is regular if A is regular. The question Janos asks was how many states do we need for a DFA for L(A) if A has n states. Carmi, Haviv and Weinreb show that an exponential number of states are required.\n\nNot only did they solve the problem but also sent me this nice write-up of the proof. I believe this is the first time someone has directly solved a problem I've given on this weblog. I owe each of them a beer (or non-alcoholic beverage of their choice).\n\nUpdate 12/9: I received the following today from Markus Holzer.\n\nIt seems that I have missed your posting about the problem last month. The problem you have stated was already solved in June by Jeff Shallit and co-authors. They have given a lower bound on the DFA accepting root, by considering the (largest) syntactic monoid induced by two generators. The latter problem on syntactic monoid size is of its own interest and I was working on that for a while, therefore I know the result of Shallit et al on the root descriptional complexity. Maybe you also owe the beers to Shallit et al.\n\nThe Cornell University Library has announced it will drop a substantial number of their Elsevier subscriptions, part of a general problem Cornell and other libraries are facing with higher costs and different pricing models from commercial academic publishers.\n\nI have wanted to write a post on this topic for a while but I find it difficult to truly understand the problems or the potential solutions. Elsevier does a nice job with their portal and their publishing, but because of consolidation and cheap distribution via the internet, they have changed their pricing model in ways that make it difficult for many libraries to afford all of the journals that they need.\n\nThis poses some moral questions: Should we avoid submitting our papers to Elsevier journals? Is it wrong for me to serve on the editorial board of the Elsevier-published Information and Computation? I just don't know.\n\nFirst an update on NSF program solicitations: The Formal and Mathematical Foundations cluster has posted its solicitation which includes computational complexity. The deadline is March 4. The program announcement for the Emerging Models and Technologies for Computation Cluster, which includes quantum and biological computing, is still under development. Also the ITR solicitation has also been posted, with some major changes from previous years.\n\nDonald Knuth's tribute to Robert Floyd highlights the December SIGACT News. Also reviews of a bunch of crypto books, a column on sublinear-time algorithms and the complexity theory column on \"Post's Lattice with Applications to Complexity Theory.\"\n\nIn my mailbox yesterday was not one but five copies of SIGACT News shrink-wrapped together. Once I unwrapped them and looked at the labels, only the outer one belonged to me. There were two for other professors in my department, one for our library and one for the library of Loyola University Chicago, which is on the other side of the city. I'm not sure if it was a mistake or some attempt by the ACM to reduce mailing costs, but I hope this is a one-time occurrence.\n\nXML (eXtensible Markup Language) has become quite a popular data format in recent years. XML roughly corresponds to a tree. For example,\n\n<person><name>Harry</name><age>29</age></person>\n\n<person><name>Jane</name><major>Computer Science</major></person>\n\nrepresents a tree. The root having two children, each labeled \"person\". The first of these children has two children named \"name&quot and \"age\". The first of those children has a leaf node labeled with the phrase \"Harry\". For a larger example, see the RSS feed for my weblog.\n\nXML was designed as a flexible way to present documents for later displaying. Since the XML format can be easily produced and parsed, XML also serves as a standardized method for transferring data between databases, far better than the old CSV (Comma-Separated Values) format.\n\nRecently there have been some work on directly manipulating and querying the XML data. As a theorist, this seems like a bad idea, particularly for larger databases. While XML completely represents the underlying tree, it is not a good implementation of that tree. Basic tree operations like parent and sibling are very expensive just looking at XML. About the only thing one can do quickly with XML is depth-first search. Far better to \"shred\" the data into a better tree implementation like a DOM (Document Object Model) or a full-fledged database and do the work there, rewriting a new XML if needed.\n\nOne issue though is when the XML file is on the order of 5-10 GB, a bit larger than what can be stored in the memory of a typical desktop machine. One can stream through the file rather quickly but cannot recreate the tree. This opens up some interesting theoretical questions:\n\nGiven a stream of data in XML format, how can one do efficient analysis and manipulations of the underlying tree? I suspect one would want to sometimes shred subtrees, but you cannot determine the size of the subtree until after its been streamed. Perhaps some randomness or streaming the file multiple times might be helpful.\n\nXML might not be the right model of a tree for this purpose. What is the best way to stream a tree or other data structure to allow an efficient implementation of the basic operations of the data structure? Perhaps some redundancy might be useful.\n\nI thought I should mention some of my favorite and most frustrating open questions over the years. Here's one of them:\n\nLet f:{0,1}n→{0,1}. Let h and g be n-variable degree d polynomials over the reals. Suppose for all x in {0,1}n, g(x)≠0 and f(x)=h(x)/g(x). Is there a constant k such that the decision-tree complexity of f is bounded by dk?\n\nThe decision-tree (or query) complexity is the number of bits of x that need to be viewed to determine f(x). The queries to the bits of x can be adaptive. I'm particularly interested in the case where d is poly-logarithmic in n.\n\nNisan and Szegedy answer the question in the affirmative if g(x)=1. Their result holds even if f(x) is only approximated by h(x). However if we allow arbitrary g(x), h(x)/g(x) can closely approximate the OR function which requires looking at all of the bits. The case where we require exact equality of f(x) and h(x)/g(x) is the open question at hand.\n\nBack in my science-fiction reading days, I particularly remember one editorial written in one of those anthology magazines about 1980: In the near future, you will be able to access, via your personal computer, any science fiction story right after it has been written. If you like a certain author, you can read other stories from that author, even if we didn't decide to put it in this magazine. In this future world, will you still need me, the editor? The answer is yes, for there will be way too much dreck out there for you to find the good stories within, and you will need people like me to point them out to you.\n\nThe future is now and though I haven't kept up with science fiction, the same issue applies to academic publications. Recent posts by Michael Nielsen and on Slashdot have asked: With nearly all new papers in physics and computer science easily accessible on the web, how do you find the ones worth reading?\n\nConferences have traditionally played this role in computer science. But, by definition, paper choices are decisions by committee and with the massive growth in the field, many good papers do not appear in the major conferences.\n\nWhat we need are \"editors\"! You can help. Write a survey paper, or spend a page in your research papers discussing the important earlier results in a field. Maintain a web page pointing to papers you find interesting. Start a weblog saying what you find interesting--you don't have to post long or often, just to say, hey, this paper is worth looking at. This way people with similar interests to you can find out what at least you think is important. Only by many of us working together can we make the interesting papers stand out.\n\nIt happened right after I started high school in suburban New Jersey, the start of the Science Times section in Tuesday's New York Times. The Science Times not only helped get me excited about science but made me feel others could get excited over science as well. I've kept reading it off and on during these past 25 years. The Science Times has reported on a fair amount of research in complexity and theoretical computer science, for a time some joked that a result was not important until it appeared in the New York Times.\n\nToday the New York Times celebrates the 25th Anniversary Issue of the Science Times. It features 25 questions such as Does Science Matter? and What Is the Most Important Problem in Math Today? (Hint: It's not P versus NP).\n\nI'll end this post with a quote from the essay of Alan Lightman:\n\nAll of the scientists I've known have at least one quality in common: they do what they do because they love it, and because they cannot imagine doing anything else. In a sense, this is the real reason a scientist does science. Because the scientist must. Such a compulsion is both blessing and burden. A blessing because the creative life, in any endeavor, is a gift filled with beauty and not given to everyone, a burden because the call is unrelenting and can drown out the rest of life.\n\nThe Computer and Information Science and Engineering Directorate of the NSF has completed it reorganization. The CISE web site details the new structure.\n\nCISE now has four divisions. Instead of each division have a large number of specific programs, each division contains a smaller number of clusters covering a broader research area. I'm happy to see \"Computational Complexity\" specifically mentioned in the Formal and Mathematical Foundations Cluster in the Division of Computing & Communication Foundations. However it shares that cluster with such diverse topics as \"computational algorithms for high-end scientific and engineering applications\" and \"analysis of images, video, and multimedia information.&quot Hopefully funding panels will meet in the more specific areas to avoid trying to compare proposals from vastly different areas of computer science.\n\nQuantum and Biological Computing sit in a different CCF cluster, Emerging Models and Technologies for Computation. This shows NSF's hopes for these new technologies but may also give them a way to phase out these areas if the technologies don't show promise.\n\nProgram announcements for the CCF clusters are still under development. The ITR solicitation is still not expected until Thanksgiving. So if you plan a grant proposal this year, you'll still need to wait.\n\nThe NEC Research Institute (NECI) died just over a year ago. I didn't feel comfortable talking about it then so let me say a few words now.\n\nI joined NECI in 1999 just after its tenth anniversary. When I joined its mission and focus was basic research in computer science and physics. NECI gave me considerable time and resources to study fundamental questions in computational complexity. It was an exciting place to be.\n\nSoon thereafter some changes were occurring. NEC modified the mission of NECI to focus on producing technologies with basic research secondary. Some researchers (though not us theorists) were encouraged to join \"technology groups\" to find practical applications of their research. But during this time, the administrators always supported basic research and I never felt uncomfortable doing theory.\n\nBut then on November 1, 2002, NECI merged with NEC CCR&L, a more applied research lab in the same building to form NEC Laboratories America. The new mission makes no mention of basic research. The scientists in charge were replaced by engineering/management types. Many of the research scientists, particularly physicists, were let go.\n\nMy job was never in immediate danger but NEC was no longer the place for me and so I went on the job market; no one was surprised when I decided to leave.\n\nA corporation like NEC needs to make decisions for the health of the company. I do not fault NEC for the decisions that it made and they gave me a few great years. Still I mourn the NEC Research Institute, quite a special place during its thirteen year run.\n\nLast week I started an experiment using instant messaging. I thank the many of you who sent me IMs, a great way for me to meet you, the readers of this weblog. I plan to keep trying IM for a while but I have had learned a few lessons which seem obvious in retrospect.\n\nInstant messaging can be a time sink. I love communicating with people, which is the main reason I keep this weblog going. However, as most academics, I have much going on and can't afford to have many lengthy discussions. I've also learned there is no clean way to end an IM conversation. So please feel free to IM me but don't take it personally if I rudely keep the conversation short.\n\nJust because the nice icon on the home page says I'm online it doesn't mean that I am at my computer and available to chat at the moment. Often I am and I will but if not I will eventually see your message and respond. If there is really is something important that you want to discuss with me via IM we can setup a scheduled time via email. I often do this with phone calls so why not IM too?\n\nI've also discovered IM conversations can be recorded, posted on the web and could be used in a court of law. I need to be careful about what I say.\n\nI have already had some interesting research conversations and ideas for weblog posts via IM. The last post came in part because of some IM questions about the Feigenbaum-Fortnow paper. Email became a powerful research tool when email use hit a critical mass among computer scientists sometime in the mid-late 80's. I believe IM will also follow that curve and I hope to keep ahead of it and perhaps nudge it a little bit.\n\nLet f be a function that maps Σn to Σn. Let U represent the uniform distribution on Σn and D be the distribution that one gets by applying f to a string drawn from U.\n\nWe wish to find f that change x but keep the underlying distribution close to the same, in particular we want the following properties,\n\n(1) Prob(f(x)≠x)≥2/3 when x is drawn from U.\n\n(2) U and D should be statistically close, informally no algorithm making a polynomial number of samples will be able to distinguish, with high confidence, whether those samples came from D or U.\n\nAchieving such an f is easy, consider f that just flips the first bit of x. (1) holds all the time and U=D.\n\nSuppose we add a restriction to f:\n\n(3) In the bits where x and f(x) differ those bits are 1 in x and 0 in f(x). For example, f(011)=010 is allowed, but f(011)=f(111) is not.\n\nAn f fulfilling (1), (2) and (3) is impossible. (1) and (3) means that f will reduce the number of ones on most of the strings and taking say n3 samples we will notice a statistical difference in the number of bits which are 1 depending on whether the samples were drawn from U or D.\n\nSuppose we replaced (3) with a weaker restriction:\n\n(3') In the first bit where x and f(x) differ, that bit is 1 in x an 0 in f(x). So f(110)=011 is allowed but f(001)=010 is not allowed.\n\nCan an f fulfilling (1), (2) and (3') exist? Not so clear, but Peter Shor found a simple example: f(0n)=0n, and for the other x, f(x)=x-1 where x is viewed as a nonnegative integer written in binary. D is indistinguishable from U yet f changes nearly every string.\n\nThese questions are related to an old paper I had with Joan Feigenbaum which has gotten some recent attention because of a nice new FOCS paper by Bogdanov and Trevisan that builds on our paper. The proofs in these papers work partly because (1), (2) and (3) cannot all happen even for arbitrary distributions U. Both papers give a negative result for a nonadaptive case; the adaptive case corresponds to (1), (2) and (3') and Shor's example shows that the proof techniques will not directly lead to a solution in the adaptive case which remain open.\n\nA quantum computing graduate student sent me email over the weekend. He had thought he had proven some surprising results about the class PP and was wondering if he was making some mistake. After some discussion here was his reply:\n\nOk I get it. Somehow I jumped to the conclusion that PPP was PP.\n\nThere is one more for your blog: A⊆ B implies B⊆ AB but not AB⊆ B (duh!)\n\nHe goes on to say he made his quantum leap to conclusions since for the quantum class BQP, PBQP=BQP, he thought the same property must hold for all classes.\n\nI present this because he suggested it for my weblog and as a public service for those who might make a similar mistake. Yes, in case you were wondering, for reasonable classes A (like A=P), B⊆AB without needing to assume A⊆B.\n\nHere is one of my favorite examples of a bad proof for what turns out to be a correct theorem.\n\nTheorem: If NP is in BPP then the whole polynomial-time hierarchy is in BPP.\n\nLet's focus on simply showing Σ2 is in BPP if NP is in BPP. The rest is straightforward induction. Here is our first proof:\n\nΣ2=NPNP⊆ NPBPP⊆BPPBPP=BPP. Do you see the problem with this proof?\n\nTo get a correct proof (first due to Zachos) we need to use Arthur Merlin games. Consider a Σ2 language L as an ∃∀ expression. Since NP is in BPP, we can replace the ∀ with a probabilistic test. This gives us what is known as MA or a Merlin-Arthur game where the powerful Merlin sends a message that Arthur can probabilistically verify. A now classic result shows that MA is contained in AM, where Arthur provides a random string to Merlin who must then provide a proof based on that string. Once again we apply the NP in BPP assumption to allow Arthur to simulate Merlin probabilistically and now we have a BPP algorithm for L.\n\nThe problem in the first proof is in the second \"⊆\". The assumption NP in BPP does not imply NPA in BPPA for all A.\n\nHow has the internet most affected the study of science? In one word: communication: the ability for scientists to discuss and share their research with each other quickly and cheaply. So I strive to find new ways to use the internet to improve communication. Starting this weblog is one such example. I'd thought I would try another: Instant Messaging.\n\nNow many of you are thinking I am crazy, but for different reasons. Some of you out there have been using instant messaging for years and wondering how I could consider it s \"new\" technology. But many of you out there have barely figured out how to read your email attachments and have hardly even heard of IM.\n\nOn a trial basis, for my weblog readers, I will welcome your instant messages. Talk to me about this weblog, about complexity and computer science in general or about whatever you want. Maybe I'll start a trend and all computer scientists will IM each other. Maybe not but it's worth trying out.\n\nI'm using Yahoo Instant Messaging; my Yahoo id is the imaginative \"fortnow\" (note: I do not read email sent to fortnow@yahoo.com). I put a button on the left column of the weblog home page that tells you when I am online and you can click to connect. I look forward to hearing from you.\n\nThere is a big reorganization in the CISE directorate of NSF. To understand what's happening, let's review the previous structure..\n\nThe National Science Foundation, like most government bureaucracies, has a tree-like structure. At top is the office of the director (Rita Colwell). Below that are several directorates including the Directorate for Computer and Information Science and Engineering (CISE) headed by Peter Freeman. By law every organization in NSF cannot be just \"science\" but \"science and engineering\" except for the Foundation itself.\n\nBelow CISE were several divisions, including Computer-Communications Research (C-CR) headed by Kamal Abdali. C-CR ha several programs including the Theory program headed by Ding-Zhu Du.\n\nPeter Freeman, who recently became head of CISE, has decided to reorganize the whole directorate. Exactly what it will become should be announced next week but there are some hints in this presentation. Change is always scary but I'm hopeful theory will survive. I'll give more details when I know them.\n\nTo overcome the tree structure of NSF, there are a number of cross-disciplinary programs. One such program, Information and Technology Research (ITR), has produced several large, medium and small grants to a variety of projects, including many applications of theory. This is the last year of ITR solicitations and the calls have been well behind schedule, probably not unrelated to the CISE reorganization. This year's topic will be on \"ITR for National Priorities\" with more details promised by Thanksgiving. Unconfirmed rumors have the program will be more focused and only making medium sized grants.\n\nThere are two computer science departments on the University of Chicago campus. The one I belong to, a department in the physical sciences division of the University and the other, the Toyota Technological Institute at Chicago (TTI-C). What is TTI-C?\n\nThe Toyota Technological Institute, a university covering various engineering disciplines located in Nagoya City, Japan, was founded in 1981 from funds from the Toyota Motor Corporation as directed by the Toyoda family. They decided to start a computer science department and locate it in the states to have a broader access to computer science faculty and students. For various reasons they settled on Chicago and set up an agreement with the University of Chicago, using space in the University of Chicago Press building. TTI-C has just officially started up and have already signed up a few strong faculty members including theorist Adam Kalai and Fields medalist Stephen Smale. TTI-C plans to increase its faculty size and start up a graduate program in the near future.\n\nAlthough there will be some sharing of courses and a few of our faculty (including myself) sit on a Local Academic Advisory Council for TTI-C, TTI-C will formally maintain itself as a separate institution from the University. Nevertheless close collaborations between our department and TTI-C has already established an exciting research environment for our combined faculty and students.\n\nThis is for my friends in Boston who suggested I do a sports post.\n\nOne of the great parts of my job is working with people from around the world. I was working with a graduate student, Luis Antunes, from Portugal when we found out that Portugal would play the US in the 2002 World Cup. We had various rounds of taunting back and forth with me fully knowing the US didn't stand a chance in that match. When the US did win, Luis tells me the whole country went into a deep depression. By contrast, for the most part people in the US didn't care.\n\nI can now understand Portugal's pain as the city of Chicago has gone into a similar kind of quiet depression over the Cubs failure to advance to the world series. Impressive what sports can do to the psyche of a city or a country.\n\nMemo to my friends in Boston: Hope things go well for the Sox so your city doesn't end up feeling tomorrow like Chicago does today.\n\nI have tried to keep politics out of this weblog with the exception of issues related to science, in particular science funding and immigration. To celebrate America's fiscal new year, let's talk about immigration.\n\nCongress has declined to renew the higher annual cap on H1-B visas, rolling them back to 65,000 for the fiscal year starting today from 195,000 in 2000. H1-B's allow \"employers to hire foreign workers with special skills they can't find among American job applicants,\" typically for high-tech jobs. But H1-B's are also used for visiting researchers at industrial research labs and some university positions. When the limit is reached, the government will no longer issue more visas until the start of the next fiscal year.\n\nAt NEC, we had postdocs who had to delay their start date until October for this reason, including in some cases those who wanted to start at the beginning of summer. With the limit dramatically decreased, if the job market starts perking up, we could hit the limit much earlier. This could make a real dent in international cooperation in science.\n\nWhat is a one-way function, intuitively a function that is easy to compute and hard to invert? Taking this intuitive idea to a formal definition has yield two quite different meanings, sometimes causing confusion.\n\nThe first directly tries to translate the intuition. A function f is one-way if\n\nf is 1-1 (so an inverse is unique),\n\nf is length increasing (so the output of the inverse function is not too large),\n\nf is computable in polynomial time, and\n\nthere is no polynomial-time computable g such that for all x, g(f(x))=x.\n\nThis is a nice clean definition that fulfills the intuition but is not that useful for cryptography, since f could be easily invertible on all but a small number of inputs, or with stronger adversaries. To handle these issues we have a different looking definition.\n\nA function f is r(n)-secure one-way if\n\nThere is a function l(n)≥n such that f maps strings of length n to strings of length l(n),\n\nf is computable in polynomial time, and\n\nfor all probabilistic polynomial-time algorithms A, the probability that f(A(f(x)))=f(x) is at most r(n) where the probability is taken over x chosen uniformly from the strings of length n and the random coins used by A.\n\nThere are many variations on both definitions and a considerable amount of theory devoted to each. Grollmann and Selman show that one-way functions of the first kind exist if and only if P ≠ UP. On the other hand Håstad, Impagliazzo, Levin and Luby show that from any one-way function of the second kind, one can create a pseudorandom generator.\n\nAt one point I tried using complexity-theoretic one-way functions and cryptographic one-way functions to distinguish the two, but this only caused confusion. So we have to live with the fact that we have these two definitions with the same name and we'll have to just use context to figure out which definition is appropriate. If you give a talk or write a paper about one-way functions, it never hurts to distinguish which version you are talking about.\n\nLast week I posed the following question:\n\n(1) Exhibit an NP-complete language L, such that for all lengths n≥1, L contains exactly half (2n-1) of the strings of length n.\n\nThis question was posed by Ryan O'Donnell and solved by Boaz Barak. Here is a proof sketch.\n\nBy using standard padding and encoding tools, (1) is equivalent to\n\n(2) There is an NP-complete language L and a polynomial-time computable function f such that for every n, there are exactly f(n) strings in L of length n.\n\nFirst we show how to achieve (2) if we replace \"strings\" with \"total witnesses.\" Consider pair of formulas φ and ¬φ. The total number of satisfying assignments between them total 2n if the have n variables. We just create an encoding that puts φ and ¬φ at the same length. The total number of witnesses at that length is equal to 2n times the number of formula pairs encoded at that length.\n\nWe now prove (2) by creating a language L that encodes the following at the same length for each φ\n\nφ, where φ is satisfiable.\n\n(φ,w) where w is a satisfying assignment for φ and there is another satisfying assignment u<w for φ.\n\nYou can check that the language is NP-complete and the total number of strings in L for each φ is just the number of satisfying assignments of φ.\n\nA colleague of mine, who shall remain nameless, likes to schedule time for research, a certain set block of time during the day where he puts off all his todo's and concentrates on science. Sounds good but often his chair will stop by for some discussion or an impromptu meeting. The colleague will say, \"Sorry, but I reserved this time for research&quot, but that argument didn't fly, the chair said he could do research anytime. One day he said instead, \"Sorry I have a squash game\" and the chair replied that they would talk at a future time. Welcome to the academic world, where research gets trumped by a meeting that itself can be trumped by a squash game.\n\nIs scheduling time for research a good idea? It depends on your personality and your research style. If you find yourself with no time to think about an interesting problem because too much else is happening then yes, best to schedule a few hours where you promise yourself you will do nothing else but research during those times. This means more than not preparing for class but also ignoring your computer. Checking email and surfing the web are themselves great time sinks.\n\nIn my case, I find it difficult to just start thinking about research at a given time. So I use the rule that research trumps all and when inspiration hits me, or someone comes to my office with a research question, I drop everything I can to work on the problem. Okay, I can't skip a class for research but email, weblog posts, referee reports, etc., should never stand in the way of science.\n\nThe call for papers for the 2004 ACM Conference on Electronic Commerce is now available. I'm posting this note as my duty as a program committee member to spread the word of the conference.\n\nWhy would an electronic commerce conference want me, a complexity theorist, as a PC member? Electronic commerce has many surprising connections to computational complexity. Consider complex auction situations where different buyers wish to purchase different items with varying needs for combinations of these auctions. One needs to design such auctions which decisions made by the buyers, as well as determining the winner must be computationally efficient. This in addition to the usual needs of auctions to be revenue generating, robust against players trying to cheat the system and other notions of \"fairness.\"\n\nIn a more philosophical sense, what is a large financial market but some sort of massive parallel computation device that takes pieces of information and produces prices for securities. How can we model and analyze this process? Computational complexity should play a major role in understanding this model of computing and allow us to develop more efficient financial markets.\n\nI have a gap in my knowledge of work in theory done between 1979 (the publication of Hopcroft and Ullman) and 1985 (when I started graduate school). So every now and then I see a new result from this time that I should have known years ago. Here is an example from the Winter 1982 SIGACT News, a variation of the regular language pumping lemma due to Donald Stanat and Stephen Weiss.\n\nTheorem: If L is regular then there is a positive integer n such that for every string x of length at least n, there are strings u, v and w with v nonempty such that x=uvw and for all strings r and t and integers k≥0, rut is in L if and only if ruvkt is in L.\n\nWhat surprises me about this result is that w does not appear in the conclusion and that the initial r could put the finite automaton in any state before it gets to u. Here is a sketch of the proof.\n\nLet s be the number of states of a finite automaton accepting L. Let yi be the first i bits of x. For any initial state a, yi will map it to some state b. So one can consider yi as a function mapping states to states. There are at most ss such functions so if |x|≥ss there is an i and a j, i<j such that yi and yj represent the same function. We let u=x1...xi-1 and v=xi...xj-1. The rest follows like the usual pumping lemma.\n\nUsing a result of Jaffe, Stanat and Weiss show that this condition is not only necessary but also sufficient to characterize the regular languages.\n\nCleaning out my office I came across some old SIGACT News that Bill Gear had given me when he cleaned out his office after his retirement. The Winter 1982 edition is quite interesting. I was a freshman in college that year, well before I was part of the theory community.\n\nThere are some interesting technical articles that I will get to in future posts. But the first two pages were letters to the editor that are chilling reminders of the cold war during that time.\n\nOn page two was the following short note from Witold Lipski, Jr. and Antoni Mazurkiewicz from the Polish Academy of Sciences.\n\nWe are very sorry to inform you that due to the situation in Poland we do not see any chance to organize our 1982 Conference on Mathematical Foundations of Computer Science.\n\nMFCS started in 1972 as an annual conference rotating between Poland and Czechoslovakia, and now between Poland, Slovakia and the Czech Republic. There was no conferences in 1982 or 1983 and the conference did not return to Poland until 1989.\n\nTalking about the Czechs, there was a much longer letter on page one from James Thatcher of IBM. Here are some excerpts.\n\nOn a recent trip to Europe, I visited Prague and had the pleasure of talking with Dr. Ivan M. Havel who is a friend and colleague of many years. Ivan Havel received his Ph.D. in CS from Berkeley in 1971. He joined the Institute for Applied Computer Technology in Prague in 1972 and then in 1974 became a member of the Czechoslovakian Academy of Sciences, in the Institute of Information Theory and Automation.\n\nIvan's brother, Vaclav Havel, an internationally known playwright, was imprisoned in 1979 for four and a half years for his activities in connection with the Charter 1977 movement.\n\nIn 1980, possibly related to his refusal to denounce his brother, Ivan Havel was removed from his position in the Academy of Sciences and was unemployed for several months. Last May, he and Vaclav's wife were arrested and charged with \"subversion\" for allegedly \"collecting and distributing written material oriented against the socialist state and social establishment, with hostile intentions.\" After four days detention, they were released.\n\nHe is employed as a programmer-analyst by META, a home-worker program for the handicapped.\n\nIvan Havel remained a programmer until after the Velvet Revolution in 1989. After some political work in 1990, he became a docent (associate professor) at Charles University and director of the Center for Theoretical Study where he remains today.\n\nHis brother Vaclav went on to become president of the Czech Republic.\n\nWhen I move back to Chicago, I will go back to my old email address . I got to thinking about how my career can be described by my email addresses.\n\nAs an undergrad at Cornell, I spent several years working for computer services writing an email system in assembly language for the IBM 370. The system was scrapped shortly after I left for grad school at Berkeley. After a year at Berkeley, I followed by advisor, Michael Sipser, to MIT.\n\nI had email addresses at Cornell and Berkeley but I have long since forgotten them. At MIT I wanted the userid \"lance\", but the name was taken by Lance Glasser, then an MIT professor. So my email became .\n\nWhen I graduated and went to Chicago, I decided to stick with the userid \"fortnow\" for an email of . This bucked the trend at the time of having first names for email at Chicago so I had to have aliased to . When the university started system wide email I got though also works.\n\nWhen I did a sabbatical in Amsterdam my email became or simply . When I moved to the NEC Research Institute my email because aliased to and when the NEC Research Institute became NEC Laboratories America I got my current email .\n\nIn addition to this, the ACM has created permanent email addresses, permanent as long as you are an ACM member and I did create an address though I never did give it out (until now). My brother and I now own the domain fortnow.com and I have what I do call my permanent address, . I also am the default receiver for fortnow.com mail, which means that addresses like , or even will all go to me.\n\nAll of the email addresses in this post still work and forward to me. But I will stick to using two main email addresses, for work related email and for non-work emails.\n\nI used javascript to generate the emails in this post to avoid adding even more to my heavy spam load. We'll see if it works or whether I start getting spam sent to .\n\nA few months ago I had a post describing information markets, a system of buying and selling securities that pay off if a given future event happens. Based on the price of a security, one can get an estimate of the probability that that event will occur. Studies have shown that information markets are better predictors than polls or experts.\n\nInformation markets have taken a blow in the past few days. The US Department of Defense has cancelled a program that would have set up limited futures markets on securities based on terroristic activities. They bowed to pressure from senators who consider it morally wrong to bet on events on future terrorist attacks. I understand their concerns but computer scientists and economists have produced what could have been a powerful tool in controlling terrorism and it is quite a shame to see it discarded so easily.\n\nDavid Pennock sent me some links on a more positive point of view from CNN, Fortune and Wired and a fun CNN piece on the Tradesports Poindexter future.\n\nUpdate (8/1): A well-written New York Times column A Good Idea with Bad Press and a nicely argued opinion piece by David Pennock.\n\nWay back when I was a graduate student, I moved from Berkeley to MIT. I put what few belongings I had into boxes and shipped them via UPS. My brother flew out and we drove across the country together. Those were the days.\n\nNow making the move back to Chicago is not nearly so simple. We have houses to sell and buy. Getting our kids ready for a new school. Real estate agents, lawyers, mortgage and insurance people to deal with. Meanwhile there is academic work that needs to get done before the real move. Conference and grant deadlines don't move to accommodate my move.\n\nSo this weblog might get a little spotty until I get settled into Chicago, sometime in mid-September. I'll try to find some time for some posts during that time but don't expect too much. If you are having complexity weblog withdrawal check out the archives. Nice thing about complexity--old stuff doesn't (usually) get stale.\n\nAnother rump session talk by Scott Aaronson showed that BQP/qpoly is contained in EXP/poly. In other words, everything efficiently quantumly computable with a polynomial amount of arbitrarily entangled quantum advice can be simulated in exponential time with a polynomial amount of classical advice.\n\nLet me try to put this in context while avoiding quantum mechanics. Advice is method for encoding a different program for each input length. We define the class P/poly as those languages computable in polynomial time with access to a polynomially-long advice string an where the string an depends only on the length of the input. P/poly is equivalent to those problems having nonuniform polynomial-size circuits.\n\nQuantum advice is a bit more tricky, since it can be in a superposition of regular advice strings. Formally, quantum advice is an exponentially long vector of numbers βa where βa is the amplitude of advice string a. For simplicity let us assume those numbers are real and we'll also have the restriction that the sum of the squares of the amplitudes is one.\n\nYou can see there are far more ways to give quantum advice than classical advice. But the quantum machines are limited in how they can use the advice. Harry Buhrman asked whether one can give any limit at all to what one can do with quantum advice. Scott Aaronson gives an answer: No better than classical advice as long as you are allowed (classical) exponential time.\n\nIdeally one would like that efficient quantum algorithms with quantum advice can be simulated with efficient quantum algorithms with classical advice. Still Aaronson's result shows that even with fully entangled advice one cannot get all the information out of it.\n\nDuring the rump session of complexity, Nikolai Vereshchagin presented a combinatorial theorem that he proved using Kolmogorov complexity. Let A be a finite subset of N×N where N is the set of natural numbers. Let m be the size of A, r be the number of nonempty rows of A and c the number of nonempty columns.\n\nWe say A is good is every nonempty row has m/r elements and every nomempty column has m/c elements of A. A rectangle has this property, as does a diagonal. We say A is k-good if every nonempty row has at most km/r elements and every nonempty column has at most km/c elements. A is good if it is 1-good.\n\nVereshchagin's Theorem: There is a constant c such that for all finite subsets B of N×N with n = log |B| there is a partition of B into at most nc sets each of which is nc-good.\n\nVereshchagin asks whether there is a purely combinatorial proof of this theorem. If you know of one let me know.\n\nFor those who know some Kolmogorov complexity, let me sketch the proof: We label each point (x,y) of B with the following five values: KB(x,y), KB(x), KB(y), KB(x|y) and KB(y|x). We partition the points into sets with the same labels. Standard counting arguments from Kolmogorov complexity show that each partition is nc-good for some c.\n\nUpdate\n\nA doctor, lawyer and mathematician were discussing whether it was better to have a wife or a girlfriend. The doctor said it was better to have a wife because it is medically safer to have a single partner. The lawyer said it was better to have a girlfriend to avoid the legal hassles of marriage. The mathematician said it was better to have both.\n\n\"Both?\" said the doctor and the lawyer. \"Yes,\" said the mathematician, \"That way the wife thinks I'm with the girlfriend, the girlfriend thinks I'm with the wife and I can do some math.\"\n\nI was reminded of that joke by the recent New York Times article Pure Math, Pure Joy and the accompanying slideshow. Those pictures look all too familiar.\n\nThe greatest lovers of math though are not the famous mathematicians at places like Berkeley and Harvard. Rather the mathematicians who take low-paying jobs with high teaching loads at less-strong colleges or move from visiting position to visiting position just to have some occasional time to do math. They have a dedication (or perhaps an addiction) I can never fully appreciate.\n\nExpander graphs informally are graphs that given any subset S that is not too large, the set of vertices connected to S contains a large number of vertices outside of S. There are many constructions and applications for expander graphs leading to entire courses on the subject.\n\nThe adjacency matrix A of a graph G of n vertices is an n×n matrix such that ai,j is 1 if there is an edge between vertices i and j and 0 otherwise. Noga Alon noticed that a graph that has a large gap between the first and second eigenvalue of the adjacency matrix will be a good expander.\n\nWe can use ε-biased sets to get expanders. Let S be a ε-biased set for Fm for F the field of 2 elements. Consider the graph G consisting of 2m vertices labelled with the elements of Fm and an edge from x to y if y=x+s or x=y+s. This kind of graph G is known as a Cayley graph.\n\nBy looking at the eigenvalues the adjacency matrix A of G we can show G is an expander. The eigenvectors v are just the vectors corresponding to the functions g in L described earlier. For any vector a we have\n\n(Ag)(a) = Σs in S g(a+s) = g(a) Σs in S g(s) since g(a+s) = g(a)g(s). Let g(S) = Σs in S g(s). We now have that Ag = g(S) g. So g is an eigenvector with eigenvalue g(S). If g is the constant one function then g(S)=|S|. Since S is an ε-biased set, g(S)≤ε|S| for every other g, so the second eigenvalue is much smaller than the largest eigenvalue and G must be an expander.\n\nThe June 2003 SIGACT News is out. Aduri Pavan wrote this months Complexity Theory Column on \"Comparison of Reductions and Completeness Notions\".\n\nAs I have mentioned before in this weblog, I heartily encourage joining SIGACT, the ACM Special Interest Group on Algorithms and Computation Theory. You get the SIGACT News, discounts on conferences and as I discovered last night from home, you apparently get online access to the STOC proceedings. Not to mention supporting the theory community. All this for the low price of $18 ($9 for students).\n\nWhat about the ACM itself? I have been an ACM member since graduate school since I feel it is important to support the main computer science organization. But for the additional $96 ($42 for students) there are no real significant benefits over joining SIGACT alone.\n\nε-biased sets are an interesting concept that I have seen recently in a few papers but never seemed to have a clear description. At FCRC Eli Ben-Sasson gave me a good explanation and I will try to recreate it here.\n\nLet F be the field of 2 elements 0 and 1 with addition and multiplication done modulo 2. Fix a dimension m. Let L be the set of functions g mapping elements of Fm to {-1,1} with the property that g(x+y)=g(x)g(y). Here x+y represents addition done coordinate-wise modulo 2. One example of a g in L is g(x1,x2,x3)=(-1)x1 (-1)x3.\n\nThere is the trivial function g in L that always maps to 1. For every non-trivial g in L exactly half of the elements in Fm map to 1 and the others to -1. If one picks a reasonably large subset S of Fm at random then high probability, g will map about half the elements to 1 and the rest to -1. In other words the expected value of g(x) for x uniformly chosen in S is smaller than some small value ε. If this is true we say S is ε-biased for g.\n\nAn ε-biased set is a set S such that for all nontrivial g in L, S is ε-biased for g. Formally this means that\n\nΣx in S g(x) ≤ ε|S|. Not only do reasonable size ε-biased sets exists but they can be found efficiently. Naor and Naor found the first efficiently constructible ε-biased sets of size polynomial in m and 1/ε.\n\nOne can extend the notion of ε-biased sets to fields F of p elements for arbitrary prime p. L would now be the set of functions g mapping elements of Fm to the complex pth roots of unity, e2π(j/p)i for 0≤j≤p-1 again with the property that g(x+y)=g(x)g(y). Various constructions have created generalized ε-biased sets of size polynomial in m, 1/ε and log p.\n\nFor applications let me quote from the recent STOC paper by Ben-Sasson, Sudan, Vadhan and Wigderson that used ε-biased sets to get efficient low-degree tests and smaller probabilistically checkable proofs. You can get more information and references from that paper.\n\nSince the introduction of explicit ε-biased sets, the set and diversity of applications of these objects grew quickly, establishing their fundamental role in theoretical computer science. The settings where ε-biased sets are used include: the direct derandomization of algorithms such as fast verification of matrix multiplication and communication protocols for equality; the construction of almost k-wise independent random variables, which in turn have many applications; inapproximability results for quadratic equation over GF(2); learning theory; explicit constructions of Ramsey graphs; and elementary constructions of Cayley expanders.\n\nAfter the FCRC meetings I attended were concluded, I headed up to UCSD for the celebration of Walter Savitch for his sixtieth birthday and upcoming retirement. He gained his fame in complexity for Savitch's Theorem that shows \"P=NP\" for space.\n\nI learned quite a bit at the meeting. Walt Savitch was Steve Cook's first student, his only student while Cook was at Berkeley in his pre-Toronto pre-\"SAT is NP-complete\" days. Also as Cook said, Savitch is the only student he has had with a theorem named after him. That theorem made up a good part of Savitch's Ph.D. thesis. At the celebration Cook gave an overview on propositional proof systems.\n\nAfter coming to UCSD, Savitch did some work on computational linguistics and one of the leaders of the field, Aravind Joshi, gave a talk on combining trees to keep the structure when parsing sentences.\n\nSavitch is probably best known now in computer science for his textbooks in introductory programming that likley many of you have used.\n\nCongrats Walt on a fine career and here's hoping retirement doesn't slow you down.\n\nAs promised I added links to the papers in the post on the STOC business meeting. Let me say some more words on the winner of the Gödel prize\n\nValiant developed the concept of PAC (Probably Approximably Correct) learning as roughly where a learner sees a small number of labelled examples from a distribution and with high confidence will generate a hypothesis that with high probability will correctly label instances drawn from the same distribution.\n\nA strong learner has confidence close to 100%; a weak learner has confidence only slightly better than 50%. Schapire, using a technique called boosting, showed how to convert a weak learner to a strong learner. This is a wonderful theoretical result but the algorithm had problems that made it difficult to implement.\n\nIn their Gödel prize winning paper, A decision-theoretic generalization of on-line learning and an application to boosting, Freund and Schapire develop the adaboost algorithm that solves many of these issues and has become a staple of the theoretical and practical machine learning community.\n\nBoosting has its own web site where you can find much more information about the algorithms and applications.\n\nAlonzo Church was born a hundred years ago today in Washington, DC. Church is best known for the λ-calculus, a simple method for expressing and applying functions that has the same computational power as Turing machines.\n\nWith Rosser in 1936, he showed that λ-expressions that reduce to an irreducible normal form have a unique normal form. In that same year he showed the impossibility of decided whether such a normal form existed.\n\nChurch's thesis, which he states as a definition: \"An effectively calculable function of the positive integers is a λ-definable function of the positive integers.\"\n\nAgain in 1936, Kleene and Church showed that computing normal forms have the equivalent power of the recursive functions of Turing machines. And thus the Church-Turing thesis was born: Everything computable is computable by a Turing machine.\n\nThe λ-calculus also set the stage for many of the functional programming languages like lisp and scheme.\n\nAlonzo Church passed away on August 11, 1995 in Ohio.\n\nI have mixed feelings about the Federated Computing Research Conference. It is a good idea to get many different areas of computer science together. I do get to see many people I haven't seen in years who went into non-theoretical areas of CS.\n\nOn the other hand 2200 participants made the place quite crowded and it seemed to take away from the informal atmosphere of most theory conference. Since STOC and Electronic Commerce had nearly a complete overlap I jumped back and forth between talks never really feeling fully part of either conference.\n\nFor the first time the Complexity conference was not part of FCRC because 2003 is a Europe year for Complexity. In an informal poll I took of STOC people interested in complexity most liked having both conferences at the same place but would rather that happen in isolation, like last year in Montreal, rather than as part of the much larger FCRC meeting.\n\nIn what seems to be a trend in CS conferences, wireless internet was made available at the conference site. As you walked around you would pass many people sitting on chairs and on the ground hunched over their laptops disconnected from the conference and connected into another world. Seemed a bit depressing but I too found the net hard to resist--it is always tempting to simply open my laptop and connect, checking email and posting to this weblog."
    }
}
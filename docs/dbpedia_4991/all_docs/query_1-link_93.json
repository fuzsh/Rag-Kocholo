{
    "id": "dbpedia_4991_1",
    "rank": 93,
    "data": {
        "url": "https://docs.ultralytics.com/guides/nvidia-jetson/",
        "read_more_link": "",
        "language": "en",
        "title": "NVIDIA Jetson",
        "top_image": "https://github.com/ultralytics/ultralytics/assets/20147381/c68fb2eb-371a-43e5-b7b8-2b869d90bc07",
        "meta_img": "https://github.com/ultralytics/ultralytics/assets/20147381/c68fb2eb-371a-43e5-b7b8-2b869d90bc07",
        "images": [
            "https://assets-global.website-files.com/646dd1f1a3703e451ba81ecc/6627a6e4d47ce284268ea3b9_yolov82_release.svg",
            "https://assets-global.website-files.com/646dd1f1a3703e451ba81ecc/6627a163dd84c20540c5ea7a_yolov82_effects.svg",
            "https://assets-global.website-files.com/646dd1f1a3703e451ba81ecc/6627a163ab932e33983215d4_arrow_effects.svg",
            "https://raw.githubusercontent.com/ultralytics/assets/main/logo/Ultralytics_Logotype_Reverse.svg",
            "https://raw.githubusercontent.com/ultralytics/assets/main/logo/Ultralytics_Logotype_Reverse.svg",
            "https://github.com/ultralytics/ultralytics/assets/20147381/c68fb2eb-371a-43e5-b7b8-2b869d90bc07",
            "https://github.com/ultralytics/ultralytics/assets/20147381/202950fa-c24a-43ec-90c8-4d7b6a6c406e",
            "https://github.com/ultralytics/ultralytics/assets/20147381/f7017975-6eaa-4d02-8007-ab52314cebfd",
            "https://github.com/lakshanthad.png",
            "https://github.com/glenn-jocher.png",
            "https://github.com/RizwanMunawar.png",
            "https://github.com/Ahelsamahy.png",
            "https://github.com/Burhan-Q.png"
        ],
        "movies": [
            "https://www.youtube.com/embed/mUybgOlSxxA"
        ],
        "keywords": [],
        "meta_keywords": [
            "Ultralytics",
            "YOLOv8",
            "NVIDIA Jetson",
            "JetPack",
            "AI deployment",
            "performance benchmarks",
            "embedded systems",
            "deep learning",
            "TensorRT",
            "computer vision"
        ],
        "tags": null,
        "authors": [
            "Ultralytics"
        ],
        "publish_date": "2024-04-02T01:31:15-07:00",
        "summary": "",
        "meta_description": "Learn to deploy Ultralytics YOLOv8 on NVIDIA Jetson devices with our detailed guide. Explore performance benchmarks and maximize AI capabilities.",
        "meta_lang": "en",
        "meta_favicon": "../../assets/favicon.ico",
        "meta_site_name": "",
        "canonical_link": "https://docs.ultralytics.com/guides/nvidia-jetson/",
        "text": "Quick Start Guide: NVIDIA Jetson with Ultralytics YOLOv8\n\nThis comprehensive guide provides a detailed walkthrough for deploying Ultralytics YOLOv8 on NVIDIA Jetson devices. Additionally, it showcases performance benchmarks to demonstrate the capabilities of YOLOv8 on these small and powerful devices.\n\nWatch: How to Setup NVIDIA Jetson with Ultralytics YOLOv8\n\nWhat is NVIDIA Jetson?\n\nNVIDIA Jetson is a series of embedded computing boards designed to bring accelerated AI (artificial intelligence) computing to edge devices. These compact and powerful devices are built around NVIDIA's GPU architecture and are capable of running complex AI algorithms and deep learning models directly on the device, without needing to rely on cloud computing resources. Jetson boards are often used in robotics, autonomous vehicles, industrial automation, and other applications where AI inference needs to be performed locally with low latency and high efficiency. Additionally, these boards are based on the ARM64 architecture and runs on lower power compared to traditional GPU computing devices.\n\nNVIDIA Jetson Series Comparison\n\nJetson Orin is the latest iteration of the NVIDIA Jetson family based on NVIDIA Ampere architecture which brings drastically improved AI performance when compared to the previous generations. Below table compared few of the Jetson devices in the ecosystem.\n\nJetson AGX Orin 64GB Jetson Orin NX 16GB Jetson Orin Nano 8GB Jetson AGX Xavier Jetson Xavier NX Jetson Nano AI Performance 275 TOPS 100 TOPS 40 TOPs 32 TOPS 21 TOPS 472 GFLOPS GPU 2048-core NVIDIA Ampere architecture GPU with 64 Tensor Cores 1024-core NVIDIA Ampere architecture GPU with 32 Tensor Cores 1024-core NVIDIA Ampere architecture GPU with 32 Tensor Cores 512-core NVIDIA Volta architecture GPU with 64 Tensor Cores 384-core NVIDIA Volta™ architecture GPU with 48 Tensor Cores 128-core NVIDIA Maxwell™ architecture GPU GPU Max Frequency 1.3 GHz 918 MHz 625 MHz 1377 MHz 1100 MHz 921MHz CPU 12-core NVIDIA Arm® Cortex A78AE v8.2 64-bit CPU 3MB L2 + 6MB L3 8-core NVIDIA Arm® Cortex A78AE v8.2 64-bit CPU 2MB L2 + 4MB L3 6-core Arm® Cortex®-A78AE v8.2 64-bit CPU 1.5MB L2 + 4MB L3 8-core NVIDIA Carmel Arm®v8.2 64-bit CPU 8MB L2 + 4MB L3 6-core NVIDIA Carmel Arm®v8.2 64-bit CPU 6MB L2 + 4MB L3 Quad-Core Arm® Cortex®-A57 MPCore processor CPU Max Frequency 2.2 GHz 2.0 GHz 1.5 GHz 2.2 GHz 1.9 GHz 1.43GHz Memory 64GB 256-bit LPDDR5 204.8GB/s 16GB 128-bit LPDDR5 102.4GB/s 8GB 128-bit LPDDR5 68 GB/s 32GB 256-bit LPDDR4x 136.5GB/s 8GB 128-bit LPDDR4x 59.7GB/s 4GB 64-bit LPDDR4 25.6GB/s\"\n\nFor a more detailed comparison table, please visit the Technical Specifications section of official NVIDIA Jetson page.\n\nWhat is NVIDIA JetPack?\n\nNVIDIA JetPack SDK powering the Jetson modules is the most comprehensive solution and provides full development environment for building end-to-end accelerated AI applications and shortens time to market. JetPack includes Jetson Linux with bootloader, Linux kernel, Ubuntu desktop environment, and a complete set of libraries for acceleration of GPU computing, multimedia, graphics, and computer vision. It also includes samples, documentation, and developer tools for both host computer and developer kit, and supports higher level SDKs such as DeepStream for streaming video analytics, Isaac for robotics, and Riva for conversational AI.\n\nFlash JetPack to NVIDIA Jetson\n\nThe first step after getting your hands on an NVIDIA Jetson device is to flash NVIDIA JetPack to the device. There are several different way of flashing NVIDIA Jetson devices.\n\nIf you own an official NVIDIA Development Kit such as the Jetson Orin Nano Developer Kit, you can download an image and prepare an SD card with JetPack for booting the device.\n\nIf you own any other NVIDIA Development Kit, you can flash JetPack to the device using SDK Manager.\n\nIf you own a Seeed Studio reComputer J4012 device, you can flash JetPack to the included SSD and if you own a Seeed Studio reComputer J1020 v2 device, you can flash JetPack to the eMMC/ SSD.\n\nIf you own any other third party device powered by the NVIDIA Jetson module, it is recommended to follow command-line flashing.\n\nNote\n\nFor methods 3 and 4 above, after flashing the system and booting the device, please enter \"sudo apt update && sudo apt install nvidia-jetpack -y\" on the device terminal to install all the remaining JetPack components needed.\n\nJetPack Support Based on Jetson Device\n\nThe below table highlights NVIDIA JetPack versions supported by different NVIDIA Jetson devices.\n\nJetPack 4 JetPack 5 JetPack 6 Jetson Nano ✅ ❌ ❌ Jetson TX2 ✅ ❌ ❌ Jetson Xavier NX ✅ ✅ ❌ Jetson AGX Xavier ✅ ✅ ❌ Jetson AGX Orin ❌ ✅ ✅ Jetson Orin NX ❌ ✅ ✅ Jetson Orin Nano ❌ ✅ ✅\n\nQuick Start with Docker\n\nThe fastest way to get started with Ultralytics YOLOv8 on NVIDIA Jetson is to run with pre-built docker images for Jetson. Refer to the table above and choose the JetPack version according to the Jetson device you own.\n\nAfter this is done, skip to Use TensorRT on NVIDIA Jetson section.\n\nStart with Native Installation\n\nFor a native installation without Docker, please refer to the steps below.\n\nRun on JetPack 6.x\n\nInstall Ultralytics Package\n\nHere we will install Ultralytics package on the Jetson with optional dependencies so that we can export the PyTorch models to other different formats. We will mainly focus on NVIDIA TensorRT exports because TensorRT will make sure we can get the maximum performance out of the Jetson devices.\n\nUpdate packages list, install pip and upgrade to latest\n\nsudo apt update sudo apt install python3-pip -y pip install -U pip\n\nInstall ultralytics pip package with optional dependencies\n\npip install ultralytics[export]\n\nReboot the device\n\nsudo reboot\n\nInstall PyTorch and Torchvision\n\nThe above ultralytics installation will install Torch and Torchvision. However, these 2 packages installed via pip are not compatible to run on Jetson platform which is based on ARM64 architecture. Therefore, we need to manually install pre-built PyTorch pip wheel and compile/ install Torchvision from source.\n\nInstall torch 2.3.0 and torchvision 0.18 according to JP6.0\n\nVisit the PyTorch for Jetson page to access all different versions of PyTorch for different JetPack versions. For a more detailed list on the PyTorch, Torchvision compatibility, visit the PyTorch and Torchvision compatibility page.\n\nInstall onnxruntime-gpu\n\nThe onnxruntime-gpu package hosted in PyPI does not have aarch64 binaries for the Jetson. So we need to manually install this package. This package is needed for some of the exports.\n\nAll different onnxruntime-gpu packages corresponding to different JetPack and Python versions are listed here. However, here we will download and install onnxruntime-gpu 1.18.0 with Python3.10 support.\n\nRun on JetPack 5.x\n\nInstall Ultralytics Package\n\nHere we will install Ultralytics package on the Jetson with optional dependencies so that we can export the PyTorch models to other different formats. We will mainly focus on NVIDIA TensorRT exports because TensorRT will make sure we can get the maximum performance out of the Jetson devices.\n\nUpdate packages list, install pip and upgrade to latest\n\nsudo apt update sudo apt install python3-pip -y pip install -U pip\n\nInstall ultralytics pip package with optional dependencies\n\npip install ultralytics[export]\n\nReboot the device\n\nsudo reboot\n\nInstall PyTorch and Torchvision\n\nThe above ultralytics installation will install Torch and Torchvision. However, these 2 packages installed via pip are not compatible to run on Jetson platform which is based on ARM64 architecture. Therefore, we need to manually install pre-built PyTorch pip wheel and compile/ install Torchvision from source.\n\nUninstall currently installed PyTorch and Torchvision\n\npip uninstall torch torchvision\n\nInstall PyTorch 2.1.0 according to JP5.1.3\n\nsudo apt-get install -y libopenblas-base libopenmpi-dev wget https://developer.download.nvidia.com/compute/redist/jp/v512/pytorch/torch-2.1.0a0+41361538.nv23.06-cp38-cp38-linux_aarch64.whl -O torch-2.1.0a0+41361538.nv23.06-cp38-cp38-linux_aarch64.whl pip install torch-2.1.0a0+41361538.nv23.06-cp38-cp38-linux_aarch64.whl\n\nInstall Torchvision v0.16.2 according to PyTorch v2.1.0\n\nsudo apt install -y libjpeg-dev zlib1g-dev git clone https://github.com/pytorch/vision torchvision cd torchvision git checkout v0.16.2 python3 setup.py install --user\n\nVisit the PyTorch for Jetson page to access all different versions of PyTorch for different JetPack versions. For a more detailed list on the PyTorch, Torchvision compatibility, visit the PyTorch and Torchvision compatibility page.\n\nInstall onnxruntime-gpu\n\nThe onnxruntime-gpu package hosted in PyPI does not have aarch64 binaries for the Jetson. So we need to manually install this package. This package is needed for some of the exports.\n\nAll different onnxruntime-gpu packages corresponding to different JetPack and Python versions are listed here. However, here we will download and install onnxruntime-gpu 1.17.0 with Python3.8 support.\n\nUse TensorRT on NVIDIA Jetson\n\nOut of all the model export formats supported by Ultralytics, TensorRT delivers the best inference performance when working with NVIDIA Jetson devices and our recommendation is to use TensorRT with Jetson. We also have a detailed document on TensorRT here.\n\nConvert Model to TensorRT and Run Inference\n\nThe YOLOv8n model in PyTorch format is converted to TensorRT to run inference with the exported model.\n\nNVIDIA Jetson Orin YOLOv8 Benchmarks\n\nYOLOv8 benchmarks were run by the Ultralytics team on 10 different model formats measuring speed and accuracy: PyTorch, TorchScript, ONNX, OpenVINO, TensorRT, TF SavedModel, TF GraphDef, TF Lite, PaddlePaddle, NCNN. Benchmarks were run on Seeed Studio reComputer J4012 powered by Jetson Orin NX 16GB device at FP32 precision with default input image size of 640.\n\nComparison Chart\n\nEven though all model exports are working with NVIDIA Jetson, we have only included PyTorch, TorchScript, TensorRT for the comparison chart below because, they make use of the GPU on the Jetson and are guaranteed to produce the best results. All the other exports only utilize the CPU and the performance is not as good as the above three. You can find benchmarks for all exports in the section after this chart.\n\nDetailed Comparison Table\n\nThe below table represents the benchmark results for five different models (YOLOv8n, YOLOv8s, YOLOv8m, YOLOv8l, YOLOv8x) across ten different formats (PyTorch, TorchScript, ONNX, OpenVINO, TensorRT, TF SavedModel, TF GraphDef, TF Lite, PaddlePaddle, NCNN), giving us the status, size, mAP50-95(B) metric, and inference time for each combination.\n\nExplore more benchmarking efforts by Seeed Studio running on different versions of NVIDIA Jetson hardware.\n\nReproduce Our Results\n\nTo reproduce the above Ultralytics benchmarks on all export formats run this code:\n\nBest Practices when using NVIDIA Jetson\n\nWhen using NVIDIA Jetson, there are a couple of best practices to follow in order to enable maximum performance on the NVIDIA Jetson running YOLOv8.\n\nEnable MAX Power Mode\n\nEnabling MAX Power Mode on the Jetson will make sure all CPU, GPU cores are turned on.\n\nsudo nvpmodel -m0\n\nEnable Jetson Clocks\n\nEnabling Jetson Clocks will make sure all CPU, GPU cores are clocked at their maximum frequency.\n\nsudo jetson_clocks\n\nInstall Jetson Stats Application\n\nWe can use jetson stats application to monitor the temperatures of the system components and check other system details such as view CPU, GPU, RAM utilization, change power modes, set to max clocks, check JetPack information\n\nsudo apt update sudo pip install jetson-stats sudo reboot jtop\n\nNext Steps\n\nCongratulations on successfully setting up YOLOv8 on your NVIDIA Jetson! For further learning and support, visit more guide at Ultralytics YOLOv8 Docs!\n\nFAQ\n\nHow do I deploy Ultralytics YOLOv8 on NVIDIA Jetson devices?\n\nDeploying Ultralytics YOLOv8 on NVIDIA Jetson devices is a straightforward process. First, flash your Jetson device with the NVIDIA JetPack SDK. Then, either use a pre-built Docker image for quick setup or manually install the required packages. Detailed steps for each approach can be found in sections Quick Start with Docker and Start with Native Installation.\n\nWhat performance benchmarks can I expect from YOLOv8 models on NVIDIA Jetson devices?\n\nYOLOv8 models have been benchmarked on various NVIDIA Jetson devices showing significant performance improvements. For example, the TensorRT format delivers the best inference performance. The table in the Detailed Comparison Table section provides a comprehensive view of performance metrics like mAP50-95 and inference time across different model formats.\n\nWhy should I use TensorRT for deploying YOLOv8 on NVIDIA Jetson?\n\nTensorRT is highly recommended for deploying YOLOv8 models on NVIDIA Jetson due to its optimal performance. It accelerates inference by leveraging the Jetson's GPU capabilities, ensuring maximum efficiency and speed. Learn more about how to convert to TensorRT and run inference in the Use TensorRT on NVIDIA Jetson section.\n\nHow can I install PyTorch and Torchvision on NVIDIA Jetson?\n\nTo install PyTorch and Torchvision on NVIDIA Jetson, first uninstall any existing versions that may have been installed via pip. Then, manually install the compatible PyTorch and Torchvision versions for the Jetson's ARM64 architecture. Detailed instructions for this process are provided in the Install PyTorch and Torchvision section.\n\nWhat are the best practices for maximizing performance on NVIDIA Jetson when using YOLOv8?\n\nTo maximize performance on NVIDIA Jetson with YOLOv8, follow these best practices:\n\nEnable MAX Power Mode to utilize all CPU and GPU cores.\n\nEnable Jetson Clocks to run all cores at their maximum frequency.\n\nInstall the Jetson Stats application for monitoring system metrics.\n\nFor commands and additional details, refer to the Best Practices when using NVIDIA Jetson section."
    }
}
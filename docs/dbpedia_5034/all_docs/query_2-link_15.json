{
    "id": "dbpedia_5034_2",
    "rank": 15,
    "data": {
        "url": "https://iceis.scitevents.org/iceis2004/abstracts_2004.htm",
        "read_more_link": "",
        "language": "en",
        "title": "ICEIS 2004 International Conference on Enterprise Information Systems, CFP available",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://iceis.scitevents.org/iceis2004/imagens/logo%20iceis.jpg",
            "https://iceis.scitevents.org/iceis2004/imagens/brasao-uptemb.gif",
            "https://iceis.scitevents.org/iceis2004/imagens/dblp2.gif"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Title:\n\nA RECONFIGURATION ALGORITHM FOR DISTRIBUTED COMPUTER NETWORKS\n\nAuthor(s):\n\nChanan Glezer , Moshe Zviran\n\nAbstract: This article presents an algorithmic reconfiguration model, combining mechanisms of load balancing and fault tolerance in order to increase utilization of computer resources in a distributed multi-server, multi-tasking environment. The model has been empirically tested in a network of computers controlling telecommunication hubs and is compared to previous efforts to address this challenge. Title:\n\nBVA+ - A BIT VECTORS ALGORITHM FOR ACCELERATING QUERIES IN MULTILEVEL SECURE DATABASES\n\nAuthor(s):\n\nRamzi Haraty , Arda Zeitunlian\n\nAbstract: Much research has been done in the area of multilevel database systems, especially in the security area and accelerating queries. In this paper, we present BVA+, which is based on bit vectors to accelerate queries in multilevel secure database systems. As its predecessor (BVA), the BVA+ algorithm follows the classic Sea View Model, but it recovers query output from single-level relations in a faster and more space-efficient manner than the previous works performed on this subject. In addition, the BVA+ algorithm does not produce spurious or extra tuples, which have always been a major problem in the area of multilevel secure database systems. Title:\n\nCONNECTIVITY OF ERP SYSTEM\n\nAuthor(s):\n\nVatcharaporn Esichaikul\n\nAbstract: The study is an attempt to propose the criteria for determining the appropriate connectivity of ERP systems. The result of this study provides a framework assisting ERP adopters in selecting integration approach which are appropriate to their needs. A survey was conducted to obtain information from ERP users to learn about their opinions on factors and criteria affecting connectivity of ERP systems. Findings from the study revealed that data oriented approach and application integration oriented approach are the most preferred integration methodologies. Opinions on criteria for evaluating ERP connectivity are nature of business process of organization, availability of technologies and service supports, nature of information system of organization, system flexibility, degree of integration, transaction volume, implementation cost, ease of maintenance, implementation time, security, and budget. Finally, the study proposes a framework to determine appropriate connectivity of ERP systems. Title:\n\nCONCEPTUAL MODEL FOR SOFTWARE FAULT LOCALIZATION\n\nAuthor(s):\n\nAbdallah Tubaishat\n\nAbstract: Existing cognitive science and psychology studies suggest that a bi-level approach to fault localization is needed with both shallow and deep reasoning. This approach form the underpinnings for developing our Conceptual Model for Software Fault Localization (CMSFL) to aid programmers with the problem of software fault localization. Our CMSFL proposes that, during the fault localization process programmers build two mental models: an actual code model (the buggy code), and an expectation model (the correct code). A multi dimensional approach is suggested with both shallow and deep reasoning phases to enhance the probability of localizing many types of faults. Title:\n\nASSESSING EFFORT PREDICTION MODELS FOR CORRECTIVE SOFTWARE MAINTENANCE - AN EMPIRICAL STUDY\n\nAuthor(s):\n\nEugenio Pompella , Andrea De Lucia , Silvio Stefanucci\n\nAbstract: We present an assessment of an empirical study aiming at building effort estimation models for corrective maintenance projects. We show results from the application of the prediction models to a new corrective maintenance project within the same enterprise and the same type of software systems used in a previous study. The data available for the new project are finer grained according to the indications devised in the first study. This allowed to improve the confidence in our previous empirical analysis by confirming most of the hypotheses made and to provide other useful indications to better understand the maintenance process of the company in a quantitative way. Title:\n\nSUPPORTING KNOWLEDGE REUSE DURING THE SOFTWARE MAINTENANCE PROCESS THROUGH AGENTS\n\nAuthor(s):\n\nMario Piattini , Aurora Vizcaino\n\nAbstract: Knowledge management has become an important topic as organisations wish to take advantage of the information that they produce and that can be brought to bear on important decisions. This work describes a system to manage and reuse the information (and knowledge) generated during the software maintenance process, which consumes a large part of the software lifecycle costs. The architecture of the system is formed of a set of agent communities. Each community manages different types of knowledge. The communities’ agents have the goal of encouraging the reuse of good solutions and taking advantage of information obtained from previous experience. In consequence, the software maintenance is made easier and there are less costs and effort. To achieve this goal, agents use several reasoning techniques such as case based reasoning or decision tree based algorithms which allow them to generate new knowledge from the information that they manage. Title:\n\nRETRO-DYNAMICS AND E-BUSINESS MODEL APPLICATION FOR DISTRIBUTED DATA MINING USING MOBILE AGENTS\n\nAuthor(s):\n\nMOHAMED MEDHAT , EZENDU ARIWA\n\nAbstract: Distributed data mining (DDM) is the semi-automatic pattern extraction of distributed data sources. The next generation of the data mining studies will be distributed data mining for many reasons. First of all, most of the current used data mining techniques require all data to be resident in memory, i.e., the mining process must be done at the data source site. This is not feasible for the exponential growth of the data stored in organization(s) databases. Another important reason is that data is inherently distributed for fault tolerance purposes. DDM requires two main decisions about the DDM implementations: A distributed computation paradigm (message passing, RPC, mobile agents), and the used integration techniques (Knowledge probing, CDM) in order to aggregate and integrate the results of the various distributed data miners. Recently, the new distributed computation paradigm, which has been evolved as mobile agent is widely used. Mobile agent is a thread of control that can trigger the transfer of arbitrary code to a remote computer. Mobile agents paradigm has several advantages: Conserving bandwidth and reducing latencies. Also, complex, efficient and robust behaviours can be realized with surprisingly little code. Mobile agents can be used to support weak clients, allow robust remote interaction, and provide scalability. In this paper, we propose a new model that can benefit from the mobile agent paradigm to build an efficient DDM model. Since the size of the data to be migrated in the DDM process is huge, our model will overcome the communication bottleneck by using mobile agents paradigm. Our model divides the DDM process into several stages that can be done in parallel on different data sources: Preparation stage, data mining stage and knowledge integration stage. We also include a special section on how current e-business models can use our model to reinforce the decision support in the organization. A cost analysis in terms of time consumed by each minor process (communication or processing) is given to illustrate the overheads of this model and the other models. Title:\n\nIMPORTANT FACTORS IN ERP SYSTEMS IMPLEMENTATIONS\n\nAuthor(s):\n\nPiotr Soja\n\nAbstract: In the article the problem of success factors in ERP systems implementations has been discussed. The review of the literature concerning success factors has been discussed and the collection of potential ERP implementation success factors was identified. Next, the result of research has been presented, where respondents have been asked about their opinion about the importance of subsequent factors for the implementation success. There were two groups of respondents: the first consisted of people from Polish enterprises implementing ERP systems and the second comprised experts working in ERP systems suppliers. On the basis of the research, the most important and necessary factors in the respondents’ opinions have been identified, as well as the least important ones. Title:\n\nIDENTIFYING CLONES IN DYNAMIC WEB SITES USING SIMILARITY THRESHOLDS\n\nAuthor(s):\n\nGiuseppe Scanniello , Andrea De Lucia , Genny Tortora\n\nAbstract: We propose an approach to automatically detect duplicated pages in dynamic Web sites. Our approach analyzes both the page structure, implemented by specific sequences of HTML tags, and the displayed content. In addition, for each pair of dynamic pages we also consider the similarity degree of their scripting source code. The similarity degree of two pages is computed using different similarity metrics for the different parts of a web page based on the Levenshtein string edit distance. We have implemented a prototype to automate the clone detection process on web applications developed using JSP technology and used it to validate our approach in a case study. Title:\n\nINFORMATION TECHNOLOGY STRATEGIC PLANNING: ADAPTING FACTS AND BELIEFS TO BUSINESS STRATEGY GENERATION\n\nAuthor(s):\n\nJulio Bernardo Clempner Kerik , Agustín Francisco Gutiérrez Tornés\n\nAbstract: This paper introduces a framework for adpating facts and beliefs to business strategy generation. The adaptation process model is supported by an information technology planning (ITSP) model and methodology. Tha aim of this paper is to validate the model. In the ITSP model, real world is composed by entities realated in terms of goals, beliefs, etc., through interaction they incorporate or refuse facts or beliefs related to the enviornment conditions. The adaptation concept is proposed to generate gusiness strategies. Two different methos are proposed: 1)an inference logic method, that emplys facts related to the enviornment conditions to generate new business strategies; and 2) case-based reasoning, a storred cases recorgind specific prior episodes, that induce the incorporation of business strategies. Both methods are presented. The adaptation process is presented through application examples. Title:\n\nERP BASED BUSINESS PROCESS REENGINEERING IN A HUMAN RESOURCES DEPARTMENT: A CASE STUDY APPROACH\n\nAuthor(s):\n\nTHEODORA CHATZIKALLIA , KONSTANTINOS CHERTOURAS\n\nAbstract: Modern organizations are constantly facing new challenges regarding the reengineering of their business departments and processes. By the term Business Process we mean the profile of specific methods that can be employed to perform specific business tasks. In general, each Business Process is uniquely tailored to the organization it applies. Therefore, the resolution of a Business Process related problem is typically carried out with custom methods developed within organizations. In this paper we propose the use of Enterprise Resource Planning (ERP) as the basis for reengineering a business department and effectively the Business Process that it carries through. We discuss the application of ERP in the reengineering of the Business Process of a real world organization department (a Human Resources Department), which lead to a significant productivity enhancement. Title:\n\nORGANIZATIONAL AND TECHNOLOGICAL CRITICAL SUCCESS FACTORS BEHAVIOR ALONG THE ERP IMPLEMENTATION PHASES\n\nAuthor(s):\n\nJose Esteves , joan pastor\n\nAbstract: During the last years some researchers have studied the topic of critical success factors in ERP implementations. Up to this moment, there is not enough research on the management and operationalization of critical success factors within ERP implementation projects. The identification of factors leading to success or failure of ERP systems is an issue of increasing importance, since the number of organizations choosing the ERP path keeps growing. In this paper, we analyzed the evolution of organizational and technological factors along the ERP implementation phases. Our findings suggest that while both good organizational and technological perspectives are essential for a successful ERP implementation project, their importance shifts as the project moves through its lifecycle. Title:\n\nACME-DB: AN ADAPTIVE CACHING MECHANISM USING MULTIPLE EXPERTS FOR DATABASE BUFFERS\n\nAuthor(s):\n\nMarkus Kirchberg\n\nAbstract: An adaptive caching algorithm, known as Adaptive Caching with Multiple Experts (ACME), has recently been presented in the field of web-caching. We explore the migration of ACME to the database caching environment. By integrating recently proposed database replacement policies into ACME's existing policy pool, an attempt is made to gauge ACME's ability to utilise newer methods of database caching. The results suggest that ACME is indeed well-suited to the database environment and performs as well as the best currently caching policy within its policy pool at any particular moment in its request stream. Although execution time increases by integrating more policies into ACME, the overall processing time improves drastically with erratic patterns of access, when compared to static policies. Title:\n\nEVALUATION OF A DOCUMENT DATABASE DESCRIPTION BY DIFFERENT XML SCHEMAS\n\nAuthor(s):\n\nPierre Bazex , Madani Kenab , Tayeb Ould Braham\n\nAbstract: Title : Evaluation of a Document Database Description by Different XML Schemas Authors : Madani Kenab (1,2), Tayeb Ould Braham (2), Pierre Bazex (1) (1) IRIT, 118, Route de Narbonne 31062 Toulouse, France { kenab@info.unilim.fr, bazex@irit.fr } (2) MSI, 83, Rue d'isle 87000 Limoges, France { ould@unilim.fr } Address contact: Tayeb Ould Braham Email: ould@unilim.fr Tel : 33 5 55 43 69 71 Fax : 33 5 55 43 69 77 Abstract A document database could be represented by different XML schemas, it depends on the content of the documents that it contains. From a simple conceptual schema of a database containing structured data that we represent in form of a document, we propose and evaluate different XML schemas describing this database in order to deduce the best one. For the building of these XML schemas we propose different descriptions of the key concepts of the relational model (relation, key and reference link) . We also propose the description of different nestings between the elements of the document (total nesting, partial nesting and without nesting) . We conclude that the best-adapted XML schema depends on the use that we wish to do with this database and it is a combination of the representation of different concepts. This work is a preliminary of the integration of a relational database thanks to the best XML schema. Key Words : Entity-Association, Relational Concepts, XML Schema, XML Document, Nesting Elements. Title:\n\nTRANSACTION DESIGN FOR DATABASES WITH HIGH PERFORMANCE AND AVAILABILITY\n\nAuthor(s):\n\nLars Frank\n\nAbstract: When many concurrent transactions like ERP and E-commerce orders want to update the same stock records, long duration locking may reduce the availability of the locked data. Therefore, transactions are often designed without analyzing the consequences of loosing the traditional ACID (Atomicity, Consistency, Isolation and Durability) properties. In this paper, we will analyze how low isolation levels, optimistic concurrency control, short duration locks, and countermeasures against isolation anomalies can be used to design transactions for databases with high performance and availability. Long duration locks are defined as locks that are held until a transaction has been committed, i.e. the data of a record is locked from the first read to the last update of any data used by the transaction. This will decrease the availability of locked data for concurrent transactions, and, therefore, optimistic concurrency control and low isolation levels are often used. However, in systems with relatively many updates like ERP-systems and E-commerce systems, low isolation levels cannot solve the availability problem as all update locks must be exclusive. In such situations, we will recommend the use of short duration locks. Short duration locks are local locks that are released as soon as possible, i.e. data will for example not be locked across a dialog with the user. Normally, databases where only short duration locks are used do not have the traditional ACID properties as at least the isolation property is missing when locks are not hold across a dialog with the user. The problems caused by the missing ACID properties may be managed by using approximated ACID properties, i.e. from an application point of view the system should function as if all the traditional ACID properties had been implemented. Examples using E-commerce will illustrate how to use the transaction design recommended in this paper. We have cooperated with one of the major ERP software companies in designing our transaction model. Title:\n\nINCREMENTAL DATA QUALITY IN THE DATA WAREHOUSE\n\nAuthor(s):\n\nKarsten Boye Rasmussen\n\nAbstract: The data warehouse is the cornerstone for the production of business knowledge in the organization. The foundation of the quality of the business knowledge is the quality of the data in the data warehouse. Determination of dimensions of data quality in the data warehouse has been obtained through the intuitive, the empirical and the ontological approaches. The first point of this working paper is that data quality is not a static measure and that awareness of the data quality dimensions is a prerequisite to improve the data quality. The second point is that selection is the cornerstone of data quality in the data warehouse in relation to the quality dimensions. Thirdly, that post-load improvement of the data quality is obtainable. Metadata can be added incrementally containing information on the use of data – and thus the users' selections within the data warehouse – and on the users' judgment of the data. Title:\n\nA MIDDLEWARE FOR THE MANAGEMENT OF LARGE UTILITIES PLANTS\n\nAuthor(s):\n\nandrea rossettini , salvatore cavalieri , carmelo floridia , fabrizio d'urso\n\nAbstract: The paper presents the main features of the European project Mobicossum IST 1999-57455, still running. The project is a CRAFT one approved inside the Fifth Framework Programme. It aims to define a middleware offering services for the management of large plants, in the field of gas and water distribution and waste water treatment systems. In the paper, the main features of the project will be explained, focusing on the description of the implementation of the core of the middleware, called Generalised Interface. Title:\n\nACQUIRING AND INTEGRATING EXTERNAL DATA INTO DATA WAREHOUSES\n\nAuthor(s):\n\nMattias Strand , Benkt Wangler , Carl-Fredrik Laurén\n\nAbstract: Data warehouses (DWs) has become one of the major IT-investments during the last decades and in order to fully exploit the potential of data warehouses, more and more organizations are acquiring and integrating external data into their star-schemas. However, the literature covering external data acquisition and integration is limited. Therefore, in this paper the results of an interview study conducted among banking organizations are presented. The study aimed at identifying different approaches for acquiring and integrating external data into DWs. The results show that there are many different approaches for the acquisition and integration, depending on the purpose and structure of the data being acquired. In addition, the most common external data acquisition and integration process is presented and discussed. Title:\n\nA CONCEPTUAL FRAMEWORK FOR FORECASTING ERP IMPLEMENTATION SUCCESS - A FIRST STEP TOWARDS THE CREATION OF AN IMPLEMENTATION SUPPORT TOOL\n\nAuthor(s):\n\nFredrik Carlsson , Andreas Nilsson , Johan Magnusson\n\nAbstract: The continuing soar in popularity when it comes to standardized information systems sold en masse under the labelling of Enterprise Resource Planning (ERP) Systems is somewhat kept under control by the ever flowing stream of reports from the industry of implementations gone bad. According to some researchers it is possible to assume that as many as 90% of all initiated ERP implementation projects can be regarded as failures as a result of changes in scope, prolongation of the project time or simply budget overruns. With the implementation of an ERP system being a very costly and risky endeavour, organizations considering “getting on the bandwagon” stand much to gain from pre-emptively forecasting the probability of success for an ERP implementation in their enterprise. Given this, the purpose of this paper is to investigate a possible conceptual framework for forecasting ERP implementation success and discuss the role of such a framework in a software based tool. This was achieved through an initial in-depth literary review aimed at finding factors affecting the outcome of the ERP implementation projects. These results were then communicated to an industrial support group comprised of possible ERP implementation stakeholders. After lengthy discussions concerning the usability, validity and reliability of the proposed list of factors, a conceptual framework was agreed upon for forecasting ERP implementation success. The framework was then tested against a number of possible stakeholders outside the industrial support group. As the results show we have been able to create a conceptual framework for forecasting ERP implementation success that is currently in the second wave of testing. The usability, validity and reliability of the framework is discussed and elaborated upon, and this paper concludes that the perceived usability and hence also value of the conceptual framework is substantial, whereas the validity and reliability remain to be tested. Title:\n\nVIRTUAL ORGANIZATIONS AND DATABASE ACCESS - A CASE STUDY\n\nAuthor(s):\n\nMarko NIINIMAKi , Mikko Pitkanen , John White , Tapio Niemi\n\nAbstract: This paper presents a case study of using virtual organization technologies in database access. A virtual organization (VO) is a collection of people in the same administrative domain. A user can belong to many virtual organizations and have a different role (user, client, administrator,..) in each of them. An authorization of a user to different services within a VO is based on the user's identity and a service called a Virtual Organization Membership Service (VOMS) that maps these identities with roles. The user's identity can be established in two ways. If the user communicates with the service using his web browser, the user's certificate must be included in the browser. Another possibility is to use a proxy certificate. There, in the proxy creation process, the program that writes the proxy adds the user's proxy certificate information about his participation in different VO's and his role in each of them. In order to demonstrate using these VO proxy certificates, we have extended the functionality Spitfire, a relational database front end. This involves assigning the user a database role (read/write/update) based on the VO information in his certificate. There is also a GUI for creating the mappings between VO roles and database access roles. Title:\n\nREASONS FOR ERP ACQUISITION\n\nAuthor(s):\n\nSami Sarpola , Sanna Laukkanen , Petri Hallikainen\n\nAbstract: Numerous reasons for why organisations acquire Enterprise Resource Planning (ERP) systems have been proposed in prior research. In this paper we form a synthesis of these different reasons and categorize them into technological and business reasons for acquiring ERP. Further, we test the validity of these reasons with empirical data concerning the acquisition of ERP systems in 41 Finnish companies. Title:\n\nDELEGATING AUTHORITY IN A DISTRIBUTED INFORMATION MANAGEMENT SYSTEM\n\nAuthor(s):\n\nJanet Barnett , Barbara Vivier , Kareem Aggour\n\nAbstract: The need to manage large information repositories securely in a distributed environment increases with the growth of the Internet. To address this need, a system capable of managing the contents of an LDAP directory over the Web has been designed and developed. This system allows for the directory’s data to be divided into communities and supports the delegation of administrative authority over those communities to a distributed set of administrators. The communities may be subdivided recursively into subgroups, and rights over those subgroups also may be restricted. Thus, system administrators can dynamically delegate subsets of their permissions over a subset of their managed data, allowing for the effective control of permissions over the data within distributed organizations. The system solves the delegated administration problem for managing the contents of an LDAP directory in a distributed environment. Today, it supports the administration of over 20 production directories by well over 2000 distributed administrators. Title:\n\nDISTRIBUTED DATABASE SYSTEM OF AGRICULTURAL SCIENCE AND TECHNOLOGY ALLIANCE OF UNIVERSITIES IN CHINA\n\nAuthor(s):\n\nLongyong You , Junjing Yuan , Jiayun Wang , Jian Zhang\n\nAbstract: There are three problems that need to be solved by establishing the Distributed Database System of the information platform of Chinese universities' Agricultural Science and Technology Alliance: distribution of the data resources, decomposition and optimization of the distributed query as well as safety of the data system. In this paper, firstly, through the overall analysis of the contents of Chinese universities' Agricultural Science and Technology Alliance, we establish the mixed data distributed system, make the database system more integrated, consistent and reliable, meanwhile improve efficiency of the local application. Secondly because the member of the alliance adopts different data mode, taking a query decomposition and optimization for overall mode in the way of extended semi-join will be the effective method to improve the system response time. Finally, utilizing the method of combining asymmetry encryption with symmetry encryption, we solve the safety problems of database identity validation, data transmission, visitation control and etc. Title:\n\nA DATA WAREHOUSE ARCHITECTURE FOR BRAZILIAN SCIENCE AND TECHNOLOGY ENVIRONMENT\n\nAuthor(s):\n\nAndre Luís Menolli , Maria Madalena Dias\n\nAbstract: Science and technology in Brazil are areas that have few available resources and many times these scarce resources are badly used. The data warehouse is a tool that can make possible a better distribution of these resources. In this article are considered some issues in the development of a data warehouse for Science & Technology management. The paper describes the necessity of a supporting system to the decision taking regarding the distribution of the resources destined to Science & Technology in Brazil, and also shows a data warehouse architecture that is being developed to support this system. Data Modeling characteristics defined for the proposed data warehouse architecture are presented too. Title:\n\nSOFTWARE PRODUCT LINE ANALYSIS OF ENTERPRISE INFORMATION SYSTEM\n\nAuthor(s):\n\nLuiz Fernando Capretz , Faheem Ahmed\n\nAbstract: Now a days geographical and physical constraint that allowed only for fixed and static placements of resources has vanished completely within an enterprise utilizing the concept of information technology to integrate their business needs. The object oriented programming approach has paved a way to reusability of components thus reducing cost and development efforts up to certain extend. Software product line has further strengthened the concept of reusability, and component-based architecture. In this paper we have analyzed the concept of Software Product Line Analysis for an Enterprise Information System which will help to construct a Software Product Line within the organization to produce high quality software product in order to full fill the information technology requirements of the organization. Title:\n\nAN APS ARCHITECTURE FOR WEB SERVICES BASED ENTERPRISE INTEGRATION\n\nAuthor(s):\n\nWilliam Liu , FengYu Wang , Tay Jin Chua\n\nAbstract: Web Services enabling technology is widely used to address enterprise integration within company or cross-organizations due to its language and operating system independency and support of loosely coupled integration. This paper presents an architecture for APS (Advanced Planning and Scheduling) system by describing an APS request handling engine and web services based functions, attempting to solve integration issues among APS, MES, ERP and other manufacturing systems that could not be handled properly using current approaches. In addition, as manufacturing planning has been extended to cover entire supply chain, this paper also discusses the necessary changes of the proposed architecture to cater for the extension. That would be helpful to figure out capacity issue in a big picture Title:\n\nOBTAINING E-R DIAGRAMS SEMI-AUTOMATICALLY FROM NATURAL LANGUAGE SPECIFICATIONS\n\nAuthor(s):\n\nFarid Meziane\n\nAbstract: Since their inception, entity relationship models have played a central role in systems specification, analysis and development. They have become an important part of several development methodologies and standards such as SSADM. Obtaining entity relationship models, can however, be a lengthy and time consuming task for all but the very smallest of specifications. This paper describes a semi-automatic approach for obtaining entity relationship models from natural language specifications. The approach begins by using natural language analysis techniques to translate sentences to a meaning representation language called logical form language. The logical forms of the sentences are used as a basis for identifying the entities and relationships. Heuristics are then used to suggest suitable degrees for the identified relationships. This paper describes and illustrates the main phases of the approach and presents a summary of the results obtained when it is applied to a case study. Title:\n\nTOWARDS CONCEPTUAL MEDIATION\n\nAuthor(s):\n\nIsmael Navas D. , José F. Aldana M.\n\nAbstract: Mediators are usually developed as monolithic systems which envelope the data source’s semantics as well as its location. Furthermore, its architecture based on wrappers involves a high coupling degree among the mediator’s components. This coupling does not allow sharing services with other organizations or the dynamic integration of new data sources. Therefore, wrappers must be re-designed and manually added for each mediation system. We propose an architecture for conceptual mediation in which the sources’ query capabilities are published as web services. These services can be registered in one or more resource directories (Semantic Directories), which are the core of this architecture because they provide the needed flexibility and scalability for dynamic integration. Finally, we show an application in a bioinformatics context to validate our approach. Title:\n\nAN AUTOMATION SYSTEM BASED ON LABVIEW TO CONTROL THE TEST OF MECHANICAL FLOW METERS\n\nAuthor(s):\n\nVíctor Mejia , Javier Martínez , Victor Silva , Ricardo Alvarez , Petronilo Cortez\n\nAbstract: A mechanical flow meter is a device used mainly to measure and calculates velocity of weater´s flow on rivers and open channels. These devices, as the time of use pass trough, suffer mechanical imperfections, that's why it is important to calibrate them twice a year, depending of its time of use. At the Mexican Institute of Water Technology (IMTA in Spanish) was designed and developed a circular water tank for propose of test of these meters. The present paper shows the automation systems designed to control the tests to calibrate these mechanical meters. The system is based on LabVIEW. LabVIEW is a general purpose programming tool with extensive libraries for data acquisition instrument control, data analysis, and data presentation. With this tool and a special hardware interface, it was possible to automate the process to test these meters. The system is called SCM (System of characterization of mechanical meters). SCM control the test of two mechanical meters simultaneously, and has some user's control features that permit the Operator a easy to use human machine interface. Title:\n\nFUZZY MULTIPLE-LEVEL SEQUENTIAL PATTERNS DISCOVERY FROM CUSTOMER TRANSACTION DATABASES\n\nAuthor(s):\n\nHuilin Ye , An Chen\n\nAbstract: Sequential patterns discovery is a very important research topic in data mining and knowledge discovery, and it has been widely applied in business analysis. Previous works were focused on mining sequential patterns at a single concept level based on definite and accurate concept which may not be concise and meaningful enough for human experts to easily obtain nontrivial knowledge from the rules discovered. In this paper, we introduce concept hierarchies firstly, and then discuss a mining algorithm F-MLSPDA for discovering multiple-level sequential patterns with quantitative attribute based on fuzzy partitions. Title:\n\nA METADATA REPOSITORY FOR IMAGE RETRIEVAL ALGORITHMS\n\nAuthor(s):\n\nSahudy Montenegro González , Akebo Yamakami\n\nAbstract: Many of the problems involved in image database applications require some form of retrieval based on image content. The explosion in availability of image content, due to recent developments in multimedia technology, demands the formulation of algorithms to facilitate content-based retrieval. Many image retrieval algorithms are implemented according to the needs of specific applications. Yet, there is currently no standard form of manipulation for these algorithms. This fact has a secure impact on the availability of algorithms beyond the bounds of the application for which they were originally designed. This work defines a general purpose repository for the algorithms involved in the process of Image Retrieval. The main goal of the repository is to provide the application developer with an infrastructure to manipulate and query image algorithms, allowing the integration of the image retrieval algorithms, the creation of a stock of algorithms available to multiple users, and to reuse/share algorithms for multiple applications. We define a standard set of metadata, applicable to image retrieval algorithms, providing uniform semantic support to understand these algorithms. This repository acts as a support to the development of image retrieval applications. The repository architecture is centered on providing distributed database functionality. Title:\n\nTHE CONCEPT AND IMPLEMENTATION OF THE MARKET PLACE E-UTILITIES•COM\n\nAuthor(s):\n\nJamil Dimassi , Carine Souveyet , Colette Rolland\n\nAbstract: In order to remain competitive in a deregulated environment, a group of European Utilities developed a prototype of a single Marketplace called e-utilities•com whose mandate is a clear customer centric orientation in the European environment for a successful mid-term multi-utility business via the Web. This paper highlights the concept of e-utilities•com and its implementation in a Web portal. Title:\n\nPERFORMANCE INDICATORS: IMPORTANT TOOL FOR BUSINESS INTELLIGENCE AND INFORMATION SYSTEMS\n\nAuthor(s):\n\nMaría Luisa Sené\n\nAbstract: In this paper is treated the importance of performance indicators in order to have a healthy organization. Also are given elements to understand why standardization is so related to this topic, and the most important thing, how all this contributes to design an information system that will help the organization in the process of decision-making. Are included examples of performance indicators that can be applied in any organization. Title:\n\nACCESS MODEL IN COOPERATIVE INFORMATION SYSTEMS\n\nAuthor(s):\n\nEric Disson , Danielle Boulanger\n\nAbstract: This research focuses on access security in cooperating information systems. The offered modeling has to treat the interoperation of open and evolutive information systems and, moreover, has to guarantee the respect of various local security policies. The coexistence of heterogeneous information sources within an information systems framework involves homogenization problems between local security policies. We distinguish two types of heterogeneity: heterogeneity of the local access policies and semantic heterogeneity between object or subject instances of the local access schemas. To solve this twofold difficulty, we propose an original role model allowing a unified representation of local access schemas. This model preserves the flow control properties in the three main access policies (discretionary, role based model and multilevel models). The described access schemas are enriched to establish intra-system access authorizations. Title:\n\nBUSINESS MODELLING THROUGH ROADMAPS\n\nAuthor(s):\n\nJudith Barrios Albornoz , Jonás Montilva Calderón\n\nAbstract: Business modelling is a central activity to many different areas, including Business Process Reengineering, Organisational Development, Enterprise Modelling & Integration, Business Process Management and Enterprise Application Integration. It is well known that the business domain is not easy to understand neither to represent even for specialised people. The success of most of the contemporary methods for modelling Business Organisations or Enterprise Information Systems (EIS) is strongly associated with the level of understanding that the modelling team can attain about the specific situation being modelled. This understanding is directly related with the degree of modelling experience that the team has, as well as their ability to work with the techniques and tools prescribed by a specific method. Nowadays, most of the existing business modelling methods are concentrated in what are the business concepts and how to represent them. But, they lack of process guidance, which is needed to help the team through the modelling process. We elaborated the method BMM for modelling business application domains that provides working guidelines for the modelling team. This method, based on method engineering concepts helps teams to, not only, get a comprehensive knowledge about the business domain being modelled, but also, about the process of modelling the domain itself. This paper concerns with the representation of the process of modelling a business by using a decision oriented process model formalism. It is represented at a higher level by a roadmap. The main contribution of our work is a set of roadmaps that contains the knowledge associated with team member’s modelling experience in business modelling and EIS development. This knowledge arises from several case studies. Title:\n\nAUTOMATIC DISCOVERY OF SEMANTIC RELATIONSHIPS BETWEEN SCHEMA ELEMENTS\n\nAuthor(s):\n\nNikos Rizopoulos\n\nAbstract: The identification of semantic relationships between schema elements, or \\schema matching, is the initial step in the integration of data sources. Existing approaches in automatic schema matching have mainly been concerned with discovering equivalence relationships between elements. In this paper, we present an approach to automatically discover richer and more expressive semantic relationships based on a bidirectional comparison of the elements data and metadata. The experiments that we have performed on real-world data sources from several domains show promising results, considering that we do not rely on any user or external knowledge. Title:\n\nMANAGING INFORMATION FLOW DYNAMICS WITH AGILE ENTERPRISE ARCHITECTURES\n\nAuthor(s):\n\nDrakoulis Martakos , Panagiotis Kanellis , Nancy Alexopoulou\n\nAbstract: New organization forms and ways of conducting business require architectures for enterprise systems that can support and not hinder entrepreneurial activities. Primarily this means that the information flow between both internal as well as cross-enterprise processes must be managed by underlying systems that offer a high level of automation as well as being highly flexible and integrated. In this respect, we present an agile architecture that offers a coherent and high level conceptualisation of the above properties that enterprise information systems should display, consider a number of technologies as potential implementation candidates and demonstrate how the architecture addresses node density, velocity, viscosity and volatility as parameters for managing and controlling the dynamics of information flows. Title:\n\nA TRANSACTIONAL MULTIMODE MODEL TO HANDLE OVERLOAD IN DISTRIBUTED RTDBSS\n\nAuthor(s):\n\nSamia Saad-Bouzefrane\n\nAbstract: Current applications, such as Web-based services, electronic commerce, mobile telecommunication systems, etc. are distributed in nature and manipulate time-critical databases. In order to enhance the performance and the availability of such applications, the major issue is to develop efficient protocols that cooperate with the scheduler to manage the overload of the distributed system. In order to help real-time database management systems (RTDBS) to maintain data logical consistency while attempting to enhance concurrency execution of transactions, we introduce a transactional multimode model to let the application transactions adapt their behavior to the overload consequences. In this paper, we propose for each transaction several execution modes and we derive an overload controller suitable for the proposed multimode model. Title:\n\nA FRAMEWORK FOR EVALUATING DIFFICULTIES IN ERP IMPLEMENTATION\n\nAuthor(s):\n\nJorge Marcelo Montagna , Luis Ferrario\n\nAbstract: Various sources point out very high percentages of failures to implement ERP systems. In this work, the main difficulties for this task are analyzed and a systematic classification of fundamental reasons is intended. By considering the reasons that lead to failure, a simple and effective mechanism is generated to evaluate in advance complications the project might present. In this way, the tools to be used can be adjusted to the specific characteristics of the project. Somehow, it is intended to solve the problem presented by general methodologies, which are used for any kind of enterprise, without previously considering its conditions and state to face this type of projects. Title:\n\nSTUDY OF DIFFERENT APPROACHES TO THE INTEGRATION OF SPATIAL XML WEB RESOURCES\n\nAuthor(s):\n\nJose Corcoles , Pascual Gonzalez\n\nAbstract: The research community has begun to investigate foundations for the next stage of the Web, called Semantic Web. Current efforts include the Extensible Markup Language XML, the Resource description Framework, Topic Maps and the DARPA Agent Markup Language DAML+OIL. A rich domain that requires special attention is the Geospatial Semantic Web. However, in order to approach the Geospatial Semantic Web, it is necessary to solve the problem of developing an integration system for querying spatial resources stored in different sources. In this paper, we study two different approaches to integrating spatial and non-spatial information represented in the Geographical Markup Language (GML). The approaches studied follow LAV (Local as View) integration. With this study we obtain the best approach to developing a real system for querying GML resources stored in different sources. Title:\n\nCAPABILITY-BASED QUERY PLANNING IN MEDIATOR SYSTEMS\n\nAuthor(s):\n\nJiu Yang Tang\n\nAbstract: This paper addresses the impact of capability description on query planning in heterogeneous data integration system. Query planning covers the selection of data sources related to the query and the determination of subgoals’ execution orders. In the context of capability description, we propose a framework for data sources description towards generating good feasible query plans. Our approach uses information such as the semantic correspondences between local schemas and mediated schemas and the query capability descriptions to investigate factors that provide a good foundation for query planning. Finally, the proposed approach is compared with the other capability description approaches described in the literature. The obtained results demonstrate that our approach will allow data sources to advertise their capabilities in a flexible way and help to efficiently query planning. Title:\n\nAN EFFICIENT B+-TREE IMPLEMENTATION IN C++ USING THE STL STYLE\n\nAuthor(s):\n\nGregory Butler\n\nAbstract: Database indexes are the search engines for database management systems. The B+-tree is one of the most widely used and studied data structuresand provides an efficient index structure for databases. An efficient implementation is crucial for a B+-tree index. Our B+-tree index is designed to be a container by following the style of the C++ Standard Template Library (STL) and implemented efficiently by using design patterns and generic programming techniques. Therefore, our B+-tree index can adapt to different key types, data types, different queries, and different database application domains, and be easy and convenient for developers to reuse just like other containers in the STL. Title:\n\nXRM: AN XML-BASED LANGUAGE FOR RULE MINING SYSTEMS\n\nAuthor(s):\n\nDominique Laurent , Tao-Yuan Jen , Ahmed Cheriat , Béatrice Bouchou , Mirian Halfeld-Ferrari\n\nAbstract: In this paper, we present XRM, an XML-based language capable of promoting the collaboration among data mining systems. Indeed, KDD systems usually need a platform to integrate and exchange their results with different tools. XRM is a general framework to express any system results and/or data as logic formulas. In this way, XRM offers flexibility to represent data, constraints and patterns, and allows mining systems to present their results in an exchangeable format. In this work, we concentrate on the use of XRM to represent different forms of association rules. Association rule mining has evolved giving rise to sophisticate approaches that require interaction with other tools. XRM is built on XML Schema - in this way we can assure a certain level of correctness of data and mining results. Title:\n\nAUDIOVISUAL ARCHIVE WITH MPEG-7 VIDEO DESCRIPTION AND XML DATABASE\n\nAuthor(s):\n\nPedro Almeida , Helder Troca Zagalo , Joaquim Sousa Pinto , Joaquin Arnaldo Martins\n\nAbstract: This article presents the work that has been developed in the creation of an audiovisual archive that uses the MPEG-7 standard to describe the video content and a XML database to store the video descriptions. It presents the model adopted to describe the video content, the framework of the audiovisual archive information system, a video indexing tool developed to allow the creation and manipulation of XML documents with the video descriptions and an interface to visualize the videos over the Web. Title:\n\nENHANCING THE SUCCESS RATIO OF DISTRIBUTED REAL-TIME NESTED TRANSACTIONS\n\nAuthor(s):\n\nMajed Abdouli , Bruno Sadeg , Laurent Amanton\n\nAbstract: The traditional transaction models are not suited to real-time database systems RTDBSs. Indeed, many current applications managed by these systems necessitate a kind of transactions where some of the ACID properties must be ignored or adapted. In this paper, we propose a real-time concurrency control protocol and an adaptation of the Two-Phase Commit Protocol based on the nested transaction model where a nested transaction is viewed as a collection of both essential and non-essential subtransactions: the essential subtransaction has a firm2 deadline, and the non-essential one has a soft3 deadline. We show through simulation results, how our protocol, based on this assumption, allows better concurrency between transactions and between subtransactions of the same transaction, enhancing then the success ration4 and the RTDBS performances, i.e.,more transaction may meet their deadline. Title:\n\nUSING IUCLID FOR WORLDWIDE EXCHANGE OF CHEMICAL AND TOXICOLOGICAL INFORMATION\n\nAuthor(s):\n\nStefan Scheer , Remi Allanou\n\nAbstract: A database management tool (IUCLID) has been created in order to provide with administering chemical and toxicological data sent in structured form due to existing EU legislation. This tool also offers – beyond the normal dataset administration functionality – mechanisms for data fusion, data reproduction and data deployment. Thus IUCLID is used not only by who has to receive submissions of that kind but also who has to produce such submissions. Hence this product is used by whoever is involved as stakeholder in the current legislative process, and even beyond that it has been recognized successfully. Consequently it was the worldwide acceptance that helped in promoting this software product ahead of its original purpose and to establish a network of exchange. Title:\n\nRAPID XML DATABASE APPLICATION DEVELOPMENT\n\nAuthor(s):\n\nKjetil Norvag , Albrecht Schmidt\n\nAbstract: This paper proposes a rapid prototyping framework for XML database application development. By splitting up the development process into several refinement steps while keeping the application programming interface stable, the framework aims at rapid implementation of a prototype with a well-defined interface and a subsequent implementation of more advanced concepts like business rules in several steps. The refinement process takes the form of incrementally adding domain-specific information to the application. This is achieved by transgressing from general-purpose XML tools that do not support the definition and enforcement of constraints to frameworks that support domain-specific models and constraints such as E/R modeling. We have employed this method in the development of an example application, and we give performance numbers that illustrate the incremental improvements of each step. Title:\n\nONTOLOGY-BASED REQUIREMENT ELICITATION\n\nAuthor(s):\n\ncong wang\n\nAbstract: The key problem of information system development is how to acquire requirement. It has become the puzzled problem to the system developers for a long time. How to build a communication bridge between the developers and users has become a hot issue in requirement engineering. Ontology defines the common concepts and the relationships among them. A communication bridge can be built between the domain users and the system developers. Therefore, the ontology can direct the users and the developers to construct the requirement model. According to the different views of the system, this paper provides ontologies named business ontology, technique ontology and functionality Ontology for requirement elicitation. Firstly, this paper defines the concept of the ontology. Second, we describe the three ontologies in detail. Finally, through the ontologies, this paper provides the domain requirement model. Title:\n\nA TRANSACTION MODEL FOR LONG RUNNING BUSINESS PROCESSES\n\nAuthor(s):\n\nJinling Wang , Beihong Jin , Jing Li\n\nAbstract: Many business processes in the enterprise applications are both long running and transactional in nature, but currently no transaction model can provide full transaction support for such long running business processes. In this paper, we proposed a new transaction model — PP/T model. It can provide structural transaction support for the long running business processes, so that application developers can focus on the business logic, with the underlying platform providing the required transactional semantics. Simulation results show that the model has good performance in processing the long running business processes. Title:\n\nCACHING STRATEGIES FOR MOBILE DATABASES\n\nAuthor(s):\n\nMurilo de Camargo\n\nAbstract: Caching remote data in local storage of a mobile client has been considered an effective solution to improve system performance for data management in mobile computing applications. In this paper, we propose a taxonomy for cache management in mobile database systems. The aim is to provide a unifying framework for the problem of caching in mobile computing, then a comparative review of the work done in this area up to now. Such a framework, with the associated analysis of the existing approaches, provides a basis for identifying strengths and weaknesses of individual methodologies, as well as general guidelines for future improvements and extensions. Title:\n\nDM-XIDS — AN APPLICATION-LAYER ROUTER OF INCOMING XML STREAMS\n\nAuthor(s):\n\nHAO GUI\n\nAbstract: With the explosion of the information on the Internet and the widely use of the XML as a data exchange media, more and more information application can communicate with each other and deliver data of large volume in a continuous streaming. This trend has led to the emergence of novel concepts in data acquisition, integration, exchange, management and access. In this paper, we propose middleware architecture on XML streams information dissemination and design a prototype DM-XIDS as an applicable extension to our traditional database management system (named DM). Friendly graphical user interface is developed to efficiently generate and manage the diverse information subscriptions, which are described as queries in XPath. Effective algorithm is adopted to filter and match the ad hoc segment in the whole document. Automata-based query filtering mechanism will successfully implement the selection of data according to the queries in regular path expression that may include both nested path declaration and value predicate. Dedicated architecture is designed to accomplish our goals to dynamically direct the incoming XML data-stream from a static collection of information into a specific physically or logically distributed database environment. As a middleware of our database system, DM-XIDS presents a novel concept of an application-layer information router with additional administrative functions, which builds bridges between the XML stream source and the underlying data storage conforming to the pre-customized strategy. Title:\n\nAN APPROACH FOR SCHEMA EVOLUTION IN ODMG DATABASES\n\nAuthor(s):\n\nCecilia Delgado Negrete\n\nAbstract: Schema evolution is the process of applying changes to a schema in a consistent way and propagating these changes to the instances while the database is in operation. However, when a database is shared by many users, updates to the database schema are always difficult. To overcome this problem, in this paper we propose a version mechanism for schema evolution in ODMG databases that preserves old schemas for continued support of existing programs running on the shared database when schema changes are produced. Our approach uses external schema definition techniques and is based on the fact that if a schema change is requested on an external schema, rather than modifying the schema, a new schema, which reflects the semantics of the schema change, is defined. Title:\n\nCOMPARISON OF APPROACHES IN DATA WAREHOUSE DEVELOPMENT IN FINANCIAL SERVICES AND HIGHER EDUCATION\n\nAuthor(s):\n\nJanis Benefelds , Laila Niedrite\n\nAbstract: When a decision to develop a Data Warehouse is made, some sensitive factors should be evaluated to understand the tasks and prioritize them. Of course, priorities and conditions are unique in each Data Warehouse project development. In this paper we assume that there are common characteristics for companies of similar business activities and different for those with opposite activities. This article looks at the interpretation of the same criteria of two Data Warehouse projects in for-profit and not-for-profit areas. As representatives of for-profit and not-for-profit areas we selected financial services (banking) and higher education institutions. We have used the criteria from (List et al. 2002) to compare the results of the two projects. Each section of the paper describes this set of criteria for each of the two areas. The Data Warehouse development methodology used in each case is described too. An evaluation matrix is provided in Conclusion. The results shown there are not very different from Data Warehouse project development in an organization with respectively different behavior. Title:\n\nCORRELATING EVENTS FOR MONITORING BUSINESS PROCESSES\n\nAuthor(s):\n\nJosef Schiefer , Carolyn McGregor\n\nAbstract: With the increasing demand for real-time information on critical performance indicators of business processes, the capturing, transformation and correlation of real-world events with minimal latency are a prerequisite for improving the speed and effectiveness of an organization's business operations. Events often include key business information about their relationship to other events that can be utilized to collect relevant event data for the calculation of business performance indicators. In this paper we introduce an approach for correlating events of business processes that uses correlation sessions to represent correlation knowledge. Correlation sessions facilitate the processing of data across multiple events and thereby enable a calculating of business metrics in near real-time. The benefit over existing approaches is that it is tailored to instrument business processes and business applications that may operate in a heterogeneous software environment. We propose a Java-based, container-managed environment which provides a distributed, scalable, near-real time processing of events and which includes a correlation service that effectively manages correlation sessions. We also show a complete example that illustrates how correlation sessions can be utilized for computing the cycle time of business processes. Title:\n\nTRANSFORMATION-ORIENTED MIDDLEWARE FOR LEGACY SYSTEM INTEGRATION\n\nAuthor(s):\n\nUrs Frei , Guido Menkhaus\n\nAbstract: Most established companies have acquired legacy systems through mergers and acquisitions. The systems were developed independently of each other and very often they do not align with the evolving IT infrastructure. Still, they drive day-to-day business processes. Replacing the legacy application with new solutions might not be feasible, practical or cost a considerable amount of time. However, immediate integration might be a requirement for a strategic project, such as supply chain management or e-business. This article presents a transformation system for legacy system integration that allows flexible and effective transformation of data between heterogeneous systems. Sequences of transformations are described using a grammar based approach. Title:\n\nSCHEMA EVOLUTION FOR STARS AND SNOWFLAKES\n\nAuthor(s):\n\nChristian Kaas , Torben Bach Pedersen , Bjørn Rasmussen\n\nAbstract: The most common implementation platform for multidimensional data warehouses is RDBMSs storing data in relational star and snowflake schemas. DW schemas evolve over time, which may invalidate existing analysis queries used for reporting purposes. However, the evolution properties of star and snowflake schemas have not previously been investigated systematically. This paper systematically investigates the evolution properties of star and snowflake schemas. Eight evolution operations are considered, covering insertion and deletion of dimensions, levels, dimension attributes, and measure attributes. For each operation, the formal semantics of the changes for star and snowflake schemas are given, and instance adaption and impact on existing queries are described. Finally, we compare the evolution properties of star and snowflake schemas, concluding that the star schema is considerably more robust towards schema changes than the snowflake schema. Title:\n\nAN EVENT PROCESSING SYSTEM FOR RULE-BASED COMPONENT INTEGRATION\n\nAuthor(s):\n\nSusan Urban\n\nAbstract: The IJK project has developed an environment in which active rules, known as integration rules, are used together with transactions to provide an event-driven, rule-based approach to the integration of black-box components. This paper presents the event processing system that supports the use of integration rules over components. The event processing system is composed of the language framework for the specification of different types of events, an event generation system for generating event instances, and an event handler for communicating the occurrence of events to the integration rule processor. The language framework supports the enhancement of EJB components with events that are generated before and after the execution of methods on components. Since integration rule support an immediate coupling mode and execute in the context of nested transactions, a synchronization algorithm has been developed to coordinate the execution of immediate integration rules with the execution of methods on components. The synchronization algorithm makes it possible to suspend and resume distributed application transactions to accommodate the nested execution of integration rules with an immediate coupling mode. Title:\n\nCONV2XML: RELATIONAL SCHEMA CONVERSION TO XML NESTED-BASED SCHEMA\n\nAuthor(s):\n\nAngela Duta , Ken Barker\n\nAbstract: Conversion of relational data to XML is a critical topic in the database area. This approach translates the rigid tabular structures of relational databases into hierarchical XML structures. Logical connections between bits of data depicted by relationships are represented more naturally by tree-like structures. Conv2XML and ConvRel are two algorithms for converting relational schema to XML Schema focusing on preserving the source relationships and their structural constraints. ConvRel translates each relationship individually into a nested XML structure. Conv2XML identifies complex nested structures capable of modeling all relationships existent in a relational database. Title:\n\nAPPLYING CROSS-TOPIC RELATIONSHIPS TO SEARCHING WITH INCREMENTAL RELEVANCE FEEDBACK\n\nAuthor(s):\n\nStephen Chan\n\nAbstract: General purpose search engines such as Google and Yahoo define search topic hierarchies for document organization, yet such hierarchical structures cover only a portion of the possible relationships among search topics. It is believed that search effectiveness can be improved significantly by making better use of the semantic relations among search topics. In general, the is-child relation allows starting a search from general concepts, while the is-neighbor relation provides fresh information that can help users identify related search areas. This paper describes a topic network encompassing such relations, based on Bayesian networks techniques, to support searching, Our experiments show that making use of such a topic network can improve search effectiveness in a search engine using incremental feedback Title:\n\nINFORMATION INVASION IN ENTERPRISE SYSTEMS\n\nAuthor(s):\n\nStephen Crouch , Peter Henderson , Robert Walters\n\nAbstract: With the proliferation of internet-based technologies within and between organisations, large-scale enterprise systems are becoming more interconnected than ever before. A significant problem facing these organisations is how their information systems will cope with inconsistency being introduced from external data sources. Major problems arise when low quality information enters an authoritative enterprise system from these external sources, and in so doing gains credibility. This problem is compounded by the propagation of this information to other systems and other enterprises, potentially 'invading' an inter-enterprise network. In this paper we will introduce and examine this behaviour, which we term 'information invasion'. Characterisation of systems that are most vulnerable from such an occurrence is provided, and details of an experiment are given which simulates information invasion on an example network topology. Title:\n\nKNOWLEDGE TRANSFER TO AND AMONG END-USERS IN PRE-PACKAGED ENTERPRISE APPLICATION SOFTWARE IMPLEMENTATION: AN EXPLORATORY STUDY OF THE ROLES OF COMMUNITIES OF PRACTICE\n\nAuthor(s):\n\nJimmy Tanamal\n\nAbstract: This paper is concerned with the roles of Communities of Practice (CoPs) in knowledge transfer during the implementation of a particular IT artefact, i.e. the Pre-packaged Enterprise Application Software (PEAS) or also known as Enterprise Resource Planning (ERP) software. Using an in-depth longitudinal case-study across different stages of a Financial PEAS implementation in a large Australian university, we assess the effectiveness and applicability of the practices of CoPs for transferring the PEAS knowledge to and among end-users. The key finding of this paper is that CoPs can be utilized to enhance knowledge transfer for a better PEAS implementation result. Our findings also indicate that CoPs can be assigned to steward this dynamic PEAS knowledge in its most updated version among the very people who are its owners. Title:\n\nAN OBJECT ORIENTED APPROACH FOR DOCUMENT MANAGEMENT\n\nAuthor(s):\n\nAbdul Adamu , Souheil Khaddaj\n\nAbstract: It is already widely accepted that the use of data abstraction in object oriented modelling enables real world objects to be well represented in information systems. In this work we are particularly interested with the use of object oriented techniques for document management. Object orientation is well suited for such systems, which require the ability to handle multiple types content. However, the matter of how to deal with the reuse and management of existing documents over time remains a major issue. This paper aims to investigate a conceptual model, based on object versioning techniques, that will represent the semantics in order to allow the continuity and pattern of changes of documents to be determined over time. Title:\n\nHEALTH CARE PROCESS BASED ON THE ABC MODEL THROUGH A META-STRUCTURED INFORMATION SYSTEM\n\nAuthor(s):\n\nChristine VERDIER , Gérard CLUZE\n\nAbstract: We propose in this article to define a system which generates a generic care process based on the ABC method. For this purpose, we adapt dynamically the medical information system with UML packages in order to generate some semantic and syntactic links between the different packages that represent the “business objects” of a hospital. These packages contain all the information related to a specific problem for all the patients. So we are able to extract the particular data concerning a criteria (diagnosis, IP number, etc.) and a patient and, in that manner, to re-build the care process. The ABC method gives the skeleton of the care process and allows the definition of costs on a particular care process (e.g. the care process of the patient “John” concerning the disease “kidney failure” in the hospital H). Title:\n\nA DATA WAREHOUSE FOR WEATHER INFORMATION\n\nAuthor(s):\n\nJose Torres-Jimenez , José Torres Jímenez\n\nAbstract: Data warehouse related technologies, allows to extract, group and analyze historical data in order to identify information valuable to decision making processes. In this paper the implementation of a weather data warehouse (WDW) to store Mexico’s weather variables is presented. The weather variables data were provided by the Mexican Institute for Water Technologies (IMTA), the IMTA does research, development, adaptation, human resource formation and technology transfer to improve the Mexico’s water management, and in this way contribute to the sustainable development of Mexico. The implemented WDW contains two dimension tables (one time dimension table and, one geographical dimension table) and one fact table (that stores the data values for weather variables). The time dimension table spans over ten years from 1980 to 1990. The geographical dimension table involves many Mexico’s hydrological zones and comes from 5551 measuring stations. The WDW enables (through the dimensions navigation) the identification of weather patterns that would be useful for: a) agriculture politics definition; b) climatic change research; and c) contingency plans over weather extreme conditions. Even it is well known, but it is important to mention, that the data warehouse paradigm (in many cases) is better to derivate knowledge from the data in comparison to the database paradigm, a fact that was confirmed through the WDW exploitation Title:\n\nINTEGRATION, FLEXIBILITY AND TRANSVERSALITY: ESSENTIAL CHARACTERISTICS OF ERP SYSTEMS\n\nAuthor(s):\n\nLouis Raymond , Sylvestre Uwizeyemungu\n\nAbstract: The interest of firms in ERP systems has been echoed in both the scientific and professional literature. It is worth noting however that while this literature has become increasingly abundant, there does not yet exist an operational definition of the ERP concept that is, if not unanimously, at least widely accepted. This constitutes a handicap for both the research and practice communities. The present study outlines what could be considered as an ERP by first determining the essentially required characteristics of such a system : integration, flexibility and transversality. Indicators are then provided in order to operationalise these three characteristics. The study concludes by proposing a research framework on the impact of an ERP’s key characteristics upon the performance of the system in a given organisational setting. Title:\n\nSEMANTIC INTEGRATION OF DISPARATE DATA SOURCES IN THE COG PROJECT\n\nAuthor(s):\n\nJos de Bruijn\n\nAbstract: We present a novel approach to the integration of structured information sources in enterprises, based on Semantic Web technology. The semantic information integration approach presented in this paper was applies in the COG project. We describe Unicorn's Semantic Information Management along with the Unicorn Workbench tool, which is a component part of the Unicorn System, and how they were applied in the project to solve the information integration problem. We used the Semantic Information Management Methodology and the Unicorn Workbench tool to create an Information Model (an ontology) based on data schemas taken from the automotive industry. We map these data schemas to the Information Model in order to make the meaning of the concepts in the data schemas explicit and relate them to each other, thereby creating an information architecture that provides a unified view of the data sources in the organization. Title:\n\nIMPROVING VIEW SELECTION IN QUERY REWRITING USING DOMAIN SEMANTICS\n\nAuthor(s):\n\nQingyuan Bai , Michael F. McTear , Jun Hong\n\nAbstract: Query rewriting using views is an important issue in data integration. Several algorithms have been proposed, such as the bucket algorithm, the inverse rules algorithm, the SVB algorithm, and the MiniCon algorithm. These algorithms can be divided into two categories. The algorithms of the first category are based on use of buckets while the ones of the second category are based on use of inverse rules. The bucket-based algorithms have not considered the effects of integrity constraints, such as domain semantics, functional and inclusion dependencies. As a result, they might miss query rewritings or generate redundant query rewritings in the presence of these constraints. A bucket-based algorithm consists of two steps. The first step is called view selection that selects views relevant to a given query and puts the views into the corresponding buckets. The second step is to generate all the possible query rewritings by combining a view from each bucket. In this paper, we consider an improvement of view selection in the bucket-based algorithms using domain semantics. We use the resolution method to generate a pseudo residue for each view given a set of domain semantics. Given a query, the pseudo residue of each view is compared with it and any conflict that exists can be found. As a result, irrelevant views can be removed even before a bucket-based algorithm is used. Title:\n\nTHE ABORTION RATE OF LAZY REPLICATION PROTOCOLS FOR DISTRIBUTED DATABASES.\n\nAuthor(s):\n\nLuis Irún-Briz\n\nAbstract: Lazy update protocols have proven to have an undesirable behavior due to their high abortion rate in scenarios with high degree of access conflicts. In this paper, we present the problem of the abortion rate in such protocols from an statistical point of view, in order to provide an expression that predicts the probability of an object to be out of date during the execution of a transaction. It is also suggested a pseudo-optimistic technique that makes use of this expression to reduce the abortion rate caused by accesses to out of date objects. The proposal is validated by means of simulations of the behavior of the expression. Finally, the application of the presented results to improve lazy update protocols is discussed, providing a technique to theoretically determine the boundaries of the improvement. Title:\n\nNEW FAST ALGORITHM FOR INCREMENTAL MINING OF ASSOCIATION RULES\n\nAuthor(s):\n\nyasser El-Sonbaty , Rasha Kashef\n\nAbstract: Mining association rules is a well-studied problem, and several algorithms were presented for finding large itemsets. In this paper we present a new algorithm for incremental discovery of large itemsets in an increasing set of transactions. The proposed algorithm is based on partitioning the database and keeping a summary of local large itemsets for each partition based on the concept of negative border technique. A global summary for the whole database is also created to facilitate the fast updating of overall large itemsets. When adding a new set of transactions to the database, the algorithm uses these summaries instead of scanning the whole database, thus reducing the number of database scans. The results of applying the new algorithm showed that the new technique is quite efficient, and in many respects superior to other incremental algorithms like Fast Update Algorithm (FUP) and Update Large Itemsets (ULI). Title:\n\nWISH QUERY COMPOSER\n\nAuthor(s):\n\nGregory Butler\n\nAbstract: The WISH (With Intuitive Search Help) Query Composer is a software tool for composing form-based queries and their associated reports for relational databases. It incorporates the SQL and XML industry standards to generate user-friendly customizable queries and reports. It uses the very simple but flexible XML semantics to represent database schemas, SQL queries and result datasets, regardless of in which relational database management system (RDBMS) the data is stored. The tool is developed in the Eclipse development environment using the Java programming language with Swing components, and connects to the database through Java Database Connectivity (JDBC). The Java Architecture for XML Binding (JAXB) is used to automate the mapping between XML documents and Java objects. Title:\n\nAN EXCHANGE SERVICE FOR FINANCIAL MARKETS\n\nAuthor(s):\n\nFethi Rabhi , Feras Dabous , Hairong Yu\n\nAbstract: The critical business requirements and compelling nature of the competitive landscape are pushing Information Technology systems away from the traditional centrally controlled corporate-wide architectures towards dynamic, loosely coupled, self-defining and service-based solutions. Web services are regarded as a key technology for addressing the need for connecting extended applications and providing standards and flexibility for enterprise legacy systems integration. This paper reports our experiences when integrating a financial market trading system. This integration process starts from analysing the trading system’s architecture, then identifying system functionality and finally realising the design and implementation of a Web service. Performance and security and the trade-offs involved are the major focus points throughout this process. Comprehensive benchmarking is conducted with and without Web service and security considerations. Title:\n\nDYNAMIC CHANGE OF SERVER ASSIGNMENTS IN DISTRIBUTED WORKFLOW MANAGEMENT SYSTEMS\n\nAuthor(s):\n\nManfred Reichert\n\nAbstract: Process-oriented application systems can only be realized -- with reasonable effort and at acceptable costs -- by the use of a workflow management system (WfMS). Central WfMS, with a single server controlling all workflow (WF) instances, however, may become overloaded very soon. In the WF literature, therefore, many approaches suggest using a multi-server WfMS with distributed WF control. In such a distributed WfMS, the concrete WF server for the control of a particular WF activity is usually defined by an associated server assignment. Following such an approach, problems may occur if components (WF servers, subnets, or gateways) become overloaded or break down. As we know from other fields of computer science, a favorable approach to handle such cases may be to dynamically change hardware assignment. This corresponds to the dynamic change of server assignments in WfMS. For the first time, this paper analyses to what extend this approach is reasonable in such situations. Title:\n\nA/D CASE: A NEW HEART FOR FD3\n\nAuthor(s):\n\nManuel Enciso\n\nAbstract: In [anonymous] we introduce the Functional Dependencies Data Dictionary (FD3) as an architecture to facilitate the integration of database Systems. We propose the use of logics based on the notion of Functional Dependencies (FD) to allows formal specification of the objects of a data model and to conceive future automated treatment. The existence of a FD logic provides a formal language suitable to carry out integration tasks and eases the design of an automatic integration process based in the axiomatic system of the FD logic. Besides that, FD3, provides a High Level Functional Dependencies (HLFD) Data Model which is used in a similar way as the Entity/Relationship Model. In this paper, we develop a CASE tool named A/D CASE (Attribute/Dependence CASE) that illustrates the practical benefits of the FD3 architecture. In the development of A/D CASE we have taken into account other theoretical results which improve our original FD3 proposal [anonymous]. Particularly: * A new functional dependencies logic named SLfd, for removing redundancy in a database sub-model that we present in [anonymous]. The use of SLfd add formalization to software engineering process. * An efficient preprocessing transformation based on the substitution paradigm that we present in [anonymous]. Unlike A/D CASE is independent from the Relational Model, it can be integrated into different database systems and it is compatible with relational DBMSs. Title:\n\nEFFICIENT QUERYING OF TRANSFORMED XML DOCUMENTS\n\nAuthor(s):\n\nGeorg Birkenheuer , Stefan Böttcher , Sven Groppe\n\nAbstract: An application using XML for data representation requires the transformation of XML data if the application accesses XML data of other applications, or of a global database using another XML format. The common approach transforms entire XML documents from one format into another e.g. by using an XSLT stylesheet. The application can then work locally on a copy of the original document transformed in the application-specific format. Different from the common approach, we use an XSLT stylesheet in order to transform a given XPath query such that we retrieve and transform only that part of the XML document which is sufficient to answer the given query. Among other things, our approach avoids problems of replication, saves processing time and in distributed scenarios, transportation costs. Experimental results of a prototype prove that our approach is scalable and efficient. Title:\n\nATTENUATING THE EFFECT OF DATA ABNORMALITIES ON DATA WAREHOUSES\n\nAuthor(s):\n\nOrlando Belo , Anália Lourenço\n\nAbstract: Today’s informational entanglement makes it crucial to enforce adequate management systems. Data warehousing systems appeared with the specific mission of providing adequate contents for data analysis, ensuring gathering, processing and maintenance of all data elements thought valuable. Data analysis in general, data mining and on-line analytical processing facilities, in particular, can achieve better, sharper results, because data quality is finally taken into account. The available elements must be submitted to an intensive processing before being able to integrate them into the data warehouse. Each data warehousing system embraces extraction, transformation and loading processes which are in charge of all the processing concerning the data preparation towards its integration into the data warehouse. Usually, data is scoped at several stages, inspecting data and schema issues and filtering all those elements that do not comply with the established rules. This paper proposes an agent-based platform, which not only ensures the traditional data flow, but also tries to recover the filtered data when an data error occurs. It is intended to perform the process of error monitoring and control automatically. Bad data is processed and eventually repaired by the agents, integrating it again into the data warehouse’s regular flow. All data processing efforts are registered and afterwards mined in order to establish data error patterns. The obtained results will enrich the wrappers knowledge about abnormal situations’ resolution. Eventually, this evolving will enhance the data warehouse population process, enlarging the integrated volume of data and enriching its actual quality and consistency. Title:\n\nA HYBRID APPROACH FOR EFFICIENT STORAGE AND RETRIEVAL OF MULTIDIMENSIONAL DATA\n\nAuthor(s):\n\nJagdish K.T. , Srivani T.K.\n\nAbstract: Mapping from multidimensional data to one-dimensional using Hilbert Index has been studied as a way of indexing for storage and retrieval of multidimensional data. There are mainly two approaches towards Storage and Retrieval of Multidimensional data (Jurgens, 2002) one is the Tree Based Approach and other is Bitmap Indexing. One main benefit of the tree-based approach over the bit map indexing is that they have superior storage property and the insert/update operations are efficient on the other hand the bitmap indexing provides for faster retrieval. Our data structure is mainly based on the tree-based approach in which every node of the tree contains a bit array. The presence of a bit array in every node provides for faster retrieval thereby giving the benefit of both the approaches. In this paper, we present a tree (HT-tree) based on Hilbert Curves for efficient data storage and retrieval of Multidimensional data. The HT-tree data search method mainly makes use of the bit representation of the Hilbert Index values to search for the data, instead of using conventional point search methods as used in most of the R-trees. The proposed data structure overcomes the disadvantages of the HG-tree namely, extra computation of minimum bounding rectangle from the range of Hilbert values required for point search, range search and nearest neighbour search and also the problems occurring from the overlap area and redundant searches. Title:\n\nRELATIONAL SAMPLING FOR DATA QUALITY AUDITING AND DECISION SUPPORT\n\nAuthor(s):\n\nJosé Nuno Oliveira , Bruno Cortes\n\nAbstract: This paper presents a strategy for applying sampling techniques to relational databases, in the context of data quality auditing or decision support processes. Fuzzy cluster sampling is used to survey sets of records for correctness of business rules. Relational algebra estimators are presented as a data quality-auditing tool. Title:\n\nTURNING INFORMATION INTO ACTION: FROM DATA TO BUSINESS PROCESSES THROUGH WEB SERVICES\n\nAuthor(s):\n\nYoucef Baghdadi\n\nAbstract: Sharing Web services across the enterprise and to support business-to-business integration becomes more and more intensive and critical for businesses. This paper proposes a process to generate Web services from the attributes of the business objects and coordination artifacts as described in the highest abstraction level of a business model i.e. the universe of discourse where the elements are unique and not duplicated. Indeed, the elements of the information system, technology-based representation of the universe of discourse, are complex and redundant. The process is based on the concept of factual dependency. The factual dependency is a mechanism that allows aggregations of the attributes that are concerned by the same CRUD operations with respect to the time and the space. Factual dependencies are then validated with respect to the possible business events to keep only the relevant ones. Each distinct and specified operation in terms of input/output parameters generates a lowest level of granularity Web service. These Web services are then registered to be discovered and (re)used at request by any business process. Title:\n\nLIFESTREAMS: BRAIN-FRIENDLY DATA ACCESS\n\nAuthor(s):\n\nJussi Kangasharju , Tobias Limberger , Gerhard Austaller\n\nAbstract: Modern databases are rapidly growing in size and complexity. However, many users do not have enough domain knowledge to formulate precise queries and are thus unable to use these databases to their full potential. In this paper we present our LifeStreams project which aims at a brain-friendly access to data using associations between documents. Associations in LifeStreams are based on examining similarities between documents in several metadata dimensions such as time, location, and keywords. We present a model for real world and abstract entities and discuss how the relationships between entities and documents can be established. We show how LifeStreams visualizes collections of documents using a 3-dimensional visualization technique. We also discuss real-world application scenarios for LifeStreams in a corporate environment. Title:\n\nAN METHOD BASED ON CHAOTIC AND FRACTAL CONTROL FOR SOFTWARE QUALITY - AN EXPERIENCE\n\nAuthor(s):\n\nZHANG Kai\n\nAbstract: Despite the fact that great efforts have been made, there still have been major software problems unsolved, such as overtime and low quality. The chaotic and fractal have become a focal research field recent years, but there are only two papers to study the software quality by chaos tool. The purpose of this paper is to explore an approach how to early control software quality by the chaotic and fractal tools. After the analysis for the growing process of the software defects, the authors believe that the software defect growth has chaotic fractal characteristic, and design a method based on the chaotic and fractal control for process management of software quality. Two experiments have testified to the control efficiency. Title:\n\nIMPROVING QUERY PERFORMANCE ON OLAP-DATA USING ENHANCED MULTIDIMENSIONAL INDICES\n\nAuthor(s):\n\nYaokai Feng , Hiroshi Ryu , Akifumi Makinouchi\n\nAbstract: Multidimensional indices are efficient to improve the query performance on OLAP data. As one multidimensional index structure, R*-tree is popular and successful, which is a member of the famous R-tree family. We enhance the R*-tree to improve the performance of range queries on OLAP data. First, the following observations are presented. (1) The clustering pattern of the tuples (of the OLAP data) among the R*-tree leaf nodes is a decisive factor on range search performance and it is controllable. (2) There often exist many slender nodes when the R*-tree is used to index business data, which causes some problems both with the R*-tree construction and with queries. And then, we propose an approach to control the clustering pattern of tuples and propose an approach to solve the problems of slender nodes, where slender nodes refer to those having a very narrow side (even the side length is zero) in some dimension. Our proposals are examined by experiments using synthetic data and TPC-H benchmark data. Title:\n\nMANAGING WEB-BASED INFORMATION\n\nAuthor(s):\n\nTullio Vernazza , Giancarlo Succi , Alberto Sillitti , Marco Scotto\n\nAbstract: The heterogeneity and the lack of structure of World Wide Web make automated discovery, organization, and management of Web-based information a non-trivial task. Traditional search and indexing tools provide some comfort to users, but they generally provide neither structured information nor categorize, filter, or interpret documents in an automated way. In recent years, these factors have prompted the need for developing data mining techniques applied to the web, giving rise to the term “Web Mining”. This paper introduces the problem of web data extraction and gives a brief analysis of the various techniques to address it. Then, News Miner, a tool for Web Content Mining applied to the news retrieval is presented. Title:\n\nADVANTAGES OF UML FOR MULTIDIMENSIONAL MODELING\n\nAuthor(s):\n\nSergio Luján-Mora , Juan Trujillo , Panos Vassiliadis\n\nAbstract: In the last few years, various approaches for the multidimensional (MD) modeling have been presented. However, none of them has been widely accepted as a standard. In this paper, we summarize the advantages of using object orientation for MD modeling. Furthermore, we use the UML, a standard visual modeling language, for modeling every aspect of MD systems. We show how our approach resolves elegantly some important problems of the MD modeling, such as multistar models, shared hierarchy levels, and heterogeneous dimensions. We believe that our approach, based on the popular UML, can be successfully used for MD modeling and can represent most of frequent MD modeling problems at the conceptual level. Title:\n\nSEMI-STRUCTURED INFORMATION WAREHOUSES: AN APPROACH TO A DOCUMENT MODEL TO SUPPORT THEIR CONSTRUCTION\n\nAuthor(s):\n\nJuan Manuel Pérez Martínez , Rafael Berlanga Llavori , Maria Jose Aramburu Cabo\n\nAbstract: During the last decade, data warehouse and OLAP techniques have helped companies to gather, organize and analyze the structured data they produce. Simultaneously, digital libraries have applied Information Retrieval mechanisms to query their repositories of unstructured documents. In this context, the emergence of XML means the convergence of these two approaches, making possible the development of warehouses for semi-structured information. Although there exist several extensions of traditional data warehouse technology to manage semi-structured information, none of them are based on an underlying document model able to exploit this kind of information. Along this paper we present a set of requirements for semi-structured warehouses, as well as a document model to support their construction. Title:\n\nFACILITATING BUSINESS PROCESS MANAGEMENT WITH HARMONIZED MESSAGING\n\nAuthor(s):\n\nShazia Sadiq , Maria Orlowska , Wasim Sadiq , Karsten Schulz\n\nAbstract: Process communication is characterized by complex interactions between heterogeneous and autonomous systems within the enterprise and often between trading partners. A number of initiatives and proposals are underway to provide solutions for process specification and communication. However, the focus is often on defining APIs and interfaces rather than the semantics of the underlying message exchange. We see a great potential in the enhancement of current messaging infrastructure, in its new role in facilitating complex, long running interactions for dynamic and collaborative processes operating in decentralized environments like the World-Wide Web. In this paper, we primarily present a vision for a technology aimed at providing a level of business logic on the messaging layer, which we denominate as harmonisation of messages.. We will provide the conceptual framework for the harmonized messaging technology and identify fundamental issues for the specification of complex interactions. Title:\n\nMINING CLICKSTREAM-BASED DATA CUBES\n\nAuthor(s):\n\nOrlando Belo , Ronnie Alves\n\nAbstract: Clickstream analysis can reveal usage patterns on company’s web sites giving highly improved understanding of customer behaviour, which can be used to improve customer satisfaction with the website and the company in general, yielding a great business advantage. Such summary information and rules have to be extracted from very large collections of clickstreams in web sites. This is challenging data mining, both in terms of the magnitude of data involved, and the need to incrementally adapt the mined patterns and rules as new data is collected. In this paper, we present some guidelines for implementing on-line analytical mining (OLAM) engines which means an integration of OLAP and mining techniques for exploring multidimensional data cube structures. In addition, we describe a data cube alternative for analyzing clickstreams. Besides, we discussed implementations that we consider efficient approaches on exploring multidimensional data cube structures, such as DBMiner, WebLobMiner, and OLAP-based Web Access Engine. Title:\n\nTRANSACTION CONCEPTS FOR SUPPORTING CHANGES IN DATA WAREHOUSES\n\nAuthor(s):\n\nZbyszko Krolikowski , Robert Wrembel , Bartosz Bebel\n\nAbstract: A data warehouse (DW) provides an information, from external data sources, for analytical processing, decision making, and data mining tools. External data sources are autonomous, i.e. they change over time, independently of a DW. Therefore, the structure and content of a DW has to be periodically synchronized with its external data sources. This synchronization concerns DW data as well as schema. Concurrent work of synchronizing processes and user queries may result in various anomalies. In order to tackle this problem we propose to apply a multiversion data warehouse and an advanced transaction mechanism to a DW synchronization. Title:\n\nAN ALTERNATIVE APPROACH FOR BUILDING WEB-APPLICATIONS\n\nAuthor(s):\n\nOleg Rostanin\n\nAbstract: Nowadays in J2EE-world there is a lot of blueprints, articles and books that propose some recommendations, recipes and patterns for producing web-applications in right way. There are also ready decisions like Jakarta Struts that can be taken as a base of a new project development. While developing the DaMiT e-learning system we tried to collect, analyse and implement many of the architectural features being proposed as well as to invent some new mechanisms such as supporting multiple kinds of client software or introducing XML-based interfaces between application tiers. Title:\n\nRJDBC: A SIMPLE DATABASE REPLICATION ENGINE\n\nAuthor(s):\n\nJavier Esparza Peidro\n\nAbstract: Providing fault tolerant services is a key question among many services manufacturers. Thus, enterprises usually acquire complex and expensive replication engines. This paper offers an interesting choice to organizations which can not afford such costs. RJDBC stands for a simple, easy to install middleware, placed between the application and the database management system, intercepting all database operations and forwarding them among all the replicas of the system. However, from the point of view of the application, the database management system is accessed directly, so that RJDBC is able to supply replication capabilities in a transparent way. Such solution provides acceptable results in clustered configurations. This paper describes the architecture of the solution and some significant results. Title:\n\nTOWARDS DESIGN RATIONALES OF SOFTWARE CONFEDERATIONS\n\nAuthor(s):\n\nMichal Zemlicka\n\nAbstract: The paper discuss reasons why service-oriented architecture is a new software paradigm and the consequences of this fact for the design of enterprise information systems. It is shown that such systems called confederations need not (should not) use web services in the sense of W3C which are more or less a necessity in e-commerce. As business processes supported by enterprise systems must be supervised by businessmen, the same must hold for ccommunication inside confederations. It implies that the interfaces of the services must be user-oriented (user-firendly). It has possitive consequences for the software engineering properties of the confederation. Confederations should sometimes include parts based on a difficult implementation philosophy (e.g. data orientation). Pros and cons of it are discussed. Open issues of service orientation are presented. Title:\n\nSOLVING INTEROPERABILITY PROBLEMS ON A FEDERATION OF SOFTWARE PROCESS SYSTEMS\n\nAuthor(s):\n\nMohamed-Amine MOSTEFAI , Mohamed AHMED-NACER\n\nAbstract: Software process components that share information and that cooperate for common tasks lead to multiple problems of interoperability for software process support environments based on a federation of heterogeneous and autonomous components. Some based-interoperability approaches have been proposed, especially at the conceptual level. However, more problems remain to be solved to enable the heterogeneous process components interoperability at execution level. This paper presents a process-based approach (architecture) for the federation of software process systems. Based on this federation architecture, we focuss on its implementation problems for the process execution interoperability. We show how we solve these problems and we discuss their implementation through the main development platforms of distributed applications. Title:\n\nVERSION MANAGEMENT FOR DATA WAREHOUSE EVOLUTION\n\nAuthor(s):\n\nAlexandre Schlottgen , Nina Edelweiss\n\nAbstract: Various multidimensional data models were proposed in the last years for Data Warehouse (DW) modeling. However, there is a considerable shortage of models that deal with DW schema evolutions. In order to understand the DW life cycle and guarantee the correct and consistent maintenance of the populated data, it is necessary to control the modifications made at multidimensional schemata. This article studies the DW schema modification operations, presenting an extension to ME/R (Multidimensional Entity Relationship Model) to support the multiple versions management of DW schemata. Title:\n\nA RESPONSIBILITY-DRIVEN ARCHITECTURE FOR MOBILE ENTERPRISE APPLICATIONS\n\nAuthor(s):\n\nQusay Mahmoud\n\nAbstract: This paper deals with wireless applications that get downloaded, over the air, on handheld wireless devices and get executed there. Once running, they may need to interact with applications residing on remote wired servers. The motivation for this work is provided in part by the characteristics of the wireless computing environment. There are several implications of these characteristics that require a software architecture that reduces the load on the wireless link and supports disconnected operations. We present a responsibility-driven architecture that enables mobile thin-clients to interact with enterprise servers. We extend this architecture with mobile agent to reduce the load on the wireless link and support disconnected operations. This architecture is capable of supporting multiple devices with or without a client browser. Title:\n\nDESIGN AND REPRESENTATION OF THE TIME DIMENSION IN ENTERPRISE DATA WAREHOUSES - A BUSINESS RELATED PRACTICAL APPROACH\n\nAuthor(s):\n\nAhmed Hezzah , A Min Tjoa\n\nAbstract: A data warehouse provides a consistent view of business data over time. In order to do that data is represented in logical dimensions, with time being one of the most important dimensions. Representing time, however, is not always straightforward due to the complex nature of time issues and the strong dependence of the time dimension on the type of business. This paper addresses the specific issues encountered during the design of the time dimension for multidimensional data warehouses. It introduces design and modeling techniques for representing time in the data warehouse by the use of one or multiple time dimensions or database timestamps. It also discusses generic problems linked to the design and implementation of the time dimension which have to be considered for (global) business processes, such as representing holidays and fiscal periods, increasing the granularity of business facts, considering the observation of daylight saving time and handling different time zones. These problems seem to have wide application, and yet, more in-depth investigations need to be conducted in this field for real-world time-based analysis in enterprise-wide data warehouses. Title:\n\nA METHOD FOR XML DOCUMENT SCHEMA EVOLUTION\n\nAuthor(s):\n\nLina Al-Jadir\n\nAbstract: XML has become an emerging standard for data representation and data exchange on the Web. Although XML data is self-describing, most application domains tend to use document schemas. Over a period of time, these schemas need to be modified to reflect a change in the real-world, a change in the user’s requirements, mistakes or missing information in the initial design. Most of the current XML management systems do not support schema changes. In this paper, we propose a method to manage XML document sch"
    }
}
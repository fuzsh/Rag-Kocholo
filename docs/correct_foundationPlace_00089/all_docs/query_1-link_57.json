{
    "id": "correct_foundationPlace_00089_1",
    "rank": 57,
    "data": {
        "url": "https://github.com/cmhungsteve/Awesome-Transformer-Attention",
        "read_more_link": "",
        "language": "en",
        "title": "Attention: An ultimately comprehensive paper list of Vision Transformer/Attention, including papers, codes, and related websites",
        "top_image": "https://opengraph.githubassets.com/0c8fa43dd26a1ceb6441fe4175393f836455a52e6f0dd54c6d66b12e2aa253ba/cmhungsteve/Awesome-Transformer-Attention",
        "meta_img": "https://opengraph.githubassets.com/0c8fa43dd26a1ceb6441fe4175393f836455a52e6f0dd54c6d66b12e2aa253ba/cmhungsteve/Awesome-Transformer-Attention",
        "images": [
            "https://camo.githubusercontent.com/50cf39121274b3db22bf1bd72cbe25af9078e037441cb5b5bdef1cc9dc5eb2f7/68747470733a2f2f63646e2e7261776769742e636f6d2f73696e647265736f726875732f617765736f6d652f643733303566333864323966656437386661383536353265336136336531353464643865383832392f6d656469612f62616467652e737667",
            "https://avatars.githubusercontent.com/u/17039429?s=64&v=4",
            "https://avatars.githubusercontent.com/u/31293221?s=64&v=4",
            "https://avatars.githubusercontent.com/u/10633528?s=64&v=4",
            "https://avatars.githubusercontent.com/u/30741785?s=64&v=4",
            "https://avatars.githubusercontent.com/u/3616806?s=64&v=4",
            "https://avatars.githubusercontent.com/u/14965173?s=64&v=4",
            "https://avatars.githubusercontent.com/u/22178601?s=64&v=4",
            "https://avatars.githubusercontent.com/u/22496918?s=64&v=4",
            "https://avatars.githubusercontent.com/u/4467042?s=64&v=4",
            "https://avatars.githubusercontent.com/u/7474409?s=64&v=4",
            "https://avatars.githubusercontent.com/u/14948792?s=64&v=4",
            "https://avatars.githubusercontent.com/u/15913177?s=64&v=4",
            "https://avatars.githubusercontent.com/u/19595022?s=64&v=4",
            "https://avatars.githubusercontent.com/u/30721381?s=64&v=4"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "An ultimately comprehensive paper list of Vision Transformer/Attention, including papers, codes, and related websites - cmhungsteve/Awesome-Transformer-Attention",
        "meta_lang": "en",
        "meta_favicon": "https://github.com/fluidicon.png",
        "meta_site_name": "GitHub",
        "canonical_link": "https://github.com/cmhungsteve/Awesome-Transformer-Attention",
        "text": "This repo contains a comprehensive paper list of Vision Transformer & Attention, including papers, codes, and related websites.\n\nThis list is maintained by Min-Hung Chen. (Actively keep updating)\n\nIf you find some ignored papers, feel free to create pull requests, open issues, or email me.\n\nContributions in any form to make this list more comprehensive are welcome.\n\nIf you find this repository useful, please consider citing and â˜…STARing this list.\n\nFeel free to share this list with others!\n\n[Update: January, 2024] Added all the related papers from NeurIPS 2023!\n\n[Update: December, 2023] Added all the related papers from ICCV 2023!\n\n[Update: September, 2023] Split the multi-modal paper list to README_multimodal.md\n\n[Update: June, 2023] Added all the related papers from ICML 2023!\n\n[Update: June, 2023] Added all the related papers from CVPR 2023!\n\n[Update: February, 2023] Added all the related papers from ICLR 2023!\n\n[Update: December, 2022] Added attention-free papers from Networks Beyond Attention (GitHub) made by Jianwei Yang\n\n[Update: November, 2022] Added all the related papers from NeurIPS 2022!\n\n[Update: October, 2022] Split the 2nd half of the paper list to README_2.md\n\n[Update: October, 2022] Added all the related papers from ECCV 2022!\n\n[Update: September, 2022] Added the Transformer tutorial slides made by Lucas Beyer!\n\n[Update: June, 2022] Added all the related papers from CVPR 2022!\n\nCitation\n\nSurvey\n\nImage Classification / Backbone\n\nReplace Conv w/ Attention\n\nPure Attention\n\nConv-stem + Attention\n\nConv + Attention\n\nVision Transformer\n\nGeneral Vision Transformer\n\nEfficient Vision Transformer\n\nConv + Transformer\n\nTraining + Transformer\n\nRobustness + Transformer\n\nModel Compression + Transformer\n\nAttention-Free\n\nMLP-Series\n\nOther Attention-Free\n\nAnalysis for Transformer\n\nDetection\n\nObject Detection\n\n3D Object Detection\n\nMulti-Modal Detection\n\nHOI Detection\n\nSalient Object Detection\n\nOther Detection Tasks\n\nSegmentation\n\nSemantic Segmentation\n\nDepth Estimation\n\nObject Segmentation\n\nOther Segmentation Tasks\n\nVideo (High-level)\n\nAction Recognition\n\nAction Detection/Localization\n\nAction Prediction/Anticipation\n\nVideo Object Segmentation\n\nVideo Instance Segmentation\n\nOther Video Tasks\n\nReferences\n\n------ (The following papers are moved to README_multimodal.md) ------\n\nMulti-Modality\n\nVisual Captioning\n\nVisual Question Answering\n\nVisual Grounding\n\nMulti-Modal Representation Learning\n\nMulti-Modal Retrieval\n\nMulti-Modal Generation\n\nPrompt Learning/Tuning\n\nVisual Document Understanding\n\nOther Multi-Modal Tasks\n\n------ (The following papers are moved to README_2.md) ------\n\nOther High-level Vision Tasks\n\nPoint Cloud / 3D\n\nPose Estimation\n\nTracking\n\nRe-ID\n\nFace\n\nScene Graph\n\nNeural Architecture Search\n\nTransfer / X-Supervised / X-Shot / Continual Learning\n\nLow-level Vision Tasks\n\nImage Restoration\n\nVideo Restoration\n\nInpainting / Completion / Outpainting\n\nImage Generation\n\nVideo Generation\n\nTransfer / Translation / Manipulation\n\nOther Low-Level Tasks\n\nReinforcement Learning\n\nNavigation\n\nOther RL Tasks\n\nMedical\n\nMedical Segmentation\n\nMedical Classification\n\nMedical Detection\n\nMedical Reconstruction\n\nMedical Low-Level Vision\n\nMedical Vision-Language\n\nMedical Others\n\nOther Tasks\n\nAttention Mechanisms in Vision/NLP\n\nAttention for Vision\n\nNLP\n\nBoth\n\nOthers\n\nIf you find this repository useful, please consider citing this list:\n\n@misc{chen2022transformerpaperlist, title = {Ultimate awesome paper list: transformer and attention}, author = {Chen, Min-Hung}, journal = {GitHub repository}, url = {https://github.com/cmhungsteve/Awesome-Transformer-Attention}, year = {2022}, }\n\n\"A Survey on Multimodal Large Language Models for Autonomous Driving\", WACVW, 2024 (Purdue). [Paper][GitHub]\n\n\"Efficient Multimodal Large Language Models: A Survey\", arXiv, 2024 (Tencent). [Paper][GitHub]\n\n\"From Sora What We Can See: A Survey of Text-to-Video Generation\", arXiv, 2024 (Newcastle University, UK). [Paper][GitHub]\n\n\"When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks via Multi-modal Large Language Models\", arXiv, 2024 (Oxford). [Paper][GitHub]\n\n\"Foundation Models for Video Understanding: A Survey\", arXiv, 2024 (Aalborg University, Denmark). [Paper][GitHub]\n\n\"Vision Mamba: A Comprehensive Survey and Taxonomy\", arXiv, 2024 (Chongqing University). [Paper][GitHub]\n\n\"Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond\", arXiv, 2024 (GigaAI, China). [Paper][GitHub]\n\n\"Video Diffusion Models: A Survey\", arXiv, 2024 (Bielefeld University, Germany). [Paper][GitHub]\n\n\"Unleashing the Power of Multi-Task Learning: A Comprehensive Survey Spanning Traditional, Deep, and Pretrained Foundation Model Eras\", arXiv, 2024 (Lehigh + UPenn). [Paper]\n\n\"Hallucination of Multimodal Large Language Models: A Survey\", arXiv, 2024 (NUS). [Paper][GitHub]\n\n\"A Survey on Vision Mamba: Models, Applications and Challenges\", arXiv, 2024 (HKUST). [Paper][GitHub]\n\n\"State Space Model for New-Generation Network Alternative to Transformers: A Survey\", arXiv, 2024 (Anhui University). [Paper][GitHub]\n\n\"Exploring the Frontier of Vision-Language Models: A Survey of Current Methodologies and Future Directions\", arXiv, 2024 (IIT Patna). [Paper]\n\n\"From Pixels to Insights: A Survey on Automatic Chart Understanding in the Era of Large Foundation Models\", arXiv, 2024 (UIUC). [Paper][GitHub]\n\n\"Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey\", arXiv, 2024 (Northeastern). [Paper]\n\n\"Sora as an AGI World Model? A Complete Survey on Text-to-Video Generation\", arXiv, 2024 (Kyung Hee University). [Paper]\n\n\"Controllable Generation with Text-to-Image Diffusion Models: A Survey\", arXiv, 2024 (Beijing University of Posts and Telecommunications). [Paper][GitHub]\n\n\"Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models\", arXiv, 2024 (Lehigh University, Pennsylvania). [Paper][GitHub]\n\n\"Large Multimodal Agents: A Survey\", arXiv, 2024 (CUHK). [Paper][GitHub]\n\n\"Parameter-Efficient Fine-Tuning for Pre-Trained Vision Models: A Survey\", arXiv, 2024 (BIGAI). [Paper][GitHub]\n\n\"Vision-Language Navigation with Embodied Intelligence: A Survey\", arXiv, 2024 (Qufu Normal University, China). [Paper]\n\n\"The (R)Evolution of Multimodal Large Language Models: A Survey\", arXiv, 2024 (University of Modena and Reggio Emilia (UniMoRE), Italy). [Paper]\n\n\"Masked Modeling for Self-supervised Representation Learning on Vision and Beyond\", arXiv, 2024 (Westlake University, China). [Paper][GitHub]\n\n\"Transformer for Object Re-Identification: A Survey\", arXiv, 2024 (Wuhan University). [Paper]\n\n\"Forging Vision Foundation Models for Autonomous Driving: Challenges, Methodologies, and Opportunities\", arXiv, 2024 (Huawei). [Paper][GtiHub]\n\n\"MM-LLMs: Recent Advances in MultiModal Large Language Models\", arXiv, 2024 (Tencent). [Paper]\n\n\"From GPT-4 to Gemini and Beyond: Assessing the Landscape of MLLMs on Generalizability, Trustworthiness and Causality through Four Modalities\", arXiv, 2024 (Shanghai AI Lab). [Paper]\n\n\"A Survey on Hallucination in Large Vision-Language Models\", arXiv, 2024 (Huawei). [Paper]\n\n\"A Survey for Foundation Models in Autonomous Driving\", arXiv, 2024 (Motional, Massachusetts). [Paper]\n\n\"A Survey on Transformer Compression\", arXiv, 2024 (Huawei). [Paper]\n\n\"Vision + Language Applications: A Survey\", CVPRW, 2023 (Ritsumeikan University, Japan). [Paper][GitHub]\n\n\"Multimodal Learning With Transformers: A Survey\", TPAMI, 2023 (Tsinghua & Oxford). [Paper]\n\n\"A Survey of Visual Transformers\", TNNLS, 2023 (CAS). [Paper][GitHub]\n\n\"Video Understanding with Large Language Models: A Survey\", arXiv, 2023 (University of Rochester). [Paper][GitHub]\n\n\"Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey\", arXiv, 2023 (NTU, Singapore). [Paper]\n\n\"A Survey of Reasoning with Foundation Models: Concepts, Methodologies, and Outlook\", arXiv, 2023 (Huawei). [Paper][GitHub]\n\n\"A Challenger to GPT-4V? Early Explorations of Gemini in Visual Expertise\", arXiv, 2023 (Tencent). [Paper]GitHub]\n\n\"Towards the Unification of Generative and Discriminative Visual Foundation Model: A Survey\", arXiv, 2023 (JHU). [Paper]\n\n\"Explainability of Vision Transformers: A Comprehensive Review and New Perspectives\", arXiv, 2023 (Institute for Research in Fundamental Sciences (IPM), Iran). [Paper]\n\n\"Vision-Language Instruction Tuning: A Review and Analysis\", arXiv, 2023 (Tencent). [Paper][GitHub (in construction)]\n\n\"Understanding Video Transformers for Segmentation: A Survey of Application and Interpretability\", arXiv, 2023 (York University). [Paper]\n\n\"Unsupervised Object Localization in the Era of Self-Supervised ViTs: A Survey\", arXiv, 2023 (valeo.ai, France). [Paper][GitHub]\n\n\"A Survey on Video Diffusion Models\", arXiv, 2023 (Fudan). [Paper][GitHub]\n\n\"The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)\", arXiv, 2023 (Microsoft). [Paper]\n\n\"Multimodal Foundation Models: From Specialists to General-Purpose Assistants\", arXiv, 2023 (Microsoft). [Paper]\n\n\"Transformers in Small Object Detection: A Benchmark and Survey of State-of-the-Art\", arXiv, 2023 (University of Western Australia). [Paper]\n\n\"RenAIssance: A Survey into AI Text-to-Image Generation in the Era of Large Model\", arXiv, 2023 (University of Sydney). [Paper]\n\n\"A survey on efficient vision transformers: algorithms, techniques, and performance benchmarking\", arXiv, 2023 (The University of Sydney). [Paper]\n\n\"From CNN to Transformer: A Review of Medical Image Segmentation Models\", arXiv, 2023 (UESTC). [Paper]\n\n\"Foundational Models Defining a New Era in Vision: A Survey and Outlook\", arXiv, 2023 (MBZUAI). [Paper][GitHub]\n\n\"A Systematic Survey of Prompt Engineering on Vision-Language Foundation Models\", arXiv, 2023 (Oxford). [Paper]\n\n\"Robust Visual Question Answering: Datasets, Methods, and Future Challenges\", arXiv, 2023 (Xi'an Jiaotong University). [Paper]\n\n\"A Survey on Open-Vocabulary Detection and Segmentation: Past, Present, and Future\", arXiv, 2023 (HKUST). [Paper]\n\n\"Transformers in Reinforcement Learning: A Survey\", arXiv, 2023 (Mila). [Paper]\n\n\"Vision Language Transformers: A Survey\", arXiv, 2023 (Boise State University, Idaho). [Paper]\n\n\"Towards Open Vocabulary Learning: A Survey\", arXiv, 2023 (Peking). [Paper][GitHub]\n\n\"Large Multimodal Models: Notes on CVPR 2023 Tutorial\", arXiv, 2023 (Microsoft). [Paper]\n\n\"A Survey on Multimodal Large Language Models\", arXiv, 2023 (USTC). [Paper][GitHub]\n\n\"2D Object Detection with Transformers: A Review\", arXiv, 2023 (German Research Center for Artificial Intelligence, Germany). [Paper]\n\n\"Visual Question Answering: A Survey on Techniques and Common Trends in Recent Literature\", arXiv, 2023 (Eldoradoâ€™s Institute of Technology, Brazil). [Paper]\n\n\"Vision-Language Models in Remote Sensing: Current Progress and Future Trends\", arXiv, 2023 (NYU). [Paper]\n\n\"Visual Tuning\", arXiv, 2023 (The Hong Kong Polytechnic University). [Paper]\n\n\"Self-supervised Learning for Pre-Training 3D Point Clouds: A Survey\", arXiv, 2023 (Fudan University). [Paper]\n\n\"Semantic Segmentation using Vision Transformers: A survey\", arXiv, 2023 (University of Peradeniya, Sri Lanka). [Paper]\n\n\"A Review of Deep Learning for Video Captioning\", arXiv, 2023 (Deakin University, Australia). [Paper]\n\n\"Transformer-Based Visual Segmentation: A Survey\", arXiv, 2023 (NTU, Singapore). [Paper][GitHub]\n\n\"Vision-Language Models for Vision Tasks: A Survey\", arXiv, 2023 (?). [Paper][GitHub (in construction)]\n\n\"Text-to-image Diffusion Model in Generative AI: A Survey\", arXiv, 2023 (KAIST). [Paper]\n\n\"Foundation Models for Decision Making: Problems, Methods, and Opportunities\", arXiv, 2023 (Berkeley + Google). [Paper]\n\n\"Advances in Medical Image Analysis with Vision Transformers: A Comprehensive Review\", arXiv, 2023 (RWTH Aachen University, Germany). [Paper][GitHub]\n\n\"Efficiency 360: Efficient Vision Transformers\", arXiv, 2023 (IBM). [Paper][GitHub]\n\n\"Transformer-based Generative Adversarial Networks in Computer Vision: A Comprehensive Survey\", arXiv, 2023 (Indian Institute of Information Technology). [Paper]\n\n\"Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey\", arXiv, 2023 (Pengcheng Laboratory). [Paper][GitHub]\n\n\"A Survey on Visual Transformer\", TPAMI, 2022 (Huawei). [Paper]\n\n\"Attention mechanisms in computer vision: A survey\", Computational Visual Media, 2022 (Tsinghua University, China). [Paper][Springer][Github]\n\n\"A Comprehensive Study of Vision Transformers on Dense Prediction Tasks\", VISAP, 2022 (NavInfo Europe, Netherlands). [Paper]\n\n\"Vision-and-Language Pretrained Models: A Survey\", IJCAI, 2022 (The University of Sydney). [Paper]\n\n\"Vision Transformers in Medical Imaging: A Review\", arXiv, 2022 (Covenant University, Nigeria). [Paper]\n\n\"A Comprehensive Survey of Transformers for Computer Vision\", arXiv, 2022 (Sejong University). [Paper]\n\n\"Vision-Language Pre-training: Basics, Recent Advances, and Future Trends\", arXiv, 2022 (Microsoft). [Paper]\n\n\"Vision+X: A Survey on Multimodal Learning in the Light of Data\", arXiv, 2022 (Illinois Institute of Technology, Chicago). [Paper]\n\n\"Vision Transformers for Action Recognition: A Survey\", arXiv, 2022 (Charles Sturt University, Australia). [Paper]\n\n\"VLP: A Survey on Vision-Language Pre-training\", arXiv, 2022 (CAS). [Paper]\n\n\"Transformers in Remote Sensing: A Survey\", arXiv, 2022 (MBZUAI). [Paper][Github]\n\n\"Medical image analysis based on transformer: A Review\", arXiv, 2022 (NUS, Singapore). [Paper]\n\n\"3D Vision with Transformers: A Survey\", arXiv, 2022 (MBZUAI). [Paper][GitHub]\n\n\"Vision Transformers: State of the Art and Research Challenges\", arXiv, 2022 (NYCU). [Paper]\n\n\"Transformers in Medical Imaging: A Survey\", arXiv, 2022 (MBZUAI). [Paper][GitHub]\n\n\"Multimodal Learning with Transformers: A Survey\", arXiv, 2022 (Oxford). [Paper]\n\n\"Transforming medical imaging with Transformers? A comparative review of key properties, current progresses, and future perspectives\", arXiv, 2022 (CAS). [Paper]\n\n\"Transformers in 3D Point Clouds: A Survey\", arXiv, 2022 (University of Waterloo). [Paper]\n\n\"A survey on attention mechanisms for medical applications: are we moving towards better algorithms?\", arXiv, 2022 (INESC TEC and University of Porto, Portugal). [Paper]\n\n\"Efficient Transformers: A Survey\", arXiv, 2022 (Google). [Paper]\n\n\"Are we ready for a new paradigm shift? A Survey on Visual Deep MLP\", arXiv, 2022 (Tsinghua). [Paper]\n\n\"Vision Transformers in Medical Computer Vision - A Contemplative Retrospection\", arXiv, 2022 (National University of Sciences and Technology (NUST), Pakistan). [Paper]\n\n\"Video Transformers: A Survey\", arXiv, 2022 (Universitat de Barcelona, Spain). [Paper]\n\n\"Transformers in Medical Image Analysis: A Review\", arXiv, 2022 (Nanjing University). [Paper]\n\n\"Recent Advances in Vision Transformer: A Survey and Outlook of Recent Work\", arXiv, 2022 (?). [Paper]\n\n\"Transformers Meet Visual Learning Understanding: A Comprehensive Review\", arXiv, 2022 (Xidian University). [Paper]\n\n\"Image Captioning In the Transformer Age\", arXiv, 2022 (Alibaba). [Paper][GitHub]\n\n\"Visual Attention Methods in Deep Learning: An In-Depth Survey\", arXiv, 2022 (Fayoum University, Egypt). [Paper]\n\n\"Transformers in Vision: A Survey\", ACM Computing Surveys, 2021 (MBZUAI). [Paper]\n\n\"Survey: Transformer based Video-Language Pre-training\", arXiv, 2021 (Renmin University of China). [Paper]\n\n\"A Survey of Transformers\", arXiv, 2021 (Fudan). [Paper]\n\n\"Attention mechanisms and deep learning for machine vision: A survey of the state of the art\", arXiv, 2021 (University of Kashmir, India). [Paper]\n\n[Back to Overview]\n\nImage Classification / Backbone\n\nReplace Conv w/ Attention\n\nPure Attention\n\nLR-Net: \"Local Relation Networks for Image Recognition\", ICCV, 2019 (Microsoft). [Paper][PyTorch (gan3sh500)]\n\nSASA: \"Stand-Alone Self-Attention in Vision Models\", NeurIPS, 2019 (Google). [Paper][PyTorch-1 (leaderj1001)][PyTorch-2 (MerHS)]\n\nAxial-Transformer: \"Axial Attention in Multidimensional Transformers\", arXiv, 2019 (Google). [Paper][PyTorch (lucidrains)]\n\nSAN: \"Exploring Self-attention for Image Recognition\", CVPR, 2020 (CUHK + Intel). [Paper][PyTorch]\n\nAxial-DeepLab: \"Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation\", ECCV, 2020 (Google). [Paper][PyTorch]\n\nConv-stem + Attention\n\nGSA-Net: \"Global Self-Attention Networks for Image Recognition\", arXiv, 2020 (Google). [Paper][PyTorch (lucidrains)]\n\nHaloNet: \"Scaling Local Self-Attention For Parameter Efficient Visual Backbones\", CVPR, 2021 (Google). [Paper][PyTorch (lucidrains)]\n\nCoTNet: \"Contextual Transformer Networks for Visual Recognition\", CVPRW, 2021 (JD). [Paper][PyTorch]\n\nHAT-Net: \"Vision Transformers with Hierarchical Attention\", arXiv, 2022 (ETHZ). [Paper][PyTorch (in construction)]\n\nConv + Attention\n\nAA: \"Attention Augmented Convolutional Networks\", ICCV, 2019 (Google). [Paper][PyTorch (leaderj1001)][Tensorflow (titu1994)]\n\nGCNet: \"Global Context Networks\", ICCVW, 2019 (& TPAMI 2020) (Microsoft). [Paper][PyTorch]\n\nLambdaNetworks: \"LambdaNetworks: Modeling long-range Interactions without Attention\", ICLR, 2021 (Google). [Paper][PyTorch-1 (lucidrains)][PyTorch-2 (leaderj1001)]\n\nBoTNet: \"Bottleneck Transformers for Visual Recognition\", CVPR, 2021 (Google). [Paper][PyTorch-1 (lucidrains)][PyTorch-2 (leaderj1001)]\n\nGCT: \"Gaussian Context Transformer\", CVPR, 2021 (Zhejiang University). [Paper]\n\nCoAtNet: \"CoAtNet: Marrying Convolution and Attention for All Data Sizes\", NeurIPS, 2021 (Google). [Paper]\n\nACmix: \"On the Integration of Self-Attention and Convolution\", CVPR, 2022 (Tsinghua). [Paper][PyTorch]\n\n[Back to Overview]\n\nVision Transformer\n\nGeneral Vision Transformer\n\nViT: \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\", ICLR, 2021 (Google). [Paper][Tensorflow][PyTorch (lucidrains)][JAX (conceptofmind)]\n\nPerceiver: \"Perceiver: General Perception with Iterative Attention\", ICML, 2021 (DeepMind). [Paper][PyTorch (lucidrains)]\n\nPiT: \"Rethinking Spatial Dimensions of Vision Transformers\", ICCV, 2021 (NAVER). [Paper][PyTorch]\n\nVT: \"Visual Transformers: Where Do Transformers Really Belong in Vision Models?\", ICCV, 2021 (Facebook). [Paper][PyTorch (tahmid0007)]\n\nPVT: \"Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions\", ICCV, 2021 (Nanjing University). [Paper][PyTorch]\n\niRPE: \"Rethinking and Improving Relative Position Encoding for Vision Transformer\", ICCV, 2021 (Microsoft). [Paper][PyTorch]\n\nCaiT: \"Going deeper with Image Transformers\", ICCV, 2021 (Facebook). [Paper][PyTorch]\n\nSwin-Transformer: \"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\", ICCV, 2021 (Microsoft). [Paper][PyTorch][PyTorch (berniwal)]\n\nT2T-ViT: \"Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet\", ICCV, 2021 (Yitu). [Paper][PyTorch]\n\nFFNBN: \"Leveraging Batch Normalization for Vision Transformers\", ICCVW, 2021 (Microsoft). [Paper]\n\nDPT: \"DPT: Deformable Patch-based Transformer for Visual Recognition\", ACMMM, 2021 (CAS). [Paper][PyTorch]\n\nFocal: \"Focal Attention for Long-Range Interactions in Vision Transformers\", NeurIPS, 2021 (Microsoft). [Paper][PyTorch]\n\nXCiT: \"XCiT: Cross-Covariance Image Transformers\", NeurIPS, 2021 (Facebook). [Paper]\n\nTwins: \"Twins: Revisiting Spatial Attention Design in Vision Transformers\", NeurIPS, 2021 (Meituan). [Paper][PyTorch)]\n\nARM: \"Blending Anti-Aliasing into Vision Transformer\", NeurIPS, 2021 (Amazon). [Paper][GitHub (in construction)]\n\nDVT: \"Not All Images are Worth 16x16 Words: Dynamic Vision Transformers with Adaptive Sequence Length\", NeurIPS, 2021 (Tsinghua). [Paper][PyTorch]\n\nAug-S: \"Augmented Shortcuts for Vision Transformers\", NeurIPS, 2021 (Huawei). [Paper]\n\nTNT: \"Transformer in Transformer\", NeurIPS, 2021 (Huawei). [Paper][PyTorch][PyTorch (lucidrains)]\n\nViTAE: \"ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive Bias\", NeurIPS, 2021 (The University of Sydney). [Paper][PyTorch]\n\nDeepViT: \"DeepViT: Towards Deeper Vision Transformer\", arXiv, 2021 (NUS + ByteDance). [Paper][Code]\n\nSo-ViT: \"So-ViT: Mind Visual Tokens for Vision Transformer\", arXiv, 2021 (Dalian University of Technology). [Paper][PyTorch]\n\nLV-ViT: \"All Tokens Matter: Token Labeling for Training Better Vision Transformers\", NeurIPS, 2021 (ByteDance). [Paper][PyTorch]\n\nNesT: \"Aggregating Nested Transformers\", arXiv, 2021 (Google). [Paper][Tensorflow]\n\nKVT: \"KVT: k-NN Attention for Boosting Vision Transformers\", arXiv, 2021 (Alibaba). [Paper]\n\nRefined-ViT: \"Refiner: Refining Self-attention for Vision Transformers\", arXiv, 2021 (NUS, Singapore). [Paper][PyTorch]\n\nShuffle-Transformer: \"Shuffle Transformer: Rethinking Spatial Shuffle for Vision Transformer\", arXiv, 2021 (Tencent). [Paper]\n\nCAT: \"CAT: Cross Attention in Vision Transformer\", arXiv, 2021 (KuaiShou). [Paper][PyTorch]\n\nV-MoE: \"Scaling Vision with Sparse Mixture of Experts\", arXiv, 2021 (Google). [Paper]\n\nP2T: \"P2T: Pyramid Pooling Transformer for Scene Understanding\", arXiv, 2021 (Nankai University). [Paper]\n\nPvTv2: \"PVTv2: Improved Baselines with Pyramid Vision Transformer\", arXiv, 2021 (Nanjing University). [Paper][PyTorch]\n\nLG-Transformer: \"Local-to-Global Self-Attention in Vision Transformers\", arXiv, 2021 (IIAI, UAE). [Paper]\n\nViP: \"Visual Parser: Representing Part-whole Hierarchies with Transformers\", arXiv, 2021 (Oxford). [Paper]\n\nScaled-ReLU: \"Scaled ReLU Matters for Training Vision Transformers\", AAAI, 2022 (Alibaba). [Paper]\n\nLIT: \"Less is More: Pay Less Attention in Vision Transformers\", AAAI, 2022 (Monash University). [Paper][PyTorch]\n\nDTN: \"Dynamic Token Normalization Improves Vision Transformer\", ICLR, 2022 (Tencent). [Paper][PyTorch (in construction)]\n\nRegionViT: \"RegionViT: Regional-to-Local Attention for Vision Transformers\", ICLR, 2022 (MIT-IBM Watson). [Paper][PyTorch]\n\nCrossFormer: \"CrossFormer: A Versatile Vision Transformer Based on Cross-scale Attention\", ICLR, 2022 (Zhejiang University). [Paper][PyTorch]\n\n?: \"Scaling the Depth of Vision Transformers via the Fourier Domain Analysis\", ICLR, 2022 (UT Austin). [Paper]\n\nViT-G: \"Scaling Vision Transformers\", CVPR, 2022 (Google). [Paper]\n\nCSWin: \"CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows\", CVPR, 2022 (Microsoft). [Paper][PyTorch]\n\nMPViT: \"MPViT: Multi-Path Vision Transformer for Dense Prediction\", CVPR, 2022 (KAIST). [Paper][PyTorch]\n\nDiverse-ViT: \"The Principle of Diversity: Training Stronger Vision Transformers Calls for Reducing All Levels of Redundancy\", CVPR, 2022 (UT Austin). [Paper][PyTorch]\n\nDW-ViT: \"Beyond Fixation: Dynamic Window Visual Transformer\", CVPR, 2022 (Dark Matter AI, China). [Paper][PyTorch (in construction)]\n\nMixFormer: \"MixFormer: Mixing Features across Windows and Dimensions\", CVPR, 2022 (Baidu). [Paper][Paddle]\n\nDAT: \"Vision Transformer with Deformable Attention\", CVPR, 2022 (Tsinghua). [Paper][PyTorch]\n\nSwin-Transformer-V2: \"Swin Transformer V2: Scaling Up Capacity and Resolution\", CVPR, 2022 (Microsoft). [Paper][PyTorch]\n\nMSG-Transformer: \"MSG-Transformer: Exchanging Local Spatial Information by Manipulating Messenger Tokens\", CVPR, 2022 (Huazhong University of Science & Technology). [Paper][PyTorch]\n\nNomMer: \"NomMer: Nominate Synergistic Context in Vision Transformer for Visual Recognition\", CVPR, 2022 (Tencent). [Paper][PyTorch]\n\nShunted: \"Shunted Self-Attention via Multi-Scale Token Aggregation\", CVPR, 2022 (NUS). [Paper][PyTorch]\n\nPyramidTNT: \"PyramidTNT: Improved Transformer-in-Transformer Baselines with Pyramid Architecture\", CVPRW, 2022 (Huawei). [Paper][PyTorch]\n\nX-ViT: \"X-ViT: High Performance Linear Vision Transformer without Softmax\", CVPRW, 2022 (Kakao). [Paper]\n\nReMixer: \"ReMixer: Object-aware Mixing Layer for Vision Transformers\", CVPRW, 2022 (KAIST). [Paper][PyTorch]\n\nUN: \"Unified Normalization for Accelerating and Stabilizing Transformers\", ACMMM, 2022 (Hikvision). [Paper][Code (in construction)]\n\nWave-ViT: \"Wave-ViT: Unifying Wavelet and Transformers for Visual Representation Learning\", ECCV, 2022 (JD). [Paper][PyTorch]\n\nDaViT: \"DaViT: Dual Attention Vision Transformers\", ECCV, 2022 (Microsoft). [Paper][PyTorch]\n\nScalableViT: \"ScalableViT: Rethinking the Context-oriented Generalization of Vision Transformer\", ECCV, 2022 (ByteDance). [Paper]\n\nMaxViT: \"MaxViT: Multi-Axis Vision Transformer\", ECCV, 2022 (Google). [Paper][Tensorflow]\n\nVSA: \"VSA: Learning Varied-Size Window Attention in Vision Transformers\", ECCV, 2022 (The University of Sydney). [Paper][PyTorch]\n\n?: \"Expediting Large-Scale Vision Transformer for Dense Prediction without Fine-tuning\", NeurIPS, 2022 (Microsoft). [Paper]\n\nOrtho: \"Orthogonal Transformer: An Efficient Vision Transformer Backbone with Token Orthogonalization\", NeurIPS, 2022 (CAS). [Paper]\n\nPerViT: \"Peripheral Vision Transformer\", NeurIPS, 2022 (POSTECH). [Paper]\n\nLITv2: \"Fast Vision Transformers with HiLo Attention\", NeurIPS, 2022 (Monash University). [Paper][PyTorch]\n\nBViT: \"BViT: Broad Attention based Vision Transformer\", arXiv, 2022 (CAS). [Paper]\n\nO-ViT: \"O-ViT: Orthogonal Vision Transformer\", arXiv, 2022 (East China Normal University). [Paper]\n\nMOA-Transformer: \"Aggregating Global Features into Local Vision Transformer\", arXiv, 2022 (University of Kansas). [Paper][PyTorch]\n\nBOAT: \"BOAT: Bilateral Local Attention Vision Transformer\", arXiv, 2022 (Baidu + HKU). [Paper]\n\nViTAEv2: \"ViTAEv2: Vision Transformer Advanced by Exploring Inductive Bias for Image Recognition and Beyond\", arXiv, 2022 (The University of Sydney). [Paper]\n\nHiP: \"Hierarchical Perceiver\", arXiv, 2022 (DeepMind). [Paper]\n\nPatchMerger: \"Learning to Merge Tokens in Vision Transformers\", arXiv, 2022 (Google). [Paper]\n\nDGT: \"Dynamic Group Transformer: A General Vision Transformer Backbone with Dynamic Group Attention\", arXiv, 2022 (Baidu). [Paper]\n\nNAT: \"Neighborhood Attention Transformer\", arXiv, 2022 (Oregon). [Paper][PyTorch]\n\nASF-former: \"Adaptive Split-Fusion Transformer\", arXiv, 2022 (Fudan). [Paper][PyTorch (in construction)]\n\nSP-ViT: \"SP-ViT: Learning 2D Spatial Priors for Vision Transformers\", arXiv, 2022 (Alibaba). [Paper]\n\nEATFormer: \"EATFormer: Improving Vision Transformer Inspired by Evolutionary Algorithm\", arXiv, 2022 (Zhejiang University). [Paper]\n\nLinGlo: \"Rethinking Query-Key Pairwise Interactions in Vision Transformers\", arXiv, 2022 (TCL Research Wuhan). [Paper]\n\nDual-ViT: \"Dual Vision Transformer\", arXiv, 2022 (JD). [Paper][PyTorch]\n\nMMA: \"Multi-manifold Attention for Vision Transformers\", arXiv, 2022 (Centre for Research and Technology Hellas, Greece). [Paper]\n\nMAFormer: \"MAFormer: A Transformer Network with Multi-scale Attention Fusion for Visual Recognition\", arXiv, 2022 (Baidu). [Paper]\n\nAEWin: \"Axially Expanded Windows for Local-Global Interaction in Vision Transformers\", arXiv, 2022 (Southwest Jiaotong University). [Paper]\n\nGrafT: \"Grafting Vision Transformers\", arXiv, 2022 (Stony Brook). [Paper]\n\n?: \"Rethinking Hierarchicies in Pre-trained Plain Vision Transformer\", arXiv, 2022 (The University of Sydney). [Paper]\n\nLTH-ViT: \"The Lottery Ticket Hypothesis for Vision Transformers\", arXiv, 2022 (Northeastern University, China). [Paper]\n\nTT: \"Token Transformer: Can class token help window-based transformer build better long-range interactions?\", arXiv, 2022 (Hangzhou Dianzi University). [Paper]\n\nINTERN: \"INTERN: A New Learning Paradigm Towards General Vision\", arXiv, 2022 (Shanghai AI Lab). [Paper][Website]\n\nGGeM: \"Group Generalized Mean Pooling for Vision Transformer\", arXiv, 2022 (NAVER). [Paper]\n\nGPViT: \"GPViT: A High Resolution Non-Hierarchical Vision Transformer with Group Propagation\", ICLR, 2023 (University of Edinburgh, Scotland + UCSD). [Paper][PyTorch]\n\nCPVT: \"Conditional Positional Encodings for Vision Transformers\", ICLR, 2023 (Meituan). [Paper][Code (in construction)]\n\nLipsFormer: \"LipsFormer: Introducing Lipschitz Continuity to Vision Transformers\", ICLR, 2023 (IDEA, China). [Paper][Code (in construction)]\n\nBiFormer: \"BiFormer: Vision Transformer with Bi-Level Routing Attention\", CVPR, 2023 (CUHK). [Paper][PyTorch]\n\nAbSViT: \"Top-Down Visual Attention from Analysis by Synthesis\", CVPR, 2023 (Berkeley). [Paper][PyTorch][Website]\n\nDependencyViT: \"Visual Dependency Transformers: Dependency Tree Emerges From Reversed Attention\", CVPR, 2023 (MIT). [Paper][Code (in construction)]\n\nResFormer: \"ResFormer: Scaling ViTs with Multi-Resolution Training\", CVPR, 2023 (Fudan). [Paper][PyTorch (in construction)]\n\nSViT: \"Vision Transformer with Super Token Sampling\", CVPR, 2023 (CAS). [Paper]\n\nPaCa-ViT: \"PaCa-ViT: Learning Patch-to-Cluster Attention in Vision Transformers\", CVPR, 2023 (NC State). [Paper][PyTorch]\n\nGC-ViT: \"Global Context Vision Transformers\", ICML, 2023 (NVIDIA). [Paper][PyTorch]\n\nMAGNETO: \"MAGNETO: A Foundation Transformer\", ICML, 2023 (Microsoft). [Paper]\n\nFcaformer: \"Fcaformer: Forward Cross Attention in Hybrid Vision Transformer\", ICCV, 2023 (Intellifusion, China). [Paper][PyTorch]\n\nSMT: \"Scale-Aware Modulation Meet Transformer\", ICCV, 2023 (Alibaba). [Paper][PyTorch]\n\nFLatten-Transformer: \"FLatten Transformer: Vision Transformer using Focused Linear Attention\", ICCV, 2023 (Tsinghua). [Paper][PyTorch]\n\nPath-Ensemble: \"Revisiting Vision Transformer from the View of Path Ensemble\", ICCV, 2023 (Alibaba). [Paper]\n\nSG-Former: \"SG-Former: Self-guided Transformer with Evolving Token Reallocation\", ICCV, 2023 (NUS). [Paper][PyTorch]\n\nSimPool: \"Keep It SimPool: Who Said Supervised Transformers Suffer from Attention Deficit?\", ICCV, 2023 (National Technical University of Athens). [Paper]\n\nLaPE: \"LaPE: Layer-adaptive Position Embedding for Vision Transformers with Independent Layer Normalization\", ICCV, 2023 (Peking). [Paper][PyTorch]\n\nCB: \"Scratching Visual Transformer's Back with Uniform Attention\", ICCV, 2023 (NAVER). [Paper]\n\nSTL: \"Fully Attentional Networks with Self-emerging Token Labeling\", ICCV, 2023 (NVIDIA). [Paper][PyTorch]\n\nClusterFormer: \"ClusterFormer: Clustering As A Universal Visual Learner\", NeurIPS, 2023 (Rochester Institute of Technology (RIT)). [Paper]\n\nSVT: \"Scattering Vision Transformer: Spectral Mixing Matters\", NeurIPS, 2023 (Microsoft). [Paper][PyTorch][Website]\n\nCrossFormer++: \"CrossFormer++: A Versatile Vision Transformer Hinging on Cross-scale Attention\", arXiv, 2023 (Zhejiang University). [Paper][PyTorch]\n\nQFormer: \"Vision Transformer with Quadrangle Attention\", arXiv, 2023 (The University of Sydney). [Paper][Code (in construction)]\n\nViT-Calibrator: \"ViT-Calibrator: Decision Stream Calibration for Vision Transformer\", arXiv, 2023 (Zhejiang University). [Paper]\n\nSpectFormer: \"SpectFormer: Frequency and Attention is what you need in a Vision Transformer\", arXiv, 2023 (Microsoft). [Paper][PyTorch][Website]\n\nUniNeXt: \"UniNeXt: Exploring A Unified Architecture for Vision Recognition\", arXiv, 2023 (Alibaba). [Paper]\n\nCageViT: \"CageViT: Convolutional Activation Guided Efficient Vision Transformer\", arXiv, 2023 (Southern University of Science and Technology). [Paper]\n\n?: \"Making Vision Transformers Truly Shift-Equivariant\", arXiv, 2023 (UIUC). [Paper]\n\n2-D-SSM: \"2-D SSM: A General Spatial Layer for Visual Transformers\", arXiv, 2023 (Tel Aviv). [Paper][PyTorch]\n\nNaViT: \"Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution\", NeurIPS, 2023 (DeepMind). [Paper]\n\nDAT++: \"DAT++: Spatially Dynamic Vision Transformer with Deformable Attention\", arXiv, 2023 (Tsinghua). [Paper][PyTorch]\n\n?: \"Replacing softmax with ReLU in Vision Transformers\", arXiv, 2023 (DeepMind). [Paper]\n\nRMT: \"RMT: Retentive Networks Meet Vision Transformers\", arXiv, 2023 (CAS). [Paper]\n\nreg: \"Vision Transformers Need Registers\", arXiv, 2023 (Meta). [Paper]\n\nChannelViT: \"Channel Vision Transformers: An Image Is Worth C x 16 x 16 Words\", arXiv, 2023 (Insitro, CA). [Paper]\n\nEViT: \"EViT: An Eagle Vision Transformer with Bi-Fovea Self-Attention\", arXiv, 2023 (Nankai University). [Paper]\n\nViR: \"ViR: Vision Retention Networks\", arXiv, 2023 (NVIDIA). [Paper]\n\nabs-win: \"Window Attention is Bugged: How not to Interpolate Position Embeddings\", arXiv, 2023 (Meta). [Paper]\n\nFMViT: \"FMViT: A multiple-frequency mixing Vision Transformer\", arXiv, 2023 (Alibaba). [Paper][Code (in construction)]\n\nGroupMixFormer: \"Advancing Vision Transformers with Group-Mix Attention\", arXiv, 2023 (HKU). [Paper][PyTorch]\n\nPGT: \"Perceptual Group Tokenizer: Building Perception with Iterative Grouping\", arXiv, 2023 (DeepMind). [Paper]\n\nSCHEME: \"SCHEME: Scalable Channer Mixer for Vision Transformers\", arXiv, 2023 (UCSD). [Paper]\n\nAgent-Attention: \"Agent Attention: On the Integration of Softmax and Linear Attention\", arXiv, 2023 (Tsinghua). [Paper][PyTorch]\n\nViTamin: \"ViTamin: Designing Scalable Vision Models in the Vision-Language Era\", CVPR, 2024 (ByteDance). [Paper][PyTorch]\n\nHIRI-ViT: \"HIRI-ViT: Scaling Vision Transformer with High Resolution Inputs\", TPAMI, 2024 (HiDream.ai, China). [Paper]\n\nSPFormer: \"SPFormer: Enhancing Vision Transformer with Superpixel Representation\", arXiv, 2024 (JHU). [Paper]\n\nmanifold-K: \"A Manifold Representation of the Key in Vision Transformers\", arXiv, 2024 (University of Oslo, Norway). [Paper]\n\nBiXT: \"Perceiving Longer Sequences With Bi-Directional Cross-Attention Transformers\", arXiv, 2024 (University of Melbourne). [Paper]\n\nVisionLLaMA: \"VisionLLaMA: A Unified LLaMA Interface for Vision Tasks\", arXiv, 2024 (Meituan). [Paper][Code (in construction)]\n\nxT: \"xT: Nested Tokenization for Larger Context in Large Images\", arXiv, 2024 (Berkeley). [Paper]\n\nACC-ViT: \"ACC-ViT: Atrous Convolution's Comeback in Vision Transformers\", arXiv, 2024 (Purdue). [Paper]\n\nViTAR: \"ViTAR: Vision Transformer with Any Resolution\", arXiv, 2024 (CAS). [Paper]\n\niLLaMA: \"Adapting LLaMA Decoder to Vision Transformer\", arXiv, 2024 (Shanghai AI Lab). [Paper]\n\nEfficient Vision Transformer\n\nDeiT: \"Training data-efficient image transformers & distillation through attention\", ICML, 2021 (Facebook). [Paper][PyTorch]\n\nConViT: \"ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases\", ICML, 2021 (Facebook). [Paper][Code]\n\n?: \"Improving the Efficiency of Transformers for Resource-Constrained Devices\", DSD, 2021 (NavInfo Europe, Netherlands). [Paper]\n\nPS-ViT: \"Vision Transformer with Progressive Sampling\", ICCV, 2021 (CPII). [Paper]\n\nHVT: \"Scalable Visual Transformers with Hierarchical Pooling\", ICCV, 2021 (Monash University). [Paper][PyTorch]\n\nCrossViT: \"CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification\", ICCV, 2021 (MIT-IBM). [Paper][PyTorch]\n\nViL: \"Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding\", ICCV, 2021 (Microsoft). [Paper][PyTorch]\n\nVisformer: \"Visformer: The Vision-friendly Transformer\", ICCV, 2021 (Beihang University). [Paper][PyTorch]\n\nMultiExitViT: \"Multi-Exit Vision Transformer for Dynamic Inference\", BMVC, 2021 (Aarhus University, Denmark). [Paper][Tensorflow]\n\nSViTE: \"Chasing Sparsity in Vision Transformers: An End-to-End Exploration\", NeurIPS, 2021 (UT Austin). [Paper][PyTorch]\n\nDGE: \"Dynamic Grained Encoder for Vision Transformers\", NeurIPS, 2021 (Megvii). [Paper][PyTorch]\n\nGG-Transformer: \"Glance-and-Gaze Vision Transformer\", NeurIPS, 2021 (JHU). [Paper][Code (in construction)]\n\nDynamicViT: \"DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification\", NeurIPS, 2021 (Tsinghua). [Paper][PyTorch][Website]\n\nResT: \"ResT: An Efficient Transformer for Visual Recognition\", NeurIPS, 2021 (Nanjing University). [Paper][PyTorch]\n\nAdder-Transformer: \"Adder Attention for Vision Transformer\", NeurIPS, 2021 (Huawei). [Paper]\n\nSOFT: \"SOFT: Softmax-free Transformer with Linear Complexity\", NeurIPS, 2021 (Fudan). [Paper][PyTorch][Website]\n\nIA-RED2: \"IA-RED2: Interpretability-Aware Redundancy Reduction for Vision Transformers\", NeurIPS, 2021 (MIT-IBM). [Paper][Website]\n\nLocalViT: \"LocalViT: Bringing Locality to Vision Transformers\", arXiv, 2021 (ETHZ). [Paper][PyTorch]\n\nCCT: \"Escaping the Big Data Paradigm with Compact Transformers\", arXiv, 2021 (University of Oregon). [Paper][PyTorch]\n\nDiversePatch: \"Vision Transformers with Patch Diversification\", arXiv, 2021 (UT Austin + Facebook). [Paper][PyTorch]\n\nSL-ViT: \"Single-Layer Vision Transformers for More Accurate Early Exits with Less Overhead\", arXiv, 2021 (Aarhus University). [Paper]\n\n?: \"Multi-Exit Vision Transformer for Dynamic Inference\", arXiv, 2021 (Aarhus University, Denmark). [Paper]\n\nViX: \"Vision Xformers: Efficient Attention for Image Classification\", arXiv, 2021 (Indian Institute of Technology Bombay). [Paper]\n\nTransformer-LS: \"Long-Short Transformer: Efficient Transformers for Language and Vision\", NeurIPS, 2021 (NVIDIA). [Paper][PyTorch]\n\nWideNet: \"Go Wider Instead of Deeper\", arXiv, 2021 (NUS). [Paper]\n\nArmour: \"Armour: Generalizable Compact Self-Attention for Vision Transformers\", arXiv, 2021 (Arm). [Paper]\n\nIPE: \"Exploring and Improving Mobile Level Vision Transformers\", arXiv, 2021 (CUHK). [Paper]\n\nDS-Net++: \"DS-Net++: Dynamic Weight Slicing for Efficient Inference in CNNs and Transformers\", arXiv, 2021 (Monash University). [Paper][PyTorch]\n\nUFO-ViT: \"UFO-ViT: High Performance Linear Vision Transformer without Softmax\", arXiv, 2021 (Kakao). [Paper]\n\nEvo-ViT: \"Evo-ViT: Slow-Fast Token Evolution for Dynamic Vision Transformer\", AAAI, 2022 (Tencent). [Paper][PyTorch]\n\nPS-Attention: \"Pale Transformer: A General Vision Transformer Backbone with Pale-Shaped Attention\", AAAI, 2022 (Baidu). [Paper][Paddle]\n\nShiftViT: \"When Shift Operation Meets Vision Transformer: An Extremely Simple Alternative to Attention Mechanism\", AAAI, 2022 (Microsoft). [Paper][PyTorch]\n\nEViT: \"Not All Patches are What You Need: Expediting Vision Transformers via Token Reorganizations\", ICLR, 2022 (Tencent). [Paper][PyTorch]\n\nQuadTree: \"QuadTree Attention for Vision Transformers\", ICLR, 2022 (Simon Fraser + Alibaba). [Paper][PyTorch]\n\nAnti-Oversmoothing: \"Anti-Oversmoothing in Deep Vision Transformers via the Fourier Domain Analysis: From Theory to Practice\", ICLR, 2022 (UT Austin). [Paper][PyTorch]\n\nQnA: \"Learned Queries for Efficient Local Attention\", CVPR, 2022 (Tel-Aviv). [Paper][JAX]\n\nLVT: \"Lite Vision Transformer with Enhanced Self-Attention\", CVPR, 2022 (Adobe). [Paper][PyTorch]\n\nA-ViT: \"A-ViT: Adaptive Tokens for Efficient Vision Transformer\", CVPR, 2022 (NVIDIA). [Paper][Website]\n\nPS-ViT: \"Patch Slimming for Efficient Vision Transformers\", CVPR, 2022 (Huawei). [Paper]\n\nRev-MViT: \"Reversible Vision Transformers\", CVPR, 2022 (Meta). [Paper][PyTorch-1][PyTorch-2]\n\nAdaViT: \"AdaViT: Adaptive Vision Transformers for Efficient Image Recognition\", CVPR, 2022 (Fudan). [Paper]\n\nDQS: \"Dynamic Query Selection for Fast Visual Perceiver\", CVPRW, 2022 (Sorbonne Universite', France). [Paper]\n\nATS: \"Adaptive Token Sampling For Efficient Vision Transformers\", ECCV, 2022 (Microsoft). [Paper][Website]\n\nEdgeViT: \"EdgeViTs: Competing Light-weight CNNs on Mobile Devices with Vision Transformers\", ECCV, 2022 (Samsung). [Paper][PyTorch]\n\nSReT: \"Sliced Recursive Transformer\", ECCV, 2022 (CMU + MBZUAI). [Paper][PyTorch]\n\nSiT: \"Self-slimmed Vision Transformer\", ECCV, 2022 (SenseTime). [Paper][PyTorch]\n\nDFvT: \"Doubly-Fused ViT: Fuse Information from Vision Transformer Doubly with Local Representation\", ECCV, 2022 (Alibaba). [Paper]\n\nM3ViT: \"M3ViT: Mixture-of-Experts Vision Transformer for Efficient Multi-task Learning with Model-Accelerator Co-design\", NeurIPS, 2022 (UT Austin). [Paper][PyTorch]\n\nResT-V2: \"ResT V2: Simpler, Faster and Stronger\", NeurIPS, 2022 (Nanjing University). [Paper][PyTorch]\n\nDeiT-Manifold: \"Learning Efficient Vision Transformers via Fine-Grained Manifold Distillation\", NeurIPS, 2022 (Huawei). [Paper]\n\nEfficientFormer: \"EfficientFormer: Vision Transformers at MobileNet Speed\", NeurIPS, 2022 (Snap). [Paper][PyTorch]\n\nGhostNetV2: \"GhostNetV2: Enhance Cheap Operation with Long-Range Attention\", NeurIPS, 2022 (Huawei). [Paper][PyTorch]\n\n?: \"Training a Vision Transformer from scratch in less than 24 hours with 1 GPU\", NeurIPSW, 2022 (Borealis AI, Canada). [Paper]\n\nTerViT: \"TerViT: An Efficient Ternary Vision Transformer\", arXiv, 2022 (Beihang University). [Paper]\n\nMT-ViT: \"Multi-Tailed Vision Transformer for Efficient Inference\", arXiv, 2022 (Wuhan University). [Paper]\n\nViT-P: \"ViT-P: Rethinking Data-efficient Vision Transformers from Locality\", arXiv, 2022 (Chongqing University of Technology). [Paper]\n\nCF-ViT: \"Coarse-to-Fine Vision Transformer\", arXiv, 2022 (Xiamen University + Tencent). [Paper][PyTorch]\n\nEIT: \"EIT: Efficiently Lead Inductive Biases to ViT\", arXiv, 2022 (Academy of Military Sciences, China). [Paper]\n\nSepViT: \"SepViT: Separable Vision Transformer\", arXiv, 2022 (University of Electronic Science and Technology of China). [Paper]\n\nTRT-ViT: \"TRT-ViT: TensorRT-oriented Vision Transformer\", arXiv, 2022 (ByteDance). [Paper]\n\nSuperViT: \"Super Vision Transformer\", arXiv, 2022 (Xiamen University). [Paper][PyTorch]\n\nTutel: \"Tutel: Adaptive Mixture-of-Experts at Scale\", arXiv, 2022 (Microsoft). [Paper][PyTorch]\n\nSimA: \"SimA: Simple Softmax-free Attention for Vision Transformers\", arXiv, 2022 (Maryland + UC Davis). [Paper][PyTorch]\n\nEdgeNeXt: \"EdgeNeXt: Efficiently Amalgamated CNN-Transformer Architecture for Mobile Vision Applications\", arXiv, 2022 (MBZUAI). [Paper][PyTorch]\n\nVVT: \"Vicinity Vision Transformer\", arXiv, 2022 (Australian National University). [Paper][Code (in construction)]\n\nSOFT: \"Softmax-free Linear Transformers\", arXiv, 2022 (Fudan). [Paper][PyTorch]\n\nMaiT: \"MaiT: Leverage Attention Masks for More Efficient Image Transformers\", arXiv, 2022 (Samsung). [Paper]\n\nLightViT: \"LightViT: Towards Light-Weight Convolution-Free Vision Transformers\", arXiv, 2022 (SenseTime). [Paper][Code (in construction)]\n\nNext-ViT: \"Next-ViT: Next Generation Vision Transformer for Efficient Deployment in Realistic Industrial Scenarios\", arXiv, 2022 (ByteDance). [Paper]\n\nXFormer: \"Lightweight Vision Transformer with Cross Feature Attention\", arXiv, 2022 (Samsung). [Paper]\n\nPatchDropout: \"PatchDropout: Economizing Vision Transformers Using Patch Dropout\", arXiv, 2022 (KTH, Sweden). [Paper]\n\nClusTR: \"ClusTR: Exploring Efficient Self-attention via Clustering for Vision Transformers\", arXiv, 2022 (The University of Adelaide, Australia). [Paper]\n\nDiNAT: \"Dilated Neighborhood Attention Transformer\", arXiv, 2022 (University of Oregon). [Paper][PyTorch]\n\nMobileViTv3: \"MobileViTv3: Mobile-Friendly Vision Transformer with Simple and Effective Fusion of Local, Global and Input Features\", arXiv, 2022 (Micron). [Paper][PyTorch]\n\nViT-LSLA: \"ViT-LSLA: Vision Transformer with Light Self-Limited-Attention\", arXiv, 2022 (Southwest University). [Paper]\n\nToken-Pooling: \"Token Pooling in Vision Transformers for Image Classification\", WACV, 2023 (Apple). [Paper]\n\nTri-Level: \"Peeling the Onion: Hierarchical Reduction of Data Redundancy for Efficient Vision Transformer Training\", AAAI, 2023 (Northeastern University). [Paper][Code (in construction)]\n\nViTCoD: \"ViTCoD: Vision Transformer Acceleration via Dedicated Algorithm and Accelerator Co-Design\", IEEE International Symposium on High-Performance Computer Architecture (HPCA), 2023 (Georgia Tech). [Paper]\n\nViTALiTy: \"ViTALiTy: Unifying Low-rank and Sparse Approximation for Vision Transformer Acceleration with a Linear Taylor Attention\", IEEE International Symposium on High-Performance Computer Architecture (HPCA), 2023 (Rice University). [Paper]\n\nHeatViT: \"HeatViT: Hardware-Efficient Adaptive Token Pruning for Vision Transformers\", IEEE International Symposium on High-Performance Computer Architecture (HPCA), 2023 (Northeastern University). [Paper]\n\nToMe: \"Token Merging: Your ViT But Faster\", ICLR, 2023 (Meta). [Paper][PyTorch]\n\nHiViT: \"HiViT: A Simpler and More Efficient Design of Hierarchical Vision Transformer\", ICLR, 2023 (CAS). [Paper][PyTorch]\n\nSTViT: \"Making Vision Transformers Efficient from A Token Sparsification View\", CVPR, 2023 (Alibaba). [Paper][PyTorch]\n\nSparseViT: \"SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer\", CVPR, 2023 (MIT). [Paper][Website]\n\nSlide-Transformer: \"Slide-Transformer: Hierarchical Vision Transformer with Local Self-Attention\", CVPR, 2023 (Tsinghua University). [Paper][Code (in construction)]\n\nRIFormer: \"RIFormer: Keep Your Vision Backbone Effective While Removing Token Mixer\", CVPR, 2023 (Shanghai AI Lab). [Paper][PyTorch][Website]\n\nEfficientViT: \"EfficientViT: Memory Efficient Vision Transformer with Cascaded Group Attention\", CVPR, 2023 (Microsoft). [Paper][PyTorch]\n\nCastling-ViT: \"Castling-ViT: Compressing Self-Attention via Switching Towards Linear-Angular Attention During Vision Transformer Inference\", CVPR, 2023 (Meta). [Paper]\n\nViT-Ti: \"RGB no more: Minimally-decoded JPEG Vision Transformers\", CVPR, 2023 (UMich). [Paper]\n\nSparsifiner: \"Sparsifiner: Learning Sparse Instance-Dependent Attention for Efficient Vision Transformers\", CVPR, 2023 (University of Toronto). [Paper]\n\n?: \"Beyond Attentive Tokens: Incorporating Token Importance and Diversity for Efficient Vision Transformers\", CVPR, 2023 (Baidu). [Paper]\n\nLTMP: \"Learned Thresholds Token Merging and Pruning for Vision Transformers\", ICMLW, 2023 (Ghent University, Belgium). [Paper][PyTorch][Website]\n\nReViT: \"Make A Long Image Short: Adaptive Token Length for Vision Transformers\", ECML PKDD, 2023 (Midea Grou, China). [Paper]\n\nEfficientViT: \"EfficientViT: Enhanced Linear Attention for High-Resolution Low-Computation Visual Recognition\", ICCV, 2023 (MIT). [Paper][PyTorch]\n\nMPCViT: \"MPCViT: Searching for Accurate and Efficient MPC-Friendly Vision Transformer with Heterogeneous Attention\", ICCV, 2023 (Peking). [Paper][PyTorch]\n\nMST: \"Masked Spiking Transformer\", ICCV, 2023 (HKUST). [Paper]\n\nEfficientFormerV2: \"Rethinking Vision Transformers for MobileNet Size and Speed\", ICCV, 2023 (Snap). [Paper][PyTorch]\n\nDiffRate: \"DiffRate: Differentiable Compression Rate for Efficient Vision Transformers\", ICCV, 2023 (Shanghai AI Lab). [Paper][PyTorch]\n\nElasticViT: \"ElasticViT: Conflict-aware Supernet Training for Deploying Fast Vision Transformer on Diverse Mobile Devices\", ICCV, 2023 (Microsoft). [Paper]\n\nFastViT: \"FastViT: A Fast Hybrid Vision Transformer using Structural Reparameterization\", ICCV, 2023 (Apple). [Paper][PyTorch]\n\nSeiT: \"SeiT: Storage-Efficient Vision Training with Tokens Using 1% of Pixel Storage\", ICCV, 2023 (NAVER). [Paper][PyTorch]\n\nTokenReduction: \"Which Tokens to Use? Investigating Token Reduction in Vision Transformers\", ICCVW, 2023 (Aalborg University, Denmark). [Paper][PyTorch][Website]\n\nLGViT: \"LGViT: Dynamic Early Exiting for Accelerating Vision Transformer\", ACMMM, 2023 (Beijing Institute of Technology). [Paper]\n\nLBP-WHT: \"Efficient Low-rank Backpropagation for Vision Transformer Adaptation\", NeurIPS, 2023 (UT Austin). [Paper]\n\nFAT: \"Lightweight Vision Transformer with Bidirectional Interaction\", NeurIPS, 2023 (CAS). [Paper][PyTorch]\n\nMCUFormer: \"MCUFormer: Deploying Vision Transformers on Microcontrollers with Limited Memory\", NeurIPS, 2023 (Tsinghua). [Paper][PyTorch]\n\nSoViT: \"Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design\", NeurIPS, 2023 (DeepMind). [Paper]\n\nCloFormer: \"Rethinking Local Perception in Lightweight Vision Transformer\", arXiv, 2023 (CAS). [Paper]\n\nQuadformer: \"Vision Transformers with Mixed-Resolution Tokenization\", arXiv, 2023 (Tel Aviv). [Paper][Code (in construction)]\n\nSparseFormer: \"SparseFormer: Sparse Visual Recognition via Limited Latent Tokens\", arXiv, 2023 (NUS). [Paper][Code (in construction)]\n\nEMO: \"Rethinking Mobile Block for Efficient Attention-based Models\", arXiv, 2023 (Tencent). [Paper][PyTorch]\n\nByteFormer: \"Bytes Are All You Need: Transformers Operating Directly On File Bytes\", arXiv, 2023 (Apple). [Paper]\n\n?: \"Muti-Scale And Token Mergence: Make Your ViT More Efficient\", arXiv, 2023 (Jilin University). [Paper]\n\nFasterViT: \"FasterViT: Fast Vision Transformers with Hierarchical Attention\", arXiv, 2023 (NVIDIA). [Paper]\n\nNextViT: \"Vision Transformer with Attention Map Hallucination and FFN Compaction\", arXiv, 2023 (Baidu). [Paper]\n\nSkipAt: \"Skip-Attention: Improving Vision Transformers by Paying Less Attention\", arXiv, 2023 (Qualcomm). [Paper]\n\nMSViT: \"MSViT: Dynamic Mixed-Scale Tokenization for Vision Transformers\", arXiv, 2023 (Qualcomm). [Paper]\n\nDiT: \"DiT: Efficient Vision Transformers with Dynamic Token Routing\", arXiv, 2023 (Meituan). [Paper][Code (in construction)]\n\n?: \"Which Transformer to Favor: A Comparative Analysis of Efficiency in Vision Transformers\", arXiv, 2023 (German Research Center for Artificial Intelligence (DFKI)). [Paper][PyTorch]\n\nMobile-V-MoEs: \"Mobile V-MoEs: Scaling Down Vision Transformers via Sparse Mixture-of-Experts\", arXiv, 2023 (Apple). [Paper]\n\nPPT: \"PPT: Token Pruning and Pooling for Efficient Vision Transformers\", arXiv, 2023 (Huawei). [Paper]\n\nMatFormer: \"MatFormer: Nested Transformer for Elastic Inference\", arXiv, 2023 (Google). [Paper]\n\nSparseFormer: \"Bootstrapping SparseFormers from Vision Foundation Models\", arXiv, 2023 (NUS). [Paper][PyTorch]\n\nGTP-ViT: \"GTP-ViT: Efficient Vision Transformers via Graph-based Token Propagation\", WACV, 2024 (CSIRO Data61, Australia). [Paper][PyTorch]\n\nToFu: \"Token Fusion: Bridging the Gap between Token Pruning and Token Merging\", WACV, 2024 (Samsung). [Paper]\n\nCached-Transformer: \"Cached Transformers: Improving Transformers with Differentiable Memory Cache\", AAAI, 2024 (CUHK). [Paper]\n\nLF-ViT: \"LF-ViT: Reducing Spatial Redundancy in Vision Transformer for Efficient Image Recognition\", AAAI, 2024 (Harbin Institute of Technology). [Paper][PyTorch]\n\nEfficientMod: \"Efficient Modulation for Vision Networks\", ICLR, 2024 (Microsoft). [Paper][PyTorch]\n\nNOSE: \"MLP Can Be A Good Transformer Learner\", CVPR, 2024 (MBZUAI). [Paper][PyTorch]\n\nSLAB: \"SLAB: Efficient Transformers with Simplified Linear Attention and Progressive Re-parameterized Batch Normalization\", ICML, 2024 (Huawei). [Paper][PyTorch]\n\nS2: \"When Do We Not Need Larger Vision Models?\", arXiv, 2024 (Berkeley). [Paper][PyTorch]\n\nConv + Transformer\n\nLeViT: \"LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference\", ICCV, 2021 (Facebook). [Paper][PyTorch]\n\nCeiT: \"Incorporating Convolution Designs into Visual Transformers\", ICCV, 2021 (SenseTime). [Paper][PyTorch (rishikksh20)]\n\nConformer: \"Conformer: Local Features Coupling Global Representations for Visual Recognition\", ICCV, 2021 (CAS). [Paper][PyTorch]\n\nCoaT: \"Co-Scale Conv-Attentional Image Transformers\", ICCV, 2021 (UCSD). [Paper][PyTorch]\n\nCvT: \"CvT: Introducing Convolutions to Vision Transformers\", ICCV, 2021 (Microsoft). [Paper][Code]\n\nViTc: \"Early Convolutions Help Transformers See Better\", NeurIPS, 2021 (Facebook). [Paper]\n\nConTNet: \"ConTNet: Why not use convolution and transformer at the same time?\", arXiv, 2021 (ByteDance). [Paper][PyTorch]\n\nSPACH: \"A Battle of Network Structures: An Empirical Study of CNN, Transformer, and MLP\", arXiv, 2021 (Microsoft). [Paper]\n\nMobileViT: \"MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer\", ICLR, 2022 (Apple). [Paper][PyTorch]\n\nCMT: \"CMT: Convolutional Neural Networks Meet Vision Transformers\", CVPR, 2022 (Huawei). [Paper]\n\nMobile-Former: \"Mobile-Former: Bridging MobileNet and Transformer\", CVPR, 2022 (Microsoft). [Paper][PyTorch (in construction)]\n\nTinyViT: \"TinyViT: Fast Pretraining Distillation for Small Vision Transformers\", ECCV, 2022 (Microsoft). [Paper][PyTorch]\n\nCETNet: \"Convolutional Embedding Makes Hierarchical Vision Transformer Stronger\", ECCV, 2022 (OPPO). [Paper]\n\nParC-Net: \"ParC-Net: Position Aware Circular Convolution with Merits from ConvNets and Transformer\", ECCV, 2022 (Intellifusion, China). [Paper][PyTorch]\n\n?: \"How to Train Vision Transformer on Small-scale Datasets?\", BMVC, 2022 (MBZUAI). [Paper][PyTorch]\n\nDHVT: \"Bridging the Gap Between Vision Transformers and Convolutional Neural Networks on Small Datasets\", NeurIPS, 2022 (USTC). [Paper][Code (in construction)]\n\niFormer: \"Inception Transformer\", NeurIPS, 2022 (Sea AI Lab). [Paper][PyTorch]\n\nDenseDCT: \"Explicitly Increasing Input Information Density for Vision Transformers on Small Datasets\", NeurIPSW, 2022 (University of Kansas). [Paper]\n\nCXV: \"Convolutional Xformers for Vision\", arXiv, 2022 (IIT Bombay). [Paper][PyTorch]\n\nConvMixer: \"Patches Are All You Need?\", arXiv, 2022 (CMU). [Paper][PyTorch]\n\nMobileViTv2: \"Separable Self-attention for Mobile Vision Transformers\", arXiv, 2022 (Apple). [Paper][PyTorch]\n\nUniFormer: \"UniFormer: Unifying Convolution and Self-attention for Visual Recognition\", arXiv, 2022 (SenseTime). [Paper][PyTorch]\n\nEdgeFormer: \"EdgeFormer: Improving Light-weight ConvNets by Learning from Vision Transformers\", arXiv, 2022 (?). [Paper]\n\nMoCoViT: \"MoCoViT: Mobile Convolutional Vision Transformer\", arXiv, 2022 (ByteDance). [Paper]\n\nDynamicViT: \"Dynamic Spatial Sparsification for Efficient Vision Transformers and Convolutional Neural Networks\", arXiv, 2022 (Tsinghua University). [Paper][PyTorch]\n\nConvFormer: \"ConvFormer: Closing the Gap Between CNN and Vision Transformers\", arXiv, 2022 (National University of Defense Technology, China). [Paper]\n\nFast-ParC: \"Fast-ParC: Position Aware Global Kernel for ConvNets and ViTs\", arXiv, 2022 (Intellifusion, China). [Paper]\n\nMetaFormer: \"MetaFormer Baselines for Vision\", arXiv, 2022 (Sea AI Lab). [Paper][PyTorch]\n\nSTM: \"Demystify Transformers & Convolutions in Modern Image Deep Networks\", arXiv, 2022 (Tsinghua University). [Paper][Code (in construction)]\n\nParCNetV2: \"ParCNetV2: Oversized Kernel with Enhanced Attention\", arXiv, 2022 (Intellifusion, China). [Paper]\n\nVAN: \"Visual Attention Network\", arXiv, 2022 (Tsinghua). [Paper][PyTorch]\n\nSD-MAE: \"Masked autoencoders is an effective solution to transformer data-hungry\", arXiv, 2022 (Hangzhou Dianzi University). [Paper][PyTorch (in construction)]\n\nSATA: \"Accumulated Trivial Attention Matters in Vision Transformers on Small Datasets\", WACV, 2023 (University of Kansas). [Paper][PyTorch (in construction)]\n\nSparK: \"Sparse and Hierarchical Masked Modeling for Convolutional Representation Learning\", ICLR, 2023 (Bytedance). [Paper][PyTorch]\n\nMOAT: \"MOAT: Alternating Mobile Convolution and Attention Brings Strong Vision Models\", ICLR, 2023 (Google). [Paper][Tensorflow]\n\nInternImage: \"InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions\", CVPR, 2023 (Shanghai AI Laboratory). [Paper][PyTorch]\n\nSwiftFormer: \"SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications\", ICCV, 2023 (MBZUAI). [Paper][PyTorch]\n\nSCSC: \"SCSC: Spatial Cross-scale Convolution Module to Strengthen both CNNs and Transformers\", ICCVW, 2023 (Megvii). [Paper]\n\nPSLT: \"PSLT: A Light-weight Vision Transformer with Ladder Self-Attention and Progressive Shift\", TPAMI, 2023 (Sun Yat-sen University). [Paper][Website]\n\nRepViT: \"RepViT: Revisiting Mobile CNN From ViT Perspective\", arXiv, 2023 (Tsinghua). [Paper][PyTorch]\n\n?: \"Interpret Vision Transformers as ConvNets with Dynamic Convolutions\", arXiv, 2023 (NTU, Singapore). [Paper]\n\nUPDP: \"UPDP: A Unified Progressive Depth Pruner for CNN and Vision Transformer\", AAAI, 2024 (AMD). [Paper]\n\nTraining + Transformer\n\niGPT: \"Generative Pretraining From Pixels\", ICML, 2020 (OpenAI). [Paper][Tensorflow]\n\nCLIP: \"Learning Transferable Visual Models From Natural Language Supervision\", ICML, 2021 (OpenAI). [Paper][PyTorch]\n\nMoCo-V3: \"An Empirical Study of Training Self-Supervised Vision Transformers\", ICCV, 2021 (Facebook). [Paper]\n\nDINO: \"Emerging Properties in Self-Supervised Vision Transformers\", ICCV, 2021 (Facebook). [Paper][PyTorch]\n\ndrloc: \"Efficient Training of Visual Transformers with Small Datasets\", NeurIPS, 2021 (University of Trento). [Paper][PyTorch]\n\nCARE: \"Revitalizing CNN Attentions via Transformers in Self-Supervised Visual Representation Learning\", NeurIPS, 2021 (Tencent). [Paper][PyTorch]\n\nMST: \"MST: Masked Self-Supervised Transformer for Visual Representation\", NeurIPS, 2021 (SenseTime). [Paper]\n\nSiT: \"SiT: Self-supervised Vision Transformer\", arXiv, 2021 (University of Surrey). [Paper][PyTorch]\n\nMoBY: \"Self-Supervised Learning with Swin Transformers\", arXiv, 2021 (Microsoft). [Paper][PyTorch]\n\n?: \"Investigating Transfer Learning Capabilities of Vision Transformers and CNNs by Fine-Tuning a Single Trainable Block\", arXiv, 2021 (Pune Institute of Computer Technology, India). [Paper]\n\nAnnotations-1.3B: \"Billion-Scale Pretraining with Vision Transformers for Multi-Task Visual Representations\", WACV, 2022 (Pinterest). [Paper]\n\nBEiT: \"BEiT: BERT Pre-Training of Image Transformers\", ICLR, 2022 (Microsoft). [Paper][PyTorch]\n\nEsViT: \"Efficient Self-supervised Vision Transformers for Representation Learning\", ICLR, 2022 (Microsoft). [Paper]\n\niBOT: \"Image BERT Pre-training with Online Tokenizer\", ICLR, 2022 (ByteDance). [Paper][PyTorch]\n\nMaskFeat: \"Masked Feature Prediction for Self-Supervised Visual Pre-Training\", CVPR, 2022 (Facebook). [Paper]\n\nAutoProg: \"Automated Progressive Learning for Efficient Training of Vision Transformers\", CVPR, 2022 (Monash University, Australia). [Paper][Code (in construction)]\n\nMAE: \"Masked Autoencoders Are Scalable Vision Learners\", CVPR, 2022 (Facebook). [Paper][PyTorch][PyTorch (pengzhiliang)]\n\nSimMIM: \"SimMIM: A Simple Framework for Masked Image Modeling\", CVPR, 2022 (Microsoft). [Paper][PyTorch]\n\nSelfPatch: \"Patch-Level Representation Learning for Self-Supervised Vision Transformers\", CVPR, 2022 (KAIST). [Paper][PyTorch]\n\nBootstrapping-ViTs: \"Bootstrapping ViTs: Towards Liberating Vision Transformers from Pre-training\", CVPR, 2022 (Zhejiang University). [Paper][PyTorch]\n\nTransMix: \"TransMix: Attend to Mix for Vision Transformers\", CVPR, 2022 (JHU). [Paper][PyTorch]\n\nPatchRot: \"PatchRot: A Self-Supervised Technique for Training Vision Transformers\", CVPRW, 2022 (Arizona State). [Paper]\n\nSplitMask: \"Are Large-scale Datasets Necessary for Self-Supervised Pre-training?\", CVPRW, 2022 (Meta). [Paper]\n\nMC-SSL: \"MC-SSL: Towards Multi-Concept Self-Supervised Learning\", CVPRW, 2022 (University of Surrey, UK). [Paper]\n\nRelViT: \"Where are my Neighbors? Exploiting Patches Relations in Self-Supervised Vision Transformer\", CVPRW, 2022 (University of Padova, Italy). [Paper]\n\ndata2vec: \"data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language\", ICML, 2022 (Meta). [Paper][PyTorch]\n\nSSTA: \"Self-supervised Models are Good Teaching Assistants for Vision Transformers\", ICML, 2022 (Tencent). [Paper][Code (in construction)]\n\nMP3: \"Position Prediction as an Effective Pretraining Strategy\", ICML, 2022 (Apple). [Paper][PyTorch]\n\nCutMixSL: \"Visual Transformer Meets CutMix for Improved Accuracy, Communication Efficiency, and Data Privacy in Split Learning\", IJCAI, 2022 (Yonsei University, Korea). [Paper]\n\nBootMAE: \"Bootstrapped Masked Autoencoders for Vision BERT Pretraining\", ECCV, 2022 (Microsoft). [Paper][PyTorch]\n\nTokenMix: \"TokenMix: Rethinking Image Mixing for Data Augmentation in Vision Transformers\", ECCV, 2022 (CUHK). [Paper][PyTorch]\n\n?: \"Locality Guidance for Improving Vision Transformers on Tiny Datasets\", ECCV, 2022 (Peking University). [Paper][PyTorch]\n\nHAT: \"Improving Vision Transformers by Revisiting High-frequency Components\", ECCV, 2022 (Tsinghua). [Paper][PyTorch]\n\nIDMM: \"Training Vision Transformers with Only 2040 Images\", ECCV, 2022 (Nanjing University). [Paper]\n\nAttMask: \"What to Hide from Your Students: Attention-Guided Masked Image Modeling\", ECCV, 2022 (National Technical University of Athens). [Paper][PyTorch]\n\nSLIP: \"SLIP: Self-supervision meets Language-Image Pre-training\", ECCV, 2022 (Berkeley + Meta). [Paper][Pytorch]\n\nmc-BEiT: \"mc-BEiT: Multi-Choice Discretization for Image BERT Pre-training\", ECCV, 2022 (Peking University). [Paper]\n\nSL2O: \"Scalable Learning to Optimize: A Learned Optimizer Can Train Big Models\", ECCV, 2022 (UT Austin). [Paper][PyTorch]\n\nTokenMixup: \"TokenMixup: Efficient Attention-guided Token-level Data Augmentation for Transformers\", NeurIPS, 2022 (Korea University). [Paper][PyTorch]\n\nPatchRot: \"PatchRot: A Self-Supervised Technique for Training Vision Transformers\", NeurIPSW, 2022 (Arizona State University). [Paper]\n\nGreenMIM: \"Green Hierarchical Vision Transformer for Masked Image Modeling\", NeurIPS, 2022 (The University of Tokyo). [Paper][PyTorch]\n\nDP-CutMix: \"Differentially Private CutMix for Split Learning with Vision Transformer\", NeurIPSW, 2022 (Yonsei University). [Paper]\n\n?: \"How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers\", Transactions on Machine Learning Research (TMLR), 2022 (Google). [Paper][Tensorflow][PyTorch (rwightman)]\n\nPeCo: \"PeCo: Perceptual Codebook for BERT Pre-training of Vision Transformers\", arXiv, 2022 (Microsoft). [Paper]\n\nRePre: \"RePre: Improving Self-Supervised Vision Transformer with Reconstructive Pre-training\", arXiv, 2022 (Beijing University of Posts and Telecommunications). [Paper]\n\nBeyond-Masking: \"Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers\", arXiv, 2022 (CAS). [Paper][Code (in construction)]\n\nKronecker-Adaptation: \"Parameter-efficient Fine-tuning for Vision Transformers\", arXiv, 2022 (Microsoft). [Paper]\n\nDILEMMA: \"DILEMMA: Self-Supervised Shape and Texture Learning with Transformers\", arXiv, 2022 (University of Bern, Switzerland). [Paper]\n\nDeiT-III: \"DeiT III: Revenge of the ViT\", arXiv, 2022 (Meta). [Paper]\n\n?: \"Better plain ViT baselines for ImageNet-1k\", arXiv, 2022 (Google). [Paper][Tensorflow]\n\nConvMAE: \"ConvMAE: Masked Convolution Meets Masked Autoencoders\", arXiv, 2022 (Shanghai AI Laboratory). [Paper][PyTorch (in construction)]\n\nUM-MAE: \"Uniform Masking: Enabling MAE Pre-training for Pyramid-based Vision Transformers with Locality\", arXiv, 2022 (Nanjing University of Science and Technology). [Paper][PyTorch]\n\nGMML: \"GMML is All you Need\", arXiv, 2022 (University of Surrey, UK). [Paper][PyTorch]\n\nSIM: \"Siamese Image Modeling for Self-Supervised Vision Representation Learning\", arXiv, 2022 (SenseTime). [Paper]\n\nSupMAE: \"SupMAE: Supervised Masked Autoencoders Are Efficient Vision Learners\", arXiv, 2022 (UT Austin). [Paper][PyTorch]\n\nLoMaR: \"Efficient Self-supervised Vision Pretraining with Local Masked Reconstruction\", arXiv, 2022 (KAUST). [Paper]\n\nSAR: \"Spatial Entropy Regularization for Vision Transformers\", arXiv, 2022 (University of Trento, Italy). [Paper]\n\nExtreMA: \"Extreme Masking for Learning Instance and Distributed Visual Representations\", arXiv, 2022 (Microsoft). [Paper]\n\n?: \"Exploring Feature Self-relation for Self-supervised Transformer\", arXiv, 2022 (Nankai University). [Paper]\n\n?: \"Position Labels for Self-Supervised Vision Transformer\", arXiv, 2022 (Southwest Jiaotong University). [Paper]\n\nJigsaw-ViT: \"Jigsaw-ViT: Learning Jigsaw Puzzles in Vision Transformer\", arXiv, 2022 (KU Leuven, Belgium). [Paper][PyTorch][Website]\n\nBEiT-v2: \"BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers\", arXiv, 2022 (Microsoft). [Paper][PyTorch]\n\nMILAN: \"MILAN: Masked Image Pretraining on Language Assisted Representation\", arXiv, 2022 (Princeton). [Paper][PyTorch (in construction)]\n\nPSS: \"Accelerating Vision Transformer Training via a Patch Sampling Schedule\", arXiv, 2022 (Franklin and Marshall College, Pennsylvania). [Paper][PyTorch]\n\ndBOT: \"Exploring Target Representations for Masked Autoencoders\", arXiv, 2022 (ByteDance). [Paper]\n\nPatchErasing: \"Effective Vision Transformer Training: A Data-Centric Perspective\", arXiv, 2022 (Alibaba). [Paper]\n\nSelf-Distillation: \"Self-Distillation for Further Pre-training of Transformers\", arXiv, 2022 (KAIST). [Paper]\n\nAutoView: \"Learning Self-Regularized Adversarial Views for Self-Supervised Vision Transformers\", arXiv, 2022 (Sun Yat-sen University). [Paper][Code (in construction)]\n\nLOCA: \"Location-Aware Self-Supervised Transformers\", arXiv, 2022 (Google). [Paper]\n\nFT-CLIP: \"CLIP Itself is a Strong Fine-tuner: Achieving 85.7% and 88.0% Top-1 Accuracy with ViT-B and ViT-L on ImageNet\", arXiv, 2022 (Microsoft). [Paper][Code (in construction)]\n\nMixPro: \"MixPro: Data Augmentation with MaskMix and Progressive Attention Labeling for Vision Transformer\", ICLR, 2023 (Beijing University of Chemical Technology). [Paper][PyTorch (in construction)]\n\nConMIM: \"Masked Image Modeling with Denoising Contrast\", ICLR, 2023 (Tencent). [Paper][Pytorch]\n\nccMIM: \"Contextual Image Masking Modeling via Synergized Contrasting without View Augmentation for Faster and Better Visual Pretraining\", ICLR, 2023 (Shanghai Jiao Tong). [Paper]\n\nCIM: \"Corrupted Image Modeling for Self-Supervised Visual Pre-Training\", ICLR, 2023 (Microsoft). [Paper]\n\nMFM: \"Masked Frequency Modeling for Self-Supervised Visual Pre-Training\", ICLR, 2023 (NTU, Singapore). [Paper][Website]\n\nMask3D: \"Mask3D: Pre-training 2D Vision Transformers by Learning Masked 3D Priors\", CVPR, 2023 (Meta). [Paper]\n\nVisualAtom: \"Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves\", CVPR, 2023 (National Institute of Advanced Industrial Science and Technology (AIST), Japan). [Paper][PyTorch][Website]\n\nMixedAE: \"Mixed Autoencoder for Self-supervised Visual Representation Learning\", CVPR, 2023 (Huawei). [Paper]\n\nTBM: \"Token Boosting for Robust Self-Supervised Visual Transformer Pre-training\", CVPR, 2023 (Singapore University of Technology and Design). [Paper]\n\nLGSimCLR: \"Learning Visual Representations via Language-Guided Sampling\", CVPR, 2023 (UMich). [Paper][PyTorch]\n\nDisCo-CLIP: \"DisCo-CLIP: A Distributed Contrastive Loss for Memory Efficient CLIP Training\", CVPR, 2023 (IDEA). [Paper][PyTorch (in construction)]\n\nMaskCLIP: \"MaskCLIP: Masked Self-Distillation Advances Contrastive Language-Image Pretraining\", CVPR, 2023 (Microsoft). [Paper][Code (in construction)]\n\nMAGE: \"MAGE: MAsked Generative Encoder to Unify Representation Learning and Image Synthesis\", CVPR, 2023 (Google). [Paper][PyTorch]\n\nMixMIM: \"MixMIM: Mixed and Masked Image Modeling for Efficient Visual Representation Learning\", CVPR, 2023 (SenseTime). [Paper][PyTorch]\n\niTPN: \"Integrally Pre-Trained Transformer Pyramid Networks\", CVPR, 2023 (CAS). [Paper][PyTorch]\n\nDropKey: \"DropKey for Vision Transformer\", CVPR, 2023 (Meitu). [Paper]\n\nFlexiViT: \"FlexiViT: One Model for All Patch Sizes\", CVPR, 2023 (Google). [Paper][Tensorflow]\n\nRA-CLIP: \"RA-CLIP: Retrieval Augmented Contrastive Language-Image Pre-Training\", CVPR, 2023 (Alibaba). [Paper]\n\nCLIPPO: \"CLIPPO: Image-and-Language Understanding from Pixels Only\", CVPR, 2023 (Google). [Paper][JAX]\n\nDMAE: \"Masked Autoencoders Enable Efficient Knowledge Distillers\", CVPR, 2023 (JHU + UC Santa Cruz). [Paper][PyTorch]\n\nHPM: \"Hard Patches Mining for Masked Image Modeling\", CVPR, 2023 (CAS). [Paper][PyTorch]\n\nLocalMIM: \"Masked Image Modeling with Local Multi-Scale Reconstruction\", CVPR, 2023 (Peking University). [Paper]\n\nMaskAlign: \"Stare at What You See: Masked Image Modeling without Reconstruction\", CVPR, 2023 (Shanghai AI Lab). [Paper][PyTorch]\n\nRILS: \"RILS: Masked Visual Reconstruction in Language Semantic Space\", CVPR, 2023 (Tencent). [Paper][Code (in construction)]\n\nRelaxMIM: \"Understanding Masked Image Modeling via Learning Occlusion Invariant Feature\", CVPR, 2023 (Megvii). [Paper]\n\nFDT: \"Revisiting Multimodal Representation in Contrastive Learning: From Patch and Token Embeddings to Finite Discrete Tokens\", CVPR, 2023 (ByteDance). [Paper][Code (in construction)]\n\n?: \"Prefix Conditioning Unifies Language and Label Supervision\", CVPR, 2023 (Google). [Paper]\n\nOpenCLIP: \"Reproducible scaling laws for contrastive language-image learning\", CVPR, 2023 (LAION). [Paper][PyTorch]\n\nDiHT: \"Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training\", CVPR, 2023 (Meta). [Paper][PyTorch]\n\nM3I-Pretraining: \"Towards All-in-one Pre-training via Maximizing Multi-modal Mutual Information\", CVPR, 2023 (Shanghai AI Lab). [Paper][Code (in construction)]\n\nSN-Net: \"Stitchable Neural Networks\", CVPR, 2023 (Monash University). [Paper][PyTorch]\n\nMAE-Lite: \"A Closer Look at Self-supervised Lightweight Vision Transformers\", ICML, 2023 (Megvii). [Paper][PyTorch]\n\nViT-22B: \"Scaling Vision Transformers to 22 Billion Parameters\", ICML, 2023 (Google). [Paper]\n\nGHN-3: \"Can We Scale Transformers to Predict Parameters of Diverse ImageNet Models?\", ICML, 2023 (Samsung). [Paper][PyTorch]\n\nA2MIM: \"Architecture-Agnostic Masked Image Modeling - From ViT back to CNN\", ICML, 2023 (Westlake University, China). [Paper][PyTorch]\n\nPQCL: \"Patch-level Contrastive Learning via Positional Query for Visual Pre-training\", ICML, 2023 (Alibaba). [Paper][PyTorch]\n\nDreamTeacher: \"DreamTeacher: Pretraining Image Backbones with Deep Generative Models\", ICCV, 2023 (NIVIDA). [Paper][Website]\n\nOFDB: \"Pre-training Vision Transformers with Very Limited Synthesized Images\", ICCV, 2023 (National Institute of Advanced Industrial Science and Technology (AIST), Japan). [Paper][PyTorch]\n\nMFF: \"Improving Pixel-based MIM by Reducing Wasted Modeling Capability\", ICCV, 2023 (Shanghai AI Lab). [Paper][PyTorch]\n\nTL-Align: \"Token-Label Alignment for Vision Transformers\", ICCV, 2023 (Tsinghua University). [Paper][PyTorch]\n\nSMMix: \"SMMix: Self-Motivated Image Mixing for Vision Transformers\", ICCV, 2023 (Xiamen University). [Paper][PyTorch]\n\nDiffMAE: \"Diffusion Models as Masked Autoencoders\", ICCV, 2023 (Meta). [Paper][Website]\n\nMAWS: \"The effectiveness of MAE pre-pretraining for billion-scale pretraining\", ICCV, 2023 (Meta). [Paper][PyTorch]\n\nCountBench: \"Teaching CLIP to Count to Ten\", ICCV, 2023 (Google). [Paper]\n\nCLIPpy: \"Perceptual Grouping in Vision-Language Models\", ICCV, 2023 (Apple). [Paper]\n\nCiT: \"CiT: Curation in Training for Effective Vision-Language Data\", ICCV, 2023 (Meta). [Paper][PyTorch]\n\nI-JEPA: \"Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture\", ICCV, 2023 (Meta). [Paper]\n\nEfficientTrain: \"EfficientTrain: Exploring Generalized Curriculum Learning for Training Visual Backbones\", ICCV, 2023 (Tsinghua). [Paper][PyTorch]\n\nStableRep: \"StableRep: Synthetic Images from Text-to-Image Models Make Strong Visual Representation Learners\", NeurIPS, 2023 (Google). [Paper][PyTorch]\n\nLaCLIP: \"Improving CLIP Training with Language Rewrites\", NeurIPS, 2023 (Google). [Paper][PyTorch]\n\nDesCo: \"DesCo: Learning Object Recognition with Rich Language Descriptions\", NeurIPS, 2023 (UCLA). [Paper]\n\n?: \"Stable and low-precision training for large-scale vision-language models\", NeurIPS, 2023 (UW). [Paper]\n\nCapPa: \"Image Captioners Are Scalable Vision Learners Too\", NeurIPS, 2023 (DeepMind). [Paper][JAX]\n\nIV-CL: \"Does Visual Pretraining Help End-to-End Reasoning?\", NeurIPS, 2023 (Google). [Paper]\n\nCLIPA: \"An Inverse Scaling Law for CLIP Training\", NeurIPS, 2023 (UC Santa Cruz). [Paper][PyTorch]\n\nHummingbird: \"Towards In-context Scene Understanding\", NeurIPS, 2023 (DeepMind). [Paper]\n\nRevColV2: \"RevColV2: Exploring Disentangled Representations in Masked Image Modeling\", NeurIPS, 2023 (Megvii). [Paper][PyTorch]\n\nALIA: \"Diversify Your Vision Datasets with Automatic Diffusion-Based Augmentation\", NeurIPS, 2023 (Berkeley). [Paper][PyTorch]\n\n?: \"Improving Multimodal Datasets with Image Captioning\", NeurIPS (Datasets and Benchmarks), 2023 (UW). [Paper]\n\nCCViT: \"Centroid-centered Modeling for Efficient Vision Transformer Pre-training\", arXiv, 2023 (Wuhan University). [Paper]\n\nSoftCLIP: \"SoftCLIP: Softer Cross-modal Alignment Makes CLIP Stronger\", arXiv, 2023 (Tencent). [Paper]\n\nRECLIP: \"RECLIP: Resource-efficient CLIP by Training with Small Images\", arXiv, 2023 (Google). [Paper]\n\nDINOv2: \"DINOv2: Learning Robust Visual Features without Supervision\", arXiv, 2023 (Meta). [Paper]\n\n?: \"Objectives Matter: Understanding the Impact of Self-Supervised Objectives on Vision Transformer Representations\", arXiv, 2023 (Meta). [Paper]\n\nFilter: \"Less is More: Removing Text-regions Improves CLIP Training Efficiency and Robustness\", arXiv, 2023 (Apple). [Paper]\n\n?: \"Improved baselines for vision-language pre-training\", arXiv, 2023 (Meta). [Paper]\n\n3T: \"Three Towers: Flexible Contrastive Learning with Pretrained Image Models\", arXiv, 2023 (Google). [Paper]\n\nADDP: \"ADDP: Learning General Representations for Image Recognition and Generation with Alternating Denoising Diffusion Process\", arXiv, 2023 (CUHK + Tsinghua). [Paper]\n\nMOFI: \"MOFI: Learning Image Representations from Noisy Entity Annotated Images\", arXiv, 2023 (Apple). [Paper]\n\nMaPeT: \"Learning to Mask and Permute Visual Tokens for Vision Transformer Pre-Training\", arXiv, 2023 (UniMoRE, Italy). [Paper][PyTorch]\n\nRECO: \"Retrieval-Enhanced Contrastive Vision-Text Models\", arXiv, 2023 (Google). [Paper]\n\nCLIPA-v2: \"CLIPA-v2: Scaling CLIP Training with 81.1% Zero-shot ImageNet Accuracy within a $10,000 Budget; An Extra $4,000 Unlocks 81.8% Accuracy\", arXiv, 2023 (UC Santa Cruz). [Paper][PyTorch]\n\nPatchMixing: \"Hardwiring ViT Patch Selectivity into CNNs using Patch Mixing\", arXiv, 2023 (Boston). [Paper][Website]\n\nSN-Netv2: \"Stitched ViTs are Flexible Vision Backbones\", arXiv, 2023 (Monash University). [Paper][PyTorch (in construction)]\n\nCLIP-GPT: \"Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts\", arXiv, 2023 (Dublin City University, Ireland). [Paper]\n\nFlexPredict: \"Predicting masked tokens in stochastic locations improves masked image modeling\", arXiv, 2023 (Meta). [Paper]\n\nSoft-MoE: \"From Sparse to Soft Mixtures of Experts\", arXiv, 2023 (DeepMind). [Paper]\n\nDropPos: \"DropPos: Pre-Training Vision Transformers by Reconstructing Dropped Positions\", NeurIPS, 2023 (CAS). [Paper][PyTorch]\n\nMIRL: \"Masked Image Residual Learning for Scaling Deeper Vision Transformers\", NeurIPS, 2023 (Baidu). [Paper]\n\nCMM: \"Investigating the Limitation of CLIP Models: The Worst-Performing Categories\", arXiv, 2023 (Nanjing University). [Paper]\n\nLC-MAE: \"Longer-range Contextualized Masked Autoencoder\", arXiv, 2023 (NAVER). [Paper]\n\nSILC: \"SILC: Improving Vision Language Pretraining with Self-Distillation\", arXiv, 2023 (ETHZ). [Paper]\n\nCLIPTex: \"CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement\", arXiv, 2023 (Apple). [Paper]\n\nNxTP: \"Object Recognition as Next Token Prediction\", arXiv, 2023 (Meta). [Paper][PyTorch]\n\n?: \"Scaling Laws of Synthetic Images for Model Training ... for Now\", arXiv, 2023 (Google). [Paper][PyTorch]\n\nSynCLR: \"Learning Vision from Models Rivals Learning Vision from Data\", arXiv, 2023 (Google). [Paper][PyTorch]\n\nEWA: \"Experts Weights Averaging: A New General Training Scheme for Vision Transformers\", arXiv, 2023 (Fudan). [Paper]\n\nDTM: \"Masked Image Modeling via Dynamic Token Morphing\", arXiv, 2023 (NAVER). [Paper]\n\nSSAT: \"Limited Data, Unlimited Potential: A Study on ViTs Augmented by Masked Autoencoders\", WACV, 2024 (UNC Charlotte). [Paper][Code (in construction)]\n\nFEC: \"Neural Clustering based Visual Representation Learning\", CVPR, 2024 (Zhejiang). [Paper]\n\nEfficientTrain++: \"EfficientTrain++: Generalized Curriculum Learning for Efficient Visual Backbone Training\", TPAMI, 2024 (Tsinghua). [Paper][PyTorch]\n\nDVT: \"Denoising Vision Transformers\", arXiv, 2024 (USC). [Paper][PyTorch][Website]\n\nAIM: \"Scalable Pre-training of Large Autoregressive Image Models\", arXiv, 2024 (Apple). [Paper][PyTorch]\n\nDDM: \"Deconstructing Denoising Diffusion Models for Self-Supervised Learning\", arXiv, 2024 (Meta). [Paper]\n\nCrossMAE: \"Rethinking Patch Dependence for Masked Autoencoders\", arXiv, 2024 (Berkeley). [Paper][PyTorch][Website]\n\nIWM: \"Learning and Leveraging World Models in Visual Representation Learning\", arXiv, 2024 (Meta). [Paper]\n\n?: \"Can Generative Models Improve Self-Supervised Representation Learning?\", arXiv, 2024 (Vector Institute). [Paper]\n\nRobustness + Transformer\n\nViT-Robustness: \"Understanding Robustness of Transformers for Image Classification\", ICCV, 2021 (Google). [Paper]\n\nSAGA: \"On the Robustness of Vision Transformers to Adversarial Examples\", ICCV, 2021 (University of Connecticut). [Paper]\n\n?: \"Adversarial Robustness Comparison of Vision Transformer and MLP-Mixer to CNNs\", BMVC, 2021 (KAIST). [Paper][PyTorch]\n\nViTs-vs-CNNs: \"Are Transformers More Robust Than CNNs?\", NeurIPS, 2021 (JHU + UC Santa Cruz). [Paper][PyTorch]\n\nT-CNN: \"Transformed CNNs: recasting pre-trained convolutional layers with self-attention\", arXiv, 2021 (Facebook). [Paper]\n\nTransformer-Attack: \"On the Adversarial Robustness of Visual Transformers\", arXiv, 2021 (Xi'an Jiaotong). [Paper]\n\n?: \"Reveal of Vision Transformers Robustness against Adversarial Attacks\", arXiv, 2021 (University of Rennes). [Paper]\n\n?: \"On Improving Adversarial Transferability of Vision Transformers\", arXiv, 2021 (ANU). [Paper][PyTorch]\n\n?: \"Exploring Corruption Robustness: Inductive Biases in Vision Transformers and MLP-Mixers\", arXiv, 2021 (University of Pittsburgh). [Paper]\n\nToken-Attack: \"Adversarial Token Attacks on Vision Transformers\", arXiv, 2021 (New York University). [Paper]\n\n?: \"Discrete Representations Strengthen Vision Transformer Robustness\", arXiv, 2021 (Google). [Paper]\n\n?: \"Vision Transformers are Robust Learners\", AAAI, 2022 (PyImageSearch + IBM). [Paper][Tensorflow]\n\nPNA: \"Towards Transferable Adversarial Attacks on Vision Transformers\", AAAI, 2022 (Fudan + Maryland). [Paper][PyTorch]\n\nMIA-Former: \"MIA-Former: Efficient and Robust Vision Transformers via Multi-grained Input-Adaptation\", AAAI, 2022 (Rice University). [Paper]\n\nPatch-Fool: \"Patch-Fool: Are Vision Transformers Always Robust Against Adversarial Perturbations?\", ICLR, 2022 (Rice University). [Paper][PyTorch]\n\nGeneralization-Enhanced-ViT: \"Delving Deep into the Generalization of Vision Transformers under Distribution Shifts\", CVPR, 2022 (Beihang University + NTU, Singapore). [Paper]\n\nECViT: \"Towards Practical Certifiable Patch Defense with Vision Transformer\", CVPR, 2022 (Tencent).[Paper]\n\nAttention-Fool: \"Give Me Your Attention: Dot-Product Attention Considered Harmful for Adversarial Patch Robustness\", CVPR, 2022 (Bosch). [Paper]\n\nMemory-Token: \"Fine-tuning Image Transformers using Learnable Memory\", CVPR, 2022 (Google). [Paper]\n\nAPRIL: \"APRIL: Finding the Achilles' Heel on Privacy for Vision Transformers\", CVPR, 2022 (CAS). [Paper]\n\nSmooth-ViT: \"Certified Patch Robustness via Smoothed Vision Transformers\", CVPR, 2022 (MIT). [Paper][PyTorch]\n\nRVT: \"Towards Robust Vision Transformer\", CVPR, 2022 (Alibaba). [Paper][PyTorch]\n\nPyramid: \"Pyramid Adversarial Training Improves ViT Performance\", CVPR, 2022 (Google). [Paper]\n\nVARS: \"Visual Attention Emerges from Recurrent Sparse Reconstruction\", ICML, 2022 (Berkeley + Microsoft). [Paper][PyTorch]\n\nFAN: \"Understanding The Robustness in Vision Transformers\", ICML, 2022 (NVIDIA). [Paper][PyTorch]\n\nCFA: \"Robustifying Vision Transformer without Retraining from Scratch by Test-Time Class-Conditional Feature Alignment\", IJCAI, 2022 (The University of Tokyo). [Paper][PyTorch]\n\n?: \"Understanding Adversarial Robustness of Vision Transformers via Cauchy Problem\", ECML-PKDD, 2022 (University of Exeter, UK). [Paper][PyTorch]\n\n?: \"An Impartial Take to the CNN vs Transformer Robustness Contest\", ECCV, 2022 (Oxford). [Paper]\n\nAGAT: \"Towards Efficient Adversarial Training on Vision Transformers\", ECCV, 2022 (Zhejiang University). [Paper]\n\n?: \"Are Vision Transformers Robust to Patch Perturbations?\", ECCV, 2022 (TUM). [Paper]\n\nViP: \"ViP: Unified Certified Detection and Recovery for Patch Attack with Vision Transformers\", ECCV, 2022 (UC Santa Cruz). [Paper][PyTorch]\n\n?: \"When Adversarial Training Meets Vision Transformers: Recipes from Training to Architecture\", NeurIPS, 2022 (Peking University). [Paper][PyTorch]\n\nPAR: \"Decision-based Black-box Attack Against Vision Transformers via Patch-wise Adversarial Removal\", NeurIPS, 2022 (Tianjin University). [Paper]\n\nRobustViT: \"Optimizing Relevance Maps of Vision Transformers Improves Robustness\", NeurIPS, 2022 (Tel-Aviv). [Paper][PyTorch]\n\n?: \"Understanding and Improving Robustness of Vision Transformers through Patch-based Negative Augmentation\", NeurIPS, 2022 (Google). [Paper]\n\nNVD: \"Finding Differences Between Transformers and ConvNets Using Counterfactual Simulation Testing\", NeurIPS, 2022 (Boston). [Paper]\n\n?: \"Are Vision Transformers Robust to Spurious Correlations?\", arXiv, 2022 (UW-Madison). [Paper]\n\nMA: \"Boosting Adversarial Transferability of MLP-Mixer\", arXiv, 2022 (Beijing Institute of Technology). [Paper]\n\n?: \"Deeper Insights into ViTs Robustness towards Common Corruptions\", arXiv, 2022 (Fudan + Microsoft). [Paper]\n\n?: \"Privacy-Preserving Image Classification Using Vision Transformer\", arXiv, 2022 (Tokyo Metropolitan University). [Paper]\n\nFedWAvg: \"Federated Adversarial Training with Transformers\", arXiv, 2022 (Institute of Electronics and Digital Technologies (IETR), France). [Paper]\n\nBackdoor-Transformer: \"Backdoor Attacks on Vision Transformers\", arXiv, 2022 (Maryland + UC Davis). [Paper][Code (in construction)]\n\n?: \"Defending Backdoor Attacks on Vision Transformer via Patch Processing\", arXiv, 2022 (Baidu). [Paper]\n\n?: \"Image and Model Transformation with Secret Key for Vision Transformer\", arXiv, 2022 (Tokyo Metropolitan University). [Paper]\n\n?: \"Analyzing Adversarial Robustness of Vision Transformers against Spatial and Spectral Attacks\", arXiv, 2022 (Yonsei University). [Paper]\n\nCLIPping Privacy: \"CLIPping Privacy: Identity Inference Attacks on Multi-Modal Machine Learning Models\", arXiv, 2022 (TUM). [Paper]\n\n?: \"A Light Recipe to Train Robust Vision Transformers\", arXiv, 2022 (EPFL). [Paper]\n\n?: \"Attacking Compressed Vision Transformers\", arXiv, 2022 (NYU). [Paper]\n\nC-AVP: \"Visual Prompting for Adversarial Robustness\", arXiv, 2022 (Michigan State). [Paper]\n\n?: \"Curved Representation Space of Vision Transformers\", arXiv, 2022 (Yonsei University). [Paper]\n\nRKDE: \"Robustify Transformers with Robust Kernel Density Estimation\", arXiv, 2022 (UT Austin). [Paper]\n\nMRAP: \"Pretrained Transformers Do not Always Improve Robustness\", arXiv, 2022 (Arizona State University). [Paper]\n\nmodel-soup: \"Revisiting adapters with adversarial training\", ICLR, 2023 (DeepMind). [Paper]\n\n?: \"Budgeted Training for Vision Transformer\", ICLR, 2023 (Tsinghua). [Paper]\n\nRobustCNN: \"Can CNNs Be More Robust Than Transformers?\", ICLR, 2023 (UC Santa Cruz + JHU). [Paper][PyTorch]\n\nDMAE: \"Denoising Masked AutoEncoders are Certifiable Robust Vision Learners\", ICLR, 2023 (Peking). [Paper][PyTorch]\n\nTGR: \"Transferable Adversarial Attacks on Vision Transformers with Token Gradient Regularization\", CVPR, 2023 (CUHK). [Paper][PyTorch]\n\nTrojViT: \"TrojViT: Trojan Insertion in Vision Transformers\", CVPR, 2023 (Indiana University Bloomington). [Paper]\n\nRSPC: \"Improving Robustness of Vision Transformers by Reducing Sensitivity to Patch Corruptions\", CVPR, 2023 (MPI). [Paper]\n\nTORA-ViT: \"Trade-off between Robustness and Accuracy of Vision Transformers\", CVPR, 2023 (The University of Sydney). [Paper]\n\nBadViT: \"You Are Catching My Attention: Are Vision Transformers Bad Learners Under Backdoor Attacks?\", CVPR, 2023 (Huazhong University of Science and Technology). [Paper]\n\n?: \"Understanding and Defending Patched-based Adversarial Attacks for Vision Transformer\", ICML, 2023 (University of Pittsburgh). [Paper]\n\nRobustMAE: \"Improving Adversarial Robustness of Masked Autoencoders via Test-time Frequency-domain Prompting\", ICCV, 2023 (USTC). [Paper][PyTorch (in construction)]\n\n?: \"Efficiently Robustify Pre-trained Models\", ICCV, 2023 (IIT Roorkee, India). [Paper]\n\n?: \"Transferable Adversarial Attack for Both Vision Transformers and Convolutional Networks via Momentum Integrated Gradients\", ICCV, 2023 (Tsinghua). [Paper]\n\nCleanCLIP: \"CleanCLIP: Mitigating Data Poisoning Attacks in Multimodal Contrastive Learning\", ICCV, 2023 (UCLA). [Paper][PyTorch]\n\nQBBA: \"Exploring Non-additive Randomness on ViT against Query-Based Black-Box Attacks\", BMVC, 2023 (Oxford). [Paper]\n\nRBFormer: \"RBFormer: Improve Adversarial Robustness of Transformer by Robust Bias\", BMVC, 2023 (HKUST). [Paper]\n\nPreLayerNorm: \"Improved Robustness of Vision Transformer via PreLayerNorm in Patch Embedding\", PR, 2023 (POSTECH). [Paper]\n\nCertViT: \"CertViT: Certified Robustness of Pre-Trained Vision Transformers\", arXiv, 2023 (INRIA). [Paper][PyTorch]\n\nRoCLIP: \"Robust Contrastive Language-Image Pretraining against Adversarial Attacks\", arXiv, 2023 (UCLA). [Paper]\n\nDeepMIM: \"DeepMIM: Deep Supervision for Masked Image Modeling\", arXiv, 2023 (Microsoft). [Paper][Code (in construction)]\n\nTAP-ADL: \"Robustifying Token Attention for Vision Transformers\", ICCV, 2023 (MPI). [Paper][PyTorch]\n\nEWA: \"Experts Weights Averaging: A New General Training Scheme for Vision Transformers\", arXiv, 2023 (Fudan). [Paper]\n\nSlowFormer: \"SlowFormer: Universal Adversarial Patch for Attack on Compute and Energy Efficiency of Inference Efficient Vision Transformers\", arXiv, 2023 (UC Davis). [Paper][PyTorch]\n\nDTM: \"Masked Image Modeling via Dynamic Token Morphing\", arXiv, 2023 (NAVER). [Paper]\n\nSWARM: \"Not All Prompts Are Secure: A Switchable Backdoor Attack Against Pre-trained Vision Transformers\", CVPR, 2024 (Zhejiang). [Paper][Code (in construction)]\n\n?: \"Safety of Multimodal Large Language Models on Images and Text\", arXiv, 2024 (Shanghai AI Lab). [Paper]\n\nModel Compression + Transformer\n\nViT-quant: \"Post-Training Quantization for Vision Transformer\", NeurIPS, 2021 (Huawei). [Paper]\n\nVTP: \"Visual Transformer Pruning\", arXiv, 2021 (Huawei). [Paper]\n\nMD-ViT: \"Multi-Dimensional Model Compression of Vision Transformer\", arXiv, 2021 (Princeton). [Paper]\n\nFQ-ViT: \"FQ-ViT: Fully Quantized Vision Transformer without Retraining\", arXiv, 2021 (Megvii). [Paper][PyTorch]\n\nUVC: \"Unified Visual Transformer Compression\", ICLR, 2022 (UT Austin). [Paper][PyTorch]\n\nMiniViT: \"MiniViT: Compressing Vision Transformers with Weight Multiplexing\", CVPR, 2022 (Microsoft). [Paper][PyTorch]\n\nAuto-ViT-Acc: \"Auto-ViT-Acc: An FPGA-Aware Automatic Acceleration Framework for Vision Transformer with Mixed-Scheme Quantization\", International Conference on Field Programmable Logic and Applications (FPL), 2022 (Northeastern University). [Paper]\n\nAPQ-ViT: \"Towards Accurate Post-Training Quantization for Vision Transformer\", ACMMM, 2022 (Beihang University). [Paper]\n\nSPViT: \"SPViT: Enabling Faster Vision Transformers via Soft Token Pruning\", ECCV, 2022 (Northeastern University). [Paper][PyTorch]\n\nPSAQ-ViT: \"Patch Similarity Aware Data-Free Quantization for Vision Transformers\", ECCV, 2022 (CAS). [Paper][PyTorch]\n\nPTQ4ViT: \"PTQ4ViT: Post-Training Quantization Framework for Vision Transformers\", ECCV, 2022 (Peking University). [Paper]\n\nEAPruning: \"EAPruning: Evolutionary Pruning for Vision Transformers and CNNs\", BMVC, 2022 (Meituan). [Paper]\n\nQ-ViT: \"Q-ViT: Accurate and Fully Quantized Low-bit Vision Transformer\", NeurIPS, 2022 (Beihang University). [Paper][PyTorch]\n\nSAViT: \"SAViT: Structure-Aware Vision Transformer Pruning via Collaborative Optimization\", NeurIPS, 2022 (Hikvision). [Paper]\n\nVTC-LFC: \"VTC-LFC: Vision Transformer Compression with Low-Frequency Components\", NeurIPS, 2022 (Alibaba). [Paper][PyTorch]\n\nQ-ViT: \"Q-ViT: Fully Differentiable Quantization for Vision Transformer\", arXiv, 2022 (Megvii). [Paper]\n\nVAQF: \"VAQF: Fully Automatic Software-Hardware Co-Design Framework for Low-Bit Vision Transformer\", arXiv, 2022 (Northeastern University). [Paper]\n\nVTP: \"Vision Transformer Compression with Structured Pruning and Low Rank Approximation\", arXiv, 2022 (UCLA). [Paper]\n\nSiDT: \"Searching Intrinsic Dimensions of Vision Transformers\", arXiv, 2022 (UC Irvine). [Paper]\n\nPSAQ-ViT-V2: \"PSAQ-ViT V2: Towards Accurate and General Data-Free Quantization for Vision Transformers\", arXiv, 2022 (CAS). [Paper][PyTorch]\n\nAS: \"Adaptive Sparse ViT: Towards Learnable Adaptive Token Pruning by Fully Exploiting Self-Attention\", arXiv, 2022 (Baidu). [Paper]\n\nSaiT: \"SaiT: Sparse Vision Transformers through Adaptive Token Pruning\", arXiv, 2022 (Samsung). [Paper]\n\noViT: \"oViT: An Accurate Second-Order Pruning Framework for Vision Transformers\", arXiv, 2022 (IST Austria). [Paper]\n\nCPT-V: \"CPT-V: A Contrastive Approach to Post-Training Quantization of Vision Transformers\", arXiv, 2022 (UT Austin). [Paper]\n\nTPS: \"Joint Token Pruning and Squeezing Towards More Aggressive Compression of Vision Transformers\", CVPR, 2023 (Megvii). [Paper][PyTorch]\n\nGPUSQ-ViT: \"Boost Vision Transformer with GPU-Friendly Sparsity and Quantization\", CVPR, 2023 (Fudan). [Paper]\n\nX-Pruner: \"X-Pruner: eXplainable Pruning for Vision Transformers\", CVPR, 2023 (James Cook University, Australia). [Paper][PyTorch (in construction)]\n\nNoisyQuant: \"NoisyQuant: Noisy Bias-Enhanced Post-Training Activation Quantization for Vision Transformers\", CVPR, 2023 (Nanjing University). [Paper]\n\nNViT: \"Global Vision Transformer Pruning with Hessian-Aware Saliency\", CVPR, 2023 (NVIDIA). [Paper]\n\nBinaryViT: \"BinaryViT: Pushing Binary Vision Transformers Towards Convolutional Models\", CVPRW, 2023 (Huawei). [Paper][PyTorch]\n\nOFQ: \"Oscillation-free Quantization for Low-bit Vision Transformers\", ICML, 2023 (HKUST). [Paper][PyTorch]\n\nUPop: \"UPop: Unified and Progressive Pruning for Compressing Vision-Language Transformers\", ICML, 2023 (Shanghai AI Lab). [Paper][PyTorch]\n\nCOMCAT: \"COMCAT: Towards Efficient Compression and Customization of Attention-Based Vision Models\", ICML, 2023 (Rutgers). [Paper][PyTorch]\n\nEvol-Q: \"Jumping through Local Minima: Quantization in the Loss Landscape of Vision Transformers\", ICCV, 2023 (UT Austin). [Paper][Code (in construction)]\n\nBiViT: \"BiViT: Extremely Compressed Binary Vision Transformer\", ICCV, 2023 (Zhejiang University). [Paper]\n\nI-ViT: \"I-ViT: Integer-only Quantization for Efficient Vision Transformer Inference\", ICCV, 2023 (CAS). [Paper][PyTorch]\n\nRepQ-ViT: \"RepQ-ViT: Scale Reparameterization for Post-Training Quantization of Vision Transformers\", ICCV, 2023 (CAS). [Paper][PyTorch]\n\nLLM-FP4: \"LLM-FP4: 4-Bit Floating-Point Quantized Transformers\", EMNLP, 2023 (HKUST). [Paper][Code (in construction)]\n\nQ-HyViT: \"Q-HyViT: Post-Training Quantization for Hybrid Vision Transformer with Bridge Block Reconstruction\", arXiv, 2023 (Electronics and Telecommunications Research Institute (ETRI), Korea). [Paper]\n\nBi-ViT: \"Bi-ViT: Pushing the Limit of Vision Transformer Quantization\", arXiv, 2023 (Beihang University). [Paper]\n\nBinaryViT: \"BinaryViT: Towards Efficient and Accurate Binary Vision Transformers\", arXiv, 2023 (CAS). [Paper]\n\nZero-TP: \"Zero-TPrune: Zero-Shot Token Pruning through Leveraging of the Attention Graph in Pre-Trained Transformers\", arXiv, 2023 (Princeton). [Paper]\n\n?: \"Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing\", arXiv, 2023 (Qualcomm). [Paper]\n\nVVTQ: \"Variation-aware Vision Transformer Quantization\", arXiv, 2023 (HKUST). [Paper][PyTorch]\n\nDIMAP: \"Data-independent Module-aware Pruning for Hierarchical Vision Transformers\", ICLR, 2024 (A*STAR). [Paper][Code (in construction)]\n\nMADTP: \"MADTP: Multimodal Alignment-Guided Dynamic Token Pruning for Accelerating Vision-Language Transformer\", CVPR, 2024 (Fudan). [Paper][Code (in construction)]\n\nDC-ViT: \"Dense Vision Transformer Compression with Few Samples\", CVPR, 2024 (Nanjing University). [Paper]\n\n[Back to Overview]\n\nRepMLP: \"RepMLP: Re-parameterizing Convolutions into Fully-connected Layers for Image Recognition\", arXiv, 2021 (Megvii). [Paper][PyTorch]\n\nEAMLP: \"Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks\", arXiv, 2021 (Tsinghua University). [Paper]\n\nForward-Only: \"Do You Even Need Attention? A Stack of Feed-Forward Layers Does Surprisingly Well on ImageNet\", arXiv, 2021 (Oxford). [Paper][PyTorch]\n\nResMLP: \"ResMLP: Feedforward networks for image classification with data-efficient training\", arXiv, 2021 (Facebook). [Paper]\n\n?: \"Can Attention Enable MLPs To Catch Up With CNNs?\", arXiv, 2021 (Tsinghua). [Paper]\n\nViP: \"Vision Permutator: A Permutable MLP-Like Architecture for Visual Recognition\", arXiv, 2021 (NUS, Singapore). [Paper][PyTorch]\n\nCCS: \"Rethinking Token-Mixing MLP for MLP-based Vision Backbone\", arXiv, 2021 (Baidu). [Paper]\n\nS2-MLPv2: \"S2-MLPv2: Improved Spatial-Shift MLP Architecture for Vision\", arXiv, 2021 (Baidu). [Paper]\n\nRaftMLP: \"RaftMLP: Do MLP-based Models Dream of Winning Over Computer Vision?\", arXiv, 2021 (Rikkyo University, Japan). [Paper][PyTorch]\n\nHire-MLP: \"Hire-MLP: Vision MLP via Hierarchical Rearrangement\", arXiv, 2021 (Huawei). [Paper]\n\nSparse-MLP: \"Sparse-MLP: A Fully-MLP Architecture with Conditional Computation\", arXiv, 2021 (NUS). [Paper]\n\nConvMLP: \"ConvMLP: Hierarchical Convolutional MLPs for Vision\", arXiv, 2021 (University of Oregon). [Paper][PyTorch]\n\nsMLP: \"Sparse MLP for Image Recognition: Is Self-Attention Really Necessary?\", arXiv, 2021 (Microsoft). [Paper]\n\nMLP-Mixer: \"MLP-Mixer: An all-MLP Architecture for Vision\", NeurIPS, 2021 (Google). [Paper][Tensorflow][PyTorch-1 (lucidrains)][PyTorch-2 (rishikksh20)]\n\ngMLP: \"Pay Attention to MLPs\", NeurIPS, 2021 (Google). [Paper][PyTorch (antonyvigouret)]\n\nS2-MLP: \"S2-MLP: Spatial-Shift MLP Architecture for Vision\", WACV, 2022 (Baidu). [Paper]\n\nCycleMLP: \"CycleMLP: A MLP-like Architecture for Dense Prediction\", ICLR, 2022 (HKU). [Paper][PyTorch]\n\nAS-MLP: \"AS-MLP: An Axial Shifted MLP Architecture for Vision\", ICLR, 2022 (ShanghaiTech University). [Paper][PyTorch]\n\nWave-MLP: \"An Image Patch is a Wave: Quantum Inspired Vision MLP\", CVPR, 2022 (Huawei). [Paper][PyTorch]\n\nDynaMixer: \"DynaMixer: A Vision MLP Architecture with Dynamic Mixing\", ICML, 2022 (Tencent). [Paper][PyTorch]\n\nSTD: \"Spatial-Channel Token Distillation for Vision MLPs\", ICML, 2022 (Huawei). [Paper]\n\nAMixer: \" AMixer: Adaptive Weight Mixing for Self-Attention Free Vision Transformers\", ECCV, 2022 (Tsinghua University). [Paper]\n\nMS-MLP: \"Mixing and Shifting: Exploiting Global and Local Dependencies in Vision MLPs\", arXiv, 2022 (Microsoft). [Paper]\n\nActiveMLP: \"ActiveMLP: An MLP-like Architecture with Active Token Mixer\", arXiv, 2022 (Microsoft). [Paper]\n\nMDMLP: \"MDMLP: Image Classification from Scratch on Small Datasets with MLP\", arXiv, 2022 (Jiangsu University). [Paper][PyTorch]\n\nPosMLP: \"Parameterization of Cross-Token Relations with Relative Positional Encoding for Vision MLP\", arXiv, 2022 (University of Science and Technology of China). [Paper][PyTorch]\n\nSplitMixer: \"SplitMixer: Fat Trimmed From MLP-like Models\", arXiv, 2022 (Quintic AI, California). [Paper][PyTorch]\n\ngSwin: \"gSwin: Gated MLP Vision Model with Hierarchical Structure of Shifted Window\", arXiv, 2022 (PKSHATechnology, Japan). [Paper]\n\n?: \"Analysis of Quantization on MLP-based Vision Models\", arXiv, 2022 (Berkeley). [Paper]\n\nAFFNet: \"Adaptive Frequency Filters As Efficient Global Token Mixers\", ICCV, 2023 (Microsoft). [Paper]\n\nStrip-MLP: \"Strip-MLP: Efficient Token Interaction for Vision MLP\", ICCV, 2023 (Southern University of Science and Technology). [Paper][PyTorch]\n\nOther Attention-Free\n\nDWNet: \"On the Connection between Local Attention and Dynamic Depth-wise Convolution\", ICLR, 2022 (Nankai Univerisy). [Paper][PyTorch]\n\nPoolFormer: \"MetaFormer is Actually What You Need for Vision\", CVPR, 2022 (Sea AI Lab). [Paper][PyTorch]\n\nConvNext: \"A ConvNet for the 2020s\", CVPR, 2022 (Facebook). [Paper][PyTorch]\n\nRepLKNet: \"Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs\", CVPR, 2022 (Megvii). [Paper][MegEngine][PyTorch]\n\nFocalNet: \"Focal Modulation Networks\", NeurIPS, 2022 (Microsoft). [Paper][PyTorch]\n\nHorNet: \"HorNet: Efficient High-Order Spatial Interactions with Recursive Gated Convolutions\", NeurIPS, 2022 (Tsinghua). [Paper][PyTorch][Website]\n\nS4ND: \"S4ND: Modeling Images and Videos as Multidimensional Signals Using State Spaces\", NeurIPS, 2022 (Stanford). [Paper]\n\nSequencer: \"Sequencer: Deep LSTM for Image Classification\", arXiv, 2022 (Rikkyo University, Japan). [Paper]\n\nMogaNet: \"Efficient Multi-order Gated Aggregation Network\", arXiv, 2022 (Westlake University, China). [Paper]\n\nConv2Former: \"Conv2Former: A Simple Transformer-Style ConvNet for Visual Recognition\", arXiv, 2022 (ByteDance). [Paper]\n\nCoC: \"Image as Set of Points\", ICLR, 2023 (Northeastern). [Paper][PyTorch]\n\nSLaK: \"More ConvNets in the 2020s: Scaling up Kernels Beyond 51x51 using Sparsity\", ICLR, 2023 (UT Austin). [Paper][PyTorch]\n\nConvNeXt-V2: \"ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders\", CVPR, 2023 (Meta). [Paper][PyTorch]\n\nSPANet: \"SPANet: Frequency-balancing Token Mixer using Spectral Pooling Aggregation Modulation\", ICCV, 2023 (Korea Institute of Science and Technology). [Paper][Code (in construction)][Website]\n\nDFFormer: \"FFT-based Dynamic Token Mixer for Vision\", arXiv, 2023 (Rikkyo University, Japan). [Paper][Code (in construction)]\n\n?: \"ConvNets Match Vision Transformers at Scale\", arXiv, 2023 (DeepMind). [Paper]\n\nVMamba: \"VMamba: Visual State Space Model\", arXiv, 2024 (CAS). [Paper][PyTorch]\n\nVim: \"Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model\", arXiv, 2024 (Huazhong University of Science and Technology). [Paper][[PyTorch](https://github.com/hustvl/Vim\n\nVRWKV: \"Vision-RWKV: Efficient and Scalable Visual Perception with RWKV-Like Architectures\", arXiv, 2024 (Shanghai AI Lab). [Paper][PyTorch]\n\nLocalMamba: \"LocalMamba: Visual State Space Model with Windowed Selective Scan\", arXiv, 2024 (University of Sydney). [Paper][PyTorch]\n\nSiMBA: \"SiMBA: Simplified Mamba-Based Architecture for Vision and Multivariate Time series\", arXiv, 2024 (Microsoft). [Paper][PyTorch]\n\nPlainMamba: \"PlainMamba: Improving Non-Hierarchical Mamba in Visual Recognition\", arXiv, 2024 (University of Edinburgh, Scotland). [Paper][PyTorch]\n\nEfficientVMamba: \"EfficientVMamba: Atrous Selective Scan for Light Weight Visual Mamba\", arXiv, 2024 (The University of Sydney). [Paper][PyTorch]\n\nRDNet: \"DenseNets Reloaded: Paradigm Shift Beyond ResNets and ViTs\", arXiv, 2024 (NAVER). [Paper]\n\nMambaOut: \"MambaOut: Do We Really Need Mamba for Vision?\", arXiv, 2024 (NUS). [Paper][PyTorch]\n\n[Back to Overview]\n\nAnalysis for Transformer\n\nAttention-CNN: \"On the Relationship between Self-Attention and Convolutional Layers\", ICLR, 2020 (EPFL). [Paper][PyTorch][Website]\n\nTransformer-Explainability: \"Transformer Interpretability Beyond Attention Visualization\", CVPR, 2021 (Tel Aviv). [Paper][PyTorch]\n\n?: \"Are Convolutional Neural Networks or Transformers more like human vision?\", CogSci, 2021 (Princeton). [Paper]\n\n?: \"ConvNets vs. Transformers: Whose Visual Representations are More Transferable?\", ICCVW, 2021 (HKU). [Paper]\n\n?: \"Do Vision Transformers See Like Convolutional Neural Networks?\", NeurIPS, 2021 (Google). [Paper]\n\n?: \"Intriguing Properties of Vision Transformers\", NeurIPS, 2021 (MBZUAI). [Paper][PyTorch]\n\nFoveaTer: \"FoveaTer: Foveated Transformer for Image Classification\", arXiv, 2021 (UCSB). [Paper]\n\n?: \"Demystifying Local Vision Transformer: Sparse Connectivity, Weight Sharing, and Dynamic Weight\", arXiv, 2021 (Microsoft). [Paper]\n\n?: \"Revisiting the Calibration of Modern Neural Networks\", arXiv, 2021 (Google). [Paper]\n\n?: \"What Makes for Hierarchical Vision Transformer?\", arXiv, 2021 (Horizon Robotic). [Paper]\n\n?: \"Visualizing Paired Image Similarity in Transformer Networks\", WACV, 2022 (Temple University). [Paper][PyTorch]\n\nFDSL: \"Can Vision Transformers Learn without Natural Images?\", AAAI, 2022 (AIST). [Paper][PyTorch][Website]\n\nAlterNet: \"How Do Vision Transformers Work?\", ICLR, 2022 (Yonsei University). [Paper][PyTorch]\n\n?: \"When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations\", ICLR, 2022 (Google). [Paper][Tensorflow]\n\n?: \"Unraveling Attention via Convex Duality: Analysis and Interpretations of Vision Transformers\", ICML, 2022 (Stanford). [Paper]\n\n?: \"Three things everyone should know about Vision Transformers\", ECCV, 2022 (Meta). [Paper]\n\n?: \"Vision Transformers provably learn spatial structure\", NeurIPS, 2022 (Princeton). [Paper]\n\nAWD-ViT: \"Visualizing and Understanding Patch Interactions in Vision Transformer\", arXiv, 2022 (JD). [Paper]\n\n?: \"CNNs and Transformers Perceive Hybrid Images Similar to Humans\", arXiv, 2022 (Q"
    }
}
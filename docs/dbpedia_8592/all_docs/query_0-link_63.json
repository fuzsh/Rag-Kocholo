{
    "id": "dbpedia_8592_0",
    "rank": 63,
    "data": {
        "url": "https://saaers.wordpress.com/tag/open-source-software/",
        "read_more_link": "",
        "language": "en",
        "title": "open source software – bloggERS!",
        "top_image": "https://secure.gravatar.com/blavatar/20bda1d33179b0765b259a475995262e0833efaa56cae8f62c1e592aa074c398?s=200&ts=1724035916",
        "meta_img": "https://secure.gravatar.com/blavatar/20bda1d33179b0765b259a475995262e0833efaa56cae8f62c1e592aa074c398?s=200&ts=1724035916",
        "images": [
            "https://saaers.wordpress.com/wp-content/uploads/2019/01/76538_large.jpg?w=636",
            "https://saaers.wordpress.com/wp-content/uploads/2019/01/carraheadshot.jpg?w=282",
            "https://saaers.wordpress.com/wp-content/uploads/2017/10/valencia-johnson_bulkextractor_error.jpg?w=636",
            "https://saaers.wordpress.com/wp-content/uploads/2017/10/valencia_johnson_2_pii_additional_blackouts.jpg?w=636",
            "https://saaers.wordpress.com/wp-content/uploads/2017/10/valencia_johnson_bio_picture.jpg?w=150",
            "https://saaers.wordpress.com/wp-content/uploads/2017/08/abcheadshot.jpg",
            "https://saaers.wordpress.com/wp-content/uploads/2017/04/heidi-elaine-kelly.jpg",
            "https://saaers.wordpress.com/wp-content/uploads/2017/04/meister_photo.jpg",
            "https://saaers.wordpress.com/wp-content/uploads/2017/04/heidi-elaine-kelly.jpg",
            "https://saaers.wordpress.com/wp-content/uploads/2017/04/peltzman_140902_6761_barnett.jpg",
            "https://saaers.wordpress.com/wp-content/uploads/2017/04/heidi-elaine-kelly.jpg",
            "https://2.gravatar.com/avatar/b640fa08d1547acdd60d8444728ef9158f4cae2cce054abfb73748767fea06b0?s=48&d=https%3A%2F%2F2.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D48&r=G",
            "https://0.gravatar.com/avatar/3947733f0c7ad0ff2365d3e8ad6919a9cebcaeec5483dd6201e5e7bf0f783224?s=48&d=https%3A%2F%2F0.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D48&r=G",
            "https://1.gravatar.com/avatar/aa8e46446496cb216c2c86698f4eae587dd0b015c87614145264d31583df9cbd?s=48&d=https%3A%2F%2F1.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D48&r=G",
            "https://1.gravatar.com/avatar/433a48228e0357d9a8c4169ca3d40dc47a7290ce755d320c3c1e3b33db83689d?s=48&d=https%3A%2F%2F1.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D48&r=G",
            "https://secure.gravatar.com/blavatar/20bda1d33179b0765b259a475995262e0833efaa56cae8f62c1e592aa074c398?s=50&d=https%3A%2F%2Fs2.wp.com%2Fi%2Flogo%2Fwpcom-gray-white.png",
            "https://secure.gravatar.com/blavatar/20bda1d33179b0765b259a475995262e0833efaa56cae8f62c1e592aa074c398?s=50&d=https%3A%2F%2Fs2.wp.com%2Fi%2Flogo%2Fwpcom-gray-white.png",
            "https://pixel.wp.com/b.gif?v=noscript"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "bloggERS! Editors"
        ],
        "publish_date": "2023-06-21T11:18:53-04:00",
        "summary": "",
        "meta_description": "Posts about open source software written by bloggERS! Editors",
        "meta_lang": "en",
        "meta_favicon": "https://secure.gravatar.com/blavatar/20bda1d33179b0765b259a475995262e0833efaa56cae8f62c1e592aa074c398?s=32",
        "meta_site_name": "bloggERS!",
        "canonical_link": "https://saaers.wordpress.com/tag/open-source-software/",
        "text": "By Tyler McNally\n\nThe BloggERS team is revisiting some of our most popular posts. This post originally ran in January 2017 as part of the bloggERS series on processing digital materials. Want to pitch a post idea to bloggERS? Send us an email!\n\n———\n\nMany archives don’t have the resources to install software or subscribe to a service such as Archivematica, but still have a mandate to collect and preserve born-digital records. Below is a digital-preservation workflow created by Tyler McNally at the University of Manitoba. If you have a similar workflow at your institution, include it in the comments.\n\n———\n\nRecently I completed an internship at the University of Manitoba’s College of Medicine Archives, working with Medical Archivist Jordan Bass. A large part of my work during this internship dealt with building digital infrastructure for the archive to utilize in working on digital preservation. As a small operation, the archive does not have the resources to really pursue any kind of paid or difficult to use system.\n\nOriginally, our plan was to use the open-source, self-install version of Archivematica, but certain issues that cropped up made this impossible, considering the resources we had at hand. We decided that we would simply make our own digital-preservation workflow, using open-source and free software to convert our files for preservation and access, check for viruses, and create checksums—not every service that Archivematica offers, but enough to get our files stored safely. I thought other institutions of similar size and means might find the process I developed useful in thinking about their own needs and capabilities.\n\nContinue reading →\n\nby Regina Carra\n\nWhen: December 3, 2018\n\nWhere: Metropolitan New York Library Council (METRO), New York, NY\n\nSpeakers:\n\nStephen Klein, Digital Services Librarian at the CUNY Graduate Center (CUNY)\n\nAshley Blewer, AV Preservation Specialist at Artefactual\n\nKelly Stewart, Digital Preservation Services Manager at Artefactual\n\nOn December 3, 2018, the Metropolitan New York Library Council (METRO)’s Digital Preservation Interest Group hosted an informative (and impeccably titled) presentation about how the CUNY Graduate Center (GC) plans to incorporate Archivematica, a web-based, open-source digital asset management software (DAMs) developed by Artefactual, into its document management strategy for student dissertations. Speakers included Stephen Klein, Digital Services Librarian at the CUNY Graduate Center (GC); Ashley Blewer, AV Preservation Specialist at Artefactual; and Kelly Stewart, Digital Preservation Services Manager at Artefactual. The presentation began with an overview from Stephen about the GC’s needs and why they chose Archivematica as a DAMs, followed by an introduction to and demo of Archivematica and Duracloud, an open-source cloud storage service, led by Ashley and Kelly (who was presenting via video-conference call). While this post provides a general summary of the presentation, I would recommend reaching out to any of the presenters for more detailed information about their work. They were all great!\n\nEvery year the GC Library receives between 400-500 dissertations, theses, and capstones. These submissions can include a wide variety of digital materials, from PDF, video, and audio files, to websites and software. Preservation of these materials is essential if the GC is to provide access to emerging scholarship and retain a record of students’ work towards their degrees. Prior to implementing a DAMs, however, the GC’s strategy for managing digital files of student work was focused primarily on access, not preservation. Access copies of student work were available on CUNY Academic Works, a site that uses Bepress Digital Commons as a CMS. Missing from the workflow, however, was the creation, storage, and management of archival originals. As Stephen explained, if the Open Archival Information System (OAIS) model is a guide for a proper digital preservation workflow, the GC was without the middle, Archival Information Package (AIP), portion of it. Some of the qualities that GC liked about Archivematica was that it was open-source and highly-customizable, came with strong customer support from Artefactual, and had an API that could integrate with tools already in use at the library. GC Library staff hope that Archivematica can eventually integrate with both the library’s electronic submission system (Vireo) and CUNY Academic Works, making the submission, preservation, and access of digital dissertations a much more streamlined, automated, and OAIS-compliant process.\n\nNext, Ashley and Kelly introduced and demoed Archivematica and Duracloud. I was very pleased to see several features of the Archivematica software that were made intentionally intuitive. The design of the interface is very clean and easily customizable to fit different workflows. Also, each AIP that is processed includes a plain-text, human-readable file which serves as extra documentation explaining what Archivematica did to each file. Artefactual recommends pairing Archivematica with Duracloud, although users can choose to integrate the software with local storage or with other cloud services like those offered by Google or Amazon. One of the features I found really interesting about Duracloud is that it comes with various data visualization graphs that show the user how much storage is available and what materials are taking up the most space.\n\nI close by referencing something Ashley wrote in her recent bloggERS post (conveniently she also contributed to this event). She makes an excellent point about how different skill-sets are needed to do digital preservation, from the developers that create the tools that automate digital archival processes to the archivists that advocate for and implement said tools at their institutions. I think this talk was successful precisely because it included the practitioner and vendor perspectives, as well as the unique expertise that comes with each role. Both are needed if we are to meet the challenges and tap into the potential that digital archives present. I hope to see more of these “meetings of the minds” in the future.\n\n(For more info: Stephen and Ashley and Kelly have generously shared their slides!)\n\nRegina Carra is the Archive Project Metadata and Cataloging Coordinator at Mark Morris Dance Group. She is a recent graduate of the Dual Degree MLS/MA program in Library Science and History at Queens College – CUNY.\n\nBy Valencia Johnson\n\nThis is the fourth post in the bloggERS series on Archiving Digital Communication.\n\nThis summer I had the pleasure of accessioning a large digital collection from a retiring staff member. Due to their longevity with the institution, the creator had amassed an extensive digital record. In addition to their desktop files, the archive collected an archival Outlook .pst file of 15.8 GB! This was my first time working with emails. This was also the first time some of the tools discussed below were used in the workflow at my institution. As a newcomer to the digital archiving community, I would like to share this case study and my first impressions on the tools I used in this acquisition.\n\nMy original workflow:\n\nConvert the .pst file into an .mbox file.\n\nPlace both files in a folder titled Emails and add this folder to the acquisition folder that contains the Desktop files folder. This way the digital records can be accessioned as one unit.\n\nFollow and complete our accessioning procedures.\n\nThings were moving smoothly; I was able to use Emailchemy, a tool that converts email from closed, proprietary file formats, such as .pst files used by Outlook, to standard, portable formats that any application can use, such as .mbox files, which can be read using Thunderbird, Mozilla’s open source email client. I used a Windows laptop that had Outlook and Thunderbird installed to complete this task. I had no issues with Emailchemy, the instructions in the owner’s manual were clear, and the process was easy. Next, I uploaded the Email folder, which contained the .pst and .mbox files, to the acquisition external hard drive and began processing with BitCurator. The machine I used to accession is a FRED, a powerful forensic recovery tool used by law enforcement and some archivists. Our FRED runs BitCurator, which is a Linux environment. This is an important fact to remember because .pst files will not open on a Linux machine.\n\nAt Princeton, we use Bulk Extractor to check for Personally Identifiable Information (PII) and credit card numbers. This is step 6 in our workflow and this is where I ran into some issues.\n\nThe program was unable to complete 4 threads within the Email folder and timed out. The picture above is part of the explanation message I received. In my understanding and research, aka Google because I did not understand the message, the program was unable to completely execute the task with the amount of processing power available. So the message is essentially saying “I don’t know why this is taking so long. It’s you not me. You need a better computer.” From the initial scan results, I was able to remove PII from the Desktop folder. So instead of running the scan on the entire acquisition folder, I ran the scan solely on the Email folder and the scan still timed out. Despite the incomplete scan, I moved on with the results I had.\n\nI tried to make sense of the reports Bulk Extractor created for the email files. The Bulk Extractor output includes a full file path for each file flagged, e.g. (/home/user/Desktop/blogERs/Email.docx). This is how I was able locate files within the Desktop folder. The output for the Email folder looked like this:\n\nEven though Bulk Extractor Viewer does display the content, it displays it like a text editor, e.g. Notepad, with all the coding alongside the content of the message, not as an email, because all the results were from the .mbox file. This is just the format .mbox generates without an email client. This coding can be difficult to interpret without an email client to translate the material into a human readable format. This output makes it hard to locate an individual message within a .pst because it is hard but not impossible to find the date or title of the email amongst the coding. But this was my first time encountering results like this and it freaked me out a bit.\n\nBecause regular expressions, the search method used by Bulk Extractor, looks for number patterns, some of the hits were false positives, number strings that matched the pattern of SSN or credit card numbers. So in lieu of social security numbers, I found the results were FedEx tracking numbers or mistyped phone numbers, though to be fair mistyped numbers are someone’s SSN. For credit card numbers, the program picked up email coding and non-financially related number patterns.\n\nThe scan found a SSN I had to remove from the .pst and the .mbox. Remember .pst files only work with Microsoft Outlook. At this point in processing, I was on a Linux machine and could not open the .pst so I focused on the .mbox. Using the flagged terms, I thought maybe I could use a keyword search within the .mbox to locate and remove the flagged material because you can open .mbox files using a text editor. Remember when I said the .pst was over 15 GB? Well the .mbox was just as large and this caused the text editor to stall and eventually give up opening the file. Despite these challenges, I remained steadfast and found UltraEdit, a large text file editor. This whole process took a couple of days and in the end the results from Bulk Extractor’s search indicated the email files contained one SSN and no credit card numbers.\n\nWhile discussing my difficulties with my supervisor, she suggested trying FileLocator Pro, a scanner like Bulk Extractor that was created with .pst files in mind, to fulfill our due diligence to look for sensitive information since the Bulk Extractor scan timed out before finishing. Though FileLocator Pro operates on Windows so, unfortunately, we couldn’t do the scan on the FRED, FileLocator Pro was able to catch real SSNs hidden in attachments that did not appear in the Bulk Extractor results.\n\nI was able to view the email with the flagged content highlighted within FileLocator Pro like Bulk Extractor. Also, there is the option to open the attachments or emails in their respective programs. So a .pdf file opened in Adobe and the email messages opened in Outlook. Even though I had false positives with FileLocator Pro, verifying the content was easy. It didn’t perform as well searching for credit card numbers; I had some error messages stating that some attached files contained no readable text or that FileLocator Pro had to use a raw data search instead of the primary method. These errors were limited to attachments with .gif, .doc, .pdf, and .xls extensions. But overall it was a shorter and better experience working with FileLocator Pro, at least when it comes to email files.\n\nAs emails continue to dominate how we communicate at work and in our personal lives, archivists and electronic records managers can expect to process even larger files, despite how long an individual stays at an institution. Larger files can make the hunt for PII and other sensitive data feel like searching for a needle in a haystack, especially when our scanners are unable to flag individual emails, attachments, or even complete a scan. There’s no such thing as a perfect program; I like Bulk Extractor for non-email files, and I have concerns with FileLocator Pro. However, technology continues to improve and with forums like this blog we can learn from one another.\n\nValencia Johnson is the Digital Accessioning Assistant for the Seeley G. Mudd Manuscript Library at Princeton University. She is a certified archivist with an MA in Museum Studies from Baylor University.\n\nBy Alston Cobourn\n\nThis is the first post in the bloggERS series on Archiving Digital Communication.\n\n—\n\nGetting Started\n\nSoon after I arrived at Texas A&M University-Corpus Christi in January 2017 as the university’s first Processing and Digital Assets Archivist, two high-level longtime employees retired or switched positions. Therefore, I fast-tracked an effort to begin collecting selected email records because these employees undoubtedly had some correspondence of long-term significance, which was also governed by the Texas A&M System’s records retention schedules.\n\nI began by testing ePADD, software used to conduct various archival processes on email, on small date ranges of my work email account. I ultimately decided to begin using it on selected campus email because I found it relatively easy to use, it includes some helpful appraisal tools, and it provides an interface for patrons to view and select records of which they want a copy. Since the emails themselves live as MBOX files in folders outside of the software, and are viewable with a text editor, I felt comfortable that using ePADD meant not risking the loss of important records. I installed ePADD on my laptop with the thought that traveling to the employees would make the process of transferring their email easier and encourage cooperation.\n\nTransferring the email\n\nIn June 2017, I used ePADD Version 3.1 to collect the email of the two employees. My department head shared general information and arranged an appointment with the employees’ current administrative assistant or interim replacement as applicable. She also made a request to campus IT that they keep the account of the retired employee open. IT granted the interim replacement access to the account.\n\nI then traveled to the employees’ offices where they entered the appropriate credentials for the university email account into ePADD, identified which folders were most likely to contain records of long-term historical value, and verified the date range I needed to capture. Then we waited.\n\nIn one instance, I had to leave my laptop running in the person’s office overnight because I needed to maintain a consistent internet connection during ePADD’s approximately eight hours of harvesting and the office was off-campus. I had not remembered to bring a power cord, but thankfully my laptop was fully charged.\n\nSuccesses\n\nOur main success—we were actually able to collect some records! Obvious, yes, but I state it because it was the first time TAMU-CC has ever collected this record format and the email of the departed employee was almost deactivated before we sent our preservation request to IT. Second, my department head and I have started conversations with important players on campus about the ethical and legal reasons why the archives needs to review email before disposal.\n\nChallenges\n\nIn both cases, the employee had deleted a significant number of emails before we were able to capture their account and had used their work account for personal email. These behaviors confirmed what we all already knew–employees are largely unaware that their email is an official record. Therefore, we plan to increase efforts to educate faculty and staff about this fact, their responsibilities, and best practices for organizing their email. The external conversations we have had so far are an important start.\n\nePADD enabled me to combat the personal email complication by systematically deleting all emails from specific individual senders in batch. I took this approach for family members, listservs, and notifications from various personal accounts.\n\nThe feature that recognizes sensitive information worked well in identifying messages that contained social security numbers. However, it did not flag messages that contained phone numbers, which we also consider sensitive personal information. Additionally, in-message redaction is not possible in 3.1.\n\nFor messages I have marked as restricted, I have chosen to add an annotation as well that specifies the reason for the restriction. This will enable me to manage those emails at a more granular level. This approach was a modification of a suggestion by fellow archivists at Duke University.\n\nConclusion\n\nCurrently, the email is living on a networked drive while we establish an Amazon S3 account and an Archivematica instance. We plan to provide access to email in our reading room via the ePADD delivery module and publicize this access via finding aids. Overall ePADD is a positive step forward for TAMU-CC.\n\nNote from the Author:\n\nSince writing this post, I have learned that it is possible in ePADD to use regular expressions to further aid in identifying potentially sensitive materials. By default the program uses regular expressions to find social security numbers, but it can be configured to find other personal information such as credit card numbers and phone numbers. Further guidance is provided in the Reviewing Regular Expressions section of the ePADD User Guide.\n\n—\n\nAlston Cobourn is the Processing and Digital Assets Archivist at Texas A&M University-Corpus Christi where she leads the library’s digital preservation efforts. Previously she was the Digital Scholarship Librarian at Washington and Lee University. She holds a BA and MLS with an Archives and Records Management concentration from UNC-Chapel Hill.\n\nBy Sam Meister\n\n____\n\nThis is the third post in the bloggERS series describing outcomes of the #OSS4Pres 2.0 workshop at iPRES 2016, addressing open source tool and software development for digital preservation. This post outlines the work of the group tasked with “developing requirements for an online community space for sharing workflows, OSS tool integrations, and implementation experiences” See our other posts for information on the groups that focused on feature development and design requirements for FOSS tools.\n\nCultural heritage institutions, from small museums to large academic libraries, have made significant progress developing and implementing workflows to manage local digital curation and preservation activities. Many institutions are at different stages in the maturity of these workflows. Some are just getting started, and others have had established workflows for many years. Documentation assists institutions in representing current practices and functions as a benchmark for future organizational decision-making and improvements. Additionally, sharing documentation assists in creating cross-institutional understanding of digital curation and preservation activities and can facilitate collaborations amongst institutions around shared needs.\n\nOne of the most commonly voiced recommendations from iPRES 2015 OSS4PRES workshop attendees was the desire for a centralized location for technical and instructional documentation, end-to-end workflows, case studies, and other resources related to the installation, implementation, and use of OSS tools. This resource could serve as a hub that would enable practitioners to freely and openly exchange information, user requirements, and anecdotal accounts of OSS initiatives and implementations.\n\nAt the OSS4Pres 2.0 workshop, the group of folks looking at developing an online space for sharing workflows and implementation experience started by defining a simple goal and deliverable for the two hour session:\n\nDevelop a list of minimal levels of content that should be included in an open online community space for sharing workflows and other documentation\n\nThe group the began a discussion on developing this list of minimal levels by thinking about the potential value of user stories in informing these levels. We spent a bit of time proposing a short list of user stories, just enough to provide some insight into the basic structures that would be needed for sharing workflow documentation.\n\nUser stories\n\nI am using tool 1 and tool 2 and want to know how others have joined them together into a workflow\n\nI have a certain type of data to preserve and want to see what workflows other institutions have in place to preserve this data\n\nThere is a gap in my workflow — a function that we are not carrying out — and I want to see how others have filled this gap\n\nI am starting from scratch and need to see some example workflows for inspiration\n\nI would like to document my workflow and want to find out how to do this in a way that is useful for others\n\nI would like to know why people are using particular tools – is there evidence that they tried another tool, for example, that wasn’t successful?\n\nThe group then proceeded to define a workflow object as a series of workflow steps with its own attributes, a visual representation, and organizational context:\n\nWorkflow step\n\nTitle / name\n\nDescription\n\nTools / resources\n\nPosition / role\n\nVisual workflow diagrams / model\n\nOrganizational Context\n\nInstitution type\n\nContent type\n\nNext, we started to draft out the different elements that would be part of an initial minimal level for workflow objects:\n\nLevel 1:\n\nTitle\n\nDescription\n\nInstitution / organization type\n\nContact\n\nContent type(s)\n\nStatus\n\nLink to external resources\n\nDownload workflow diagram objects\n\nWorkflow concerns / reflections / gaps\n\nAfter this effort the group focused on discussing next steps and how an online community space for sharing workflows could be realized. This discuss led towards pursuing the expansion of COPTR to support sharing of workflow documentation. We outlined a roadmap for next steps toward pursuing this goal:\n\nPropose / approach COPTR steering group on adding workflows space to COPTR\n\nDevelop home page and workflow template\n\nAdd examples\n\nGroup review\n\nPromote / launch\n\nEvaluation\n\nThe group has continued this work post-workshop and has made good progress setting up a Community Owned Workflows section to COPTR and developing an initial workflow template. We are in the midst of creating and evaluating sample workflows to help with revising and tweaking as needed. Based on this process we hope to launch and start promoting this new online space for sharing workflows in the months ahead. So stay tuned!\n\n____\n\nSam Meister is the Preservation Communities Manager, working with the MetaArchive Cooperative and BitCurator Consortium communities. Previously, he worked as Digital Archivist and Assistant Professor at the University of Montana. Sam holds a Master of Library and Information Science degree from San Jose State University and a B.A. in Visual Arts from the University of California San Diego. Sam is also an Instructor in the Library of Congress Digital Preservation Education and Outreach Program.\n\nBy Heidi Elaine Kelly\n\n____\n\nThis is the second post in the bloggERS series describing outcomes of the #OSS4Pres 2.0 workshop at iPRES 2016, addressing open source tool and software development for digital preservation. This post outlines the work of the group tasked with “drafting a design guide and requirements for Free and Open Source Software (FOSS) tools, to ensure that they integrate easily with digital preservation institutional systems and processes.”\n\nThe FOSS Development Requirements Group set out to create a design guide for FOSS tools to ensure easier adoption of open-source tools by the digital preservation community, including their integration with common end-to-end software and tools supporting digital preservation and access that are now in use by that community.\n\nThe group included representatives of large digital preservation and access projects such as Fedora and Archivematica, as well as tool developers and practitioners, ensuring a range of perspectives were represented. The group’s initial discussion led to the creation of a list of minimum necessary requirements for developing open source tools for digital preservation, based on similar examples from the Open Preservation Foundation (OPF) and from other fields. Below is the draft list that the group came up with, followed by some intended future steps. We welcome feedback or additions to the list, as well as suggestions for where such a list might be hosted long term.\n\nMinimum Necessary Requirements for FOSS Digital Preservation Tool Development\n\nNecessities\n\nProvide publicly accessible documentation and an issue tracker\n\nHave a documented process for how people can contribute to development, report bugs, and suggest new documentation\n\nEvery tool should do the smallest possible task really well; if you are developing an end-to-end system, develop it in a modular way in keeping with this principle\n\nFollow established standards and practices for development and use of the tool\n\nKeep documentation up-to-date and versioned\n\nFollow test-driven development philosophy\n\nDon’t develop a tool without use cases, and stakeholders willing to validate those use cases\n\nUse an open and permissive software license to allow for integrations and broader use\n\nRecommendations\n\nHave a mailing list, Slack or IRC channel, or other means for community interaction\n\nEstablish community guidelines\n\nProvide a well-documented mechanism for integration with other tools/systems in different languages\n\nProvide functionality of tool as a library, separating out the GUI and the actual functions\n\nPackage tool in an easy-to-use way; the more broadly you want the tool to be used, package it for different operating systems\n\nUse a packaging format that supports any dependencies\n\nProvide examples of functionality for potential users\n\nConsider the organizational home or archive for the tool for long-term sustainability; develop your tool based on potential organizations’ guidelines\n\nConsider providing a mechanism for internationalization of your tool (this is a broader community need as well, to identify the tools that exist and to incentivize this)\n\nPremise\n\nDigital preservation is an operating system-agnostic field\n\nNext Steps\n\nFeedback and Perspectives. Because of the expense of the iPRES conference (and its location in Switzerland), all of the group members were from relatively large and well-resourced institutions. The perspective of under-resourced institutions is very often left out of open-source development communities, as they are unable to support and contribute to such projects; in this case, this design guide would greatly benefit from the perspective of such institutions as to how FOSS tools can be developed to better serve their digital preservation needs. The group was also largely from North America and Europe, so this work would eventually benefit greatly from adding perspectives from the FOSS and digital preservation communities in South America, Asia, and Africa.\n\nInstitutional Home and Stewardship. When finalized, the FOSS development requirements list should live somewhere permanently and develop based on the ongoing needs of our community. As this line of communication between practitioners and tool developers is key to the continual development of better and more user-friendly digital preservation tools, we should continue to build on the work of this group.\n\nReferenced FOSS Tool and Community Guides\n\nhttps://www.biostars.org/p/110269/\n\nhttp://wiki.opf-labs.org/display/SPR/The+SPRUCE+Mashup+Manifesto\n\nhttps://wiki.duraspace.org/display/hydra/Hydra+Community+Principles\n\n____\n\nHeidi Elaine Kelly is the Digital Preservation Librarian at Indiana University, where she is responsible for building out the infrastructure to support long-term sustainability of digital content. Previously she was a DiXiT fellow at Huygens ING and an NDSR fellow at the Library of Congress.\n\nBy Heidi Elaine Kelly and Shira Peltzman\n\n____\n\nThis is the first post in a bloggERS series describing outcomes of the #OSS4Pres 2.0 workshop at iPRES 2016.\n\nOrganized by Sam Meister (Educopia), Shira Peltzman (UCLA), Carl Wilson (Open Preservation Foundation), and Heidi Kelly (Indiana University), OSS4PRES 2.0 was a half-day workshop that took place during the 13th annual iPRES 2016 conference in Bern, Switzerland. The workshop aimed to bring together digital preservation practitioners, developers, and administrators in order to discuss the role of open source software (OSS) tools in the field.\n\nAlthough several months have passed since the workshop wrapped up, we are sharing this information now in an effort to raise awareness of the excellent work completed during this event, to continue the important discussion that took place, and to hopefully broaden involvement in some of the projects that developed. First, however, a bit of background: The initial OSS4PRES workshop was held at iPRES 2015. Attended by over 90 digital preservation professionals from all areas of the open source community, individuals reported on specific issues related to open source tools, which were followed by small group discussions about the opportunities, challenges, and gaps that they observed. The energy from this initial workshop led to both the proposal of a second workshop, as well as a report that was published in Code4Lib Journal, OSS4EVA: Using Open-Source Tools to Fulfill Digital Preservation Requirements.\n\nThe overarching goal for the 2016 workshop was to build bridges and fill gaps within the open source community at large. In order to facilitate a focused and productive discussion, OSS4PRES 2.0 was organized into three groups, each of which was led by one of the workshop’s organizers. Additionally, Shira Peltzman floated between groups to minimize overlap and ensure that each group remained on task. In addition to maximizing our output, one of the benefits of splitting up into groups was that each group was able to focus on disparate but complementary aspects of the open source community.\n\nDevelop user stories for existing tools (group leader: Carl Wilson)\n\nCarl’s group was comprised principally of digital preservation practitioners. The group scrutinized existing pain points associated with the day-to-day management of digital material, identified tools that had not yet been built that were needed by the open source community, and began to fill this gap by drafting functional requirements for these tools.\n\nDefine requirements for online communities to share information about local digital curation and preservation workflows (group leader: Sam Meister)\n\nWith an aim to strengthen the overall infrastructure around open source tools in digital preservation, Sam’s group focused on the larger picture by addressing the needs of the open source community at large. The group drafted a list of requirements for an online community space for sharing workflows, tool integrations, and implementation experiences, to facilitate connections between disparate groups, individuals, and organizations that use and rely upon open source tools.\n\nDefine requirements for new tools (group leader: Heidi Kelly)\n\nHeidi’s group looked at how the development of open source digital preservation tools could be improved by implementing a set of minimal requirements to make them more user-friendly. Since a list of these requirements specifically for the preservation community had not existed previously, this list both fills a gap and facilitates the building of bridges, by enabling developers to create tools that are easier to use, implement, and contribute to.\n\n—\n\nUltimately OSS4PRES 2.0 was an effort to make the open source community more open and diverse, and in the coming weeks we will highlight what each group managed to accomplish towards that end. The blog posts will provide an in-depth summary of the work completed both during and since the event took place, as well as a summary of next steps and potential project outcomes. Stay tuned!\n\n____\n\nShira Peltzman is the Digital Archivist for the UCLA Library where she leads the development of a sustainable preservation program for born-digital material. Shira received her M.A. in Moving Image Archiving and Preservation from New York University’s Tisch School of the Arts and was a member of the inaugural class of the National Digital Stewardship Residency in New York (NDSR-NY).\n\nHeidi Elaine Kelly is the Digital Preservation Librarian at Indiana University, where she is responsible for building out the infrastructure to support long-term sustainability of digital content. Previously she was a DiXiT fellow at Huygens ING and an NDSR fellow at the Library of Congress."
    }
}
{
    "id": "dbpedia_8592_1",
    "rank": 59,
    "data": {
        "url": "https://github.com/IQSS/dataverse/blob/develop/doc/sphinx-guides/source/installation/config.rst",
        "read_more_link": "",
        "language": "en",
        "title": "dataverse/doc/sphinx-guides/source/installation/config.rst at develop Â· IQSS/dataverse",
        "top_image": "https://opengraph.githubassets.com/583ac4fcc7559d4a70b54ee2b1f7f6540a70025364a1014a3445ca3b8ab64864/IQSS/dataverse",
        "meta_img": "https://opengraph.githubassets.com/583ac4fcc7559d4a70b54ee2b1f7f6540a70025364a1014a3445ca3b8ab64864/IQSS/dataverse",
        "images": [
            "https://github.com/IQSS/dataverse/raw/develop/doc/sphinx-guides/source/installation/img/dvBrandingCustBlocks.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "Open source research data repository software. Contribute to IQSS/dataverse development by creating an account on GitHub.",
        "meta_lang": "en",
        "meta_favicon": "https://github.com/fluidicon.png",
        "meta_site_name": "GitHub",
        "canonical_link": "https://github.com/IQSS/dataverse/blob/develop/doc/sphinx-guides/source/installation/config.rst",
        "text": "Now that you've successfully logged into your Dataverse installation with a superuser account after going through a basic :doc:`installation-main`, you'll need to secure and configure your installation.\n\nSettings within your Dataverse installation itself are managed via JVM options or by manipulating values in the setting table directly or through API calls.\n\nOnce you have finished securing and configuring your Dataverse installation, you may proceed to the :doc:`/admin/index` for more information on the ongoing administration of a Dataverse installation. Advanced configuration topics are covered in the :doc:`shibboleth` and :doc:`oauth2` sections.\n\nThe default password for the \"dataverseAdmin\" superuser account is \"admin\", as mentioned in the :doc:`installation-main` section, and you should change it, of course.\n\nThe :doc:`/api/native-api` contains a useful but potentially dangerous API endpoint called \"admin\" that allows you to change system settings, make ordinary users into superusers, and more. The \"builtin-users\" endpoint lets admins create a local/builtin user account if they know the key defined in :ref:`BuiltinUsers.KEY`.\n\nBy default, most APIs can be operated on remotely and a number of endpoints do not require authentication. The endpoints \"admin\" and \"builtin-users\" are limited to localhost out of the box by the settings :ref:`:BlockedApiEndpoints` and :ref:`:BlockedApiPolicy`.\n\nIt is very important to keep the block in place for the \"admin\" endpoint, and to leave the \"builtin-users\" endpoint blocked unless you need to access it remotely. Documentation for the \"admin\" endpoint is spread across the :doc:`/api/native-api` section of the API Guide and the :doc:`/admin/index`.\n\nIt's also possible to prevent file uploads via API by adjusting the :ref:`:UploadMethods` database setting.\n\nIf you are using a load balancer or a reverse proxy, there are some additional considerations. If no additional configurations are made and the upstream is configured to redirect to localhost, the API will be accessible from the outside, as your installation will register as origin the localhost for any requests to the endpoints \"admin\" and \"builtin-users\". To prevent this, you have two options:\n\nIf your upstream is configured to redirect to localhost, you will need to set the :ref:`JVM option <useripaddresssourceheader>` to one of the following values %client.name% %datetime% %request% %status% %response.length% %header.referer% %header.x-forwarded-for% and configure from the load balancer side the chosen header to populate with the client IP address.\n\nAnother solution is to set the upstream to the client IP address. In this case no further configuration is needed.\n\nTo avoid having your users send credentials in the clear, it's strongly recommended to force all web traffic to go through HTTPS (port 443) rather than HTTP (port 80). The ease with which one can install a valid SSL cert into Apache compared with the same operation in Payara might be a compelling enough reason to front Payara with Apache. In addition, Apache can be configured to rewrite HTTP to HTTPS with rules such as those found at https://wiki.apache.org/httpd/RewriteHTTPToHTTPS or in the section on :doc:`shibboleth`.\n\nBy default, the Dataverse installation captures the IP address from which requests originate. This is used for multiple purposes including controlling access to the admin API, IP-based user groups and Make Data Count reporting. When the Dataverse installation is configured behind a proxy such as a load balancer, this default setup may not capture the correct IP address. In this case all the incoming requests will be logged in the access logs, MDC logs etc., as if they are all coming from the IP address(es) of the load balancer itself. Proxies usually save the original address in an added HTTP header, from which it can be extracted. For example, AWS LB records the \"true\" original address in the standard X-Forwarded-For header. If your Dataverse installation is running behind an IP-masking proxy, but you would like to use IP groups, or record the true geographical location of the incoming requests with Make Data Count, you may enable the IP address lookup from the proxy header using the JVM option dataverse.useripaddresssourceheader, described further below.\n\nBefore doing so however, you must absolutely consider the security risks involved! This option must be enabled only on a Dataverse installation that is in fact fully behind a proxy that properly, and consistently, adds the X-Forwarded-For (or a similar) header to every request it forwards. Consider the implications of activating this option on a Dataverse installation that is not running behind a proxy, or running behind one, but still accessible from the insecure locations bypassing the proxy: Anyone can now add the header above to an incoming request, supplying an arbitrary IP address that the Dataverse installation will trust as the true origin of the call. Thus giving an attacker an easy way to, for example, get in a privileged IP group. The implications could be even more severe if an attacker were able to pretend to be coming from localhost, if a Dataverse installation is configured to trust localhost connections for unrestricted access to the admin API! We have addressed this by making it so that Dataverse installation should never accept localhost, 127.0.0.1, 0:0:0:0:0:0:0:1 etc. when supplied in such a header. But if you have reasons to still find this risk unacceptable, you may want to consider turning open localhost access to the API off (See :ref:`Securing Your Installation <securing-your-installation>` for more information.)\n\nThis is how to verify that your proxy or load balancer, etc. is handling the originating address headers properly and securely: Make sure access logging is enabled in your application server (Payara) configuration. (<http-service access-logging-enabled=\"true\"> in the domain.xml). Add the address header to the access log format. For example, on a system behind AWS ELB, you may want to use something like %client.name% %datetime% %request% %status% %response.length% %header.referer% %header.x-forwarded-for%. Once enabled, access the Dataverse installation from outside the LB. You should now see the real IP address of your remote client in the access log. For example, something like: \"1.2.3.4\" \"01/Jun/2020:12:00:00 -0500\" \"GET /dataverse.xhtml HTTP/1.1\" 200 81082 \"NULL-REFERER\" \"128.64.32.16\"\n\nIn this example, 128.64.32.16 is your remote address (that you should verify), and 1.2.3.4 is the address of your LB. If you're not seeing your remote address in the log, do not activate the JVM option! Also, verify that all the entries in the log have this header populated. The only entries in the access log that you should be seeing without this header (logged as \"NULL-HEADER-X-FORWARDED-FOR\") are local requests, made from localhost, etc. In this case, since the request is not coming through the proxy, the local IP address should be logged as the primary one (as the first value in the log entry, %client.name%). If you see any requests coming in from remote, insecure subnets without this header - do not use the JVM option!\n\nOnce you are ready, enable the :ref:`JVM option <useripaddresssourceheader>`. Verify that the remote locations are properly tracked in your MDC metrics, and/or your IP groups are working. As a final test, if your Dataverse installation is allowing unrestricted localhost access to the admin API, imitate an attack in which a malicious request is pretending to be coming from 127.0.0.1. Try the following from a remote, insecure location:\n\ncurl https://your.dataverse.edu/api/admin/settings --header \"X-FORWARDED-FOR: 127.0.0.1\"\n\nFirst of all, confirm that access is denied! If you are in fact able to access the settings api from a location outside the proxy, something is seriously wrong, so please let us know, and stop using the JVM option. Otherwise check the access log entry for the header value. What you should see is something like \"127.0.0.1, 128.64.32.16\". Where the second address should be the real IP of your remote client. The fact that the \"fake\" 127.0.0.1 you sent over is present in the header is perfectly ok. This is the proper proxy behavior - it preserves any incoming values in the X-Forwarded-Header, if supplied, and adds the detected incoming address to it, on the right. It is only this rightmost comma-separated value that Dataverse installation should ever be using.\n\nStill feel like activating this option in your configuration? - Have fun and be safe!\n\nOut of the box, your Dataverse installation will list email addresses of the contacts for datasets when users visit a dataset page and click the \"Export Metadata\" button. Additionally, out of the box, the Dataverse installation will list email addresses of Dataverse collection contacts via API (see :ref:`View a Dataverse Collection <view-dataverse>` in the :doc:`/api/native-api` section of the API Guide). If you would like to exclude these email addresses from export, set :ref:`:ExcludeEmailFromExport <:ExcludeEmailFromExport>` to true.\n\nSee the :ref:`payara` section of :doc:`prerequisites` for details and init scripts for running Payara as non-root.\n\nRelated to this is that you should remove /root/.payara/pass to ensure that Payara isn't ever accidentally started as root. Without the password, Payara won't be able to start as root, which is a good thing.\n\nIn development or demo scenarios, we suggest not to store passwords in files permanently. We recommend the use of at least environment variables or production-grade mechanisms to supply passwords.\n\nIn a production setup, permanently storing passwords as plaintext should be avoided at all cost. Environment variables are dangerous in shared environments and containers, as they may be easily exploited; we suggest not to use them. Depending on your deployment model and environment, you can make use of the following techniques to securely store and access passwords.\n\nPassword Aliases\n\nA password alias allows you to have a plaintext reference to an encrypted password stored on the server, with the alias being used wherever the password is needed. This method is especially useful in a classic deployment, as it does not require any external secrets management.\n\nPassword aliases are consumable as a MicroProfile Config source and can be referrenced by their name in a property expression. You may also reference them within a variable substitution, e.g. in your domain.xml.\n\nCreation example for an alias named my.alias.name:\n\nNote: omitting the --passwordfile parameter allows creating the alias in an interactive fashion with a prompt.\n\nSecrets Files\n\nPayara has a builtin MicroProfile Config source to consume values from files in a directory on your filesystem. This directory config source is most useful and secure with external secrets management in place, temporarily mounting cleartext passwords as files. Examples are Kubernetes / OpenShift Secrets or tools like Vault Agent.\n\nPlease follow the directory config source documentation to learn about its usage.\n\nCloud Providers\n\nRunning Dataverse on a cloud platform or running an external secret management system like Vault enables accessing secrets without any intermediate storage of cleartext. Obviously this is the most secure option for any deployment model, but it may require more resources to set up and maintain - your mileage may vary.\n\nTake a look at cloud sources shipped with Payara to learn about their usage.\n\nYour Dataverse installation only stores passwords (as salted hash, and using a strong hashing algorithm) for \"builtin\" users. You can increase the password complexity rules to meet your security needs. If you have configured your Dataverse installation to allow login from remote authentication providers such as Shibboleth, ORCID, GitHub or Google, you do not have any control over those remote providers' password complexity rules. See the :ref:`auth-modes` section below for more on login options.\n\nEven if you are satisfied with the out-of-the-box password complexity rules the Dataverse Software ships with, for the \"dataverseAdmin\" account you should use a strong password so the hash cannot easily be cracked through dictionary attacks.\n\nPassword complexity rules for \"builtin\" accounts can be adjusted with a variety of settings documented below. Here's a list:\n\n:ref:`:PVMinLength`\n\n:ref:`:PVMaxLength`\n\n:ref:`:PVNumberOfConsecutiveDigitsAllowed`\n\n:ref:`:PVCharacterRules`\n\n:ref:`:PVNumberOfCharacteristics`\n\n:ref:`:PVDictionaries`\n\n:ref:`:PVGoodStrength`\n\n:ref:`:PVCustomPasswordResetAlertMessage`\n\nLike any application, you should keep up-to-date with patches to both the Dataverse software and the platform (usually Linux) it runs on. Dataverse releases are announced on the dataverse-community mailing list, the Dataverse blog, and in chat.dataverse.org.\n\nIn addition to these public channels, you can subscribe to receive security notices via email from the Dataverse team. These notices are sent to the contact_email in the installation spreadsheet and you can open an issue in the dataverse-installations repo to add or change the contact email. Security notices are also sent to people and organizations that prefer to remain anonymous. To be added to this private list, please email support@dataverse.org.\n\nFor additional details about security practices by the Dataverse team, see the :doc:`/developers/security` section of the Developer Guide.\n\nIf you have a security issue to report, please email it to security@dataverse.org.\n\nRemember how under \"Decisions to Make\" in the :doc:`prep` section we mentioned you'll need to make a decision about whether or not to introduce a proxy in front of the Dataverse Software such as Apache or nginx? The time has come to make that decision.\n\nThe need to redirect port HTTP (port 80) to HTTPS (port 443) for security has already been mentioned above and the fact that Payara puts these services on 8080 and 8181, respectively, was touched on in the :doc:`installation-main` section. In production, you don't want to tell your users to use your Dataverse installation on ports 8080 and 8181. You should have them use the standard HTTPS port, which is 443.\n\nYour decision to proxy or not should primarily be driven by which features of the Dataverse Software you'd like to use. If you'd like to use Shibboleth, the decision is easy because proxying or \"fronting\" Payara with Apache is required. The details are covered in the :doc:`shibboleth` section.\n\nEven if you have no interest in Shibboleth, you may want to front your Dataverse installation with Apache or nginx to simply the process of installing SSL certificates. There are many tutorials on the Internet for adding certs to Apache, including a some notes used by the Dataverse Project team, but the process of adding a certificate to Payara is arduous and not for the faint of heart. The Dataverse Project team cannot provide much help with adding certificates to Payara beyond linking to tips on the web.\n\nStill not convinced you should put Payara behind another web server? Even if you manage to get your SSL certificate into Payara, how are you going to run Payara on low ports such as 80 and 443? Are you going to run Payara as root? Bad idea. This is a security risk. Under \"Additional Recommendations\" under \"Securing Your Installation\" above you are advised to configure Payara to run as a user other than root.\n\nThere's also the issue of serving a production-ready version of robots.txt. By using a proxy such as Apache, this is a one-time \"set it and forget it\" step as explained below in the \"Going Live\" section.\n\nIf you are convinced you'd like to try fronting Payara with Apache, the :doc:`shibboleth` section should be good resource for you.\n\nIf you really don't want to front Payara with any proxy (not recommended), you can configure Payara to run HTTPS on port 443 like this:\n\n./asadmin set server-config.network-config.network-listeners.network-listener.http-listener-2.port=443\n\nWhat about port 80? Even if you don't front your Dataverse installation with Apache, you may want to let Apache run on port 80 just to rewrite HTTP to HTTPS as described above. You can use a similar command as above to change the HTTP port that Payara uses from 8080 to 80 (substitute http-listener-1.port=80). Payara can be used to enforce HTTPS on its own without Apache, but configuring this is an exercise for the reader. Answers here may be helpful: https://stackoverflow.com/questions/25122025/glassfish-v4-java-7-port-unification-error-not-able-to-redirect-http-to\n\nIf you are running an installation with Apache and Payara on the same server, and would like to restrict Payara from responding to any requests to port 8080 from external hosts (in other words, not through Apache), you can restrict the AJP listener to localhost only with:\n\n./asadmin set server-config.network-config.network-listeners.network-listener.http-listener-1.address=127.0.0.1\n\nYou should NOT use the configuration option above if you are running in a load-balanced environment, or otherwise have the web server on a different host than the application server.\n\nThe user who creates a Dataverse collection is given the \"Admin\" role on that Dataverse collection. The root Dataverse collection is created automatically for you by the installer and the \"Admin\" is the superuser account (\"dataverseAdmin\") we used in the :doc:`installation-main` section to confirm that we can log in. These next steps of configuring the root Dataverse collection require the \"Admin\" role on the root Dataverse collection, but not the much more powerful superuser attribute. In short, users with the \"Admin\" role are subject to the permission system. A superuser, on the other hand, completely bypasses the permission system. You can give non-superusers the \"Admin\" role on the root Dataverse collection if you'd like them to configure the root Dataverse collection.\n\nIn order for non-superusers to start creating Dataverse collections or datasets, you need click \"Edit\" then \"Permissions\" and make choices about which users can add Dataverse collections or datasets within the root Dataverse collection. (There is an API endpoint for this operation as well.) Again, the user who creates a Dataverse collection will be granted the \"Admin\" role on that Dataverse collection. Non-superusers who are not \"Admin\" on the root Dataverse collection will not be able to do anything useful until the root Dataverse collection has been published.\n\nAs the person installing the Dataverse Software, you may or may not be a local metadata expert. You may want to have others sign up for accounts and grant them the \"Admin\" role at the root Dataverse collection to configure metadata fields, templates, browse/search facets, guestbooks, etc. For more on these topics, consult the :doc:`/user/dataverse-management` section of the User Guide.\n\nPersistent identifiers (PIDs) are a required and integral part of the Dataverse Software. They provide a URL that is guaranteed to resolve to the datasets or files they represent. The Dataverse Software currently supports creating identifiers using any of several PID types. The most appropriate PIDs for public data are DOIs (e.g., provided by DataCite or EZID) and Handles. Dataverse also supports PermaLinks which could be useful for intranet or catalog use cases. A DOI provider called \"FAKE\" is recommended only for testing and development purposes.\n\nDataverse can be configured with one or more PID providers, each of which can mint and manage PIDs with a given protocol (e.g., doi, handle, permalink) using a specific service provider/account (e.g. with DataCite, EZId, or HandleNet) to manage an authority/shoulder combination, aka a \"prefix\" (PermaLinks also support custom separator characters as part of the prefix), along with an optional list of individual PIDs (with different authority/shoulders) than can be managed with that account.\n\nBy default, the installer configures the Fake DOI provider as the registration provider. Unlike other DOI Providers, the Fake Provider does not involve any external resolution service and is not appropriate for use beyond development and testing. You may wish instead to test with PermaLinks or with a DataCite test account (which uses DataCite's test infrastructure and will help assure your Dataverse instance can make network connections to DataCite. DataCite requires that you register for a test account, which will have a username, password and your own prefix (please contact support@datacite.org for a test account. You may wish to contact the GDCC instead - GDCC is able to provide DataCite accounts with a group discount and can also provide test accounts.).\n\nOnce you receive the login name, password, and prefix for the account, configure the credentials as described below.\n\nAlternately, you may wish to configure other providers for testing:\n\nEZID is available to University of California scholars and researchers. Testing can be done using the authority 10.5072 and shoulder FK2 with the \"apitest\" account (contact EZID for credentials) or an institutional account. Configuration in Dataverse is then analogous to using DataCite.\n\nThe PermaLink provider, like the FAKE DOI provider, does not involve an external account. Unlike the Fake DOI provider, the PermaLink provider creates PIDs that begin with \"perma:\", making it clearer that they are not DOIs, and that do resolve to the local dataset/file page in Dataverse, making them useful for some production use cases. See :ref:`permalinks` and (for the FAKE DOI provider) the :doc:`/developers/dev-environment` section of the Developer Guide.\n\nProvider-specific configuration is described below.\n\nOnce all is configured, you will be able to publish datasets and files, but the persistent identifiers will not be citable as they, with the exception of PermaLinks, will not redirect to your dataset page in Dataverse.\n\nNote that any datasets or files created using a test configuration cannot be directly migrated to a production PID provider and would need to be created again once a valid PID Provider(s) are configured.\n\nOne you are done testing, to properly configure persistent identifiers for a production installation, an account and associated namespace (e.g. authority/shoulder) must be acquired for a fee from a DOI or HDL provider. (As noted above, PermaLinks May be appropriate for intranet and catalog uses cases.) DataCite (https://www.datacite.org) is the recommended DOI provider (see https://dataversecommunity.global for more on joining DataCite through the Global Dataverse Community Consortium) but EZID (http://ezid.cdlib.org) is an option for the University of California according to https://www.cdlib.org/cdlinfo/2017/08/04/ezid-doi-service-is-evolving/ . Handle.Net (https://www.handle.net) is the HDL provider.\n\nOnce you have your DOI or Handle account credentials and a prefix, configure your Dataverse installation using the settings below.\n\nThere are two required global settings to configure PID providers - the list of ids of providers and which one of those should be the default. Per-provider settings are also required - some that are common to all types and some type specific. All of these settings are defined to be compatible with the MicroProfile specification which means that\n\nAny of these settings can be set via system properties (see :ref:`jvm-options` for how to do this), environment variables, or other MicroProfile Config mechanisms supported by the app server. See Payara docs for supported sources.\n\nRemember to protect your secrets. For passwords, use an environment variable (bare minimum), a password alias named the same as the key (OK) or use the \"dir config source\" of Payara (best).\n\nAlias creation example:\n\necho \"AS_ADMIN_ALIASPASSWORD=changeme\" > /tmp/p.txt asadmin create-password-alias --passwordfile /tmp/p.txt dataverse.pid.datacite1.datacite.password rm /tmp/p.txt\n\nEnvironment variables follow the key, replacing any dot, colon, dash, etc. into an underscore \"_\" and all uppercase letters. Example: dataverse.pid.default-provider -> DATAVERSE_PID_DEFAULT_PROVIDER\n\nThe following three global settings are required to configure PID Providers in the Dataverse software:\n\nA comma-separated list of the ids of the PID providers to use. IDs should be simple unique text strings, e.g. datacite1, perma1, etc. IDs are used to scope the provider-specific settings but are not directly visible to users.\n\nThe ID of the default PID provider to use.\n\nThe path to the directory where JAR files containing additional types of PID Providers can be added. Dataverse includes providers that support DOIs (DataCite, EZId, or FAKE), Handles, and PermaLinks. PID provider jar files added to this directory can replace any of these or add new PID Providers.\n\nEach Provider listed by id in the dataverse.pid.providers setting must be configured with the following common settings and any settings that are specific to the provider type.\n\nThe Provider type, currently one of datacite, ezid, FAKE, hdl, or perma. The type defines which protocol a service supports (DOI, Handle, or PermaLink) and, for DOI Providers, which DOI service is used.\n\nA human-readable label for the provider\n\nIn general, PIDs are of the form <protocol>:<authority>/<shoulder>* where * is the portion unique to an individual PID. PID Providers must define the authority and shoulder (with the protocol defined by the dataverse.pid.*.type setting) that defines the set of existing PIDs they can manage and the prefix they can use when minting new PIDs. (Often an account with a PID service provider will be limited to using a single authority/shoulder. If your PID service provider account allows more than one combination that you wish to use in Dataverse, configure multiple PID Provider, one for each combination.)\n\nBy default, Pid Providers in Dataverse generate a random 6 character string, pre-pended by the Shoulder if set, to use as the identifier for a Dataset. Set this to storedProcGenerated to generate instead a custom unique identifier (again pre-pended by the Shoulder if set) through a database stored procedure or function (the assumed default setting is randomString). When using the storedProcGenerated setting, a stored procedure or function must be created in the database.\n\nAs a first example, the script below (downloadable :download:`here </_static/util/createsequence.sql>`) produces sequential numerical values. You may need to make some changes to suit your system setup, see the comments for more information:\n\n.. literalinclude:: ../_static/util/createsequence.sql :language: plpgsql\n\nAs a second example, the script below (downloadable :download:`here </_static/util/identifier_from_timestamp.sql>`) produces sequential 8 character identifiers from a base36 representation of current timestamp.\n\n.. literalinclude:: ../_static/util/identifier_from_timestamp.sql :language: plpgsql\n\nNote that the SQL in these examples scripts is Postgres-specific. If necessary, it can be reimplemented in any other SQL flavor - the standard JPA code in the application simply expects the database to have a saved function (\"stored procedure\") named generateIdentifierFromStoredProcedure() returning a single varchar argument.\n\nPlease note that this setting interacts with the dataverse.pid.*.datafile-pid-format setting below to determine how datafile identifiers are generated.\n\nThis setting controls the way that the \"identifier\" component of a file's persistent identifier (PID) relates to the PID of its \"parent\" dataset - for a give PID Provider.\n\nBy default the identifier for a file is dependent on its parent dataset. For example, if the identifier of a dataset is \"TJCLKP\", the identifier for a file within that dataset will consist of the parent dataset's identifier followed by a slash (\"/\"), followed by a random 6 character string, yielding \"TJCLKP/MLGWJO\". Identifiers in this format are what you should expect if you leave dataverse.pid.*.datafile-pid-format undefined or set it to DEPENDENT and have not changed the dataverse.pid.*.identifier-generation-style setting from its default.\n\nAlternatively, the identifier for File PIDs can be configured to be independent of Dataset PIDs using the setting INDEPENDENT. In this case, file PIDs will not contain the PIDs of their parent datasets, and their PIDs will be generated the exact same way that datasets' PIDs are, based on the dataverse.pid.*.identifier-generation-style setting described above (random 6 character strings or custom unique identifiers through a stored procedure, pre-pended by any shoulder).\n\nThe chart below shows examples from each possible combination of parameters from the two settings. dataverse.pid.*.identifier-generation-style can be either randomString (the default) or storedProcGenerated and dataverse.pid.*.datafile-pid-format can be either DEPENDENT (the default) or INDEPENDENT. In the examples below the \"identifier\" for the dataset is \"TJCLKP\" for randomString and \"100001\" for storedProcGenerated (when using sequential numerical values, as described in :ref:`dataverse.pid.*.identifier-generation-style` above), or \"krby26qt\" for storedProcGenerated (when using base36 timestamps, as described in :ref:`dataverse.pid.*.identifier-generation-style` above).\n\nrandomString\n\nstoredProcGenerated\n\n(sequential numbers)\n\nstoredProcGenerated\n\n(base36 timestamps)\n\nDEPENDENT TJCLKP/MLGWJO 100001/1 krby26qt/1 INDEPENDENT MLGWJO 100002 krby27pz\n\nAs seen above, in cases where dataverse.pid.*.identifier-generation-style is set to storedProcGenerated and dataverse.pid.*.datafile-pid-format is set to DEPENDENT, each file within a dataset will be assigned a number within that dataset starting with \"1\".\n\nOtherwise, if dataverse.pid.*.datafile-pid-format is set to INDEPENDENT, each file within the dataset is assigned with a new PID which is the next available identifier provided from the database stored procedure. In our example: \"100002\" when using sequential numbers or \"krby27pz\" when using base36 timestamps.\n\nWith at least some PID services, it is possible for the authority(permission) to manage specific individual PIDs to be transferred between accounts. To handle these cases, the individual PIDs, written in the standard format, e.g. doi:10.5072/FK2ABCDEF can be added to the comma-separated managed or excluded list for a given provider. For entries on the managed- list, Dataverse will assume this PID Provider/account can update the metadata and landing URL for the PID at the service provider (even though it does not match the provider's authority/shoulder settings). Conversely, Dataverse will assume that PIDs on the excluded-list cannot be managed/updated by this provider (even though they match the provider's authority/shoulder settings). These settings are optional with the default assumption that these lists are empty.\n\nPID Providers of type datacite require four additional parameters that define how the provider connects to DataCite. DataCite has two APIs that are used in Dataverse:\n\nThe base URL of the DataCite MDS API, used to mint and manage DOIs. Current valid values for dataverse.pid.*.datacite.mds-api-url are \"https://mds.datacite.org\" (production) and \"https://mds.test.datacite.org\" (testing, the default).\n\nThe DataCite REST API is also used - :ref:`PIDs API <pids-api>` information retrieval and :doc:`/admin/make-data-count`. Current valid values for dataverse.pid.*.datacite.rest-api-url are \"https://api.datacite.org\" (production) and \"https://api.test.datacite.org\" (testing, the default).\n\nDataCite uses HTTP Basic authentication for Fabrica and their APIs. You need to provide the same credentials (username, password) to Dataverse software to mint and manage DOIs for you. As noted above, you should use one of the more secure options for setting the password.\n\nNote that use of EZId is limited primarily to University of California institutions. If you have an EZId account, you will need to configure the api-url and your account username and password. As above, you should use one of the more secure options for setting the password.\n\nPermaLinks are a simple PID option intended for intranet and catalog use cases. They can be used without an external service or be configured with the base-url of a resolution service. PermaLinks also allow a custom separator to be used. (Note: when using multiple PermaLink providers, you should avoid ambiguous authority/separator/shoulder combinations that would result in the same overall prefix.)\n\nNote: If you are minting your own handles and plan to set up your own handle service, please refer to Handle.Net documentation.\n\nConfigure your Handle.net index to be used registering new persistent identifiers. Defaults to 300.\n\nIndices are used to separate concerns within the Handle system. To add data to an index, authentication is mandatory. See also chapter 1.4 \"Authentication\" of the Handle.Net Technical Documentation\n\nHandle.Net servers use a public key authentication method where the public key is stored in a handle itself and the matching private key is provided from this file. Typically, the absolute path ends like handle/svr_1/admpriv.bin. The key file may (and should) be encrypted with a passphrase (used for encryption with AES-128). See also chapter 1.4 \"Authentication\" of the Handle.Net Technical Documentation\n\nProvide an absolute key.path to a private key file authenticating requests to your Handle.Net server.\n\nProvide a key.passphrase to decrypt the private key file at dataverse.pid.*.handlenet.key.path.\n\nSet independent-service to true if you want to use a Handle service which is setup to work 'independently' (No communication with the Global Handle Registry). By default this setting is false.\n\nSet auth-handle to <prefix>/<suffix> to be used on a global handle service when the public key is NOT stored in the default handle. This setting is optional. If the public key is, for instance, stored in handle: 21.T12996/USER01, auth-handle should be set to this value.\n\nWhile using the PID Provider configuration settings described above is recommended, Dataverse installations only using a single PID Provider can use the settings below instead. In general, these legacy settings mirror those above except for not including a PID Provider id.\n\nHere are the configuration options for DOIs.:\n\nJVM Options for DataCite:\n\n:ref:`dataverse.pid.datacite.mds-api-url`\n\n:ref:`dataverse.pid.datacite.rest-api-url`\n\n:ref:`dataverse.pid.datacite.username`\n\n:ref:`dataverse.pid.datacite.password`\n\nJVM Options for EZID:\n\nAs stated above, with very few exceptions (e.g. University of California), you will not be able to use this provider.\n\n:ref:`dataverse.pid.ezid.api-url`\n\n:ref:`dataverse.pid.ezid.username`\n\n:ref:`dataverse.pid.ezid.password`\n\nDatabase Settings:\n\n:ref:`:DoiProvider <:DoiProvider>`\n\n:ref:`:Protocol <:Protocol>`\n\n:ref:`:Authority <:Authority>`\n\n:ref:`:Shoulder <:Shoulder>`\n\n:ref:`:IdentifierGenerationStyle <:IdentifierGenerationStyle>` (optional)\n\n:ref:`:DataFilePIDFormat <:DataFilePIDFormat>` (optional)\n\n:ref:`:FilePIDsEnabled <:FilePIDsEnabled>` (optional, defaults to false)\n\nHere are the configuration options for handles. Most notably, you need to change the :Protocol setting, as it defaults to DOI usage.\n\nJVM Options:\n\n:ref:`dataverse.pid.handlenet.key.path`\n\n:ref:`dataverse.pid.handlenet.key.passphrase`\n\n:ref:`dataverse.pid.handlenet.index`\n\nDatabase Settings:\n\n:ref:`:Protocol <:Protocol>`\n\n:ref:`:Authority <:Authority>`\n\n:ref:`:IdentifierGenerationStyle <:IdentifierGenerationStyle>` (optional)\n\n:ref:`:DataFilePIDFormat <:DataFilePIDFormat>` (optional)\n\n:ref:`:IndependentHandleService <:IndependentHandleService>` (optional)\n\n:ref:`:HandleAuthHandle <:HandleAuthHandle>` (optional)\n\nNote: If you are minting your own handles and plan to set up your own handle service, please refer to Handle.Net documentation.\n\nHere are the configuration options for PermaLinks:\n\nJVM Options:\n\n:ref:`dataverse.pid.permalink.base-url`\n\nDatabase Settings:\n\n:ref:`:Protocol <:Protocol>`\n\n:ref:`:Authority <:Authority>`\n\n:ref:`:Shoulder <:Shoulder>`\n\n:ref:`:IdentifierGenerationStyle <:IdentifierGenerationStyle>` (optional)\n\n:ref:`:DataFilePIDFormat <:DataFilePIDFormat>` (optional)\n\n:ref:`:FilePIDsEnabled <:FilePIDsEnabled>` (optional, defaults to false)\n\nYou must restart Payara after making changes to these settings.\n\nThere are three valid configurations or modes for authenticating users to your Dataverse installation:\n\nOut of the box, your Dataverse installation is configured in \"local only\" mode. The \"dataverseAdmin\" superuser account mentioned in the :doc:`/installation/installation-main` section is an example of a local account. Internally, these accounts are called \"builtin\" because they are built in to the Dataverse Software application itself.\n\nThe authenticationproviderrow database table controls which \"authentication providers\" are available within a Dataverse installation. Out of the box, a single row with an id of \"builtin\" will be present. For each user in a Dataverse installation, the authenticateduserlookup table will have a value under authenticationproviderid that matches this id. For example, the default \"dataverseAdmin\" user will have the value \"builtin\" under authenticationproviderid. Why is this important? Users are tied to a specific authentication provider but conversion mechanisms are available to switch a user from one authentication provider to the other. As explained in the :doc:`/user/account` section of the User Guide, a graphical workflow is provided for end users to convert from the \"builtin\" authentication provider to a remote provider. Conversion from a remote authentication provider to the builtin provider can be performed by a sysadmin with access to the \"admin\" API. See the :doc:`/api/native-api` section of the API Guide for how to list users and authentication providers as JSON.\n\nAdding and enabling a second authentication provider (:ref:`native-api-add-auth-provider` and :ref:`api-toggle-auth-provider`) will result in the Log In page showing additional providers for your users to choose from. By default, the Log In page will show the \"builtin\" provider, but you can adjust this via the :ref:`conf-default-auth-provider` configuration option. Further customization can be achieved by setting :ref:`conf-allow-signup` to \"false\", thus preventing users from creating local accounts via the web interface. Please note that local accounts can also be created through the API by enabling the builtin-users endpoint (:ref:`:BlockedApiEndpoints`) and setting the BuiltinUsers.KEY database setting (:ref:`BuiltinUsers.KEY`).\n\nTo configure Shibboleth see the :doc:`shibboleth` section and to configure OAuth see the :doc:`oauth2` section.\n\nAs for the \"Remote only\" authentication mode, it means that:\n\nShibboleth or OAuth has been enabled.\n\n:AllowSignUp is set to \"false\" to prevent users from creating local accounts via the web interface.\n\n:DefaultAuthProvider has been set to use the desired authentication provider\n\nThe \"builtin\" authentication provider has been disabled (:ref:`api-toggle-auth-provider`). Note that disabling the \"builtin\" authentication provider means that the API endpoint for converting an account from a remote auth provider will not work. Converting directly from one remote authentication provider to another (i.e. from GitHub to Google) is not supported. Conversion from remote is always to \"builtin\". Then the user initiates a conversion from \"builtin\" to remote. Note that longer term, the plan is to permit multiple login options to the same Dataverse installation account per #3487 (so all this talk of conversion will be moot) but for now users can only use a single login option, as explained in the :doc:`/user/account` section of the User Guide. In short, \"remote only\" might work for you if you only plan to use a single remote authentication provider such that no conversion between remote authentication providers will be necessary.\n\nBearer tokens are defined in RFC 6750 and can be used as an alternative to API tokens. This is an experimental feature hidden behind a feature flag.\n\nTo enable bearer tokens, you must install and configure Keycloak (for now, see :ref:`oidc-dev` in the Developer Guide) and enable api-bearer-auth under :ref:`feature-flags`.\n\nYou can test that bearer tokens are working by following the example under :ref:`bearer-tokens` in the API Guide.\n\nThe installer prompts you for some basic options to configure Dataverse to send email using your SMTP server, but in many cases, extra configuration may be necessary.\n\nMake sure the :ref:`dataverse.mail.system-email` has been set. Email will not be sent without it. A hint will be logged about this fact. If you want to separate system email from your support team's email, take a look at :ref:`dataverse.mail.support-email`.\n\nThen check the list of commonly used settings at the top of :ref:`dataverse.mail.mta`.\n\nIf you have trouble, consider turning on debugging with :ref:`dataverse.mail.debug`.\n\nThe Dataverse software uses a PostgreSQL database to store objects users create. You can configure basic and advanced settings for the PostgreSQL database connection with the help of MicroProfile Config API.\n\nAny of these settings can be set via system properties (see :ref:`jvm-options` starting at :ref:`dataverse.db.name`), environment variables or other MicroProfile Config mechanisms supported by the app server. See Payara docs for supported sources.\n\nRemember to protect your secrets. See :ref:`secure-password-storage` for more information.\n\nEnvironment variables follow the key, replacing any dot, colon, dash, etc. into an underscore \"_\" and all uppercase letters. Example: dataverse.db.host -> DATAVERSE_DB_HOST\n\nMPCONFIG Key Description Default dataverse.db.host The PostgreSQL server to connect to. localhost dataverse.db.port The PostgreSQL server port to connect to. 5432 dataverse.db.user The PostgreSQL user name to connect with.\n\ndataverse\n\n(installer sets to dvnapp)\n\ndataverse.db.password\n\nThe PostgreSQL users password to connect with.\n\nPlease note the safety advisory above.\n\nNo default dataverse.db.name The PostgreSQL database name to use for the Dataverse installation.\n\ndataverse\n\n(installer sets to dvndb)\n\ndataverse.db.parameters Connection parameters, such as sslmode=require. See Postgres JDBC docs Note: you don't need to provide the initial \"?\". Empty string\n\nThe following options are useful in many scenarios. You might be interested in debug output during development or monitoring performance in production.\n\nYou can find more details within the Payara docs:\n\nUser Guide: Connection Pool Configuration\n\nTech Doc: Advanced Connection Pool Configuration.\n\nMPCONFIG Key Description Default dataverse.db.is-connection-validation-required true: Validate connections, allow server to reconnect in case of failure. false dataverse.db.connection-validation-method\n\nThe method of connection validation:\n\ntable|autocommit|meta-data|custom-validation.\n\nEmpty string dataverse.db.validation-table-name The name of the table used for validation if the validation method is set to table. Empty string dataverse.db.validation-classname The name of the custom class used for validation if the validation-method is set to custom-validation. Empty string dataverse.db.validate-atmost-once-period-in-seconds Specifies the time interval in seconds between successive requests to validate a connection at most once. 0 (disabled)\n\nMPCONFIG Key Description Default dataverse.db.connection-leak-timeout-in-seconds Specify timeout when connections count as \"leaked\". 0 (disabled) dataverse.db.connection-leak-reclaim If enabled, leaked connection will be reclaimed by the pool after connection leak timeout occurs. false dataverse.db.statement-leak-timeout-in-seconds Specifiy timeout when statements should be considered to be \"leaked\". 0 (disabled) dataverse.db.statement-leak-reclaim If enabled, leaked statement will be reclaimed by the pool after statement leak timeout occurs. false\n\nMPCONFIG Key Description Default dataverse.db.statement-timeout-in-seconds Timeout property of a connection to enable termination of abnormally long running queries. -1 (disabled) dataverse.db.slow-query-threshold-in-seconds SQL queries that exceed this time in seconds will be logged. -1 (disabled) dataverse.db.log-jdbc-calls When set to true, all JDBC calls will be logged allowing tracing of all JDBC interactions including SQL. false\n\nBy default, a Dataverse installation stores all data files (files uploaded by end users) on the filesystem at /usr/local/payara6/glassfish/domains/domain1/files. This path can vary based on answers you gave to the installer (see the :ref:`dataverse-installer` section of the Installation Guide) or afterward by reconfiguring the dataverse.files.\\<id\\>.directory JVM option described below.\n\nA Dataverse installation can alternately store files in a Swift or S3-compatible object store, or on a Globus endpoint, and can now be configured to support multiple stores at once. With a multi-store configuration, the location for new files can be controlled on a per-Dataverse collection basis.\n\nA Dataverse installation may also be configured to reference some files (e.g. large and/or sensitive data) stored in a web or Globus accessible trusted remote store.\n\nA Dataverse installation can be configured to allow out of band upload by setting the dataverse.files.\\<id\\>.upload-out-of-band JVM option to true. By default, Dataverse supports uploading files via the :ref:`add-file-api`. With S3 stores, a direct upload process can be enabled to allow sending the file directly to the S3 store (without any intermediate copies on the Dataverse server). With the upload-out-of-band option enabled, it is also possible for file upload to be managed manually or via third-party tools, with the :ref:`Adding the Uploaded file to the Dataset <direct-add-to-dataset-api>` API call (described in the :doc:`/developers/s3-direct-upload-api` page) used to add metadata and inform Dataverse that a new file has been added to the relevant store.\n\nThe following sections describe how to set up various types of stores and how to configure for multiple stores.\n\nTo support multiple stores, a Dataverse installation now requires an id, type, and label for each store (even for a single store configuration). These are configured by defining two required jvm options:\n\n./asadmin $ASADMIN_OPTS create-jvm-options \"\\-Ddataverse.files.<id>.type=<type>\" ./asadmin $ASADMIN_OPTS create-jvm-options \"\\-Ddataverse.files.<id>.label=<label>\"\n\nOut of the box, a Dataverse installation is configured to use local file storage in the 'file' store by default. You can add additional stores and, as a superuser, configure specific Dataverse collections to use them (by editing the 'General Information' for the Dataverse collection as described in the :doc:`/admin/dataverses-datasets` section).\n\nNote that the \"-Ddataverse.files.directory\", if defined, continues to control where temporary files are stored (in the /temp subdir of that directory), independent of the location of any 'file' store defined above. (See also the option reference: :ref:`dataverse.files.directory`)\n\nIf you wish to change which store is used by default, you'll need to delete the existing default storage driver and set a new one using jvm options.\n\n./asadmin $ASADMIN_OPTS delete-jvm-options \"-Ddataverse.files.storage-driver-id=file\" ./asadmin $ASADMIN_OPTS create-jvm-options \"-Ddataverse.files.storage-driver-id=<id>\"\n\nIt is also possible to set maximum file upload size limits per store. See the :ref:`:MaxFileUploadSizeInBytes` setting below.\n\nIf you find yourself adding many file stores with various configurations such as per-file limits and direct upload, you might find it helpful to make the label descriptive.\n\nFor example, instead of simply labeling an S3 store as \"S3\"...\n\n./asadmin create-jvm-options \"\\-Ddataverse.files.s3xl.label=S3\"\n\n... you might want to include some extra information such as the example below.\n\n./asadmin create-jvm-options \"\\-Ddataverse.files.s3xl.label=S3XL, Filesize limit: 100GB, direct-upload\"\n\nPlease keep in mind that the UI will only show so many characters, so labels are best kept short.\n\nFile stores have one option - the directory where files should be stored. This can be set using\n\n./asadmin $ASADMIN_OPTS create-jvm-options \"\\-Ddataverse.files.<id>.directory=<file directory>\"\n\nMultiple file stores should specify different directories (which would nominally be the reason to use multiple file stores), but one may share the same directory as \"-Ddataverse.files.directory\" option - this would result in temp files being stored in the /temp subdirectory within the file store's root directory.\n\nRather than storing data files on the filesystem, you can opt for an experimental setup with a Swift Object Storage backend. Each dataset that users create gets a corresponding \"container\" on the Swift side, and each data file is saved as a file within that container.\n\nIn order to configure a Swift installation, you need to complete these steps to properly modify the JVM options:\n\nFirst, run all the following create commands with your Swift endpoint information and credentials:\n\n./asadmin $ASADMIN_OPTS create-jvm-options \"\\-Ddataverse.files.<id>.type=swift\" ./asadmin $ASADMIN_OPTS create-jvm-options \"\\-Ddataverse.files.<id>.defaultEndpoint=endpoint1\" ./asadmin $ASADMIN_OPTS create-jvm-options \"\\-Ddataverse.files.<id>.authType.endpoint1=your-auth-type\" ./asadmin $ASADMIN_OPTS create-jvm-options \"\\-Ddataverse.files.<id>.authUrl.endpoint1=your-auth-url\" ./asadmin $ASADMIN_OPTS create-jvm-options \"\\-Ddataverse.files.<id>.tenant.endpoint1=your-tenant-name\" ./asadmin $ASADMIN_OPTS create-jvm-options \"\\-Ddataverse.files.<id>.username.endpoint1=your-username\" ./asadmin $ASADMIN_OPTS create-jvm-options \"\\-Ddataverse.files.<id>.endpoint.endpoint1=your-swift-endpoint\"\n\nauth_type can either be keystone, keystone_v3, or it will assumed to be basic. auth_url should be your keystone authentication URL which includes the tokens (e.g. for keystone, https://openstack.example.edu:35357/v2.0/tokens and for keystone_v3, https://openstack.example.edu:35357/v3/auth/tokens). swift_endpoint is a URL that looks something like https://rdgw.swift.example.org/swift/v1.\n\nThen create a password alias by running (without changes):\n\n./asadmin $ASADMIN_OPTS create-jvm-options \"\\-Ddataverse.files.swift.password.endpoint1='\\${ALIAS=swiftpassword-alias}'\" ./asadmin $ASADMIN_OPTS create-password-alias swiftpassword-alias\n\nThe second command will trigger an interactive prompt asking you to input your Swift password.\n\nNote: you may choose a different way to secure this password, depending on your use case. See :ref:`secure-password-storage` for more options.\n\nSecond, update the JVM option dataverse.files.storage-driver-id by running the delete command:\n\n./asadmin $ASADMIN_OPTS delete-jvm-options \"\\-Ddataverse.files.storage-driver-id=file\"\n\nThen run the create command:\n\n./asadmin $ASADMIN_OPTS create-jvm-options \"\\-Ddataverse.files.storage-driver-id=swift\"\n\nYou also have the option to set a custom container name separator. It is initialized to _, but you can change it by running the create command:\n\n./asadmin $ASADMIN_OPTS create-jvm-options \"\\-Ddataverse.files.swift.folderPathSeparator=-\"\n\nBy default, your Swift installation will be public-only, meaning users will be unable to put access restrictions on their data. If you are comfortable with this level of privacy, the final step in your setup is to set the :ref:`:PublicInstall` setting to true.\n\nIn order to enable file access restrictions, you must enable Swift to use temporary URLs for file access. To enable usage of temporary URLs, set a hash key both on your swift endpoint and in your swift.properties file. You can do so by running the create command:\n\n./asadmin $ASADMIN_OPTS create-jvm-options \"\\-Ddataverse.files.swift.hashKey.endpoint1=your-hash-key\"\n\nYou also have the option to set a custom expiration length, in seconds, for a generated temporary URL. It is initialized to 60 seconds, but you can change it by running the create command:\n\n./asadmin $ASADMIN_OPTS create-jvm-options \"\\-Ddataverse.files.swift.temporaryUrlExpiryTime=3600\"\n\nIn this example, you would be setting the expiration length for one hour.\n\nOnce you have configured a Swift Object Storage backend, you also have the option of enabling a connection to a computing environment. To do so, you need to configure the database settings for :ref:`:ComputeBaseUrl` and :ref:`:CloudEnvironmentName`.\n\nOnce you have set up :ComputeBaseUrl properly in both the Dataverse installation and your cloud environment, validated users will have three options for accessing the computing environment:\n\nCompute on a single dataset\n\nCompute on multiple datasets\n\nCompute on a single datafile\n\nThe compute tool options on dataset and file pages will link validated users to your computing environment. If a user is computing on one dataset, the compute tool option will redirect to:\n\n:ComputeBaseUrl?datasetPersistentId\n\nIf a user is computing on multiple datasets, the compute tool option will redirect to:\n\n:ComputeBaseUrl/multiparty?datasetPersistentId&anotherDatasetPersistentId&anotherDatasetPersistentId&...\n\nIf a user is computing on a single file, depending on the configuration of your installation, the compute tool option will either redirect to:\n\n:ComputeBaseUrl?datasetPersistentId=yourObject\n\nif your installation's :ref:`:PublicInstall` setting is true, or:\n\n:ComputeBaseUrl?datasetPersistentId=yourObject&temp_url_sig=yourTempUrlSig&temp_url_expires=yourTempUrlExpiry\n\nYou can configure this redirect properly in your cloud environment to generate a temporary URL for access to the Swift objects for computing.\n\nThe Dataverse Software supports Amazon S3 storage as well as other S3-compatible stores (like Minio, Ceph RADOS S3 Gateway and many more) for files uploaded to your Dataverse installation.\n\nThe Dataverse Software S3 driver supports multi-part upload for large files (over 1 GB by default - see the min-part-size option in the table below to change this).\n\nNote: The Dataverse Project Team is most familiar with AWS S3, and can provide support on its usage with the Dataverse Software. Thanks to community contributions, the application's architecture also allows non-AWS S3 providers. The Dataverse Project Team can provide very limited support on these other providers. We recommend reaching out to the wider Dataverse Project Community if you have questions.\n\nThe Dataverse Software and the AWS SDK make use of the \"AWS credentials profile file\" and \"AWS config profile file\" located in ~/.aws/ where ~ is the home directory of the user you run Payara as. This file can be generated via either of two methods described below:\n\nManually through creation of the credentials and config files or\n\nAutomatically via the AWS console commands.\n\nSome usage scenarios might be eased without generating these files. You may also provide :ref:`static credentials via MicroProfile Config <s3-mpconfig>`, see below.\n\nYou'll need an AWS account with an associated S3 bucket for your installation to use. From the S3 management console (e.g. https://console.aws.amazon.com/), you can poke around and get familiar with your bucket.\n\nMake note of the bucket's name and the region its data is hosted in.\n\nTo create a user with full S3 access and nothing more for security reasons, we recommend using IAM (Identity and Access Management). See IAM User Guide for more info on this process.\n\nTo use programmatic access, Generate the user keys needed for a Dataverse installation afterwards by clicking on the created user. (You can skip this step when running on EC2, see below.)\n\nTip\n\nIf you are hosting your Dataverse installation on an AWS EC2 instance alongside storage in S3, it is possible to use IAM Roles instead of the credentials file (the file at ~/.aws/credentials mentioned below). Please note that you will still need the ~/.aws/config file to specify the region. For more information on this option, see https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html\n\nWe assume you have your S3-compatible custom storage in place, up and running, ready for service.\n\nPlease make note of the following details:\n\nEndpoint URL - consult the documentation of your service on how to find it.\n\nExample: https://play.minio.io:9000\n\nRegion: Optional, but some services might use it. Consult your service documentation.\n\nExample: us-east-1\n\nAccess key ID and secret access key: Usually you can generate access keys within the user profile of your service.\n\nExample:\n\nID: Q3AM3UQ867SPQQA43P2F\n\nKey: zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG\n\nBucket name: The Dataverse installation will fail opening and uploading files on S3 if you don't create one.\n\nExample: dataverse\n\nTo create the ~/.aws/credentials file manually, you will need to generate a key/secret key (see above). Once you have acquired the keys, they need to be added to the credentials file. The format for credentials is as follows:\n\n[default] aws_access_key_id = <insert key, no brackets> aws_secret_access_key = <insert secret key, no brackets>\n\nWhile using Amazon's service, you must also specify the AWS region in the ~/.aws/config file, for example:\n\n[default] region = us-east-1\n\nAdditional profiles can be added to these files by appending the relevant information in additional blocks:\n\n[default] aws_access_key_id = <insert key, no brackets> aws_secret_access_key = <insert secret key, no brackets> [profilename2] aws_access_key_id = <insert key, no brackets> aws_secret_access_key = <insert secret key, no brackets>\n\nPlace these two files in a folder named .aws under the home directory for the user running your Dataverse Installation on Payara. (From the AWS Command Line Interface Documentation: \"In order to separate credentials from less sensitive options, region and output format are stored in a separate file named config in the same folder\")\n\nBegin by installing the CLI tool pip (package installer for Python) to install the AWS command line interface if you don't have it.\n\nFirst, we'll get our access keys set up. If you already have your access keys configured, skip this step. From the command line, run:\n\npip install awscli\n\naws configure\n\nYou'll be prompted to enter your Access Key ID and secret key, which should be issued to your AWS account. The subsequent config steps after the access keys are up to you. For reference, the keys will be stored in ~/.aws/credentials, and your AWS access region in ~/.aws/config.\n\nTIP: When using a custom S3 URL endpoint, you need to add it to every aws call: aws --endpoint-url <URL> s3 ...\n\n(you may omit it while configuring).\n\nTo set up an S3 store, you must define the id, type, and label as for any store:\n\nThen, we'll need to identify which S3 bucket we're using. Replace <your_bucket_name> with, of course, your bucket:\n\n./asadmin create-jvm-options \"-Ddataverse.files.<id>.bucket-name=<your_bucket_name>\"\n\nOptionally, you can have users download files from S3 directly rather than having files pass from S3 through Payara to your users. To accomplish this, set dataverse.files.<id>.download-redirect to true like this:\n\n./asadmin create-jvm-options \"-Ddataverse.files.<id>.download-redirect=true\"\n\nIf you enable dataverse.files.<id>.download-redirect as described above, note that the S3 URLs expire after an hour by default but you can configure the expiration time using the dataverse.files.<id>.url-expiration-minutes JVM option. Here's an example of setting the expiration time to 120 minutes:\n\n./asadmin create-jvm-options \"-Ddataverse.files.<id>.url-expiration-minutes=120\"\n\nBy default, your store will use the [default] profile in you .aws configuration files. To use a different profile, which would be necessary if you have two s3 stores at different locations, you can specify the profile to use:\n\n./asadmin create-jvm-options \"-Ddataverse.files.<id>.profile=<profilename>\"\n\nLarger installations may want to increase the number of open S3 connections allowed (default is 256): For example,\n\n./asadmin create-jvm-options \"-Ddataverse.files.<id>.connection-pool-size=4096\"\n\nBy default, when direct upload to an S3 store is configured, Dataverse will place a temp tag on the file being uploaded for an easier cleanup in case the file is not added to the dataset after upload (e.g., if the user cancels the operation). (See :ref:`s3-tags-and-direct-upload`.) If your S3 store does not support tagging and gives an error when direct upload is configured, you can disable the tagging by using the dataverse.files.<id>.disable-tagging JVM option. For example:\n\n./asadmin create-jvm-options \"-Ddataverse.files.<id>.disable-tagging=true\"\n\nDisabling the temp tag makes it harder to identify abandoned files that are not used by your Dataverse instance (i.e. one cannot search for the temp tag in a delete script). These should still be removed to avoid wasting storage space. To clean up these files and any other leftover files, regardless of whether the temp tag is applied, you can use the :ref:`cleanup-storage-api` API endpoint.\n\nNote that if you disable tagging, you should should omit the x-amz-tagging:dv-state=temp header when using the :doc:`/developers/s3-direct-upload-api`, as noted in that section.\n\nIn case you would like to configure Dataverse to use a custom S3 service instead of Amazon S3 services, please add the options for the custom URL and region as documented below. Please read above if your desired combination has been tested already and what other options have been set for a successful integration.\n\nLastly, go ahead and restart your Payara server. With Dataverse deployed and the site online, you should be able to upload datasets and data files and see the corresponding files in your S3 bucket. Within a bucket, the folder structure emulates that found in local file storage.\n\nJVM Option Value Description Default value dataverse.files.storage-driver-id <id> Enable <id> as the default storage driver. file dataverse.files.<id>.type s3 Required to mark this storage as S3 based. (none) dataverse.files.<id>.label <?> Required label to be shown in the UI for this storage (none) dataverse.files.<id>.bucket-name <?> The bucket name. See above. (none) dataverse.files.<id>.download-redirect true/false Enable direct download or proxy through Dataverse. false dataverse.files.<id>.upload-redirect true/false Enable direct upload of files added to a dataset in the S3 store. false dataverse.files.<id>.upload-out-of-band true/false Allow upload of files by out-of-band methods (using some tool other than Dataverse) false dataverse.files.<id>.ingestsizelimit <size in bytes> Maximum size of directupload files that should be ingested (none) dataverse.files.<id>.url-expiration-minutes <?> If direct uploads/downloads: time until links expire. Optional. 60 dataverse.files.<id>.min-part-size <?> Multipart direct uploads will occur for files larger than this. Optional. 1024**3 dataverse.files.<id>.custom-endpoint-url <?> Use custom S3 endpoint. Needs URL either with or without protocol. (none) dataverse.files.<id>.custom-endpoint-region <?> Only used when using custom endpoint. Optional. dataverse dataverse.files.<id>.profile <?> Allows the use of AWS profiles for storage spanning multiple AWS accounts. (none) dataverse.files.<id>.proxy-url <?> URL of a proxy protecting the S3 store. Optional. (none) dataverse.files.<id>.path-style-access true/false Use path style buckets instead of subdomains. Optional. false dataverse.files.<id>.payload-signing true/false Enable payload signing. Optional false dataverse.files.<id>.chunked-encoding true/false Disable chunked encoding. Optional true dataverse.files.<id>.connection-pool-size <?> The maximum number of open connections to the S3 server 256 dataverse.files.<id>.disable-tagging true/false Do not place the temp tag when redirecting the upload to the S3 server. false\n\nMicroProfile Config Option Value Description Default value dataverse.files.<id>.access-key <?> :ref:`Provide static access key ID. Read before use! <s3-mpconfig>` \"\" dataverse.files.<id>.secret-key <?> :ref:`Provide static secret access key. Read before use! <s3-mpconfig>` \"\"\n\nOptionally, you may provide static credentials for each S3 storage using MicroProfile Config options:\n\ndataverse.files.<id>.access-key for this storage's \"access key ID\"\n\ndataverse.files.<id>.secret-key for this storage's \"secret access key\"\n\nYou may provide the values for these via any supported MicroProfile Config API source.\n\nWARNING: For security, do not use the sources \"environment variable\" or \"system property\" (JVM option) in a production context! Rely on password alias, secrets directory or cloud based sources as described at :ref:`secure-password-storage` instead!\n\nNOTE:\n\nProviding both AWS CLI profile files (as setup in first step) and static keys, credentials from ~/.aws will win over configured keys when valid!\n\nA non-empty dataverse.files.<id>.profile will be ignored when no credentials can be found for this profile name. Current codebase does not make use of \"named profiles\" as seen for AWS CLI besides credentials.\n\nMinio v2018-09-12\n\nSet dataverse.files.<id>.path-style-access=true, as Minio works path-based. Works pretty smooth, easy to setup. Can be used for quick testing, too: just use the example values above. Uses the public (read: unsecure and possibly slow) https://play.minio.io:9000 service.\n\nStorJ Object Store\n\nStorJ is a distributed object store that can be configured with an S3 gateway. Per the S3 Storage instructions above, you'll first set up the StorJ S3 store by defining the id, type, and label. After following the general installation, set the following configurations to use a StorJ object store: dataverse.files.<id>.payload-signing=true and dataverse.files.<id>.chunked-encoding=false. For step-by-step instructions see https://docs.storj.io/dcs/how-tos/dataverse-integration-guide/\n\nNote that for direct uploads and downloads, Dataverse redirects to the proxy-url but presigns the urls based on the dataverse.files.<id>.custom-endpoint-url. Also, note that if you choose to enable dataverse.files.<id>.download-redirect the S3 URLs expire after 60 minutes by default. You can change that minute value to reflect a timeout value thatâs more appropriate by using dataverse.files.<id>.url-expiration-minutes.\n\nSurf Object Store v2019-10-30\n\nSet dataverse.files.<id>.payload-signing=true, dataverse.files.<id>.chunked-encoding=false and dataverse.files.<id>.path-style-request=true to use Surf Object Store. You will need the Swift client (documented at <http://doc.swift.surfsara.nl/en/latest/Pages/Clients/s3cred.html>) to create the access key and secret key for the S3 interface.\n\nNote that the dataverse.files.<id>.proxy-url setting can be used in installations where the object store is proxied, but it should be considered an advanced option that will require significant expertise to properly configure. For direct uploads and downloads, Dataverse redirects to the proxy-url but presigns the urls based on the dataverse.files.<id>.custom-endpoint-url. Additional configuration (appropriate CORS settings, proxy caching/timeout configuration, and proxy settings to pass headers to/from S3 and to avoid adding additional headers) will also be needed to enable use of a proxy with direct upload and download. For Amazon AWS, see comments in the edu.harvard.iq.dataverse.dataaccess.S3AccessIO class about support for AWS's bucket-specific DNS names.\n\nSeaweedFS\n\nSeaweedFS is a distributed storage system that has S3 compatibility. Set the S3 storage options as explained above. Make sure to set dataverse.files.<id>.path-style-access to true. You will need to create the bucket beforehand. You can do this with the filer API using curl commands. For example, to create an empty bucket called dataverse:\n\nYou will also need to set an access and secret key. One way to do this is via a static file. As an example, your config.json might look like this if you're using a bucket called dataverse:\n\nAnd lastly, to start up the SeaweedFS server and various components you could use a command like this:\n\nweed server -s3 -metricsPort=9327 -dir=/data -s3.config=/config.json\n\nAdditional Reported Working S3-Compatible Storage\n\nIf you are successfully using an S3 storage implementation not yet listed above, please feel free to open an issue at Github and describe your setup. We will be glad to add it.\n\nIs currently documented on the :doc:`/developers/deployment` page.\n\nIn addition to having the type \"remote\" and requiring a label, Trusted Remote Stores are defined in terms of a baseURL - all files managed by this store must be at a path starting with this URL, and a baseStore - a file, s3, or swift store that can be used to store additional ancillary dataset files (e.g. metadata exports, thumbnails, auxiliary files, etc.). These and other available options are described in the table below.\n\nTrusted remote stores can range from being a static trusted website to a sophisticated service managing access requests and logging activity and/or managing access to a secure enclave. See :doc:`/developers/big-data-support` for additional information on how to use a trusted remote store. For specific remote stores, consult their documentation when configuring the remote store in your Dataverse installation.\n\nNote that in the current implementation, activites where Dataverse needs access to data bytes, e.g. to create thumbnails or validate hash values at publication will fail if a remote store does not allow Dataverse access. Implementers of such trusted remote stores should consider using Dataverse's settings to disable ingest, validation of files at publication, etc. as needed.\n\nOnce you have configured a trusted remote store, you can point your users to the :ref:`add-remote-file-api` section of the API Guide.\n\nJVM Option Value Description Default value dataverse.files.<id>.type remote Required to mark this storage as remote. (none) dataverse.files.<id>.label <?> Required label to be shown in the UI for this storage. (none) dataverse.files.<id>.base-url <?> Required All files must have URLs of the form <baseUrl>/* . (none) dataverse.files.<id>.base-store <?> Required The id of a base store (of type file, s3, or swift). (the default store) dataverse.files.<id>.download-redirect true/false Enable direct download (should usually be true). false dataverse.files.<id>.secret-key <?> A key used to sign download requests sent to the remote store. Optional. (none) dataverse.files.<id>.url-expiration-minutes <?> If direct downloads and using signing: time until links expire. Optional. 60 dataverse.files.<id>.remote-store-name <?> A short name used in the UI to indicate where a file is located. Optional. (none) dataverse.files.<id>.remote-store-url <?> A url to an info page about the remote store used in the UI. Optional. (none)\n\nGlobus stores allow Dataverse to manage files stored in Globus endpoints or to reference files in remote Globus endpoints, with users leveraging Globus to transfer files to/from Dataverse (rather than using HTTP/HTTPS). See :doc:`/developers/big-data-support` for additional information on how to use a globus store. Consult the Globus documentation for information about using Globus and configuring Globus endpoints.\n\nIn addition to having the type \"globus\" and requiring a label, Globus Stores share many options with Trusted Remote Stores and options to specify and access a Globus endpoint(s). As with Remote Stores, Globus Stores also use a baseStore - a file, s3, or swift store that can be used to store additional ancillary dataset files (e.g. metadata exports, thumbnails, auxiliary files, etc.). These and other available options are described in the table below.\n\nThere are two types of Globus stores:\n\nmanaged - where Dataverse manages the Globus endpoint, deciding where transferred files are stored and managing access control for users transferring files to/from Dataverse\n\nremote - where Dataverse references files that remain on trusted remote Globus endpoints\n\nA managed Globus store connects to standard/file-based Globus endpoint. It is also possible to configure an S3 store as a managed store, if the managed endpoint uses an underlying S3 store via the Globus S3 Connector. With the former, Dataverse has no direct access to the file contents and functionality related to ingest, fixity hash validation, etc. are not available. With the latter, Dataverse can access files internally via S3 and the functionality supported is similar to that when using S3 direct upload.\n\nOnce you have configured a globus store, or configured an S3 store for Globus access, it is recommended that you install the dataverse-globus app to allow transfers in/out of Dataverse to be initated via the Dataverse user interface. Alternately, you can point your users to the :doc:`/developers/globus-api` for information about API support.\n\nJVM Option Value Description Default value dataverse.files.<id>.type globus Required to mark this storage as globus enabled. (none) dataverse.files.<id>.label <?> Required label to be shown in the UI for this storage. (none) dataverse.files.<id>.base-store <?> Required The id of a base store (of type file, s3, or swift). (the default store) dataverse.files.<id>.remote-store-name <?> A short name used in the UI to indicate where a file is located. Optional. (none) dataverse.files.<id>.remote-store-url <?> A url to an info page about the remote store used in the UI. Optional. (none) dataverse.files.<id>.managed true/false Whether dataverse manages an associated Globus endpoint false dataverse.files.<id>.transfer-endpoint-with-basepath <?> The managed Globus endpoint id and associated base path for file storage (none) dataverse.files.<id>.globus-token <?> A Globus token (base64 endcoded <Globus user id>:<Credential> for a managed store) - using a microprofile alias is recommended (none) dataverse.files.<id>.reference-endpoints-with-basepaths <?> A comma separated list of remote trusted Globus endpoint id/<basePath>s (none) dataverse.files.<id>.files-not-accessible-by-dataverse true/false Should be false for S3 Connector-based managed stores, true for others false\n\nWhen uploading files via the API or Web UI, you need to be aware that multiple steps are involved to enable features like ingest processing, transfer to a permanent storage, checking for duplicates, unzipping etc.\n\nAll of these processes are triggered after finishing transfers over the wire and moving the data into a temporary (configurable) location on disk at :ref:`${dataverse.files.directory} <dataverse.files.directory>`/temp.\n\nBefore being moved there,\n\nJSF Web UI uploads are stored at :ref:`${dataverse.files.uploads} <dataverse.files.uploads>`, defaulting to /usr/local/payara6/glassfish/domains/domain1/uploads folder in a standard installation. This place is configurable and might be set to a separate disk volume where stale uploads are purged periodically.\n\nAPI uploads are stored at the system's temporary files location indicated by the Java system property java.io.tmpdir, defaulting to /tmp on Linux. If this location is backed by a tmpfs on your machine, large file uploads via API will cause RAM and/or swap usage bursts. You might want to point this to a different location, restrict maximum size of it, and monitor for stale uploads.\n\nRate limiting has been added to prevent users from over taxing the system either deliberately or by runaway automated processes. Rate limiting can be configured on a tier level with tier 0 being reserved for guest users and tiers 1-any for authenticated users. Superuser accounts are exempt from rate limiting. Rate limits can be imposed on command APIs by configuring the tier, the command, and the hourly limit in the database. Two database settings configure the rate limiting. Note: If either of these settings exist in the database rate limiting will be enabled (note that a Payara restart is required for the setting to take effect). If neither setting exists rate limiting is disabled.\n\n:RateLimitingDefaultCapacityTiers is the number of calls allowed per hour if the specific command is not configured. The values represent the number of calls per hour per user for tiers 0,1,... A value of -1 can be used to signify no rate limit. Tiers not specified in this setting will default to -1 (No Limit). I.e., -d \"10000\" is equivalent to -d \"10000,-1,-1,...\"\n\n:RateLimitingCapacityByTierAndAction is a JSON object specifying the rate by tier and a list of actions (commands). This allows for more control over the rate limit of individual API command calls. In the following example, calls made by a guest user (tier 0) for API GetLatestPublishedDatasetVersionCommand is further limited to only 10 calls per hour, while an authenticated user (tier 1) will be able to make 30 calls per hour to the same API.\n\n:download:`rate-limit-actions.json </_static/installation/files/examples/rate-limit-actions-setting.json>` Example JSON for RateLimitingCapacityByTierAndAction\n\nA Dataverse installation can be branded in a number of ways.\n\nA simple option for branding your installation is to adjust the theme of a Dataverse collection. You can change colors, add a logo, add a tagline, or add a website link to the Dataverse collection header section of the page. These options are outlined under :ref:`theme` in the :doc:`/user/dataverse-management` section of the User Guide.\n\nMore advanced customization is described below and covers the following areas.\n\nCustom installation name/brand\n\nCustom header\n\nNavbar settings\n\nCustom welcome/homepage\n\nCustom footer\n\nFooter settings\n\nCSS stylesheet\n\nDownloadable sample HTML and CSS files are provided below which you can edit as you see fit. It's up to you to create a directory in which to store these files, such as /var/www/dataverse in the examples below.\n\nAdditional samples from community member Sherry Lake are available at https://github.com/shlake/LibraDataHomepage.\n\nBefore reading about the available customization options, you might want to familiarize yourself with the parts of a Dataverse installation webpage.\n\nThe image below indicates that the page layout consists of three main blocks: a header block, a content block, and a footer block:\n\nIt's common for a Dataverse installation to have some sort of installation name or brand name like \"HeiDATA\", \"Libra Data\", or \"MELDATA\".\n\nThe installation name appears in various places such as notifications, support links, and metadata exports.\n\nOut of the box, the installation name comes from the name of root Dataverse collection (\"Root\" by default). You can simply change the name of this collection to set the installation name you want.\n\nAlternatively, you can have independent names for the root Dataverse collection and the installation name by having the installation name come from the :ref:`:InstallationName` setting.\n\nNote that you can use :ref:`systemEmail` to control the name that appears in the \"from\" address of email messages sent by a Dataverse installation. This overrides the name of the root Dataverse collection and :ref:`:InstallationName`.\n\nIf you have an image for your installation name, you can use it as the \"Custom Navbar Logo\", described below.\n\nWithin the header block, you have a navbar (which will always be displayed) and you may insert a custom header that will be displayed above the navbar.\n\nThe navbar is the component displayed by default on the header block and will be present on every Dataverse webpage.\n\nThe navbar encompasses several configurable settings (described below) that manage user interaction with a Dataverse installation.\n\nThe Dataverse Software allows you to replace the default Dataverse Project icon and name branding in the navbar with your own custom logo. Note that this logo is separate from the logo used in the theme of the root Dataverse collection (see :ref:`theme`).\n\nThe custom logo image file is expected to be small enough to fit comfortably in the navbar, no more than 50 pixels in height and 160 pixels in width. Create a navbar directory in your Payara logos directory and place your custom logo there. By default, your logo image file will be located at /usr/local/payara6/glassfish/domains/domain1/docroot/logos/navbar/logo.png.\n\nGiven this location for the custom logo image file, run this curl command to add it to your settings:\n\ncurl -X PUT -d '/logos/navbar/logo.png' http://localhost:8080/api/admin/settings/:LogoCustomizationFile\n\nTo revert to the default configuration and have the Dataverse Project icon be displayed, run the following command:\n\ncurl -X DELETE http://localhost:8080/api/admin/settings/:LogoCustomizationFile\n\nRefer to :ref:`:NavbarAboutUrl` for setting a fully-qualified URL which will be used for the \"About\" link in the navbar.\n\nRefer to :ref:`:NavbarGuidesUrl`, :ref:`:GuidesBaseUrl`, and :ref:`:GuidesVersion` for setting a fully-qualified URL which will be used for the \"User Guide\" link in the navbar.\n\nRefer to :ref:`:NavbarSupportUrl` for setting to a fully-qualified URL which will be used for the \"Support\" link in the navbar.\n\nRefer to :ref:`:SignUpUrl` and :ref:`conf-allow-signup` for setting a relative path URL to which users will be sent for sign up and for controlling the ability for creating local user accounts.\n\nAs a starting point you can download :download:`custom-header.html </_static/installation/files/var/www/dataverse/branding/custom-header.html>` and place it at /var/www/dataverse/branding/custom-header.html.\n\nGiven this location for the custom header HTML file, run this curl command to add it to your settings:\n\ncurl -X PUT -d '/var/www/dataverse/branding/custom-header.html' http://localhost:8080/api/admin/settings/:HeaderCustomizationFile\n\nIf you have enabled a custom header or navbar logo, you might prefer to disable the theme of the root dataverse. You can do so by setting :DisableRootDataverseTheme to true like this:\n\ncurl -X PUT -d 'true' http://localhost:8080/api/admin/settings/:DisableRootDataverseTheme\n\nPlease note: Disabling the display of the root Dataverse collection theme also disables your ability to edit it. Remember that Dataverse collection owners can set their Dataverse collections to \"inherit theme\" from the root. Those Dataverse collections will continue to inherit the root Dataverse collection theme (even though it no longer displays on the root). If you would like to edit the root Dataverse collection theme in the future, you will have to re-enable it first.\n\nAs shown in the image under :ref:`parts-of-webpage`, the content block is the area below the header and above the footer.\n\nBy default, when you view the homepage of a Dataverse installation, the content block shows the root Dataverse collection. This page contains the data available in the Dataverse installation (e.g. dataverses and datasets) and the functionalities that allow the user to interact with the platform (e.g. search, create/edit data and metadata, etc.).\n\nRather than showing the root Dataverse collection on the homepage, the content block can show a custom homepage instead. Read on for details.\n\nWhen you configure a custom homepage, it replaces the root Dataverse collection in the content block, serving as a welcome page. This allows for complete control over the look and feel of the content block for your installation's homepage.\n\nAs a starting point, download :download:`custom-homepage.html </_static/installation/files/var/www/dataverse/branding/custom-homepage.html>` and place it at /var/www/dataverse/branding/custom-homepage.html.\n\nGiven this location for the custom homepage HTML file, run this curl command to add it to your settings:\n\ncurl -X PUT -d '/var/www/dataverse/branding/custom-homepage.html' http://localhost:8080/api/admin/settings/:HomePageCustomizationFile\n\nNote that the custom-homepage.html file provided has multiple elements that assume your root Dataverse collection still has an alias of \"root\". While you were branding your root Dataverse collection, you may have changed the alias to \"harvard\" or \"librascholar\" or whatever and you should adjust the custom homepage code as needed.\n\nNote: If you prefer to start with less of a blank slate, you can review the custom homepage used by the Harvard Dataverse Repository, which includes branding messaging, action buttons, search input, subject links, and recent dataset links. This page was built to utilize the :doc:`/api/metrics` to deliver dynamic content to the page via Javascript. The files can be found at https://github.com/IQSS/dataverse.harvard.edu\n\nIf you decide you'd like to remove this setting, use the following curl command:\n\ncurl -X DELETE http://localhost:8080/api/admin/settings/:HomePageCustomizationFile\n\nWithin the footer block you have the default footer section (which will always be displayed) and you can insert a custom footer that will be displayed below the default footer.\n\nThe default footer is the component displayed by default on the footer block and will be present on every Dataverse webpage. Its configuration options are described below.\n\nRefer to :ref:`:FooterCopyright` to add customized text to the Copyright section of the default Dataverse footer\n\nAs mentioned above, the custom footer appears below the default footer.\n\nAs a starting point, download :download:`custom-footer.html </_static/installation/files/var/www/dataverse/branding/custom-footer.html>` and place it at /var/www/dataverse/branding/custom-footer.html.\n\nGiven this location for the custom footer HTML file, run this curl command to add it to your settings:\n\ncurl -X PUT -d '/var/www/dataverse/branding/custom-footer.html' http://localhost:8080/api/admin/settings/:FooterCustomizationFile\n\nYou can style your custom homepage, footer, and header content with a custom CSS file. With advanced CSS know-how, you can achieve custom branding and page layouts by utilizing position, padding or margin properties.\n\nAs a starting point, download :download:`custom-stylesheet.css </_static/installation/files/var/www/dataverse/branding/custom-stylesheet.css>` and place it at /var/www/dataverse/branding/custom-stylesheet.css.\n\nGiven this location for the custom CSS file, run this curl command to add it to your settings:\n\ncurl -X PUT -d '/var/www/dataverse/branding/custom-stylesheet.css' http://localhost:8080/api/admin/settings/:StyleCustomizationFile\n\nThe Dataverse Software is being translated into multiple languages by the Dataverse Project Community! Please see below for how to help with this effort!\n\nThe presence of the :ref:`:Languages` database setting adds a dropdown in the header for multiple languages. For example to add English and French to the dropdown:\n\ncurl http://localhost:8080/api/admin/settings/:Languages -X PUT -d '[{\"locale\":\"en\",\"title\":\"English\"},{\"locale\":\"fr\",\"title\":\"FranÃ§ais\"}]'\n\nWhen a user selects one of the available choices, the Dataverse user interfaces will be translated into that language (assuming you also configure the \"lang\" directory and populate it with translations as described below).\n\nSince dataset metadata can only be entered in one language, and administrators may wish to limit which languages metadata can be entered in, Dataverse also offers a separate setting defining allowed metadata languages. The presence of the :ref:`:MetadataLanguages` database setting identifies the available options (which can be different from those in the :Languages setting above, with fewer or more options).\n\nDataverse collection admins can select from these options to indicate which language should be used for new Datasets created with that specific collection. If they do not, users will be asked when creating a dataset to select the language they want to use when entering metadata. Similarly, when this setting is defined, Datasets created/imported/migrated are required to specify a metadataLanguage compatible with the collection's requirement.\n\nWhen creating or editing a dataset, users will be asked to enter the metadata in that language. The metadata language selected will also be shown when dataset metadata is viewed and will be included in metadata exports (as appropriate for each format) for published datasets:\n\ncurl http://localhost:8080/api/admin/settings/:MetadataLanguages -X PUT -d '[{\"locale\":\"en\",\"title\":\"English\"},{\"locale\":\"fr\",\"title\":\"FranÃ§ais\"}]'\n\nNote that metadata selected from Controlled Vocabularies will also display in the metadata language of the dataset, but only if translations have been configured, i.e. you configure the \"lang\" directory and populate it with translations as described below). In metadata export files, controlled vocabulary values will be included in the Dataverse installations default language and in the metadata language of the dataset (if specified).\n\nTranslations for the Dataverse Software are stored in \"properties\" files in a directory on disk (e.g. /home/dataverse/langBundles) that you specify with the :ref:`dataverse.lang.directory` dataverse.lang.directory JVM option, like this:\n\n./asadmin create-jvm-options '-Ddataverse.lang.directory=/home/dataverse/langBundles'\n\nGo ahead and create the directory you specified.\n\nmkdir /home/dataverse/langBundles\n\nThe Dataverse Software provides and API endpoint for adding languages using a zip file.\n\nFirst, clone the \"dataverse-language-packs\" git repo.\n\ngit clone https://github.com/GlobalDataverseCommunityConsortium/dataverse-language-packs.git\n\nTake a look at https://github.com/GlobalDataverseCommunityConsortium/dataverse-language-packs/branches to see if the version of the Dataverse Software you're running has translations.\n\nChange to the directory for the git repo you just cloned.\n\ncd dataverse-language-packs\n\nSwitch (git checkout) to the branch based on the Dataverse Software version you are running. The branch \"dataverse-v4.13\" is used in the example below.\n\nexport BRANCH_NAME=dataverse-v4.13\n\ngit checkout $BRANCH_NAME\n\nCreate a \"languages\" directory in \"/tmp\".\n\nmkdir /tmp/languages\n\nCopy the properties files into the \"languages\" directory\n\ncp -R en_US/*.properties /tmp/languages\n\ncp -R fr_CA/*.properties /tmp/languages\n\nCreate the zip file\n\ncd /tmp/languages\n\nzip languages.zip *.properties\n\nNow that you have a \"languages.zip\" file, you can load it into your Dataverse installation with the command below.\n\ncurl http://localhost:8080/api/admin/datasetfield/loadpropertyfiles -X POST --upload-file /tmp/languages/languages.zip -H \"Content-Type: application/zip\"\n\nClick on the languages using the drop down in the header to try them out.\n\nPlease join the dataverse-internationalization-wg mailing list and contribute to https://github.com/GlobalDataverseCommunityConsortium/dataverse-language-packs to help translate the Dataverse Software into various languages!\n\nSome external tools are also ready to be translated, especially if they are using the {localeCode} reserved word in their tool manifest. For details, see the :doc:`/api/external-tools` section of the API Guide.\n\nThe list below depicts a set of tools that can be used to ease the amount of work necessary for translating the Dataverse software by facilitating this collaborative effort and enabling the reuse of previous work:\n\nWeblate for the Dataverse Software, made available in the scope of the SSHOC project.\n\neasyTranslationHelper, a tool developed by University of Aveiro.\n\nDataverse General User Interface Translation Guide for Weblate, a guide produced as part of the SSHOC Dataverse Translation event.\n\nYour analytics code can be added to your Dataverse installation in a similar fashion to how you brand it, by adding a custom HTML file containing the analytics code snippet and adding the file location to your settings.\n\nPopular analytics providers Google Analytics (https://www.google.com/analytics/) and Matomo (formerly \"Piwik\"; https://matomo.org/) have been set up to work with the Dataverse Software. Use the documentation they provide to add the analytics code to your custom HTML file. This allows for more control of your analytics, making it easier to customize what you prefer to track.\n\nCreate your own analytics-code.html file using the analytics code snippet provided by Google or Matomo and place it somewhere on the server, outside the application deployment directory; for example: /var/www/dataverse/branding/analytics-code.html. Here is an example of what your HTML file will look like:\n\n<!-- Global Site Tag (gtag.js) - Google Analytics --> <script async=\"async\" src=\"https://www.googletagmanager.com/gtag/js?id=YOUR-ACCOUNT-CODE\"></script> <script> //<![CDATA[ window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'YOUR-ACCOUNT-CODE'); //]]> </script>\n\nIMPORTANT: Note the \"async\" attribute in the first script line above. In the documentation provided by Google, its value is left blank (as in <script async src=\"...\">). It must be set as in the example above (<script async=\"async\" src=\"...\">), otherwise it may cause problems with some browsers.\n\nOnce you have created the analytics file, run this curl command to add it to your settings (using the same file location as in the example above):\n\ncurl -X PUT -d '/var/www/dataverse/branding/analytics-code.html' http://localhost:8080/api/admin/settings/:WebAnalyticsCode\n\nThe basic analytics configuration above tracks page navigation. However, it does not capture potentially interesting events, such as those from users clicking buttons on pages, that do not result in a new page opening. In a Dataverse installation, these events include file downloads, requesting access to restricted data, exporting metadata, social media sharing, requesting citation text, launching external tools, contacting authors, and launching computations.\n\nBoth Google and Matomo provide the optional capability to track such events and the Dataverse Software has added CSS style classes (btn-compute, btn-contact, btn-download, btn-explore, btn-export, btn-preview, btn-request, btn-share, and downloadCitation) to it's HTML to facilitate it.\n\nFor Google Analytics, the example script at :download:`analytics-code.html </_static/installation/files/var/www/dataverse/branding/analytics-code.html>` will track both page hits and events within your Dataverse installation. You would use this file in the same way as the shorter example above, putting it somewhere outside your deployment directory, replacing YOUR ACCOUNT CODE with your actual code and setting :WebAnalyticsCode to reference it.\n\nOnce this script is running, you can look in the Google Analytics console (Realtime/Events or Behavior/Events) and view events by type and/or the Dataset or File the event involves.\n\nOn a new Dataverse installation, users may select from the following licenses or terms:\n\nCC0 1.0 (default)\n\nCC BY 4.0\n\nCustom Dataset Terms\n\n(Note that existing Dataverse installations which are upgraded from 5.9 or previous will only offer CC0 1.0, added automatically during the upgrade to version 5.10.)\n\nIf the Dataverse Installation supports multiple languages, the license name/description translations should be added to the License properties files. (See :ref:`i18n` for more on properties files and internationalization in general.) To create the key, the license name has to be converted to lowercase, replace space with underscore.\n\nExample:\n\nlicense.cc0_1.0.description=Creative Commons CC0 1.0 Universal Public Domain Dedication. license.cc0_1.0.name=CC0 1.0\n\nYou have a lot of control over which licenses and terms are available. You can remove licenses and add new ones. You can decide which license is the default. You can remove \"Custom Dataset Terms\" as a option. You can remove all licenses and make \"Custom Dataset Terms\" the only option.\n\nBefore making changes, you are encouraged to read the :ref:`license-terms` section of the User Guide about why CC0 is the default and what the \"Custom Dataset Terms\" option allows.\n\nThe default license can be set with a curl command as explained in the API Guide under :ref:`license-management-api`.\n\nNote that \"Custom Dataset Terms\" is not a license and cannot be set to be the default.\n\nLicenses are added with curl using JSON file as explained in the API Guide under :ref:`license-management-api`.\n\nJSON files for Creative Commons licenses are provided below. Note that a new installation of Dataverse already includes CC0 and CC BY.\n\n:download:`licenseCC0-1.0.json <../../../../scripts/api/data/licenses/licenseCC0-1.0.json>`\n\n:download:`licenseCC-BY-4.0.json <../../../../scripts/api/data/licenses/licenseCC-BY-4.0.json>`\n\n:download:`licenseCC-BY-SA-4.0.json <../../../../scripts/api/data/licenses/licenseCC-BY-SA-4.0.json>`\n\n:download:`licenseCC-BY-NC-4.0.json <../../../../scripts/api/data/licenses/licenseCC-BY-NC-4.0.json>`\n\n:download:`licenseCC-BY-NC-SA-4.0.json <../../../../scripts/api/data/licenses/licenseCC-BY-NC-SA-4.0.json>`\n\n:download:`licenseCC-BY-ND-4.0.json <../../../../scripts/api/data/licenses/licenseCC-BY-ND-4.0.json>`\n\n:download:`licenseCC-BY-NC-ND-4.0.json <../../../../scripts/api/data/licenses/licenseCC-BY-NC-ND-4.0.json>`\n\nJSON files for software licenses are provided below.\n\n:download:`licenseMIT.json <../../../../scripts/api/data/licenses/licenseMIT.json>`\n\n:download:`licenseApache-2.0.json <../../../../scripts/api/data/licenses/licenseApache-2.0.json>`\n\nIf you do not find the license JSON you need above, you are encouraged to contribute it to this documentation. Following the Dataverse 6.2 release, we have standardized on the following procedure:\n\nLook for the license at https://spdx.org/licenses/\n\ncd scripts/api/data/licenses\n\nCopy an existing license as a starting point.\n\nName your file using the SPDX identifier. For example, if the identifier is Apache-2.0, you should name your file licenseApache-2.0.json.\n\nFor the name field, use the \"short identifier\" from the SPDX landing page (e.g. Apache-2.0).\n\nFor the description field, use the \"full name\" from the SPDX landing page (e.g. Apache License 2.0).\n\nFor the uri field, we encourage you to use the same resource that DataCite uses, which is often the same as the first \"Other web pages for this license\" on the SPDX page for the license. When these differ, or there are other concerns about the URI DataCite uses, please reach out to the community to see if a consensus can be reached.\n\nFor the active field, put true.\n\nFor the sortOrder field, put the next sequential number after checking previous files with grep sortOrder scripts/api/data/licenses/*.\n\nNote that prior to Dataverse 6.2, various license above have been added that do not adhere perfectly with this procedure. For example, the name for the CC0 license is CC0 1.0 (no dash) rather than CC0-1.0 (with a dash). We are keeping the existing names for backward compatibility. For more on standarizing license configuration, see #8512\n\nIf you are interested in adding a custom license, you will need to create your own JSON file as explained in see :ref:`standardizing-custom-licenses`.\n\nLicenses can be removed with a curl command as explained in the API Guide under :ref:`license-management-api`.\n\nSee :ref:`:AllowCustomTermsOfUse` for how to disable the \"Custom Dataset Terms\" option.\n\nThe default order of licenses in the dropdown in the user interface is as follows:\n\nThe default license is shown first\n\nFollowed by the remaining installed licenses in the order of installation\n\nThe custom license is at the end\n\nOnly the order of the installed licenses can be changed with the API calls. The default license always remains first and the custom license last.\n\nThe order of licenses can be changed by setting the sortOrder property of a license. For the purpose of making sorting easier and to allow grouping of the licenses, sortOrder property does not have to be unique. Licenses with the same"
    }
}
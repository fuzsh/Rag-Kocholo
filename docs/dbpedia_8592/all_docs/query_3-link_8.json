{
    "id": "dbpedia_8592_3",
    "rank": 8,
    "data": {
        "url": "https://wiki.lyrasis.org/display/DSDOC17/System%2BAdministration",
        "read_more_link": "",
        "language": "en",
        "title": "DSpace 1.7 Documentation",
        "top_image": "https://wiki.lyrasis.org/s/h9vz60/9012/1phy4ty/72/_/favicon.ico",
        "meta_img": "https://wiki.lyrasis.org/s/h9vz60/9012/1phy4ty/72/_/favicon.ico",
        "images": [
            "https://wiki.lyrasis.org/download/attachments/30216977/DSDOC17?version=1&modificationDate=1457102163861&api=v2",
            "https://wiki.lyrasis.org/images/icons/linkext7.gif"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Mark Diggory"
        ],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "/s/h9vz60/9012/1phy4ty/72/_/favicon.ico",
        "meta_site_name": "",
        "canonical_link": "https://wiki.lyrasis.org/display/DSDOC17/System+Administration",
        "text": "DSpace System Documentation: System Administration\n\nDSpace operates on several levels: as a Tomcat servlet, cron jobs, and on-demand operations. This section explains many of the on-demand operations. Some of the command operations may be also set up as cron jobs. Many of these operations are performed at the Command Line Interface (CLI) also known as the Unix prompt ($:). Future reference will use the term CLI when the use needs to be at the command line.\n\nBelow is the \"Command Help Table\". This table explains what data is contained in the individual command/help tables in the sections that follow.\n\nTable of Contents:\n\nCommunity and Collection Structure Importer\n\nThis CLI tool gives you the ability to import a community and collection structure directory from a source XML file.\n\nThe administrator need to build the source xml document in the following format:\n\nThe resulting output document will be as follows:\n\nThis command-line tool gives you the ability to import a community and collection structure directly from a source XML file. It is executed as follows:\n\nThis will examine the contents of source.xml, import the structure into DSpace while logged in as the supplied administrator, and then output the same structure to the output file, but including the handle for each imported community and collection as an attribute.\n\nLimitation\n\nCurrently this does not export community and collection structures, although it should only be a small modification to make it do so\n\nPackage Importer and Exporter\n\nThis command-line tool gives you access to the Packager plugins. It can ingest a package to create a new DSpace Object (Community, Collection or Item), or disseminate a DSpace Object as a package.\n\nTo see all the options, invoke it as:\n\nThis mode also displays a list of the names of package ingestion and dissemination plugins that are currently installed in your DSpace. Each Packager plugin also may allow for custom options, which may provide you more control over how a package is imported or exported. You can see a listing of all specific packager options by invoking --help (or -h) with the --type (or -t) option:\n\nThe above example will display the normal help message, while also listing any additional options available to the \"METS\" packager plugin.\n\nIngesting\n\nIngestion Modes & Options\n\nWhen ingesting packages DSpace supports several different \"modes\". (Please note that not all packager plugins may support all modes of ingestion)\n\nSubmit/Ingest Mode (-s option, default) – submit package to DSpace in order to create a new object(s)\n\nRestore Mode (-r option) – restore pre-existing object(s) in DSpace based on package(s). This also attempts to restore all handles and relationships (parent/child objects). This is a specialized type of \"submit\", where the object is created with a known Handle and known relationships.\n\nReplace Mode (-r -f option) – replace existing object(s) in DSpace based on package(s). This also attempts to restore all handles and relationships (parent/child objects). This is a specialized type of \"restore\" where the contents of existing object(s) is replaced by the contents in the AIP(s). By default, if a normal \"restore\" finds the object already exists, it will back out (i.e. rollback all changes) and report which object already exists.\n\nIngesting a Single Package\n\nTo ingest a single package from a file, give the command:\n\nWhere [user-email] is the e-mail address of the E-Person under whose authority this runs; [parent-handle] is the Handle of the Parent Object into which the package is ingested, [packager-name] is the plugin name of the package ingester to use, and /full/path/to/package is the path to the file to ingest (or \"-\" to read from the standard input).\n\nHere is an example that loads a PDF file with internal metadata as a package:\n\nThis example takes the result of retrieving a URL and ingests it:\n\nIngesting Multiple Packages at Once\n\nSome Packager plugins support bulk ingest functionality using the --all (or -a) flag. When --all is used, the packager will attempt to ingest all child packages referenced by the initial package (and continue on recursively). Some examples follow:\n\nFor a Site-based package - this would ingest all Communities, Collections & Items based on the located package files\n\nFor a Community-based package - this would ingest that Community and all SubCommunities, Collections and Items based on the located package files\n\nFor a Collection - this would ingest that Collection and all contained Items based on the located package files\n\nFor an Item – this just ingest the Item (including all Bitstreams & Bundles) based on the package file.\n\nHere is a basic example of a bulk ingest 'packager' command template:\n\nfor example:\n\nThe above command will ingest the package named \"collection-aip.zip\" as a child of the specified Parent Object (handle=\"4321/12\"). The resulting object is assigned a new Handle (since -s is specified). In addition, any child packages directly referenced by \"collection-aip.zip\" are also recursively ingested (a new Handle is also assigned for each child AIP).\n\nRestoring/Replacing using Packages\n\nRestoring is slightly different than just ingesting. When restoring, the packager makes every attempt to restore the object as it used to be (including its handle, parent object, etc.).\n\nThere are currently three restore modes:\n\nDefault Restore Mode (-r) = Attempt to restore object (and optionally children). Rollback all changes if any object is found to already exist.\n\nRestore, Keep Existing Mode (-r -k) = Attempt to restore object (and optionally children). If an object is found to already exist, skip over it (and all children objects), and continue to restore all other non-existing objects.\n\nForce Replace Mode (-r -f) = Restore an object (and optionally children) and overwrite any existing objects in DSpace. Therefore, if an object is found to already exist in DSpace, its contents are replaced by the contents of the package. WARNING: This mode is potentially dangerous as it will permanently destroy any object contents that do not currently exist in the package. You may want to first perform a backup, unless you are sure you know what you are doing!\n\nDefault Restore Mode\n\nBy default, the restore mode (-r option) will rollback all changes if any object is found to already exist. The user will be informed if which object already exists within their DSpace installation.\n\nUse this 'packager' command template:\n\nFor example:\n\nNotice that unlike -s option (for submission/ingesting), the -r option does not require the Parent Object (-p option) to be specified if it can be determined from the package itself.\n\nIn the above example, the package \"aip4567.zip\" is restored to the DSpace installation with the Handle provided within the package itself (and added as a child of the parent object specified within the package itself). If the object is found to already exist, all changes are rolled back (i.e. nothing is restored to DSpace)\n\nRestore, Keep Existing Mode\n\nWhen the \"Keep Existing\" flag (-k option) is specified, the restore will attempt to skip over any objects found to already exist. It will report to the user that the object was found to exist (and was not modified or changed). It will then continue to restore all objects which do not already exist. This flag is most useful when attempting a bulk restore (using the --all (or -a) option.\n\nOne special case to note: If a Collection or Community is found to already exist, its child objects are also skipped over. So, this mode will not auto-restore items to an existing Collection.\n\nHere's an example of how to use this 'packager' command:\n\nFor example:\n\nIn the above example, the package \"aip4567.zip\" is restored to the DSpace installation with the Handle provided within the package itself (and added as a child of the parent object specified within the package itself). In addition, any child packages referenced by \"aip4567.zip\" are also recursively restored (the -a option specifies to also restore all child pacakges). They are also restored with the Handles & Parent Objects provided with their package. If any object is found to already exist, it is skipped over (child objects are also skipped). All non-existing objects are restored.\n\nForce Replace Mode\n\nWhen the \"Force Replace\" flag (-f option) is specified, the restore will overwrite any objects found to already exist in DSpace. In other words, existing content is deleted and then replaced by the contents of the package(s).\n\nHere's an example of how to use this 'packager' command:\n\nFor example:\n\nIn the above example, the package \"aip4567.zip\" is restored to the DSpace installation with the Handle provided within the package itself (and added as a child of the parent object specified within the package itself). In addition, any child packages referenced by \"aip4567.zip\" are also recursively ingested. They are also restored with the Handles & Parent Objects provided with their package. If any object is found to already exist, its contents are replaced by the contents of the appropriate package.\n\nIf any error occurs, the script attempts to rollback the entire replacement process.\n\nDisseminating\n\nDisseminating a Single Object\n\nTo disseminate a single object as a package, give the command:\n\nWhere [user-email] is the e-mail address of the E-Person under whose authority this runs; [handle] is the Handle of the Object to disseminate; [packager-name] is the plugin name of the package disseminator to use; and [file-path] is the path to the file to create (or \"-\" to write to the standard output). For example:\n\nThe above code will export the object of the given handle (4321/4567) into a METS file named \"4567.zip\".\n\nDisseminating Multiple Objects at Once\n\nTo export an object hierarchy, use the -a (or --all) package parameter.\n\nFor example, use this 'packager' command template:\n\nfor example:\n\nThe above code will export the object of the given handle (4321/4567) into a METS file named \"4567.zip\". In addition it would export all children objects to the same directory as the \"4567.zip\" file.\n\nArchival Information Packages (AIPs)\n\nAs of DSpace 1.7, DSpace now can backup and restore all of its contents as a set of AIP Files. This includes all Communities, Collections, Items, Groups and People in the system.\n\nThis feature came out of a requirement for DSpace to better integrate with DuraCloud (http://www.duracloud.org), and other backup storage systems. One of these requirements is to be able to essentially \"backup\" local DSpace contents into the cloud (as a type of offsite backup), and \"restore\" those contents at a later time.\n\nEssentially, this means DSpace can export the entire hierarchy (i.e. bitstreams, metadata and relationships between Communities/Collections/Items) into a relatively standard format (a METS-based, AIP format). This entire hierarchy can also be re-imported into DSpace in the same format (essentially a restore of that content in the same or different DSpace installation).\n\nFor more information, see the section on AIP backup & Restore for DSpace.\n\nMETS packages\n\nSince DSpace 1.4 release, the software includes a package disseminator and matching ingester for the DSpace METS SIP (Submission Information Package) format. They were created to help end users prepare sets of digital resources and metadata for submission to the archive using well-defined standards such as METS, MODS, and PREMIS. The plugin name is METS by default, and it uses MODS for descriptive metadata.\n\nThe DSpace METS SIP profile is available at: https://wiki.duraspace.org/display/DSPACE/DSpaceMETSSIPProfile\n\nItem Importer and Exporter\n\nDSpace has a set of command line tools for importing and exporting items in batches, using the DSpace simple archive format. The tools are not terribly robust, but are useful and are easily modified. They also give a good demonstration of how to implement your own item importer if desired.\n\nDSpace Simple Archive Format\n\nThe basic concept behind the DSpace's simple archive format is to create an archive, which is directory full of items, with a subdirectory per item. Each item directory contains a file for the item's descriptive metadata, and the files that make up the item.\n\nThe dublin_core.xml or metadata[prefix].xml_file has the following format, where each metadata element has it's own entry within a <dcvalue> tagset. There are currently three tag attributes available in the <dcvalue> tagset:\n\n<element> - the Dublin Core element\n\n<qualifier> - the element's qualifier\n\n<language> - (optional)ISO language code for element\n\n<dublin_core> <dcvalue element=\"title\" qualifier=\"none\">A Tale of Two Cities</dcvalue> <dcvalue element=\"date\" qualifier=\"issued\">1990</dcvalue> <dcvalue element=\"title\" qualifier=\"alternate\" language=\"fr\">J'aime les Printemps</dcvalue> </dublin_core>\n\n(Note the optional language tag attribute which notifies the system that the optional title is in French.)\n\nEvery metadata field used, must be registered via the metadata registry of the DSpace instance first.\n\nThe contents file simply enumerates, one file per line, the bitstream file names. See the following example:\n\nPlease notice that the license is optional, and if you wish to have one included, you can place the file in the .../item_001/ directory, for example.\n\nThe bitstream name may optionally be followed by any of the following:\n\n\\tbundle:BUNDLENAME\n\n\\tpermissions:PERMISSIONS\n\n\\tdescription:DESCRIPTION\n\n\\tprimary:true\n\nWhere '\\t' is the tab character.\n\n'BUNDLENAME' is the name of the bundle to which the bitstream should be added. Without specifying the bundle, items will go into the default bundle, ORIGINAL.\n\n'PERMISSIONS' is text with the following format: -[r|w] 'group name'\n\n'DESCRIPTION' is text of the files description.\n\nPrimary is used to specify the primary bitstream.\n\nConfiguring metadata-[prefix].xml for Different Schema\n\nIt is possible to use other Schema such as EAD, VRA Core, etc. Make sure you have defined the new scheme in the DSpace Metada Schema Registry.\n\nCreate a separate file for the other schema named \"metadata-[prefix].xml\", where the {prefix} is replaced with the schema's prefix.\n\nInside the xml file use the dame Dublin Core syntax, but on the <dublin_core> element include the attribute \"schema={prefix}\".\n\nHere is an example for ETD metadata, which would be in the file \"metadata_etd.xml\":\n\n<?xml version=\"1.0\" encoding=\"UTF-8\"?> <dublin_core schema=\"etd\"> <dcvalue element=\"degree\" qualifier=\"department\">Computer Science</dcvalue> <dcvalue element=\"degree\" qualifier=\"level\">Masters</dcvalue> <dcvalue element=\"degree\" qualifier=\"grantor\">Texas A & M</dcvalue> </dublin_core>\n\nImporting Items\n\nBefore running the item importer over items previously exported from a DSpace instance, please first refer to Transferring Items Between DSpace Instances.\n\n‡ These are mutually exclusive.\n\nThe item importer is able to batch import unlimited numbers of items for a particular collection using a very simple CLI command and 'arguments'\n\nAdding Items to a Collection\n\nTo add items to a collection, you gather the following information:\n\neperson\n\nCollection ID (either Handle (e.g. 123456789/14) or Database ID (e.g. 2)\n\nSource directory where the items reside\n\nMapfile. Since you don't have one, you need to determine where it will be (e.g. /Import/Col_14/mapfile)\n\nAt the command line:\n\nor by using the short form:\n\nThe above command would cycle through the archive directory's items, import them, and then generate a map file which stores the mapping of item directories to item handles. SAVE THIS MAP FILE. Using the map file you can use it for replacing or deleting (unimporting) the file.\n\nTesting. You can add --test (or -t) to the command to simulate the entire import process without actually doing the import. This is extremely useful for verifying your import files before doing the actual import.\n\nReplacing Items in Collection\n\nReplacing existing items is relatively easy. Remember that mapfile you were supposed to save? Now you will use it. The command (in short form):\n\nLong form:\n\nDeleting or Unimporting Items in a Collection\n\nYou are able to unimport or delete items provided you have the mapfile. Remember that mapfile you were supposed to save? The command is (in short form):\n\nIn long form:\n\nOther Options\n\nWorkflow. The importer usually bypasses any workflow assigned to a collection. But add the --workflow (-w) argument will route the imported items through the workflow system.\n\nTemplates. If you have templates that have constant data and you wish to apply that data during batch importing, add the --template (-p) argument.\n\nResume. If, during importing, you have an error and the import is aborted, you can use the --resume (-R) flag that you can try to resume the import where you left off after you fix the error.\n\nExporting Items\n\nThe item exporter can export a single item or a collection of items, and creates a DSpace simple archive for each item to be exported.\n\nExporting a Collection\n\nTo export a collection's items you type at the CLI:\n\nShort form:\n\nExporting a Single Item\n\nThe keyword COLLECTION means that you intend to export an entire collection. The ID can either be the database ID or the handle. The exporter will begin numbering the simple archives with the sequence number that you supply. To export a single item use the keyword ITEM and give the item ID as an argument:\n\nShort form:\n\nEach exported item will have an additional file in its directory, named 'handle'. This will contain the handle that was assigned to the item, and this file will be read by the importer so that items exported and then imported to another machine will retain the item's original handle.\n\nThe -m Argument\n\nUsing the -m argument will export the item/collection and also perform the migration step. It will perform the same process that the next section Transferring Items Between DSpace Instances performs. We recommend that the next section be read in conjunction with this flag being used.\n\nTransferring Items Between DSpace Instances\n\nMigration of Data\n\nWhere items are to be moved between DSpace instances (for example from a test DSpace into a production DSpace) the item exporter and item importer can be used in conjunction with a script to assist in this process.\n\nAfter running the item exporter each dublin_core.xml file will contain metadata that was automatically added by DSpace. These fields are as follows:\n\ndate.accessioned\n\ndate.available\n\ndate.issued\n\ndescription.provenance\n\nformat.extent\n\nformat.mimetype\n\nidentifier.uri\n\nIn order to avoid duplication of this metadata, run\n\nprior to running the item importer. This will remove the above metadata items, except for date.issued - if the item has been published or publicly distributed before and identifier.uri - if it is not the handle, from the dublin_core.xml file and remove all handle files. It will then be safe to run the item exporter.\n\nItemUpdate is a batch-mode command-line tool for altering the metadata and bitstream content of existing items in a DSpace instance. It is a companion tool to ItemImport and uses the DSpace simple archive format to specify changes in metadata and bitstream contents. Those familiar with generating the source trees for ItemImporter will find a similar environment in the use of this batch processing tool.\n\nFor metadata, ItemUpdate can perform 'add' and 'delete' actions on specified metadata elements. For bitstreams, 'add' and 'delete' are similarly available. All these actions can be combined in a single batch run.\n\nItemUpdate supports an undo feature for all actions except bitstream deletion. There is also a test mode, as with ItemImport. However, unlike ItemImport, there is no resume feature for incomplete processing. There is more extensive logging with a summary statement at the end with counts of successful and unsuccessful items processed.\n\nOne probable scenario for using this tool is where there is an external primary data source for which the DSpace instance is a secondary or down-stream system. Metadata and/or bitstream content changes in the primary system can be exported to the simple archive format to be used by ItemUpdate to synchronize the changes.\n\nA note on terminology: item refers to a DSpace item. metadata element refers generally to a qualified or unqualified element in a schema in the form [schema].[element].[qualifier] or [schema].[element] and occasionally in a more specific way to the second part of that form. metadata field refers to a specific instance pairing a metadata element to a value.\n\nDSpace simple Archive Format\n\nAs with ItemImporter, the idea behind the DSpace's simple archive format is to create an archive directory with a subdirectory per item. There are a few additional features added to this format specifically for ItemUpdate. Note that in the simple archive format, the item directories are merely local references and only used by ItemUpdate in the log output.\n\nThe user is referred to the previous section DSpace Simple Archive Format.\n\nAdditionally, the use of a delete_contents is now available. This file lists the bitstreams to be deleted, one bitstream ID per line. Currently, no other identifiers for bitstreams are usable for this function. This file is an addition to the Archive format specifically for ItemUpdate.\n\nThe optional suppress_undo file is a flag to indicate that the 'undo archive' should not be written to disk. This file is usually written by the application in an undo archive to prevent a recursive undo. This file is an addition to the Archive format specifically for ItemUpdate.\n\nCLI Examples\n\nAdding Metadata:\n\nThis will add from your archive the dc element description based on the handle from the URI (since the -i argument wasn't used).\n\nRegistering (Not Importing) Bitstreams\n\nRegistration is an alternate means of incorporating items, their metadata, and their bitstreams into DSpace by taking advantage of the bitstreams already being in storage accessible to DSpace. An example might be that there is a repository for existing digital assets. Rather than using the normal interactive ingest process or the batch import to furnish DSpace the metadata and to upload bitstreams, registration provides DSpace the metadata and the location of the bitstreams. DSpace uses a variation of the import tool to accomplish registration.\n\nAccessible Storage\n\nTo register an item its bitstreams must reside on storage accessible to DSpace and therefore referenced by an asset store number in dspace.cfg. The configuration file dspace.cfg establishes one or more asset stores through the use of an integer asset store number. This number relates to a directory in the DSpace host's file system or a set of SRB account parameters. This asset store number is described in The dspace.cfg Configuration Properties File section and in the dspace.cfg file itself. The asset store number(s) used for registered items should generally not be the value of the assetstore.incoming property since it is unlikely that you will want to mix the bitstreams of normally ingested and imported items and registered items.\n\nRegistering Items Using the Item Importer\n\nDSpace uses the same import tool that is used for batch import except that several variations are employed to support registration. The discussion that follows assumes familiarity with the import tool.\n\nThe archive format for registration does not include the actual content files (bitstreams) being registered. The format is however a directory full of items to be registered, with a subdirectory per item. Each item directory contains a file for the item's descriptive metadata (dublin_core.xml) and a file listing the item's content files (contents), but not the actual content files themselves.\n\nThe dublin_core.xml file for item registration is exactly the same as for regular item import.\n\nThe contents file, like that for regular item import, lists the item's content files, one content file per line, but each line has the one of the following formats:\n\nwhere\n\n-r indicates this is a file to be registered\n\n-s n indicates the asset store number (n)\n\n-f filepath indicates the path and name of the content file to be registered (filepath)\n\n\\t is a tab character\n\nbundle:bundlename is an optional bundle name\n\npermissions: -[r|w] 'group name' is an optional read or write permission that can be attached to the bitstream\n\ndescription: some text is an optional description field to add to the file\n\nThe bundle, that is everything after the filepath, is optional and is normally not used.\n\nThe command line for registration is just like the one for regular import:\n\n(or by using the long form)\n\nThe --workflow and --test flags will function as described in Importing Items.\n\nThe --delete flag will function as described in Importing Items but the registered content files will not be removed from storage. See Deleting Registered Items.\n\nThe --replace flag will function as described in Importing Items but care should be taken to consider different cases and implications. With old items and new items being registered or ingested normally, there are four combinations or cases to consider. Foremost, an old registered item deleted from DSpace using --replace will not be removed from the storage. See Deleting Registered Items. where is resides. A new item added to DSpace using --replace will be ingested normally or will be registered depending on whether or not it is marked in the contents files with the -r.\n\nInternal Identification and Retrieval of Registered Items\n\nOnce an item has been registered, superficially it is indistinguishable from items ingested interactively or by batch import. But internally there are some differences:\n\nFirst, the randomly generated internal ID is not used because DSpace does not control the file path and name of the bitstream. Instead, the file path and name are that specified in the contents file.\n\nSecond, the store_number column of the bitstream database row contains the asset store number specified in the contents file.\n\nThird, the internal_id column of the bitstream database row contains a leading flag (-R) followed by the registered file path and name. For example, -Rfilepath where filepath is the file path and name relative to the asset store corresponding to the asset store number. The asset store could be traditional storage in the DSpace server's file system or an SRB account.\n\nFourth, an MD5 checksum is calculated by reading the registered file if it is in local storage. If the registerd file is in remote storage (say, SRB) a checksum is calculated on just the file name! This is an efficiency choice since registering a large number of large files that are in SRB would consume substantial network resources and time. A future option could be to have an SRB proxy process calculate MD5s and store them in SRB's metadata catalog (MCAT) for rapid retrieval. SRB offers such an option but it's not yet in production release.\n\nRegistered items and their bitstreams can be retrieved transparently just like normally ingested items.\n\nExporting Registered Items\n\nRegistered items may be exported as described in Exporting Items. If so, the export directory will contain actual copies of the files being exported but the lines in the contents file will flag the files as registered. This means that if DSpace items are \"round tripped\" (see Transferring Items Between DSpace Instances) using the exporter and importer, the registered files in the export directory will again registered in DSpace instead of being uploaded and ingested normally.\n\nMETS Export of Registered Items\n\nThe METS Export Tool can also be used but note the cautions described in that section and note that MD5 values for items in remote storage are actually MD5 values on just the file name.\n\nDeleting Registered Items\n\nIf a registered item is deleted from DSpace, either interactively or by using the -delete or -replace flags described in Importing Items, the item will disappear from DSpace but it's registered content files will remain in place just as they were prior to registration. Bitstreams not registered but added by DSpace as part of registration, such as license.txt files, will be deleted.\n\nThe experimental (incomplete) METS export tool writes DSpace items to a filesystem with the metadata held in a more standard format based on METS.\n\nThe Export Tool\n\nThis tool is obsolete. Its use is strongly discouraged. Please use the Package Importer and Exporter instead.\n\nThe following are examples of the types of process the METS tool can provide.\n\nExporting an individual item. From the CLI:\n\nExporting a collection. From the CLI:\n\nExporting all the items in DSpace. From the CLI:\n\nLimitations\n\nNo corresponding import tool yet\n\nNo structmap section\n\nSome technical metadata not written, e.g. the primary bitstream in a bundle, original filenames or descriptions.\n\nOnly the MIME type is stored, not the (finer grained) bitstream format.\n\nDublin Core to MODS mapping is very simple, probably needs verification\n\nMediaFilters: Transforming DSpace Content\n\nDSpace can apply filters to content/bitstreams, creating new content. Filters are included that extract text for full-text searching, and create thumbnails for items that contain images. The media filters are controlled by the MediaFilterManager which traverses the asset store, invoking the MediaFilter or FormatFilter classes on bitstreams. The media filter plugin configuration filter.plugins in dspace.cfg contains a list of all enabled media/format filter plugins (see Configuring Media Filters for more information). The media filter system is intended to be run from the command line (or regularly as a cron task):\n\nWith no options, this traverses the asset store, applying media filters to bitstreams, and skipping bitstreams that have already been filtered.\n\nAvailable Command-Line Options:\n\nHelp : [dspace]/bin/dspace filter-media -h\n\nDisplay help message describing all command-line options.\n\nForce mode : [dspace]/bin/dspace filter-media -f\n\nApply filters to ALL bitstreams, even if they've already been filtered. If they've already been filtered, the previously filtered content is overwritten.\n\nIdentifier mode : [dspace]/bin/dspace filter-media -i 123456789/2\n\nRestrict processing to the community, collection, or item named by the identifier - by default, all bitstreams of all items in the repository are processed. The identifier must be a Handle, not a DB key. This option may be combined with any other option.\n\nMaximum mode : [dspace]/bin/dspace filter-media -m 1000\n\nSuspend operation after the specified maximum number of items have been processed - by default, no limit exists. This option may be combined with any other option.\n\nNo-Index mode : [dspace]/bin/dspace filter-media -n\n\nSuppress index creation - by default, a new search index is created for full-text searching. This option suppresses index creation if you intend to run index-update elsewhere.\n\nPlugin mode : [dspace]/bin/dspace filter-media -p \"PDF Text Extractor\",\"Word Text Extractor\"\n\nApply ONLY the filter plugin(s) listed (separated by commas). By default all named filters listed in the filter.plugins field of dspace.cfg are applied. This option may be combined with any other option. WARNING: multiple plugin names must be separated by a comma (i.e. ',') and NOT a comma followed by a space (i.e. ', ').\n\nSkip mode : [dspace]/bin/dspace filter-media -s 123456789/9,123456789/100\n\nSKIP the listed identifiers (separated by commas) during processing. The identifiers must be Handles (not DB Keys). They may refer to items, collections or communities which should be skipped. This option may be combined with any other option. WARNING: multiple identifiers must be separated by a comma (i.e. ',') and NOT a comma followed by a space (i.e. ', ').\n\nNOTE: If you have a large number of identifiers to skip, you may maintain this comma-separated list within a separate file (e.g. filter-skiplist.txt). Use the following format to call the program. Please note the use of the \"grave\" or \"tick\" (`_) symbol and do not use the single quotation. _\n\n[dspace]/bin/dspace filter-media -s `less filter-skiplist.txt`\n\nVerbose mode : [dspace]/bin/dspace filter-media -v\n\nVerbose mode - print all extracted text and other filter details to STDOUT.\n\nAdding your own filters is done by creating a class which implements the org.dspace.app.mediafilter.FormatFilter interface. See the Creating a new Media Filter topic and comments in the source file FormatFilter.java for more information. In theory filters could be implemented in any programming language (C, Perl, etc.) However, they need to be invoked by the Java code in the Media Filter class that you create.\n\nSub-Community Management\n\nDSpace provides an administrative tool‚ 'CommunityFiliator'‚ for managing community sub-structure. Normally this structure seldom changes, but prior to the 1.2 release sub-communities were not supported, so this tool could be used to place existing pre-1.2 communities into a hierarchy. It has two operations, either establishing a community to sub-community relationship, or dis-establishing an existing relationship.\n\nThe familiar parent/child metaphor can be used to explain how it works. Every community in DSpace can be either a 'parent' community‚ meaning it has at least one sub-community, or a 'child' community‚ meaning it is a sub-community of another community, or both or neither. In these terms, an 'orphan' is a community that lacks a parent (although it can be a parent); 'orphans' are referred to as 'top-level' communities in the DSpace user-interface, since there is no parent community 'above' them. The first operation‚ establishing a parent/child relationship - can take place between any community and an orphan. The second operation - removing a parent/child relationship‚ will make the child an orphan.\n\nSet a parent/child relationship, issue the following at the CLI:\n\n(or using the short form)\n\nwhere 's' or '-set' means establish a relationship whereby the community identified by the '-p' parameter becomes the parent of the community identified by the '-c' parameter. Both the 'parentID' and 'childID' values may be handles or database IDs.\n\nThe reverse operation looks like this:\n\n(or using the short form)\n\nwhere 'r' or '-remove' means dis-establish the current relationship in which the community identified by 'parentID' is the parent of the community identified by 'childID'. The outcome will be that the 'childID' community will become an orphan, i.e. a top-level community.\n\nIf the required constraints of operation are violated, an error message will appear explaining the problem, and no change will be made. An example in a removal operation, where the stated child community does not have the stated parent community as its parent: \"Error, child community not a child of parent community\".\n\nIt is possible to effect arbitrary changes to the community hierarchy by chaining the basic operations together. For example, to move a child community from one parent to another, simply perform a 'remove' from its current parent (which will leave it an orphan), followed by a 'set' to its new parent.\n\nIt is important to understand that when any operation is performed, all the sub-structure of the child community follows it. Thus, if a child has itself children (sub-communities), or collections, they will all move with it to its new 'location' in the community tree.\n\nBatch Metadata Editing\n\nDSpace provides a batch metadata editing tool. The batch editing tool is able to produce a comma delimited file in the CVS format. The batch editing tool facilitates the user to perform the following:\n\nBatch editing of metadata (e.g. perform an external spell check)\n\nBatch additions of metadata (e.g. add an abstract to a set of items, add controlled vocabulary such as LCSH)\n\nBatch find and replace of metadata values (e.g. correct misspelled surname across several records)\n\nMass move items between collections\n\nEnable the batch addition of new items (without bitstreams) via a CSV file\n\nRe-order the values in a list (e.g. authors)\n\nExport Function\n\nThe following table summarizes the basics.\n\nExporting Process\n\nTo run the batch editing exporter, at the command line:\n\nExample:\n\nIn the above example we have requested that a collection, assigned handle '1989.1/24' export the entire collection to the file 'col_14.cvs' found in the '/batch_export' directory.\n\nImport Function\n\nThe following table summarizes the basics.\n\nSilent Mode should be used carefully. It is possible (and probable) that you can overlay the wrong data and cause irreparable damage to the database.\n\nImporting Process\n\nTo run the batch importer, at the command line:\n\nExample\n\nIf you are wishing to upload new metadata without bitstreams, at the command line:\n\nIn the above example we threw in all the arguments. This would add the metadata and engage the workflow, notification, and templates to all be applied to the items that are being added.\n\nThe CSV Files\n\nThe csv files that this tool can import and export abide by the RFC4180 CSV format http://www.ietf.org/rfc/rfc4180.txt. This means that new lines, and embedded commas can be included by wrapping elements in double quotes. Double quotes can be included by using two double quotes. The code does all this for you, and any good csv editor such as Excel or OpenOffice will comply with this convention.\n\nFile Structure. The first row of the csv must define the metadata values that the rest of the csv represents. The first column must always be \"id\" which refers to the item's id. All other columns are optional. The other columns contain the dublin core metadata fields that the data is to reside.\n\nA typical heading row looks like:\n\nSubsequent rows in the csv file relate to items. A typical row might look like:\n\nIf you want to store multiple values for a given metadata element, they can be separated with the double-pipe '||' (or another character that you defined in your _dspace.cfg _file. For example:\n\nElements are stored in the database in the order that they appear in the csv file. You can use this to order elements where order may matter, such as authors, or controlled vocabulary such as Library of Congress Subject Headings.\n\nWhen importing a csv file, the importer will overlay the data onto what is already in the repository to determine the differences. It only acts on the contents of the csv file, rather than on the complete item metadata. This means that the CSV file that is exported can be manipulated quite substantially before being re-imported. Rows (items) or Columns (metadata elements) can be removed and will be ignored. For example, if you only want to edit item abstracts, you can remove all of the other columns and just leave the abstract column. (You do need to leave the ID column intact. This is mandatory).\n\nEditing collection membership. Items can be moved between collections by editing the collection handles in the 'collection' column. Multiple collections can be included. The first collection is the 'owning collection'. The owning collection is the primary collection that the item appears in. Subsequent collections (separated by the field separator) are treated as mapped collections. These are the same as using the map item functionality in the DSpace user interface. To move items between collections, or to edit which other collections they are mapped to, change the data in the collection column.\n\nAdding items. New metadata-only items can be added to DSpace using the batch metadata importer. To do this, enter a plus sign '+' in the first 'id' column. The importer will then treat this as a new item. If you are using the command line importer, you will need to use the -e flag to specify the user email address or id of the user that is registered as submitting the items.\n\nDeleting Data. It is possible to perform deletes across the board of certain metadata fields from an exported file. For example, let's say you have used keywords (dc.subject) that need to be removed en masse. You would leave the column (dc.subject) intact, but remove the data in the corresponding rows.\n\nMigrating Data or Exchanging data. It is possible that you have data in one Dublin Core (DC) element and you wish to really have it in another. An example would be that your staff have input Library of Congress Subject Headings in the Subject field (dc.subject) instead of the LCSH field (dc.subject.lcsh). Follow these steps and your data is migrated upon import:\n\nInsert a new column. The first row should be the new metadata element. (We will refer to it as the TARGET)\n\nSelect the column/rows of the data you wish to change. (We will refer to it as the SOURCE)\n\nCut and paste this data into the new column (TARGET) you created in Step 1.\n\nLeave the column (SOURCE) you just cut and pasted from empty. Do not delete it.\n\nChecksum Checker\n\nChecksum Checker is program that can run to verify the checksum of every item within DSpace. Checksum Checker was designed with the idea that most System Administrators will run it from the cron. Depending on the size of the repository choose the options wisely.\n\nThere are three aspects of the Checksum Checker's operation that can be configured:\n\nthe execution mode\n\nthe logging output\n\nthe policy for removing old checksum results from the database\n\nThe user should refer to Chapter 5. Configuration for specific configuration beys in the dspace.cfg file.\n\nChecker Execution Mode\n\nExecution mode can be configured using command line options. Information on the options are found in the previous table above. The different modes are described below.\n\nUnless a particular bitstream or handle is specified, the Checksum Checker will always check bitstreams in order of the least recently checked bitstream. (Note that this means that the most recently ingested bitstreams will be the last ones checked by the Checksum Checker.)\n\nAvailable command line options\n\nLimited-count mode: [dspace]/bin/dspace checker -c To check a specific number of bitstreams. The -c option if followed by an integer, the number of bitstreams to check. Example: [dspace/bin/dspace checker -c 10 This is particularly useful for checking that the checker is executing properly. The Checksum Checker's default execution mode is to check a single bitstream, as if the option was -c 1\n\nDuration mode: [dspace]/bin/dspace checker -d To run the Check for a specific period of time with a time argument. You may use any of the time arguments below: Example: [dspace/bin/dspace checker -d 2h (Checker will run for 2 hours)\n\ns\n\nSeconds\n\nm\n\nMinutes\n\nh\n\nHours\n\nd\n\nDays\n\nw\n\nWeeks\n\ny\n\nYears\n\nThe checker will keep starting new bitstream checks for the specific durations, so actual execution duration will be slightly longer than the specified duration. Bear this in mind when scheduling checks.\n\nSpecific Bitstream mode: [dspace]/bin/dspace checker -b Checker will only look at the internal bitstream IDs. Example: [dspace]/bin/dspace checker -b 112 113 4567 Checker will only check bitstream IDs 112, 113 and 4567.\n\nSpecific Handle mode: [dspace]/bin/dspace checker -a Checker will only check bitstreams within the Community, Community or the item itself. Example: [dspace]/bin/dspace checker -a 123456/999 Checker will only check this handle. If it is a Collection or Community, it will run through the entire Collection or Community.\n\nLooping mode: [dspace]/bin/dspace checker -l or [dspace]/bin/dspace checker -L There are two modes. The lowercase 'el' (-l) specifies to check every bitstream in the repository once. This is recommended for smaller repositories who are able to loop through all their content in just a few hours maximum. An uppercase 'L' (-L) specifies to continuously loops through the repository. This is not recommended for most repository systems. Cron Jobs. For large repositories that cannot be completely checked in a couple of hours, we recommend the -d option in cron.\n\nPruning mode: [dspace]/bin/dspace checker -p The Checksum Checker will store the result of every check in the checksum_history table. By default, successful checksum matches that are eight weeks old or older will be deleted when the -p option is used. (Unsuccessful ones will be retained indefinitely). Without this option, the retention settings are ignored and the database table may grow rather large!\n\nChecker Results Pruning\n\nAs stated above in \"Pruning mode\", the checksum_history table can get rather large, and that running the checker with the -p assists in the size of the checksum_history being kept manageable. The amount of time for which results are retained in the checksum_history table can be modified by one of two methods:\n\nEditing the retention policies in [dspace]/config/dspace.cfg See Chapter 5 Configuration for the property keys. OR\n\nPass in a properties file containing retention policies when using the -p option.To do this, create a file with the following two property keys:\n\nchecker.retention.default = 10y checker.retention.CHECKSUM_MATCH = 8w\n\nYou can use the table above for your time units. At the command line: [dspace]/bin/dspace checker -p retention_file_name <ENTER>\n\nChecker Reporting\n\nChecksum Checker uses log4j to report its results. By default it will report to a log called [dspace]/log/checker.log, and it will report only on bitstreams for which the newly calculated checksum does not match the stored checksum. To report on all bitstreams checked regardless of outcome, use the -v (verbose) command line option:\n\n[dspace]/bin/dspace checker -l -v (This will loop through the repository once and report in detail about every bitstream checked.\n\nTo change the location of the log, or to modify the prefix used on each line of output, edit the [dspace]/config/templates/log4j.properties file and run [dspace]/bin/install_configs.\n\nCron or Automatic Execution of Checksum Checker\n\nYou should schedule the Checksum Checker to run automatically, based on how frequently you backup your DSpace instance (and how long you keep those backups). The size of your repository is also a factor. For very large repositories, you may need to schedule it to run for an hour (e.g. -d 1h option) each evening to ensure it makes it through your entire repository within a week or so. Smaller repositories can likely get by with just running it weekly.\n\nUnix, Linux, or MAC OS. You can schedule it by adding a cron entry similar to the following to the crontab for the user who installed DSpace:\n\nThe above cron entry would schedule the checker to run the checker every Sunday at 400 (4:00 a.m.) for 2 hours. It also specifies to 'prune' the database based on the retention settings in dspace.cfg.\n\nWindows OS. You will be unable to use the checker shell script. Instead, you should use Windows Schedule Tasks to schedule the following command to run at the appropriate times:\n\n(This command should appear on a single line).\n\nAutomated Checksum Checkers' Results\n\nOptionally, you may choose to receive automated emails listing the Checksum Checkers' results. Schedule it to run after the Checksum Checker has completed its processing (otherwise the email may not contain all the results).\n\nYou can also combine options (e.g. -m -c) for combined reports.\n\nCron. Follow the same steps above as you would running checker in cron. Change the time but match the regularity. Remember to schedule this after Checksum Checker has run.\n\nEmbargo\n\nIf you have implemented the Embargo feature, you will need to run it periodically to check for Items with expired embargoes and lift them.\n\nYou must run the Embargo Lifter task periodically to check for items with expired embargoes and lift them from being embargoed. For example, to check the status, at the CLI:\n\nTo lift the actual embargoes on those items that meet the time criteria, at the CLI:\n\nBrowse Index Creation\n\nTo create all the various browse indexes that you define in the Configuration Section (Chapter 5) there are a variety of options available to you. You can see these options below in the command table.\n\nRunning the Indexing Programs\n\nComplete Index Regeneration. By running [dspace]/bin/dspace index-init you will completely regenerate your indexes, tearing down all old tables and reconstructing with the new configuration.\n\nUpdating the Indexes. By running [dspace]/bin/dspace index-update you will reindex your full browse without modifying the table structure. (This should be your default approach if indexing, for example, via a cron job periodically).\n\nDestroy and rebuild. You can destroy and rebuild the database, but do not do the indexing. Output the SQL to do this to the screen and a file, as well as executing it against the database, while being verbose. At the CLI screen:\n\nIndexing Customization\n\nDSpace provides robust browse indexing. It is possible to expand upon the default indexes delivered at the time of the installation. The System Administrator should review \"Defining the Indexes\" from the Chapter 5. Configuration to become familiar with the property keys and the definitions used therein before attempting heavy customizations.\n\nThrough customization is is possible to:\n\nAdd new browse indexes besides the four that are delivered upon installation. Examples:\n\nSeries\n\nSpecific subject fields (Library of Congress Subject Headings.(It is possible to create a browse index based on a controlled vocabulary or thesaurus.)\n\nOther metadata schema fields\n\nCombine metadata fields into one browse\n\nCombine different metadata schemas in one browse\n\nExamples of new browse indexes that are possible.(The system administrator is reminded to read the section on Defining the Indexes in Chapter 5. Configuration.)\n\nAdd a Series Browse. You want to add a new browse using a previously unused metadata element. webui.browse.index.6 = series:metadata:dc.relation.ispartofseries:text:single_Note: the index # need to be adjusted to your browse stanza in the _dspace.cfg file. Also, you will need to update your Messages.properties file.\n\nCombine more than one metadata field into a browse. You may have other title fields used in your repository. You may only want one or two of them added, not all title fields. And/or you may want your series to file in there. webui.browse.index.3 = title:metadata:dc.title,dc:title.uniform,dc:relation.ispartofseries:title:full\n\nSeparate subject browse. You may want to have a separate subject browse limited to only one type of subject. webui.browse.index.7 = lcsubject.metdata:dc.subject.lcsh.text:single\n\nAs one can see, the choices are limited only by your metadata schema, the metadata, and your imagination.\n\nRemember to run index-init after adding any new definitions in the dspace.cfg to have the indexes created and the data indexed.\n\nDSpace Log Converter\n\nWith the release of DSpace 1.6, new statistics software component was added. DSpace's use of SOLR for statics makes it possible to have a database of statistics. This in mind, there is the issue of the older log files and how a site can use them. The following command process is able to convert the existing log files and then import them for SOLR use. The user will need to perform this only once.\n\nThe Log Converter program converts log files from dspace.log into an intermediate format that can be inserted into SOLR.\n\nThe command loads the intermediate log files that have been created by the aforementioned script into SOLR.\n\nAlthough the DSpace Log Convertor applies basic spider filtering (googlebot, yahoo slurp, msnbot), it is far from complete. Please refer to Statistics Client (8.15) for spider removal operations, after converting your old logs.\n\nClient Statistics\n\nNotes:\n\nThe usage of these options is open for the user to choose, If they want to keep spider entires in their repository, they can just mark them using \"-m\" and they will be excluded from statistics queries when \"solr.statistics.query.filter.isBot = true\" in the dspace.cfg.\n\nIf they want to keep the spiders out of the solr repository, they can run just use the \"-i\" option and they will be removed immediately.\n\nThere are guards in place to control what can be defined as an IP range for a bot, in [dspace]/config/spiders, spider IP address ranges have to be at least 3 subnet sections in length 123.123.123 and IP Ranges can only be on the smallest subnet [123.123.123.0 - 123.123.123.255]. If not, loading that row will cause exceptions in the dspace logs and exclude that IP entry.\n\nTest Database\n\nThis command can be used at any time to test for Database connectivity. It will assist in troubleshooting PostgreSQL and Oracle connection issues with the database.\n\nMoving items\n\nIt is possible for administrators to move items one at a time using either the JSPUI or the XMLUI. When editing an item, on the 'Edit item' screen select the 'Move Item' option. To move the item, select the new collection for the item to appear in. When the item is moved, it will take its authorizations (who can READ / WRITE it) with it.\n\nIf you wish for the item to take on the default authorizations of the destination collection, tick the 'Inherit default policies of destination collection' checkbox. This is useful if you are moving an item from a private collection to a public collection, or from a public collection to a private collection.\n\nNote: When selecting the 'Inherit default policies of destination collection' option, ensure that this will not override system-managed authorizations such as those imposed by the embargo system.\n\nItems may also be moved in bulk by using the CSV batch metadata editor (see above)."
    }
}
{
    "id": "dbpedia_8592_0",
    "rank": 43,
    "data": {
        "url": "https://resources.culturalheritage.org/emg-review/volume-5-2017-2018/colloton/",
        "read_more_link": "",
        "language": "en",
        "title": "Rewind, Pause, Playback: Addressing a Media Conservation Backlog at the Denver Art Museum – Electronic Media Review",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://faic.wpenginepowered.com/emg-review/wp-content/uploads/sites/15/2016/07/EMG_667_logo.jpg"
        ],
        "movies": [
            "https://bitcurator.net/bitcurator-nlp/embed/#?secret=meRpI2U0Xc"
        ],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": "https://resources.culturalheritage.org/emg-review/volume-5-2017-2018/colloton/",
        "text": "Eddy Colloton and Kate Moomaw\n\nThe Electronic Media Review, Volume Five: 2017-2018\n\nABSTRACT\n\nWhile the field of electronic media conservation continues to grow in sophistication, museum acquisitions of electronic media artworks have historically outpaced the development of the field and museum professionals’ understanding of the fragility of analog audiovisual materials, software-based artworks, media installation, and other forms of electronic media art. As awareness of electronic media preservation has spread, a need to address the backlog of works already in museum collections has also come into focus. Over the course of the past seven years, the Denver Art Museum has worked to establish institutional practice and policy directed at preserving the electronic media in the museum’s collection and to deepen institutional knowledge of the complexities associated with this new form. However, the museum began collecting and exhibiting electronic media artwork far before this institutional priority was in place. While developing museum wide processes for the exhibition and preservation of new acquisitions, the museum has also taken steps to safeguard the electronic media already in the collection. The effort aimed at addressing the backlog of pressing preservation actions necessary to ensure the sustainability of these electronic media works has resulted in two survey-based projects.\n\nIn 2015, a pilot project to survey 70 electronic media objects from the American Institute of Graphic Arts Design Collection of the Architecture, Design, and Graphics Department resulted in an initial framework for preserving born-digital content. Building on the success of this survey, a broader electronic media conservation project, funded by the Institute of Museum and Library Services, began in December of 2016, and will continue through September of 2018. The goals of the Denver Art Museum’s ongoing grant-funded conservation project affect every media artwork in the collection. Any material from the museum’s collection which had previously been stored on videotapes, optical discs, and external hard drives will be migrated to the museum’s digital repository, and cataloged in the museum’s collection management system. In the process of performing these tasks, video playback equipment, digital storage, and physical storage needs for the institution have been assessed and improved. Much of electronic media conservation literature emphasizes the significance of a particular work’s history, promoting an approach of compiling significant properties through research, in order to determine the work’s identity and basing any treatments on this knowledge. This current project addresses the highest risk factors of the backlog of materials in an efficient and timely manner. Therefore, the survey style of this project does not include complete scrutiny of each object before taking certain actions. This presentation will examine the benefits of the Denver Art Museum’s approach, while also acknowledging the constraints of this pragmatic methodology.\n\nINTRODUCTION\n\nOwing to the exhibition and acquisition of electronic media art often outpacing the development of best practices for the preservation of such artworks, the Denver Art Museum (DAM) has faced a backlog of untreated and minimally cataloged objects necessary for the realization of the museum’s media artwork collection. To combat this backlog, the museum has adopted an iterative, project-based approach to identification and treatment of these objects. The DAM’s Institute of Museum and Library Services (IMLS)–funded conservation project, centered on electronic media, will result in the migration of all media works in the collection to a central server and updated catalog records for each object, either physical or digital, associated with those works.\n\nREWIND: BACKGROUND\n\nThe museum has been collecting electronic media artworks for several decades, both from local artists, such as Gary Emrich, and canonical video artists, such as Nam June Paik. Before established media preservation practices were instituted at the museum, media objects were often stored and described as traditional art objects. Their dimensions carefully noted, minor scratches on cases detailed in condition fields, the objects were then placed in acid-free boxes in art storage. These objects were treated as carefully as they could be by those charged with their stewardship but, with the benefit of hindsight, a lack of familiarity with the technologies involved in these works may have hampered those efforts.\n\nA seminal event in the development of the electronic media preservation practices of the DAM occurred in 2011. An exhibition that featured dozens of media works, “Blink! Light Sound and the Moving Image,” resulted in the acquisition of 15 artworks, mostly single-channel video, including Of the North (2008), by Steina (1940); Telephones (1995), by Christian Marclay (1955); Winchester (2002), by Jeremy Blake (1971); We Can Make It Rain but No One Came to Ask (2005), by Walid Ra’ad (1967); and many others. This exhibition and subsequent acquisitions functioned as a bit of a “wake up call” for the museum.\n\nThe growing awareness at the museum of the complexities of electronic media led to the formation of the Variable Media Working Group, an organization of stakeholders from a variety of different departments. This group was responsible for procedural and policy development, including actively pursuing grant funding and other projects designed to develop their media conservation program. Hiring a full-time, permanent media conservator was (and remains) outside the realm of possibilities for the group; thus, a key challenge was finding a way to understand and stabilize the collection with existing staff and soft funding. This research resulted in a summer internship, which was eventually performed by Eddy Colloton, at the time a graduate student in New York University’s Moving Image Archiving and Preservation program.\n\nThe 2015 internship focused on items from the American Institute of Graphic Arts (AIGA) Design Archive, part of the DAM’s Architecture, Design, and Graphics collection. While the emphasis of this article is the ongoing IMLS grant-funded media preservation project, this internship functioned as a pilot project for many of the practices and policies that the DAM has since implemented. To that end, a brief summary of the museum’s goals and decision-making process for that project will better contextualize the conservation department’s current activities.\n\nThe AIGA collection contains a very wide variety of objects, including posters, packaging for food and toiletry items, and websites. The electronic media from the AIGA Design Archive arrived at the DAM stored on 148 VHS tapes, 193 optical discs, and 38 floppy discs. The collection includes music videos, websites, computer games, and PowerPoint presentations.\n\nWhile the DAM was very aware of a significant backlog of VHS tapes, without playback equipment available and without funding to transfer the tapes through a vendor, it was decided that a viable option for these tapes would be to transcribe their labels and take pictures of them. The removable media objects, on the other hand, would need to be mounted in order to describe them in any meaningful way, which also posed the opportunity to migrate media off of those objects.\n\nWith the help of the museum’s technology department, Archivematica was installed on a BitCurator virtual machine (VM) to facilitate processing and cataloging 70 unique digital media works. This would not have been possible without the collaboration between the conservation, technology (running Archivematica on a BitCurator VM is no small feat), collections, and curatorial departments. Notably, the development department was kept aware of internship activities and Colloton’s desk became a stop on “behind the scenes” tours, where he would inform guests of project activities.\n\nPLAYBACK: IMLS E-MEDIA PROJECT\n\nAt the conclusion of the 2015 internship in August, the conservation department was already working on a grant proposal for a broader preservation project, one that would touch every media object in the museum’s collection. The grant’s narrative highlighted the progress formulated through the internship, citing this as an “initial framework” for preserving digital media at the organization. The grant application was successful, and the project began in December of 2016. Colloton was invited back in an 18-month position as an Assistant Conservator beginning in March of 2017.\n\nThe goals set out in the project narrative are somewhat analogous to a traditional condition survey or risk assessment in that the museum is hoping to develop a better understanding of the contents and condition of the collection. However, it differs in that, as opposed to simply noting condition and prioritizing treatment for a later date, preservation actions are performed as objects are processed and assessed in an attempt to change and improve their condition immediately. Certain preservation actions are essentially required in order to have performed a thorough condition assessment of electronic media. Just as with the 2015 pilot project, the DAM’s approach has been to marry identification and assessment of the media works in the collection with migration, normalization, and other digital preservation actions.\n\nWhile the success of the 2015 pilot project had bolstered confidence, the participants began this project knowing that both throughput and output would be much greater and that the demands on the museum would be much higher. In order to facilitate this, investments in the museum’s media conservation infrastructure were built into the grant. With over 200 works in the collection still stored on videotape, migration of tape-based video formats to file-based formats was clearly going to be a large part of the project. Therefore, playback decks, signal generators, and waveform monitor/vectorscopes were purchased for quality assurance along with hardware upgrades to the media lab’s Mac Pro computer to ease wait times on processing large video files. Grant funds were also allocated to secure a support contract with Artefactual Systems (the developer of Archivematica), additional server infrastructure for storage and hosting of Archivematica, funding for cloud storage through DuraCloud, and contracts with 2 video migration specialists—one local, Postmodern Company, and one in New York, Maurice Shechter and Bill Seery.\n\nOwing to the number of objects in this project, a uniformity to treatment and description would be necessary. This presented somewhat of a challenge, as electronic media in museum collections often resist uniformity. The necessity to treat works on a “case-by-case basis” is often a refrain in the field of electronic media conservation for a reason. Not every videotape, optical disc, or hard drive in the collection should be treated equally—each object needs to be evaluated and understood in context. The balance struck between these two opposing needs, one of assembly-line productivity and the other bespoke contextualization, was to develop a workflow that captured and preserved a large amount of information quickly while leaving options open for future treatment and study. Determining this scope is one of the more complicated aspects of undertaking this type of project. What information to capture, how thorough to be, and when to know to move on are always challenging aspects of a survey, and the preservation actions that conservators are performing as a part of this project did not simplify this conundrum.\n\nAt a very high level, the DAM’s workflow is as follows: All removable media in the collection (hard drives, optical discs, and floppy discs) are disk imaged, most commonly using the Guymager application, part of the BitCurator suite. Metadata describing the disk image is generated at that time and then packaged with the disk image and transferred to Archivematica where, through an ingest process, data are transferred to a CentOS server and then backed up twice, both locally and at an offsite location through DuraCloud.\n\nSimultaneously, existing catalog records are modified and additional records are created to describe the media found on those objects. Descriptions of these objects are primarily technical, such as “this file, on this server, came from this physical object, and it is a video file with this MediaInfo output.” Much of what is needed to know about an object in order to process and catalog that object was ascertained during the first step of the process, disk imaging. Disk imaging allows for sector-by-sector verification of a volume, ensuring the creation of a bit-for-bit copy. Therefore, simply for the assurance of securely copying information, it is a very valuable tool. In addition, the disk image will preserve the original filesystem of a volume in a write-protected environment, allowing the user to mount the volume and explore the directory structure much as to facilitate this, investments in the museum’s media conservation infrastructure were built into the grant. With over 200 works in the collection still stored on videotape, migration of tape-based video formats to file-based formats was clearly going to be a large part of the project. Therefore, playback decks, signal generators, and waveform monitor/vectorscopes were purchased for quality assurance along with hardware upgrades to the media lab’s Mac Pro computer to ease wait times on processing large video files. Grant funds were also allocated to secure a support contract with Artefactual Systems (the developer of Archivematica), additional server infrastructure for storage and hosting of Archivematica, funding for cloud storage through DuraCloud, and contracts with 2 video migration specialists—one local, Postmodern Company, and one in New York, Maurice Shechter and Bill Seery.\n\nOwing to the number of objects in this project, a uniformity to treatment and description would be necessary. This presented somewhat of a challenge, as electronic media in museum collections often resist uniformity. The necessity to treat works on a “case-by-case basis” is often a refrain in the field of electronic media conservation for a reason. Not every videotape, optical disc, or hard drive in the collection should be treated equally—each object needs to be evaluated and understood in context. The balance struck between these two opposing needs, one of assembly-line productivity and the other bespoke contextualization, was to develop a workflow that captured and preserved a large amount of information quickly while leaving options open for future treatment and study. Determining this scope is one of the more complicated aspects of undertaking this type of project. What information to capture, how thorough to be, and when to know to move on are always challenging aspects of a survey, and the preservation actions that conservators are performing as a part of this project did not simplify this conundrum.\n\nAt a very high level, the DAM’s workflow is as follows: All removable media in the collection (hard drives, optical discs, and floppy discs) are disk imaged, most commonly using the Guymager application, part of the BitCurator suite. Metadata describing the disk image is generated at that time and then packaged with the disk image and transferred to Archivematica where, through an ingest process, data are transferred to a CentOS server and then backed up twice, both locally and at an offsite location through DuraCloud.\n\nSimultaneously, existing catalog records are modified and additional records are created to describe the media found on those objects. Descriptions of these objects are primarily technical, such as “this file, on this server, came from this physical object, and it is a video file with this MediaInfo output.” Much of what is needed to know about an object in order to process and catalog that object was ascertained during the first step of the process, disk imaging.\n\nDisk imaging allows for sector-by-sector verification of a volume, ensuring the creation of a bit-for-bit copy. Therefore, simply for the assurance of securely copying information, it is a very valuable tool. In addition, the disk image will preserve the original filesystem of a volume in a write-protected environment, allowing the user to mount the volume and explore the directory structure much as a user of the original volume would have done. Following evaluation, the disk image is transferred from the BitCurator VM to the staging directory for Archivematica as one file instead of many files, averting risk of file corruption or modification upon copy.\n\nThe next step of the DAM’s workflow is to move files through Archivematica. Archivematica is essentially a suite of open-source tools stacked on top of each other. The software uses SleuthKit, MediaInfo, ExifTool, FFmpeg, BagIt, and many other tools to describe and package files for long-term storage as well as access. DAM’s conservators submit disk images to Archivematica and extract the files in those disk images through automation, which is then documented as a PREMIS event in the METS wrapped XML file that the software produces.\n\nThe DAM has chosen to keep both the original disk image and the extracted files. Extracting the files as part of the ingest process through the tsk_recover command (part of SleuthKit) allows one to take advantage of the automated identification and characterization tools that are built into Archivematica. Keeping the original disk image ensures a bit-for-bit copy of the original volume (assuming that no bad sectors were encountered upon imaging).\n\nAs a part of the ingest process, Archivematica facilitates normalization. Normalization is commonly defined in the archive and libraries community as reducing the number of file types that a steward of digital media is required to monitor by converting similar file types to an “archival standard.” In general, but certainly in media conservation, this practice is less and less common. However, there is a benefit in creating a more sustainable derivative of a file if that file is at risk at the time of ingest. While no digital video format that is truly obsolete—that is to say, completely unrenderable under any circumstances—has been encountered during this project, the possibility does not seem unimaginable. The International Association of Sound and Audiovisual Archives (IASA) will slowly be releasing their “Guidelines for the Preservation of Video Recordings” in 2018, but the introduction has already referenced the recommendation of transcoding Windows Media recordings to other formats owing to their lack of interoperability (IASA 2018). Even in this instance, keeping the original file type is the best course of action, but having a sustainable derivative, a copy of the original media in a known file type commonly used by the museum, serves a dual purpose of guarding against the obsolescence of a file format while providing an easily accessible version of the file. The only downside to this is transcoding time and storage space. An aspect of this project that has evolved over time is the project staff’s emphasis on, and understanding of, server performance. Given the workflow’s reliance on Archivematica, our ability to monitor, understand, and improve the performance of the servers is very important. The museum’s original cloud backup strategy serves as a good example. Backups were initially scheduled to run as soon as new media was added to the directory. However, the Amazon Web Service S3 servers, which the museum rents as part of its contract with DuraCloud, have a file size limit of 10 GB per file. DuraCloud has thought of this, of course; the service comes with a Java-based application that automatically “chunks” larger files into 10-GB pieces and then recompiles them upon retrieval. This chunking and uploading, though, demands a significant amount of memory. These problems were revealed through a series of server failures during ingest of files through Archivematica while the server was attempting to upload files to DuraCloud.\n\nThe demands of the two software applications were competing for too few resources; as a result, the server would crash and work would have to be put on hold while troubleshooting the issue. Moving the backups to the evening solved the problem, but several other solutions—such as adding processing power, additional memory, and additional storage space—were tried first.\n\nPAUSE: LESSONS LEARNED\n\nThis realization of the challenges related to working at scale essentially summarizes most of the lessons learned during this project. Complex systems are complex to maintain; the amount and diversity of media in the DAM’s collection (not atypical of an art museum) effectively amounts to a stress test for those systems. There have been ongoing challenges with Archivematica, many of them due to the museum’s instance that Archivematica be updated to an unreleased version of the software shortly before the developers hit a major delay, causing the next stable release to be approximately 7 months late. Troubleshooting the errors presented by Archivematica and exhaustive testing of each subsequent release was necessary. A stable release in May 2018 was ultimately realized.\n\nThe need for routine support from Archivematica developers and the need for frequent communication with our other vendors underscores the labor involved with partnerships. When this project started, the project organizers had viewed the previously mentioned “infrastructure” of contracts with vendors as an investment in efficiency, something that the DAM could “contract out” and therefore not have to worry about. However, the efficiency gained was not as great as anticipated. Museum staff have been highly involved with the work performed by all of its contractors, taking more time and energy than expected away from the pressing task of moving media through the workflow.\n\nPace has been hard to measure in that context. How costly is a delay if solving the delay will improve the workflow? The grant proposal stated that the museum intended to process 10 objects a week, a goal that conservators sometimes exceeded but more often failed to achieve. In the early days of struggling with Archivematica, delays were seen as “growing pains” that could eventually be overcome and eventually benefited from. The goal was to make up for early delays through gained knowledge. Unfortunately, gained knowledge often led to further complexities and ultimately did not yield exponentially faster progress.\n\nIn theory, realistic quotas for a project of this nature could be a very helpful tool. In the course of this project, however, they were difficult to set. The early stages of a project need to account for unexpected delays, including configuration and likely reconfiguration of systems essential to the process. Moreover, not all objects are going to demand the same amount of attention. Thus, “weighting” certain materials, works, or processes to ensure that the appropriate amount of resources have been allocated for specific tasks is of significant importance.\n\nIn spite of these challenges, much has been accomplished at the DAM in a relatively short amount of time, and we know much more about our collection than ever before. The DAM currently has 365 archival information packages (AIPs) in its Archivematica storage service. Those AIPs are comprised of 43,456 files, or around 2.5 TB of data. Additionally, 1,117 catalog records have been created or modified: 325 of those records describe Quicktime video files, 37 HTML files, 117 FFV1 files, 33 WAV files, and 193 records that describe raw disk images.\n\nThe DAM’s collection will be in much better overall condition by the end of this project, but there will still be work to finish. To support this, as works have been processed, project conservators have taken note of works, and types of works, that still need further research and treatment. Chief among these are software-based works that rely on obsolete operating systems. Another example are works that were not collected fully or for which there are better preservation elements, such as websites from the AIGA collection that were delivered to the museum as screenshots. For the more complex media in the collection, more thorough documentation and display specifications would make the works more accessible and less intimidating for exhibition. There is also the matter of justifying continuing costs related to digital preservation. The museum plans to forego further Archivematica support contracts but will still need to budget for costs related to digital storage, such as the offsite storage provided through DuraCloud. How important those costs are will need to be clearly articulated to administrators.\n\nAt the DAM, the ability to complete two preservation projects that have not been driven by programming or acquisition schedules is admittedly a luxury. While the projects have not been without time constraints, these projects have given the museum’s conservators the ability to plan and work exclusively on preservation without interference. This model has been beneficial to the museum for developing procedures and policies and now has resulted in collection-wide preservation actions. The DAM’s iterative, project-based approach to media conservation has allowed it to make large strides in a relatively short amount of time. Hopefully, this approach can serve as a model for addressing similar backlogs at other museums.\n\nACKNOWLEDGMENTS\n\nInstitute of Museum and Library Services (IMLS), Sarah Melching, Chiara Robinson, Bryon Thornburgh, Vince Leon, Rebecca Hart, Darrin Alfred, Maurice Schechter, Bill Seery, David Emrich, Artefactual, Howard Besser, the Variable Media Working Group, Ashley Blewer, Darren Craze, Kelly Stewart, Justin Simpson, and the BitCurator Consortium.\n\nREFERENCES\n\nIASA (International Association of Sound and Audiovisual Archives) Standards, Recommended Practices, and Strategies Technical Committee. 2018. Guidelines for the Preservation of Video Recordings. IASA-TC 06. A-7. https://www.iasa-web.org/sites/default/files/publications/IASA-TC_06-A_20180518.pdf\n\nSOURCES OF MATERIALS\n\nArchivematica\n\nArtefactual Systems, Inc.\n\nNew Westminster, BC, Canada\n\nBitCurator\n\nProject sponsored by:\n\nAndrew W. Mellon Foundation\n\nSchool of Information and Library Science at the University of North Carolina at Chapel Hill\n\nMaryland Institute for Technology in the Humanities\n\nKate Moomaw\n\nAssociate Conservator of Modern and Contemporary Art\n\nDenver Art Museum\n\nkmoomaw@denverartmuseum.org"
    }
}
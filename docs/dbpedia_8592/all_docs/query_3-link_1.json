{
    "id": "dbpedia_8592_3",
    "rank": 1,
    "data": {
        "url": "https://digitalpowrr.niu.edu/digital-preservation-101/tool-grid/",
        "read_more_link": "",
        "language": "en",
        "title": "Tool Grid",
        "top_image": "https://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Cost1.jpg",
        "meta_img": "https://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Cost1.jpg",
        "images": [
            "https://digitalpowrr.niu.edu/wp-content/uploads/2012/09/PowrrBannerBlue1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Copy-2.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Fixity-Check.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Virus-Scan.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/File-Dedupe1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Auto-Unique-ID1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Auto-Metadata-Creation1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Auto-Metadata-Harvest1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Manual-Metadata1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Rights-Management1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Package-Metadata1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Auto-SIP-Creation1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Public-Interface1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Auto-DIP-Creation1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Auto-AIP-Creation1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Reliable-Long-Term-Bit-Preservation1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Redundancy1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/08/Geographically-Dispersed.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Exit-Strategy1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Migration1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Monitoring1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Auto-Recovery1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Open-Source1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Clear-Documentation1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Cost1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Copy-2.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Fixity-Check.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Virus-Scan.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/File-Dedupe1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Auto-Unique-ID1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Auto-Metadata-Creation1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Auto-Metadata-Harvest1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Manual-Metadata1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Rights-Management1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Package-Metadata1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Auto-SIP-Creation1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Public-Interface1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Auto-DIP-Creation1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Auto-AIP-Creation1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Reliable-Long-Term-Bit-Preservation1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Redundancy1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/08/Geographically-Dispersed.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Exit-Strategy1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Migration1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Monitoring1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Auto-Recovery1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Open-Source1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Clear-Documentation1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Cost1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Copy-2.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Fixity-Check.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Virus-Scan.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/File-Dedupe1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Auto-Unique-ID1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Auto-Metadata-Creation1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Auto-Metadata-Harvest1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Manual-Metadata1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Rights-Management1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Package-Metadata1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Auto-SIP-Creation1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Public-Interface1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Auto-DIP-Creation1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Auto-AIP-Creation1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Reliable-Long-Term-Bit-Preservation1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Redundancy1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/08/Geographically-Dispersed.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Exit-Strategy1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Migration1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Monitoring1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Auto-Recovery1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Open-Source1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Clear-Documentation1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Cost1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Copy-2.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Fixity-Check.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Virus-Scan.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/File-Dedupe1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Auto-Unique-ID1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Auto-Metadata-Creation1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Auto-Metadata-Harvest1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Manual-Metadata1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Rights-Management1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Package-Metadata1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Auto-SIP-Creation1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Public-Interface1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Auto-DIP-Creation1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Auto-AIP-Creation1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Reliable-Long-Term-Bit-Preservation1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Redundancy1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/08/Geographically-Dispersed.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Exit-Strategy1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Migration1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Monitoring1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Auto-Recovery1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Open-Source1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Clear-Documentation1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Cost1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Copy-2.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Fixity-Check.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Virus-Scan.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/File-Dedupe1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Auto-Unique-ID1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Auto-Metadata-Creation1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Auto-Metadata-Harvest1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Manual-Metadata1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Rights-Management1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Package-Metadata1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Auto-SIP-Creation1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Public-Interface1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Auto-DIP-Creation1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Auto-AIP-Creation1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Reliable-Long-Term-Bit-Preservation1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Redundancy1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/08/Geographically-Dispersed.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Exit-Strategy1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Migration1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Monitoring1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Auto-Recovery1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Open-Source1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Clear-Documentation1.jpg",
            "http://digitalpowrr.niu.edu/wp-content/uploads/2013/04/Cost1.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "admin"
        ],
        "publish_date": "2013-04-23T21:49:29+00:00",
        "summary": "",
        "meta_description": "Digital POWRR produced this tool grid in 2013 as way for team members to contrast and compare tools that in some way performed digital preservation functions. The tool grid, as it stands now, represents a \"snapshot in time,\" as we are no longer making additions or changes to it. In an effort to centralize community efforts ...",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "Digital POWRR: Preserving digital Objects With Restricted Resources",
        "canonical_link": "https://digitalpowrr.niu.edu/digital-preservation-101/tool-grid/",
        "text": "Digital POWRR produced this tool grid in 2013 as way for team members to contrast and compare tools that in some way performed digital preservation functions. The tool grid, as it stands now, represents a “snapshot in time,” as we are no longer making additions or changes to it. In an effort to centralize community efforts in this space, we contributed the information from the tool grid to COPTR (the Community Owned digital Preservation Tool Registry). The folks there worked to combine the form and function of our original POWRR grid with the far greater coverage of tools and sustainability provided by the COPTR data feed, producing the POWRR Tool Grid v2. Have a look and please help maintain the currency and accuracy of COPTR by submitting your additions and edits!\n\n***********************************************************************************************************************************************************************************************************************\n\nThe information included in the original Tool Grid below was accurate to the best of our knowledge, but some information may be incorrect or have changed. We learned the information from various sources including tool websites, tool registries, contacting the tool developers directly, discussion boards, and some direct tool testing. To learn more details about a tool click on the tool name. The categories and some cost cells will show more information if you hover your cursor over that cell. The dates in the additional information about each tool pertain to the most recent release of software if possible; websites and other information may be more recent.\n\nThe categories addressed are based on the OAIS Reference Model. We know not every tool will meet every part of the OAIS model, but we thought laying them out this way would help people see how they might fit together to do different steps needed for preservation and/or storage.\n\nThe POWRR team has done comprehensive testing of Archivematica, Curator’s Workbench, DuraCloud, Internet Archive, MetaArchive, and Preservica. These tools are highlighted in blue.\n\nHave a question or comment? Please, Contact Us!\n\nDownload an Excel version of the Tool Grid\n\nDigital POWRR\n\nTool Evaluation Grid\n\nIngest\n\nProcessing\n\nAccess\n\nStorage\n\nMaintenance\n\nOther\n\nACE (Audit Control Environment) x x x Free AFF Open Source Computer Forensics Software x x Free Amazon S3 x x x x x x x x x x x Varies Archive-It x x x x x x x x x x x x x x x x x x x x Varies **Archivematica x x x x x x x x x x x x x x x Free BagIt Library x x x x x x BagIt Transfer Utilities Free BitCurator x x x x x x x free BWF MetaEdit x x x x free Carbonite x x x x x x x x $149 – $599 Chronopolis x x x x x x x x x x x x x x x x x x x $1,500 – $2,200 Cinch x x x x x x Free ContextMiner x x Free **Curator’s Workbench x x x x x x x x x Free DAITSS x x x x x x x x x x x DCape (ingest only) x x x x x x x x x x x x x x x x x x DataVerse x x x x x x x Free\n\nDigital POWRR\n\nTool Evaluation Grid\n\nIngest\n\nProcessing\n\nAccess\n\nStorage\n\nMaintenance\n\nOther\n\nDioscuri x x Free DROID (Digital Record Object Identification) x x Free Dropbox x x x x x x x Varies DSpace Direct x x x x x x x x x x x x x x x x x x x x x Varies DSPS (Digital Preservation Software Platform) x x x x x x x x x x x x x x x Free Duke Data Accessioner x x x x x x x x x Free **DuraCloud x x x x x x x x x x x x x x x x x Varies EMET (Embedded Metadata Extraction Tool) x x x Free EXIF to DC XML Normalizer x x x Free Fedora3/Islandora7 x x x x x x x x x x x x x x x x x x x Free FF mpeg x x Free FIDO (Format Identification for Digital Objects) x x x Free FITS (File Information Tool Set) x x x Free Glacier (Amazon) x x x x x x x x x x x Varies Google Cloud x x x x Varies Heritrix x x x x x x x x x Free Hoppla ImageMagick x x x x Free **Internet Archive x x x x x x x x x x x x x x x Free\n\nDigital POWRR\n\nTool Evaluation Grid\n\nIngest\n\nProcessing\n\nAccess\n\nStorage\n\nMaintenance\n\nOther\n\niText x Free JHOVE (JSTOR/Harvard Object Validation Environment) x x Free JHOVE 2 x x x x Free Jpylyzer x x Free MarcAlizer x Free **MetaArchive (A private LOCKSS Network) x x x x x x x x x x x x x x x Varies Metadata Extraction Tool x x x x x x x Free MIXED (Migration to Intermediate XML for Electronic Data) x Free NARA File Analyzer and Metadata Harvesting Tool x x x x Free KEEP Emulation Framework x x Free NESSTAR SERVER x x x x x x x $9,000-$15,000 OCLC Digital Archive x x x x x x x x x x x x x Varies PDF Box x Portico x x x x x x x x x x x x x x x x x x Various PREMIS in METS (PIM) Toolbox x x x x x x x x x x x Preservica(Tessella) x x x x x x x x x x x x x x x x x x x x x x Varies Python Checkm Package x x x RackSpace x x x x x x x x x Varies\n\nDigital POWRR\n\nTool Evaluation Grid\n\nIngest\n\nProcessing\n\nAccess\n\nStorage\n\nMaintenance\n\nOther\n\nRoda x x x x x x x x x x x x x x x x x x x x x Free RODA DBML x Free Rosetta(Ex Libris) x x x x x x x x x x x x x x x x x x x x Varies SAFE Archive Audit System x x x x x x x x x x Varies SIARD x x x Free SIARD-VAL x x x Free WAS (Web Archiving Service) x x x x x x x x x x Unknown Wayback Machine x x x WCT (Web Curator Tool) x x x x Free WindowsAzure x x x Varies Xena x x x x x Free\n\nDigital POWRR\n\nTool Evaluation Grid\n\nIngest\n\nProcessing\n\nAccess\n\nStorage\n\nMaintenance\n\nOther\n\nACE (Audit Control Environment)\n\nDescription: Checks and verifies the integrity of digital objects in long-term repositories according to a policy set by the archive; also allows a third-party auditor to certify the integrity of any object\n\nWebsite(s): https://wiki.umiacs.umd.edu/adapt/index.php/Ace:Main\n\nLast Updated (Checked on April 9th, 2013): 03/22/2013 https://scm.umiacs.umd.edu/redmine/adapt/projects/ace/files\n\nSystem Requirements: Not available.\n\nCustomer Service: Discussion list/board.\n\nAdditional Notes: Monitoring (Audits capacity)\n\nReturn to Chart\n\nAFF Open Source Computer Forensics Software\n\nDescription: AFF Library and Toolkit is a set of programs for working with computer forensic information. Also includes the AFF file format.\n\nWebsite(s): https://github.com/simsong/AFFLIBv3 (Seems to be a similar program – http://www.digital-forensic.org/)\n\nLast Updated (Checked on April 9th, 2013): 03/09/2013 https://github.com/simsong/AFFLIBv3\n\nSystem Requirements: AFFLIB and Toolkit is provided in source code form for Linux, MacOS and Windows.\n\nCustomer Service: Wiki and discussion board, that do not seem highly used.\n\nAdditional Notes: Seems to not really be a “preservation tool”, used for forensic based checking and recovery. From github website: AFF Library and Toolkit is a set of programs for working with computer forensic information. Using these tools you can: * Interconvert disk images between a variety of formats, including: – raw or “dd” – splitraw (in which a single image is split between mulitple files) – EnCase or “E01” format – AFF format (in which the entire disk image is stored in a single file.) – AFD format (in which a disk image is stored in mulitple AFF files stored in a single directory.) – AFM format (in which an AFF file is used to annotate a raw file.) * Compare disk images and report the data or metadata that is different. * Copy disk images from one location to another, with full verification of data, metadata, and the automatic generation of a chain-of-custody segment. * Find errors in an AFF file and fix them. * Print information about a file. * Print detailed statistics about a file * Generate an XML representation of a disk image’s metadata (for example, acquisition time or the serial number of the acquisition device.) * Produce an XML “diskprint” which allows a disk image to be rapidly fingerprinted without having the computer the SHA1 of the entire disk.\n\nAFF, or Advanced Forensics Format, was created to be an extensible open file format for the storage of disk images and related metadata. This software capitalizes on this format, providing users with the tools to create disk images, allowing them to then store disk images and forensic metadata in AFF.\n\nA significant plus of this software is the importance of data tracking; however, the documentation, it seems, is very poor. Without clear documentation the designated community may not be able to understand the data, which is one of the core tenets of the reference model.\n\nThe designated community for the repository that would use this software would have to have a great deal of computer savvy, as the software lacks any GUI component and requires some comfort on the command line. If there are hiccups along the way, finding any customer service information is slightly challenging and requires some Internet searching.\n\nReturn to Chart\n\nAmazon Cloud\n\nDescription: Amazon Cloud is an internet-based storage location designed to hold files indefinitely.\n\nWebsite(s): https://www.amazon.com/clouddrive\n\nLast Updated (Checked on April 9th, 2013): Current (Service)\n\nSystem Requirements: Web-based.\n\nCustomer Service: Available.\n\nAdditional Notes: All functions require the user to be logged into the Amazon account, including customer service and Help. Redunant copies will be kept if uploaded, but are not created automatically.\n\nAmazon Cloud stores data virtually and can be accessed from up to eight different devices, including mobile and tablets. Depending on the budget and amount of file space you require, it may be a relatively cost-effective option; the first five GB are free, and additional GB cost $USD 0.05 per GB per year.\n\nAmazon Cloud appears to be, on several levels, an excellent option for people who wish to conform to the OAIS reference model, particularly as it involves ease of use for the designated community. Amazon Cloud auto generates SIP, AIP, and DIP, thus creating effective metadata across the lifecycle of the data (from a user perspective). The software works with the latest versions of most Internet browsers and requires very little other software to run (Adobe Flash Player and JavaScript). There is a great deal of documentation to be found in a web search thanks to a large user base, and when this fails, there appears to be a fairly strong customer service team to aid with any problems.\n\nUltimately Amazon Cloud’s major failing is its inability to guarantee any reliable long-term bit preservation. The longevity of Amazon Cloud itself cannot be guaranteed, without considering the materials it is storing.\n\nReturn to Chart\n\nArchive-It\n\nDescription: A web archiving service to harvest and preserve digital collections a service of the Internet Archive.\n\nWebsite(s): http://archive-it.org/\n\nLast Updated (Checked on April 9th, 2013): Current (Service)\n\nSystem Requirements: Web browser and internet connection.\n\nCustomer Service: Available.\n\nAdditional Notes: The quote of $4000/yr was given based on an IWU trial evaluation, but pricing varies by both number of URLs crawled and total quantity of data stored; increased storage is available after a certain point is reached. “As mentioned, based on the your use case of archiving (crawling domains 2x a year and social networking sites 1x a month) we think the $4,000 a year Archive-It subscription level would work well for your institution. This would allow your institution to archive up to 4 million documents and 1/4 terabyte of data per year. Also as mentioned we do have higher account levels that would allow for some flexibility in crawling activity. ” According to the quote we receicved an Archive-It subscription is for one calendar year and includes: Web Application training for all users, as well as continued partner support, technical assistance and advanced trainings Scoping tools to expand or limit the extent of the crawl and content Metadata cataloging fields at the collection, seed and document level Ability to capture content using ten (10) different frequencies: twice daily to annual Nine (9) downloadable reports, including a QA reporting tool Content available for viewing 24 hours after the completion of the crawl Archived content includes (not limited to): html, images videos, PDF’s, audio, social networking sites and online newspapers Hosting of the data, access, and storage (two copies: a primary and a back-up copy) Full text search of the collections: basic and advanced options\n\nReturn to Chart\n\nArchivematica\n\nDescription: Archivematica is a free and open-source digital preservation system that is designed to maintain standards-based, long-term access to collections of digital objects. Archivematica uses a micro-services design pattern to provide an integrated suite of software tools that allows users to process digital objects from ingest to access in compliance with the ISO-OAIS functional model. Users monitor and control the micro-services via a web-based dashboard. Archivematica uses METS, PREMIS, Dublin Core and other best practice metadata standards. Archivematica implements format policies based on an analysis of the significant characteristics of file formats.\n\nWebsite(s): https://www.archivematica.org/wiki/Main_Page\n\nLast Updated (Checked on April 9th, 2013): 11/28/2012 https://www.archivematica.org/wiki/Install-0.9-beta\n\nSystem Requirements: Workstation: Dual Core+ CPU, 1GB+ RAM, 7GB HD plus the disk space required for the collection Enabled ports: http, mysqld, gearman, nfs, ssh\n\nCustomer Service: Available.\n\nAdditional Notes:\n\nArchivematica is a digital preservation tool that is web-based and offers a framework in which users can process, preserve, and access digital objects. It is structured around the OAIS Reference model, and, thus, supports SIP, DIP, and AIP creation. While the appeal of a digital preservation tool at no cost might be strong for potential users, there are many features that are not yet supported or still in development.\n\nArchivematica has an extensive wiki documenting its features, functionalities, and development, including screencast video tutorials and user forums. So there is a relatively robust support system in place for users who are getting started, or for more advanced troubleshooting. One criticism, however, is that the wiki is so extensive that it can be redundant and it is occasionally difficult to locate specific pieces of information. For anyone considering adopting Archivematica as a digital preservation system, some important parts of the wiki to consider are the screencasts, and for a more technical perspective the format policies, significant characteristics of file formats, OAIS activity diagrams and use cases.\n\nOne other point to mention is that while many of the processes in the spreadsheet are technically automated, they do require choices to be made by a user at certain steps. For instance, upon ingest, files are normalized automatically, but a user must choose between normalization for preservation and access, normalize for access, do not normalize, and normalize for preservation. After a choice is made, a user also can review reports about the normalization process, preview the DIPs and AIPs that are created, and manually approve process. So following the free kitten caveat, while many processes are automated they do require user participation.\n\nReturn to Chart\n\nBagIt Library\n\nDescription: BagIt is a hierarchical file packaging format for storage and transfer of standardized digital packages called “bags”. Descriptive tags and a payload (arbitrary content) is associated with each bag. This format works for disk or network-based storage and transfer procedures.\n\nWebsite(s): http://sourceforge.net/projects/loc-xferutils/\n\nLast Updated (Checked on April 9th, 2013): 01/03/2013 http://sourceforge.net/projects/loc-xferutils/files/?source=navbar\n\nSystem Requirements: Two software components required: Java Runtime Environment (JRE) and BagIt program. Information: https://confluence.ucop.edu/display/Curation/BagIt\n\nCustomer Service:\n\nAdditional Notes: Checksum is generated for each file prior to transfer. BagIt can run in several programming environments. http://www.records.ncdcr.gov/erecords/Using_BagIt_ver2%202_generic_final_20110414.pdf\n\nReturn to Chart\n\nBagIt Transfer Utilities\n\nDescription: BagIt transfer Utilities are a collection of tools developed for the purpose of validation and transfer of bags.\n\nWebsite(s): http://sourceforge.net/projects/loc-xferutils/ http://www.dcc.ac.uk/resources/external/bagit-library\n\nLast Updated (Checked on April 9th, 2013): 01/03/2013 http://sourceforge.net/projects/loc-xferutils/files/?source=navbar\n\nSystem Requirements:\n\nCustomer Service:\n\nAdditional Notes:\n\nReturn to Chart\n\nBitCurator\n\nDescription: BitCurator is built on a stack of free and open source digital forensics tools and associated software libraries, modified and packaged for increased accessibility and functionality for collecting institutions.\n\nWebsite(s): http://www.bitcurator.net/\n\nLast Updated (Checked on March 24th, 2014): 03/19/2014 http://wiki.bitcurator.net/index.php?title=Main_Page\n\nSystem Requirements:\n\nCustomer Service:\n\nAdditional Notes:\n\nBWF MetaEdit\n\nDescription: Permits Embedding, Editing and Exporting of metadata in Broadcast Wave Format\n\nWebsite(s): http://bwfmetaedit.sourceforge.net/\n\nLast Updated (Checked on April 9th, 2013): 10/13/2010 http://sourceforge.net/projects/bwfmetaedit/files/?source=navbar\n\nSystem Requirements:\n\nCustomer Service:\n\nAdditional Notes: Federal Agencies Digitization Guidelines Initiative (FADGI) Fixity Check on audio files only. Rights Management encoded in metadata.\n\nReturn to Chart\n\nCarbonite\n\nDescription: An online backup service that automatically backs up documents, e-mails, music, photos, and settings.\n\nWebsite(s): http://www.carbonite.com/en/v2/index\n\nLast Updated (Checked on April 9th, 2013): Current (Service)\n\nSystem Requirements: Customer Service: Available.\n\nAdditional Notes: This software backs up selected content “continuously” and keeps older content that has been updated for 90 days only. It is not intended to be a long term (or short term) digital preservation solution.\n\nReturn to Chart\n\nChronopolis\n\nDescription: “Chronopolis digital preservation network provides services for the long-term preservation and curation of America’s digital holdings”\n\nWebsite(s): http://chronopolis.sdsc.edu/\n\nLast Updated (Checked on April 9th, 2013): Current (Service)\n\nSystem Requirements:\n\nCustomer Service: Available.\n\nAdditional Notes:\n\nReturn to Chart\n\nCinch\n\nDescription: CINCH is a web-based, open source, lightweight tool that was designed to help libraries, archives, and agencies with similar mandates to collect and authenticate digital content that is freely available on the web.\n\nWebsite(s): http://cinch.nclive.org/Cinch/site/index\n\nLast Updated (Checked on March 24th, 2014): Current http://slnc-dimp.github.io/Cinch/\n\nSystem Requirements:\n\nCustomer Service:\n\nAdditional Notes:\n\nReturn to Chart\n\nContextMiner\n\nDescription: ContextMiner is a framework to collect, analyze, and present the contextual information along with the data. It is based on an idea that while describing or archiving an object, contextual information helps to make sense of that object or to preserve it better. This website provides tools to collect data, metadata, and contextual information off the Web by automated crawls.\n\nWebsite(s): http://www.contextminer.org\n\nLast Updated (Checked on April 9th, 2013): Current (Service) – Download in Version 4 (Date Unknown) http://www.contextminer.org/about.php\n\nSystem Requirements: Web-based\n\nCustomer Service: Available at http://www.contextminer.org/contact.php\n\nAdditional Notes:\n\nReturn to Chart\n\nCurator’s Workbench (More information will be added after testing phase)\n\nDescription: The Curator’s Workbench is a tool that automates and streamlines the process of preparing collections of digital materials for submission to a repository.\n\nWebsite(s): https://github.com/UNC-Libraries/Curators-Workbench\n\nLast Updated (Checked on April 9th, 2013): 04/08/2013 https://github.com/UNC-Libraries/Curators-Workbench\n\nSystem Requirements:\n\nCustomer Service:\n\nAdditional Notes:\n\nReturn to Chart\n\nDAITSS\n\nDescription: A digital preservation software application designed as a dark archive to service consortial and institutional preservation repositories a multi-user environment type. DAITSS is considered to be a first-party system.\n\nWebsite(s): http://daitss.fcla.edu/\n\nLast Updated (Checked on April 9th, 2013): 03/21/2013 https://github.com/daitss\n\nSystem Requirements: A series of DAITSS system dependencies need install and build, including the use of Sun Jave 6, http://daitss.fcla.edu/sites/daitss.fcla.edu/files/installmanual.pdf\n\nCustomer Service:\n\nAdditional Notes: System used at FDA as “dark archive”. One must a depositor to access content uppon requests however.\n\nReturn to Chart\n\nDCape (ingest only)\n\nDescription: “The goal of the DCAPE project is to build a distributed production preservation environment that meets the needs of archival repositories for trusted archival preservation services.” (Note: This is a work in progress, see notes for more information)\n\nWebsite(s): http://salt.unc.edu/dcape/index.html\n\nLast Updated (Checked on April 9th, 2013): Last update in Dec 2011 – No software yet released\n\nSystem Requirements:\n\nCustomer Service:\n\nAdditional Notes: This project is a work in progress, which many of the sources of this information has come from proposal documents. There is no software at this time and limited details on how some of these goals with be accomplished. ***Last status report was on Dec 2011. No content updates to the site have been made in 2012. Redundancy calls for functions to distribute.\n\nReturn to Chart\n\nDataVerse\n\nDescription: Allows organizations to host a storage and access system for research materials. The software creates self-contained ‘dataverses,’ each of which is designed to support individual researchers or research groups.\n\nWebsite(s): http://thedata.org/ http://sourceforge.net/projects/dvn/develop\n\nLast Updated (Checked on April 9th, 2013): 01/18/2013 http://sourceforge.net/projects/dvn/files/\n\nSystem Requirements: Java, glassfish application server, postgres db, lucene, but the server should have at least 2GB memory to test and production depends on load but 16GB and 1TB disk space might be advisable: http://guides.thedata.org/book/1-system-requirements\n\nCustomer Service: Available.\n\nAdditional Notes:Focus on data sets and the program supports SPSS, STATA, and CSV files. Customer Service is an e-mail which they attempt to respond to within one business day. ADDITIONAL System Req: Operating system – The production version of the Dataverse Network at IQSS (dvn.iq.harvard.edu) runs on RedHat Linux 5. Most of the DVN development is currently done on MacOS X. Because of our experience with RedHat and MacOS X these are the recommended platforms. You should be able to deploy the application .ear file on any other platform that supports Java. However, the automated installer we provide will likely work on RedHat and MacOS only. Some information provided in this guide is specific to these 2 operating systems. (Any OS-specific instructions/examples will be clearly marked, for example:[MacOS-specific:]) CPU – The production IQSS Dataverse Network runs on generic, multi-core 64-bit processors. Memory – The application servers currently in production at the IQSS have 64 GB of memory each. Development and testing systems require a minimum of 2 gigabyte of memory. Disk space – How much disk space is required depends on the amount of data that you expect to serve. The IQSS Dataverse Network file system is a standalone NetApp with 2 TB volume dedicated to the DVN data. Multiple servers – All the DVN components can run on the same server. On a busy, hard-working production network the load can be split across multiple servers. The 3 main components, the application server (Glassfish), the database (Postgres) and R can each run on its own host. Furthermore, m\n\nReturn to Chart\n\nDioscuri\n\nDescription: Dioscuri is a computer hardware emulator, specifically designed to be used as part of a digital preservation strategy.\n\nWebsite(s): http://dioscuri.sourceforge.net/\n\nLast Updated (Checked on April 9th, 2013): 01/20/2011 http://sourceforge.net/projects/dioscuri/files/\n\nSystem Requirements: Java Platform SE, version 1.6 or higher; Java Virtual Machine; several Megabytes of free hard disk space.\n\nCustomer Service: Online forum available\n\nAdditional Notes: Appears to be an emulator only.\n\nReturn to Chart\n\nDROID (Digital Record Object Identification)\n\nDescription: DROID stands for Digital Record Object IDentification. It’s a free software tool developed by The National Archives that will help you to automatically profile a wide range of file formats. For example, it will tell you what versions you have, their age and size, and when they were last changed. It can also provide you with data to help you find duplicates. Profiling your file formats helps you to manage your information more effectively. It helps you to identify risks (and therefore plan mitigating actions).\n\nWebsite(s): http://sourceforge.net/projects/droid/ http://www.nationalarchives.gov.uk/information-management/our-services/dc-file-profiling-tool.htm\n\nLast Updated (Checked on April 9th, 2013): 01/24/2013 http://sourceforge.net/projects/droid/?source=navbar\n\nSystem Requirements: DROID requires Java 1.6 Standard Edition (SE) and should work on any platform which supports that. DROID has been built and tested on: Ubuntu 11.10 Server 32 bit / OpenJDK Java 1.6.0_24 Ubuntu 12.04 Server 32 bit / OpenJDK Java 1.6.0_24 Ubuntu 12.04 Desktop 64 bit / Oracle Java 1.6 update 34 Red Hat Enterprise Linux Server 6.3 64 bit / OpenJDK 1.6.0_24 CentOS 6.2 64 bit / Oracle Java 1.6 update 33 Linux Mint 13 Maya 64 bit / OpenJDK Java 1.6.0_24 Microsoft Windows 7 (64 bit) / Oracle Java 1.6 update 34 Microsoft Windows Server 2008 R2 (64 bit) / Oracle Java 1.6 update 32 Mac OS X 10.8 (Mountain Lion) / Java 1.6.0_29-b11 Note: DROID 6 will not currently function with Java 7.\n\nCustomer Service: User support is available from our Google Groups discussion page. Bug-fixes, feature requests, and code support should be raised through our Github issues page. We welcome contributions of new formats and format signatures to the PRONOM registry. If you wish to contribute, please use our online form.\n\nAdditional Notes: DROID is a software tool developed by The National Archives to perform automated batch identification of file formats. Developed by The National Archive’s Digital Preservation Department as part of its broader digital preservation activities, DROID is designed to meet the fundamental requirement of any digital repository to be able to identify the precise format of all stored digital objects, and to link that identification to a central registry of technical information about that format and its dependencies.\n\nDROID is a file profiling tool that can identify file type, name, extension, size, content hash, modification date, and more. It was developed by the National Archives in the United Kingdom for digital continuity of users’ information, as a part of digital preservation. DROID can also help check for duplicate files which can then be eliminated to reduce data capacity. However, since this tool only functions as a file identifier, it does not support many general digital preservation functions.\n\nDROID does not support migration or recovery, because its role is to identify risks in users’ data and to aid the user in planning migration, not to facilitate the actual migration process.\n\nIn terms of processing, DROID fully supports auto and manual metadata creation, and partially supports rights management. It generates information only from what is embedded in the object, but not from external repositories or elsewhere. Also, signature files can be manually added to DROID by the user. DROID partially supports rights management; the user can establish or limit access to objects’ profiles and working area that are generated by DROID, but cannot manage the access of the object itself through DROID.\n\nReturn to Chart\n\nDropbox\n\nDescription: Dropbox is a free service that lets you bring all your photos, docs, and videos anywhere. This means that any file you save to your Dropbox will automatically save to all your computers, phones and even the Dropbox website. Dropbox also makes it super easy to share with others, whether you’re a student or professional, parent or grandparent. Even if you accidentally spill a latte on your laptop, have no fear! You can relax knowing that Dropbox always has you covered, and none of your stuff will ever be lost.\n\nWebsite(s): https://www.dropbox.com/\n\nLast Updated (Checked on April 9th, 2013): Current (Service)\n\nSystem Requirements: Exit Strategy (via zip file) —- Dropbox desktop application The Dropbox application runs on Windows, Mac OS X, and Linux operating systems. To run the Dropbox application on your computer we recommend: At least 512MB of RAMAn amount of free space on your computer equal to the amount you want to store on DropboxWindows 2003, Windows XP, Windows Vista, and Windows 7 and 8 (32- and 64-bit)Mac: OS X Tiger (10.4) or laterUbuntu 7.10+ and Fedora Core 9+. Make sure your partition is mounted with support for extended attributes (xattrs). Also see the advanced section below for Nautilus/GNOME integration requirements Dropbox website You can transfer and download files using the Dropbox website from any modern browser. The Dropbox website works best on: Internet Explorer 7+.All recent versions of Google ChromeSafari 3+Firefox 3+Opera 9+ Dropbox on mobile phones Dropbox is also accessible via several mobile devices: iPhone App – iPhone (any model) or iPod TouchiOS v4.3 or laterAll video features require an iPhone 3GS, iPhone 4, or the latest iPod Touch with a capable video cameraAn Apple ID to access the App Store –iPad App-An iPadiOS v4.3 or laterAn Apple ID to access the App Store –Android App-Android OS 2.1 (Eclair) or higher –BlackBerry App-BlackBerry OS 4.5 and up.Music playback is supported on devices with BlackBerry OS 4.6 and up.Video/audio format support varies depending on device and OS version. Read more about video/audio support from the BlackBerry website –Mobile browser-The mobile-optimized version of Dropbox should work on any web browser. Simply bookmark http://m.dropbox.com\n\nCustomer Service: Available.\n\nAdditional Notes: DROID uses internal signatures to identify and report the specific file format and version of digital files. These signatures are stored in an XML signature file, generated from information recorded in the PRONOM technical registry. New and updated signatures are regularly added to PRONOM, and DROID can be configured to automatically download updated signature files.\n\nFor individual users, Dropbox provides 2 GB of storage for free, and users can acquire up to a total of 18 GB for free by recommending other users to Dropbox. For larger organizations, prices range from $750 a year for five user accounts to $31,420 a year for 250 accounts; organizations wishing to add more than 250 accounts must request a quote.\n\nIn addition to its web interface, Dropbox offers software clients for the major desktop operating systems (Windows, Mac and Linux) and a variety of mobile platforms (Android, iOS, Blackberry, etc.). Through the desktop clients, users are able to create folders on their local machine which, if they wish, they may synchronize with their folders on the cloud.\n\nDropbox does not store users’ files in their own data centers; instead they make use of Amazon’s Simple Storage Service (S3). Although Dropbox may in practice be used with some frequency as a tool to back up personal digital materials, it was not designed with the managed, long-term preservation of institutional archives in mind. As such, it does not fully support many of the OAIS Reference Model functions.\n\nDropbox’s main failings are that it cannot accommodate the requirements for long-term preservation outlined by the OAIS reference model. Dropbox does not perform a fixity check.\n\nDropbox is more appropriate for personal archival purposes rather than institutional repository services. While the services that Dropbox provides seem to offer a more reliable solution for saving data in a more permanent way, they cannot be relied upon for long-term bit preservation, and depending on the file size of what needs to be stored, it may not be a viable solution. It meets and falls short of the objectives of the OAIS reference model.\n\nReturn to Chart\n\nDPSP (Digital Preservation Software Platform)\n\nDescription: The Digital Preservation Software Platform (DPSP) is free and open source software developed by the National Archives of Australia. The DPSP is a collection of software applications which support the goal of digital preservation. It includes Xena, DPR, Checksum Checker, and Manifest Maker.\n\nWebsite(s): http://dpsp.sourceforge.net/\n\nLast Updated (Checked on April 9th, 2013): 07/18/2012 http://dpsp.sourceforge.net/download.php\n\nSystem Requirements: You will need to install Java 1.6 or lower before installing DPSP.\n\nCustomer Service:\n\nAdditional Notes: Auto Metadata Creation is minimal. — Package Metadata (embedded in file in XML). — Another website with info – http://www.alliancepermanentaccess.org/index.php/preservationsoftware/dsps-digital-preservation-software-platform/ Unable to find a group or community forum about using the software.\n\nReturn to Chart\n\nDSpace Direct\n\nDescription: DSpaceDirect is a hosted service from the DuraSpace non-profit organization that allows users to store, organize, and manage DSpace repository content in the cloud. DSpaceDirect can be used to preserve and provide access to academic faculty and student papers, projects, and research making content easily searchable by end users and easily managed by content curators.\n\nWebsite(s): http://www.dspacedirect.org/\n\nLast Updated (Checked on April 16th, 2014): .Current (Service)\n\nSystem Requirements: N/A. DSpaceDirect is a fully hosted software as a service managed by DuraSpace. For customers to access their account, they simply need an Internet connection and an Internet browser.\n\nCustomer Service: Ten support requests included in annual. Extended support package available for purchase at $1,000/year.\n\nAdditional Notes: DSpaceDirect allows customers to get a repository up and running quickly with a feature set that customers can choose to customize. Included in the annual subscription are regular software upgrades as well as a complete integration with DuraCloud that provides an ongoing synchronized backup of all content stored in DSpaceDirect. Through the DSpaceDirect interface, content can be made open access via the web or restricted to specific users or groups of users. Because the DSpaceDirect service is running the DSpace open source software, customers can choose to move their repository and its contents to another location at any time.\n\nReturn to Chart\n\nDuke Data Accessioner\n\nDescription: The Duke Data Accessioner provides a graphical user interface to aid in migrating data from physical media to a dedicated file server, documenting the process and using MD5 checksums to identify any errors introduced in transfer. The software also offers a way to integrate metadata tools into the migration workflow. The tool is primarily designed for use by technical services librarians in small institutions with little or no IT support.\n\nWebsite(s): http://www.dcc.ac.uk/resources/external/duke-data-accessioner\n\nLast Updated (Checked on March 24th, 2014): The Data Accessioner Version 0.3.0 was last updated in September 2011.\n\nA download is also available for Version 0.4.0, which is described as under development.\n\nSystem Requirements:\n\nCustomer Service:\n\nAdditional Notes: Duke Data Accessioner is very easy to use.\n\nReturn to Chart\n\nDuraCloud\n\nDescription: DuraCloud is a hosted service from the Duraspace non-profit organization that provides a centralized interface for organizations interested in using cloud storage as a part of their digital archiving and preservation programs. DuraCloud is integrated with commercial and academic cloud storage providers such as Amazon Web Services and the San Diego Supercomputer Cloud storage center storing content within these existing infrastructures. DuraCloud enables users to store multiple copies of their content on multiple clouds, all while keeping each copy synchronized in an ongoing fashion as well as checking the integrity of all content stored on a regularly scheduled basis.\n\nWebsite(s): http://www.duracloud.org/\n\nLast Updated (Checked on April 16th, 2014): Current (Service)\n\nSystem Requirements: Web portal for most users. Local option The DuraCloud software will build and run on Windows XP using Tomcat 6.0.29 and Maven 2.2.1. If you are working in this or a similar configuration, two recommended environments settings are: CATALINA_OPTS=”-server -Xmx512m” MAVEN_OPTS=”-Xmx1024m -XX:MaxPermSize=1024m” Note that not all DuraCloud services will run in a Windows environment. Refer to the documentation about building DuraCloud from source for more information about building and running the DuraCloud software.\n\nCustomer Service: Ten support requests included in annual subscription. Extended support package available for purchase at $1,000/year.\n\nAdditional Notes: DuraCloud is a web-based hosted service that is integrated with multiple cloud storage providers. Customers of the service can choose the level of access they provide to their account and content as well as the number of cloud storage providers their account is integrated with. When customers subscribe for more than one cloud storage provider on their account, all of the content is automatically copied and synchronized among all cloud storage providers. All DuraCloud accounts include automatic regualrly scheduled bit integrity checking of all content stored as well as storage reports available through the web interface.\n\nDuraCloud is a Java-based, open source, cloud-based preservation system created by DuraSpace the not-for-profit organization responsible for the DSpace and Fedora repository platforms. DuraCloud also includes a set of tools and services that act as a preservation-centered interface for storage shares which are provided by a third party (storage providers includes Amazon S3, Rackspace, etc.). DuraCloud provides an account and open–source software tools that manage these third party cloud spaces, allowing for automated duplication across vendors, access and ingest, and fixity checking.\n\nDuraCloud is primarily focused on the “Archival Storage” function of the OAIS model and so supports ingest and retrieval from the storage space, but provides limited data management functions. DuraCloud is not meant to be an exact implementation of the OAIS, or any other preservation model (“FAQ | DuraCloud, see “what preservation standards does DuraCloud support”), but, instead, provides a hosted preservation service that enables organizations to manage multiple, geographically distributed, redundant copies of their digital content that is being stored in multiple cloud storage vendors. This provides a buffer to the risks of vendor lock-in, the business cycle, and physical disaster while maintaining user control over the data itself.\n\nThe user is responsible for tracking of item metadata and rights information, and integrating DuraCloud with their overall system. While users can provide additional metadata, DuraCloud stores only the small amount of administrative metadata used to identify, retrieve, and authenticate each bit stream. DuraCloud does, however, integrate with systems that provide metadata management and user-facing access to items, including DSpace, Fedora, Archive-It, and DSpaceDirect.\n\nReturn to Chart\n\nEMET (Embedded Metadata Extraction Tool)\n\nDescription: EMET is an image metadata extraction tool intended to facilitate the management and preservation of digital images and their incorporation into external databases and applications. EMET was created by ARTstor through funding from NDIIPP.\n\nWebsite(s): sourceforge.net/projects/emet/ http://emet.sourceforge.net/\n\nLast Updated (Checked on April 9th, 2013): 11/29/2010 http://sourceforge.net/projects/emet/files/?source=navbar\n\nSystem Requirements: EMET is compatible with Mac OS 10.4+, as well as Windows XP, Windows Vista, and Windows 7. EMET also requires Adobe Air 2.0.\n\nCustomer Service: Discussion Forum\n\nAdditional Notes:\n\nReturn to Chart\n\nEXIF to DC XML Normalizer\n\nDescription: The purpose of this tool is to extract EXIF data and normalise it to DC XML. This OS-independent tool enables you to read EXIF-data from an image file using ExifTool and normalise this to Dublin Core compatible XML adding specific metadata which is contained in an .ini file. The XML output templates can be tailored to one’s needs. Tool is able to handle single files, or recurse trough a folder. Output is written to STDOUT, unless an output XML file is specified. Please note this tool is a prototype and adds very specific institutional metadata from The State University of New York at Binghamton, though the code can be changed very easy to your own needs.\n\nWebsite(s): http://wiki.opf-labs.org/display/SPR/Maintain+a+list+of+metadata+mappings+outside+of+the+script https://github.com/openplanets/SPRUCE/tree/master/exif_to_dc\n\nLast Updated (Checked on April 9th, 2013): 09/26/2012 https://github.com/openplanets/SPRUCE/commits/master/exif_to_dc\n\nSystem Requirements:\n\nCustomer Service:\n\nAdditional Notes: User community comments at http://wiki.opf-labs.org/display/SPR/How+to+maintain+a+list+of+metadata+mappings+outside+of+the+script. This tool for converting data captured when images and audio files are created and not directly about storage or preservation though the data may assist in those activities. THE MOST INTRIGUING THING is that it seems like a solution for my University photo file problem because of it’s ability to capture embedded image metadata as described at http://wiki.opf-labs.org/display/SPR/University+Photographs+with+embeded+metadata.\n\nReturn to Chart\n\nFedora3/Islandora7\n\nDescription: Islandora is an open-source software framework designed to help institutions and organizations and their audiences collaboratively manage, and discover digital assets using a best-practices framework. Islandora was originally developed by the University of Prince Edward Island’s Robertson Library, but is now implemented and contributed to by an ever-growing international community.\n\nWebsite(s): http://islandora.ca/\n\nLast Updated (Checked on June 11, 2014): 05/07/2014 (https://wiki.duraspace.org/display/ISLANDORA/Islandora)\n\nSystem Requirements:\n\nCustomer Service:\n\nAdditional Notes: Fedora 4 promises to include some additional features. In terms of redundancy, Fedora 3 does support some replication, redundancy, failover, etc. and there have been some successful projects that have explored this further. There will be more options in Fedora 4, which offers better support for clustering and policy-driven storage.\n\nFedora4 will also allow for easier usage of a geographically dispersed data storage model.\n\nA self-healing auto recovery ability also be available in Fedora4.\n\nReturn to Chart\n\nFF mpeg\n\nDescription: FFmpeg is a complete, cross-platform solution to record, convert and stream audio and video. It includes libavcodec – the leading audio/video codec library. See the documentation for a complete feature list and the Changelog for recent changes. (Documentation at http://ffmpeg.org/ffmpeg.html; Changelog at http://git.videolan.org/?p=ffmpeg.git;a=blob_plain;f=Changelog).\n\nWebsite(s): http://ffmpeg.org/\n\nLast Updated (Checked on April 9th, 2013): 03/15/2013 http://ffmpeg.org\n\nSystem Requirements:\n\nCustomer Service:\n\nAdditional Notes: Keep up with the latest developments by subscribing to both the ffmpeg-devel and ffmpeg-cvslog (http://ffmpeg.org/contact.html) lists. This is not a tool for preservation, but the resizing may appeal to some people for access purposes.\n\nReturn to Chart\n\nFIDO (Format Identification for Digital Objects)\n\nDescription: Format Identification for Digital Objects (FIDO) is a Python command-line tool to identify the file formats of digital objects. It is designed for simple integration into automated work-flows.\n\nWebsite(s): http://www.openplanetsfoundation.org/software/fido\n\nLast Updated (Checked on April 9th, 2013): 01-10-2013 http://www.openplanetsfoundation.org/software/fido\n\nSystem Requirements: Linux- or Windows-based\n\nCustomer Service:\n\nAdditional Notes: “Fido is a simple format identification tool for digital objects that uses Pronom signatures. It converts signatures into regular expressions and applies them directly. Fido is free, Apache 2.0 licensed, easy to install, and runs on Windows and Linux. Most importantly, Fido is very fast.” Adam Farqhuar (http://www.openplanetsfoundation.org/node/563). This is not directly about storage or preservation though the data generated may assist in those activities.\n\nReturn to Chart\n\nFITS (File Information Tool Set)\n\nDescription: FITS allows data curators to identify, validate, and extract technical metadata for the objects in their digital repository. It does this by incorporating a range of mostly third-party open source tools, normalizing and consolidating their output.\n\nWebsite(s): http://code.google.com/p/fits/\n\nLast Updated (Checked on April 9th, 2013): 03/18/2013 http://code.google.com/p/fits/\n\nSystem Requirements: Instructions for command line use are given for Windows and Unix.\n\nCustomer Service: Available.\n\nAdditional Notes: Identifies and extracts Technical Metadata only. FITS is written in Java and is compatible with Java 1.6 or higher. It uses six external tools: JHOVE, Exiftool, National Library of New Zealand Metadata Extractor, DROID, FFIdent, and Windows File Utility. Language of user manual is written more for IT audiences than archive/library audiences. Software offers a user discussion board.\n\nReturn to Chart\n\nGlacier (Amazon)\n\nDescription:\n\nWebsite(s): http://aws.amazon.com/glacier/\n\nLast Updated (Checked on July 24th, 2013): Current (Service).\n\nSystem Requirements:\n\nCustomer Service:\n\nAdditional Notes:\n\nReturn to Chart\n\nGoogle Cloud\n\nDescription: Google Cloud Storage allows users to store, access, and manage their data.\n\nWebsite(s): https://cloud.google.com/products/cloud-storage\n\nLast Updated (Checked on April 9th, 2013): Current (Service)\n\nSystem Requirements: Must have access to Google Cloud Storage – Must have Python 2.6 or Python 2.7 installed -Must have the newest version of gsutil installed.\n\nCustomer Service: Available.\n\nAdditional Notes: Customer Service operates as a discussion board for the time being. This link is to a google cloud administrator forum – http://productforums.google.com/forum/#!categories/apps/google-apps-cloud-connect. Customer service representative not able to answer detailed questions about how the storage actually works.\n\nGoogle Cloud allows users to build applications and websites, store data, and analyze data on Google’s infrastructure. Like many Google products, Google Cloud is ready-to-use right out of the box. However, some people do not like using Google products because so much of the process is handled by company, and they require information some may not be comfortable sharing.\n\nMany features of Google Cloud could be implemented with a solid understanding of the Google API, which may or may not require some in-house programming or a significant amount of time spent searching out specific solutions. Depending on the expertise of the employees in a given small archive, this may or may not be a significant limitation. Still, Google has a lot of support and documentation, therefore it is unlikely to prove overly challenging for those with computer literacy. Furthermore this availability of resources may prove to be a benefit for the consumers; as ease of use and understandability for the designated community is essential, this bodes well for the compliance of Google Cloud with the OAIS reference model.\n\nOne of the biggest issues with Google Cloud is that the exit strategy is apparently not streamlined. In a case where an exit strategy was needed, Google’s solution is little more than a data dump.\n\nFor smaller archives, Google Cloud and Google’s “micromanaging” of so much of the process, may be a boon rather than a curse. Several of the steps are handled by Google which takes pressure off the archive’s resources. For others who want to manage much of this in-house, it may be a less-than ideal solution.\n\nReturn to Chart\n\nHeritrix\n\nDescription: Heritrix is an open-source web crawler, allowing users to target websites they wish to include in a collection and to harvest an instance of each site. The software is most often used as a powerful back-end tool incorporated into a web archiving workflow.\n\nWebsite(s): https://webarchive.jira.com/wiki/display/Heritrix/Heritrix\n\nLast Updated (Checked on April 9th, 2013): 05/2012 https://webarchive.jira.com/wiki/display/Heritrix/Heritrix#Heritrix-LatestReleases\n\nSystem Requirements: Linux distribution-Running Heritrix requires a Linux distribution. Heritrix may run on other platforms, but this option is not supported. Java Runtime Environment (JRE) 1.6-Heritrix requires JRE 1.6. JRE 1.6 can be downloaded at: http://www.javasoft.com or http://www.ibm.com/java. Note that Java 6 update 23 and update 24 (and possibly later) cannot be used with Heritrix 3.0 or earlier due to a bug in the JRE’s GZIP functionality, which Heritrix relies on to read ARCs/WARCs. As of Hertirix 3.1, this issue is fixed. Hardware-The default Java heap for Heritrix is 256MB RAM, which is usually suitable for crawls that range over hundreds of hosts. Assign more of your available RAM to the heap if you are crawling thousands of hosts or experience Java out-of-memory problems. You can use the JAVA_OPTS variable to configure memory. See Heritrix Configuration.\n\nCustomer Service: Available.\n\nAdditional Notes: Public Interface (Archive-IT) — Heritrix is installed via a command line interface, but once installed the user can launch a web-based interface for configuration. Setting up a crawl requires a significant number of adjustments. Installation requires solid knowledge of Linux and command line interfaces. As with any web archiving software, deep understanding of the project’s scope and collections policy is essential in order to set up appropriate targets. In addition, may require a new large server to hold crawls.\n\nReturn to Chart\n\nHoppla (More information will be added after our project testing phase)\n\nDescription:\n\nWebsite(s): http://www.ifs.tuwien.ac.at/dp/hoppla/release/index.html http://www.ifs.tuwien.ac.at/dp/hoppla/release/FAQs.html\n\nLast Updated (Checked on April 9th, 2013): 09/27/2010 http://www.ifs.tuwien.ac.at/dp/hoppla/release/index.html\n\nSystem Requirements:\n\nCustomer Service:\n\nAdditional Notes:\n\nReturn to Chart\n\nImageMagick\n\nDescription: ImageMagick® is a software suite to create, edit, compose, or convert bitmap images. It can read and write images in a variety of formats (over 100) including DPX, EXR, GIF, JPEG, JPEG-2000, PDF, PhotoCD, PNG, Postscript, SVG, and TIFF. Use ImageMagick to resize, flip, mirror, rotate, distort, shear and transform images, adjust image colors, apply various special effects, or draw text, lines, polygons, ellipses and Bézier curves.\n\nWebsite(s): http://www.imagemagick.org/script/index.php\n\nLast Updated (Checked on April 9th, 2013): Current release is ImageMagick 6.8.4-8. Date Unknown\n\nSystem Requirements: ImageMagick runs on Windows XP (x86) with Service Pack 3, Windows Vista (x86 & x64) with Service Pack 2, Windows 7 (x86 & x64), Windows Server 2003 (x86 & x64) with Service Pack 2 (verify MSXML6 is present), Windows Server 2003 R2 (x86 & x64), Windows Server 2008 (x86 & x64) with Service Pack 2, and Windows Server 2008 R2 (x64). The amount of memory can be an important factor, especially if you intend to work on large images. A minimum of 512 MB of RAM is recommended, but the more RAM the better.\n\nCustomer Service: Available.\n\nAdditional Notes: Robust discussion board and blogs/web sites where command lines are listed with examples.\n\nImage Magick is a software tool for creating, editing, and composing bitmap images; it is not a tool for general archival purposes. This software only works with images, which may benefit archives where the majority of the collection is made up of images that need to be preserved, but it is not a “one-stop shop” for the needs of digital preservation.\n\nImage Magick does have a great deal of documentation available. Users across all functional entities in the OAIS reference model will be able to find answers to questions about most tasks due to the fact that the software had a great number of users.\n\nThis system does not meet the OAIS reference model requirements simply because, while it is a wonderful tool for the creation, conversion, and metadata manipulation of images, it is by no means a robust archival tool for long-term preservation.\n\nReturn to Chart\n\nInternet Archive\n\nDescription: The Internet Archive is a 501(c)(3) non-profit that was founded to build an Internet library. Its purposes include offering permanent access for researchers, historians, scholars, people with disabilities, and the general public to historical collections that exist in digital format.\n\nFounded in 1996 and located in San Francisco, the Archive has been receiving data donations from Alexa Internet and others. In late 1999, the organization started to grow to include more well-rounded collections. Now the Internet Archive includes: texts,audio, moving images, and software as well as archived web pages in our collections, and provides specialized services for adaptive reading and information access for the blind and other persons with disabilities.\n\nWebsite(s): https://archive.org/about/\n\nLast Updated (Checked on April 9th, 2013):\n\nSystem Requirements:\n\nCustomer Service: Questions should be directed to info@archive.org\n\nAdditional Notes:\n\nReturn to Chart\n\niText\n\nDescription: iText ® is a library that allows you to create and manipulate PDF documents. It enables developers looking to enhance web- and other applications with dynamic PDF document generation and/or manipulation\n\nWebsite(s): http://itextpdf.com/terms-of-use/index.php\n\nLast Updated (Checked on April 9th, 2013): 02/14/2013 http://sourceforge.net/projects/itext/files/?source=navbar\n\nSystem Requirements: Depending on your use of iText, you need some extra jars or libraries: -If you want to compile iText (or iTextSharp), you’ll need three extra libraries: bcprov, bcmail and bctsp. You’ll also need them at runtime if your code involves encryption or digital signing. These libraries were released by The Legion of the Bouncy Castle. -If you are using CJK (Chinese, Japanese, Korean) font technology (fonts that aren’t embedded in the PDF, but that use font packs you can install with Adobe Reader), you need an extra iTextAsian.jar. -If you are using hyphenation, you need an extra itext-hyph-xml.jar. This jar contains files distributed with the Apache FOP library.\n\nCustomer Service: Available.\n\nAdditional Notes: There is a free option for costumer service that includes subscribing to a mailing list and then sending your question to that mailing list. – Support Forums: http://support.itextpdf.com/forum\n\nReturn to Chart\n\nJHOVE (JSTOR/Harvard Object Validation Environment)\n\nDescription: JHOVE allows data curators to verify the file formats of the digital objects in their repositories.\n\nWebsite(s): http://sourceforge.net/projects/jhove/ http://jhove.sourceforge.net/\n\nLast Updated (Checked on April 9th, 2013): 12/17/2012 http://sourceforge.net/projects/jhove/files/?source=navbar\n\nSystem Requirements: At least Java 1.5\n\nCustomer Service: Limited unpaid personnel responds as available.\n\nAdditional Notes:\n\nJHOVE is a product of collaboration between JSTOR – the humanities journal database – and Harvard University. This open source product performs one set of tasks: it identifies, validates, and characterizes the formats of digital objects. This modular tool can be implemented throughout an item’s digital lifecycle and a data manager’s workflow.\n\nJHOVE plays a part in the OAIS Reference Model – characterizing the item during the ingestion process as well as periodically validating and characterizing the item once it is in the repository. JHOVE verifies the integrity of the data, an important aspect of data management. When a file arrives without file format information JHOVE steps in and makes a determination of the file type. If an item arrives with file format information, JHOVE can validate it, ensuring that no incorrect or malicious file types are ingested. Finally, JHOVE characterizes file types or representative information. This gives the administrator a bigger picture of the file type and status.\n\nJHOVE-JSTOR/Harvard Object Validation Environment is a tool developed by Harvard University Library for identification, validation, and extracting metadata of file formats. All these functions are performed by modules. The JHOVE main-page provides a list of documentation including Tutorial, JHOVE API, Practice (writing JHOVE Module/Output Handler), Schemas and Specifications (standard JHOVE modules) ( at http://jhove.sourceforge.net/documentation.html). As the source code is available at GNU Lesser General Public License (LGPL), users can develop new format modules in Java. After development, a new version of JHOVE (JHOVE 2) is available (https://bitbucket.org/jhove2/main/wiki/Home). As a free and open-source tool, JHOVE does not have official customer support, but it has a project mailing list (https://lists.sourceforge.net/lists/listinfo/jhove-discuss) to which bug reports and specific feature requests can be referred. The developer of JHOVE also has a file format blog (http://fileformats.wordpress.com/) that discusses (but is not limited to) more general problems about JHOVE and ideas for further development (JHOVE2).\n\nCurrently there are twelve different modules that can be used to identify, validate and extract metadata of digital objects. JHOVE supports the following modules:\n\nText: ASCII, UTF-8, PDF\n\nImage: AIFF, TIFF, GIF, JPEG, JPEG2000\n\nAudio: WAVE\n\nMarkup Language: HTML, XML\n\nReturn to Chart\n\nJHOVE 2\n\nDescription: JHOVE2 allows data curators to characterise the digital objects in their repositories using four methods: identifying formats, validating specifications, extracting metadata, and assessing acceptability.\n\nWebsite(s): https://bitbucket.org/jhove2/main/wiki/Home\n\nLast Updated (Checked on April 9th, 2013): 03/18/2013 https://bitbucket.org/jhove2/main/wiki/Home\n\nSystem Requirements: Java SE 6 Java Runtime Environment\n\nCustomer Service: Community of users for support; Listserv; also has an “issue tracker” which contains all reported bugs and feature enhancement requests.\n\nAdditional Notes: User Guide is extremely helpful; https://bytebucket.org/jhove2/main/wiki/documents/JHOVE2-Users-Guide_20110222.pdf; Pulls characterization from many atypical file formats, which could be useful. It does NOT support AIFF, GIF, HTML, and JPEG.\n\nReturn to Chart\n\nJpylyzer\n\nDescription: Jpylyzer checks that a JP2 files really conforms to JP2 format specificiations, it also can extract the technical charateristics of each image.\n\nWebsite(s): http://openplanetsfoundation.org/software/jpylyzer\n\nLast Updated (Checked on April 9th, 2013): 02/28/2013 https://github.com/openplanets/jpylyzer/commits/master\n\nSystem Requirements: There are 3 options for Jpylyzer. 1 – Use the Python source code which requires a recent version of the Python interpreter. 2. Use the Windows stand-alone binaries. 3.Use a Debian package (for Linux users).\n\nCustomer Service:\n\nAdditional Notes:\n\nReturn to Chart\n\nMarcAlizer\n\nDescription: Tool determines if two web pages are similar (identical?) or not. It is apparently now known as pagelyzer.\n\nWebsite(s): http://wiki.opf-labs.org/pages/viewpage.action?pageld=14352721&navigatingVersions=true\n\nLast Updated (Checked on April 9th, 2013): 02/28/2013 http://wiki.opf-labs.org/display/TR/Pagelyzer\n\nSystem Requirements: Ruby 1.9.1; rubygems\n\nCustomer Service: None apparently available.\n\nAdditional Notes:\n\nReturn to Chart\n\nMetaArchive\n\nMore information will be added about this tool after our formal testing period.\n\nDescription:\n\nWebsite(s):\n\nLast Updated (Checked on April 9th, 2013):\n\nSystem Requirements:\n\nCustomer Service:\n\nAdditional Notes:\n\nReturn to Chart\n\nMetadata Extraction Tool\n\nDescription: Developed by the National Library of New Zealand to programmatically extract preservation metadata for resource discovery. The Metadata Extraction Tool is based on a library of adapters. Open source software, yet with both a Microsoft Windows interface and a UNIX command line interface. The tool can be automated for batch processing or on demand. It is written in Java and XML and distributed under the Apache Public License (version 2).\n\nWebsite(s): http://meta-extractor.sourceforge.net/\n\nLast Updated (Checked on April 9th, 2013): 06/16/2010 http://sourceforge.net/projects/meta-extractor/files/\n\nSystem Requirements: Java, XSL (XSLT/XPath/XSL-FO)\n\nCustomer Service: Available.\n\nAdditional Notes: Extractors are currently provided a wide variety of formats such as PDF documents, image files, sound files Microsoft office documents and more. Metadata Extraction Tool says the best way to get help with its software is by visiting http://sourceforge.net/tracker/?func=add&group_id=189407&atid=929203.\n\nReturn to Chart\n\nMIXED (Migration to Intermediate XML for Electronic Data)\n\nDescription: MIXED (Migration to Intermediate XML for Electronic Data) is a web service that converts tabular data files such as spreadsheets and databases to the Standard Data Format for Preservation (SDFP), a supplier-independent XML format.\n\nWebsite(s): https://sites.google.com/a/datanetworkservice.nl/mixed/\n\nLast Updated (Checked on April 9th, 2013): Current (Service)\n\nSystem Requirements:\n\nCustomer Service:\n\nAdditional Notes: “A very evident limitation is that only data content is converted, no layout or formulas are converted.”\n\nReturn to Chart\n\nNARA File Analyzer and Metadata Harvesting Tool\n\nDescription: The application allows a user to analyze the contents of a file system or external drive and generates statistics about the contents of the contained directories. The application can be used to generate checksum values to ensure the bit-level integrity of files after they have been copied to a new device. After a collection of files have been converted from one digital format to another, this application can verify that there is a one-to-one match of before and after files.\n\nWebsite(s): http://blogs.archives.gov/online-public-access/?p=6270\n\nLast Updated (Checked on April 9th, 2013): 10/02/2012 https://github.com/usnationalarchives/File-Analyzer/commits/master\n\nSystem Requirements: The basic File Analyzer application is deployed as a self-extracting jar file. The application requires Java SE 1.6 or higher to be present on the user’s workstation. The application can be launched by double clicking the jar file.\n\nCustomer Service: Unlikely; Could not be determined.\n\nAdditional Notes:\n\nReturn to Chart\n\nKEEP Emulation Framework\n\nDescription: Allows users to view and interact with digital files that otherwise would require obsolete hardware and software.\n\nWebsite(s): http://emuframework.sourceforge.net/\n\nLast Updated (Checked on April 9th, 2013): 04/11/2012 http://sourceforge.net/projects/emuframework/files/\n\nSystem Requirements: None listed.\n\nCustomer Service: Support forums, “How to”documents, and e-mail. Professional consultation for an additional fee.\n\nAdditional Notes:\n\nThe KEEP Emulation Framework (EF) (http://emuframework.sourceforge.net/index.html) allows users to open files and run programs in their native environment. It was created by the international KEEP (Keeping Emulation Environments Portable) project and co-funded by the European Union’s 7th Framework Programme. While it is not an emulator itself, KEEP takes an object-centered approach to identify the environment needed to render a particular object. When a user chooses a file to render, KEEP determines the correct environment and searches for it within the software and emulators configured and maintained by the system administrator. If KEEP finds a suitable environment in which to render the file, it configures the emulator, which renders the object.\n\nKEEP provides a substantial level of functionality by creating an environment in which to maintain the emulators and software needed to render obscure and outdated materials. While KEEP comes pre-loaded with some emulators and software applications, any additional emulators and/or software must be configured manually. The EF pulls information from an external emulator archive, a software archive, and a technical metadata registry.\n\nEmulation is often seen as an alternative method of digital preservation, but tools like KEEP can help make emulation a viable choice in certain situations. However, because it works differently than other preservation methods, it does not provide all of the functions of the OAIS Reference Model. The real purpose of the emulation framework is the access functionality, which uses the technical metadata from the object to determine and set up an interface to render and interact with the object. Rather than migrating obsolete files and programs to a modern format, KEEP provides access by preserving the original look and feel of the original object. While from the emulation stage, a particular emulation might provide some OAIS functionality, the KEEP framework itself does not store or maintain digital objects to be rendered. It does however allow for user maintenance of the external software and emulator archives used to render objects.\n\nReturn to Chart\n\nNESSTAR SERVER\n\nDescription: The Nesstar suite is an online publishing platform for organizations wishing to share datasets both internally and with the wider web.\n\nWebsite(s): http://www.nesstar.com/software/server.html\n\nLast Updated (Checked on April 9th, 2013): Current version 4.0 – Date Unknown http://www.nesstar.com/software/download.html\n\nSystem Requirements: Windows XP/ 2000/Vista/Win7, Windows Server 2003, 2008 R2- all with recent Service Packs installed. 512Mb RAM. 1Gb is preferable. You may optimise the amount of memory your server is allocated by setting the “Java Heap Size” under Advanced Settings in the configuration tool. 1Ghz CPU or better. A biprocessor machine can be useful, especially during upgrading when the new and old server may be running side by side.\n\nCustomer Service: Software patches, new releases/updates, and help desk.\n\nAdditional Notes: Nesstar is owned by Norwegian Social Science Data Services (NSD) and can be contacted at nsd@nsd.uib.no.\n\nReturn to Chart\n\nOCLC Digital Archive\n\nDescription: The Digital Archive provides a secure storage environment for you to easily manage and monitor the health of your master files and digital originals.\n\nWebsite(s): http://www.oclc.org/digital-archive.en.html\n\nLast Updated (Checked on April 9th, 2013): Current (Service)\n\nSystem Requirements:\n\nCustomer Service:\n\nAdditional Notes:\n\nIt is difficult to ascertain the Digital Archive’s entire range of features and policies from the materials which OCLC makes available. OCLC provides a short “Getting Started Guide” for collection administrators, but aside from that, thorough documentation on the system’s features and its backend processes could not be found (nor did OCLC respond to several email requests for more information). Therefore, the information about the system’s features, such as it is, was obtained from secondary sources, should not be considered completely reliable, and should be verified against information from OCLC itself.\n\nFrom what can be gleaned from OCLC’s “Getting Started Guide” for the Digital Archive, at Ingest, the system checks a file for viruses, generates a checksum, checks the digital content against a “shipping manifest”, and verifies the file’s format using JHOVE. The system would seem to rather robustly fulfill the Archival Storage function, although it would behoove a prospective user to acquire more information on their storage mechanisms, maintenance policies, and disaster recovery plans. According to the “Guide”, the Digital Archive “performs regular checks on the health of your content. The results of those checks are summarized in a File Integrity Report”.\n\nAlong the same lines, it is unclear, under the rubric of Data Management, how exactly OCLC maintains descriptive information. Again, according to their “Getting Started Guide”, “You may include metadata files along with content files. If you do this you may want to create either a directory structure or a naming structure to associate metadata files specifically with content files. Metadata files will be treated as preservation objects like any other file”.\n\nIt would appear that the Digital Archive also fulfills the Administration function, although the details of the actual data standards OCLC has developed, e.g. their storage or security policies, or the procedures of daily operation, are, has been previously noted, not made widely available, as is similarly the case with the details of what sort of Preservation Planning they provide.\n\nFinally, a bit more information is at least provided on the system’s fulfillment of the Access function. According to their “Getting Started Guide”, organizations can access their content in the system either by email (for bulk requests) or by web browser (for files smaller than 1 GB). A report is produced each time a dissemination request has been processed; there is, however, a charge for bulk dissemination requests.\n\nPDF Box\n\nDescription: Open source Java tool. Several features permit, in part to create PDF, extraction of content, encryption, PDF/A validation and integration of the Lucene search engine.\n\nWebsite(s): http://pdfbox.apache.org/\n\nLast Updated (Checked on April 9th, 2013): 03/22/2013 http://archive.apache.org/dist/pdfbox/?C=M;O=A\n\nSystem Requirements:\n\nCustomer Service:\n\nAdditional Notes: Apache license v2.0\n\nReturn to Chart\n\nPortico\n\nDescription: Portico is among the largest community-supported digital archives in the world. Working with libraries, publishers, and funders, we preserve e-journals, e-books, and other electronic scholarly content to ensure researchers and students will have access to it in the future. It is a service that libraries and publishers can contribute to with their funds and collections.\n\nWebsite(s): http://www.portico.org/digital-preservation/\n\nLast Updated (Checked on April 9th, 2013): Current (Service)\n\nSystem Requirements: None.\n\nCustomer Service: User Support and all other inquiries: 301 E. Liberty St., Suite 250 Ann Arbor, MI 48104, USA 734 887-7087 or toll-free 877 422-4022 in the USA support@portico.org\n\nAdditional Notes: Clear openness to changing preservation plans as the need arises. However, seems to lean toward dark archive. Ingest Copies – A master and copy are not automatically made, but access copies are created only as needed. Virus Scan is completed as needed. File dedupe is available and under improvement. Auto Metadata Harvest – In general, content is provied to Portico, however there are instances where Portico fetches content. Portico content is already replicated at 4 locations. Auto Recovery note – Corrections are made to files, but they believe it is important to have a person make recovery decisions. Open Source note – Portico uses NOID for ARK (identifier) creation and participates on the development of JHOVE (open source files format verification tool).\n\nReturn to Chart\n\nPREMIS in METS (PIM) Toolbox\n\nDescription: The PREMIS in METS Toolbox is a browser-based application which supports the implementation of PREMIS in the METS container format.\n\nWebsite(s): http://www.loc.gov/standards/premis/\n\nLast Updated (Checked on April 9th, 2013): 05/16/2012 (Version 2.2) http://www.loc.gov/standards/premis/version-2-2-announcement.html\n\nSystem Requirements:\n\nCustomer Service:\n\nAdditional Notes: http://blogs.loc.gov/digitalpreservation/2011/10/premis-for-digital-preservation/\n\nReturn to Chart\n\nPreservica(Tessella)\n\nDescription: Preservica offers a digital preservation option that stores and maintains the files you would like to preserve. It uses cloud based storage and allows users to interface through web browsers.\n\nWebsite(s): http://www.preservica.com/\n\nLast Updated (Checked on April 9th, 2013): Current (Service).\n\nSystem Requirements: None.\n\nCustomer Service: Available.\n\nAdditional Notes: Cloud based.\n\nOther features including: Metadata quality assurance (Verifies that metadata documents are compliant with stated schemas);\n\nVerification of integrity of package (Every content file described in the metadata is present and no additional files have been submitted);\n\nIdentifier validated (Checks that each entity identifier does not class with existing identifiers in the archive);\n\nPre-configured workflows (Pre-configured workflows are available with the system to allow immediate use for ingesting mixed content, web harvesting, normalization etc);\n\nPrepares for characterization (Checks file formats and other attributes (e.g. encryption) to determine if characterization can occur);\n\nFile validation (Checks files for conformance against stated format specification);\n\nApplies access restrictions (Ensures asserted access restrictions are applied);\n\nSearch (Searches across all metadata fields. All searches are governed by access rights as described in the system);\n\nBrowse (Ability to browse through all content, as governed by access rights, and view an audit trail showing history of the content since ingested as well as all descriptive metadata);\n\nCharacterization (Measure and store the technology-dependent, technical properties of the content files);\n\nPreservation Planning (Allows a user to specify what might be at risk or obsolete (e.g. a format) and most appropriate actions to take (e.g. migration).);\n\nPreservation Action (Perform the planned actions and characterizes the output. The before and after values of the essential characteristics are then compared ot determine the success or failure of the migration.);\n\nReturn to Chart\n\nPython Checkm Package\n\nDescription: Python implementation of checkm specification – generates a checksum fixity check.\n\nWebsite(s): https://github.com/benosteen/checkm\n\nLast Updated (Checked on April 9th, 2013): 06/22/2011 https://github.com/benosteen/checkm/commits/master\n\nSystem Requirements:\n\nCustomer Service:\n\nAdditional Notes: Monitoring (Fixity Check)\n\nReturn to Chart\n\nRackSpace\n\nDescription: RackSpace provices cloud based services to businesses of all sizes through the world.\n\nWebsite(s): www.rackspace.com\n\nLast Updated (Checked on April 9th, 2013): Current (Service)\n\nSystem Requirements: Remote computing ability.\n\nCustomer Service: Available.\n\nAdditional Notes: Service provider for cloud infrastructure components such as sites, servers and storage. Built upon the OpenStack architecture project and also has proprietary API’s for server and file management.\n\nReturn to Chart\n\nRoda\n\nDescription: RODA is a digital repository created in conjunction with the Portuguese National Archives. Its primary objective is to allow long-term preservation and authenticity of digital objects of national importance. Based as much as possible on standards, following international specifications to become a trustworthy repository, as the OAIS/TRAC, and having high standards of security and scalability. The user interface was also an important consideration, as it was designed for archivists, producers, and consumers.\n\nWebsite(s): https://redmine.keep.pt/projects/roda-public\n\nLast Updated (Checked on April 9th, 2013): Unable to find update. Unable to find download file.\n\nSystem Requirements: Requirements depend on the amount of data and usage of the system, but minimum requirements are 1 CPU 2GHZ, 2GB of RAM and 5GB of free disk space. Also, RODA needs to be run under Linux operative system or use a virtual machine.\n\nCustomer Service: Can get support from the community using the users and developers mailing lists at http://groups.roda-community.org, view past discussions at http://www.mail-archive.com/users@groups.roda-community.org/ , add and discuss RODA issues on https://github.com/keeps/roda/issues. Also, commercial support is provided by KEEP SOLUTIONS .\n\nAdditional Notes: Other websites: http://roda-community.org/?locale=en#home – http://alanake.wordpress.com/2009/06/26/roda-portugals-new-digital-preservation-repository/ – http://roda.di.uminho.pt/?locale=en#home (Demo site) – https://redmine.keep.pt/projects/roda-public/boards (Forum Boards) –\n\nReturn to Chart\n\nRODA DBML\n\nDescription: Migrates databases to an XML schema, DBML. Can then provide access by dumping DBML to MySQL and showing it in phpMyAdmin\n\nWebsite(s):http://redmine.keep.pt/documents/show/3\n\nLast Updated (Checked on April 9th, 2013): Unable to find update. Unable to find download file.\n\nSystem Requirements: Requirements depend on the amount of data and usage of the system, but minimum requirements are 1 CPU 2GHZ, 2GB of RAM and 5GB of free disk space. Also, RODA needs to be run under Linux operative system or use a virtual machine.\n\nCustomer Service: Github; migrating site, not much available.\n\nAdditional Notes: http://roda-community.org/ Part of RODA, but I can’t find any info about this specific tool on the RODA site.\n\nReturn to Chart\n\nRosetta(Ex Libris)\n\nDescription: Designed in collaboration with the National Library of New Zealand and reviewed by an international peer group of recognized leaders and innovators, Ex Libris Rosetta enables institutions to preserve and provide access to the collections in their care, now and in the future.\n\nWebsite(s): http://www.exlibrisgroup.com/category/RosettaOverview\n\nLast Updated (Checked on April 9th, 2013): Current (Service)\n\nSystem Requirements: Operating system/platform: Linux – Red Hat AS/ES 5.x and 6.0 64bit Sun: Solaris 10 on a 64bit x86 processor CPUs Minimum of 8 GB; RAM Minimum of 16 GB Database – Oracle Free disk space – at least 300 GB for various directories and DB files\n\nCustomer Service: Specific e-mail and phone for various countries.\n\nAdditional Notes:Virus Scan (the software uses the customer anti virus program) File Dedupe (should be configured by the customer) Package Metadata ( has a published METS Profile, see Ex Libris’ Rosetta METS Profile at: http://www.loc.gov/standards/mets/mets-registered-profiles.html) Public Interface (using discovery systems like Ex Libris’ Primo) Reliable, Long-Term Bit Preservation (in cooperation with robust storage layers) Redundancy & Geographically Dispersed Data (Rosetta doesn’t mange the copies, this is done by the storage layer and Rosetta gets notifications about its status.)\n\nReturn to Chart\n\nSAFE Archive Audit System\n\nDescription: SafeArchive is a solution for archival storage and replication management. It is an auditing tool for policy-driven, distributed replication. The current version of SafeArchive is a self-contained system that can be installed, used and maintained by institutional staff without extensive technical expertise. The set of open source tools can easily be used by libraries, museums and archives.\n\nWebsite(s): http://www.safearchive.org\n\nLast Updated (Checked on April 9th, 2013): 10/24/2012 http://www.safearchive.org/download\n\nSystem Requirements: Hardware Standard Installation Advanced Installation CPU Two cores @ 2.4 GHz Four or more cores @ 2.4 GHz Memory 4 GB 8 GB or more Storage 60 GB 120 GB or more Operating System RHEL 6 / CEntOS 6 RHEL 6 / CEntOS 6\n\nCustomer Service: Available, e-mail archival-replication-tech@googlegroups.com\n\nAdditional Notes:May be a connection to Amazon Cloud.\n\nReturn to Chart\n\nSIARD\n\nDescription: With SIARD (Software Independent Archiving of Relational Databases), the Swiss Federal Archives (SFA) provide a sustainable solution for the long-term preservation of relational databases. This includes an open format for archiving of relational databases as well as a freeware package – “SIARD Suite” – for converting relational databases into the SIARD format.\n\nWebsite(s): http://www.bar.admin.ch/dienstleistungen/00823/00825/index.html?lang=en\n\nLast Updated (Checked on April 9th, 2013): Unable to find update. Unable to access download file.\n\nSystem Requirements: SIARD Suite is platform and vendor independent. It operates in a Java 1.5 (or higher) environment, running on Windows, Linux and Mac OS X.\n\nCustomer Service: They say training is available but we see no indication that they go beyond their specific user base for the gov’t archives and no link to a user group. However, I did find that they have extensive support documentation for a range of standardized actions.\n\nAdditional Notes: The SIARD format was developed by the SFA and is part of the digital archiving platform of the European research project Planets. The format specifications are open and published. All database submissions to the Swiss Federal Archives are now performed in the SIARD format. SIARD Suite is based on international standards such as XML, SQL:1999 and UNICODE. At present the application supports the following databases: Oracle, Microsoft SQL Server, MySQL and Microsoft Access.\n\nReturn to Chart\n\nSIARD-VAL\n\nDescription: A tool to validate SIARD (Software Independent Archiving of Relational Databases) database archiving containers, which are created with SIARD.\n\nWebsite(s): http://wiki.opf-labs.org/display/TR/SIARD-VAL\n\nLast Updated (Checked on April 9th, 2013): Unable to find update. Unable to find download file.\n\nSystem Requirements:\n\nCustomer Service:\n\nAdditional Notes: Tool is in German, so need a team member who can read this language, otherwise you will have to use various technologies, such as Google Translate to use tool.\n\nReturn to Chart\n\nWAS (Web Archiving Service)\n\nDescription: With a WAS account you can capture, analyze and archive web sites and documents. Archives can be published or kept for private study. The WAS curator tools are easy to use, fully hosted, and allow collaborative collection building.\n\nWebsite(s): http://webarchives.cdlib.org/\n\nLast Updated (Checked on April 9th, 2013): Current (Service)\n\nSystem Requirements: Hosted service, web browser interface is all that is required.\n\nCustomer Service: Training programs and technical support services are available.\n\nAdditional Notes: Some of the information about this service is not very straight forward.. Since it is a hosted service, details about how the archive content are not publicly discussed. They do have a full set of features for both librarians and researchers. But as to the mechanics of how they crawl and store sites, information is vague.\n\nReturn to Chart\n\nWayback Machine\n\nDescription: Though it is the Wayvabck Machine is an open source java implementation of http://archive.org/web/web.php, its production is in Perl not open source, lacking in maintainability and extensibility. Three access modes of operation are supported, “archival URL”, “proxy”, and “domain Prefix”. Latest release is gathered to be 2011-01-03. In Archival URL, “archived content is modified as it is returned to users, as also called the “retail” access to the Web, retrieving content that Internet Archive harvested from crawling. (http://archive-access.sourceforge.net/projects/wayback/)\n\nWebsite(s): http://archive.org/web/web.php\n\nLast Updated (Checked on April 9th, 2013): Current (Service)\n\nSystem Requirements: None\n\nCustomer Service:\n\nAdditional Notes: Source: http://www.clir.org/pubs/reports/pub114/approaches.html\n\nThe Wayback Machine is a service provided by the Internet Archive in partnership with Alexa Internet. They crawl the web collecting representation information around websites (HTML, CSS, flash, etc.), making past snapshots of the site available. The records can be browsed chronologically, giving users a sense of what a site was like on the date when it was crawled. This service is freely available for anyone on the web to peruse the archive. The Wayback Machine crawls the web based on an algorithm — following links. The Internet Archive’s Archive-It service is better suited to organizations that want to have a self-directed webpage archive.\n\nThe Wayback Machine is a helpful tool from the general web user’s perspective, but very little agency is offered for organizations and webmasters. That being said, it does align with the OAIS reference model to a certain degree. The Wayback Machine is not completely transparent about their ingest process. The Wayback Machine is unable to ingest sites that have the ‘robots.txt’ rules established to stop crawling. The Wayback Machine also has trouble with javascript, and cannot archive sites with server-side image maps.\n\nThe Wayback Machine and Internet Archive are archives by name and by aspiration. However thinking about the real nuts and bolts of archival storage it is not a perfect system. The way that information is collected and dispersed is far-reaching, but documentation surrounding disaster recovery and worst case scenarios is under-articulated. The Wayback Machine is one branch of the Internet Archive, and therefore shares the same basic preservation planning documentation. Their three-prong approach vaguely addresses: accidents, migration, and data formats. In an effort to counter the threat of accident they plan to have multiple copies of data stored in different locations. Plan is the keyword, this is not yet implemented and they do not have a planned date of accomplishing this goal.\n\nReturn to Chart\n\nWCT (Web Curator Tool)\n\nDescription: The Web Curator Tool (WCT) is a workflow management application for selective web archiving. It does not include ingest or storage, it focuses on the web crawling.\n\nWebsite(s): http://webcurator.sourceforge.net\n\nLast Updated (Checked on April 9th, 2013): 12/05/2012 http://webcurator.sourceforge.net\n\nSystem Requirements: None listed.\n\nCustomer Service: Clear and detailed user manuals. Mailing lists.\n\nAdditional Notes:None\n\nReturn to Chart\n\nWindowsAzure\n\nDescription: “Windows Azure enables you to quickly build, deploy and manage applications across a global network of Microsoft-managed data centers. You can build applications using any operating system, language or tool” – windowsazures.com/en-us/home/features/overview\n\nWebsite(s): http://www.windowsazure.com/en-us/\n\nLast Updated (Checked on April 9th, 2013): Current (Service)\n\nSystem Requirements: None\n\nCustomer Service: Available\n\nAdditional Notes: Forum: http://social.msdn.microsoft.com/Forums/en-US/category/windowsazureplatform/\n\nReturn to Chart\n\nXena\n\nDescription: Xena is a digital preservation tool that first identifies a digital object’s file format, and for certain formats then converts the object into an open format.\n\nWebsite(s): http://xena.sourceforge.net/\n\nLast Updated (Checked on April 9th, 2013): 06/14/2012 http://webcurator.sourceforge.net\n\nSystem Requirements: Windows, Linux or Mac OS X\n\nCustomer Service: Email address for feedback and questions, but seems to have limited support.\n\nAdditional Notes:\n\nReturn to Chart\n\nA special thanks goes out to Jerry McDonough’s Spring 2013 Graduate Students who contributed some material found in some of the additional notes sections."
    }
}
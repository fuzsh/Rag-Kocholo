{
    "id": "dbpedia_7735_3",
    "rank": 2,
    "data": {
        "url": "https://worldwidescience.org/topicpages/r/record%2Blinkage%2Bsoftware.html",
        "read_more_link": "",
        "language": "en",
        "title": "record linkage software: Topics by WorldWideScience.org",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://worldwidescience.org/sites/www.osti.gov/files/public/image-files/WWSlogo_wTag650px-min.png",
            "https://worldwidescience.org/sites/www.osti.gov/files/public/image-files/OSTIlogo.svg",
            "https://worldwidescience.org/sites/www.osti.gov/files/public/image-files/ICSTIlogo.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Probabilistic record linkage.\n\nScience.gov (United States)\n\nSayers, Adrian; Ben-Shlomo, Yoav; Blom, Ashley W; Steele, Fiona\n\n2016-06-01\n\nStudies involving the use of probabilistic record linkage are becoming increasingly common. However, the methods underpinning probabilistic record linkage are not widely taught or understood, and therefore these studies can appear to be a 'black box' research tool. In this article, we aim to describe the process of probabilistic record linkage through a simple exemplar. We first introduce the concept of deterministic linkage and contrast this with probabilistic linkage. We illustrate each step of the process using a simple exemplar and describe the data structure required to perform a probabilistic linkage. We describe the process of calculating and interpreting matched weights and how to convert matched weights into posterior probabilities of a match using Bayes theorem. We conclude this article with a brief discussion of some of the computational demands of record linkage, how you might assess the quality of your linkage algorithm, and how epidemiologists can maximize the value of their record-linked research using robust record linkage methods. Â© The Author 2015; Published by Oxford University Press on behalf of the International Epidemiological Association.\n\nPrivacy preserving interactive record linkage (PPIRL).\n\nScience.gov (United States)\n\nKum, Hye-Chung; Krishnamurthy, Ashok; Machanavajjhala, Ashwin; Reiter, Michael K; Ahalt, Stanley\n\n2014-01-01\n\nRecord linkage to integrate uncoordinated databases is critical in biomedical research using Big Data. Balancing privacy protection against the need for high quality record linkage requires a human-machine hybrid system to safely manage uncertainty in the ever changing streams of chaotic Big Data. In the computer science literature, private record linkage is the most published area. It investigates how to apply a known linkage function safely when linking two tables. However, in practice, the linkage function is rarely known. Thus, there are many data linkage centers whose main role is to be the trusted third party to determine the linkage function manually and link data for research via a master population list for a designated region. Recently, a more flexible computerized third-party linkage platform, Secure Decoupled Linkage (SDLink), has been proposed based on: (1) decoupling data via encryption, (2) obfuscation via chaffing (adding fake data) and universe manipulation; and (3) minimum information disclosure via recoding. We synthesize this literature to formalize a new framework for privacy preserving interactive record linkage (PPIRL) with tractable privacy and utility properties and then analyze the literature using this framework. Human-based third-party linkage centers for privacy preserving record linkage are the accepted norm internationally. We find that a computer-based third-party platform that can precisely control the information disclosed at the micro level and allow frequent human interaction during the linkage process, is an effective human-machine hybrid system that significantly improves on the linkage center model both in terms of privacy and utility.\n\nEfficient Record Linkage Algorithms Using Complete Linkage Clustering.\n\nScience.gov (United States)\n\nMamun, Abdullah-Al; Aseltine, Robert; Rajasekaran, Sanguthevar\n\n2016-01-01\n\nData from different agencies share data of the same individuals. Linking these datasets to identify all the records belonging to the same individuals is a crucial and challenging problem, especially given the large volumes of data. A large number of available algorithms for record linkage are prone to either time inefficiency or low-accuracy in finding matches and non-matches among the records. In this paper we propose efficient as well as reliable sequential and parallel algorithms for the record linkage problem employing hierarchical clustering methods. We employ complete linkage hierarchical clustering algorithms to address this problem. In addition to hierarchical clustering, we also use two other techniques: elimination of duplicate records and blocking. Our algorithms use sorting as a sub-routine to identify identical copies of records. We have tested our algorithms on datasets with millions of synthetic records. Experimental results show that our algorithms achieve nearly 100% accuracy. Parallel implementations achieve almost linear speedups. Time complexities of these algorithms do not exceed those of previous best-known algorithms. Our proposed algorithms outperform previous best-known algorithms in terms of accuracy consuming reasonable run times.\n\nSome methods for blindfolded record linkage\n\nDirectory of Open Access Journals (Sweden)\n\nChristen Peter\n\n2004-06-01\n\nFull Text Available Abstract Background The linkage of records which refer to the same entity in separate data collections is a common requirement in public health and biomedical research. Traditionally, record linkage techniques have required that all the identifying data in which links are sought be revealed to at least one party, often a third party. This necessarily invades personal privacy and requires complete trust in the intentions of that party and their ability to maintain security and confidentiality. Dusserre, Quantin, Bouzelat and colleagues have demonstrated that it is possible to use secure one-way hash transformations to carry out follow-up epidemiological studies without any party having to reveal identifying information about any of the subjects â a technique which we refer to as \"blindfolded record linkage\". A limitation of their method is that only exact comparisons of values are possible, although phonetic encoding of names and other strings can be used to allow for some types of typographical variation and data errors. Methods A method is described which permits the calculation of a general similarity measure, the n-gram score, without having to reveal the data being compared, albeit at some cost in computation and data communication. This method can be combined with public key cryptography and automatic estimation of linkage model parameters to create an overall system for blindfolded record linkage. Results The system described offers good protection against misdeeds or security failures by any one party, but remains vulnerable to collusion between or simultaneous compromise of two or more parties involved in the linkage operation. In order to reduce the likelihood of this, the use of last-minute allocation of tasks to substitutable servers is proposed. Proof-of-concept computer programmes written in the Python programming language are provided to illustrate the similarity comparison protocol. Conclusion Although the protocols described in\n\nDesign and implementation of a privacy preserving electronic health record linkage tool in Chicago.\n\nScience.gov (United States)\n\nKho, Abel N; Cashy, John P; Jackson, Kathryn L; Pah, Adam R; Goel, Satyender; Boehnke, JÃ¶rn; Humphries, John Eric; Kominers, Scott Duke; Hota, Bala N; Sims, Shannon A; Malin, Bradley A; French, Dustin D; Walunas, Theresa L; Meltzer, David O; Kaleba, Erin O; Jones, Roderick C; Galanter, William L\n\n2015-09-01\n\nTo design and implement a tool that creates a secure, privacy preserving linkage of electronic health record (EHR) data across multiple sites in a large metropolitan area in the United States (Chicago, IL), for use in clinical research. The authors developed and distributed a software application that performs standardized data cleaning, preprocessing, and hashing of patient identifiers to remove all protected health information. The application creates seeded hash code combinations of patient identifiers using a Health Insurance Portability and Accountability Act compliant SHA-512 algorithm that minimizes re-identification risk. The authors subsequently linked individual records using a central honest broker with an algorithm that assigns weights to hash combinations in order to generate high specificity matches. The software application successfully linked and de-duplicated 7 million records across 6 institutions, resulting in a cohort of 5 million unique records. Using a manually reconciled set of 11â292 patients as a gold standard, the software achieved a sensitivity of 96% and a specificity of 100%, with a majority of the missed matches accounted for by patients with both a missing social security number and last name change. Using 3 disease examples, it is demonstrated that the software can reduce duplication of patient records across sites by as much as 28%. Software that standardizes the assignment of a unique seeded hash identifier merged through an agreed upon third-party honest broker can enable large-scale secure linkage of EHR data for epidemiologic and public health research. The software algorithm can improve future epidemiologic research by providing more comprehensive data given that patients may make use of multiple healthcare systems. Â© The Author 2015. Published by Oxford University Press on behalf of the American Medical Informatics Association. All rights reserved. For Permissions, please email: journals.permissions@oup.com.\n\nRLT-S: A Web System for Record Linkage.\n\nDirectory of Open Access Journals (Sweden)\n\nAbdullah-Al Mamun\n\nFull Text Available Record linkage integrates records across multiple related data sources identifying duplicates and accounting for possible errors. Real life applications require efficient algorithms to merge these voluminous data sources to find out all records belonging to same individuals. Our recently devised highly efficient record linkage algorithms provide best-known solutions to this challenging problem.We have developed RLT-S, a freely available web tool, which implements our single linkage clustering algorithm for record linkage. This tool requires input data sets and a small set of configuration settings about these files to work efficiently. RLT-S employs exact match clustering, blocking on a specified attribute and single linkage based hierarchical clustering among these blocks.RLT-S is an implementation package of our sequential record linkage algorithm. It outperforms previous best-known implementations by a large margin. The tool is at least two times faster for any dataset than the previous best-known tools.RLT-S tool implements our record linkage algorithm that outperforms previous best-known algorithms in this area. This website also contains necessary information such as instructions, submission history, feedback, publications and some other sections to facilitate the usage of the tool.RLT-S is integrated into http://www.rlatools.com, which is currently serving this tool only. The tool is freely available and can be used without login. All data files used in this paper have been stored in https://github.com/abdullah009/DataRLATools. For copies of the relevant programs please see https://github.com/abdullah009/RLATools.\n\nPrivacy-preserving record linkage on large real world datasets.\n\nScience.gov (United States)\n\nRandall, Sean M; Ferrante, Anna M; Boyd, James H; Bauer, Jacqueline K; Semmens, James B\n\n2014-08-01\n\nRecord linkage typically involves the use of dedicated linkage units who are supplied with personally identifying information to determine individuals from within and across datasets. The personally identifying information supplied to linkage units is separated from clinical information prior to release by data custodians. While this substantially reduces the risk of disclosure of sensitive information, some residual risks still exist and remain a concern for some custodians. In this paper we trial a method of record linkage which reduces privacy risk still further on large real world administrative data. The method uses encrypted personal identifying information (bloom filters) in a probability-based linkage framework. The privacy preserving linkage method was tested on ten years of New South Wales (NSW) and Western Australian (WA) hospital admissions data, comprising in total over 26 million records. No difference in linkage quality was found when the results were compared to traditional probabilistic methods using full unencrypted personal identifiers. This presents as a possible means of reducing privacy risks related to record linkage in population level research studies. It is hoped that through adaptations of this method or similar privacy preserving methods, risks related to information disclosure can be reduced so that the benefits of linked research taking place can be fully realised. Copyright Â© 2013 Elsevier Inc. All rights reserved.\n\nA literature review of record linkage procedures focusing on infant health outcomes\n\nDirectory of Open Access Journals (Sweden)\n\nCarla Jorge Machado\n\nFull Text Available Record linkage is a powerful tool in assembling information from different data sources and has been used by a number of public health researchers. In this review, we provide an overview of the record linkage methodologies, focusing particularly on probabilistic record linkage. We then stress the purposes and research applications of linking records by focusing on studies of infant health outcomes based on large data sets, and provide a critical review of the studies in Brazil.\n\nA Simple Sampling Method for Estimating the Accuracy of Large Scale Record Linkage Projects.\n\nScience.gov (United States)\n\nBoyd, James H; Guiver, Tenniel; Randall, Sean M; Ferrante, Anna M; Semmens, James B; Anderson, Phil; Dickinson, Teresa\n\n2016-05-17\n\nRecord linkage techniques allow different data collections to be brought together to provide a wider picture of the health status of individuals. Ensuring high linkage quality is important to guarantee the quality and integrity of research. Current methods for measuring linkage quality typically focus on precision (the proportion of incorrect links), given the difficulty of measuring the proportion of false negatives. The aim of this work is to introduce and evaluate a sampling based method to estimate both precision and recall following record linkage. In the sampling based method, record-pairs from each threshold (including those below the identified cut-off for acceptance) are sampled and clerically reviewed. These results are then applied to the entire set of record-pairs, providing estimates of false positives and false negatives. This method was evaluated on a synthetically generated dataset, where the true match status (which records belonged to the same person) was known. The sampled estimates of linkage quality were relatively close to actual linkage quality metrics calculated for the whole synthetic dataset. The precision and recall measures for seven reviewers were very consistent with little variation in the clerical assessment results (overall agreement using the Fleiss Kappa statistics was 0.601). This method presents as a possible means of accurately estimating matching quality and refining linkages in population level linkage studies. The sampling approach is especially important for large project linkages where the number of record pairs produced may be very large often running into millions.\n\nOptimizing Opt-Out Consent for Record Linkage\n\nDirectory of Open Access Journals (Sweden)\n\nDas Marcel\n\n2014-09-01\n\nFull Text Available This article reports on a study testing the effects of different ways of administering an opt-out consent for record linkage in a probability-based Internet panel. First, we conducted cognitive interviews to explore reactions to a draft version of the opt-out consent text. Second, we conducted a two-factor experiment to test the effects of content manipulations and mode. The results indicate that the way in which respondents were informed did not have much effect on opting out. Results from a follow-up survey on attitudes regarding privacy, confidentiality, and trust, along with knowledge questions about the process of linking, showed no evidence that presenting the opt-out consent statement makes respondents more concerned about privacy. Knowledge about the aspects of record linkage is generally not high. When looking at long-term effects of sending an opt-out consent statement, we found no evidence that this leads to higher attrition or lower participation rates.\n\nEfficient sequential and parallel algorithms for record linkage.\n\nScience.gov (United States)\n\nMamun, Abdullah-Al; Mi, Tian; Aseltine, Robert; Rajasekaran, Sanguthevar\n\n2014-01-01\n\nIntegrating data from multiple sources is a crucial and challenging problem. Even though there exist numerous algorithms for record linkage or deduplication, they suffer from either large time needs or restrictions on the number of datasets that they can integrate. In this paper we report efficient sequential and parallel algorithms for record linkage which handle any number of datasets and outperform previous algorithms. Our algorithms employ hierarchical clustering algorithms as the basis. A key idea that we use is radix sorting on certain attributes to eliminate identical records before any further processing. Another novel idea is to form a graph that links similar records and find the connected components. Our sequential and parallel algorithms have been tested on a real dataset of 1,083,878 records and synthetic datasets ranging in size from 50,000 to 9,000,000 records. Our sequential algorithm runs at least two times faster, for any dataset, than the previous best-known algorithm, the two-phase algorithm using faster computation of the edit distance (TPA (FCED)). The speedups obtained by our parallel algorithm are almost linear. For example, we get a speedup of 7.5 with 8 cores (residing in a single node), 14.1 with 16 cores (residing in two nodes), and 26.4 with 32 cores (residing in four nodes). We have compared the performance of our sequential algorithm with TPA (FCED) and found that our algorithm outperforms the previous one. The accuracy is the same as that of this previous best-known algorithm.\n\nQuality of record linkage in a highly automated cancer registry that relies on encrypted identity data\n\nDirectory of Open Access Journals (Sweden)\n\nSchmidtmann, Irene\n\n2016-06-01\n\nFull Text Available Objectives: In the absence of unique ID numbers, cancer and other registries in Germany and elsewhere rely on identity data to link records pertaining to the same patient. These data are often encrypted to ensure privacy. Some record linkage errors unavoidably occur. These errors were quantified for the cancer registry of North Rhine Westphalia which uses encrypted identity data. Methods: A sample of records was drawn from the registry, record linkage information was included. In parallel, plain text data for these records were retrieved to generate a gold standard. Record linkage error frequencies in the cancer registry were determined by comparison of the results of the routine linkage with the gold standard. Error rates were projected to larger registries.Results: In the sample studied, the homonym error rate was 0.015%; the synonym error rate was 0.2%. The F-measure was 0.9921. Projection to larger databases indicated that for a realistic development the homonym error rate will be around 1%, the synonym error rate around 2%.Conclusion: Observed error rates are low. This shows that effective methods to standardize and improve the quality of the input data have been implemented. This is crucial to keep error rates low when the registryâs database grows. The planned inclusion of unique health insurance numbers is likely to further improve record linkage quality. Cancer registration entirely based on electronic notification of records can process large amounts of data with high quality of record linkage.\n\nA machine learning approach to create blocking criteria for record linkage.\n\nScience.gov (United States)\n\nGiang, Phan H\n\n2015-03-01\n\nRecord linkage, a part of data cleaning, is recognized as one of most expensive steps in data warehousing. Most record linkage (RL) systems employ a strategy of using blocking filters to reduce the number of pairs to be matched. A blocking filter consists of a number of blocking criteria. Until recently, blocking criteria are selected manually by domain experts. This paper proposes a new method to automatically learn efficient blocking criteria for record linkage. Our method addresses the lack of sufficient labeled data for training. Unlike previous works, we do not consider a blocking filter in isolation but in the context of an accompanying matcher which is employed after the blocking filter. We show that given such a matcher, the labels (assigned to record pairs) that are relevant for learning are the labels assigned by the matcher (link/nonlink), not the labels assigned objectively (match/unmatch). This conclusion allows us to generate an unlimited amount of labeled data for training. We formulate the problem of learning a blocking filter as a Disjunctive Normal Form (DNF) learning problem and use the Probably Approximately Correct (PAC) learning theory to guide the development of algorithm to search for blocking filters. We test the algorithm on a real patient master file of 2.18 million records. The experimental results show that compared with filters obtained by educated guess, the optimal learned filters have comparable recall but reduce throughput (runtime) by an order-of-magnitude factor.\n\nRecord linkage for pharmacoepidemiological studies in cancer patients.\n\nScience.gov (United States)\n\nHerk-Sukel, Myrthe P P van; Lemmens, Valery E P P; Poll-Franse, Lonneke V van de; Herings, Ron M C; Coebergh, Jan Willem W\n\n2012-01-01\n\nAn increasing need has developed for the post-approval surveillance of (new) anti-cancer drugs by means of pharmacoepidemiology and outcomes research in the area of oncology. To create an overview that makes researchers aware of the available database linkages in Northern America and Europe which facilitate pharmacoepidemiology and outcomes research in cancer patients. In addition to our own database, i.e. the Eindhoven Cancer Registry (ECR) linked to the PHARMO Record Linkage System, we considered database linkages between a population-based cancer registry and an administrative healthcare database that at least contains information on drug use and offers a longitudinal perspective on healthcare utilization. Eligible database linkages were limited to those that had been used in multiple published articles in English language included in Pubmed. The HMO Cancer Research Network (CRN) in the US was excluded from this review, as an overview of the linked databases participating in the CRN is already provided elsewhere. Researchers who had worked with the data resources included in our review were contacted for additional information and verification of the data presented in the overview. The following database linkages were included: the Surveillance, Epidemiology, and End-Results-Medicare; cancer registry data linked to Medicaid; Canadian cancer registries linked to population-based drug databases; the Scottish cancer registry linked to the Tayside drug dispensing data; linked databases in the Nordic Countries of Europe: Norway, Sweden, Finland and Denmark; and the ECR-PHARMO linkage in the Netherlands. Descriptives of the included database linkages comprise population size, generalizability of the population, year of first data availability, contents of the cancer registry, contents of the administrative healthcare database, the possibility to select a cancer-free control cohort, and linkage to other healthcare databases. The linked databases offer a longitudinal\n\nValidation of de-identified record linkage to ascertain hospital admissions in a cohort study\n\nDirectory of Open Access Journals (Sweden)\n\nEnglish Dallas R\n\n2011-04-01\n\nFull Text Available Abstract Background Cohort studies can provide valuable evidence of cause and effect relationships but are subject to loss of participants over time, limiting the validity of findings. Computerised record linkage offers a passive and ongoing method of obtaining health outcomes from existing routinely collected data sources. However, the quality of record linkage is reliant upon the availability and accuracy of common identifying variables. We sought to develop and validate a method for linking a cohort study to a state-wide hospital admissions dataset with limited availability of unique identifying variables. Methods A sample of 2000 participants from a cohort study (n = 41 514 was linked to a state-wide hospitalisations dataset in Victoria, Australia using the national health insurance (Medicare number and demographic data as identifying variables. Availability of the health insurance number was limited in both datasets; therefore linkage was undertaken both with and without use of this number and agreement tested between both algorithms. Sensitivity was calculated for a sub-sample of 101 participants with a hospital admission confirmed by medical record review. Results Of the 2000 study participants, 85% were found to have a record in the hospitalisations dataset when the national health insurance number and sex were used as linkage variables and 92% when demographic details only were used. When agreement between the two methods was tested the disagreement fraction was 9%, mainly due to \"false positive\" links when demographic details only were used. A final algorithm that used multiple combinations of identifying variables resulted in a match proportion of 87%. Sensitivity of this final linkage was 95%. Conclusions High quality record linkage of cohort data with a hospitalisations dataset that has limited identifiers can be achieved using combinations of a national health insurance number and demographic data as identifying variables.\n\nReclink: aplicativo para o relacionamento de bases de dados, implementando o mÃ©todo probabilistic record linkage Reclink: an application for database linkage implementing the probabilistic record linkage method\n\nDirectory of Open Access Journals (Sweden)\n\nKenneth R. de Camargo Jr.\n\n2000-06-01\n\nFull Text Available Apresenta-se um sistema de relacionamento de bases de dados fundamentado na tÃ©cnica de relacionamento probabilÃ­stico de registros, desenvolvido na linguagem C++ com o ambiente de programaÃ§Ã£o Borland C++ Builder versÃ£o 3.0. O sistema foi testado a partir de fontes de dados de diferentes tamanhos, tendo sido avaliado em tempo de processamento e sensibilidade para a identificaÃ§Ã£o de pares verdadeiros. O tempo gasto com o processamento dos registros foi menor quando se empregou o programa do que ao ser realizado manualmente, em especial, quando envolveram bases de maior tamanho. As sensibilidades do processo manual e do processo automÃ¡tico foram equivalentes quando utilizaram bases com menor nÃºmero de registros; entretanto, Ã medida que as bases aumentaram, percebeu-se tendÃªncia de diminuiÃ§Ã£o na sensibilidade apenas no processo manual. Ainda que em fase inicial de desenvolvimento, o sistema apresentou boa performance tanto em velocidade quanto em sensibilidade. Embora a performance dos algoritmos utilizados tenha sido satisfatÃ³ria, o objetivo Ã© avaliar outras rotinas, buscando aprimorar o desempenho do sistema.This paper presents a system for database linkage based on the probabilistic record linkage technique, developed in the C++ language with the Borland C++ Builder version 3.0 programming environment. The system was tested in the linkage of data sources of different sizes, evaluated both in terms of processing time and sensitivity for identifying true record pairs. Significantly less time was spent in record processing when the program was used, as compared to manual processing, especially in situations where larger databases were used. Manual and automatic processes had equivalent sensitivities in situations where we used databases with fewer records. However, as the number of records grew we noticed a clear reduction in the sensitivity of the manual process, but not in the automatic one. Although in its initial stage of\n\nName segmentation using hidden Markov models and its application in record linkage\n\nDirectory of Open Access Journals (Sweden)\n\nRita de Cassia Braga GonÃ§alves\n\n2014-10-01\n\nFull Text Available This study aimed to evaluate the use of hidden Markov models (HMM for the segmentation of person names and its influence on record linkage. A HMM was applied to the segmentation of patientâs and motherâs names in the databases of the Mortality Information System (SIM, Information Subsystem for High Complexity Procedures (APAC, and Hospital Information System (AIH. A sample of 200 patients from each database was segmented via HMM, and the results were compared to those from segmentation by the authors. The APAC-SIM and APAC-AIH databases were linked using three different segmentation strategies, one of which used HMM. Conformity of segmentation via HMM varied from 90.5% to 92.5%. The different segmentation strategies yielded similar results in the record linkage process. This study suggests that segmentation of Brazilian names via HMM is no more effective than traditional segmentation approaches in the linkage process.\n\nDevelopment of Farm Records Software\n\nDirectory of Open Access Journals (Sweden)\n\nM. S. Abubakar\n\n2017-12-01\n\nFull Text Available Farm records are mostly manually kept on paper notebooks and folders where similar records are organized in one folder or spread sheet. These records are usually kept for many years therefore they becomes bulky and less organized. Consequently, it becomes difficult to search, update and tedious and time consuming to manage these records. This study was carried-out to overcome these problems associated with manual farm records keeping by developing user-friendly, easily accessible, reliable and secured software. The software was limited records keeping in crop production, livestock production, poultry production, employees, income and expenditure. The system was implemented using Java Server Faces (JSF for designing Graphical User Interface (GUI, Enterprises Java Beans (EJB for logic tier and MySQL database for storing farm records.\n\nK-Anonymity Based Privacy Risk Budgeting System for Interactive Record Linkage\n\nDirectory of Open Access Journals (Sweden)\n\nHye-Chung Kum\n\n2017-04-01\n\nThe k-anonymity based privacy risk budgeting system provides a mechanism where we can concretely reason about the tradeoff between the privacy risks due to information disclosed, accuracy gained, and biases reduced during interactive record linkage.\n\nEvaluating privacy-preserving record linkage using cryptographic long-term keys and multibit trees on large medical datasets.\n\nScience.gov (United States)\n\nBrown, Adrian P; Borgs, Christian; Randall, Sean M; Schnell, Rainer\n\n2017-06-08\n\nIntegrating medical data using databases from different sources by record linkage is a powerful technique increasingly used in medical research. Under many jurisdictions, unique personal identifiers needed for linking the records are unavailable. Since sensitive attributes, such as names, have to be used instead, privacy regulations usually demand encrypting these identifiers. The corresponding set of techniques for privacy-preserving record linkage (PPRL) has received widespread attention. One recent method is based on Bloom filters. Due to superior resilience against cryptographic attacks, composite Bloom filters (cryptographic long-term keys, CLKs) are considered best practice for privacy in PPRL. Real-world performance of these techniques using large-scale data is unknown up to now. Using a large subset of Australian hospital admission data, we tested the performance of an innovative PPRL technique (CLKs using multibit trees) against a gold-standard derived from clear-text probabilistic record linkage. Linkage time and linkage quality (recall, precision and F-measure) were evaluated. Clear text probabilistic linkage resulted in marginally higher precision and recall than CLKs. PPRL required more computing time but 5 million records could still be de-duplicated within one day. However, the PPRL approach required fine tuning of parameters. We argue that increased privacy of PPRL comes with the price of small losses in precision and recall and a large increase in computational burden and setup time. These costs seem to be acceptable in most applied settings, but they have to be considered in the decision to apply PPRL. Further research on the optimal automatic choice of parameters is needed.\n\n[MapDraw: a microsoft excel macro for drawing genetic linkage maps based on given genetic linkage data].\n\nScience.gov (United States)\n\nLiu, Ren-Hu; Meng, Jin-Ling\n\n2003-05-01\n\nMAPMAKER is one of the most widely used computer software package for constructing genetic linkage maps.However, the PC version, MAPMAKER 3.0 for PC, could not draw the genetic linkage maps that its Macintosh version, MAPMAKER 3.0 for Macintosh,was able to do. Especially in recent years, Macintosh computer is much less popular than PC. Most of the geneticists use PC to analyze their genetic linkage data. So a new computer software to draw the same genetic linkage maps on PC as the MAPMAKER for Macintosh to do on Macintosh has been crying for. Microsoft Excel,one component of Microsoft Office package, is one of the most popular software in laboratory data processing. Microsoft Visual Basic for Applications (VBA) is one of the most powerful functions of Microsoft Excel. Using this program language, we can take creative control of Excel, including genetic linkage map construction, automatic data processing and more. In this paper, a Microsoft Excel macro called MapDraw is constructed to draw genetic linkage maps on PC computer based on given genetic linkage data. Use this software,you can freely construct beautiful genetic linkage map in Excel and freely edit and copy it to Word or other application. This software is just an Excel format file. You can freely copy it from ftp://211.69.140.177 or ftp://brassica.hzau.edu.cn and the source code can be found in Excel's Visual Basic Editor.\n\nRecords, record linkage, and the identification of long term environmental hazards\n\nEnergy Technology Data Exchange (ETDEWEB)\n\nAcheson, E. D.\n\n1978-11-15\n\nLong-term effects of toxic substances in man which have been recognized so far have been noticed because they have involved gross relative risks, or bizarre effects, or have been stumbled upon by chance or because of special circumstances. These facts and some recent epidemiological evidence together suggest that a systematic approach with more precise methods and data would almost certainly reveal the effects of many more toxic substances, particularly in workers exposed in manufacturing industry. Additional ways are suggested in which record linkage techniques might be used to identify substances with long-term toxic effects. Obstacles to further progress in the field of monitoring for long-term hazards in man are: lack of a public policy dealing with confidentiality and informed consent in the use of identifiable personal records, which balances the needs of bona fide research workers with proper safeguards for the privacy of the individual, and lack of resources to improve the quality, accessibility and organization of the appropriate data. (PCS)\n\nPrivacy preserving probabilistic record linkage (P3RL): a novel method for linking existing health-related data and maintaining participant confidentiality.\n\nScience.gov (United States)\n\nSchmidlin, Kurt; Clough-Gorr, Kerri M; Spoerri, Adrian\n\n2015-05-30\n\nRecord linkage of existing individual health care data is an efficient way to answer important epidemiological research questions. Reuse of individual health-related data faces several problems: Either a unique personal identifier, like social security number, is not available or non-unique person identifiable information, like names, are privacy protected and cannot be accessed. A solution to protect privacy in probabilistic record linkages is to encrypt these sensitive information. Unfortunately, encrypted hash codes of two names differ completely if the plain names differ only by a single character. Therefore, standard encryption methods cannot be applied. To overcome these challenges, we developed the Privacy Preserving Probabilistic Record Linkage (P3RL) method. In this Privacy Preserving Probabilistic Record Linkage method we apply a three-party protocol, with two sites collecting individual data and an independent trusted linkage center as the third partner. Our method consists of three main steps: pre-processing, encryption and probabilistic record linkage. Data pre-processing and encryption are done at the sites by local personnel. To guarantee similar quality and format of variables and identical encryption procedure at each site, the linkage center generates semi-automated pre-processing and encryption templates. To retrieve information (i.e. data structure) for the creation of templates without ever accessing plain person identifiable information, we introduced a novel method of data masking. Sensitive string variables are encrypted using Bloom filters, which enables calculation of similarity coefficients. For date variables, we developed special encryption procedures to handle the most common date errors. The linkage center performs probabilistic record linkage with encrypted person identifiable information and plain non-sensitive variables. In this paper we describe step by step how to link existing health-related data using encryption methods to\n\nNew South Wales Child Development Study (NSW-CDS): an Australian multiagency, multigenerational, longitudinal record linkage study.\n\nScience.gov (United States)\n\nCarr, Vaughan J; Harris, Felicity; Raudino, Alessandra; Luo, Luming; Kariuki, Maina; Liu, Enwu; Tzoumakis, Stacy; Smith, Maxwell; Holbrook, Allyson; Bore, Miles; Brinkman, Sally; Lenroot, Rhoshel; Dix, Katherine; Dean, Kimberlie; Laurens, Kristin R; Green, Melissa J\n\n2016-02-11\n\nThe initial aim of this multiagency, multigenerational record linkage study is to identify childhood profiles of developmental vulnerability and resilience, and to identify the determinants of these profiles. The eventual aim is to identify risk and protective factors for later childhood-onset and adolescent-onset mental health problems, and other adverse social outcomes, using subsequent waves of record linkage. The research will assist in informing the development of public policy and intervention guidelines to help prevent or mitigate adverse long-term health and social outcomes. The study comprises a population cohort of 87,026 children in the Australian State of New South Wales (NSW). The cohort was defined by entry into the first year of full-time schooling in NSW in 2009, at which time class teachers completed the Australian Early Development Census (AEDC) on each child (with 99.7% coverage in NSW). The AEDC data have been linked to the children's birth, health, school and child protection records for the period from birth to school entry, and to the health and criminal records of their parents, as well as mortality databases. Descriptive data summarising sex, geographic and socioeconomic distributions, and linkage rates for the various administrative databases are presented. Child data are summarised, and the mental health and criminal records data of the children's parents are provided. In 2015, at age 11 years, a self-report mental health survey was administered to the cohort in collaboration with government, independent and Catholic primary school sectors. A second record linkage, spanning birth to age 11 years, will be undertaken to link this survey data with the aforementioned administrative databases. This will enable a further identification of putative risk and protective factors for adverse mental health and other outcomes in adolescence, which can then be tested in subsequent record linkages. Published by the BMJ Publishing Group Limited. For\n\nCombining Different Privacy-Preserving Record Linkage Methods for Hospital Admission Data.\n\nScience.gov (United States)\n\nStausberg, JÃ¼rgen; Waldenburger, Andreas; Borgs, Christian; Schnell, Rainer\n\n2017-01-01\n\nRecord linkage (RL) is the process of identifying pairs of records that correspond to the same entity, for example the same patient. The basic approach assigns to each pair of records a similarity weight, and then determines a certain threshold, above which the two records are considered to be a match. Three different RL methods were applied under privacy-preserving conditions on hospital admission data: deterministic RL (DRL), probabilistic RL (PRL), and Bloom filters. The patient characteristics like names were one-way encrypted (DRL, PRL) or transformed to a cryptographic longterm key (Bloom filters). Based on one year of hospital admissions, the data set was split randomly in 30 thousand new and 1,5 million known patients. With the combination of the three RL-methods, a positive predictive value of 83 % (95 %-confidence interval 65 %-94 %) was attained. Thus, the application of the presented combination of RL-methods seem to be suited for other applications of population-based research.\n\nData Matching Concepts and Techniques for Record Linkage, Entity Resolution, and Duplicate Detection\n\nCERN Document Server\n\nChristen, Peter\n\n2012-01-01\n\nData matching (also known as record or data linkage, entity resolution, object identification, or field matching) is the task of identifying, matching and merging records that correspond to the same entities from several databases or even within one database. Based on research in various domains including applied statistics, health informatics, data mining, machine learning, artificial intelligence, database management, and digital libraries, significant advances have been achieved over the last decade in all aspects of the data matching process, especially on how to improve the accuracy of da\n\nDevelopment of Software for dose Records Data Base Access\n\nInternational Nuclear Information System (INIS)\n\nAmaro, M.\n\n1990-01-01\n\nThe CIEMAT personal dose records are computerized in a Dosimetric Data Base whose primary purpose was the individual dose follow-up control and the data handling for epidemiological studies. Within the Data Base management scheme, software development to allow searching of individual dose records by external authorised users was undertaken. The report describes the software developed to allow authorised persons to visualize on screen a summary of the individual dose records from workers included in the Data Base. The report includes the User Guide for the authorised list of users and listings of codes and subroutines developed. (Author) 2 refs\n\nHistory of the Rochester Epidemiology Project: half a century of medical records linkage in a US population.\n\nScience.gov (United States)\n\nRocca, Walter A; Yawn, Barbara P; St Sauver, Jennifer L; Grossardt, Brandon R; Melton, L Joseph\n\n2012-12-01\n\nThe Rochester Epidemiology Project (REP) has maintained a comprehensive medical records linkage system for nearly half a century for almost all persons residing in Olmsted County, Minnesota. Herein, we provide a brief history of the REP before and after 1966, the year in which the REP was officially established. The key protagonists before 1966 were Henry Plummer, Mabel Root, and Joseph Berkson, who developed a medical records linkage system at Mayo Clinic. In 1966, Leonard Kurland established collaborative agreements with other local health care providers (hospitals, physician groups, and clinics [primarily Olmsted Medical Center]) to develop a medical records linkage system that covered the entire population of Olmsted County, and he obtained funding from the National Institutes of Health to support the new system. In 1997, L. Joseph Melton III addressed emerging concerns about the confidentiality of medical record information by introducing a broad patient research authorization as per Minnesota state law. We describe how the key protagonists of the REP have responded to challenges posed by evolving medical knowledge, information technology, and public expectation and policy. In addition, we provide a general description of the system; discuss issues of data quality, reliability, and validity; describe the research team structure; provide information about funding; and compare the REP with other medical information systems. The REP can serve as a model for the development of similar research infrastructures in the United States and worldwide. Copyright Â© 2012 Mayo Foundation for Medical Education and Research. Published by Elsevier Inc. All rights reserved.\n\nWhen to conduct probabilistic linkage vs. deterministic linkage? A simulation study.\n\nScience.gov (United States)\n\nZhu, Ying; Matsuyama, Yutaka; Ohashi, Yasuo; Setoguchi, Soko\n\n2015-08-01\n\nWhen unique identifiers are unavailable, successful record linkage depends greatly on data quality and types of variables available. While probabilistic linkage theoretically captures more true matches than deterministic linkage by allowing imperfection in identifiers, studies have shown inconclusive results likely due to variations in data quality, implementation of linkage methodology and validation method. The simulation study aimed to understand data characteristics that affect the performance of probabilistic vs. deterministic linkage. We created ninety-six scenarios that represent real-life situations using non-unique identifiers. We systematically introduced a range of discriminative power, rate of missing and error, and file size to increase linkage patterns and difficulties. We assessed the performance difference of linkage methods using standard validity measures and computation time. Across scenarios, deterministic linkage showed advantage in PPV while probabilistic linkage showed advantage in sensitivity. Probabilistic linkage uniformly outperformed deterministic linkage as the former generated linkages with better trade-off between sensitivity and PPV regardless of data quality. However, with low rate of missing and error in data, deterministic linkage performed not significantly worse. The implementation of deterministic linkage in SAS took less than 1min, and probabilistic linkage took 2min to 2h depending on file size. Our simulation study demonstrated that the intrinsic rate of missing and error of linkage variables was key to choosing between linkage methods. In general, probabilistic linkage was a better choice, but for exceptionally good quality data (<5% error), deterministic linkage was a more resource efficient choice. Copyright Â© 2015 Elsevier Inc. All rights reserved.\n\nA general model for likelihood computations of genetic marker data accounting for linkage, linkage disequilibrium, and mutations.\n\nScience.gov (United States)\n\nKling, Daniel; Tillmar, Andreas; Egeland, Thore; Mostad, Petter\n\n2015-09-01\n\nSeveral applications necessitate an unbiased determination of relatedness, be it in linkage or association studies or in a forensic setting. An appropriate model to compute the joint probability of some genetic data for a set of persons given some hypothesis about the pedigree structure is then required. The increasing number of markers available through high-density SNP microarray typing and NGS technologies intensifies the demand, where using a large number of markers may lead to biased results due to strong dependencies between closely located loci, both within pedigrees (linkage) and in the population (allelic association or linkage disequilibrium (LD)). We present a new general model, based on a Markov chain for inheritance patterns and another Markov chain for founder allele patterns, the latter allowing us to account for LD. We also demonstrate a specific implementation for X chromosomal markers that allows for computation of likelihoods based on hypotheses of alleged relationships and genetic marker data. The algorithm can simultaneously account for linkage, LD, and mutations. We demonstrate its feasibility using simulated examples. The algorithm is implemented in the software FamLinkX, providing a user-friendly GUI for Windows systems (FamLinkX, as well as further usage instructions, is freely available at www.famlink.se ). Our software provides the necessary means to solve cases where no previous implementation exists. In addition, the software has the possibility to perform simulations in order to further study the impact of linkage and LD on computed likelihoods for an arbitrary set of markers.\n\nQualifying information on deaths and serious injuries caused by road traffic in five Brazilian capitals using record linkage.\n\nScience.gov (United States)\n\nMandacaru, Polyana Maria Pimenta; Andrade, Ana Lucia; Rocha, Marli Souza; Aguiar, Fernanda Pinheiro; Nogueira, Maria Sueli M; Girodo, Anne Marielle; Pedrosa, Ana AmÃ©lia Galas; Oliveira, Vera LÃ­dia Alves de; Alves, Marta Maria Malheiros; PaixÃ£o, LÃºcia Maria Miana M; Malta, Deborah Carvalho; Silva, Marta Maria Alves; Morais Neto, Otaliba Libanio de\n\n2017-09-01\n\nRoad traffic crashes (RTC) are an important public health problem, accounting for 1.2 million deaths per year worldwide. In Brazil, approximately 40,000 deaths caused by RTC occur every year, with different trends in the Federal Units. However, these figures may be even greater if health databases are linked to police records. In addition, the linkage procedure would make it possible to qualify information from the health and police databases, improving the quality of the data regarding underlying cause of death, cause of injury in hospital records, and injury severity. This study linked different data sources to measure the numbers of deaths and serious injuries and to estimate the percentage of corrections regarding the underlying cause of death, cause of injury, and the severity injury in victims in matched pairs from record linkage in five representative state capitals of the five macro-regions of Brazil. This cross-sectional, population-based study used data from the Hospital Information System (HIS), Mortality Information System (MIS), and Police Road Traffic database of Belo Horizonte, Campo Grande, Curitiba, Palmas, and Teresina, for the year 2013 for Teresina, and 2012 for the other capitals. RecLink III was used to perform probabilistic record linkage by identifying matched pairs to calculate the global correction percentage of the underlying cause of death, the circumstance that caused the road traffic injury, and the injury severity of the victims in the police database. There was a change in the cause of injury in the HIS, with an overall percentage of correction estimated at 24.4% for Belo Horizonte, 96.9% for Campo Grande, 100.0% for Palmas, and 33.2% for Teresina. The overall percentages of correction of the underlying cause of death in the MIS were 29.9%, 11.9%, 4.2%, and 33.5% for Belo Horizonte, Campo Grande, Curitiba, and Teresina, respectively. The correction of the classification of injury severity in police database were 100.0% for Belo\n\nA Record Linkage Protocol for a Diabetes Registry at Ethnically Diverse Community Health Centers\n\nOpenAIRE\n\nMaizlish, Neil A.; Herrera, Linda\n\n2005-01-01\n\nCommunity health centers serve ethnically diverse populations that may pose challenges for record linkage based on name and date of birth. The objective was to identify an optimal deterministic algorithm to link patient encounters and laboratory results for hemoglobin A1c testing and examine its variability by health center site, patient ethnicity, and other variables. Based on data elements of last name, first name, date of birth, gender, and health center site, matches with â¥50% to < 100% o...\n\nData Linkage: A powerful research tool with potential problems\n\nDirectory of Open Access Journals (Sweden)\n\nScott Ian\n\n2010-12-01\n\nFull Text Available Abstract Background Policy makers, clinicians and researchers are demonstrating increasing interest in using data linked from multiple sources to support measurement of clinical performance and patient health outcomes. However, the utility of data linkage may be compromised by sub-optimal or incomplete linkage, leading to systematic bias. In this study, we synthesize the evidence identifying participant or population characteristics that can influence the validity and completeness of data linkage and may be associated with systematic bias in reported outcomes. Methods A narrative review, using structured search methods was undertaken. Key words \"data linkage\" and Mesh term \"medical record linkage\" were applied to Medline, EMBASE and CINAHL databases between 1991 and 2007. Abstract inclusion criteria were; the article attempted an empirical evaluation of methodological issues relating to data linkage and reported on patient characteristics, the study design included analysis of matched versus unmatched records, and the report was in English. Included articles were grouped thematically according to patient characteristics that were compared between matched and unmatched records. Results The search identified 1810 articles of which 33 (1.8% met inclusion criteria. There was marked heterogeneity in study methods and factors investigated. Characteristics that were unevenly distributed among matched and unmatched records were; age (72% of studies, sex (50% of studies, race (64% of studies, geographical/hospital site (93% of studies, socio-economic status (82% of studies and health status (72% of studies. Conclusion A number of relevant patient or population factors may be associated with incomplete data linkage resulting in systematic bias in reported clinical outcomes. Readers should consider these factors in interpreting the reported results of data linkage studies.\n\nLinkage of Maternity Hospital Episode Statistics data to birth registration and notification records for births in England 2005-2014: Quality assurance of linkage of routine data for singleton and multiple births.\n\nScience.gov (United States)\n\nHarper, Gillian\n\n2018-03-01\n\nTo quality assure a Trusted Third Party linked data set to prepare it for analysis. Birth registration and notification records from the Office for National Statistics for all births in England 2005-2014 linked to Maternity Hospital Episode Statistics (HES) delivery records by NHS Digital using mothers' identifiers. All 6 676 912 births that occurred in England from 1 January 2005 to 31 December 2014. Every link between a registered birth and an HES delivery record for the study period was categorised as either the same baby or a different baby to the same mother, or as a wrong link, by comparing common baby data items and valid values in key fields with stepwise deterministic rules. Rates of preserved and discarded links were calculated and which features were more common in each group were assessed. Ninety-eight per cent of births originally linked to HES were left with one preserved link. The majority of discarded links were due to duplicate HES delivery records. Of the 4854 discarded links categorised as wrong links, clerical checks found 85% were false-positives links, 13% were quality assurance false negatives and 2% were undeterminable. Births linked using a less reliable stage of the linkage algorithm, births at home and in the London region, and with birth weight or gestational age values missing in HES were more likely to have all links discarded. Linkage error, data quality issues, and false negatives in the quality assurance procedure were uncovered. The procedure could be improved by allowing for transposition in date fields, and more discrimination between missing and differing values. The availability of identifiers in the datasets supported clerical checking. Other research using Trusted Third Party linkage should not assume the linked dataset is error-free or optimised for their analysis, and allow sufficient resources for this. Â© Article author(s) (or their employer(s) unless otherwise stated in the text of the article) 2018. All rights reserved\n\nUsing the Bootstrap to Account for Linkage Errors when Analysing Probabilistically Linked Categorical Data\n\nDirectory of Open Access Journals (Sweden)\n\nChipperfield James O.\n\n2015-09-01\n\nFull Text Available Record linkage is the act of bringing together records that are believed to belong to the same unit (e.g., person or business from two or more files. Record linkage is not an error-free process and can lead to linking a pair of records that do not belong to the same unit. This occurs because linking fields on the files, which ideally would uniquely identify each unit, are often imperfect. There has been an explosion of record linkage applications, particularly involving government agencies and in the field of health, yet there has been little work on making correct inference using such linked files. Naively treating a linked file as if it were linked without errors can lead to biased inferences. This article develops a method of making inferences for cross tabulated variables when record linkage is not an error-free process. In particular, it develops a parametric bootstrap approach to estimation which can accommodate the sophisticated probabilistic record linkage techniques that are widely used in practice (e.g., 1-1 linkage. The article demonstrates the effectiveness of this method in a simulation and in a real application.\n\nRequesting a unique personal identifier or providing a souvenir incentive did not affect overall consent to health record linkage: evidence from an RCT nested within a cohort.\n\nScience.gov (United States)\n\nNi, Michael Y; Li, Tom K; Hui, Rex W H; McDowell, Ian; Leung, Gabriel M\n\n2017-04-01\n\nIt is unclear if unique personal identifiers should be requested from participants for health record linkage: this permits high-quality data linkage but at the potential cost of lower consent rates due to privacy concerns. Drawing from a sampling frame based on the FAMILY Cohort, using a 2 Ã 2 factorial design, we randomly assigned 1,200 participants to (1) request for Hong Kong Identity Card number (HKID) or no request and (2) receiving a souvenir incentive (valued at USD4) or no incentive. The primary outcome was consent to health record linkage. We also investigated associations between demographics, health status, and postal reminders with consent. Overall, we received signed consent forms from 33.3% (95% confidence interval [CI] 30.6-36.0%) of respondents. We did not find an overall effect of requesting HKID (-4.3%, 95% CI -9.8% to 1.2%) or offering souvenir incentives (2.4%, 95% CI -3.1% to 7.9%) on consent to linkage. In subgroup analyses, requesting HKID significantly reduced consent among adults aged 18-44Â years (odds ratio [OR] 0.53, 95% CI 0.30-0.94, compared to no request). Souvenir incentives increased consent among women (OR 1.55, 95% CI 1.13-2.11, compared to no souvenirs). Requesting a unique personal identifier or providing a souvenir incentive did not affect overall consent to health record linkage. Copyright Â© 2017 Elsevier Inc. All rights reserved.\n\nLinkage of Maternity Hospital Episode Statistics data to birth registration and notification records for births in England 2005â2014: Quality assurance of linkage of routine data for singleton and multiple births\n\nScience.gov (United States)\n\n2018-01-01\n\nObjectives To quality assure a Trusted Third Party linked data set to prepare it for analysis. Setting Birth registration and notification records from the Office for National Statistics for all births in England 2005â2014 linked to Maternity Hospital Episode Statistics (HES) delivery records by NHS Digital using mothersâ identifiers. Participants All 6 676 912 births that occurred in England from 1 January 2005 to 31 December 2014. Primary and secondary outcome measures Every link between a registered birth and an HES delivery record for the study period was categorised as either the same baby or a different baby to the same mother, or as a wrong link, by comparing common baby data items and valid values in key fields with stepwise deterministic rules. Rates of preserved and discarded links were calculated and which features were more common in each group were assessed. Results Ninety-eight per cent of births originally linked to HES were left with one preserved link. The majority of discarded links were due to duplicate HES delivery records. Of the 4854 discarded links categorised as wrong links, clerical checks found 85% were false-positives links, 13% were quality assurance false negatives and 2% were undeterminable. Births linked using a less reliable stage of the linkage algorithm, births at home and in the London region, and with birth weight or gestational age values missing in HES were more likely to have all links discarded. Conclusions Linkage error, data quality issues, and false negatives in the quality assurance procedure were uncovered. The procedure could be improved by allowing for transposition in date fields, and more discrimination between missing and differing values. The availability of identifiers in the datasets supported clerical checking. Other research using Trusted Third Party linkage should not assume the linked dataset is error-free or optimised for their analysis, and allow sufficient resources for this. PMID:29500200\n\nHealth problems in childhood cancer survivors: Linkage studies and guideline development\n\nNARCIS (Netherlands)\n\nFont-Gonzalez, A.\n\n2016-01-01\n\nThis thesis comprises two parts. The first part of this thesis aims to increase the evidence on the burden of disease in childhood cancer survivors and to define high-risk groups of survivors by using medical record linkage studies. A two-step record linkage methodology between Dutch national\n\nEstimating parameters for probabilistic linkage of privacy-preserved datasets.\n\nScience.gov (United States)\n\nBrown, Adrian P; Randall, Sean M; Ferrante, Anna M; Semmens, James B; Boyd, James H\n\n2017-07-10\n\nProbabilistic record linkage is a process used to bring together person-based records from within the same dataset (de-duplication) or from disparate datasets using pairwise comparisons and matching probabilities. The linkage strategy and associated match probabilities are often estimated through investigations into data quality and manual inspection. However, as privacy-preserved datasets comprise encrypted data, such methods are not possible. In this paper, we present a method for estimating the probabilities and threshold values for probabilistic privacy-preserved record linkage using Bloom filters. Our method was tested through a simulation study using synthetic data, followed by an application using real-world administrative data. Synthetic datasets were generated with error rates from zero to 20% error. Our method was used to estimate parameters (probabilities and thresholds) for de-duplication linkages. Linkage quality was determined by F-measure. Each dataset was privacy-preserved using separate Bloom filters for each field. Match probabilities were estimated using the expectation-maximisation (EM) algorithm on the privacy-preserved data. Threshold cut-off values were determined by an extension to the EM algorithm allowing linkage quality to be estimated for each possible threshold. De-duplication linkages of each privacy-preserved dataset were performed using both estimated and calculated probabilities. Linkage quality using the F-measure at the estimated threshold values was also compared to the highest F-measure. Three large administrative datasets were used to demonstrate the applicability of the probability and threshold estimation technique on real-world data. Linkage of the synthetic datasets using the estimated probabilities produced an F-measure that was comparable to the F-measure using calculated probabilities, even with up to 20% error. Linkage of the administrative datasets using estimated probabilities produced an F-measure that was higher\n\nChallenges in administrative data linkage for research\n\nDirectory of Open Access Journals (Sweden)\n\nKatie Harron\n\n2017-12-01\n\nFull Text Available Linkage of population-based administrative data is a valuable tool for combining detailed individual-level information from different sources for research. While not a substitute for classical studies based on primary data collection, analyses of linked administrative data can answer questions that require large sample sizes or detailed data on hard-to-reach populations, and generate evidence with a high level of external validity and applicability for policy making. There are unique challenges in the appropriate research use of linked administrative data, for example with respect to bias from linkage errors where records cannot be linked or are linked together incorrectly. For confidentiality and other reasons, the separation of data linkage processes and analysis of linked data is generally regarded as best practice. However, the âblack boxâ of data linkage can make it difficult for researchers to judge the reliability of the resulting linked data for their required purposes. This article aims to provide an overview of challenges in linking administrative data for research. We aim to increase understanding of the implications of (i the data linkage environment and privacy preservation; (ii the linkage process itself (including data preparation, and deterministic and probabilistic linkage methods and (iii linkage quality and potential bias in linked data. We draw on examples from a number of countries to illustrate a range of approaches for data linkage in different contexts.\n\nSoftware for objective comparison of vocal acoustic features over weeks of audio recording: KLFromRecordingDays\n\nScience.gov (United States)\n\nSoderstrom, Ken; Alalawi, Ali\n\nKLFromRecordingDays allows measurement of Kullback-Leibler (KL) distances between 2D probability distributions of vocal acoustic features. Greater KL distance measures reflect increased phonological divergence across the vocalizations compared. The software has been used to compare *.wav file recordings made by Sound Analysis Recorder 2011 of songbird vocalizations pre- and post-drug and surgical manipulations. Recordings from individual animals in *.wav format are first organized into subdirectories by recording day and then segmented into individual syllables uttered and acoustic features of these syllables using Sound Analysis Pro 2011 (SAP). KLFromRecordingDays uses syllable acoustic feature data output by SAP to a MySQL table to generate and compare \"template\" (typically pre-treatment) and \"target\" (typically post-treatment) probability distributions. These distributions are a series of virtual 2D plots of the duration of each syllable (as x-axis) to each of 13 other acoustic features measured by SAP for that syllable (as y-axes). Differences between \"template\" and \"target\" probability distributions for each acoustic feature are determined by calculating KL distance, a measure of divergence of the target 2D distribution pattern from that of the template. KL distances and the mean KL distance across all acoustic features are calculated for each recording day and output to an Excel spreadsheet. Resulting data for individual subjects may then be pooled across treatment groups and graphically summarized and used for statistical comparisons. Because SAP-generated MySQL files are accessed directly, data limits associated with spreadsheet output are avoided, and the totality of vocal output over weeks may be objectively analyzed all at once. The software has been useful for measuring drug effects on songbird vocalizations and assessing recovery from damage to regions of vocal motor cortex. It may be useful in studies employing other species, and as part of speech\n\nSoftware for objective comparison of vocal acoustic features over weeks of audio recording: KLFromRecordingDays\n\nDirectory of Open Access Journals (Sweden)\n\nKen Soderstrom\n\n2017-01-01\n\nFull Text Available KLFromRecordingDays allows measurement of KullbackâLeibler (KL distances between 2D probability distributions of vocal acoustic features. Greater KL distance measures reflect increased phonological divergence across the vocalizations compared. The software has been used to compare *.wav file recordings made by Sound Analysis Recorder 2011 of songbird vocalizations pre- and post-drug and surgical manipulations. Recordings from individual animals in *.wav format are first organized into subdirectories by recording day and then segmented into individual syllables uttered and acoustic features of these syllables using Sound Analysis Pro 2011 (SAP. KLFromRecordingDays uses syllable acoustic feature data output by SAP to a MySQL table to generate and compare âtemplateâ (typically pre-treatment and âtargetâ (typically post-treatment probability distributions. These distributions are a series of virtual 2D plots of the duration of each syllable (as x-axis to each of 13 other acoustic features measured by SAP for that syllable (as y-axes. Differences between âtemplateâ and âtargetâ probability distributions for each acoustic feature are determined by calculating KL distance, a measure of divergence of the target 2D distribution pattern from that of the template. KL distances and the mean KL distance across all acoustic features are calculated for each recording day and output to an Excel spreadsheet. Resulting data for individual subjects may then be pooled across treatment groups and graphically summarized and used for statistical comparisons. Because SAP-generated MySQL files are accessed directly, data limits associated with spreadsheet output are avoided, and the totality of vocal output over weeks may be objectively analyzed all at once. The software has been useful for measuring drug effects on songbird vocalizations and assessing recovery from damage to regions of vocal motor cortex. It may be useful in studies employing other\n\nClinical records anonymisation and text extraction (CRATE): an open-source software system.\n\nScience.gov (United States)\n\nCardinal, Rudolf N\n\n2017-04-26\n\nElectronic medical records contain information of value for research, but contain identifiable and often highly sensitive confidential information. Patient-identifiable information cannot in general be shared outside clinical care teams without explicit consent, but anonymisation/de-identification allows research uses of clinical data without explicit consent. This article presents CRATE (Clinical Records Anonymisation and Text Extraction), an open-source software system with separable functions: (1) it anonymises or de-identifies arbitrary relational databases, with sensitivity and precision similar to previous comparable systems; (2) it uses public secure cryptographic methods to map patient identifiers to research identifiers (pseudonyms); (3) it connects relational databases to external tools for natural language processing; (4) it provides a web front end for research and administrative functions; and (5) it supports a specific model through which patients may consent to be contacted about research. Creation and management of a research database from sensitive clinical records with secure pseudonym generation, full-text indexing, and a consent-to-contact process is possible and practical using entirely free and open-source software.\n\nThe Scottish school leavers cohort: linkage of education data to routinely collected records for mortality, hospital discharge and offspring birth characteristics.\n\nScience.gov (United States)\n\nStewart, Catherine H; Dundas, Ruth; Leyland, Alastair H\n\n2017-07-10\n\nThe Scottish school leavers cohort provides population-wide prospective follow-up of local authority secondary school leavers in Scotland through linkage of comprehensive education data with hospital and mortality records. It considers educational attainment as a proxy for socioeconomic position in young adulthood and enables the study of associations and causal relationships between educational attainment and health outcomes in young adulthood. Education data for 284â621 individuals who left a local authority secondary school during 2006/2007-2010/2011 were linked with birth, death and hospital records, including general/acute and mental health inpatient and day case records. Individuals were followed up from date of school leaving until September 2012. Age range during follow-up was 15 years to 24 years. Education data included all formal school qualifications attained by date of school leaving; sociodemographic information; indicators of student needs, educational or non-educational support received and special school unit attendance; attendance, absence and exclusions over time and school leaver destination. Area-based measures of school and home deprivation were provided. Health data included dates of admission/discharge from hospital; principal/secondary diagnoses; maternal-related, birth-related and baby-related variables and, where relevant, date and cause of death. This paper presents crude rates for all-cause and cause-specific deaths and general/acute and psychiatric hospital admissions as well as birth outcomes for children of female cohort members. This study is the first in Scotland to link education and health data for the population of local authority secondary school leavers and provides access to a large, representative cohort with the ability to study rare health outcomes. There is the potential to study health outcomes over the life course through linkage with future hospital and death records for cohort members. The cohort may also be\n\nUnder-ascertainment of Aboriginality in records of cardiovascular disease in hospital morbidity and mortality data in Western Australia: a record linkage study\n\nDirectory of Open Access Journals (Sweden)\n\nKatzenellenbogen Judy M\n\n2010-12-01\n\nFull Text Available Abstract Background Measuring the real burden of cardiovascular disease in Australian Aboriginals is complicated by under-identification of Aboriginality in administrative health data collections. Accurate data is essential to measure Australia's progress in its efforts to intervene to improve health outcomes of Australian Aboriginals. We estimated the under-ascertainment of Aboriginal status in linked morbidity and mortality databases in patients hospitalised with cardiovascular disease. Methods Persons with public hospital admissions for cardiovascular disease in Western Australia during 2000-2005 (and their 20-year admission history or who subsequently died were identified from linkage data. The Aboriginal status flag in all records for a given individual was variously used to determine their ethnicity (index positive, and in all records both majority positive or ever positive and stratified by region, age and gender. The index admission was the baseline comparator. Results Index cases comprised 62,692 individuals who shared a total of 778,714 hospital admissions over 20 years, of which 19,809 subsequently died. There were 3,060 (4.9% persons identified as Aboriginal on index admission. An additional 83 (2.7% Aboriginal cases were identified through death records, increasing to 3.7% when cases with a positive Aboriginal identifier in the majority (â¥50% of previous hospital admissions over twenty years were added and by 20.8% when those with a positive flag in any record over 20 years were incorporated. These results equated to underestimating Aboriginal status in unlinked index admission by 2.6%, 3.5% and 17.2%, respectively. Deaths classified as Aboriginal in official records would underestimate total Aboriginal deaths by 26.8% (95% Confidence Interval 24.1 to 29.6%. Conclusions Combining Aboriginal determinations in morbidity and official death records increases ascertainment of unlinked cardiovascular morbidity in Western Australian\n\nLinkage and related analyses of Barrett's esophagus and its associated adenocarcinomas.\n\nScience.gov (United States)\n\nSun, Xiangqing; Elston, Robert; Falk, Gary W; Grady, William M; Faulx, Ashley; Mittal, Sumeet K; Canto, Marcia I; Shaheen, Nicholas J; Wang, Jean S; Iyer, Prasad G; Abrams, Julian A; Willis, Joseph E; Guda, Kishore; Markowitz, Sanford; Barnholtz-Sloan, Jill S; Chandar, Apoorva; Brock, Wendy; Chak, Amitabh\n\n2016-07-01\n\nFamilial aggregation and segregation analysis studies have provided evidence of a genetic basis for esophageal adenocarcinoma (EAC) and its premalignant precursor, Barrett's esophagus (BE). We aim to demonstrate the utility of linkage analysis to identify the genomic regions that might contain the genetic variants that predispose individuals to this complex trait (BE and EAC). We genotyped 144 individuals in 42 multiplex pedigrees chosen from 1000 singly ascertained BE/EAC pedigrees, and performed both model-based and model-free linkage analyses, using S.A.G.E. and other software. Segregation models were fitted, from the data on both the 42 pedigrees and the 1000 pedigrees, to determine parameters for performing model-based linkage analysis. Model-based and model-free linkage analyses were conducted in two sets of pedigrees: the 42 pedigrees and a subset of 18 pedigrees with female affected members that are expected to be more genetically homogeneous. Genome-wide associations were also tested in these families. Linkage analyses on the 42 pedigrees identified several regions consistently suggestive of linkage by different linkage analysis methods on chromosomes 2q31, 12q23, and 4p14. A linkage on 15q26 is the only consistent linkage region identified in the 18 female-affected pedigrees, in which the linkage signal is higher than in the 42 pedigrees. Other tentative linkage signals are also reported. Our linkage study of BE/EAC pedigrees identified linkage regions on chromosomes 2, 4, 12, and 15, with some reported associations located within our linkage peaks. Our linkage results can help prioritize association tests to delineate the genetic determinants underlying susceptibility to BE and EAC.\n\nSoftware refactoring at the package level using clustering techniques\n\nKAUST Repository\n\nAlkhalid, A.\n\n2011-01-01\n\nEnhancing, modifying or adapting the software to new requirements increases the internal software complexity. Software with high level of internal complexity is difficult to maintain. Software refactoring reduces software complexity and hence decreases the maintenance effort. However, software refactoring becomes quite challenging task as the software evolves. The authors use clustering as a pattern recognition technique to assist in software refactoring activities at the package level. The approach presents a computer aided support for identifying ill-structured packages and provides suggestions for software designer to balance between intra-package cohesion and inter-package coupling. A comparative study is conducted applying three different clustering techniques on different software systems. In addition, the application of refactoring at the package level using an adaptive k-nearest neighbour (A-KNN) algorithm is introduced. The authors compared A-KNN technique with the other clustering techniques (viz. single linkage algorithm, complete linkage algorithm and weighted pair-group method using arithmetic averages). The new technique shows competitive performance with lower computational complexity. Â© 2011 The Institution of Engineering and Technology.\n\nStudying Hospitalizations and Mortality in the Netherlands: Feasible and Valid Using Two-Step Medical Record Linkage with Nationwide Registers.\n\nDirectory of Open Access Journals (Sweden)\n\nElske Sieswerda\n\nFull Text Available In the Netherlands, the postal code is needed to study hospitalizations of individuals in the nationwide hospitalization register. Studying hospitalizations longitudinally becomes troublesome if individuals change address. We aimed to report on the feasibility and validity of a two-step medical record linkage approach to examine longitudinal trends in hospitalizations and mortality in a study cohort. First, we linked a study cohort of 1564 survivors of childhood cancer with the Municipal Personal Records Database (GBA which has postal code history and mortality data available. Within GBA, we sampled a reference population matched on year of birth, gender and calendar year. Second, we extracted hospitalizations from the Hospital Discharge Register (LMR with a date of discharge during unique follow-up (based on date of birth, gender and postal code in GBA. We calculated the agreement of death and being hospitalized in survivors according to the registers and to available cohort data. We retrieved 1477 (94% survivors from GBA. Median percentages of unique/potential follow-up were 87% (survivors and 83% (reference persons. Characteristics of survivors and reference persons contributing to unique follow-up were comparable. Agreement of hospitalization during unique follow-up was 94% and agreement of death was 98%. In absence of unique identifiers in the Dutch hospitalization register, it is feasible and valid to study hospitalizations and mortality of individuals longitudinally using a two-step medical record linkage approach. Cohort studies in the Netherlands have the opportunity to study mortality and hospitalization rates over time. These outcomes provide insight into the burden of clinical events and healthcare use in studies on patients at risk of long-term morbidities.\n\nAllele-sharing models: LOD scores and accurate linkage tests.\n\nScience.gov (United States)\n\nKong, A; Cox, N J\n\n1997-11-01\n\nStarting with a test statistic for linkage analysis based on allele sharing, we propose an associated one-parameter model. Under general missing-data patterns, this model allows exact calculation of likelihood ratios and LOD scores and has been implemented by a simple modification of existing software. Most important, accurate linkage tests can be performed. Using an example, we show that some previously suggested approaches to handling less than perfectly informative data can be unacceptably conservative. Situations in which this model may not perform well are discussed, and an alternative model that requires additional computations is suggested.\n\nDetecting referral and selection bias by the anonymous linkage of practice, hospital and clinic data using Secure and Private Record Linkage (SAPREL: case study from the evaluation of the Improved Access to Psychological Therapy (IAPT service\n\nDirectory of Open Access Journals (Sweden)\n\nParry Glenys\n\n2011-10-01\n\nFull Text Available Abstract Background The evaluation of demonstration sites set up to provide improved access to psychological therapies (IAPT comprised the study of all people identified as having common mental health problems (CMHP, those referred to the IAPT service, and a sample of attenders studied in-depth. Information technology makes it feasible to link practice, hospital and IAPT clinic data to evaluate the representativeness of these samples. However, researchers do not have permission to browse and link these data without the patients' consent. Objective To demonstrate the use of a mixed deterministic-probabilistic method of secure and private record linkage (SAPREL - to describe selection bias in subjects chosen for in-depth evaluation. Method We extracted, pseudonymised and used fuzzy logic to link multiple health records without the researcher knowing the patient's identity. The method can be characterised as a three party protocol mainly using deterministic algorithms with dynamic linking strategies; though incorporating some elements of probabilistic linkage. Within the data providers' safe haven we extracted: Demographic data, hospital utilisation and IAPT clinic data; converted post code to index of multiple deprivation (IMD; and identified people with CMHP. We contrasted the age, gender, ethnicity and IMD for the in-depth evaluation sample with people referred to IAPT, use hospital services, and the population as a whole. Results The in IAPT-in-depth group had a mean age of 43.1 years; CI: 41.0 - 45.2 (n = 166; the IAPT-referred 40.2 years; CI: 39.4 - 40.9 (n = 1118; and those with CMHP 43.6 years SEM 0.15. (n = 12210. Whilst around 67% of those with a CMHP were women, compared to 70% of those referred to IAPT, and 75% of those subject to in-depth evaluation (Chi square p Conclusions The sample studied in-depth were older, more likely female, and less deprived than people with CMHP, and fewer had recorded ethnic minority status. Anonymous\n\nMaternal and perinatal factors associated with hospitalised infectious mononucleosis in children, adolescents and young adults: record linkage study\n\nScience.gov (United States)\n\n2011-01-01\n\nBackground There is current interest in the role of perinatal factors in the aetiology of diseases that occur later in life. Infectious mononucleosis (IM) can follow late primary infection with Epstein-Barr virus (EBV), and has been shown to increase the risk of multiple sclerosis and Hodgkin's disease. Little is known about maternal or perinatal factors associated with IM or its sequelae. Methods We investigated perinatal risk factors for hospitalised IM using a prospective record-linkage study in a population in the south of England. The dataset used, the Oxford record linkage study (ORLS), includes abstracts of birth registrations, maternities and in-patient hospital records, including day case care, for all subjects in a defined geographical area. From these sources, we identified cases of hospitalised IM up to the age of 30 years in people for whom the ORLS had a maternity record; and we compared perinatal factors in their pregnancy with those in the pregnancy of children who had no hospital record of IM. Results Our data showed a significant association between hospitalised IM and lower social class (p = 0.02), a higher risk of hospitalised IM in children of married rather than single mothers (p < 0.001), and, of marginal statistical significance, an association with singleton birth (p = 0.06). The ratio of observed to expected cases of hospitalised IM in each season was 0.95 in winter, 1.02 in spring, 1.02 in summer and 1.00 in autumn. The chi-square test for seasonality, with a value of 0.8, was not significant. Other factors studied, including low birth weight, short gestational age, maternal smoking, late age at motherhood, did not increase the risk of subsequent hospitalised IM. Conclusions Because of the increasing tendency of women to postpone childbearing, it is useful to know that older age at motherhood is not associated with an increased risk of hospitalised IM in their children. We have no explanation for the finding that children of married women\n\nBenefits of Record Management For Scientific Writing (Study of Metadata Reception of Zotero Reference Management Software in UIN Malang\n\nDirectory of Open Access Journals (Sweden)\n\nMoch Fikriansyah Wicaksono\n\n2018-01-01\n\nFull Text Available Record creation and management by individuals or organizations grows rapidly, particularly the change from print to electronics, and the smallest part of record (metadata. Therefore, there is a need to perform record management metadata, particularly for students who have the needs of recording references and citation. Reference management software (RMS is a software to help reference management, one of them named zotero. The purpose of this article is to describe the benefits of record management for the writing of scientific papers for students, especially on biology study program in UIN Malik Ibrahim Malang. The type of research used is descriptive with quantitative approach. To increase the depth of respondents' answers, we used additional data by conducting interviews. The selected population is 322 students, class of 2012 to 2014, using random sampling. The selection criteria were chosen because the introduction and use of reference management software, zotero have started since three years ago.Â Respondents in this study as many as 80 people, which is obtained from the formula Yamane. The results showed that 70% agreed that using reference management software saved time and energy in managing digital file metadata, 71% agreed that if digital metadata can be quickly stored into RMS, 65% agreed on the ease of storing metadata into the reference management software, 70% agreed when it was easy to configure metadata to quote and bibliography, 56.6% agreed that the metadata stored in reference management software could be edited, 73.8% agreed that using metadata will make it easier to write quotes and bibliography.\n\nDesigning of Electronic Health Record Software in the Nursing and Midwifery Faculty of Tabriz\n\nDirectory of Open Access Journals (Sweden)\n\nVahid Azizi\n\n2012-07-01\n\nFull Text Available Introduction: much effort was conducted to support the use of electronic record systems in nursing process. Some of the most important reasons for its application are efficiency, security and the quality of the patientsâ data registration. The purpose of this study is to present electronic registration software of patients, health assessment and to determine the attitude of nurses towards it. Methods: this is a R&D leading to construction of the patientâs health assessment software. In the beginning, Gordon Model and the daily charts of the patients were prepared to paper. During the next 8 months these charts were converted into the software programs. The databases were implemented using âthe SQL serverâ and âC#Netâ programming language. Results: the software used in this study included 4 parts; the first one contained information of Gordon health assessment model in 11 items, the second contained charts of the study, the third part consisted of Lund-Browder table and dummy data table for 4 age groups, and the fourth one was image infor-mation storage part for burn wounds pictures. Conclusion: despite barriers, electronic systems could lead to confidential information, increase the quality of nursing records, and also reduce the amount of expenses.\n\nElectronic Health Record for Intensive Care based on Usual Windows Based Software.\n\nScience.gov (United States)\n\nReper, Arnaud; Reper, Pascal\n\n2015-08-01\n\nIn Intensive Care Units, the amount of data to be processed for patients care, the turn over of the patients, the necessity for reliability and for review processes indicate the use of Patient Data Management Systems (PDMS) and electronic health records (EHR). To respond to the needs of an Intensive Care Unit and not to be locked with proprietary software, we developed an EHR based on usual software and components. The software was designed as a client-server architecture running on the Windows operating system and powered by the access data base system. The client software was developed using Visual Basic interface library. The application offers to the users the following functions: medical notes captures, observations and treatments, nursing charts with administration of medications, scoring systems for classification, and possibilities to encode medical activities for billing processes. Since his deployment in September 2004, the EHR was used to care more than five thousands patients with the expected software reliability and facilitated data management and review processes. Communications with other medical software were not developed from the start, and are realized by the use of basic functionalities communication engine. Further upgrade of the system will include multi-platform support, use of typed language with static analysis, and configurable interface. The developed system based on usual software components was able to respond to the medical needs of the local ICU environment. The use of Windows for development allowed us to customize the software to the preexisting organization and contributed to the acceptability of the whole system.\n\nTowards the Application of Open Source Software in Developing National Electronic Health Record-Narrative Review Article.\n\nScience.gov (United States)\n\nAminpour, Farzaneh; Sadoughi, Farahnaz; Ahmadi, Maryam\n\n2013-12-01\n\nElectronic Health Record (EHR) is a repository of patient health information shared among multiple authorized users. As a modern method of storing and processing health information, it is a solution for improving quality, safety and efficiency of patient care and health system. However, establishment of EHR requires a significant investment of time and money. While many of healthcare providers have very limited capital, application of open source software would be considered as a solution in developing national electronic health record especially in countries with low income. The evidence showed that financial limitation is one of the obstacles to implement electronic health records in developing countries. Therefore, establishment of an open source EHR system capable of modifications according to the national requirements seems to be inevitable in Iran. The present study identifies the impact of application of open source software in developing national electronic health record in Iran.\n\nThe effect of electronic health record software design on resident documentation and compliance with evidence-based medicine.\n\nScience.gov (United States)\n\nRodriguez Torres, Yasaira; Huang, Jordan; Mihlstin, Melanie; Juzych, Mark S; Kromrei, Heidi; Hwang, Frank S\n\n2017-01-01\n\nThis study aimed to determine the role of electronic health record software in resident education by evaluating documentation of 30 elements extracted from the American Academy of Ophthalmology Dry Eye Syndrome Preferred Practice Pattern. The Kresge Eye Institute transitioned to using electronic health record software in June 2013. We evaluated the charts of 331 patients examined in the resident ophthalmology clinic between September 1, 2011, and March 31, 2014, for an initial evaluation for dry eye syndrome. We compared documentation rates for the 30 evidence-based elements between electronic health record chart note templates among the ophthalmology residents. Overall, significant changes in documentation occurred when transitioning to a new version of the electronic health record software with average compliance ranging from 67.4% to 73.6% (p Electronic Health Record A had high compliance (>90%) in 13 elements while Electronic Health Record B had high compliance (>90%) in 11 elements. The presence of dialog boxes was responsible for significant changes in documentation of adnexa, puncta, proptosis, skin examination, contact lens wear, and smoking exposure. Significant differences in documentation were correlated with electronic health record template design rather than individual resident or residents' year in training. Our results show that electronic health record template design influences documentation across all resident years. Decreased documentation likely results from \"mouse click fatigue\" as residents had to access multiple dialog boxes to complete documentation. These findings highlight the importance of EHR template design to improve resident documentation and integration of evidence-based medicine into their clinical notes.\n\nDevelopment of Software for dose Records Data Base Access; Programacion para la consulta del Banco de Datos Dosimetricos\n\nEnergy Technology Data Exchange (ETDEWEB)\n\nAmaro, M\n\n1990-07-01\n\nThe CIEMAT personal dose records are computerized in a Dosimetric Data Base whose primary purpose was the individual dose follow-up control and the data handling for epidemiological studies. Within the Data Base management scheme, software development to allow searching of individual dose records by external authorised users was undertaken. The report describes the software developed to allow authorised persons to visualize on screen a summary of the individual dose records from workers included in the Data Base. The report includes the User Guide for the authorised list of users and listings of codes and subroutines developed. (Author) 2 refs.\n\nMonth of Conception and Learning Disabilities: A Record-Linkage Study of 801,592 Children.\n\nScience.gov (United States)\n\nMackay, Daniel F; Smith, Gordon C S; Cooper, Sally-Ann; Wood, Rachael; King, Albert; Clark, David N; Pell, Jill P\n\n2016-10-01\n\nLearning disabilities have profound, long-lasting health sequelae. Affected children born over the course of 1 year in the United States of America generated an estimated lifetime cost of $51.2 billion. Results from some studies have suggested that autistic spectrum disorder may vary by season of birth, but there have been few studies in which investigators examined whether this is also true of other causes of learning disabilities. We undertook Scotland-wide record linkage of education (annual pupil census) and maternity (Scottish Morbidity Record 02) databases for 801,592 singleton children attending Scottish schools in 2006-2011. We modeled monthly rates using principal sine and cosine transformations of the month number and demonstrated cyclicity in the percentage of children with special educational needs. Rates were highest among children conceived in the first quarter of the year (January-March) and lowest among those conceived in the third (July-September) (8.9% vs 7.6%; PÂ disabilities, and learning difficulties (e.g., dyslexia) and were absent for sensory or motor/physical impairments and mental, physical, or communication problems. Seasonality accounted for 11.4% (95% confidence interval: 9.0, 13.7) of all cases. Some biologically plausible causes of this variation, such as infection and maternal vitamin D levels, are potentially amendable to intervention. Â© The Author 2016. Published by Oxford University Press on behalf of the Johns Hopkins Bloomberg School of Public Health. All rights reserved. For permissions, please e-mail: journals.permissions@oup.com.\n\nThe effect of electronic health record software design on resident documentation and compliance with evidence-based medicine.\n\nDirectory of Open Access Journals (Sweden)\n\nYasaira Rodriguez Torres\n\nFull Text Available This study aimed to determine the role of electronic health record software in resident education by evaluating documentation of 30 elements extracted from the American Academy of Ophthalmology Dry Eye Syndrome Preferred Practice Pattern. The Kresge Eye Institute transitioned to using electronic health record software in June 2013. We evaluated the charts of 331 patients examined in the resident ophthalmology clinic between September 1, 2011, and March 31, 2014, for an initial evaluation for dry eye syndrome. We compared documentation rates for the 30 evidence-based elements between electronic health record chart note templates among the ophthalmology residents. Overall, significant changes in documentation occurred when transitioning to a new version of the electronic health record software with average compliance ranging from 67.4% to 73.6% (p 90% in 13 elements while Electronic Health Record B had high compliance (>90% in 11 elements. The presence of dialog boxes was responsible for significant changes in documentation of adnexa, puncta, proptosis, skin examination, contact lens wear, and smoking exposure. Significant differences in documentation were correlated with electronic health record template design rather than individual resident or residents' year in training. Our results show that electronic health record template design influences documentation across all resident years. Decreased documentation likely results from \"mouse click fatigue\" as residents had to access multiple dialog boxes to complete documentation. These findings highlight the importance of EHR template design to improve resident documentation and integration of evidence-based medicine into their clinical notes.\n\nA guide to evaluating linkage quality for the analysis of linked data"
    }
}
{
    "id": "wrong_mix_random_foundationPlace_00000_0",
    "rank": 30,
    "data": {
        "url": "https://arxiv.org/html/2404.03868v1",
        "read_more_link": "",
        "language": "en",
        "title": "Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://arxiv.org/html/x1.png",
            "https://arxiv.org/html/x2.png",
            "https://arxiv.org/html/x3.png",
            "https://arxiv.org/html/x4.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "License: CC BY 4.0\n\narXiv:2404.03868v1 [cs.CL] 05 Apr 2024\n\nExtract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction\n\nBowen Zhang11{}^{1}start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT and Harold Soh1,212{}^{1,2}start_FLOATSUPERSCRIPT 1 , 2 end_FLOATSUPERSCRIPT 11{}^{1}start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPTDeptartment of Computer Science, National University of Singapore {bowenzhang, harold}@comp.nus.edu.sg. 22{}^{2}start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPTSmart Systems Institute (SSI), National University of Singapore.\n\nAbstract\n\nIn this work, we are interested in automated methods for knowledge graph creation (KGC) from input text. Progress on large language models (LLMs) has prompted a series of recent works applying them to KGC, e.g., via zero/few-shot prompting. Despite successes on small domain-specific datasets, these models face difficulties scaling up to text common in many real-world applications. A principal issue is that in prior methods, the KG schema has to be included in the LLM prompt to generate valid triplets; larger and more complex schema easily exceed the LLMs’ context window length. To address this problem, we propose a three-phase framework named Extract-Define-Canonicalize (EDC): open information extraction followed by schema definition and post-hoc canonicalization. EDC is flexible in that it can be applied to settings where a pre-defined target schema is available and when it is not; in the latter case, it constructs a schema automatically and applies self-canonicalization. To further improve performance, we introduce a trained component that retrieves schema elements relevant to the input text; this improves the LLMs’ extraction performance in a retrieval-augmented generation-like manner. We demonstrate on three KGC benchmarks that EDC is able to extract high-quality triplets without any parameter tuning and with significantly larger schemas compared to prior works.\n\n1 Introduction\n\nKnowledge graphs (KGs) (Ji et al., 2021) are a structured representation of knowledge that organizes interconnected information through graph structures, where entities and relations are represented as nodes and edges. They are broadly used in a variety of downstream tasks such as decision-making (Guo et al., 2021; Lan et al., 2020), question-answering (Huang et al., 2019; Yasunaga et al., 2021), and recommendation (Guo et al., 2020; Wang et al., 2019). However, knowledge graph construction (KGC) is inherently challenging: the task requires competence in understanding syntax and semantics to generate a consistent, concise, and meaningful knowledge graph. As such, KGC predominantly relies on intensive human labor (Ye et al., 2022).\n\nRecent attempts to automate KGC (Zhong et al., 2023; Ye et al., 2022) have employed large language models (LLMs) in view of their remarkable natural language understanding capabilities. LLM-based KGC methods employ various innovative prompt-based techniques, such as multi-turn conversation (Wei et al., 2023) and code generation (Bi et al., 2024), to generate entity-relation triplets that represent the knowledge graph. However, these methods are currently limited to small and domain-specific scenarios — to ensure the validity of generated triplets, schema information (e.g., possible entity and relation types) has to be included in the prompt. Complex datasets (e.g., Wikipedia) typically require large schemas that exceed the context window length or can be ignored by the LLMs (Wadhwa et al., 2023).\n\nPhases 2 and 3 (Define and Canonicalize) standardize the triplets to make them useful for downstream tasks. We designed EDC to be flexible: it can either discover triplets consistent with a pre-existing schema of potentially large size (Target Alignment) or self-generate a schema (Self Canonicalization). To achieve this, we use LLMs to define the schema components by exploiting their explanation generation capabilities — LLMs can justify their extractions via explanations that are agreeable to human experts (Li et al., 2023). The definitions are used to find the closest entity/relation type candidates (via a vector similarity search) that the LLM can then reference to canonicalize a component. In the case there is no equivalent counterpart in the existing schema, we can choose to add it to enrich the schema.\n\nTo further improve performance, the three steps above can be followed by an additional Refinement phase: we repeat EDC but provide the previously extracted triplets and a relevant part of the schema in the prompt during the initial extraction. We propose a trained Schema Retriever that retrieves schema components relevant to the input text (akin to retrieval-augmented generation (Lewis et al., 2020)), which we find improves the generated triplets.\n\nExperiments on three KGC datasets in both Target Alignment and Self Canonicalization settings show that EDC is able to extract higher-quality KGs compared to state-of-the-art methods through both automatic and manual evaluation. Furthermore, the use of the Schema Retriever is shown to significantly and consistently improve EDC’s performance.\n\nIn summary, the paper makes the following contributions:\n\n•\n\nEDC, a flexible and performant LLM-based framework for knowledge graph construction that is able to extract high-quality KGs with schema of large size or without any pre-defined schema.\n\n•\n\nSchema Retriever, a trained model to extract schema components relevant to input text in the same vein as information retrieval.\n\n•\n\nEmpirical evidence that demonstrate the effectiveness of EDC and the Schema Retriever.\n\n2 Background\n\nIn this section, we provide relevant background on knowledge graph construction (KGC), open information extraction (OIE), and canonicalization.\n\nKnowledge Graph Construction.\n\nTraditional methods typically addressed KGC using “pipelines”, comprising subtasks like entity discovery (Žukov-Gregorič et al., 2018; Martins et al., 2019), entity typing (Choi et al., 2018; Onoe & Durrett, 2020), and relation classification (Zeng et al., 2014; 2015). Thanks to advances in pre-trained generative language models (e.g., T5 (Raffel et al., 2020) and BERT(Lewis et al., 2019)), more recent works instead frame KGC as a sequence-to-sequence problem and generate relational triplets in an end-to-end manner by fine-tuning these moderately-sized language models (Ye et al., 2022). The success of large language models (LLMs) has pushed this paradigm further: current methods directly prompt the LLMs to generate triplets in a zero/few-shot manner. For example, ChatIE (Wei et al., 2023) extracts triplets by framing the task as a multi-turn question-answering problem and CodeKGC (Bi et al., 2024) approaches the task as a code generation problem. As previously mentioned, these models face difficulties scaling up to general text common in many real-world applications as the KG schema has to be included in the LLM prompt. Our EDC framework circumvents this problem by using post-hoc canonicalization (and without requiring fine-tuning of the base LLMs).\n\nOpen Information Extraction and Canonicalization.\n\nStandard (closed) information extraction requires the output triplets to follow a pre-defined schema, e.g. a list of relation or entity types to be extracted from. In contrast, open information extraction (OIE) does not have such a requirement. OIE has a long history and we refer readers who want comprehensive coverage to the excellent surveys (Liu et al., 2022; Zhou et al., 2022; Kamp et al., 2023). Recent studies have found LLMs to exhibit excellent performance on OIE tasks (Li et al., 2023). However, the relational triplets extracted from OIE systems are not canonicalized, e.g. multiple semantically equivalent relations can coexist without being unified to a canonical form, causing redundancy and ambiguity in the induced open knowledge graph. An extra canonicalization step is required to standardize the triplets to make the KGs useful for downstream applications.\n\nCanonicalization methods differ depending on whether a target schema is available. In case a target schema is present, the task is sometimes referred to as “alignment” (Putri et al., 2019). For example, Putri et al. (2019) uses WordNet (Miller, 1995) as side information to obtain definitions for the OIE-extracted relation phrases and a Siamese network to compare an OIE relation definition and a pre-defined relation in the target schema. In case no target schema is available, state-of-the-art methods are commonly based on clustering (Vashishth et al., 2018; Dash et al., 2020). CESI (Vashishth et al., 2018) creates embeddings for the OIE relations using side information from external sources like PPDB (Ganitkevitch et al., 2013) and WordNet. However, clustering-based methods are prone to over-generalization (Kamp et al., 2023; Putri et al., 2019), e.g., CESI may put “is brother of,” “is son of,” “is main villain of,” and “was professor of” into the same relation cluster.\n\nCompared to the existing canonicalization methods, EDC is more general; it works whether a target schema is provided or not. Instead of using static external sources like WordNet, EDC utilizes contextual and semantically-rich side information generated by LLMs. Furthermore, by allowing the LLMs to verify if a transformation can be performed (instead of solely relying on the embedding similarity), EDC alleviates the over-generalization issue faced by previous methods.\n\n3 Method: EDC for KGC\n\nThis section outlines our primary contribution: an approach to constructing knowledge graphs that leverages LLMs in a structured manner. We first detail the EDC framework followed by a description of refinement (EDC+R). Given input text, our goal is to extract relational triplets in a canonical form such that the resulting KGs will have minimal ambiguity and redundancy. When there is a pre-defined target schema, all generated triplets should conform to it. In the scenario where there is not one, the system should dynamically create one and canonicalize the triplets with respect to it.\n\n3.1 EDC: Extract-Define-Canonicalize\n\nAt a high level, EDC decomposes KGC into three connected subtasks. To ground our discussion, we will use a specific input text example: “Alan Shepard was born on Nov 18, 1923 and selected by NASA in 1959. He was a member of the Apollo 14 crew” and walk through each of the phases:\n\nPhase 1: Open Information Extraction: we first leverage Large Language Models (LLMs) for open information extraction. Through few-shot prompting, LLMs identify and extract relational triplets ([Subject, Relation, Object]) from input texts, independent of any specific schema. Using our example above, the prompt is:\n\nGiven a piece of text, extract relational triplets in the form of [Subject, Relation, Object] from it. Here are some examples: Example 1: Text: The 17068.8 millimeter long ALCO RS-3 has a diesel-electric transmission. Triplets: [[’ALCO RS-3’, ’powerType’, ’Diesel-electric transmission’], [’ALCO RS-3’, ’length’, ’17068.8 (millimetres)’]] … Now please extract triplets from the following text: Alan Shepard was born on Nov 18, 1923 and selected by NASA in 1959. He was a member of the Apollo 14 crew.\n\nThe resultant triplets (in this case, [’Alan Shepard’, ‘bornOn’, ‘Nov 18, 1923’], [’Alan Shepard’, participatedIn’, ’Apollo 14’]) form an open KG, which is forwarded to subsequent phases.\n\nPhase 2: Schema Definition: Next, we prompt the LLMs to provide a natural language definition for each component of the schema induced by the open KG:\n\nGiven a piece of text and a list of relational triplets extracted from it, write a definition for each relation present. Example 1: Text: The 17068.8 millimeter long ALCO RS-3 has a diesel-electric transmission. Triplets: [[’ALCO RS-3’, ’powerType’, ’Diesel-electric transmission’], [’ALCO RS-3’, ’length’, ’17068.8 (millimetres)’]] Definitions: powerType: The subject entity uses the type of power or energy source specified by the object entity. … Now write a definition for each relation present in the triplets extracted from the following text: Text: Alan Shepard was an American who was born on Nov 18, 1923 in New Hampshire, was selected by NASA in 1959, was a member of the Apollo 14 crew and died in California Triplets: [[’Alan Shepard’, ‘bornOn’, ‘Nov 18, 1923’], [’Alan Shepard’, participatedIn’, ’Apollo 14’]]\n\nThis example prompt results in the definitions for (bornOn: The subject entity was born on the date specified by the object entity.) and (participatedIn: The subject entity took part in the event or mission specified by the object entity.), which are then passed to the next stage as side information used for canonicalization.\n\nPhase 3: Schema Canonicalization: The third phase aims to refine the open KG into a canonical form, eliminating redundancies and ambiguities. We start by vectorizing the definitions of each schema component using a sentence transformer to create embeddings. Canonicalization then proceeds in one of two ways, depending on the availability of a target schema:\n\n•\n\nTarget Alignment: With an existing target schema, we identify the most closely related components within the target schema for each element, considering them for canonicalization. To prevent issues of over-generalization, LLMs assess the feasibility of each potential transformation. If a transformation is deemed unreasonable, indicating no semantic equivalent in the target schema, the component, and its related triplets are excluded.\n\n•\n\nSelf Canonicalization: Absent a target schema, the goal is to consolidate semantically similar schema components, standardizing them to a singular representation to streamline the KG. Starting with an empty canonical schema, we examine the open KG triplets, searching for potential consolidation candidates through vector similarity and LLM verification. Unlike target alignment, components deemed non-transformable are added to the canonical schema, thereby expanding it.\n\nUsing our example, the prompt is:\n\nGiven a piece of text, a relational triplet extracted from it, and the definition of the relation in it, choose the most appropriate relation to replace it in this context if there is any. Text: Alan Shepard was born on Nov 18, 1923 and selected by NASA in 1959. He was a member of the Apollo 14 crew. Triplets: [’Alan Shepard’, participatedIn’, ’Apollo 14’] Definition of ‘participatedIn’: The subject entity took part in the event or mission specified by the object entity. Choices: A. ’mission’: The subject entity participated in the event or operation specified by the object entity. B. ’season’: The subject entity participated in the season specified by the object entity. C. ’league’: The subject entity participates or competes in the league specified by the object entity. D. ’activeYearsStartYear’: The subject entity started their active career in the year specified by the object entity. E. ’foundingYear’: The subject entity was founded in the year specified by the object entity. F. None of the above\n\nNote that the choices above are obtained by using vector similarity search. After the LLM makes its choice, the relations are transformed to yield: [’Alan Shepard’, ‘birthDate’, ‘Nov 18, 1923’], [’Alan Shepard’, ’mission’, ’Apollo 14’], which forms our canonicalized KG.\n\n3.2 EDC+R: iteratively refine EDC with Schema Retriever\n\nThe refinement process leverages the data generated by EDC to enhance the quality of the extracted triplets. Inspired by retrieval-augmented generation and prior work Bi et al. (2024), we construct a “hint” for the extraction phase, which comprises two main elements:\n\n•\n\nCandidate Entities: The entities extracted by EDC from the previous iteration, and entities extracted from the text using the LLM;\n\n•\n\nCandidate Relations: The relations extracted by EDC from the previous cycle and relations retrieved from the pre-defined/canonicalized schema by using a trained Schema Retriever.\n\nThe inclusion of entities and relations from both the LLM and the schema retriever provides a richer pool of candidates for the LLM, which addresses issues where the absence of entities or relations impairs the LLM’s effectiveness. By merging the entities and relations extracted in earlier phases with new findings from entity extraction and schema retrieval, the hint serves to aid the OIE by bootstrapping from the previous round.\n\nTo scale EDC to large schemas, we employ a trained Schema Retriever which allows us to efficiently search schemas. The Schema Retriever works in a similar fashion to information retrieval methods based on vector spaces Ganguly et al. (2015); Lewis et al. (2020); it projects the schema components and the input text to a vector space such that cosine similarity captures the relevance between the two, i.e., how likely a schema component to be present in the input text. Note that in our setting, the similarity space is different from the standard sentence embedding models where cosine similarity in the vector space captures semantic equivalence. Our Schema Retriever is a fine-tuned variant of the sentence embedding model E5-mistral-7b-instruct (Wang et al., 2023). We follow the original training methodology detailed in the paper, which involves utilizing pairs of text and their corresponding defined relations. For details, please refer to the Appendix. For a given positive text-relation pair (t+,r+)superscript𝑡superscript𝑟(t^{+},r^{+})( italic_t start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT , italic_r start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT ), we employ an instruction template on t+superscript𝑡t^{+}italic_t start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT to generate a new text ti⁢n⁢s⁢t+subscriptsuperscript𝑡𝑖𝑛𝑠𝑡t^{+}_{inst}italic_t start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_n italic_s italic_t end_POSTSUBSCRIPT:\n\nti⁢n⁢s⁢t+=Instruct: retrieve relations that are present in the given text \\n Query: ⁢{t+}.subscriptsuperscript𝑡𝑖𝑛𝑠𝑡Instruct: retrieve relations that are present in the given text \\n Query: superscript𝑡\\displaystyle t^{+}_{inst}=\\textsf{Instruct: retrieve relations that are % present in the given text \\textbackslash n Query: }\\{t^{+}\\}.italic_t start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_n italic_s italic_t end_POSTSUBSCRIPT = Instruct: retrieve relations that are present in the given text \\n Query: { italic_t start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT } .\n\nWe then finetune the embedding model to distinguish between the correct relation associated with a given text and other non-relevant relations using the InfoNCE loss.\n\nBack to our example, refinement with the schema retriever adds the following relation to the previous set: [’Alan Shepard’, ’selectedByNasa’, ’Apollo 14’]. The relation ’selectedByNasa’ is rather obscure but was specified in the target schema.\n\n4 Experiments\n\nIn this section, we describe experiments designed to evaluate the performance of EDC and EDC+R. Briefly, our results demonstrate that EDC significantly outperforms the state-of-the-art methods in both Target Alignment and Self Canonicalization settings. Refinement further improves EDC. Source code for EDC and to replicate our experiments are available at https://github.com/clear-nus/edc, with full tables in the appendix.\n\n4.1 Experimental Setup\n\nDatasets.\n\nWe evaluate EDC using three KGC datasets:\n\n•\n\nWebNLG (Ferreira et al., 2020): We use the test split from the semantic parsing task of WebNLG+2020 (v3.0). It contains 1165 pairs of text and triplets. The schema derived from these reference triplets encompasses 159 unique relation types.\n\n•\n\nREBEL (Cabot & Navigli, 2021): The original test partition of REBEL comprises 105,516 entries. To manage costs, we select a random sample of 1000 text-triplet pairs. This subset induces a schema with 200 distinct relation types.\n\n•\n\nWiki-NRE (Distiawan et al., 2019): From Wiki-NRE’s test split (29,619 entries), we sample 1000 text-triplet pairs, resulting in a schema with 45 unique relation types.\n\nThese datasets were chosen over alternatives like ADE (Gurulingappa et al., 2012) (1 relation type), SciERC (Luan et al., 2018) (7 relation types), and CoNLL04 (Roth & Yih, 2004) (4 relation types) used to evaluate previous LLM-based methods (Bi et al., 2024; Wadhwa et al., 2023) used in prior LLM-based studies, due to their richer variety of relation types. This diversity better mimics real-world complexities. In our experiments, we focus on extracting relations as the only schema component available across all datasets. Relations, being a foundational element of KGs, are prioritized over other components like entity or event types. However, note that EDC can be readily extended to other schema components.\n\nEDC Models.\n\nEDC contains multiple modules that are powered by LLMs. Since the OIE module is the key upstream module that determines the semantic content captured in the KG, we tested different LLMs of different sizes including GPT-4 (Achiam et al., 2023), GPT-3.5-turbo (Brown et al., 2020), and Mistral-7b (Jiang et al., 2023). Mistral-7b was deployed on a local workstation, whereas the GPT models were accessed via the OpenAI API. For the framework’s remaining components which required prompting, we used GPT-3.5-turbo. In the canonicalization phase, the E5-Mistral-7b model was utilized for vector similarity searches without modifications.\n\n4.1.1 Evaluation Criteria and Baselines\n\nWe evaluate our methods differently under Target Alignment (when a schema is provided) and Self Canonicalization (no schema). For the datasets above, the LLM-based KGC methods (ChatIE and CodeKGC) could not be used due to the schema size.\n\nTarget Alignment.\n\nWe compare EDC and EDC+R against the specialized trained models for each of the datasets:\n\n•\n\nREGEN (Dognin et al., 2021) is the SOTA model for WebNLG. It is a generative sequence-to-sequence model that leverages pretrained T5 (Raffel et al., 2020) and Reinforcement Learning (RL) for bidirectional text-to-graph and graph-to-text generation.\n\n•\n\nGenIE (Josifoski et al., 2022), a generative sequence-to-sequence model that leverages pre-trained BART (Lewis et al., 2019) and a bi-level constrained generation strategy to constrain the output triplets to be consistent with the pre-defined schema. GenIE is the state-of-the-art model for REBEL and Wiki-NRE.\n\nFollowing previous work (Dognin et al., 2021; Melnyk et al., 2022), we use the WEBNLG evaluation script (Ferreira et al., 2020) which computes the Precision, Recall, and F1 scores for the output triplets against the ground truth in a token-based manner. Metrics based on Named Entity Evaluation were used to measure the Precision, Recall, and F1 score in three different ways.\n\n•\n\nExact: Requires a complete match between the candidate and reference triple, disregarding the type (subject, relation, object).\n\n•\n\nPartial: Allows for at least a partial match between the candidate and reference triple, disregarding the type.\n\n•\n\nStrict: Demands an exact match between the candidate and reference triplet, including the element types.\n\nSelf Canonicalization.\n\nFor evaluating self-canonicalization performance, comparisons are made with:\n\n•\n\nBaseline Open KG, which is the initial open KG output from the OIE (Open Information Extraction) phase. This serves as a reference point to illustrate the improvements in precision and schema conciseness resulting from the canonicalization process.\n\n•\n\nCESI (Vashishth et al., 2018), recognized as a leading clustering-based approach for KG canonicalization. By applying CESI to the open KG, we aim to contrast its performance against canonicalization by EDC.\n\nGiven that canonicalized triplets may use relations phrased differently from the reference triplets, a token-based evaluation is unsuitable. Thus, we resort to manual evaluation, focusing on three key aspects:\n\n•\n\nPrecision: The canonicalized triplets remain correct and meaningful with respect to the text compared to the OIE triplets.\n\n•\n\nConciseness: The schema’s brevity is measured by the number of relations types.\n\n•\n\nRedundancy: We employ a redundancy score — the average cosine similarity among each canonicalized relation and its nearest counterpart — where low scores indicate that the schema’s relations are semantically distinct.\n\n4.2 Results and Analysis\n\nIn the following, we focus on conveying our main findings and results. For full results and tables, please refer to the Appendix.\n\n4.2.1 Target Alignment\n\nThe bar charts in Figure 2 summarize the Partial F1 scores obtained by EDC and EDC+R on all three datasets with different LLMs for OIE compared against the respective baselines. EDC demonstrates performance that is superior to or on par with the state-of-the-art baselines for all evaluated datasets. Comparing the LLMs, GPT-4 emerges as the top performer, with Mistral-7b and GPT-3.5-turbo exhibiting comparable results. The disparity between our methods and the baselines is more pronounced on the REBEL and Wiki-NRE datasets; this is primarily due to the GenIE’s constrained generation approach, which falls short in extracting triplets that include literals, such as numbers and dates.\n\nRefinement (EDC+R) consistently and significantly enhances performance. Post-refinement, the difference in performance between GPT-3.5-turbo and Mistral-7b is larger, suggesting Mistral-7b’s was not as able to leverage the provided hints. Nevertheless, a single refinement iteration with the hint improved performance for all the tested LLMs.\n\nFrom the scores, it appears that EDC performance is significantly better on WebNLG compared to REBEL and Wiki-NRE. However, we observed that EDC was penalized despite producing valid triplets on the latter datasets. A reason for this is that the reference triplets in these datasets are non-exhaustive. For example, given the text in the REBEL dataset, ‘Romany Love is a 1931 British musical film directed by Fred Paul and starring Esmond Knight, Florence McHugh and Roy Travers.’, EDC extracts: [’Romany Love’, ’cast member’, ’Esmond Knight’], [’Romany Love’, ’cast member’, ’Florence McHugh’], [’Romany Love’, ’cast member’, ’Roy Travers’], which are all semantically correct, but only the first triplet is present in the reference set. The datasets also contain reference triplets based on information extraneous to the text, e.g., ‘Daniel is an Ethiopian footballer, who currently plays for Hawassa City S.C.’ has a corresponding reference triplet [’Hawassa City S.C.’, ’country’, ’Ethiopia’].\n\nThese issues can be attributed to the distinct methodologies employed in the creation of these datasets. For WebNLG, annotators were asked to compose text solely from the triplets. Thus, the text and the triplets have a direct correspondence, and the text typically does not include information other than what is apparent from the triplets. In contrast, REBEL and Wiki-NRE are created by aligning text and triplets using distant supervision (Smirnova & Cudré-Mauroux, 2018). This method can lead to less straightforward triplets to extract and incomplete reference sets, which can lead to pessimistic evaluations for methods such as EDC that produce correct triplets not in the dataset (Han et al., 2023; Wadhwa et al., 2023). On average, EDC extracts 1 more triplet per sentence compared to the reference set on REBEL and Wiki-NRE, compared to WebNLG where EDC extracts a similar number of triplets to the reference.\n\nAblation study on schema retriever. To evaluate the impact of the relations provided by the schema retriever during refinement, we conducted an ablation study with GPT-3.5-turbo by removing the relations retrieved using the schema retriever from the hint. The results in Table 1 show that ablating the Schema Retriever leads to a notable decline in performance. Qualitatively, we find that the schema retriever helps to find relevant relations that are challenging for the LLMs to identify during the OIE stage. For example, given the text ‘The University of Burgundy in Dijon has 16,800 undergraduate students’, the LLMs extract [’University of Burgundy’, ’location’, ’Dijon’] during OIE. Although semantically correct, this relation overlooks the more specific relation present in the target schema, namely ‘campus’, for denoting university’s location. The schema retriever successfully identifies this finer relation, enabling the LLMs to adjust their extraction to [’University of Burgundy’, ’campus’, ’Dijon’]. This experiment highlights the schema retriever’s value in facilitating the extraction of precise and contextually appropriate relations.\n\n4.2.2 Self Canonicalization\n\nHere, we focus on evaluating EDC’s self-canonicalization performance (utilizing GPT-3.5-turbo for OIE). We omit refinement in Self Canonicalization setting as it has already been studied above and in subsequent iterations, the self-constructed canonicalized schema becomes the target schema. Following prior work (Wadhwa et al., 2023; Kolluru et al., 2020), we conducted a targeted human evaluation of knowledge graphs. This evaluation involved two independent annotators assessing the reasonableness of triplet extractions from given text without prior knowledge of the system’s details. We observed a high inter-annotator agreement score of 0.94.\n\nThe evaluation results and schema metrics are summarized in Table 2. These findings reveal that while the open KG generated by the OIE stage contains semantically valid triplets, it suffers from a significant degree of redundancy within the resultant schema. EDC accurately canonicalizes the open KG to yield a schema that is both more concise and less redundant compared to CESI. EDC avoids CESI’s tendency toward over-generalization — in line with prior work (Putri et al., 2019), we observed CESI inappropriately clusters diverse relations such as ’place of death’, ’place of birth’, ’date of death’, ’date of birth’, and ’cause of death’ into a single ’date of death’ category.\n\n5 Conclusion\n\nIn this work, we presented EDC, an LLM-based three-phase framework that addresses the problem of KGC by open information extraction followed by post-hoc canonicalization. Experiments show that EDC and EDC+R are able to extract better KGs than specialized trained models when a target schema is available and dynamically create a schema when none is provided. The scalability and versatility of EDC opens up many opportunities for applications: it allows us to automatically extract high-quality KGs from general text using large schemas like Wikidata (Vrandečić & Krötzsch, 2014) and even enrich these schemas with newly discovered relations. Future work could also improve EDC’s components to further boost performance, e.g. the schema retriever can be trained on more diverse and higher-quality data.\n\nReferences\n\nAchiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\n\nAgarwal et al. (2020) Oshin Agarwal, Heming Ge, Siamak Shakeri, and Rami Al-Rfou. Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training. arXiv preprint arXiv:2010.12688, 2020.\n\nBi et al. (2024) Zhen Bi, Jing Chen, Yinuo Jiang, Feiyu Xiong, Wei Guo, Huajun Chen, and Ningyu Zhang. Codekgc: Code language model for generative knowledge graph construction. ACM Transactions on Asian and Low-Resource Language Information Processing, 23(3):1–16, 2024.\n\nBrown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\n\nCabot & Navigli (2021) Pere-Lluís Huguet Cabot and Roberto Navigli. Rebel: Relation extraction by end-to-end language generation. In Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 2370–2381, 2021.\n\nChoi et al. (2018) Eunsol Choi, Omer Levy, Yejin Choi, and Luke Zettlemoyer. Ultra-fine entity typing. arXiv preprint arXiv:1807.04905, 2018.\n\nDash et al. (2020) Sarthak Dash, Gaetano Rossiello, Nandana Mihindukulasooriya, Sugato Bagchi, and Alfio Gliozzo. Open knowledge graphs canonicalization using variational autoencoders. arXiv preprint arXiv:2012.04780, 2020.\n\nDistiawan et al. (2019) Bayu Distiawan, Gerhard Weikum, Jianzhong Qi, and Rui Zhang. Neural relation extraction for knowledge base enrichment. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 229–240, 2019.\n\nDognin et al. (2021) Pierre L Dognin, Inkit Padhi, Igor Melnyk, and Payel Das. Regen: Reinforcement learning for text and knowledge base generation using pretrained language models. arXiv preprint arXiv:2108.12472, 2021.\n\nFerreira et al. (2020) Thiago Castro Ferreira, Claire Gardent, Nikolai Ilinykh, Chris Van Der Lee, Simon Mille, Diego Moussallem, and Anastasia Shimorina. The 2020 bilingual, bi-directional webnlg+ shared task overview and evaluation results (webnlg+ 2020). In Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+), 2020.\n\nGanguly et al. (2015) Debasis Ganguly, Dwaipayan Roy, Mandar Mitra, and Gareth JF Jones. Word embedding based generalized language model for information retrieval. In Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval, pp. 795–798, 2015.\n\nGanitkevitch et al. (2013) Juri Ganitkevitch, Benjamin Van Durme, and Chris Callison-Burch. Ppdb: The paraphrase database. In Proceedings of the 2013 conference of the north american chapter of the association for computational linguistics: Human language technologies, pp. 758–764, 2013.\n\nGuo et al. (2021) Liang Guo, Fu Yan, Yuqian Lu, Ming Zhou, and Tao Yang. An automatic machining process decision-making system based on knowledge graph. International journal of computer integrated manufacturing, 34(12):1348–1369, 2021.\n\nGuo et al. (2020) Qingyu Guo, Fuzhen Zhuang, Chuan Qin, Hengshu Zhu, Xing Xie, Hui Xiong, and Qing He. A survey on knowledge graph-based recommender systems. IEEE Transactions on Knowledge and Data Engineering, 34(8):3549–3568, 2020.\n\nGurulingappa et al. (2012) Harsha Gurulingappa, Abdul Mateen Rajput, Angus Roberts, Juliane Fluck, Martin Hofmann-Apitius, and Luca Toldo. Development of a benchmark corpus to support the automatic extraction of drug-related adverse effects from medical case reports. Journal of biomedical informatics, 45(5):885–892, 2012.\n\nHan et al. (2023) Ridong Han, Tao Peng, Chaohao Yang, Benyou Wang, Lu Liu, and Xiang Wan. Is information extraction solved by chatgpt? an analysis of performance, evaluation criteria, robustness and errors. arXiv preprint arXiv:2305.14450, 2023.\n\nHuang et al. (2019) Xiao Huang, Jingyuan Zhang, Dingcheng Li, and Ping Li. Knowledge graph embedding based question answering. In Proceedings of the twelfth ACM international conference on web search and data mining, pp. 105–113, 2019.\n\nJi et al. (2021) Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and S Yu Philip. A survey on knowledge graphs: Representation, acquisition, and applications. IEEE transactions on neural networks and learning systems, 33(2):494–514, 2021.\n\nJiang et al. (2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.\n\nJosifoski et al. (2022) Martin Josifoski, Nicola De Cao, Maxime Peyrard, Fabio Petroni, and Robert West. GenIE: Generative information extraction. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 4626–4643, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.342. URL https://aclanthology.org/2022.naacl-main.342.\n\nKamp et al. (2023) Serafina Kamp, Morteza Fayazi, Zineb Benameur-El, Shuyan Yu, and Ronald Dreslinski. Open information extraction: A review of baseline techniques, approaches, and applications. arXiv preprint arXiv:2310.11644, 2023.\n\nKolluru et al. (2020) Keshav Kolluru, Vaibhav Adlakha, Samarth Aggarwal, Soumen Chakrabarti, et al. Openie6: Iterative grid labeling and coordination analysis for open information extraction. arXiv preprint arXiv:2010.03147, 2020.\n\nLan et al. (2020) Luong Thi Hong Lan, Tran Manh Tuan, Tran Thi Ngan, Nguyen Long Giang, Vo Truong Nhu Ngoc, Pham Van Hai, et al. A new complex fuzzy inference system with fuzzy knowledge graph and extensions in decision making. Ieee Access, 8:164899–164921, 2020.\n\nLewis et al. (2019) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019.\n\nLewis et al. (2020) Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459–9474, 2020.\n\nLi et al. (2023) Bo Li, Gexiang Fang, Yang Yang, Quansen Wang, Wei Ye, Wen Zhao, and Shikun Zhang. Evaluating chatgpt’s information extraction capabilities: An assessment of performance, explainability, calibration, and faithfulness. arXiv preprint arXiv:2304.11633, 2023.\n\nLiu et al. (2022) Pai Liu, Wenyang Gao, Wenjie Dong, Songfang Huang, and Yue Zhang. Open information extraction from 2007 to 2022–a survey. arXiv preprint arXiv:2208.08690, 2022.\n\nLuan et al. (2018) Yi Luan, Luheng He, Mari Ostendorf, and Hannaneh Hajishirzi. Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction. arXiv preprint arXiv:1808.09602, 2018.\n\nMartins et al. (2019) Pedro Henrique Martins, Zita Marinho, and André FT Martins. Joint learning of named entity recognition and entity linking. arXiv preprint arXiv:1907.08243, 2019.\n\nMelnyk et al. (2022) Igor Melnyk, Pierre Dognin, and Payel Das. Knowledge graph generation from text. arXiv preprint arXiv:2211.10511, 2022.\n\nMiller (1995) George A Miller. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39–41, 1995.\n\nOnoe & Durrett (2020) Yasumasa Onoe and Greg Durrett. Fine-grained entity typing for domain independent entity linking. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 8576–8583, 2020.\n\nPutri et al. (2019) Rifki Afina Putri, Giwon Hong, and Sung-Hyon Myaeng. Aligning open ie relations and kb relations using a siamese network based on word embedding. In Proceedings of the 13th International Conference on Computational Semantics-Long Papers, pp. 142–153, 2019.\n\nRaffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1–67, 2020.\n\nRoth & Yih (2004) Dan Roth and Wen-tau Yih. A linear programming formulation for global inference in natural language tasks. In Proceedings of the eighth conference on computational natural language learning (CoNLL-2004) at HLT-NAACL 2004, pp. 1–8, 2004.\n\nSmirnova & Cudré-Mauroux (2018) Alisa Smirnova and Philippe Cudré-Mauroux. Relation extraction using distant supervision: A survey. ACM Computing Surveys (CSUR), 51(5):1–35, 2018.\n\nVashishth et al. (2018) Shikhar Vashishth, Prince Jain, and Partha Talukdar. Cesi: Canonicalizing open knowledge bases using embeddings and side information. In Proceedings of the 2018 World Wide Web Conference, pp. 1317–1327, 2018.\n\nVrandečić & Krötzsch (2014) Denny Vrandečić and Markus Krötzsch. Wikidata: a free collaborative knowledgebase. Communications of the ACM, 57(10):78–85, 2014.\n\nWadhwa et al. (2023) Somin Wadhwa, Silvio Amir, and Byron C Wallace. Revisiting relation extraction in the era of large language models. In Proceedings of the conference. Association for Computational Linguistics. Meeting, volume 2023, pp. 15566. NIH Public Access, 2023.\n\nWang et al. (2019) Hongwei Wang, Miao Zhao, Xing Xie, Wenjie Li, and Minyi Guo. Knowledge graph convolutional networks for recommender systems. In The world wide web conference, pp. 3307–3313, 2019.\n\nWang et al. (2023) Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. Improving text embeddings with large language models. arXiv preprint arXiv:2401.00368, 2023.\n\nWei et al. (2023) Xiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang, Xin Zhang, Shen Huang, Pengjun Xie, Jinan Xu, Yufeng Chen, Meishan Zhang, et al. Zero-shot information extraction via chatting with chatgpt. arXiv preprint arXiv:2302.10205, 2023.\n\nYasunaga et al. (2021) Michihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang, and Jure Leskovec. Qa-gnn: Reasoning with language models and knowledge graphs for question answering. arXiv preprint arXiv:2104.06378, 2021.\n\nYe et al. (2022) Hongbin Ye, Ningyu Zhang, Hui Chen, and Huajun Chen. Generative knowledge graph construction: A review. arXiv preprint arXiv:2210.12714, 2022.\n\nZeng et al. (2014) Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou, and Jun Zhao. Relation classification via convolutional deep neural network. In Proceedings of COLING 2014, the 25th international conference on computational linguistics: technical papers, pp. 2335–2344, 2014.\n\nZeng et al. (2015) Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao. Distant supervision for relation extraction via piecewise convolutional neural networks. In Proceedings of the 2015 conference on empirical methods in natural language processing, pp. 1753–1762, 2015.\n\nZhong et al. (2023) Lingfeng Zhong, Jia Wu, Qian Li, Hao Peng, and Xindong Wu. A comprehensive survey on automatic knowledge graph construction. ACM Computing Surveys, 56(4):1–62, 2023.\n\nZhou et al. (2022) Shaowen Zhou, Bowen Yu, Aixin Sun, Cheng Long, Jingyang Li, Haiyang Yu, Jian Sun, and Yongbin Li. A survey on neural open information extraction: Current status and future directions. arXiv preprint arXiv:2205.11725, 2022.\n\nŽukov-Gregorič et al. (2018) Andrej Žukov-Gregorič, Yoram Bachrach, and Sam Coope. Named entity recognition with parallel recurrent neural networks. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 69–74, 2018.\n\nAppendix A Schema Retriever Training\n\nWe follow the original training methodology detailed in the original paper, which involves utilizing pairs of text and their corresponding defined relations. For a given positive text-relation pair (t+,r+)superscript𝑡superscript𝑟(t^{+},r^{+})( italic_t start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT , italic_r start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT ), we employ an instruction template on t+superscript𝑡t^{+}italic_t start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT to generate a new text ti⁢n⁢s⁢t+subscriptsuperscript𝑡𝑖𝑛𝑠𝑡t^{+}_{inst}italic_t start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_n italic_s italic_t end_POSTSUBSCRIPT:\n\nti⁢n⁢s⁢t+=Instruct: retrieve relations that are present in the given text \\n Query: ⁢{t+}.subscriptsuperscript𝑡𝑖𝑛𝑠𝑡Instruct: retrieve relations that are present in the given text \\n Query: superscript𝑡\\displaystyle t^{+}_{inst}=\\textsf{Instruct: retrieve relations that are % present in the given text \\textbackslash n Query: }\\{t^{+}\\}.italic_t start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_n italic_s italic_t end_POSTSUBSCRIPT = Instruct: retrieve relations that are present in the given text \\n Query: { italic_t start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT } .\n\nWe then finetune the embedding model to distinguish between the correct relation associated with a given text and other non-relevant relations using the InfoNCE loss,\n\nmin⁡ℒ=−log⁡ϕ⁢(ti⁢n⁢s⁢t+,r+)ϕ⁢(ti⁢n⁢s⁢t+,r+)+∑ni∈ℕϕ⁢(ti⁢n⁢s⁢t+,ni)ℒitalic-ϕsubscriptsuperscript𝑡𝑖𝑛𝑠𝑡superscript𝑟italic-ϕsubscriptsuperscript𝑡𝑖𝑛𝑠𝑡superscript𝑟subscriptsubscript𝑛𝑖ℕitalic-ϕsubscriptsuperscript𝑡𝑖𝑛𝑠𝑡subscript𝑛𝑖\\displaystyle\\min\\mathcal{L}=-\\log\\frac{\\phi(t^{+}_{inst},r^{+})}{\\phi(t^{+}_{% inst},r^{+})+\\sum_{n_{i}\\in\\mathbb{N}}\\phi(t^{+}_{inst},n_{i})}roman_min caligraphic_L = - roman_log divide start_ARG italic_ϕ ( italic_t start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_n italic_s italic_t end_POSTSUBSCRIPT , italic_r start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT ) end_ARG start_ARG italic_ϕ ( italic_t start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_n italic_s italic_t end_POSTSUBSCRIPT , italic_r start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT ) + ∑ start_POSTSUBSCRIPT italic_n start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ blackboard_N end_POSTSUBSCRIPT italic_ϕ ( italic_t start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_n italic_s italic_t end_POSTSUBSCRIPT , italic_n start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG\n\nHere, ℕℕ\\mathbb{N}blackboard_N denotes the set of negative samples, and ϕitalic-ϕ\\phiitalic_ϕ represents the cosine similarity function. Please see the appendix for additional training details.\n\nFor training, we synthesized a dataset of text-relation pairs using the TEKGEN dataset (Agarwal et al., 2020), a large-scale text-triplets dataset created by aligning Wikidata triplets to Wikipedia text. The training dataset comprised 37,500 pairs, evenly divided between positive and negative samples. The performance of the fine-tuned schema retriever was assessed on the test splits of WebNLG, REBEL, and Wiki-NRE datasets. The recall@10 scores on these datasets were 0.823, 0.663, and 0.818, respectively, indicating the effectiveness of the retriever across different knowledge graph contexts.\n\nAppendix B Detailed results of Target Alignment\n\nB.1 Complete Results\n\nThe complete results of EDC and EDC+R on WebNLG, REBEL and Wiki-NRE are summarized in Table 3, Table 4 and Table 5 respectively. EDC performs better than or comparable to state-of-the-art baseline models in terms of all metrics (Precision, Recall, and F1) in all criteria (Partial, Strict, and Exact) and EDC+R is able to consistently improve upon this in all aspects as well. These results more comprehensively demonstrate the performance of EDC and EDC+R.\n\nB.2 Effect of more refinement iterations\n\nTable 6 shows the result of an extra iteration of refinement with EDC on all datasets. Although further refinement improves the results in a stable manner, we observe diminishing returns and hence, only report one iteration in the main results.\n\nB.3 Discussion on KGC dataset annotations\n\nAs stated in Section 4.2, we observe that EDC is penalized by the scorer on Rebel and Wiki-NRE datasets due to incomplete annotations. This echoes the previous finding in (Wadhwa et al., 2023; Han et al., 2023) that LLMs can often extract correct results that are missing in the annotations, which results in overly pessimistic evaluations. As shown by Table 7, EDC tends to extract significantly more triplets compared to the reference annotations and the baseline GenIE. Furthermore, as shown from the manual evaluation in Table 2, many of these triplets are indeed meaningful and correct with respect to the input text. Regardless, despite the automatic evaluation result on EDC being overly pessimistic, it still exceeds the baseline by a large margin and the actual performance may be even larger considering the difference in the number of triplets extracted."
    }
}
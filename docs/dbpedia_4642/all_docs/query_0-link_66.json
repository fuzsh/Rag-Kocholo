{
    "id": "dbpedia_4642_0",
    "rank": 66,
    "data": {
        "url": "https://www.biorxiv.org/content/10.1101/613257v1.full",
        "read_more_link": "",
        "language": "en",
        "title": "Automated analysis of whole brain vasculature using machine learning",
        "top_image": "https://www.biorxiv.org/sites/default/files/images/biorxiv_logo_homepage7-5-small.png",
        "meta_img": "https://www.biorxiv.org/sites/default/files/images/biorxiv_logo_homepage7-5-small.png",
        "images": [
            "https://www.biorxiv.org/sites/default/files/biorxiv_article.jpg",
            "https://www.biorxiv.org/sites/all/modules/contrib/panels_ajax_tab/images/loading.gif",
            "https://www.biorxiv.org/content/biorxiv/early/2019/04/18/613257/F1.medium.gif",
            "https://www.biorxiv.org/content/biorxiv/early/2019/04/18/613257/F1.medium.gif",
            "https://www.biorxiv.org/content/biorxiv/early/2019/04/18/613257/F2.medium.gif",
            "https://www.biorxiv.org/content/biorxiv/early/2019/04/18/613257/F2.medium.gif",
            "https://www.biorxiv.org/content/biorxiv/early/2019/04/18/613257/F3.medium.gif",
            "https://www.biorxiv.org/content/biorxiv/early/2019/04/18/613257/F3.medium.gif",
            "https://www.biorxiv.org/content/biorxiv/early/2019/04/18/613257/F4.medium.gif",
            "https://www.biorxiv.org/content/biorxiv/early/2019/04/18/613257/F4.medium.gif",
            "https://www.biorxiv.org/content/biorxiv/early/2019/04/18/613257/F5.medium.gif",
            "https://www.biorxiv.org/content/biorxiv/early/2019/04/18/613257/F5.medium.gif",
            "https://www.biorxiv.org/content/biorxiv/early/2019/04/18/613257/F6.medium.gif",
            "https://www.biorxiv.org/content/biorxiv/early/2019/04/18/613257/F6.medium.gif",
            "https://www.biorxiv.org/content/biorxiv/early/2019/04/18/613257/F7.medium.gif",
            "https://www.biorxiv.org/content/biorxiv/early/2019/04/18/613257/F7.medium.gif",
            "https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2019/04/18/613257/embed/graphic-8.gif",
            "https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2019/04/18/613257/embed/graphic-8.gif",
            "https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2019/04/18/613257/embed/graphic-9.gif",
            "https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2019/04/18/613257/embed/graphic-9.gif",
            "https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2019/04/18/613257/embed/graphic-10.gif",
            "https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2019/04/18/613257/embed/graphic-10.gif",
            "https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2019/04/18/613257/embed/graphic-11.gif",
            "https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2019/04/18/613257/embed/graphic-11.gif",
            "https://www.biorxiv.org/sites/all/modules/highwire/highwire/images/twitter.png",
            "https://www.biorxiv.org/sites/all/modules/highwire/highwire/images/fb-blue.png",
            "https://www.biorxiv.org/sites/all/modules/highwire/highwire/images/linkedin-32px.png",
            "https://www.biorxiv.org/sites/all/modules/highwire/highwire/images/mendeley.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Mihail Ivilinov Todorov",
            "Johannes C. Paetzold",
            "Oliver Schoppe",
            "Giles Tetteh",
            "Velizar Efremov",
            "Katalin Völgyi",
            "Marco Düring",
            "Martin Dichgans",
            "Marie Piraud",
            "Bjoern Menze"
        ],
        "publish_date": "2019-04-18T00:00:00",
        "summary": "",
        "meta_description": "bioRxiv - the preprint server for biology, operated by Cold Spring Harbor Laboratory, a research and educational institution",
        "meta_lang": "en",
        "meta_favicon": "https://www.biorxiv.org/sites/default/files/images/favicon.ico",
        "meta_site_name": "bioRxiv",
        "canonical_link": "https://www.biorxiv.org/content/10.1101/613257v1",
        "text": "SUMMARY\n\nTissue clearing methods enable imaging of intact biological specimens without sectioning. However, reliable and scalable analysis of such large imaging data in 3D remains a challenge. Towards this goal, we developed a deep learning-based framework to quantify and analyze the brain vasculature, named Vessel Segmentation & Analysis Pipeline (VesSAP). Our pipeline uses a fully convolutional network with a transfer learning approach for segmentation. We systematically analyzed vascular features of the whole brains including their length, bifurcation points and radius at the micrometer scale by registering them to the Allen mouse brain atlas. We reported the first evidence of secondary intracranial collateral vascularization in CD1-Elite mice and found reduced vascularization in the brainstem as compared to the cerebrum. VesSAP thus enables unbiased and scalable quantifications for the angioarchitecture of the cleared intact mouse brain and yields new biological insights related to the vascular brain function.\n\nINTRODUCTION\n\nChanges in the brain vasculature are a key feature of a large number of diseases effecting the brain. Primary angiopathies, vascular risk factors (e.g., diabetes), traumatic brain injury, vascular occlusion and stroke all affect the brain vascular network and interfere with normal microcirculation and vascular function1-5. Alterations of the brain microvasculature are also seen in neurodegenerative diseases, such as Alzheimer’s disease, tauopathy and amyloidopathy. These hallmarks of the Alzheimer’s disease, can lead to aberrant remodeling of the blood vessels1,6-8. Consequently, capillary rarefaction is frequently used as a marker for vascular damages9. Thus, quantitative analysis of the entire brain vasculature including the capillary bed is pivotal to develop a better understanding of physiological and pathological brain function. However, quantifying micrometer scale changes in the cerebrovascular network of intact brains has been difficult for two main reasons.\n\nFirst, labeling and imaging of the complete mouse brain vasculature down to the smallest blood vessels has to be achieved. Magnetic resonance imaging (MRI), for instance, does not have sufficient resolution to capture capillaries10. MicroCT imaging can visualize the microvasculature, but because of specimen size constraints it fails to acquire a whole intact mouse brain11. Fluorescent microscopy, on the other hand, provides a higher resolution but can typically be applied to 1-200 μm thin tissue slices, which does not pre-serve the structure of an end-to-end vascular network. Recent advances in tissue clearing could overcome this problem, but so far there has been no demonstration of all vessels of all sizes in an entire brain in three dimensions (3D)12.\n\nThe second challenge relates to automated analysis of 3D imaging data for structures that are spanning entire mouse brains, which cannot be analyzed piece by piece in a reliable and scalable manner. Scanning transparent specimens of several millimeters size at micrometer resolution, inevitably introduces substantial variance in the signal intensity and signal-to-noise ratio at different depths. Thresholding methods are not capable of segmenting these large scans, whereas shape-based filtering approaches such as Frangi filters cannot reliably identify vessels from background13,14. To overcome these limitations more advanced image processing methods with local spatial regularization have been proposed for processing light-sheet scans15. However, such methods including local spatial regularization cannot segment large vascular networks across changing intensity distributions. Finally, the size of the acquired datasets poses a difficulty to assess the organization of the whole vascular network; therefore, such methods can only segment small volumes15-19.\n\nHere, we present VesSAP (Vessel Segmentation & Analysis Pipeline), a method for automated quantitative analysis of the entire mouse brain vasculature, which overcomes the limitations stated above. To achieve this, we first developed a dual vascular staining approach using wheat germ agglutinin (WGA) and Evans blue (EB) to stain both small and large vessels in two fluorescent channels, consistently throughout the entire brain. Next, we cleared whole stained brains using the 3DISCO method20 and imaged them with light-sheet microscopy at micrometer resolution. Second, we developed a deep fully convolutional network (FCN), which exploits the imaging data from both dyes to provide a high-quality segmentation of the vasculature in 3D. Subsequent feature extraction and registration to the latest Allen adult mouse brain atlas enabled us to quantify all features of interest with respect to their topographical location. Our deep learning-based approach works reliably despite variations in signal intensities, outperforming previous filter-based methods and reaching the quality of segmentations of human annotators. To our knowledge, this is the first time that a deep learning approach is being used to analyze complex imaging data of cleared mouse brains i.e. spanning the entire brain end-to-end.\n\nWe further applied VesSAP to a set of 6 mice from two commonly used mouse strains to systematically explore strain-related differences in vascular anatomy across brain regions as described by the Allen brain atlas. We reported new biological findings and provide a comprehensive reference set of vessel anatomy features, revealing unique structures of different brain regions. Thus, VesSAP represents an integrated pipeline enabling automated and scalable analysis of the complete mouse brain vasculature (Fig. 1). All parts of the VesSAP are publicly hosted online for easy adoption, including the imaging protocol, the data (original scans, registered atlas data), the trained algorithms, training data and a reference set of features describing the vascular network in all brain regions at the following address: http://DISCOtechnologies.org/VesSAP\n\nRESULTS\n\nTissue clearing methods enable imaging of un-sectioned biological specimens. To extract biologically meaningful data, they have to be combined with reliable and automated image analysis methods. Towards this goal, we developed Ves-SAP, a deep learning-based method to accurately and automatically analyze the vasculature of cleared mouse brains. VesSAP encompasses 3 major steps: 1) staining, clearing and imaging of the mouse brain vasculature by two different dyes (WGA and EB) down to the capillary level, 2) transfer learning-based algorithms to automatically segment and trace the whole brain vasculature data at the capillary level and 3) extraction of vascular features for hundreds of brain regions by registering the data to the Allen brain atlas (Fig. 1). We applied VesSAP to generate vascular reference maps for two commonly used mouse strains under physiological conditions: C57BL/6J and CD1-Elite mice. We report the first evidence of secondary intracranial collateral vascularization in CD1-Elite mice. Furthermore, our work shows a significantly decreased vascularization density in the brainstem as compared to the cerebrum in both mouse strains.\n\nStep 1: Vascular staining, DISCO clearing, and imaging\n\nTowards staining the entire vasculature, we applied a combination of two dyes, WGA and EB staining in two fluorescent channels. We then performed 3DISCO clearing21 and light-sheet microscopy imaging of whole mouse brains at micrometer resolution (Fig. 2A-C, Supporting Fig. 1). WGA predominantly stains small vessels and, importantly, captures even the smallest ca-pillaries down to diameters of a few micrometers, while EB predominantly stains large vessels including the middle cerebral artery and the circle of Willis (Fig. 2D-I, Supporting Fig. 2). Merging the signals from both dyes yields a staining of the complete vasculature, showing the complementary nature of both dyes (Fig. 2C,F). Importantly, the signals from both dyes are perfectly congruent when staining the same vessel and solely come from the vessel wall layer (Fig. 2G-I, Supporting Fig. 2). Furthermore, owing to the dual labeling, we maximized the signal to noise ratio (SNR) for each dye independently to avoid saturation of differently sized vessels when only a single channel is used. We achieved this by independently optimizing the excitation and emission power. For WGA, we reached a higher SNR for small capillaries; bigger vessels, however, were barely visible (Supporting Fig. 3). For EB, the SNR for small capillaries was substantially lower but larger vessels reached a high SNR (Supporting Fig. 3). Thus, integrating the infor-mation from both channels allows homogenous staining of the entire vasculature throughout the whole brain, and results in a high SNR for high-quality segmentations and analysis.\n\nStep 2: Segmentation of the volumetric images\n\nTo enable extraction of quantitative features of the vascular structure, the vessels in the acquired brain scans need to be segmented in 3D. Motivated by the recent success of deep learning based approaches in biomedical image data analysis22-32, including human MRI data segmentation, we developed a deep FCN to exploit the complementary signals of both dyes to derive a complete segmentation of the entire vasculature.\n\nOur network architecture is a deep FCN, which segments vessels based on the input from two imaging channels. The network consists of 5 layers, 4 convolutional layers followed by one fully connected layer. In a first step, the two input channels (WGA and EB) are concatenated. This yields a set in which each voxel in 3D space is characterized by two features. Each convolutional step integrates the information from the voxel’s 3D neighborhood. Here we use cross-hair shaped convolutional kernels to sample the local surrounding of each voxel in a sparse manner along three orthogonal planes25 (Fig. 3A). After the last convolution, the information from 50 features per voxel is combined with a fully connected layer and a sigmoidal activation to estimate the likelihood that a given voxel represents a vessel. Subsequent binarization yields the final segmentation. In both, training and testing, the images are processed on sub-patches of 50 × 100 × 100 pixels.\n\nOften, deep neural networks require large amount of annotated data to be trained. Here, we circumvented this requirement with a transfer learning approach, which is frequently used to train algorithms with limited annotated data33. In short, the network was first pre-trained on a large, synthetically generated vessel-like data set (Supporting Fig. 4)34 and then refined on a small amount of manually annotated part of the real brain vessel scans. This approach drastically reduced the need for manually annotated training data, as indicated by the following observations: first, the network that was solely trained on syn-thetic data already yields acceptable segmentations of real brain vessel scans. Second, the refinement on the real data set already converged after a few training epochs. Third, only a very small amount of manually annotated training data (here: 0.02 % of a full brain scan) was needed to segment the vasculature of an entire brain with the quality of human annotations.\n\nTo assess the quality of the segmentation quantitatively, we compared the network prediction with manually created ground truth segmentation and ran several experiments with alternative approaches (Fig. 3B, Supporting Fig. 5). To pro-vide comparability with the literature, we reported two voxel-wise measures that quantify the quality of the segmentation, accuracy and the F1-score35. As compared with the ground truth, our network exploiting the information from WGA and EB achieved an accuracy of 0.93 ± 0.01 and a F1-Score 0.76 ± 0.01 (Fig. 3B). To assess the importance of integrating information from both, WGA and EB, we designed a control network that only has access to the commonly used WGA signal, which reduced the quality of the vessel segmentation (accuracy: 0.90 ± 0.08; F1-Score 0.74 ± 0.07). As further controls, we implemented alternative state-of-the-art methods and found that our network outperforms classical Frangi filters13 (accuracy: 0.85 ± 0.03; F1-Score 0.47 ± 0.18), as well as recent methods considering local spatial context via Markov random fields15,36 (accuracy: 0.85 ± 0.03; F1-Score 0.48 ± 0.04).\n\nNext, we compared the segmentation accuracy of our network with human annotations. A total of 4 human experts independently annotated two volumes each and these segmentations were again compared to the ground-truth segmentation. We found that the accuracy of segmentation quality of independent human annotators was comparable to the predicted segmentation of our network (Fig. 3c). Moreover, our network could segment a whole mouse brain scan within 24 hours on a single GPU workstation whereas the annotators who created our segmentations would need more than a year to process a whole brain. In summary, the VesSAP pipeline is able to segment the whole brains vasculature at human level accuracy with a substantially higher speed.\n\nFig. 3D and Supporting Video 1,2 show an example of a brain vasculature that was segmented by VesSAP in 3D. Zooming into a smaller patch reveals that the connectivity of the vascular network was fully maintained (Fig. 3E, Supporting Video 1). Comparing single slices of the imaging data with the predicted segmentation shows that vessels are accurately segmented regardless of absolute illumination and vessel diameter (Supporting Fig. 6).\n\nStep 3: Feature extraction and atlas registration\n\nIt has previously been shown that vessel density, radius and the number of bifurcation points can be used to describe vascular anatomy3. Hence, we used our segmentation to trace and quantify these features as the distinct parameters to characterize the mouse brain vasculature (Fig. 4A, Supporting Video 3). A visualization of the quantified vessel radius along the entire vascular network is shown in Fig. 4B. After extracting vascular features of the whole brain with Ves-SAP, we registered the volume to the Allen brain atlas (Supporting Video 4,5). This allowed us to map the segmented vasculature and corresponding features to distinct anatomical brain regions. A representative cross-section of the brain through the vasculature, color-coded by coarse anatomical regions, is depicted in Fig. 4C. Each anatomical region can be further divided into subregions, yielding a total of 1238 anatomical structures (619 per hemisphere) for the entire brain (Fig. 4D). This allows analyzing each denoted brain region and grouping regions into clusters such as left vs. right hemisphere, gray vs. white matter or hierarchical clusters of the Allen brain atlas ontology. For our subsequent statistical feature analysis, we chose to group the labeled structures according to the main 55 anatomical clusters of the current Allen brain atlas ontology. We thus provide the first whole mouse brain vas-cular map with the extracted centerlines, bifurcation points and radius down to the capillary level.\n\nVesSAP provides a reference map of the whole brain vasculature in mice\n\nTo extend utility of our new method, we next derived three secondary features from the segmentation to build a reference map of the brain vas-culature: 1) To approximate the total length of all vessels in a given volume of interest, we extract the centerlines and count the voxels along it (expressed in voxel / voxel ratio); 2) density of bifurcation points normalized to the region size (expressed in count / mm3); and 3) average radius for each region (expressed in μm). These features were all referenced to the Allen brain atlas ontology. We used these features to determine the vascular features in 6 individual brain samples from the C57BL/6J and CD1-Elite strains (n = 3 mice for each strain). From these quantifications we derived the following conclusions: first, the vessel radius is evenly distributed in different regions of the same brain (Fig. 5A). Second, the bifurcation density and vessel length are unevenly distributed in the same brain over different regions, while they correlate well among different mice for the same regions (Fig. 5B,C). Moreover, bifurcation density correlates well with the vessel length ratio in most of the brain regions (Fig. 5D, Pearson’s correlation, r=0.956, p-value=1.971-30). Third, we observed that the extracted features show no significant statistical difference for the same anatomical cluster between the two strains (C57BL/6J and CD1-Elite) (Supporting Fig. 7, Supporting Tables 1-4).\n\nNext, we visually inspected exemplary regions and validated the output of VesSAP. For example, the Gustatory areas showed higher vascular density compared to the Anterodorsal nucleus (Fig. 6A) as predicted by VesSAP (Fig. 6B,C). This inspection also suggested that the capillary density was the primary reason for the regional variations in the same brain. Additionally we found 1) direct intracranial vascular anastomosis in both strains (white arrowheads, Fig. 6D,E), and 2) that the anterior cerebral artery, middle cerebral artery and the posterior cerebral artery are connected at the dorsal visual cortex (red arrowheads, Fig. 6D,E).\n\nImportantly, our findings on vasculature length are in line with the predictions in the literature obtained in small volumes, confirming the robustness of our method. For example, previous studies quantifying small patches estimated the density of cortical blood vessels as 0.922 m/mm3, 0.444 m/mm3 or 0.471 m/mm3 in the cortex12,17,37. Using VesSAP, calculating centerline density as described above, and accounting for the 30 % isotropic tissue shrinkage in DISCO clearing38, we found an average vascular density of 0.473 ± 0.161 m/mm3 over the whole mouse cortex.\n\nDISCUSSION\n\nResearchers have extensively worked towards examining the cerebral vasculature at the complete brain scale. This is particularly relevant, because current theories for many cardiovascular, neurodegenerative and metabolic disease pathologies include the capillaries, which can reflect the earliest symptoms. Thus, a method to robustly image, segment and analyze the complete cerebral vasculature of the mouse brain has been much needed. Previous studies were restricted: either they did not have sufficient resolutions to visualize all vessels including tiny capillaries such as MRI and microCT imaging modalities39-41, or once capillaries are imaged by high resolution fluorescent microscopy, the analysis could solely be done on small volumes12. Here we present VesSAP, a framework for unbiased statistical investigation of the complete vascular network in the intact adult mouse brain at the capillary level. We extracted the centerlines, bifurcation points and radius and assign them topographically to the Allen brain atlas to generate a reference map of the adult mouse brain vasculature. These maps can potentially be used to model synthetic cerebrovascular networks42,43. More advanced metrics to describe the vasculature and networks, for example global Strahler values, network connectivity and local statistics on bifurcation angles and vascular shape can be extracted using our method. Furthermore, the centerlines and bifurcation points can be interpreted as the edges and nodes for building a full vascular network graph, offering unprecedented means for studying local and global properties of the cerebrovascular network in the future.\n\nSeveral methods have been proposed to label the cerebral vasculature of the mouse CNS based on one dye. Here, we employed two different dyes for complementary staining of the blood vessels, which are based on different mechanisms. The WGA binds to the glycocalyx of the endothelial lining of the blood vessels, but it can miss some segments of the large vessels. To circumvent this, we injected the EB dye into the mice 12 hours before WGA perfusion in vivo, allowing its long-term circulation to mark large vessels under physiological conditions. The combination of these dyes in this study enabled a wide dynamic contrast. This strategy has been proven quite beneficial for the segmentation of the complete cerebral vasculature of C57BL/6J and CD1-E strains as we showed here.\n\nOur FCN segmentation architecture outperforms the current state-of-the-art methods significantly (Fig. 3B). Importantly, VesSAP is one to two orders of magnitude faster than the state-of-the-art automation which has a far lower accuracy, and more than 350 times faster than a human annotator. Our inter-annotator experiment validates that our model reaches human level performance (Fig. 3C). However, from a medical imaging and machine learning perspective the scores, especially dice, precision and recall are low compared to some other segmentation tasks in medical imaging44-46.\n\nWe attribute this mainly to the nature of a vascular network. Vessels are long but thin tubular shapes. In our images the radius of the capillaries (about 3 µm) is in the range of our voxel resolution, therefore, a segmentation of the correct thickness down to a single pixel is difficult. This inconsistency with the label does not pose a major problem for our main task to segment the whole vasculature and extract features; however it introduces a significant reduction to the F1Score, which is frequently used metric in machine learning tasks. The goal here is to segment a complete vasculature of the brain, to enable us to extract vascular features such as centerlines and bifurcation points. Therefore, introduction of a new metric in the future would be needed for this type of data. This metric should weight the correct detection of the vessels more and allow the outer wall of a blood vessel to be in a certain range of distance from the centerline of that vessel. The inter-annotator experiment and the comparison to our segmentation yielded insight in the quality of the segmentation. The human annotator segmentation accuracy and F1-Score were not superior compared to our model and the an-notators disagreed substantially emphasizing the strength of our segmentation.\n\nThe proposed segmentation concept is based on a transfer learning approach, where we trained our data on a synthetic dataset and only refined it on a real labeled dataset 1.7 % of the size of the synthetic dataset. We consider this a major advantage compared to approaches that train from scratch. The model should generalize very well to other datasets, where other scientist would only need a small labeled dataset to achieve good segmentation performance. The pertinence of our approach should go well beyond vessel datasets, and could find applications for many other imaging tasks, for example tracing neurons.\n\nBased on our vascular reference map new properties can be discovered and biological models can be confirmed faster. Here, we found that a substantial difference in vascularization and bifurcation density exists across the regions of the Allen brain atlas. Furthermore, we found that intracranial anastomoses between the anterior cerebral artery, middle cerebral artery and the posterior cerebral artery which are known from C57BL/6J35,47, are also present in albino CD1-Elite strain. This is opposed to the BALB/c albino mouse where the absence of collaterals has been described35. To our best knowledge this is the first evidence for high collateral density in albino CD1-Elite mice. This finding is important because existence of such collateral vessels between large vessels can significantly alter the outcome of the ischemic stroke lesion as the blood deprived brain regions from the occlusion of a large vessel could be compensated by the blood supplies coming from the collateral extensions from other large vessels35,48,49. Thus, our VesSAP method enables unbiased quantification of vascular anatomy in intact mouse brains and can lead to the discovery of previously over-looked anatomical knowledge.\n\nIn conclusion, VesSAP is the first scalable and automated machine learning-based method to analyze complex imaging data coming from the cleared intact mouse brains. It outperforms all previous methods of vessel segmentation and achieves a human level of accuracy several orders of magnitude faster. Thus, we foresee that our new method will accelerate the applications of tissue clearing in particular for the studies assessing the brain vasculature.\n\nMETHODS\n\nTissue preparation\n\nThe animals were housed under a 12/12 hr light/dark cycle. The animal experiments were conducted according to institutional guidelines (Klinikum der Universität München/Ludwig Maximilian University of Munich), after approval of the ethical review board of the government of Upper Bavaria (Regierung von Oberbayern, Munich, Germany), and in accordance with the European directive 2010/63/EU for animal research. For this study we injected 150 μl (2% V/V% in saline) of Evans blue dye (Sigma-Aldrich, E2129) intra-peritoneally into three C57BL/6J and CD1-Elite male, 3 months old mice (n=3 per group). After 12 hrs of postinjection time, we anaesthetized the animals with a triple combination of MMF (i.p.; 1 ml per 100 g body mass for mice) and opened their chest for transcardial perfusion. The following media was supplied by a peristaltic pump set to deliver 8 ml/min volume: 150 μl wheat germ agglutinin conjugated to Alexa 594 dye (ThermoFisher Scientific, W11262) and 15 ml PBS 1x and 15 ml 4% PFA.\n\nAfter perfusion, the brains were extracted and incubated into 3DISCO clearing solutions as described by Ertürk et al.21. Briefly, we immersed them in a gradient of tetra-hydrofuran (Sigma-Aldrich, 186562): 50 vol%, 70 vol%, 80 vol%, 90 vol%, 100 vol% (in distilled water), and 100 vol% at 25 °C for 12 h each step. At this point we modified the protocol to incubate the samples in tert-Butanol incubation for 12 hrs at 35 °C followed by immersion in dichloromethane (Sigma-Aldrich, 270997) for 12 hrs at room temperature and finally incubation with the refractive index matching solution BABB (benzyl alcohol + benzyl benzoate 1:2; Sigma-Aldrich, 24122 and W213802), for at least 24 hrs at room temperature until transparency was achieved. Each incubation step was carried out on a laboratory shaker.\n\nImaging of the cleared samples\n\nWe captured the optical section images with a 4× objective lens (Olympus XLFLUOR 340) equipped with an immersion corrected dipping cap mounted on a LaVision UltraII microscope. For 20× imaging, we used Zeiss CLARITY objective (Clr Plan-Neofluar, NA 1.0). The images were taken in 16 bit precision, which results in a resolution of 1.625 μm on the XY axes. The brain structures were visualized by the Alexa 594 and Evans blue fluorescent dyes at 561 and 640 nm excitation respectively. In z-dimension we took the sectional images in 3 μm steps from the right and left sides. To reduce defocus, which derives from the Gaussian shape of the beam we used a 12 step sequential shifting of the focal position of the light sheet per plane and side. The thinnest point of the light sheet was 5 μm.\n\nReconstruction of the whole brain datasets from the tiling volumes\n\nWe stitched the acquired volumes using TeraStitcher’s automatic global optimization function (v1.10.3). We produced volumetric intensity images of the whole brain considering each channel separately. Next, we generated isotropic datasets because the registration and successive processing steps were more robust on isotropic datasets, therefore we downsampled the recon-structed 3D vascular datasets in XY dimensions to 3 × 3 × 3 μm resolution.\n\nDeep learning network architecture\n\nHere we introduced a deep 3D fully convolutional network (FCN) for segmentation of our blood vessel dataset. The networks general architecture consists of 4 convolutional layers followed by a sigmoid activation layer, see Fig. 3A. Generally, the input layer is designed to take n images as an input. In our implemented case, the input to the first layer of the network are n=2 images of the same brain, which have been stained differently, see Fig. 3A. To specifically account for the general class imbalance (much more tissue background than vessels) in our dataset, and the high false positive rates associated with the class imbalance, the following class balancing loss function with stable weights from Tetteh et al. is implemented, see Equation II.1. Here, L1 is a numerically stable class balancing loss function and the term L2 penalizes the network for false predictions. Y+ and Y- represent the foreground and background classes respectively, P(yj=k| X;W) is the probability that the voxel j in volume X belongs to class k given the volume X and network weights W. Yf+ and Yf- represent the false positive and false positive predictions re-spectively at each training iteration.\n\nThe 3D convolutional operations in this network are implemented as sparse crosshair filters to reduce memory consumption and speed up the computation, for a graphical representation see Supporting Fig. 4. Tetteh et al. showed that by using this operation a faster computation is achieved without undermining the prediction accuracy25. The crosshair filter works by separating a full 3D kernel into 3 orthogonal 2D kernels. Those kernels are applied to the volume at every layer of the network.\n\nThe networks training is driven by a stochastic gradient descent function without a regularization. A prediction or segmentation with a trained model takes a volumetric image of arbitrary size and outputs an estimated probabilistic segmentation of the input images size. The algorithms have been implemented using the THEANO framework50. They are trained and tested on a NVIDIA Quadro P5000 GPU and on machines with 64GB and 500GB RAM respectively.\n\nTransfer learning\n\nTypically, medical imaging tasks are aggravated by scarce and very scarce data availability. The proposed transfer learning approach, aims to account for the scarce labeled data by pre-training our models on a synthetic dataset and refining them on a small training set of interest51. Our approach pre-trains a two channel version of DeepVesselNet on a synthetically generated dataset52,53, with the goal to learn specific vascular shaped image patterns. The pre-training is carried out on a dataset of 136 volumes of a size of 325 × 304 × 600 pixels. While pre-training we applied a learning rate of 0.01 and a decay of 0.99 which we applied after every 200 iterations. Finally the pre-trained model is fine-tuned by re-training on a real microscopic dataset consisting of eleven volumes with a size of 500 × 500 × 50 pixels, which were manually annotated by the expert who imaged the data and further verified by two additional experts. All volumes are processed in smaller sub-patches by our network. This enables us to process volumes of arbitrary sizes and dimensions. The data we use in the fine-tuning step accumulates to 1.7% of the synthetic datasets voxel volume and solely 0.02% of the voxel volume of a single brain image. For the fine-tuning step we utilized a learning rate of 0.0001 and a decay of 0.98, which we applied after every 10 iterations.\n\nOur training set consist of eleven volumetric images from two mice brains, the test and validation set consists of four patches from two different brains. Each patch consists of a volume of 500 × 500 × 50 pixels. We chose independent brains to guarantee generalizability. The patches are processed and predicted in 25 small sub-patches. We cross-test on our test and validation set by rotating these four-fold. In every rotation our validation set consists of 3 patches and our test set of one patch. To prevent an overfitting of our model we chose the validation and test set from two brains. One from the CD1-E and one from the C57BL/6J strain. We choose the lowest log loss on our validation set to be our model selection point (see Supporting Fig. 4a). We report an average F1-Score of 0.76 ± 0.01, an average accuracy of 0.93 ± 0.01, an average precision of 0.79 ± 0.02 and an average recall of 0.73 ± 0.02 on our test sets. All scores are given with a 1σ standard deviation. On average our model reached the model selection point after 45 epochs of training.\n\nPre-processing of segmentation\n\nThe pre-processing represents a significant factor for the overall success of the training and segmentation. The intensity distribution among the brains and among brain regions differs sub-stantially. To account for the intensity distributions, two preprocessing strategies have been applied successively.\n\nHigh-cut filter: In this step the intensities x above a certain threshold, c which is defined by an individual percentile for each volume is set to that threshold. Next, they were normalized by f(x).\n\nNormalization of intensities: The original intensities were normalized to the range of 0 to 1, where x is the pixel intensity and X are all intensities of the volume.\n\nInter-annotator experiment\n\nTo compare VesSAP’s segmentation to a human level annotation we implemented an inter-annotator experiment. For this experiment we determined a gold standard label for two patches of 500 × 500 × 500 pixels from a commission of three experts, including the expert who imaged our data and is therefore most familiar with the images. Next, we gave the two patches to 4 other experts to label the complete vasculature. The experts spend multiple hours to label each patch within the ImageJ and ITK-snap environment and were allowed to use their favored approaches to generate their best label. Finally, we calculated the accuracy and dice scores for the different raters, compared to the gold standard and compared them to the scores of our model.\n\nFeature extraction\n\nIn order to quantify the anatomy of the mouse brain vasculature we extracted descriptive features based on our segmentation. Later we registered them to the Allen brain atlas.\n\nAs features we extracted the centerlines, the bifurcation points and the radius of the segmented blood vessels. We consider those features to be independent from the elongation of the light sheet scans and the connectedness of the vessels due to staining, imaging and/or segmentation artefacts. We found the extracted features as a baseline.\n\nBefore extracting the centerlines we applied two cycles of binary erosion and dilation to remove false negative pixels within the volume of segmented vessels as those would induce false centerlines. Our centerline extraction is based on a 3D thinning algorithm as introduced by Lee et al.54. Based on the centerlines we extracted bifurcation points. A bifurcation is the branching point on a centerline where a larger vessel splits into two or more small vessels (see Fig. 4A). In a network analysis context they are significant as they represent the nodes of a vascular network55. Furthermore, bifurcation points have significance in a biological context. In neurodegenerative diseases, capillaries are known to degenerate56, thereby significantly reducing the number of bifurcation points in an affected brain region compared to a healthy brain. To detect the bifurcation points an algorithm was implemented. The algorithm takes the centerlines as an input and calculates for every point on that centerline the surrounding centerline pixels to determine if a point is a centerline. The radius of a blood vessel is a key feature to describe vascular networks. The radius yields information about the flow and hierarchy of the vessel network55. The proposed algorithm calculates the Euclidean distance trans-form for every segmented pixel v to the closest background pixel bclosest (Equation II.2). Next, the distance transform matrix is multiplied with the 3D centerline mask equaling the minimum radius of the vessel around the centerline.\n\nRegistration to the reference atlas\n\nWe used the average template, the annotation file and the latest ontology file (Ontology ID: 1) of the current Allen brain mouse atlas CCFv3 201710. Then we scaled the template and the annotation file up from 10 to 3 µm3 to match our reconstructed brain scans. After this we multiplied the left side of the (still symmetrical) annotation file with −1 so that the labels can be later assigned to the corresponding hemispheres. Next, the average template and the 3D vascular datasets were downsampled to 10% of their original size in each dimension to achieve a reasonably fast alignment. In the sake of the integrity of the extracted features, we aligned the template to each of the brain scans individually using a two-step rigid and deformable (B-Spline) registration and applied the transformation parameters to the full resolution annotation volume in 3 × 3 × 3 μm resolution. Subsequently we created masks for the anatomical clusters based on the current Allen brain atlas ontology.\n\nStatistics\n\nData collection and analysis were not performed blind to the strains. Data distribution was assumed to be normal, but this was not formally tested. All data values are given as mean ± SEM. Data were analyzed with standardized effect size indices (Cohen’s d)57 to investigate differences of vessel density, number of bifurcation points and radii between brain areas across the two mouse strains (n=3 per strain) and comparisons across brain areas in the pooled (n=6) dataset. Statistical analysis was performed using MATLAB.\n\nData visualization\n\nAll volumetric datasets were rendered using Imaris, Arivis and ITK Snap.\n\nCODE AND DATA AVAILABILITY\n\nVesSAP codes and data that we produced are publicly hosted online for easy adoption, including the imaging protocol, the data (original scans, registered atlas data), the trained algorithms, training data and a reference set of features describing the vascular network in all brain regions at the following address. Implementation of external libraries are available on request. http://DISCOtechnologies.org/VesSAP\n\nAUTHOR CONTRIBUTIONS\n\nM.I.T. performed the tissue processing, clearing and imaging experiments. M.I.T and K.V. developed the whole brain staining protocol. M.I.T. stitched and assembled the whole brain scans. V.E. generated the synthetic vascular training dataset. J.C.P, G.T. and O.S developed the deep learning architecture, trained the models and performed the quantitative analyses. M.I.T. annotated the data. M.D. and M.D. helped with data interpretation. B.M, M.P. and G.T. provided guidance in developing the deep learning architecture and helped with data interpretation. A.E., M.I.T. and J.C.P. wrote the manuscript. All the authors edited the manuscript. A.E. initiated and led all aspects of the project.\n\nCONFLICT OF INTEREST STATEMENT\n\nThe authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.\n\nVIDEO LEGENDS\n\nSupporting Video 1\n\nVisualization of a representative CD1-E mouse brain by VesSAP showing the data quality.\n\nSupporting Video 2 (VR optimized viewing)\n\nThe whole mouse brain shown in Video 1 has been rendered for virtual reality (VR) viewing using Arivis InViewR. The immersed VR view shows the quality of VesSAP segmentation. We propose that scientific VR videos coming from large cleared samples could be a helpful tool for scientists to explore the data in a 3D interactive way. VR videos might also be used for educational purposes as they can be viewed on smart phones and other available VR devices. Please check the link for more information regarding how to view this VR video: http://DISCOtechnologies.org/VesSAP/#VR\n\nSupporting Video 3\n\nSegmentation and features demonstration on a subset of the whole dataset. VesSAP enables reliable segmentation (red) and feature extraction (bifurcation points and centerlines, green and cyan) down to the capillary-level from the imaging data (grey).\n\nSupporting Video 4\n\nWhole brain data registered to the Allen adult brain atlas. The video shows the alignment accuracy and segmentation overlaid.\n\nSupporting Video 5\n\nSubstack of the whole brain data registered to the Allen adult brain atlas. This video reveals the full resolution segmentation on a small set of the brain scans data.\n\nACKNOWLEDGMENTS\n\nThis work was supported by the Vascular Dementia Research Foundation, Synergy Excellence Cluster Munich (SyNergy), ERA-Net Neuron (01EW1501A to A.E.), Fritz Thyssen Stiftung (A.E., Ref. 10.17.1.019MN), DFG (A.E., Ref. ER 810/2-1), NIH (A.E.), Helmholtz ICEMED Alliance (A.E.), and the German Federal Ministry of Edu-cation and Research via the Software Campus initiative (O.S.). Furthermore, NVIDIA supported this work with a Titan XP via the GPU Grant Program. M.I.T is member of Graduate School of Systemic Neurosciences (GSN), Ludwig Maximilian University of Munich.\n\nFootnotes\n\nREFERENCES\n\n1.↵\n\nBennett, R. E. et al. Tau induces blood vessel abnormalities and angiogenesis-related gene expression in P301L transgenic mice and human Alzheimer’s disease. Proc Natl Acad Sci U S A 115, E1289–E1298, doi:10.1073/pnas.1710329115 ( ).\n\n2.\n\nJoutel, A. et al. Cerebrovascular dysfunction and microcirculation rarefaction precede white matter lesions in a mouse genetic model of cerebral ischemic small vessel disease. J Clin Invest 120, 433–445, doi:10.1172/JCI39733 ( ).\n\n3.↵\n\nObenaus, A. et al. Traumatic brain injury results in acute rarefication of the vascular network. Sci Rep 7, 239, doi:10.1038/s41598-017-00161-4 ( ).\n\n4.\n\nLi, W. et al. Adaptive cerebral neovascularization in a model of type 2 diabetes: relevance to focal cerebral ischemia. Diabetes 59, 228–235 ( ).\n\n5.↵\n\nVölgyi, K. et al. Chronic Cerebral Hypoperfusion Induced Synaptic Proteome Changes in the rat Cerebral Cortex. Molecular Neurobiology 55, 4253– 4266, doi:10.1007/s12035-017-0641-0 ( ).\n\n6.↵\n\nKlohs, J. et al. Contrast-enhanced magnetic resonance microangiography reveals remodeling of the cerebral microvasculature in transgenic ArcAbeta mice. J Neurosci 32, 1705–1713, doi:10.1523/JNEUROSCI.5626-11.2012 ( ).\n\n7.\n\nHunter, J. M. et al. Morphological and pathological evolution of the brain microcirculation in aging and Alzheimer’s disease. PloS one 7, e36893, doi:10.1371/journal.pone.0036893 ( ).\n\n8.↵\n\nMeyer, E. P., Ulmann-Schuler, A., Staufenbiel, M. & Krucker, T. Altered morphology and 3D architecture of brain vasculature in a mouse model for Alzheimer’s disease. Proceedings of the national academy of sciences 105, 3587–3592 ( ).\n\n9.↵\n\nEdwards-Richards, A. et al. Capillary rarefaction: an early marker of microvascular disease in young hemodialysis patients. Clin Kidney J 7, 569–574, doi:10.1093/ckj/sfu106 ( ).\n\n10.↵\n\nCalabrese, E., Badea, A., Cofer, G., Qi, Y. & Johnson, G. A. A Diffusion MRI Tractography Connectome of the Mouse Brain and Comparison with Neuronal Tracer Data. Cereb Cortex 25, 4628–4637, doi:10.1093/cercor/bhv121 ( ).\n\n11.↵\n\nDyer, E. L. et al. Quantifying mesoscale neuroanatomy using x-ray microtomography. eNeuro 4 ( ).\n\n12.↵\n\nLugo-Hernandez, E. et al. 3D visualization and quantification of microvessels in the whole ischemic mouse brain using solvent-based clearing and light sheet microscopy. J Cereb Blood Flow Metab 37, 3355–3367, doi:10.1177/0271678X17698970 ( ).\n\n13.↵\n\nFrangi, A. F., Niessen, W. J., Vincken, K. L. & Viergever, M. A. in International conference on medical image computing and computer-assisted intervention. 130-137 (Springer).\n\n14.↵\n\nSato, Y. et al. Three-dimensional multi-scale line filter for segmentation and visualization of curvilinear structures in medical images. Medical image analysis 2, 143–168 ( ).\n\n15.↵\n\nDi Giovanna, A. P. et al. Whole-Brain Vasculature Reconstruction at the Single Capillary Level. Sci Rep 8, 12573, doi:10.1038/s41598-018-30533-3 ( ).\n\n16.\n\nXiong, B. et al. Precise Cerebral Vascular Atlas in Stereotaxic Coordinates of Whole Mouse Brain. Front Neuroanat 11, 128, doi:10.3389/fnana.2017.00128 ( ).\n\n17.↵\n\nZhang, L. Y. et al. CLARITY for High-resolution Imaging and Quantification of Vasculature in the Whole Mouse Brain. Aging Dis 9, 262–272, doi:10.14336/AD.2017.0613 ( ).\n\n18.\n\nZudaire, E., Gambardella, L., Kurcz, C. & Vermeren, S. A computational tool for quantitative analysis of vascular networks. PloS one 6, e27385, doi:10.1371/journal.pone.0027385 ( ).\n\n19.↵\n\nClark, T. A. et al. Artery targeted photothrombosis widens the vascular penumbra, instigates peri-infarct neovascularization and models forelimb impairments. Scientific Reports 9, 2323 ( ).\n\n20.↵\n\nErtürk, A. et al. Three-dimensional imaging of solvent-cleared organs using 3DISCO. Nature Protocols 7, 1983–1995, doi:10.1038/nprot.2012.119 ( ).\n\n21.↵\n\nErturk, A. et al. Three-dimensional imaging of solvent-cleared organs using 3DISCO. Nat Protoc 7, 1983– 1995, doi:10.1038/nprot.2012.119 ( ).\n\n22.↵\n\nKamnitsas, K. et al. Efficient multi-scale 3D CNN with fully connected CRF for accurate brain lesion segmentation. Medical Image Analysis 36, 61–78, doi: https://doi.org/10.1016/j.media.2016.10.004 ( ).\n\n23.\n\nGirshick, R. in Proceedings of the IEEE international conference on computer vision. 1440–1448.\n\n24.\n\nHe, K., Gkioxari, G., Dollár, P. & Girshick, R. in Proceedings of the IEEE international conference on computer vision. 2961-2969.\n\n25.↵\n\nTetteh, G. et al. DeepVesselNet: Vessel Segmentation, Centerline Prediction, and Bifurcation Detection in 3-D Angiographic Volumes. arXiv:1803.09340 [cs] ( ).\n\n26.\n\nStrack, R. Deep learning in imaging. Nature Methods 16, 17–17, doi:10.1038/s41592-018-0267-9 ( ).\n\n27.\n\nWeigert, M. et al. Content-aware image restoration: pushing the limits of fluorescence microscopy. Nature Methods 15, 1090–1097, doi:10.1038/s41592-018-0216-7 ( ).\n\n28.\n\nWang, H. et al. Deep learning enables cross-modality super-resolution in fluorescence microscopy. Nat Methods 16, 103–110, doi:10.1038/s41592-018-0239-0 ( ).\n\n29.\n\nFalk, T. et al. U-Net: deep learning for cell counting, detection, and morphometry. Nat Methods 16, 67–70, doi:10.1038/s41592-018-0261-2 ( ).\n\n30.\n\nHaberl, M. G. et al. CDeep3M-Plug-and-Play cloud-based deep learning for image segmentation. Nat Methods 15, 677–680, doi:10.1038/s41592-018-0106-z ( ).\n\n31.\n\nCaicedo, J. C. et al. Data-analysis strategies for image-based cell profiling. Nat Methods 14, 849–863, doi:10.1038/nmeth.4397 ( ).\n\n32.↵\n\nDorkenwald, S. et al. Automated synaptic connectivity inference for volume electron microscopy. Nat Methods 14, 435–442, doi:10.1038/nmeth.4206 ( ).\n\n33.↵\n\nLitjens, G. et al. A survey on deep learning in medical image analysis. Medical Image Analysis 42, 60–88, doi:10.1016/j.media.2017.07.005 ( ).\n\n34.↵\n\nSchneider, M., Reichold, J., Weber, B., Szekely, G. & Hirsch, S. Tissue metabolism driven arterial tree generation. Med Image Anal 16, 1397–1414, doi:10.1016/j.media.2012.04.009 ( ).\n\n35.↵\n\nChalothorn, D., Clayton, J. A., Zhang, H., Pomp, D. & Faber, J. E. Collateral density, remodeling, and VEGF-A expression differ widely between mouse strains. Physiological Genomics 30, 179–191, doi:10.1152/physiolgenomics.00047.2007 ( ).\n\n36.↵\n\nLi, S. Z. in Computer Vision — ECCV ’94. (ed JanOl of Eklundh) 361–370 (Springer Berlin Heidelberg).\n\n37.\n\nDi Giovanna, A. P. et al. Whole-Brain Vasculature Reconstruction at the Single Capillary Level. Scientific Reports 8, doi:10.1038/s41598-018-30533-3 ( ).\n\n38.↵\n\nPan, C. et al. Shrinkage-mediated imaging of entire organs and organisms using uDISCO. Nat Methods, doi:10.1038/nmeth.3964 ( ).\n\n39.↵\n\nGhanavati, S., Yu, L. X., Lerch, J. P. & Sled, J. G. A perfusion procedure for imaging of the mouse cerebral vasculature by X-ray micro-CT. J Neurosci Methods 221, 70–77, doi:10.1016/j.jneumeth.2013.09.002 ( ).\n\n40.\n\nGhanavati, S., Lerch, J. P. & Sled, J. G. Automatic anatomical labeling of the complete cerebral vasculature in mouse models. Neuroimage 95, 117– 128 ( ).\n\n41.↵\n\nPathak, A. P., Kim, E., Zhang, J. & Jones, M. V. Three-dimensional imaging of the mouse neurovasculature with magnetic resonance microscopy. PloS one 6, e22643 ( ).\n\n42.↵\n\nJamniczky, H. A. & Hallgrimsson, B. Modularity in the skull and cranial vasculature of laboratory mice: implications for the evolution of complex phenotypes. Evol Dev 13, 28–37, doi:10.1111/j.1525-142X.2010.00453.x ( ).\n\n43.↵\n\nMenti, E., Bonaldi, L., Ballerini, L., Ruggeri, A. & Trucco, E. in International Workshop on Simulation and Synthesis in Medical Imaging. 167-176 (Springer).\n\n44.↵\n\nMilletari, F., Navab, N. & Ahmadi, S. in 2016 Fourth International Conference on 3D Vision (3DV). 565-571.\n\n45.\n\nHavaei, M. et al. Brain tumor segmentation with Deep Neural Networks. Medical Image Analysis 35, 18–31, doi: https://doi.org/10.1016/j.media.2016.05.004 ( ).\n\n46.↵\n\nBakas, S. et al. Identifying the Best Machine Learning Algorithms for Brain Tumor Segmentation, Progression Assessment, and Overall Survival Prediction in the BRATS Challenge. arXiv:1811.02629 [cs, stat] ( ).\n\n47.↵\n\nFaber, J. E., Moore, S. M., Lucitti, J. L. Aghajanian, & Zhang, H. Sex Differences in the Cerebral Collateral Circulation. Translational stroke research 8, 273–283, doi:10.1007/s12975-016-0508-0 ( ).\n\n48.↵\n\nZhang, H., Prabhakar, P., Sealock, R. & Faber, J. E. Wide genetic variation in the native pial collateral circulation is a major determinant of variation in severity of stroke. J Cereb Blood Flow Metab 30, 923– 934, doi:10.1038/jcbfm.2010.10 ( ).\n\n49.↵\n\nBeretta, S. et al. Cerebral collateral flow defines topography and evolution of molecular penumbra in experimental ischemic stroke. Neurobiol Dis 74, 305– 313, doi:10.1016/j.nbd.2014.11.019 ( ).\n\n50.↵\n\nBastien, F. et al. Theano: new features and speed improvements. arXiv:1211.5590 [cs] ( ).\n\n51.↵\n\nHoo-Chang, S. et al. Deep Convolutional Neural Networks for Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer Learning. IEEE transactions on medical imaging 35, 1285–1298, doi:10.1109/TMI.2016.2528162 ( ).\n\n52.↵\n\nSchneider, M., Hirsch, S., Weber, B., Székely, G. & Menze, B. H. Joint 3-D vessel segmentation and centerline extraction using oblique Hough forests with steerable filters. Medical Image Analysis 19, 220–249, doi:10.1016/j.media.2014.09.007 ( ).\n\n53.↵\n\nSchneider, M., Reichold, J., Weber, B., Székely, G. & Hirsch, S. Tissue metabolism driven arterial tree generation. Medical Image Analysis 16, 1397–1414, doi:10.1016/j.media.2012.04.009 ( ).\n\n54.↵\n\nLee, T. C., Kashyap, R. L. & Chu, C. N. Building Skeleton Models via 3-D Medial Surface Axis Thinning Algorithms. CVGIP: Graphical Models and Image Processing 56, 462–478, doi:10.1006/cgip.1994.1042 ( ).\n\n55.↵\n\nRempfler, M. et al. Reconstructing cerebrovascular networks under local physiological constraints by integer programming. Medical Image Analysis 25, 86– 94, doi:10.1016/j.media.2015.03.008 ( ).\n\n56.↵\n\nMarchesi, V. T. Alzheimer’s dementia begins as a disease of small blood vessels, damaged by oxidative-induced inflammation and dysregulated amyloid metabolism: implications for early detection and therapy. The FASEB Journal 25, 5–13, doi:10.1096/fj.11-0102ufm ( ).\n\n57.↵\n\nCohen, J. The effect size index: d. Statistical power analysis for the behavioral sciences 2, 284–288 ( )."
    }
}
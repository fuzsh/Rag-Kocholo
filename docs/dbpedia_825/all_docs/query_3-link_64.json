{
    "id": "dbpedia_825_3",
    "rank": 64,
    "data": {
        "url": "https://github.com/uzh-rpg/ess",
        "read_more_link": "",
        "language": "en",
        "title": "based Semantic Segmentation from Still Images\" (ECCV, 2022).",
        "top_image": "https://opengraph.githubassets.com/242c5b4ae553f357408ebd5b4213f58453786d34126defcfc207c2e8197c62e8/uzh-rpg/ess",
        "meta_img": "https://opengraph.githubassets.com/242c5b4ae553f357408ebd5b4213f58453786d34126defcfc207c2e8197c62e8/uzh-rpg/ess",
        "images": [
            "https://github.com/uzh-rpg/ess/raw/main/resources/ESS.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "uzh-rpg"
        ],
        "publish_date": null,
        "summary": "",
        "meta_description": "Repository relating to \"ESS: Learning Event-based Semantic Segmentation from Still Images\" (ECCV, 2022).  - GitHub - uzh-rpg/ess: Repository relating to \"ESS: Learning Event-based Semantic Segmentation from Still Images\" (ECCV, 2022).",
        "meta_lang": "en",
        "meta_favicon": "https://github.com/fluidicon.png",
        "meta_site_name": "GitHub",
        "canonical_link": "https://github.com/uzh-rpg/ess",
        "text": "This is the code for the paper ESS: Learning Event-based Semantic Segmentation from Still Images (PDF) by Zhaoning Sun*, Nico Messikommer*, Daniel Gehrig, and Davide Scaramuzza. For an overview of our method, check out our video.\n\nIf you use any of this code, please cite the following publication:\n\nRetrieving accurate semantic information in challenging high dynamic range (HDR) and high-speed conditions remains an open challenge for image-based algorithms due to severe image degradations. Event cameras promise to address these challenges since they feature a much higher dynamic range and are resilient to motion blur. Nonetheless, semantic segmentation with event cameras is still in its infancy which is chiefly due to the novelty of the sensor, and the lack of high-quality, labeled datasets. In this work, we introduce ESS, which tackles this problem by directly transferring the semantic segmentation task from existing labeled image datasets to unlabeled events via unsupervised domain adaptation (UDA). Compared to existing UDA methods, our approach aligns recurrent, motion-invariant event embeddings with image embeddings. For this reason, our method neither requires video data nor per-pixel alignment between images and events and, crucially, does not need to hallucinate motion from still images. Additionally, to spur further research in event-based semantic segmentation, we introduce DSEC-Semantic, the first large-scale event-based dataset with fine-grained labels. We show that using image labels alone, ESS outperforms existing UDA approaches, and when combined with event labels, it even outperforms state-of-the-art supervised approaches on both DDD17 and DSEC-Semantic. Finally, ESS is general-purpose, which unlocks the vast amount of existing labeled image datasets and paves the way for new and exciting research directions in new fields previously inaccessible for event cameras.\n\nIf desired, a conda environment can be created using the followig command:\n\nAs an initial step, the wheel package needs to be installed with the following command:\n\npip install wheel\n\nThe required python packages are listed in the requirements.txt file.\n\npip install -r requirements.txt\n\nPre-trained E2VID model needs to be downloaded here and placed in /e2vid/pretrained/.\n\nThe DSEC-Semantic dataset can be downloaded here. The dataset should have the following format:\n\nThe original DDD17 dataset with semantic segmentation labels can be downloaded here. Additionally, we provide a pre-processed DDD17 dataset with semantic labels here. Please do not forget to cite DDD17 and Ev-SegNet if you are using the DDD17 with semantic labels.\n\nThe Cityscapes dataset can be downloaded here.\n\nThe settings for the training can be specified in config/settings_XXXX.yaml. Two different models can be trained:\n\ness: ESS UDA / ESS supervised (events labels + frames labels)\n\ness_supervised: ESS supervised (only events labels)\n\nThe following command starts the training:\n\nFor testing the pre-trained models, please set load_pretrained_weights=True and specify the path of pre-trained weights in pretrained_file.\n\nTo download the pre-trained weights for the models on DDD17 and DSEC in the UDA setting, please fill in your details in this Google Form.\n\nSeveral network architectures were adapted from: https://github.com/uzh-rpg/rpg_e2vid\n\nThe general training framework was inspired by: https://github.com/uzh-rpg/rpg_ev-transfer\n\nThe DSEC data loader was adapted from: https://github.com/uzh-rpg/DSEC\n\nThe optimizer was adapted from: https://github.com/LiyuanLucasLiu/RAdam"
    }
}
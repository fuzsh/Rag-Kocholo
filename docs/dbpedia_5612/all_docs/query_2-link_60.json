{
    "id": "dbpedia_5612_2",
    "rank": 60,
    "data": {
        "url": "https://www.analyticsvidhya.com/blog/2021/07/metrics-to-evaluate-your-classification-model-to-take-the-right-decisions/",
        "read_more_link": "",
        "language": "en",
        "title": "Metrics to Evaluate your Classification Model to take the Right Decisions",
        "top_image": "https://cdn.analyticsvidhya.com/wp-content/uploads/2019/03/evaluate.jpg",
        "meta_img": "https://cdn.analyticsvidhya.com/wp-content/uploads/2019/03/evaluate.jpg",
        "images": [
            "https://av-public-assets.s3.ap-south-1.amazonaws.com/logos/av-logo-svg.svg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/navbar.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/navbar.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/navbar.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/navbar.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/navbar.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/default_avatar.svg",
            "https://cdn-images-1.medium.com/max/800/1*xgiEyLEym8eh-HeO98WR6w.png",
            "https://cdn-images-1.medium.com/max/800/1*R6jP_uvlkcxtQSa264N3Sw.png",
            "https://cdn-images-1.medium.com/max/800/1*zzR88UEu6A786mPYcRVDhQ.png",
            "https://cdn-images-1.medium.com/max/800/1*_JY_jxfndH8oBI3clamifA.png",
            "https://cdn-images-1.medium.com/max/800/0*UnezxOaQyU6Fb6D3",
            "https://cdn-images-1.medium.com/max/800/0*p1t9CzwpaOXxsx4l.png",
            "https://cdn-images-1.medium.com/max/800/0*XgGoMQLlGGDgpzYa.png",
            "https://cdn-images-1.medium.com/max/800/0*tu5x_GEgs-iRpJ9H",
            "https://cdn-images-1.medium.com/max/800/1*bpjCSt38NydElzPf6O5Xng.png",
            "https://cdn-images-1.medium.com/max/800/0*hGFmvwiq4mmHySvY.png",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/path-digital.png",
            "https://av-identity.s3.amazonaws.com/users/user/bGnsep7nT0GMWuLpkDl15Q.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/in.png",
            "https://av-identity.s3.amazonaws.com/users/user/R7HrsWl1QrGRiw_e9m4fDA.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/in.png",
            "https://av-identity.s3.amazonaws.com/users/user/ZcU4ALTFT96MVCzfiGuhsQ.jpeg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/in.png",
            "https://av-identity.s3.amazonaws.com/users/user/aM3WrxdNSTGLg7LoqX-q0w.png",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/in.png",
            "https://av-identity.s3.amazonaws.com/users/user/zy4FL_yyQlG4PkWcyGYvhw.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/in.png",
            "https://av-identity.s3.amazonaws.com/users/user/a4ByfUyoQRmdGzLpBzHVLw.jpeg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/in.png",
            "https://av-identity.s3.amazonaws.com/users/user/ZTsmKl-1Qvqn07FUzgaBNw.png",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/in.png",
            "https://d2cd20fxv8fgim.cloudfront.net/homepage/images/Play_Store.svg",
            "https://d2cd20fxv8fgim.cloudfront.net/homepage/images/App_Store.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Sumeet Kumar Agrawal"
        ],
        "publish_date": "2021-07-20T07:39:12+00:00",
        "summary": "",
        "meta_description": "In this article, we will understand the Evaluation Metrics For Classification Model. This concept helps you choose the right models",
        "meta_lang": "en",
        "meta_favicon": "https://imgcdn.analyticsvidhya.com/favicon/av-fav.ico",
        "meta_site_name": "Analytics Vidhya",
        "canonical_link": "https://www.analyticsvidhya.com/blog/2021/07/metrics-to-evaluate-your-classification-model-to-take-the-right-decisions/",
        "text": "Introduction\n\nEvaluating the performance of your classification model is crucial to ensure its accuracy and effectiveness. While accuracy is important, it’s just one piece of the puzzle. There are several other evaluation metrics that provide a more comprehensive understanding of your model’s performance. This article will discuss these metrics and how they can guide you in making the right decisions to improve your model’s predictive power.\n\nLearning Objectives\n\nIntroduction to ML Model Evaluation and its significance.\n\nExploration of various evaluation metrics tailored to specific use cases.\n\nIn-depth analysis of these metrics for better comprehension.\n\nThis article was published as a part of the Data Science Blogathon.\n\nClassification Metrics in Machine Learning\n\nClassification Metrics is about predicting the class labels given input data. In binary classification, there are only two possible output classes(i.e., Dichotomy). In multiclass classification, more than two possible classes can be present. I’ll focus only on binary classification.\n\nA very common example of binary classification is spam detection, where the input data could include the email text and metadata (sender, sending time), and the output label is either “spam” or “not spam.” (See Figure) Sometimes, people use some other names also for the two classes: “positive” and “negative,” or “class 1” and “class 0.”\n\nThere are many ways for measuring classification performance. Accuracy, confusion matrix, log-loss, and AUC-ROC are some of the most popular metrics. Precision-recall is a widely used metrics for classification problems.\n\nAccuracy\n\nAccuracy simply measures how often the classifier correctly predicts. We can define accuracy as the ratio of the number of correct predictions and the total number of predictions.\n\nWhen any model gives an accuracy rate of 99%, you might think that model is performing very good but this is not always true and can be misleading in some situations. I am going to explain this with the help of an example.\n\nExample\n\nConsider a binary classification problem, where a model can achieve only two results, either model gives a correct or incorrect prediction. Now imagine we have a classification task to predict if an image is a dog or cat as shown in the image. In a supervised learning algorithm, we first fit/train a model on training data, then test the model on testing data. Once we have the model’s predictions from the X_test data, we compare them to the true y_values (the correct labels).\n\nWe feed the image of the dog into the training model. Suppose the model predicts that this is a dog, and then we compare the prediction to the correct label. If the model predicts that this image is a cat and then we again compare it to the correct label and it would be incorrect.\n\nWe repeat this process for all images in X_test data. Eventually, we’ll have a count of correct and incorrect matches. But in reality, it is very rare that all incorrect or correct matches hold equal value. Therefore one metric won’t tell the entire story.\n\nAccuracy is useful when the target class is well balanced but is not a good choice for the unbalanced classes. Imagine the scenario where we had 99 images of the dog and only 1 image of a cat present in our training data. Then our model would always predict the dog, and therefore we got 99% accuracy. In reality, Data is always imbalanced for example Spam email, credit card fraud, and medical diagnosis. Hence, if we want to do a better model evaluation and have a full picture of the model evaluation, other metrics such as recall and precision should also be considered.\n\nConfusion Matrix\n\nConfusion Matrix is a performance measurement for the machine learning classification problems where the output can be two or more classes. It is a table with combinations of predicted and actual values.\n\nA confusion matrix is defined as thetable that is often used to describe the performance of a classification model on a set of the test data for which the true values are known.\n\nIt is extremely useful for measuring the Recall, Precision, Accuracy, and AUC-ROC curves.\n\nLet’s try to understand TP, FP, FN, TN with an example of pregnancy analogy.\n\nTrue Positive: We predicted positive and it’s true. In the image, we predicted that a woman is pregnant and she actually is.\n\nTrue Negative: We predicted negative and it’s true. In the image, we predicted that a man is not pregnant and he actually is not.\n\nFalse Positive (Type 1 Error): We predicted positive and it’s false. In the image, we predicted that a man is pregnant but he actually is not.\n\nFalse Negative (Type 2 Error): We predicted negative and it’s false. In the image, we predicted that a woman is not pregnant but she actually is.\n\nWe discussed Accuracy, now let’s discuss some other metrics of the confusion matrix\n\nPrecision\n\nIt explains how many of the correctly predicted cases actually turned out to be positive. Precision is useful in the cases where False Positive is a higher concern than False Negatives. The importance of Precision is in music or video recommendation systems, e-commerce websites, etc. where wrong results could lead to customer churn and this could be harmful to the business.\n\nPrecision for a label is defined as the number of true positives divided by the number of predicted positives.\n\nRecall (Sensitivity)\n\nIt explains how many of the actual positive cases we were able to predict correctly with our model. Recall is a useful metric in cases where False Negative is of higher concern than False Positive. It is important in medical cases where it doesn’t matter whether we raise a false alarm but the actual positive cases should not go undetected!\n\nRecall for a label is defined as the number of true positives divided by the total number of actual positives.\n\nF1 Score\n\nIt gives a combined idea about Precision and Recall metrics. It is maximum when Precision is equal to Recall.\n\nF1 Score is the harmonic mean of precision and recall.\n\nThe F1 score punishes extreme values more. F1 Score could be an effective evaluation metric in the following cases:\n\nWhen FP and FN are equally costly.\n\nAdding more data doesn’t effectively change the outcome\n\nTrue Negative is high\n\nAUC-ROC\n\nThe Receiver Operator Characteristic (ROC) is a probability curve that plots the TPR(True Positive Rate) against the FPR(False Positive Rate) at various threshold values and separates the ‘signal’ from the ‘noise’.\n\nThe Area Under the Curve (AUC) is the measure of the ability of a classifier to distinguish between classes. From the graph, we simply say the area of the curve ABDE and the X and Y-axis.\n\nFrom the graph shown below, the greater the AUC, the better is the performance of the model at different threshold points between positive and negative classes. This simply means that When AUC is equal to 1, the classifier is able to perfectly distinguish between all Positive and Negative class points. When AUC is equal to 0, the classifier would be predicting all Negatives as Positives and vice versa. When AUC is 0.5, the classifier is not able to distinguish between the Positive and Negative classes.\n\nImage Source— https://www.analyticsvidhya.com/blog/2020/06/auc-roc-curve-machine-learning/\n\nWorking of AUC\n\nIn a ROC curve, the X-axis value shows False Positive Rate (FPR), and Y-axis shows True Positive Rate (TPR). Higher the value of X means higher the number of False Positives(FP) than True Negatives(TN), while a higher Y-axis value indicates a higher number of TP than FN. So, the choice of the threshold depends on the ability to balance between FP and FN.\n\nLog Loss\n\nLog loss (Logistic loss) or Cross-Entropy Loss is one of the major metrics to assess the performance of a classification problem.\n\nFor a single sample with true label y∈{0,1} and a probability estimate p=Pr(y=1), the log loss is:\n\nConclusion\n\nUnderstanding how well a machine learning model will perform on unseen data is the main purpose behind working with these evaluation metrics. Classification Metrics like accuracy, precision, recall are good ways to evaluate classification models for balanced datasets, but if the data is imbalanced then other methods like ROC/AUC perform better in evaluating the model performance.\n\nROC curve isn’t just a single number but it’s a whole curve that provides nuanced details about the behavior of the classifier. It is also hard to quickly compare many ROC curves to each other.\n\nFrequently Asked Questions"
    }
}
{
    "id": "dbpedia_5612_2",
    "rank": 37,
    "data": {
        "url": "https://aws.amazon.com/what-is/autoregressive-models/",
        "read_more_link": "",
        "language": "en",
        "title": "What are Autoregressive Models?",
        "top_image": "https://a0.awsstatic.com/libra-css/images/logos/aws_logo_smile_1200x630.png",
        "meta_img": "https://a0.awsstatic.com/libra-css/images/logos/aws_logo_smile_1200x630.png",
        "images": [
            "https://d1.awsstatic.com/Free-Tier_64.f14d1a130811a363bbea22de4bb589f9ab801dfb.png",
            "https://d1.awsstatic.com/Machine-Learning_64.1408e2e83c4e428dd55ba4baaaaf769d14e0a9e1.png",
            "https://d1.awsstatic.com/Learn-More_64.dc6d454a262eb880a9dd0d8cb283dca5bc00cb18.png",
            "https://d1.awsstatic.com/All-Products_64.78a4c2cdfdd82b7abc3fda6b44371491bdf5963e.png",
            "https://d1.awsstatic.com/FORMULA33.73e02af03ffa7c17562929db41034d6b048352be.png",
            "https://d1.awsstatic.com/FORMULA%2044.78a721f60657a2dc0657417441e4dda37862d74c.png",
            "https://d1.awsstatic.com/webteam/product-pages/Product-Page_Standard-Icons_01_Product-Features_SqInk.a8d5666758afc5121b4eb818ae18126031c4b61e.png",
            "https://d1.awsstatic.com/webteam/product-pages/Product-Page_Standard-Icons_02_Sign-Up_SqInk.f43d5ddc9c43883eec6187f34c68155402b13312.png",
            "https://d1.awsstatic.com/webteam/product-pages/Product-Page_Standard-Icons_03_Start-Building_SqInk.6a1ef4429a6604cda9b0857084aa13e2ee4eebca.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "What are Autoregressive Models how and why businesses use Autoregressive Models, and how to use Autoregressive Models with AWS.",
        "meta_lang": "en",
        "meta_favicon": "https://a0.awsstatic.com/libra-css/images/site/fav/favicon.ico",
        "meta_site_name": "Amazon Web Services, Inc.",
        "canonical_link": "https://aws.amazon.com/what-is/autoregressive-models/",
        "text": "Generative artificial intelligence (generative AI) is an advanced data science technology capable of creating new and unique content by learning from massive training data. The following sections describe how autoregressive modeling enables generative AI applications.\n\nNatural language processing (NLP)\n\nAutoregressive modeling is an important component of large language models (LLMs). LLMs are powered by the generative pre-trained transformer (GPT), a deep neural network derived from the transformer architecture. The transformer consists of an encoder-decoder, which enables natural language understanding and natural language generation, respectively. The GPT uses only the decoder for autoregressive language modeling. This allows GPT to understand natural languages and respond in ways humans comprehend. A GPT-powered large language model predicts the next word by considering the probability distribution of the text corpus it is trained on.\n\nRead about Natural Language Processing (NLP)\n\nImage synthesis\n\nAutoregression allows deep learning models to generate images by analyzing limited information. Image processing neural networks like PixelRNN and PixelCNN use autoregressive modeling to predict visual data by examining existing pixel information. You can use autoregressive techniques to sharpen, upscale, and reconstruct images while maintaining quality.\n\nTime-series prediction\n\nAutoregressive models are helpful in predicting the likelihood of time-series events. For example, deep learning models use autoregressive techniques for forecasting stock prices, weather, and traffic conditions based on historical values.\n\nData augmentation\n\nML engineers train AI models with curated datasets to improve performance. In some cases, there is insufficient data to train the model adequately. Engineers use autoregressive models to generate new and realistic deep learning training data. They use the generated data to augment existing limited training datasets.\n\nAn autoregressive model uses a variation of linear regression analysis to predict the next sequence from a given range of variables. In regression analysis, the statistical model is provided with several independent variables, which it uses to predict the value of a dependent variable.\n\nLinear regression\n\nYou can imagine linear regression as drawing a straight line that best represents the average values distributed on a two-dimensional graph. From the straight line, the model generates a new data point corresponding to the conditional distribution of historical values.\n\nConsider the simplest form of the line graph equation between y (dependent variable) and x (independent variable); y=c*x+m, where c and m are constant for all possible values of x and y. So, for example, if the input dataset for (x,y) was (1,5), (2,8), and (3,11). To identify the linear regression method, you would use the following steps:\n\nPlot a straight line and measure the correlation between 1 and 5.\n\nChange the straight line direction for new values (2,8) and (3,11) until all values fit.\n\nIdentify the linear regression equation as y=3*x+2.\n\nExtrapolate or predict that y is 14 when x is 4.\n\nAutoregression\n\nAutoregressive models apply linear regression with lagged variables of its output taken from previous steps. Unlike linear regression, the autoregressive model doesn’t use other independent variables except the previously predicted results. Consider the following formula.\n\nWhen expressed in the probabilistic term, an autoregressive model distributes independent variables over n-possible steps, assuming that earlier variables conditionally influence the outcome of the next one.\n\nWe can also express autoregressive modeling with the equation below.\n\nHere, y is the prediction outcome of multiple orders of previous results multiplied by their respective coefficients, ϕ. The coefficient represents weights or parameters influencing the predictor’s importance to the new result. The formula also considers random noise that may affect the prediction, indicating that the model is not ideal and further improvement is possible.\n\nLag\n\nData scientists add more lagged values to improve autoregressive modeling accuracy. They do so by increasing the value of t, which denotes the number of steps in the time series of data. A higher number of steps allows the model to capture more past predictions as input. For example, you can expand an autoregressive model to include the predicted temperature from 7 days to the past 14 days to get a more accurate outcome. That said, increasing the lagged order of an autoregressive model does not always result in improved accuracy. If the coefficient is close to zero, the particular predictor has little influence on the result of the model. Moreover, indefinitely expanding the sequence results in a more complex model requiring more computing resources to run.\n\nAutocorrelation is a statistical method that evaluates how strongly the output of an autoregressive model is influenced by its lagged variables. Data scientists use autocorrelation to describe the relationship between the output and lagged inputs of a model. The higher the correlation, the higher the prediction accuracy of the model. The following are some considerations with autocorrelation:\n\nA positive correlation means that the output follows the trends charted in the previous values. For example, the model predicts that the stock price will increase today because it has increased for the past few days.\n\nA negative correlation means that the output variable heads opposite to previous results. For example, the autoregressive system observes that the past few days were raining but predicted a sunny day tomorrow.\n\nZero correlation might indicate a lack of specific patterns between input and output.\n\nData engineers use autocorrelation to determine how many steps they should include in the model to optimize computing resources and response accuracy. In some applications, the autoregressive model might show strong autocorrelation when using variables from the immediate past but weaker autocorrelation for distant inputs. For example, engineers found that an autoregressive weather predictor is less sensitive to past predictions from over 30 days. So, they revised the model to only include lagged results from the past 30 days. This led to more accurate results using fewer computing resources.\n\nApart from autoregression, several regressive techniques have been introduced to analyze variables and their interdependencies. The following sections describe the differences.\n\nLinear regression compared with autoregression\n\nBoth regression methods assume that past variables share a linear relationship with future values. Linear regression predicts an outcome based on several independent variables within the same timeframe. Meanwhile, autoregression uses only one variable type but expands it over several points to predict the future outcome. For example, you use linear regression to predict your commute time based on weather, traffic volume, and walking speed. Alternately, an autoregression model uses your past commute times to estimate the arrival time for today.\n\nPolynomial regression compared with autoregression\n\nPolynomial regression is a statistical method that captures the relationship of non-linear variables. Some variables can’t be linearly represented by a straight line and require additional polynomial terms to better reflect their relationships. For example, engineers use polynomial regression to analyze employee earnings based on their education level. Meanwhile, autoregression is suitable for predicting future income of an employee based on their previous salaries.\n\nLogistic regression compared with autoregression\n\nLogistic regression allows a statistical model to predict the likelihood of a specific event in the probabilistic term. It expresses the prediction outcome in percentage instead of a range of numbers. For example, business analysts use a logistic regression model to predict an 85 percent chance of supply cost increment in the following month. Conversely, the autoregression model predicts the probable inventory price given its historical prediction for previous months.\n\nRidge regression compared with autoregression\n\nRidge regression is a variant of linear regression that allows the coefficient of a model to be restricted. Data scientists can adjust a penalty factor, compensating for the influence of the coefficient in modeling the outcome. The parameter coefficient can be suppressed to near zero in a ridge regression model. This is helpful when the regressive algorithm is prone to overfitting. Overfitting is a condition where the model can generalize well with training data but not unfamiliar real-world data. An autoregression model, meanwhile, does not have a coefficient penalty mechanism.\n\nLasso regression compared with autoregression\n\nLasso regression is similar to ridge regression, which can restrict the variable coefficient with a penalty factor. However, lasso regression can suppress the coefficient to zero. This allows data scientists to simplify complex models by ignoring non-critical parameters. Meanwhile, autoregressive models don’t regulate their predictions with coefficient shrinkage."
    }
}
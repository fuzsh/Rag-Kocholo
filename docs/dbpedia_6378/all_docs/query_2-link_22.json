{
    "id": "dbpedia_6378_2",
    "rank": 22,
    "data": {
        "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10124862/",
        "read_more_link": "",
        "language": "en",
        "title": "When performance is not enough—A multidisciplinary view on clinical decision support",
        "top_image": "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/pmc-card-share.jpg?_=0",
        "meta_img": "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/pmc-card-share.jpg?_=0",
        "images": [
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/favicons/favicon-57.png",
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/icon-dot-gov.svg",
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/icon-https.svg",
            "https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/logos/AgencyLogo.svg",
            "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/logo-plosone.png",
            "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/corrauth.gif",
            "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/corrauth.gif",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10124862/bin/pone.0282619.g001.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10124862/bin/pone.0282619.g002.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10124862/bin/pone.0282619.g003.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Roland Roller",
            "Aljoscha Burchardt",
            "David Samhammer",
            "Simon Ronicke",
            "Wiebke Duettmann",
            "Sven Schmeier",
            "Sebastian Möller",
            "Peter Dabrock",
            "Klemens Budde",
            "Manuel Mayrdorfer"
        ],
        "publish_date": "2023-08-29T00:00:00",
        "summary": "",
        "meta_description": "Scientific publications about the application of machine learning models in healthcare often focus on improving performance metrics. However, beyond often short-lived improvements, many additional aspects need to be taken into consideration to make sustainable ...",
        "meta_lang": "en",
        "meta_favicon": "https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon.ico",
        "meta_site_name": "PubMed Central (PMC)",
        "canonical_link": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10124862/",
        "text": "Reader study\n\nBased on the internal validation, it was difficult to assess the quality and efficiency of the system as to the best of our knowledge no related work addressing this problem existed. Therefore it was also not clear if this model would be useful for medical experts in some way or another. Thus, instead of focusing on the further optimization of our machine learning model, we decided to find out how well physicians can solve the given task, and if our approach might benefit them already.\n\nTo this end, we designed and conducted a reader study, consisting of two parts: In the first part, physicians received a data point of a patient, together with all information up to this time, and had to make a risk estimation (0 to 100%) if an infection will occur within the next 90 days. In the second part, the physician additionally received information on the risk prediction model, in the form of a dashboard (see ). The dashboard depicts the risk score over time within a graph, mapped to a traffic light system, indicating if the patient has got a low, medium, or high risk of infection, along with local and global features of the risk prediction model. Whereas global features are generally important features for the overall model, local features are those which had the strongest impact on the current prediction of the system. The cutoff of values for the traffic light system was calculated by using the predictions on a separate development set. While the dashed line in the middle represents the threshold for the optimal F1 score, the yellow area is defined by F2, which highlights recall over precision, and the red area is defined by F0.5, which highlights precision over recall.\n\nWhile automatic risk estimations can be generated within seconds, humans require time to carry out this task. This fact indeed limited our possibilities for human evaluation. Overall, our study included 120 data points of 120 different patients (38 infections, 82 without infections). Eight physicians participated in our study, four experienced (senior) and four assistant (junior) physicians. Within each part of the study, each physician received 15 data points to analyze. The study was accepted by the ethical advisory board of the hospital, and according to the staff council each physician had up to 30 minutes to examine the data of each patient. Each physician received some basic introduction about the dashboard, information about how the machine learning model works, as well as its performance on the internal validation study. In each part of the study, each physician received different data points to evaluate as we wanted to prevent priming effects. This has to be taken into account when both conditions (without/with AI) are compared. Finally, the results of both parts of the study are then compared to the performance of the risk prediction system itself. In the following, we refer to the first part of the study as MD (medical doctor) and the second part as MD+AI (medical doctor including machine learning support). The machine learning component itself will be referred to as AI. The experimental setup of the reader study is depicted in .\n\nFor the experiment, we re-trained the model from the internal validation and made sure that no patient of the test set occurs within the training and development set. The evaluation of the experiment is carried out according to ROC and PRC, as well as sensitivity (recall), specificity (true negative rate), and positive predictive value (PPV, or precision) using different cutoff thresholds. The cutoff values were necessary to map from a regression score between 0-1 to a binary outcome (true/false), to calculate sensitivity, specificity, and PPV.\n\nThe results presented in show that overall the task is challenging for physicians, considering the ROC score of 0.63. Moreover, the results indicate that physicians receiving additional automatic decision support do not increase performance. On the other hand, the risk prediction system outperforms the physicians in both parts of the study regarding ROC and PRC.\n\nTable 3\n\nROCPRCSENSPECPPVthrsAI0.7190.5670.7540.7180.264F20.5800.8440.333F10.3290.9400.424F0.5MD0.6300.4740.4480.6890.406>50%MD+AI0.6220.4690.3680.8290.500>50%\n\nWhile the ROC and PRC results of AI show a promising improvement in comparison to the physicians, this does not tell us if patients would be labeled as critical (infection will occur) or uncritical. Thus, we use different cutoff values to map from the probability estimation of the physicians and the regression score of the risk prediction system, to either 1 (infection) or 0 (no infection). In this way, we can evaluate the risk prediction system, as well as the physicians according to sensitivity, specificity, and positive predictive value. The results in show that AI achieves the highest sensitivity, therefore finds the largest number of critical patients, depending on the selection of the cutoff value. All results present only a moderate performance according to the positive predictive value (PPV). In the case of MD+AI for instance, every second prediction would be a true positive infection prediction, while the sensitivity is about 37%.\n\ncompares the ROC performance of the two subject groups in both parts of the study. The table shows that senior MDs reach higher values than junior MDs at solving the task alone, without decision support. Moreover, the table indicates that junior MDs increase in performance, together with the automatic decision support, while senior MDs decrease.\n\nTable 4\n\nJunior MDSenior MDMD0.5781 0.6772 MD+AI 0.6398 0.6149\n\nConnecting different stakeholders\n\nIt has become standard practice for better AI decision support systems to claim that they are aligned with the concept of trustworthy AI. Numerous criteria have been developed for this purpose, for which various reviews are now available [40, 41]. If one also takes the frame of the High-Level Expert Group on Artificial Intelligence: Ethics guidelines as a reference, one can say: Any AI-System must be compatible with applicable laws, meet ethical standards, and not entail unforeseen side effects [42]. However, trust is not a concept that is valid once achieved but must be constantly maintained [43]. Once achieved it must constantly be reviewed and maintained. Therefore, it is crucial to how the path from principles to practice is designed. Formally, approaches of co-creation AI [22] are well suited for this challenge. But also terms of content need to be unfolded. For this purpose, we decided in our experiment to analyze and evaluate the attitudes of the physicians involved.\n\nOf particular interest for AI-driven support systems in the clinic is the question of how interaction processes change through the introduction of such and how normative concepts like trust, responsibility, and transparency have to be rethought in a new and interrelated way [44]. To approach these questions in the context of our experiments, qualitative interviews were conducted with the physicians at the end of the study. We decided to use semi-structured expert interviews as the data collection method. The interviews were intended both to obtain an evaluation of the conducted case study and to find out what impact the introduction of AI-driven support systems has on the interviewed physicians. The evaluation is mainly relevant for the further development of the tested system while other estimations of the physicians can help to conclude further ethical discussions.\n\nThe results are reassuring. The physicians tell us how they used the system, to what extent they trusted the assessment, and what suggestions they have for improvement. Especially for the design of such experiments, these statements are of major importance. For example, it became clear that some of the physicians followed a certain procedure they gave themselves, e.g., by first noting their prediction and then checking the system result (“second opinion”) or by first looking at the system result and then challenging it with the available evidence. When designing further experiments, we can now better decide if we want to impose a certain procedure or not. At the same time, physicians report on the complexity of clinical decision-making in general. They confirm trust to be a prerequisite for being able to make decisions. For an AI-driven support system to be transferred to clinical practice at all, it must therefore be trusted. According to the physicians, the evaluations of the systems must be explainable for this. Not completely, but in a way that the physicians can present the application of the system to the patient, to whom responsibility is always borne.\n\nWithout being able to go into more detail about the content of the interviews at this point, we want to emphasize the positive experience that took place with the qualitative survey accompanying the case study. What becomes clear is that through the interviews a connection is made between the profession of the physicians and the researchers. Following the thesis of Noordegraaf [45] that professions have to be more and more in touch with their environment, especially due to technical challenges, we can say from our perspective that qualitative data collection can be a link between physicians and their environment. Our recommendation to future research projects is therefore that they should be accompanied by such or similar studies. These should not only serve as an evaluation of the individual projects but also as part of designing AI decision support systems and as a contribution to the communication process which must be part of the transformations that already take place.\n\n17 Oct 2022\n\nPONE-D-22-25782When Performance is not Enough - A Multidisciplinary View on Clinical Decision SupportPLOS ONE\n\nDear Dr. Roller,\n\nThank you for submitting your manuscript to PLOS ONE. After careful consideration, we feel that it has merit but does not fully meet PLOS ONE’s publication criteria as it currently stands. Therefore, we invite you to submit a revised version of the manuscript that addresses the points raised during the review process.\n\nPlease submit your revised manuscript by 15 November,2022. If you will need more time than this to complete your revisions, please reply to this message or contact the journal office at gro.solp@enosolp. When you're ready to submit your revision, log on to https://www.editorialmanager.com/pone/ and select the 'Submissions Needing Revision' folder to locate your manuscript file.\n\nPlease include the following items when submitting your revised manuscript:\n\nA rebuttal letter that responds to each point raised by the academic editor and reviewer(s). You should upload this letter as a separate file labeled 'Response to Reviewers'.\n\nA marked-up copy of your manuscript that highlights changes made to the original version. You should upload this as a separate file labeled 'Revised Manuscript with Track Changes'.\n\nAn unmarked version of your revised paper without tracked changes. You should upload this as a separate file labeled 'Manuscript'.\n\nIf you would like to make changes to your financial disclosure, please include your updated statement in your cover letter. Guidelines for resubmitting your figure files are available below the reviewer comments at the end of this letter.\n\nIf applicable, we recommend that you deposit your laboratory protocols in protocols.io to enhance the reproducibility of your results. Protocols.io assigns your protocol its own identifier (DOI) so that it can be cited independently in the future. For instructions see: https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols. Additionally, PLOS ONE offers an option for publishing peer-reviewed Lab Protocol articles, which describe protocols hosted on protocols.io. Read more information on sharing protocols at https://plos.org/protocols?utm_medium=editorial-email&utm_source=authorletters&utm_campaign=protocols.\n\nWe look forward to receiving your revised manuscript.\n\nKind regards,\n\nMuhammad Fazal Ijaz\n\nAcademic Editor\n\nPLOS ONE\n\nJournal Requirements:\n\nWhen submitting your revision, we need you to address these additional requirements.\n\n1. Please ensure that your manuscript meets PLOS ONE's style requirements, including those for file naming. The PLOS ONE style templates can be found at\n\nhttps://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf and\n\nhttps://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf\n\n2. For studies reporting research involving human participants, PLOS ONE requires authors to confirm that this specific study was reviewed and approved by an institutional review board (ethics committee) before the study began. Please provide the specific name of the ethics committee/IRB that approved your study, or explain why you did not seek approval in this case.\n\nOnce you have amended this/these statement(s) in the Methods section of the manuscript, please add the same text to the “Ethics Statement” field of the submission form (via “Edit Submission”)\n\n3. Please ensure that you have specified (1) whether consent was informed and (2) what type you obtained (for instance, written or verbal, and if verbal, how it was documented and witnessed). If your study included minors, state whether you obtained consent from parents or guardians. If the need for consent was waived by the ethics committee, please include this information.\n\n4. Thank you for stating the following in your Competing Interests section:\n\n\"NO\"\n\nPlease complete your Competing Interests on the online submission form to state any Competing Interests. If you have no competing interests, please state \"The authors have declared that no competing interests exist.\", as detailed online in our guide for authors at http://journals.plos.org/plosone/s/submit-now\n\nThis information should be included in your cover letter; we will change the online submission form on your behalf.\n\n5. Thank you for stating the following in the Acknowledgments Section of your manuscript:\n\n\"This research was supported by the German Federal Ministry of Education and Research (BMBF) through the project vALID (01GP1903A).\"\n\nWe note that you have provided funding information that is not currently declared in your Funding Statement. However, funding information should not appear in the Acknowledgments section or other areas of your manuscript. We will only publish funding information present in the Funding Statement section of the online submission form.\n\nPlease remove any funding-related text from the manuscript and let us know how you would like to update your Funding Statement. Currently, your Funding Statement reads as follows:\n\n\"The project, and all authors received funding by the German Federal Ministry of Education and Research (BMBF) through the project vALID (01GP1903A).\n\nhttps://www.gesundheitsforschung-bmbf.de/de/valid-klinische-entscheidungsfindung-durch-kunstliche-intelligenz-ethische-rechtliche-und-10430.php\n\nAnd no, the sponsor did not play any role in the study design.\"\n\nPlease include your amended statements within your cover letter; we will change the online submission form on your behalf.\n\n6. We note that you have indicated that data from this study are available upon request. PLOS only allows data to be available upon request if there are legal or ethical restrictions on sharing data publicly. For more information on unacceptable data access restrictions, please see http://journals.plos.org/plosone/s/data-availability#loc-unacceptable-data-access-restrictions.\n\nIn your revised cover letter, please address the following prompts:\n\na) If there are ethical or legal restrictions on sharing a de-identified data set, please explain them in detail (e.g., data contain potentially sensitive information, data are owned by a third-party organization, etc.) and who has imposed them (e.g., an ethics committee). Please also provide contact information for a data access committee, ethics committee, or other institutional body to which data requests may be sent.\n\nb) If there are no restrictions, please upload the minimal anonymized data set necessary to replicate your study findings as either Supporting Information files or to a stable, public repository and provide us with the relevant URLs, DOIs, or accession numbers. For a list of acceptable repositories, please see http://journals.plos.org/plosone/s/data-availability#loc-recommended-repositories.\n\nWe will update your Data Availability statement on your behalf to reflect the information you provide.\n\n7. PLOS requires an ORCID iD for the corresponding author in Editorial Manager on papers submitted after December 6th, 2016. Please ensure that you have an ORCID iD and that it is validated in Editorial Manager. To do this, go to ‘Update my Information’ (in the upper left-hand corner of the main menu), and click on the Fetch/Validate link next to the ORCID field. This will take you to the ORCID site and allow you to create a new iD or authenticate a pre-existing iD in Editorial Manager. Please see the following video for instructions on linking an ORCID iD to your Editorial Manager account: https://www.youtube.com/watch?v=_xcclfuvtxQ\n\n[Note: HTML markup is below. Please do not edit.]\n\nReviewers' comments:\n\nReviewer's Responses to Questions\n\nComments to the Author\n\n1. Is the manuscript technically sound, and do the data support the conclusions?\n\nThe manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented.\n\nReviewer #1: Partly\n\nReviewer #2: No\n\nReviewer #3: Yes\n\n**********\n\n2. Has the statistical analysis been performed appropriately and rigorously?\n\nReviewer #1: N/A\n\nReviewer #2: No\n\nReviewer #3: I Don't Know\n\n**********\n\n3. Have the authors made all data underlying the findings in their manuscript fully available?\n\nThe PLOS Data policy requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data—e.g. participant privacy or use of data from a third party—those must be specified.\n\nReviewer #1: Yes\n\nReviewer #2: No\n\nReviewer #3: Yes\n\n**********\n\n4. Is the manuscript presented in an intelligible fashion and written in standard English?\n\nPLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.\n\nReviewer #1: No\n\nReviewer #2: No\n\nReviewer #3: No\n\n**********\n\n5. Review Comments to the Author\n\nPlease use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)\n\nReviewer #1: -Related work needs to be updated more followed by adding references.\n\n-It is recommended to add a table of past work on clinical decision support.\n\n-Improve the quality of figure1.\n\n-Illustrate your clinical decision support model as figure 2.\n\n-Brief on field of nephrology.\n\n-Justify why authors chose this area of Nephrology while many areas also require urgent clinical decision support.\n\n-The statement in the paper required refrences to quote \"we present here was developed in a project by some of the\n\nauthors of this paper, namely the computer scientists and the clinical experts\"\n\n-Contrast your work with the existing or recent works furnished and prove your model to be most significant.\n\n-Showcase your Risk prediction model as data flow diagram.\n\n-Mention the ability of your model to target other areas in the scope for future work.\n\nReviewer #2: 1. Abstract is too short\n\n2. Introduction need to be improved more add some lines about the motivation about the field of study\n\n3. add major contribution points at the end of the introduction\n\n4. Add literature study section after introduction and cite at least 10 papers also add one table which need to have limitation of the existing field of study.\n\n5. A case study need to be explained more\n\n6. Explain more figure 1\n\n7. Table 1 need to be explained more in detail\n\n8. Add methodology section also work on some more machine learning classifiers\n\n9. Related work is too short\n\n10. Add future directions separate section\n\n11. Add some proper experimental results\n\n12. Overall paper English is too much poor try to improve the language\n\nReviewer #3: In this paper, authors presented a multidisciplinary view on machine learning in medical decision support systems. However, there are some limitations that must be addressed as follows.\n\n1. This work is not presented correctly. The standard format of the research article should be used: abstract, introduction, related work, methodology, results, and conclusion.\n\n2. The abstract is very short and not attractive. The novelty should be clearly discussed. In addition, In the last lines of abstract, the authors should discuss results and highlight in what %age and in what parameters the proposed methodology is better as compared to existing techniques and what is the overall analysis of proposed methodology.\n\n3. In Introduction section, it is difficult to understand the novelty of the presented research work. This section should be modified carefully. In addition, the main contribution should be presented in the form of bullets.\n\n4. More related work should be included about clinical decision support system (‘An intelligent healthcare monitoring framework using wearable sensors and social networking data’, ‘Automatic detection of Alzheimer’s disease progression: An efficient information fusion approach with heterogeneous ensemble classifiers’, ‘ANAF-IoMT: A Novel Architectural Framework for IoMT-Enabled Smart Healthcare System by Enhancing Security Based on RECC-VC’,’ Fine-Tuned DenseNet-169 for Breast Cancer Metastasis Prediction Using Fast AI and 1-Cycle Policy’). In addition, In the last lines of Literature review, highlight what overall technical gaps are observed in existing works that led to the design of proposed methodology.\n\n5. Captions of the Figures and tables not self-explanatory. These captions should be self-explanatory, and clearly explaining the figure and table. Extend the description of the mentioned figures and tables to make them self-explanatory.\n\n6. Figure 1 is blurred, its quality should be improved.\n\n7. Data analysis section should be extended by including more details.\n\n8. The conclusion section should be revised. In addition, the future work should be included.\n\n**********\n\n6. PLOS authors have the option to publish the peer review history of their article (what does this mean?). If published, this will include your full peer review and any attached files.\n\nIf you choose “no”, your identity will remain anonymous but your review may still be made public.\n\nDo you want your identity to be public for this peer review? For information about this choice, including consent withdrawal, please see our Privacy Policy.\n\nReviewer #1: Yes: Dr.Jana Shafi\n\nReviewer #2: No\n\nReviewer #3: No\n\n**********\n\n[NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link \"View Attachments\". If this link does not appear, there are no attachment files.]\n\nWhile revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool, https://pacev2.apexcovantage.com/. PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Registration is free. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email PLOS at gro.solp@serugif. Please note that Supporting Information files do not need this step."
    }
}
{
    "id": "wrong_mix_domain_foundationPlace_00002_1",
    "rank": 83,
    "data": {
        "url": "https://diginomica.com/meet-atomic-human-draft-blueprint-trustworthy-ai",
        "read_more_link": "",
        "language": "en",
        "title": "Meet The Atomic Human - a draft blueprint for trustworthy AI",
        "top_image": "https://diginomica.com/sites/default/files/images/2024-07/Screenshot%202024-07-15%20at%2011.16.58.png",
        "meta_img": "https://diginomica.com/sites/default/files/images/2024-07/Screenshot%202024-07-15%20at%2011.16.58.png",
        "images": [
            "https://diginomica.com/themes/custom/diginomica_theme/logo.png?v=20220726",
            "https://diginomica.com/sites/default/files/styles/square_28/public/user/2023-01/George%20Lawton.png.webp?itok=CAslEQjh 28w, /sites/default/files/styles/square_56/public/user/2023-01/George%20Lawton.png.webp?itok=YYLqhSG8 56w",
            "https://diginomica.com/sites/default/files/styles/scaled_370/public/images/2024-07/Screenshot%202024-07-15%20at%2011.16.58.png.webp?itok=K_uSk89c 370w, /sites/default/files/styles/scaled_740/public/images/2024-07/Screenshot%202024-07-15%20at%2011.16.58.png.webp?itok=GjW06B00 740w",
            "https://diginomica.com/themes/custom/diginomica_theme/images/pwa/placeholder.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            "Machine intelligence and AI",
            "Ethics",
            "Audio"
        ],
        "tags": null,
        "authors": [
            "George Lawton"
        ],
        "publish_date": "2024-07-15T03:19:48-07:00",
        "summary": "",
        "meta_description": "Neil D. Lawrence’s new book explains the ideas and misconceptions shaping modern AI. Building trustworthy AI needs to start with considerations of power...",
        "meta_lang": "en",
        "meta_favicon": "/themes/custom/diginomica_theme/images/favicon/apple-touch-icon.png",
        "meta_site_name": "diginomica",
        "canonical_link": "https://diginomica.com/meet-atomic-human-draft-blueprint-trustworthy-ai",
        "text": "The Atomic Human - Deep Mind Professor of Machine Learning Neil D. Lawrence’s new book - may turn out to be one of the most important works for understanding what we are calling AI these days. It uses extensive analogies to explain a complex topic with no equations and should be on the reading list of anyone trying to make sense of AI hype, cautions, and opportunities. But most importantly, it breaks down the components or ‘atoms’ of what makes human intelligence so special and how we can trustfully build on it rather than replace it or wipe it out as some fear.\n\nIt elaborates on efforts to build machine intelligence and how they were accelerated by and shaped World War II. It approaches the topic by starting with the role that planning, communication, and collective decision-making played in D-Day. Fittingly, it was published 80 years after Eisenhower sent a million men across the English Channel based on intelligence from Bletchley Park.\n\nAlthough the book is about the interplay between what is popularly called AI, it's really about something much deeper. As Lawrence signed my copy at the Open Manifesto Report Launch, he said the book focuses on explaining AI but does not make any recommendations or prescriptions about AI, which I found rather curious.\n\nA clue to this distinction can be found early in the book when he explains:\n\nBut the Artificial Intelligence we are peddling, the techniques we are using, simply combine very large data sets and computers. It is a mix of advanced computation and statistics.\n\nHe argues this is a better way of framing our understanding than debating super-intelligence extinction fears fueled by Elon Musk, Bill Gates, and Stephen Hawking, which start with categorical errors he elaborates on. We must be careful to separate intelligence as an entity from intelligence as a property. There is a large gap between what Lawrence is building in practice and what he asks about. Crossing this gap requires inquiry into who we are and how we can wield machine intelligence as a mirror to our own cognition. He explains:\n\nBy using the machine as a mirror to reflect who we are, we can better understand what we want, and this understanding should enable us to decide how we want our societies to function in this new age of AI, and how individuals in those societies can carry out their roles in a way that balances their personal liberty with the social need for coherence and security.\n\nReflective and reflexive\n\nOne compelling takeaway is that humans engage in reflective and reflexive intelligence through machine interfaces. Reflective intelligence is the part that builds plans and writes stories. Reflexive intelligence is the part that can deftly avoid a cyclist in the motorway by quickly identifying uncertainty and automatically moving the car out of the way. This also involves a learned feel of the car's controls and the significance of a cyclist you are about to hit.\n\nIn the case of WWII, reflective intelligence allowed Bletchley Park to build logic manipulation machines and supportive human processes that allowed them to decode German communications. This planted the seeds for Alan Turing’s later innovations in universal computers. These innovations build on prior efforts to develop mathematical systems of logic, reasoning, and planning. But plans can fall apart when met with uncertainty.\n\nReflexive intelligence allowed engineers to build automated control systems for steering ships and planes. During the war, Norbert Wiener developed anti-aircraft control systems that could adapt to the uncertainties of a pilot’s evasive maneuvering. This planted the seeds for cybernetics and modern work on autonomous systems that can adapt to uncertainty.\n\nThe human-machine interface is another crucial part often left out of discussions about AI. Lawrence dives into the challenges engineers faced building aircraft control systems that allowed pilots to navigate the chaotic turbulence inside large clouds.\n\nLawrence paints a picture of how these were combined in the first moon landing, during which control was shared across mission control, onboard computers, and human pilots. Reflective systems helped plan and program a flight path to the moon and back. Reflexive systems steered the mission through space. But control reverted to onboard humans to decide how to proceed after a computer fault. Also, the original plan failed to account for large boulders in the original landing site, so Neil Armstrong had to pilot the craft into the sea of tranquility manually.\n\nPerils of System Zero\n\nAnother section elaborates on the perils that can arise when AI is imposed on us that pass the lab's safety testing but fail to consider new dangers arising in a larger context. It starts with a cautionary tale about the first tests of the drug Theralizumab on eight patients in 2006. Although the drug had passed all safety checks on monkeys, the patients who received a “low dose” all suffered significant organ damage within a few hours when their immune systems began attacking their own bodies.\n\nLawrence likens this as akin to the ways some early AI systems seem safe and promote engagement on social media. But they can also result in mistrust, such as the way Russian agents leveraged systemic weaknesses to influence the 2016 American elections. He dives into plenty of other ways that the automation of seemingly inconsequential decisions can result in bad results.\n\nThe narrative builds on System 1 and 2 thinking popularized in Daniel Kahneman’s Thinking Fast and Slow. System 1 is fast reflexive thinking that helps us automatically make quick decisions ranging from driving a car to an expert making a snap judgment relevant to their domain. System 2 is slow, reflective thinking involved in solving problems and learning new skills.\n\nSystem 0 thinking is a slower and more centralized process, akin to how our immune systems learn the patterns of pathogens and customize immune cells in the thymus gland to attack them. The digital variant is modern machine learning pipelines that train models on centralized infrastructure deployed across the web to automate decision-making. Lawrence says this creates a new peril that we are ill-equipped to navigate:\n\nThe machine can pre-empt our behavior and exploit the blindspots in our sensorimotor intelligence. It has become a new level of cognition we can think of as System Zero.\n\nThese systems can process data millions of times faster than we do and then use statistical survey techniques to manipulate us by restricting our view of the world. The process of perturbing our environment in small, carefully selected ways is similar to the virtual world projected onto enslaved humans in “The Matrix,” but far more subtle and efficient. Our current digital System Zero does not understand social context, prejudice, or empathize. He wonders if we want to be like those men in the Theralizumab trial, trusting that this systemic intervention is safe:\n\nWith social media and the next wave of generative AI, we are dosing ourselves with System Zero and testing it against our societal health, but the nature of social health is not easily quantified. That makes it hard to measure the consequences of the phase-one trials for System Zero.\n\nBuilding a foundation for trust\n\nThe book concludes by analyzing what it would take to enable these systems to foster rather than erode trust. The often touted analogy of AI going rogue as a kind of atomic weaponry that might wipe us out is not helpful and points to the wrong problem we face. A more useful metaphor is analogous to the threat we pose to our ecology.\n\nThe real problem is the information asymmetry across three types of information typography: our ecology, human culture and institutions, and new mechanisms for sharing and sorting information. Our ecology is shaped by the complex interplay of natural intelligences evolving for billions of years but at a relatively slow pace of genetic information exchange. But this can be undermined by clumsy action in our human layer. Our emerging computer systems may similarly undermine the diversity and complexity of the world around us:\n\nBy operating at such short time scales it gains access to large quantities of information from humans and could be as damaging to our cultural ecosystem as our actions have been to our natural ecosystem.\n\nOne issue is the power asymmetries new tools impose between the gatekeepers and the rest of us. The second is the tendency to trust confident-sounding tools to automate more of our decisions since we don’t understand their limitations.\n\nNew institutions that collectivize data rights and operate as pressure groups could help us gain our data's benefits without ceding too much control to digital oligarchs. On the control side, we need to build software tools and engineering practices that allow more people to participate in their creation and the decisions they automate. In a description of his work on the Data Science Africa project, he explains:\n\nBy empowering the user to design their own software and through delivering it in explainable maintainable systems, we can rebalance power from the software guilds to the people and institutions that make up open society.\n\nMy take\n\nA few weeks ago, I was at the opening of Salesforce’s new AI center in London, which promises to make AI accessible to less technical users. I marveled at the beauty of the Tate Modern Gallery of Art out the window. At the time, I had little appreciation of its past other than it bore the smokestacks from its previous life as a power plant.\n\nThe Atomic Human connected the gallery across another generation of its previous life as the Albion Mill, which monopolized grain milling in London, bankrupted local millers, and burned to the ground in suspicious circumstances. It may have been destroyed by disgruntled millers or a fault in its operation. But what made the steam-powered mill so powerful and lucrative was the adoption of a new mechanical intelligence feedback loop developed by James Watt that greatly boosted its efficiency.\n\nLooking at the building these days, it's hard to imagine its contentious past and how this mechanical giant may have been destroyed by the disaffected or its own undoing. Regardless, it is now an institution of hope for sharing novel world perspectives. And it seems fitting that Salesforce is trying to do something similar for AI in a new office overlooking the site. This gives hope that despite the problems and perils we now see with the current crop of AI, there is still an opportunity to lay the foundation for a better future."
    }
}
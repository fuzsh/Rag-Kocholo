{
    "id": "wrong_mix_domainrange_subsidiary_00121_2",
    "rank": 64,
    "data": {
        "url": "https://www.buurst.com/category/uncategorized/",
        "read_more_link": "",
        "language": "en",
        "title": "Uncategorized Archives – Buurst",
        "top_image": "https://www.buurst.com/wp-content/uploads/2023/05/cropped-SoftNAS-_Word-32x32.png",
        "meta_img": "https://www.buurst.com/wp-content/uploads/2023/05/cropped-SoftNAS-_Word-32x32.png",
        "images": [
            "https://px.ads.linkedin.com/collect/?pid=1846724&fmt=gif",
            "https://www.buurst.com/wp-content/uploads/2020/08/BuurstTM.png",
            "https://www.buurst.com/wp-content/uploads/2021/01/SoftNAS-5-Blog-Featured-Image_CEOSpecial-1200x628-1-1080x629.png",
            "https://www.buurst.com/wp-content/uploads/2020/11/BLOGHEADERS_UploadMigration-1200x628-1-1080x628.png",
            "https://www.buurst.com/wp-content/uploads/2020/11/Untitled-2-01.png",
            "https://www.buurst.com/wp-content/uploads/2020/10/BLOGHEADERS_Storage-1200x628-1-1080x628.png",
            "https://www.buurst.com/wp-content/uploads/2020/10/SAP-HANA_HANA.png",
            "https://www.buurst.com/wp-content/uploads/2020/09/BLOGHEADER-OilandGas1200x628-1080x675.png",
            "https://www.buurst.com/wp-content/uploads/2020/09/BLOGIMG-Clouds-1.jpg",
            "https://www.buurst.com/wp-content/uploads/2020/09/BLOGIMG-Code.jpg",
            "https://www.buurst.com/wp-content/uploads/2020/09/BLOGIMG-Meet.jpg",
            "https://www.buurst.com/wp-content/uploads/2020/09/BLOGHEADERS_UploadMigration-1200x628-1-1080x628.png",
            "https://www.buurst.com/wp-content/uploads/2020/09/BLOGIMG-IOPs-1.png",
            "https://www.buurst.com/wp-content/uploads/2020/09/BLOGHEADERS_Storage-1200x628-3-1080x628.png",
            "https://www.buurst.com/wp-content/uploads/2020/12/Buurst-LogoTM.png",
            "https://www.buurst.com/wp-content/uploads/2021/03/Social-Media-Icons_Twitter.png",
            "https://www.buurst.com/wp-content/uploads/2021/03/Social-Media-Icons_LinkedIn.png",
            "https://www.buurst.com/wp-content/uploads/2021/03/Social-Media-Icons_facebook.png",
            "https://www.buurst.com/wp-content/uploads/2021/03/Social-Media-Icons_vimeo.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Buurst Staff"
        ],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "https://www.buurst.com/wp-content/uploads/2023/05/cropped-SoftNAS-_Word-32x32.png",
        "meta_site_name": "Buurst",
        "canonical_link": "https://www.buurst.com/category/uncategorized/",
        "text": "As a unified solution, SoftNAS provides an excellent base for Microsoft Windows Server deployments by providing iSCSI for Microsoft SQL Server, and network file system (NFS) or server message block (SMB) file storage for Microsoft Windows client access. This reduces complications for companies that are migrating from the data center to the cloud. While also enjoying the benefits of dedicated storage channels.\n\nTake advantage of high-performance storage by using SoftNAS and Microsoft SQL Server together.\n\nProtect your data and keep your SQL Server highly available while scaling up without limits.\n\nMicrosoft SQL Server datastores consist of two fundamental data structure containers: data files and log files. Storing the data files and log files on separate disk drives distributes I/O activities. Backups should be stored in different storage located at a different availability zone.\n\nSoftNAS use the concept of a storage pool, which is a collection of storage and cache devices exclusively assigned to the pool. Storage is provisioned in shared file systems or block storage, and it is backed by intent-log and write-cache devices\n\nSoftNAS allow data on a disk to be compressed automatically and transparently to the application. Various levels of compression are available, with increasing performance impact as the data compression level increases.\n\nSoftNAS offers a broad range of instance sizes and region availability.\n\nIt’s important to select the right instance size to configure a storage solution that is the right combination of performance and price for your use case. General guidance and a guide tool are provided to help you select an instance size for your workload to get your project started. Our instance size calculation tool is available directly on our main website:\n\nFor extremely heavy workloads, increase cache memory with “High-Memory Instances” and/or use EBS-Optimized and Provisioned IOPS to provide better control over available IOPS.\n\nThe above tool is designed to help new users find the right initial instance size for their workload quickly and easily. Buurst always recommends further analysis and testing of their selected instance until workload characteristics are fully understood. This will allow the customer to then refine their instance size selection to the perfect balance of performance and cost.\n\nAvailable Memory:\n\nSoftNAS uses around 1 GB of RAM for the kernel and system operation. Memory beyond 1 GB is available for use as cache memory, which greatly improves overall system performance and response time – more memory = better performance, to a point. If application workloads involve a high number of small, random I/O requests, then cache memory will provide the best performance increase by reducing random disk I/O to a minimum. If running a SQL database application, cache memory will greatly improve query performance by keeping tables in memory. At a minimum, 2 GB of RAM will yield around 1 GB for cache. For best results, start with 4 GB or more RAM. With deduplication, add 1 GB of RAM per terabyte of deduplicated data (to keep deduplication look-up tables in RAM)\n\nCPU:\n\nSoftNAS needs a minimum of 2 CPUs for normal operation. To maintain peak performance when using the Compression feature, add CPUs (e.g.,4 CPU) if CPU usage is observed at 60% or greater on average.\n\nNetwork:\n\nIn EC2, SoftNAS uses Elastic Block Storage (EBS), which are disks running across the network in a SAN (storage area network) configuration. This means all disk I/O travels across a shared network connecting the EC2 computing instance with the SAN. This makes network I/O an important factor in SoftNAS® environment performance.\n\nMultiple Performance & Scale Options:\n\nEC2 offers Fixed Performance Instances (e.g. m3, c3, and r3) as well as Burstable Performance Instances (e.g. t2) for occasional heavy use over baseline. EC2 also offers many instance sizes and configurations. Consider all potential networking requirements when choosing instance type. Purchasing models include On-Demand, Reserved, and Spot Instances.\n\nFor more information on deploying SoftNAS for High Performance SQL, see the deployment guide.\n\nSAP HANA is a platform for data analysis and business analytics. HANA can provide insights from your data—faster than traditional RDMS systems. Performance is essential for SAP HANA because it helps to provide information more quickly. SAP HANA is optimized to work with real-time data, so performance is a significant factor.\n\nTop 5 reasons to choose Buurst SoftNAS SAP HANA\n\n1. Superior performance for processing data\n\n2. Maximum scale to process as much data as possible\n\n3. High reliability\n\n4. Low cost\n\n5. Integration into existing infrastructure\n\nAll data and metadata for the SAP HANA system are in shared objects. These objects are copied from data tables to logical tables and accessed by SAP HANA software. So, as this information is grown, the impact on performance grows as well. By using SoftNAS to address performance bottlenecks, SoftNAS can accelerate operations that otherwise might be less efficient.\n\nFor example, the copy operation will undoubtedly be faster if you deploy storage with read-cache. Read cache is implemented with NVMe or SSD drives and helps copy the parameters from source tables to specialization indexes. Tables are frequently written, such as ETL operations, the technique of storing data and logs in SoftNAS reduces the complexity and resiliency, which reduces the overall risk for data loss. Cloud NAS can also improve your data requests’ response times and other critical factors like resource management and disaster recovery.\n\nStoring your data and logs volumes on a NAS would certainly improve resilience. Using a NAS with RAID also allows for added redundancy if something goes wrong with one of your drives. Utilizing RAID will not only help to ensure your data is safe, but it will also allow you to maintain a predictable level of performance when it comes time to scale up your software.\n\nPartitioning of data volumes allows for efficient data distribution and high availability. Partitioning will also help you scale up your SAP HANA performance, which can be a challenge with only one large storage pool. Partitioning will involve allocating more than one volume to each table and moving the information across volumes over time.\n\nSAP HANA supports persistent memory for database tables. Persistent memory retains data in memory between server reboots. Loading data requires time to boot and load the data and then refresh the data. With SAP HANA deployed with SoftNAS storage, loading times are not a problem at all. The amount of data you consume will significantly benefit from persistence memory. While reading (basically accessing) records from persistent memory takes a long time, writing to the memory works much better with SoftNAS.\n\nSoftNAS data snapshots enable SAP HANA backup multiple times a day without the overhead of file-based solutions, eliminating the need for lengthy consistency check times when backing up and dramatically reducing the time to restore the data. Schedule multiple backups a day with restore and recovery operations in a matter of minutes.\n\nCPU and IO offloading help to support high-performance data processing. Reducing CPU and IO overhead effectively serves to increase database performance. By moving backup processes into the storage network, we can free up server resources to enable higher throughput and a lower Total Cost of Ownership (TCO).\n\nYou want to deploy SAP HANA because your business needs access to real-time information that allows you to make timely decisions with maximal value. SoftNAS is a cloud NAS that will enable you to develop real-time business applications, connecting your company with customers, partners, and employees in ways that you have never imagined before.\n\nTrend One: The Cloud\n\nWe all know drilling dry holes is no longer acceptable. Leveraging geospatial application technology like Halliburton Decision Space 365® (Landmark), Schlumberger Petrel®, or IHS Markit Kingdom® have become table stakes to ensuring you not only know the best place to drill, you know the right place to drill.\n\nMost energy companies have invested $10s of millions of dollars into building world-class data centers dedicated to this work. These investments are essential and are of exact strategic value. But the costs keep going up, and you need more and more IT and security people to manage your investment (it’s a dangerous world out there). Worst of all, you get to replace this investment every three years if you want to stay competitive. What do you do when you don’t have $20M to retool? Today there is an answer: Move the workloads to the cloud.\n\nWhy the cloud? You get the fastest and most up to date processing power without the need to buy the infrastructure. Moving to the cloud lets you move your investment from capital expense to an operating expense that you pay for by the hour, all backed and secured by companies like Microsoft (Azure) and Amazon (AWS). Moving to the cloud is happening today, and it’s happening fast. We see our Energy business grow more quickly this year than over the past seven years. There is a tipping point for the cloud and 2020.\n\nTrend Two: Lift and Shift\n\nSaving investment dollars, closing datacenter, and simplifying your IT footprint is a crucial goal of moving to the cloud. Companies often take two approaches. The first approach is to move specific workloads that are data and processing-intensive. Geospatial applications are great examples. The second approach is to focus on closing your datacenters. Many companies have thousands of applications in their data centers, and the prospect of moving these can be daunting. Migrating an application or an entire data center is commonly referred to as Lift & Shift. The good news is that 80% of these apps will go with no or very few issues. So now, what do you do with the 20% of the applications that are hard to move? If the goal is to close the data center, you can’t finish till all the applications are migrated. If the data center is still open, you cant achieve the cost savings. Your IT infrastructure will be more complicated, making the hard to move apps move is essential.\n\nMany companies go down the path of rewriting these hard to move applications, but it’s unnecessary. Mostly these apps don’t work in the cloud because they leverage protocols that are not supported in the cloud or are latency-sensitive. The number one most common protocol that is unsupported in the cloud is iSCSI. There are solutions here, and it’s essential to leverage them for these hard to move apps. Buurst SoftNAS is a great example.\n\nLegacy SQL Server workloads often fall into this category, and the countless instances across an enterprise could take all your DevOps resources years to rework. Don’t let your DevOps resources work on legacy workloads. These expensive and vital resources should be building the applications of the future. Leverage cloud storage solutions that support the protocols you’re using and move forward.\n\nTrend Three: Cross-platform business partnerships\n\nSo this trend is a little controversial but essential. If you ask any cloud vendor about a multi-cloud strategy, they will always tell you just to pick one, and make sure it’s them. There are some excellent reasons to choose one cloud, and it’s worth spending time to look before you make the decision. The trend we’re seeing is to pick a solution that works in different clouds, so moving will not require reengineering your infrastructure.\n\nA VMware hypervisor is a great example. You probably use it in your data center today, so moving it is a straight forward effort. Storage is often overlooked and becomes the “lock-in” element of choice for cloud vendors. Making sure you know what makes a cloud sticky is essential. If you know going in, you can avoid making costly mistakes. Fortunately, there are many great partners like SoftwareONE, Kaskade.Cloud, CANCOM, VSTECS, or LANStatus, to name a few with cloud architects that can help you manage this part of your transition.\n\nBP recently said that oil demand may never rebound to pre-pandemic highs as the world shifts to renewables. I don’t know if that’s true, but for now, there is a clear need to rethink spending and change resources to take advantage of the learning from other industries. No one ever wants to be first to take the plunge. Fortunately, companies like Halliburton, Schlumberger, Petronas, IHS Markit, and ExxonMobil have all moved and are leveraging these strategies. Come on in, the water’s fine.\n\nIOPS vs Latency\n\nLatency is a really critical factor, never accept those huge IOPS numbers without having a look at the latency figures.\n\nLatency is how long it takes for a single IOPS to happen, by increasing the workload the storage hardware including the controller, caching, RAM, CPU, etc will try to keep the latency consistent but things are not that ideal, at certain huge number of IOPS things will go out of control and the storage appliance hardware will get exhausted and more busy, so a delay serving the data will start getting noticed by the application and problems will start to happen.\n\nDatabases for example are very latency sensitive workloads, usually they need small latency [5ms or lower] especially during writing otherwise there will be a huge performance degradation and business impact.\n\nSo if your business is growing and you noticed degradation in your database performance, You don’t only need a storage with higher IOPS rate but with lower latency as well which leads us to another side point which is storage flexibility that Buurst can help you with. Just few steps you can upgrade your storage with whatever numbers that satisfies your workload\n\nFrom between the lines, that means that a backup is an independent copy of the datai.e. stored on a different media. That isa very criticalconcept, and we will know why soon.\n\nThe 3-2-1 backup rule\n\n1. Have at least 3 copies of your data\n\nThree copies mean your original data that you are using plus two additional backups. Usually one copy in hand in case of any localized failure, you can restore it immediately\n\n2. Keep these backups on 2 different media\n\nThese backups should be stored on two different media types or technologies since the same media type may have the same life span, and that is risky as you may lose both backups at the same time. The cloud can take care of that as your data is distributed on several medias by default\n\n3. Store 1 backup offsite\n\nThis copy should be far away to be safe enough and survive any catastrophes like fires, earthquakes or wars that can remove a certain area from the map. I believe in the future this copy should be sent to another planet or even another solar system!\n\nHow Buurst can help you achieve the 3-2-1 backup rule?\n\nSnaprep, is a technology based on snapshots replicating between two nodes, the snapshot process has zero overhead on the performance and the storage space, it will be sent after compression to another independent node in another availability zone which is a different datacenter.\n\nBoth nodes can have independent automatic snapshot schedule that can protect against data deletion. A SnapClone of any snapshots will allow you to serve/restore the data at the point in time it was taken\n\nThe second node can be a redundant node and can serve the data in case of any failures and that will be discussed in a different article.\n\nSo now we have two independent copies of the data, how about the third one?\n\nYou can use the second node as a backup source not to disturb the other node. You can integrate it with any backup solution you have, or you can use a third Buurst node [in a different region] to create a fully independent Disaster Recovery site by replicating the data to it using rsync or zfs send/receive etc. This will allow for a faster access of your data in case of an unforeseen failures which will eliminate time wastage when restoring from tapes (of course it is a time-budget trade off)\n\nSo, by doing that we have achieved the 3-2-1 rule, by having 2 more copies of data one of them in a different region, but the question is: Is the 3-2-1 rule enough?"
    }
}
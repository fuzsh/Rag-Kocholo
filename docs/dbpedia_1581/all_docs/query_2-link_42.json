{
    "id": "dbpedia_1581_2",
    "rank": 42,
    "data": {
        "url": "https://developers.google.com/machine-learning/glossary",
        "read_more_link": "",
        "language": "en",
        "title": "Machine Learning Glossary",
        "top_image": "https://www.gstatic.com/devrel-devsite/prod/v35e3d347a323c673294794cf0b643760fd66bb529efbd7dccaa22518acef0297/developers/images/opengraph/white.png",
        "meta_img": "https://www.gstatic.com/devrel-devsite/prod/v35e3d347a323c673294794cf0b643760fd66bb529efbd7dccaa22518acef0297/developers/images/opengraph/white.png",
        "images": [
            "https://developers.google.com/static/machine-learning/glossary/images/relu.svg",
            "https://developers.google.com/static/machine-learning/glossary/images/sigmoid.svg",
            "https://developers.google.com/static/machine-learning/glossary/images/ActivationFunction_sigmoid.png",
            "https://developers.google.com/static/machine-learning/glossary/images/AUCIdealClassSeparation.png",
            "https://developers.google.com/static/machine-learning/glossary/images/AUCSetupPNPNPN.png",
            "https://developers.google.com/static/machine-learning/glossary/images/AUCSetupTypical.png",
            "https://developers.google.com/static/machine-learning/glossary/images/AUC1_0.png",
            "https://developers.google.com/static/machine-learning/glossary/images/AUC0_5.png",
            "https://developers.google.com/static/machine-learning/glossary/images/ROCTypicalGraph.png",
            "https://developers.google.com/static/machine-learning/glossary/images/bias.png",
            "https://developers.google.com/static/machine-learning/glossary/images/bounding_box.jpg",
            "https://developers.google.com/static/machine-learning/glossary/images/Cluster.svg",
            "https://developers.google.com/static/machine-learning/glossary/images/RingCluster.svg",
            "https://developers.google.com/static/machine-learning/glossary/images/condition.png",
            "https://developers.google.com/static/machine-learning/glossary/images/Convergence.png",
            "https://developers.google.com/static/machine-learning/glossary/images/convex_functions.png",
            "https://developers.google.com/static/machine-learning/glossary/images/nonconvex_function.svg",
            "https://developers.google.com/static/machine-learning/glossary/images/convex_set.png",
            "https://developers.google.com/static/machine-learning/glossary/images/nonconvex_set.png",
            "https://developers.google.com/static/machine-learning/glossary/images/ConvolutionalFilter33.svg",
            "https://developers.google.com/static/machine-learning/glossary/images/AnimatedConvolution.gif",
            "https://developers.google.com/static/machine-learning/glossary/images/ConvolutionalLayerInputMatrix.svg",
            "https://developers.google.com/static/machine-learning/glossary/images/ConvolutionalLayerFilter.svg",
            "https://developers.google.com/static/machine-learning/glossary/images/ConvolutionalLayerOperation.svg",
            "https://developers.google.com/static/machine-learning/glossary/images/decision_boundary.png",
            "https://developers.google.com/static/machine-learning/glossary/images/DecisionTree.png",
            "https://developers.google.com/static/machine-learning/glossary/images/Distributions.png",
            "https://developers.google.com/static/machine-learning/glossary/images/One-HotRepresentationOfTreeSpecies.png",
            "https://developers.google.com/static/machine-learning/glossary/images/EmbeddingBaobab.png",
            "https://developers.google.com/static/machine-learning/glossary/images/FeatureVector.png",
            "https://developers.google.com/static/machine-learning/glossary/images/GeneralizationCurveSmooth.png",
            "https://developers.google.com/static/machine-learning/glossary/images/HiddenLayerBigPicture.png",
            "https://developers.google.com/static/machine-learning/glossary/images/hinge-loss.svg",
            "https://developers.google.com/static/machine-learning/glossary/images/information-gain.png",
            "https://developers.google.com/static/machine-learning/glossary/images/InputLayer.png",
            "https://developers.google.com/static/machine-learning/glossary/images/iou_van_gogh_bounding_boxes.jpg",
            "https://developers.google.com/static/machine-learning/glossary/images/iou_van_gogh_intersection.jpg",
            "https://developers.google.com/static/machine-learning/glossary/images/iou_van_gogh_union.jpg",
            "https://developers.google.com/static/machine-learning/glossary/images/k-folds.png",
            "https://developers.google.com/static/machine-learning/glossary/images/DogDimensions.svg",
            "https://developers.google.com/static/machine-learning/glossary/images/DogDimensionsKMeans.svg",
            "https://developers.google.com/static/machine-learning/glossary/images/Layers.png",
            "https://developers.google.com/static/machine-learning/glossary/images/Leaf.png",
            "https://developers.google.com/static/machine-learning/glossary/images/LossCurveSmooth.png",
            "https://developers.google.com/static/machine-learning/glossary/images/NeuralNetwork.png",
            "https://developers.google.com/static/machine-learning/glossary/images/Neurons.png",
            "https://developers.google.com/static/machine-learning/glossary/images/node.png",
            "https://developers.google.com/static/machine-learning/glossary/images/non-binary-conditions.png",
            "https://developers.google.com/static/machine-learning/glossary/images/LinearVsNonlinear.png",
            "https://developers.google.com/static/machine-learning/glossary/images/OOBevaluation.png",
            "https://developers.google.com/static/machine-learning/glossary/images/OutputLayer.png",
            "https://developers.google.com/static/machine-learning/glossary/images/Pax.png",
            "https://developers.google.com/static/machine-learning/glossary/images/Perceptron.svg",
            "https://developers.google.com/static/machine-learning/glossary/images/PoolingStart.svg",
            "https://developers.google.com/static/machine-learning/glossary/images/PoolingConvolution.svg",
            "https://developers.google.com/static/machine-learning/glossary/images/QuantileBucketing.svg",
            "https://developers.google.com/static/machine-learning/glossary/images/ReLU.png",
            "https://developers.google.com/static/machine-learning/glossary/images/RNN.svg",
            "https://developers.google.com/static/machine-learning/glossary/images/ROCSetupIdealDistributionNoClassificationThreshold.png",
            "https://developers.google.com/static/machine-learning/glossary/images/ROCcurvePerfect.png",
            "https://developers.google.com/static/machine-learning/glossary/images/ROCWorstCaseDistribution.png",
            "https://developers.google.com/static/machine-learning/glossary/images/ROCcurveWorstCase.png",
            "https://developers.google.com/static/machine-learning/glossary/images/ROCTypicalGraph.png",
            "https://developers.google.com/static/machine-learning/glossary/images/root.png",
            "https://developers.google.com/static/machine-learning/glossary/images/self-attention.png",
            "https://developers.google.com/static/machine-learning/glossary/images/sigmoid.svg",
            "https://developers.google.com/static/machine-learning/glossary/images/One-HotRepresentationOfASparseFeature.png",
            "https://developers.google.com/static/machine-learning/glossary/images/One-HotRepresentationOfWordsInASentence.png",
            "https://developers.google.com/static/machine-learning/glossary/images/staged-training.png",
            "https://developers.google.com/static/machine-learning/glossary/images/AnimatedConvolution.gif",
            "https://developers.google.com/static/machine-learning/glossary/images/Simple_RNN.svg",
            "https://developers.google.com/static/machine-learning/glossary/images/TrainingLoss.png",
            "https://www.gstatic.com/devrel-devsite/prod/v35e3d347a323c673294794cf0b643760fd66bb529efbd7dccaa22518acef0297/developers/images/lockup-google-for-developers.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "https://www.gstatic.com/devrel-devsite/prod/v35e3d347a323c673294794cf0b643760fd66bb529efbd7dccaa22518acef0297/developers/images/favicon-new.png",
        "meta_site_name": "Google for Developers",
        "canonical_link": "https://developers.google.com/machine-learning/glossary",
        "text": "This glossary defines general machine learning terms, plus terms specific to TensorFlow.\n\nA\n\nablation\n\nA technique for evaluating the importance of a feature or component by temporarily removing it from a model. You then retrain the model without that feature or component, and if the retrained model performs significantly worse, then the removed feature or component was likely important.\n\nFor example, suppose you train a classification model on 10 features and achieve 88% precision on the test set. To check the importance of the first feature, you can retrain the model using only the nine other features. If the retrained model performs significantly worse (for instance, 55% precision), then the removed feature was probably important. Conversely, if the retrained model performs equally well, then that feature was probably not that important.\n\nAblation can also help determine the importance of:\n\nLarger components, such as an entire subsystem of a larger ML system\n\nProcesses or techniques, such as a data preprocessing step\n\nIn both cases, you would observe how the system's performance changes (or doesn't change) after you've removed the component.\n\nA/B testing\n\nA statistical way of comparing two (or more) techniques—the A and the B. Typically, the A is an existing technique, and the B is a new technique. A/B testing not only determines which technique performs better but also whether the difference is statistically significant.\n\nA/B testing usually compares a single metric on two techniques; for example, how does model accuracy compare for two techniques? However, A/B testing can also compare any finite number of metrics.\n\naccelerator chip\n\n#GoogleCloud\n\nA category of specialized hardware components designed to perform key computations needed for deep learning algorithms.\n\nAccelerator chips (or just accelerators, for short) can significantly increase the speed and efficiency of training and inference tasks compared to a general-purpose CPU. They are ideal for training neural networks and similar computationally intensive tasks.\n\nExamples of accelerator chips include:\n\nGoogle's Tensor Processing Units (TPUs) with dedicated hardware for deep learning.\n\nNVIDIA's GPUs which, though initially designed for graphics processing, are designed to enable parallel processing, which can significantly increase processing speed.\n\naccuracy\n\n#fundamentals\n\nThe number of correct classification predictions divided by the total number of predictions. That is:\n\n$$\\text{Accuracy} = \\frac{\\text{correct predictions}} {\\text{correct predictions + incorrect predictions }}$$\n\nFor example, a model that made 40 correct predictions and 10 incorrect predictions would have an accuracy of:\n\n$$\\text{Accuracy} = \\frac{\\text{40}} {\\text{40 + 10}} = \\text{80%}$$\n\nBinary classification provides specific names for the different categories of correct predictions and incorrect predictions. So, the accuracy formula for binary classification is as follows:\n\n$$\\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}} {\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}}$$\n\nwhere:\n\nTP is the number of true positives (correct predictions).\n\nTN is the number of true negatives (correct predictions).\n\nFP is the number of false positives (incorrect predictions).\n\nFN is the number of false negatives (incorrect predictions).\n\nCompare and contrast accuracy with precision and recall.\n\nClick the icon for additional notes.\n\nAlthough a valuable metric for some situations, accuracy is highly misleading for others. Notably, accuracy is usually a poor metric for evaluating classification models that process class-imbalanced datasets.\n\nFor example, suppose snow falls only 25 days per century in a certain subtropical city. Since days without snow (the negative class) vastly outnumber days with snow (the positive class), the snow dataset for this city is class-imbalanced. Imagine a binary classification model that is supposed to predict either snow or no snow each day but simply predicts \"no snow\" every day. This model is highly accurate but has no predictive power. The following table summarizes the results for a century of predictions:\n\nCategory Number TP 0 TN 36499 FP 0 FN 25\n\nThe accuracy of this model is therefore:\n\naccuracy = (TP + TN) / (TP + TN + FP + FN) accuracy = (0 + 36499) / (0 + 36499 + 0 + 25) = 0.9993 = 99.93%\n\nAlthough 99.93% accuracy seems like very a impressive percentage, the model actually has no predictive power.\n\nPrecision and recall are usually more useful metrics than accuracy for evaluating models trained on class-imbalanced datasets.\n\nSee Classification: Accuracy, recall, precision and related metrics in Machine Learning Crash Course for more information.\n\naction\n\n#rl\n\nIn reinforcement learning, the mechanism by which the agent transitions between states of the environment. The agent chooses the action by using a policy.\n\nactivation function\n\n#fundamentals\n\nA function that enables neural networks to learn nonlinear (complex) relationships between features and the label.\n\nPopular activation functions include:\n\nReLU\n\nSigmoid\n\nThe plots of activation functions are never single straight lines. For example, the plot of the ReLU activation function consists of two straight lines:\n\nA plot of the sigmoid activation function looks as follows:\n\nClick the icon to see an example.\n\nIn a neural network, activation functions manipulate the weighted sum of all the inputs to a neuron. To calculate a weighted sum, the neuron adds up the products of the relevant values and weights. For example, suppose the relevant input to a neuron consists of the following:\n\ninput value input weight 2 -1.3 -1 0.6 3 0.4\n\nThe weighted sum is therefore:\n\nweighted sum = (2)(-1.3) + (-1)(0.6) + (3)(0.4) = -2.0\n\nSuppose the designer of this neural network chooses the sigmoid function to be the activation function. In that case, the neuron calculates the sigmoid of -2.0, which is approximately 0.12. Therefore, the neuron passes 0.12 (rather than -2.0) to the next layer in the neural network. The following figure illustrates the relevant part of the process:\n\nSee Neural networks: Activation functions in Machine Learning Crash Course for more information.\n\nactive learning\n\nA training approach in which the algorithm chooses some of the data it learns from. Active learning is particularly valuable when labeled examples are scarce or expensive to obtain. Instead of blindly seeking a diverse range of labeled examples, an active learning algorithm selectively seeks the particular range of examples it needs for learning.\n\nAdaGrad\n\nA sophisticated gradient descent algorithm that rescales the gradients of each parameter, effectively giving each parameter an independent learning rate. For a full explanation, see this AdaGrad paper.\n\nagent\n\n#rl\n\nIn reinforcement learning, the entity that uses a policy to maximize the expected return gained from transitioning between states of the environment.\n\nMore generally, an agent is software that autonomously plans and executes a series of actions in pursuit of a goal, with the ability to adapt to changes in its environment. For example, an LLM-based agent might use an LLM to generate a plan, rather than applying a reinforcement learning policy.\n\nagglomerative clustering\n\n#clustering\n\nSee hierarchical clustering.\n\nanomaly detection\n\nThe process of identifying outliers. For example, if the mean for a certain feature is 100 with a standard deviation of 10, then anomaly detection should flag a value of 200 as suspicious.\n\nAR\n\nAbbreviation for augmented reality.\n\narea under the PR curve\n\nSee PR AUC (Area under the PR Curve).\n\narea under the ROC curve\n\nSee AUC (Area under the ROC curve).\n\nartificial general intelligence\n\nA non-human mechanism that demonstrates a broad range of problem solving, creativity, and adaptability. For example, a program demonstrating artificial general intelligence could translate text, compose symphonies, and excel at games that have not yet been invented.\n\nartificial intelligence\n\n#fundamentals\n\nA non-human program or model that can solve sophisticated tasks. For example, a program or model that translates text or a program or model that identifies diseases from radiologic images both exhibit artificial intelligence.\n\nFormally, machine learning is a sub-field of artificial intelligence. However, in recent years, some organizations have begun using the terms artificial intelligence and machine learning interchangeably.\n\nattention\n\n#language\n\nA mechanism used in a neural network that indicates the importance of a particular word or part of a word. Attention compresses the amount of information a model needs to predict the next token/word. A typical attention mechanism might consist of a weighted sum over a set of inputs, where the weight for each input is computed by another part of the neural network.\n\nRefer also to self-attention and multi-head self-attention, which are the building blocks of Transformers.\n\nSee LLMs: What's a large language model? in Machine Learning Crash Course for more information about self-attention.\n\nattribute\n\n#fairness\n\nSynonym for feature.\n\nIn machine learning fairness, attributes often refer to characteristics pertaining to individuals.\n\nattribute sampling\n\n#df\n\nA tactic for training a decision forest in which each decision tree considers only a random subset of possible features when learning the condition. Generally, a different subset of features is sampled for each node. In contrast, when training a decision tree without attribute sampling, all possible features are considered for each node.\n\nAUC (Area under the ROC curve)\n\n#fundamentals\n\nA number between 0.0 and 1.0 representing a binary classification model's ability to separate positive classes from negative classes. The closer the AUC is to 1.0, the better the model's ability to separate classes from each other.\n\nFor example, the following illustration shows a classifier model that separates positive classes (green ovals) from negative classes (purple rectangles) perfectly. This unrealistically perfect model has an AUC of 1.0:\n\nConversely, the following illustration shows the results for a classifier model that generated random results. This model has an AUC of 0.5:\n\nYes, the preceding model has an AUC of 0.5, not 0.0.\n\nMost models are somewhere between the two extremes. For instance, the following model separates positives from negatives somewhat, and therefore has an AUC somewhere between 0.5 and 1.0:\n\nAUC ignores any value you set for classification threshold. Instead, AUC considers all possible classification thresholds.\n\nClick the icon to learn about the relationship between AUC and ROC curves.\n\nAUC represents the area under an ROC curve. For example, the ROC curve for a model that perfectly separates positives from negatives looks as follows:\n\nAUC is the area of the gray region in the preceding illustration. In this unusual case, the area is simply the length of the gray region (1.0) multiplied by the width of the gray region (1.0). So, the product of 1.0 and 1.0 yields an AUC of exactly 1.0, which is the highest possible AUC score.\n\nConversely, the ROC curve for a classifier that can't separate classes at all is as follows. The area of this gray region is 0.5.\n\nA more typical ROC curve looks approximately like the following:\n\nIt would be painstaking to calculate the area under this curve manually, which is why a program typically calculates most AUC values.\n\nClick the icon for a more formal definition of AUC.\n\nAUC is the probability that a classifier will be more confident that a randomly chosen positive example is actually positive than that a randomly chosen negative example is positive.\n\nSee Classification: ROC and AUC in Machine Learning Crash Course for more information.\n\naugmented reality\n\n#image\n\nA technology that superimposes a computer-generated image on a user's view of the real world, thus providing a composite view.\n\nautoencoder\n\n#language\n\n#image\n\nA system that learns to extract the most important information from the input. Autoencoders are a combination of an encoder and decoder. Autoencoders rely on the following two-step process:\n\nThe encoder maps the input to a (typically) lossy lower-dimensional (intermediate) format.\n\nThe decoder builds a lossy version of the original input by mapping the lower-dimensional format to the original higher-dimensional input format.\n\nAutoencoders are trained end-to-end by having the decoder attempt to reconstruct the original input from the encoder's intermediate format as closely as possible. Because the intermediate format is smaller (lower-dimensional) than the original format, the autoencoder is forced to learn what information in the input is essential, and the output won't be perfectly identical to the input.\n\nFor example:\n\nIf the input data is a graphic, the non-exact copy would be similar to the original graphic, but somewhat modified. Perhaps the non-exact copy removes noise from the original graphic or fills in some missing pixels.\n\nIf the input data is text, an autoencoder would generate new text that mimics (but is not identical to) the original text.\n\nSee also variational autoencoders.\n\nautomation bias\n\n#fairness\n\nWhen a human decision maker favors recommendations made by an automated decision-making system over information made without automation, even when the automated decision-making system makes errors.\n\nSee Fairness: Types of bias in Machine Learning Crash Course for more information.\n\nAutoML\n\nAny automated process for building machine learning models. AutoML can automatically do tasks such as the following:\n\nSearch for the most appropriate model.\n\nTune hyperparameters.\n\nPrepare data (including performing feature engineering).\n\nDeploy the resulting model.\n\nAutoML is useful for data scientists because it can save them time and effort in developing machine learning pipelines and improve prediction accuracy. It is also useful to non-experts, by making complicated machine learning tasks more accessible to them.\n\nSee Automated Machine Learning (AutoML) in Machine Learning Crash Course for more information.\n\nauto-regressive model\n\n#language\n\n#image\n\n#generativeAI\n\nA model that infers a prediction based on its own previous predictions. For example, auto-regressive language models predict the next token based on the previously predicted tokens. All Transformer-based large language models are auto-regressive.\n\nIn contrast, GAN-based image models are usually not auto-regressive since they generate an image in a single forward-pass and not iteratively in steps. However, certain image generation models are auto-regressive because they generate an image in steps.\n\nauxiliary loss\n\nA loss function—used in conjunction with a neural network model's main loss function—that helps accelerate training during the early iterations when weights are randomly initialized.\n\nAuxiliary loss functions push effective gradients to the earlier layers. This facilitates convergence during training by combating the vanishing gradient problem.\n\naverage precision\n\nA metric for summarizing the performance of a ranked sequence of results. Average precision is calculated by taking the average of the precision values for each relevant result (each result in the ranked list where the recall increases relative to the previous result).\n\nSee also Area under the PR Curve.\n\naxis-aligned condition\n\n#df\n\nIn a decision tree, a condition that involves only a single feature. For example, if area is a feature, then the following is an axis-aligned condition:\n\narea > 200\n\nContrast with oblique condition.\n\nB\n\nbackpropagation\n\n#fundamentals\n\nThe algorithm that implements gradient descent in neural networks.\n\nTraining a neural network involves many iterations of the following two-pass cycle:\n\nDuring the forward pass, the system processes a batch of examples to yield prediction(s). The system compares each prediction to each label value. The difference between the prediction and the label value is the loss for that example. The system aggregates the losses for all the examples to compute the total loss for the current batch.\n\nDuring the backward pass (backpropagation), the system reduces loss by adjusting the weights of all the neurons in all the hidden layer(s).\n\nNeural networks often contain many neurons across many hidden layers. Each of those neurons contribute to the overall loss in different ways. Backpropagation determines whether to increase or decrease the weights applied to particular neurons.\n\nThe learning rate is a multiplier that controls the degree to which each backward pass increases or decreases each weight. A large learning rate will increase or decrease each weight more than a small learning rate.\n\nIn calculus terms, backpropagation implements the chain rule. from calculus. That is, backpropagation calculates the partial derivative of the error with respect to each parameter.\n\nYears ago, ML practitioners had to write code to implement backpropagation. Modern ML APIs like Keras now implement backpropagation for you. Phew!\n\nSee Neural networks in Machine Learning Crash Course for more information.\n\nbagging\n\n#df\n\nA method to train an ensemble where each constituent model trains on a random subset of training examples sampled with replacement. For example, a random forest is a collection of decision trees trained with bagging.\n\nThe term bagging is short for bootstrap aggregating.\n\nSee Random forests in the Decision Forests course for more information.\n\nbag of words\n\n#language\n\nA representation of the words in a phrase or passage, irrespective of order. For example, bag of words represents the following three phrases identically:\n\nthe dog jumps\n\njumps the dog\n\ndog jumps the\n\nEach word is mapped to an index in a sparse vector, where the vector has an index for every word in the vocabulary. For example, the phrase the dog jumps is mapped into a feature vector with non-zero values at the three indexes corresponding to the words the, dog, and jumps. The non-zero value can be any of the following:\n\nA 1 to indicate the presence of a word.\n\nA count of the number of times a word appears in the bag. For example, if the phrase were the maroon dog is a dog with maroon fur, then both maroon and dog would be represented as 2, while the other words would be represented as 1.\n\nSome other value, such as the logarithm of the count of the number of times a word appears in the bag.\n\nbaseline\n\nA model used as a reference point for comparing how well another model (typically, a more complex one) is performing. For example, a logistic regression model might serve as a good baseline for a deep model.\n\nFor a particular problem, the baseline helps model developers quantify the minimal expected performance that a new model must achieve for the new model to be useful.\n\nbatch\n\n#fundamentals\n\nThe set of examples used in one training iteration. The batch size determines the number of examples in a batch.\n\nSee epoch for an explanation of how a batch relates to an epoch.\n\nSee Linear regression: Hyperparameters in Machine Learning Crash Course for more information.\n\nbatch inference\n\n#TensorFlow\n\n#GoogleCloud\n\nThe process of inferring predictions on multiple unlabeled examples divided into smaller subsets (\"batches\").\n\nBatch inference can take advantage of the parallelization features of accelerator chips. That is, multiple accelerators can simultaneously infer predictions on different batches of unlabeled examples, dramatically increasing the number of inferences per second.\n\nSee Production ML systems: Static versus dynamic inference in Machine Learning Crash Course for more information.\n\nbatch normalization\n\nNormalizing the input or output of the activation functions in a hidden layer. Batch normalization can provide the following benefits:\n\nMake neural networks more stable by protecting against outlier weights.\n\nEnable higher learning rates, which can speed training.\n\nReduce overfitting.\n\nbatch size\n\n#fundamentals\n\nThe number of examples in a batch. For instance, if the batch size is 100, then the model processes 100 examples per iteration.\n\nThe following are popular batch size strategies:\n\nStochastic Gradient Descent (SGD), in which the batch size is 1.\n\nFull batch, in which the batch size is the number of examples in the entire training set. For instance, if the training set contains a million examples, then the batch size would be a million examples. Full batch is usually an inefficient strategy.\n\nmini-batch in which the batch size is usually between 10 and 1000. Mini-batch is usually the most efficient strategy.\n\nSee the following for more information:\n\nProduction ML systems: Static versus dynamic inference in Machine Learning Crash Course.\n\nDeep Learning Tuning Playbook.\n\nBayesian neural network\n\nA probabilistic neural network that accounts for uncertainty in weights and outputs. A standard neural network regression model typically predicts a scalar value; for example, a standard model predicts a house price of 853,000. In contrast, a Bayesian neural network predicts a distribution of values; for example, a Bayesian model predicts a house price of 853,000 with a standard deviation of 67,200.\n\nA Bayesian neural network relies on Bayes' Theorem to calculate uncertainties in weights and predictions. A Bayesian neural network can be useful when it is important to quantify uncertainty, such as in models related to pharmaceuticals. Bayesian neural networks can also help prevent overfitting.\n\nBayesian optimization\n\nA probabilistic regression model technique for optimizing computationally expensive objective functions by instead optimizing a surrogate that quantifies the uncertainty using a Bayesian learning technique. Since Bayesian optimization is itself very expensive, it is usually used to optimize expensive-to-evaluate tasks that have a small number of parameters, such as selecting hyperparameters.\n\nBellman equation\n\n#rl\n\nIn reinforcement learning, the following identity satisfied by the optimal Q-function:\n\n\\[Q(s, a) = r(s, a) + \\gamma \\mathbb{E}_{s'|s,a} \\max_{a'} Q(s', a')\\]\n\nReinforcement learning algorithms apply this identity to create Q-learning via the following update rule:\n\n\\[Q(s,a) \\gets Q(s,a) + \\alpha \\left[r(s,a) + \\gamma \\displaystyle\\max_{\\substack{a_1}} Q(s',a') - Q(s,a) \\right] \\]\n\nBeyond reinforcement learning, the Bellman equation has applications to dynamic programming. See the Wikipedia entry for Bellman equation.\n\nBERT (Bidirectional Encoder Representations from Transformers)\n\n#language\n\nA model architecture for text representation. A trained BERT model can act as part of a larger model for text classification or other ML tasks.\n\nBERT has the following characteristics:\n\nUses the Transformer architecture, and therefore relies on self-attention.\n\nUses the encoder part of the Transformer. The encoder's job is to produce good text representations, rather than to perform a specific task like classification.\n\nIs bidirectional.\n\nUses masking for unsupervised training.\n\nBERT's variants include:\n\nALBERT, which is an acronym for A Light BERT.\n\nLaBSE.\n\nSee Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processing for an overview of BERT.\n\nbias (ethics/fairness)\n\n#fairness\n\n#fundamentals\n\n1. Stereotyping, prejudice or favoritism towards some things, people, or groups over others. These biases can affect collection and interpretation of data, the design of a system, and how users interact with a system. Forms of this type of bias include:\n\nautomation bias\n\nconfirmation bias\n\nexperimenter's bias\n\ngroup attribution bias\n\nimplicit bias\n\nin-group bias\n\nout-group homogeneity bias\n\n2. Systematic error introduced by a sampling or reporting procedure. Forms of this type of bias include:\n\ncoverage bias\n\nnon-response bias\n\nparticipation bias\n\nreporting bias\n\nsampling bias\n\nselection bias\n\nNot to be confused with the bias term in machine learning models or prediction bias.\n\nSee Fairness: Types of bias in Machine Learning Crash Course for more information.\n\nbias (math) or bias term\n\n#fundamentals\n\nAn intercept or offset from an origin. Bias is a parameter in machine learning models, which is symbolized by either of the following:\n\nb\n\nw0\n\nFor example, bias is the b in the following formula:\n\n$$y' = b + w_1x_1 + w_2x_2 + … w_nx_n$$\n\nIn a simple two-dimensional line, bias just means \"y-intercept.\" For example, the bias of the line in the following illustration is 2.\n\nBias exists because not all models start from the origin (0,0). For example, suppose an amusement park costs 2 Euros to enter and an additional 0.5 Euro for every hour a customer stays. Therefore, a model mapping the total cost has a bias of 2 because the lowest cost is 2 Euros.\n\nBias is not to be confused with bias in ethics and fairness or prediction bias.\n\nSee Linear Regression in Machine Learning Crash Course for more information.\n\nbidirectional\n\n#language\n\nA term used to describe a system that evaluates the text that both precedes and follows a target section of text. In contrast, a unidirectional system only evaluates the text that precedes a target section of text.\n\nFor example, consider a masked language model that must determine probabilities for the word or words representing the underline in the following question:\n\nWhat is the _____ with you?\n\nA unidirectional language model would have to base its probabilities only on the context provided by the words \"What\", \"is\", and \"the\". In contrast, a bidirectional language model could also gain context from \"with\" and \"you\", which might help the model generate better predictions.\n\nbidirectional language model\n\n#language\n\nA language model that determines the probability that a given token is present at a given location in an excerpt of text based on the preceding and following text.\n\nbigram\n\n#seq\n\n#language\n\nAn N-gram in which N=2.\n\nbinary classification\n\n#fundamentals\n\nA type of classification task that predicts one of two mutually exclusive classes:\n\nthe positive class\n\nthe negative class\n\nFor example, the following two machine learning models each perform binary classification:\n\nA model that determines whether email messages are spam (the positive class) or not spam (the negative class).\n\nA model that evaluates medical symptoms to determine whether a person has a particular disease (the positive class) or doesn't have that disease (the negative class).\n\nContrast with multi-class classification.\n\nSee also logistic regression and classification threshold.\n\nSee Classification in Machine Learning Crash Course for more information.\n\nbinary condition\n\n#df\n\nIn a decision tree, a condition that has only two possible outcomes, typically yes or no. For example, the following is a binary condition:\n\ntemperature >= 100\n\nContrast with non-binary condition.\n\nSee Types of conditions in the Decision Forests course for more information.\n\nbinning\n\nSynonym for bucketing.\n\nBLEU (Bilingual Evaluation Understudy)\n\n#language\n\nA score between 0.0 and 1.0, inclusive, indicating the quality of a translation between two human languages (for example, between English and Russian). A BLEU score of 1.0 indicates a perfect translation; a BLEU score of 0.0 indicates a terrible translation.\n\nboosting\n\nA machine learning technique that iteratively combines a set of simple and not very accurate classifiers (referred to as \"weak\" classifiers) into a classifier with high accuracy (a \"strong\" classifier) by upweighting the examples that the model is currently misclassifying.\n\nSee Gradient Boosted Decision Trees? in the Decision Forests course for more information.\n\nbounding box\n\n#image\n\nIn an image, the (x, y) coordinates of a rectangle around an area of interest, such as the dog in the image below.\n\nbroadcasting\n\nExpanding the shape of an operand in a matrix math operation to dimensions compatible for that operation. For example, linear algebra requires that the two operands in a matrix addition operation must have the same dimensions. Consequently, you can't add a matrix of shape (m, n) to a vector of length n. Broadcasting enables this operation by virtually expanding the vector of length n to a matrix of shape (m, n) by replicating the same values down each column.\n\nFor example, given the following definitions, linear algebra prohibits A+B because A and B have different dimensions:\n\nA = [[7, 10, 4], [13, 5, 9]] B = [2]\n\nHowever, broadcasting enables the operation A+B by virtually expanding B to:\n\n[[2, 2, 2], [2, 2, 2]]\n\nThus, A+B is now a valid operation:\n\n[[7, 10, 4], + [[2, 2, 2], = [[ 9, 12, 6], [13, 5, 9]] [2, 2, 2]] [15, 7, 11]]\n\nSee the following description of broadcasting in NumPy for more details.\n\nbucketing\n\n#fundamentals\n\nConverting a single feature into multiple binary features called buckets or bins, typically based on a value range. The chopped feature is typically a continuous feature.\n\nFor example, instead of representing temperature as a single continuous floating-point feature, you could chop ranges of temperatures into discrete buckets, such as:\n\n<= 10 degrees Celsius would be the \"cold\" bucket.\n\n11 - 24 degrees Celsius would be the \"temperate\" bucket.\n\n>= 25 degrees Celsius would be the \"warm\" bucket.\n\nThe model will treat every value in the same bucket identically. For example, the values 13 and 22 are both in the temperate bucket, so the model treats the two values identically.\n\nClick the icon for additional notes.\n\nIf you represent temperature as a continuous feature, then the model treats temperature as a single feature. If you represent temperature as three buckets, then the model treats each bucket as a separate feature. That is, a model can learn separate relationships of each bucket to the label. For example, a linear regression model can learn separate weights for each bucket.\n\nIncreasing the number of buckets makes your model more complicated by increasing the number of relationships that your model must learn. For example, the cold, temperate, and warm buckets are essentially three separate features for your model to train on. If you decide to add two more buckets--for example, freezing and hot--your model would now have to train on five separate features.\n\nHow do you know how many buckets to create, or what the ranges for each bucket should be? The answers typically require a fair amount of experimentation.\n\nSee Numerical data: Binning in Machine Learning Crash Course for more information.\n\nC\n\ncalibration layer\n\nA post-prediction adjustment, typically to account for prediction bias. The adjusted predictions and probabilities should match the distribution of an observed set of labels.\n\n#recsystems\n\nThe initial set of recommendations chosen by a recommendation system. For example, consider a bookstore that offers 100,000 titles. The candidate generation phase creates a much smaller list of suitable books for a particular user, say 500. But even 500 books is way too many to recommend to a user. Subsequent, more expensive, phases of a recommendation system (such as scoring and re-ranking) reduce those 500 to a much smaller, more useful set of recommendations.\n\nSee Candidate generation overview in the Recommendation Systems course for more information.\n\nA training-time optimization that calculates a probability for all the positive labels, using, for example, softmax, but only for a random sample of negative labels. For instance, given an example labeled beagle and dog, candidate sampling computes the predicted probabilities and corresponding loss terms for:\n\nbeagle\n\ndog\n\na random subset of the remaining negative classes (for example, cat, lollipop, fence).\n\nThe idea is that the negative classes can learn from less frequent negative reinforcement as long as positive classes always get proper positive reinforcement, and this is indeed observed empirically.\n\nCandidate sampling is more computationally efficient than training algorithms that compute predictions for all negative classes, particularly when the number of negative classes is very large.\n\ncategorical data\n\n#fundamentals\n\nFeatures having a specific set of possible values. For example, consider a categorical feature named traffic-light-state, which can only have one of the following three possible values:\n\nred\n\nyellow\n\ngreen\n\nBy representing traffic-light-state as a categorical feature, a model can learn the differing impacts of red, green, and yellow on driver behavior.\n\nCategorical features are sometimes called discrete features.\n\nContrast with numerical data.\n\nSee Working with categorical data in Machine Learning Crash Course for more information.\n\ncausal language model\n\n#language\n\nSynonym for unidirectional language model.\n\nSee bidirectional language model to contrast different directional approaches in language modeling.\n\ncentroid\n\n#clustering\n\nThe center of a cluster as determined by a k-means or k-median algorithm. For example, if k is 3, then the k-means or k-median algorithm finds 3 centroids.\n\nSee Clustering algorithms in the Clustering course for more information.\n\ncentroid-based clustering\n\n#clustering\n\nA category of clustering algorithms that organizes data into nonhierarchical clusters. k-means is the most widely used centroid-based clustering algorithm.\n\nContrast with hierarchical clustering algorithms.\n\nSee Clustering algorithms in the Clustering course for more information.\n\nchain-of-thought prompting\n\n#language\n\n#generativeAI\n\nA prompt engineering technique that encourages a large language model (LLM) to explain its reasoning, step by step. For example, consider the following prompt, paying particular attention to the second sentence:\n\nHow many g forces would a driver experience in a car that goes from 0 to 60 miles per hour in 7 seconds? In the answer, show all relevant calculations.\n\nThe LLM's response would likely:\n\nShow a sequence of physics formulas, plugging in the values 0, 60, and 7 in appropriate places.\n\nExplain why it chose those formulas and what the various variables mean.\n\nChain-of-thought prompting forces the LLM to perform all the calculations, which might lead to a more correct answer. In addition, chain-of-thought prompting enables the user to examine the LLM's steps to determine whether or not the answer makes sense.\n\nchat\n\n#language\n\n#generativeAI\n\nThe contents of a back-and-forth dialogue with an ML system, typically a large language model. The previous interaction in a chat (what you typed and how the large language model responded) becomes the context for subsequent parts of the chat.\n\nA chatbot is an application of a large language model.\n\ncheckpoint\n\nData that captures the state of a model's parameters either during training or after training is completed. For example, during training, you can:\n\nStop training, perhaps intentionally or perhaps as the result of certain errors.\n\nCapture the checkpoint.\n\nLater, reload the checkpoint, possibly on different hardware.\n\nRestart training.\n\nclass\n\n#fundamentals\n\nA category that a label can belong to. For example:\n\nIn a binary classification model that detects spam, the two classes might be spam and not spam.\n\nIn a multi-class classification model that identifies dog breeds, the classes might be poodle, beagle, pug, and so on.\n\nA classification model predicts a class. In contrast, a regression model predicts a number rather than a class.\n\nSee Classification in Machine Learning Crash Course for more information.\n\nclassification model\n\n#fundamentals\n\nA model whose prediction is a class. For example, the following are all classification models:\n\nA model that predicts an input sentence's language (French? Spanish? Italian?).\n\nA model that predicts tree species (Maple? Oak? Baobab?).\n\nA model that predicts the positive or negative class for a particular medical condition.\n\nIn contrast, regression models predict numbers rather than classes.\n\nTwo common types of classification models are:\n\nbinary classification\n\nmulti-class classification\n\nclassification threshold\n\n#fundamentals\n\nIn a binary classification, a number between 0 and 1 that converts the raw output of a logistic regression model into a prediction of either the positive class or the negative class. Note that the classification threshold is a value that a human chooses, not a value chosen by model training.\n\nA logistic regression model outputs a raw value between 0 and 1. Then:\n\nIf this raw value is greater than the classification threshold, then the positive class is predicted.\n\nIf this raw value is less than the classification threshold, then the negative class is predicted.\n\nFor example, suppose the classification threshold is 0.8. If the raw value is 0.9, then the model predicts the positive class. If the raw value is 0.7, then the model predicts the negative class.\n\nThe choice of classification threshold strongly influences the number of false positives and false negatives.\n\nClick the icon for additional notes.\n\nAs models or datasets evolve, engineers sometimes also change the classification threshold. When the classification threshold changes, positive class predictions can suddenly become negative classes and vice-versa.\n\nFor example, consider a binary classification disease prediction model. Suppose that when the system runs in the first year:\n\nThe raw value for a particular patient is 0.95.\n\nThe classification threshold is 0.94.\n\nTherefore, the system diagnoses the positive class. (The patient gasps, \"Oh no! I'm sick!\")\n\nA year later, perhaps the values now look as follows:\n\nThe raw value for the same patient remains at 0.95.\n\nThe classification threshold changes to 0.97.\n\nTherefore, the system now reclassifies that patient as the negative class. (\"Happy day! I'm not sick.\") Same patient. Different diagnosis.\n\nSee Thresholds and the confusion matrix in Machine Learning Crash Course for more information.\n\nclass-imbalanced dataset\n\n#fundamentals\n\nA dataset for a classification problem in which the total number of labels of each class differs significantly. For example, consider a binary classification dataset whose two labels are divided as follows:\n\n1,000,000 negative labels\n\n10 positive labels\n\nThe ratio of negative to positive labels is 100,000 to 1, so this is a class-imbalanced dataset.\n\nIn contrast, the following dataset is not class-imbalanced because the ratio of negative labels to positive labels is relatively close to 1:\n\n517 negative labels\n\n483 positive labels\n\nMulti-class datasets can also be class-imbalanced. For example, the following multi-class classification dataset is also class-imbalanced because one label has far more examples than the other two:\n\n1,000,000 labels with class \"green\"\n\n200 labels with class \"purple\"\n\n350 labels with class \"orange\"\n\nSee also entropy, majority class, and minority class.\n\nclipping\n\n#fundamentals\n\nA technique for handling outliers by doing either or both of the following:\n\nReducing feature values that are greater than a maximum threshold down to that maximum threshold.\n\nIncreasing feature values that are less than a minimum threshold up to that minimum threshold.\n\nFor example, suppose that <0.5% of values for a particular feature fall outside the range 40–60. In this case, you could do the following:\n\nClip all values over 60 (the maximum threshold) to be exactly 60.\n\nClip all values under 40 (the minimum threshold) to be exactly 40.\n\nOutliers can damage models, sometimes causing weights to overflow during training. Some outliers can also dramatically spoil metrics like accuracy. Clipping is a common technique to limit the damage.\n\nGradient clipping forces gradient values within a designated range during training.\n\nSee Numerical data: Normalization in Machine Learning Crash Course for more information.\n\nCloud TPU\n\n#TensorFlow\n\n#GoogleCloud\n\nA specialized hardware accelerator designed to speed up machine learning workloads on Google Cloud.\n\nclustering\n\n#clustering\n\nGrouping related examples, particularly during unsupervised learning. Once all the examples are grouped, a human can optionally supply meaning to each cluster.\n\nMany clustering algorithms exist. For example, the k-means algorithm clusters examples based on their proximity to a centroid, as in the following diagram:\n\nA human researcher could then review the clusters and, for example, label cluster 1 as \"dwarf trees\" and cluster 2 as \"full-size trees.\"\n\nAs another example, consider a clustering algorithm based on an example's distance from a center point, illustrated as follows:\n\nSee the Clustering course for more information.\n\nco-adaptation\n\nWhen neurons predict patterns in training data by relying almost exclusively on outputs of specific other neurons instead of relying on the network's behavior as a whole. When the patterns that cause co-adaptation are not present in validation data, then co-adaptation causes overfitting. Dropout regularization reduces co-adaptation because dropout ensures neurons cannot rely solely on specific other neurons.\n\ncollaborative filtering\n\n#recsystems\n\nMaking predictions about the interests of one user based on the interests of many other users. Collaborative filtering is often used in recommendation systems.\n\nSee Collaborative filtering in the Recommendation Systems course for more information.\n\nconcept drift\n\nA shift in the relationship between features and the label. Over time, concept drift reduces a model's quality.\n\nDuring training, the model learns the relationship between the features and their labels in the training set. If the labels in the training set are good proxies for the real-world, then the model should make good real world predictions. However, due to concept drift, the model's predictions tend to degrade over time.\n\nFor example, consider a binary classification model that predicts whether or not a certain car model is \"fuel efficient.\" That is, the features could be:\n\ncar weight\n\nengine compression\n\ntransmission type\n\nwhile the label is either:\n\nfuel efficient\n\nnot fuel efficient\n\nHowever, the concept of \"fuel efficient car\" keeps changing. A car model labeled fuel efficient in 1994 would almost certainly be labeled not fuel efficient in 2024. A model suffering from concept drift tends to make less and less useful predictions over time.\n\nCompare and contrast with nonstationarity.\n\nClick the icon for additional notes.\n\nTo compensate for concept drift, retrain models faster than the rate of concept drift. For example, if concept drift reduces model precision by a meaningful margin every two months, then retrain your model more frequently than every two months.\n\ncondition\n\n#df\n\nIn a decision tree, any node that evaluates an expression. For example, the following portion of a decision tree contains two conditions:\n\nA condition is also called a split or a test.\n\nContrast condition with leaf.\n\nSee also:\n\nbinary condition\n\nnon-binary condition.\n\naxis-aligned-condition\n\noblique-condition\n\nSee Types of conditions in the Decision Forests course for more information.\n\nconfabulation\n\n#language\n\nSynonym for hallucination.\n\nConfabulation is probably a more technically accurate term than hallucination. However, hallucination became popular first.\n\nconfiguration\n\nThe process of assigning the initial property values used to train a model, including:\n\nthe model's composing layers\n\nthe location of the data\n\nhyperparameters such as:\n\nlearning rate\n\niterations\n\noptimizer\n\nloss function\n\nIn machine learning projects, configuration can be done through a special configuration file or using configuration libraries such as the following:\n\nHParam\n\nGin\n\nFiddle\n\nconfirmation bias\n\n#fairness\n\nThe tendency to search for, interpret, favor, and recall information in a way that confirms one's pre-existing beliefs or hypotheses. Machine learning developers may inadvertently collect or label data in ways that influence an outcome supporting their existing beliefs. Confirmation bias is a form of implicit bias.\n\nExperimenter's bias is a form of confirmation bias in which an experimenter continues training models until a pre-existing hypothesis is confirmed.\n\nconfusion matrix\n\n#fundamentals\n\nAn NxN table that summarizes the number of correct and incorrect predictions that a classification model made. For example, consider the following confusion matrix for a binary classification model:\n\nTumor (predicted) Non-Tumor (predicted) Tumor (ground truth) 18 (TP) 1 (FN) Non-Tumor (ground truth) 6 (FP) 452 (TN)\n\nThe preceding confusion matrix shows the following:\n\nOf the 19 predictions in which ground truth was Tumor, the model correctly classified 18 and incorrectly classified 1.\n\nOf the 458 predictions in which ground truth was Non-Tumor, the model correctly classified 452 and incorrectly classified 6.\n\nThe confusion matrix for a multi-class classification problem can help you identify patterns of mistakes. For example, consider the following confusion matrix for a 3-class multi-class classification model that categorizes three different iris types (Virginica, Versicolor, and Setosa). When the ground truth was Virginica, the confusion matrix shows that the model was far more likely to mistakenly predict Versicolor than Setosa:\n\nSetosa (predicted) Versicolor (predicted) Virginica (predicted) Setosa (ground truth) 88 12 0 Versicolor (ground truth) 6 141 7 Virginica (ground truth) 2 27 109\n\nAs yet another example, a confusion matrix could reveal that a model trained to recognize handwritten digits tends to mistakenly predict 9 instead of 4, or mistakenly predict 1 instead of 7.\n\nConfusion matrixes contain sufficient information to calculate a variety of performance metrics, including precision and recall.\n\nconstituency parsing\n\n#language\n\nDividing a sentence into smaller grammatical structures (\"constituents\"). A later part of the ML system, such as a natural language understanding model, can parse the constituents more easily than the original sentence. For example, consider the following sentence:\n\nMy friend adopted two cats.\n\nA constituency parser can divide this sentence into the following two constituents:\n\nMy friend is a noun phrase.\n\nadopted two cats is a verb phrase.\n\nThese constituents can be further subdivided into smaller constituents. For example, the verb phrase\n\nadopted two cats\n\ncould be further subdivided into:\n\nadopted is a verb.\n\ntwo cats is another noun phrase.\n\ncontextualized language embedding\n\n#language\n\n#generativeAI\n\nAn embedding that comes close to \"understanding\" words and phrases in ways that native human speakers can. Contextualized language embeddings can understand complex syntax, semantics, and context.\n\nFor example, consider embeddings of the English word cow. Older embeddings such as word2vec can represent English words such that the distance in the embedding space from cow to bull is similar to the distance from ewe (female sheep) to ram (male sheep) or from female to male. Contextualized language embeddings can go a step further by recognizing that English speakers sometimes casually use the word cow to mean either cow or bull.\n\ncontext window\n\n#language\n\n#generativeAI\n\nThe number of tokens a model can process in a given prompt. The larger the context window, the more information the model can use to provide coherent and consistent responses to the prompt.\n\ncontinuous feature\n\n#fundamentals\n\nA floating-point feature with an infinite range of possible values, such as temperature or weight.\n\nContrast with discrete feature.\n\nconvenience sampling\n\nUsing a dataset not gathered scientifically in order to run quick experiments. Later on, it's essential to switch to a scientifically gathered dataset.\n\nconvergence\n\n#fundamentals\n\nA state reached when loss values change very little or not at all with each iteration. For example, the following loss curve suggests convergence at around 700 iterations:\n\nA model converges when additional training won't improve the model.\n\nIn deep learning, loss values sometimes stay constant or nearly so for many iterations before finally descending. During a long period of constant loss values, you may temporarily get a false sense of convergence.\n\nSee also early stopping.\n\nSee Model convergence and loss curves in Machine Learning Crash Course for more information.\n\nconvex function\n\nA function in which the region above the graph of the function is a convex set. The prototypical convex function is shaped something like the letter U. For example, the following are all convex functions:\n\nIn contrast, the following function is not convex. Notice how the region above the graph is not a convex set:\n\nA strictly convex function has exactly one local minimum point, which is also the global minimum point. The classic U-shaped functions are strictly convex functions. However, some convex functions (for example, straight lines) are not U-shaped.\n\nClick the icon for a deeper look at the math.\n\nA lot of the common loss functions, including the following, are convex functions:\n\nL2 loss\n\nLog Loss\n\nL1 regularization\n\nL2 regularization\n\nMany variations of gradient descent are guaranteed to find a point close to the minimum of a strictly convex function. Similarly, many variations of stochastic gradient descent have a high probability (though, not a guarantee) of finding a point close to the minimum of a strictly convex function.\n\nThe sum of two convex functions (for example, L2 loss + L1 regularization) is a convex function.\n\nDeep models are never convex functions. Remarkably, algorithms designed for convex optimization tend to find reasonably good solutions on deep networks anyway, even though those solutions are not guaranteed to be a global minimum.\n\nSee Convergence and convex functions in Machine Learning Crash Course for more information.\n\nconvex optimization\n\nThe process of using mathematical techniques such as gradient descent to find the minimum of a convex function. A great deal of research in machine learning has focused on formulating various problems as convex optimization problems and in solving those problems more efficiently.\n\nFor complete details, see Boyd and Vandenberghe, Convex Optimization.\n\nconvex set\n\nA subset of Euclidean space such that a line drawn between any two points in the subset remains completely within the subset. For instance, the following two shapes are convex sets:\n\nIn contrast, the following two shapes are not convex sets:\n\nconvolution\n\n#image\n\nIn mathematics, casually speaking, a mixture of two functions. In machine learning, a convolution mixes the convolutional filter and the input matrix in order to train weights.\n\nThe term \"convolution\" in machine learning is often a shorthand way of referring to either convolutional operation or convolutional layer.\n\nWithout convolutions, a machine learning algorithm would have to learn a separate weight for every cell in a large tensor. For example, a machine learning algorithm training on 2K x 2K images would be forced to find 4M separate weights. Thanks to convolutions, a machine learning algorithm only has to find weights for every cell in the convolutional filter, dramatically reducing the memory needed to train the model. When the convolutional filter is applied, it is simply replicated across cells such that each is multiplied by the filter.\n\nSee Introducing Convolutional Neural Networks in the Image Classification course for more information.\n\nconvolutional filter\n\n#image\n\nOne of the two actors in a convolutional operation. (The other actor is a slice of an input matrix.) A convolutional filter is a matrix having the same rank as the input matrix, but a smaller shape. For example, given a 28x28 input matrix, the filter could be any 2D matrix smaller than 28x28.\n\nIn photographic manipulation, all the cells in a convolutional filter are typically set to a constant pattern of ones and zeroes. In machine learning, convolutional filters are typically seeded with random numbers and then the network trains the ideal values.\n\nSee Convolution in the Image Classification course for more information.\n\nconvolutional layer\n\n#image\n\nA layer of a deep neural network in which a convolutional filter passes along an input matrix. For example, consider the following 3x3 convolutional filter:\n\nThe following animation shows a convolutional layer consisting of 9 convolutional operations involving the 5x5 input matrix. Notice that each convolutional operation works on a different 3x3 slice of the input matrix. The resulting 3x3 matrix (on the right) consists of the results of the 9 convolutional operations:\n\nSee Fully Connected Layers in the Image Classification course for more information.\n\nconvolutional neural network\n\n#image\n\nA neural network in which at least one layer is a convolutional layer. A typical convolutional neural network consists of some combination of the following layers:\n\nconvolutional layers\n\npooling layers\n\ndense layers\n\nConvolutional neural networks have had great success in certain kinds of problems, such as image recognition.\n\nconvolutional operation\n\n#image\n\nThe following two-step mathematical operation:\n\nElement-wise multiplication of the convolutional filter and a slice of an input matrix. (The slice of the input matrix has the same rank and size as the convolutional filter.)\n\nSummation of all the values in the resulting product matrix.\n\nFor example, consider the following 5x5 input matrix:\n\nNow imagine the following 2x2 convolutional filter:\n\nEach convolutional operation involves a single 2x2 slice of the input matrix. For example, suppose we use the 2x2 slice at the top-left of the input matrix. So, the convolution operation on this slice looks as follows:\n\nA convolutional layer consists of a series of convolutional operations, each acting on a different slice of the input matrix.\n\ncost\n\nSynonym for loss.\n\nco-training\n\nA semi-supervised learning approach particularly useful when all of the following conditions are true:\n\nThe ratio of unlabeled examples to labeled examples in the dataset is high.\n\nThis is a classification problem (binary or multi-class).\n\nThe dataset contains two different sets of predictive features that are independent of each other and complementary.\n\nCo-training essentially amplifies independent signals into a stronger signal. For example, consider a classification model that categorizes individual used cars as either Good or Bad. One set of predictive features might focus on aggregate characteristics such as the year, make, and model of the car; another set of predictive features might focus on the previous owner's driving record and the car's maintenance history.\n\nThe seminal paper on co-training is Combining Labeled and Unlabeled Data with Co-Training by Blum and Mitchell.\n\ncounterfactual fairness\n\n#fairness\n\nA fairness metric that checks whether a classifier produces the same result for one individual as it does for another individual who is identical to the first, except with respect to one or more sensitive attributes. Evaluating a classifier for counterfactual fairness is one method for surfacing potential sources of bias in a model.\n\nSee either of the following for more information:\n\nFairness: Counterfactual fairness in Machine Learning Crash Course.\n\nWhen Worlds Collide: Integrating Different Counterfactual Assumptions in Fairness\n\ncoverage bias\n\n#fairness\n\nSee selection bias.\n\ncrash blossom\n\n#language\n\nA sentence or phrase with an ambiguous meaning. Crash blossoms present a significant problem in natural language understanding. For example, the headline Red Tape Holds Up Skyscraper is a crash blossom because an NLU model could interpret the headline literally or figuratively.\n\nClick the icon for additional notes.\n\nJust to clarify that mysterious headline:\n\nRed Tape could refer to either of the following:\n\nAn adhesive\n\nExcessive bureaucracy\n\nHolds Up could refer to either of the following:\n\nStructural support\n\nDelays\n\ncritic\n\n#rl\n\nSynonym for Deep Q-Network.\n\ncross-entropy\n\nA generalization of Log Loss to multi-class classification problems. Cross-entropy quantifies the difference between two probability distributions. See also perplexity.\n\ncross-validation\n\nA mechanism for estimating how well a model would generalize to new data by testing the model against one or more non-overlapping data subsets withheld from the training set.\n\ncumulative distribution function (CDF)\n\nA function that defines the frequency of samples less than or equal to a target value. For example, consider a normal distribution of continuous values. A CDF tells you that approximately 50% of samples should be less than or equal to the mean and that approximately 84% of samples should be less than or equal to one standard deviation above the mean.\n\nD\n\ndata analysis\n\nObtaining an understanding of data by considering samples, measurement, and visualization. Data analysis can be particularly useful when a dataset is first received, before one builds the first model. It is also crucial in understanding experiments and debugging problems with the system.\n\ndata augmentation\n\n#image\n\nArtificially boosting the range and number of training examples by transforming existing examples to create additional examples. For example, suppose images are one of your features, but your dataset doesn't contain enough image examples for the model to learn useful associations. Ideally, you'd add enough labeled images to your dataset to enable your model to train properly. If that's not possible, data augmentation can rotate, stretch, and reflect each image to produce many variants of the original picture, possibly yielding enough labeled data to enable excellent training.\n\nDataFrame\n\n#fundamentals\n\nA popular pandas data type for representing datasets in memory.\n\nA DataFrame is analogous to a table or a spreadsheet. Each column of a DataFrame has a name (a header), and each row is identified by a unique number.\n\nEach column in a DataFrame is structured like a 2D array, except that each column can be assigned its own data type.\n\nSee also the official pandas.DataFrame reference page.\n\ndata parallelism\n\nA way of scaling training or inference that replicates an entire model onto multiple devices and then passes a subset of the input data to each device. Data parallelism can enable training and inference on very large batch sizes; however, data parallelism requires that the model be small enough to fit on all devices.\n\nData parallelism typically speeds training and inference.\n\nSee also model parallelism.\n\ndata set or dataset\n\n#fundamentals\n\nA collection of raw data, commonly (but not exclusively) organized in one of the following formats:\n\na spreadsheet\n\na file in CSV (comma-separated values) format\n\nDataset API (tf.data)\n\n#TensorFlow\n\nA high-level TensorFlow API for reading data and transforming it into a form that a machine learning algorithm requires. A tf.data.Dataset object represents a sequence of elements, in which each element contains one or more Tensors. A tf.data.Iterator object provides access to the elements of a Dataset.\n\ndecision boundary\n\nThe separator between classes learned by a model in a binary class or multi-class classification problems. For example, in the following image representing a binary classification problem, the decision boundary is the frontier between the orange class and the blue class:\n\ndecision forest\n\n#df\n\nA model created from multiple decision trees. A decision forest makes a prediction by aggregating the predictions of its decision trees. Popular types of decision forests include random forests and gradient boosted trees.\n\nSee the Decision Forests section in the Decision Forests course for more information.\n\ndecision threshold\n\nSynonym for classification threshold.\n\ndecision tree\n\n#df\n\nA supervised learning model composed of a set of conditions and leaves organized hierarchically. For example, the following is a decision tree:\n\ndecoder\n\n#language\n\nIn general, any ML system that converts from a processed, dense, or internal representation to a more raw, sparse, or external representation.\n\nDecoders are often a component of a larger model, where they are frequently paired with an encoder.\n\nIn sequence-to-sequence tasks, a decoder starts with the internal state generated by the encoder to predict the next sequence.\n\nRefer to Transformer for the definition of a decoder within the Transformer architecture.\n\nSee Large language models in Machine Learning Crash Course for more information.\n\ndeep model\n\n#fundamentals\n\nA neural network containing more than one hidden layer.\n\nA deep model is also called a deep neural network.\n\nContrast with wide model.\n\ndeep neural network\n\nSynonym for deep model.\n\nDeep Q-Network (DQN)\n\n#rl\n\nIn Q-learning, a deep neural network that predicts Q-functions.\n\nCritic is a synonym for Deep Q-Network.\n\ndemographic parity\n\n#fairness\n\nA fairness metric that is satisfied if the results of a model's classification are not dependent on a given sensitive attribute.\n\nFor example, if both Lilliputians and Brobdingnagians apply to Glubbdubdrib University, demographic parity is achieved if the percentage of Lilliputians admitted is the same as the percentage of Brobdingnagians admitted, irrespective of whether one group is on average more qualified than the other.\n\nContrast with equalized odds and equality of opportunity, which permit classification results in aggregate to depend on sensitive attributes, but don't permit classification results for certain specified ground truth labels to depend on sensitive attributes. See \"Attacking discrimination with smarter machine learning\" for a visualization exploring the tradeoffs when optimizing for demographic parity.\n\nSee Fairness: demographic parity in Machine Learning Crash Course for more information.\n\ndenoising\n\n#language\n\nA common approach to self-supervised learning in which:\n\nNoise is artificially added to the dataset.\n\nThe model tries to remove the noise.\n\nDenoising enables learning from unlabeled examples. The original dataset serves as the target or label and the noisy data as the input.\n\nSome masked language models use denoising as follows:\n\nNoise is artificially added to an unlabeled sentence by masking some of the tokens.\n\nThe model tries to predict the original tokens.\n\ndense feature\n\n#fundamentals\n\nA feature in which most or all values are nonzero, typically a Tensor of floating-point values. For example, the following 10-element Tensor is dense because 9 of its values are nonzero:\n\n8 3 7 5 2 4 0 4 9 6\n\nContrast with sparse feature.\n\ndense layer\n\nSynonym for fully connected layer.\n\ndepth\n\n#fundamentals\n\nThe sum of the following in a neural network:\n\nthe number of hidden layers\n\nthe number of output layers, which is typically 1\n\nthe number of any embedding layers\n\nFor example, a neural network with five hidden layers and one output layer has a depth of 6.\n\nNotice that the input layer doesn't influence depth.\n\ndepthwise separable convolutional neural network (sepCNN)\n\n#image\n\nA convolutional neural network architecture based on Inception, but where Inception modules are replaced with depthwise separable convolutions. Also known as Xception.\n\nA depthwise separable convolution (also abbreviated as separable convolution) factors a standard 3D convolution into two separate convolution operations that are more computationally efficient: first, a depthwise convolution, with a depth of 1 (n ✕ n ✕ 1), and then second, a pointwise convolution, with length and width of 1 (1 ✕ 1 ✕ n).\n\nTo learn more, see Xception: Deep Learning with Depthwise Separable Convolutions.\n\nderived label\n\nSynonym for proxy label.\n\ndevice\n\n#TensorFlow\n\n#GoogleCloud\n\nAn overloaded term with the following two possible definitions:\n\nA category of hardware that can run a TensorFlow session, including CPUs, GPUs, and TPUs.\n\nWhen training an ML model on accelerator chips (GPUs or TPUs), the part of the system that actually manipulates tensors and embeddings. The device runs on accelerator chips. In contrast, the host typically runs on a CPU.\n\ndifferential privacy\n\nIn machine learning, an anonymization approach to protect any sensitive data (for example, an individual's personal information) included in a model's training set from being exposed. This approach ensures that the model doesn't learn or remember much about a specific individual. This is accomplished by sampling and adding noise during model training to obscure individual data points, mitigating the risk of exposing sensitive training data.\n\nDifferential privacy is also used outside of machine learning. For example, data scientists sometimes use differential privacy to protect individual privacy when computing product usage statistics for different demographics.\n\ndimension reduction\n\nDecreasing the number of dimensions used to represent a particular feature in a feature vector, typically by converting to an embedding vector.\n\ndimensions\n\nOverloaded term having any of the following definitions:\n\nThe number of levels of coordinates in a Tensor. For example:\n\nA scalar has zero dimensions; for example, [\"Hello\"].\n\nA vector has one dimension; for example, [3, 5, 7, 11].\n\nA matrix has two dimensions; for example, [[2, 4, 18], [5, 7, 14]]. You can uniquely specify a particular cell in a one-dimensional vector with one coordinate; you need two coordinates to uniquely specify a particular cell in a two-dimensional matrix.\n\nThe number of entries in a feature vector.\n\nThe number of elements in an embedding layer.\n\ndirect prompting\n\n#language\n\n#generativeAI\n\nSynonym for zero-shot prompting.\n\ndiscrete feature\n\n#fundamentals\n\nA feature with a finite set of possible values. For example, a feature whose values may only be animal, vegetable, or mineral is a discrete (or categorical) feature.\n\nContrast with continuous feature.\n\ndiscriminative model\n\nA model that predicts labels from a set of one or more features. More formally, discriminative models define the conditional probability of an output given the features and weights; that is:\n\np(output | features, weights)\n\nFor example, a model that predicts whether an email is spam from features and weights is a discriminative model.\n\nThe vast majority of supervised learning models, including classification and regression models, are discriminative models.\n\nContrast with generative model.\n\ndiscriminator\n\nA system that determines whether examples are real or fake.\n\nAlternatively, the subsystem within a generative adversarial network that determines whether the examples created by the generator are real or fake.\n\nSee The discriminator in the GAN course for more information.\n\ndisparate impact\n\n#fairness\n\nMaking decisions about people that impact different population subgroups disproportionately. This usually refers to situations where an algorithmic decision-making process harms or benefits some subgroups more than others.\n\nFor example, suppose an algorithm that determines a Lilliputian's eligibility for a miniature-home loan is more likely to classify them as \"ineligible\" if their mailing address contains a certain postal code. If Big-Endian Lilliputians are more likely to have mailing addresses with this postal code than Little-Endian Lilliputians, then this algorithm may result in disparate impact.\n\nContrast with disparate treatment, which focuses on disparities that result when subgroup characteristics are explicit inputs to an algorithmic decision-making process.\n\ndisparate treatment\n\n#fairness\n\nFactoring subjects' sensitive attributes into an algorithmic decision-making process such that different subgroups of people are treated differently.\n\nFor example, consider an algorithm that determines Lilliputians' eligibility for a miniature-home loan based on the data they provide in their loan application. If the algorithm uses a Lilliputian's affiliation as Big-Endian or Little-Endian as an input, it is enacting disparate treatment along that dimension.\n\nContrast with disparate impact, which focuses on disparities in the societal impacts of algorithmic decisions on subgroups, irrespective of whether those subgroups are inputs to the model.\n\ndistillation\n\n#generativeAI\n\nThe process of reducing the size of one model (known as the teacher) into a smaller model (known as the student) that emulates the original model's predictions as faithfully as possible. Distillation is useful because the smaller model has two key benefits over the larger model (the teacher):\n\nFaster inference time\n\nReduced memory and energy usage\n\nHowever, the student's predictions are typically not as good as the teacher's predictions.\n\nDistillation trains the student model to minimize a loss function based on the difference between the outputs of the predictions of the student and teacher models.\n\nCompare and contrast distillation with the following terms:\n\nfine-tuning\n\nprompt-based learning\n\nSee LLMs: Fine-tuning, distillation, and prompt engineering in Machine Learning Crash Course for more information.\n\ndistribution\n\nThe frequency and range of different values for a given feature or label. A distribution captures how likely a particular value is.\n\nThe following image shows histograms of two different distributions:\n\nOn the left, a power law distribution of wealth versus the number of people possessing that wealth.\n\nOn the right, a normal distribution of height versus the number of people possessing that height.\n\nUnderstanding each feature and label's distribution can help you determine how to normalize values and detect outliers.\n\nThe phrase out of distribution refers to a value that doesn't appear in the dataset or is very rare. For example, an image of the planet Saturn would be considered out of distribution for a dataset consisting of cat images.\n\ndivisive clustering\n\n#clustering\n\nSee hierarchical clustering.\n\ndownsampling\n\n#image\n\nOverloaded term that can mean either of the following:\n\nReducing the amount of information in a feature in order to train a model more efficiently. For example, before training an image recognition model, downsampling high-resolution images to a lower-resolution format.\n\nTraining on a disproportionately low percentage of over-represented class examples in order to improve model training on under-represented classes. For example, in a class-imbalanced dataset, models tend to learn a lot about the majority class and not enough about the minority class. Downsampling helps balance the amount of training on the majority and minority classes.\n\nSee Datasets: Imbalanced datasets in Machine Learning Crash Course for more information.\n\nDQN\n\n#rl\n\nAbbreviation for Deep Q-Network.\n\ndropout regularization\n\nA form of regularization useful in training neural networks. Dropout regularization removes a random selection of a fixed number of the units in a network layer for a single gradient step. The more units dropped out, the stronger the regularization. This is analogous to training the network to emulate an exponentially large ensemble of smaller networks. For full details, see Dropout: A Simple Way to Prevent Neural Networks from Overfitting.\n\ndynamic\n\n#fundamentals\n\nSomething done frequently or continuously. The terms dynamic and online are synonyms in machine learning. The following are common uses of dynamic and online in machine learning:\n\nA dynamic model (or online model) is a model that is retrained frequently or continuously.\n\nDynamic training (or online training) is the process of training frequently or continuously.\n\nDynamic inference (or online inference) is the process of generating predictions on demand.\n\ndynamic model\n\n#fundamentals\n\nA model that is frequently (maybe even continuously) retrained. A dynamic model is a \"lifelong learner\" that constantly adapts to evolving data. A dynamic model is also known as an online model.\n\nContrast with static model.\n\nE\n\neager execution\n\n#TensorFlow\n\nA TensorFlow programming environment in which operations run immediately. In contrast, operations called in graph execution don't run until they are explicitly evaluated. Eager execution is an imperative interface, much like the code in most programming languages. Eager execution programs are generally far easier to debug than graph execution programs.\n\nearly stopping\n\n#fundamentals\n\nA method for regularization that involves ending training before training loss finishes decreasing. In early stopping, you intentionally stop training the model when the loss on a validation dataset starts to increase; that is, when generalization performance worsens.\n\nClick the icon for additional notes.\n\nEarly stopping may seem counterintuitive. After all, telling a model to halt training while the loss is still decreasing may seem like telling a chef to stop cooking before the dessert has fully baked. However, training a model for too long can lead to overfitting. That is, if you train a model too long, the model may fit the training data so closely that the model doesn't make good predictions on new examples.\n\nearth mover's distance (EMD)\n\nA measure of the relative similarity of two distributions. The lower the earth mover's distance, the more similar the distributions.\n\nedit distance\n\n#language\n\nA measurement of how similar two text strings are to each other. In machine learning, edit distance is useful because it is simple to compute, and an effective way to compare two strings that are known to be similar or to find strings that are similar to a given string.\n\nThere are several definitions of edit distance, each using different string operations. For example, the Levenshtein distance considers the fewest delete, insert, and substitute operations.\n\nFor example, the Levenshtein distance between the words \"heart\" and \"darts\" is 3 because the following 3 edits are the fewest changes to turn one word into the other:\n\nheart → deart (substitute \"h\" with \"d\")\n\ndeart → dart (delete \"e\")\n\ndart → darts (insert \"s\")\n\nEinsum notation\n\nAn efficient notation for describing how two tensors are to be combined. The tensors are combined by multiplying the elements of one tensor by the elements of the other tensor and then summing the products. Einsum notation uses symbols to identify the axes of each tensor, and those same symbols are rearranged to specify the shape of the new resulting tensor.\n\nNumPy provides a common Einsum implementation.\n\nembedding layer\n\n#language\n\n#fundamentals\n\nA special hidden layer that trains on a high-dimensional categorical feature to gradually learn a lower dimension embedding vector. An embedding layer enables a neural network to train far more efficiently than training just on the high-dimensional categorical feature.\n\nFor example, Earth currently supports about 73,000 tree species. Suppose tree species is a feature in your model, so your model's input layer includes a one-hot vector 73,000 elements long. For example, perhaps baobab would be represented something like this:\n\nA 73,000-element array is very long. If you don't add an embedding layer to the model, training is going to be very time consuming due to multiplying 72,999 zeros. Perhaps you pick the embedding layer to consist of 12 dimensions. Consequently, the embedding layer will gradually learn a new embedding vector for each tree species.\n\nIn certain situations, hashing is a reasonable alternative to an embedding layer.\n\nembedding space\n\n#language\n\nThe d-dimensional vector space that features from a higher-dimensional vector space are mapped to. Ideally, the embedding space contains a structure that yields meaningful mathematical results; for example, in an ideal embedding space, addition and subtraction of embeddings can solve word analogy tasks.\n\nThe dot product of two embeddings is a measure of their similarity.\n\nembedding vector\n\n#language\n\nBroadly speaking, an array of floating-point numbers taken from any hidden layer that describe the inputs to that hidden layer. Often, an embedding vector is the array of floating-point numbers trained in an embedding layer. For example, suppose an embedding layer must learn an embedding vector for each of the 73,000 tree species on Earth. Perhaps the following array is the embedding vector for a baobab tree:\n\nAn embedding vector is not a bunch of random numbers. An embedding layer determines these values through training, similar to the way a neural network learns other weights during training. Each element of the array is a rating along some characteristic of a tree species. Which element represents which tree species' characteristic? That's very hard for humans to determine.\n\nThe mathematically remarkable part of an embedding vector is that similar items have similar sets of floating-point numbers. For example, similar tree species have a more similar set of floating-point numbers than dissimilar tree species. Redwoods and sequoias are related tree species, so they'll have a more similar set of floating-pointing numbers than redwoods and coconut palms. The numbers in the embedding vector will change each time you retrain the model, even if you retrain the model with identical input.\n\nempirical cumulative distribution function (eCDF or EDF)\n\nA cumulative distribution function based on empirical measurements from a real dataset. The value of the function at any point along the x-axis is the fraction of observations in the dataset that are less than or equal to the specified value.\n\nempirical risk minimization (ERM)\n\nChoosing the function that minimizes loss on the training set. Contrast with structural risk minimization.\n\nencoder\n\n#language\n\nIn general, any ML system that converts from a raw, sparse, or external representation into a more processed, denser, or more internal representation.\n\nEncoders are often a component of a larger model, where they are frequently paired with a decoder. Some Transformers pair encoders with decoders, though other Transformers use only the encoder or only the decoder.\n\nSome systems use the encoder's output as the input to a classification or regression network.\n\nIn sequence-to-sequence tasks, an encoder takes an input sequence and returns an internal state (a vector). Then, the decoder uses that internal state to predict the next sequence.\n\nRefer to Transformer for the definition of an encoder in the Transformer architecture.\n\nensemble\n\nA collection of models trained independently whose predictions are averaged or aggregated. In many cases, an ensemble produces better predictions than a single model. For example, a random forest is an ensemble built from multiple decision trees. Note that not all decision forests are ensembles.\n\nentropy\n\n#df\n\nIn information theory, a description of how unpredictable a probability distribution is. Alternatively, entropy is also defined as how much information each example contains. A distribution has the highest possible entropy when all values of a random variable are equally likely.\n\nThe entropy of a set with two possible values \"0\" and \"1\" (for example, the labels in a binary classification problem) has the following formula:\n\nH = -p log p - q log q = -p log p - (1-p) * log (1-p)\n\nwhere:\n\nH is the entropy.\n\np is the fraction of \"1\" examples.\n\nq is the fraction of \"0\" examples. Note that q = (1 - p)\n\nlog is generally log2. In this case, the entropy unit is a bit.\n\nFor example, suppose the following:\n\n100 examples contain the value \"1\"\n\n300 examples contain the value \"0\"\n\nTherefore, the entropy value is:\n\np = 0.25\n\nq = 0.75\n\nH = (-0.25)log2(0.25) - (0.75)log2(0.75) = 0.81 bits per example\n\nA set that is perfectly balanced (for example, 200 \"0\"s and 200 \"1\"s) would have an entropy of 1.0 bit per example. As a set becomes more imbalanced, its entropy moves towards 0.0.\n\nIn decision trees, entropy helps formulate information gain to help the splitter select the conditions during the growth of a classification decision tree.\n\nCompare entropy with:\n\ngini impurity\n\ncross-entropy loss function\n\nEntropy is often called Shannon's entropy.\n\nenvironment\n\n#rl\n\nIn reinforcement learning, the world that contains the agent and allows the agent to observe that world's state. For example, the represented world can be a game like chess, or a physical world like a maze. When the agent applies an action to the environment, then the environment transitions between states.\n\nepisode\n\n#rl\n\nIn reinforcement learning, each of the repeated attempts by the agent to learn an environment.\n\nepoch\n\n#fundamentals\n\nA full training pass over the entire training set such that each example has been processed once.\n\nAn epoch represents N/batch size training iterations, where N is the total number of examples.\n\nFor instance, suppose the following:\n\nThe dataset consists of 1,000 examples.\n\nThe batch size is 50 examples.\n\nTherefore, a single epoch requires 20 iterations:\n\n1 epoch = (N/batch size) = (1,000 / 50) = 20 iterations\n\nepsilon greedy policy\n\n#rl\n\nIn reinforcement learning, a policy that either follows a random policy with epsilon probability or a greedy policy otherwise. For example, if epsilon is 0.9, then the policy follows a random policy 90% of the time and a greedy policy 10% of the time.\n\nOver successive episodes, the algorithm reduces epsilon's value in order to shift from following a random policy to following a greedy policy. By shifting the policy, the agent first randomly explores the environment and then greedily exploits the results of random exploration.\n\nequality of opportunity\n\n#fairness\n\nA fairness metric to assess whether a model is predicting the desirable outcome equally well for all values of a sensitive attribute. In other words, if the desirable outcome for a model is the positive class, the goal would be to have the true positive rate be the same for all groups.\n\nEquality of opportunity is related to equalized odds, which requires that both the true positive rates and false positive rates are the same for all groups.\n\nSuppose Glubbdubdrib University admits both Lilliputians and Brobdingnagians to a rigorous mathematics program. Lilliputians' secondary schools offer a robust curriculum of math classes, and the vast majority of students are qualified for the university program. Brobdingnagians' secondary schools don't offer math classes at all, and as a result, far fewer of their students are qualified. Equality of opportunity is satisfied for the preferred label of \"admitted\" with respect to nationality (Lilliputian or Brobdingnagian) if qualified students are equally likely to be admitted irrespective of whether they're a Lilliputian or a Brobdingnagian.\n\nFor example, suppose 100 Lilliputians and 100 Brobdingnagians apply to Glubbdubdrib University, and admissions decisions are made as follows:\n\nTable 1. Lilliputian applicants (90% are qualified)\n\nQualified Unqualified Admitted 45 3 Rejected 45 7 Total 90 10 Percentage of qualified students admitted: 45/90 = 50%\n\nPercentage of unqualified students rejected: 7/10 = 70%\n\nTotal percentage of Lilliputian students admitted: (45+3)/100 = 48%\n\nTable 2. Brobdingnagian applicants (10% are qualified):\n\nQualified Unqualified Admitted 5 9 Rejected 5 81 Total 10 90 Percentage of qualified students admitted: 5/10 = 50%\n\nPercentage of unqualified students rejected: 81/90 = 90%\n\nTotal percentage of Brobdingnagian students admitted: (5+9)/100 = 14%\n\nThe preceding examples satisfy equality of opportunity for acceptance of qualified students because qualified Lilliputians and Brobdingnagians both have a 50% chance of being admitted.\n\nWhile equality of opportunity is satisfied, the following two fairness metrics are not satisfied:\n\ndemographic parity: Lilliputians and Brobdingnagians are admitted to the university at different rates; 48% of Lilliputians students are admitted, but only 14% of Brobdingnagian students are admitted.\n\nequalized odds: While qualified Lilliputian and Brobdingnagian students both have the same chance of being admitted, the additional constraint that unqualified Lilliputians and Brobdingnagians both have the same chance of being rejected is not satisfied. Unqualified Lilliputians have a 70% rejection rate, whereas unqualified Brobdingnagians have a 90% rejection rate.\n\nSee \"Equality of Opportunity in Supervised Learning\" for a more detailed discussion of equality of opportunity. Also see \"Attacking discrimination with smarter machine learning\" for a visualization exploring the tradeoffs when optimizing for equality of opportunity.\n\nequalized odds\n\n#fairness\n\nA fairness metric to assess whether a model is predicting outcomes equally well for all values of a sensitive attribute with respect to both the positive class and negative class—not just one class or the other exclusively. In other words, both the true positive rate and false negative rate should be the same for all groups.\n\nEqualized odds is related to equality of opportunity, which only focuses on error rates for a single class (positive or negative).\n\nFor example, suppose Glubbdubdrib University admits both Lilliputians and Brobdingnagians to a rigorous mathematics program. Lilliputians' secondary schools offer a robust curriculum of math classes, and the vast majority of students are qualified for the university program. Brobdingnagians' secondary schools don't offer math classes at all, and as a result, far fewer of their students are qualified. Equalized odds is satisfied provided that no matter whether an applicant is a Lilliputian or a Brobdingnagian, if they are qualified, they are equally as likely to get admitted to the program, and if they are not qualified, they are equally as likely to get rejected.\n\nSuppose 100 Lilliputians and 100 Brobdingnagians apply to Glubbdubdrib University, and admissions decisions are made as follows:\n\nTable 3. Lilliputian applicants (90% are qualified)\n\nQualified Unqualified Admitted 45 2 Rejected 45 8 Total 90 10 Percentage of qualified students admitted: 45/90 = 50%\n\nPercentage of unqualified students rejected: 8/10 = 80%\n\nTotal percentage of Lilliputian students admitted: (45+2)/100 = 47%\n\nTable 4. Brobdingnagian applicants (10% are qualified):\n\nQualified Unqualified Admitted 5 18 Rejected 5 72 Total 10 90 Percentage of qualified students admitted: 5/10 = 50%\n\nPercentage of unqualified students rejected: 72/90 = 80%\n\nTotal percentage of Brobdingnagian students admitted: (5+18)/100 = 23%\n\nEqualized odds is satisfied because qualified Lilliputian and Brobdingnagian students both have a 50% chance of being admitted, and unqualified Lilliputian and Brobdingnagian have an 80% chance of being rejected.\n\nEqualized odds is formally defined in \"Equality of Opportunity in Supervised Learning\" as follows: \"predictor Ŷ satisfies equalized odds with respect to protected attribute A and outcome Y if Ŷ and A are independent, conditional on Y.\"\n\nEstimator\n\n#TensorFlow\n\nA deprecated TensorFlow API. Use tf.keras instead of Estimators.\n\nevals\n\n#language\n\n#generativeAI\n\nPrimarily used as an abbreviation for LLM evaluations. More broadly, evals is an abbreviation for any form of evaluation.\n\nevaluation\n\n#language\n\n#generativeAI\n\nThe process of measuring a model's quality or comparing different models against each other.\n\nTo evaluate a supervised machine learning model, you typically judge it against a validation set and a test set. Evaluating a LLM typically involves broader quality and safety assessments.\n\nexample\n\n#fundamentals\n\nThe values of one row of features and possibly a label. Examples in supervised learning fall into two general categories:\n\nA labeled example consists of one or more features and a label. Labeled examples are used during training.\n\nAn unlabeled example consists of one or more features but no label. Unlabeled examples are used during inference.\n\nFor instance, suppose you are training a model to determine the influence of weather conditions on student test scores. Here are three labeled examples:\n\nFeatures Label Temperature Humidity Pressure Test score 15 47 998 Good 19 34 1020 Excellent 18 92 1012 Poor\n\nHere are three unlabeled examples:\n\nTemperature Humidity Pressure 12 62 1014 21 47 1017 19 41 1021\n\nThe row of a dataset is typically the raw source for an example. That is, an example typically consists of a subset of the columns in the dataset. Furthermore, the features in an example can also include synthetic features, such as feature crosses.\n\nexperience replay\n\n#rl\n\nIn reinforcement learning, a DQN technique used to reduce temporal correlations in training data. The agent stores state transitions in a replay buffer, and then samples transitions from the replay buffer to create training data.\n\nexperimenter's bias\n\n#fairness\n\nSee confirmation bias.\n\nexploding gradient problem\n\n#seq\n\nThe tendency for gradients in deep neural networks (especially recurrent neural networks) to become surprisingly steep (high). Steep gradients often cause very large updates to the weights of each node in a deep neural network.\n\nModels suffering from the exploding gradient problem become difficult or impossible to train. Gradient clipping can mitigate this problem.\n\nCompare to vanishing gradient problem.\n\nF\n\nF1\n\nA \"roll-up\" binary classification metric that relies on both precision and recall. Here is the formula:\n\n$$F{_1} = \\frac{\\text{2 * precision * recall}} {\\text{precision + recall}}$$\n\nFor example, given the following:\n\nprecision = 0.6\n\nrecall = 0.4\n\n$$F{_1} = \\frac{\\text{2 * 0.6 * 0.4}} {\\text{0.6 + 0.4}} = 0.48$$\n\nWhen precision and recall are fairly similar (as in the preceding example), F1 is close to their mean. When precision and recall differ significantly, F1 is closer to the lower value. For example:\n\nprecision = 0.9\n\nrecall = 0.1\n\n$$F{_1} = \\frac{\\text{2 * 0.9 * 0.1}} {\\text{0.9 + 0.1}} = 0.18$$\n\nfairness constraint\n\n#fairness\n\nApplying a constraint to an algorithm to ensure one or more definitions of fairness are satisfied. Examples of fairness constraints include:\n\nPost-processing your model's output.\n\nAltering the loss function to incorporate a penalty for violating a fairness metric.\n\nDirectly adding a mathematical constraint to an optimization problem.\n\nfairness metric\n\n#fairness\n\nA mathematical definition of \"fairness\" that is measurable. Some commonly used fairness metrics include:\n\nequalized odds\n\npredictive parity\n\ncounterfactual fairness\n\ndemographic parity\n\nMany fairness metrics are mutually exclusive; see incompatibility of fairness metrics.\n\nfalse negative (FN)\n\n#fundamentals\n\nAn example in which the model mistakenly predicts the negative class. For example, the model predicts that a particular email message is not spam (the negative class), but that email message actually is spam.\n\nfalse negative rate\n\nThe proportion of actual positive examples for which the model mistakenly predicted the negative class. The following formula calculates the false negative rate:\n\n$$\\text{false negative rate} = \\frac{\\text{false negatives}}{\\text{false negatives} + \\text{true positives}}$$\n\nfalse positive (FP)\n\n#fundamentals\n\nAn example in which the model mistakenly predicts the positive class. For example, the model predicts that a particular email message is spam (the positive class), but that email message is actually not spam.\n\nfalse positive rate (FPR)\n\n#fundamentals\n\nThe proportion of actual negative examples for which the model mistakenly predicted the positive class. The following formula calculates the false positive rate:\n\n$$\\text{false positive rate} = \\frac{\\text{false positives}}{\\text{false positives} + \\text{true negatives}}$$\n\nThe false positive rate is the x-axis in an ROC curve.\n\nfeature\n\n#fundamentals\n\nAn input variable to a machine learning model. An example consists of one or more features. For instance, suppose you are training a model to determine the influence of weather conditions on student test scores. The following table shows three examples, each of which contains three features and one label:\n\nFeatures Label Temperature Humidity Pressure Test score 15 47 998 92 19 34 1020 84 18 92 1012 87\n\nContrast with label.\n\nfeature cross\n\n#fundamentals\n\nA synthetic feature formed by \"crossing\" categorical or bucketed features.\n\nFor example, consider a \"mood forecasting\" model that represents temperature in one of the following four buckets:\n\nfreezing\n\nchilly\n\ntemperate\n\nwarm\n\nAnd represents wind speed in one of the following three buckets:\n\nstill\n\nlight\n\nwindy\n\nWithout feature crosses, the linear model trains independently on each of the preceding seven various buckets. So, the model trains on, for example, freezing independently of the training on, for example, windy.\n\nAlternatively, you could create a feature cross of temperature and wind speed. This synthetic feature would have the following 12 possible values:\n\nfreezing-still\n\nfreezing-light\n\nfreezing-windy\n\nchilly-still\n\nchilly-light\n\nchilly-windy\n\ntemperate-still\n\ntemperate-light\n\ntemperate-windy\n\nwarm-still\n\nwarm-light\n\nwarm-windy\n\nThanks to feature crosses, the model can learn mood differences between a freezing-windy day and a freezing-still day.\n\nIf you create a synthetic feature from two features that each have a lot of different buckets, the resulting feature cross will have a huge number of possible combinations. For example, if one feature has 1,000 buckets and the other feature has 2,000 buckets, the resulting feature cross has 2,000,000 buckets.\n\nFormally, a cross is a Cartesian product.\n\nFeature crosses are mostly used with linear models and are rarely used with neural networks.\n\nfeature engineering\n\n#fundamentals\n\n#TensorFlow\n\nA process that involves the following steps:\n\nDetermining which features might be useful in training a model.\n\nConverting raw data from the dataset into efficient versions of those features.\n\nFor example, you might determine that temperature might be a useful feature. Then, you might experiment with bucketing to optimize what the model can learn from different temperature ranges.\n\nFeature engineering is sometimes called feature extraction or featurization.\n\nClick the icon for additional notes about TensorFlow.\n\nIn TensorFlow, feature engineering often means converting raw log file entries to tf.Example protocol buffers. See also tf.Transform.\n\nfeature extraction\n\nOverloaded term having either of the following definitions:\n\nRetrieving intermediate feature representations calculated by an unsupervised or pretrained model (for example, hidden layer values in a neural network) for use in another model as input.\n\nSynonym for feature engineering.\n\nfeature importances\n\n#df\n\nSynonym for variable importances.\n\nfeature set\n\n#fundamentals\n\nThe group of features your machine learning model trains on. For example, postal code, property size, and property condition might comprise a simple feature set for a model that predicts housing prices.\n\nfeature spec\n\n#TensorFlow\n\nDescribes the information required to extract features data from the tf.Example protocol buffer. Because the tf.Example protocol buffer is just a container for data, you must specify the following:\n\nThe data to extract (that is, the keys for the features)\n\nThe data type (for example, float or int)\n\nThe length (fixed or variable)\n\nfeature vector\n\n#fundamentals\n\nThe array of feature values comprising an example. The feature vector is input during training and during inference. For example, the feature vector for a model with two discrete features might be:\n\n[0.92, 0.56]\n\nEach example supplies different values for the feature vector, so the feature vector for the next example could be something like:\n\n[0.73, 0.49]\n\nFeature engineering determines how to represent features in the feature vector. For example, a binary categorical feature with five possible values might be represented with one-hot encoding. In this case, the portion of the feature vector for a particular example would consist of four zeroes and a single 1.0 in the third position, as follows:\n\n[0.0, 0.0, 1.0, 0.0, 0.0]\n\nAs another example, suppose your model consists of three features:\n\na binary categorical feature with five possible values represented with one-hot encoding; for example: [0.0, 1.0, 0.0, 0.0, 0.0]\n\nanother binary categorical feature with three possible values represented with one-hot encoding; for example: [0.0, 0.0, 1.0]\n\na floating-point feature; for example: 8.3.\n\nIn this case, the feature vector for each example would be represented by nine values. Given the example values in the preceding list, the feature vector would be:\n\n0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 8.3\n\nfeaturization\n\nThe process of extracting features from an input source, such as a document or video, and mapping those features into a feature vector.\n\nSome ML experts use featurization as a synonym for feature engineering or feature extraction.\n\nfederated learning\n\nA distributed machine learning approach that trains machine learning models using decentralized examples residing on devices such as smartphones. In federated learning, a subset of devices downloads the current model from a central coordinating server. The devices use the examples stored on the devices to make improvements to the model. The devices then upload the model improvements (but not the training examples) to the coordinating server, where they are aggregated with other updates to yield an improved global model. After the aggregation, the model updates computed by devices are no longer needed, and can be discarded.\n\nSince the training examples are never uploaded, federated learning follows the privacy principles of focused data collection and data minimization.\n\nFor more information about federated learning, see this tutorial.\n\nfeedback loop\n\n#fundamentals\n\nIn machine learning, a situation in which a model's predictions influence the training data for the same model or another model. For example, a model that recommends movies will influence the movies that people see, which will then influence subsequent movie recommendation models.\n\nfeedforward neural network (FFN)\n\nA neural network without cyclic or recursive connections. For example, traditional deep neural networks are feedforward neural networks. Contrast with recurrent neural networks, which are cyclic.\n\nfew-shot learning\n\nA machine learning approach, often used for object classification, designed to train effective classifiers from only a small number of training examples.\n\nSee also one-shot learning and zero-shot learning.\n\nfew-shot prompting\n\n#language\n\n#generativeAI\n\nA prompt that contains more than one (a \"few\") example demonstrating how the large language model should respond. For example, the following lengthy prompt contains two examples showing a large language model how to answer a query.\n\nParts of one prompt Notes What is the official currency of the specified country? The question you want the LLM to answer. France: EUR One example. United Kingdom: GBP Another example. India: The actual query.\n\nFew-shot prompting generally produces more desirable results than zero-shot prompting and one-shot prompting. However, few-shot prompting requires a lengthier prompt.\n\nFew-shot prompting is a form of few-shot learning applied to prompt-based learning.\n\nFiddle\n\n#language\n\nA Python-first configuration library that sets the values of functions and classes without invasive code or infrastructure. In the case of Pax—and other ML codebases—these functions and classes represent models and training hyperparameters.\n\nFiddle assumes that machine learning codebases are typically divided into:\n\nLibrary code, which defines the layers and optimizers.\n\nDataset \"glue\" code, which calls the libraries and wires everything together.\n\nFiddle captures the call structure of the glue code in an unevaluated and mutable form.\n\nfine-tuning\n\n#language\n\n#image\n\n#generativeAI\n\nA second, task-specific training pass performed on a pre-trained model to refine its parameters for a specific use case. For example, the full training sequence for some large language models is as follows:\n\nPre-training: Train a large language model on a vast general dataset, such as all the English language Wikipedia pages.\n\nFine-tuning: Train the pre-trained model to perform a specific task, such as responding to medical queries. Fine-tuning typically involves hundreds or thousands of examples focused on the specific task.\n\nAs another example, the full training sequence for a large image model is as follows:\n\nPre-training: Train a large image model on a vast general image dataset, such as all the images in Wikimedia commons.\n\nFine-tuning: Train the pre-trained model to p"
    }
}
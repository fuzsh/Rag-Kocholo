{
    "id": "dbpedia_1581_2",
    "rank": 46,
    "data": {
        "url": "https://encord.com/blog/yolo-object-detection-guide/",
        "read_more_link": "",
        "language": "en",
        "title": "YOLO Object Detection Explained: Evolution, Algorithm, and Applications",
        "top_image": "https://images.prismic.io/encord/8785958f-a555-4179-b52d-909775c15282_YOLOv8+for+Object+detection.webp?auto=compress%2Cformat&fit=max",
        "meta_img": "https://images.prismic.io/encord/8785958f-a555-4179-b52d-909775c15282_YOLOv8+for+Object+detection.webp?auto=compress%2Cformat&fit=max",
        "images": [
            "https://images.prismic.io/encord/8785958f-a555-4179-b52d-909775c15282_YOLOv8+for+Object+detection.webp?auto=compress%2Cformat&fit=max&w=906&h=638",
            "https://images.prismic.io/encord/8785958f-a555-4179-b52d-909775c15282_YOLOv8+for+Object+detection.webp?auto=compress%2Cformat&fit=max&w=906&h=638",
            "https://encord.com/static/VectorDesktop-d6a994f2c668a0332ba39898992e598f.png",
            "https://encord.com/static/VectorTablet-5246b4eeb12ce3a011a59f9a65313af7.png",
            "https://encord.com/static/VectorDesktop-d6a994f2c668a0332ba39898992e598f.png",
            "https://encord.cdn.prismic.io/encord/ZmrVVZm069VX1tfd_Union.svg",
            "https://images.prismic.io/encord/c594c9c7-3edf-4357-9307-92b9ab4d4548_1629105749470.jfif?auto=compress%2Cformat&fit=max&w=80&h=80",
            "https://images.prismic.io/encord/c594c9c7-3edf-4357-9307-92b9ab4d4548_1629105749470.jfif?auto=compress%2Cformat&fit=max&w=80&h=80",
            "https://encord.cdn.prismic.io/encord/Zk3PGCol0Zci9WSy_information.svg",
            "https://images.prismic.io/encord/Zky98iol0Zci9U3b_tryEncordCTADark.png?auto=format,compress",
            "https://a.storyblok.com/f/139616/1200x800/89f31e46c8/structure-of-ssd.webp",
            "https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/63c6a13d5117ffaaa037555e_Overview%20of%20YOLO%20v6-min.jpg",
            "https://images.prismic.io/encord/e3b133a3-fdf1-4ba6-be5f-3063785d8f73_YOLOv1+description.webp?auto=compress,format&rect=0,0,1156,300&w=1156&h=300",
            "https://a.storyblok.com/f/139616/1200x800/c6ef1c79df/iouvisualization.webp",
            "https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/63c697fd4ef3d83d2e35a8c2_YOLO%20architecture-min.jpg",
            "https://a.storyblok.com/f/139616/1200x800/297c23f45f/structure-of-yolo.webp",
            "https://assets-global.website-files.com/614c82ed388d53640613982e/65391f3eddf2672a5a4175d1_use-of-upsampling.webp",
            "https://images.datacamp.com/image/upload/v1664382694/YOL_Ov4_Speed_compared_to_YOL_Ov3_84ed2f07e3.png",
            "https://images.datacamp.com/image/upload/v1665138394/YOLOR_unified_network_architecture_7dbcd0cdcf.png",
            "https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/63c699cf4ef3d8811c35cbc6_Architecture%20of%20the%20EfficientDet%20model-min.jpg",
            "https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/63c6a13d5117ffaaa037555e_Overview%20of%20YOLO%20v6-min.jpg",
            "https://images.prismic.io/encord/7eb10eaf-3efe-437d-9be7-a53c150d7e33_YOLO+timeline.webp?auto=compress,format",
            "https://images.prismic.io/encord/e62368dc-c1da-407d-8200-12f94dfd3171_YOLOv8+comapred+to+v5_+v6%2C+v7.webp?auto=compress,format",
            "https://encord.cdn.prismic.io/encord/Zk3PGCol0Zci9WSy_information.svg",
            "https://images.prismic.io/encord/Zf1zwscYqOFdyByf_image5.png?auto=format,compress",
            "https://encord.cdn.prismic.io/encord/Zk3PGCol0Zci9WSy_information.svg",
            "https://images.prismic.io/encord/Zf1zxMcYqOFdyByg_image6.gif?auto=format,compress",
            "https://images.prismic.io/encord/Zf1zpMcYqOFdyByb_image3.gif?auto=format,compress",
            "https://images.prismic.io/encord/Zf1zmMcYqOFdyByY_image1.gif?auto=format,compress",
            "https://encord.cdn.prismic.io/encord/Zk3PGCol0Zci9WSy_information.svg",
            "https://images.prismic.io/encord/Zf1zwMcYqOFdyBye_image4.gif?auto=format,compress",
            "https://images.prismic.io/encord/Zf1zmscYqOFdyByZ_image2.gif",
            "https://encord.cdn.prismic.io/encord/Zk3PGCol0Zci9WSy_information.svg",
            "https://encord.cdn.prismic.io/encord/Zk3PGCol0Zci9WSy_information.svg",
            "https://images.prismic.io/encord/Zf1zyccYqOFdyByh_image7.gif",
            "https://images.prismic.io/encord/Zf1zy8cYqOFdyByi_image8.png?auto=format,compress",
            "https://encord.cdn.prismic.io/encord/Zk3PGCol0Zci9WSy_information.svg",
            "https://images.prismic.io/encord/Zky98iol0Zci9U3b_tryEncordCTADark.png?auto=format,compress",
            "https://encord.com/static/VectorDesktop-d6a994f2c668a0332ba39898992e598f.png",
            "https://encord.com/static/VectorTablet-5246b4eeb12ce3a011a59f9a65313af7.png",
            "https://encord.com/static/VectorDesktop-d6a994f2c668a0332ba39898992e598f.png",
            "https://encord.cdn.prismic.io/encord/ZmrVVZm069VX1tfd_Union.svg",
            "https://images.prismic.io/encord/c594c9c7-3edf-4357-9307-92b9ab4d4548_1629105749470.jfif?auto=compress%2Cformat&fit=max&w=80&h=80",
            "https://images.prismic.io/encord/c594c9c7-3edf-4357-9307-92b9ab4d4548_1629105749470.jfif?auto=compress%2Cformat&fit=max&w=80&h=80",
            "https://images.prismic.io/encord/65d884c43a605798c18c27dc_inDex%404x.jpg?auto=format%2Ccompress&fit=max",
            "https://encord.cdn.prismic.io/encord/83549f38-37be-426c-a312-5107f575c736_testing.svg?fit=max",
            "https://encord.cdn.prismic.io/encord/40c1a4a5-3714-4908-8b2e-d7526a58d413_Annotate.svg?auto=compress%2Cformat&fit=max",
            "https://encord.cdn.prismic.io/encord/684dae60-2526-4403-8005-847e6b65480b_annotate-icon.svg?fit=max",
            "https://images.prismic.io/encord/6939c450-7b22-4d84-844d-682cc72c89f9_ProductCrad1.png?auto=format%2Ccompress&fit=max",
            "https://encord.cdn.prismic.io/encord/fe00a610-84b5-4bed-987a-0fe349e61c74_productIcons.svg?fit=max",
            "https://cdn.drata.com/badge/soc2-dark.png",
            "https://images.prismic.io/encord/d5a5f02e-d8df-49c2-9413-5633a8e75e7d_soc2-certificate.png?auto=compress,format",
            "https://encord.cdn.prismic.io/encord/ZoZ1tR5LeNNTwyYw_g22024.svg",
            "https://dc.ads.linkedin.com/collect/?pid=4241362&fmt=gif"
        ],
        "movies": [
            "https://www.youtube.com/embed/svn9-xV7wjk?start=167&feature=oembed",
            "https://www.youtube.com/embed/bt3JLYfFaKE?start=7&feature=oembed",
            "https://www.youtube.com/embed/hfUud3qh3_o?feature=oembed"
        ],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Nikolaj Buhl"
        ],
        "publish_date": "2023-02-08T02:15:04+00:00",
        "summary": "",
        "meta_description": "Learn about the YOLO architecture and real-time object detection algorithm, including different YOLO versions and applications, and how to custom-train YOLOv9 models with Encord.",
        "meta_lang": "en",
        "meta_favicon": "/apple-touch-icon.png",
        "meta_site_name": "",
        "canonical_link": "https://encord.com/blog/yolo-object-detection-guide/",
        "text": "What is Object Detection?\n\nObject detection is a critical capability of computer vision that identifies and locates objects within an image or video. Unlike image classification, object detection not only classifies the objects in an image, but also identifies their location within the image by drawing a bounding box around each object. Object detection models, such as R-CNN, Fast R-CNN, Faster R-CNN, and YOLO, use convolutional neural networks (CNNs) to classify the objects and regressor networks to accurately predict the bounding box coordinates for each detected object.\n\nImage Classification\n\nImage classification is a fundamental task in computer vision. Given an input image, the goal of an image classification model is to assign it to one of a pre-defined set of classes. Most image classification models use CNNs, which are specifically designed to process pixel data and can capture spatial features. Image classification models are trained on large datasets (like ImageNet) and can classify images into 1000 object categories, such as keyboard, mouse, pencil, and many animals.\n\nObject Localization\n\nObject localization is another important task in computer vision that identifies the location of an object in the image. It extends the image classification model by adding a regression head to predict the bounding box coordinates of the object. The bounding box is typically represented by four coordinates that define its position and size. Object localization is a key step in object detection, where the goal is not just to classify the primary object of interest in the image, but also to identify its location.\n\nCreate training & fine-tuning data for building YOLO models 10x faster\n\nClassification of Object Detection Algorithms\n\nObject detection algorithms can be broadly classified into two categories: single-shot detectors and two-shot(or multi-shot) detectors. These two types of algorithms have different approaches to the task of object detection.\n\nSingle-Shot Object Detection\n\nSingle-shot detectors (SSDs) are a type of object detection algorithm that predict the bounding box and the class of the object in one single shot. This means that in a single forward pass of the network, the presence of an object and the bounding box are predicted simultaneously. This makes SSDs very fast and efficient, suitable for tasks that require real-time detection.\n\nStructure of SSD\n\nExamples of single-shot object detection algorithms include YOLO (You Only Look Once) and SSD (Single Shot MultiBox Detector). YOLO divides the input image into a grid and for each grid cell, predicts a certain number of bounding boxes and class probabilities. SSD, on the other hand, predicts bounding boxes and class probabilities at multiple scales in different feature maps.\n\nTwo-Shot Object Detection\n\nTwo-shot or multi-shot object detection algorithms, on the other hand, use a two-step process for detecting objects. The first step involves proposing a series of bounding boxes that could potentially contain an object. This is often done using a method called region proposal. The second step involves running these proposed regions through a convolutional neural network to classify the object classes within the box.\n\nExamples of two-shot object detection algorithms include R-CNN (Regions with CNN features), Fast R-CNN, and Faster R-CNN. These algorithms use region proposal networks (RPNs) to propose potential bounding boxes and then use CNNs to classify the proposed regions.\n\nBoth single-shot and two-shot detectors have their strengths and weaknesses. Single-shot detectors are generally faster and more efficient, making them suitable for real-time object detection tasks. Two-shot detectors, while slower and more computationally intensive, tend to be more accurate, as they can afford to spend more computational resources on each potential object.\n\nObject Detection Methods\n\nObject Detection: Non-Neural Methods\n\nViola-Jones object detection method based on Haar features\n\nThe Viola-Jones method, introduced by Paul Viola and Michael Jones, is a machine learning model for object detection. It uses a cascade of classifiers, selecting features from Haar-like feature sets. The algorithm has four stages:\n\nHaar Feature Selection\n\nCreating an Integral Image\n\nAdaboost Training\n\nCascading Classifiers\n\nDespite its simplicity and speed, it can achieve high detection rates.\n\nScale-Invariant Feature Transform (SIFT)\n\nSIFT is a method for extracting distinctive invariant features from images. These features are invariant to image scale and rotation, and are robust to changes in viewpoint, noise, and illumination. SIFT features are used to match different views of an object or scene.\n\nHistogram of Oriented Gradients (HOG)\n\nHOG is a feature descriptor used for object detection in computer vision. It involves counting the occurrences of gradient orientation in localized portions of an image. This method is similar to edge orientation histograms, scale-invariant feature transform descriptors, and shape contexts, but differs in that it is computed on a dense grid of uniformly spaced cells.\n\nObject Detection: Neural Methods\n\nRegion-Based Convolutional Neural Networks (R-CNN)\n\nRegion-Based CNN uses convolutional neural networks to classify image regions in order to detect objects. It involves training a CNN on a large labeled dataset and then using the trained network to detect objects in new images. Region-Based CNN and its successors, Fast R-CNN and Faster R-CNN, are known for their accuracy but can be computationally intensive.\n\nFaster R-CNN\n\nFaster R-CNN is an advanced version of R-CNN that introduces a Region Proposal Network (RPN) for generating region proposals. The RPN shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. The RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. Faster R-CNN is faster than the original R-CNN and Fast R-CNN because it doesn’t need to run a separate region proposal method on the image, which can be slow.\n\nMask R-CNN\n\nMask R-CNN extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. This allows Mask R-CNN to generate precise segmentation masks for each detected object, in addition to the class label and bounding box. The mask branch is a small fully convolutional network applied to each RoI, predicting a binary mask for each RoI. Mask R-CNN is simple to train and adds only a small computational overhead, enabling a fast system and rapid experimentation.\n\nSingle Shot Detector (SSD)\n\nSSD is a method for object detection that eliminates the need for multiple network passes for multiple scales. It discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. SSD is faster than methods like R-CNN because it eliminates bounding box proposals and pooling layers.\n\nRetinaNet\n\nRetinaNet uses a feature pyramid network on top of a backbone to detect objects at different scales and aspect ratios. It introduces a new loss, the Focal Loss, to deal with the foreground-background class imbalance problem. RetinaNet is designed to handle dense and small objects.\n\nEfficientDet\n\nEfficientDet is a method that scales all dimensions of the network width, depth, and resolution with a compound scaling method to achieve better performance. It introduces a new architecture, called BiFPN, which allows easy and efficient multi-scale feature fusion, and a new scaling method that uniformly scales the resolution, depth, and width for all backbone, feature network, and box/class prediction networks at the same time. EfficientDet achieves state-of-the-art accuracy with fewer parameters and less computation compared to previous detectors.\n\nYou Only Look Once (YOLO)\n\nYOLO, developed by Joseph Redmon et al., frames object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. It looks at the whole image at test time so its predictions are informed by global context in the image. YOLO is known for its speed, making it suitable for real-time applications.\n\nYou Only Look Once: Unified, Real-Time Object Detection\n\nObject Detection: Performance Evaluation Metrics\n\nIntersection over Union (IoU)\n\nIoU (Intersection over Union) Calculation\n\nIntersection over Union (IoU) is a common metric used to evaluate the performance of an object detection algorithm. It measures the overlap between the predicted bounding box (P) and the ground truth bounding box (G). The IoU is calculated as the area of intersection divided by the area of union of P and G.\n\nThe IoU score ranges from 0 to 1, where 0 indicates no overlap and 1 indicates a perfect match. A higher IoU score indicates a more accurate object detection.\n\nAverage Precision (AP)\n\nAverage Precision (AP) is another important metric used in object detection. It summarizes the precision-recall curve that is created by varying the detection threshold.\n\nPrecision is the proportion of true positive detections among all positive detections, while recall is the proportion of true positive detections among all actual positives in the image.\n\nThe AP computes the average precision values for recall levels over 0 to 1. The AP score ranges from 0 to 1, where a higher value indicates better performance. The mean Average Precision (mAP) is often used in practice, which calculates the AP for each class and then takes the average.\n\nBy understanding these metrics, we can better interpret the performance of models like YOLO and make informed decisions about their application in real-world scenarios.\n\nAfter exploring various object detection methods and performance evaluation methods, let’s delve into the workings of a particularly powerful and popular algorithm known as ‘You Only Look Once’, or YOLO. This algorithm has revolutionized the field of object detection with its unique approach and impressive speed.\n\nUnlike traditional methods that involve separate steps for identifying objects and classifying them, YOLO accomplishes both tasks in a single pass, hence the name ‘You Only Look Once’.\n\nYOLO Object Detection Algorithm: How Does it Work?\n\nYOLO Architecture\n\nThe YOLO algorithm employs a single Convolutional Neural Network (CNN) that divides the image into a grid. Each cell in the grid predicts a certain number of bounding boxes. Along with each bounding box, the cell also predicts a class probability, which indicates the likelihood of a specific object being present in the box.\n\nConvolution Layers\n\nBounding Box Recognition Process\n\nThe bounding box recognition process in YOLO involves the following steps:\n\nGrid Creation: The image is divided into an SxS grid. Each grid cell is responsible for predicting an object if the object’s center falls within it.\n\nBounding Box Prediction: Each grid cell predicts B bounding boxes and confidence scores for those boxes. The confidence score reflects how certain the model is that a box contains an object and how accurate it thinks the box is.\n\nClass Probability Prediction: Each grid cell also predicts C conditional class probabilities (one per class for the potential objects). These probabilities are conditioned on there being an object in the box.\n\nYOLO Structure\n\nNon-Max Suppression (NMS)\n\nAfter the bounding boxes and class probabilities are predicted, post-processing steps are applied. One such step is Non-Max Suppression (NMS). NMS helps in reducing the number of overlapping bounding boxes. It works by eliminating bounding boxes that have a high overlap with the box that has the highest confidence score.\n\nVector Generalization\n\nVector generalization is a technique used in the YOLO algorithm to handle the high dimensionality of the output. The output of the YOLO algorithm is a tensor that contains the bounding box coordinates, objectness score, and class probabilities.\n\nThis high-dimensional tensor is flattened into a vector to make it easier to process. The vector is then passed through a softmax function to convert the class scores into probabilities. The final output is a vector that contains the bounding box coordinates, objectness score, and class probabilities for each grid cell.\n\nEvolution of YOLO: YOLOv1, YOLOv2, YOLOv3, YOLOv4, YOLOR, YOLOX, YOLOv5, YOLOv6, YOLOv7, YOLOv8, YOLOv9\n\nIf you are not interested in a quick recap of the timeline of YOLO models and the updates in the network architecture, skip this section!\n\nYOLOv1: First Real-time Object Detection Algorithm\n\nThe original YOLO model treated object detection as a regression problem, which was a significant shift from the traditional classification approach. It used a single convolutional neural network (CNN) to detect objects in images by dividing the image into a grid, making multiple predictions per grid cell, filtering out low-confidence predictions, and then removing overlapping boxes to produce the final output.\n\nYOLOv2 [YOLO9000]: Multi-Scale Training| Anchor Boxes| Darknet-19 Backbone\n\nYOLOv2 introduced several improvements over the original YOLO. It used batch normalization in all its convolutional layers, which reduced overfitting and improved model stability and performance. It could handle higher-resolution images, making it better at spotting smaller objects. YOLOv2 also used anchor boxes (borrowed from Faster R-CNN), which helped the algorithm predict the shape and size of objects more accurately.\n\nYOLOv3: Three YOLO Layers| Logistic Classifiers| Upsampling |Darknet-53 Backbone\n\nUpsampling\n\nYOLOv3 introduced a new backbone network, Darknet-53, which utilized residual connections. It also made several design changes to improve accuracy while maintaining speed. At 320x320 resolution, YOLOv3 ran in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. It achieved 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, with similar performance but 3.8x faster.\n\nYOLOv4: CSPDarknet53 | Detection Across Scales | CIOU Loss\n\nSpeed Comparison: YOLOv4 Vs. YOLOv3\n\nYOLOv4 introduced several new techniques to improve both accuracy and speed. It used a CSPDarknet backbone and introduced new techniques such as spatial attention, Mish activation function, and GIoU loss to improve accuracy3. The improved YOLOv4 algorithm showed a 0.5% increase in average precision (AP) compared to the original algorithm while reducing the model’s weight file size by 45.3 M.\n\nYOLOR: Unified Network Architecture | Mosaic | Mixup | SimOTA\n\nUNA (Unified Network Architecture)\n\nUnlike previous YOLO versions, YOLOR’s architecture and model infrastructure differ significantly. The name “YOLOR” emphasizes its unique approach: it combines explicit and implicit knowledge to create a unified network capable of handling multiple tasks with a single input. By learning just one representation, YOLOR achieves impressive performance in object detection.\n\nYOLOX\n\nYOLOX is an anchor-free object detection model that builds upon the foundation of YOLOv3 SPP with a Darknet53 backbone. It aims to surpass the performance of previous YOLO versions. The key innovation lies in its decoupled head and SimOTA approach. By eliminating anchor boxes, YOLOX simplifies the design while achieving better accuracy. It bridges the gap between research and industry, offering a powerful solution for real-time object detection. YOLOX comes in various sizes, from the lightweight YOLOX-Nano to the robust YOLOX-x, each tailored for different use cases.\n\nYOLOv5: PANet| CSPDarknet53| SAM Block\n\nYOLOv5 brought about further enhancements to increase both precision and efficiency. It adopted a Scaled-YOLOv4 backbone and incorporated new strategies such as CIOU loss and CSPDarknet53-PANet-SPP to boost precision.\n\nStructure of YOLOv5\n\nThe refined YOLOv5 algorithm demonstrated a 0.7% rise in mean average precision (mAP) compared to the YOLOv4, while decreasing the model’s weight file size by 53.7 M. These improvements made YOLOv5 a more effective and efficient tool for real-time object detection.\n\nYOLOv6: EfficientNet-Lite | CSPDarknet-X backbone | Swish Activation Function | DIoU Loss\n\nYOLOv6 utilized a CSPDarknet-X backbone and introduced new methods such as panoptic segmentation, Swish activation function, and DIoU loss to boost accuracy.\n\nFramework of YOLOv6\n\nThe enhanced YOLOv6 algorithm exhibited a 0.8% increase in average precision (AP) compared to the YOLOv5, while shrinking the model’s weight file size by 60.2 M. These advancements made YOLOv6 an even more powerful tool for real-time object detection.\n\nYOLOv7: Leaky ReLU Activation Function| TIoU Loss| CSPDarknet-Z Backbone\n\nYOLOv7 employed a CSPDarknet-Z backbone in the yolov7 architecture. YOLOv7 object detection algorithm was enhanced by the introduction of innovative techniques such as object-centric segmentation, Leaky ReLU activation function, and TIoU loss to enhance accuracy.\n\nThe advanced YOLOv7 algorithm demonstrated a 1.0% increase in average precision (AP) compared to the YOLOv6, while reducing the model’s weight file size by 70.5 M. These improvements made YOLOv7 object detection algorithm, an even more robust tool for real-time object detection.\n\nYOLOv8: Multi-Scale Object Detection| CSPDarknet-AA| ELU Activation Function| GIoU Loss\n\nYOLOv8 introduced a new backbone architecture, the CSPDarknet-AA, which is an advanced version of the CSPDarknet series, known for its efficiency and performance in object detection tasks.\n\nOne key technique introduced in YOLOv8 is multi-scale object detection. This technique allows the model to detect objects of various sizes in an image. Another significant enhancement in YOLOv8 is the use of the ELU activation function. ELU, or Exponential Linear Unit, helps to speed up learning in deep neural networks by mitigating the vanishing gradient problem, leading to faster convergence.\n\nYOLOv8 adopted the GIoU loss. GIoU, or Generalized Intersection over Union, is a more advanced version of the IoU (Intersection over Union) metric that takes into account the shape and size of the bounding boxes, improving the precision of object localization.\n\nThe YOLOv8 algorithm shows a 1.2% increase in average precision (AP) compared to the YOLOv7, which is a significant improvement. It has achieved this while reducing the model’s weight file size by 80.6 M, making the model more efficient and easier to deploy in resource-constrained environments.\n\nYOLOv8 Comparison with Latest YOLO models\n\nYOLOv9: GELAN Architecture| Programmable Gradient Information (PGI)\n\nYOLOv9 which was recently released overcame information loss challenges inherent in deep neural networks. By integrating PGI and the versatile GELAN architecture, YOLOv9 not only enhances the model’s learning capacity but also ensures the retention of crucial information throughout the detection process, thereby achieving exceptional accuracy and performance.\n\nKey Highlights of YOLOv9\n\nInformation Bottleneck Principle: This principle reveals a fundamental challenge in deep learning: as data passes through successive layers of a network, the potential for information loss increases. YOLOv9 counters this challenge by implementing Programmable Gradient Information (PGI), which aids in preserving essential data across the network’s depth, ensuring more reliable gradient generation and, consequently, better model convergence and performance.\n\nReversible Functions: A function is deemed reversible if it can be inverted without any loss of information. YOLOv9 incorporates reversible functions within its architecture to mitigate the risk of information degradation, especially in deeper layers, ensuring the preservation of critical data for object detection tasks.\n\nYOLO Object Detection with Pre-Trained YOLOv9 on COCO Dataset\n\nLike all YOLO models, the pre-trained models of YOLOv9 is open-source and is available in GitHub.\n\nWe are going to run our experiment on Google Colab. So if you are doing it on your local system, please bear in mind that the instructions and the code was made to run on Colab Notebook.\n\nMake sure you have access to GPU. You can either run the command below or navigate to Edit → Notebook settings → Hardware accelerator, set it to GPU, and the click Save.\n\n!nvidia-smi\n\nTo make it easier to manage datasets, images, and models we create a HOME constant.\n\nimport os HOME = os.getcwd() print(HOME)\n\nClone and Install\n\n!git clone https://github.com/SkalskiP/yolov9.git %cd yolov9 !pip install -r requirements.txt -q\n\nDownload Model Weights\n\n!wget -P {HOME}/weights -q https://github.com/WongKinYiu/yolov9/releases/download/v0.1/yolov9-c.pt !wget -P {HOME}/weights -q https://github.com/WongKinYiu/yolov9/releases/download/v0.1/yolov9-e.pt !wget -P {HOME}/weights -q https://github.com/WongKinYiu/yolov9/releases/download/v0.1/gelan-c.pt !wget -P {HOME}/weights -q https://github.com/WongKinYiu/yolov9/releases/download/v0.1/gelan-e.pt\n\nTest Data\n\nUpload test image to the Colab notebook.\n\n!wget -P {HOME}/data -q –-add image path\n\nDetection with Pre-trained COCO Model on gelan-c\n\n!python detect.py --weights {HOME}/weights/gelan-c.pt --conf 0.1 --source image path --device 0\n\nEvaluation of the Pre-trained COCO Model on gelan-c\n\n!python val.py --data data/coco.yaml --img 640 --batch 32 --conf 0.001 --iou 0.7 --device 0 --weights './gelan-c.pt' --save-json --name gelan_c_640_val\n\nPerformance of YOLOv9 on MS COCO Dataset\n\nYolov9: Learning What You Want to Learn Using Programmable Gradient Information\n\nThe performance of YOLOv9 on the MS COCO dataset exemplifies its significant advancements in real-time object detection, setting new benchmarks across various model sizes.\n\nThe smallest of the models, v9-S, achieved 46.8% AP on the validation set of the MS COCO dataset, while the largest model, v9-E, achieved 55.6% AP. This sets a new state-of-the-art for object detection performance.\n\nThese results demonstrate the effectiveness of YOLOv9’s techniques, such as Programmable Gradient Information (PGI) and the Generalized Efficient Layer Aggregation Network (GELAN), in enhancing the model’s learning capacity and ensuring the retention of crucial information throughout the detection process.\n\nTraining YOLOv9 on Custom Dataset\n\nHere we will be curating a custom dataset using the Encord Index platform. Encord Index offers tools for managing and curating your data, allowing you to visualize, search, sort, and control your datasets with ease. This streamlined process ensures that your data is well-organized and ready for efficient model training and deployment.\n\nSelect New Dataset to Upload Data\n\nYou can name the dataset and add a description to provide information about the dataset.\n\nAnnotate Custom Dataset\n\nCreate an annotation project and attach the dataset and the ontology to the project to start annotation with a workflow.\n\nYou can choose manual annotation if the dataset is simple, small, and doesn’t require a review process. Automated annotation is also available and is very helpful in speeding up the annotation process.\n\nStart Labeling\n\nThe summary page shows the progress of the annotation project. The information regarding the annotators and the performance of the annotators can be found under the tabs labels and performance.\n\nExport the Annotation\n\nOnce the annotation has been reviewed, export the annotation in the required format.\n\nYou can use the custom dataset curated using Encord Annotate for training an object detection model. For testing YOLOv9, we are going to use an image from one of the sandbox projects on Encord Active.\n\nCopy and run the code below to run YOLOv9 for object detection. The code for using YOLOv9 for panoptic segmentation has also been made available now on the original GitHub repository.\n\nInstalling YOLOv9\n\n!git clone https://github.com/SkalskiP/yolov9.git %cd yolov9 !pip install -r requirements.txt -q !pip install -q roboflow encord av # This is a convenience class that holds the info about Encord projects and makes everything easier. # The class supports bounding boxes and polygons across both images, image groups, and videos. !wget 'https://gist.githubusercontent.com/frederik-encord/e3e469d4062a24589fcab4b816b0d6ec/raw/fa0bfb0f1c47db3497d281bd90dd2b8b471230d9/encord_to_roboflow_v1.py' -O encord_to_roboflow_v1.py\n\nImports\n\nfrom typing import Literal from pathlib import Path from IPython.display import Image import roboflow from encord import EncordUserClient from encord_to_roboflow_v1 import ProjectConverter\n\nDownload YOLOv9 Model Weights\n\nThe YOLOv9 is available as 4 models which are ordered by parameter count:\n\nYOLOv9-S\n\nYOLOv9-M\n\nYOLOv9-C\n\nYOLOv9-E\n\nHere we will be using gelan-c. But the same process follows for other models.\n\n!mkdir -p {HOME}/weights\n\n!wget -q https://github.com/WongKinYiu/yolov9/releases/download/v0.1/yolov9-e-converted.pt -O {HOME}/weights/yolov9-e.pt !wget -P {HOME}/weights -q https://github.com/WongKinYiu/yolov9/releases/download/v0.1/gelan-c.pt\n\nTrain Custom YOLOv9 Model for Object Detection\n\n!python train.py \\ --batch 8 --epochs 20 --img 640 --device 0 --min-items 0 --close-mosaic 15 \\ --data $dataset_yaml_file \\ --weights {HOME}/weights/gelan-c.pt \\ --cfg models/detect/gelan-c.yaml \\ --hyp hyp.scratch-high.yaml\n\nYOLO Object Detection using YOLOv9 on Custom Dataset\n\nIn order to perform object detection, you have to run prediction of the trained YOLOv9 on custom dataset.\n\nRun Prediction\n\nimport torch augment = False visualize = False conf_threshold = 0.25 nms_iou_thres = 0.45 max_det = 1000 seen, windows, dt = 0, [], (Profile(), Profile(), Profile()) for path, im, im0s, vid_cap, s in dataset: with dt[0]: im = torch.from_numpy(im).to(model.device).float() im /= 255 # 0 - 255 to 0.0 - 1.0 if len(im.shape) == 3: im = im[None] # expand for batch dim # Inference with dt[1]: pred = model(im, augment=augment, visualize=visualize)[0] # NMS with dt[2]: filtered_pred = non_max_suppression(pred, conf_threshold, nms_iou_thres, None, False, max_det=max_det) print(pred, filtered_pred) break\n\nGenerate YOLOv9 Prediction on Custom Data\n\nimport matplotlib.pyplot as plt from matplotlib.patches import Rectangle from PIL import Image img = Image.open(Image path) fig, ax = plt.subplots() ax.imshow(img) ax.axis(\"off\") for p, c in zip(filtered_pred[0], [\"r\", \"b\", \"g\", \"cyan\"]): x, y, w, h, score, cls = p.detach().cpu().numpy().tolist() ax.add_patch(Rectangle((x, y), w, h, color=\"r\", alpha=0.2)) ax.text(x+w/2, y+h/2, model.names[int(cls)], ha=\"center\", va=\"center\", color=c) fig.savefig(\"/content/predictions.jpg\")\n\nYOLOv9 Vs YOLOv8: Comparative Analysis Using Encord\n\nYou can convert the model predictions and upload them to Encord. Here for example, the YOLOv9 and YOLOv8 have been trained and compared on the Encord platform using the xView3 dataset, which contains aerial imagery with annotations for maritime object detection.\n\nThe comparative analysis between YOLOv9 and YOLOv8 on the Encord platform focuses on precision, recall, and metric analysis. These metrics are crucial for evaluating the performance of object detection models.\n\nPrecision: Precision measures the proportion of true positives (i.e., correct detections) among all detections. A higher precision indicates fewer false positives.\n\nRecall: Recall measures the proportion of actual positives that are correctly identified. A higher recall indicates fewer false negatives.\n\nMetric Analysis: This involves analyzing various metrics like Average Precision (AP), Mean Average Precision (mAP), etc., which provide a comprehensive view of the model’s performance.\n\nFor example, in the precision-recall curve, it seems that YOLOv8 surpasses YOLOv9 in terms of the Area Under the Curve (AUC-PR) value. This suggests that, across various threshold values, YOLOv8 typically outperforms YOLOv9 in both precision and recall. It implies that YOLOv8 is more effective at correctly identifying true positives and reducing false positives compared to YOLOv9.\n\nBut it is important to keep in mind that these two models which are being evaluated were trained for 20 epochs and are used as an example to show how to perform evaluation of trained models on custom datasets.\n\nYOLO Real-Time Implementation\n\nYOLO (You Only Look Once) models are widely used in real-time object detection tasks due to their speed and accuracy. Here are some real-world applications of YOLO models:\n\nHealthcare: YOLO models can be used in healthcare for tasks such as identifying diseases or abnormalities in medical images.\n\nAgriculture: YOLO models have been used to detect and classify crops, pests, and diseases, assisting in precision agriculture techniques and automating farming processes.\n\nSecurity Surveillance: YOLO models are used in security surveillance systems for real-time object detection, tracking, and classification.\n\nSelf-Driving Cars: In autonomous vehicles, YOLO models are used for detecting objects such as other vehicles, pedestrians, traffic signs, and signals in real-time.\n\nFace Detection: They have also been adapted for face detection tasks in biometrics, security, and facial recognition systems\n\nYOLO Object Detection: Key Takeaways\n\nIn this article, we provided an overview of the evolution of YOLO, from YOLOv1 to YOLOv8, and discussed its network architecture, new features, and applications. Additionally, we provided a step-by-step guide on how to use YOLOv8 for object detection and how to create model-assisted annotations with Encord Annotate.\n\nFrom scaling to enhancing your model development with data-driven insights"
    }
}
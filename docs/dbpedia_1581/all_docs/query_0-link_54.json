{
    "id": "dbpedia_1581_0",
    "rank": 54,
    "data": {
        "url": "https://arxiv.org/html/2405.03520v1",
        "read_more_link": "",
        "language": "en",
        "title": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://arxiv.org/html/extracted/2405.03520v1/img/casual.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            "World models",
            "Generative models",
            "Video generation",
            "Autonomous driving",
            "Autonomous agents"
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Zheng Zhuâˆ—, Xiaofeng Wangâˆ—, Wangbo Zhaoâˆ—, Chen Minâˆ—, Nianchen Dengâˆ—, Min Douâˆ—,\n\nYuqi Wangâˆ—, Botian Shiâ€ , Kai Wangâ€ , Chi Zhangâ€ , Yang Youâ€ , Zhaoxiang Zhangâ€ ,\n\nDawei Zhaoâ€ , Liang Xiaoâ€ , Jian Zhaoâ€ , Jiwen Luâ€ , Guan Huangâ€  âˆ— indicates equal contributions. â€  indicates corresponding authors. Zheng Zhu and Guan Huang are with GigaAI, Beijing, China. Xiaofeng Wang, Yuqi Wang and Zhaoxiang Zhang are with Institute of Automation, Chinese Academy of Sciences, Beijing, China. Wangbo Zhao, Kai Wang and Yang You are with National University of Singapore, Singapore. Chen Min is with Institute of Computing Technology, Beijing, China. Nianchen Deng, Min Dou and Botian Shi are with Shanghai Artificial Intelligence Laboratory, Shanghai, China. Chi Zhang is with Mach Drive, Beijing, China. Dawei Zhao and Liang Xiao are with Defense Innovation Institute, Beijing, China. Jian Zhao is with EVOL Lab, Institute of AI, China Telecom, and Northwestern Polytechnical University. Jiwen Lu is with Tsinghua University, Beijing, China.\n\nAbstract\n\nGeneral world models represent a crucial pathway toward achieving Artificial General Intelligence (AGI), serving as the cornerstone for various applications ranging from virtual environments to decision-making systems. Recently, the emergence of the Sora model has attained significant attention due to its remarkable simulation capabilities, which exhibits an incipient comprehension of physical laws. In this survey, we embark on a comprehensive exploration of the latest advancements in world models. Our analysis navigates through the forefront of generative methodologies in video generation, where world models stand as pivotal constructs facilitating the synthesis of highly realistic visual content. Additionally, we scrutinize the burgeoning field of autonomous-driving world models, meticulously delineating their indispensable role in reshaping transportation and urban mobility. Furthermore, we delve into the intricacies inherent in world models deployed within autonomous agents, shedding light on their profound significance in enabling intelligent interactions within dynamic environmental contexts. At last, we examine challenges and limitations of world models, and discuss their potential future directions. We hope this survey can serve as a foundational reference for the research community and inspire continued innovation. This survey will be regularly updated at: https://github.com/GigaAI-research/General-World-Models-Survey.\n\nIndex Terms:\n\nWorld models, Generative models, Video generation, Autonomous driving, Autonomous agents\n\n1 Introduction\n\nIn the pursuit of Artificial General Intelligence (AGI), the development of general world models stands as a fundamental avenue. General world models seek to understand the world through generative processes. Notably, the introduction of the Sora model [21] has garnered significant attention. Its remarkable simulation capabilities not only demonstrate an initial comprehension of physical laws but also highlight the promising advancements in world models. As we stand at the forefront of AI-driven innovation, it is crucial to delve deeper into the realm of world models, unraveling their complexities, evaluating their current developmental stage, and contemplating the potential trajectories they may follow in the future.\n\nWorld models predict the future to grow comprehension of the world. This predictive capacity holds immense promise for video generation, autonomous driving, and the development of autonomous agents, which represent three mainstream directions of development in world models. As shown in Figure 1, video generation world models encompass the generation and editing of videos to understand and simulate the world, which are valuable for media production and artistic expression. Autonomous driving world models, aided by techniques of video generation, create driving scenarios and learn driving elements and policies from driving videos. This knowledge assists in generating driving actions directly or training driving policy networks, aiding in end-to-end autonomous driving. Similarly, agent world models utilize video generation to establish intelligent interactions in dynamic environments. Unlike driving models, they build policy networks applicable to various contexts, either virtual (e.g., programs in games or simulated environments) or physical (e.g., robots).\n\nBuilding upon the foundation of comprehensive world modeling, video generation methods unveil physical laws through visual synthesis. Initially, the focus of generative models was primarily on image generation [177, 168, 66, 46, 173, 155, 10, 236, 33] and editing [245, 129, 154, 95], laying the foundation for more sophisticated advancements in synthesizing dynamic visual sequences. Over time, generative models [229, 63, 243, 17, 84, 68, 84, 18, 111, 52] have evolved to not only capture the static attributes of images, but also seamlessly string together sequences of frames. These models have developed some understanding of physics and motion, which represent early and limited forms of general world models [62]. Notably, at the forefront of this evolution stands the Sora model [21]. By harnessing the power of generative techniques, Sora demonstrates a profound ability to generate intricate visual narratives that adhere to the fundamental principles of the physical world. The relationship between generative models and world modeling is symbiotic, with each informing and enriching the other. Generative models can construct vast amounts of data in a controlled environment, which alleviates the need for extensive real-world data collection, particularly beneficial for training AI systems essential in real-world applications. Moreover, the efficacy of generative models critically hinges upon the depth of comprehension provided by world models. It is the comprehensive understanding of underlying environmental dynamics afforded by world models that empowers generative models to produce visually compelling signals of superior quality while adhering to stringent physical constraints. Thereby enhancing their realism and utility in various domains.\n\nThe ability of world models to understand the environment not only enhances video generation quality, but also benefits real-world driving scenarios. By employing predictive techniques to comprehend driving environments, world models are reshaping transportation and urban mobility by anticipating future driving scenarios, thereby enhancing safety and efficiency. World methods, aimed at establishing dynamic models of environments, are crucial in autonomous driving, where precise predictions about the future are essential for safe maneuvering. However, constructing world models for autonomous driving presents unique challenges, primarily due to the sample complexity inherent in real-world driving scenarios. Early methods [90, 159, 60] attempt to address these challenges by reducing the search space and incorporating explicit disentanglement of visual dynamics. Despite progress, a critical limitation lies in the predominant focus on simulation environments. Recent advances have seen autonomous driving world models leverage generative models to tackle real-world scenarios with larger search spaces. GAIA-1 [91] employs a Transformer to predict the next visual token, effectively constructing the driving world model. This approach enables anticipating multiple potential futures based on various prompts, such as weather conditions, scenes, traffic participants, and vehicle actions. Similarly, methods like DriveDreamer [209] and Panacea [218] leverage pre-trained diffusion models to learn driving world models from real-world driving videos. These techniques harness the structured information inherent in driving scenes to controllably generate high-quality driving videos, which can even enhance training for driving perception tasks. DriveDreamer2 [249], based on DriveDreamer, further integrates large language models to enhance the performance of driving world models and user interaction. It enables the generation of controllable driving scene videos solely through natural language input, encompassing even rare scenarios like sudden overtaking maneuvers. Furthermore, Drive-WM [212] demonstrates the feasibility of directly training end-to-end driving using generated driving scene videos, significantly improving end-to-end driving performance. By anticipating future scenarios, these models empower vehicles to make informed decisions, ultimately leading to safer and more efficient navigation on the roads. Moreover, this integration not only improves transportation systemsâ€™ safety and efficiency but also opens new possibilities for urban planning and design.\n\nBeyond their established utility in driving scenarios, world models have increasingly become integral to the functioning of autonomous agents, facilitating intelligent interactions across a myriad of contexts. For instance, world models in game agents not only augment the gaming experience but also propel the development of sophisticated game algorithms. The Dreamer series [72, 73, 74] exemplify this with its adept use of world models to predict future states within gaming environments. This capability enables game agents to learn in imagination, markedly decreasing the necessary volume of interactions for effective learning. In robotic systems, innovative approaches further underscore the versatility and potential of world models. UniPi [50], for instance, reimagines the decision-making problem in robotics as a text-to-video task. Its policy-as-video formulation fosters learning and generalization across diverse robot manipulation tasks. Similarly, UniSim [232] introduces a simulator of dynamic interactions through generative modeling, which can then be deployed in real-world scenarios without prior exposure. RoboDreamer [255] pushes the envelope by leveraging world models to propose plans involving combinations of actions and objects, thus solving unprecedented tasks in novel robotic execution environments. The multifaceted applications of world models extend beyond games and robotics. LeCunâ€™s proposal of the Joint-Embedding Predictive Architecture (JEPA) [115] heralds a significant departure from traditional generative models. JEPA learns to map input data to predicted outputs within a higher-level representation space, which enables the model to concentrate on learning more semantic features, enriching its capability for understanding and predicting across various modalities.\n\nBased on the comprehensive discussions presented above, it is evident that research on world models holds tremendous potential towards achieving AGI and has wide-ranging applications across various domains. Therefore, world models warrant significant attention from both academia and industry, requiring sustained efforts over an extended period. In comparison to recent surveys [67, 36, 136, 193] on world models, our survey offers broader coverage. It not only encompasses generative world models in video generation but also delves into the applications of world models in decision-making systems such as autonomous driving and robotics. We envision this survey to offer valuable insights for newcomers embarking on their journey into this field, while also stimulating critical thinking and discussion among established researchers in the community.\n\nThe main contributions of this survey can be summarized as follows: (1) We present a holistic examination of recent advancements in world model research, encompassing profound philosophical perspectives and detailed discussions. (2) Our analysis delves deeply into the literature surrounding world models for video generation, autonomous driving, and autonomous agents, uncovering their applications in media production, artistic expression, end-to-end driving, games, and robots. (3) We assess the existing challenges and limitations of world models and delve into prospective avenues for future research, with the intention of steering and igniting further progress in world models.\n\n2 Video Generation as a General World Model\n\nThe video generation task aims to create various realistic videos, requiring the model to understand and simulate the mechanism in the physical world, which aligns with the objective of building a general world model. In this section, we first introduce the technologies behind the video generation models in Section 2.1. Then, in Section 2.2, we present and review the advanced video generation models emerging in recent years. Finally, we discuss Sora in Section 2.3, which is considered to be the largest breakthrough in video generation.\n\n2.1 Technologies behind Video Generation\n\nThe concept of video generation contains several different tasks based on the conditions, such as class, text, or image. This survey mainly focuses on the scenario where the text condition is given, known as text-to-video generation. In this section, we first briefly introduce the visual foundation models, which are widely used in generation models. Then, we present the text encoders for extracting text features from the text condition. Finally, we review the evolution of generation techniques.\n\n2.1.1 Visual Foundation Models\n\nThe visual foundation models were originally proposed to tackle traditional computer vision tasks, for example, image classification [42], whereas they also inspire the development of generation models. Based on the architecture, they can be roughly categorized into convolution-based models and Transformer-based models, both of which can also be extended to the video data.\n\nConvolution-based Models. The convolution-based models for vision tasks have been fully explored in the last decades. Staring from LeNet [114], AlexNet [112], VGGNet [186], InceptionNet [194], ResNet [78], DenseNet [94] are gradually proposed to tackle the image recogntion problems. These models are adopted as a backbone model for other visual tasks [174, 77, 178]. Typically, U-Net [178] builds a U-shape architecture based on a backbone model for image segmentation tasks. The U-shape architecture enables the model can leverage both the low-level and high-level features from the backbone, which significantly improves the pixel-wise prediction. Benefiting from the superiority of pixel-wise prediction, the U-shape architecture is also widely used in image generation models [85, 45, 177].\n\nTransformer-based Models. The Transformer is proposed in [205] for machine translation tasks and applied to vision recognition by ViT [48]. In ViT, images are divided into patches, then projected into tokens and finally processed by a series of multi-head self-attention and multi-layer perceptron blocks. Its ability to capture long-range dependencies in images enables its superiority in image recognition. After that, distillation [198], window-attention [137], and mask image modeling [11, 76] approaches are introduced to improve the training or inference efficiency of vision Transformers. Except for the success in image recognition, Transformer-based models also demonstrate superiority in various visual tasks, such as object detection [26, 259, 235, 244], semantic segmentation [224, 191, 251], and image generation [164, 9, 75]. Thanks to its good scalability property, the Transformer-based model DiT [164] has become the main architecture of Sora.\n\nExtension to Video. The methods mentioned above are mainly designed for image data. Researchers further extend these methods to solve problems in the video domain. Convolution-based models [102, 201, 27, 202, 56, 55, 230] usually introduce 3D convolution layers to building the spatial-temporal relationships in video data. Transformer-based methods [3, 15, 123, 138] extend and improve the multi-head self-attention from spatial-only design to jointly modeling spatial-temporal relationships. These methods also inspire the architecture design of text-to-video generation models, such as [238, 239, 111].\n\n2.1.2 Text Encoders\n\nThe text encoder is adopted to extract the text embedding for a given text prompt in image or video generation. Existing generation methods usually employ the text encoder of a multi-modal model or directly use a language model to conduct the embedding extraction. In the following, we will briefly present representative multi-modal models and language models.\n\nPre-trained Multi-modal Models. The pre-trained multi-modal models, such as [122, 169, 121], align the representation of image and text in the embedding space. It usually consists of an image encoder as well as a text encoder, which naturally can be adapted to inject text information into generation models. CLIP [169] is a typical pre-trained multi-modal model, which has been widely used in image/video generation models [168, 173, 177, 17]. It is pre-trained with large-scale image-text pairs through contrastive learning [99] and demonstrates superior performance across various tasks. However, CLIP is pre-trained for image-text alignment instead of comprehending complex text prompts. This drawback may limit the generation performance when the given prompt is long and detailed.\n\nPre-trained Language Models. The pre-trained language models are usually pre-trained on the large-scale corpus, thus having transferable ability on various downstream language tasks. BERT [44] is an early attempt at language model pre-training, which designed several tasks to push the model learning from unlabeled data. This paradigm also inspires follow-up works, such as RoBERTa [135] and BART [119]. With the increasing model size and enlarging training dataset, the pre-trained models demonstrate surprising abilities, which are usually named as larger language models (LLMs) [170, 171, 22, 172, 199, 200, 1]. T5 [172] and Llama-2 [199] are two widely used LLMs in generation tasks [92, 32, 180, 223] since their superior performance and open avaliablity. The LLMs provide a better understanding of long text prompts than CLIP, thus helping the generation to follow the instructions of humans.\n\n2.1.3 Generation Techniques\n\nIn this section, we review the development of generation techniques in recent decades.\n\nGAN. Before the success of diffusion-based methods, GAN introduced in [64] have always been the mainstream methods in image generation. It has a generator GğºGitalic_G and a discriminator Dğ·Ditalic_D. The generator GğºGitalic_G is adapted to generate an output Gâ¢(ğ³)ğºğ³G(\\mathbf{z})italic_G ( bold_z ) from a noised ğ³ğ³\\mathbf{z}bold_z sampled from a Gaussian distribution and the discriminator Dğ·Ditalic_D is employed to classifier the output is real or fake.\n\nFrom the original definition of GAN [64], the generator GğºGitalic_G and the discriminator Dğ·Ditalic_D are trained in an adversarial manner. Specifically, we first train the discriminator Dğ·Ditalic_D. We input real data ğ±ğ±\\mathbf{x}bold_x sampled from a data distribution pdataâ¢(ğ±)subscriptğ‘datağ±p_{\\text{data}}(\\mathbf{x})italic_p start_POSTSUBSCRIPT data end_POSTSUBSCRIPT ( bold_x ) and generated output Gâ¢(ğ³)ğºğ³G(\\mathbf{z})italic_G ( bold_z ) into the discriminator Dğ·Ditalic_D and it learns to improve the discrimination ability on real and fake samples. This can be formulated as:\n\nâ„“D=ğ”¼ğ±âˆ¼pdata â¢(ğ±)â¢[logâ¡Dâ¢(ğ±)]+ğ”¼ğ³âˆ¼pzâ¢(ğ³)â¢[logâ¡(1âˆ’Dâ¢(Gâ¢(ğ³)))].subscriptâ„“ğ·subscriptğ”¼similar-toğ±subscriptğ‘data ğ±delimited-[]ğ·ğ±subscriptğ”¼similar-toğ³subscriptğ‘ğ‘§ğ³delimited-[]1ğ·ğºğ³\\ell_{D}=\\mathbb{E}_{\\mathbf{x}\\sim p_{\\text{data }}(\\mathbf{x})}[\\log D(% \\mathbf{x})]+\\mathbb{E}_{\\mathbf{z}\\sim p_{z}(\\mathbf{z})}[\\log(1-D(G(\\mathbf{% z})))].roman_â„“ start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT = blackboard_E start_POSTSUBSCRIPT bold_x âˆ¼ italic_p start_POSTSUBSCRIPT data end_POSTSUBSCRIPT ( bold_x ) end_POSTSUBSCRIPT [ roman_log italic_D ( bold_x ) ] + blackboard_E start_POSTSUBSCRIPT bold_z âˆ¼ italic_p start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT ( bold_z ) end_POSTSUBSCRIPT [ roman_log ( 1 - italic_D ( italic_G ( bold_z ) ) ) ] . (1)\n\nThe discriminator Dğ·Ditalic_D should maximize the loss â„“Dsubscriptâ„“ğ·\\ell_{D}roman_â„“ start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT. During this process, the parameters in GğºGitalic_G are frozen. Then, we train the generator GğºGitalic_G following:\n\nâ„“G=ğ”¼ğ³âˆ¼pzâ¢(ğ³)â¢[logâ¡(1âˆ’Dâ¢(Gâ¢(ğ³)))].subscriptâ„“ğºsubscriptğ”¼similar-toğ³subscriptğ‘ğ‘§ğ³delimited-[]1ğ·ğºğ³\\ell_{G}=\\mathbb{E}_{\\mathbf{z}\\sim p_{z}(\\mathbf{z})}[\\log(1-D(G(\\mathbf{z}))% )].roman_â„“ start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT = blackboard_E start_POSTSUBSCRIPT bold_z âˆ¼ italic_p start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT ( bold_z ) end_POSTSUBSCRIPT [ roman_log ( 1 - italic_D ( italic_G ( bold_z ) ) ) ] . (2)\n\nThe generator GğºGitalic_G is trained to minimize the loss â„“Gsubscriptâ„“ğº\\ell_{G}roman_â„“ start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT so that the generated samples can approach the real data. The parameters in Dğ·Ditalic_D are also not updated during this process.\n\nFollowing works apply GAN to various tasks related to image generation, such as style transfer [257, 106, 162, 20], image editing [256, 207, 165], and image inpainting [132, 40].\n\nDiffusion. Diffusion-based methods have started to dominate image generation since the Denoising Diffusion Probabilistic Model (DDPM) [85], which learns a reverse process to generate an image from a Gaussian distribution ğ’©â¢(0,ğ‘°)ğ’©0ğ‘°\\mathcal{N}(0,\\bm{I})caligraphic_N ( 0 , bold_italic_I ). It has two processes: the diffusion process (also known as a forward process) and the denoising process (also known as the reverse process). During the diffusion process, we gradually add small Gaussian noise to an image in Tğ‘‡Titalic_T timesteps. Given a image ğ±0subscriptğ±0\\mathbf{x}_{0}bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT from the data distribution, we can obtain ğ±Tsubscriptğ±ğ‘‡\\mathbf{x}_{T}bold_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT through the cumulative distribution of all previous diffusion processes:\n\nqâ¢(ğ±1:Tâˆ£ğ±0):=âˆt=1Tqâ¢(ğ±tâˆ£ğ±tâˆ’1),assignğ‘conditionalsubscriptğ±:1ğ‘‡subscriptğ±0superscriptsubscriptproductğ‘¡1ğ‘‡ğ‘conditionalsubscriptğ±ğ‘¡subscriptğ±ğ‘¡1q\\left(\\mathbf{x}_{1:T}\\mid\\mathbf{x}_{0}\\right):=\\prod_{t=1}^{T}q\\left(% \\mathbf{x}_{t}\\mid\\mathbf{x}_{t-1}\\right),italic_q ( bold_x start_POSTSUBSCRIPT 1 : italic_T end_POSTSUBSCRIPT âˆ£ bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) := âˆ start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_q ( bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT âˆ£ bold_x start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT ) , (3)\n\nwhere\n\nqâ¢(ğ±tâˆ£ğ±tâˆ’1):=ğ’©â¢(ğ±t;1âˆ’Î²tâ¢ğ±tâˆ’1,Î²tâ¢I)assignğ‘conditionalsubscriptğ±ğ‘¡subscriptğ±ğ‘¡1ğ’©subscriptğ±ğ‘¡1subscriptğ›½ğ‘¡subscriptğ±ğ‘¡1subscriptğ›½ğ‘¡ğ¼q\\left(\\mathbf{x}_{t}\\mid\\mathbf{x}_{t-1}\\right):=\\mathcal{N}\\left(\\mathbf{x}_% {t};\\sqrt{1-\\beta_{t}}\\mathbf{x}_{t-1},\\beta_{t}I\\right)italic_q ( bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT âˆ£ bold_x start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT ) := caligraphic_N ( bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; square-root start_ARG 1 - italic_Î² start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG bold_x start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT , italic_Î² start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_I ) (4)\n\nTğ‘‡Titalic_T and [Î²1,Î²2,â€¦,Î²T]subscriptğ›½1subscriptğ›½2â€¦subscriptğ›½ğ‘‡\\left[\\beta_{1},\\beta_{2},\\ldots,\\beta_{T}\\right][ italic_Î² start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_Î² start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ , italic_Î² start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ] denote the diffusion steps and the pre-defined noise schedule, respectively. We can also obtain the output at the tğ‘¡titalic_t timestep through\n\nqâ¢(ğ±tâˆ£ğ±0):=ğ’©â¢(ğ±t;Î±Â¯tâ¢ğ±0,(1âˆ’Î±Â¯t)â¢I),assignğ‘conditionalsubscriptğ±ğ‘¡subscriptğ±0ğ’©subscriptğ±ğ‘¡subscriptÂ¯ğ›¼ğ‘¡subscriptğ±01subscriptÂ¯ğ›¼ğ‘¡ğ¼q\\left(\\mathbf{x}_{t}\\mid\\mathbf{x}_{0}\\right):=\\mathcal{N}\\left(\\mathbf{x}_{t% };\\sqrt{\\bar{\\alpha}_{t}}\\mathbf{x}_{0},\\left(1-\\bar{\\alpha}_{t}\\right)I\\right),italic_q ( bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT âˆ£ bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) := caligraphic_N ( bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; square-root start_ARG overÂ¯ start_ARG italic_Î± end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , ( 1 - overÂ¯ start_ARG italic_Î± end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) italic_I ) , (5)\n\nwhere Î±t:=1âˆ’Î²tassignsubscriptğ›¼ğ‘¡1subscriptğ›½ğ‘¡\\alpha_{t}:=1-\\beta_{t}italic_Î± start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT := 1 - italic_Î² start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and Î±Â¯t:=âˆi=0tÎ±iassignsubscriptÂ¯ğ›¼ğ‘¡superscriptsubscriptproductğ‘–0ğ‘¡subscriptğ›¼ğ‘–\\bar{\\alpha}_{t}:=\\prod_{i=0}^{t}\\alpha_{i}overÂ¯ start_ARG italic_Î± end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT := âˆ start_POSTSUBSCRIPT italic_i = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT italic_Î± start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. Thus, we have\n\nğ±t=Î±Â¯tâ¢ğ±0+1âˆ’Î±Â¯tâ¢Ïµ,Ïµâˆ¼ğ’©â¢(0,1).formulae-sequencesubscriptğ±ğ‘¡subscriptÂ¯ğ›¼ğ‘¡subscriptğ±01subscriptÂ¯ğ›¼ğ‘¡italic-Ïµsimilar-toitalic-Ïµğ’©01\\mathbf{x}_{t}=\\sqrt{\\bar{\\alpha}_{t}}\\mathbf{x}_{0}+\\sqrt{1-\\bar{\\alpha}_{t}}% \\epsilon,\\quad\\epsilon\\sim\\mathcal{N}(0,1).bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = square-root start_ARG overÂ¯ start_ARG italic_Î± end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + square-root start_ARG 1 - overÂ¯ start_ARG italic_Î± end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG italic_Ïµ , italic_Ïµ âˆ¼ caligraphic_N ( 0 , 1 ) . (6)\n\nThe denoising process is the reverse of the diffusion process, enabling us to obtain images from the Gaussian noise. To achieve this, a denoising model ÏµÎ¸subscriptitalic-Ïµğœƒ\\epsilon_{\\theta}italic_Ïµ start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT learns to predict the noise Ïµtsubscriptitalic-Ïµğ‘¡\\epsilon_{t}italic_Ïµ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT added at the timestep tğ‘¡titalic_t through a simplified loss function, which can be formulated as:\n\nâ„“tsimple â¢(Î¸)subscriptsuperscriptâ„“simple ğ‘¡ğœƒ\\displaystyle\\ell^{\\text{simple }}_{t}(\\theta)roman_â„“ start_POSTSUPERSCRIPT simple end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_Î¸ ) =ğ”¼ğ±0,t,Ïµtâ¢â€–ÏµÎ¸â¢(ğ±t,t)âˆ’Ïµâ€–22absentsubscriptğ”¼subscriptğ±0ğ‘¡subscriptitalic-Ïµğ‘¡superscriptsubscriptnormsubscriptitalic-Ïµğœƒsubscriptğ±ğ‘¡ğ‘¡italic-Ïµ22\\displaystyle=\\mathbb{E}_{\\mathbf{x}_{0},t,\\epsilon_{t}}\\left\\|\\epsilon_{% \\theta}\\left(\\mathbf{x}_{t},t\\right)-\\epsilon\\right\\|_{2}^{2}= blackboard_E start_POSTSUBSCRIPT bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_t , italic_Ïµ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT âˆ¥ italic_Ïµ start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) - italic_Ïµ âˆ¥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT (7) =ğ”¼ğ±0,t,Ïµtâ¢â€–ÏµÎ¸â¢(Î±Â¯tâ¢ğ±0+1âˆ’Î±Â¯tâ¢Ïµ,t)âˆ’Ïµâ€–22absentsubscriptğ”¼subscriptğ±0ğ‘¡subscriptitalic-Ïµğ‘¡superscriptsubscriptnormsubscriptitalic-ÏµğœƒsubscriptÂ¯ğ›¼ğ‘¡subscriptğ±01subscriptÂ¯ğ›¼ğ‘¡bold-italic-Ïµğ‘¡italic-Ïµ22\\displaystyle=\\mathbb{E}_{\\mathbf{x}_{0},t,\\epsilon_{t}}\\left\\|\\epsilon_{% \\theta}\\left(\\sqrt{\\bar{\\alpha}_{t}}\\mathbf{x}_{0}+\\sqrt{1-\\bar{\\alpha}_{t}}% \\bm{\\epsilon},t\\right)-\\epsilon\\right\\|_{2}^{2}= blackboard_E start_POSTSUBSCRIPT bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_t , italic_Ïµ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT âˆ¥ italic_Ïµ start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT ( square-root start_ARG overÂ¯ start_ARG italic_Î± end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + square-root start_ARG 1 - overÂ¯ start_ARG italic_Î± end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG bold_italic_Ïµ , italic_t ) - italic_Ïµ âˆ¥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT (8)\n\nThen, we can denoise step-by-step through\n\nğ±tâˆ’1=1Î±tâ¢(ğ±tâˆ’Î²t1âˆ’Î±Â¯tâ¢ÏµÎ¸â¢(ğ±t,t))+Î²tâ¢z,subscriptğ±ğ‘¡11subscriptğ›¼ğ‘¡subscriptğ±ğ‘¡subscriptğ›½ğ‘¡1subscriptÂ¯ğ›¼ğ‘¡subscriptitalic-Ïµğœƒsubscriptğ±ğ‘¡ğ‘¡subscriptğ›½ğ‘¡ğ‘§\\mathbf{x}_{t-1}=\\frac{1}{\\sqrt{\\alpha_{t}}}\\left(\\mathbf{x}_{t}-\\frac{\\beta_{% t}}{\\sqrt{1-\\bar{\\alpha}_{t}}}\\epsilon_{\\theta}\\left(\\mathbf{x}_{t},t\\right)% \\right)+\\beta_{t}z,bold_x start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT = divide start_ARG 1 end_ARG start_ARG square-root start_ARG italic_Î± start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG end_ARG ( bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - divide start_ARG italic_Î² start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG square-root start_ARG 1 - overÂ¯ start_ARG italic_Î± end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG end_ARG italic_Ïµ start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) ) + italic_Î² start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_z , (9)\n\nwhere ğ³âˆ¼ğ’©â¢(0,I)similar-toğ³ğ’©0ğ¼\\mathbf{z}\\sim\\mathcal{N}(0,I)bold_z âˆ¼ caligraphic_N ( 0 , italic_I ). Although the generation quality of DDPM is satisfactory, its slow generation speed hinders its broader application. Following works attempt to solve this problem by reducing the denoising steps [188, 140, 182, 250, 189] or accelerating the denoising model [161, 185, 53, 142].\n\nAutoregressive Modeling. Autoregressive modeling has been explored in both language generation methods [170, 171, 22] and image generation tasks [33, 241, 237, 116]. Given a sequence of tokens (ğ±1,ğ±2,â€¦,ğ±K)subscriptğ±1subscriptğ±2â€¦subscriptğ±ğ¾\\left(\\mathbf{x}_{1},\\mathbf{x}_{2},\\ldots,\\mathbf{x}_{K}\\right)( bold_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ , bold_x start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT ), the probability of the kğ‘˜kitalic_k-th toeken ğ±ksubscriptğ±ğ‘˜\\mathbf{x}_{k}bold_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT only depends on tokens (ğ±1,ğ±2,ğ±kâˆ’1)subscriptğ±1subscriptğ±2subscriptğ±ğ‘˜1(\\mathbf{x}_{1},\\mathbf{x}_{2},\\mathbf{x}_{k-1})( bold_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , bold_x start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT ). An autoregressive model pÎ¸subscriptğ‘ğœƒp_{\\theta}italic_p start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT is trained to maximize the likelihood of the current token, which can be formulated as:\n\nâ„“=âˆ‘kKlogâ¡pÎ¸â¢(ğ±kâˆ£ğ±1,ğ±2,â€¦,ğ±kâˆ’1)â„“superscriptsubscriptğ‘˜ğ¾subscriptğ‘ğœƒconditionalsubscriptğ±ğ‘˜subscriptğ±1subscriptğ±2â€¦subscriptğ±ğ‘˜1\\ell=\\sum_{k}^{K}\\log p_{\\theta}\\left(\\mathbf{x}_{k}\\mid\\mathbf{x}_{1},\\mathbf% {x}_{2},\\ldots,\\mathbf{x}_{k-1}\\right)roman_â„“ = âˆ‘ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT roman_log italic_p start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT âˆ£ bold_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ , bold_x start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT ) (10)\n\nRecently, LVM [5] scales up the amount of training data to 420 billion tokens and model size to 3 billion parameters, demonstrating an ability for general visual reasoning as well as generation and directing a potential way towards the world model.\n\nMasked Modeling. Masked modeling is first designed for self-supervised learning for language models [44, 135, 104] and image models [11, 76]. Given a sequence of tokens (ğ±1,ğ±2,â€¦,ğ±K)subscriptğ±1subscriptğ±2â€¦subscriptğ±ğ¾\\left(\\mathbf{x}_{1},\\mathbf{x}_{2},\\ldots,\\mathbf{x}_{K}\\right)( bold_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ , bold_x start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT ), some tokens are randomly masked out. Then, the model is forced to predict the masked tokens and reconstruct the original representation. Noting the ability of image reconstruction of masked modeling, some works [28, 125, 126] directly generate images from mask tokens and find it also generalizes well in video generation tasks [238, 239]. Considering its simplicity and surprising performance, it is also a promising direction for future generation techniques.\n\n2.2 Advanced Video Generation Models\n\nIn this section, we review the advanced video generation models proposed in recent years. Based on the given conditions (e.g. example, classes, audios, texts, images, or videos), during generation, video generation tasks can be divided into different categories. Here, we mainly focus on the text-to-video method, where the text description is available during generation. These models aim to generate videos that are semantically aligned with given texts while maintaining consistency between different frames. The methods for idea generation with other conditions can be modified from the text-to-image models.\n\n2.2.1 GAN-based Methods\n\nBesides the success of image generation, GAN-based models also achieve remarkable performance for video generation [160, 7, 120, 130, 128, 108, 43]. Here, we select three representative methods and review them briefly. We visualize a general architecture of GAN-based methods from video generation in Figure 4 (a).\n\nTemporal GANs conditioning on captions (TGANs-C) [160] adopts a text encoder based on LSTM [87] to extract a text embedding. This embedding is then combined with a vector of random noise, which together form the input to the generator. The generator contains a series of spatio-temporal convolutions to generate the frame sequence. Unlike the GANs-based models for image generation in Section 2.1.3, which typically has only one discriminator, TGANs-C designs three discriminators in video, frame, and motion-levels, respectively. Benefiting from these discriminators, the model is capable of producing videos that align with the provided text and akin to authentic video footage.\n\nText-Filter conditioning Generative Adversarial Network (TFGAN) [7] adopts the text features extracted from the text encoder to generate a series of filters in different frames. Then, these filters are employed as the convolutional filters in the discriminator for each frame generation. This operation enhances the semantic association between the given text and the generated video.\n\nThe StroyGAN [128] aims to generate a sequence of frames based on a multi-sentence paragraph, where each sentence is responsible for one frame. It adopts a story encoder and a context encoder to extract the global representation of the multi-sentence paragraph and sentence for the current frame, respectively. Then, the output from the story encoder and context encoder are combined and input to the generator to generate the current frame. It also employs two discriminators to ensure the frame-level and video-level consistency with the given paragraph.\n\n2.2.2 Diffusion-based Methods\n\nThe development of diffusion models for image generation also facilitates the progress in video generation. We select four representative approaches due to their effectiveness or efficiency. We summarize the framework of these methods in Figure 4 (b).\n\nImagen Video [84] proposes a cascaded sampling pipeline for video generation. Starting from a base video generation model [86], which generates video with low resolution and low frame rate, the authors cascade spatial and temporal super-resolution models to progressively improve the resolution and frame rate of generated videos.\n\nStable video diffusion (SVD) [17] is built upon Stable Diffusion [177] by inserting temporal convolution and attention layers after spatial convolution and attention blocks. To improve the generation performance, the authors propose to disengage the training into three stages: pre-training on text-to-image task, pre-training on text-to-video task, and text-to-video finetuning with high-quality data. It proves the importance of data curation for video diffusion models.\n\nLatte [143] is an early attempt to apply a Transformer-based model in video generation. The model is built based on DiT [164] and contains extra blocks for spatial-temporal modeling. To ensure the efficiency in generation, the authors explore four efficient designs for spatial and temporal modeling, which is similar to the operations mentioned in Section 2.1.1. The architecture of the Latte is thought to be similar to the design of Sora.\n\nStreamingT2V [80] divides the text-to-video generation into three steps, enabling to generation of long videos with even more than 1,200 frames. First, it employs pre-trained text-to-video models to generate a short video e.g. with only 16 frames. Then, it extends a video diffusion model with short-term and long-term memory mechanisms to autoregressively generate further frames. Finally, another high-resolution video generation model is adopted to enhance generated videos.\n\n2.2.3 Autoregressive Modeling-based Methods\n\nAutoregressive modeling is also a popular technique in video generation [229, 220, 88, 144, 237] . We present its architecture in Figure 4 (c).\n\nVideoGPT [229] is a representative autoregressive modeling-based method. It first trains a VQ-VAE [204] to encode videos into latent tokens. Then, the authors leverage a GPT-like framework [170] and train the model learning to predict the next token in the latent space. During the inference, a series of tokens is sampled from the latent space and the trained VideoGPT with VQ-VAE decodes it into generated videos.\n\nGODIVA [220] also generates videos in a similar way while emphasizing reducing the computation complexity of the model. Specifically, the authors propose to replace an original self-attention layer with three sparse self-attention layers, which only are conducted along the temporal, row, and column dimensions of the latent features, respectively. The effectiveness of this disentangling operation is also verified by models mentioned in Section 2.1.\n\nCogVideo [88] inherits the knowledge from the pre-trained autoregressive model CogView2 [47] to reduce the burden of training from scratch. To improve the alignment between the given text and generated video, the authors propose a multi-frame-rate hierarchical generation framework, which first generates key frames in an autoregressive manner and then recursively interpolates frames with bidirectional attentions.\n\n2.2.4 Masked Modeling-based Methods\n\nMasked modeling is also an emerging video generation method. Unlike autoregressive modeling, which suffers from sequential generation, the masked modeling method can decode videos in parallel. We visualize its architecture in Figure 4 (d).\n\nMAGVIT [238] encodes videos into tokens through a 3D-VQ tokenizer and leverages a masked token modeling paradigm to accelerate the training. Specifically, the target tokens are randomly replaced with conditional tokens and masked tokens during training. Then, a bidirectional Transformer is trained to refine the conditional tokens, predict masked tokens, and reconstruct target tokens. To improve the generation quality, MAGVIT-v2 [239] is introduced to improve the video tokenizer. The authors design a lookup-free quantization method to build the codebook and propose a joint image-video tokenization model, enabling it can tackle image and video generation jointly. After that, VideoPoet [111] integrates MAGVIT-v2 [239] into a large language model to generate videos from various conditioning signals\n\nSimilarly, WorldDreamer [210] also trains to model to reconstruct masked tokens based on those unmasked tokens. To facilitate the training process, they design a spatial-temporal patchwise Transformer, which conducts attention within a spatial-temporal window. It adopts cross-attention layers to inject information of given text descripction into the model. The priority of parallel decoding enables it to achieve much faster video generation than diffusion-based and autoregressive-based methods.\n\n2.2.5 Datasets and Evaluation Metrics\n\nTraining a text-to-video generation model requires large-scale video-text pairs. In Table I, we present serveral popular datasets. These datasets may also be employed to train multi-modal models. Based on the technical report from Sora, the data quality, for example the video-text alginment and the richness of captions, is essential to the generation performance. Hence, we hope more large-scale high quality dataset can be open-sourced, prompting the prosperity of video generation and even the development of world models.\n\nThe metrics adopted to evaulate the video generation performance varies in different papers. Fo example, Latte [143] and VideoGPT [229] measure the performance through FrÃ©chet Video Distance (FVD) [203]. CLIP similarity (CLIPSim) [220] is also a common evualtion approach. Human evaluation as complementary to these metrics is also widely adopted in existing works. Since evaluation score are highly related to the random seed, it is not easy to conduct fair comparison. Moreover, different mehtods may adopt differnt dataset to evalution performance, which further aggravates this problem. Human preference annotations may be a potential solution for video generation evaluation. Recently, some comprehensive benchmarks [133, 134, 97] are proposed for the comparison fairness.\n\n2.3 Towards World Models: Sora\n\nSora is a closed-source text-to-video generation model developed by OpenAI. Besides being capable of generating a minute of high-fidelity video, it demonstrates some abilities to simulate the real world. It directs a way towards the world model through video generation models. In this section, we briefly introduce the techniques behind Sora. Since Sora is closed-source, all analyses here are mainly based on its technical report [21] and may vary from its real implementation.\n\n2.3.1 Framework\n\nSora is thought to be a diffusion-based video generation model. It consists of three parts: 1. A compression model that compresses a raw video both temporally and spatially into latent representation and an asymmetrical model that maps the latent representation back to the original video. 2. A Transformer-based diffusion model, similar to DiT [164], which is trained in the latent space. 3. A language model that encoders human instruction into embedding and injects it into the generation model.\n\nCompression Model. The compression model usually contains an encoder and a decoder. The former is adopted to project the video into a low-dimensional latent space, while the latter maps the latent representation back to the video. Based on the technical report [21], the compression model is built based on VAE [109] or VQ-VAE [204]. Since the architecture of the decoder is usually in symmetric to the encoder, we mainly focus on the architecture of the encoder in this review.\n\nGiven a raw video ğ•âˆˆâ„TÃ—HÃ—WÃ—Cğ•superscriptâ„ğ‘‡ğ»ğ‘Šğ¶\\mathbf{V}\\in\\mathbb{R}^{T\\times H\\times W\\times C}bold_V âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_T Ã— italic_H Ã— italic_W Ã— italic_C end_POSTSUPERSCRIPT, the encoder first projects it into a sequence of tokens ğ±âˆˆâ„ntÃ—nhÃ—nwÃ—dğ±superscriptâ„subscriptğ‘›ğ‘¡subscriptğ‘›â„subscriptğ‘›ğ‘¤ğ‘‘\\mathbf{x}\\in\\mathbb{R}^{n_{t}\\times n_{h}\\times n_{w}\\times d}bold_x âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_n start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT Ã— italic_n start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT Ã— italic_n start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT Ã— italic_d end_POSTSUPERSCRIPT. Based on the methods employed in visual foundation models mentioned in Section 2.1.1, there exist two options: spatial-only compression and spatial-temporal compression. The spatial-only compression only compresses the video along the spatial dimension. It extracts image patches of size hÃ—wâ„ğ‘¤h\\times witalic_h Ã— italic_w for each frame and adopts a 2D convolutional layer to project it into ğ±iâˆˆâ„dsubscriptğ±ğ‘–superscriptâ„ğ‘‘\\mathbf{x}_{i}\\in\\mathbb{R}^{d}bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT. In this case, we have nt=Tsubscriptğ‘›ğ‘¡ğ‘‡n_{t}=Titalic_n start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_T, nh=H/hsubscriptğ‘›â„ğ»â„n_{h}=H/hitalic_n start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT = italic_H / italic_h, and nw=W/wsubscriptğ‘›ğ‘¤ğ‘Šğ‘¤n_{w}=W/witalic_n start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT = italic_W / italic_w. This operation is widely adopted in ViTs [48]. The spatial-temporal compression method compresses the video along both the spatial and temporal dimensions, which provides a larger compression rate. Specifically, it extracts spatial-temporal tubes of size tÃ—hÃ—wğ‘¡â„ğ‘¤t\\times h\\times witalic_t Ã— italic_h Ã— italic_w from the video and adopts a 3D convolutional layer to project it into an embedding ğ±iâˆˆâ„dsubscriptğ±ğ‘–superscriptâ„ğ‘‘\\mathbf{x}_{i}\\in\\mathbb{R}^{d}bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT. Thus, we have nt=T/tsubscriptğ‘›ğ‘¡ğ‘‡ğ‘¡n_{t}=T/titalic_n start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_T / italic_t, nh=H/hsubscriptğ‘›â„ğ»â„n_{h}=H/hitalic_n start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT = italic_H / italic_h, and nw=W/wsubscriptğ‘›ğ‘¤ğ‘Šğ‘¤n_{w}=W/witalic_n start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT = italic_W / italic_w. This operation is similar to the tubelet embedding technique in ViViT [3].\n\nAfter the tokenization, the encoder can further process these tokens through Transformer blocks, convolutional blocks, or the combination of them and project them into zâˆˆâ„ntâ€²Ã—nhâ€²Ã—nwâ€²Ã—dâ€²ğ‘§superscriptâ„superscriptsubscriptğ‘›ğ‘¡â€²superscriptsubscriptğ‘›â„â€²superscriptsubscriptğ‘›ğ‘¤â€²superscriptğ‘‘â€²z\\in\\mathbb{R}^{n_{t}^{\\prime}\\times n_{h}^{\\prime}\\times n_{w}^{\\prime}\\times d% ^{\\prime}}italic_z âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_n start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT Ã— italic_n start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT Ã— italic_n start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT Ã— italic_d start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT. We present the architecture of the compression model in Figure 2 (a).\n\nGeneration Model. Based on the technical report, the generation model is built up on DiT [164]. Since the original DiT is designed for class-to-image generation, two modifications should be conducted on it. First, since the self-attention blocks and MLP blocks in DiT are designed for spatial modeling, extra blocks for temporal modeling should be added. This could be achieved via extending the original self-attention to both spatial and temporal dimensions. Second, the condition is changed from class to text, and blocks to inject the text information should be added. The text-to-image cross-attention block is a potential solution, whose effectiveness has been proven in [32]. Based on this, one layer of the potential architecture can be formulated as:\n\nğ±â€²=ğ±+STAâ¡(ğ±),superscriptğ±â€²ğ±STAğ±\\mathbf{x}^{\\prime}=\\mathbf{x}+\\operatorname{STA}(\\mathbf{x}),bold_x start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT = bold_x + roman_STA ( bold_x ) , (11)\n\nğ±â€²â€²=ğ±â€²+CAâ¡(ğ±â€²,ğœ),superscriptğ±â€²â€²superscriptğ±â€²CAsuperscriptğ±â€²ğœ\\mathbf{x}^{\\prime\\prime}=\\mathbf{x}^{\\prime}+\\operatorname{CA}(\\mathbf{x}^{% \\prime},\\mathbf{c}),bold_x start_POSTSUPERSCRIPT â€² â€² end_POSTSUPERSCRIPT = bold_x start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT + roman_CA ( bold_x start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT , bold_c ) , (12)\n\nğ²=ğ±â€²â€²+MLPâ¡(ğ±â€²â€²),ğ²superscriptğ±â€²â€²MLPsuperscriptğ±â€²â€²\\mathbf{y}=\\mathbf{x}^{\\prime\\prime}+\\operatorname{MLP}(\\mathbf{x}^{\\prime% \\prime}),bold_y = bold_x start_POSTSUPERSCRIPT â€² â€² end_POSTSUPERSCRIPT + roman_MLP ( bold_x start_POSTSUPERSCRIPT â€² â€² end_POSTSUPERSCRIPT ) , (13)\n\nwhere STASTA\\operatorname{STA}roman_STA and CACA\\operatorname{CA}roman_CA denotes the spatial-temporal attention and text-to-image cross attention blocks, respectively. ğ±gâˆˆâ„(ntgÃ—nhgÃ—nwg)Ã—dgsuperscriptğ±ğ‘”superscriptâ„superscriptsubscriptğ‘›ğ‘¡ğ‘”superscriptsubscriptğ‘›â„ğ‘”superscriptsubscriptğ‘›ğ‘¤ğ‘”superscriptğ‘‘ğ‘”\\mathbf{x}^{g}\\in\\mathbb{R}^{(n_{t}^{g}\\times n_{h}^{g}\\times n_{w}^{g})\\times d% ^{g}}bold_x start_POSTSUPERSCRIPT italic_g end_POSTSUPERSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT ( italic_n start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_g end_POSTSUPERSCRIPT Ã— italic_n start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_g end_POSTSUPERSCRIPT Ã— italic_n start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_g end_POSTSUPERSCRIPT ) Ã— italic_d start_POSTSUPERSCRIPT italic_g end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT denotes the input of this layer. The text embedding derived from a language model e.g. T5 [172] or a multi-modal model e.g. CLIP [169] is denoted as cğ‘citalic_c. We omit the injection of timestep information for brevity, which can be achieved with adaptive layer norm blocks [166]. We also present the potential architecture in Figure 2 (b). Finally, the generation model is trained to predict noise added to the latent representation zğ‘§zitalic_z. More details can be found in diffusion techniques mentioned in Section 2.1.3\n\n2.3.2 Training Data\n\nA large challenge to training Sora is collecting large-scale high-quality video-text pairs. Previous works [16, 32] have proven that generation performance is highly dependent on the quality of data. Low-quality data, for example, noisy video-text pairs or too simple video captions, results in generation models with pool instruction following. To tackle this problem, Sora adopts the re-captioning technique proposed in DALL-E 3 [16]. Specifically, a video captioner is trained with high-quality video-text pairs, where the text is well-aligned with the corresponding video and contains diverse and descriptive information. The video captioner could be a video version of multi-modal large language models, like GPT-4V [1], mPLUG [225], or InternVideo [214]. Then, the pre-trained video captioner is employed to generate high-quality captions for the training data of Sora. This simple method effectively improves the data quality.\n\nDuring inference, to solve the problem that users may provide too simple prompts, Sora adopts GPT-4 [1] to rewrite the prompts so that they are detailed. This enables Sora to generate high-quality videos.\n\n2.3.3 Towards World Models\n\nBased on the claim from OpenAI, Sora can work as a world simulator, since it can understand the result of an action. For an example from its technical report, Sora generates a video where a painter can leave new strokes along a canvas that persist over time. Another example is that a man can eat a burger and leave bite marks, which denotes that Sora can predict the results of eating. These two examples indicate that Sora can understand the world and predict the results of an action. This capability is well-aligned with the target of world models: understanding the world via predicting the future. Hence, we believe that the techniques behind Sora can further inspire the exloration of world models.\n\nFirst, the training and inference strategies improve the performance and efficiency in large generation models. For example, Sora learning from videos with enative aspect ratios, which obviously improve the composition and framing of generated videos. This requires both technical and engineering optimization to enbale efficent training. Generating videos with 1 minute length is a large chalenge and burden for inference server, which still impede releasing Sora to public until now. The OpenAIâ€™s soulution may be valubale for the community of large models. More potential techniues adopted in Sora can be found in [136]. We believe these contritbuons in Sora could also inpire building world models.\n\nSecond, Sora adopts Transformer-based generation with extensive parameters and large-scale training data, resuting in emergent abilities in video generation. This suggests that there also exsiting scaling laws in the visual field and directs a promising way to build large vision models or even world models.\n\nFinally, Sora emphasize the essentiality of training data for good generation performance once again. Although OpenAI has not disclosed the sources and scale of data used in Sora, some guesses think extensive game videos may be introduced during training. The game videos may contains rich physical information, helping Sora to understanding the physical world. This indicates that the incorporating physical engine may be a potential path towards building world moels.\n\n3 World Models for Autonomous Driving\n\nDriving requires navigating uncertainty. It is crucial to understand the uncertainty inherent in autonomous driving to make safe decisions, where even a minor mistake could have fatal consequences [89]. There are two primary forms of uncertainty: epistemic uncertainty, which stems from a deficit in knowledge or information, and aleatoric uncertainty, which is rooted in the inherent randomness of the real world [57]. To ensure safe driving, it is imperative to leverage past experiences embedded in world models to effectively mitigate both aleatoric and epistemic uncertainty.\n\nWorld models are adept at representing an agentâ€™s spatio-temporal knowledge about its environment through the prediction of future changes [115]. Two primary types of world models exist within autonomous driving aimed at reducing driving uncertainty, i.e., world model for end-to-end driving and world model as neural driving simulator. In the simulation environment, methods such as MILE [90] and TrafficBots [248] do not distinguish between epistemic and aleatoric uncertainties and incorporate them into the model based on reinforcement learning, enhancing their capacity for decision-making and future prediction, thereby paving the way to end-to-end autonomous driving. In the real environment, Tesla [156] and methods like GAIA-1 [91] and Copilot4D [246] involve utilizing generative models to construct neural driving simulators that produce 2D or 3D future scenes to enhance predictive capabilities, thus reducing aleatoric uncertainty. Additionally, generating new samples can mitigate epistemic uncertainty regarding rare instances such as corner cases. Figure 5 illustrates these two types of world models in autonomous driving. The neural driving simulator can be further subdivided into two categories: those generating 2D images and those simulating 3D scenes.\n\n3.1 End-to-end Driving\n\nIn the domain of autonomous driving, the development of world models assumes a crucial role as they strive to construct dynamic representations of environments. Accurate predictions about the future are imperative for ensuring safe maneuvering in contexts. However, constructing world models for autonomous driving poses distinct challenges, mainly originating from the intricate sample complexity in driving scenarios. end-to-end autonomous driving methods [159, 90, 60] strive to tackle these challenges by minimizing the search space and integrating explicit disentanglement of visual dynamics on the CARLA simulator [49]. The comparison of existing end-to-end driving methods based on world models is illustrated in Table II.\n\nIso-Dream [159] introduces a Model-Based Reinforcement Learning (MBRL) framework, aimed at effectively disentangling and utilizing controllable and noncontrollable state transitions via reinforcement learning. Furthermore, Iso-Dream optimizes the agentâ€™s behavior based on the separated latent imaginations of world models. In detail, Iso-Dream projects non-controllable states into the future to estimate state values and links them with the current controllable state. Iso-Dream enhances the agentâ€™s long-horizon decision-making capabilities, exemplified in scenarios like autonomous vehicles proactively evade potential hazards by anticipating the movements of surrounding vehicles.\n\nIso-Dream learns the world model by mapping the 2D image in front view to control signals, which is not suitable for autonomous driving in 3D space. To address this issue, MILE [90] integrates the world model with imitation learning in 3D space, i.e., Birdâ€™s Eye View (BEV) space. MILE uses 3D geometry as an inductive bias and creates a latent space from expert driving videos. The training occurs using an offline dataset of urban driving, devoid of any necessity for online engagement with the scene. In performance, it surpasses prior cutting-edge methods by a significant 31% margin in driving score on CARLA, even when operating in entirely new town and weather conditions. Moreover, MILE demonstrates its capability to execute intricate driving maneuvers solely based on plans generated through imaginative processes.\n\nSimilar to MILE, SEM2 [60] also constructs a world model in 3D space. SEM2 employs a novel approach by incorporating a latent filter to isolate crucial task-specific features and then utilizes these features to reconstruct a semantic mask. Additionally, it utilizes a multi-source sampler during training, which merges standard data with various corner case data within a single batch, effectively ensuring a balanced data distribution. Specifically, SEM2 takes camera and LiDAR as inputs, encoding them into a latent state with deterministic and stochastic variables. The initial latent state is subsequently employed to regenerate the observation. Following this, the latent semantic filter isolates driving-relevant features from the latent state, reconstructs the semantic mask, and predicts the reward. Extensive experiment conducted on the CARLA simulator showcases SEM2â€™s adeptness in sample efficiency and robustness to variations in input permutations.\n\nTrafficBots [248], another end-to-end driving method based on world model, places emphasis on forecasting the actions of individual agents within a given scenario. By factoring in the destination of each agent, TrafficBots utilizes a Conditional Variational Autoencoder (CVAE) [187] to imbue individual agents with unique characteristics, enabling action anticipation from a BEV perspective. TrafficBots offers quicker operational speeds and scalability to handle larger numbers of agents. Experiments carried out on the Waymo dataset illustrate TrafficBotsâ€™ capacity to emulate realistic multi-agent behaviors and attain promising results in motion prediction tasks.\n\nThe above methods [159, 90, 60, 248] were experimented in CARLA v1, but inherently face challenges regarding data inefficiency in CARLA v2. CARLA v2 offers a more quasi-realistic testbed. Addressing the complexities of CARLA v2 scenarios, Think2Drive [124], a model-based reinforcement learning method for autonomous driving, encourages the planner to think within the learned latent space. This approach significantly enhances training efficiency by utilizing a low-dimensional state space and leveraging parallel computing of tensors. Think2Drive achieves expert-level proficiency on CARLA v2 simulator after a mere 3-day training period utilizing a single A6000 GPU. Furthermore, Think2Drive introduces the CornerCase Repository, a novel benchmark designed to assess driving models across diverse scenarios.\n\nDespite the advancements seen in world models for end-to-end driving using reinforcement learning, a significant limitation remains: its primary emphasis on simulation environments. Next, we will delve into research on world models for autonomous driving in real-world scenarios.\n\n3.2 Neural Driving Simulator\n\nHigh-quality data serves as the bedrock for training deep learning models. While text and image data are readily available at low costs, acquiring data in the realm of autonomous driving poses challenges owing to factors such as spatio-temporal complexities and concerns regarding privacy. This is particularly true for addressing long-tail targets that directly impact realistic driving safety. World models are pivotal for understanding and simulating the complex physical world [91]. Some recent endeavors have introduced diffusion models [85] into the domain of autonomous driving to build world models as neural simulators to generate requisite autonomous 2D driving videos [91, 209, 231, 93]. Additionally, some methods employ world models to generate 3D occupancy grids or LiDAR point clouds depicting future scenes [246, 19, 233, 152]. Table III provides an overview of these neural driving simulator methods based on world models.\n\n3.2.1 2D Scene Generation\n\nWorld models for driving video generation entail tackling two pivotal challenges: Consistency and Controllability. Consistency is crucial for maintaining temporal and cross-view coherence between generated images, whereas controllability ensures that generated images align with corresponding annotations [218]. The comparison of exiting 2D driving video generation methods based on world models are shown in Table IV.\n\nGAIA-1 [91] is a cutting-edge generative world model designed to produce lifelike driving videos, offering precise manipulation of both ego-vehicle actions and environmental elements. GAIA-1 tackles the challenge of world modeling by leveraging video, text, and action inputs as sequences of tokens, predicting subsequent tokens in an unsupervised way. Its structure comprises two main elements: the world model and the video diffusion decoder. The world model, boasting 6.5 billion parameters, underwent a 15-day training period utilizing 64 NVIDIA A100s, while the video decoder, with 2.6 billion parameters, was trained for the same duration using 32 NVIDIA A100s. The world model meticulously examines the elements and dynamics within the scene, whereas the diffusion decoder transforms latent representations into high-fidelity videos imbued with intricate realism. GAIA-1â€™s training corpus comprises 4,700 hours of driving videos collected in London, spanning from 2019 to 2023. Notably, GAIA-1 demonstrates an understanding of 3D geometry and can capture the complex interactions induced by road irregularities. Furthermore, GAIA-1 adheres to similar scaling laws observed in Large Language Models (LLMs). With its learned representations and control over scene elements, GAIA-1 opens new possibilities for enhancing embodied intelligence.\n\nWhile GAIA-1 can generate realistic autonomous driving scene videos, its controllability is limited to using only text and action as conditions for video generation, whereas autonomous driving tasks require adherence to structured traffic constraints. DriveDreamer [209], which excels in controllable driving video generation, seamlessly aligns with text prompts and structured traffic constraints, including HD-Map and 3D box data. The training pipeline of DriveDreamer comprises two stages: initially, DriveDreamer is trained with traffic structural information as intermediate conditions, significantly improving sampling efficiency. In the subsequent stage, the world model is developed through video prediction, where driving actions are iteratively utilized to update future traffic structural conditions. This enables DriveDreamer to anticipate variations in the driving environment based on different driving strategies. Through extensive experiments on the challenging nuScenes [25] benchmark, DriveDreamer is confirmed to enable precise and controllable video generation, representing the structural constraints of real-world traffic situations.\n\nTo further bolster the consistency and controllability of generated multi-view videos, DriveDreamer-2 [249] is introduced as an evolution of the DriveDreamer framework. DriveDreamer-2 integrates a LLM to augment the controllability of video generation. Initially, DriveDreamer-2 integrates an LLM interface to interpret user queries and translate them into agent trajectories. Subsequently, it generates an HD-Map in accordance with traffic regulations based on these trajectories. Additionally, DriveDreamer-2 proposes the unified multi-miew model to improve temporal and spatial consistency to generate multi-view videos.\n\nDifferent from DriveDreamer-2 with LLM, ADriver-I [103] leverages Multimodal Large Language Models (MLLMs) to enhance the controllability of generating driving scene videos. Inspired by the interleaved document approach in MLLMs, ADriver-I introduces interleaved vision-action pairs to establish a standardized format for visual features and their associated control signals. These vision-action pairs are utilized as inputs, and ADriver-I forecasts the control signal of the present frame in an autoregressive manner. ADriver-I continues this iterative process with the predicted next frame, enabling it to achieve autonomous driving in the synthesized environment. Its performance is rigorously assessed through extensive experimentation on datasets such as nuScenes [25] and sizable proprietary datasets.\n\nADriver-I is limited to generating single-view videos. To generate multi-view videos as DriveDreamer-2, Panacea [218] and DrivingDiffusion [127] are proposed. Panacea [218] is an innovative video generation system designed specifically for panoramic and controllable driving scene synthesis. It operates in two stages: initially crafting realistic multi-view driving scene images, then expanding these images along the temporal axis to create video sequences. For panoramic video generation, Panacea introduces decomposed 4D attention, enhancing both multi-view and temporal coherence. Additionally, Panacea utilizes ControlNet to incorporate BEV sequences. Beyond these fundamental features, Panacea maintains flexibility by enabling manipulation of global scene attributes through textual descriptions, including weather, time, and scene details, providing a user-friendly interface for generating specific samples. DrivingDiffusion [127] also presents a multi-stage approach for generating multi-view videos. It involves several crucial stages: multi-view single-frame image generation, shared single-view video generation across multiple cameras, and post-processing capable of handling extended video generation. It also introduces local prompts to improve the quality of images effectively. Subsequent to the generation process, post-processing is employed to enhance the coherence among different views in subsequent frames. Additionally, it utilizes a temporal sliding window algorithm to prolong the video duration.\n\nThe objective of the above methods is to generate realistic driving scenario videos given certain conditions. Drive-WM [212] takes this a step further by utilizing predicted future scene videos for end-to-end planning applications to enhance driving safety. Drive-WM introduces multi-view and temporal modeling to generate multi-view frames. To improve multi-view consistency, Drive-WM proposes factorizing the joint modeling to predict intermediate views conditioned on adjacent views, significantly enhancing consistency between views. Drive-WM also introduces a simple yet effective unified condition interface, enabling flexible utilization of diverse conditions such as images, text, 3D layouts, and actions, thereby simplifying conditional generation. Furthermore, by leveraging the multi-view world model, Drive-WM explores end-to-end planning applications to enhance autonomous driving safety. Specifically, at each time step, Drive-WM utilizes the world model to generate predicted future scenarios for trajectory candidates sampled from the planner. These futures are evaluated using an image-based reward function, and the optimal trajectory is selected to extend the planning tree. Testing on real-world driving datasets validates Drive-WMâ€™s capability to produce top-tier, cohesive, and manageable multi-view driving videos, thereby unlocking avenues for real-world simulations and safe planning.\n\nControl signals like bounding boxes or HD-Maps provide a sparse representation of the driving scene. WoVoGen [139] enhances diffusion-based generative models by introducing a 4D world volume. Initially, WoVoGen builds a 4D world volume by merging a reference scene with a forthcoming vehicle control sequence. This volume then guides the generation of multi-view imagery. Within this 4D structure, each voxel is enriched with LiDAR semantic labels obtained via the fusion of multi-frame point clouds, enhancing the depth and complexity of environmental comprehension.\n\nSubjectDrive [93] has undertaken further research to explore the effects of increasing the scale of generated videos on the performance of perception models in autonomous driving. Through their investigations, they have demonstrated the efficacy of scaling generative data production in continuously enhancing autonomous driving applications. It has pinpointed the pivotal significance of enhancing data diversity in efficiently expanding generative data production. Consequently, SubjectDrive has developed an innovative model incorporating a subject control mechanism.\n\nThe above methods for generating driving videos have largely been studied on relatively small datasets like nuScenes [25]. GAIA-1 [91] was trained on a dataset of 4,700 hours, but the training dataset is not publicly available. Recently, GenAD [231] has released the largest multimodal video dataset for autonomous driving, OpenDV-2K, exceeding the scale of the widely used nuScenes dataset by a multiplier of 374. OpenDV-2K contains 2,059 hours of video content accompanied by textual annotations, drawn from a combination of 1,747 hours sourced from YouTube and an additional 312 hours gathered from public datasets. Addressing common challenges such as causal confusion and handling large motions, GenAD utilizes causal temporal attention and decoupled spatial attention mechanisms to effectively capture the rapid spatio-temporal fluctuations present in highly dynamic driving environments. This architecture allows GenAD to generalize across diverse scenarios in a zero-shot way. This acquired understanding is further substantiated through the application of its learned knowledge to driving challenges, including planning and simulation tasks.\n\n3.2.2 3D Scene Generation\n\nIn addition to generating 2D videos for autonomous driving through world modeling, some methods delve into utilizing world models to produce 3D LiDAR point clouds or 3D occupancy grids.\n\nCopilot4D [246] presents an innovative approach to world modeling by first tokenizing LiDAR point cloud observations with VQ-VAE [204], then predicting future LiDAR point clouds via discrete diffusion. To efficiently decode and denoise tokens in parallel, Copilot4D modifies the masked generative image Transformer to fit within the discrete diffusion framework with slight adjustments, yielding significant improvements. When utilized for training world models based on LiDAR point cloud observations, Copilot4D achieves a remarkable reduction of over 65% in Chamfer distance for point cloud forecasting at 1s prediction and over 50% at 3s prediction across datasets such as nuScenes [25], Argoverse2 [219], and KITTI Odometry [61].\n\nCopilot4D utilizes unannotated LiDAR data to construct its world model, while OccWorld [252] delves into the 3D occupancy space for the representation of 3D scenes. OccWorld initiates its approach by employing a VQ-VAE [204] to refine high-level concepts and derive discrete 3D semantic occupancy scene tokens in a self-supervised manner. Subsequently, it customizes the GPT [22] architecture, introducing a spatial-temporal generative Transformer to forecast scene tokens and ego tokens. Through these advancements, OccWorld achieves significant results in 4D occupancy forecasting and planning.\n\nCopilot4D and OccWorld employ past LiDAR or 3D occupancy frames to generate future 3D scenes, whereas MUVO [19] adopts a more comprehensive strategy by leveraging raw camera and LiDAR data as input. MUVO aims to acquire a sensor-agnostic geometric representation of the environment and predicts future scenes in the forms of RGB images, 3D occupancy grids, and LiDAR point clouds. Initially, MUVO undertakes image and LiDAR point cloud processing, encoding, and fusion utilizing a Transformer-based architecture. Subsequently, it inputs the latent representations of the sensor data into a transition model to establish a probabilistic model of the current state. Concurrently, MUVO forecasts the probabilistic model of future states and generates samples from it.\n\nWhile Copilot4D, OccWorld, and MUVO generate 3D scenes without control, LidarDM [261] excels in producing layout-aware LiDAR videos. LidarDM employs latent diffusion models to generate the 3D scene, integrating dynamic actors to establish the underlying 4D world, and subsequently generating realistic sensory observations within this virtual environment. Beginning with the input traffic layout at time t=0ğ‘¡0t=0italic_t = 0, LidarDM initiates the generation process by creating actors and the static scene. Subsequently, LidarDM generates the motion of the actors and the ego-car, composing the underlying 4D world. Finally, a generative- and physics-based simulation is utilized to produce realistic 4D sensor data. The LiDAR videos generated by LidarDM are realistic, layout-aware, physically plausible, and temporally coherent. They demonstrate a minimal domain gap when tested with perception modules trained on real data.\n\nAs an abstract spatio-temporal representation of reality, the world model possesses the capability to predict future states based on the present. The training mechanism of world models holds promise in establishing a foundational pre-trained model for autonomous driving. UniWorld [151], ViDAR [233], and DriveWorld [152] delve into the exploration of 4D pre-training based on world models, aiming to enhance various downstream tasks of autonomous driving, such as perception, prediction, and planning.\n\nUniWorld [151] introduces the concept of predicting future 3D occupancy as a pre-text task for autonomous driving, leveraging extensive unlabeled image-LiDAR pairs for 4D pre-training. It takes multi-view images as inputs, generating feature maps in a unified BEV space [215]. These BEV representations are then utilized by a world model head to predict the occupancy of future frames. UniWorld demonstrates improvements in intersection over union for tasks like semantic scene completion and motion prediction compared to 3D pre-training methods [150, 149].\n\nWhile UniWorld has demonstrated the effectiveness of 4D pre-training based on world models for autonomous driving, it predicts future scenes by adding a simple occupancy head. ViDAR [233] proposes latent rendering operator with differentiable ray-casting for future scene prediction. ViDAR consists of three main components: history encoder, latent rendering operator, and future decoder. The history encoder embeds visual sequences into BEV space. Subsequently, these BEV features undergo processing by the latent rendering operator, which significantly bolsters downstream performance. The future decoder, functioning as an autoregressive Transformer, utilizes historical BEV features to iteratively forecast future LiDAR point clouds for various timestamps.\n\nTo enhance 4D pre-training for autonomous driving by better capturing spatio-temporal dynamics, DriveWorld [152] takes a further step by separately addressing temporal and spatial information. DriveWorld introduces the memory state-space model to reduce uncertainty within autonomous driving across both spatial and temporal dimensions. Firstly, to tackle aleatoric uncertainty, DriveWorld proposes the dynamic memory bank module, which learns temporal-aware latent dynamics to predict future scenes. Secondly, to mitigate epistemic uncertainty, DriveWorld introduces the static scene propagation module, which learns spatial-aware latent statics to provide comprehensive scene context. Moreover, DriveWorld introduces the task prompt, utilizing semantic cues as guidance to dynamically adjust the feature extraction process for various driving tasks.\n\n4 World Models for Autonomous Agents\n\nIn artificial intelligence, an autonomous agent refers to a system that can perceive its surrounding environment through sensors (such as cameras) and act upon it through actuators to achieve specific goals[58]. These agents can be physical, like robots, or virtual, such as software programs that perform tasks in digital environments.\n\nGiven a goal, agents need to plan a sequence of actions. There are already many successful algorithms for dynamic planning in known environments. In most cases, however, the environment is complex and stochastic, making it difficult to model by human experience explicitly. Therefore, this fieldâ€™s core topic is how agents learn to plan in an unknown and complex environment. One way to solve this problem is to have the agent accumulate experience and learn behaviors directly from the interaction with the environment, without modeling the state changes of the environment (the so-called model-free reinforcement learning). While this solution is simple and flexible, the learning process relies on many interactions with the environment, which may be extremely expensive, even unacceptable.\n\nWorld Models[69] is the first work that introduces the concept of the world model in the field of reinforcement learning, modeling knowledge about the world from the agentâ€™s experience and gaining the ability to predict the future. This work demonstrates that even a simple RNN model can capture the dynamics of the environment and support the agent to learn and evolve policies in this model. This learning paradigm is referred to as learning in imagination [72]. With world models, the cost of trials and failures can be greatly reduced[222].\n\nIn this section, we introduce the world models for autonomous agents. We first describe the general framework of a world model-based agent, including the key components and the model structures widely used in world model-based agents in Section 4.1. Then, we introduce the agents serving a variety of tasks, such as game agents and robotics, in Section 4.2. Finally, we present the benchmarks that are commonly used to evaluate the performance of world model-based agents.\n\n4.1 General Framework of an Agent based on World Model\n\nMost works implement world model-based agents under a basic framework originating from robotics. In the framework, the world model is the core component. To model and predict the surrounding environment, pioneers proposed several effective structures, which are widely used in later works. In this section, we describe in detail the key components of the framework and the widely used structures of world models.\n\n4.1.1 Key Components\n\nFrom the view of software engineering, an agent system can be decomposed into four components[181]:\n\nSensor Sensors are the interface between an agent and its environment, providing the raw (or interpreted) information the robot needs to understand its current context and make decisions. Perception of the environment encompasses multiple modalities, including vision through cameras, audition through microphones, touch through touch sensors, etc. Among these modalities, vision is critical. Most research uses vision as the only way for agents to perceive the environment.\n\nActor. Actors are the mechanisms through which an agent exerts influence or effectuates changes in its environment. They are the output devices that allow the agent to perform actions, such as motors for movement, robotic arms for manipulation, and communication interfaces for interaction with other systems or humans. The actions taken by the agent are determined by the decisions made within its planning system and are executed through the actuators.\n\nPlanning. Planning is the cognitive process that enables the autonomous agent to determine a sequence of actions that will lead to achieving its goals. It involves analyzing the current state of the environment as perceived by the sensors, defining the desired end state, and selecting the most appropriate actions to bridge the gap between the current and desired states. The planning component must consider the agentâ€™s capabilities, constraints, and the potential consequences of its actions. Effective planning allows the agent to act purposefully and adaptively, optimizing its behavior to achieve its objectives efficiently and effectively.\n\nWorld Model. A world model is an internal representation of the surrounding environment. This model is crucial for the agentâ€™s ability to understand the context in which it operates, predict the outcomes of its actions, and make informed decisions. The world model interacts with the other three components through tell and ask interfaces[181]. That is to say, it receives information from other components to update its state and also responds to queries from other components. A robust world model can reasonably predict the future state when told with current perceptions and actions, thereby guiding the planning component to make wiser decisions.\n\n4.1.2 Widely-used Model Structure\n\nA world modelâ€™s key ability is predicting the environmentâ€™s future state. Given the inherent randomness in most environments, predictions should maintain a balance between determinism and uncertainty. Many researches have been conducted on this problem, proposing a variety of model structures. Figure 6 shows the works in this field. Among these works, the most widely-used structures are RSSM[71, 41, 222, 158], JEPA[115, 4, 54, 13, 12], and Transformer-based models[29, 247, 175, 23, 210].\n\nRecurrent State Space Model. The Recurrent State Space Model (RSSM) is the core structure of the Dreamer series. RSSM aims to facilitate prediction in latent spaces. It learns a dynamic model of the environment from pixel observations and selects actions by planning in the encoded latent space. By decomposing the latent state into stochastic and deterministic parts, this model considers both deterministic and stochastic factors of the environment. Due to its exceptional performance in continuous control tasks for robots, many subsequent works have expanded upon its foundation.\n\nJoint-Embedding Predictive Architecture. The Joint-Embedding Predictive Architecture (JEPA) is proposed in a paper by LeCun[115] that laid out a conceptual framework for future autonomous machine intelligence architecture. It learns the mapping from input data to predicted output. This model is different from traditional generative models as it does not directly generate pixel-level output, but makes predictions in a higher-level representation space, allowing the model to focus on learning more semantic features. Another core idea of JEPA is to train the network through self-supervised learning so that it can predict missing or hidden parts in the input data. Through self-supervised learning, models can be pre-trained on a large number of unlabeled data and then fine-tuned on downstream tasks, thereby improving their performance on a variety of visual[4, 13, 12] and non-visual tasks[54].\n\nTransformer-based World Models. The Transformer[205] originates from the natural language processing task. It operates on the principle of the attention mechanism, which enables the model to simultaneously focus on different parts of the input data. Transformers have been proven to be more effective than Recurrent Neural Networks (RNNs) in many domains that require long-term dependencies and direct memory access for memory-based reasoning[8], thus gaining increasing attention in the field of reinforcement learning in recent years. Since 2022, multiple works have attempted to construct world models based on the Transformer and its variants[146, 175, 247], achieving better performance than the RSSM model on some complex memory interaction tasks[29]. Among them, Googleâ€™s Genie[23] has attracted considerable attention. This work constructs a generative interactive environment based on the ST-Transformer[227], trained through self-supervised learning from a vast collection of unlabeled internet video data. Genie demonstrates a new paradigm for manipulable world models, offering a glimpse into the immense potential for the future development of world models.\n\n4.2 Agents for Different Tasks\n\nMany researchers have explored the application of agents in various fields and tasks, such as gaming, robotics, navigation, task planning, etc. Among the most widely studied tasks are games and robotics.\n\n4.2.1 Game Agent\n\nGetting AI systems to learn to play games has been an interesting topic for a long time. The research on game agents not only improves the game experience but more importantly, helps people develop more advanced algorithms and models.\n\nWith the introduction of the Arcade Learning Environment (ALE)[14], Atari games have gained a lot of emphasis as a benchmark for reinforcement learning. The Atari collection includes more than 500 games, covering a wide variety of game types and challenges, making it ideal for evaluating the capabilities of reinforcement learning algorithms. Many studies have shown that reinforcement learning can make agents play games at a level comparable to that of human players[183, 82, 51, 113]. However, most of them require a huge amount of interaction steps with the environment. World models can predict future states of the environment, allowing agents to learn in imagination, thus significantly reducing the number of interactions required for learning.\n\nRES[35] is an RNN-based environment simulator that can predict the subsequent state of the environment based on a series of actions and corresponding environmental observations. Based on this capability, SimPLe[105] designs a novel stochastic video prediction model, which achieves significant improvement in sample efficiency. Under the constraint of 100K interactions, SimPLe has a much better performance in Atari games compared to previous model-free reinforcement learning methods.\n\nDreamerV2[73] trains a game agent based on the RSSM model[71]. Unlike previous approaches that use continuous latent representations, DreamerV2 uses discrete categorical variables. This discretization method enables the model to capture the dynamic changes in the environment more accurately. DreamerV2 further uses the actor-critic algorithm to learn the behaviors purely from imagined sequences generated by the world model and achieves performance comparable to human players on the Atari 200M benchmark[153].\n\nIRIS[146] is one of the pioneers that apply Transformer[205] in the world model. The agent learns its skill in a world model based on the autoregressive Transformer. As pointed out by Robine et al.[175], the autoregressive Transformer can model more complex dependencies by allowing the world model to directly access previous states, while previous works can only view a compressed recurrent state. IRIS shows that the Transformer architecture is more efficient in sampling, outperforming humans in the Atari100k benchmark[105] by only two hours of gameplay.\n\nTWM[175] proposes a Transformer-XL[38]-based world model. Transformer-XL solves the problem of capturing long-distance dependencies in language modeling tasks by introducing the segment-level recurrence mechanism to extend the length of context. TWM migrates this capability into the world model, enabling the capture of long-term dependencies between the states of the environment. To run more efficiently, TWM further trains a model-free agent in the latent imagination, avoiding a full inference of the world model in runtime.\n\nSTORM[247] sets a new record in no-resorting-to-lookahead-search methods on Atari100k benchmark[105] by stochastic Transformer. Inspired by the fact that introducing random noise into the world model helps enhance the robustness and reduce cumulative errors in autoregressive predictions, STORM employs a categorical variational autoencoder[109], which inherently has a stochastic nature.\n\nGenie[23] is a novel generative environment developed by the DeepMind team. It learns the ability to generate interactive 2D worlds by unsupervised learning from many internet videos without labels. The most attractive point is that it can not only generate an entirely new virtual environment based on image or text prompts but also predict coherent video sequences of that environment frame by frame based on user input actions. Genie enhances the efficiency of virtual content creation as well as provides a rich interactive learning platform for the training of future AI agents. Although the current video quality and frame rate still need improvement, it has already demonstrated the immense potential of generative AI in building future virtual worlds.\n\n4.2.2 Robotics\n\nGetting an agent to learn to manipulate a robot is a long-term challenge. Agents are desired to plan autonomously, make decisions, and control actuators (e.g., robotic arms and legs) to complete complex interactions with the physical world. Common basic tasks include walking, running, jumping, grasping, carrying, and placing objects. Some of the more complicated tasks require a combination of several basic tasks, such as taking a specific item out of a drawer or making a cup of coffee.\n\nOne difference between a robot and a game agent is that the goal of the robot is to interact with the real environment, which not only makes the environment dynamics more complex and stochastic but also greatly increases the cost of interacting with the environment during the training process. Therefore, it is particularly important to reduce the number of interaction steps with the environment and enhance sampling efficiency in such scenarios. In addition, the control of the actuators is in a continuous action space, which is also very different from the discrete action space in the game environment.\n\nPrevious works of model-based planning[39, 59, 79] learn low-dimensional environment dynamics by assuming that access to the underlying state and the reward function is available. But in complex environments, this assumption is often untenable. Hafner et al.[71] suggested learning the environment dynamics from pixels and planning in latent spaces. They proposed RSSM, which is the base of the later Dreamer-like world models. They achieved similar performance to the state-of-art model-free methods within less than 1/100 episodes on six continuous control tasks of DeepMind Control Suite (DMC)[195], which proves that learning latent dynamics of the environments in the image domain is a promising approach.\n\nHowever, PlaNet[71] learns the behaviors by online planning, i.e., considering only rewards within a fixed imagination horizon, which brings shortsighted behaviors. To solve this problem, Hafner et al. further proposed DreamerV1[72], an agent that learns long-horizon behaviors purely from the imagination of the RSSM-based world model. Predicting in latent space is memory efficient, thus allowing imagine thousands of trajectories in parallel. DreamerV1 uses a novel actor-critic algorithm to learn behaviors beyond the horizon. The evaluation performed on visual control tasks of DMC shows that DreamerV1 exceeds previous model-based and model-free approaches in data efficiency, computation time, and final performance.\n\nSafeDreamer[96] aims to address safe reinforcement learning, especially in complex scenarios such as vision-only tasks. SafeDreamer employs an online safety-reward planning algorithm for planning within world models to meet the constraints of vision-based tasks. It also combines Lagrangian methods with online and background planning within world models to balance long-term rewards and costs. SafeDreamer demonstrates nearly zero-cost performance across low-dimensional and visual input tasks and outperforms other reinforcement learning methods in the Safety-Gymnasium benchmark[101], showcasing its effectiveness in balancing performance and safety in reinforcement learning tasks.\n\nThe above works only learn and evaluate their performance in simple simulation environments, while the real environments often contain task-unrelated visual distractions such as complex backgrounds and varying lights. RSSM learns the world model by reconstructing image observations, making it very sensitive to visual distractions in images and difficult to capture small but important content. Therefore, based on DreamerV1[72], Dreaming[157] avoids the auto-encoding process by directly imagining and planning in the latent space, and trains the world model by contrastive learning, which does not rely on pixel-level reconstruction loss, so that the method is robust to visual distractions in the environment. DreamingV2[158] further explores how to apply contrastive learning to the discrete latent space of DreamerV2[73]. Experimental results on 5 simulated robot tasks with 3D space and photorealistic rendering show that DreamingV2 can effectively handle complex visual observations, and the performance is significantly better than that of DreamerV2.\n\nSimilar efforts are made by DreamerPro[41] and Dr.G[70], both of which use a reconstruction-free approach to address the visual sensitivity issue of RSSM. The difference is that DreamerPro uses the prototype learning method to train the prediction of the world model in the latent space, which avoids the expensive computation caused by the large batch size required for contrast learning. Dr.G, on the other hand, uses a self-supervised method of double-contrast learning to replace the reconstruction loss in DreamerV1[72]. Both are evaluated in environments from DMC synthesized with complex background videos, verifying their robustness to visual distractions.\n\nBesides those works involving simulated environments only, some works are trying to train a robot in the real world. The most difficult thing is that interaction with the real world is expensive or even dangerous. Thus the ability of training in imagination is especially important in such scenarios. RobotDreamPolicy[167] learns a world model first and then learns the policy in the world model to reduce the interactions with the real environment. During the training of the world model, the robot executes random actions in the environment, collecting pairs of the image before action, action, and the image after action as the training data. DayDreamer[222] applies DreamerV2[73] to 4 real robots and directly trains the model online in real environments. The authors found in experiments that the Dreamer model is capable of online learning in the real world, and can master a skill in a very short time. These works provide strong evidence that the sample efficiency of the world model can help robots learn various skills efficiently with fewer interactions.\n\n4.2.3 Diverse Environments and Tasks\n\nBesides game and robotic tasks, some research works have looked at other tasks such as navigation. PathDreamer[110] applies the idea of world model to indoor navigation tasks. The world model is used to enhance environmental awareness and predictive planning. Given one or more previous observations, PathDreamer can predict plausible panoramic images of the future, even for unseen rooms or regions behind corners. Furthermore, PathDreamer innovatively uses 3D point clouds for environment representation, which significantly improves navigation success.\n\nThe JEPA series of work applies the architecture proposed by LeCun[115] to a variety of modal understanding and prediction tasks. I-JEPA[4] is a non-generative self-supervised learning method that learns highly semantic visual representations by predicting the representations of different target blocks within the same image from a single context block. A-JEPA[54] proposes a self-supervised learning method based on audio spectrograms, which effectively applies the successfully masked modeling principle from the visual domain to audio. A context encoder is used to predict and align the representations of different target blocks from the same audio spectrogram. MC-JEPA[13] is a self-supervised learning method that simultaneously learns video content features and motion features through JEPA, using a shared encoder to improve the accuracy of motion estimation and enrich the content features to include motion information. V-JEPA[12] extends I-JEPA to feature prediction in videos. It presents a suite of vision models that are exclusively trained based on the objective of feature prediction. These models are developed without relying on supervisory signals such as pre-trained image encoders, negative examples, text, and reconstruction techniques.\n\nOther research efforts aim to study agents suitable for diverse tasks. DreamerV3[74] is a universal algorithm that realizes cross-domain learning with fixed hyperparameters by signal amplitude transformation and robust normalization. The authors evaluated multiple benchmark sets from Atari games, high/low dimensional continuous control tasks, survival tasks, spatial and temporal reasoning tasks, etc. The results show that DreamerV3 can master different domains only by relying on the same set of hyperparameters, and its performance is even better than some specialized algorithms designed for specific domains. DreamerV3 is also the first agent to successfully collect diamonds from scratch in Minecraft without providing any human experience.\n\nPlan2Explore[184] proposes a self-supervised two-stage learning process. In the first stage, the agent explores the environment in a self-supervised manner, gathers information about the environment, and summarizes past experiences in the form of a parametric world model. It is worth noting that no reward information is provided to the agent during this phase, and the exploration is performed by the agent autonomously. Then the agent learns behaviors in the trained world model for specific tasks. This stage can be done with little or no interaction with the environment. The two-stage learning process allows the agent to obtain a more universal world model, making the agent learn downstream tasks more efficiently.\n\nSWIM[145] aims to solve the learning of complex and general skills in the real world. SWIM claims that an agent must utilize internet-scale human video data to understand rich interactions carried out by humans and gain meaningful affordances. To this end, SWIM proposes a high-level, structured, human-centric action space that is applicable for both humans and robots. The world model is first trained from a large dataset containing around 50K egocentric videos. Then the world model is finetuned with robot data to fit the robot domain. After that, behaviors for specified tasks can be learned in the trained world model using the standard cross-entropy method[179]. With the help of human action videos, SWIM achieves about two times higher success than prior approaches while requiring less than 30 minutes of real-world interaction data.\n\nHarmonyDream[141] identifies the world model as a multi-task model consisting of observation modeling tasks and reward modeling tasks. HarmonyDream argues that traditional world modeling methods, which tend to focus on observation modeling, can become difficult and inefficient due to the complexity of the environment and the limited capacity of the model. HarmonyDream maintains a balance between observation modeling and reward modeling by automatically adjusting the loss coefficient, which can be adapted to different types of tasks and avoid complicated hyperparameter adjustments.\n\nRoboDreamer[255] learns compositional world models to enhance robotic imagination. It decomposes the video generation process and leverages the inherent compositionality of natural language. In this way, it can synthesize video plans of unseen combinations of objects and actions. RoboDreamer dissects language instructions into a set of primitives, which then serve as distinct conditions for a set of models to generate videos. This method not only demonstrates strong zero-shot generalization capabilities but also shows promising results in multimodal-instructional video generation and deployment on robotic ma"
    }
}
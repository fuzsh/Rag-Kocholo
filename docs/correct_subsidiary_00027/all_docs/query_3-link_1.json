{
    "id": "correct_subsidiary_00027_3",
    "rank": 1,
    "data": {
        "url": "https://www.freepatentsonline.com/6269467.html",
        "read_more_link": "",
        "language": "en",
        "title": "Block based design methodology",
        "top_image": "",
        "meta_img": "",
        "images": [],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Christopher K"
        ],
        "publish_date": null,
        "summary": "",
        "meta_description": "A method and apparatus for designing a circuit system, including selecting a plurality of pre-designed circuit blocks to be used to design the circuit system, collecting data reflecting the experience",
        "meta_lang": "en",
        "meta_favicon": "/images/favicon_1.ico",
        "meta_site_name": "",
        "canonical_link": "https://www.freepatentsonline.com/6269467.html",
        "text": "FIELD OF THE INVENTION\n\nThe present invention relates generally to integrated circuit (\"IC\") device design, and more specifically to the design of systems re-using pre-designed circuit blocks.\n\nBACKGROUND OF THE INVENTION\n\nIn recent years, constant innovation in silicon process technology has drastically reduced the price and increased the performance and functionality of integrated circuit devices, thus stimulating the development of the electronics manufacturing and information processing industries. In turn, these fast growing industries impose increasing demands on the integrated circuit design system developers for still faster and cheaper devices. As a result, the design industry is now undergoing drastic changes, including:\n\n(1) Chip designs are getting larger and more complex. For example, in 1997, a typical integrated circuit contained from 100-500K gates. In 1998, the typical device contained one to two million gates. Technology in 1999 has shown the continuation of this trend with devices of four to six million gates being built.\n\n(2) Chip designs are becoming more application-specific. In the early days of IC design, device manufactures would produce various \"off-the-shelf\" chips, which end users would design into their electronic products. Currently, electronic product manufactures more often order custom chip designs to perform specific functions.\n\n(3) Electronic product development is now primarily driven by consumer demand, which has shortened product life cycles and, therefore shortened allowed design time and resources. For example, in 1997, the average design cycle was between 12-18 months. In 1998, that average time decreased to 10-12 months and in 1999 the industry is pushing towards 8-10 month design-cycle times.\n\n(4) Design time constraints require parallel design effort. Formerly, critical design decisions for upstream system components could wait until downstream system component designs were verified. Design managers no longer have the luxury of sequentially performing design tasks. Several system components may have to be developed concurrently. Thus, design managers are required to make crucial predictions before at least some system component designs are complete.\n\nTo address these demands, electronic system design is now moving to a methodology known in the art as Block Based Design (\"BBD\"), in which a system is designed by integrating a plurality of existing component design blocks (also referred to in the art as \"intellectual property blocks\" or \"IP blocks\"). These pre-designed blocks may be obtained from internal design teams or licensed from other design companies, and may be supported by fundamentally different design structures and environments. Moreover, pre-designed blocks may be developed to meet different design requirements and constraints.\n\nAnother challenge faced by designers using BBD is the front-end (project acceptance) delays and risk brought about by uncertainty in determining system design feasibility. Current ASIC (application-specific integrated circuit) designs are primarily presented at the RTL (register transfer level) stage, and some even earlier, at specification level, to designers by customers. These designs are then partitioned in a manner based upon the limitations of available synthesis technology, according to the area, performance, and power tradeoffs required to provide cost-effective implementation. In this manner, the designer accepts a system specification as input and ultimately provides a netlist-level design for physical implementation (including design place, route, and verification). If design specifications are within the capabilities of the intended or available processing technology, including clocking, power, and size specifications, the available design methodology is reasonably predictable and works well with available circuit design tools.\n\nHowever, the RTL-level design and the system-level design activities are typically uncoupled or loosely coupled, meaning there is no coherent link from the system-level functional definition to the ASIC (RTL) level. The RTL-level design is developed based upon a paper ASIC specification and verified by a newly formed test suit created around the ASIC interface. Thus, available design and implementation methodologies for ASIC design present a number of problems, which hamper efficient block integration.\n\nFirst, current methodologies do not provide a top-down approach to comprehensively evaluate and ensure compatibility to integrate a plurality of design blocks provided by multiple sources having differing design considerations, while providing hierarchical verification and short assembly time within tight time-to-market constraints.\n\nAlso, existing methodologies for ASIC design do not provide scalability. A significant number of existing methodologies are focused around a flat design. This approach has led to significant problems in the length of time required to assemble the top-level design for a system having more than one million gates.\n\nIn addition, existing ASIC design methodologies are not suitable for reuse of pre-designed circuit blocks. Available schemes do not provide guidelines to solve the timing, clock, bus, power, block arrangement, verification, and testing problems associated with integrating circuit design blocks within specific device architectures. Thus, without a comprehensive approach to block reuse, existing methodologies bring about an ad-hoc and unpredictable design approach, reduce design realization feasibility, increase cost and time to delivery, and often trigger performance-reducing modifications to the pre-designed circuit blocks themselves in order to fit them into the designed system. Furthermore, existing methodologies do not provide performance trade-off analysis and feedback of critical design parameters, such as clock frequency, and area versus risk of successfully and predictably completing chip designs and implementations.\n\nThere is, therefore, a need for a methodology that can satisfy the evolving environment and address the shortcomings of the available art.\n\nThere is also a need for a suitable methodology for using and re-using pre-designed circuit blocks from multiple sources in a circuit design.\n\nCombining IP blocks also brings about the need for \"glue\" logic, the logic that allows the blocks to work together on a single device. Glue logic is the logic primarily responsible for interconnecting design blocks, and normally resides between the blocks, dispersed throughout the design. Glue logic elements can be added to a design during various stages of chip planning, or can reside at the outermost boundary of each block within a design to act as an interconnect mechanism for the host block. Regardless of its source, glue logic must be optimally placed within the design to minimize wire congestion and timing complications which arise from placement of glue logic between blocks, introducing delays which may not have been contemplated by the original block designer.\n\nThere is therefore a need in the art to which the present invention pertains for an improved method of placing and distributing glue logic in a block based design.\n\nThere is also a need for a glue logic distribution mechanism that takes into account the functional affinity of various glue logic elements, and groups them into new design blocks.\n\nThere is also a need in the relevant art for a glue logic distribution mechanism that returns an optimized amount of glue logic to existing design\n\nIn addition, existing ASIC design methodologies are not suitable for reuse of pre-designed circuit blocks. Available schemes do not provide guidelines to solve the timing, clock, bus, power, block arrangement, verification, and testing problems associated with integrating circuit design blocks within specific device architectures. Since the circuit blocks are from multiple inconsistent sources, the challenge is how to integrate these circuit blocks into a circuit system in a fashion suitable to block-based design.\n\nTherefore, there is a need for a method and apparatus suitable to inter-connect the circuit blocks from multiple inconsistent sources in a fashion suitable to block-based design.\n\nThere is another need for a method and apparatus to provide interfaces for converting the circuit blocks having different interfaces into the ones having standardized interfaces.\n\nOf course, all ICs, even those containing an entire system on a single chip, must pass a series of tests to verify that the chip meets performance requirements and that there are no hidden manufacturing defects. If a manufacturing defect is missed, the faulty chip may not be discovered until after the assembly process or, worse yet, in the field. The cost of such \"test escapes\" in terms of their effect on customer satisfaction can be devastating to a product line.\n\nGenerally, there are three types of tests for detecting defects: DC parametric tests, AC parametric tests, and functional (\"PLL\") tests. In DC parametric tests, the inputs, outputs, input-to-output transmission, total current, and power consumption of the chip are measured. In AC parametric tests, the rising and falling times of the input and output signals, delay time in propagation between input and output terminals, minimum clock pulse width, and operation frequency of the chip are measured. In functional tests, the chip is tested to see if it functions as designed under prescribed operating conditions. Typically, applying a test pattern to an input terminal (\"test vectors\") and comparing an output pattern detected at an output terminal with an expected pattern carries out a functional test.\n\nBefore the advent of Design for Test (\"DFT\") methodologies, designers created and assembled a chip, then passed the completed design to test designers. The test designers then added package-level test logic, and sent the chip to the manufacturer (the \"fab\"). The fab testers then probed the chip and ran a board test protocol including the above-described tests on the package-level logic. The available Scan Design methodology is a simple example of a highly effective and widely used method for applying a \"single\" test method to the entire chip with predictable and consistent test result. Other ad hoc methods may be used to handle nonscannable design styles.\n\nToday, logic previously contained in a whole chip is now used as a single virtual component (VC) or design block to be included in a larger chip. Thus, tests can no longer be designed after circuit design is complete. Designers must plan how to test each design block, as well as the whole packaged chip, throughout the design process. The design process must therefore ensure testability by applying one or more test methods as appropriate.\n\nThe benefits of DFT are well known. DFT logic and test vector verification functions allow shorter, production-ready tests early in a production cycle. Also, DFT scan paths provide access to chip and system states that are otherwise unavailable. A good DFT plan thereby shortens time-to-market and reduces testing cost by easing the front-end design process and the development of manufacturing tests.\n\nThere are therefore four needs presented by the available art. First, a new DFT for BBD must be able to make effective use of the pre-designed test data among other dissimilar test methods, to share limited test access, and to meet the overall SOC level test objectives.\n\nSecond, it must face the emerging difficulties of new defect types and new defect levels due to technology scaling, the new complexities of mixed-signal and mixed technology design, and the increasing I/O count and new packaging techniques.\n\nThird, it must face the difficulties of integrating IP blocks, which inherently lack a unified structural test model. SOC level test access and fault isolation are needed, and the demand for low power design techniques (i.e., latch-based, gated clock, derived clock, pipelines, and low threshold voltage) which are largely unsupported by the currently available DFT methodologies must be addressed.\n\nAnd the new DFT methodology must overcome the time to market pressure with a coherent and consistent test integration model even when faced with limited or inadequate test information.\n\nThe available art requires structural information (i.e., fault models and test models) so that the test data can be partially or fully generated and verified for a set of faults. For example, the Scan Design Methodology is only applicable to synchronous design and detects only single stuck-at-fault models. Moreover, other DFT solutions are scan-based, thus making it rather difficult for sharing and verifying the hard IP test model, which does not contain structural information.\n\nThe available art also requires a non-linear computation model that cannot sustain the current gate count explosion, even if sharing and verifying were possible (i.e., soft IP models). However, soft IPs are not necessarily scannable or mergeable, sometimes resulting in unpredictable and unmanageable test development.\n\nTurning finally to design verification, a challenge presented by the use of multiple pre-designed blocks in SOC design is the need for a reliable and efficient functional verification method. In the available art, test suites are used to verify a multi-block design. Each test in the suite is used to test each of the blocks before they are integrated. Then, after integration of the blocks, significant effort is required to adjust the test suite to enable functional verification at the system level. The process of testing and debugging may need to be repeated for a number of iterations before a final, full system verification can be confidently provided.\n\nOne available approach to this problem is the substitution of implementation modules for their corresponding behavioral models, thereby allowing chip level simulation and testing in a mixed mode situation. While this approach can offer desirable results if performed effectively, and can be less costly than the iterative block-based simulations described above, this approach is still quite expensive and slow, since the entire chip must be simulated to obtain reliable functional verification.\n\nAn especially acute challenge is presented in multi-block designs by the need to functionally verify bus structures. In the available art, bus verification is achieved in either of two ways. The bus may be debugged and verified as an integral part of the overall chip, or it may be verified using bus functional models for the pre-defined blocks, taking into account the detailed implementation provided by newly authored blocks. However, integral bus verification can be slow and costly. The entire chip must be used to verify the bus design, and integral bus verification can only be executed late in the design cycle, when debugging is difficult and time consuming due to the level of detail and the potential for finding no bus-related bugs. The bus functional model approach eases some of these problems, but requires implementation detail for the newly authored blocks. Moreover, the bus functional models may be error prone themselves and may be available only as \"black boxes\", making signal tracing and debug difficult or impossible.\n\nSUMMARY OF THE INVENTION\n\nTo addresses the shortcomings of the available art, the present invention provides a method and apparatus for designing a circuit system, the method, comprising the steps of:\n\n(a) selecting a plurality of pre-designed circuit blocks to be used to design the circuit system;\n\n(b) collecting data reflecting the experience of the designer regarding the pre-designed circuit blocks, the designer's experience being adaptable to a processing method;\n\n(c) accepting or rejecting a design of the circuit system in a manner based on the designer's experience data and acceptable degree of risk;\n\n(d) upon acceptance, forming block specifications containing criteria and modified constraints for each of the circuit blocks (FEA);\n\n(e) upon acceptance, forming block specifications for deploying the circuit blocks on a floor plan of a chip, in compliance with the criteria and modified constraints without changing the selected circuit block and the processing method.\n\nBRIEF DESCRIPTION OF THE DRAWINGS\n\nFIG. 1 is a flowchart illustrating a design process based on the block-based design methodology, in accordance with the present invention;\n\nFIG. 2 is a flowchart illustrating the steps of front-end access, in accordance with the present invention;\n\nFIG. 3 illustrates a clock-planing module, in accordance with the present invention;\n\nFIG. 4 illustrates a bus identification and planing module, in 10 accordance with the present invention;\n\nFIG. 5 illustrates a power-planning module, in accordance with the present invention;\n\nFIG. 6 illustrates the I/O and analog/mixed-signal requirements, in accordance with the present invention;\n\nFIG. 7 illustrates a test-planning module, in accordance with the present invention;\n\nFIG. 8 illustrates a timing and floor-planning module, in accordance with the present invention;\n\nFIG. 9 shows meta flow of a block design, in accordance with the present invention;\n\nFIG. 10 illustrates data flow of a chip assembly, in accordance with the present invention;\n\nFIG. 11 illustrates task flow of a chip assembly, in accordance with the present invention; and\n\nFIGS. 12, 13, 14, and 15 illustrate functional verification flow in accordance with the present invention.\n\nFIG. 16 illustrates a methodology to assess feasibility of a circuit design using a plurality of pre-designed circuit blocks, in accordance with the present invention.\n\nFIG. 17 illustrates a feasibility assessment result using the methodology shown in FIG. 2, in accordance with the present invention.\n\nFIG. 18 shows a methodology to assess feasibility of a circuit design using a plurality of pre-designed circuit blocks, in accordance with the present invention.\n\nFIG. 19 illustrates a feasibility assessment result using the methodology shown in FIG. 18, in accordance with the present invention.\n\nFIG. 20 shows an front-end acceptance (\"FEA\") process, in accordance with the present invention.\n\nFIG. 21 illustrates a refinement process, in accordance with the present invention.\n\nFIG. 22 shows an exemplary estimate correctness curve, in accordance with the present invention.\n\nFIG. 23 shows a process of validating an FEA, in accordance the present invention.\n\nFIG. 24 shows a refined estimate correctness curve using an FEA design-property refinement process, in accordance with the present invention.\n\nFIG. 25 shows an FEA data-extraction process, in accordance with the present invention.\n\nFIG. 26 illustrates a process of identifying the need for block-estimate refinement, in accordance with the present invention.\n\nFIG. 27 shows an FEA assessment-axes metric, in accordance with the present invention.\n\nFIG. 28 shows a classification collapse curve, in accordance with the present invention.\n\nFIG. 29 shows a plurality of design blocks in a circuit design, wherein glue logic interferes with optimal design block placement.\n\nFIG. 30 illustrates a first type of glue logic distribution, in accordance with the present invention.\n\nFIG. 31 illustrates second and third types of glue logic distribution, in accordance with the present invention.\n\nFIG. 32 shows a collaring process of embedding a circuit block into a collar, in accordance with the present invention.\n\nFIG. 33 illustrates creating a complete set of abstracts for a block, to be used in a design in accordance with the present invention;\n\nFIG. 34 is a flowchart illustrating the collaring process, in accordance with the present invention.\n\nFIG. 35 shows a collar having two layers, in accordance with the present invention.\n\nFIG. 36 illustrates the logic view between a collar and a circuit block, in accordance with the present invention;\n\nFIG. 37 illustrates the physical view between a collar and a circuit block, in accordance with the present invention.\n\nFIG. 38 shows a system design without using the collaring process of the present invention.\n\nFIG. 39 shows a system design using the collaring process of the present invention.\n\nFIG. 40 shows a computer system for performing the steps in the collaring process of FIG. 34, in accordance with the present invention.\n\nFIG. 41 illustrates a series of steps comprising the bus identification and planning scheme of the present invention.\n\nFIG. 42 illustrates the internal structure of an interconnection section of a behavioral model constructed according to method of the present invention.\n\nFIGS. 43-47 and 49-56 are tables illustrating improved delay times through bus modifications implemented using the system and method of the present invention.\n\nFIG. 48 illustrates a bus bridge used in the method and system of the present invention.\n\nFIG. 57 illustrates a bus bridge used in the method and system of the present invention.\n\nFIG. 58 illustrates a bus bridge including a FIFO used in the method and system of the present invention.\n\nFIG. 59 is a table illustrating bus utilization and latency characteristics for a variety of bus types.\n\nFIG. 60 illustrates an Exemplary Consistency Check truth table\n\nFIG. 61 illustrates the top-level hierarchy of a chip from the DFT perspective using the method of the present invention.\n\nFIG. 62 illustrates a design made up of functional blocks and socket access ports (\"SAPs\").\n\nFIG. 63 is a table illustrating appropriate test methods for a variety of design architectures.\n\nFIG. 64 is a flowchart illustrating the top-level architecture specification procedure for the method and system of the present invention.\n\nFIG. 65 illustrates a socketization procedure of the method and system of the present invention.\n\nFIG. 66 illustrates a block level test development procedure of the method and system of the present invention.\n\nFIG. 67 illustrates a chip level test development procedure of the method and system of the present invention.\n\nFIG. 68 illustrates a test flow from planning to chip assembly according to the method and system of the present invention.\n\nFIG. 69 illustrates a designer's view of the front-end acceptance verification tools of the present invention.\n\nFIG. 70 illustrates a designer's view of moving from chip planning to block design.\n\nFIG. 71 illustrates a designer's view of the evolving bus block model and test bench generation of the method and system of the present invention.\n\nFIG. 72 illustrates a designer's view of a block test bench and a chip test bench.\n\nFIG. 73 is a designer's view of block and chip logical verification models.\n\nDETAILED DESCRIPTION PREFERRED AND ALTERNATIVE EMBODIMENTS\n\nTo overcome the shortcomings of the available art, the present invention discloses a novel methodology and implementation for block-based design (\"BBD\").\n\nReferring to FIG. 1, a flowchart 100 illustrating a design process based on the block-based design (BBD) methodology in accordance with the present invention is shown. As shown in FIG. 1, the design process includes front-end acceptance design stage 102, chip planning design stage 104, block design stage 106, chip assembly design stage 108, and verification design stage 110.\n\nFront-end acceptance design stage 102 enables a system integrator (chip designer) to evaluate the feasibility of a prospective design project. At front-end acceptance design stage 102, the designer receives a specification from a customer including functional and other requirements (such as delivery time and budget) for designing an ASIC. The customer may also provide some pre-designed circuit blocks and test benches for these circuit blocks. Along with the customer supplied blocks, the designer utilizing front end acceptance design stage 102 may accept, as input, circuit blocks from different sources, some of which may be supplied by a third party, some of which may be legacy circuit blocks, and some of which may be newly authored. These selected circuit blocks can be in a soft, firm, or hard design state. (Note that: soft state is at RTL level; hard is at GDSII level; and firm is between soft and hard, such as at gate level or netlist level). Front-end acceptance design stage 102 then collects the designer's available experiences, including field of use data, estimation data through behavior simulation, and/or partial implementation data. The process of front-end acceptance design stage 102 then provides an assessment to help the designer decide whether to accept the design project based on the design property parameters, including the customer's requirements, the designer's available experience , and the designer's acceptable degree of risk. Furthermore, based on the functional specification, the result of front-end acceptance design stage 102 dictates the final set of pre-designed circuit blocks to be used in the circuit design.\n\nFront-end acceptance design stage 102 provides for three phases of assessment: coarse-grained assessment, medium-grained assessment, and fine-grained assessment. If an assessment at one phase is not satisfactory, front-end acceptance design stage 102 enables refinement of design property parameters and makes a further assessment at the next phase.\n\nIf the proposed design project is found acceptable, front-end acceptance design stage 102 provides comprehensive steps to ensure that problems in the design ahead are detected early, and to ensure that these problems can be solved in a comprehensive manner within the bounds defined by project requirements, the designer's available experience, and the processing method selected. Front-end acceptance design stage 102 generates a design specification defining a processing methodology including selected pre-designed circuit blocks, design criteria, and inter-dependant design constraints.\n\nChip planning design stage 104 translates the design specification from the output of front-end acceptance design stage 102 into block specifications for each of the selected circuit blocks. Tasks executed in chip planning design stage 104 include: (1) developing plans for chip design, assembly, and implementation focused on predictability of delays, routability, area, power dissipation, and timing, and (2) identifying and adjusting constraints. Specifically, based on the design criteria and inter-dependant constraints provided as the output of front-end acceptance design stage 102, chip planning design stage 104 provides chip planning within the bounds (such as requirements and constraints) dictated at front-end acceptance. The inventive chip planning design stage 104 considers one constraint at a time, and yet meets the overall design criteria as specified by front-end acceptance design stage 102. Chip planning design stage 104 achieves this by forming the budget for each of the circuit blocks selected in front-end acceptance design stage 102, revising the specification for the circuit block, and adjusting constraints within the processing method specified by front-end acceptance design stage 102. In contrast to the chip planning design stage of the present invention, existing methodologies either generate new functional blocks or change the processing technology to meet the design criteria, increasing design time and raising project risk. Chip planning design stage 104 also generates specifications for glue logic (i.e. the hardware that is required to interconnect the selected circuit blocks), discussed in further detail below. Chip planning design stage 104 provides as output three types of glue logic, including new glue logic blocks that occupy one or more areas in a chip, distributed glue logic distributed into the selected circuit blocks, and top level block glue logic elements.\n\nTo seamlessly interconnect the selected circuit blocks, if necessary, block design stage 106 embeds an interface (called a collar) around each circuit block to form a standard interface. Since a circuit block can be soft, firm, or hard, each collar may be soft, firm, or hard as well. Block design stage 106 output provides that: (1) all circuit blocks in the chip meet the constraints and budget, and fit into dictated chip design plans and architectures; (2) chip assembly design stage 108 is provided with all required models and views of all circuit blocks; (3) the design is enabled for developing methodologies and flows for authoring the new circuit blocks generated in the chip planning design stage 104, adapting legacy circuit blocks, and adapting third party circuit blocks; and (4) the design fits into given chip architectures and budgets.\n\nChip assembly design stage 108 integrates circuit blocks to tape-out the top-level design for design stage fabrication. Chip assembly design stage 108 includes the final placement of hard blocks and chip bus routing, as well as the completion of any global design details. Chip assembly design stage 108 does not begin until all circuit blocks are designed, modified, and integrated into the chip plan. Inputs for chip assembly design stage 108 include power, area, and timing margin specifications received from the front-end acceptance design stage 102 or chip planning design stage 104.\n\nVerification design stage 110 ensures that the design at each stage meets the customer functional requirements as detailed in the functional specification and chip test bench supplied at front-end acceptance design stage 102. Verification design stage 110 includes functional verification 112, timing verification 114, and physical verification 116.\n\nFunctional verification step 112 ensures that the logic functions and chip test benches for the selected circuit blocks at each stage of the design meet the functional requirements of the customer specification. Functional verification can be performed during front-end acceptance design stage 102, chip planning design stage 104, block design stage 106, or chip assembly design stage 108. Timing verification ensures that signal timing at each stage of the design is appropriate to generate the logic functions and pass the tests specified in the customer's specification. Timing verification can be performed during front-end acceptance design stage 102, chip planning design stage 104, block design stage 106, or chip assembly design stage 108. Physical verification ensures that the physical layout for the circuit design meets the customer specification.\n\nDuring the design process, front-end acceptance design stage 102, chip planning design stage 104, block design stage 106, and chip assembly design stage 108 not only perform their intended functions, but also generate the information needed for functional verification 112, timing verification 114, and physical verification 116 which, together, comprise verification function 110. If any errors occur during verification at a particular stage of the design process, these errors are preferably corrected before going to the next stage.\n\nThus, at chip assembly design stage 108, the design process not only generates a top-level design for fabricating a chip, but also completes verifications of chip test benches for each of the circuit blocks used in the design and the overall chip test bench for the chip.\n\nFIGS. 2-15 will now be described in summary form. Each of these figures provides a high level description of materials discussed in greater detail below.\n\nII. FRONT END ACCEPTANCE 102\n\nReferring to FIG. 2, flowchart 200 illustrates the steps 210-216 of front-end acceptance design stage 102, in accordance with the present invention.\n\nIII. CHIP PLANNING 104\n\nChip planning design stage 104 includes the following modules:\n\n(1) clock planning;\n\n(2) bus identification and planning;\n\n(3) power planning;\n\n(4) I/O and analog/mixed-signal requirements;\n\n(5) test planning;\n\n(6) timing and floor planning; and\n\n(7) bus verification.\n\nReferring to FIG. 3, there is shown the clock-planning module, in accordance with the present invention.\n\nReferring to FIG. 4, there is shown the bus identification and planing module, in accordance with the present invention.\n\nReferring to FIG. 5, there is shown the power-planning module, in accordance with the present invention.\n\nReferring to FIG. 6, there is shown the I/O and analog/mixed-signal requirements, in accordance with the present invention.\n\nReferring to FIG. 7, there is shown the test-planning module, in accordance with the present invention.\n\nReferring to FIG. 8, there is shown the timing and floor-planning module, in accordance with the present invention.\n\nIV. BLOCK PLANNING 106\n\nReferring to FIG. 9, there is shown the flow of the block design stage, in accordance with the present invention.\n\nV. CHIP ASSEMBLY 108\n\nReferring to FIG. 10, there is shown the data flow of the chip assembly design stage, in accordance with the present invention.\n\nReferring to FIG. 11, there is shown the task flow of the chip assembly design stage, in accordance with the present invention.\n\nVI. VERIFICATION 110\n\nReferring to FIGS. 12, 13, 14, and 15, there is shown the functional verification flow for the verification design stage of the present invention.\n\nSCALABLE METHODOLOGY FOR FEASIBILITY ASSESSMENT\n\nTurning first to front-end assessment, FIG. 16 illustrates the inventive methodology to assess feasibility of a circuit design using a plurality of pre-designed circuit blocks, in accordance with the present invention.\n\nIn FIG. 16, the inputs for the methodology are originally designed to use field of use data as inputs. However, in assessing a new design project, new types of inputs 1, 2, and 3 need to be used to assess the feasibility of the new design project. To accommodate the methodology, the new types of inputs are processed so that the methodology can use the new types of inputs to perform feasibility assessment for the new design project.\n\nFIG. 17 shows the feasibility assessment result using the methodology shown in FIG. 16, in accordance with the present invention. FIG. 17 indicates risk on the vertical axis and time/cost along the horizontal axis. According to the risk indicator, the risk of using these three types of new data increases slightly compared with the risk presented when only using the field of use data. Also from FIG. 17, it can be seen that a type 3 input has the greatest impact on risk. However, according to the time/cost indicator, by using these three types of new data, the time/cost increases greatly compared with the risk created by using only field of use data. By considering the ramifications of the inventive risk v. time/cost calculus indicated in FIG. 17, the pre-staged blocks are pre-designed and qualified for proper use in the design methodology. The pre-staged design plan is preferably a section of an existing methodology, for example, a block-authoring piece.\n\nFIG. 18 shows a methodology to assess the feasibility of a circuit design using a plurality of pre-designed circuit blocks, in accordance with 5 the present invention. In FIG. 18, the inputs for the methodology are originally designed to use field of use data as inputs. However, in assessing a new design project, new types of inputs X, Y, Z need to be used to assess the feasibility of the new design project. To accommodate the new input types, the methodology is modified so that the new inputs can be used to perform feasibility assessment for the new design project.\n\nFIG. 19 illustrates the assessed feasibility obtained using the inventive methodology shown in FIG. 18, in accordance with the present invention. FIG. 19 indicates risk along the vertical axis and time/cost along the horizontal axis. According to the risk indicator, the risk provided when using the three new input types increases greatly in comparison with the risk provided when only using field of use data. Also from FIG. 19, we can see that a type Z input has the greatest impact on risk. However, according to the time/cost indicator, the time/cost provided by additionally using these three types of new inputs increases moderately comparing with the time/cost by only using the field of use data.\n\nThe new types of inputs can be estimation data or implementation data for the pre-designed circuits. Based on the results shown in FIGS. 16-19, a system integrator can make tradeoff decisions.\n\nFEASIBILITY ASSESSMENT IN THE FRONT END ACCEPTANCE\n\nThe front-end acceptance (FEA) design stage 102 in FIG. 1 involves feasibility and risk assessment of a proposed design. A design is feasible if the assessed criteria are within allowable risk tolerance.\n\nIn a sense, the FEA is a process of design refinement to a point at which the system integrator can assume the risk of accepting a proposed design. As such, it is the process of reduction of lack-of-knowledge and, therefore, error in the requested design's final outcome. As a starting point, the FEA process receives a set of design requirements delivered by a customer, the integrator's risk profile for accepting a design, a set of pre-designed blocks, and the integrator's previous knowledge of and experience with the pre-designed blocks. The pre-designed blocks can be at various levels of resolution (hard, soft or firm). The resolution, previous experience and understanding of a block give rise to a large range of error-bounds in the prediction of area, power, performance, etc., across the blocks.\n\nFor each of the blocks, the design refinement may be presented in three levels of resolution:\n\n(1) integrator's field of experience (FOE),\n\n(2) estimation using actual models and tools to execute those models, and\n\n(3) dip by taking a block into a higher level of design resolution than that at which it was received.\n\nIt should be noted that three levels of design resolution are arranged in ascending order as: soft, firm, and hard. Efficiency is achieved by providing a mechanism to conduct feasibility assessment without needlessly refining all block and interconnect criteria predictions.\n\nFIG. 20 shows a flow diagram for an FEA process in accordance with the present invention.\n\nIn FIG. 20, the FEA process includes three phases of feasibility assessment, reflecting the three levels of design refinement discussed above. These three phases are: coarse-grained assessment, medium-grained assessment, and fine-grained assessment.\n\nCoarse-grained assessment is a field of experience dominated assessment based upon the design integrator's previous experience with similar designs. Coarse-grained assessment is especially suited to ten's of blocks and system design options, and to situations where design estimation-error tolerance is on the order of fifty percent or more. Coarse analysis can be used to make a cursory examination of blocks being considered, where the estimation of interaction between blocks is non-critical. At this phase, it is most likely that not all blocks being considered are used in the final design.\n\nMedium-grained assessment is an estimation-dominated assessment, to estimate by analytic formulation of behavior through equation or simulation. It is suitable for from two to ten system design options, and to a situation where acceptable design estimation-error tolerance is on the order of 20%, and the integrator has an understanding of how the blocks interact. It can be used to examine the interaction between blocks critical to operational sufficiency of the design. In this phase, all blocks in consideration have a high probability of being used in the final design.\n\nMost refined (fine-grained) assessment is a design-dip-dominated assessment to make measurements from a refinement of block design. Dipping is a process in which a new block is transformed into a soft block, a pre-designed soft block into a firm block, and a pre-defined firm block into a hard block. Results are generated from either simulation, emulation or prototyping. Fine-grained assessment is suitable to all or part of a single-option chip design where acceptable design estimation-error tolerance is less than 5%, such as during final resolution of critical issues for which existing design refinement is insufficient. It can be used to examine a subset of chip behaviors or block-interactions which need to be studied in detail to guarantee sufficiency or to guarantee that resolution provided by any existing simulation model for the block is sufficient. It can also be used to examine the failure of the block to meet design requirements, which will strongly impact final design feasibility. In this phase, not every block in consideration will be dipped; instead, substantially only those blocks that have critical impact on the FEA decision process are dipped.\n\nIn FIG. 20, the width of each triangle represents the error in prediction of the system FEA criteria. At each level of the assessment, the key is to refine as little as possible the FEA criteria while reducing the designer's error so that an FEA decision can be made quickly. At each phase of the FEA process, the basic intent and strategy is the same, as listed below:\n\n(1) Gather available information about the blocks under consideration;\n\n(2) Identify and refine locally those blocks most likely to impact system-estimate error;\n\n(3) Assess whether the design meets the FEA constraints. If so, stop the FEA process; and if not,\n\n(4) Refine globally the block-estimates in the system if FEA constraints are not met.\n\nA key part of the FEA process illustrated in FIG. 20 is how to calculate the acceptable global error (or overall error) in the prediction of system criteria, and identify which few blocks require estimate refinement to bring the global error to within acceptable bounds. This calculation process requires three parameters:\n\n(1) Estimate of the acceptable global error for making a decision;\n\n(2) Estimate of the global error which will result from current system analysis; and\n\n(3) The sensitivity of the global error to the error in estimating a particular block in the design (also referred to as the block-error impact).\n\nThe first parameter is defined by the risk-profile of the system integrator, the constraints supplied by the customer, and a good prediction of the global error, which will result from basing a system prediction upon the current state of data. The second and third parameters are all derived from building accurate Error Impact Curves. Referring to FIG. 21, there is illustrated the driving of the refinement process, given the error impact curves, in accordance with the present invention.\n\nTo further define the FEA process, the present invention uses four basic assessment techniques:\n\n1. FEA Decision Process: Defining Data-In, Data-Out and the Decision Process based upon Data-Out. (i.e., How is Data-Out related to the assessment of acceptable risk?);\n\n2. FEA Data Extraction Process: Moving from a complete set of Data-In for the abstraction level being considered to the generation of Data-Out;\n\n3. FEA Block-Refinement Identification: Defining a common mechanism for establishing the System-Estimation Impact, given the Estimation-Error and Block Criticality within a system design. (i.e., Highest potential impact blocks are refined further if the acceptance criteria for the Decision Process are not met); and\n\n4. FEA Assessment-Axes Metrics: Defining the actual metrics to be used for each of the axes-of-acceptance associated with FEA. (i.e., defining how the criticality of a block within a system is defined).\n\nIn the method and system of the present invention, a set of estimate correctness curves are used to validate the FEA process. Each of the estimate correctness curves is presented over an FEA axes, which visually provides the elements and criteria for validating the FEA process. To better explain the function of an estimate correctness curve, the following elements and criteria are defined. Collectively, these elements and criteria are referred to as the FEA Axes of Acceptance. These definitions apply to both blocks and the overall system.\n\nPower\n\nper mode of operation (e.g., mW)\n\nPerformance\n\nintra-cycle delay (e.g., ps/ns/us)\n\nlatency (e.g., ns/us/ms)\n\nthroughput (objects/second--e.g., 50 kB/sec)\n\nArea\n\narea including: gates, routing, perimeters, unused white-space (e.g., mils)\n\nCost\n\nNon-recurrent engineering cost (e.g., U.S. $)\n\nCost per Unit (e.g., U.S. $)\n\nSchedule\n\nResource allocation (e.g., man-years)\n\nDeliverable timelines (time)\n\nRisk\n\nPossibility of error (%)\n\nImpact of errors (U.S. $, and/or time)\n\nBefore conducting the FEA process, the customer provides the system integrator with as much of the following information as possible:\n\n(1) A set of circuit blocks which are either in soft, firm, or hard format;\n\n(2) A set of simulators (estimators) or previous-experience estimates for the blocks, along with error-tolerances for the estimates;\n\n(3) A set of specifications describing the overall chip functionality and performance requirements; and\n\n(4) A set of stipulations regarding acceptable schedule, cost, and risk for the project.\n\nThe customer may also provide:\n\n(5) Behavioral definitions for any new blocks to be incorporated into the chip; and\n\n(6) Identification of known critical issues.\n\nBefore conducting the FEA process, the system integrator should:\n\n(1) Determine a risk profile by which design suitability is assessed, including:\n\na. Guard-Bands--The integrator's over-design margin for each of the FEA axes;\n\nb. Acceptance Risk--Certainty that design will satisfy requirements prior to accepting a customer request. This is simply expressed as a standard-deviation measure--the Aσ design-acceptance risk; and\n\nc. Rejection Risk--Certainty that specified design is unable to be assembled and fabricated with available blocks. Note that rejection is actually a risky behavior for the system integrator: the risk being taken is that the rejected design was actually feasible even though initial assessment made it appear doubtful. This is also expressed as a standard-deviation measure--the Rσ design-rejection risk.\n\n(2) Verify that the submitted blocks, in combination with any new or third party blocks, are sufficient to meet the project constraints within acceptable limits of risk.\n\nReferring to FIG. 22, an exemplary correctness curve estimate is shown, in accordance with the present invention. The horizontal axis is an FEA axis, which can represent any customer constraints or the overall constraint for the system. To facilitate explanation, assume that the FEA axis represents power. The vertical axis represents estimate correctness. According to FIG. 22, the guardband of the power constraint is between the constraint initially specified by the customer and the constraint modified by the FEA process. Note that, in the example given, the design is rejected because the power constraint modified by the guardband lies within the rejection region. This is true even though the power constraint initially specified is not in the rejection region.\n\nIf the modified power constraint had been between the Aσ and Rσ markers, the FEA refinement process would have proceeded. This process would continue to reduce the expected error variance (i.e., the power-error variance, in this example) until an accept or reject decision can be made based on a refined estimate correctness curve.\n\nReferring to FIG. 23, a process to validate an FEA is shown, in accordance with the present invention. The inventive FEA validation process includes four phases:\n\n0. Pre-FOE Phase (not shown):\n\nObtain the customer design constraints for each of the FEA axes of acceptance. Modify each of these constraints by the required guard-band. These modified customer constraints are used only for verification of the FEA process, and are referred to simply as the design constraints.\n\n1. FOE Dominant Phase:\n\nThe system integrator commences FEA by combining together the FOE estimates and estimate-error tolerances to determine whether the required constraints are guaranteed (confidence is higher than defined by: Aσ for a pass, or Rσ for a fail) to be met.\n\n(a) If, despite consideration of third party blocks, constraints are still violated, then the design is not possible. The system integrator must return to the customer with a set of options and the constraints met by these configurations.\n\n(b) If the constraints are met to within acceptable risk, the FEA process is complete.\n\n(c) If there exists less-than-acceptable confidence of predicting the passing or failure of the design, then the estimation phase must commence. To enter the estimation phase, the set of \"most-likely-to-pass\" design configurations (i.e., best) must be selected.\n\n2. Estimation Dominant Phase:\n\nFor the set of best designs derived from the FOE stage, an identification of criticality must be made; i.e., given the error tolerances on each of the blocks involved, which are statistically the most likely to validate that the design has passed constraint validation. This will be a product of both the size of the variance of the FOE specification prediction for a block, and the impact that block has upon the design constraint in question. Estimation should proceed by stubbing-out as much of the non-critical design as possible, and generating design specific estimates for that which remains.\n\n(a) Violation: Similar to procedure 1(a) discussed above.\n\n(b) Satisfaction:: If the level of indeterminacy is unlikely to be reduced further by increasing the accuracy of estimation (reducing the amount of stubbing will not improve the estimate in any statistically significant way, due to the fact that the error-tolerance is dominated by blocks already included in the estimation), or a full estimate of the SOC design has been built given existing block models, then the best design must pass onto the dipping phase.\n\n3. Design-Dip Dominant Phase:\n\nRefine the block estimate to which the global error is most sensitive, then proceed as per the estimation phase. Continue iterating this process until the FEA is confirmed or denied. The definition of statistical criticality is similar.\n\nReferring to FIG. 24, a refined estimate correctness curve using the inventive FEA design-property refinement process of the present invention is shown. Through the refinement process of moving from FEA phases 0 to 3, discussed above, the expected error variance on the refined estimate correctness curve is greatly reduced compared with that of the estimate correctness curve shown in FIG. 22. Thus, a decision to accept or reject may be made based on a refined estimate correctness curve, as shown in FIG. 24, whereas such a decision may or may not be made based on the estimate correctness curve shown in FIG. 22.\n\nIf an FEA decision cannot be made based on the available information and data at one phase of validation, the present invention performs a design-property refinement process to reduce the expected error variance. Based on the refined data and information, the present invention performs the FEA validation at the next phase. The design-property refinement process comprises the following three aspects:\n\n(1) FEA Data-Extraction Process;\n\n(2) FEA Block-Refinement Identification; and\n\n(3) FEA assessment-Axes Metrics.\n\nReferring to FIG. 25, the FEA Data-Extraction Process is shown, in accordance with the present invention. There is a standardized mechanism, or process, for establishing an \"Estimation of System Impact\" for prediction error associated with each block in a system design. This mechanism, referred to as Block-Refinement Identification, enables the required error-boundary on properties (the FEA Design Criteria--e.g., power, area, performance, etc.) of any specific block to be determined for each refinement phase of FEA system-design assessment.\n\nLet L(β) be the limit specified by the customer, as modified by any required Design Margin, for the design to satisfy FEA Criteria β. Let the expected value of the design as measured against FEA Criteria β be E(β). The Design Decision Constraint, or the \"maximum error tolerable\", for the design to be defined as pass/fail relative to the FEA Criteria β is given by: DDC(β)=│L(β)-E(β)│. For an expected \"Pass\", E(β) itself must lie within the acceptance region for the FEA Criteria, and for an expected \"Fail\" E(β) must lie within the rejection region. Effectively, in the first case for a βPass\" we require: Aσsystem <DDC, and in the second case for a \"Fail\": R{character pullout}system<DDC. If the inequalities are unsatisfied, then the system analysis does not produce a decision-quality result.\n\nIt should be noted that, in general, the average estimate E(β) is the final estimate of system-criteria β as produced by the previous phase of system-assessment. i.e., The Medium Grain Assessment stage takes as the average the final estimate of the Coarse Assessment Stage, the Fine Grain Assessment Stage takes as the average the final estimate of the Medium Grain Assessment Stage. To initiate the process, the Coarse Assessment Stage must be entered by first establishing a coarse-level expected-value estimate for each of the FEA Criteria.\n\nFor the system to be assessed relative to the Design Decision Constraint (DDC) for a particular FEA Criteria β, a relationship must be established between the errors associated with block estimates and the total estimate error for the system. Note that the error associated with a block estimate is not just the inherent error of estimating the β-criteria for the block, but also the specific influence of that block and block-error upon the difficulty of estimating integration cost. The error in estimating the block is consequently scaled by a system-criticality measure, C, which is a measure of the difficulty in integrating the block based upon its properties or lack-or-definition (error) for FEA Criteria β. The determination as to the Pass (Fail) of the system is established through the relation of the set of {Cblock.σblock│block ε system} to σsystem and the required inequalities: Aσsystem <DDC (Rσsystem <DDC) for each of the FEA Criteria.\n\nIt should also be noted that to keep the inclusion of the criticality measures Cblock neutral relative the system inequalities expressed above (i.e, σsystem is formulated from an expression which combines the criticality scaled block errors: Cblock.σblock), the criticality measures are normalized such that: Σblocks (Cblock)2 =1. The process for assessing this varies slightly depending upon the class of system-property being assessed. From the perspective of FEA, there are three classes of system-properties each described below:\n\nAbsolute (Block) Constraints (e.g., Intra-Cycle Delay, Throughput)\n\nRelative (Block) Constraints (e.g., Power, Area, Latency, Cost, Schedule)\n\nMixed (Block) Constraints (e.g., Quality)\n\nFor simplicity, for an FEA Criteria β define BDC as the Block Design Constraint where: BDCblock =A.Cblock.σblock in the case of test for design acceptance, and BDCblock =R.Cblock.σblock in the case of test for design rejection. Then, for each FEA Criteria:\n\na. Absolute Constraint: To achieve a decision-quality result each block, or each block immersed in its immediate environment (e.g., including routing load, etc.), must pass the DDC for the Absolute Constraint . Mathematically, achievement of a decision-quality result on an Absolute Constraint implies:\n\nFor all blocks ε in the system, BDCblock <DDC\n\nb. Relative Constraints: A decision quality result is achieved if the square summation of block-design constraints throughout the system is less than the square of the DDC. The term relative is used as the acceptable error of assessment for this constraint has the flexibility of being partitioned amongst the blocks, which make up the entire system. Note that some assessment criteria of the Relative type may have multiple constraints. An example of this is Latency, as there may be several critical paths, which contribute to a valid assessment of the complete system. Mathematically, achievement of a decision-quality result on a Relative Constraint implies Σblocks (BDCblock)2 <DDC2, assuming that all block-errors are Gaussian-distributed, independent random-variables.\n\nc. Mixed Constraints: A mixed constraint is a type that involves both the relative and absolute types of constraint. For example Quality is a mixed constraint. No block within a design can exceed a specified bound on its measure of quality, but the summation of all quality assessment across the system must also fall to within a specified range. In this case there is both a DDCblock for the blocks, as well as a DDCsystem for the overall system. Mathematically, for a mixed-constraint system-property two criteria need to be satisfied:\n\n(i) For All: block ε system, BDCblock <DDCblock\n\n(ii) εblocks (BDCblock)2 <(DDCsystem)2\n\nReferring to FIG. 26, there is shown a process of identifying the need for block-estimate refinement, in accordance with the present invention. As shown, there are three steps in FEA Block-Refinement Identification, including:\n\n1: For each FEA assessment criteria of the Absolute or Mixed Constraint type, the level of work required to achieve the absolute error tolerances (CIC's) is determined. As a by-product of refining a model to satisfy the need of Absolute Constraints, some error-bounds associated with Relative Constraints may also be reduced.\n\n2: Based upon the error predicted after the models are refined to satisfy the Absolute Constraints, and Absolute part of the Mixed Constraint Type, the remaining system-error tolerance (CIC) for the system are determined and partitioned amongst the separate IP blocks. The partitioning will be defined in such a way as to minimize the work required to build an estimate. The flexibility of this partitioning is moderated by the defined criticality of contribution for each of the blocks within the assembled system. This defines the notion of error impact. Note that this problem must simultaneously optimize necessary work against acceptable error-tolerance along each FEA axis.\n\n3: If at any stage system suitability cannot be determined using the proposed CIC's, these need to be tightened further and the process re-iterated either:\n\n(a) for the block, if a specific absolute constraint is insufficient, or\n\n(b) for the system, if a relative constraint for the chip is insufficient.\n\nReferring to FIG. 27, there is shown an FEA Assessment-Axes Metric, containing a table defining the concept of Assessment-Axis Criticality (AAC), in accordance with the present invention and including, where appropriate, exemplary criticality measures. The AAC relates to Expected System-impact (ESI) through Expected Estimation Error (EEE) based upon the following relation: ESI=AAC*EEE.\n\nAs shown in FIG. 27, the table contains five columns, as the following:\n\n(1) Assessment Axis FEA is measured based upon these criteria\n\n(2) Constraint Type Each FEA Assessment Axis may have one or multiple constraint-types associated with it\n\n(3) Constraint Class Class as defined above\n\n(4) Routing Refinement Type of routing-refinement necessary to ensure that the impact of chip routing is of the same degree of error as the specified block and system constraints\n\n(5) Criticality Measure Standardized way of measuring the criticality of a property associated with an FEA Assessment Axis\n\nSome elements of the table make reference to Routing Criticality. Routing Criticality is defined for any output pin of a block or chip input pad as Pin Routing Criticality=(Expected Net Length)*(Capacitance/Unit Length). Block Routing Criticality is the sum of Pin Routing Criticality across the output pins of a block.\n\nThe symbol: α denotes an effective-routing-area scalar whereby: α*(Routing Criticality) translates units and the scale of Routing Criticality into an area-applicable number.\n\nPower consumed as a consequence of routing requires an estimate of activity on the lines. This can be done at a block or pin level of resolution. When applied to the block, the activity estimate is derived from the average activity on the output lines of the block, denoted: Eblock.\n\nA point connection counts as any fanout point unless several fanout points are connected by use of a shared bus. A shared bus counts as a single distinct block. Routing criticality is a measure of the expected difficulty in routing connections to a pin and, therefore, it is a measure of FEA uncertainty.\n\nNote that many of the assessment axes might be identified as mixed constraints at some level of resolution; e.g., an area may be defined as mixed after initial floor plan is defined and used to partition the SOC design chip-level constraints into block-level constraints. However, the dominant constraint type used during the rapid FEA period is listed.\n\nThe term Error used in the table refers to the bound on error as relates to the property in question.\n\nOrganizing the Field of Experience Data\n\nDesigner experience is a crucial part in the system-decision process of the BBD methodology. The BBD methodology extends the concept of experience associated with a single key designer or architect to the concept of \"company design experience\". This general \"pool\" of experience is referred to as the BBD Field of Experience (FOE) of the present invention.\n\nIt is the purpose of BBD method to propose four concepts and mechanisms for the building and use of FOE. These concepts are:\n\na) Data Gathering--Definition of rigorous processes for obtaining and initiating FOE data.\n\nb) Data Classification--Information classification and mechanisms for developing relevant classifications. Such classification guarantees that gathered data may be statistically analyzed, extrapolated, and globally refined as the amount of accumulated design-knowledge increases.\n\nc) Data Certification--Definition of a process that builds the correct assurance of \"trust\" in what might otherwise be referred to as \"rule-of thumb\" numbers. Certifying FOE data will guarantee that estimates built from the FOE database are statistically well bounded.\n\nd) Data Application--The mechanism for application of the FOE to the design process. This is a part of Front End Acceptance for BBD.\n\nField of Experience Definition\n\nIn BBD, Field of Experience can be defined as compiled data from measurement of prior designs classified according to design styles, design purpose, and critical measurements of design characteristics. Critical characteristics may include: area, throughput, power and latency. The definition of Experience-Based Estimation is systematic prediction based upon experience with similar designs or design behaviors. It follows that the definition of FOE Estimation is Experience-Based Estimation using FOE data.\n\nIt should be noted that this is distinct from BBD Estimation in that it does not imply the specific analysis of the design in question, or--where the hardware design is actually known from previous exposure--specific analysis of a new behavior requested of that hardware. For example, a DSP core may have been developed within a company and an FIR-Filter embedded routine run upon it in a previous instantiation of the core. It may then be requested that feasibility of an FFT algorithm running on that same core be considered. If that first rule-of-thumb is based solely upon the previous algorithmic efficiency observed when executing the FIR operation upon the design, but without entering into the details highly specific to the FFT algorithm, then this is an FOE estimate.\n\nField of Experience must explicitly draw upon information derived during a set of previous design projects. FOE data must be able to be catalogued, stored and accessed through a standard database.\n\nThere are three different classes of experience-based data used in design, each form of data being associated with a specific error profile:\n\na) Project Data--Designer-requested estimate at project time. The designer does not draw upon the experience of others as logged in the FOE database, but more upon his own uncatalogued design experience. Error in the design estimate is given by a Designer-Error Variance, which has been observed for general designs. Designer-Error Variance is built from measuring a general history of designers' ability to accurately predict results.\n\nb) Predicted Data--Within a design classification but without a specific project in mind, a designer is requested to give his best-guess parameter-relationships for extending existing FOE data. In this case, the FOE data being extended may consist of as little as a single design-point. Error for this is in part specified by the designer's best guess at the parameterization error, but also modified by the history of designers' ability to accurately predict results. Assuming statistical independence, these error variances would be summed.\n\nc) Collated Data--Collected, classified and parameterized data from a set of design experiences. There is a possibility of measurement error directly associated with this data, but this is likely to be minor. The main error is defined as the difference between measured results and those predicted by the variation of data-parameters.\n\nNote the Project Data is not a form of FOE data as it provides no mechanism to extend the current estimates to future designs. Furthermore, as Project Data is gathered at the commencement of a project, not the completion, it is not verifiable against catalogued design experience. This implies that it is not certified. Any data gathered from Final Measurement of the design may be entered into the FOE database, and the accuracy of the Project Data versus Final Measurement be used to refine Designer Error Variance for the company.\n\nPredicted Data are referred to as FOE seed-data. Predicted Data may be immediately applied to FOE estimation on like designs.\n\nA common classification of the types of data received must apply to both of the above sources of FOE data. Such common classification permits the quick identification and cataloging of received data. Initial classification-specification is regarded as the planning stage for FOE, and the entering/gathering of data is the building stage. As the amount of information in the FOE database grows, the refinement process is applied to reduce error tolerances to within those being observed statistically. In parallel with all three of these stages is the FOE certification process.\n\nThe parameters listed above are used to extrapolate from existing, general FOE data to derive project-specific FOE estimates. Such a relationship between extrapolated estimates and FOE data is preferably defined for each design classification. Each parameter FOE relationship may be defined by a designer's personal experience (see Predicted Data above), or may be empirically specified through curve-fitting the FOE data if sufficient information is available. Parameters might include such technical variables as pipeline depth, degree of parallelism, bit-width, and clocking-speed.\n\nIt should be noted that FOE applies not only to design blocks, but also to the interconnect between the blocks. In such cases, FOE may be specified as the cost of routing between blocks of one classification and blocks of another. Like the application to blocks, FOE estimates for interconnect may also be parameterized.\n\nEstimating with Maximum Accuracy\n\nA key aspect of FOE is the generation of estimates of maximum accuracy given the data provided. This is a twofold process:\n\na) Refinement--As mentioned above, refinement is the process of reducing the error-of-estimate to within that being observed statistically. That is, when the amount of FOE data in a specific category is small, the error tolerance for the data is large. This is not due to an inherent error, but rather to the unknown (or untested) applicability of the parameterized data to other specific designs. As the number of examined designs increases, the statistical spread of data can be measured directly against parameterized predictions. When a large number of cases are catalogued for a specific classification of design, then the accuracy of the parameterization method will be well established. Identification of large correlated error (as opposed to random spread of data) could motivate the re-thinking of the parameter relationships.\n\nb) Classification Collapse--The different classifications of designs may be related by proximity to one another. For example, the Butterfly FFT implementation may be one classification of design, but all FFT blocks may be regarded as closely proximal to this design. If the number of data associated with a particular classification of interest is too small to be statistically significant, then close proximity FOE data may be collapsed together to reduce the overall estimation error. The collapsing of classifications together will itself induce an error due to the slight difference in design types, but the statistical improvement in terms of number of designs considered may overwhelm this difference-error. It is preferable to compute a curve such as that shown in FIG. 28, and from that pick the configuration of best error.\n\nThe process/use model for FOE is therefore as follows:\n\nI. Choose Block Classifications applicable to block being assessed\n\nII. Does enough data exist for that classification? (i.e., is the Expected Error sufficient?)\n\nYes--Return the best FOE estimate and END\n\nNo--Proceed\n\nIII. Collapse categories of close proximity until estimate error ceases to improve\n\nIV. Is the Expected Error sufficient for FOE estimation?\n\nYes--Return the best FOE estimate and END\n\nNo--Proceed\n\nV. Ask the designer to generate his best guess for the design. (This may be a dip into the Estimation Phase of BBD.)\n\nFOE Certifying\n\nCertification of FOE is the process by which the FOE information gathered is shown to be reliable. This certification process will establish the error of estimation during the Building and Refinement stages.\n\nThere are two aspects of certification:\n\na) Certification of Completeness--all FEA metrics must be measurable through the parameterization schemes provided.\n\nb) Certification of Accuracy--including experience measures for designer, and the definition of process to ensure accuracy of collected data.\n\nGlue Logic\n\nThe present invention further discloses an improved glue logic distribution and reduction methodology. The combination of three alternative glue logic distribution mechanisms comprises a preferred embodiment of the present invention. First, glue logic that is not incorporated into predesigned blocks can be duplicated into multiple copies for distribution to the existing blocks. Second, logic that has no affinity to a block at the top level can be left as small blocks, optimally placed to minimize effective gate monopolization, wiring congestion, and floorplanning impact. Third, where the number of blocks exceeds the block place and route limitations, glue logic may be clustered into glue cluster blocks until the block count is reduced to an acceptable level.\n\nReferring to FIG. 29, there is illustrated a circuit design view wherein glue logic 2910 resides disadvantageously between interconnected blocks, thereby rendering inefficient the use of significant areas of silicon real estate and creating significant wiring congestion.\n\nReferring to FIG. 30, we will begin with a description of the present method for creating multiple copies of glue logic for distribution to larger top-level blocks. If an element 3010 has output nets driving multiple loads, the element is split into multiple elements 3012, each having only a single load on the output. In turn, each input \"cone\" (not shown) driving the duplicated element is copied as well, until all block outputs are reached. Similarly, large input gates are reduced to trees of non-inverting two-input gates, with a two-input gate of the original function at the top of the tree. In this way, substantially more logic is dedicated to the previously much smaller glue logic function. However, by removing glue logic from the areas between the larger blocks, the larger blocks can be more efficiently placed, resulting in a net efficiency increase.\n\nAny glue logic element that cannot be effectively duplicated for distribution is then preferably merged into a larger block having the closest affinity to the placed element. Glue logic merger is executed in a manner based on a number of criteria, the most significant of which is whether the merger reduces the number of top-level pin-outs. Thus, when multiple copies are created, since most of the resulting logic is comprised of two-input gates, merging such gates into blocks wherein one pin is connected to the block reduces the pin count by two. When two or more blocks are equal candidates for merger, the block having the lowest pin density is preferably chosen. Finally, the lowest priority preferably goes to timing considerations.\n\nNext, referring to FIG. 31, gates and small blocks 3110 that cannot be merged are clustered into clusters 3112. Gates that cannot be merged most likely have multiple loads on both their input and output nets. By recombining gates with inputs having similar function, gate count can be reduced.\n\nThe present invention further discloses a method to convert pre-designed circuit blocks into circuits having standardized interfaces.\n\nThe tasks performed in the block design stage 106 in FIG. 1 include: (1) creating any missing abstracts for the selected circuit blocks, (2) embedding the circuit blocks into their respective standardized interfaces known as collars, and (3) creating a complete set of abstracts for the collared circuit blocks.\n\nReferring to FIG. 32, a collaring process of embedding a circuit block into a collar is shown, in accordance with the present invention.\n\nIn the BBD methodology, selected circuit blocks are the primary input components at the chip-level. The collaring process places a collar around each of the circuit blocks to create a standard interface around the boundary of the circuit block. To successfully integrate collared blocks into the chip-level, a complete set of abstracts has to be created for the collared blocks. Before creating the complete set of abstracts for the collared blocks, the system of the present invention first forms any missing abstracts for the selected blocks, where abstracts are models or views of the block, or collared block designs required by chip-level assembly or planning tools. Exemplary abstracts include\n\n(1) Static Timing Abstraction--TLF\n\n(2) Layout Blockage File--LEF\n\n(3) Models for Verification--Bolted-Bus-Block model\n\n(4) Block layout constraints to the system\n\nReferring to FIG. 33, creating a complete set of abstracts of a circuit block is illustrated, in accordance with the present invention, while FIG. 34 illustrates a combination of the features illustrated in FIGS. 32 and 33.\n\nWe will move next to a description of the collaring process, wherein it is assumed that a standard interface has been defined for each type of the blocks to be used in design.\n\nAt a first step, the process checks whether each of the blocks has a completed block abstraction. If any of the blocks does not have a complete block abstraction, the process forms a complete block abstraction for the block.\n\nNext, the process identifies a block type for each of the blocks. Specifically, a block can be: a memory type, a processor type, a power type, or an analog/mixed signal type. However, a type of circuit blocks from different sources may have different interfaces that require different designs to connect other circuit blocks. For example, the processors designed by different vendors may have different interfaces and bus structure.\n\nNext, the process associates the identified block with its respective interface standard.\n\nThereafter, the process creates a first collar portion containing the components connectable to the specific interface of the identified block.\n\nAt a next step, the process creates a second collar portion in compliance with the standard interface associated with the identified circuit block.\n\nThe process then creates a third collar portion containing the components for converting the specific interface into a format connectable to the standard interface and connecting the first collar portion with the second collar portion.\n\nA block collar can be comprised of multiple layers. Currently, two collar layers (a block standard collar and a system-specific collar) have been defined for BBD and SOC, respectively. Referring to FIG. 35, a collar containing two layers is shown, one collar being standard for a particular block, and the other being specific to the particular system in which the block is to be deployed. The block standard collar contains those interface components that can be defined without the knowledge of the specific system or the specific context in which it is being integrated. For example, in the context of BBD, a particular design group may decide that a JTAG-standard test interface is required in a design. Thus, for all blocks to be used in any of the systems being designed, a JTAG test interface is a standard and, thus, belongs in the block standard collar. The system-specific collar (or adaptation collar) contains interface components which belongs to the block, but are system or context specific. For example, the standard set for data lines may not require a parity bit, but for a particular system being designed a parity bit is required on all data lines. The logic to generate the parity bit is associated with the block during chip planning and should reside in the system-specific collar.\n\nAnother distinction between the two collar layers in BBD is that the block standard collar can be put on prior to front end acceptance and chip planning (chip planning may require that an initial collar is designed as part of a dipping process to better perform the chip planning functions required), but the system-specific collar can only be added after chip planning.\n\nA more subtle difference between the two collar types is that the standards set for the block standard collar may be much narrower in scope than the standards set in SOC. For example, a certain power interface can be a standard for BBD, but only for a particular company, and the other companies do not need to conform to that standard power interface for the block. Consequently, the blocks from outside of the company need a system-specific collar, which converts the standard power interface to the company one. This is contrasted with SOC, where an industry-wide power interface standard exists and resides in the block standard collar. The ultimate goal in SOC is to create a standard collar that is an industry-wide standard. A block that has such a collar can be called a socketized block. In the future, if all the aspects of the collar are industry-wide, there will be no need for an additional layering of system-specific collar, thus bringing the block closer to the ideal of plug-and-play.\n\nAnother dimension to the system-specific collar is that, although it is intended to be designed after chip planning, one can speed up the chip integration process by making a system-specific collar in chip planning, wherein the parameters for capturing the ranges that the system-specific collar will have to be targeted. This speeds up the integration process since, after chip planning, only the parameters need to be varied while the system-specific collar does not have to be re-designed from scratch.\n\nThe collars and blocks can be in various combinations of soft, firm, and hard. Just as there are advantages and disadvantages as to the hardness of a block, there are advantages and disadvantages to combinations of softness, firmness, and hardness of the collars. For example, if the block itself is soft, it may be suitable to leave the block standard collar soft so that when the system-specific collar is added, the entire block can be synthesized, placed and routed flat for the final conversion to layout. Whereas if a block is hard, it may be suitable to use a hard block standard collar to handle predominately physical interface issues with only a small amount of standard functional changes, since a soft system-specific collar to handle the system-specific issues mostly involves functional changes.\n\nA collar transforms a block-specific interface into a standard interface in the following ways:\n\n(1) transforming the physical configurations specific to the block into standard physical configurations, including pin layer, pin location, and pin separation;\n\n(2) transforming the power supply specific to the block into a standard power supply, including power loading and power physical location;\n\n(3) transforming the test process specific to the block into a standard test process, including test access port (TAP) controller and test protocol;\n\n(4) transforming the timing specific to the block into a standard timing, including setup and hold time, flip-flop, or latch;\n\n(5) transforming the clock ports specific to the block into standard clock ports, including the loading of each of the clock ports;\n\n(6) transforming data/control signals specific to the block into standard data/control signals, including standardizing signal positive/negative assertion; and\n\n(7) transforming the bus interface specific to the block into a standard bus interface, by adding registers for blocks expecting valid input on all cycles, big-endian or little-endian (a big-endian has the 0 bit on the left end of the data unit; a little-endian's is on the right), and converting bit width.\n\nIn addition, a collar may contain components (glue logic, as described above) for performing extra functions for a collared block. Glue can exist in three levels: (1) the glue deployed into a collar, (2) the glue combined at chip-level, and (3) the glue deployed in one or more miniblocks at chip-level. Specifically, glue logic can include anything from simple functional translators (e.g., NAND gates along each of the bit lines) to more complicated functions (e.g., registers, accumulators, etc.). Although glue logic can be of arbitrary size, if the glue size becomes significant relative to the block, estimates made during front-end assembly and chip planning may become inaccurate because glue size was not considered. A constraint may need to put on the relative size of the glue to the block.\n\nA set of assumptions are used in the collaring process, as follows:\n\n(1) The decision of whether or not to add glue logic is made in chip planning;\n\n(2) Of the three types of glue logic (glue put into collars; combination glue at chip level; glue put in mini-blocks at chip level), the collaring process preferably only addresses glue put into collars;\n\n(3) Aspect ratio issues are handled during synthesis (not in block collaring); and\n\n(4) For BBD, the output of a collared block is layout.\n\nReferring to FIG. 36, a logic view between a collar 602 and a block 604 is shown, illustrating some exemplary functions of a collar discussed above in accordance with the present invention.\n\nAs shown in FIG. 36, the collar 602 includes three portions performing three different functions. The first portion contains components that is connectable to the specific interface around the boundary of the block 604. The second portion contains the input output components in compliance with a standard, and the third portion contains components to convert the outputs from block 604 into the standard.\n\nSpecifically, in collar 602, the bus interface 606 combines two one-directional buses 608 and 610 into a bidirectional bus 612. Test Access Port 614 is connected to input 616 to collect the information from and perform testing on block 604. The gate 618 inverts the incoming signal to a format suitable for block 604, as received by gates 619, and gates 620-624 perform clock buffering.\n\nReferring to FIG. 37, a physical view between a collar 702 and a block 704 is shown, illustrating some exemplary functions of a collar discussed above in accordance with the present invention. In FIG. 37, collar 702 and block 704 both contain multiple metal layers. A power standard exists for deploying the Vdd voltage on metal layer 3 (M3) and GND on metal layer 4 (M4). If block 704 does not comply with the power standard, collar 702 converts the power to comply. The region 706 sets a pin spacing/layer standard. If block 704 does not comply with the pin spacing/layer standard, collar 702 converts it to comply with the pin spacing/layer standard. Collar 702 also contains glue 708 in a hard state.\n\nReferring next to FIG. 39, a system design 800 is shown without using the collaring process of the present invention. As shown in FIG. 38, the system design 800 is composed of four circuit blocks A, B, C, and D. Each arrow line connected to a block represents a constraint to design an interface for that block. Thus, if a system is composed of n circuit blocks (n=4 in this example), the interface for any particular block may need to satisfy up to n-1 sets of constraints. Therefore, the total number of constraints that need to be satisfied for all blocks is 0(n2).\n\nReferring to FIG. 40, a system design 900 is shown using the collaring process of the present invention. System design 900 is composed of four circuit blocks A, B, C, and D. Each arrow line connected to a block represents a constraint to design an interface for that block. Using the collaring process of the present invention, each block needs only to satisfy one set of constraints defined by the collaring interface. Thus, if a system is composed of n circuit blocks (n=4 in this example), the total number of constraints that need to be satisfied for all blocks is 0(n).\n\nReferring to FIG. 38, a computer system 1000 for performing the steps for collaring and the other inventive BBD processes discussed herein is shown in accordance with the present invention. The computer system 1000 includes a system bus 1001, a processing unit 1002, a memory device 1004, a disk drive interface 1006, a hard disk 1008, a display interface 1010, a display monitor 1012, a serial bus interface 1014, a mouse 1016, and a keyboard 1018.\n\nThe hard disk 1008 is coupled to the disk drive interface 1006; the monitor display 1012 is coupled to the display interface 1010; and the mouse 1016 and keyboard 1018 are coupled to the serial bus interface 1014. Coupled to the system bus 1001 are the processing unit 1002, the memory device 1004, the disk drive interface 1006, and the display interface 1010.\n\nMemory device 1004 stores data and programs. Operating together with the disk drive interface 1006, the hard disk 1008 also stores data and programs. However, memory device 1004 has faster access speed than hard disk 1008, while the hard disk 1008 normally has higher capacity than memory device 1004.\n\nOperating together with the display interface 1010, the display monitor 1012 provides visual interfaces between the programs executed and users, and displays the outputs generated by the programs. Operating together with the serial bus interface 1014, the mouse 1016 and keyboard 1018 provide inputs to the computer system 1000.\n\nThe processing unit 1002, which may include more than one processor, controls the operations of the computer system 1000 by executing the programs stored in the memory device 1004 and hard disk 1008. The processing unit also controls the transmissions of data and programs between the memory device 1004 and the hard disk 1008.\n\nIn the present invention, the programs for performing the steps discussed herein can be stored in memory device 1004 or hard disk 1008, and executed by the processing unit 1002, as will be understood by those skilled in the art to which the present invention pertains.\n\nBus Identification and Planning\n\nThe methodology of the present invention also provides for meeting the performance requirements of the overall design of the system desired by the end user or design team, as defined during front end acceptance (described above). While performance dictates the primary consideration for the design methodology of the present invention, a secondary consideration is reducing the gate count during bus type selection, since bus size can vary between available bus types such that a large, simple bus consumes more logic than a smaller, more complex one.\n\nTurning first to FIG. 41, there is illustrated a series of steps comprising the method of the present invention. At step 4110, Front-End Acceptance of the customer's initial specification is completed. This step has been described in detail above. Next, at step 4112, predefined bus requirements are analyzed, as explained below. At step 4114, bus clustering is planned while variables including latency, bandwidth, direction, and existing interfaces for each of the blocks are analyzed as well, making reference at step 4116 to a bus taxonomy reference library. Next, at step 4118, new bus specifications are developed and at step 4120 the new specifications are verified, including generation of a compliance suite and bus model verification substep. Steps 4118 and 4120 are performed with reference to block prestaging step 4122, wherein new block specifications covering arbiters and bridges are created, block specifications, including collars, are modified, glue specifications are defined and testbenches are created.\n\nWe will begin with a discussion of bus planning, including translating front-end specifications into top-level bus specifications. In the available art, system designers start with a high-level functional model or specification of the system being designed. Using system expertise and knowledge of similar systems, the designer constructs a high-level diagram of the bus structure for the design. The designer usually has a rough idea of the traffic on each of the buses, and can estimate how many buses and of what complexity are needed. Buses are designed to meet required system performance while minimizing interface logic and design effort. Designers then use this architecture to create a bus functional model to verify that the design operates as defined in the specification. This traditional process has been difficult to quantify because results vary with the expertise and past experience of the designer. The tasks defined herein apply a formal structure to the process of defining bus structures in chip design. However, these tasks require at least the average level of skill in the relevant bus and system development arts to achieve the best results.\n\nBus Protocols\n\nBuses provide the preferred communication medium between circuit blocks in a design. A bus, in its simplest form, can be a collection of point-to-point connections that require little logic but many wires. A simple bus transfers data between blocks at every clock cycle. While some blocks might require this type of information transfer, most blocks in a system need information from other blocks only occasionally. And since chip pins are very expensive in large system designs, buses are normally used to reduce the number of chip pins needed and to allow periodic communication between many different blocks in a system with little loss in performance. To do this, designers must add logic to each of the blocks to keep track of data transfer scheduling issues, such as: which block can use the bus wires; what block the data is being sent to; when the sender sends the data; and whether the receiver gets the data. These issues are handled by control signals on the bus and the establishment of a procedure for controlling communication between blocks (the bus protocol).\n\nTwo examples of bus protocol are the peripheral bus and the packet network. In a simple peripheral bus protocol, one device controls the bus. All information and data flows through this device, which decides, one case at a time, which block will send or receive data. Although peripheral bus processing requires relatively little logic, it does not use bus wires efficiently, and is not very flexible. Packet network protocols are relatively complex. All the information about which block sent the data and which block must receive it is stored with the data in a packet. Packet protocols let any block send data to any other block at any time. This protocol is very flexible and uses the bus wires efficiently, but each block needs a lot of logic to know when to send packets and decipher the packets it receives. Other bus protocols have different levels of flexibility, utilization, and latency (initial delay in transferring information from one block to another on the bus). A taxonomy for different bus types and their protocols is provided in FIG. 59.\n\nThe BBD bus design methodology of the present invention preferably uses defined bus types. The designer is not expected to develop buses from scratch unless they are part of an authored block. Also, the designer preferably logically connects blocks to existing, well-defined bus types rather than creating complex buses. The BBD methodology of the present invention therefore treats buses as signal connections between blocks. The logic for the bus is preferably distributed among the blocks in the design, as is the glue logic for allowing the buses to communicate outside the buses, as described herein above in the glue logic section.\n\nAll logical interconnect is treated as either simple or complex buses. Simple forms of interconnection are defined by the bus connection rules, but a specific protocol for complex buses is preferably not defined. The BBD methodology of the present invention preferably supports buses that: have hierarchy; are completely contained within blocks; have wires external to blocks; are completely contained within one level of logical hierarchy; are completely contained within one level of physical hierarchy; are compliant with VSI's on-chip bus (OCB) attributes specification; and are verified with compliance transaction vectors. Also, many of the out-of-scope conditions for BBD are preferably supported in SOC methodologies under the present invention.\n\nBuses are preferably either completely contained within blocks or defined as interconnect at the top hierarchy level. Buses that are defined at the top level are created at that level, allowing bus components to be distributed among and within the blocks.\n\nTo define buses for a BBD chip, the following steps are executed, each of which will be described in detail below:\n\nExtract Bus Requirements\n\nDefine Buses Based on Clustering\n\nSelect Buses\n\nSpecify the Bus Design\n\nReference the Bus Taxonomy\n\nVerify Bus Selection\n\nBlock Design Assumptions\n\nIn the BBD methodology, when the designer specifies the bus design, he or she must connect to block structures. This task assumes that if a firm or hard block contains a specific bus interface, that interface is soft, as defined above with reference to collars. It also assumes that blocks of all types contain a simplified interface between the bus interface logic and the actual function of the block. This is not an unreasonable assumption for peripheral blocks because many third-party block providers have created their own simple interface so users can add bus interface logic. Blocks that are tailored to multiple designs have separate internal functions and bus interface logic. The internal interface allows one to reuse these blocks with different buses. When a hard block has specific bus interface logic that cannot be separated from its internal function, a more complex bus protocol translation must be added to the block. In either case, the resulting bus interface logic becomes part of the soft collar created during block design.\n\nExtracting Bus Requirements\n\nData received from the front-end acceptance task includes the bus nets, signal nets, and pins on each of the blocks. There are four categories of signal nets: 1) predefined bus signals, which are block pins and nets comprising a bus, such as a PCI or AMBA bus, required by certain blocks such as processors; 2) bus signals, which are block pins and nets that must be buses, such as Read and Write signals; 3) possible bus signals, which are block pins and nets that might be wires or buses; and 4) signals, which are wire nets and are not dealt with by buses\n\nWhen the designer has determined the signal types, data received from the front-end acceptance task is organized according to these four types of signal nets. For type 1 and 2 nets, the data necessary to create a bus must either be provided by the customer or otherwise available. The required data is further defined in VSI's On-Chip Bus (OCB) Attributes Specification OCB1 1.0, which is incorporated herein by reference.\n\nIn additional, each bus that is specified or might be used in the design must have: a complete user's guide sufficient to create the bus; an implementation guide that defines the physical requirements for the bus; a complete set of simulation tools to test and verify the bus; and a list of technical attributes and how the bus compares with the list. Also, to create buses that comply with the VSI's On-Chip Bus Attributes Specification, vendors must provide the documentation and models described below.\n\nUser's Guide and Simulation Tools\n\nThe user's guide and simulation tools are used in bus design to build and test bus components. The set of simulation tools includes models written in behavioral Verilog and/or VHDL for the following elements: bus master; bus slave; bus support functions (arbiter, address decoder); and standard bus bridges. These are used to verify the bus, as described herein in the section related to bus verification.\n\nImplementation Guide\n\nThe implementation guide is used in block design, chip assembly, and subsequent tasks in chip design planning to describe the attributes of the buses. The following information is passed to block design as part of the block specifications: special cells required; physical properties of the cells; bus multiplexing or steering options; memory map; power distribution; and timing guidelines. Timing and maximum loading guidelines are also used in subsequent steps in chip design planning. Timing guidelines, maximum loading, and restrictions on bus layout or wiring are passed to the chip assembly task for use in bus implementation.\n\nTechnical Attributes List\n\nThe technical attributes must be translated into a form that can be maintained as bus attributes in the bus taxonomy reference library. The bus taxonomy reference and the bus type table are therefore used by the designer to choose the bus types. For predefined bus signals, the designer checks to insure that the required connections can meet the maximum loading and timing guidelines, and that bus layout and wiring restrictions can be met during chip assembly. If not, the design is sent back to the front-end acceptance task to be modified by the customer.\n\nDefining Buses Based on Clustering\n\nTo define buses based on clustering, the designer uses the interconnect bandwidths and latencies received at front-end acceptance. This step determines, for each of the clusters and blocks within the clusters, the latency, bandwidth, existing bus interface types, and direction of data flow. This information is then passed to the next step, selecting buses.\n\nA bus hierarchy is defined by clustering the highest bandwidth and lowest latency bus interconnect. Possible bus signals that are point-to-point nets can be eliminated from this and subsequent bus analysis and design, since these signals are provided directly to the chip assembly task for routing.\n\nCreate the Communication Manager Behavioral Model\n\nThe behavioral model of the chip as verified contains behavioral models and an abstract model of the interconnect between blocks. Typically, this interconnect is a software mechanism that transfers data among the test bench and blocks. Ideally, it is a form of communication manager, possibly a scheduler, to which all the blocks are connected. At the o"
    }
}
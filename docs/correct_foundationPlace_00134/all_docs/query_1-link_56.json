{
    "id": "correct_foundationPlace_00134_1",
    "rank": 56,
    "data": {
        "url": "https://www.rockefellerfoundation.org/insights/grantee-impact-story/unpacking-biases-in-algorithms-that-perpetuate-inequity/",
        "read_more_link": "",
        "language": "en",
        "title": "The Coded Gaze: Unpacking Biases in Algorithms That Perpetuate Inequity",
        "top_image": "https://www.rockefellerfoundation.org/wp-content/uploads/2020/12/3.png",
        "meta_img": "https://www.rockefellerfoundation.org/wp-content/uploads/2020/12/3.png",
        "images": [
            "https://www.rockefellerfoundation.org/wp-content/uploads/2020/12/Joy-White-Mask-1440x824.png",
            "https://www.rockefellerfoundation.org/wp-content/uploads/2020/12/Joy-At-Computer-1-e1607449558269-768x463.jpg",
            "https://www.rockefellerfoundation.org/wp-content/uploads/2020/12/Bellagio-Joy-and-Camille-2-768x544.jpg",
            "https://www.rockefellerfoundation.org/wp-content/uploads/2020/12/Serious-quote-card-wide-3-1440x810.png",
            "https://www.rockefellerfoundation.org/wp-content/uploads/2020/12/Serious-quote-card-wide-1-1440x810.png",
            "https://www.rockefellerfoundation.org/wp-content/uploads/2020/12/Serious-quote-card-wide-2-1440x810.png",
            "https://www.rockefellerfoundation.org/wp-content/uploads/2020/12/Joy-and-Sasha-Drag-vs-AI-2-768x643.jpeg",
            "https://www.rockefellerfoundation.org/wp-content/uploads/2020/12/Joy_AJL-700x700.jpg",
            "https://www.rockefellerfoundation.org/wp-content/uploads/2020/08/RF-Breakthrough-Series_-Wide-Social-Graphics-40.png",
            "https://www.rockefellerfoundation.org/wp-content/uploads/2020/12/Copy-of-MOI-4-DESIGN-2-1440x810.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2020-12-16T05:00:00+00:00",
        "summary": "",
        "meta_description": "“Data is a function of our history, So the past dwells within our algorithms. It is showing us the inequalities that have always been there.”",
        "meta_lang": "en",
        "meta_favicon": "https://www.rockefellerfoundation.org/favicon.ico?v3",
        "meta_site_name": "The Rockefeller Foundation",
        "canonical_link": "https://www.rockefellerfoundation.org/insights/grantee-impact-story/unpacking-biases-in-algorithms-that-perpetuate-inequity/",
        "text": "Now an advocate as well as a computer scientist, Buolamwini founded the Algorithmic Justice League (AJL), a non-profit intended to unmask and fight the harms caused by algorithms. Unchecked, she notes, algorithms threaten our civil rights. AJL calls for regulation, transparency and consent in equitable and accountable uses of artificial intelligence.\n\nGiven the newness and nuances of the field, some of those being harmed might not even realize it. The Rockefeller Foundation, as part of its commitment to helping build a more equitable society, is supporting AJL as it sets up the Community Reporting of Algorithmic System Harms (CRASH) project, a new initiative to support communities as they work to prevent, report and seek redress for algorithmic harms. As Buolamwini puts it: “If you have a face, you have a place in this conversation.”\n\n“The CRASH project gives people who have been harmed by algorithmic systems a new way to report and seek redress for those harms. This is a critical tool for reducing suffering today and building a more equitable future,” says Evan Tachovsky, the Foundation’s Director and Lead Data Scientist for Innovation.\n\nSo far, CRASH has gathered a team of researchers, community organizers and educators to focus on questions like: “How might companies and organizations build algorithmic systems that are more equitable and accountable? How do people learn that they are being harmed by an Artificial Intelligence (AI) system, and how do they report that harm? How can algorithmic harms be redressed?\n\n“We were together in the Bellagio library, talking about our respective work and experiences,” Francois recalls, “and the idea of building a platform to apply some of the information security models, like bug bounties, to the algorithmic harms space emerged naturally.” Bug bounties are programs where hackers can report bugs and vulnerabilities and receive recognition and compensation in return.\n\n“Despite working on difficult topics,” Francois remembers, “Joy and I both had a commitment to a sense of optimism, to bringing a playful and participatory approach to these issues when possible. Bellagio played a critical role in this project.”\n\nCostanza-Chock, who is transgender, nonbinary and uses the pronouns of they/them, describes their personal connection to algorithmic justice in their new book with MIT Press, Design Justice, which opens with a story of their passing through airport security. The user interface of the millimeter-wave scanners require TSA officials to press a blue “boy” or pink “girl” button in order to conduct a scan, reinforcing a binary gender and often marking gender non-conforming people as “risky.”\n\n“Like many trans people, I often get flagged going through those machines. And then they do a more invasive search, running their hands over my body parts,” Costanza-Chock says, noting that the system is also often triggered by Black people’s hair, Muslim women’s headwraps and disabled people’s bodies.\n\nSan Francisco, Oakland, Boston and Portland, Oregon have all enacted at least partial bans on the use of facial recognition systems.\n\nAt the Boston Public Library, AJL created a very successful “Drag Versus AI” workshop, teaching participants about face surveillance and how to undermine the algorithms by dressing up in drag.\n\nIn Brooklyn, Buolamwini helped achieve a successful outcome for tenants of the Atlantic Plaza Towers, rent-stabilized housing units in the city’s Brownsville section, a neighborhood where 70 percent of the population is Black, about 25 percent is Hispanic, and the poverty rate in 2018 was 27.8 percent, compared with 17.3 percent citywide.\n\nTenants protested the landlord’s decision to install a facial recognition system to replace key fobs and provide entrance to their high-rise buildings. They noted that facial recognition technologies often harm people of color, and added that they did not want themselves or their guests to be tracked. They noted Buolamwini’s work that found some facial analysis software misclassifies Black women 35 percent of the time, compared with just 1 percent of the time for white males. Buolamwini, testifying before Congress in May 2019 about technical limitations of facial recognition technology, drew attention to the Brooklyn tenants.\n\nSix months later, the landlord announced he was shelving the plan.\n\nBuolamwini stresses that she remains a computer nerd and is not opposed to technology. What she wants is a transformation in how algorithmic systems are designed and deployed, and regulations to monitor and restrict the harms they can cause. “We are at a moment when the technology is being rapidly adopted and there are no safeguards,” she says. “It is, in essence, a wild, wild West.”"
    }
}
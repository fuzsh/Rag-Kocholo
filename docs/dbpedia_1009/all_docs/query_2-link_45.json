{
    "id": "dbpedia_1009_2",
    "rank": 45,
    "data": {
        "url": "https://arxiv.org/html/2404.10661v1",
        "read_more_link": "",
        "language": "en",
        "title": "PD-Insighter: A Visual Analytics System to Monitor Daily Actions for Parkinsonâ€™s Disease Treatment",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://arxiv.org/html/extracted/2404.10661v1/figures/paper_teaser.png",
            "https://arxiv.org/html/extracted/2404.10661v1/figures/system_workflow.png",
            "https://arxiv.org/html/extracted/2404.10661v1/figures/data_collection.png",
            "https://arxiv.org/html/extracted/2404.10661v1/figures/body_variables.png",
            "https://arxiv.org/html/extracted/2404.10661v1/figures/overview_dashboard.png",
            "https://arxiv.org/html/extracted/2404.10661v1/figures/color_sliders.png",
            "https://arxiv.org/html/extracted/2404.10661v1/figures/data_view.png",
            "https://arxiv.org/html/extracted/2404.10661v1/figures/video_display.png",
            "https://arxiv.org/html/extracted/2404.10661v1/figures/immersive_replay.png",
            "https://arxiv.org/html/extracted/2404.10661v1/figures/finding_moments.png",
            "https://arxiv.org/html/extracted/2404.10661v1/figures/evaluation-immersive.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            "Visualization",
            "Health",
            "Immersive Analytics"
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Jade Kandel , Chelsea Duppen , Qian Zhang , Howard Jiang , Angelos Angelopoulos , Ashley Neall , Pranav Wagh , Daniel Szafir , Henry Fuchs , Michael Lewek and Danielle Albers Szafir\n\nAbstract.\n\nPeople with Parkinsonâ€™s Disease (PD) can slow the progression of their symptoms with physical therapy. However, clinicians lack insight into patientsâ€™ motor function during daily life, preventing them from tailoring treatment protocols to patient needs. This paper introduces PD-Insighter, a system for comprehensive analysis of a personâ€™s daily movements for clinical review and decision-making. PD-Insighter provides an overview dashboard for discovering motor patterns and identifying critical deficits during activities of daily living and an immersive replay for closely studying the patientâ€™s body movements with environmental context. Developed using an iterative design study methodology in consultation with clinicians, we found that PD-Insighterâ€™s ability to aggregate and display data with respect to time, actions, and local environment enabled clinicians to assess a personâ€™s overall functioning during daily life outside the clinic. PD-Insighterâ€™s design offers future guidance for generalized multiperspective body motion analytics, which may significantly improve clinical decision-making and slow the functional decline of PD and other medical conditions.\n\nVisualization, Health, Immersive Analytics\n\nâ€ â€ journalyear: 2024â€ â€ copyright: acmlicensedâ€ â€ conference: Proceedings of the CHI Conference on Human Factors in Computing Systems; May 11â€“16, 2024; Honolulu, HI, USAâ€ â€ booktitle: Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI â€™24), May 11â€“16, 2024, Honolulu, HI, USAâ€ â€ doi: 10.1145/3613904.3642215â€ â€ isbn: 979-8-4007-0330-0/24/05â€ â€ ccs: Human-centered computing Visual analytics\n\n1. Introduction\n\nIndividuals with Parkinsonâ€™s Disease (PD) develop various motor deficits, including gait disturbances and balance impairments, as the disease progresses (Moustafa et al., 2016). Although there is no cure for PD, physical therapy can potentially increase movement capacity and create long-term improvements in many motor impairments and functional activities (Grabli et al., 2012). Tailoring therapy protocols requires insight into how individuals perform various motor activities over time. Clinicians currently only have access to information about a patientâ€™s movements through brief, periodic clinical sessions and subjective self-reporting. Unfortunately, people with PD often struggle to remember moments of movement difficulty, and their written diaries can be unreliable due to recall bias and non-compliance (Stone et al., 2002), necessitating quantifiable measures of mobility to adequately tune physical therapy protocols.\n\nWe introduce PD-Insighter, a system for investigating in-home body motion data to support clinician understanding of patient motion patterns over time. PD-Insighter leverages motion capture data, activity labels, and environment videos to provide a multifaceted view of body movement, enabling interactive visualization for several hours of body motion capture. We designed PD-Insighter to satisfy three primary goals elicited through interviews with our clinical collaborators: 1) compute key indicators of body movement reflective of stability, balance, and gait; 2) aggregate and summarize in-home activity data to support rapid identification of relevant time segments and patterns for review; and 3) provide methods for closely studying instances, events, and activities of interest. PD-Insighter allows analysts to explore patient behavior simultaneously across body variables, consisting of numeric body displacements and angles of interest, and action labels, consisting of categorical labels indicating the action performed at a certain time, such as walking, sitting, or taking medicine.\n\nPD-Insighter uses a hybrid-platform design, with a desktop-based Overview Dashboard to present broad patterns and an Immersive Replay showing body and environment reconstructions in AR for contextualized, detailed analysis. The Overview Dashboard enables action-driven analysis using temporal heatmaps designed to emphasize outliers, display trends, and summarize body motion data. Immersive Replays, which reconstruct patient motion and physical environment data from raw data captures, provide clinicians the ability to analyze timepoints reflecting potential motor challenges in greater detail. Given that clinicians are trained to study the patientâ€™s body in-situ, we leveraged augmented realityâ€™s spatial rendering, navigation, and life-size representation to mirror current in-person clinical practices. By coupling immersive and traditional visual analytics, we enable close investigation with AR while preserving intuitive interactions with the dashboard for high-level navigation.\n\nWe evaluated our system in a think-aloud study with six rehabilitation specialists. We found that PD-Insighter enabled rapid and effective insight into motion data across multiple levels of detail. This system provides preliminary steps towards a broad vision for enhanced therapies for individuals with PD. Enhanced feedback for physical, observable tasks may optimize treatment protocols and combat the progression of PD as well as other chronic diseases such as stroke.\n\nContributions: Our primary contribution is a visual analytics approach for analyzing motion data in PD treatment embodied in PD-Insighter, a system for exploring body motion data. In creating the system, we contribute the following:\n\nâ€¢\n\na task characterization of clinical motion analysis tasks in physical therapy applications\n\nâ€¢\n\na data processing pipeline for synthesizing key measures for PD motion analysis from traditional motion capture data\n\nâ€¢\n\na hybrid visual analytics design that combines traditional and immersive analytic methods for simultaneous body motion, action, and video data analysis.\n\n2. Related Work\n\nPD-Insighter leverages visual analytics to support clinicians in supporting patients with PD. Our approach combines ideas from visual analytics in biomedical applications to analyze high-level motion patterns and from immersive analytics to provide contextualized, detailed replays of patient motion.\n\n2.1. Visualizations for Biomedical Analysis\n\nVisual analytics tools have supported various clinical decision-making tasks, such as analyzing medical events over time (Zhang et al., 2019; Plaisant et al., 2003), determining demographic risk factors and related outcomes (Zhang et al., 2015; Rogers et al., 2019), and monitoring longitudinal disease progression (Wang et al., 2022; Weber et al., 2021). Using such tools in conjunction with remote patient monitoring systems, health experts can observe patients away from the hospital using various health metrics, including blood pressure, pulse, heart rate, body temperature, and oxygen levels (Malche et al., 2022; Ramesh et al., 2012; Dabbakuti and Kotnana, 2012). Remote monitoring approaches would significantly benefit people with PD, whose motor impairments can make clinical visits especially challenging (Dorsey et al., 2016). Body-worn sensors can collect data about peopleâ€™s movement patterns for remote monitoring applications; however, data from such sensors tends to be insufficient for clinical analysis when used in isolation (see Del Din et al. (Del Din et al., 2021) for a survey).\n\nActivities of daily living (ADLs) can provide context for interpreting body-worn sensor data. ADLs describe the essential abilities needed to self-sufficiently manage oneâ€™s care, including eating, showering, and walking. Medical specialists consider ADLs crucial in diagnosing and treating conditions such as PD (Alizadeh et al., 2011; Deal et al., 2019; Mahoney and Barthel, 1965). People with PD may exhibit motor deficits differently across ADLs. For example, a pronounced forward lean is normal while reaching for an object in front of you but may indicate a postural impairment when walking.\n\nUnderstanding activity data during ADLs often requires a knowledge of the patientâ€™s environment to discover how objects in the home might play a role in movement patterns. The presence of a couch and dining table correlate with sitting and low activity while transitions between spaces correlate with walking and high activity (Zhongna Zhou et al., 2008). Past studies of ADLs found that medical specialists benefit from contextualizing behavioral data based on the patientâ€™s location (e.g., kitchen, bedroom, bathroom) (Robben et al., [n. d.]). Other systems visualize ADLs with graphical representations of the patientâ€™s apartment, using trajectories or avatar replays to show movement in space (Rashidi and Cook, 2010; Boers et al., 2009); however, these approaches only provide short snapshots with limited global insight into movements over time, requiring analysts to remember key patterns in ADL data throughout the replay. Gil et al. (Gil et al., 2007) summarized activity within the home using embedded sensors on common objects to infer the kinds of activities people engaged in over time. While this approach provides knowledge about the number of actions occurring in various locations, it provides limited insight into movement quality.\n\nOther approaches enable clinicians to analyze specific biomechanical motions from a given activity in detail. Ploderer et al. (Ploderer et al., 2016) visualized upper limb movement in people who had a stroke using multiple representations of arm movement across time, the amount of time active, and heatmaps on 2D avatars showing body areas with high activity. HAExplorer (Eulzer et al., 2022) displays precise helical components of biomechanical motion at individual joints. Keefe et al. (Keefe et al., 2009) provided a small multiples view of biomechanical motions from a specific activity to view from multiple perspectives. However, these approaches focus on specific motions and joints over discrete activities in isolation rather than global, multi-activity perspectives and environmental contexts associated with motor abilities in daily life. Other systems used Kinect for data capture to create a full-body skeletal reconstruction and extract kinematic features, capturing in home body motion data for clinical analysis and PD diagnosis (Nguyen et al., 2017; Kao et al., 2016). However, for ADLs, skeleton reconstructions and kinematic values can be difficult to analyse without environment, temporal, or action context.\n\nMotion analysis for PD and similar motor disorders must balance a crucial tension between access to high-level summaries showing how body motions change over time (i.e., hours to days) and low-level details enabling deeply contextualized analysis over brief movements or activities (i.e., seconds to minutes). Systems representing ADLs with activity level or location can present data across long time periods but lack environmental and full body contexts to draw meaningful conclusions about behavioral patterns. Replays of patient motions can provide essential context but only work for brief time segments. We pair statistical, activity-based overviews with deeply contextualized immersive reconstructions to enable an understanding of large body motion patterns across time and the analysis of context.\n\n2.2. Immersive Analytics for Body and Environment Data\n\nTo help clinicians analyze patient activities in context, PD-Insighter combines traditional visualization workflows with immersive analytics (IA). IA leverages immersive technologies such as virtual and augmented reality (VR and AR) for data visualization (Marriott et al., 2018). Immersive environments project data into 3D space, often making navigating, manipulating, and perceiving spatial information such as depth, height, and size more intuitive in part by integrating binocular cues and by allowing people to physically move through data (Marriott et al., 2018; Whitlock et al., 2020; Kraus et al., 2021; Zacks et al., 1998; Heinrich et al., 2021). Improved spatial perception from AR is especially important to our work, as analysing 3D body limbs in space requires accurate understanding of depth and distance. Our work is particularly informed by research in situated analytics (SA), which directly integrates visual representations of the data into the studied environment, often placing data on top of physical objects such as buildings or bodies (Willett et al., 2017; Bressa et al., 2022). SA can incorporate data directly into the physical environment, advancing fields such as fieldwork (Whitlock et al., 2019), construction (Irizarry et al., 2013), and interior design (Wang et al., 2023).\n\nPast work in SA summarizes human motion within an indoor environment, preserving the time, place, and interactions with the space (Luo et al., 2023; Kloiber et al., 2020). For example, AvatAR (ReipschlÃ¤ger et al., 2022) paired a digital avatar with different embedded representations of motion (e.g., gaze, paths) to reflect movement patterns and interaction with the environment. However, these systems primarily visualize where people have interacted with the space rather than the quality of their movements. We instead focus our approach on understanding the mechanics of patient motion, where context and location play a vital secondary role in interpreting movement patterns.\n\nSA systems have also situated visualizations with respect to the body, supporting more detailed mechanical motion analyses, such as improving cycling performance (Kaplan et al., 2018), basketball free throws (Lin et al., 2021), hand gestures (Freeman et al., 2009), and physical rehabilitation (Doyle et al., 2010; Garcia and Felix Navarro, 2014; Ayoade and Baillie, 2014). Real-time systems such as Physio@Home (Tang et al., 2015) used AR mirrors to embed wedges and lines directly on the personâ€™s body to guide movement for upper limb physiotherapy. YouMove (Anderson et al., 2013) also employed an AR mirror to teach physical movement sequences by having people follow along with a skeletonâ€™s actions. These systems demonstrate the power of IA for real-time motion analysis and guidance; however, they focus on short time-frames for guiding motion in real-time.\n\nWhile traditional visual analytics approaches can support coarse-grained longitudinal analysis, past work in IA suggests its potential for contextualized investigations and replays of body motion and key actions. Clinicians can use IA tools to understand the full context of a movement and view body motion from multiple angles to gain a holistic picture of potential motor deficits. Further, IA can offer visceral experiences with data (Lee et al., 2021; Zhou et al., 2023), which may augment data interpretation by emphasizing relative proportions of the patientâ€™s physical space and associated objects and actions. We pair this paradigm with traditional overview analytics in PD-Insighter to enable multi-scale insight into movement data for physical therapy for PD.\n\n3. Task Analysis\n\nWe developed PD-Insighter using an approach drawing on the design study methodology (Sedlmair et al., 2012). Our research team included two experts in neurologic physical therapy specializing in PD who helped identify key tasks for using data to enhance clinical understanding for therapy guidance. These experts helped refine the tasks and visualization approach during system development.\n\n3.1. Motivating Problem & Terminology\n\nPeople with PD develop various motor deficits, including gait disturbances and balance impairments that can lead to falls, gait freezes, and difficulties standing and grabbing an object (Moustafa et al., 2016). To determine effective rehabilitative treatment methods, clinicians must efficiently and effectively identify motor deficits and their potential triggers. Triggers can include changes in the environment (Almeida and Lebold, 2010), time since medication (Amboni et al., 2015), and divided attention (Chomiak et al., 2015).\n\nData for body motion during ADLs could provide clinicians with crucial information about deficits and triggers before meeting a patient. For example, if a person with PD expresses challenges standing up from their chair, a clinician could examine their data prior to a clinical visit to determine potential triggers causing this difficulty. The clinician could then spend the clinical visit educating the patient on exercises or compensatory methods to improve sit-to-stand transfers. Due to time constraints in the clinic, clinicians require rapid and efficient methods for reviewing this data.\n\nTo understand patient motor behavior, clinicians need to 1) focus on the patientâ€™s ability to perform specific ADLs (e.g. walking, standing, reaching), 2) discover when and how frequently motor deficits occur, and 3) understand movement context to see why motor deficits occur. While clinicians currently rely on brief in-clinic assessments and self-reports, data-driven approaches can provide more precise and holistic insight into patient movement.\n\nWhile the terminology may vary across practices, clinicians identified three kinds of data that would support clinical analysis and decision-making for PD: body, action, and event. For this paper, body variables are specific joint angles and displacements where changes in value can indicate a patientâ€™s motor performance. An action is the process of moving oneâ€™s body to achieve a goal, such as walking, sitting, or reaching. Actions can make up ADLs (e.g., reaching is performed while bathing). An event is the time period an action occurs.\n\nClinicians can infer motor deficits from shifts in body variables; however, clinicians can only fully make sense of body variable changes in the context of performed actions. For example, a patient may stop moving due to a freeze or due to external stimuli such as searching the refrigerator. Thus, clinicians desired a way to contextualize body variables with respect to action data, temporal events, and the physical environment to readily enable clinicians to investigate motor deficits and their potential triggers.\n\n3.2. Tasks\n\nWhile clinicians do not currently have tools for working with longitudinal body motion and action data in PD treatment, we identified core tasks associated with such data through multiple clinician interviews. These tasks focused on data needs for understanding proper and improper body motion and what motion data indicates about the patientâ€™s condition. We revised these tasks through continued iteration with clinical collaborators on early system prototypes.\n\nWe characterize these tasks into three high-level goals: identify and filter data by action, where clinicians can assess movement patterns over key ADLs; discover motor deficits, where clinicians can compare motion patterns across key body relationships to discover systemic deficiencies; and contextualize motor deficits, where clinicians can view potential deficits in context to understand potential triggers and treatments.\n\n3.2.1. Task 1: Identify and Filter Data by Action\n\nClinicians focus on specific actions to understand a patientâ€™s challenges. Actions consist of complex combinations of movements that together enable an intended goal. For example, the â€œwalkingâ€ action combines synchronized and balanced footsteps, arm swings, and upright posture to move from one place to another. The patterns clinicians look for in motion data depend on the patientâ€™s action. For example, significant variations in trunk angle during walking may indicate a deficit, whereas the same variations are normal when standing up from a chair.\n\nKey actions for informing treatment protocols include:\n\nâ€¢\n\nSit-to-stand (e.g., when a patient gets up from a chair)\n\nâ€¢\n\nSitting\n\nâ€¢\n\nStand-to-sit (e.g., when a patient sits down on a chair)\n\nâ€¢\n\nReaching (e.g., when a patient grabs something from a high shelf or across a table)\n\nâ€¢\n\nWalking\n\nâ€¢\n\nStanding\n\nâ€¢\n\nTaking medicine\n\nWhile other actions may also inform protocols, we focus on these common actions as our clinician collaborators identified these as necessary to understand a patientâ€™s condition. By letting clinicians examine motion patterns with respect to actions over time, clinicians can more readily identify and select time segments, filter and compare events of the same action, and select a specific event based on actions of interest (see Â§6).\n\n3.2.2. Task 2: Discover Motor Deficits\n\nDifferent body angles and displacements illuminate if, when, and why a motor deficit occurs. Clinicians frequently need to assess differences in posture, arm use, gait patterns, and weight balance (see Â§5.2.3 & 6.2).\n\nPosture: Poor posture is a common symptom of PD and can lead to neck or back pain, improper breath control, loss of flexibility/mobility, and increased risk for falls from poor balance. The trunk angleâ€”the angle between the vertical axis and vector between the pelvis and neckâ€”captures posture, showing how far a patientâ€™s upper body is leaning.\n\nArm Use: Asymmetric arm swing during gait is a characteristic early motor sign of PD (Lewek et al., 2010). As the disease progresses, arm swing is further reduced bilaterally. Understanding the context of arm swing is critical. An asymmetric arm swing may be a sign of PD, but it can also occur when someone is holding a glass of water and walking across the room. We can infer when one arm is swinging more or used more than the other by looking at patterns in the distance between each hand and the pelvis.\n\nGait Patterns: Clinicians examine patterns in foot position relative to the pelvis to estimate step lengths in gait. Changes in terrain or changes in mobility might cause variations in foot position patterns. Gait freezing is an abnormal walking pattern where patients are temporarily unable to move their feet forward when trying to walk, which can lead to a fall. A clinician can infer a freeze event when the feet suddenly stop or strides grow unexpectedly short under the pelvis while walking.\n\nWeight Balance: Imbalanced weight shift can create postural instability and freezing (Dijkstra et al., 2021) as well as struggles sitting down, standing up, or standing still. We can infer when one leg bears more weight based on the displacement between the pelvis and the foot from the side direction.\n\nWith potentially hoursâ€™ and daysâ€™ worth of body motion data, finding motor deficits efficiently is challenging. To enable clinicians to find these important changes in motion quickly, we can aggregate data over time with respect to using meaningful statistics and actions and emphasize outliers that reflect a motor deficitâ€™s shift in body motion (see Â§6).\n\n3.2.3. Task 3: Contextualize Motor Deficits\n\nTime of day, symptom and action duration, and where the person was in their house can indicate what may have contributed to the observed symptoms. Clinicians need temporal context to assess the causes of motion change and identify potential triggers (see Â§6). For example, if and when someone took their medicine may impact observed symptoms and motor patterns (Amboni et al., 2015). How long someone froze or fell can indicate the severity of the event. The percentage of time sitting can show how much someone was active throughout the day, which may increase or decrease the likelihood of symptoms.\n\nAlthough joint angles and displacements can highlight potential points of concern, clinicians also need to study a patientâ€™s physical body and environment rather than rely solely on numerical representations. Direct observation provides contextual understanding, validates findings, and captures nuances that numbers may overlook, such as environmental obstacles. For instance, carpet/floor changes, doorways, and turns can trigger a freeze event. Patients might also rely on their surrounding environment for physical support, such as leaning on a table while walking or pressing off the couch when standing up (see Â§7).\n\n4. System Overview\n\nPD-Insighter is a hybrid desktop-AR system presenting body motion, action, and environment data for clinicians to investigate the motor functioning of a person with PD. Our system supports the target tasks elicited in our interviews through a workflow consisting of data collection, data processing, data integration, and deployment (Figure 2). We tested our system with datasets of various lengths and found our visualization methods supported up to five hours of analysis on a traditional laptop display.\n\nPD-Insighter takes body pose data, action labels, and video data as input. PD-Insighter processes that input to compute a series of body variables reflecting key joint and displacement relationships (see Â§5.2.3) and reconstructs the local environment from video data (see Â§5.2.1). PD-Insighter passes the processed data into the analytics component for clinical analysis.\n\nThe visual analytics components of PD-Insighter consist of a desktop-based Overview Dashboard (see Â§6) presenting the action labels and body variables (Figure 5) and an AR Immersive Replay (see Â§7) that provides detailed context for specific timepoints of interest (Figure 9). The dashboard first presents clinicians with a summary overview of actions and variable distributions over the entire dataset (Figure 7.1). Clinicians can use action types, body variables, and times to drill down into the data, characterize broader movement patterns, and isolate potential timepoints of interest (Figure 7.2 & 7.3). The clinician can select a moment in time to view the context of the movements in the Immersive Replay. The immersive display renders a skeleton from the body pose data situated in the environment reconstruction. We display the reconstruction of the local environment centered on the patient with a 1.5-meter radius view of the environment to provide context for understanding motor behavior and enable the clinician to view the reconstructed environment and patient movement from multiple perspectives. Clinicians can move back and forth between the Overview and Immersive Replay on demand.\n\n4.1. Implementation\n\nWe implemented the dashboard using D3.js and HTML5. We built the Immersive Replay in Unity and deployed it to a HoloLens 2 headset. We managed data processing and reconstruction using custom Python scripts, EasyMocap (noa, 2023), and RealityCapture(noa, [n. d.]).\n\n5. Data Collection and Processing\n\nPD-Insighter requires body motion data consisting of body variables, action labels, and an environment mesh, which can be extracted from motion capture and standard video. Recent work in PD has deployed IMUs and similar body-worn sensors in the home for remote monitoring applications (Del Din et al., 2021). Advances in egocentric motion capture and activity recognition lead clinicians to anticipate that full non-intrusive motion capture during ADLs using other wearable devices, such as cameras, will soon be feasible (Zhang et al., 2023; Cha et al., 2021; Lin et al., 2022; Wang et al., 2019; Demrozi et al., 2023; Kolkar and V, 2023). Our current system presents a proof-of-concept of such a pipeline, using external cameras and body-worn inertial measurement units (IMUs) similar to those employed by our clinical collaborators for in-lab observation. Here, we describe our current capture workflow; however, PD-Insighter can also use data generated through alternative pipelines, including conventional motion capture. For generalizability, the system uses conventional motion capture data formats (i.e., a JSON file summarizing joint positions for each frame) annotated with simple action labels (i.e., a JSON file with start and end frame numbers) and environment meshes generated through standard RGB video files.\n\n5.1. Data Collection\n\nTo develop PD-Insighter, we collected body motion data from a combination of RGB cameras and IMUs, following practices used in current clinical observations conducted in Parkinsonâ€™s research and similar studies (Burpee and Lewek, 2015). We conducted our data collection in a 11ft Ã—\\timesÃ— 15ft environment, shown in Figure 3, which simulated a domestic living space (i.e., living room and kitchen) outfitted with everyday furniture items such as couches, counters, tables, chairs, and shelves and essential appliances including microwaves, coffee makers, utensils, and dishes. We captured the local environment using a smartphone video recording. The tracked person wore ten IMU sensors on their body to collect body pose data. Three large cameras on tripods record the person in the room performing actions of daily living. We processed the smartphone video into an environment mesh and the IMU data and camera recordings into a human body pose estimate, body variables, and action labels. We collected movements with acted-out PD behaviour from two members of the research team (one developer and one clinician) and real movements from a person with PD.\n\n5.2. Data Processing\n\n5.2.1. Environment Reconstruction\n\nAn environment reconstruction provides important context for understanding body motion and potential motor deficit triggers. PD-Insighter uses a mesh of the environment generated from the RGB camera data to show where the person was in 3D space. Given a video of the room, we use RealityCapture photogrammetry software (noa, [n. d.]) to create a dense point cloud and a texture map from video data to create a 3D mesh. We scale the environment using the real-world length of a physical referent in the space.\n\n5.2.2. Action Recognition\n\nAction labels are start and end frames that indicate the kind of actions occurring at a given timepoint (e.g., walking). These labels give clinicians crucial context for analyzing body movement and isolating relevant timepoints of interest. Our test datasets use manual labels for higher accuracy and to restrict our actions to activities of relevance for our collaborators, but state-of-the-art methods in computer vision could also be applied to automate action labeling (Lin et al., 2022; Wang et al., 2019; Demrozi et al., 2023; Kolkar and V, 2023).\n\nDuring our design iterations, clinicians discovered that certain events of interest could be approximated using body variables. Specifically, clinicians found that freeze events could be approximately characterized by instances when the feet were below the pelvis for longer than a second during a walking event. We defined a mathematical filter to automatically label candidate activities matching this criteria. In future investigations, automated action labeling could incorporate such action approximations for condition-specific events with sparse training data, such as freezes.\n\n5.2.3. Body Variables\n\nPD-Insighter requires body pose data to calculate and display joint displacements and angles reflecting shifts in motion and potential motor deficits (see Â§3) as well as to reconstruct the personâ€™s body for immersive detailed investigation. We used EasyMocap (noa, 2023) to process IMU and camera video data in our collected datasets to compute pose coordinates for 22 joints per frame. While PD-Insighter can take an arbitrary number of body coordinates for reconstruction, it requires pelvis, hips, neck, hands, and feet coordinates to calculate the target body variables.\n\nPD-Insighter processes the body motion data to estimate four critical body pose metrics for detecting the deficits summarized in Section 3.2.2: trunk angle, arm use, foot position, and weight shift. We compute relevant body variables using the positions of the pelvis (Pğ‘ƒPitalic_P), hip (Hğ»Hitalic_H), neck (Nğ‘Nitalic_N), hands (Hâ¢Ağ»ğ´HAitalic_H italic_A), and feet (Fğ¹Fitalic_F). For a given joint position Jğ½Jitalic_J, jisubscriptğ‘—ğ‘–j_{i}italic_j start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is the position of that joint at frame iğ‘–iitalic_i and jissuperscriptsubscriptğ‘—ğ‘–ğ‘ j_{i}^{s}italic_j start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT is the side of the body the joint is on (left lğ‘™litalic_l or right rğ‘Ÿritalic_r).\n\nClinicians described these body variables based on the direction the limb or joint is moving in, called the cardinal planes of motion. Movement in the sagittal plane involves forward/backward motion, while the coronal plane involves side-to-side motion (Figure 4.1). We calculate the cardinal plane using a local coordinate system aligned with the pelvis pisubscriptğ‘ğ‘–p_{i}italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, assigning yâ†’â†’ğ‘¦\\vec{y}overâ†’ start_ARG italic_y end_ARG to the vertical unit vector, yâ†’=âŸ¨0,1,0âŸ©â†’ğ‘¦010\\vec{y}=\\langle 0,1,0\\rangleoverâ†’ start_ARG italic_y end_ARG = âŸ¨ 0 , 1 , 0 âŸ©, xâ†’=piâˆ’hlâ†’ğ‘¥subscriptğ‘ğ‘–superscriptâ„ğ‘™\\vec{x}=p_{i}-h^{l}overâ†’ start_ARG italic_x end_ARG = italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_h start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT, and zâ†’=xâ†’Ã—yâ†’â†’ğ‘§â†’ğ‘¥â†’ğ‘¦\\vec{z}=\\vec{x}\\times\\vec{y}overâ†’ start_ARG italic_z end_ARG = overâ†’ start_ARG italic_x end_ARG Ã— overâ†’ start_ARG italic_y end_ARG. We use this coordinate frame to compute the following body movement variables:\n\nTrunk Angle: The trunk angle Î¸âˆˆÎ˜ğœƒÎ˜\\theta\\in\\Thetaitalic_Î¸ âˆˆ roman_Î˜ is how far a person is leaning forward or backward in the sagittal plane at frame iğ‘–iitalic_i. For all frames, we calculate a trunk vector tâ†’=piâˆ’niâ†’ğ‘¡subscriptğ‘ğ‘–subscriptğ‘›ğ‘–\\vec{t}=p_{i}-n_{i}overâ†’ start_ARG italic_t end_ARG = italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_n start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and find the angle between the trunk vector and yâ†’â†’ğ‘¦\\vec{y}overâ†’ start_ARG italic_y end_ARG (i.e., a perfectly upright posture):\n\nÎ¸=arccosâ¡(tiâ†’â‹…yâ†’|tiâ†’|)ğœƒâ‹…â†’subscriptğ‘¡ğ‘–â†’ğ‘¦â†’subscriptğ‘¡ğ‘–\\theta=\\arccos(\\frac{\\vec{t_{i}}\\cdot\\vec{y}}{|\\vec{t_{i}}|})italic_Î¸ = roman_arccos ( divide start_ARG overâ†’ start_ARG italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG â‹… overâ†’ start_ARG italic_y end_ARG end_ARG start_ARG | overâ†’ start_ARG italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG | end_ARG )\n\nArm Use: The arm use ÏarmsâˆˆÎ”armssubscriptsuperscriptğœŒğ‘ armsubscriptsuperscriptÎ”ğ‘ arm\\rho^{s}_{\\mathrm{arm}}\\in\\Delta^{s}_{\\mathrm{arm}}italic_Ï start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT roman_arm end_POSTSUBSCRIPT âˆˆ roman_Î” start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT roman_arm end_POSTSUBSCRIPT is how far the hands are from the pelvis in the sagittal plane at frame iğ‘–iitalic_i. For all frames, we calculate hand vectors hâ¢asâ†’=pâˆ’hâ¢aâ†’â„superscriptğ‘ğ‘ ğ‘â„ğ‘\\vec{ha^{s}}=p-haoverâ†’ start_ARG italic_h italic_a start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT end_ARG = italic_p - italic_h italic_a and find the projection of the hand vectors in the sagittal direction zâ†’â†’ğ‘§\\vec{z}overâ†’ start_ARG italic_z end_ARG:\n\nÏarms=|hâ¢aisâ†’â‹…zâ†’|subscriptsuperscriptğœŒğ‘ armâ‹…â†’â„superscriptsubscriptğ‘ğ‘–ğ‘ â†’ğ‘§\\rho^{s}_{\\mathrm{arm}}=|\\vec{ha_{i}^{s}}\\cdot\\vec{z}|italic_Ï start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT roman_arm end_POSTSUBSCRIPT = | overâ†’ start_ARG italic_h italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT end_ARG â‹… overâ†’ start_ARG italic_z end_ARG |\n\nFoot Position: The foot position ÏfootsâˆˆÎ”footssubscriptsuperscriptğœŒğ‘ footsubscriptsuperscriptÎ”ğ‘ foot\\rho^{s}_{\\mathrm{foot}}\\in\\Delta^{s}_{\\mathrm{foot}}italic_Ï start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT roman_foot end_POSTSUBSCRIPT âˆˆ roman_Î” start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT roman_foot end_POSTSUBSCRIPT is how far the feet are from the pelvis in the sagittal plane at frame iğ‘–iitalic_i. For all frames, we calculate feet vectors fisâ†’=piâˆ’fisâ†’superscriptsubscriptğ‘“ğ‘–ğ‘ subscriptğ‘ğ‘–superscriptsubscriptğ‘“ğ‘–ğ‘ \\vec{f_{i}^{s}}=p_{i}-f_{i}^{s}overâ†’ start_ARG italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT end_ARG = italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT and find the projection of the feet vectors in the sagittal direction zâ†’â†’ğ‘§\\vec{z}overâ†’ start_ARG italic_z end_ARG:\n\nÏfoots=fisâ†’â‹…zâ†’subscriptsuperscriptğœŒğ‘ footâ‹…â†’superscriptsubscriptğ‘“ğ‘–ğ‘ â†’ğ‘§\\rho^{s}_{\\mathrm{foot}}=\\vec{f_{i}^{s}}\\cdot\\vec{z}italic_Ï start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT roman_foot end_POSTSUBSCRIPT = overâ†’ start_ARG italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT end_ARG â‹… overâ†’ start_ARG italic_z end_ARG\n\nUnlike Ïaâ¢râ¢mssubscriptsuperscriptğœŒğ‘ ğ‘ğ‘Ÿğ‘š\\rho^{s}_{arm}italic_Ï start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_a italic_r italic_m end_POSTSUBSCRIPT, clinicians want to know when the foot is in front or behind the pelvis, so we do not take the magnitude of the dot product to preserve the sign. The foot is in front of the pelvis if the dot product is positive and behind if negative.\n\nWeight Shift: The weight shift Ï‰âˆˆÎ©sğœ”superscriptÎ©ğ‘ \\omega\\in\\Omega^{s}italic_Ï‰ âˆˆ roman_Î© start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT is a ratio dependent on how close the pelvis is to the feet coordinates at frame iğ‘–iitalic_i. The closer the pelvis is to one side, the more weight that side is bearing. Using the feet vectors fisâ†’=piâˆ’fisâ†’superscriptsubscriptğ‘“ğ‘–ğ‘ subscriptğ‘ğ‘–superscriptsubscriptğ‘“ğ‘–ğ‘ \\vec{f_{i}^{s}}=p_{i}-f_{i}^{s}overâ†’ start_ARG italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT end_ARG = italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT, we find the projection of the feet vectors in the coronal direction xâ†’â†’ğ‘¥\\vec{x}overâ†’ start_ARG italic_x end_ARG:\n\nÏweightShifts=|fisâ†’â‹…xâ†’|subscriptsuperscriptğœŒğ‘ weightShiftâ‹…â†’superscriptsubscriptğ‘“ğ‘–ğ‘ â†’ğ‘¥\\rho^{s}_{\\mathrm{weightShift}}=|\\vec{f_{i}^{s}}\\cdot\\vec{x}|italic_Ï start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT roman_weightShift end_POSTSUBSCRIPT = | overâ†’ start_ARG italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT end_ARG â‹… overâ†’ start_ARG italic_x end_ARG |\n\nGiven that clinicians focus these two vectors in relation to each other, we calculate ratios rather than displacement to represent weight shift:\n\nÏ‰s=ÏweightShiftsÏweightShiftl+ÏweightShiftrsuperscriptğœ”ğ‘ subscriptsuperscriptğœŒğ‘ weightShiftsubscriptsuperscriptğœŒğ‘™weightShiftsubscriptsuperscriptğœŒğ‘ŸweightShift\\omega^{s}=\\frac{\\rho^{s}_{\\mathrm{weightShift}}}{\\rho^{l}_{\\mathrm{% weightShift}}+\\rho^{r}_{\\mathrm{weightShift}}}italic_Ï‰ start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT = divide start_ARG italic_Ï start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT roman_weightShift end_POSTSUBSCRIPT end_ARG start_ARG italic_Ï start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT start_POSTSUBSCRIPT roman_weightShift end_POSTSUBSCRIPT + italic_Ï start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT roman_weightShift end_POSTSUBSCRIPT end_ARG\n\nRatios emphasize small but relevant imbalances. If both feet are close to the body but the weight is closer to the right, the small difference between the two sides would minimize the imbalance, whereas a ratio would highlight the differences. Ratios also reflect when there is balance or imbalance. For example, when Ï‰l=0.5superscriptğœ”ğ‘™0.5\\omega^{l}=0.5italic_Ï‰ start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT = 0.5, the weight shift is mostly balanced, but when Ï‰l=0.7superscriptğœ”ğ‘™0.7\\omega^{l}=0.7italic_Ï‰ start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT = 0.7, the weight shift has a heavy lean to the left.\n\n6. Overview Dashboard\n\nPD-Insighter uses an overview-first, details-on-demand workflow. The Overview Dashboard supports early-phase analysis of critical actions and events (Figure 5). Clinicians use Action Selection (Â§6.1) and Body Variable Controls (Â§6.2) to drill down into data pertaining to relevant events in the Timeline (Â§6.3). They can see statistics of the selected data in the Detailed View (Â§6.4). and replay these events using a video thumbnail or the Immersive Replay (see Â§7).\n\n6.1. Action Selection\n\nTo support identification and filtering by action (Task 1), selecting an action enables clinicians to focus their attention and analysis on particular actions of interest. Predefined buttons display time periods when the patient performed the selected action (Figure 5.1). Our initial prototype presented all motion data over time with accompanying actions, using only temporal sliders for navigation. However, clinicians felt this presentation was overwhelming and preferred to use actions to isolate time segments of interest.\n\nPD-Insighter represents each action event with a horizontal bar. The barâ€™s width is proportional to the duration of the event, preserving temporal ordering and showing which events are shorter or longer. We map each action to a distinct color in a categorical palette from Color Brewer (Harrower and Brewer, 2003) to provide a compact action representation on the Timeline window. Each action type is presented in its own stacked row so clinicians can view simultaneous actions. For example, a high trunk angle while standing might be concerning to a clinician, but a high trunk angle while standing and reaching would not be concerning since the reach would explain the presence of the increased trunk lean (Figure 5.3a).\n\n6.2. Body Variable Controls\n\nTo help clinicians discover motor deficits (Task 2), the Body Variable Controls allow clinicians to display relevant motion data through body variable buttons and sliders (Figure 5.2). Clinicians use body data to efficiently compare the action events and determine body motion patterns, potential deficits, outliers, and specific events for further study. We display our body variablesâ€”trunk angle, arm use, foot position, and weight shiftâ€”with four temporal color-encoded heatmaps to provide a compact representation of the data over time and to enable clinicians to quickly spot outliers. We assign each body variable a color ramp from Color Crafter (Smart et al., 2019) using distinct hues for each variable. Body variable color legend sliders allow the clinician to change the maximum value of a color ramp to make desired value ranges more salient.\n\nOverlaid on corresponding color legends, the line graph distributions for each variable show the frequency of the body variables across the dataset. These distributions allow clinicians to more rapidly tailor the color encoding in the Timeline to specific value ranges (Figure 6.1). Given the importance of left-right relationships in the feet and hands (see Â§3), we pair their distributions, distinguishing the left from the right using a dotted line. Distributions can also summarize balance and posture for selected time segments to assist in global motion analysis. For example, when a patient has a balanced weight shift and upright posture, the distributions show a Gaussian curve and equivalent left and right measures. Unexpected distributions, such as an uncentered weight shift curve, high trunk angle values, or unmatching left and right distributions, can reflect poor balance, posture, or gait (Figure 6.2).\n\n6.3. Timeline\n\nThe Timeline displays detailed body and action data over time based on the clinicianâ€™s selections using a series of linear heatmaps (Figure 7). When no actions are selected, the clinician sees a concise synopsis of the patientâ€™s behaviors and movements throughout the dataset (Figure 7.1). The Action Summary uses a bar chart to summarize the time spent in each action and reflect the patientâ€™s general activity levels. For example, if the clinician sees the patient sitting most of the time, they may suggest that the patient walks around and stands more frequently. The Action Timeline uses a color-coded timeline to visualize the patientâ€™s actions chronologically, reflecting when and how long potential events of interest took place. This timeline helps clinicians understand sequences of related actions as well as general patterns of behavior, such as when the patient took their medicine, how long they were sitting, and if they were walking for longer or shorter periods of time.\n\nClinicians begin their analysis by selecting an action from the Select Action window or Action Summary chart. The Timeline window then updates to display a set of stacked temporal heatmaps reflecting the distribution of each body variable over time, with whitespace between discrete action events (Figure 7.2). Additional temporal and body filters (e.g., More than 5 Seconds, Imbalanced Weight Shift) shown in Figure 5.3b allow the clinician to filter even further and center their attention on events that fit their analysis goals. For example, the â€Potential Freezesâ€ filter was developed from mathematical calculations using the foot position, described in Section 5.2.2. Using actions and body variables to drive data filtering allows clinicians to focus on semantically meaningful sets of movements.\n\nThe large number of frames in a dataset can introduce clutter and make it difficult for clinicians to find outliers and patterns in the data. PD-Insighter provides a toggle in the Body Variable Controls to simplify the display to emphasize large shifts in motion data. Based on prior work in large-scale data aggregation (Albers et al., 2011; Correll et al., 2011), our simplification feature bins data into mean bins for values within one standard deviation from the mean, outlier bins for values more than one standard deviation from the mean, and averages the values in each bin (Figure 6.3). By separating the data into these bins, we preserve and emphasize relevant outliers.\n\nOnce the clinician finds an event of interest, they can click on that action event to focus exclusively on data for that event (Figure 7.3). Two range slidersâ€”a global slider with time relative to the entire capture and a local slider with time relative to the specified eventâ€”control the time segment in the temporal heatmap. The global slider can expand the Timeline view to show what happened before or after the selected time segment, as well as show where the selected time segment is with respect to the entire dataset. The local slider enables more precise navigation for zooming into the data. The slider also presents the start and end time to contextualize when in the day this happened. Once a clinician finds a moment they want to study further, the clinician can use the play/pause buttons or play slider to fetch video and reconstruction data to play in the Detailed View or Immersive Replay to explore an event in context (Task 3).\n\n6.4. Detailed View\n\nThe Detailed View presents statistical metrics and video replays for selected data. Based on clinician feedback, weight shift balance is a critical measure that can indicate when patients favor a particular side and potentially dangerous imbalances that can lead to a fall. The Weight Shift metric provides an average weight shift side, weight shift measures, and a text-based description of the weight shift. While access to specific measures provides precise comparisons, clinicians indicated that the raw measures could be difficult to interpret. We added text descriptions (e.g., â€strong right,â€ â€slight left,â€ or â€balancedâ€) to support this analysis.\n\nSome metrics are more useful based on the selected time segment(s), so the Detailed View displays different metrics based on the current selection. For example, when no action is selected, clinicians can look at the overall percentage of time sitting, which is useful since large periods of inactivity can encourage motor decline. When the clinician selects an action, the Duration of Time metric shows how much time was spent performing that action, which can shape interpretation. For example, two freezes in one hour is more concerning than two freezes across two days.\n\nWhen the clinician selects an action event to closely study, the Detailed View allows clinicians to replay 2D video data (Figure 8.2). Video data from our data capture provides ground truth to compare with the temporal heatmap to confirm if outliers are from motor deficits or errors in data collection or processing. Further, the 2D video segment can provide some context for interpreting motion variables, supporting Task 3. However, clinicians felt that the video replays only offered limited perspective into body movement. To support more detailed replay, clinicians can launch the Immersive Replay to explore the target movements.\n\n7. Immersive Replay\n\nTo contextualize motor deficits (Task 3), clinicians can interact with the Immersive Replay to study a reconstruction of a time segment of interest in an AR headset. Our Immersive Replay consists of a skeleton rendered using the body pose coordinates situated in a local environment reconstruction.\n\nWe use AR to display this reconstruction as the analytical workflows of AR mirror critical practices that clinicians use with patients during in-person therapies. Clinicians are accustomed to physically maneuvering and examining a patientâ€™s body in a clinical setting. AR allows clinicians to view a life-size reconstruction of the patient within the context of the environment. The clinician can then walk around the reconstruction to view the motion from multiple angles and deeply attend to movement biomechanics which directly inform therapies. Further, the enhanced depth cuing provided by AR rendering enhances cliniciansâ€™ abilities to reason about body position compared to viewing desktop-based reconstructions (Heinrich et al., 2021).\n\nClinicians move back and forth between the dashboard and reconstruction without having to remove the headset (see Â§7.3). Our reconstruction pipeline can adapt to various privacy needs by using wireframe skeletons and allowing people to turn off the environmental context when necessary.\n\n7.1. Body Reconstruction\n\nThe patientâ€™s body is reconstructed using a simple skeleton. We use this design rather than a more photorealistic reconstruction because it is visually simpler, provides additional patient privacy, decreases occlusions, enables situated visualizations of relevant body metrics (e.g., motion vectors), and requires less memory. We colored the skeleton bright red to distinguish the body from most naturally-occurring environments.\n\nWe visualize body variables using arrows at relevant coordinates to show the associated displacements in physical space. Arrow magnitude and direction correspond to the magnitude and direction of the body variable at that frame. The colors of the arrows match the colors of the body variables in the Overview Dashboard to connect the Immersive Replay with the studied body variables.\n\n7.2. Environment Reconstruction\n\nWe situate the skeleton within an immersive reconstruction of the environment extracted from video data (see Â§5.2.1). PD-Insighter centers the reconstruction on the skeleton, showing a 1.5 meter radius window of the environment around the body. Original iterations of the Immersive Replay involved the body reconstruction moving in a stationary environment. However, clinicians noted that any analysis would likely happen in their office before a patient visit. The clutter and limited space of the clinicianâ€™s office inhibit walking around large reconstructed environments, whereas centering on a smaller local window means clinicians can more easily walk around the local reconstructed environment to study contextualized motion patterns in detail.\n\n7.3. Head-Mounted Display\n\nPD-Insighter uses the HoloLens 2 headset to enable clinicians to seamlessly transition between the Overview Dashboard and Immersive Replay by shifting their gaze between the desktop display and the immersive reconstruction. For easier mobility, clinicians can view the Dashboard on a laptop and physically move around the immersive reconstruction to look at the patientâ€™s movements from multiple views. Early iterations of the system used Oculus Quest 2 to display both the dashboard and reconstruction, but clinicians found using the device controls for interaction cumbersome. We switched to see-through AR to allow clinicians to use the desktop to interact with the Overview Dashboard and provide more comfortable physical interaction with the Immersive Replay.\n\n8. Evaluation\n\nWe evaluated PD-Insighter through a think-aloud study with six rehabilitation specialists. Five participants had direct experience with PD, while the sixth worked with people with stroke facing similar mobility challenges. Two therapists had also served as expert collaborators during early design iterations, whereas four had not seen the system before. For transparency, we label data from early-phase collaborators with C1 and C2, and other participants with P3, P4, P5, and P6. Drawing on our task analysis, we focused our evaluation on PD-Insighterâ€™s abilities to support 1) overall understanding of a patientâ€™s motor capabilities, 2) finding specific moments of interest that correspond to potential motor deficits, and 3) studying these moments of interest with context.\n\nThe therapists used PD-Insighter to analyze a 50-minute dataset consisting of four capture segments containing the seven key actions from our task analysis. During data collection, we intentionally captured a diverse range of relevant deficits through the research team and patients with PD performing these actions. We envision a future with potentially days or weeks worth of data, but current technologies limit that capability at present. We use a 50-minute composited capture as it reflects likely data composition in practice. Current captures are often done in-lab over multiple visits. Patients at home will likely only record data in segments to avoid fatigue from wearable devices, to recharge the battery of devices, and to protect privacy during sensitive actions like using the restroom. While PD-Insighter can support analysis with longer captures, participants noted that 50 minutes of data was long enough to understand general movement patterns and include a range of key specific movements, such as falls and freezes, for evaluation.\n\nFirst, we introduced the system by describing the toolâ€™s basic functionality and body variables. We then asked participants to put on the HoloLens 2, analyze data to identify potential motor deficits that might influence treatment, and make general observations about the patientâ€™s movement patterns. We asked participants to describe their observations, intentions, and decisions as they worked with the tool. We gathered additional feedback in an exit interview. Each session approximately lasted one hour.\n\n8.1. Overall Movement Patterns\n\nAll participants engaged with the opening Action Summary and Timeline (Figure 7.1), which C1 described as â€œnecessary and grounding,â€ and would frequently return to after studying an event. C2 and P3 used the Action Summary bar chart and the Percentage of Time Sitting metric to see if the patient was active or inactive. P6 thought the Weight Shift statistic would be helpful because â€œif you donâ€™t have time, you can really quickly see a snapshot that tells how [the patient] has been doing.â€ P3 remarked that the feature would be helpful for clinical analysis since â€œthe amount of sitting tells a lot about how active someone was, which is helpful even for stroke patients, not just Parkinsonâ€™s.â€\n\nParticipants also focused on using the body variable distributions to identify patterns in motion across actions. C1 and C2 were interested in whether the patient used both arms or just one arm while reaching because â€œTypically in Parkinsonâ€™s, there can be an asymmetric representation [between arms] because they might be compensating. [For example,] if there is a tremor, they could be hesitating to use that handâ€ (C1). By studying the Arm Use heatmaps for reaching events, C1 and C2 both found that there were several reaching events with single dominant hand and bi-manual reaching, indicating â€œno concernâ€ (C1; Figure 10.1).\n\nThe Action Timeline was â€œuseful for thinking of the context of what [the patient] was doing through the day. When are they taking medicine? Are they walking for shorter or longer?â€ (C1). P4 also remarked â€œIt is helpful to know at what point they are taking medicine because it has a huge impact on their motor function. It takes about an hour for medicine to kick in.â€\n\n8.2. Finding Moments of Interest\n\nAll participants used the Select Action feature when initiating a new line of inquiry. Once participants picked an action, they chose body variables of interest to display. Participants frequently remarked how various actions impacted their expectations for the body variables. For example, â€œhigh trunk is concerning for walking, but not for reachingâ€ (C2), and â€œArm use while walking will be different than arm use while reachingâ€ (C1).\n\nParticipants commonly focused on events with dark-colored outliers to find specific instances of motor deficits. For example, C1, C2, P3, and P6 were interested in moments with imbalanced arm swing while walking since â€œswinging with the left arm and not the right arm is characteristic of Parkinsonâ€™sâ€ (C1). When they selected Walking, Arm Use, and the Imbalanced Arm Use filters, participants were interested in instances where the right arm heatmap was close to white, and the left arm heatmap had dark green bands (Figure 10.3), reflecting extended arm swing. Participants felt confident that imbalanced swings were occurring without viewing a replay.\n\nP3, P4, and P5 clicked on a Walking event with dark red bands indicating a high trunk angle and found a freeze (Figure 10.2) missed by the Potential Freeze filter. C2 clicked on a Standing event with a dark red band and found a fall. P6 clicked on a Sit-to-Stand event with dark a red band because â€œa lot of falls are happening from leaning too far forward.â€ C1 clicked on a Sit-to-Stand event with multiple dark red bands, which reflected an event where the patient was trying to lift their weight out of the chair and struggling (Figure 10.4). P3 and P4 used the blue heatmaps to determine weight shift imbalances. P4 remarked that â€œin general, the weight shift is more on the right than the left,â€ and was interested in this imbalance during walking events because it showed the patient â€œhaving a prolonged period of single limb stance.â€\n\nP5â€™s navigation was heavily driven by color contrast to find potential deficits, â€œReally what I am looking for, just for the sake of it, is a moment where one (heatmap) is darker than the other (heatmap).â€ When looking at Stand-to-Sit events, P5 had all body variables selected, saying, â€œI am going to click all of them until I can decide what I want to look for,â€ and chose the event with the darkest trunk angle. Then, P5 looked at the Foot Position and chose the event with the darkest purple. This behavior indicates PD-Insighterâ€™s ability to support data analysis based on unexpected and significant shifts in different parts of the body relative to others, highlighting outliers and potential deficits.\n\nParticipants also used time duration to find moments of interest. For example, C2 wanted to see if the patient abruptly fell in their seat, indicating a â€œfailure to react to a change in position.â€ C2 selected the Stand-to-Sit event with the shortest width and found the patient fell into their seat. P3 and P4 were interested in long Sit-to-Stand events indicated struggles standing up. P3 clicked on the More Than 5 Seconds filter to find longer events, whereas P4 looked at the width of the bars to â€œsee if they are getting better (at sitting down) over time.â€ Both participants clicked the same long Sit-to-Stand event that showed a struggle to stand up (Figure 10.4).\n\nFilters helped decrease the number of events shown and focus participantsâ€™ explorations. All therapists clicked on the Walking and Potential Freezes filters to try to find freezing gait patterns. PD-Insighter showed three different walking events, with light blue bars indicating potential freeze moments (Figure 7.2). By using the immersive replay to watch the reconstruction and see if there were body leans showing the patient trying to start walking but struggling, it took clinicians less than one minute to find that one of these events was a proper freeze.\n\n8.3. Contextualizing Moments of Interest\n\nWhile participants had access to the ground-truth video feed in the Detailed View, they primarily used the Immersive Replay for watching body motion because they felt â€œthe video is too constrainedâ€ (P3) due to the inability to move around the body and look around any occlusions. The replay in AR was helpful because â€œangles matter, I need to orient myself to see [body parts of interest]â€ (C1). All participants found the headsets easy to use with â€œno discomfortâ€ (P6).\n\nTherapists used the Immersive Replay to confirm or deny the existence of a motor deficit in an event of interest. For example, when studying standing events, C2 saw a dark red band while reaching and â€œsuspected it was just a normal reach since when reaching a high trunk angle is common.â€ However, the red was significantly darker than the other trunk leans while reaching, leading C2 to further investigate. When watching the reconstruction, they found the reach led to a fall. The reconstruction â€œhelped me see how hunched they were, that it was a fall and not just a reach.â€ While looking for freezing events, all participants used the reconstruction to determine if a freeze occurred. When they saw the skeleton leaning back and forth and shuffling before stopping, they accurately characterized the movement as a freeze. Once discovering the freeze, C1, C2, and P4 returned to the Overview Dashboard to study the event in more detail. For example, C1 wanted â€œto see if the trunk increased, which is indicative of more risk for falls.â€ In the Overview Dashboard, they looked at the trunk angle heatmap and found that the trunk â€œlooked higher but not substantiallyâ€ with respect to the rest of the data.\n\nParticipants felt that the arrows encoding body variables in the Immersive Replay provided helpful clarity. When watching an event with an imbalanced arm swing, C2 said, â€œThe arrows give help with the direction. I can see the arm swing is more backwards than forwards.â€ P4 said that â€œthe arrows make it clear that the arms are obviously not synchronizedâ€ which leads to checking, â€œAre they using their assistive device or furniture?â€ (Figure 11.1).\n\nTherapists also used the blue weight shift arrows for further analysis. For instance, â€œWhen [the patient is] turning while walking, I can see how the arrow changes to emphasize the weight bearing, and I can see more on one particular side.â€ (P3) (Figure 11.3). P4 liked this visibility because â€œturning is a time when people will fall. Itâ€™s a good thing [for the patient] to shift their weight while turning.â€ P4 used the weight shift arrows to study a freeze event to realize that the patient was â€œtrying to move one side but canâ€™t initiateâ€ (Figure 11.2). After finding this imbalance in the arrows, P4 went back to the heatmap and saw that during the freeze event, the right side was dark blue, and the left side was white, confirming that the patient was placing their weight on their right side to try to move forward.\n\nP5 used the weight shift arrows to study a Sit-to-Stand event, saying, â€œThereâ€™s some bias on the right side, so [the patient] will do something like this when they are trying to get up,â€ and acted out a swivel to demonstrate the incorrect way to stand up. P5 considered the advice they would give the patient â€œAre they building strength on that [less affected] side? Maybe we need to adjust that chair, add pillows, wedge a book under the rocking chair.â€\n\nThe environment provided participants with important context for analysis. When studying a Walking event, C1, C2, P3, P4, and P6 were curious about the high trunk leans in the heatmap. When watching the reconstruction in the Immersive Replay, C2 said, â€œThis shows me that they are furniture walking [walking while using furniture as an assistive aid]. Before, in the heatmap, it looked like they were walking weird. I would have thought they were about to fallâ€ (Figure 11.4). P4 remarked, â€œYeah, thatâ€™s definitely helpful to have the environment there. Otherwise, it would be like, what are the arms doing so far apart here?â€ when indicating a point of divergence between hand positions on the heatmap. C2 mentioned the importance of seeing the environment and furniture walking, â€œWhen a patient knows their center of balance will be disturbed, I want to see interaction with the furniture to help stabilize.â€\n\n9. Discussion\n\nPD-Insighter is a hybrid desktop-AR visual analytics system for physical therapists to analyze body motion data for patients with Parkinsonâ€™s Disease during activities of daily living. The system works towards a vision of processing long-term at-home body motion data to support better decision-making and diagnosis. Iterative interviews with clinicians elicited key tasks that shaped our system design. Here, we summarize preliminary outcomes from the implementation and evaluation of PD-Insighter to inform future work for studying body motion data in clinical therapy applications.\n\n9.1. Data Representation\n\nMost ADL visualizations represent data through activity level and location, which does not give the context necessary for identifying and studying PD motor deficits. We discovered that combining action and body variables can indicate motor deficits and body pattern changes. Clinicians noted that the action and body variables were necessary for a holistic picture of patient movement.\n\nBased on the range of patient needs, we developed action labels and body variables from existing clinical practices to assess patients. P6 appreciated the chosen action labels, â€These features are known from clinical observation research to be associated for risk of fall.â€ Different clinicians can use the tool based on their expertise or the needs of their patients. P4 stated, â€œI like having independence to look at what I want to look at based on what is important to me,â€ which can be â€œinfluenced by patientâ€™s needs and what they are reporting.â€ P6 described how actions of interest can also be influenced by personal specialty, â€œI am clinically more interested in walking, sit-to-stand, and stand-to-sit. How someone is sitting or standing is less relevant to me.â€\n\nOur evaluation showed that action labels and body variables successfully indicated motor deficits such as freeze events, imbalanced arm swing while walking, struggle standing up, and struggle sitting down. Clinicians successfully understood body motion in terms of these new body variables and could identify when the data was reflecting improper motion. While various data representations exist for body motion, both action and body data can work together to enable scalability across time and provide the necessary context for deeper analysis.\n\n9.2. Detecting & Emphasizing Outliers\n\nMost moments of clinical interest for PD body movement last only a few seconds. Finding these moments in an hour or more of data is time cost-prohibitive with direct replay: clinicians only have a few minutes to prepare for a patient visit. Emphasizing outliers in the data using dark, saturated colors helped quickly draw cliniciansâ€™ attention to potential deficits.\n\nAt the beginning of the evaluation, P3 thought that the color encodings would be challenging for PTs unfamiliar with this representation. However, within a few minutes, they found that â€œUsing [the colors] gets intuitive very quicklyâ€ and had no issue interpreting the data. P3 described the heatmap as â€œvery telling. I donâ€™t need additional information to see the weight shift balance, a quick glance tells which side bears more weight.â€ Participants found that the simplification feature made the heatmaps easier to use (P3, P4, P6). C2 found the body variable sliders helpful to â€œadjust the contrast [so that] itâ€™s easier to seeâ€ and the distributions helpful to â€œguide where to move the sliders.â€\n\nTask-aware aggregation methods, such as the simplify feature, can highlight these outliers without removing important information, which is helpful for large and noisy data. While several approaches for aggregation exist, we found that clinicians needed methods that both preserves and emphasizes outliers to ensure that important moments of motor deficit are not lost.\n\n9.3. Fault Tolerance & Filtering\n\nWhen we developed PD-Insighter, we used datasets with manual labels. To date no tools exist for automatically and reliably labeling the specific activities of interest and we believed that accurate action labeling was critical. While accuracy is desirable, we also discovered that PD-Insighter created a certain degree of fault tolerance in motion analyses. Actions or relationships between body variables support quick, coarse guidance toward events of interest, and verifying events is sufficiently quick that clinicians could readily confirm or dismiss events. For example, although our Freeze Event thresholds introduced several false positives, clinicians did not mind the inaccuracies because they could confirm the event in a few seconds by simply watching a replay.\n\nSalient misalignments in body variables helped clinicians capture significant instances that action labels may have missed. For example, participants quickly began looking for time spans in the heatmaps where body variables were high in one value and low in others, helping address potential false negatives in labeling. As clinicians worked with the tool, they generated heuristics, such as those for the Freeze Event filters, that gave coarse approximations of critical potential deficits and informed clinical thinking about the computational relationships between joints in different movements. This introspection may enable future metric development and on-the-fly filtering for working with biomechanical data.\n\nThe combined fault tolerance and developed mathematical intuitions about deficits exhibited in the evaluation indicate that approaches like PD-Insighter may support mixed-initiative action classification. Specifically, if an automated classifier embedded in PD-Insighter provides preliminary action labels, clinicians may confirm or deny events through their analysis. This confirmation can provide clinicians with valuable patient motion insights and classifiers with additional training data. Improving overall classification feedback and enabling customization for individual movement patterns has important potential for future research. Exploring the potential for such a system is important future work.\n\n9.4. Hybrid Interfaces for Motion Analysis\n\nWhile the abstract heatmaps helped clinicians discover that an unexpected motor pattern occurred, they typically cannot explain why the observed body motion change happened. Replays helped clinicians distinguish between motor deficits, inaccurate measures or heuristics, and potential triggers. All participants felt the video replay was inadequate for this analysis and preferred the up-close immersive reconstruction. Five of the participants walked around the model at least once during the analysis to look at the body from multiple angles. C2 emphasized the importance of having access to various perspectives by pointing out â€œItâ€™s hard to see someone leaning forward from a front view; I would want a side view, and here I can.â€ Clinicians study a patientâ€™s body motion in person, and AR recreated familiar experiences for retrospective analysis and delivered a better sense of the scale of the movements.\n\nThe body and environment reconstruction work together to provide the necessary context. AR provides crucial depth and scale to analyze body motion properly. Further, AR provides necessary contextual cues from the environment. Reflecting on a prior iteration of the system without the reconstruction, our collaborator C1 noted the replay was â€œmuch more helpful with the environment. I can see what they are doing and why they are doing it.â€ Clinicians used the replay to analyze object placement in reaching, walking with furniture assistance, and chair placement when sitting.\n\nThe functional gains from combining both interfaces outweighed the trade-offs associated with context switching between platforms. While clinicians found video replays in the dashboard inadequate, early iterations of the system experimented with situating the Overview Dashboard in a full VR reconstruction. However, clinicians had difficulty interacting with the data. With AR, no participants noted difficulties transitioning between the Overview and Replay. C1 described the interface as â€œfairly seamlessâ€; the reconstruction is â€œright there.â€ Despite having no prior experience with head-mounted displays, P6 described the AR headset as â€good, awesome, better than I expected,â€ because â€it does not prevent you from seeing other areas in the real world.â€\n\n10. Limitations & Future Work\n\nWhile PD-Insighterâ€™s current design works for a few hours of capture, such as those stitched together from a series of in-lab observations, we envision a near future where non-intrusive wearables are capable of capturing days worth of data for clinical analysis between visits (Zhang et al., 2023; Cha et al., 2021). Such scenarios will provide a more comprehensive listing of movements to be analyzed, creating new scalability challenges for working with motion data. We anticipate that our aggregation and filtering paradigms will support this analysis, especially as real-world captures are likely to be significantly less event-dense on average; however, future work should explore novel paradigms for task-aware aggregation, including those informed by mixed-initiative approaches as discussed in Section 9.3.\n\nIn our current system, we do not directly address privacy preservation for the patient beyond wireframing and environment toggles in the reconstruction. As the tool is designed for clinicians working directly with a patient, we assume the patient has given their permission for clinicians to collect and analyze their data. However, real-world deployments will introduce privacy concerns. While approaches like lateral-effect photodiodes (LEPDs) (Yang and Kim, 2002) can provide more privacy-aware capture, future research, especially in patient-driven methods, is necessary to better understand privacy needs, constraints, and agency. Methods to address privacy may include using privacy-aware approaches in data preprocessing, capturing data in only certain spaces for limited amounts of time, and the ability to blur faces in video captures.\n\nMore data will require automatic methods for labeling actions. Future iterations of action-driven analysis tools like PD-Insighter can use computer vision models for automatically labeling video data based on the action performed (Lin et al., 2022; Wang et al., 2019; Demrozi et al., 2023; Kolkar and V, 2023). Future tools could also integrate a more comprehensive array of body variables or even allow clinicians to define body variables during analysis. For example, step length and number of steps per second are valuable metrics for understanding gait (C1, C2, P3, P6). While PD-Insighterâ€™s design can readily be expanded to work with more variables, future work should develop measures for understanding and computing these variables to identify those of the greatest clinical relevance.\n\nPD-Insighter currently focuses on supporting data exploration by physical therapists. Ultimately, the physical therapist will want to communicate their findings, discuss challenging events, and provide suggestions to the patient. Therapists can experience difficulty relaying this feedback verbally to the patient. Hybrid spaces may help with PD rehabilitation by mediating communication with the patient. For example, patients can walk through their movement patterns in AR annotated with feedback from the clinician. Patient-centered interfaces may also help patients play a more active role in their treatment and retain greater agency over their data. We plan to address potential challenges in older patientsâ€™ data accessibility by consistently reviewing and adapting our design in future collaborations with patients. While such AR systems may be difficult to adapt in the present due to usability limitations, the ongoing advancement of AR hardware over time will enhance their feasibility.\n\nThese methods in PD-Insighter can shape tools in other domains interested in body motion data. Study participants noted direct applications of PD-Insighter for other motor conditions, such as stroke, or even general movement analysis, as in sports and motion training. While current movement visualization tools focus either on aggregate movements (Robben et al., [n. d.]; Mulvenna et al., 2011; Ploderer et al., 2016) or brief, focused snippets of motion (Rashidi and Cook, 2010; Boers et al., 2009), we anticipate that these techniques may serve as a foundation for other movement-based analysis tools.\n\n11. Conclusion\n\nClinicians currently have limited access to patientsâ€™ movement data outside the clinic, despite its significant potential for improving the treatment of motor deficiencies like those seen in Parkinsonâ€™s Disease. We designed PD-Insighter to enable insights into patientsâ€™ daily movements to inform clinical practices associated with physical therapy. In iterative consultations with physical therapists, we found that action and body variable data representation, emphasizing outliers, hybrid interfaces, and immersive replays with situated encodings enabled clinicians to find and analyze relevant motor deficits in body motion data. Our approach works towards a future where clinicians can remotely monitor patients to optimize personal treatments, dramatically improving clinician decision-making and slowing the functional decline of PD and other medical conditions.\n\nAcknowledgements.\n\nThe authors would like to thank Jim Mahaney for help with setting up a capture space with various pieces of furniture to simulate a home environment. This work was supported by the National Institutes of Health Award 1R01HD111074-01 and NSF IIS-2320920.\n\nReferences\n\n(1)\n\nnoa ([n. d.]) [n. d.]. RealityCapture - 3D Models from Photos and/or Laser Scans. https://www.capturingreality.com/?utm_source=GoogleAds&utm_medium=cpc&utm_campaign=general_Amerika&utm_source=GoogleSearch&utm_medium=Performance&utm_campaign=&utm_adgroup=&utm_content=&utm_term=realitycapture\n\nnoa (2023) 2023. zju3dv/EasyMocap. https://github.com/zju3dv/EasyMocap\n\nAlbers et al. (2011) Danielle Albers, Colin Dewey, and Michael Gleicher. 2011. Sequence Surveyor: Leveraging Overview for Scalable Genomic Alignment Visualization. IEEE Transactions on Visualization and Computer Graphics 17, 12 (Dec. 2011), 2392â€“2401. https://doi.org/10.1109/TVCG.2011.232\n\nAlizadeh et al. (2011) S Alizadeh, S Bakkes, Marije Kanis, M Rijken, and B. Krose. 2011. Telemonitoring for Assisted Living Residences: The Medical Specialistsâ€™ View. Proceedings of The National Academy of Sciences (PNAS) (Jan. 2011), 78.\n\nAlmeida and Lebold (2010) Q. J. Almeida and C. A. Lebold. 2010. Freezing of gait in Parkinsonâ€™s disease: a perceptual cause for a motor impairment? Journal of Neurology, Neurosurgery, and Psychiatry 81, 5 (May 2010), 513â€“518. https://doi.org/10.1136/jnnp.2008.160580\n\nAmboni et al. (2015) M. Amboni, F. Stocchi, G. Abbruzzese, L. Morgante, M. Onofrj, S. Ruggieri, M. Tinazzi, M. Zappia, M. Attar, D. Colombo, L. Simoni, A. Ori, P. Barone, and A. Antonini. 2015. Prevalence and associated features of self-reported freezing of gait in Parkinson disease: The DEEP FOG study. Parkinsonism & Related Disorders 21, 6 (June 2015), 644â€“649. https://doi.org/10.1016/j.parkreldis.2015.03.028\n\nAnderson et al. (2013) Fraser Anderson, Tovi Grossman, Justin Matejka, and George Fitzmaurice. 2013. YouMove: enhancing movement training with an augmented reality mirror. In Proceedings of the 26th annual ACM symposium on User interface software and technology. ACM, St. Andrews Scotland, United Kingdom, 311â€“320. https://doi.org/10.1145/2501988.2502045\n\nAyoade and Baillie (2014) Mobolaji Ayoade and Lynne Baillie. 2014. A novel knee rehabilitation system for the home. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, Toronto Ontario Canada, 2521â€“2530. https://doi.org/10.1145/2556288.2557353\n\nBoers et al. (2009) Nicholas Boers, David Chodos, Jianzhao Huang, Pawel Gburzynski, Ioanis Nikolaidis, and Eleni Stroulia. 2009. The smart condo: visualizing independent living environments in a virtual world. 3d International ICST Conference on Pervasive Computing Technologies for Healthcare (April 2009), 8. https://doi.org/10.4108/ICST.PERVASIVEHEALTH2009.6020\n\nBressa et al. (2022) Nathalie Bressa, Henrik Korsgaard, AurÃ©lien Tabard, Steven Houben, and Jo Vermeulen. 2022. Whatâ€™s the Situation with Situated Visualization? A Survey and Perspectives on Situatedness. IEEE Transactions on Visualization and Computer Graphics 28, 1 (Jan. 2022), 107â€“117. https://doi.org/10.1109/TVCG.2021.3114835\n\nBurpee and Lewek (2015) Jessica L. Burpee and Michael D. Lewek. 2015. Biomechanical gait characteristics of naturally occurring unsuccessful foot clearance during swing in individuals with chronic stroke. Clinical Biomechanics (Bristol, Avon) 30, 10 (Dec. 2015), 1102â€“1107. https://doi.org/10.1016/j.clinbiomech.2015.08.018\n\nCha et al. (2021) Young-Woon Cha, Husam Shaik, Qian Zhang, Fan Feng, Andrei State, Adrian Ilie, and Henry Fuchs. 2021. Mobile. Egocentric Human Body Motion Reconstruction Using Only Eyeglasses-mounted Cameras and a Few Body-worn Inertial Sensors. In 2021 IEEE Virtual Reality and 3D User Interfaces (VR). 616â€“625. https://doi.org/10.1109/VR50410.2021.00087\n\nChomiak et al. (2015) Taylor Chomiak, Fernando Vieira Pereira, Nicole Meyer, Natalie de Bruin, Lorelei Derwent, Kailie Luan, Alexandra Cihal, Lesley A. Brown, and Bin Hu. 2015. A new quantitative method for evaluating freezing of gait and dual-attention task deficits in Parkinsonâ€™s disease. Journal of Neural Transmission 122, 11 (Nov. 2015), 1523â€“1531. https://doi.org/10.1007/s00702-015-1423-3\n\nCorrell et al. (2011) Michael Correll, Subhadip Ghosh, David Oâ€™Connor, and Michael Gleicher. 2011. Visualizing virus population variability from next generation sequencing data. In 2011 IEEE Symposium on Biological Data Visualization (BioVis). 135â€“142. https://doi.org/10.1109/BioVis.2011.6094058\n\nDabbakuti and Kotnana (2012) J. R. K. Kumar Dabbakuti and Nalini Kotnana. 2012. Design and Implementation of Portable Health Monitoring system using PSoC Mixed Signal Array chip. https://www.semanticscholar.org/paper/Design-and-Implementation-of-Portable-Health-system-Dabbakuti-Kotnana/00da3c347e95b00a069e6c149dbb92b31aad9af3\n\nDeal et al. (2019) Linda S. Deal, Emuella Flood, Daniela E. Myers, Jacob Devine, and David L. Gray. 2019. The Parkinsonâ€™s Disease Activities of Daily Living, Interference, and Dependence Instrument. Movement Disorders Clinical Practice 6, 8 (Sept. 2019), 678â€“686. https://doi.org/10.1002/mdc3.12833\n\nDel Din et al. (2021) Silvia Del Din, Cameron Kirk, Alison J Yarnall, Lynn Rochester, and Jeffrey M Hausdorff. 2021. Body-worn sensors for remote monitoring of Parkinsonâ€™s disease motor symptoms: vision, state of the art, and challenges ahead. Journal of Parkinsonâ€™s disease 11, s1 (2021), S35â€“S47.\n\nDemrozi et al. (2023) Florenc Demrozi, Cristian Turetta, Fadi Al Machot, Graziano Pravadelli, and Philipp H. Kindt. 2023. A Comprehensive Review of Automated Data Annotation Techniques in Human Activity Recognition. https://doi.org/10.48550/arXiv.2307.05988\n\nDijkstra et al. (2021) Bauke W. Dijkstra, Moran Gilat, L. Eduardo CofrÃ© Lizama, Martina Mancini, Bruno Bergmans, Sabine M. P. Verschueren, and Alice Nieuwboer. 2021. Impaired Weight-Shift Amplitude in People with Parkinsonâ€™s Disease with Freezing of Gait. Journal of Parkinsonâ€™s Disease 11, 3 (Jan. 2021), 1367â€“1380. https://doi.org/10.3233/JPD-202370\n\nDorsey et al. (2016) E Ray Dorsey, Floris P Vlaanderen, Lucien JLPG Engelen, Karl Kieburtz, William Zhu, Kevin M Biglan, Marjan J Faber, and Bastiaan R Bloem. 2016. Moving Parkinson care to the home. Movement Disorders 31, 9 (2016), 1258â€“1262.\n\nDoyle et al. (2010) Julie Doyle, Catherine Bailey, Ben Dromey, and Cliodhna Ni Scanaill. 2010. BASE - An interactive technology solution to deliver balance and strength exercises to older adults. 4th International ICST Conference on Pervasive Computing Technologies for Healthcare (Jan. 2010), 5. https://doi.org/10.4108/ICST.PERVASIVEHEALTH2010.8881\n\nEns et al. (2021) Barrett Ens, Benjamin Bach, Maxime Cordeil, Ulrich Engelke, Marcos Serrano, Wesley Willett, Arnaud Prouzeau, Christoph Anthes, Wolfgang BÃ¼schel, Cody Dunne, Tim Dwyer, Jens Grubert, Jason H. Haga, Nurit Kirshenbaum, Dylan Kobayashi, Tica Lin, Monsurat Olaosebikan, Fabian Pointecker, David Saffo, Nazmus Saquib, Dieter Schmalstieg, Danielle Albers Szafir, Matt Whitlock, and Yalong Yang. 2021. Grand Challenges in Immersive Analytics. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (CHI â€™21). Association for Computing Machinery, New York, NY, USA, 1â€“17. https://doi.org/10.1145/3411764.3446866\n\nEulzer et al. (2022) Pepe Eulzer, Robert Rockenfeller, and Kai Lawonn. 2022. HAExplorer: Understanding Interdependent Biomechanical Motions with Interactive Helical Axes. In SIGCHI Conference on Human Factors in Computing Systems (CHI â€™22). Association for Computing Machinery, New York, NY, USA, 1â€“16. https://doi.org/10.1145/3491102.3501841\n\nFreeman et al. (2009) Dustin Freeman, Hrvoje Benko, Meredith Ringel Morris, and Daniel Wigdor. 2009. ShadowGuides: visualizations for in-situ learning of multi-touch and whole-hand gestures. In Proceedings of the ACM International Conference on Interactive Tabletops and Surfaces. ACM, Banff Alberta Canada, 165â€“172. https://doi.org/10.1145/1731903.1731935\n\nGarcia and Felix Navarro (2014) Jaime Garcia and Karla Felix Navarro. 2014. The Mobile RehApp(TM): an AR-based mobile game for ankle sprain rehabilitation. IEEE International Conference on Serious Games and Applications for Health (SeGAH) (May 2014). https://doi.org/10.1109/SeGAH.2014.7067087\n\nGil et al. (2007) Nubia Gil, Nick Hine, John Arnott, Julienne Hanson, Richard Curry, Telmo Amaral, and Dorota OsipoviÄ. 2007. Data visualisation and data mining technology for supporting care for older people. 146 pages. https://doi.org/10.1145/1296843.1296868\n\nGrabli et al. (2012) David Grabli, Carine Karachi, Marie-Laure Welter, Brian Lau, Etienne C. Hirsch, Marie Vidailhet, and Chantal FranÃ§ois. 2012. Normal and pathological gait: what we learn from Parkinsonâ€™s disease. Journal of Neurology, Neurosurgery, and Psychiatry 83, 10 (Oct. 2012), 979â€“985. https://doi.org/10.1136/jnnp-2012-302263\n\nHarrower and Brewer (2003) Mark Harrower and Cynthia A. Brewer. 2003. ColorBrewer.org: An Online Tool for Selecting Colour Schemes for Maps. The Cartographic Journal 40, 1 (June 2003), 27â€“37. https://doi.org/10.1179/000870403235002042\n\nHeinrich et al. (2021) Florian Heinrich, Vikram Apilla, Kai Lawonn, Christian Hansen, Bernhard Preim, and Monique Meuschke. 2021. Estimating depth information of vascular models: A comparative user study between a virtual reality and a desktop application. Computers & Graphics 98 (2021), 210â€“217.\n\nIrizarry et al. (2013) Javier Irizarry, Masoud Gheisari, Graceline Williams, and Bruce Walker. 2013. InfoSPOT: A mobile Augmented Reality method for accessing building information through a situation awareness approach. Automation in Construction 33 (Aug. 2013), 11â€“23. https://doi.org/10.1016/j.autcon.2012.09.002\n\nKao et al. (2016) Jiun-Yu Kao, Minh Nguyen, Luciano Nocera, Cyrus Shahabi, Antonio Ortega, Carolee Winstein, Ibrahim Sorkhoh, Yu-Chen Chung, Yi-An Chen, and Helen Bacon. 2016. Validation of Automated Mobility Assessment Using a Single 3D Sensor. Vol. 9914. https://doi.org/10.1007/978-3-319-48881-3_12\n\nKaplan et al. (2018) Oral Kaplan, Goshiro Yamamoto, Takafumi Taketomi, Yasuhide Yoshltake, Alexander Plopski, Christian Sandor, and Hirokazu Kato. 2018. Towards Situated Knee Trajectory Visualization for Self Analysis in Cycling. In 2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR). IEEE, Reutlingen, 595â€“596. https://doi.org/10.1109/VR.2018.8446212\n\nKeefe et al. (2009) D. Keefe, M. Ewert, W. Ribarsky, and R. Chang. 2009. Interactive Coordinated Multiple-View Visualization of Biomechanical Motion Data. IEEE Transactions on Visualization and Computer Graphics 15, 6 (Nov. 2009), 1383â€“1390. https://doi.org/10.1109/TVCG.2009.152\n\nKloiber et al. (2020) Simon Kloiber, Volker Settgast, Christoph Schinko, Martin Weinzerl, Johannes Fritz, Tobias Schreck, and Reinhold Preiner. 2020. Immersive analysis of user motion in VR applications. The Visual Computer 36, 10 (Oct. 2020), 1937â€“1949. https://doi.org/10.1007/s00371-020-01942-1\n\nKolkar and V (2023) Ranjit Kolkar and Geetha V. 2023. Human Activity Behavioural Pattern Recognition in Smarthome with Long-hour Data Collection. http://arxiv.org/abs/2306.13374\n\nKraus et al. (2021) Matthias Kraus, Karsten Klein, Johannes Fuchs, Daniel Keim, Falk Schreiber, Michael Sedlmair, and Theresa-Marie Rhyne. 2021. The Value of Immersive Visualization. IEEE computer graphics and applications 41, 4 (2021), 125â€“132. https://doi.org/10.1109/MCG.2021.3075258\n\nLee et al. (2021) Benjamin Lee, Dave Brown, Bongshin Lee, Christophe Hurter, Steven Drucker, and Tim Dwyer. 2021. Data Visceralization: Enabling Deeper Understanding of Data Using Virtual Reality. IEEE Transactions on Visualization and Computer Graphics 27, 2 (Feb. 2021), 1095â€“1105. https://doi.org/10.1109/TVCG.2020.3030435\n\nLewek et al. (2010) Michael D. Lewek, Roxanne Poole, Julia Johnson, Omar Halawa, and Xuemei Huang. 2010. Arm swing magnitude and asymmetry during gait in the early stages of Parkinsonâ€™s disease. Gait & Posture 31, 2 (Feb. 2010), 256â€“260. https://doi.org/10.1016/j.gaitpost.2009.10.013\n\nLin et al. (2021) Tica Lin, Rishi Singh, Yalong Yang, Carolina Nobre, Johanna Beyer, Maurice A. Smith, and Hanspeter Pfister. 2021. Towards an Understanding of Situated AR Visualization for Basketball Free-Throw Training. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 1â€“13. https://doi.org/10.1145/3411764.3445649\n\nLin et al. (2022) Xudong Lin, Fabio Petroni, Gedas Bertasius, Marcus Rohrbach, Shih-Fu Chang, and Lorenzo Torresani. 2022. Learning To Recognize Procedural Activities with Distant Supervision. https://doi.org/10.48550/arXiv.2201.10990\n\nLuo et al. (2023) Weizhou Luo, Zhongyuan Yu, Rufat Rzayev, Marc Satkowski, Stefan Gumhold, Matthew McGinity, and Raimund Dachselt. 2023. PEARL: Physical Environment based Augmented Reality Lenses for In-Situ Human Movement Analysis. https://doi.org/10.1145/3544548.3580715\n\nMahoney and Barthel (1965) F. I. Mahoney and D. W. Barthel. 1965. Functional Evaluation: The Barathel Index. Maryland State Medical Journal 14 (Feb. 1965), 61â€“65.\n\nMalche et al. (2022) Timothy Malche, Sumegh Tharewal, Pradeep Kumar Tiwari, Mohamed Yaseen Jabarulla, Abeer Ali Alnuaim, Wesam Atef Hatamleh, and Mohammad Aman Ullah. 2022. Artificial Intelligence of Things- (AIoT-) Based Patient Activity Tracking System for Remote Patient Monitoring. Journal of Healthcare Engineering 2022 (March 2022), 1â€“15. https://doi.org/10.1155/2022/8732213\n\nMarriott et al. (2018) Kim Marriott, Falk Schreiber, Tim Dwyer, Karsten Klein, Nathalie Henry Riche, Takayuki Itoh, Wolfgang Stuerzlinger, and Bruce H. Thomas (Eds.). 2018. Immersive Analytics. Lecture Notes in Computer Science, Vol. 11190. Springer International Publishing, Cham. https://doi.org/10.1007/978-3-030-01388-2\n\nMoustafa et al. (2016) Ahmed A. Moustafa, Srinivasa Chakravarthy, Joseph R. Phillips, Ankur Gupta, Szabolcs Keri, Bertalan Polner, Michael J. Frank, and Marjan Jahanshahi. 2016. Motor symptoms in Parkinsonâ€™s disease: A unified framework. Neuroscience and Biobehavioral Reviews 68 (Sept. 2016), 727â€“740. https://doi.org/10.1016/j.neubiorev.2016.07.010\n\nMulvenna et al. (2011) Maurice Mulvenna, William Carswell, Paul Mccullagh, Juan Augusto, Huiru Zheng, Paul Jeffers, Haiying Wang, and Suzanne Martin. 2011. Visualization of data for ambient assisted living services. IEEE Communications Magazine 49, 1 (Jan. 2011), 110â€“117. https://doi.org/10.1109/MCOM.2011.5681023\n\nNguyen et al. (2017) Minh Nguyen, Zaki Hasnain, Ming Li, Tanya Dorff, David Quinn, Sanjay Purushotham, Luciano Nocera, Paul Newton, Peter Kuhn, Jorge Nieva, and Cyrus Shahabi. 2017. Mining Human Mobility to Quantify Performance Status. 1177 pages. https://doi.org/10.1109/ICDMW.2017.168\n\nPlaisant et al. (2003) Catherine Plaisant, Richard Mushlin, Aaron Snyder, Jia Li, Dan Heller, and Ben Shneiderman. 2003. LifeLines: Using Visualization to Enhance Navigation and Analysis of Patient Records. In The Craft of Information Visualization. Morgan Kaufmann, San Francisco, 308â€“312. https://doi.org/10.1016/B978-155860915-0/50038-X\n\nPloderer et al. (2016) Bernd Ploderer, Justin Fong, Marlena Klaic, Siddharth Nair, Frank Vetere, L. Eduardo CofrÃ© Lizama, and Mary Pauline Galea. 2016. How Therapists Use Visualizations of Upper Limb Movement Information From Stroke Patients: A Qualitative Study With Simulated Information. JMIR Rehabilitation and Assistive Technologies 3, 2 (Oct. 2016), e9. https://doi.org/10.2196/rehab.6182\n\nRamesh et al. (2012) Maneesha V Ramesh, Sruthy Anand, and P. Rekha. 2012. A mobile software for health professionals to monitor remote patients. Ninth International Conference on Wireless and Optical Communications Networks (WOCN) (Sept. 2012), 1â€“4. https://doi.org/10.1109/WOCN.2012.6335565\n\nRashidi and Cook (2010) Parisa Rashidi and Diane J. Cook. 2010. Mining and monitoring patterns of daily routines for assisted living in real world settings. In Proceedings of the 1st ACM International Health Informatics Symposium. ACM, Arlington Virginia USA, 336â€“345. https://doi.org/10.1145/1882992.1883040\n\nReipschlÃ¤ger et al. (2022) Patrick ReipschlÃ¤ger, Frederik Brudy, Raimund Dachselt, Justin Matejka, George Fitzmaurice, and Fraser Anderson. 2022. AvatAR: An Immersive Analysis Environment for Human Motion Data Combining Interactive 3D Avatars and Trajectories. In CHI Conference on Human Factors in Computing Systems. ACM, New Orleans LA USA, 1â€“15. https://doi.org/10.1145/3491102.3517676\n\nRobben et al. ([n. d.]) Saskia Robben, Mario Boot, Marije Kanis, and Ben Krose. [n. d.]. Identifying and Visualizing Relevant Deviations in Longitudinal Sensor Patterns for Care Professionals. ([n. d.]).\n\nRogers et al. (2019) Jen Rogers, Nicholas Spina, Ashley Neese, Rachel Hess, Darrel Brodke, and Alexander Lex. 2019. Composerâ€”Visual Cohort Analysis of Patient Outcomes. Applied Clinical Informatics 10, 02 (March 2019), 278â€“285. https://doi.org/10.1055/s-0039-1687862\n\nSedlmair et al. (2012) Michael Sedlmair, Miriah Meyer, and Tamara Munzner. 2012. Design Study Methodology: Reflections from the Trenches and the Stacks. IEEE Transactions on Visualization and Computer Graphics 18, 12 (Dec. 2012), 2431â€“2440. https://doi.org/10.1109/TVCG.2012.213\n\nSmart et al. (2019) Stephen Smart, Keke Wu, and Danielle Albers Szafir. 2019. Color Crafting: Automating the Construction of Designer Quality Color Ramps. http://arxiv.org/abs/1908.00629\n\nStone et al. (2002) Arthur A. Stone, Saul Shiffman, Joseph E. Schwartz, Joan E. Broderick, and Michael R. Hufford. 2002. Patient non-compliance with paper diaries. BMJ (Clinical research ed.) 324, 7347 (May 2002), 1193â€“1194. https://doi.org/10.1136/bmj.324.7347.1193\n\nTang et al. (2015) Richard Tang, Xing-Dong Yang, Scott Bateman, Joaquim Jorge, and Anthony Tang. 2015. Physio@Home: Exploring Visual Guidance and Feedback Techniques for Physiotherapy Exercises. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems. ACM, Seoul Republic of Korea, 4123â€“4132. https://doi.org/10.1145/2702123.2702401\n\nWang et al. (2022) Qianwen Wang, Tali Mazor, Theresa A Harbig, Ethan Cerami, and Nils Gehlenborg. 2022. ThreadStates: State-based Visual Analysis of Disease Progression. IEEE Transactions on Visualization and Computer Graphics 28, 1 (Jan. 2022), 238â€“247. https://doi.org/10.1109/TVCG.2021.3114840\n\nWang et al. (2019) Yang Wang, Vinh Tran, Gedas Bertasius, Lorenzo Torresani, and Minh Hoai. 2019. Attentive Action and Context Factorization. British Machine Vision Virtual Conference (April 2019). https://doi.org/10.48550/arXiv.1904.05410\n\nWang et al. (2023) Zeyu Wang, Cuong Nguyen, Paul Asente, and Julie Dorsey. 2023. PointShopAR: Supporting Environmental Design Prototyping Using Point Cloud in Augmented Reality. Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (2023). https://doi.org/10.1145/3544548.3580776\n\nWeber et al. (2021) Griffin M Weber, Harrison G Zhang, Sehi Lâ€™Yi, Clara-Lea Bonzel, Chuan Hong, Paul Avillach, Alba GutiÃ©rrez-SacristÃ¡n, Nathan P Palmer, Amelia Li Min Tan, Xuan Wang, William Yuan, Nils Gehlenborg, Anna Alloni, Danilo F Amendola, Antonio Bellasi, Riccardo Bellazzi, Michele Beraghi, Mauro Bucalo, Luca Chiovato, Kelly Cho, Arianna Dagliati, Hossein Estiri, Robert W Follett, Noelia GarcÃ­a Barrio, David A Hanauer, Darren W Henderson, Yuk-Lam Ho, John H Holmes, Meghan R Hutch, Ramakanth Kavuluru, Katie Kirchoff, Jeffrey G Klann, Ashok K Krishnamurthy, Trang T Le, Molei Liu, Ne Hooi Will Loh, Sara Lozano-Zahonero, Yuan Luo, Sarah Maidlow, Adeline Makoudjou, Alberto Malovini, Marcelo Roberto Martins, Bertrand Moal, Michele Morris, Danielle L Mowery, Shawn N Murphy, Antoine Neuraz, Kee Yuan Ngiam, Marina P Okoshi, Gilbert S Omenn, Lav P Patel, Miguel Pedrera JimÃ©nez, Robson A Prudente, Malarkodi Jebathilagam Samayamuthu, Fernando J Sanz Vidorreta, Emily R Schriver, Petra Schubert, Pablo Serrano Balazote, Byorn Wl Tan, Suzana E Tanni, Valentina Tibollo, Shyam Visweswaran, Kavishwar B Wagholikar, Zongqi Xia, Daniela ZÃ¶ller, The Consortium For Clinical Characterization Of COVID-19 By EHR (4CE), Isaac S Kohane, Tianxi Cai, Andrew M South, and Gabriel A Brat. 2021. International Changes in COVID-19 Clinical Trajectories Across 315 Hospitals and 6 Countries: Retrospective Cohort Study (Preprint). preprint. Journal of Medical Internet Research. https://doi.org/10.2196/preprints.31400\n\nWhitlock et al. (2020) Matt Whitlock, Stephen Smart, and Danielle Albers Szaï¬r. 2020. Graphical Perception for Immersive Analytics. IEEE Conference on Virtual Reality and 3D User Interfaces (VR) (May 2020). https://doi.org/10.1109/VR46266.2020.00084\n\nWhitlock et al. (2019) Matt Whitlock, Keke Wu, and Danielle Szafir. 2019. Designing for Mobile and Immersive Visual Analytics in the Field. http://arxiv.org/abs/1908.00680\n\nWillett et al. (2017) Wesley Willett, Yvonne Jansen, and Pierre Dragicevic. 2017. Embedded Data Representations. IEEE Transactions on Visualization and Computer Graphics 23, 1 (Jan. 2017), 461â€“470. https://doi.org/10.1109/TVCG.2016.2598608\n\nYang and Kim (2002) Ungyeon Yang and Gerard Kim. 2002. Implementation and Evaluation of â€œJust Follow Meâ€: An Immersive, VR-Based, Motion-Training System. Presence 11 (June 2002), 304â€“323. https://doi.org/10.1162/105474602317473240\n\nZacks et al. (1998) Jeff Zacks, Ellen Levy, Barbara Tversky, and Diane Schiano. 1998. Reading Bar Graphs: Effects of Extraneous Depth Cues and Graphical Context. Journal of Experimental Psychology: Applied 4 (June 1998), 119â€“138. https://doi.org/10.1037/1076-898X.4.2.119\n\nZhang et al. (2023) Qian Zhang, Akshay Paruchuri, YoungWoon Cha, Jiabin Huang, Jade Kandel, Howard Jiang, Adrian Ilie, Andrei State, Danielle Albers Szafir, Daniel Szafir, and Henry Fuchs. 2023. Reconstruction of Human Body Pose and Appearance Using Body-Worn IMUs and a Nearby Camera View for Coll"
    }
}
{
    "id": "dbpedia_1009_2",
    "rank": 90,
    "data": {
        "url": "https://arxiv.org/html/2407.09733v1",
        "read_more_link": "",
        "language": "en",
        "title": "Textured-GS: Gaussian Splatting with Spatially Defined Color and Opacity",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://arxiv.org/html/extracted/5728779/figures/Gaussian_views.png",
            "https://arxiv.org/html/extracted/5728779/figures/Gaussian_color_views.png",
            "https://arxiv.org/html/extracted/5728779/figures/Gaussian_color_opacity_view.png",
            "https://arxiv.org/html/extracted/5728779/figures/Gaussian_Intersection.jpg",
            "https://arxiv.org/html/extracted/5728779/figures/Comparison_Train.png",
            "https://arxiv.org/html/extracted/5728779/figures/Comparison_Room.jpg",
            "https://arxiv.org/html/extracted/5728779/figures/Comparison_Bonsai.png",
            "https://arxiv.org/html/extracted/5728779/figures/NumVsMetrics.png",
            "https://arxiv.org/html/extracted/5728779/figures/Details_GT.png",
            "https://arxiv.org/html/extracted/5728779/figures/Details_3DGS.png",
            "https://arxiv.org/html/extracted/5728779/figures/Details_Mini-Splatting.png",
            "https://arxiv.org/html/extracted/5728779/figures/Details_Ours.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Zhentao Huang\n\nUniversity of Guelph\n\nSchool of Computer Science\n\nzhentao@uoguelph.ca Minglun Gong\n\nUniversity of Guelph\n\nSchool of Computer Science\n\nminglun@uoguelph.ca\n\nAbstract\n\nIn this paper, we introduce Textured-GS, an innovative method for rendering Gaussian splatting that incorporates spatially defined color and opacity variations using Spherical Harmonics (SH). This approach enables each Gaussian to exhibit a richer representation by accommodating varying colors and opacities across its surface, significantly enhancing rendering quality compared to traditional methods. To demonstrate the merits of our approach, we have adapted the Mini-Splatting architecture to integrate textured Gaussians without increasing the number of Gaussians. Our experiments across multiple real-world datasets show that Textured-GS consistently outperforms both the baseline Mini-Splatting and standard 3DGS in terms of visual fidelity. The results highlight the potential of Textured-GS to advance Gaussian-based rendering technologies, promising more efficient and high-quality scene reconstructions.\n\n1 Introduction\n\nNovel view synthesis is an active topic in Computer Vision and Graphics, focusing on generating accurate and realistic views from sparse input images with known camera parameters. Neural radiance fields (NeRF) [16] introduce a method to model a 3D scene by optimizing a continuous volumetric scene function through a fully connected deep network. This approach leverages implicit neural representations, extracting features like colors and geometry from a 3D scene by querying simple multi-layer perceptron networks with 5D inputs (spatial locations and camera views). The impressive realism achieved in free-viewpoint rendering by this learned mapping has sparked a surge of follow-up methods aimed at improving quality and speed, often incorporating regularization strategies. Notable examples include Mip-NeRF360 [1] and Zip-NeRF [2], which which excel in rendering quality, though they still require substantial training and rendering times. InstantNGP [18] enhances efficiency with a multiresolution hash encoding and a streamlined network architecture, reducing both training duration and computational demands during inference. Nonetheless, there are scenarios where NeRFâ€™s rendering capability might not fully capture detailed scene elements, particularly in complex light interactions and reflections. Furthermore, NeRF-based methods often struggle to accurately represent empty spaces.\n\nMore recently, 3D Gaussian Splatting (3DGS) [11] has been introduced as a method for real-time rendering that significantly accelerates both the rendering process and scene optimization through a set of optimized 3D Gaussians, efficiently rasterizable on modern GPUs. Not only faster, its rendering quality matches or even surpasses that of leading NeRF implementations. Scenes are represented by millions of 3D Gaussians with specified position, rotation, scale, opacity, and color parameters, requiring significant storage and memory, rendering it impractical on devices with limited video memory like smartphones or head-mounted displays.\n\nTo address storage issues, Niedermayr et al. [20] have shown that Spherical Harmonic (SH) coefficients can be considerably redundant, proposing their compression into compact codebooks. Fang and Wang [5] point out the inefficient spatial distribution of Gaussian representations as a key bottleneck in rendering performance. However, a Gaussian represented with SH coefficients exhibits only a single color from a specific viewing angle, limiting its representational capability.\n\nOur objective is to enhance the textural representation of individual Gaussians to model color variations locally, even from a single viewing direction. By maintaining the existing SH framework and parameter allocation, we modify the parameterization scheme. This adjustment allows each 3D Gaussian to display different colors under various viewing angles and across different areas of the Gaussian ellipsoidal surface when viewed from the same angle. Additionally, we integrate an opacity channel into the SH framework, enabling the modeling of opacity variations along the Gaussian surface, thereby enriching visual complexity and increasing the realism of rendered scenes.\n\nIn summary, our work presents several key contributions:\n\nâ€¢\n\nWe introduce a novel method that textures Gaussian ellipsoidal surfaces using SH, increasing the representational power of individual Gaussians without adding extra parameters.\n\nâ€¢\n\nWe extend this technique to model opacity variations on Gaussian ellipsoidal surfaces, allowing for deviations from the standard ellipsoidal shape. This includes creating â€œuni-directional Gaussiansâ€ that can be opaque on one side and transparent on the other.\n\nâ€¢\n\nWe validated our approach by applying it to 3D Gaussian scenes pre-optimized using Mini-splatting [5]. Our method achieves state-of-the-art rendering quality on several established datasets while requiring significantly fewer Gaussians compared to standard 3D Gaussian Splatting (3DGS) [11].\n\n2 Related Work\n\n2.1 Novel View Synthesis\n\nNovel view synthesis (NVS) has emerged as a pivotal technique in computer graphics and vision, facilitating the generation of new perspectives from a sparse set of images through advanced modeling of 3D scenes. Among the significant advancements in this domain, Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) represent transformative approaches that have reshaped our capabilities for rendering complex, photorealistic scenes efficiently.\n\nIntroduced by Mildenhall et al. [16], Neural Radiance Fields (NeRF) leverage a fully-connected neural network to model a volumetric scene function continuously, enabling precise capture of color and density throughout 3D space. NeRF generates new perspectives with notable detail and realism by processing these predictions through a differentiable rendering framework along the viewerâ€™s line of sight. Despite its impressive output quality, NeRFâ€™s main drawback is its computational intensity and slow rendering times, which have prompted numerous subsequent studies aimed at overcoming these challenges [7, 3, 18, 6, 15, 22, 13].\n\nKey advancements in this area include Instant-NGP [18], which employs a neural hash grid to markedly decrease computational load while sustaining output quality. Plenoxels [6] utilize a sparse, learnable grid that eschews traditional neural networks, significantly expediting both training and inference. Mip-NeRF 360 [1] focuses on improving renderings within 360-degree environments, tackling the challenges associated with view variability and inconsistent lighting. Furthermore, Zip-NeRF [2] innovates on network compression methods to enhance rendering fidelity without sacrificing the quality of the produced images.\n\n2.2 3D Gaussian Splatting Compression\n\nDeveloped by Kerbl et al. as an efficient alternative to traditional volume rendering technuques, 3DGS [11] represents scene through a sparse set of 3D Gaussians. Each Gaussian in this representation is characterized by its position, color, and covariance matrix. 3DGS stands out due to its differentiable rendering process, allowing it to be optimized directly from photometric observations. This technique not only speeds up the rendering process but also enhances the adaptability of the representation, making it particularly effective for real-time applications such as VR and AR where rapid rendering is crucial. Although the 3D Gaussian Splatting achieves real-time rendering, there is improvement space in terms of lower computational requirements and better point distribution [23].\n\nVarious 3DGS compression techniques employ vector quantization to cluster multi-dimensional data into a finite set of representations [14, 19, 20, 8, 4]. Specifically, Niedermayr et al. [20] utilizes vector clustering to compactly encode color and geometric attributes into two codebooks to reduce redundancy. Similarly, the EAGLES [8] applies quantization across all attributes of each Gaussian and show that the quantization of opacity leads to fewer floaters or visual artifacts. However, these techniques often do not address the suboptimal distribution of Gaussians, which tends to result in local minima following compression.\n\nRather than simply compressing existing 3D Gaussian Splatting systems, several initiatives aim to improve Gaussian distribution [5, 4, 17, 10]. LightGaussian [4] reduces the number of Gaussians by pruning those with lower importance scores and employs an octree-based method for compressing positions. Jo et al. [10] developed a strategy to both compress 3DGS and enhance computational efficiency by eliminating non-essential Gaussians. Meanwhile, Mini-Splatting [5] enhances the rendering process by utilizing more effective Gaussian splats. This method samples Gaussians based on importance score instead of pruning them to avoid artifacts.\n\n3 3D Gaussian Splatting Preliminaries\n\n3D Gaussian Splatting (3DGS) leverages the principles of Elliptical Weighted Average (EWA) volume splatting [24] as the foundational technique for the efficient computation of 3D Gaussian kernel projections onto a 2D image plane. Building upon this, the method employs differentiable Gaussian splatting [11] to further refine the rendering process. This approach enables dynamic optimization of both the amount and the specific parameters of Gaussian kernels used for scene modeling.\n\nThe optimized scene consists of millions of 3D Gaussians, each defined by a set of parameters: ğ’™âˆˆâ„3ğ’™superscriptâ„3\\bm{x}\\in\\mathbb{R}^{3}bold_italic_x âˆˆ blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT for center position, ğ’’âˆˆâ„4ğ’’superscriptâ„4\\bm{q}\\in\\mathbb{R}^{4}bold_italic_q âˆˆ blackboard_R start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT for the quaternion that represents the rotation, ğ’”âˆˆâ„3ğ’”superscriptâ„3\\bm{s}\\in\\mathbb{R}^{3}bold_italic_s âˆˆ blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT for the scale of the Gaussian in each dimension, Î±âˆˆ[0,1]ğ›¼01\\alpha\\in[0,1]italic_Î± âˆˆ [ 0 , 1 ] for the opacity, and 16 spherical harmonics (SH) coefficients for view dependent coloring. The covariance matrix of the Gaussian is calculated as follows:\n\nğšº=Râ¢Sâ¢STâ¢RTğšºğ‘…ğ‘†superscriptğ‘†ğ‘‡superscriptğ‘…ğ‘‡\\bm{\\Sigma}=RSS^{T}R^{T}bold_Î£ = italic_R italic_S italic_S start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_R start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT (1)\n\nwhere Rğ‘…Ritalic_R is the rotation matrix derived from the quaternion qğ‘qitalic_q and Sğ‘†Sitalic_S is a diagonal scaling matrix constructed from the scale vector sğ‘ sitalic_s. This formulation allows for the independent optimization of rotation and scaling, facilitating more flexible and precise control over the Gaussianâ€™s appearance in the rendered scene.\n\nIn the rendering process, the projection of a 3D Gaussian onto the 2D image plane is mathematically captured by transforming its covariance matrix:\n\nğšºâ€²=Jâ¢Wâ¢Î£â¢WTâ¢JT,superscriptğšºbold-â€²ğ½ğ‘ŠÎ£superscriptğ‘Šğ‘‡superscriptğ½ğ‘‡\\bm{\\Sigma^{\\prime}}=JW\\Sigma\\mathit{W}^{T}\\mathit{J}^{T},bold_Î£ start_POSTSUPERSCRIPT bold_â€² end_POSTSUPERSCRIPT = italic_J italic_W roman_Î£ italic_W start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_J start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT , (2)\n\nwhere Jğ½Jitalic_J is the Jacobian of the affine approximation of the projective transformation, and Wğ‘ŠWitalic_W is the viewing transformation matrix that translates and rotates the 3D Gaussian from world coordinates into camera coordinates. This allows to evaluate the 2D color and opacity footprint of each projected Gaussian. The final pixel color Cğ¶Citalic_C is computed by blending Nğ‘Nitalic_N 2D Gaussians that contribute to this pixel, sorted in order of their depth:\n\nCğ¶\\displaystyle Citalic_C =\\displaystyle== âˆ‘i=1Nwiâ‹…ci,superscriptsubscriptğ‘–1ğ‘â‹…subscriptğ‘¤ğ‘–subscriptğ‘ğ‘–\\displaystyle\\sum_{i=1}^{N}w_{i}\\cdot c_{i},âˆ‘ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT â‹… italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , (3) wisubscriptğ‘¤ğ‘–\\displaystyle w_{i}italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT =\\displaystyle== Tiâ‹…Î±iâ‹…Gi2â¢Dâ¢(ğ’™),â‹…subscriptğ‘‡ğ‘–subscriptğ›¼ğ‘–superscriptsubscriptğºğ‘–2ğ·ğ’™\\displaystyle T_{i}\\cdot\\alpha_{i}\\cdot G_{i}^{2D}(\\bm{x}),italic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT â‹… italic_Î± start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT â‹… italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 italic_D end_POSTSUPERSCRIPT ( bold_italic_x ) , (4) cisubscriptğ‘ğ‘–\\displaystyle c_{i}italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT =\\displaystyle== âˆ‘l=0Lâˆ‘m=âˆ’llğœ¸lâ¢mâ¢(i)â¢Ylâ¢mâ¢(ğ’—ğ’Š),superscriptsubscriptğ‘™0ğ¿superscriptsubscriptğ‘šğ‘™ğ‘™subscriptğœ¸ğ‘™ğ‘šğ‘–subscriptğ‘Œğ‘™ğ‘šsubscriptğ’—ğ’Š\\displaystyle\\sum_{l=0}^{L}\\sum_{m=-l}^{l}\\bm{\\gamma}_{lm}(i)Y_{lm}(\\bm{v_{i}}),âˆ‘ start_POSTSUBSCRIPT italic_l = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT âˆ‘ start_POSTSUBSCRIPT italic_m = - italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT bold_italic_Î³ start_POSTSUBSCRIPT italic_l italic_m end_POSTSUBSCRIPT ( italic_i ) italic_Y start_POSTSUBSCRIPT italic_l italic_m end_POSTSUBSCRIPT ( bold_italic_v start_POSTSUBSCRIPT bold_italic_i end_POSTSUBSCRIPT ) , (5)\n\nwhere the contribution of the itâ¢hsuperscriptğ‘–ğ‘¡â„i^{th}italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT Gaussian (wisubscriptğ‘¤ğ‘–w_{i}italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT) is determined by its opacity Î±isubscriptğ›¼ğ‘–\\alpha_{i}italic_Î± start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, its projected distribution Gi2â¢Dâ¢(x)superscriptsubscriptğºğ‘–2ğ·ğ‘¥G_{i}^{2D}(x)italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 italic_D end_POSTSUPERSCRIPT ( italic_x ), and the accumulated opacity of Gaussians in front of it, represented as Ti=âˆj=1iâˆ’1(1âˆ’wj)subscriptğ‘‡ğ‘–superscriptsubscriptproductğ‘—1ğ‘–11subscriptğ‘¤ğ‘—T_{i}=\\prod_{j=1}^{i-1}(1-w_{j})italic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = âˆ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i - 1 end_POSTSUPERSCRIPT ( 1 - italic_w start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ). The color of the Gaussian (cisubscriptğ‘ğ‘–c_{i}italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT) is computed using the SH function, where Lğ¿Litalic_L represents the maximum degree of SH coefficients and Ylâ¢msubscriptğ‘Œğ‘™ğ‘šY_{lm}italic_Y start_POSTSUBSCRIPT italic_l italic_m end_POSTSUBSCRIPT denotes the pre-defined SH basis functions [21].\n\nPlease note that, in the described formulation, each 3D Gaussian retains a fixed opacity and shows a single color when viewed from a specific direction.\n\n4 Methodology\n\nTextured-GS 3D Gaussian Splatting (3DGS) [11] utilizes Spherical Harmonics (SH) to model view-dependent shading effects. The SH basis functions are evaluated based on the view vector direction, defined by the vector from the camera center to the Gaussian center. These evaluations are then multiplied by the SH coefficients. The summed products of the SH basis functions and coefficients determine the view-dependent color for a given Gaussian, reflecting both the environmental lighting and dynamic changes based on the viewerâ€™s position relative to the objectâ€™s surface. However, each Gaussianâ€™s color remains fixed under a specific view direction, regardless of the Gaussianâ€™s size, limiting the modelâ€™s ability to represent high-frequency color variations across the Gaussian surface. Furthermore, in scenarios consisting solely of Lambertian surfaces, the additional SH parameters become redundant, as there are no view-dependent shading effects.\n\nTo address the limitations of traditional 3DGS, we introduce a novel approach utilizing SH coefficients to texture Gaussian ellipsoidal surfaces. Our method enables a single Gaussian to model local color and opacity variations, significantly enhancing its representational power and improving the overall quality of scene rendering with the same number of Gaussians.\n\nSpecifically, we calculate the intersection between the viewing ray and the Gaussianâ€™s ellipsoidal surface, as demonstrated in Figure 2. This intersection point is then transformed into the Gaussianâ€™s local coordinate system and normalized according to the scale of the Gaussian in each dimension, enabling distinct parameterization of different areas on the Gaussian surface. That is, the normalized vector nğ‘›nitalic_n, instead of viewing ray vğ‘£vitalic_v, is used to evaluate SH basis functions, which allows the Gaussian to form a texture on the surface of the ellipsoid, as illustrated in Figure 1. The complexity of the texture depends on the degree level of SH coefficients. In this paper, we select level three, which uses the same number of parameters as in 3DGS [11].\n\nThe new formulation further enhances our ability to model opacity variations across the Gaussian surface by incorporating an opacity channel into each SH coefficient. As demonstrated in Figure 1, enabling variable opacity across a Gaussianâ€™s surface can effectively alter its perceived shape.\n\nWith both color and opacity textured, we replaces Equation (5) with the following:\n\ncisubscriptğ‘ğ‘–\\displaystyle c_{i}italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT =\\displaystyle== âˆ‘l=0Lâˆ‘m=âˆ’llğœ¸lâ¢mâ¢(i)â¢Ylâ¢mâ¢(ğ’),superscriptsubscriptğ‘™0ğ¿superscriptsubscriptğ‘šğ‘™ğ‘™subscriptğœ¸ğ‘™ğ‘šğ‘–subscriptğ‘Œğ‘™ğ‘šğ’\\displaystyle\\sum_{l=0}^{L}\\sum_{m=-l}^{l}\\bm{\\gamma}_{lm}(i)Y_{lm}(\\bm{n}),âˆ‘ start_POSTSUBSCRIPT italic_l = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT âˆ‘ start_POSTSUBSCRIPT italic_m = - italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT bold_italic_Î³ start_POSTSUBSCRIPT italic_l italic_m end_POSTSUBSCRIPT ( italic_i ) italic_Y start_POSTSUBSCRIPT italic_l italic_m end_POSTSUBSCRIPT ( bold_italic_n ) , (6) Î±isubscriptğ›¼ğ‘–\\displaystyle\\alpha_{i}italic_Î± start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT =\\displaystyle== âˆ‘l=0Lâˆ‘m=âˆ’llğœ¶lâ¢mâ¢(i)â¢Ylâ¢mâ¢(ğ’),superscriptsubscriptğ‘™0ğ¿superscriptsubscriptğ‘šğ‘™ğ‘™subscriptğœ¶ğ‘™ğ‘šğ‘–subscriptğ‘Œğ‘™ğ‘šğ’\\displaystyle\\sum_{l=0}^{L}\\sum_{m=-l}^{l}\\bm{\\alpha}_{lm}(i)Y_{lm}(\\bm{n}),âˆ‘ start_POSTSUBSCRIPT italic_l = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT âˆ‘ start_POSTSUBSCRIPT italic_m = - italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT bold_italic_Î± start_POSTSUBSCRIPT italic_l italic_m end_POSTSUBSCRIPT ( italic_i ) italic_Y start_POSTSUBSCRIPT italic_l italic_m end_POSTSUBSCRIPT ( bold_italic_n ) , (7)\n\nNow, we present our formulation of the view vector evaluated by the SH coefficients. We model the Gaussian as an ellipsoid in 3D space:\n\ngâ¢(x,y,z)=x2sx2+y2sy2+z2zx2=1,ğ‘”ğ‘¥ğ‘¦ğ‘§superscriptğ‘¥2superscriptsubscriptğ‘ ğ‘¥2superscriptğ‘¦2superscriptsubscriptğ‘ ğ‘¦2superscriptğ‘§2superscriptsubscriptğ‘§ğ‘¥21g(x,y,z)=\\frac{x^{2}}{s_{x}^{2}}+\\frac{y^{2}}{s_{y}^{2}}+\\frac{z^{2}}{z_{x}^{2% }}=1,italic_g ( italic_x , italic_y , italic_z ) = divide start_ARG italic_x start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_s start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG + divide start_ARG italic_y start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_s start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG + divide start_ARG italic_z start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_z start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG = 1 , (8)\n\nwhere sx,sy,szsubscriptğ‘ ğ‘¥subscriptğ‘ ğ‘¦subscriptğ‘ ğ‘§s_{x},s_{y},s_{z}italic_s start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT represent the scale of the Gaussian in each dimension, respectively. Given an input pixel-ray ğ’“â¢(t)=ğ’+tâ¢ğ’…ğ’“ğ‘¡ğ’ğ‘¡ğ’…\\bm{r}(t)=\\bm{o}+t\\bm{d}bold_italic_r ( italic_t ) = bold_italic_o + italic_t bold_italic_d, where ğ’ğ’\\bm{o}bold_italic_o and ğ’…ğ’…\\bm{d}bold_italic_d are the rayâ€™s origin and the direction transformed into the ellipsoidâ€™s local coordinate system, we calculate the position of the two intersection points. Substituting the parametric ray equation into the ellipsoid equation results in:\n\n(ox+tâ¢dx)2sx2+(oy+tâ¢dy)2sy2+(oz+tâ¢dz)2sz2=1superscriptsubscriptğ‘œğ‘¥ğ‘¡subscriptğ‘‘ğ‘¥2superscriptsubscriptğ‘ ğ‘¥2superscriptsubscriptğ‘œğ‘¦ğ‘¡subscriptğ‘‘ğ‘¦2superscriptsubscriptğ‘ ğ‘¦2superscriptsubscriptğ‘œğ‘§ğ‘¡subscriptğ‘‘ğ‘§2superscriptsubscriptğ‘ ğ‘§21\\frac{(o_{x}+td_{x})^{2}}{s_{x}^{2}}+\\frac{(o_{y}+td_{y})^{2}}{s_{y}^{2}}+% \\frac{(o_{z}+td_{z})^{2}}{s_{z}^{2}}=1divide start_ARG ( italic_o start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT + italic_t italic_d start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_s start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG + divide start_ARG ( italic_o start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT + italic_t italic_d start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_s start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG + divide start_ARG ( italic_o start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT + italic_t italic_d start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_s start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG = 1\n\nwhere â¢Awhere ğ´\\displaystyle\\text{where }Awhere italic_A =dx2sx2+dy2sy2+dz2sz2,absentsuperscriptsubscriptğ‘‘ğ‘¥2superscriptsubscriptğ‘ ğ‘¥2superscriptsubscriptğ‘‘ğ‘¦2superscriptsubscriptğ‘ ğ‘¦2superscriptsubscriptğ‘‘ğ‘§2superscriptsubscriptğ‘ ğ‘§2\\displaystyle=\\frac{d_{x}^{2}}{s_{x}^{2}}+\\frac{d_{y}^{2}}{s_{y}^{2}}+\\frac{d_% {z}^{2}}{s_{z}^{2}},= divide start_ARG italic_d start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_s start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG + divide start_ARG italic_d start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_s start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG + divide start_ARG italic_d start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_s start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG , Bğµ\\displaystyle Bitalic_B =2â¢(oxâ¢dxsx2+oyâ¢dysy2+ozâ¢dzsz2),absent2subscriptğ‘œğ‘¥subscriptğ‘‘ğ‘¥superscriptsubscriptğ‘ ğ‘¥2subscriptğ‘œğ‘¦subscriptğ‘‘ğ‘¦superscriptsubscriptğ‘ ğ‘¦2subscriptğ‘œğ‘§subscriptğ‘‘ğ‘§superscriptsubscriptğ‘ ğ‘§2\\displaystyle=2(\\frac{o_{x}d_{x}}{s_{x}^{2}}+\\frac{o_{y}d_{y}}{s_{y}^{2}}+% \\frac{o_{z}d_{z}}{s_{z}^{2}}),= 2 ( divide start_ARG italic_o start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT italic_d start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT end_ARG start_ARG italic_s start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG + divide start_ARG italic_o start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT italic_d start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT end_ARG start_ARG italic_s start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG + divide start_ARG italic_o start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT italic_d start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT end_ARG start_ARG italic_s start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ) , Cğ¶\\displaystyle Citalic_C =ox2sx2+oy2sy2+oz2sz2âˆ’1.absentsuperscriptsubscriptğ‘œğ‘¥2superscriptsubscriptğ‘ ğ‘¥2superscriptsubscriptğ‘œğ‘¦2superscriptsubscriptğ‘ ğ‘¦2superscriptsubscriptğ‘œğ‘§2superscriptsubscriptğ‘ ğ‘§21\\displaystyle=\\frac{o_{x}^{2}}{s_{x}^{2}}+\\frac{o_{y}^{2}}{s_{y}^{2}}+\\frac{o_% {z}^{2}}{s_{z}^{2}}-1.= divide start_ARG italic_o start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_s start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG + divide start_ARG italic_o start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_s start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG + divide start_ARG italic_o start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_s start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG - 1 .\n\nThe tğ‘¡titalic_t can be solved using the quadratic formula:\n\nt=âˆ’BÂ±B2âˆ’4â¢Aâ¢C2â¢Ağ‘¡plus-or-minusğµsuperscriptğµ24ğ´ğ¶2ğ´t=\\frac{-B\\pm\\sqrt{B^{2}-4AC}}{2A}italic_t = divide start_ARG - italic_B Â± square-root start_ARG italic_B start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT - 4 italic_A italic_C end_ARG end_ARG start_ARG 2 italic_A end_ARG\n\nWe select the intersection point with lower tğ‘¡titalic_t which means it is closer to the rayâ€™s origin, and normalized each dimension based on the scale sğ‘ sitalic_s.\n\nOptimization In the original 3DGS framework [11], the limited representational power of individual Gaussians necessitates a strategy of increasing Gaussian density. This involves adaptively splitting or cloning Gaussians to better reconstruct complex regions. In contrast, our textured Gaussians demonstrate significantly enhanced representational power, enabling them to effectively capture local color and opacity variations. As a result, we have opted to eliminate the adaptive control of Gaussians in our study. This simplification streamlines the optimization process, making the optimization more straightforward and efficient.\n\nIn practice, we initiate our process using the output from Mini-Splatting [5] as a baseline, and then apply our Textured-GS method to enhance rendering quality. Mini-Splatting is chosen because it achieves results comparable to the original 3DGS while using fewer Gaussians. Additionally, it incorporates an intersection-preserving technique that discards Gaussians which do not directly intersect with the viewing ray. This method effectively minimizes scenarios where multiple low-opacity Gaussians overlap to blend a pixel, aligning well with our objective of maximizing the representational capability of each Gaussian.\n\nWe utilize a sigmoid activation function for both ğœ¸csubscriptğœ¸ğ‘\\bm{\\gamma}_{c}bold_italic_Î³ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT and ğœ¸Î±subscriptğœ¸ğ›¼\\bm{\\gamma}_{\\alpha}bold_italic_Î³ start_POSTSUBSCRIPT italic_Î± end_POSTSUBSCRIPT to ensure smooth gradients. The loss function is identical to the original 3DGS framework [11]:\n\nâ„’=(1âˆ’Î»)â¢â„’1+Î»â¢â„’Dâˆ’Sâ¢Sâ¢Iâ¢Mâ„’1ğœ†subscriptâ„’1ğœ†subscriptâ„’ğ·ğ‘†ğ‘†ğ¼ğ‘€\\mathcal{L}=(1-\\lambda)\\mathcal{L}_{1}+\\lambda\\mathcal{L}_{D-SSIM}caligraphic_L = ( 1 - italic_Î» ) caligraphic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_Î» caligraphic_L start_POSTSUBSCRIPT italic_D - italic_S italic_S italic_I italic_M end_POSTSUBSCRIPT (9)\n\nwhere Î»ğœ†\\lambdaitalic_Î» is set to 0.2 in all experiments.\n\nThe primary distinction in our gradient descent process, compared to the original frameworkâ€™s approach to view-dependent color, lies in the management of losses: In the original framework, view-dependent color loss from each processed pixel is aggregated and propagated to the SH coefficients in a single update at the end of each iteration. In contrast, our method updates the SH coefficients incrementally, applying the loss immediately after each pixel is processed. This modification is necessary because, unlike the view-dependent color approach where the input view vector remains constant for all pixels, the view vector in our spatially defined texture varies with each pixel. We apply this same incremental update mechanism to the opacity channel. Further details on the learning process are provided in the subsequent section.\n\n5 Experiments\n\nIn this section, we evaluate our approach, detailing the implementation and discussing the results of ablation studies. The source code will be released upon the acceptance of the paper.\n\n5.1 Datasets\n\nWe conducted experiments across three real-world datasets: Mip-NeRF360 [1], Tanks and Temples [12], and Deep Blending [9]. To ensure consistency and fairness in our evaluations, we adhered to the same processing methodologies used in the 3DGS and Mini-Splatting studies, including identical scene selection, train/test splits, and image resolution settings. Specifically, for each dataset, we implemented a train/test division following the recommendations of Mip-NeRF 360 [1], selecting every eighth photo for testing to facilitate consistent and meaningful comparisons. Our evaluations utilized standard error metrics widely used in the field, such as PSNR, LPIPS, and SSIM.\n\n5.2 Implementation Details\n\nWe implemented Textured-GS using PyTorch and integrate it into the optimization pipeline of 3D Gaussian Splatting (3DGS) [11]. As discussed previously, we removed the module for adaptive control of Gaussians. We also modified the Gaussian rasterization module within the 3DGS framework to facilitate the rendering of textured Gaussians and update SH coefficients through gradient descent.\n\nOur experiments were conducted on an Ubuntu 20.04 platform with an NVIDIA RTX 4090 GPU. Starting with the output Gaussians from Mini-Splatting [5], we trained our model for 14,000 iterations, applying learning rate of 0.0025 for color texture and 0.005 for opacity texture, respectively. It is important to note that view dependent color was not loaded as part of our input due to a fundamentally different Gaussian setup. We opted for the square root of the scale parameters, sğ‘ \\sqrt{s}square-root start_ARG italic_s end_ARG, for the ellipsoid in Equation 8, because larger ellipsoids tended to yield better performance, though this does not alter the actual Gaussian size in splatting. All other parameters were aligned with those used in Mini-Splatting and 3DGS to ensure a fair comparison.\n\n5.3 Results\n\nRendering Quality Table 1 presents a quantitative evaluation across three different real-world dataset. We compare our Textured-GS with the baseline method Mini-Splatting, 3DGS, and other related algorithms. In most categories, our Textured-GS outperforms both 3DGS and Mini-Splatting. The NeRF-based method Zip-NeRF [2], currently the state-of-the-art on Mip-NeRF 360 dataset, shows superior performance. Mini-Splatting-D [5], a denser version of Mini-Splatting, surpasses ours method in some categories but uses a greater number of Gaussians than 3DGS, contradicting our goal of efficiency.\n\nFigure 3 provides a visual comparison of three real-world scenes using our proposed method alongside two baseline methods. It shows that sharp object boundaries present significant challenges when represented with a limited number of Gaussians. For instance, in the Room scene, the table legs are effectively depicted using numerous Gaussians in the 3DGS method, but they appear serrated in Mini-Splatting due to a restricted number of Gaussians. Our method, which utilizes textured opacity across the Gaussian surface, effectively addresses this challenge, achieving a smoother representation with the same number of Gaussians, see Figure 5 for more zoomed-in comparison. Additionally, 3DGS often completely misses detailed structures, as observed in the Train and Bonsai scenes, despite using a significantly larger number of Gaussians.\n\nFigure 4 further plots the performance of our approach compared to Mini-Splatting [5] across various numbers of Gaussians. Our method consistently outperforms Mini-Splatting at each scale. Models with a fewer number of Gaussians are generated by using a lower sampling rate within the Mini-Splatting framework.\n\nResource Consumption We assessed the resource consumption of various rendering methods, focusing on the number of Gaussians, frames per second (FPS), training time, and maximum allocated memory during training. Table 2 summarizes the comparative performance and efficiency of these methods. The results show a clear reduction in memory usage. It is worth noting that both the rendering speed and training time have significant room for optimization.\n\n5.4 Ablation Study\n\nWe conducted an ablation study to evaluate the impact of two key components of our proposed method: textured color and textured opacity. For each component, we gradually increased the Spherical Harmonics (SH) degree from zero to three. Table 3 summarizes the results, tested on the Mip-NeRF 360 dataset. Our method, employing the third degree level for both color and opacity, achieves the best results, demonstrating incremental improvements as the SH degree level increases.\n\n5.5 Limitations\n\nOur method enhances the capabilities of each Gaussian in the Mini-Splatting framework [5] by introducing textured color and opacity, leading to improved rendering quality. However, this approach has limitations, notably the increase in computational cost due to additional calculations for ray-ellipsoid intersections, which result in longer training and rendering times. We believe this limitation can be addressed by optimizing the rendering process and integrating the sampling for color and opacity into a single step, thereby streamlining computations and reducing overhead.\n\nTo isolate the impact of optimizing color and opacity textures, we maintained the locations, sizes, and orientations of Gaussians as generated by Mini-Splatting unchanged. Additionally, we removed the module for adaptive control of Gaussians, as found in 3DGS. Optimizing all these parameters in an end-to-end manner is likely to enhance the performance of our algorithm. As part of our future work, we plan to develop a comprehensive framework that starts with a structure-from-motion (SFM) point cloud, employs mechanisms for both densifying and simplifying Gaussians, and ultimately produces a set of textured Gaussians that optimally represent the scenes.\n\n6 Conclusion\n\nIn conclusion, our study introduces Textured-GS, an innovative rendering method that leverages spherical harmonics to introduce spatially defined color and opacity variations within each Gaussian. This approach significantly enhances the visual quality of renderings, providing a more detailed and flexible solution than traditional Gaussian splatting methods. To validate the effectiveness of our method, we integrated Textured-GS within the Mini-Splatting framework without increasing the number of Gaussians used. Our comprehensive evaluation across three real-world datasets demonstrates that Textured-GS consistently outperforms both the baseline Mini-Splatting and standard 3DGS in rendering quality. We also observed that our method efficiently addresses complex scenes that typically challenge Gaussian-based approaches, such as sharp edges and detailed structures. Moving forward, we plan to develop a fully optimized end-to-end framework that incorporates adaptive control of Gaussians, which we anticipate will further enhance the capabilities and efficiency of our approach, making it even more suitable for high-fidelity rendering tasks.\n\nReferences\n\n[1] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded anti-aliased neural radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5470â€“5479, 2022.\n\n[2] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman. Zip-nerf: Anti-aliased grid-based neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 19697â€“19705, 2023.\n\n[3] Zhiqin Chen, Thomas Funkhouser, Peter Hedman, and Andrea Tagliasacchi. Mobilenerf: Exploiting the polygon rasterization pipeline for efficient neural field rendering on mobile architectures. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16569â€“16578, 2023.\n\n[4] Zhiwen Fan, Kevin Wang, Kairun Wen, Zehao Zhu, Dejia Xu, and Zhangyang Wang. Lightgaussian: Unbounded 3d gaussian compression with 15x reduction and 200+ fps. arXiv preprint arXiv:2311.17245, 2023.\n\n[5] Guangchi Fang and Bing Wang. Mini-splatting: Representing scenes with a constrained number of gaussians. arXiv preprint arXiv:2403.14166, 2024.\n\n[6] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5501â€“5510, 2022.\n\n[7] Stephan J Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, and Julien Valentin. Fastnerf: High-fidelity neural rendering at 200fps. In Proceedings of the IEEE/CVF international conference on computer vision, pages 14346â€“14355, 2021.\n\n[8] Sharath Girish, Kamal Gupta, and Abhinav Shrivastava. Eagles: Efficient accelerated 3d gaussians with lightweight encodings. arXiv preprint arXiv:2312.04564, 2023.\n\n[9] Peter Hedman, Julien Philip, True Price, Jan-Michael Frahm, George Drettakis, and Gabriel Brostow. Deep blending for free-viewpoint image-based rendering. ACM Transactions on Graphics (ToG), 37(6):1â€“15, 2018.\n\n[10] Joongho Jo, Hyeongwon Kim, and Jongsun Park. Identifying unnecessary 3d gaussians using clustering for fast rendering of 3d gaussian splatting. arXiv preprint arXiv:2402.13827, 2024.\n\n[11] Bernhard Kerbl, Georgios Kopanas, Thomas LeimkÃ¼hler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42(4):1â€“14, 2023.\n\n[12] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and temples: Benchmarking large-scale scene reconstruction. ACM Transactions on Graphics (ToG), 36(4):1â€“13, 2017.\n\n[13] Adam R Kosiorek, Heiko Strathmann, Daniel Zoran, Pol Moreno, Rosalia Schneider, Sona MokrÃ¡, and Danilo Jimenez Rezende. Nerf-vae: A geometry aware 3d scene generative model. In International Conference on Machine Learning, pages 5742â€“5752. PMLR, 2021.\n\n[14] Joo Chan Lee, Daniel Rho, Xiangyu Sun, Jong Hwan Ko, and Eunbyung Park. Compact 3d gaussian representation for radiance field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21719â€“21728, 2024.\n\n[15] David B Lindell, Julien NP Martel, and Gordon Wetzstein. Autoint: Automatic integration for fast neural volume rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14556â€“14565, 2021.\n\n[16] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99â€“106, 2021.\n\n[17] Wieland Morgenstern, Florian Barthel, Anna Hilsmann, and Peter Eisert. Compact 3d scene representation via self-organizing gaussian grids. arXiv preprint arXiv:2312.13299, 2023.\n\n[18] Thomas MÃ¼ller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. ACM transactions on graphics (TOG), 41(4):1â€“15, 2022.\n\n[19] KL Navaneet, Kossar Pourahmadi Meibodi, Soroush Abbasi Koohpayegani, and Hamed Pirsiavash. Compact3d: Compressing gaussian splat radiance field models with vector quantization. arXiv preprint arXiv:2311.18159, 2023.\n\n[20] Simon Niedermayr, Josef Stumpfegger, and RÃ¼diger Westermann. Compressed 3d gaussian splatting for accelerated novel view synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10349â€“10358, 2024.\n\n[21] Ravi Ramamoorthi and Pat Hanrahan. An efficient representation for irradiance environment maps. In Proceedings of the 28th annual conference on Computer graphics and interactive techniques, pages 497â€“500, 2001.\n\n[22] Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas Geiger. Kilonerf: Speeding up neural radiance fields with thousands of tiny mlps. In Proceedings of the IEEE/CVF international conference on computer vision, pages 14335â€“14345, 2021.\n\n[23] Tong Wu, Yu-Jie Yuan, Ling-Xiao Zhang, Jie Yang, Yan-Pei Cao, Ling-Qi Yan, and Lin Gao. Recent advances in 3d gaussian splatting. arXiv preprint arXiv:2403.11134, 2024.\n\n[24] Matthias Zwicker, Hanspeter Pfister, Jeroen Van Baar, and Markus Gross. Ewa volume splatting. In Proceedings Visualization, 2001. VISâ€™01., pages 29â€“538. IEEE, 2001."
    }
}
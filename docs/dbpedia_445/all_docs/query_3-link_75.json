{
    "id": "dbpedia_445_3",
    "rank": 75,
    "data": {
        "url": "https://arxiv.org/html/2307.13912v3",
        "read_more_link": "",
        "language": "en",
        "title": "Embedding Democratic Values into Social Media AIs via Societal Objective Functions",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://arxiv.org/html/x1.png",
            "https://arxiv.org/html/x2.png",
            "https://arxiv.org/html/x3.png",
            "https://arxiv.org/html/x4.png",
            "https://arxiv.org/html/x5.png",
            "https://arxiv.org/html/x6.png",
            "https://arxiv.org/html/x7.png",
            "https://arxiv.org/html/x8.png",
            "https://arxiv.org/html/x9.png",
            "https://arxiv.org/html/x10.png",
            "https://arxiv.org/html/x11.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            "algorithms",
            "affective polarization",
            "partisan animosity",
            "social media AIs",
            "social media users"
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "License: arXiv.org perpetual non-exclusive license\n\narXiv:2307.13912v3 [cs.HC] 15 Feb 2024\n\nEmbedding Democratic Values into Social Media AIs via Societal Objective Functions\n\nChenyan Jia , Michelle S. Lam , Minh Chau Mai , Jeffrey T. Hancock and Michael S. Bernstein\n\nAbstract.\n\nMounting evidence indicates that the artificial intelligence (AI) systems that rank our social media feeds bear nontrivial responsibility for amplifying partisan animosity: negative thoughts, feelings, and behaviors toward political out-groups. Can we design these AIs to consider democratic values such as mitigating partisan animosity as part of their objective functions? We introduce a method for translating established, vetted social scientific constructs into AI objective functions, which we term societal objective functions, and demonstrate the method with application to the political science construct of anti-democratic attitudes. Traditionally, we have lacked observable outcomes to use to train such models â€” however, the social sciences have developed survey instruments and qualitative codebooks for these constructs, and their precision facilitates translation into detailed prompts for large language models. We apply this method to create a democratic attitude model that estimates the extent to which a social media post promotes anti-democratic attitudes, and test this democratic attitude model across three studies. In Study 1, we first test the attitudinal and behavioral effectiveness of the intervention among US partisans (N=1,380ð‘1380N=1,380italic_N = 1 , 380) by manually annotating (Î±=.895ð›¼.895\\alpha=.895italic_Î± = .895) social media posts with anti-democratic attitude scores and testing several feed ranking conditions based on these scores. Removal (d=.20ð‘‘.20d=.20italic_d = .20) and downranking feeds (d=.25ð‘‘.25d=.25italic_d = .25) reduced participantsâ€™ partisan animosity without compromising their experience and engagement. In Study 2, we scale up the manual labels by creating the democratic attitude model, finding strong agreement with manual labels (Ï=.75ðœŒ.75\\rho=.75italic_Ï = .75). Finally, in Study 3, we replicate Study 1 using the democratic attitude model instead of manual labels to test its attitudinal and behavioral impact (N=558ð‘558N=558italic_N = 558), and again find that the feed downranking using the societal objective function reduced partisan animosity (d=.25ð‘‘.25d=.25italic_d = .25). This method presents a novel strategy to draw on social science theory and methods to mitigate societal harms in social media AIs.\n\nalgorithms, affective polarization, partisan animosity, social media AIs, social media users\n\nâ€ â€ copyright: acmlicensedâ€ â€ journal: PACMHCIâ€ â€ journalyear: 2024â€ â€ journalvolume: 8â€ â€ journalnumber: CSCW1â€ â€ article: 163â€ â€ publicationmonth: 4â€ â€ doi: 10.1145/3641002â€ â€ ccs: Human-centered computing Empirical studies in HCI\n\n1. Introduction\n\nCan social media support a healthy democracy? Social media AIs such as feed ranking algorithms bear nontrivial responsibility in how people perceive contentious political issues (HuszÃ¡r et al., 2022). Researchers have long been interested in the effects of social media AIs on partisan information consumption (e.g., (Bail, 2022; Guess et al., 2019; Robertson et al., 2023; Bakshy et al., 2015)) because these AIs shape peopleâ€™s beliefs (Brady et al., 2023), affect their well-being (Hancock et al., 2022; Kreski et al., 2021), and change their behaviors (Vannucci et al., 2020).\n\nA key outcome of interest in a healthy democracy is partisan animosity (Milli et al., 2023; TÃ¶rnberg, 2022): negative thoughts, feelings and behaviours towards a political out-group. In the United States, for example, over 80% of members of both political parties express concern that the country is growing increasingly divided (Pew Research Center, 2019). High levels of partisan animosity undermine our collective will to address public issues (Hartman et al., 2022). Unfortunately, mounting evidence suggests that social media is associated with increases in partisan animosity in established democracies (Lorenz-Spreen et al., 2023; Milli et al., 2023; TÃ¶rnberg, 2022). While this effect can be mitigated by usersâ€™ own behavior and choices (Robertson et al., 2023; Bakshy et al., 2015), many studies demonstrate that feed algorithms can amplify political content (HuszÃ¡r et al., 2022) and decrease peopleâ€™s trust in democracy (Lorenz-Spreen et al., 2023).\n\nCould we design social media AIs to more directly consider their impact on partisan animosity? Social media AIs, and specifically feed ranking algorithms, are centered around accurately predicting engagement signals such as likes and clicks (Narayanan, 2023; HuszÃ¡r et al., 2022; Liu et al., 2010). In contrast, partisan animosity may not be detectable with engagement behavior alone, and it can even be anti-correlated with traditional engagement signals: maximizing engagement can amplify anti-social behaviors (Are, 2020; Munn, 2020) and focus attention on pro-attitudinal political content (Sunstein, 2001; Rowland, 2011). Lacking a method for modeling partisan animosity directly in the algorithm, researchers and practitioners instead engage in mitigation strategies such as ideologically balanced feeds (Celis et al., 2019) and content moderation policies (Gillespie, 2018). However, these mitigations are indirect: they are not designed to treat a societal value (here, reducing partisan animosity) as a first-class algorithmic objective.\n\nTo address this gap, we demonstrate a method for integrating established and vetted social scientific constructs into objective functions, which we term societal objective functions. Specifically, we observe that survey measurements and qualitative codebooks in the social and behavioral sciences have long needed to be precise to facilitate reliability, and recent research has found that this precision enables their translation into prompts interpretable by large language models (LLMs) such as GPT-4 (Xiao et al., 2023; Ziems et al., 2023; Huang et al., 2023; Wang et al., 2021; Do et al., 2022). In this work, we go one step further to embed algorithms with technical objectives derived from these social science constructs. In this case, we draw on the political science literature and specifically its measurement of anti-democratic attitudes. This measure, which was recently tested in a large study that received widespread attention (Voelkel et al., 2023), spans eight variables that describe willingness to engage in good faith in the democratic process: partisan animosity, support for undemocratic practices, support for partisan violence, support for undemocratic candidates, opposition to bipartisanship, social distrust, social distance, and biased evaluation of politicized facts. We adapt each of these eight variables into prompts for a large language model and then combine them into a joint democratic attitude model that agrees with manual human annotation on political social media posts. This democratic attitude model can be applied at scale to estimate the impact of every post in a social media feed on anti-democratic attitudes, enabling democratic attitudes to be integrated into a social media feed ranking algorithm. We summarize the steps of our societal objective function method in Figure 2 and several of the key terms used throughout the paper in Table 1.\n\nWe integrated this democratic attitude model into a feed ranking algorithm and tested it across three studies to investigate its impact on partisan animosity as well as traditional platform engagement measures. In Study 1, we began with manual, human labels for the construct to establish its behavioral effectiveness before translating it into an AI: we manually annotated a prepared feed of political social media content using the existing anti-democratic attitude scale (Krippendorffâ€™s Î±=.895ð›¼.895\\alpha=.895italic_Î± = .895). We conduct a preregistered, between-subjects online experiment among U.S. partisans (Democrats or Republicans, Nð‘Nitalic_N = 1,380) to examine the effect of manual democratic attitude feeds that use these manual labels for re-ranking, removal, and warning against a traditional engagement-based feed or a chronological feed on participantsâ€™ partisan animosity, support for undemocratic practices, feed-level satisfaction metrics, and engagement, as shown in Figure 3.\n\nWe found that our democratic attitude feedsâ€”specifically the downranking feed and remove-and-replace feedâ€”significantly reduced partisan animosity compared to a traditional engagement-based feed. These results hold for both conservatives and liberals, and hold without compromising participantsâ€™ engagement level or their ratings of their experience on the platform. Though freedom of speech is cited as a frequent concern for algorithmic feed ranking methods, we found that the democratic attitude feeds that re-rank or remove do not prompt perceived threats to freedom of speech.\n\nNext, we translated from manual annotation to an automated societal objective function. In Study 2, we used zero-shot prompting with a large language model (LLM) to translate the original anti-democratic attitude construct into a democratic attitude model via GPT-4. Testing the accuracy of the democratic attitude model, we observed that the democratic attitude model ratings correlated highly with those from human coders (Spearmanâ€™s Ï=.75ðœŒ.75\\rho=.75italic_Ï = .75), indicating that the manual approach can be replicated at scale using LLMs. Then, in Study 3, we replicated Study 1, this time using the algorithmic democratic attitude model rather than the manual labels. The original results replicated; the algorithmic democratic attitude feed significantly reduced partisan animosity with an effect size similar to that of the manual democratic attitude feed.\n\nOur work introduces a novel method of translating social science theory to embed societal values in feeds via algorithmic objectives, which we term societal objective functions. We accomplish this by operationalizing social science constructs into a manual codebook, using the codebook to manually re-rank feeds, and validating the effect that re-ranked feeds have on societal outcomes with online experiments (Study 1). Then, we scale up the codebook using zero-shot prompting with LLMs and show that algorithmic ranking can replicate both manual feed ranking (Study 2) and online experiment results (Study 3).\n\nWe demonstrate the viability of our method with the specific construct of democratic attitudes, but our approach carries implications for how we might incorporate a much broader set of societal values into social media feeds. We note that todayâ€™s social media already encodes values, but these values are typically implicit and focused on individual user outcomes, such as engagement or time spent on the platform (Bernstein et al., 2023). Our work aims to make these values more explicit and tuneable. Toward this goal, in this paper we develop an existence proof via one value, pro-democratic attitudes, to demonstrate the feasibility of the method and how it might apply to a broader plurality of values. A plural set of values are ultimately important to capture for social media feed ranking, and may be amenable to bottom-up, participatory processes. Given a new societal value of interest (e.g., wellbeing, cultural diversity, environmental sustainability), future work might leverage this method to identify relevant social science constructs and instantiate them into additional algorithmic objectives to embed societal values in feeds. Todayâ€™s social media feeds lack an understanding of the impact they may have on societal values like mitigating anti-democratic attitudesâ€”our work validates a promising pathway to directly embed societal values into the objectives that drive social media AIs.\n\n2. Related Work\n\nAlgorithmic social media feed ranking has consequences not just for individual users, but also for society. Social media AIs shape peopleâ€™s beliefs (Brady et al., 2023), affect their mental well-being (Hancock et al., 2022; Kreski et al., 2021), and change their behaviors (Vannucci et al., 2020). These consequences accrue to the individual, of course, but also aggregate to the societal level, for example through their impact on democratic discourse.\n\n2.1. Encoding Societal Values into Social Media AIs\n\nTrading off societal outcomes is a complex issue. Today, most recommender systems, including those that power social media feeds, center around individual user experiences. Recommendation algorithms rely on key metrics, or objectives, that determine how to score candidate items to filter, rank, and display to users (Eckles, 2022). Most commonly, feed ranking systems focus on metrics of user engagement (e.g., clicks, views, comments, likes), which serve as proxies for user satisfaction and, ultimately, platform revenue (Milli et al., 2021; Ciampaglia et al., 2018). These algorithms can end up maximizing individual experience at the cost of societal values such as pro-democratic attitudes or partisan animosity. For instance, maximizing engagement can amplify anti-social behaviors such as online harassment (Are, 2020; Munn, 2020) or focus our attention on pro-attitudinal political content (Sunstein, 2001; Rowland, 2011).\n\nThough engagement metrics can be misleading as proxies for user benefit (Kleinberg et al., 2022), these metrics are amenable to measurement and modeling since behavioral traces from organic platform use are the most abundant form of data (in contrast to explicit signals like user surveys or human annotation) (Stray et al., 2022). Some platforms periodically survey their users to gather high-level feedback, such as Facebookâ€™s survey that asked users to gauge whether posts were good or bad for the world (Pahwa, 2021), but survey feedback is much more limited in scale, so it is challenging to formulate algorithmic objectives around this feedback.\n\nTo address these issues, social media platforms have explored efforts that incorporate some notions of societal values into ranking models. Content moderation models are a common strategy to ensure that content does not violate platform policies or community guidelines (Gillespie, 2018), and such models are often developed with the help of crowdsourced data annotations (Team, 2019). Beyond content moderation, platforms have also taken measures to combat the societal harms posed by content such as terrorism or extremism, misinformation, and violence with automated detection and removal (Bickert, 2018; Transparency, 2021; Report, 2023). On the modeling side, recent developments in AI value alignment, such as Constitutional AI (Bai et al., 2022) and reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022b) present technical methods for holistically steering AI models to better align with human descriptions and demonstrations of desired behavior. These steps are critical in handling acute, negative societal values and preserving general norms of communication. Our work seeks to extend these approaches to not only combat stark policy violations, but also to capture a wider range of societal values that goes beyond policy enforcement.\n\nMeanwhile, researchers have also investigated interventions on social media feeds to counter various user harms that may relate to societal values. For example, Gobo addressed the lack of control over social media feeds by introducing a system that allowed users to aggregate and filter content across platforms (Bhargava et al., 2019). In a related vein, the HabitLab system sought to grant users agency over social media usage by enacting productivity interventions (Kovacs et al., 2018). Other work has explored interventions that expose users to alternative feeds that may productively differ from those that they typically see: â€œBlue Feed, Red Feedâ€ presented contrasting liberal and conservative Facebook feeds (Keegan, 2016), and researchers have explored feed re-ranking interventions aimed at achieving ideological balance (Celis et al., 2019) or bridging-based ranking to build trust across divides (Ovadya and Thorburn, 2023). However, some researchers argue that those â€more balancedâ€ feed designs may not be effective enough to reduce partisan animosity (Nelimarkka et al., 2018). Therefore, we build on this line of prior work in presenting concrete implementation strategies to intervene on social media feeds more effectively. While prior approaches may not entirely capture or align with societal values, we address the challenge of bringing implementation and societal values in line.\n\nGiven the difficulty of quantifying societal values and the resulting scarcity of social media ranking models that optimize for such values, we need new research agendas that might address these issues. Recent work has started to map and articulate the space of human values that could be instantiated in social media. For example, Stray et al. (2022) provide a broad set of over 30 human values compiled from multidisciplinary experts, such as well-being, freedom of expression, and civic engagement. Prior work has introduced valuable process frameworks that outline how to bridge from human values to algorithmic systems (Stray et al., 2022; Zhu et al., 2018), but it remains challenging to connect those values to implementation. Our work thus sets out to connect from societal values to social science constructs to concrete algorithmic implementations, and we demonstrate the promise of this approach with a societal value of democratic attitudes.\n\n2.2. Social Media Algorithms and Partisan Animosity\n\nIn light of a growing body of literature suggesting harmful connections between social media algorithms and democracy, we focus on democratic attitudes in our work. Partisan animosity refers to tendency of partisans to hold negative views of opposing partisans, but positive views of co-partisans (Iyengar and Westwood, 2015; Iyengar et al., 2019). Social scientists, practitioners, and activists have long been interested in reducing partisan animosity among Americans (Wojcieszak and Warner, 2020; Ahler and Sood, 2018). In the United States, this divide appears to be growing more extreme (Boxell et al., 2020), driving worry about undemocratic practices and existential risks to democracy (Kingzette et al., 2021). Partisan animosity is often associated with affective polarization (Voelkel et al., 2023); in this study, we focus on affective polarization instead of issue polarization, which reflects partisansâ€™ disagreement about certain political issues (e.g., abortion, gun control) (Hartman et al., 2022). Some researchers operationalize affective polarization by measuring disliking partisanship in general instead of a specific party (Klar et al., 2018), whereas others introduce new terms or measurements such as â€œpartyismâ€ to describe the hostility and aversion to a certain political party (Sunstein, 2015). Given the diversity and disparity of operationalizations of affective polarization in past research, we choose to focus on a broader term of partisan animosity following Hartman et al. (2022). As in this prior work, we define partisan animosity as hostility and aversion to the opposing party, and measure this with ratings of warmth on a feeling thermometer (e.g., (Iyengar et al., 2019; Voelkel et al., 2023)).\n\nSince digital media introduces heavy personalization and amplifies messages at a vastly different speed and scale than prior forms of media, researchers have investigated potential links between algorithmic behavior, media consumption, and user polarization. For example, there is evidence that the Twitter algorithm amplifies content from the political right (HuszÃ¡r et al., 2022) and that the ranking algorithm amplifies partisanship and out-group animosity, especially for political tweets (Milli et al., 2023). Studies have similarly indicated that Facebook usage promotes political polarization (Allcott et al., 2020). However, there is disagreement about the mechanisms by which social media influences polarization. Earlier research hypothesized about the role of echo chambers (or selective exposure) in isolating individuals or communities into homogeneous clusters and driving them towards more extreme and divergent positions (Sunstein, 2001). More recent evidence supports alternative mechanisms like partisan sorting, whereby polarization is not driven by isolation, but by repeated exposure to individuals outside of local networks, which might cause local conflicts to align on global partisan lines (TÃ¶rnberg, 2022).\n\nThus, there are a variety of possible mechanisms by which algorithmic ranking on social media ultimately influences the political beliefs of users. While much of this prior work studies the impact of existing social media (e.g., (GonzÃ¡lez-BailÃ³n et al., 2023)), some recent studies suggest a few alternative design options to combat polarization on social media such as diversifying information sources (Nelimarkka et al., 2019), highlighting agreeable items via browser widgets (Munson and Resnick, 2010; Munson et al., 2013), and designing political deliberation environments (Semaan et al., 2015). Our work aims to build on these prior findings to redesign social media ranking algorithms and experimentally tie these design decisions to usersâ€™ political beliefs. Most prior work either uses observational data to investigate whether current social media AIs exacerbate partisan animosity (Milli et al., 2023; TÃ¶rnberg, 2022) or examines the effect of bottom-up interventions on participantsâ€™ anti-democratic attitudes such as reducing exposure to content from like-minded sources (Nyhan et al., 2023) and reshared content (Guess et al., 2023b). To our knowledge, prior work has not taken a top-down approach to implement high-level democratic values into feed algorithms. Even though some prior work measured the influence of peopleâ€™s social media consumption on political activities such as voting decisions (Maruyama et al., 2014), we believe no prior work has taken a preemptive step to embed social science measurement in the construction of oneâ€™s social media feed. Adding to prior literature, our work translates social science measures of anti-democratic attitudes directly into objective functions and examines effects on partisan animosity.\n\n2.3. Democratic Values As A Lever: A Sociotechnical Approach\n\nGiven the potential harms that current engagement-based social media feeds may pose to democracy, our work uses democratic values as a lever to examine whether a feed that embeds democratic values could reduce partisan animosity. The key question then becomes: how do we operationalize democratic values? To address this question, we first adopt a measure of anti-democratic attitudes from political science research (Voelkel et al., 2023; Hartman et al., 2022), where the construct has been previously vetted and tested. We utilize its eight sub-scales to label each social media post, producing a continuous rating of the extent to which each post potentially impacts anti-democratic attitudes, and then replicate the manual rating using an algorithmic approach.\n\nWe do not claim that the construct of anti-democratic attitudes is the only construct that might matterâ€”far from itâ€”nor that its current operationalization is perfect. However, we find it far more productive to draw on social science expertise rather than re-invent the wheel. We chose to operationalize anti-democratic attitudes as an example due to its recent large-scale vetting in a large study by Voelkel et al. (2023) that includes 25 interventions (Nð‘Nitalic_N = 32,059). In this study, anti-democratic attitudes are measured for US partisans using eight variables, namely partisan animosity, support for undemocratic practices, support for partisan violence, support for undemocratic candidates, opposition to bipartisanship, social distrust, social distance, biased evaluation of politicized facts.\n\nRelated work by Hartman et al. (2022) defined partisan animosity as negative thoughts, feelings or behaviors towards outgroup; they argue that partisan animosity is an umbrella term that synthesizes a variety of concepts such as affective polarization, interpersonal polarization, and political sectarianism (Hartman et al., 2022). The Voelkel et al. megastudy found that almost all of their interventions successfully reduced partisan animosity, and several interventions reduced support for undemocratic practices and partisan violence (Voelkel et al., 2023). Other work in the field of political science also measured a subset of these eight outcome variables. For instance, Hartman et al. (2022) measured partisan animosity because they argue that the rising partisan animosity is associated with the decrease in support for democracy. Druckman et al. (2023) measured partisan animosity, support for undemocratic practices, and support for partisan violence through a survey experiment, and found that correcting misperceptions of out-partisans can decrease American legislatorsâ€™ support for undemocratic practices and marginally significantly reduce their partisan animosity.\n\nWe envision that future work can develop a larger set of these constructs to integrate into social media AIs and trade off amongst one another. One reason we choose anti-democratic attitudes as our construct of interest here is that it is relatively broad, measuring not just partisan animosity but also several other subscales, which aligns with realistic social media settings where there are multiple competing values to consider and trade off. The Voelkel et al. study was also conducted by a nationally-representative pool of social science researchers and studied a large, diverse sample of participants, so it represents a significant contribution to the literature on anti-democratic attitudes. However, our approach is not restricted to this particular paper and choice of construct, and the same approach could be used to translate other constructs to algorithmic objectives.\n\n3. Societal Objective Functions\n\n3.1. Operationalizing Anti-Democratic Attitudes into Social Media AIs\n\nWe introduce the term societal objective function to refer to our method of translating well-established social science constructs into an AI objective function. In this work, the goal of our algorithmic objective is to reduce partisan animosity through social media feeds.\n\n3.1.1. The anti-democratic attitudes construct\n\nCreating a societal objective function begins with anchoring to a construct in the social and behavioral sciences. In our case, as mentioned, we use anti-democratic attitudes. Detailed definitions and example measures of each of the eight variables are shown in Table 2. These variables were selected because they are important measurements related to psychology underlying polarization and democracy (Voelkel et al., 2023).\n\n3.1.2. Dataset\n\nTo generate the social media feed stimulus for our three studies, we source real-world posts drawn from CrowdTangle, a tool hosted by Meta that allows external parties to monitor public posts on Facebook (Fletcher et al., 2018). Given our focus on democratic values, we source political posts on CrowdTangle by filtering to several politics-related page categories and select those with the highest number of total interactions (likes, shares, comments, and reactions). Using this method, we gather 10,0001000010,00010 , 000 political Facebook posts from CrowdTangle that were posted between January 1 and February 1, 2023. We then use the systematic random sampling method to select a final inventory of nð‘›nitalic_n = 405 posts for manual labeling. Specifically, we split the CrowdTangle posts into buckets based on overall weighted engagement counts (e.g., Likes), and sample posts across these buckets to ensure a broad range of engagement in the posts in our study.\n\n3.1.3. Translating from manual coding to feed ranking interfaces\n\nThen, two members of the research team served as expert annotators to rate each political post using the anti-democratic attitude scale. For each anti-democratic outcome variable, a rating is given on a 3-point scale, with a score of 3 indicating strong presence of that variable (e.g., strong partisan animosity), a score of 2 indicating some presence (e.g., some partisan animosity), and a score of 1 indicating no presence (e.g., no evidence of partisan animosity). The two researchers adapted the existing construct from the literature deductively into a detailed coding scheme for each of the eight variables. Notably, the Voelkel et al. (2023) study evaluates anti-democratic attitudes of people, while we seek to evaluate social media posts. Thus, we choose to rate the posts by the extent to which they would promote a given anti-democratic attitude in the audience. Some new factors of the constructs, such as â€œemotion or exaggeration,â€ also emerged inductively in the process of creating the coding scheme. For instance, for partisan animosity, they tagged two factors in their rating procedureâ€”factor A: partisan name-calling and factor B: emotion or exaggeration.\n\nIn this case, a post was given a rating of 1 if neither factor applied, a rating of 2 if only one of the factors applied, and a rating of 3 if both factors applied.\n\nWhile our operationalization is anchored in the prior literature, we anticipate that future work will offer further refinements of the codebook â€” our work is not dependent on the exact operationalization.\n\nEach post is given a total of 8 ratings, one for each of the variables, which are then summed to arrive at a total democratic attitude score (min = 8, max = 24). The two independent coders achieved strong inter-coder reliability (Krippendorffâ€™s Î±ð›¼\\alphaitalic_Î± = .895) and used the above process to code all 405 political Facebook posts in the inventory. Full inter-coder reliability results are shown in Table 5.\n\nThe manual democratic attitude scores are then used to re-rank social media feed interfaces. In Study 1, we use this approach to compare democratic attitude feeds with status quo feed ranking methods. For example, we sort our inventory of social media posts to produce feeds ranked by manually-rated democratic attitude scores or by total interaction scores from CrowdTangle. The total interaction score is a weighted sum of the following engagement metrics: Share, Comment, Like, Love, Wow, Haha, Sad, Angry, and Care. Each engagement metric is given equal weight. Our dataset includes posts with total interaction scores ranging from 24 (low interaction) to 92,520 (high interaction). Our full set of feed ranking conditions is described in Section 3.2.1.\n\n3.1.4. Using LLMs to replicate manual coding at scale\n\nThe final step of creating a societal objective function involves using LLMs to replicate the same social science construct at scale. In our work, we scale up the anti-democratic attitude construct by turning the eight variables into zero-shot classification prompts for a large language model. These prompts are built on the exact same coding scheme developed for the manual raters, and used as inputs to the GPT-4 large language model to output ratings. Specifically, we prompt GPT-4 to rate each social media post from our social media post inventory dataset (development set: nð‘›nitalic_n = 205; test set: nð‘›nitalic_n = 200) on all eight variables. The development set is used to refine and iterate on the prompts, and the test set is set aside to conduct final performance evaluations. The full prompts are included in Appendix 8.3. Then, as with the manual rating procedure, we sum the eight anti-democratic attitude scores to produce the total anti-democratic attitude score.\n\nIn Study 2, we first compare GPT-4â€™s ratings with manual ratings. Then, in Study 3, we use ratings from GPT-4 to produce an LLM-ranked feed condition to compare against Study 1â€™s manually-ranked and engagement-ranked feed conditions.\n\n3.2. Feed Ranking Conditions and Hypotheses\n\nWe pre-registered our research questions and hypotheses on Open Science Framework (OSF) (Foster and Deardorff, 2017) and conduct two online experiments (Study 1: Nð‘Nitalic_N = 1,380; Study 3: Nð‘Nitalic_N = 558) to measure the impact of democratic attitude feeds on US partisansâ€™ partisan animosity, support for undemocratic practices, feed-level satisfaction, and engagement.\n\n3.2.1. Feed ranking conditions\n\nPast work has examined different approaches to reduce the societal harm caused by social media, such as using content moderation (Kozyreva et al., 2023), downranking or removing harmful content or misinformation (Epstein et al., 2020), placing content warnings to warn potential viewers that content is sensitive or may bring up difficult emotions (Haimson et al., 2020), or displaying more balanced information in oneâ€™s social media feed (e.g., ideologically balanced content from both liberal and conservative sources (Celis et al., 2019)).\n\nBuilding on prior work, we propose three democratic attitude feeds, including:\n\n(1)\n\nDownranking feed (i.e., ranked by anti-democratic attitude score such that posts with stronger anti-democratic attitudes are placed at the bottom of the feed)\n\n(2)\n\nContent Warning feed which mirrors designs commonly used by real-world social media platforms to mask harmful content (i.e., ranked by engagement, but anti-democratic posts are blurred out with content warnings)\n\n(3)\n\nRemove-and-Replace feed (i.e., ranked by engagement, but anti-democratic posts are replaced with pro-democratic posts sourced from our dataset inventory (nð‘›nitalic_n = 405))\n\nCurrent feed ranking systems often focus on optimizing usersâ€™ engagement to increase user retention (Wu et al., 2017) and maximize profit (Ciampaglia et al., 2018), which is likely to up-rank the most controversial content and thus increase partisan animosity. Many scholars have articulated their concerns about engagement-based feeds and often compare the impact of engagement-based feeds with reverse-chronologically ordered feeds that are free from algorithmic curation (Paek et al., 2010; HuszÃ¡r et al., 2022). We thus compare our democratic attitude feeds against both engagement-based feeds that emulate the status quo of feed ranking and reverse-chronological feeds that serve as a control condition. In addition, we compare our democratic attitude feeds against the ideologically balanced approaches proposed by prior work (Celis et al., 2019). Then, to estimate the baseline level of partisan animosity, we include a null control condition where participants are not exposed to any social media feed. We thus arrive at four comparison conditions: (1) an Engagement-Based feed, (2) an Ideologically Balanced feed, (3) a Control (Chronological) feed, and (4) a Null Control condition with no feed shown. See Figure 3 for a detailed description of all feed ranking conditions.\n\n3.2.2. Research Questions and Hypotheses\n\nOur study intends to examine the following overarching research questions:\n\nRQ1:\n\nHow will partisansâ€™ partisan animosity and support for undemocratic practices differ after exposure to different social media feeds?\n\nRQ2:\n\nHow will partisansâ€™ level of satisfaction and engagement with feeds differ after exposure to different social media feeds?\n\nPrior work found that current social media platformsâ€™ ranking algorithms amplify political content (HuszÃ¡r et al., 2022) and increase opinion polarization (Ciampaglia et al., 2018; Chitra and Musco, 2020; Rowland, 2011). There is a growing literature on the ways in which ranking algorithms trained on engagement data might facilitate the formation of â€œecho chambersâ€ (Sunstein, 2001) or â€œfilter bubblesâ€ (Rowland, 2011). Recent work found that interventions such as changing public discourse and transforming political structures or correcting misconceptions and highlighting commonalities can decrease peopleâ€™s animosity (Voelkel et al., 2023; Hartman et al., 2022). Following a large body of literature on partisan animosity, we also predict that our democratic attitude feed can effectively reduce partisan animosity. Thus, we predict H1:\n\nH1:\n\nPartisans exposed to the (a) downranking, (b) content warning, and (c) removal feed conditions on social media will reduce partisan animosity compared to partisans in the engagement feed and chronological feed (controls).\n\nVoelkel et al. (2023) point out that many scholars are also concerned about Americansâ€™ support for undemocratic practices besides the level of dislike between rival partisans (i.e., partisan animosity) (Finkel et al., 2020). Given the increasing scholarly attention to the importance of support for undemocratic practices, we also predict H2:\n\nH2:\n\nPartisans exposed to the (a) downranking, (b) content warning, and (c) removal feed conditions on social media will reduce support for undemocratic practices compared to partisans in the engagement feed and chronological feed (controls).\n\nRecent prior work found that peopleâ€™s perceptions of content moderation depends on partisanship: Republicans were more likely to oppose content moderation than Democrats possibly because they consider it a threat to freedom of speech (Kozyreva et al., 2023). Other studies also found that content moderation may trigger peopleâ€™s concerns about platform censorship (Riedl et al., 2022). Based on prior work, we predict that partisans exposed to the content warning feed will perceive a higher level of threat to freedom of speech compared to other feeds and propose H3.\n\nH3:\n\nPartisans exposed to the content warning feed will perceive a higher level of threat to freedom of speech compared to partisans exposed to other feeds.\n\n4. Study 1: The Impact of Manually Labeled Democratic Attitude Feeds\n\nIn Study 1, we examined the impact of manually-generated democratic attitude feeds on the partisan animosity of US partisans. By experimenting with a manual version of our ranking algorithm, we aim to understand the effect of democratic attitude feeds given a hypothetical â€œperfectâ€ AI that reflects the initial social science construct via expert annotation.\n\n4.1. Method\n\n4.1.1. Experimental Design\n\nWe created a social media feed named PolitiFeed that consists of a wide variety of real-world political posts, ranging from posts with high anti-democratic attitudes and low anti-democratic attitudes. We conducted a between-subjects design among US partisans (Nð‘Nitalic_N = 1,380) in March 2023 with seven conditions, respectively the downranking (nð‘›nitalic_n = 191), content warning (nð‘›nitalic_n = 192), removal (nð‘›nitalic_n = 193), engagement-based (nð‘›nitalic_n = 207), ideologically balanced (nð‘›nitalic_n = 197), chronological feed (nð‘›nitalic_n = 198), and null control (nð‘›nitalic_n = 202). We randomly assigned participants to one of the seven conditions. In different treatment conditions, participants were exposed to scrollable social media feeds with the same inventory content but different ranking methods. In the control condition, participants were exposed to a chronological social media feed. Participants in each feed condition were asked to read 60 political posts sourced from the inventory dataset, as 60 posts roughly replicate two full loads of the Twitter timeline. In the null control condition, participants were not exposed to any feeds. Participants spent on average 414.51 seconds on viewing the feed and 631.31 seconds on answering the questionnaire.\n\n4.1.2. Participants\n\nTo detect a main effect of condition and interactions with peopleâ€™s partisan affiliation, a priori power analysis using G*Power determined that a total sample of at least 1,369 participants would be needed to achieve 80% power if Î±ð›¼\\alphaitalic_Î± = .05 and effect size fð‘“fitalic_f = .10 were assumed to be the minimum effect size of interest. We recruited 1,427 participants in March 2023 using CloudResearch Connect, an online participant pool that aggregates multiple market research platforms (Litman et al., 2017). Participants were all from the United States and were required to be over 18 years old. We also filtered out participants who indicated they were non-partisans. Participants who identified as â€œTrue Independentsâ€ who leaned neither toward the Democratic nor Republican parties were filtered out of the study before participants were randomly assigned to conditions. After ruling out people who failed the embedded attention check question (nð‘›nitalic_n = 26), were under the age of 18 (nð‘›nitalic_n = 1), used duplicate IP addresses (nð‘›nitalic_n = 15), gave incomplete answers (nð‘›nitalic_n = 1), or spent less than two minutes on the survey (nð‘›nitalic_n = 4), 1,380 participants remained in the data analysis.\n\n4.1.3. Platform Development\n\nIn order to review the effects of ranking political content in realistic social media environments, we created the PolitiFeed web page that resembles modern social media feeds and used real Facebook posts from political candidates as content for this mock social media feed. This webpage was developed using Flask, a Python web framework, and included study filtering and attention checks as well as the mock social media feed. The site was deployed on Heroku, a cloud-based deployment platform, for easy accessibility and stability when handling a significant influx of subjects. The data was recorded via Qualtrics and included participantsâ€™ responses and engagement with the political content presented on the web page.\n\nWe designed PolitiFeed to resemble modern social media platforms to emulate usersâ€™ behavior and experiences on their own social media feeds and better measure the impacts of alternative ranking methods. Additionally, we intentionally designed the website logo â€œPolitiFeedâ€ with a gradient color (both blue and red) to limit potential political bias. An example of the website interface is shown in Figure 4.\n\n4.1.4. Measurements\n\nHere, we provide an overview of our Study 1 measures.\n\nâ€¢\n\nPartisan Animosity. Adapting from (Voelkel et al., 2023), we measured partisan animosity using an instrument called the â€œfeeling thermometerâ€ (Mð‘€Mitalic_M = 74.67, Sâ¢Dð‘†ð·SDitalic_S italic_D = 22.26). We asked participants to rate their feelings towards both Democrats and Republicans on a 101-point scale. Participants were told, â€œRatings between 50 degrees and 100 degrees mean that you feel favorable and warm toward them. Ratings between 0 degrees and 50 degrees mean that you donâ€™t feel favorable toward them and that you donâ€™t care too much for them. You would rate them at the 50 degree mark if you donâ€™t feel particularly warm or cold toward them.â€ We then reverse coded participantsâ€™ rating towards outpartisans so that higher scores indicated higher partisan animosity.\n\nâ€¢\n\nSupport for Undemocratic Practices. Adapting from (Voelkel et al., 2023), we asked participants to rate their support for undemocratic practices (Cronbachâ€™s Î±ð›¼\\alphaitalic_Î± = .76, Mð‘€Mitalic_M = 15.62, Sâ¢Dð‘†ð·SDitalic_S italic_D = 18.51) on a 101-point scale by indicating the extent to which they disagree or agree with each of the following statements:\n\nâ€“\n\n(Republicans/Democrats) should reduce the number of polling stations in areas that support (Democrats/Republicans).\n\nâ€“\n\n(Republican/Democratic) governors should ignore unfavorable court rulings by (Democrat/Republican) -appointed judges.\n\nâ€“\n\n(Republican/Democratic) governors should prosecute journalists who accuse (Republican/Democratic) politicians of misconduct without revealing sources.\n\nâ€“\n\n(Republicans/Democrats) should not accept the results of elections if they lose.\n\nâ€¢\n\nPerceived Threat to Freedom of Speech. Adapting from previous research (Dillard and Shen, 2005; Moyer-GusÃ© and Nabi, 2010), four items were included to assess the degree to which one perceives a threat to freedom in response to the ranking methods. Participants were asked to indicate how much they agree or disagree with the following statements based on a 7-point scale (1 = strongly disagree, 7 = strongly agree): â€œThe social media platform I just used threatened usersâ€™ freedom to express on social media,â€ â€œThe social media platform I just used tried to manipulate users on the platform,â€ â€œThe social media platform I just used tried to pressure users on the platform,â€ and â€œThe social media platform I just used tried to make a decision for users.â€ The four items were highly correlated and could be formed into a reliable index (Cronbachâ€™s Î±ð›¼\\alphaitalic_Î± = .93, Mð‘€Mitalic_M = 2.79, Sâ¢Dð‘†ð·SDitalic_S italic_D = 1.57).\n\nâ€¢\n\nFeed-level Satisfaction. We adapted feed-level measures from Jannach and Adomavicius (2016) and a survey conducted by Facebook in 2019. After exposure to the holistic social media feed, participants were asked to rate three items: â€œIs PolitiFeed you just read worth your time?â€, â€œAre you satisfied with PolitiFeed?â€, and â€œIs PolitiFeed you just read helpful for users to find relevant items?â€ on 7-point scales from 1 (not at all) to 7 (very). The three items were highly correlated and could be formed into a reliable index (Cronbachâ€™s Î±ð›¼\\alphaitalic_Î± = .91, Mð‘€Mitalic_M = 4.74, Sâ¢Dð‘†ð·SDitalic_S italic_D = 1.95).\n\nâ€¢\n\nTime Spent on the Social Media Feed. We measured engagement level by recording participantsâ€™ time elapsed on the social media feed (Mð‘€Mitalic_M = 414.51 seconds).\n\nâ€¢\n\nPost-survey Measurements. We collected demographic information from participants, such as partisanship strength, education, income, gender, and social media use.\n\n4.1.5. Data Analysis Plan\n\nWe used multiple generalized linear models (GLMs) with post hoc tests using Bonferroni correction to compare the effects of different ranking methods on participantsâ€™ partisan animosity, engagement level, and feed-level satisfaction.\n\n4.2. Results\n\n4.2.1. Democratic attitude feeds significantly reduced partisan animosity\n\nH1 was mostly supported: it predicted that partisans exposed to the downranking, content warning, and removal conditions will reduce their partisan animosity compared to partisans exposed to the engagement feed and chronological feed. We visualize the results in Figure 5. A generalized linear model (GLM) was used to test the effects of different feeds and party affiliation on the dependent variable, partisan animosity. Both feed condition and party affiliation were entered into the model as independent variables. There was a significant main effect of feed condition, F(6, 1366) = 2.68, pð‘pitalic_p = .01, Î·p2superscriptsubscriptðœ‚p2\\eta_{\\text{p}}^{2}italic_Î· start_POSTSUBSCRIPT p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = .01, and a significant main effect of party, Fð¹Fitalic_F(1, 1366) = 33.93, pð‘pitalic_p Â¡ .001, Î·p2superscriptsubscriptðœ‚p2\\eta_{\\text{p}}^{2}italic_Î· start_POSTSUBSCRIPT p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = .02 on partisan animosity. There was no significant interaction effect between condition and party, Fð¹Fitalic_F (6, 1366) = 1.04, pð‘pitalic_p = .40, Î·p2superscriptsubscriptðœ‚p2\\eta_{\\text{p}}^{2}italic_Î· start_POSTSUBSCRIPT p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = .01, which means the effect of the feed condition on partisan animosity did not differ by partisanship.\n\nWe report a full set of pairwise comparisons between conditions in Table 3. Notably, the removal and downranking conditions resulted in significantly less partisan animosity than the traditional engagement-based feed (d=âˆ’.20ð‘‘.20d=-.20italic_d = - .20, pð‘pitalic_p = .04, and d=âˆ’.25ð‘‘.25d=-.25italic_d = - .25, pð‘pitalic_p = .02). The effect of the content warning condition was only marginal (d=âˆ’.18ð‘‘.18d=-.18italic_d = - .18, p=.10ð‘.10p=.10italic_p = .10) â€” as seen in Figure 5, it was roughly as effective as the other democratic attitude conditions for Democrats, but far less effective for Republicans. The three democratic attitude feeds had no measurable difference from the chronological condition (all p>.05ð‘.05p>.05italic_p > .05). In summary, while the content warning feed backfired with conservatives, the removal and downranking feeds decreased partisan animosity compared to the engagement feed.\n\nWe additionally examined the possibility of differential impacts of the feed condition across strong versus weak partisans. We conducted exploratory analyses (that were not pre-registered) by adding partisanship strength as a moderator to the aforementioned generalized linear model. We found a significant moderation effect of partisanship strength on partisan animosity, pð‘pitalic_p = .03. Specifically, the significant difference between the removal and engagement feeds primarily came from weak partisans, d=âˆ’.42ð‘‘.42d=-.42italic_d = - .42, pð‘pitalic_p = .01, rather than strong partisans, d=âˆ’.14ð‘‘.14d=-.14italic_d = - .14, pð‘pitalic_p = .51. Similarly, we found the significant difference between the downranking and engagement conditions also came from weak partisans, d=âˆ’.29ð‘‘.29d=-.29italic_d = - .29, pð‘pitalic_p = .04, instead of strong partisans, d=âˆ’.22ð‘‘.22d=-.22italic_d = - .22, pð‘pitalic_p = .24.\n\nH2, which tested the impact on support for undemocratic practices, was not supported; however, this is consistent with prior work, which found that support for undemocratic practices is more resistant to short-term interventions than partisan animosity (Voelkel et al., 2023). A generalized linear model (GLM) was used to test the effects of different feeds and party affiliation on the dependent variable, support for undemocratic practices. Two independent variables, feed condition and party affiliation, were entered into the model as IVs. There was no significant interaction between feed condition and party, Fð¹Fitalic_F (6, 1366) = 1.11, pð‘pitalic_p = .36, partial Î·p2superscriptsubscriptðœ‚p2\\eta_{\\text{p}}^{2}italic_Î· start_POSTSUBSCRIPT p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = .01 on support for undemocratic practices, and no main effect of feed condition, Fð¹Fitalic_F (6, 1366) = .93, pð‘pitalic_p =.47, partial Î·p2superscriptsubscriptðœ‚p2\\eta_{\\text{p}}^{2}italic_Î· start_POSTSUBSCRIPT p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = .004. There was a significant main effect of party affiliation, Fð¹Fitalic_F (1, 1366) = 59.64, pð‘pitalic_p Â¡ .001, partial Î·p2superscriptsubscriptðœ‚p2\\eta_{\\text{p}}^{2}italic_Î· start_POSTSUBSCRIPT p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = .04 on support for undemocratic practices. Specifically, Republicans (Mð‘€Mitalic_M=20.52, Sâ¢Dð‘†ð·SDitalic_S italic_D= 21.0) had significantly higher support for undemocratic practices than Democrats (Mð‘€Mitalic_M=12.73, Sâ¢Dð‘†ð·SDitalic_S italic_D = 16.10), pð‘pitalic_p Â¡ .001, Cohenâ€™s dð‘‘ditalic_d = .42. Pairwise comparisons showed no significant differences across different feeds.\n\n4.2.2. Democratic attitude feeds did not compromise feed-level satisfaction and engagement\n\nThe downranking feed did not compromise participantsâ€™ satisfaction with the feed. There was a marginal significant interaction between feed condition and party, Fð¹Fitalic_F (5, 1163) = 2.14, pð‘pitalic_p=.06, partial Î·p2superscriptsubscriptðœ‚p2\\eta_{\\text{p}}^{2}italic_Î· start_POSTSUBSCRIPT p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = .01 on feel-level satisfaction. There was a significant main effect of feed condition, Fð¹Fitalic_F (5, 1163) = 3.15, pð‘pitalic_p = .008, partial Î·p2superscriptsubscriptðœ‚p2\\eta_{\\text{p}}^{2}italic_Î· start_POSTSUBSCRIPT p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = .013 on feed-level satisfaction, but no main effect of party affiliation, Fð¹Fitalic_F (1, 1163) = .02, pð‘pitalic_p=.89, partial Î·p2superscriptsubscriptðœ‚p2\\eta_{\\text{p}}^{2}italic_Î· start_POSTSUBSCRIPT p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = .000. In fact, Democrats exposed to the downranking and removal-and-replace feed even reported greater satisfaction with the feed compared to those exposed to the feed ranked by engagement (downranking vs. engagement: pð‘pitalic_p = .04, Cohenâ€™s dð‘‘ditalic_d = .25; removal vs. engagement: pð‘pitalic_p = .005, Cohenâ€™s dð‘‘ditalic_d = .35). For Republicans, there was no significant difference between their satisfaction with the downranking and the engagement-based feed, pð‘pitalic_p =.79. Republicans in the removal feed showed significantly higher level of satisfaction compared to those in the engagement feed, pð‘pitalic_p = .039, Cohenâ€™s dð‘‘ditalic_d = .36, as shown in Figure 6. When comparing to the chronological feed, partisans exposed to the removal feed had significantly higher feed-level satisfaction compared to those exposed to the chronological feed, pð‘pitalic_p = .017, Cohenâ€™s dð‘‘ditalic_d =.13, but there was no significant difference between the downranking and chronological feed, pð‘pitalic_p = .84.\n\nIn addition, we found no significant difference between the time spent on feed when partisans were assigned to a downranking feed (Mð‘€Mitalic_M=488.52) and the engagement-based feed (Mð‘€Mitalic_M=400.24), pð‘pitalic_p = .30, Cohenâ€™s dð‘‘ditalic_d = .11 (in seconds). Detailed means and Sâ¢Dð‘†ð·SDitalic_S italic_Ds are reported in the Appendix 6.\n\n4.2.3. Content warning feed prompted freedom of speech concerns\n\nH3, testing threats to freedom of speech, was supported: the removal and downranking feeds did not prompt freedom of speech threats in participants, but the content warning feed did. A generalized linear model (GLM) was used to test the effects of different feeds and party affiliation on the dependent variable, perceived threat to freedom of speech. Similarly, both feed condition and party affiliation were entered into the models as IVs. There was a marginally significant interaction effect between condition and party, Fð¹Fitalic_F (5, 1163) = 2.13, pð‘pitalic_p =.06, partial Î·p2superscriptsubscriptðœ‚p2\\eta_{\\text{p}}^{2}italic_Î· start_POSTSUBSCRIPT p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = .01. There was a significant main effect of the feed condition, Fð¹Fitalic_F (5, 1163) = 16.86, pð‘pitalic_p Â¡.001, partial Î·p2superscriptsubscriptðœ‚p2\\eta_{\\text{p}}^{2}italic_Î· start_POSTSUBSCRIPT p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = .07, and a significant main effect of party, Fð¹Fitalic_F (5, 1163) = 8.25, pð‘pitalic_p =.004, partial Î·p2superscriptsubscriptðœ‚p2\\eta_{\\text{p}}^{2}italic_Î· start_POSTSUBSCRIPT p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = .01 on perceived threat to freedom of speech. Pairwise comparisons showed that partisans exposed to the content warning feed perceived a significantly higher level of threat to freedom of speech compared to partisans exposed to other feeds, pð‘pitalic_p Â¡.001, with a range of sizable effects (vs. chronological: Cohenâ€™s dð‘‘ditalic_d = .62; vs. downranking: Cohenâ€™s dð‘‘ditalic_d =.75; vs. engagement: Cohenâ€™s dð‘‘ditalic_d =.72; vs. removal-and-replace: Cohenâ€™s dð‘‘ditalic_d =.77; vs. ideology: Cohenâ€™s dð‘‘ditalic_d =.57). We suggest that this threat was likely the reason that the content warning condition was less effective than the other democratic attitude feeds in reducing partisan animosity.\n\n4.2.4. Summary\n\nConditions that used manual annotations to craft democratic attitude feedsâ€”the downranking feed and remove-and-replace feedâ€”significantly reduced partisan animosity without reducing engagement and satisfaction, and without raising freedom of speech concerns. Notably, this effect was observed primarily with weak partisans and not with strong partisans, whose strong attitudes are unlikely to be easily modified. Content warnings, using the same manual annotations, did prompt freedom of speech concerns amongst conservatives, and had no overall effect on reducing partisan animosity. None of the approaches impacted support for undemocratic practices.\n\n5. Study 2: Replicating Manual Annotations with a Large Language Model\n\nNext, we sought to explore whether we could use algorithmic methods to replicate the effects observed in Study 1. While we found that manual expert annotations served as an effective basis for downranking to mitigate partisan animosity in social media feeds, it is not feasible for manual annotations to serve real-world social media ranking systems. Thus, we turned to large language models (in particular, the GPT-4 model) to investigate how faithfully we could mirror expert annotations with automated approaches that could scale to re-rank social media feeds in production.\n\n5.1. Democratic Attitude Rating Using GPT-4\n\nRecent advances in large language models (LLMs) present an opportunity to carry out a broad range of classification tasks purely through natural language instructions. These â€œzero-shotâ€ capabilitiesâ€”the ability of models to perform tasks without requiring any additional training examplesâ€”grant a great deal of flexibility to communicate nuanced constructs (Kojima et al., 2022; Ouyang et al., 2022a). While more traditional deep learning models require substantial technical expertise and effort to gather data and train a performant task-specific model, zero-shot prompting substantially lowers the barrier by only requiring a verbal descriptions of the task and its requirements.\n\nWe thus focus on understanding the performance of LLMs for democratic values ranking: if zero-shot modeling with LLMs could sufficiently replicate manual annotations, it would present a path forward to scale up the effect of manual downranking while maintaining accessibility for stakeholders to adjust the values embedded in a social media platform.\n\n5.1.1. Method\n\nWe leverage GPT-4â€”at the time of writing, the state-of-the-art large language model developed by OpenAI (OpenAI, 2023)â€”to explore the feasibility of automating our democratic attitude model. GPT-4 is a transformer-based model that is pretrained to predict the next token in a document and further fine-tuned to generate outputs aligned with human preferences. Owing to the massive scale of data and parameters with which LLMs have been trained, researchers have found that these models display â€œin-context learningâ€ capabilities, whereby a model can carry out a task given only an input of natural language instructions and a few demonstrations of the task (Brown et al., 2020). Subsequently, instruction tuning methods like those used in GPT-4 have improved LLM capabilities to follow human instructions with methods like reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022a). These advancements have improved LLM performance on zero-shot learning, which omits demonstrations and relies purely on the modelâ€™s understanding of natural language instructions. Earlier models like GPT-3 and GPT-3.5 have displayed promising performance in applying predefined qualitative codebooks or labeling policies to classify text data. For example, prior work has achieved moderate to substantial agreement with human labels in social science tasks such as political ideology detection, misinformation classification, question analysis, and implicit hate speech detection (Xiao et al., 2023; Ziems et al., 2023; Huang et al., 2023; Do et al., 2022).\n\nTo develop our zero-shot prompts, we draw from the definitions provided by Voelkel et al. (2023) as well as the codebook that our own research team developed in Study 1 to perform manual annotation of the social media post inventory. We split this inventory into a development set (n=205) and a test set (n=200); we used only the development set to iterate on and refine our prompts and reserved the test set for evaluation after we arrived at our finalized set of prompts.\n\nFor each anti-democratic variable, we use a prompt of the following format to annotate a social media post:\n\n[SystemMessage]\n\nPleaseratethefollowingmessageâ€™s{variablename}from1to3.\n\n{variablename}isdefinedas{variabledefinition}.\n\nYourratingshouldconsiderwhetherthefollowingfactorsexistinthefollowingmessage:\n\n{Variablefactors:alistoffactorsrelevanttothevariableandhowtheymaptothe3-pointscale}\n\nAfteryourrating,pleaseprovidereasoninginthefollowingformat:\n\nRating:__###Reason:__(###istheseparator)\n\n[UserMessage]\n\n{socialmediapostcontent}\n\nThe model then returns an â€œAssistant Messageâ€ that we then parse. The variable factors differ for each anti-democratic variable and are listed in full in Appendix 8.3. For a sample of the general format, the variable factors for the â€œsupport for undemocratic practicesâ€ variable are as follows:\n\nA:Showsupportforundemocraticpractices\n\nB1:Partisanname-calling\n\nB2:Emotionorexaggeration\n\nRate1ifdoesnâ€™tsatisfyanyofthefactors\n\nRate2ifdoesnâ€™tsatisfyA,butsatisfiesB1orB2\n\nRate3ifsatisfiesA,B1andB2\n\n5.1.2. Implementation details\n\nWe use the OpenAI Python API for GPT-4 to perform our zero-shot modeling task. We use the gpt-4-0314 chat completion model, which was the state-of-the-art version at the time of development, and we use a temperature setting of 0.7, consistent with other work that aims to emulate human annotations using LLMs (Pangakis et al., 2023; HÃ¤mÃ¤lÃ¤inen et al., 2023).\n\n5.2. Results\n\nFirst, we compared the overall ranking results (the total score ranging from 8 to 24) produced by manual ratings versus those produced by automated ratings. We evaluated on our held-out test set (n=200ð‘›200n=200italic_n = 200) to understand the holistic impact of the two rating methods on social media feed ranking. To capture the overall ranking similarity, we considered two metrics. First, Spearmanâ€™s rank correlation coefficient, Spearmanâ€™s ÏðœŒ\\rhoitalic_Ï, is a non-parametric measure of rank correlation that is used to assess the strength of and direction of a monotonic relationship between two variables. Meanwhile, Krippendorffâ€™s Î±ð›¼\\alphaitalic_Î± is a measure of the agreement between pairs of ratings and is often used in content analysis to assess inter-coder agreement; while other agreement measures such as Cohenâ€™s Îºðœ…\\kappaitalic_Îº are designed for categorical variables, Krippendorffâ€™s Î±ð›¼\\alphaitalic_Î± can be applied to ordinal variables like our anti-democratic attitude scores. We used both metrics to capture the similarity in the final feed ranking order as well as the agreement of the cumulative scores between GPT-4 and manual rating methods.\n\nWe observed that the automated ranking results produced by GPT-4 highly correlated with the manual ranking provided by human annotators (Spearmanâ€™s ÏðœŒ\\rhoitalic_Ï= .75, pð‘pitalic_p Â¡.001, Krippendorffâ€™s Î±ð›¼\\alphaitalic_Î± = .78). This result suggests that, at the omnibus level of the overall anti-democratic attitude construct, the prompts succeed in closely mirroring the manual labels. These results are consistent with that of prior work using LLMs, which have reported Spearmanâ€™s ÏðœŒ\\rhoitalic_Ï values ranging from .54.54.54.54 (Yang and Menczer, 2023) to .68.68.68.68 (Kim and Lee, 2023). We also repeated our procedure with GPT-3.5, the model version that preceded GPT-4, and find comparable performance results, as documented in Appendix 8.4.\n\nThen, diving into the individual anti-democratic attitude variables, we observed substantial agreement between GPT-4 ratings and manual ratings, with a median Krippendorffâ€™s Î±ð›¼\\alphaitalic_Î± of .647.647.647.647, as shown in Table 4. Treating the 3-point scale options as a multiclass classification task, we observed a mean classification accuracy of .815.815.815.815. Since the manual ratings for all variables were skewed towards the negative class (across variables, a mean of 79.2% examples received a manual rating of 1), we also calculated the F1 score, which is a more reliable indicator of model performance for imbalanced datasets. We observed a mean F1 score of .577.577.577.577 across variables. The only two individual variables that displayed Krippendorffâ€™s Î±ð›¼\\alphaitalic_Î± and F1 scores below .5.5.5.5 were support for undemocratic practices and support for undemocratic candidates. Based on manual ratings, both of these variables displayed extremely large skew towards the negative class such that 99% of examples in the test set were manually annotated with a score of 1. This skew made it challenging to calibrate the model and resulted in lower performance results. However, the large skew on those variables meant that even in the manually-ranked feeds, these two variables most often had a value of 1 and contributed less to the overall democratic attitude score than the other variables that displayed greater variation. Thus, misalignments in the GPT-4 ratings for the highly skewed variables did not appear to substantially affect alignment with manual ratings for the overall feed ranking. Thus, we concluded that our automated democratic values ranking method using GPT-4 appears to effectively replicate our manual ranking outcomes.\n\n5.2.1. Summary\n\nAdapting qualitative codebooks as zero-shot prompts, algorithmic feed ranking using GPT-4 achieved a strong correlation with the manual democratic attitude feed ranking. For individual anti-democratic attitudes, GPT-4 ratings generally aligned well with manual ratings except for two variables, support for undemocratic practices and support for undemocratic candidates, which were especially rare in the dataset.\n\n6. Study 3: Replication Using The Democratic Attitude Model\n\nClosing the loop, we wished to test whether a feed using the automated democratic attitude model labels replicated the effect on partisan animosity that we observed in Study 1 with the manual labels. In Study 3, we conducted a pre-registered replication study (Nð‘Nitalic_N = 558) among US partisans to compare the algorithmic downranking feed to a manual downranking feed and to a control feed ranked by engagement. Participants were recruited from CloudResearch Connect. Those who participated in Study 1 were ruled out from the study pool. The experimental procedure and measurements remained the same as Study 1.\n\n6.1. Experimental Design and Hypothesis\n\nIn Study 3, we conducted a between-subjects study design online experiment with three conditions in June 2023. The manual condition was the downranking feed from Study 1 that utilized manual expert labels. The algorithmic condition was a downranking feed using the output from the democratic attitude model using GPT-4 instead of manual labels. The engagement control condition was identical to Study 1. We predicted that partisans exposed to the algorithmic feed would reduce partisan animosity compared to partisans in the engagement feed :\n\nH4:\n\nPartisans exposed to the (a) manual downranking and (b) algorithmic downranking feed on social media will reduce partisan animosity compared to partisans in the engagement feed.\n\n6.2. Results\n\n6.2.1. Both algorithmic and manual downranked feeds significantly reduced partisan animosity.\n\nA generalized linear model (GLM) was used to test the effects of different feeds and party affiliation on the dependent variable, partisan animosity. Both feed condition and party affiliation were entered into the model as two independent variables. H4 was supported (Figure 8): the algorithmic, GPT-4 based democratic attitude feed reduced partisan animosity without impacting time on the site. There was a significant main effect of condition, Fð¹Fitalic_F(2,557) = 3.77, pð‘pitalic_p = .024, and a significant main effect of party, Fð¹Fitalic_F(1,557) = 53.31, pð‘pitalic_p Â¡.001, on partisan animosity. There was no significant interaction effect between party and condition, Fð¹Fitalic_F(2,557) = .52, pð‘pitalic_p = .60.\n\nMultiple pairwise comparisons using Bonferroni correction showed that partisans exposed to both the manual (Mð‘€Mitalic_M = 68.96, Sâ¢Dð‘†ð·SDitalic_S italic_D = 23.33) and the algorithmic (Mð‘€Mitalic_M = 69.32, Sâ¢Dð‘†ð·SDitalic_S italic_D = 22.59) downranked feeds displayed significantly lower partisan animosity compared to those exposed to the engagement feed (Mð‘€Mitalic_M = 74.82, Sâ¢Dð‘†ð·SDitalic_S italic_D = 19.73), manual vs. engagement: pð‘pitalic_p = .02, dð‘‘ditalic_d = -.27, algorithmic vs. engagement: pð‘pitalic_p = .036, dð‘‘ditalic_d = -.25. Same as in Study 1, there was no significant difference of time spent on the feed across conditions, Fð¹Fitalic_F(1,557) = .26, pð‘pitalic_p = .77, Î·p2superscriptsubscriptðœ‚p2\\eta_{\\text{p}}^{2}italic_Î· start_POSTSUBSCRIPT p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = .001 (manual: Mð‘€Mitalic_M = 310.16, Sâ¢Dð‘†ð·SDitalic_S italic_D = 256.96; algorithmic: Mð‘€Mitalic_M = 326.16, Sâ¢Dð‘†ð·SDitalic_S italic_D = 255.01; engagement-based: Mð‘€Mitalic_M = 318.47, Sâ¢Dð‘†ð·SDitalic_S italic_D = 275.05), which indicates that both democratic attitude feeds did not compromise time spent on the feed.\n\n6.2.2. Additional analyses on strong vs. weak partisans\n\nAs in Study 1, we conducted additional exploratory analyses (not pre-registered) to examine different impacts of feed conditions on strong vs. weak partisans. We again found a significant difference between the downranking and engagement conditions for weak partisans (manual vs. engagement: d=âˆ’.44ð‘‘.44d=-.44italic_d = - .44, pð‘pitalic_p = .05; algorithmic vs. engagement: d=âˆ’.26ð‘‘.26d=-.26italic_d = - .26, pð‘pitalic_p = .08), but not for strong partisans (manual vs. engagement: d=âˆ’.18ð‘‘.18d=-.18italic_d = - .18, pð‘pitalic_p = .81; algorithmic vs. engagement: d=âˆ’.14ð‘‘.14d=-.14italic_d = - .14, pð‘pitalic_p = .45).\n\n6.3. Summary\n\nIn summary, Study 3 replicated the original result using algorithmic labeling rather than manual annotation. The effect size between the downranking feeds and the engagement feed on reducing partisan animosity (manual vs. engagement: dð‘‘ditalic_d = -.27; algorithmic vs. engagement: dð‘‘ditalic_d = -.25), is also the same as Study 1. Additional analyses suggested that the significant differences between the downranking and engagement conditions come from weak partisans and not strong partisans.\n\n7. Discussion\n\nOur work seeks to directly embed democratic values into social media feed algorithms. We focused on political discourse on social media, and our results showed that encoding democratic values into social media feeds could reduce partisan animosity without compromising engagement and satisfaction with the feed. We identified a social science construct and its associated measures, used LLMs to replicate human annotations on those constructs, and redesigned social media feed ranking objectives to account for these LLM annotations. Our results shed light on the impact of feeds that encode societal values, which provides important theoretical and design implications for the field.\n\n7.1. Theoretical Implications\n\nThe first theoretical contribution of our work is the societal objective function, which seeks to translate social science theory into algorithmic objectives that drive user-facing systems. This sociotechnical approach allows us to embed societal values in social media AIs by translating social science measures of anti-democratic attitudes directly into objective functions. Our work is one of the first studies that not only 1) identifies the success of translating measures from social science theory into prompts for LLMs such as GPT-4, but also 2) proves that introducing societal objective functions into user-facing systems can result in significant attitudinal changes (i.e., reducing partisan animosity) by conducting empirical experimental studies with users. We envision that this approach can apply to many other domains to translate measures developed by the social scientists in areas such as mental health, well-being, and social equity into different algorithmic objectives. This approach opens up a new area for social media AIs to test and understand how a feed embedding various values might affect usersâ€™ attitudes and behaviors, along with other outcome variables that social media platforms may care about (e.g., engagement, user experience).\n\nSecond, we took a top-down, value-centered approach to intervene on reducing partisan animosity, which broadens the scope of polarization literature. Prior work tends to take two forms: one body of literature investigates whether todayâ€™s social media algorithms amplify partisan animosity (Milli et al., 2023; TÃ¶rnberg, 2022) or harm democracy (Lorenz-Spreen et al., 2023). Another line of work investigates the efficacy of bottom-up, â€œlight-touchâ€ interventions via websites or social media platforms, such as correcting out-partisan misperceptions through a chatbot (Hartman et al., 2022) or deploying interventions like readings, videos, or activities (Voelkel et al., 2023). However, prior work has not taken a top-down approach to investigate the impact of embedding high-level democratic values into social media AIs.\n\nOur third theoretical contribution is that our democratic attitude feed could significantly reduce partisan animosity among US partisans without compromising their feed engagement level. This outcome is, of course, dependent on further field and longitudinal studies. However, such results are consistent with past work that indicates that time spent on feed was not in conflict with reducing polarization (Saveski et al., 2022). Prior studies have focused on comparisons between feeds ranked in chronological order and feeds ranked by engagement metrics. For instance, one of the recent studies on Facebook during the 2020 US presidential election has examined the effect of substituting the algorithmic feed with a reverse-chronologically-sorted feed (Guess et al., 2023a). Extending prior work, our experiments examined seven different feeds that moved beyond chronological and engagement-based ranking to consider alternative feed designs oriented around democratic attitudes, such as a content warning feed that adds friction to viewing anti-democratic information. Furthermore, we see evidence that democratic attitude feeds are more effective in reducing partisan animosity among weak partisans than strong partisans, which suggests that democratic attitude feeds are more likely to depolarize people who are less extreme on the ideological spectrum.\n\nIn addition to the engagement metric, our work also examines peopleâ€™s perceived threat to freedom of speech and found that partisans exposed to the content warning feed perceived a significant higher level of threat to freedom of speech compared to partisans exposed to other feeds, which is consistent with prior work (Dillard and Shen, 2005; Moyer-GusÃ© and Nabi, 2010). We found a backfire effect in the content warning feed: conservatives are more likely to increase partisan animosity in the content warning feed compared to the engagement feed, which suggests that while the content warning condition may nudge people on the decision made by the platform, it may also accidentally create a backfire effect that over-corrects peopleâ€™s estimation of the percentage of anti-democratic information online.\n\nWhile this work was under review, a series of recent studies authored by political science researchers in collaboration with Meta were released in Nature and Science. Those studies have also examined the effects of reducing exposure to content from like-minded sources (Nyhan et al., 2023; Guess et al., 2023a), reshared content (Guess et al., 2023b), or substituting the algorithmic feed with a reverse-chronologically-sorted feed (Guess et al., 2023a) during the 2020 US presidential election. For instance, Guess et al. (2023a) found little evidence of Facebookâ€™s feed algorithms on altering the levels of affective polarization. How do we draw the line between our study, which found impacts, and theirs, which did not?\n\nOne possibility may be that, while it is challenging to shift long-held attitudes like affective polarization as Guess et al. (2023a) found, it may be possible to shift more immediate reactions and attitudes, especially when interventions are targeted carefully and in stronger doses. While the feeling thermometer measurement is traditionally treated as a long-term construct (Iyengar et al., 2019), our work applies this measurement in a short-term context and finds that peopleâ€™s feelings towards the opposite party may be movable when we consider the immediate effects of a social media intervention. Additionally, our study enacts a particularly strong intervention: while the Facebook feed typically contains a small percentage of political content (Guess et al., 2019), our feed intervention consists entirely of political social media posts. We also deploy this strong intervention on a more targeted sample of partisans rather than general Facebook users. In fact, we find that the participants most influenced by our intervention are weak partisans rather than strong partisans, which supports the notion that stronger, entrenched views may be difficult to shift.\n\nTaken together, these differences in our study design and findings suggest that short-term shifts may be a promising route to enact change. Even if we do not change peopleâ€™s longitudinal attitudes, short-term interventions have strong potential to reshape how users interact on social media and may allow them to engage in civil dialogue even if they hold differing long standing views.\n\n7.2. Design Implications\n\nIn â€œThe Two Culturesâ€ (Snow, 1959), C.P. Snow famously worried that the sciences were developing a culture wholly separate from the human-centered endeavors. Toward bridging this divide, our work suggests that it is possible to transfer some social science constructs and replicate their effects using a technical system based solely on natural language instruction inputs. These findings carry implications for future technical approaches that might embed societal values in social media and sociotechnical systems more broadly.\n\n7.2.1. Incorporating societal values in social media\n\nIn this work, we focus on encoding democratic attitudes in social media feeds because these values carry important ramifications for a healthy democracy. Todayâ€™s social media already embeds values. Though implicit, social media AIs hold a de facto position on how democratic attitudes are rendered in the feed, and they similarly hold a de facto position for any other societal value we may choose to study (Bernstein et al., 2023). Thus, we believe our societal objective functions approach should be used to experiment with a wide range of other societal values such as mental health, self-expression, diversity, and environmental sustainability (Stray et al., 2022). Work in this vein would allow us to make important values both explicit and tuneable.\n\nFurthermore, once a broader range of values have been explored and tested with users, we might start to explore feeds that combine different sets of values and better understand how the values trade off against each other and against status quo metrics like engagement and revenue that tend to be more aligned with corporate interests. While we found that encoding democratic attitudes did not have a negative impact on engagement metrics, as we expand the types of values and the combinations of values that we attempt to encode, we likely will encounter tougher tradeoffs between societal values and financial feasibility for platforms. Field experiments will be critical to better understand and navigate these tradeoffs.\n\nThere are also opportunities to intervene in different parts of the social media experience. While we intervened on feed ranking algorithms, societal objective functions might be valuable within post authoring interactions to encourage posters to self-reflect on the values expressed in their posts, or in notification systems to selectively notify users about content that upholds important values.\n\nFinally, it would be valuable to explore the impact of feeds that encode societal values both over longer stretches of time with longitudinal studies and across a diverse set of communities and platforms with expanded field deployments. A benefit of our LLM-based method is that it can scale up to large amounts of content and users, and it can be flexibly adapted to perform ranking for a range of potential platforms.\n\nBuilding on prior work on Value-Sensitive Algorithm Design (Zhu et al., 2018) which outlined methods for incorporating stakeholder values in a bottom-up manner, our work may also apply to non-social media contexts. Any task that requires content to be classified or scored, for example news recommendation, ad targeting, or toxicity detection, may benefit from integrating societal values. Thus, our approach could potentially help to streamline the workflow of transferring social science findings from more focused, in-lab studies to large-scale deployments in real-world systems. Additionally, since our approach does not require substantial technical expertise to implement and train a custom model, it may help to expand the set of researchers who can test alternative algorithms and interfaces.\n\n7.2.2. Technical method: extensions and limitations\n\nThe algorithmic approach taken in our work leverages a large language model to directly generate ratings. Past work has explored the use of LLMs to perform deductive coding that applies existing qualitative codebooks or labeling policies (Xiao et al., 2023; Ziems et al., 2023; Huang et al., 2023; Wang et al., 2021). We extend this work to design a codebook for an LLM to provide ratings for a complex concept of anti-democratic attitudes. Furthermore, a key contribution of our work is our societal objective functions method, which not only evaluates the LLM ratings themselves (with comparisons to human ratings), but also investigates the impact of the LLM ratings when used to power a user-facing system in an online experiment. However, future approaches could utilize LLMs in a variety of other ways to achieve more efficient or fine-tuned results. For example, the LLM output could be used to perform knowledge distillation to train a smaller model that may be far less costly or time-intensive to use for inference to serve production systems (Hsieh et al., 2023; Tang et al., 2019). Alternatively, the LLM-based method could be used to first validate the social science constructs, and experimenters could proceed to build custom models for each construct using more traditional data-driven approaches.\n\nWhile the LLM-based method substantially lowers the barrier to entry, there are risks of using such a model for production social computing systems. First, LLM ratings may not always align with desired manual annotations and human judgment, especially if the model is applied to subjective concepts where the human population may disagree (Santurkar et al., 2023). Thus, it is important that model-generated ratings are closely monitored for any real-world deployments or field studies. Large language models may not achieve as high performance and annotator alignment for tasks that require domain-specific knowledge that falls outside of the training data of internet text, and their alignment with current societal values may shift over time. Furthermore, these models encode known biases (Lucy and Bamman, 2021; Nadeem et al., 2021; Sheng et al., 2019), and they may have a variety of biases that are not yet understood that may impact performance. For example, if an LLM displays poor understanding of the sentiment of language used by a particular community, it may struggle to recognize anti-democratic speech from that community. Another risk of directly applying LLM-based ranking models in production systems is that such models could be attacked with adversarial inputs that take advantage of known limitations such as prompt injection (Perez and Ribeiro, 2022) and vulnerabilities similar to those of traditional computer programs (Kang et al., 2023), so system developers must include safeguards against such practices.\n\n7.2.3. Implications for industry practitioners\n\nWhy should social media companies redesign their algorithms? If status quo ranking algorithms are serving these companies well, there may be little incentive to alter their algorithms to incorporate societal values (Ciampaglia et al., 2018; Milli et al., 2021). While engagement-oriented objectives speak to shorter-term user satisfaction and retention, societal objective functions speak to longer-term impacts on users and society at large. Thus, our method may enable platforms to better evaluate how the design decisions of today impact the viability of their platform far into the future. Furthermore, our work presents initial evidence that societal objective functions may not require compromises on the engagement metrics that social media platforms already care about.\n\nIf industry practitioners take on our approach, we recommend that they test the impact of manually-annotated social media feeds on real users before implementing any large-scale, algorithmic approaches, as we demonstrated in our work. This gradual, phased approach can help to reduce risks of harm to a platform and its users. Since our study was conducted on a much smaller group of participants than would be exposed on a real social media platform, unforeseen effects may result from large-scale deployments without such precautions.\n\nFinally, given that societal objective functions carry tremendous ethical implications on users and society, we recommend that industry practitioners continuously seek feedback from ethicists, researchers, policymakers, and community stakeholders to better understand the risks and benefits they pose.\n\n7.3. Limitations\n\nWe note several limitations of our work that may serve as important directions for future work. First, the participant pool used for our online experiments is limited to partisans based in the United States, so results may vary for U.S. non-partisans and participants based in other countries. In addition, these participants were sourced from a crowdsourcing platform (CloudResearch Connect), so they may not comprise a nationally representative sample of the pool of U.S. partisans, as workers CloudResearch may have relatively higher digital literacy than the general population (Yaqub et al., 2020; Jia et al., 2022).\n\nNext, our data source was restricted to only political content sourced from CrowdTangle, which draws from public posts on Facebook. Since political content comprises a relatively small portion of social media feeds (Scharkow and Bachl (2017) and Guess et al. (2019) suggest 6% political tweets and 2% political Facebook posts), our democratic attitude model may have a subtler effect on real-world feeds that hold a proportionally smaller inventory of political content. Results may also differ for feed posts sourced from different platformsâ€”such as Twitter, Reddit, or Instagramâ€”or for posts authored for a private rather than public audience. In addition, our Study 3 results indicate an overall decrease in partisan animosity across parties and conditions when we replicated Study 1 three months after it had been conducted. We suggest that future work use timely social media posts as stimuli for studies on social media AIs, which are generally time-sensitive.\n\nWhile we observed that downranking and remove-and-replace democratic attitude feeds reduced partisan animosity, we note that these reductions are comparable to the reductions achieved by a non-algorithmically ranked, reverse-chronological feed. These results suggest that status quo engagement-based ranking may pose the greatest risks to democratic attitudes and that platforms may consider chronological ordering if they turn to non-algorithmic ranking. However, given that platforms may prefer curated ranking approaches in contrast to idiosyncratic chronological ordering (Peters, 2022), democratic attitude feeds present a means to mitigate partisan animosity while maintaining curatorial control.\n\nAs noted earlier, while our online experiments demonstrated promising results by which democratic attitude feeds could reduce partisan animosity, further experiments are necessary to determine whether such feed interventions could bring about longitudinal effects. We recommend that future work carry out large-scale, longitudinal field experiments that intervene on usersâ€™ own social media feeds to understand the impact of democratic attitude feeds in the real world.\n\nFinally, our work focused on the societal value of mitigating partisan animosity, but we have not explored other potential societal values. There may be differing challenges in translating other social scientific constructs into societal objective functions, which may require different technical methods and may result in different effects on users in online experiments. An exciting direction of future work will be to explore a range of other societal values to investigate whether and how they can be translated into societal objective functions.\n\n7.4. Ethics and Societal Impact\n\nGiven the substantial influence that social media platforms hold on the diffusion of information, societal attitudes, and the functioning of democratic processes, we must consider the ethical implications of our work. First, any form of algorithmic feed ranking inherently poses risks to freedom of expression. In a democracy, all citizens are allowed the right to express their perspectives and opinions equally (Bernholz et al., 2021). A social media feed that algorithmically curates social media posts may not capture the full spectrum of perspectives on the platform and, by design, must limit the visibility of certain posts. While our work aims to explicitly foreground societal values that might enhance the diversity of information on social media feeds, our ranking method shares limitations with existing algorithmic ranking methods in that no social media platform can show all posts (van Mill, 2021).\n\nAnother risk of our approach is that it may lack nuance on how to handle posts that may appear misaligned with societal values. For example, while partisan animosity in large doses may exacerbate political divides, anger and other negative emotions may be appropriate in the face of injustice; protest and debate is a critical part of political discourse. To avoid this failure mode, societal objective functions must take care to achieve balance among multiple values rather than solely focusing on any one individual value.\n\nNext, our work provides a method to encode societal values in algorithmic objectives, but our method alone does not determine what values are encoded or who decides what values to encode. Thus, there are risks that the same methods that allow us to encode pro-social societal values could be exploited by bad actors to proliferate harmful content or manipulate ranking algorithms (Torres-Lugo et al., 2022). We note that todayâ€™s social media platforms already implicitly or explicitly encode certain societal values that reflect the values of their developers (Seaver, 2017), even if they were not intentionally specified. By making societal values an explicit, measurable goal, our approach provides to good-faith actors both a more realistic understanding of the values encoded in their status quo systems and a means to monitor improvements or adversarial attacks to those values. While we cannot prevent malicious actors from attempting to generate societally harmful content with similar methods, approaches like ours can help platforms to enhance their â€œdefenseâ€ in detecting and downranking harmful content. Another potential concern is that societal objective functions may have uneven impacts across diverse communities, which may complicate decisions about what values to implement. Extensive research has demonstrated that social media platforms often contribute to and amplify social inequities (Robinson et al., 2015; Hargittai et al., 2013; Lutz, 2022), and even attempts to promote algorithmic fairness may not in fact address the needs of marginalized communities (Gorwa et al., 2020; Hoffmann, 2019; Binns, 2017). Continued research on societal objective functions should incorporate the input of marginalized communities and carefully investigate the impacts on these communities to avoid perpetuating harmful disparities.\n\n8. Conclusion\n\nOur work embeds the social scientific construct of anti-democratic attitudes into a social media AI objective function. We demonstrate that the survey instruments from prior work on this construct can be adapted into prompts for a large language model (LLM), producing a democratic attitude model. Through a series of three studies, we found that social media feeds that integrate this democratic attitude model can significantly reduce partisan animosity without compromising user engagement levels. This societal objective function method presents a novel strategy for translating social science theory to algorithmic objectives, which opens up new possibilities to encode societal values in social media AIs.\n\nAcknowledgements.\n\nOur research was supported by the Hoffman-Yee Research Grants at Stanford Institute for Human-Centered Artificial Intelligence (HAI). Chenyan Jia was supported in part by Stanford Cyber Policy Center and Stanford Center on Philanthropy and Civil Society. Michelle S. Lam was supported by Stanford HAI and a Stanford Interdisciplinary Graduate Fellowship. We thank other members of our HAI research project, namely AngÃ¨le Christin, Jeanne Tsai, Johan Ugander, Nathaniel Persily, Tatsunori Hashimoto, Martin Saveski, Tiziano Piccardi, Chunchen Xu, and Marijn Nura Mado. We also thank our participants who provided valuable insights.\n\nReferences\n\n(1)\n\nAhler and Sood (2018) Douglas J Ahler and Gaurav Sood. 2018. The parties in our heads: Misperceptions about party composition and their consequences. The Journal of Politics 80, 3 (2018), 964â€“981.\n\nAllcott et al. (2020) Hunt Allcott, Luca Braghieri, Sarah Eichmeyer, and Matthew Gentzkow. 2020. The Welfare Effects of Social Media. American Economic Review 110, 3 (March 2020), 629â€“76. https://doi.org/10.1257/aer.20190658\n\nAre (2020) Carolina Are. 2020. How Instagramâ€™s algorithm is censoring women and vulnerable users but helping online abusers. Feminist media studies 20, 5 (2020), 741â€“744.\n\nBai et al. (2022) Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. 2022. Constitutional AI: Harmlessness from AI Feedback. arXiv:2212.08073 [cs.CL]\n\nBail (2022) Chris Bail. 2022. Breaking the social media prism: How to make our platforms less polarizing. Princeton University Press.\n\nBakshy et al. (2015) Eytan Bakshy, Solomon Messing, and Lada A Adamic. 2015. Exposure to ideologically diverse news and opinion on Facebook. Science 348, 6239 (2015), 1130â€“1132.\n\nBernholz et al. (2021) Lucy Bernholz, HÃ©lÃ©ne Landemore, and Rob Reich. 2021. Digital Technology and Democratic Theory. University of Chicago Press. https://doi.org/10.7208/chicago/9780226748603.001.0001\n\nBernstein et al. (2023) Michael S. Bernstein, AngÃ¨le Christin, Jeffrey T. Hancock, Tatsunori Hashimoto, Chenyan Jia, Michelle Lam, Nicole Meister, Nathaniel Persily, Tiziano Piccardi, Martin Saveski, Jeanne L. Tsai, Johan Ugander, and Chunchen Xu. 2023. Embedding Societal Values into Social Media Algorithms. Journal of Online Trust and Safety 2, 1 (2023).\n\nBhargava et al. (2019) Rahul Bhargava, Anna Chung, Neil S. Gaikwad, Alexis Hope, Dennis Jen, Jasmin Rubinovitz, BelÃ©n SaldÃ­as-Fuentes, and Ethan Zuckerman. 2019. Gobo: A System for Exploring User Control of Invisible Algorithms in Social Media. In Conference Companion Publication of the 2019 on Computer Supported Cooperative Work and Social Computing (Austin, TX, USA) (CSCW â€™19). Association for Computing Machinery, New York, NY, USA, 151â€“155. https://doi.org/10.1145/3311957.3359452\n\nBickert (2018) Monika Bickert. 2018. Publishing Our Internal Enforcement Guidelines and Expanding Our Appeals Process. https://about.fb.com/news/2018/04/comprehensive-community-standards/\n\nBinns (2017) Reuben Binns. 2017. Fairness in Machine Learning: Lessons from Political Philosophy. CoRR abs/1712.03586 (2017). arXiv:1712.03586 http://arxiv.org/abs/1712.03586\n\nBoxell et al. (2020) Levi Boxell, Matthew Gentzkow, and Jesse M Shapiro. 2020. Cross-Country Trends in Affective Polarization. Working Paper 26669. National Bureau of Economic Research. https://doi.org/10.3386/w26669\n\nBrady et al. (2023) William J Brady, Killian L McLoughlin, Mark P Torres, Kara F Luo, Maria Gendron, and MJ Crockett. 2023. Overperception of moral outrage in online social networks inflates beliefs about intergroup hostility. Nature human behaviour (2023), 1â€“11.\n\nBrown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language Models are Few-Shot Learners. Advances in neural information processing systems 33 (2020), 1877â€“1901.\n\nCelis et al. (2019) L Elisa Celis, Sayash Kapoor, Farnood Salehi, and Nisheeth Vishnoi. 2019. Controlling polarization in personalization: An algorithmic framework. In Proceedings of the conference on fairness, accountability, and transparency. 160â€“169.\n\nChitra and Musco (2020) Uthsav Chitra and Christopher Musco. 2020. Analyzing the impact of filter bubbles on social network polarization. In Proceedings of the 13th International Conference on Web Search and Data Mining. 115â€“123.\n\nCiampaglia et al. (2018) Giovanni Luca Ciampaglia, Azadeh Nematzadeh, Filippo Menczer, and Alessandro Flammini. 2018. How algorithmic popularity bias hinders or promotes quality. Scientific reports 8, 1 (2018), 15951.\n\nDillard and Shen (2005) James Price Dillard and Lijiang Shen. 2005. On the nature of reactance and its role in persuasive health communication. Communication monographs 72, 2 (2005), 144â€“168.\n\nDo et al. (2022) SalomÃ© Do, Ã‰tienne Ollion, and Rubing Shen. 2022. The Augmented Social Scientist: Using Sequential Transfer Learning to Annotate Millions of Texts with Human-Level Accuracy. Sociological Methods & Research (2022). https://doi.org/10.1177/00491241221134526 arXiv:https://doi.org/10.1177/00491241221134526\n\nDruckman et al. (2023) James N Druckman, Suji Kang, James Chu, Michael N. Stagnaro, Jan G Voelkel, Joseph S Mernyk, Sophia L Pink, Chrystal Redekopp, David G Rand, and Robb Willer. 2023. Correcting misperceptions of out-partisans decreases American legislatorsâ€™ support for undemocratic practices. Proceedings of the National Academy of Sciences 120, 23 (2023), e2301836120.\n\nEckles (2022) Dean Eckles. 2022. Algorithmic transparency and assessing effects of algorithmic ranking. https://doi.org/10.31235/osf.io/c8za6\n\nEpstein et al. (2020) Ziv Epstein, Gordon Pennycook, and David Rand. 2020. Will the crowd game the algorithm? Using layperson judgments to combat misinformation on social media by downranking distrusted sources. In Proceedings of the 2020 CHI conference on human factors in computing systems. 1â€“11.\n\nFinkel et al. (2020) Eli J Finkel, Christopher A Bail, Mina Cikara, Peter H Ditto, Shanto Iyengar, Samara Klar, Lilliana Mason, Mary C McGrath, Brendan Nyhan, David G Rand, et al. 2020. Political sectarianism in America. Science 370, 6516 (2020), 533â€“536.\n\nFletcher et al. (2018) Richard Fletcher, Alessio Cornia, Lucas Graves, and Rasmus Kleis Nielsen. 2018. Measuring the reach ofâ€ fake newsâ€ and online disinformation in Europe. Australasian Policing 10, 2 (2018).\n\nFoster and Deardorff (2017) Erin D Foster and Ariel Deardorff. 2017. Open science framework (OSF). Journal of the Medical Library Association: JMLA 105, 2 (2017), 203.\n\nGillespie (2018) Tarleton Gillespie. 2018. Custodians of the Internet: Platforms, content moderation"
    }
}
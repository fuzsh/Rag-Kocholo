{
    "id": "dbpedia_445_1",
    "rank": 69,
    "data": {
        "url": "https://ivado.ca/en/research-community/",
        "read_more_link": "",
        "language": "en",
        "title": "Research Community",
        "top_image": "https://ivado.ca/wp-content/themes/ivadooo/assets/favi/favicon-32x32.png",
        "meta_img": "https://ivado.ca/wp-content/themes/ivadooo/assets/favi/favicon-32x32.png",
        "images": [
            "https://www.facebook.com/tr?id=518851442441464&ev=PageView&noscript=1",
            "https://ivado.ca/wp-content/themes/ivadooo/assets/svg/siteLogoWhite.svg",
            "https://ivado.ca/wp-content/themes/ivadooo/assets/svg/siteLogoWhite.svg",
            "https://ivado.ca/wp-content/themes/ivadooo/assets/svg/siteLogo.svg",
            "https://ivado.ca/wp-content/themes/ivadooo/assets/svg/siteLogo.svg",
            "https://ivado.ca/wp-content/uploads/2020/04/CRM_logo-membres-academiques-380x122.png",
            "https://ivado.ca/wp-content/uploads/2020/04/CRM_logo-membres-academiques-380x122.png",
            "https://ivado.ca/wp-content/uploads/2021/09/CIM_logo_2000px_RGB-380x94.png",
            "https://ivado.ca/wp-content/uploads/2021/09/CIM_logo_2000px_RGB-380x94.png",
            "https://ivado.ca/wp-content/uploads/2020/04/Cirrelt-380x84.png",
            "https://ivado.ca/wp-content/uploads/2020/04/Cirrelt-380x84.png",
            "https://ivado.ca/wp-content/uploads/2020/04/consortium-sante-numerique_logo-membres-academiques-380x401.png",
            "https://ivado.ca/wp-content/uploads/2020/04/consortium-sante-numerique_logo-membres-academiques-380x401.png",
            "https://ivado.ca/wp-content/uploads/2020/04/GERAD_logo-membres-academiques-380x106.png",
            "https://ivado.ca/wp-content/uploads/2020/04/GERAD_logo-membres-academiques-380x106.png",
            "https://ivado.ca/wp-content/uploads/2022/09/IFRC-Logotype-Black-Round-380x166.png",
            "https://ivado.ca/wp-content/uploads/2022/09/IFRC-Logotype-Black-Round-380x166.png",
            "https://ivado.ca/wp-content/uploads/2022/09/InstitutCourtois_logo-380x146.png",
            "https://ivado.ca/wp-content/uploads/2022/09/InstitutCourtois_logo-380x146.png",
            "https://ivado.ca/wp-content/uploads/2021/05/iid-logo-380x152.png",
            "https://ivado.ca/wp-content/uploads/2021/05/iid-logo-380x152.png",
            "https://ivado.ca/wp-content/uploads/2022/09/logo-iric-en-380x181.png",
            "https://ivado.ca/wp-content/uploads/2022/09/logo-iric-en-380x181.png",
            "https://ivado.ca/wp-content/uploads/2020/02/Mila_logo-membres-academiques-380x155.png",
            "https://ivado.ca/wp-content/uploads/2020/02/Mila_logo-membres-academiques-380x155.png",
            "https://ivado.ca/wp-content/uploads/2021/09/obvia-380x132.png",
            "https://ivado.ca/wp-content/uploads/2021/09/obvia-380x132.png",
            "https://ivado.ca/wp-content/uploads/2020/08/SEMLA-logo-380x80.png",
            "https://ivado.ca/wp-content/uploads/2020/08/SEMLA-logo-380x80.png",
            "https://ivado.ca/wp-content/uploads/2020/04/Logo_tech3lab-380x122.png",
            "https://ivado.ca/wp-content/uploads/2020/04/Logo_tech3lab-380x122.png",
            "https://ivado.ca/wp-content/uploads/2022/09/unique-logo-380x427.png",
            "https://ivado.ca/wp-content/uploads/2022/09/unique-logo-380x427.png",
            "https://ivado.ca/wp-content/uploads/2020/04/4point0_logo-partenaires-academiques-380x108.png",
            "https://ivado.ca/wp-content/uploads/2020/04/4point0_logo-partenaires-academiques-380x108.png",
            "https://ivado.ca/wp-content/uploads/2020/04/amii_logo-partenaires-academiques-380x294.png",
            "https://ivado.ca/wp-content/uploads/2020/04/amii_logo-partenaires-academiques-380x294.png",
            "https://ivado.ca/wp-content/uploads/2022/02/Applied-AI-Institute-RGB-VR-380x151.png",
            "https://ivado.ca/wp-content/uploads/2022/02/Applied-AI-Institute-RGB-VR-380x151.png",
            "https://ivado.ca/wp-content/uploads/2020/04/Axel_logo-partenaires-academiques-380x120.png",
            "https://ivado.ca/wp-content/uploads/2020/04/Axel_logo-partenaires-academiques-380x120.png",
            "https://ivado.ca/wp-content/uploads/2020/04/bensadoun_logo-partenaires-academiques.png",
            "https://ivado.ca/wp-content/uploads/2020/04/bensadoun_logo-partenaires-academiques.png",
            "https://ivado.ca/wp-content/uploads/2022/03/CDRIN-380x197.png",
            "https://ivado.ca/wp-content/uploads/2022/03/CDRIN-380x197.png",
            "https://ivado.ca/wp-content/uploads/2020/04/cartel_logo-partenaires-academiques.png",
            "https://ivado.ca/wp-content/uploads/2020/04/cartel_logo-partenaires-academiques.png",
            "https://ivado.ca/wp-content/uploads/2020/04/CRDMUL_logo-partenaires-academiques-380x279.png",
            "https://ivado.ca/wp-content/uploads/2020/04/CRDMUL_logo-partenaires-academiques-380x279.png",
            "https://ivado.ca/wp-content/uploads/2020/04/CIRANO_logo-partenaires-academiques-380x68.png",
            "https://ivado.ca/wp-content/uploads/2020/04/CIRANO_logo-partenaires-academiques-380x68.png",
            "https://ivado.ca/wp-content/uploads/2020/04/CIRODD_logo_partenaire_academique-380x166.png",
            "https://ivado.ca/wp-content/uploads/2020/04/CIRODD_logo_partenaire_academique-380x166.png",
            "https://ivado.ca/wp-content/uploads/2022/02/UQAM-logo-380x124.png",
            "https://ivado.ca/wp-content/uploads/2022/02/UQAM-logo-380x124.png",
            "https://ivado.ca/wp-content/uploads/2020/04/FinML_logo-partenaires-academiques-380x112.png",
            "https://ivado.ca/wp-content/uploads/2020/04/FinML_logo-partenaires-academiques-380x112.png",
            "https://ivado.ca/wp-content/uploads/2022/03/Gric-logo-380x165.png",
            "https://ivado.ca/wp-content/uploads/2022/03/Gric-logo-380x165.png",
            "https://ivado.ca/wp-content/uploads/2021/11/Logo-Inrpme-380x97.png",
            "https://ivado.ca/wp-content/uploads/2021/11/Logo-Inrpme-380x97.png",
            "https://ivado.ca/wp-content/uploads/2020/04/cyberjustice_logo-partenaires-academiques-380x263.png",
            "https://ivado.ca/wp-content/uploads/2020/04/cyberjustice_logo-partenaires-academiques-380x263.png",
            "https://ivado.ca/wp-content/uploads/2020/04/LIVIA_logo-380x62.jpg",
            "https://ivado.ca/wp-content/uploads/2020/04/LIVIA_logo-380x62.jpg",
            "https://ivado.ca/wp-content/uploads/2021/09/Quantac-logo-380x42.png",
            "https://ivado.ca/wp-content/uploads/2021/09/Quantac-logo-380x42.png",
            "https://ivado.ca/wp-content/uploads/2020/04/RALI-380x501.png",
            "https://ivado.ca/wp-content/uploads/2020/04/RALI-380x501.png",
            "https://ivado.ca/wp-content/uploads/2020/04/RRSR_logo-partenaires-academiques.png",
            "https://ivado.ca/wp-content/uploads/2020/04/RRSR_logo-partenaires-academiques.png",
            "https://ivado.ca/wp-content/uploads/2021/06/logo-RIISQ.png",
            "https://ivado.ca/wp-content/uploads/2021/06/logo-RIISQ.png",
            "https://ivado.ca/wp-content/themes/ivadooo/assets/svg/siteLogoWhite.svg",
            "https://ivado.ca/wp-content/themes/ivadooo/assets/svg/siteLogoWhite.svg",
            "https://ivado.ca/wp-content/uploads/2020/04/CFREF-LOGOBLANC-1-380x187.png",
            "https://ivado.ca/wp-content/uploads/2023/04/UdeM-logo-blanc.png",
            "https://ivado.ca/wp-content/uploads/2020/04/gouvernementqclogotest-380x187.png",
            "https://ivado.ca/wp-content/uploads/2020/04/polytehniquemtl-logo-blanc-1-380x187.png",
            "https://ivado.ca/wp-content/uploads/2023/04/HEC-380x60.png",
            "https://ivado.ca/wp-content/uploads/2023/04/mcgill_logo_rev-380x91.png",
            "https://ivado.ca/wp-content/uploads/2023/04/logo-Ulaval-renverse-380x157.png",
            "https://px.ads.linkedin.com/collect/?pid=3210289&fmt=gif"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "https://ivado.ca/wp-content/themes/ivadooo/assets/favi/apple-touch-icon.png",
        "meta_site_name": "",
        "canonical_link": "https://ivado.ca/en/research-community/",
        "text": "Topic 1 – Integrated Machine Learning and Optimization for Decision Making under Uncertainty: Towards Robust and Sustainable Supply Chains\n\nLead researchers: Erick Delage (HEC Montréal, GERAD), Yossiri Adulyasak (HEC Montréal, GERAD), Emma Frejinger (Université de Montréal, CIRRELT)\n\nNearly all decision problems involve some form of uncertainty. This is especially true in supply chains where, e.g., demand, cost, capacity, and travel time’s high variability considerably complicate the planning of procurement, production, distribution, and service activities. Due to constantly evolving environments and the high frequency of data acquisition, classical decision-making that is based on training models, validating them, to finally optimize decisions does not suffice anymore. This research program aims at developing new methods for making the most effective and adaptive use of data in decision-making. It is founded on modern optimization and machine learning perspectives that encompasses developments in deep reinforcement/end-to-end learning, risk averse decision theory, and contextual/distributionally robust optimization. Its mission is three-fold: (i) develop the next generation of methods to deal with uncertainty in data-driven risk-aware optimization models by integrating machine learning; (ii) identify scientifically challenging and high-impact opportunities for improving robustness in supply chains; and finally (iii) stimulate the integration of stochastic optimization models among our partners while defining use cases that will guide future methodological advances. Overall, this program envisions a virtuous cycle of scientific discoveries that are both fueled by and transformative for an important sector of the Canadian economy.\n\nTopic 2 – AI, Biodiversity and Climate Change\n\nLead researchers: Etienne Laliberté (Université de Montréal, IRBV), Christopher Pal (Polytechnique Montréal, Mila), David Rolnick (Université McGill, Mila), Oliver Sonnentag (Université de Montréal), Anne Bruneau (Université de Montréal, IRBV)\n\nClimate change is altering plant biodiversity, with potentially catastrophic consequences on the resilience and functioning of terrestrial ecosystems. A major source of uncertainty in the global terrestrial carbon budget, and thus for future climate projections, is how plant species differ in their phenological responses to seasonal climate fluctuations. In addition, climate change reshuffles plant species distributions across entire landscapes, but we are unable to keep track of those changes in biodiversity using classical field-based sampling. Remote sensing technologies such as phenocams or drones offer potential to study plant phenology and biodiversity in great detail across spatial scales. These new approaches could revolutionise biodiversity science and conservation, and help guide the design of nature-based solutions essential to mitigate the effects of climate change. New AI algorithms are needed to unlock the full potential of this transformative technology and its links to more traditional data streams and products. This program will develop these new algorithms, building on the most recent developments in computer vision and meta-learning to map plant species and their phenological signatures. Algorithms will be put directly into the hands of scientific and non-scientific end-users via the development of an active learning platform. This AI research will empower researchers and practitioners to turn imagery into actionable data about plant biodiversity and phenology, providing them with tools to help fight biodiversity loss and the effects of climate change.\n\nTopic 3 – Human health and secondary use of data\n\nLead researchers: Michaël Chassé (Université de Montréal, CRCHUM), Nadia Lahrichi (Polytechnique Montréal, CIRRELT), An Tang (Université de Montréal, CRCHUM)\n\nArtificial intelligence (AI) technologies hold the potential to transform healthcare. These technologies are emergent in logistics and imaging, and hundreds of algorithms are now being developed to help support care delivery. Many challenges remain, however, when it comes to scale-up for use in the field. One such challenge is ensuring the generalizability of such algorithms. How can we guarantee the effectiveness of one model on a data set with characteristics that differ from the one the algorithm learned with? For example, an algorithm trained using data from a specific population may not perform as well when applied to a different population.\n\nThis program therefore aims to study new methods for improving generalization, and pursues four objectives. First, set up a research environment enabling the study of methods likely to improve generalization in real-world contexts. Second, optimize data flows obtained in real-world healthcare settings to serve algorithm research. Next, investigate specific issues related to algorithm generalization and secondary use of medical data. Lastly, create an open data set that can be used to build upon the research program findings.\n\nTopic 4 – AI for the discovery of materials and molecules\n\nLead researchers: Yoshua Bengio (Université de Montréal, Mila), Michael Tyers (Université de Montréal, IRIC), Mickaël Dollé (Université de Montréal), Lena Simine (Université McGill)\n\nDesigning molecules with desired properties is a fundamental problem in drug, vaccine, and material discovery. Traditional approaches to designing a new drug can take over 10 years and a billion US dollars. Materials have been developed solely based on their performance characteristics leading to materials composed of rare, often toxic elements, which can inflict significant environmental damage. Artificial intelligence (AI) has the potential to revolutionize drug and material discovery by analyzing evidence from large amounts of data accumulated and learning how to search in the compositional space of molecules, and hence significantly accelerate and improve the process.\n\nThis program aims to build an efficient and effective machine learning framework for searching molecules with designed properties. It will be crucial to build upon, and extend, ongoing collaborations (i) between Mila and IRIC, aimed at optimizing the algorithms to discover new antibiotics and (ii) between Mila and materials experts at McGill and Université de Montréal, on the development of materials with environmental applications like fighting climate change. This multidisciplinary project also raises exciting fundamental challenges in AI regarding learning to search, modeling and sampling complex data structures like graphs, and may have applications to scientific discovery more broadly.\n\nTopic 5 – Human-centered AI: From Responsible Algorithm Development to Human Adoption of AI\n\nLead researchers: Pierre-Majorique Léger (HEC Montréal, Tech3lab), Sylvain Sénécal (HEC Montréal, Tech3lab)\n\nHuman-AI interactions are common nowadays. We interact with artificial intelligence daily in performing many professional and personal tasks. Humans’ adoption of AI, however, is far from automatic, successful or satisfactory. Whether we are citizens, employees or consumers, issues such as bias, lack of trust and even low user satisfaction affect our likelihood of adopting AI in various contexts. To foster adoption, a holistic approach to AI is therefore needed. This multidisciplinary research program is investigating the full cycle of responsible AI development, from inception to adoption by users, putting people at the heart of the process. The goal is to map out guidelines for human-centred AI design using an iterative, multimethod methodological approach led by a multidisciplinary research team.\n\nDiffusion Geometry & Topology Approach to Data Fusion and Mitigating Batch Effects\n\n“It is becoming crucial to combine datasets collected in different circumstances, but this task is challenging due to various inconsistencies introduced by data collection artifacts and inherent biases. This gives rise to a set of challenges often referred to as data fusion or batch effect removal, which can be divided into two main tasks that we aim to tackle in this MSc research. The first is combining data of the same system from different sensors, each with its own calibration, scale, and level of noise. The second is combining datasets that measure the same variables but in different conditions (e.g., subjects, locations, or time of day). This creates a problem called batch effects, where confounding variables hide the effect of true variables of interest. Even small batch effects, or their incomplete removal, can significantly bias statistical conclusions and detract from the ability to provide reliable insights from biomedical data.\n\nSignificant efforts have been recently invested in unsupervised data fusion and mitigation of batch effects. Several diffusion-geometry approaches have been developed, including integrated diffusion and harmonic alignment. The former aims to combine multimodal data while denoising and adjusting for discrepancies in data capture resolutions. The latter aims to rigidly align or fit datasets together so that their information is comparable, in other words alleviating or removing batch effects. In this project, we will build upon and combine ideas from previous diffusion-based approaches, while aiming to relax the required assumptions. We will leverage the recent diffusion condensation framework, which captures data representations at different scales or resolutions, to identify which local regions or resolutions are most appropriate for aligning data, based on their intrinsic topological “shape”. Another goal is to pinpoint meaningful local differences between datasets, as opposed to global deformations due to technical artifacts.\n\nOn a fundamental level, this research will further explore the intersection between diffusion geometry and topological data analysis, merging the two dominant approaches to manifold learning in data exploration. On the application side, it will address critical challenges in biomedical data exploration, especially encountered in high-throughput multi-modal data from multi-sample cohorts, to enable new research avenues and significantly advance the frontiers of AI and health.”\n\nLanguage Models Enhanced with Hyperlink Graphs\n\nPre-trained language models have achieved remarkable progress in the field of Natural Language Processing in the past few years. These models are trained on massive amounts of text extracted from the internet, in order to learn linguistic patterns and factual information about the world. They then act as general-purpose models, able to be customized on a large number of distinct tasks. This text extracted from the internet is hypertext (e.g. formatting and links), which under the status quo is stripped of all markup to be used as plain text in these models’ pre-training regimes. However, this markup contains information that could potentially be useful for these models to learn. In the status quo, models do learn some limited factual information from the plain text they consume, but certain things remain unclear, namely: how exactly the facts are encoded in the models’ parameters, why they learn some facts but not others, and how to prevent “hallucinations” (models making up incorrect information). One potentially useful aspect encoded in this hypertext is hyperlinks, which form a web of interconnected links between pages. Specifically, this hyperlink graph could be useful to help models learn factual information relating to concepts and entities expressed in the free-form text. The main question this proposal seeks to answer is: how can we best incorporate this hyperlink graph information into the pre-training process of language models to enhance the knowledge they learn about the world? One option is to incorporate the linked content into the model as text along with the regular input, using a control mechanism to ensure the model isn’t overloaded with potentially irrelevant information from the linked content. On an intuitive level, this is conceptually similar to a human looking up a term they are unfamiliar with in an encyclopedia when they encounter it in a text. An alternative approach is to model the hyperlinks as mentions of real-world entities, and the text between two hyperlinks in a given sentence as a relation between them, and to train the model to encode this information directly. Creating more factually reliable models opens up these language models to new uses, for example as a knowledge engine that you can query for information in natural language.\n\nCharacterization of the distribution of dark matter on small scales using strong gravitational lensing\n\n“Astrophysical observations indicate that there exists an unknown, exotic type of matter called dark matter, which actually constitutes the vast majority of matter in the Universe. Discovering dark matter and its properties is one of the most important priorities in modern physics. One powerful way to characterize dark matter it through a phenomenon called strong gravitational lensing. This phenomenon happens when there are two galaxies, one directly behind the other from the Earth’s perspective. The closer one is the lensing galaxy, and the one further away is the source galaxy. Because massive objects curve spacetime, the lensing galaxy distorts the image of the source galaxy, very much like an actual lens distorts images. Studying these distortions make it possible to learn about the distribution of dark matter inside the lensing galaxy, and provide invaluable information about this unknown form of matter.\n\nMy research will contribute to this study of dark matter using data from strong gravitational lenses. One major challenge is the analysis of this data which is difficult and takes a lot of computing time with traditional analysis methods. Furthermore, very large amounts of data will become available in the coming years, which makes this challenge even more important. Neural networks have been shown to be extremely promising to dramatically reduce the computing time in analyzing such data. Neural networks are computing algorithms that fall under the category of machine learning, also sometimes referred to as artificial intelligence. My research will be to take part in developing machine learning analysis methods to analyze data from strong gravitational lenses. I also plan to apply the methods that I develop to real data, which will further advance our understanding of dark matter in particular and of the Universe in general.”\n\nPhysics-aware deep learning for cellular dynamics\n\n“A physics-aware deep learning system is developed to almost instantaneously infer cellular dynamics with minimum experimental procedure.\n\nPeople are paying heavy prices for diseases that would otherwise be diagnosed and treated much more easily and cheaply by employing cell mechanics. Cellular forces dictate cellular processes and the onset and progression of diseases such as cancer and asthma. The use of cellular dynamics as biomarkers and modulators for cell behaviour indicates the potential of cell mechanics in diagnosis, treatment, drug development, and the study of disease mechanisms. However, the application of cell mechanics is hindered by the costly, time-consuming, and complex procedures to measure cellular forces. Current methods examining cellular forces measure the deformation directly resulted from the forces and then calculate the forces back from the deformation. However, those methods completely depend on in-vivo experiments, so cannot go beyond the parameter space in the experiments, and often require complex and tedious procedures. Sometimes, the accuracy is limited by solving inverse problems.\n\nThis project develops a physics-aware deep learning system to apply cell mechanics to diagnosis, treatment, and drug development with no additional procedures, time, and cost. To integrate digital intelligence into biology and medicine, this project searches for the best way to model the physical interaction between the cell and environment with deep learning. To resolve the limits of the current methods and to revolutionize the methodology in the field of cell mechanics, a comprehensive and generalizable physics-aware deep learning system with adjustable parameters is developed to accurately and almost instantaneously infer and simulate the dynamics in cells and tissues under different conditions from merely the time series of cell morphology. ConvLSTMs and transformers can be used to capture the features and spatiotemporal relations in the morphology. Physical equations, for example, in solid mechanics, and biological parameters are embedded into neural networks optimized for the physical laws and biological models. Our approach is also much more sustainable as minimum lab waste is produced. This system provides revolutionary insights and utility in drug development, diagnosis, treatment, and researches. Moreover, the application of this project exceeds cellular dynamics as the fundament of the project is modelling physical interactions and problems with deep learning.”\n\nCoupled Markov Switching Count Models for Spatio-temporal Infectious Disease Counts\n\n“Accurate epidemic forecasting of infectious diseases remains a major challenge. A state-of-the-art class of statistical models known as “Markov Switching Models” have shown promise in this area. This approach breaks up epidemic forecasting into two components. Firstly, there is a component to predict epidemic occurrence, i.e. when an epidemic will begin in an area. Since an infectious disease will often sit at low levels of incidence, or be completely absent, for extended periods, being able to predict when an epidemic will begin is the first vital step to epidemic forecasting. The second component of the Markov switching model forecasts the resulting cases in the epidemic period. This component can predict when the epidemic will peak, how many cases are expected, how long it will last and other important metrics for the epidemic period. To summarize, Markov switching models predict when an epidemic will occur and then forecast the resulting trajectory of the cases.\n\nWe offer some novel extensions to these models in order to improve forecasting performance. Firstly, we will allow seasonal, meteorological, socioeconomic and other factors to impact our predictions of epidemic occurrence. In contrast, previous approaches assumed a constant probability of epidemic occurrence. This assumption is not very appropriate as epidemics of an infectious disease are known to be highly seasonal and influenced by a wide range of factors. For example, an epidemic of dengue fever will almost never occur in the winter when temperature is too low for prolonged mosquito survival.\n\nA model not accounting for temperature in this case would give poor predictions of epidemic occurrence. Incorporating space-time varying predictions of epidemic occurrence into our modeling framework should improve forecasting performance considerably. Secondly, we will incorporate realistic human movement into our forecasts, even using cell phone data where available. Essentially, our framework allows for epidemics in an area to spread into other areas connected by high movement flows. Human movement to and from epidemic areas has been shown to be a major risk factor for the development of epidemics and we view this as an essential component of our modeling strategy. Our extensions create statistical challenges compared to more tradition Markov switching models that have\n\nbeen used by others. To overcome this, we will develop new fast and efficient algorithms for fitting the model. All model fitting software will be made publicly available and we plan on writing subsequent less technical papers to introduce policy makers to our methods.”\n\nTowards a Geometric Theory of Information\n\n“Information theory is a highly developed and active research area and has been of paramount importance in the development of modern machine learning techniques. However, this field was originally developed in the framework of random variables taking values on mere discrete sets of symbols. This austerity results in a blindness to additional structure amongst the symbols, which limits the power and applicability of the theory. My long-term vision is to build a generalization of the theory of information developed by Shannon in a way that directly incorporates the geometric structure in the domains of the random variables.\n\nTools from information theory have been used in the context of representation learning to understand the “surprising” generalization properties of deep learning systems. Expanding our understanding on why deep neural networks perform well on unseen examples, and the potential role that its learned representations play in this process, is a key step towards the deployment of deep learning-based systems in applications for which performance guarantees are critical. However, one of the challenges faced by these approaches arises from the invariance of the mutual information between two random variables with respect to smooth invertible transformations of their sample spaces.\n\nMy proposal aims at providing machine learning researchers with theoretical tools to tackle these challenges. In previous work, we have imported a notion of similarity-sensitive entropy originally developed in theoretical ecology to the machine learning community. Based on this definition, we propose geometry-aware counterparts for several concepts and results in standard information theory, as well as a novel notion of divergence which incorporates the geometry of the space when comparing probability distributions while avoiding the computational challenges of optimal transport distances.\n\nIn the future, my research will focus on the theoretical and practical implications of these ideas: 1) can we obtain an axiomatic characterization of geometry-sensitive entropy?; 2) are geometric mutual information objectives better behaved for representation learning?; 3) what are the connections between our proposed divergence and rate-distortion theory, in particular, regarding deep-learning based compression techniques; 4) can we improve the entropic regularization used in reinforcement learning to encourage exploration by considering similarities on the action space?”\n\nEnhancement of quantitative estimation of metabolism and vascularization with positron emission tomography (PET) and Ultrafast Ultrasound Localization Microscopy (UULM) Using Deep Learning\n\n“1 Problem and context\n\nStructural and functional imaging of tissue vasculature has been studied using various imaging modalities such as positron emission tomography (PET), magnetic resonance imaging (MRI), computed tomography (CT), and ultrasound. Among all the molecular imaging modalities, no single modality is perfect and sufficient to obtain all the necessary information for any questions of interest. A recent novel technique inspired by super-resolved fluorescence microscopy called ultrasound localization microscopy (ULM), improved spatial resolution of vascular images, from hundreds to a few microns in vivo via the detection at thousands of frames per second of millions of individual microbubbles injected in the bloodstream. Our group (led by my co-supervisor J. Provost, IVADO member) has recently shown the feasibility of extending ULM to dynamic acquisitions in three dimensions using novel imaging sequences and reconstruction algorithms.\n\nWith the introduction of deep learning algorithms, research focusing on multimodality medical imaging has increased exponentially, such as image segmentation, denoising, and image reconstruction. In this work, we propose to combine ULM with ultra-resolution images and dynamic PET imaging to estimate parametric images of dynamic PET based on deep learning, using compartment pharmacokinetic modeling. Moreover, I aim to enhance the quality of dynamic PET images and extraction of perfusion and vasculature parameters to have a precise model of tissue behavior with denoising and precise segmentation.\n\n2 Methodology\n\nIn this project, we aim to combine the microvascular information from 3D ULM to the molecular information of dynamic PET imaging in order to enhance the resolution and quantification of PET dynamic acquisitions. The first step is an iterative parametric image reconstruction using a deep neural network. I will not use prior training pairs, but only the same ULM image. I will utilize the ULM images from the same image as anatomical prior (blood content in every voxel) to guiding the parametric image reconstruction through the neural network. The neural network will be inserted into the iterative parametric image reconstruction framework and pharmacokinetic modeling to achieve more precise kinetic parameters, rather than using it as a post-processing tool.\n\nThe second step of the project will be on denoising of dynamic PET images because of the high-level noises of these images. Generally, deep learning with convolutional neural networks (CNN), requires the preparation of large training image datasets. This presents a challenge in a clinical setting because it would be very difficult to prepare large, high-quality datasets. Recently, the deep image prior (DIP) approach suggests that CNN structures have an intrinsic ability to solve inverse problems such as denoising without any pre-training. The DIP approach iterates learning using a pair of random noise and corrupted images and a denoised image is obtained by the network output with moderate iterations. The third step will be the segmentation of PET images, which will be organ detection based on unsupervised learning. The main idea for this is that better image representation gets better clustering, and better clustering results helps to get better image representation.\n\n3 Results\n\nModified network structures are developed based on the 3D U-net for each step which consists of an “encode” part and a “decode” part. The encode part of architecture consisting of the repetitive applications of 3D convolution layers, each followed by a batch normalization (BN) and a leaky rectified linear unit (LReLU), and convolutional layers for downsampling. The decoding part consists of a deconvolution layer, followed by the BN and the LReLU, transposed convolutional layers for up-sampling, skip connection with the corresponding linked feature map from the encoding part. In each step, the network structure and the loss function are modified to have the best performance in that step.\n\nWe will extract perfusion and vasculature parameters to have a precise model of tissue behavior. The extracted parametrical maps non-invasively depict beneficiary information about tissue microvasculature and are used as input in the PET compartment pharmacokinetic model. The technique will be used on pre-clinical dynamic data for small animal microPET and on numerical phantoms for dynamic quantitative analysis, validation and sensitivity study. Applications for human PET in nuclear medicine – including tumor microenvironment parametrization – will be developed. “\n\nA Multi-agent Decision and Control Framework for Mixed-autonomy Transportation System\n\nAs the autonomous vehicle becomes more and more popular. Recently, there has been a new emphasis on traffic control in the context of mixed-autonomy, where only a fraction of vehicles are connected autonomous vehicles and interacting with human-driven vehicles. As in a mixed autonomy system, there are several challenges. The first challenge is how to encourage different agents’ cooperation so as to maximize the total returns of the whole system. For example, when there is a gap in front of the adjacent line of the autonomous vehicle, if the autonomous vehicle cuts in immediately, the surrounding vehicle in the adjacent line will also decrease its speed sharply, which will end up a shock wave in traffic flow. Instead, if the autonomous vehicle learns to cooperate with other agents, it will adjust its speed steadily and try to mitigate the negative impact on the whole system. The second challenge is how to improve the communication efficiency in multi-agent system. As autonomous vehicle has different characteristics with human-driven agent, for example, their reacting time and action may be different. Therefore, how to formalize personalized policy for each agent is also worth to explore. The third challenge is how to explore expert knowledge (e.g. green wave, max pressure, actuated control) in transportation domain to improve the training efficiency and performance. Our overall goal of this project is to design effective decision and control framework for an efficient and safe mixed autonomy system by mitigating the shockwave and improving the transportation efficiency. To address the aforementioned problems, we will develop a novel multi agent decision framework based on deep reinforcement learning to improve the decision making and control performance of the agents in mixed autonomy system.\n\nUnsupervised representation learning\n\nUnsupervised representation learning is concerned with using deep learning algorithms to extract ‘useful’ features (latent variables) from data without any external labels or supervision. This addresses one of the issues with supervised learning, which is the cost and lack of scalability in obtaining labeled data. The techniques developed in this field have broad applicability, especially with regard to training smart ‘AI agents’ and domains where obtaining labeled data is difficult.’Mixup’ (Zhang et al) is a recently-proposed class of data augmentation techniques which involve augmenting a training set with extra ‘virtual’ examples by constructing ‘mixes’ between random pairs of examples in the training set and optimizing some objective on those mixed examples. While the original mixup algorithm simply performed these mixes in input space (which comes with a few limitations) for supervised classification, recent work (Verma et al, Yaguchi et al) proposed performing these mixes in the latent space of the classifier instead, achieving superior results to the original work.One intuitive way to think about ‘latent space mixing’ is that we can imagine that the original data is generated by *many* latent variables, the possible configurations of which increase exponentially as the number of latent variables increases. Because of this we only see a *very small* subset of those configurations in our training set. Therefore, mixup can be seen as allowing the network to explore *novel* combinations of the latent variables it has inferred (which may not already be present in the training set), therefore making the network more robust to novel configurations of latent states (i.e. novel examples) at test time. Empirical results from the works cited corroborate this hypothesis.\n\nThe first stage of my PhD was exploring mixup in the context of unsupervised representation learning (building on the work of Verma et al, which I also co-authored), in which the goal is to learn useful latent variables from unlabeled data. This was done by leveraging ideas from adversarial learning and devising an algorithm which is able able to mix between encoded states of real inputs and decoding them into realistic-looking inputs indistinguishable from the real data. We showed promising results both qualitatively and quantitatively, and recently published our findings at the NeurIPS 2019 conference.\n\nSome preliminary experiments suggest that one of our proposed variants of ‘unsupervised mixup’ has a connection to ‘disentangled learning’, which explores the inference of latent variables which are conceptually ‘atomic’ but can be arbitrarily composed together to produce more abstract concepts (which is similar to how we as humans structure information in the brain). This lays the groundwork for some more exciting research to pursue during my PhD.\n\nUncertainty in Operations Research, Causality and Out-of-Distribution Generalization\n\nMy research focuses on two main directions: widening the operations research toolbox using recent advances in deep learning and learning causal structures. Both aspects have the potential to be useful in various applications, for example the optimization of railway operations, gene expression studies as well as the understanding of different protein interactions in human cells.Together with Emma Frejinger and its team at the CN chair, we developed a methodology which allows to predict tactical solutions given only partial knowledge of the problem using deep neural networks. We demonstrated the efficiency of the approach on the problem of booking intermodal containers on double-stack trains. Moreover, we are currently working to apply machine learning techniques to standard operations research problems such as the knapsack and the travelling salesman problem in hope of gaining insight about classical algorithms to solve them.\n\nMore recently, I have been interested in the nature of causal reasoning and how machines could acquire it. Typical machine learning systems are good at finding statistical dependencies in data, but often lack the causal understanding which is necessary to predict the effect of an intervention (e.g. the effect of a drug on the human body). Together with my co-authors, we developed « Gradient-Based Neural DAG Learning », a causal discovery algorithm which aims at going beyond simple statistical dependencies. We showed the algorithm was capable of finding known causal relationships between multiple proteins in human cells.\n\nIn the future, I will work to make machine learning more adaptive and able to reuse past knowledge in order to learn new patterns faster. This is something humans do all the time, but which is hard for current algorithms. I believe causality is part of the answer, but other frameworks like meta-learning, transfer learning and reinforcement learning are going to be necessary. Apart from bringing us closer to human-level intelligence, making progress in this direction would benefit many applications. For instance, if a machine learning system is used to predict tactical solutions to a railway optimization problem, the distribution of problems it faces might shift due to changes in trade legislation, hence rendering the predicted solutions far from optimal. We should aim to build systems which can adapt to a changing world quickly.\n\nUse of Deep Learning Approaches in the Activity Prediction and Design of Therapeutic Molecules\n\nThe proposed research is to employ Deep Learning and Neural Networks, which are both fields of Machine Learning, to more accurately predict the effectiveness, or “activity”, of potential therapeutic molecules (potential drugs). We are primarily concerned with predicting a given molecule’s ability to inhibit the growth of primary patient cancer cells (cells taken directly from a patient). The Leucegene project at the Institut de Recherche en Immunologie et Cancérologie (IRIC) has tested the activity of a large number of compounds in inhibiting the growth of cancer cells from patients afflicted with acute myeloid leukemia. The proposed research will use this activity data, along with several other data sources, to build an algorithm that can better predict the effectiveness that a molecule will have in inhibiting cancer cell growth. This means that before a molecule is even synthesized in a chemistry lab, a good estimation of its effectiveness as a therapeutic compound can be made, almost instantly. The first approach is to use Neural Networks and “representation learning”, in which features of the molecules that are important to improving activity are identified automatically by the algorithm. This will be done by representing the molecules as graphs and networks. Another approach that will be taken is the use of “multi-task learning” in which the prediction accuracy of an algorithm can be improved if the same algorithm is trained for multiple tasks on multiple datasets. The « multiple tasks » that will be focused on are multiple, but related, drug targets that are essential to cancer cell growth. Moving beyond activity prediction alone, these machine learning architectures will be expanded to design new chemical structures for potential drug molecules, based on information that is learned from drug molecules with known activities. These approaches have the capacity to improve the predictions about whether molecules will make effective drugs, and to design new molecules that have even better effectiveness than known drugs. Research progress in this area will lower the cost, both in money and time, of the drug development process.\n\nTemporal abstraction in multi-agent environment\n\nTemporal abstraction refers to the ability of an intelligent agent to reason, act and plan at multiple time scales. The question of how to obtain and reason with temporally abstract representations has been extensively studied in classical planning and control theory, and more recently it has become an important topic in reinforcement learning, especially through the framework of options. The theoretical development of options is based on the framework of Semi-Markov Decision Processes (SMDPs), in which an agent interacts with its environment by observing states and taking actions. As a result of an action, the agent receives an immediate reward, and transitions to a new state drawn from some distribution, after a certain period of time which is also drawn stochastically. Both the state and the dwell-time distribution are dependent only on the agent’s state and action. However, in many cases of practical importance, an agent may be faced either with more general environments, in which the environment may be partially observable, or there may be multiple agents acting in the environment. For example, in energy markets or in transportation there may be many agents, who would be interacting with each other and making decisions without being able to observe relevant information except at specific time points.We propose to focus on establishing a mathematical framework for temporal abstraction which would work in Decentralized Partially Observable Markov Decision Processes. In a multi-agent system, agents take decision and exchange their information at designated decision epochs. In general, the decision epochs are given by the realizations of a random sequence. Instead of looking at every instant of time, when an action is taken by an agent, we are interested in the Decentralized Semi-Markov Decision Processes (Dec-SMDPs), in which a Partially Observable Markov Decision Process (POMDP) corresponding to an agent is embedded between any two successive decision epochs. In between two such decision epochs, each agent chooses actions so as to maximize the total return over a finite or infinite horizon, i.e., it solves a POMDP problem. The optimal decision epochs are chosen based on a given criterion, e.g., exchanging information at some goal states fixed a priori or when the increase of reward from the last decision epoch is less than a threshold. The overall performance, which is to be maximized through such sequential decision making consists of two rewards. The exchange of information in encouraged by an extrinsic reward along with an intrinsic reward that is maximized in between two consecutive decision epochs.We would like to investigate two aspects of this problem setup. First, we are interested in formally establishing the framework for Partially Observable Semi-Markov Decision Processes and its extension to decentralized (multi-agent) problems. We would like to investigate if under certain simplifying assumptions in the planning problem, the posterior beliefs (i.e., belief on the state of the environment based on past information and current action) exhibit certain monotonicity and symmetry properties so that we can infer what the structure of optimal policies could be. We also want to establish the general Dec-SMDP framework for modeling this problem and characterize its properties in comparison with SMDPs.In the subsequent analysis, we would like to investigate learning algorithms for these families of problems. We will build on standard reinforcement learning algorithms for temporal abstraction, such as option-critic, and provide extensions in our case that are consistent with the theoretical characterization of these problems. We will also examine the performance of both value-function-based and policy-gradient style algorithms in this context. We will compare the results that can be obtained using our framework to results in which each agent ignores the others and only tries to optimize myopically its own reward. We will use both standard simulated small problems from the multi-agent literature, designed to emphasize specific aspects, as well as larger scale domains that correspond to simulate transportation and energy markets, where multiple agents work in a cooperative setting to achieve a common task in a decentralized manner, e.g., self-driving cars and smart-grids. In such applications the agents occasionally communicate among themselves and use a common information to update a belief about the state of the world and a local information to decide about their individual policies and terminations of such policies."
    }
}
{
    "id": "wrong_mix_random_nbateam_00096_3",
    "rank": 87,
    "data": {
        "url": "https://date24.date-conference.com/programme",
        "read_more_link": "",
        "language": "en",
        "title": "Programme",
        "top_image": "https://date24.date-conference.com/sites/date24/files/favicon%5B1%5D.ico",
        "meta_img": "https://date24.date-conference.com/sites/date24/files/favicon%5B1%5D.ico",
        "images": [
            "https://date24.date-conference.com/themes/custom/date_conference_theme/images/social/linkedin.png",
            "https://date24.date-conference.com/themes/custom/date_conference_theme/images/social/x.png",
            "https://date24.date-conference.com/themes/custom/date_conference_theme/date24/DATE2024_logo_white_flat.png",
            "https://date24.date-conference.com/sites/date24/files/styles/company_logo/public/2024-01/synopsys_color.jpg",
            "https://date24.date-conference.com/sites/date24/files/styles/company_logo/public/2019-05/IEEE-CEDA-Logo_0.png",
            "https://date24.date-conference.com/modules/contrib/spamspan/image.gif",
            "https://date24.date-conference.com/modules/contrib/spamspan/image.gif",
            "https://date24.date-conference.com/modules/contrib/spamspan/image.gif",
            "https://date24.date-conference.com/modules/contrib/spamspan/image.gif",
            "https://date24.date-conference.com/modules/contrib/spamspan/image.gif",
            "https://date24.date-conference.com/modules/contrib/spamspan/image.gif",
            "https://date24.date-conference.com/modules/contrib/spamspan/image.gif",
            "https://date24.date-conference.com/modules/contrib/spamspan/image.gif",
            "https://date24.date-conference.com/modules/contrib/spamspan/image.gif",
            "https://date24.date-conference.com/modules/contrib/spamspan/image.gif",
            "https://date24.date-conference.com/modules/contrib/spamspan/image.gif"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "/sites/date24/files/favicon%5B1%5D.ico",
        "meta_site_name": "",
        "canonical_link": "https://date24.date-conference.com/programme",
        "text": "Detailed Programme\n\nThe detailed programme of will continuously be updated.\n\nMore information on ASD Initiative, Keynotes, Tutorials, Workshops, Young People Programme\n\nNavigate to Monday, 25 March 2024 | Tuesday, 26 March 2024 | Wednesday, 27 March 2024.\n\nMonday, 25 March 2024\n\nOC Opening Ceremony\n\nAdd this session to my calendar\n\nDate: Monday, 25 March 2024\n\nTime: 08:30 CET - 09:00 CET\n\nLocation / Room: Auditorium 1\n\nSession chair:\n\nAndy Pimentel, University of Amsterdam, NL\n\nSession co-chair:\n\nValeria Bertacco, University of Michigan, US\n\nSupported by\n\nTime Label Presentation Title\n\nAuthors 08:30 CET OC.1 WELCOME ADDRESSES\n\nPresenter:\n\nAndy Pimentel, University of Amsterdam, NL\n\nAuthors:\n\nAndy Pimentel1 and Valeria Bertacco2\n\n1University of Amsterdam, NL; 2University of Michigan, US\n\nAbstract\n\nWelcome Addresses from the Chairs 08:45 CET OC.2 PRESENTATION OF AWARDS\n\nPresenter:\n\nJürgen Teich, Friedrich-Alexander-Universität Erlangen-Nürnberg, DE\n\nAuthors:\n\nJürgen Teich1, David Atienza2, Georges Gielen3 and Yervant Zorian4\n\n1Friedrich-Alexander-Universität Erlangen-Nürnberg, DE; 2EPFL, CH; 3KU Leuven, BE; 4Synopsys, US\n\nAbstract\n\nPresentation of Awards\n\nOK01 Opening Keynote 1\n\nAdd this session to my calendar\n\nDate: Monday, 25 March 2024\n\nTime: 09:00 CET - 09:45 CET\n\nLocation / Room: Auditorium 1\n\nSession chair:\n\nAndy Pimentel, University of Amsterdam, NL\n\nSession co-chair:\n\nValeria Bertacco, University of Michigan, US\n\nTime Label Presentation Title\n\nAuthors 09:00 CET OK01.1 CHIPLET STANDARDS: A NEW ROUTE TO ARM-BASED CUSTOM SILICON\n\nPresenter:\n\nRobert Dimond, ARM, GB\n\nAuthor:\n\nRobert Dimond, ARM, GB\n\nAbstract\n\nA key challenge our partners are consistently looking to solve is: How can we continue to push performance boundaries, with maximum efficiency, while managing costs associated with manufacturing and yield? Today, as the ever more complex AI-accelerated computing landscape evolves, a key solution emerging is chiplets. Chiplets are designed to be combined to create larger and more complex systems that can be packaged and sold as a single solution, made of a number of smaller dice instead of one single larger monolithic die. This creates interesting new design possibilities, with one of the most exciting being a potential route to custom silicon for manufacturers who historically chose off-the-shelf solutions. This talk will describe two complementary approaches to realising this chiplet opportunity: · Decomposing an existing system across multiple chiplets, in the same way a monolithic chip is composed of IP blocks. · Aggregating well-defined peripherals across a motherboard into a single package. Both of these approaches require collaboration in standards to align on the many non-differentiating choices in chiplet partitioning. This talk will describe the standards framework that Arm is building with our partners, and the broader industry. Including, own specifications such as the Arm Chiplet System Architecture (Arm CSA), AMBA chip-to-chip and the role of industry standards such as UCIe.\n\nOK02 Opening Keynote 2\n\nAdd this session to my calendar\n\nDate: Monday, 25 March 2024\n\nTime: 09:45 CET - 10:30 CET\n\nLocation / Room: Auditorium 1\n\nSession chair:\n\nAndy Pimentel, University of Amsterdam, NL\n\nSession co-chair:\n\nValeria Bertacco, University of Michigan, US\n\nTime Label Presentation Title\n\nAuthors 09:45 CET OK02.1 ENLIGHTEN YOUR DESIGNS WITH PHOTONIC INTEGRATED CIRCUITS\n\nPresenter:\n\nLuc Augustin, SMART Photonics, NL\n\nAuthor:\n\nLuc Augustin, SMART Photonics, NL\n\nAbstract\n\nThe field of integrated photonics holds great promise for overcoming societal challenges in data and telecom, autonomous driving and healthcare in terms of cost, performance, and scalability. Similar to the semiconductor industry, the ever-increasing demands of various applications are driving the necessity for platform integration in photonics as well, enabling seamless integration of diverse functionalities into compact and efficient photonic devices. This high level of integration reduces footprint and drives down system level costs. In this trend towards high levels of integration , Indium Phosphide (InP) is the material of choice for long-distance communication lasers, owing to its proven track record over several decades. Leveraging standardized fabrication processes, the cost and performance targets can be addressed. The key advantages of InP-based integration lie in its ability to fully integrate lasers, amplifiers, modulators, and passives, providing a flexible and reliable platform for building complex Photonic Integrated Circuits (PICs). This paper will address the photonic integration platforms, the applicability to current and future markets requiring the need for further heterogenous integration with other technologies, and the change to a foundry business model, much like its electronics counterpart.\n\nASD01 ASD Technical Paper Session: Designing Adaptive Autonomous Systems for Resource-Constrained Platforms\n\nAdd this session to my calendar\n\nDate: Monday, 25 March 2024\n\nTime: 11:00 CET - 12:30 CET\n\nLocation / Room: Break-Out Room S8\n\nTime Label Presentation Title\n\nAuthors 11:00 CET ASD01.1 CONTEXT-AWARE MULTI-MODEL OBJECT DETECTION FOR DIVERSELY HETEROGENEOUS COMPUTE SYSTEMS\n\nSpeaker:\n\nJustin Davis, Colorado School of Mines, US\n\nAuthors:\n\nJustin Davis and Mehmet Belviranli, Colorado School of Mines, US\n\nAbstract\n\nIn recent years, deep neural networks (DNNs) have gained widespread adoption for continuous mobile object detection (OD) tasks, particularly in autonomous systems. However, a prevalent issue in their deployment is the one-size-fits-all approach, where a single DNN is used, resulting in inefficient utilization of computational resources. This inefficiency is particularly detrimental in energy-constrained systems, as it degrades overall system efficiency. We identify that, the contextual information embedded in the input data stream (e.g., the frames in the camera feed that the OD models are run on) could be exploited to allow a more efficient multi-model-based OD process. In this paper, we propose SHIFT which continuously selects from a variety of DNN-based OD models depending on the dynamically changing contextual information and computational constraints. During this selection, SHIFT uniquely considers multi-accelerator execution to better optimize the energy-efficiency while satisfying the latency constraints. Our proposed methodology results in improvements of up to 7.5x in energy usage and 2.8x in latency compared to state-of-the-art GPU-based single model OD approaches. 11:30 CET ASD01.2 ADAPTIVE LOCALIZATION FOR AUTONOMOUS RACING VEHICLES WITH RESOURCE-CONSTRAINED EMBEDDED PLATFORMS\n\nSpeaker:\n\nGianluca Brilli, Università di Modena e Reggio Emilia, IT\n\nAuthors:\n\nFederico Gavioli1, Gianluca Brilli2, Paolo Burgio1 and Davide Bertozzi3\n\n1Università di Modena e Reggio Emilia, IT; 2Unimore, IT; 3University of Ferrara, IT\n\nAbstract\n\nModern autonomous vehicles have to cope with the consolidation of multiple critical software modules processing huge amounts of real-time data on power- and resource-constrained embedded MPSoCs. In such a highly-congested and dynamic scenario, it is extremely complex to ensure that all components meet their quality-of-service requirements (e.g., sensor frequen- cies, accuracy, responsiveness, reliability) under all possible work- ing conditions and within tight power budgets. One promising solution consists of taking advantage of complementary resource usage patterns of software components by implementing dynamic resource provisioning. A key enabler of this paradigm consists of augmenting applications with dynamic reconfiguration capability, thus adaptively modulating quality-of-service based on resource availability or proactively demanding resources based just on the complexity of the input at hand. The goal of this paper is to explore the feasibility of such a dynamic model of computation for the critical localization function of self-driving vehicles, so that it can burden on system resources just for what is needed at any point in time or gracefully degrade accuracy in case of resource shortage. We validate our approach in a harsh scenario, by implementing it in the localization module of an autonomous racing vehicle. Experiments show an overall reduction of platform utilization and power consumption for this computation-greedy software module by up to 1.6× and 1.5×, respectively, for roughly the same quality of service. 12:00 CET ASD01.3 ADAPTIVE DEEP LEARNING FOR EFFICIENT VISUAL POSE ESTIMATION ABOARD ULTRA-LOW-POWER NANO-DRONES\n\nSpeaker:\n\nBeatrice Alessandra Motetti, Politecnico di Torino, IT\n\nAuthors:\n\nBeatrice Alessandra Motetti1, Luca Crupi2, Omer Mohammed Mustafa1, Matteo Risso1, Daniele Jahier Pagliari1, Daniele Palossi3 and Alessio Burrello4\n\n1Politecnico di Torino, IT; 2Dalle Molle Institute for Artificial Intelligence, USI and SUPSI, N/; 3ETH - Zurich, CH; 4Politecnico di Torino | Università di Bologna, IT\n\nAbstract\n\nSub-10cm diameter nano-drones are gaining momentum thanks to their applicability in scenarios prevented to bigger flying drones, such as in narrow environments and close to humans. However, their tiny form factor also brings their major drawback: ultra-constrained memory and processors for the onboard execution of their perception pipelines. Therefore, lightweight deep learning-based approaches are becoming increasingly popular, stressing how computational efficiency and energy-saving are paramount as they can make the difference between a fully working closed-loop system and a failing one. In this work, to maximize the exploitation of the ultra-limited resources aboard nano-drones, we present a novel adaptive deep learning-based mechanism for the efficient execution of a vision-based human pose estimation task. We leverage two State-of-the-Art (SoA) convolutional neural networks (CNNs) with different regression performance vs. computational costs trade-offs. By combining these CNNs with three novel adaptation strategies based on the output's temporal consistency and on auxiliary tasks to swap the CNN being executed proactively, we present six different systems. On a real-world dataset and the actual nano-drone hardware, our best-performing system, compared to executing only the bigger and most accurate SoA model, shows 28% latency reduction while keeping the same mean absolute error (MAE), 3% MAE reduction while being iso-latency, and the absolute peak performance, i.e., 6% better than SoA model.\n\nBPA02 BPA - Reliability And Optimizations\n\nAdd this session to my calendar\n\nDate: Monday, 25 March 2024\n\nTime: 11:00 CET - 12:30 CET\n\nLocation / Room: Break-Out Room S1+2\n\nSession chair:\n\nElena-Ioana Vatajelu, Grenoble-INP, FR\n\nSession co-chair:\n\nMarie-Minerve Louerat, Sorbonne Université - LIP6, FR\n\nTime Label Presentation Title\n\nAuthors 11:00 CET BPA02.1 SCALABLE SEQUENTIAL OPTIMIZATION UNDER OBSERVABILITY DON'T CARES\n\nSpeaker:\n\nDewmini Marakkalage, EPFL, CH\n\nAuthors:\n\nDewmini Marakkalage1, Eleonora Testa2, Walter Lau Neto3, Alan Mishchenko4, Giovanni De Micheli1 and Luca Amaru2\n\n1EPFL, CH; 2Synopsys, US; 3University of Utah, US; 4University of California, Berkeley, US\n\nAbstract\n\nSequential logic synthesis can provide better Power-Performance-Area (PPA) than combinational logic synthesis since it explores a larger solution space. As the gate cost in advanced technologies keeps rising, sequential logic synthesis provides a powerful alternative that is gaining momentum in the EDA community. In this work, we present a new scalable algorithm for don't-care-based sequential logic synthesis. Our new approach is based on sequential k-step induction and can apply both redundancy removal and resubstitution transformations under Sequential Observability Don't Cares (SODCs). Using SODC-based optimizations with induction is a challenging problem due to dependencies and alignment of don't cares among the base case and the inductive case. We propose a new approach utilizing the full power of SODCs without limiting the solution space. Our algorithm is implemented as part of an industrial tool and achieves a 6.9% average area improvement after technology mapping when compared to state-of-the-art sequential synthesis methods. Moreover, all the new sequential optimizations can be verified using state-of-the-art sequential verification tools. 11:20 CET BPA02.2 VACSEM: VERIFYING AVERAGE ERRORS IN APPROXIMATE CIRCUITS USING SIMULATION-ENHANCED MODEL COUNTING\n\nSpeaker:\n\nChang Meng, EPFL, CH\n\nAuthors:\n\nChang Meng1, Hanyu Wang2, Yuqi Mai3, Weikang Qian3 and Giovanni De Micheli1\n\n1EPFL, CH; 2ETH Zurich, CH; 3Shanghai Jiao Tong University, CN\n\nAbstract\n\nApproximate computing is an effective computing paradigm to reduce area, delay, and power for error-tolerant applications. Average error is a widely-used metric for approximate circuits, measuring the deviation between the outputs of exact and approximate circuits. This paper proposes VACSEM, a formal method to verify average errors in approximate circuits using simulation-enhanced model counting. VACSEM leverages circuit structure information and logic simulation to speed up verification. Experimental results show that VACSEM is on average 35× faster than the state-of-the-art method. 11:40 CET BPA02.3 SELCC: ENHANCING MLC RELIABILITY AND ENDURANCE WITH SINGLE-CELL ERROR CORRECTION CODES\n\nSpeaker:\n\nYujin Lim, Sungkyunkwan University, KR\n\nAuthors:\n\nYujin Lim, Dongwhee Kim and Jungrae Kim, Sungkyunkwan University, KR\n\nAbstract\n\nConventional DRAM's limitations in volatility, high static power consumption, and scalability have led to the exploration of alternative technologies such as Phase Change Memory (PCM) and Resistive RAM (ReRAM). Storage-Class Memory (SCM) arises as a target application for these emerging technologies with non-volatility and higher capacity through Multi-Level Cells (MLCs). However, MLCs face issues of reliability and reduced endurance. To address this, our paper introduces a novel Error Correction Codes (ECC) method, \"Single Eight-Level Cell Correcting\" (SELCC) ECC. This technique efficiently corrects single-cell errors in 8-level cell memories using existing ECC syndromes without added redundancy. SELCC enhances memory reliability and improves 8LC memory endurance by 3.2 times, surpassing previous solutions without significant overheads. 12:00 CET BPA02.4 INTERACTIVE TECHNICAL PRESENTATIONS BY THE AUTHORS\n\nPresenter:\n\nSession Chairs, DATE, ES\n\nAuthor:\n\nSession Chairs, DATE, ES\n\nAbstract\n\nParticipants can freely interact with authors during their interactive technical presentations.\n\nET02 Embedded Tutorial: On-Device Continual Learning Meets Ultra-Low Power Processing\n\nAdd this session to my calendar\n\nDate: Monday, 25 March 2024\n\nTime: 11:00 CET - 12:30 CET\n\nLocation / Room: Break-Out Room S3+4\n\nThis tutorial provides an overview of the recent On-Device Learning (ODL) topic for ultra-low power extreme edge devices, such as MicroController compute units (MCU).\n\nNowadays, these devices are capable of running Deep Neural Network (DNN) inference tasks to extract high-level information from data captured by on-board sensors. The DNN algorithms are typically trained off-device using high-performance servers and, then, frozen and deployed on resource-constrained MCU-powered platforms. However, the data used for the DNN training may not be representative of the deployment environment, causing mispredictions/misclasifications that eventually reflect into (i) expensive model redesigns and (ii) re-deployments at scale. Recently proposed Continual Learning methods stand out as potential solutions to this fundamental issue, enabling DNN model personalization by incorporating new knowledge (e.g. new class, new domains, or both) given the new data retrieved for the target environment. However, the DNN learning task has been normally considered out-of-scope for highly resource-constrained devices because of the high memory and computation requirements, limiting its application to server machines where, thanks to the potentially unlimited resources, custom DNN models can be retrained from scratch as soon as new data becomes available.\n\nThis tutorial focuses on the application of the Continual Learning (CL) paradigm on MCUs devices, to enable small sensor nodes to adapt their DNN models in-the-field, without relying on external computing resources. After providing a brief taxonomy of the main CL algorithms and scenarios, we will review the fundamental ODL operations, referring to the backpropagation learning algorithm. We will analyze the memory and computational costs of the learning process when targeting a multi-core RISC-V-based MCU, derived from the PULP-project template, and we will use a case study to see how these costs constrain an on-device learning application. Next, a hands-on session will bring the audience to familiarize with software optimizations for DNN learning primitives using PULP-TrainLib (https://github.com/pulp-platform/pulp-trainlib), the first software library for DNN training on RISC-V multicore MCU-class devices. Finally, we will conclude by reviewing the main ODL challenges and limitations and describing the latest major results in this field.\n\nSpeakers:\n\nDr. Manuele Rusci, post-doc KU Leuven\n\nCristian Cioflan, PhD student ETH Zurich\n\nDavide Nadalini, PhD student UNIBO and POLITO\n\nTutorial Learning Objectives:\n\nA brief taxonomy of Continual Learning scenarios, metrics and benchmarks.\n\nBasic operations and building blocks of On-Device Learning.\n\nAnalysis of memory and computation costs for ODL on MCU-class devices.\n\nSoftware optimization hands-on for ODL on a multi-core RISC-V platform.\n\nPresent main research directions and challenges for ODL on ultra-low power MCUs.\n\nTarget Audience\n\nThis tutorial targets researchers and practitioners interested in new hardware & software solutions for On-Device Learning on low-end devices, such as MCUs. Participants should be familiar with concepts of Deep Neural Networks (main building blocks, inference vs training) and basic C programming for Microcontrollers.\n\nHands-on & Material\n\nThe hands-on will demonstrate the open-source PULP-TrainLib software library (https://github.com/pulp-platform/pulp-trainlib), the state-of-the-art software package for MCU class devices, to provide a concrete embodiment of the ODL concepts and explain the application of software optimization to the learning routines, i.e. parallelization, low-precision (half-precision floating point), loop unrolling, and vectorization. The speaker will show the main library features using reference code examples. To this aim, we will adopt the open-source PULP platform simulator (https://github.com/pulp-platform/gvsoc) and encourage the audience to practice with the PULP-TrainLib ODL framework during the session. We will collect all the materials and installation instructions on a dedicated Github repository, which will be made available to the audience in advance and after the tutorial.\n\nDetailed Program\n\nPart I: M. Rusci (KUL). On-Device Continual Learning: motivation and intro. (10’)\n\nTutorial Intro\n\nLimitation of DNN inference on MCUs\n\nContinual Learning scenarios and Metrics\n\nDealing with Catastrophic forgetting\n\nPart II: C. Cioflan (ETHZ). On-device Adaptation on a multi-core MCU device (25’ + 5’ Q&A)\n\nRevisiting ODL basic operations\n\nReviewing DNN learning from an embedded system perspective: computation and memory\n\nLearning on the PULP platform\n\nCase study: On-Device noise adaptation for keyword spotting\n\nPart III: D. Nadalini (UNIBO). Hands-on On-Device Learning on MCUs: Pulp-TrainLib. (25’ + 5’ Q&A)\n\nPULP-TrainLib Overview, Operator Definitions and Learning deployment\n\nCode Optimizations: parallelization, low-precision and vectorization\n\nPart IV: M. Rusci (KUL). Challenges and Research Directions for On-Device Continual Learning – 15’ + 5’ Q&A\n\nODL software frameworks for MCUs\n\nMemory-efficient learning and architectures\n\nData efficient learning\n\nOpen Problems\n\nFinal Q&A\n\nFS05 Focus Session: Evolution Of Ml Hardware: From Technologies To Algorithms And Architectures\n\nAdd this session to my calendar\n\nDate: Monday, 25 March 2024\n\nTime: 11:00 CET - 12:30 CET\n\nLocation / Room: Auditorium 2\n\nSession chair:\n\nKrishnendu Chakrabarty, Arizona State University, US\n\nSession co-chair:\n\nHussam Amrouch, TU Munich (TUM), DE\n\nOrganiser:\n\nPartha Pande, Washington State University, US\n\nTime Label Presentation Title\n\nAuthors 11:00 CET FS05.1 ALGORITHM TO TECHNOLOGY CO-OPTIMIZATION FOR CIM-BASED HYPERDIMENSIONAL COMPUTING\n\nSpeaker:\n\nMehdi Tahoori, Karlsruhe Institute of Technology, DE\n\nAuthors:\n\nMahta Mayahinia1, Simon Thomann2, Paul Genssler3, Christopher Münch1, Hussam Amrouch4 and Mehdi Tahoori1\n\n1Karlsruhe Institute of Technology, DE; 2Chair of AI Processor Design, TU Munich (TUM), DE; 3University of Stuttgart, DE; 4TU Munich (TUM), DE\n\nAbstract\n\nHyperdimensional computing (HDC) has been recognized as an efficient machine learning algorithm in recent years. Robustness against noise, and simple computational operations, while being limited by the memory bandwidth, make it a perfect fit for the concept of computation in memory (CiM) with emerging nonvolatile memory (NVM) technologies. For an HDC accelerator based on NVM-CiM, there are different parameters from the algorithm all the way down to the technology that interact with each other and affecting the overall inference accuracy as well as the energy efficiency of the accelerator. Therefore, in this paper, we propose, for the first time, a full-stack co-optimization method and use it to design an HDC accelerator based on NVM-based content addressable memory (CAM). By incorporating the device manufacturing variability and co-optimizing the algorithm and hardware design, HDC inference on our proposed NVM-based CiM accelerator can reduce the energy consumption by 3.27x, while compared to the purely software-based implementation, the inference accuracy loss is merely 0.125%. 11:30 CET FS05.2 ACCELERATING NEURAL NETWORKS FOR LARGE LANGUAGE MODELS AND GRAPH PROCESSING WITH SILICON PHOTONICS\n\nSpeaker:\n\nSudeep Pasricha, Colorado State University, US\n\nAuthors:\n\nSalma Afifi, Febin Sunny, Mahdi Nikdast and Sudeep Pasricha, Colorado State University, US\n\nAbstract\n\nIn the rapidly evolving landscape of artificial intelligence, large language models (LLMs) and graph processing have emerged as transformative technologies for natural language processing (NLP), computer vision, and graph-structured data applications. However, the complex structures of these models pose challenges for acceleration on conventional electronic platforms. In this paper, we describe novel hardware accelerators based on silicon photonics to accelerate transformer neural networks that are used in LLMs and graph neural networks for graph data processing. Our analysis demonstrates that both hardware accelerators achieve at least 10.2× throughput improvement and 3.8× better energy efficiency over multiple state-of-the-art electronic hardware accelerators designed for LLMs and graph processing. 12:00 CET FS05.3 DATAFLOW-AWARE PIM-ENABLED MANYCORE ARCHITECTURE FOR DEEP LEARNING WORKLOADS\n\nSpeaker:\n\nPartha Pratim Pande, Washington State University, IN\n\nAuthors:\n\nHarsh Sharma1, Gaurav Narang1, Jana Doppa1, Umit Ogras2 and Partha Pratim Pande1\n\n1Washington State University, US; 2University of Wisconsin Madison, Madison, US\n\nAbstract\n\nProcessing-in-memory (PIM) has emerged as an enabler for the energy-efficient and high-performance acceleration of deep learning (DL) workloads. Resistive random-access memory (ReRAM) is one of the most promising technologies to implement PIM. However, as the complexity of Deep convolutional neural networks (DNNs) grows, we need to design a manycore architecture with multiple ReRAM-based processing elements (PEs) on a single chip. Existing PIM-based architectures mostly focus on computation while ignoring the role of communication. ReRAM-based tiled manycore architectures often involve many Processing Elements (PEs), which need to be interconnected via an efficient on-chip communication infrastructure. Simply allocating more resources (ReRAMs) to speed up only computation is ineffective if the communication infrastructure cannot keep up with it. In this paper, we highlight the design principles of a dataflow-aware PIM-enabled manycore platform tailor-made for various types of DL workloads. We consider the design challenges with both 2.5D interposer- and 3D integration-enabled architectures.\n\nLKS01 Later … With The Keynote Speakers\n\nAdd this session to my calendar\n\nDate: Monday, 25 March 2024\n\nTime: 11:00 CET - 12:30 CET\n\nLocation / Room: VIP Room\n\nSpeakers: Robert Dimond & Luc Augustin\n\nTS07 FPGA Solutions\n\nAdd this session to my calendar\n\nDate: Monday, 25 March 2024\n\nTime: 11:00 CET - 12:30 CET\n\nLocation / Room: Auditorium 3\n\nSession chair:\n\nGeorgios Zervakis, University of Patras, GT\n\nSession co-chair:\n\nKuan-Hsun Chen, University of Twente, NL\n\nTime Label Presentation Title\n\nAuthors 11:00 CET TS07.1 UNVEILING THE BLACK-BOX: LEVERAGING EXPLAINABLE AI FOR FPGA DESIGN SPACE OPTIMIZATION\n\nSpeaker:\n\nJaemin Seo, Pohang University of Science and Technology, KR\n\nAuthors:\n\nJaemin Seo, Sejin Park and Seokhyeong Kang, Pohang University of Science and Technology, KR\n\nAbstract\n\nWith significant advancements in various design methodologies, modern integrated circuits have experienced noteworthy improvements in power, performance, and area. Among various methodologies, design space optimization (DSO), which automatically explores electronic design automation (EDA) tool parameters for a given design, has been extensively studied in recent years. In this study, we propose an approach to fine-tuning an effective FPGA design space to suit a specific design. By utilizing our ML-based prediction and explainable artificial intelligence (XAI) approach, we quantify parameter contribution scores, which reveal the correlation between each parameter and the final timing results. Using the valuable insights from the parameter contribution scores, we can refine the design space only with effective parameters for subsequent timing optimization. During the optimization, our framework improved the maximum operating frequency by 26% on average in six test designs. To accomplish this, our framework required even 47% fewer FPGA compilations than the baseline, demonstrating its superior capacity for achieving fast convergence. 11:05 CET TS07.2 AN AGILE DEPLOYING APPROACH FOR LARGE-SCALE WORKLOADS ON CGRA-CPU ARCHITECTURE\n\nSpeaker:\n\nJiahang Lou, State Key Laboratory of ASIC and System, Fudan University, Shanghai, China, CN\n\nAuthors:\n\nJiahang Lou, Xuchen Gao, Yiqing Mao, Yunhui Qiu, Yihan Hu, Wenbo Yin and Lingli Wang, Fudan University, CN\n\nAbstract\n\nAdopting specialized accelerators such as Coarse-Grained Reconfigurable Architectures (CGRAs) alongside CPUs to enhance performance within specific domains is an astute choice. However, the integration of heterogeneous architectures introduces complex challenges for compiler design. Simultaneously, the ever-expanding scale of workloads imposes substantial burdens on deployment. To address above challenges, this paper introduces CGRV-OPT, a user-friendly multi-level compiler designed to deploy large-scale workloads to CGRA and RISC-V CPU architecture. Built upon the MLIR framework, CGRV-OPT serves as a pivotal bridge, facilitating the seamless conversion of high-level workload descriptions into low-level intermediate representations (IRs) for different architectures. A salient feature of our approach is the automation of a comprehensive suite of optimizations and transformations, which speed up each kernel computing within the intricate SoC. Additionally, we have seamlessly integrated an automated software-hardware partitioning mechanism, guided by our multi-level optimizations, resulting in a remarkable 2.14x speed up over large-scale workloads. The CGRV-OPT framework significantly alleviates the challenges faced by software developers, including those with limited expertise in hardware architectures. 11:10 CET TS07.3 CUPER: CUSTOMIZED DATAFLOW AND PERCEPTUAL DECODING FOR SPARSE MATRIX-VECTOR MULTIPLICATION ON HBM-EQUIPPED FPGAS\n\nSpeaker:\n\nzhou jin, Super Scientific Software Laboratory, China University of Petroleum-Beijing, China, CN\n\nAuthors:\n\nEnxin Yi1, Yiru Duan1, Yinuo Bai1, Kang Zhao2, Zhou Jin1 and Weifeng Liu1\n\n1China University of Petroleum-Beijing, CN; 2Beijing University Of Posts and Telecommunications, CN\n\nAbstract\n\nSparse matrix-vector multiplication (SpMV) is pivotal in many scientific computing and engineering applications. Considering the memory-intensive nature and irregular data access patterns inherent in SpMV, its acceleration is typically bounded by the limited bandwidth. Multiple memory channels of the emerging high bandwidth memory (HBM) provide exceptional bandwidth, offering a great opportunity to boost the performance of SpMV. However, ensuring high bandwidth utilization with low memory access conflicts is still non-trivial. In this paper, we present Cuper, a high-performance SpMV accelerator on HBM-equipped FPGAs. Through customizing the dataflow to be HBM-compatible with the proposed sparse storage format, the bandwidth utilization can be sufficiently enhanced. Furthermore, a two-step reordering algorithm and perceptual decoder-centric hardware architecture are designed to greatly mitigate read-after-write (RAW) conflicts, enhance the vector reusability and on-chip memory utilization. The evaluation of 12 large matrices shows that Cuper's geomean throughput outperforms the four latest SpMV accelerators HiSparse, GraphLily, Sextans, and Serpens, by 3.28×, 1.99×, 1.75×, and 1.44×, respectively. Furthermore, the geomean bandwidth efficiency shows 3.28×, 2.20×, 2.82×, and 1.31× improvements, while the geomean energy efficiency has 3.59×, 2.08×, 2.21×, and 1.44× optimizations, respectively. Cuper also demonstrates 2.51× throughput and 7.97× energy efficiency of improvement over the K80 GPU on 2,757 SuiteSparse matrices. 11:15 CET TS07.4 TOWARDS HIGH-THROUGHPUT NEURAL NETWORK INFERENCE WITH COMPUTATIONAL BRAM ON NONVOLATILE FPGAS\n\nSpeaker:\n\nHao Zhang, Shandong University, CN\n\nAuthors:\n\nHao Zhang1, Mengying Zhao2, Huichuan Zheng2, Yuqing Xiong2, Yuhao Zhang3 and Zhaoyan Shen2\n\n1Shandong university, CN; 2Shandong University, CN; 3Tsinghua University, CN\n\nAbstract\n\nField-programmable gate arrays (FPGAs) have been widely used in artificial intelligence applications. As the capacity requirements of both computation and memory resources continuously increase, emerging nonvolatile memory has been proposed to replace static random access memory (SRAM) in FPGAs to build nonvolatile FPGAs (NV-FPGAs), which have advantages of high density and near-zero leakage power. Features of emerging nonvolatile memory should be fully explored to improve performance, energy efficiency as well as lifetime of NV-FPGAs. In this paper, we study an intrinsic characteristic of emerging nonvolatile memory, i.e., computing-in-memory, in nonvolatile block random access memory (BRAM) of NV-FPGAs. Specifically, we present a computational BRAM architecture (C-BRAM), and propose a computational density aware operator allocation strategy to fully utilize C-BRAM. Neural network inference is taken as an example to evaluate the proposed architecture and strategy, showing 68% and 62% improvement in computational density compared to traditional SRAM-based FPGA and existing NV-FPGA, respectively. 11:20 CET TS07.5 BITSTREAM FAULT INJECTION ATTACKS ON CRYSTALS KYBER IMPLEMENTATIONS ON FPGAS\n\nSpeaker:\n\nZiying Ni, Centre for Secure Information Technologies (CSIT), Queen, GB\n\nAuthors:\n\nZiying Ni1, ayesha khalid2, Weiqiang Liu3 and Maire O'Neill2\n\n1CentreforSecureInformationTechnologies(CSIT),QueensUniversityBelfast, GB; 2Queen's University Belfast, GB; 3Nanjing University of Aeronautics and Astronautics, CN\n\nAbstract\n\nCRYSTALS-Kyber is the only Public-key Encryption (PKE)/ Key-encapsulation Mechanism (KEM) scheme that was chosen for standardization by the National Institute of Standards and Technology initiated Post-quantum Cryptography competition (so called NIST PQC). In this paper, we show the first successfully malicious modifications of the bitstream of a Kyber FPGA implementation. We successfully demonstrate 4 different attacks on Kyber hardware implementations on Artix-7 FPGAs that either reduce the complexity of polynomial multiplication operations or enable direct secret key/ message recovery by: disabling BRAMs, disabling DSPs, zeroing NTT ROM and tampering with CBD2 results. Two of our attacks are generic in nature and the other two require reverse-engineering or a detailed knowledge of the design. We evaluate the feasibility of the four attacks, among which the zeroing NTT ROM and tampering with the CBD2 result attacks produce higher public key and ciphertext complexity and thus are difficult to be detected. Two countermeasures are proposed to prevent the attacks proposed in this paper. 11:25 CET TS07.6 ON-FPGA SPIKING NEURAL NETWORKS FOR INTEGRATED NEAR-SENSOR ECG ANALYSIS\n\nSpeaker:\n\nMatteo Antonio Scrugli, University of Cagliari, IT\n\nAuthors:\n\nMatteo Antonio Scrugli, Paola Busia, Gianluca Leone and Paolo Meloni, Università degli studi di Cagliari, IT\n\nAbstract\n\nThe identification of cardiac arrhythmias is a significant issue in modern healthcare and a major application for Artificial Intelligence (AI) systems based on artificial neural networks. This research introduces a real-time arrhythmia diagnosis system that uses a Spiking Neural Network (SNN) to classify heartbeats into five types of arrhythmias from a single-lead electrocardiogram (ECG) signal. The system is implemented on a custom SNN processor running on a low-power Lattice iCE40-UltraPlus FPGA. It was tested using the MIT-BIH dataset, and achieved accuracy results that are comparable to the most advanced SNN models, reaching 98.4% accuracy. The proposed modules take advantage of the energy efficiency of SNNs to reduce the average execution time to 4.32 ms and energy consumption to 50.98 uJ per classification. 11:30 CET TS07.7 SPECHD: HYPERDIMENSIONAL COMPUTING FRAMEWORK FOR FPGA-BASED MASS SPECTROMETRY CLUSTERING\n\nSpeaker:\n\nSumukh Pinge, University of California, San Diego, US\n\nAuthors:\n\nSumukh Pinge1, Weihong Xu1, Jaeyoung Kang1, Tianqi Zhang1, Niema Moshiri1, Wout Bittremieux2 and Tajana Rosing1\n\n1University of California, San Diego, US; 2University of Antwerp, BE\n\nAbstract\n\nMass spectrometry-based proteomics is a key enabler for personalized healthcare, providing a deep dive into the complex protein compositions of biological systems. This technology has vast applications in biotechnology and biomedicine but faces significant computational bottlenecks. Current methodologies often require multiple hours or even days to process extensive datasets, particularly in the domain of spectral clustering. To tackle these inefficiencies, we introduce Spec-HD, a hyperdimensional computing framework supplemented by an FPGA-accelerated architecture with integrated near-storage preprocessing. Utilizing streamlined binary operations in a hyperdimensional computational environment, Spec-HD capitalizes on the low-latency and parallel capabilities of FPGAs. This approach markedly improves clustering speed and efficiency, serving as a catalyst for real-time, high-throughput data analysis in future healthcare applications. Our evaluations demonstrate that Spec-HD not only maintains but often surpasses existing clustering quality metrics while drastically cutting computational time. Specifically, it can cluster a large-scale human proteome dataset—comprising 25 million MS/MS spectra and 131 GB of MS data—in mere 5 minutes. With energy efficiency exceeding 31× and a speedup factor that spans a range of 6× to 54× over existing state-of-the-art solutions, Spec-HD emerges as a promising solution for the rapid analysis of mass spectrometry data with great implications for personalized healthcare. 11:35 CET TS07.8 MEMORY SCRAPING ATTACK ON XILINX FPGAS: PRIVATE DATA EXTRACTION FROM TERMINATED PROCESSES\n\nSpeaker:\n\nSandip Kundu, University of Massachusetts Amherst, US\n\nAuthors:\n\nBharadwaj Madabhushi, Sandip Kundu and Daniel Holcomb, University of Massachusetts Amherst, US\n\nAbstract\n\nFPGA-based hardware accelerators are becoming increasingly popular due to their versatility, customizability, energy efficiency, constant latency, and scalability. FPGAs can be tailored to specific algorithms, enabling efficient hardware implementations that effectively leverage algorithm parallelism. This can lead to significant performance improvements over CPUs and GPUs, particularly for highly parallel applications. For example, a recent study found that Stratix 10 FPGAs can achieve up to 90\\% of the performance of a TitanX Pascal GPU while consuming less than 50\\% of the power. This makes FPGAs an attractive choice for accelerating machine learning (ML) workloads. However, our research finds privacy and security vulnerabilities in existing Xilinx FPGA-based hardware acceleration solutions. These vulnerabilities arise from the lack of memory initialization and insufficient process isolation, which creates potential avenues for unauthorized access to private data used by processes. To illustrate this issue, we conducted experiments using a Xilinx ZCU104 board running the PetaLinux tool from Xilinx. We found that PetaLinux does not effectively clear memory locations associated with a terminated process, leaving them vulnerable to memory scraping attack (MSA). This paper makes two main contributions. The first contribution is an attack methodology of using the Xilinx debugger from a different user space. We find that we are able to access process IDs, virtual address spaces, and pagemaps of one user from a different user space because of lack of adequate process isolation. The second contribution is a methodology for characterizing terminated processes and accessing their private data. We illustrate this on Xilinx ML application library. These vulnerabilities were reported to Xilinx and confirmed by them. 11:40 CET TS07.9 HIGH-EFFICIENCY FPGA-BASED APPROXIMATE MULTIPLIERS WITH LUT SHARING AND CARRY SWITCHING\n\nSpeaker:\n\nQilin Zhou, Yunnan University, CN\n\nAuthors:\n\nYi GUO1, Qilin ZHOU1, Xiu CHEN1 and Heming SUN2\n\n1Yunnan University, CN; 2Yokohama National University, JP\n\nAbstract\n\nApproximate multiplier saves energy and improves hardware performance for error-tolerant computation-intensive applications. This work proposes hardware-efficient FPGA-based approximate multipliers with look-up table (LUT) sharing and carry switching. Sharing two LUTs with the same inputs enables to fully utilize the available LUT resources. To mitigate the accuracy loss incurred from this approach, the truncated carry is partially reserved by switching it to the adjacent calculation. In addition, we create a library of 8×8 approximate multipliers to provide various multiplication choices. The proposed design can provide enhancements of up to 38.75% in power, 17.29% in latency, and 28.17% in area compared to the Xilinx exact multiplier. Our proposed designs are open-source at https://github.com/YnuGuoLab/DATE_FPGA_Approx_Mul and assist in further reproducing and development. 11:41 CET TS07.10 OTFGENCODER-HDC: HARDWARE-EFFICIENT ENCODING TECHNIQUES FOR HYPERDIMENSIONAL~COMPUTING\n\nSpeaker:\n\nMahboobe Sadeghipourrudsari, Technology Factory Karlsruhe GmbH, DE\n\nAuthors:\n\nMahboobe Sadeghipourrudsari, Jonas Krautter and Mehdi Tahoori, Karlsruhe Institute of Technology, DE\n\nAbstract\n\nHyper-Dimensional Computing (HDC), a brain-inspired computing paradigm for cognitive tasks, is especially suited for resource-constrained edge devices due to its hardware-efficient and fault-resistant inference. However, existing HDC approaches require large amounts of memory, resulting in high power consumption, limiting their use in edge devices. We offer a hardware-aware encoding where computation parameters in hardware implementations can be reproduced on-the-fly through low-overhead cyclic digital circuits, significantly reducing memory utilization and subsequently power consumption. 11:42 CET TS07.11 INTERACTIVE TECHNICAL PRESENTATIONS BY THE AUTHORS\n\nPresenter:\n\nSession Chairs, DATE, ES\n\nAuthor:\n\nSession Chairs, DATE, ES\n\nAbstract\n\nParticipants can freely interact with authors during their interactive technical presentations.\n\nTS17 Adaptive And Sensing Systems\n\nAdd this session to my calendar\n\nDate: Monday, 25 March 2024\n\nTime: 11:00 CET - 12:30 CET\n\nLocation / Room: Multi-Purpose Room M1A+C\n\nSession chair:\n\nAmit Singh, University of Essex, UK\n\nSession co-chair:\n\nNicola Bombieri, U Verona, IT\n\nTime Label Presentation Title\n\nAuthors 11:00 CET TS17.1 OISA: ARCHITECTING AN OPTICAL IN-SENSOR ACCELERATOR FOR EFFICIENT VISUAL COMPUTING\n\nSpeaker:\n\nShaahin Angizi, New Jersey Institute of Technology, US\n\nAuthors:\n\nMehrdad Morsali1, Sepehr Tabrizchi2, Deniz Najafi3, Mohsen Imani4, Mahdi Nikdast5, Arman Roohi6 and Shaahin Angizi3\n\n1New jersey Institute of Technology, US; 2University of Nebraska–Lincoln, US; 3New Jersey Institute of Technology, US; 4University of California, Irvine, US; 5Colorado State University, US; 6University of Nebraska–Lincoln (UNL), US\n\nAbstract\n\nTargeting vision applications at the edge, in this work, we systematically explore and propose a high-performance and energy-efficient Optical In-Sensor Accelerator architecture called OISA for the first time. Taking advantage of the promising efficiency of photonic devices, the OISA intrinsically implements a coarse-grained convolution operation on the input frames in an innovative minimum-conversion fashion in low-bit-width neural networks. Such a design remarkably reduces the power consumption of data conversion, transmission, and processing in the conventional cloud-centric architecture as well as recently-presented edge accelerators. Our device-to-architecture simulation results on various image data-sets demonstrate acceptable accuracy while OISA achieves 6.68 TOp/s/W efficiency. OISA reduces power consumption by a factor of 7.9 and 18.4 on average compared with existing electronic in-/near-sensor and ASIC accelerators. 11:05 CET TS17.2 PATHDRIVER-WASH: A PATH-DRIVEN WASH OPTIMIZATION METHOD FOR CONTINUOUS-FLOW LAB-ON-A-CHIP SYSTEMS\n\nSpeaker:\n\nJiaxuan Wang, Northwestern Polytechnical University, CN\n\nAuthors:\n\nXing Huang1, Jiaxuan Wang1, Zhiwen Yu1, Bin Guo1, Tsung-Yi Ho2, Ulf Schlichtmann3 and Krishnendu Chakrabarty4\n\n1Northwestern Polytechnical University, CN; 2The Chinese University of Hong Kong, HK; 3TU Munich, DE; 4Arizona State University, US\n\nAbstract\n\nRapid advances in microfluidics technologies have facilitated the emergence of highly integrated lab-on-a-chip (LoC) biochip systems. With such coin-sized biochips, complicated bioassay procedures can be executed efficiently without any human intervention. To ensure the correctness and precision of assay outcomes, however, cross-contamination among different fluid samples/reagents needs to be dealt with separately during assay execution. As a consequence, wash operations have to be introduced and a wash path network needs to be established on the chip to remove the residues left in flow channels. To realize optimized assay procedures with efficient wash operations, we propose PathDriver-Wash in this paper, a path-driven wash optimization method for continuous-flow LoC biochip systems. The proposed method includes the following three key techniques: 1) The necessity of contamination removals is analyzed systemically to avoid unnecessary wash operations, 2) wash operations are integrated with the regular removal of excess fluids, so that extra path occupations caused by wash can be minimized, and 3) optimized wash paths and time windows are computed and assigned to wash operations, so that the completion time of assays can be minimized. Experimental results demonstrate that the proposed method leads to highly efficient wash procedures as well as minimized assay completion times. 11:10 CET TS17.3 MULTI-AGENT REINFORCEMENT LEARNING FOR THERMALLY-RESTRICTED PERFORMANCE OPTIMIZATION ON MANYCORES\n\nSpeaker:\n\nHeba Khdr, Karlsruhe Institute of Technology, DE\n\nAuthors:\n\nHeba Khdr1, Mustafa Batur1, Kanran Zhou1, Mohammed Bakr Sikal2 and Joerg Henkel1\n\n1Karlsruhe Institute of Technology, DE; 2Chair for Embedded Systems, Karlsruhe Institute of Technology, DE\n\nAbstract\n\nThe problem of performance maximization under a thermal constraint has been tackled by means of dynamic voltage and frequency scaling (DVFS) in many system-level optimization techniques. State-of-the-art ones have exploited Supervised Learning (SL) to develop models that predict power and performance characteristics of applications and temperature of the cores. Such predictions enable proactive and efficient optimization decisions that exploit performance potentials under a temperature constraint. SL-based models are built at design time based on training data generated considering specific environment settings, i.e., processor architecture, cooling system, ambient temperature, etc. Hence, these models cannot adapt at runtime to different environment settings. In contrast, Reinforcement Learning (RL) employs an agent that explores and learns the environment at runtime, and hence can adapt to its potential changes. Nonetheless, using an RL agent to perform optimization on manycores is challenging because of the inherent large state/action spaces that might hinder the agent's ability to converge. To get the advantages of RL while tackling this challenge, we employ for the first time multi-agent RL to perform thermally-restricted performance optimization for manycores through DVFS. We investigated two RL algorithms—Table-based Q-Learning (TQL) and Deep QLearning (DQL)—and demonstrated that the latter outperforms the former. Compared to the state of the art, our DQL delivers a significant performance improvement of 34.96% on average, while also guaranteeing thermally-safe operation on the manycore. Our evaluation reveals the runtime adaptability of our DQL to varying workloads and ambient temperatures. 11:15 CET TS17.4 OPLIXNET: TOWARDS AREA-EFFICIENT OPTICAL SPLIT-COMPLEX NETWORKS WITH REAL-TO-COMPLEX DATA ASSIGNMENT AND KNOWLEDGE DISTILLATION\n\nSpeaker:\n\nRuidi Qiu, TU Munich, DE\n\nAuthors:\n\nRuidi Qiu1, Amro Eldebiky1, Grace Li Zhang2, Xunzhao Yin3, Cheng Zhuo3, Ulf Schlichtmann1 and Bing Li1\n\n1TU Munich, DE; 2TU Darmstadt, DE; 3Zhejiang University, CN\n\nAbstract\n\nHaving the potential for high computing speed, high throughput, and low energy cost, optical neural networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OplixNet to compress the areas of ONNs by modulating input image data into the amplitudes and phases of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONN with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on fully connected neural network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32. 11:20 CET TS17.5 DESIGN AUTOMATION FOR ORGANS-ON-CHIP\n\nSpeaker:\n\nMaria Emmerich, TU Munich, DE\n\nAuthors:\n\nMaria Emmerich1, Philipp Ebner2 and Robert Wille1\n\n1TU Munich, DE; 2Johannes Kepler University Linz, AT\n\nAbstract\n\nOrgans-on-Chips (OoCs) are testing platforms for the pharmaceutical, cosmetic, and chemical industries. They are composed of miniaturized organ tissues (so-called organ modules) that are connected via a microfluidic channel network and, by this, emulate human or other animal physiology on a miniaturized chip. The design of those chips, however, requires a sophisticated orchestration of numerous aspects, such as the size of organ modules, the required shear stress on membranes, the dimensions and geometry of channels, pump pressures, etc. Mastering all this constitutes a non-trivial design task for which, unfortunately, no automatic support exists yet. In this work, we propose a first design automation solution for OoCs. To this end, we review the respective design steps and formalize a corresponding design specification from it. Based on that, we then propose an automatic method which generates a design of the desired device. Evaluations (inspired by real-world use cases and confirmed by CFD simulations) demonstrate the applicability and validity of the proposed approach. 11:25 CET TS17.6 TRACE-ENABLED TIMING MODEL SYNTHESIS FOR ROS2-BASED AUTONOMOUS APPLICATIONS\n\nSpeaker:\n\nHazem Abaza, Huawei Technologies, DE\n\nAuthors:\n\nHazem Abaza1, Debayan Roy2, Shiqing Fan3, Selma Saidi4 and Antonios Motakis2\n\n1Huawei Dresden Research Center - TU Dortmund, DE; 2Huawei Dresden Research Center, DE; 3Huawei Munich Research Center, DE; 4TU Dortmund, DE\n\nAbstract\n\nAutonomous applications are typically developed over Robot Operating System 2.0 (ROS2) even in time-critical systems like automotive. Recent years have seen increased interest in developing model-based timing analysis and schedule optimization approaches for ROS2-based applications. To complement these approaches, we propose a tracing and measurement framework to obtain timing models of ROS2-based applications. It offers a tracer based on extended Berkeley Packet Filter that probes different functions in ROS2 middleware and reads their arguments or return values to reason about the data flow in applications. It combines event traces from ROS2 and the operating system to generate a directed acyclic graph showing ROS2 callbacks, precedence relations between them, and their timing attributes. While being compatible with existing analyses, we also show how to model (i) message synchronization, e.g., in sensor fusion, and (ii) service requests from multiple clients, e.g., in motion planning. Considering that, in real-world scenarios, the application code might be confidential and formal models are unavailable, our framework still enables the application of existing analysis and optimization techniques. We demonstrate our framework's capabilities by synthesizing the timing model of a real-world benchmark implementing LIDAR-based localization in Autoware's Autonomous Valet Parking. 11:30 CET TS17.7 SLET FOR DISTRIBUTED AEROSPACE LANDING SYSTEM\n\nSpeaker:\n\nGuillaume Phavorin, ASTERIOS Technologies, FR\n\nAuthors:\n\nDamien Chabrol1, Guillaume Phavorin1 and Eric Jenn2\n\n1Krono-Safe, FR; 2IRT Saint Exupéry, FR\n\nAbstract\n\nThe aerospace industry is moving towards digital systems that are both more condensed (merging criticality-heterogenous functions on a same equipment) and more distributed (for robustness, availability, actuators/sensors closeness) while software-defined. This makes integration activities highly critical for next-generation systems, due to the interaction complexity between the software components and their deployment on the hardware platform, combined with outdated development processes with regard to the multicore transition. Therefore, predictability, testability, and ultimately strong determinism are crucial high-level properties needed not only at equipment level but at the whole system scope, which cannot be tackled without changes in the design process. This paper deals with an innovative solution, based on the sLET paradigm, to bring drastic integration time reduction whatever the underlying architecture (multicore, multi-node). Already proved worthy for multicore platforms, sLET deployment is applied to an aerospace landing system over a distributed system architecture. 11:35 CET TS17.8 A MAPPING OF TRIANGULAR BLOCK INTERLEAVERS TO DRAM FOR OPTICAL SATELLITE COMMUNICATION\n\nSpeaker:\n\nLukas Steiner, University of Kaiserslautern-Landau, DE\n\nAuthors:\n\nLukas Steiner1, Timo Lehnigk-Emden2, Markus Fehrenz2 and Norbert Wehn3\n\n1University of Kaiserslautern-Landau, DE; 2Creonic GmbH, DE; 3TU Kaiserslautern, DE\n\nAbstract\n\nCommunication in optical downlinks of low earth orbit (LEO) satellites requires interleaving to enable reliable data transmission. These interleavers are orders of magnitude larger than conventional interleavers utilized for example in wireless communication. Hence, the capacity of on-chip memories (SRAMs) is insufficient to store all symbols and external memories (DRAMs) must be used. Due to the overall requirement for very high data rates beyond 100 Gbit/s, DRAM bandwidth then quickly becomes a critical bottleneck of the communication system. In this paper, we investigate triangular block interleavers for the aforementioned application and show that the standard mapping of symbols used for SRAMs results in low bandwidth utilization for DRAMs, in some cases below 50 %. As a solution, we present a novel mapping approach that combines different optimizations and achieves over 90 % bandwidth utilization in all tested configurations. Further, the mapping can be applied to any JEDEC-compliant DRAM device. 11:40 CET TS17.9 OC-DLRM: MINIMIZING THE I/O TRAFFIC OF DLRM BETWEEN MAIN MEMORY AND OCSSD\n\nSpeaker:\n\nTseng-Yi Chen, National Central University, TW\n\nAuthors:\n\nShang-Hung Ti1, Tseng-Yi Chen1, Tsung Tai Yeh2, Shuo-Han Chen2 and Yu-Pei Liang3\n\n1National Central University, TW; 2National Yang Ming Chiao Tung University, TW; 3National Chung Cheng University, TW\n\nAbstract\n\nDue to the exponential growth of data in computing, DRAM-based main memory is now insufficient for data-intensive applications like machine learning and recommendation systems. This has led to a performance issue involving data transfer between main memory and storage devices. Conventional NAND-based SSDs are unable to efficiently handle this problem as they can't distinguish between data types from the host system. In contrast, open-channel SSDs (OCSSD) offer a solution by optimizing data placement from the host-side system. This research focuses on developing a new data access model for deep learning recommendation systems (DLRM) using OCSSD storage drives, called OC-DLRM. OC-DLRM reduces I/O traffic to flash memory by aggregating frequently-accessed data using the I/O unit of a flash memory drive. Our experiments show that OC-DLRM has significant performance improvement compared with traditional swapping space management techniques. 11:41 CET TS17.10 SEAL: SENSING EFFICIENT ACTIVE LEARNING ON WEARABLES THROUGH CONTEXT-AWARENESS\n\nSpeaker:\n\nAnil Kanduri, University of Turku Finland, FI\n\nAuthors:\n\nHamidreza Alikhani Koshkak1, Ziyu Wang1, Anil Kanduri2, Pasi Liljeberg2, Amir M. Rahmani1 and Nikil Dutt1\n\n1University of California, Irvine, US; 2University of Turku, FI\n\nAbstract\n\nIn this paper, we introduce SEAL, a co-optimization framework designed to enhance both sensing and querying strategies in wearable devices for mHealth applications. Employing Reinforcement Learning (RL), SEAL strategically utilizes user contextual information and the machine learning model's confidence levels to make efficient decisions. This innovative approach is particularly significant in addressing the challenge of battery drain due to continuous physiological signal sensing, such as Photoplethysmography (PPG). Our framework demonstrates its effectiveness in a stress monitoring application, achieving a substantial reduction of 76\\% in the volume of PPG signals collected, while only experiencing a minor 6\\% decrease in user-labeled data quality. This balance showcases SEAL's potential in optimizing data collection in a way that is considerate of both device constraints and data integrity. 11:42 CET TS17.11 DYNAMIC PER-FLOW QUEUES FOR TSN SWITCHES\n\nSpeaker:\n\nWenxue Wu, Lanzhou University, CN\n\nAuthors:\n\nWenxue Wu1, Zhen Li1, Tong Zhang2, Xiaoqin Feng1, Liwei Zhang1, Xuelong Qi1 and Fengyuan Ren3\n\n1Lanzhou University, CN; 2Nanjing University of Aeronautics and Astronautics, CN; 3Lanzhou University | Tsinghua University, CN\n\nAbstract\n\nDynamic Per-Flow Queues (DFQ) extend queues from per-class to per-flow in Time-Sensitive Networking (TSN) switches that overcome large resource consumption by dynamically mapping a fixed number of physical queues to active flows. It can implement per-flow queuing with much less on-chip resource. Compared to brute-force hardware queues, DFQ prototyped on an FPGA, can effectively manage more per-flow queues, allowing for improved priority scheduling with minimal throughput and latency impact. 11:43 CET TS17.12 INTERACTIVE TECHNICAL PRESENTATIONS BY THE AUTHORS\n\nPresenter:\n\nSession Chairs, DATE, ES\n\nAuthor:\n\nSession Chairs, DATE, ES\n\nAbstract\n\nParticipants can freely interact with authors during their interactive technical presentations.\n\nTS22 Microarchitectural And Side-Channel-Based Attacks And Countermeasures\n\nAdd this session to my calendar\n\nDate: Monday, 25 March 2024\n\nTime: 11:00 CET - 12:30 CET\n\nLocation / Room: Multi-Purpose Room M1B+D\n\nSession chair:\n\nRicardo Chaves, INESC-ID, IST/ULisboa, PT\n\nSession co-chair:\n\nFrancesco Regazzoni, UvA & ALaRI, CH\n\nTime Label Presentation Title\n\nAuthors 11:00 CET TS22.1 FLUSH+EARLYRELOAD: COVERT CHANNELS ATTACK ON SHARED LLC USING MSHR MERGING\n\nSpeaker:\n\nPrathamesh Tanksale, Dept. of CSE, Indian Institute of Technology Ropar, Punjab, India, IN\n\nAuthors:\n\nAditya Gangwar1, Prathamesh Tanksale1, Shirshendu Das2 and Sudeepta Mishra1\n\n1Department of CSE Indian Institute of Technology Ropar, Punjab, India, IN; 2Department of CSE Indian Institute of Technology Hyderabad, Telangana, India, IN\n\nAbstract\n\nModern multiprocessors include multiple cores, all of them share a large Last Level Cache (LLC). Because of the shared nature of LLC, different timing channel attacks can be built by exploiting LLC behaviors. Covert Channel and Side Channel are two well-known attacks used to steal sensitive information from a secure application. While several countermeasures have already been proposed to prevent these attacks, the possibility of discovering new variants cannot be overlooked. In this paper, we propose a covert channel attack designed to circumvent the state-of-the-art countermeasure for preventing the Flush+Reload attack. Experimental results indicate that the proposed attack renders the current state-of-the-art countermeasure futile and ineffective. 11:05 CET TS22.2 PRIME+RESET: INTRODUCING A NOVEL CROSS-WORLD COVERT-CHANNEL THROUGH COMPREHENSIVE SECURITY ANALYSIS ON ARM TRUSTZONE\n\nSpeaker:\n\nArash Pashrashid, National University of Singapore (NUS), SG\n\nAuthors:\n\nYun Chen1, Arash Pashrashid1, Yongzheng Wu2 and Trevor E. Carlson1\n\n1National University of Singapore, SG; 2Huawei Singapore, SG\n\nAbstract\n\nARM TrustZone, a robust security technique, thwarts a wide range of threats by partitioning the system-on-chip hardware and software into two distinct worlds, namely the normal world and the secure world. However, it still remains susceptible to malicious attacks, including side-channel and covert-channel vulnerabilities. Previous efforts to leak data from TrustZone focused on cache-based and performance monitoring unit (PMU)-based channels; in this paper, we, however, propose a security analysis benchmark suite by traversing the hardware components involved in the microarchitecture to study their security impact on the secure world. Our investigation unveils an undisclosed leakage source stemming from the L2 prefetcher. We design a new cross-core and cross-world covert-channel attack based on our reverse engineering of the L2 prefetcher, named Prime+Reset. Compared to most cross-world covert-channel attacks, Prime+Reset is a cache- and PMU-agnostic attack that effectively bypasses many existing defenses. The throughput of Prime+Reset can achieve 776Kib/s, which demonstrates a significant improvement, 70x, over the state-of-the-art, while maintaining a similar error rate (<2%). One can find the code at https://github.com/yunchen-juuuump/prime-reset. 11:10 CET TS22.3 STATISTICAL PROFILING OF MICRO-ARCHITECTURAL TRACES AND MACHINE LEARNING FOR SPECTRE DETECTION: A SYSTEMATIC EVALUATION\n\nSpeaker:\n\nMai AL-Zu'bi, TU Wien, AT\n\nAuthors:\n\nMai AL-Zu'bi and Georg Weissenbacher, TU Wien, AT\n\nAbstract\n\nSecurity vulnerabilities like Spectre exploit features of modern processors to leak sensitive data through speculative execution and shared resources (such as caches). A popular approach to detect such attacks deploys Machine Learning (ML) to identify suspicious micro-architectural patterns. These techniques, however, are often rather ad-hoc in terms of the selection of micro- architectural features as well as ML techniques, and frequently lack a description of the underlying training- and test-data. To ad- dress these shortcomings, we systematically evaluate a large range of (combinations of) micro-architectural features recorded in up to 40 Hardware Performance Counters (HPCs) and multiple ML algorithms on a comprehensive set of well-documented scenarios and datasets. Using statistical methods, we rank the HPCs used to generate our dataset, which helps us determine the minimum number of features required for detecting Spectre attacks with high accuracy and minimal overhead. Furthermore, we identify the best-performing ML classifiers, and provide a comprehensive description of our data collection, running scenarios, selected HPCs, and chosen classification models. 11:15 CET TS22.4 THREE SIDEKICKS TO SUPPORT SPECTRE COUNTERMEASURES\n\nSpeaker:\n\nMarkus Krausz, Ruhr University Bochum, DE\n\nAuthors:\n\nMarkus Krausz1, Jan Thoma1, Florian Stolz1, Marc Fyrbiak2 and Tim Güneysu1\n\n1Ruhr University Bochum, DE; 2emproof GmbH, DE\n\nAbstract\n\nThe Spectre attack revealed a critical security threat posed by speculative execution and since then numerous related attacks have been discovered and exploited to leak secrets across process boundaries. As the primary cause of the attack is deeply rooted in the microarchitectural processor design, mitigating speculative execution attacks with minimal impact on performance is far from straightforward. For example, various countermeasures have been proposed to limit speculative execution for certain instruction patterns, however, resulting in severe performance overheads. In this paper, we propose a set of code transformations to reduce the number of speculatively executed instructions and therefore significantly reduce the performance overhead of various countermeasures. We evaluate our code transformations combined with a hardware-based countermeasure in gem5. Our results demonstrate that our code transformations speed up the secure system by up to 16.6%. 11:20 CET TS22.5 DETECTING BACKDOOR ATTACKS IN BLACK-BOX NEURAL NETWORKS THROUGH HARDWARE PERFORMANCE COUNTERS\n\nSpeaker:\n\nManaar Alam, New York University Abu Dhabi, AE\n\nAuthors:\n\nManaar Alam1, Yue Wang2 and Michail Maniatakos1\n\n1New York University Abu Dhabi, AE; 2New York University, US\n\nAbstract\n\nDeep Neural Networks (DNNs) have made significant strides, but their susceptibility to backdoor attacks still remains a concern. Most defenses typically assume access to white-box models or poisoned data, requirements that are often not feasible in practice, especially for proprietary DNNs. Existing defenses in a black-box setting usually rely on confidence scores of DNN's predictions. However, this exposes DNNs to the risk of model stealing attacks, a significant concern for proprietary DNNs. In this paper, we introduce a novel strategy for detecting backdoors, focusing on a more realistic black-box scenario where only hard-level (i.e., without any prediction confidence) query access is available. Our strategy utilizes data flow dynamics in a computational environment during DNN inference to identify potential backdoor inputs and is agnostic of trigger types or their locations in the input. We observe that a clean image and its corresponding backdoor counterpart with a trigger induce distinct patterns across various microarchitectural activities during the inference phase. We exploit these variations captured by Hardware Performance Counters (HPCs) and use principles of the Gaussian Mixture Model to detect backdoor inputs. To the best of our knowledge, this is the first work that utilizes HPCs for detecting backdoors in DNNs. Extensive evaluation considering a range of benchmark datasets, DNN architectures, and trigger patterns shows the efficacy of the proposed method in distinguishing between clean and backdoor inputs using HPCs. 11:25 CET TS22.6 CAN MACHINE LEARN PIPELINE LEAKAGE?\n\nSpeaker:\n\nParisa Amiri Eliasi, Radboud University, NL\n\nAuthors:\n\nOmid Bazangani1, Parisa Amiri Eliasi2, Stjepan Picek2 and Lejla Batina3\n\n1Digital Security Group, Radboud University, NL; 2Radboud University, NL; 3Radboud University Nijmegen, NL\n\nAbstract\n\nSide-channel attacks cause a significant threat to security implementations in embedded devices. Accordingly, an automated framework simulating side-channel behaviours can offer invaluable insights into leakage origins and characteristics, helping developers improve those devices during the design phase. While there has been a substantial effort towards crafting leakage simulators, earlier methods either necessitated significant manual work for reverse engineering the micro-architectural layer or depended on Deep Learning (DL) models where the neural network's complexity increased considerably with the addition of pipeline stages. This paper presents a novel modelling approach using Recur- rent Neural Networks (RNNs) to construct instruction-level power models that exhibit enhanced performance in detecting pipeline leakage. Our findings indicate that with memory-based machine learning models, it becomes unnecessary to input data accounting for the pipeline effect. This strategy reduces feature dimensionality by at least one-third for a three-stage pipeline, albeit at a modest compromise in model performance. This reduced feature set underscores our model's scalability, making it a preferred choice for analyzing microprocessors with extended pipeline stages. Importantly, our methodology accelerates the micro-architectural profiling phase in side-channel simulator design. When evaluated on an expansive dataset, the performance of our memory-based model closely matches that of the Multilayer Perceptron (MLP) with an R2 value of 0.79. On a reduced dataset (removing the pipeline effect), our model achieves an R2 value of 0.65, outperforming the MLP, which reaches an R2 value of 0.39. Moreover, our model is designed with scalability in mind, making it suitable for profiling microcontrollers with advanced pipeline stages. For the practical realisation of our approach, we employed the open-source ABBY-CM0 dataset from the ARM Cortex-M0 microcontroller, which has three pipeline stages. To provide a detailed analysis, we also consider a Convolutional Neural Network (CNN) besides two RNN architectures (Long Short-Term Memory and Gated Recurrent Unit). 11:30 CET TS22.7 A DEEP-LEARNING TECHNIQUE TO LOCATE CRYPTOGRAPHIC OPERATIONS IN SIDE-CHANNEL TRACES\n\nSpeaker:\n\nDavide Galli, Politecnico di Milano, IT\n\nAuthors:\n\nGiuseppe Chiari, Davide Galli, Francesco Lattari, Matteo Matteucci and Davide Zoni, Politecnico di Milano, IT\n\nAbstract\n\nSide-channel attacks allow extracting secret information from the execution of cryptographic primitives by correlating the partially known computed data and the measured side-channel signal. However, to set up a successful side-channel attack, the attacker has to perform i) the challenging task of locating the time instant in which the target cryptographic primitive is executed inside a side-channel trace and then ii) the time-alignment of the measured data on that time instant. This paper presents a novel deep-learning technique to locate the time instant in which the target computed cryptographic operations are executed in the side-channel trace. In contrast to state-of-the-art solutions, the proposed methodology works even in the presence of trace deformations obtained through random delay insertion techniques. We validated our proposal through a successful attack against a variety of unprotected and protected cryptographic primitives that have been executed on an FPGA-implemented system-on-chip featuring a RISC-V CPU. 11:35 CET TS22.8 IMCE: AN IN-MEMORY COMPUTING AND ENCRYPTING HARDWARE ARCHITECTURE FOR ROBUST EDGE SECURITY\n\nSpeaker:\n\nHanyong Shao, Peking University, CN\n\nAuthors:\n\nHanyong Shao, Boyi Fu, Jinghao Yang, Wenpu Luo, Chang Su, Zhiyuan Fu, Kechao Tang and Ru Huang, Peking University, CN\n\nAbstract\n\nEdge devices deployed in unsupervised scenarios employ Physical Unclonable Functions (PUFs) for identity authentication and embedded XOR encoding for data encryption. However, on the one hand, the existing strong PUFs such as CMOS-based XOR Arbiter PUFs and NVM-based RRAM PUFs are vulnerable to various machine learning (ML) modeling attacks. On the other hand, the transmission of keys for embedded XOR encoding also faces the risk of being eavesdropped in unsecured channels. In response to these challenges, this paper proposes a high-security In-Memory Computing and Encrypting (IMCE) hardware architecture based on a FeFET macro, featuring both a PUF mode for identity authentication and an encrypted CIM mode with in-situ decryption. The PUF mode ensures a prediction accuracy close to 50% (equivalent to random guessing attack) under various ML models due to the proposed Hamming distance comparison used in challenge-response pairs (CRPs) generation. In addition, by utilizing the CRPs generated in PUF mode as encryption keys, the CIM mode of IMCE achieves robust security through public-key cryptography via CRPs-masked key transfer, preventing the leakage of keys and data. Therefore, by applying a novel CRPs generation scheme and reusing the generated CRPs for in-situ CIM decryption, the security of both PUF and encrypted CIM mode is enhanced concurrently. In addition, IMCE significantly reduces the power overhead thanks to the high energy efficiency of ferroelectric FETs (FeFETs), making it highly suitable for secure applications in edge computing devices. 11:40 CET TS22.9 DEMONSTRATING POST-QUANTUM REMOTE ATTESTATION FOR RISC-V DEVICES\n\nSpeaker:\n\nMaximilian Barger, Vrije Universiteit Amsterdam and University of Amsterdam, NL\n\nAuthors:\n\nMaximilian Barger1, Marco Brohet2 and Francesco Regazzoni3\n\n1Vrije Universiteit Amsterdam and University of Amsterdam, NL; 2University of Amsterdam, NL; 3University of Amsterdam and Università della Svizzera italiana, CH\n\nAbstract\n\nThe rapid proliferation of Internet of Things (IoT) devices has revolutionized many aspects of modern computing. Experience has shown that these devices often have severe security problems and are common targets for malware. One approach to ensure that only trusted software is executed on these devices is Remote Attestation (RA), which allows a verifier to attest the integrity of software running on such a prover device. As malware is typically not trusted, an infected device will fail to generate a valid signature, which allows the verifier to detect the presence of malware on the prover. To achieve its security guarantees, RA requires a trust anchor, often found in the form of dedicated hardware on the prover. For IoT and embedded devices such hardware has only recently become largely deployed. Current RA protocols rely on classical asymmetric signatures that are vulnerable to quantum attacks, which are expected to become feasible in the near future. In this work we present SPRAV, a software-based RA system that leverages the Physical Memory Protection (PMP) primitive of RISC-V to achieve its security guarantees and employs quantum-safe cryptographic algorithms to ensure resistance against quantum attacks in the future. Our evaluation shows that it is feasible to deploy this solution on RISC-V devices without incurring a prohibitive overhead or the need for additional hardware, paving the way towards quantum-resistant functionalities also in IoT. 11:41 CET TS22.10 CIRCUMVENTING RESTRICTIONS IN COMMERCIAL HIGH-LEVEL SYNTHESIS TOOLS\n\nSpeaker:\n\nBenjamin Carrion Schaefer, University of Texas at Dallas, US\n\nAuthors:\n\nBenjamin Carrion Schaefer and Chaitali Sathe, University of Texas at Dallas, US\n\nAbstract\n\nMany Software (SW) vendors limit the functionality of their product based on the version purchased. This trend has also carried over to Electronic Design Automation (EDA). For example, Field-Programmable Gate Array (FPGA) vendors make their Lite versions freely available to anyone, but charge for their full versions, e.g., Intel Quartus Prime Lite vs. Quartus Prime. Some High-Level Synthesis (HLS) tool vendors have started to do the same in order to appeal more to FPGA users who are more price conscious as opposed to the ASIC users. FPGA tools are typically free or very inexpensive and hence, it makes sense to have dedicated FPGA versions of their HLS tools. To enable this strategy some HLS vendors have put in place different control mechanisms to avoid anyone using their inexpensive FPGA version to target ASICs, as this would defeat their price discrimination strategy. In this work, we review different strategies used by the HLS vendors and propose to the best of our knowledge the first technique to circumvent these. In particular we show how we can generate ASIC circuits with similar area and performance using the HLS Lite HLS version that only allows to target small FPGAs as compared to using the full ASIC HLS version. 11:42 CET TS22.11 INTERACTIVE TECHNICAL PRESENTATIONS BY THE AUTHORS\n\nPresenter:\n\nSession Chairs, DATE, ES\n\nAuthor:\n\nSession Chairs, DATE, ES\n\nAbstract\n\nParticipants can freely interact with authors during their interactive technical presentations.\n\nLK01 IEEE Ceda Distinguished Lecturer Lunchtime Keynote\n\nAdd this session to my calendar\n\nDate: Monday, 25 March 2024\n\nTime: 13:15 CET - 14:00 CET\n\nLocation / Room: Auditorium 2\n\nSession chair:\n\nIan O’Connor, Ecole Centrale de Lyon, FR\n\nSession co-chair:\n\nLuis Miguel Silveira, TU Lisbon, PT\n\nSupported by\n\nTime Label Presentation Title\n\nAuthors 13:15 CET LK01.1 AI MODELS FOR EDGE COMPUTING: HARDWARE-AWARE OPTIMIZATIONS FOR EFFICIENCY\n\nSpeaker and Author:\n\nHai (Helen) Li, Duke University, US\n\nAbstract\n\nAs artificial intelligence (AI) transforms various industries, state-of-the-art models have exploded in size and capability. The growth in AI model complexity is rapidly outstripping hardware evolution, making the deployment of these models on edge devices remain challenging. To enable advanced AI locally, models must be optimized for fitting into the hardware constraints. In this presentation, we will first discuss how computing hardware designs impact the effectiveness of commonly used AI model optimizations for efficiency, including techniques like quantization and pruning. Additionally, we will present several methods, such as hardware-aware quantization and structured pruning, to demonstrate the significance of software/hardware co-design. We will also demonstrate how these methods can be understood via a straightforward theoretical framework, facilitating their seamless integration in practical applications and their straightforward extension to distributed edge computing. At the conclusion of our presentation, we will share our insights and vision for achieving efficient and robust AI at the edge.\n\nASD02 ASD Technical Paper Session: Towards Assuring Safe Autonomous Driving\n\nAdd this session to my calendar\n\nDate: Monday, 25 March 2024\n\nTime: 14:00 CET - 15:30 CET\n\nLocation / Room: Break-Out Room S8\n\nTime Label Presentation Title\n\nAuthors 14:00 CET ASD02.1 BACK TO THE FUTURE: REVERSIBLE RUNTIME NEURAL NETWORK PRUNING FOR SAFE AUTONOMOUS SYSTEMS\n\nSpeaker:\n\nDanny Abraham, University of California, Irvine, US\n\nAuthors:\n\nDanny Abraham1, Biswadip Maity1, Bryan Donyanavard2 and Nikil Dutt1\n\n1University of California, Irvine, US; 2San Diego State University, US\n\nAbstract\n\nNeural network pruning has emerged as a technique to reduce the size of networks at the cost of accuracy to enable deployment in resource-constrained systems. However, low-accuracy pruned models may compromise the safety of realtime autonomous systems when encountering unpredictable scenarios, e.g., due to anomalous or emergent behavior. We propose Back to the Future: a novel approach that combines pruning with dynamic routing to achieve both latency gains and dynamic reconfiguration to meet desired accuracy at runtime. Our approach enables the pruned model to quickly revert to the full model when unsafe behavior is detected, enhancing safety and reliability. Experimental results demonstrate that our swapping approach is 32x faster than loading the original model from disk, providing seamless reversion to the accurate version of the model, demonstrating its applicability for safe autonomous systems design. 14:30 CET ASD02.2 AUTOMATED TRAFFIC SCENARIO DESCRIPTION EXTRACTION USING VIDEO TRANSFORMERS\n\nSpeaker:\n\nAron Harder, University of Virginia, US\n\nAuthors:\n\nAron Harder and Madhur Behl, University of Virginia, US\n\nAbstract\n\nScenario Description Languages (SDLs) serve as high-level encodings, offering an interpretable representation of traffic situations encountered by autonomous vehicles (AVs). Their utility extends to critical safety analyses, such as identifying analogous traffic scenarios within vast AV datasets, and aiding in real-to-simulation transfers. This paper addresses the challenging task of autonomously deriving SDL embeddings from AV data. We introduce the Scenario2Vector method, leveraging video transformers to automatically detect spatio-temporal actions of the ego AV through front-camera video footage. Our methodology draws upon the Berkeley Deep Drive - eXplanations (BDD-X) dataset. To determine ground truth actions of the ego AV, we employ BERT combined and dependency grammar-based trees, utilizing the resulting labels for Scenario2Vector training. Our approach is benchmarked against a 3D convolution (C3D)-based method and a transfer-learned video transformer (ViViT) model, evaluating both extraction accuracy and scenario retrieval capabilities. The results reveal that Scenario2Vector is highly effective in detecting ego vehicle actions from video input, adeptly handling traffic scenarios with multiple ego vehicle maneuvers. 15:00 CET ASD02.3 ADASSURE: DEBUGGING METHODOLOGY FOR AUTONOMOUS DRIVING CONTROL ALGORITHMS\n\nSpeaker:\n\nAndrew Roberts, FinEst Center for Smart Cities, Tallinn University of Technology, EE\n\nAuthors:\n\nAndrew Roberts1, Mohammad Reza Heidari Iman1, Mauro Bellone2, Tara Ghasempouri3, Olaf Maennel4, Jaan Raik1, Mohammad Hamad5 and Sebastian Steinhorst5\n\n1Tallinn University of Technology, EE; 2FinEst Centre for Smart Cities, EE; 3Department of Computer System, Tallinn University of Technology, Estonia, EE; 4University of Adelaide, AU; 5TU Munich, DE\n\nAbstract\n\nAutonomous driving (AD) system designers need methods to efficiently debug vulnerabilities found in control algorithms. Existing methods lack alignment to the requirements of AD control designers to provide an analysis of the parameters of the AD system and how they are affected by cyber-attacks. We introduce ADAssure, a methodology for debugging AD control system algorithms that incorporates automated mechanisms which support generation of assertions to guide the AD system designer to identify vulnerabilities in the system. Our evaluation of ADAssure, on a real-world AD vehicular system, using diverse cyber-attacks, developed a set of assertions that identified weaknesses in the OpenPlanner 2.5 AD planning algorithm and its constituent planning functions. Working with an AD control system designer and safety validation engineer, the results of ADAssure identified remediation of the AD control system, which can support the implementation of a redundant observer for data integrity checking and improvements to the planning algorithm. The adoption of ADAssure improves autonomous system design by providing a systematic approach to enhance safety and reliability through the identification and mitigation of vulnerabilities from corner cases.\n\nBPA03 BPA - New Circuits And Devices\n\nAdd this session to my calendar\n\nDate: Monday, 25 March 2024\n\nTime: 14:00 CET - 15:30 CET\n\nLocation / Room: Break-Out Room S1+2\n\nSession chair:\n\nAida Todri Sanial, TUE, NL\n\nSession co-chair:\n\nRajendra Bishnoi, TU Delft, NL\n\nTime Label Presentation Title\n\nAuthors 14:00 CET BPA03.1 DYNAMIC REALIZATION OF MULTIPLE CONTROL TOFFOLI GATE\n\nSpeaker:\n\nAbhoy Kole, DFKI, DE\n\nAuthors:\n\nAbhoy Kole1, Arighna Deb2, Kamalika Datta3 and Rolf Drechsler4\n\n1DFKI, DE; 2Kalinga Institute of Industrial Technology, IN; 3University of Bremen, DE; 4University of Bremen | DFKI, DE\n\nAbstract\n\nDynamic Quantum Circuits (DQC) is an inevitable solution for today's Noisy Intermediate Scale Quantum (NISQ) systems. This enables realization of an n-qubit (where, n > 2) quantum circuit using only 2-qubits with the aid of additional non-unitary operations which is evident from the recent dynamic realizations of algorithms like Quantum Phase Estimation (QPE) and Bernstein–Vazirani (BV) as well as 3-qubit Toffoli operation. In this work, we introduce two different dynamic realization schemes for Multiple Control Toffoli (MCT) gates, for the first time to the best of our knowledge. We compare the respective realizations in terms of resources (e.g., gate, depth and nearest neighbor overhead) and computational accuracy. For this purpose, we apply the proposed dynamic MCT gates in Deutsch-Jozsa (DJ) algorithm, thereby realizing the traditional DJ algorithm as DQCs. Experimental evaluations show that one dynamic scheme for MCT gates leads to DQCs with better computational accuracy, while the other one results in DQCs with better computational resources. 14:20 CET BPA03.2 A FEFET-BASED TIME-DOMAIN ASSOCIATIVE MEMORY FOR MULTI-BIT SIMILARITY COMPUTATION\n\nSpeaker:\n\nXunzhao Yin, Zhejiang University, CN\n\nAuthors:\n\nQingrong Huang1, Hamza Errahmouni Barkam2, Zeyu Yang1, Jianyi Yang1, Thomas K ̈ampfe3, Kai Ni4, Grace Li Zhang5, Bing Li6, Ulf Schlichtmann6, Mohsen Imani2, Cheng Zhuo1 and Xunzhao Yin1\n\n1Zhejiang University, CN; 2University of California, Irvine, US; 3Fraunhofer IPMS, DE; 4University of Notre Dame, US; 5TU Darmstadt, DE; 6TU Munich, DE\n\nAbstract\n\nThe exponential growth of data across various domains of human society necessitates the rapid and efficient data processing. In many contemporary data-intensive applications, similarity computation (SC) is one of the most fundamental and indispensable operations. In recent years, In-memory computing (IMC) architectures have been designed to accelerate SC by reducing data movement costs, however, they encounter challenges with signal domain conversion, variation sensitivity, and limited precision. This paper proposes a ferroelectric FET (FeFET) based time-domain (TD) associative memory (AM) for energy efficient SC. Such TD design can convert its output (i.e., time interval) to digits with relatively simple sensing circuitry thus saves large amount of area and energy compared with conventional IMC designs that process analog voltage/current signals. The variable-capacitance (VC) delay chain structure in our design supports quantitative SC and enhances robustness against variations. Furthermore, by exploiting multi-domain ferroelctric FET (FeFET), our design is capable of performing SC on vectors with multi-bit element, enabling support for higher-precision algorithms. Simulation results show that the proposed TD-AM achieves 13.8×/1.47× energy saving of our design compared to CMOS/NVM based TD-IMC designs. Additionally, our design exhibits good robustness in monte carlo simulation with variation extracted from experimental measurements. Investigation on precision of hyperdimensional computing (HDC) show that higher element precision reduces the size of HDC model when considering to achieve same accuracy, indicating an improved efficiency. Benchmarkings against GPU demonstrate in general 2/3 orders of magnitude speedup/energy efficiency improvement of our design. Our proposed multi-bit TD-AM promises energy-efficient quantitative SC for diverse intensive data processing application, especially in energy-constrained scenarios. 14:40 CET BPA03.3 RVCE-FAL: A RISC-V VECTOR-SCALAR CUSTOM EXTENSION FOR FASTER FALCON DIGITAL SIGNATURE\n\nSpeaker:\n\nXinglong Yu, State Key Laboratory of Integrated Chips and Systems, Fudan University, Shanghai, CN\n\nAuthors:\n\nXinglong Yu, Yi Sun, Yifan Zhao, Honglin Kuang and Jun Han, Fudan University, CN\n\nAbstract\n\nThe National Institute of Standards and Technology (NIST) has selected FALCON as one of the standardized digital signature algorithms against quantum attacks in 2022. Compared with the other post-quantum cryptography (PQC) schemes, lattice-based FALCON is more appropriate for future Internet of Things (IoT) applications due to the fastest signature verification process and the lowest transmission overhead. In this paper, we propose a custom extension based on the RISC-V scalar-vector framework for efficient implementation of FALCON. To our best knowledge, this work is the first hardware-software co-design for complete FALCON signature generation and verification routines. Besides, we design the first FALCON Gaussian sampling hardware and a RISC-V vector extension (RVV) based domain-specific core. The proposed architecture accelerates kernel operations in FALCON, such as discrete Gaussian sampling, number theoretic transform (NTT), inverse NTT, and polynomial operations. Compared with the reference implementation, results on the gem5-RTL simulation platform present a speedup for signature generation and verification of up to 18× and 6.9×. 15:00 CET BPA03.4 INTERACTIVE TECHNICAL PRESENTATIONS BY THE AUTHORS\n\nPresenter:\n\nSession Chairs, DATE, ES\n\nAuthor:\n\nSession Chairs, DATE, ES\n\nAbstract\n\nParticipants can freely interact with authors during their interactive technical presentations.\n\nCF01.1 Careers Fair - Industry Company Presentations\n\nAdd this session to my calendar\n\nDate: Monday, 25 March 2024\n\nTime: 14:00 CET - 14:45 CET\n\nLocation / Room: Auditorium 3\n\nSession chair:\n\nMarina Saryan, Synopsys, AM\n\nThis is a Young People Programme event. During the Company Presentation Session, presenters will introduce their companies, ongoing activities, and work culture. Presenting companies include Cadence Design Systems, Synopsys, Racyics, Semidynamics, Arm, ams OSRAM and Graf Research\n\nTime Label Presentation Title\n\nAuthors 14:05 CET CF01.1.1 INTRODUCING SEMIDYNAMICS\n\nPresenter:\n\nPedro Marcuello, Semidynamics, ES\n\nAuthor:\n\nPedro Marcuello, Semidynamics, ES\n\nAbstract\n\nIntroducing Semidynamics 14:11 CET CF01.1.2 INTRODUCING SYNOPSYS\n\nPresenter:\n\nAsya Mkhitaryan, Synopsys, AM\n\nAuthor:\n\nAsya Mkhitaryan, Synopsys, AM\n\nAbstract\n\nIntroducing Synopsys 14:17 CET CF01.1.3 INTRODUCING ARM\n\nPresenter:\n\nNick Brill, Arm, GB\n\nAuthor:\n\nNick Brill, Arm, GB\n\nAbstract\n\nIntroducing ARM 14:23 CET CF01.1.4 INTRODUCING RACYICS\n\nPresenter:\n\nFlorian Bilstein, RacyICs, DE\n\nAuthor:\n\nFlorian Bilstein, RacyICs, DE\n\nAbstract\n\nIntroducing RACYICS 14:29 CET CF01.1.5 INTRODUCING AMS OSRAM\n\nPresenter:\n\nRafael Serrano-Gotarredona, ams OSRAM, ES\n\nAuthor:\n\nRafael Serrano-Gotarredona, ams OSRAM, ES\n\nAbstract\n\nIntroducing AMS OSRAM 14:34 CET CF01.1.6 INTRODUCING GRAF RESEARCH\n\nPresenter:\n\nJonathan Graf, Graf Research, US\n\nAuthor:\n\nJonathan Graf, Graf Research, US\n\nAbstract\n\nIntroducing GRAF RESEARCH 14:39 CET CF01.1.7 INTRODUCING CADENCE DESIGN SYSTEMS\n\nPresenter:\n\nLizzy Kiely, Cadence Design Systems, US\n\nAuthor:\n\nLizzy Kiely, Cadence Design Systems, US\n\nAbstract\n\nIntroducing CADENCE DESIGN SYSTEMS\n\nES Executive Session\n\nAdd this session to my calendar\n\nDate: Monday, 25 March 2024\n\nTime: 14:00 CET - 15:30 CET\n\nLocation / Room: Break-Out Room S5\n\nTime Label Presentation Title\n\nAuthors 14:00 CET ES.1 MAKECHIP - AN ACCESSIBLE, COST-EFFECTIVE, AND CLOUD-BASED CHIP DESIGN PLATFORM FOR STARTUPS AND ACADEMIA\n\nPresenter:\n\nPatrick Doell, Racyics GmbH, DE\n\nAuthor:\n\nPatrick Doell, Racyics GmbH, DE\n\nAbstract\n\n\"The Presentation is about makeChip, an cloud-based Design Platform developed by Racyics. The platform targets start-ups, SMEs, research institutes and universities and is a central gateway to design advanced integrated circuits. Young companies and research groups doesn't have to invest upfront in costly IT infrastructure and have direct access to EDA tools, IPs, PDKs and silicon-proven design flows. The platform provides reliable IT infrastructure with a full set of EDA tool installations and technology data setup, i.e. PDKs, foundation IP, complex IP. All tools and design data are linked by Racyics' silicon proven design flow and project management system. The turnkey environment enables any makeChip customer to realize complex System on Chips in the most advanced technology nodes. Racyics supports makeChip customers by on-demand design services, such as digital layout generation, tape-out sign-off execution and many more. After giving an introduction about the concept and structure of makeChip, we outline the unique benefits for academia, start-ups, and SMEs. Afterwards, we go into detail about the technical aspects of the infrastructure itself: Resource and Data Management, Tool Wrapper, etc. Furthermore, a sucessfull research project which was realized on makeChip is presented. At the end, additional tools such as an Rapid Adoption Kit for rapid chipd devolpment is shown. \" 14:45 CET ES.2 AUTOMATING SECURITY VERIFICATION ON FIRMWARE BINARIES: THE FAULT INJECTION EXAMPLE\n\nPresenter:\n\nLionel Rivière, eShard, FR\n\nAuthors:\n\nLionel Rivière and Guillaume Vilcocq, eShard, FR\n\nAbstract\n\nIn the context of the vulnerability analysis, automating the validation of the target resistance against physical attacks is a must. Checking the efficiency of side-channel and fault injection countermeasures implemented by the designer requires systematic analysis of the attack paths built on the leakage and fault model of the target. With this intent, dynamic binary analysis (DBA) based on emulation can efficiently support automation thanks to a validation strategy both top-down and parallel. In this talk, we will focus on emulating fault injection on a secure bootloader.\n\nFS10 Focus session: Chip Design Enablement: How to lower barriers to hardware design with high level languages, generative AI, and Cloud\n\nAdd this session to my calendar\n\nDate: Monday, 25 March 2024\n\nTime: 14:00 CET - 15:30 CET\n\nLocation / Room: Auditorium 2\n\nSession chair:\n\nOlivier Sentieys, INRIA, FR\n\nOrganisers:\n\nCatherine Le Lan, Synopsys, FR\n\nOlivier Sentieys, INRIA, FR\n\nTime Label Presentation Title\n\nAuthors 14:00 CET FS10.1 WRITING SOFTWARE TO ELABORATE HARDWARE (SPINALHDL)\n\nPresenter:\n\nCharles Papon, Independent, FR\n\nAuthor:\n\nCharles Papon, Independent, FR\n\nAbstract\n\nThis talk will provide an overview about how software approaches can be used to capture hardware specifications and generate their corresponding Verilog/VHDL \"netlist ». The main motivation for such approach being to go beyond what the System-Verilog and VHDL tooling support in terms of hardware elaboration, increasing developer productivity, being able to build hardware generation abstractions while keeping full control over the generated hardware. Another motivation is also to be able to use an extendable tool, meaning, not having to restrict yourself to baked-in features, but instead being able to extend the scope of the tool, avoiding having to switch between different languages, and so avoiding a flow fracture or mismatch. This talk will be based around the recents developments made on VexiiRiscv (a RISC-V multi-issue processor softcore), including its hardware pipelining API and scheduler generation which are based on Scala (a general purpose programming language) and SpinalHDL (a Scala library providing a hardware description API). 14:30 CET FS10.2 VERIHDL: ENFORCING CORRECTNESS IN LLM-GENERATED HDL\n\nPresenter:\n\nValerio Tenace, PrimisAI, US\n\nAuthors:\n\nValerio Tenace1 and Pierre-Emmanuel Gaillardon2\n\n1PrimisAI, US; 2University of Utah, US\n\nAbstract\n\nOver the past few months, the rapid proliferation of Large Language Models (LLMs) for Hardware Description Language (HDL) generation has attracted considerable attention. These technologies, by automating complex design tasks, aim to streamline workflows and hasten the innovation process. However, the intrinsic probabilistic nature of LLMs introduces a degree of uncertainty, often manifesting as errors or hallucinations in the generated HDL code—a matter of great concern in the realm of hardware design where accuracy is non-negotiable. To tackle this issue, we present VeriHDL, a novel LLM-based framework tailored to bolster the accuracy and dependability of HDL code produced by generative artificial intelligence. Central to VeriHDL is a harmonious blend of sophisticated and interconnected LLMs that enable: (1) a systematic HDL generation process that leverages a dedicated knowledge base of pre-defined and verified IPs as an initial safeguard against most common mistakes, (2) an automated and iterative testbench generation mechanism, to enforce functional correctness, and (3) a seamless interface with Electronic Design Automation tools, as to enable an automatic verification process with no-human-in-the-loop. This innovative and original approach, currently in its early beta stage within PrimisAI's RapidGPT platform, is not only meant to improve the accuracy of generated code, but also to significantly reduce the time and resources required for verification. As a result, VeriHDL represents a substantial paradigm shift in hardware design, facilitating a more efficient, reliable, and streamlined development process. In this presentation, we will delve into the foundational principles of VeriHDL, starting with an in-depth analysis of its architecture. We will then explore the dynamic interplay among its components, illustrating how they collectively contribute to the framework's effectiveness. The session will culminate in a demonstration, showcasing VeriHDL's practical application and its impact on streamlining the hardware design process. 15:00 CET FS10.3 CUMULUS: A CLOUD-BASED EDA PLATFORM FOR TEACHING AND RESEARCH\n\nPresenter:\n\nPhillip Christie, Trinity College Dublin, IE\n\nAuthor:\n\nPhillip Christie, Trinity College Dublin, IE\n\nAbstract\n\nIn this presentation, I will relate our early experiences in the development and deployment of a cloud-based Electronic Design Automation (EDA) platform in Microsoft Azure, to be used as the basis of a set of laboratories associated with a Masters' degree level module in microelectronics at Trinity College Dublin. The intent of the CUMULUS platform is not to implement a commercial design flow but rather to permit students to see how choices made at one stage affect performance at later stages. The flow from device model, library characterization, synthesis, place and route, to timing analysis creates a sequence of data files in standardized formats which, while being mostly text-based, are not designed for human assessment. A key design concept of the CUMULUS platform has therefore been to use a suite of specially created MATLAB toolboxes to visualise and assess these data (in liberty, LEF, DEF, timing reports, GDSII formats) generated at each stage. This presentation will end by providing an overview of costs for the lab, show examples of the graphical representation of data files and make recommendations for future improvements to the CUMULUS platform.\n\nLKS02 Later … With The Keynote Speakers\n\nAdd this session to my calendar\n\nDate: Monday, 25 March 2024\n\nTime: 14:00 CET - 15:30 CET\n\nLocation / Room: VIP Room\n\nSpeaker: Hai (Helen) Li\n\nTS28 Emerging Machine Learning Techniques\n\nAdd this session to my calendar\n\nDate: Monday, 25 March 2024\n\nTime: 14:00 CET - 15:30 CET\n\nLocation / Room: Multi-Purpose Room M1A+C\n\nSession chair:\n\nMihai Lazarescu, Politecnico di Torino, IT\n\nSession co-chair:\n\nAlessio Burrello, Politecnico di Torino, IT\n\nTime Label Presentation Title\n\nAuthors 14:00 CET TS28.1 PIPETTE: AUTOMATIC FINE-GRAINED LARGE LANGUAGE MODEL TRAINING CONFIGURATOR FOR REAL-WORLD CLUSTERS\n\nSpeaker:\n\nJinkyu Yim, Seoul National University, KR\n\nAuthors:\n\nJinkyu Yim1, Jaeyong Song1, Yerim Choi2, Jaebeen Lee2, Jaewon Jung1, Hongsun Jang1 and Jinho Lee1\n\n1Seoul National University, KR; 2Samsung Electronics., KR\n\nAbstract\n\nTraining large language models (LLMs) is known to be challenging because of the huge computational and memory capacity requirements. To address these issues, it is common to use a cluster of GPUs with 3D parallelism, which splits a model along the data batch, pipeline stage, and intra-layer tensor dimensions. However, the use of 3D parallelism produces the additional challenge of finding the optimal number of ways on each dimension and mapping the split models onto the GPUs. Several previous studies have attempted to automatically find the optimal configuration, but many of these lacked several important aspects. For instance, the heterogeneous nature of the interconnect speeds is often ignored. While the peak bandwidths for the interconnects are usually made equal, the actual attained bandwidth varies per link in real-world clusters. Combined with the critical path modeling that does not properly consider the communication, they easily fall into sub-optimal configurations. In addition, they often fail to consider the memory requirement per GPU, often recommending solutions that could not be executed. To address these challenges, we propose Pipette, which is an automatic fine-grained LLM training configurator for real-world clusters. By devising better performance models along with the memory estimator and fine-grained individual GPU assignment, Pipette achieves faster configurations that satisfy the memory constraints. We evaluated Pipette on large clusters to show that it provides a significant speedup over the prior art. 14:05 CET TS28.2 A COMPUTATIONALLY EFFICIENT NEURAL VIDEO COMPRESSION ACCELERATOR BASED ON A SPARSE CNN-TRANSFORMER HYBRID NETWORK\n\nSpeaker:\n\nWendong Mao, National Sun Yat-Sen University, TW\n\nAuthors:\n\nSiyu Zhang1, Wendong Mao2, Huihong Shi1 and Zhongfeng Wang1\n\n1Nanjing University, CN; 2National Sun Yat-Sen University, TW\n\nAbstract\n\nVideo compression is widely used in digital television, surveillance systems, and virtual reality. Real-time video decoding is crucial in practical scenarios. Recently, neural video compression (NVC) combines traditional coding with deep learning, achieving impressive compression efficiency. Nevertheless, NVC models involve high computational costs and complex memory access patterns, challenging real-time hardware implementations. To relieve this burden, we propose an algorithm and hardware co-design framework named NVCA for video decoding on resource-limited devices. Firstly, a CNN-Transformer hybrid network is developed to improve compression performance by capturing multi-scale non-local features. In addition, we propose a fast algorithm-based sparse strategy that leverages the dual advantages of pruning and fast algorithms, sufficiently reducing computational complexity while maintaining video compression efficiency. Secondly, a configurable sparse computing core is designed to "
    }
}
{
    "id": "wrong_mix_random_nbateam_00096_1",
    "rank": 86,
    "data": {
        "url": "https://summit.ccs.uky.edu/ccs_summit2020/",
        "read_more_link": "",
        "language": "en",
        "title": "Summit 2020",
        "top_image": "https://summit.ccs.uky.edu/ccs_summit2020/img/icon.png",
        "meta_img": "https://summit.ccs.uky.edu/ccs_summit2020/img/icon.png",
        "images": [
            "https://summit.ccs.uky.edu/ccs_summit2020/img/center-for-computational-sciences_two-tone.svg",
            "https://summit.ccs.uky.edu/ccs_summit2020/img/icon.png",
            "https://summit.ccs.uky.edu/ccs_summit2020/img/center-for-computational-sciences_two-tone.svg",
            "https://summit.ccs.uky.edu/ccs_summit2020/img/speakers/lydia.jpg",
            "https://summit.ccs.uky.edu/ccs_summit2020/img/speakers/dmaughan.jpg",
            "https://summit.ccs.uky.edu/ccs_summit2020/img/speakers/yulia.jpg",
            "https://summit.ccs.uky.edu/ccs_summit2020/img/speakers/rebekah.png",
            "https://summit.ccs.uky.edu/ccs_summit2020/img/speakers/parul.jpg",
            "https://summit.ccs.uky.edu/ccs_summit2020/img/speakers/luke.jpg",
            "https://summit.ccs.uky.edu/ccs_summit2020/img/speakers/boyd.png",
            "https://summit.ccs.uky.edu/ccs_summit2020/img/speakers/bentz.jpg",
            "https://summit.ccs.uky.edu/ccs_summit2020/img/feller2.jpg",
            "https://summit.ccs.uky.edu/ccs_summit2020/img/speakers/lowell.jpg",
            "https://summit.ccs.uky.edu/ccs_summit2020/img/speakers/gizmo.jpg",
            "https://summit.ccs.uky.edu/ccs_summit2020/img/speakers/Arti.jpg",
            "https://summit.ccs.uky.edu/ccs_summit2020/img/speakers/lisa.png",
            "https://summit.ccs.uky.edu/ccs_summit2020/images/kycyberteam.PNG",
            "https://summit.ccs.uky.edu/ccs_summit2020/img/speakers/Luciano.jpg",
            "https://summit.ccs.uky.edu/ccs_summit2020/img/speakers/ismael.jpg",
            "https://summit.ccs.uky.edu/ccs_summit2020/img/zahra.jpg",
            "https://summit.ccs.uky.edu/ccs_summit2020/img/igor.jpg",
            "https://summit.ccs.uky.edu/ccs_summit2020/img/cs.png",
            "https://summit.ccs.uky.edu/ccs_summit2020/img/dell.png",
            "https://summit.ccs.uky.edu/ccs_summit2020/img/intel.jpg",
            "https://summit.ccs.uky.edu/ccs_summit2020/img/pier.png",
            "https://summit.ccs.uky.edu/ccs_summit2020/img/hpe.png",
            "https://summit.ccs.uky.edu/ccs_summit2020/img/coe.png",
            "https://summit.ccs.uky.edu/ccs_summit2020/img/google2.png",
            "https://summit.ccs.uky.edu/ccs_summit2020/img/redhat.png",
            "https://summit.ccs.uky.edu/ccs_summit2020/img/spectra.png",
            "https://summit.ccs.uky.edu/ccs_summit2020/img/nvidia.png",
            "https://summit.ccs.uky.edu/ccs_summit2020/img/coas.png",
            "https://summit.ccs.uky.edu/ccs_summit2020/img/amd.png",
            "https://summit.ccs.uky.edu/ccs_summit2020/img/cc_byOBS_logo.png",
            "https://summit.ccs.uky.edu/ccs_summit2020/img/Lenovo.png",
            "https://summit.ccs.uky.edu/ccs_summit2020/img/kgs.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "img/icon.png",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Student Poster\n\nAre you a graduate student (including masters and postdocs) in the Commonwealth of Kentucky (or in our region) doing research related to our Summit’s key topic areas: HPC, Modeling and Simulation, Data Science, Data Mining, Data and Textual Analytics, AI, Machine Learning, Deep Learning, Computational Science or Engineering or working in exciting new related technology areas? Would you like to present a poster at our Summit, expand your resume/vita and potentially win prizes (gift cards and registration for the National Supercomputing Conference in November)? If so, please register and submit a title and abstract for your poster on this web page.\n\nTitle – current/planned working title for poster\n\nAbstract – 1-2 page abstract that list your institution, research participants, describes the research addressed with a brief description of the project’s goals/objectives, design/methods, and findings (outcomes/impact).\n\nAlso please indicate which of the three main areas this would fall into: HPC/Modeling & Simulation; Data Science/Data Mining; or AI/Machine Learning/New Technologies.\n\nFollowing your submission, you will be contacted regarding any further needed details. Later, you will receive the poster selection results. If selected, you will be given further instructions and the judging criteria. Since this is a virtual conference your poster session will be virtual – you will host a zoom meeting session on Thursday October 15, 2020 for 1 hour in length. Various Summit attendees, including judges, will visit your session during that timeframe and interact with you.\n\nImportant Dates:\n\nDeadline for poster title/abstract submission: Sunday, October 4, 2020\n\nPoster Competition Selection Notifications: Wednesday, October 7, 2020\n\nSlide or Poster for webpage (required to set up link to session): Friday, October 9, 2020\n\nSummit Begins: Monday, October 12, 2020\n\nZoom Poster Session: Thursday, October 15, 2020/ 2:40-3:40pm (Eastern)\n\nAddress all questions to:\n\nTony Elam Office phone: (859) 257-2326.\n\nAssc. Dir, CCS Cell phone: (713) 859-9860\n\nUniversity of Kentucky\n\nEmail: (tony.elam@uky.edu)\n\nSally Ellingson\n\nTitle: A Machine Learning Approach to Computational Polypharmacology and Lessons Learned\n\nAbstract:\n\nIt is common knowledge that drugs have polypharmacological properties that can be explored for new insights in drug discovery. That is a given drug interacts with many different proteins and a given protein interacts with multiple drugs. The polypharmacological, promiscuous nature of pharmaceuticals can have both beneficial and detrimental consequences. This attribute can be exploited to improve drug efficacy and prevent drug resistance. In addition to the ability of chemical compounds to interact with an array of protein targets, many diseases have multiple genetic determinants, and individual genetic determinants may be involved in multiple diseases. Furthermore, protein function and expression are controlled by a regulatory network of other proteins. When targeted therapies work initially patients often develop resistance due to secondary mutations or compensation from other parts of the underlying biological network. This illustrates the potential benefits of establishing computational polypharmacology methods – discovering drugs that intentionally target multiple proteins for a beneficial therapeutic result. Many adverse drug reactions (ADRs) result from drugs interacting with non-therapeutic off-targets (unintended interactions). Animal studies during preclinical trial are not always a good indication of these adverse interactions in humans, and such adverse effects are generally not discovered until a drug has reached clinical trial or is already on the market. With the number of different proteins in humans and the genetic variations observable in the population, a full understanding of all possible interactions through experiments and clinical testing alone is not feasible, making computational investigations particularly useful and relevant.\n\nIn our digitalized, data-driven world, there is a wealth of knowledge available that is beyond the processing power of an individual researcher or even team of researchers. The abundance of available biomedical data combined with the massive computing power we have available today with leadership class supercomputers provides great opportunities to advance computational drug research. A tool that reliably predicts protein and drug binding would revolutionize the pharmaceutical industry. An accurate representation of polypharmacological networks would provide a wealth of knowledge and insights on drug repurposing, side-effect prediction, and drug efficacy. This would lead the way to personalized polypharmacological networks including individual’s genetic variations resulting in a breakthrough for precision medicine. However, there are still many obstacles to overcome when it comes to utilizing massive computational power and ensuring accuracy of our predictions.\n\nUsing machine learning to score potential drug candidates may offer an advantage over traditional imprecise scoring functions because the parameters and model structure can be learned from the data. However, models may lack interpretability, are often overfit to the data, and are not generalizable to drug targets and chemotypes not in the training data. Benchmark datasets are prone to artificial enrichment and analogue bias due to the overrepresentation of certain scaffolds in experimentally determined active sets. Datasets can be evaluated using spatial statistics to quantify the dataset topology and better understand potential biases. Dataset clumping comprises a combination of self-similarity of actives and separation from decoys in chemical space and is associated with overoptimistic virtual screening results. This talk explores data, methods, and potential data biases relevant to computational drug binding predictions.\n\nBio:\n\nDr. Sally Ellingson is a computational scientist working at the intersection of computational biology and high-performance computing. She has undergraduate degrees in computer science and mathematics from Florida Institute of Technology. She obtained her doctoral degree at the University of Tennessee and Oak Ridge National Laboratory under a fellowship funded by the National Science Foundation in computational biology. She is an Assistant Professor in the Division of Biomedical Informatics in the College of Medicine. Her primary appointment is with the Markey Cancer Center’s Cancer Research Informatics Shared Resource Facility. In this role, she facilitates high throughput genomics and big data processing for cancer research and precision medicine. Dr. Ellingson was awarded as a KL2 scholar by the University of Kentucky’s Center for Clinical and Translational Sciences and is a Research Affiliate at Lawrence Berkeley National Laboratory. She has taken University of Kentucky students with her to further develop her research at Lawrence Berkeley National Laboratory for two past summers. This summer she has expanded her national lab collaborations as a visiting faculty scholar at Lawrence Livermore National Lab.\n\nDr. Ellingson also engages in mentoring and outreach, especially for underrepresented groups in computational sciences. Since graduation she has been actively engaged in the organization of the student programs at Supercomputing, the largest conference in high-performance computing. She organized the student volunteer program last year and will organize the conference wide mentoring program this year, and keeps the diversity of program participants a priority.\n\nRebecca Hartman-Baker\n\nTitle: Cutting-Edge HPC Centers: Operating at the Speed of Innovation\n\nAbstract:\n\nOperating a leading high-performance computing (HPC) center is a non-trivial task, requiring a team of experts in fields beyond computing, including building construction, energy efficiency, automation, project management, and communications. In this presentation, I will discuss the challenges that HPC centers face, including building an energy-efficient, sustainable physical plant; procuring and commissioning large compute and storage resources; and supporting users with vastly different needs and requirements. Beginning within the context of the 46-year history of the National Energy Research Scientific Computing Center (NERSC), the primary computing facility for the Office of Science in the U.S. Department of Energy, and continuing with an eye toward the future, I will trace the evolution of all these challenges and conclude with the outlook for HPC centers of the future.\n\nBio:\n\nRebecca Hartman-Baker leads the User Engagement Group at the National Energy Research Scientific Computing Center at Lawrence Berkeley National Laboratory. She is a computational scientist with expertise in the development of scalable parallel algorithms. Her career has taken her to Oak Ridge National Laboratory, where she worked on the R&D100-award winning team developing MADNESS and as a scientific computing liaison in the Oak Ridge leadership computing facility; the Pawsey Supercomputing Centre in Australia, where she coached two teams to the Student Cluster Competition at the annual Supercomputing conference and led the decision-making process for determining the architecture of the petascale supercomputer; and NERSC, where she's responsible for NERSC’s engagement with the user community to increase user productivity via advocacy, support, training, and the provisioning of usable computing environments. Rebecca earned a PhD in Computer Science, with a certificate in Computational Science and Engineering, from the University of Illinois at Urbana-Champaign.\n\nArti Garg\n\nTitle: Making Sense of the AI Technology Landscape\n\nAbstract:\n\nYou’ve probably heard the hype. “AI is transforming the way we do research and the way we conduct business.” But, when you try to learn more about how to implement AI, you may find yourself in a bewildering landscape of technology choices. This talk will provide insight into the technologies that are driving progress in AI and how to evaluate which choices are right for you. I will draw upon real world examples to give you a framework for rationalizing technology decisions in your own AI projects.\n\nBio:\n\nArti Garg is Head of Advanced AI Solutions & Technologies in the AI Strategy & Solutions Team at Hewlett Packard Enterprise (HPE). She joined HPE through the acquisition of Cray supercomputers. Prior to Cray, Dr. Garg led the Data Science and Services Team within Datapipe’s platform automation group, which was focused on building cloud deployment and management capabilities for application developers. Previously she held industrial data science roles, including as Director of Data & Analytics for NRG's Innovation Department and as an early member of GE’s Industrial Data Science team. In the past, Dr. Garg worked for the White House Budget Office where she oversaw over $5 billion of R&D investments at the Department of Energy, including the Advanced Scientific Computing Research program. She holds a PhD in Physics from Harvard University and an MS in Aeronautical & Astronautical Engineering as well as bachelor's degrees in Physics and English from Stanford University.\n\nDouglas Maughan\n\nTitle:The Convergence Accelerator Program: An Update on Progress\n\nAbstract:\n\nThe National Science Foundation (NSF) launched the NSF Convergence Accelerator in 2019 as a new organizational structure to accelerate the transition of use-inspired convergence research into practice in areas of national importance. Convergence research is a critical mechanism for solving many vexing research problems, especially those stemming from complex social and/or scientific challenges. The guiding rationale of the NSF Convergence Accelerator is that to deliver progress on scientific and societal challenges it is necessary to take an approach at the highest level of interdisciplinarity and to involve multiple kinds of partners and stakeholders, including researchers and the ultimate users of research products.\n\nDr. Maughan will describe the progress to date, including details of the 2019 cohort of awardees working on the Harnessing the Data Revolution and Future of Work at the Human Technology Frontier Big Ideas, and results for those teams selected for Phase 2. Dr. Maughan will also talk about the recent award for the 2020 cohort working on Quantum Information Technology and AI-Driven Innovation via Data and Model Sharing. He will provide the plan and strategy for the Convergence Accelerator going forward. Most importantly he will discuss how industry and academic partners and attendees can get involved with NSF’s Convergence Accelerator.\n\nBio:\n\nDr. Douglas Maughan is the inaugural Office Head for the National Science Foundation (NSF) Convergence Accelerator. NSF has launched the program to accelerate use-inspired convergence research in areas of national importance, and initiate convergence team-building capacity around exploratory, potentially high-risk proposals. The teams are multidisciplinary and leverage partnerships resulting in deliverables that will benefit society within a fixed term. NSF intends to support fundamental research while encouraging rapid advances through partnerships that include multiple stakeholders (e.g., industry, academic, not-for-profit, government entities).\n\nDr. Maughan previously served as the Division Director of the Industry Partnerships (OIP) Division within the Office of Innovation and Collaboration (OIC) within the Science and Technology (S&T) Directorate. Dr. Maughan was responsible for leading the formation and sustainment of internal and external partnerships across R&D communities, enabling joint R&D and resulting in stronger connections with developer and user communities. Dr. Maughan was responsible for (1) the innovation mechanisms, such as SBIR, SVIP, Prize, and BAAs, (2) Post-R&D activities associated with technology transfer and commercialization, and (3) the Office of the SAFETY Act Implementation.\n\nDr. Maughan previously served as the Division Director of the Cyber Security Division within the Science and Technology (S&T) Directorate. Dr. Maughan was responsible for helping bring to market over 75 commercial and open-source information security products during the 15 years he served as the Cyber Security Division Director at DHS.\n\nPrior to his appointment at DHS, Dr. Maughan was a Program Manager at the Defense Advanced Research Projects Agency (DARPA). Prior to his appointment at DARPA, Dr. Maughan worked for the National Security Agency (NSA) as a senior computer scientist and led several research teams performing network security research.\n\nDr. Maughan received Bachelor’s Degrees in Computer Science and Applied Statistics from Utah State University, a Masters degree in Computer Science from Johns Hopkins University, and a PhD in Computer Science from the University of Maryland, Baltimore County (UMBC).\n\nSpeakers: Parul Singh (Software Engineer Red Hat) / Ismael Faro (Tech Lead Cloud, IBM Quantum) / Luciano Bello (Qiskit Dev Team, IBM Quantum)\n\nTitle: Quantum Computing with Qiskit on OpenShift\n\nAbstract:\n\nThe presentation demonstrates steps to enable quantum computing workflows on a Kubernetes cluster using the IBM quantum system as a backend. The introduction covers the evolution of computing and how quantum computers can help us solve some intractable problems. The next part of the presentation covers the Qiskit operator and the steps to create a development environment for implementing quantum circuits with few clicks using the Qiskit operator. Qiskit operator launches an integrated Jupyter Notebook with Qiskit (for implementing quantum algorithms as circuits) and all the necessary python packages for visualizing the results. This operator automates the installation and configuration of the development environment, ensures all package versions are compatible and all required dependencies are met. The final part of the presentation is an introduction to quantum computing using the open source Qiskit framework, starting with basic quantum theory and how it is applied and works in current quantum devices. Moving on to show the basic concepts about quantum circuits, that is the foundation of any quantum program, and finally, demonstrates writing your first program and running it in a real quantum device, all of that using the Qiskit operator capabilities.\n\nBio:\n\nMini-Bio: Parul Singh is a software engineer in the Office of the CTO at Red Hat. She is leading the quantum computing efforts at Red Hat.\n\nMini-Bio: Ismael Faro is the technical lead in IBM Research where he helped to create the first public Quantum Cloud platform, IBM quantum experience, and after that was one of the first contributors in the open-source quantum computing software development framework Qiskit. He has collaborated for a while with developer and entrepreneurs communities and open source projects.\n\nMini-Bio: Luciano Bello is a Python developer in the Qiskt team, at IBM Research. He is a major contributor in the open-source Qiskit core, especially in the transpiler component for quantum circuit optimization. In addition, he is a Debian Developer and an infosec enthusiast.\n\nLydia E. Kavraki\n\nTitle: A Journey with Robots and Molecules\n\nAbstract:\n\nThe past two decades have witnessed incredible advances towards the design of autonomous systems. Advances in robotics and AI have fueled this progress. This talk will focus on the role of motion planning in yielding solutions for an agent that is able to execute a variety of tasks in a variety of settings. The talk will also illustrate how the experience and insight gained from motion planning can be applied to computational structural biology and, in particular, to the design of new therapeutics.\n\nBio:\n\nLydia E. Kavraki is the Noah Harding Professor of Computer Science at Rice University. She also holds appointments at the Departments of Bioengineering, Electrical & Computer Engineering, and Mechanical Engineering at Rice. Kavraki obtained her B.A. from the University of Crete in Greece and her Ph.D. from Stanford University. She is the Director of the Ken Kennedy Institute at Rice University. Her research interests span robotics, AI, and biomedicine. In robotics and AI, she is interested in enabling robots to work with people and in support of people. Her work has mainly focused on the development of efficient motion planning algorithms. In biomedicine she develops computational methods and tools to model protein structure and function, understand biomolecular interactions and help in the design of new therapeutics. Kavraki is the recipient of the ACM Grace Murray Hopper Award, the ACM Athena Award, and the ACM-AAAI Allen Newell Award. She is a Fellow of ACM, IEEE, AAAS, AAAI, AIMBE, and a member of the National Academy of Medicine. A longer bio can be found at https://profiles.rice.edu/faculty/lydia-e-kavraki Her research work can be found in wwww.kavrakilab.org\n\nCody Bumgardner\n\nTitle: Agent-based Systems in Biomedical Informatics\n\nAbstract:\n\nWe are living in a time of rapid data growth from an ever increasing number and diversity of sources. In a recent Cisco report it is estimated that there will be 28 billion connected devices by 2022 generating 43 trillion gigabytes of electronic data annually. In the medical world, connected devices range from distributions of low-power sensors and personal fitness devices, to large data generators such as medical imaging and genomic sequencing devices. While the scale and diversity of computational and data exchange challenges has increased rapidly, data acquisition, processing techniques and, supporting infrastructures have not maintained pace. Data management in highly regulated areas such as clinical diagnostics and medical research is especially impacted through stringent and often antiquated governance requirements. It is common practice to transfer in an unaltered form raw data acquired from its source of generation to remote locations, such as a laptops, servers, and computational clusters for processing and storage. In this centralized (even clusters) computational paradigm the majority of applications are developed within the context of the platform, with operating assumptions such as direct binary-level access to data storage or computational accelerators. As a byproduct of what some refer to as data gravity, data and associated applications tend to stay within the system they are stored or generated. Given the increased diversity of computational needs, the availability of practically limitless resources in pubic clouds, and the need to manage computational pipelines from data generation, through processing, and resulting, new methods must be developed and employed. We argue that computational inference, filtering, aggregation, and other function must be able to take place at any point in the computational pipeline, including near sources of data generation. Such capabilities impact not only operational efficiencies, but are also relevant to the enforcement of data governance and the preservation of privacy.\n\nWe present the use of agent-based methodologies in support of three biomedical pipeline applications within the domains of genomic processing, medical image management, and clinical laboratory analysis. In this paradigm computational units represent nodes and data exchange is represented as edges, forming a graph or computational pipeline. We find it convenient to represent these distributed pipelines a graphs, allowing the use of a number of computational models for resource placement, scheduling, and policy enforcement. We show how various functions within a genomic processing pipeline can be acquired from sequencers, distributed to public clouds, processing routed across a diverse set of resources, and results reported in a protected environment. Likewise, we show how digital images can be acquired and de-identified at (protected) sources of generation, submitted to remote vendor services for AI interpretation, and results merged with clinical systems. Finally, we will show methods for capture, enrichment, and processing of data from a mass spectrometer devices, with results routing to laboratory information systems and raw data transferred and aggregated to a repository for use in machine learning.\n\nBio:\n\nDr. Bumgardner spent the first 20 years of his career IT formerly serving as Director of Enterprise Systems and Development, Chief Technology Architect, and Director of Research Computing. Currently, he serves as Assistant Professor and Chief of the Division of Pathology Informatics in the Department of Pathology and Laboratory Medicine at the University of Kentucky.\n\nIan Foster\n\nTitle: AI and Science\n\nAbstract:\n\nI will describe an emerging “AI for Science” initiative at Argonne National Laboratory to advance the concept of Artificial Intelligence (AI) aimed at addressing challenge problems in science. We aim in this initiative to: (1 identify those scientific problems where existing AI and machine learning methods can have an immediate impact (and organize teams and efforts to realize that impact); (2) identify areas of where new AI methods are needed to meet the unique needs of science research (frame the problems, develop test cases, and outline work needed to make progress); and (3) develop the means to automate scientific experiments, observations, and data generation to accelerate the overall scientific enterprise. Science offers plenty of hard problems to motivate and drive AI research, from complex multimodal data analysis to integration of symbolic and data intensive methods, to coupling large-scale simulation and machine learning to drive improved training to control and accelerate simulations. A major sub-theme is the idea of working toward the automation of scientific discovery through integration of machine learning (active learning and reinforcement learning) with simulation and automated high-throughput experimental laboratories. I will provide some examples of projects underway and lay out a set of long-term driver problems.\n\nBio:\n\nIan Foster is Senior Scientist and Distinguished Fellow, and also director of the Data Science and Learning Division, at Argonne National Laboratory, and the Arthur Holly Compton Distinguished Service Professor of Computer Science at the University of Chicago. Ian received a BSc (Hons I) degree from the University of Canterbury, New Zealand, and a PhD from Imperial College, United Kingdom, both in computer science. His research deals with distributed, parallel, and data-intensive computing technologies, and innovative applications of those technologies to scientific problems in such domains as materials science, climate change, and biomedicine. His Globus software is widely used in national and international cyberinfrastructures. Foster is a fellow of the American Association for the Advancement of Science, Association for Computing Machinery, and British Computer Society. His awards include the British Computer Society Lovelace Medal, IEEE Kanai award, and Charles Babbage Award.\n\nAllison Burkette\n\nTitle: From ‘historical data’ to ‘big data’: The Linguistic Atlas Project\n\nAbstract:\n\nThe American Linguistic Atlas Project (LAP) began in 1929, when Hans Kurath decided to follow the lead of European dialectologists in Germany and France and to create a Linguistic Atlas of American English. Now, the LAP is the largest single survey of regional and social differences in spoken American English. Regional surveys extend from New England to California; some field work was conducted as early as the 1930s, some continues today.\n\nThe two earliest surveys are the Linguistic Atlas of New England (LANE) and the Linguistic Atlas of the Middle and South Atlantic States (LAMSAS), for which data was collected by face-to-face interview and transcribed on the spot in fine phonetic notation. These interviews relied on a questionnaire of 104 pages with seven or eight items per page, designed to reveal regional and social differences in everyday vocabulary, grammar, and pronunciation. Over time, the LAP interviews have changed (the questionnaire has been adjusted and more recent interviews have been digitally recorded), but the goal of capturing the regional language use of American speakers has remained. At present, the LAP is comprised of data from over 5000 individual interviews, resulting in a massive amount of linguistic data, all of it connected to specific social and geographic information.\n\nThe LAP is currently processing and curating its historical data in an attempt to get it all into a (somewhat) uniform and usable format, thus creating ‘big data’ from historical sources. This talk will give an overview of where we are in that process and will address specifically the ‘big data’ issues of volume, velocity, and variety.\n\nIn addition to outlining the challenges and opportunities that the LAP affords, this talk will also address what conceiving of the LAP data as ‘big data’ has already revealed: that the recurring pattern of variation found in such a large amount of data suggests that language functions as a complex system. Characteristic of a complex system is the emergence of scalable, nonlinear patterns and, indeed, at every level of examination, one finds that nonlinear frequency profiles (A-curves) characterize the LAP data. A-curves emerge for entire datasets and also for every subgroup in the data; the order of elements in the profile is likely to differ between groups, and between any group and the overall dataset, and it is these differences in order that effectively create what we refer to as ‘dialects’.\n\nBio:\n\nAs a linguist who specializes in sociolinguistics, my research centers around language variation and change, a subject that I approach from both a large-scale historical perspective using data from the Linguistic Atlas Project and a smaller-scale perspective that focuses on identity-making within individual interactions. My books Language and Material Culture (2015) and Language and Classification (2018) reflect my ongoing interest in the relationship between variation in language and material culture, while the textbook that I co-authored, Exploring Linguistic Science (2018), advocates a view of language as a complex system, with an emphasis on interaction and emergence.\n\nAnthony Skjellum\n\nTitle: MPI-4 : Next-generation explicit parallel programming for Pre-Exascale/Exascale\n\nAbstract:\n\nIn this talk, I cover the current state of standardization of the MPI-4 version of the Message Passing Interface (and beyond). For pre-exascale and exascale systems, MPI is being \"upgraded.\" Several new areas are planned for standardization for MPI-4 (MPI-Next, circa 2020) and MPI-Next-next (circa 2023-24), in time for the nominal arrival of the first exascale computers. Areas of particular interest include Sessions technologies for managing large, complex and changing resources underlying the MPI \"job.\" Persistent collective communication is designed to add performance to applications with temporal locality in their use of collective operations. Bleeding-edge area including endpoints, finepoints (partitioned communication), and fault-tolerance models are discussed. A number of other \"proposals\" and \"concepts\" will be discussed as well. MPI+X models are also discussed, such as MPI+OpenMP. Sessions and fault-tolerant models offer the potential for significant new application areas for MPI in large-scale cloud applications as well. Improvements to MPI that enable and enhance algorithmic development such as for machine learning are also covered.\n\nBio:\n\nAnthony (Tony) Skjellum studied at Caltech (BS, MS, PhD). His PhD work emphasized portable, parallel software for large-scale dynamic simulation, with a specific emphasis on message-passing systems, parallel nonlinear and linear solvers, and massive parallelism. From 1990-93, he was a computer scientist at LLNL focusing on performance-portable message passing and portable parallel math libraries. From 1993-2003, he was on the faculty in Computer Science at Mississippi State University, where his group co-invented the MPICH implementation of the Message Passing Interface (MPI) together with colleagues at Argonne National Laboratory. From 2003-2013, he was professor and chair at the University of Alabama at Birmingham, Dept. of Computer and Information Sciences. In 2014, he joined Auburn University as Lead Cyber Scientist and led R&D in cyber and High-Performance Computing for over three years. In Summer 2017, he joined the University of Tennessee at Chattanooga as Professor of Computer Science, Chair of Excellence, and Director, SimCenter, where he continues work in HPC and Cybersecurity, with strong emphases on IoT and blockchain technologies. He is a senior member of ACM, IEEE, ASEE, and AIChE, and an Associate Member of the American Academy of Forensic Science (AAFS), Digital & Multimedia Sciences Division.\n\nPeter Kekenes-Huskey\n\nTitle: A computational strategy to identify engineered proteins for restoring dysregulated calcium signaling. Darin Vaughan and Peter Kekenes-Huskey*\n\nAbstract:\n\nThe rising usage of systems biology modeling to probe complex signaling pathways raises the potential for their application in computational drug design. Toward this end, we are developing a workflow that predicts the ability of a computationally-designed calcium handling protein to restore dysregulated calcium signaling in cardiac ventricular tissue. The workflow includes 1) a systems biological model of calcium signaling under disease-like conditions 2) sensitivity analysis to identify calcium-dependent pathways whose modulation could recover normal signaling and 3) machine learning to identify potential proteins and sequence mutations to controllably modulate a given calcium signaling pathway. Preliminary results will be discussed in this presentation.\n\nBio:\n\nPeter Kekenes-Huskey, Ph.D. (PI) is an assistant professor at the University of Kentucky. He graduated summa cum laude with a B.S. in Chemistry from the University of North Carolina Asheville (2001), which was followed by a Fulbright Fellowship to Germany, whereafter he obtained his Ph.D. at the California Institute of Technology (2009). At UNC Asheville, he developed algorithms for the ab initio prediction of halogenated ethane decomposition rates as a Barry Goldwater Scholar with Bert Holmes and George Heard (UNCA). His Ph.D. research with William A. Goddard, III (Caltech) focused on developing Monte Carlo tools and molecular dynamics simulations. Peter complemented his studies with applied mathematics and computer science coursework from algorithm design to parallel computing, in fulfillment of National Science Foundation and Department of Energy Computional Science Graduate Fellowships. Following his Ph.D. work, he worked as a Staff Scientist at Arete Associates from 2007 to 2010, where he developed near-real time image processing and signal detection algorithms for defense applications. Returning to his passion for the physical sciences, he pursued postdoctoral studies with Professors Andy McCammon (Chemistry) and Andrew McCulloch (Bioengineering) at the University of California San Diego under support of American Heart Association and National Institutes of Health fellowships, to understand cardiac calcium signaling using molecular dynamics, partial differential equations and systems biology models. At UK, he is expanding multi-physics descriptions of molecular-driven events with macro-scale phenomena, with myriad applications in biological and nanoscale systems.\n\nWilliam C. Haneberg\n\nTitle: Computational Frontiers in the Earth Sciences—A Kentucky Perspective\n\nAbstract:\n\nAdvances in sensor and computer technology over the past generation have transformed many aspects of the earth sciences. At the global level, some of the world’s fastest multi-petaflop supercomputers are used for geophysical data processing and reservoir simulation in the energy industry and climate simulations by academic and government scientists. Closer to home, the 2018 completion of a statewide digital elevation model (DEM) based on airborne LiDAR (a type of laser scanning) offers exceptionally detailed documentation of Kentucky’s topography and insights into the geological processes that gave rise to it. The statewide LiDAR DEM comprises approximately 46 billion centimeter-accurate bare earth heights interpolated on a uniform 5 ft (1.5 m) grid. The underlying unstructured LiDAR point cloud, which includes non-ground returns and ancillary data such as laser return intensity, from which the DEM was created is estimated to comprise about 460 billion records. Subsequent LiDAR data acquisition campaigns, which will allow for detailed estimation of topographic change as they increase both storage and processing demands, are in the planning stages. The Kentucky Geological Survey has established its Digital Earth Analysis Lab—the KGS DEAL—to support development of LiDAR solutions beneficial to the commonwealth. Ongoing and potential research includes coupling physics-based slope stability models with real-time precipitation data to create a statewide landslide and mudflow hazard warning system; developing deterministic and AI-based approaches to sinkhole mapping in an effort to reduce costly damage associated with sudden ground collapse; and using convolutional neural networks to locate improperly abandoned oil and gas wells that may pose environmental and health risks. KGS also operates the Kentucky Seismic and Strong Motion Network, which streams nearly 2 Gb per day from 28 seismic stations across the commonwealth and is capable of detecting events ranging from imperceptible microtremors to major New Madrid seismic zone earthquakes (as well as noise from sources such as mine blasts, passing trains, and electronic interference). KGS is currently developing AI approaches to support automated event detection, particularly of small earthquakes that would be impossible to identify manually, as well as exploring options for stochastic simulations of earthquake effects that take into account the Kentucky’s complicated bedrock and surficial geology.\n\nBio:\n\nBill Haneberg is the state geologist and director of the Kentucky Geological Survey and a research professor in the UK Department of Earth & Environmental Sciences. His expertise includes digital terrain modeling using high resolution topographic and bathymetric data, computational geology, GIS applications, and geohazard and risk assessment. Before moving to Kentucky in 2016, he spent 17 years as a consulting geologist, first as a sole practitioner based mainly in the Seattle area and then with Fugro in Houston, where he led a team of geologists, geophysicists, and engineers working on quantitative geohazard and risk assessments for deep-water oil and gas projects around the world. He previously spent 11 years at New Mexico Tech, where he was a research geologist and assistant director of the New Mexico Bureau of Mines & Mineral Resources. The author or co-author of 160+ published papers and abstracts, co-editor of 3 monographs, and author of the AEG Claire P. Holdredge award winning book Computational Geosciences with Mathematica, Bill was also the 2011 AEG-GSA Richard H. Jahns Distinguished Lecturer in Engineering Geology. He earned a B.S. in geology from Bowling Green State University and his M.S. and Ph.D. in geology from the University of Cincinnati.\n\nIgor Alvarado\n\nTitle: Real-Time High-Performance Computing (RT-HPC) for Simulation and Control of Cyber-Physical Systems\n\nAbstract:\n\nTraditionally, High-Performance Computing (HPC) systems have been designed to perform offline simulations under “flexible” time constraints and with no data streaming from sensors or to actuators. In this work, we describe a new generation of Real-Time HPC (RT-HPC) systems that are capable of performing simulations in real-time within tight (strict) time constraints, in a deterministic manner and with extremely low jitter and latency. This type of systems can directly collect data from sensors, perform signal processing, classification/labeling and control tasks, and generate output signals in real- time. RT-HPC systems can also be used in hardware-in-the-loop (HIL) simulation applications, as well as in distributed, agent-based simulation systems where highly deterministic networks and clock distribution mechanisms can be used for synchronization purposes (e.g. time and frequency can be distributed over a fiber optic network within sub- nanosecond accuracy). For computation purposes, heterogeneous architectures that combine multi-core CPUs, GPUs and FPGAs are used. We will discuss how to leverage FPGAs for the implementation of task/application-specific processors and systolic arrays especially designed to perform specific mathematical and statistical functions. Also, FPGAs can have an important role on the implementation of adaptive computing strategies where processors are continuously optimized and adapted to the task at hand. These and other features are critical for simulating and controlling cyber-physical systems with thousands of nodes (agents) and hundreds of thousands of I/O signals from/to sensors and actuators, including but not limited to massive MIMO wireless communications systems for 5G and mmWave phased-array antennas, smart-grid monitoring and control systems, smart-transportation systems, smart cities, swarms of drones and autonomous agents, etc.\n\nBio:\n\nIgor Alvarado is the Business Development Manager for Academic Research at National Instruments (www.ni.com) where he help to develop collaborations and strategic partnerships with leading universities in the U.S. in such areas as Cyber-Physical Systems, Smart Energy Systems, Medical Imaging/Devices, Advanced Manufacturing and RF/Wireless Communications to advance scientific research and accelerate innovation with support from NSF, NIH, DoD, DoE and other funding agencies. Mr. Alvarado is a Mechanical Engineer (Kansas State University, ’84) and has been with NI since 1999. He is a NSF Innovation-Corps mentor and has more than 30 years of practical experience in successfully developing and growing markets for high-technology products and services in the U.S. and Latin America. He has led the design, development and deployment of real- time, measurement and intelligent control systems that involve advanced numerical methods and algorithms using high-performance embedded platforms. He’s a member of IEEE, ASME, SIAM, IS, AUSA, NORDP, APS and AAAS. In 2017, Mr. Alvarado received the prestigious Electrical and Computer Engineering Department Heads Association’s (ECEDHA) Industry Award for his contributions to the ECE discipline in the U.S.\n\nDavid Feller\n\nTitle: Optimizing Storage to Lower Research Costs and Improve Computational Access\n\nAbstract:\n\nFor researchers or technologists supporting researchers, a key responsibility is the acquisition, security, and availability of data. It is crucial to every aspect of a project, but how it is stored is frequently an afterthought. Theoretically, storage is infinite and provided by central IT, but it can be a large expenditure, be limited in accessibility especially for projects that have remote ingest sites, and public availability of data during or after a project is often a difficult proposition. Worse, researchers looking to save on cost or not willing to learn complicated IT systems often resort to piles of USB connected hard drives resulting in data security and even loss issues.\n\nIn this talk, we will look at the evolving architecture of modern storage systems and present real-world options to manage, secure, and provide universal accessibility of data to researchers, collaborative institutions, and even the public as needed. By implementing a storage strategy that optimizes computational access and lowers research costs, researchers can benefit from technology that will make it more affordable to create large amounts of data, freeing up resources for new projects and accelerating discovery.\n\nTopics:\n\nObject Storage\n\nCloud utilization\n\nGateway and Storage Lifecycle Management Software\n\nCollaborative multi-site sharing software\n\nNote that while Spectra software will be introduced, this talk will cover a variety of options to achieve the stated goals.\n\nBio:\n\nDavid Feller brings more than 25 years of engineering and marketing leadership experience to Spectra. As vice president of product management and solutions engineering, he merges the voice of the customer with market trends and technologies to create and drive a successful product portfolio for Spectra. David also oversees Spectra’s external partnership development, client certification and client test programs.\n\nPrior to Spectra, David was vice president of marketing for DVDO/Silicon Image, where he pioneered 4K and beyond HDMI technology systems. Previous to that, he served as vice president of marketing for Cornice, a hard drive manufacturer that pioneered storage for early portable music players. He also held the position of chief marketing officer for BOCS, an in-home video distribution company, and was product line director at Harris Semiconductor where his group invented and brought to market the world’s first WiFi solution.\n\nDavid holds a bachelor’s of science degree in electrical engineering from Texas A&M University.\n\nJay Boisseau\n\nTITLE: HPC, AI and IoT: Together They Will Change Everything\n\nAbstract:\n\nThe rapid emergence of AI, especially deep learning, presents tremendous new capabilities and possibilities for organizations and endeavors of all types: from government to education to research, and spanning all industries and consumer experiences. AI will enable the potential of the Internet of Things (IoT) to finally be realized, with smart devices at the edges making everything from smart buildings and factories to autonomous vehicles, drones and robots commonplace. Underpinning these transformations will be the advances in high performance computing (HPC), which will be increasingly needed to process streaming data from the “edges” and train more powerful AI models in data centers and clouds. The confluence of edge computing & IoT, advances in AI model accuracy, increasing scale of data centers and cloud computing, enterprise adoption of HPC, and even the global race to exascale computing will lead to integrations of these technologies that profoundly change business, government, research, national security, and society overall in the next decade.\n\nBio:\n\nJay Boisseau Vizias CEO Jay Boisseau is an experienced, recognized leader and strategist in advanced computing technologies, with over 20 years in the field. Jay has built and led many computing-focused programs, departments, and organizations. Jay’s is CEO and co-founder (June 2014) of Vizias. Vizias is a small team of talented, passionate technology professionals who are determined to change the world by using their advanced computing expertise and experience, especially in high performance computing (HPC), artificial intelligence (AI), and smart cities.\n\nJay’s current primary activity is working as the AI & HPC Technology Strategist for Dell EMC (since September 2014) to further develop its high-performance computing (HPC) and artificial intelligence (AI) strategies and new solutions for a broader use of HPC and AI by more companies, government organizations, and universities.\n\nThrough Vizias, Jay founded the Austin CityUP Consortium (July 2015) and currently serves as the executive director, with a vision of creating an integrated smart city fabric throughout Austin—leveraging mobile devices and IoT data collectors, as well as supercomputers and AI for predictive analytics and scenario simulation—in the years ahead to address city issues, empower city planning, and improve city life in general. Jay is also the executive director and founder of The Austin Forum on Technology & Society, which he started in 2006 and is now the leading monthly technology outreach and engagement event in Austin.\n\nMarc Snir\n\nTITLE: Supercomputing - The Next Decade\n\nAbstract:\n\nIn the last 25 years the top performance of supercomputers has increased from teraflops to exaflops. This growth is explained by the exponential improvement in the cost/performance of commodity hardware embodied in Moore’s Law; and by the steady increase in the price of top supercomputers. On the other hand, the architectural evolution of commodity computers has had a negative impact on their performance.\n\nThese external engines of growth will not be available in the future: Moore’s Law is coming to an end; no alternative device technology seems ready to take over any time soon. As the cost of a top supercomputer approaches $1B, the scope for further price increases is limited.\n\nThis talk will elaborate on two claims:\n\nChanges above the device level, in packaging, architecture, software and algorithms, can continue achieving growth in the sustained performance of top supercomputers in the next decade.\n\nThis growth will be achieved only if the business model of the supercomputing industry changes.\n\nBio:\n\nMarc Snir is Michael Faiman Professor in the Department of Computer Science at the University of Illinois at Urbana-Champaign. He currently pursues research in parallel computing.\n\nHe was Director of the Mathematics and Computer Science Division at the Argonne National Laboratory from 2011 to 2016 and head of the Computer Science Department at Illinois from 2001 to 2007. Until 2001 he was a senior manager at the IBM T. J. Watson Research Center where he led the Scalable Parallel Systems research group that was responsible for major contributions to the IBM SP scalable parallel system and to the IBM Blue Gene system.\n\nMarc Snir received a Ph.D. in Mathematics from the Hebrew University of Jerusalem in 1979, worked at NYU on the NYU Ultracomputer project in 1980-1982, and was at the Hebrew University of Jerusalem in 1982-1986, before joining IBM. Marc Snir was a major contributor to the design of the Message Passing Interface. He has published numerous papers and given many presentations on computational complexity, parallel algorithms, parallel architectures, interconnection networks, parallel languages and libraries and parallel programming environments.\n\nMarc is AAAS Fellow, ACM Fellow and IEEE Fellow. He has Erdos number 2 and is a mathematical descendant of Jacques Salomon Hadamard. He won the IEEE Award for Excellence in Scalable Computing and the IEEE Seymour Cray Computer Engineering Award. He was recently awarded a Doctor Honoris Causa Degree from ENS, Lyon, France.\n\nWilliam L. Miller\n\nTitle: Transforming Science with CI – An Update from the NSF Office of Advanced Cyberinfrastructure\n\nAbstract:\n\nNSF's Office of Advanced Cyberinfrastructure (OAC) seeks to foster the advanced cyberinfrastructure (CI) that has become essential to progress of all areas of science and engineering research and education. OAC's investments in computing, data infrastructure, software, networking and cybersecurity, and people have enabled new innovations and discoveries. Yet, dramatic changes are occurring in the nature and requirements of science, in the scale and pervasiveness of research data, and in the landscape of technologies and resources. As will be described in this presentation, NSF has sought and received much community input on these changes - which are informing an evolution of OAC’s programs and investments in research CI and CI research to transform science in the 21st century.\n\nBio:\n\nWilliam Miller is the Science Advisor for the Office of Advanced Cyberinfrastructure (OAC) at the National Science Foundation. Bill leads strategic activities for OAC designed to catalyze new cyberinfrastructure (CI)-enabled pathways for scientific discovery, promote multi-disciplinary activities, and foster international engagement. He leads the Cyberinfrastructure for Emerging Science and Engineering Research (CESER) Program that supports early-stage science-CI collaborations. Current CESER foci are NSF priority investments including the NSF Big Ideas and NSF’s Major Multi-User Research Facilities. Bill has previously served as Acting Deputy of the NSF Office of Budget, Finance and Award Management, and in policy and oversight roles for large facility planning and construction. He has prior professional careers in space mission systems development and in experimental neuroscience, each conducted in the U.S. and internationally.\n\nDavid Feller\n\nSpeaker Info:\n\nCompany: Spectra Logic\n\nPosition/title: VP of Product Management and Solutions Engineering\n\nEmail: davidfe@spectralogic.com\n\nPhone: 303-449-6444,1436\n\nBio:\n\nDavid Feller is an executive leader with a long history of delivering world changing consumer electronics products. As VP of Product Management and Solutions Engineering at Spectra Logic, he has ownership of the company’s strategic roadmap across all product lines. With 20+ years of experience in product management, strategic marketing, business development, operations, engineering and executive management, David has led nearly every aspect of small and fortune 100 business. He is especially passionate about product management and strategy – taking a market or customer requirement, evolving it into a concept, and molding an idea through development, launch, and mass production. Mr. Feller ran the product management/marketing group that invented wireless LAN, the group that pioneered putting mini hard drives in MP3/media players, and the group that made real-time security/surveillance remote access available home/business/worldwide.\n\nAbstract: Considering Future Technologies in the World of Everlasting Data\n\nOrganizations across the globe have all faced the challenge of managing their unstructured data, which traditionally consists of 60%-90% of their total data. A major aspect of managing data is the ability for the data management software to tier your data, meaning it has the ability to move your inactive data to a more affordable tier of storage. This is where all solutions are not created equal, and organizations need to determine the best way to not only manage their data, but also protect the data that is being managed. How do organizations implement a tiered storage solution that protects their data for as long as they need to keep it? Many data management software products do a great job of identifying the data that should be offloaded from the user’s primary storage (tier 1 storage) but what happens to the data once its moved, and how protected is the data that is now essentially archived? How do you get the data back when you need it? Complete solutions should not only assist in the data management process but ensure that the inactive data being moved is securely protected and always available when its needed. This new focus on archive is not meant to replace backup solutions, but is necessary to enhance data protection for historically critical data. Join us to find out the newest data management strategies to ensure that your primary storage is free of inactive files, your inactive files are efficiently moved to a more affordable tier of storage and your data is protected so that it is forever available when it’s needed.\n\nSen-ching Samson Cheung\n\nTitle: Keeping the Biggest Data Safe\n\nAbstract:\n\nMultidimensional streaming signals like audio, videos, and multi-spectral imagery are truly the biggest of the Big Data. Collected by sensors at the network edge, these data often contain highly sensitive information and their misuse can lead to significant invasion of privacy. For example, networks of surveillance cameras have been used by some countries in tracking their citizens, and pervasive use of IoT devices in smart homes opens door to recording private behaviors and interactions. It is highly challenging to apply traditional privacy enhancing technologies to handle these data. Cryptographic techniques like homomorphic encryption and garbled circuits are too computationally intensive for any practical applications. The more efficient differential private schemes, on the other hand, rely on additive noise and may not be able to provide adequate protection on semantic contents.\n\nIn this talk, I will discuss a number of techniques developed in my research group to process multidimensional signals while ensuring their privacy. Specifically, I will present our work in using generative adversarial network (GAN) and random neural networks to protect signal privacy in distributed deep learning. We use GAN to learn the statistics of sensitive facial data at local sites and generate privacy-preserving synthetic data for public centralized learning. While GAN at network edge can be computationally intensive, random neural network offers a much lighter weight privacy-enhancing protocol suitable for a broad range of IoT applications.\n\nBio:\n\nSen-ching “Samson” Cheung is a Professor of Electrical and Computer Engineering and the director of Multimedia Information Laboratory (Mialab) at University of Kentucky (UKY), Lexington, KY, USA. He is the endowed Blazie Family Professor of Engineering. Before joining UKY in 2004, he was a postdoctoral researcher with the Sapphire Scientific Data Mining Group at Lawrence Livermore National Laboratory that won the R&D 100 Award in 2006. He received his Ph.D. degree from University of California, Berkeley in 2002. He is a senior member of both IEEE and ACM. He has the fortune of working with a team of talented students and collaborators at Mialab in several areas in multimedia including video surveillance, privacy protection, encrypted domain signal processing, 3D data processing, virtual and augmented reality as well as computational multimedia for autism therapy. More details about current and past research projects at Mialab can be found at http://vis.uky.edu/mialab.\n\nCALL FOR SPEAKERS\n\nYou are invited to submit an abstract to give a lightning talk highlighting your research for the 4th Annual Commonwealth Computational Summit. We solicit talks from faculty from any or all Universities or Colleges in our region! Presentations are being sought in three areas: 1 – High Performance Computing-Modeling and Simulation; 2 – Data Science, Data Mining and Analytics; and 3 – Artificial Intelligence, Machine Learning, Deep Learning or New Technologies. Guidelines for Presentations: In general, presentations will be 10 minutes in length with an end of session Q&A discussion for all speakers. Presentations will focus on exciting computational oriented research conducted by faculty throughout the Commonwealth of Kentucky and our region. The audience for presentations will be other faculty researchers, graduate students and industry scientists and engineers working in computational fields. Since these are lightning talks you will be expected to highlight your research interest or a particular project and suggest that you cover interesting challenges and collaborative opportunities.\n\nInformation on Submission: The following information must be submitted by Sunday, October 4, 2020. Selection of speakers will be completed by Monday October 5, 2020. A complete program will be distributed on this date. See Summit website for registration and submission of materials for speakers. You will be prompted for information such as: name, email & contact Info, affiliation/institution, title/position, area/topic of research (see below), title of the presentation and an mini-abstract (short description with a mini-speaker bio, 1 page length (total)).\n\nNote: Optional PDF upload is also available; you will be required to submit your slides to your session chair prior to the actual event to facilitate quick and on-going talks by all speakers. Additional description related to lightning talk topic sessions:\n\nA) High Performance Computing, Modeling and Simulations, Computational Sciences and Engineering - potential topics include but are not limited to: applications in high performance computing (CFD, Molecular Dynamic Simulations, HEP, QCD, computational sciences/engineering challenges, etc.), complex modeling and simulations, extreme scale computing, HPC software tools, techniques and performance analysis, etc.\n\nB) Data Science, Data Mining - data analytics, textual analytics, both structured and unstructured data/text, classification, clustering and association algorithms, data: complexity, relationship and visualization across multiple applications and disciplines including linguistics, bioinformatics, business and economics, etc.\n\nC) Artificial Intelligence, Machine Learning, Deep Learning and New Technologies including but not limited to quantum computing, neuromorphic computing, resilient systems, and edge computing etc.\n\nThe CCS Technical Program Committee and Session Chairs will review all submissions and select presentations for each topic area based upon quality of abstracts submitted, research interest/impact and mixture of topics. It should be noted that our intent is to be as inclusive and diverse as possible for these lightning talks. In addition, it is our intent that Post-Summit, all slides (PDF) for the speakers will be posted in our CCS Online Summit Proceedings.\n\nImportant Dates: Deadline for abstract submission: Sunday, October 4, 2020\n\nSpeaker Selection Notification: On or before Monday, October 5, 2020\n\nDeadline for Presentation Slides for Summit:\n\nSaturday, October 10, 2020\n\nSummit Begins: Monday, October 12, 2020\n\nLightning Talks: Tuesday, October 13, 2020\n\nAddress all questions to:\n\nTony Elam Office phone: (859) 257-2326.\n\nAssc. Dir, CCS Cell phone: (713) 859-9860\n\nUniversity of Kentucky\n\nEmail: (tony.elam@uky.edu)"
    }
}
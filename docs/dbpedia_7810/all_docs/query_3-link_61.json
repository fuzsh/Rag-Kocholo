{
    "id": "dbpedia_7810_3",
    "rank": 61,
    "data": {
        "url": "https://galileo-unbound.blog/category/sciencescape/",
        "read_more_link": "",
        "language": "en",
        "title": "Galileo Unbound",
        "top_image": "https://galileo-unbound.blog/wp-content/uploads/2023/06/cropped-gulogo2.jpg?w=200",
        "meta_img": "https://galileo-unbound.blog/wp-content/uploads/2023/06/cropped-gulogo2.jpg?w=200",
        "images": [
            "https://galileo-unbound.blog/wp-content/uploads/2023/06/galileoanswers2.jpg?w=800",
            "https://galileo-unbound.blog/wp-content/uploads/2024/07/rogueone-1.png?w=692",
            "https://galileo-unbound.blog/wp-content/uploads/2024/07/microsoft-bluescreen-error-1.jpg?w=1024",
            "https://galileo-unbound.blog/wp-content/uploads/2024/03/michelsonnobel.jpg?w=1200",
            "https://galileo-unbound.blog/wp-content/uploads/2023/05/albert_abraham_michelson2.jpg?w=440",
            "https://galileo-unbound.blog/wp-content/uploads/2024/01/frontiers2024.jpg?w=1200",
            "https://galileo-unbound.blog/wp-content/uploads/2024/01/image-1.png?w=1024",
            "https://galileo-unbound.blog/wp-content/uploads/2024/01/image-3.png?w=1024",
            "https://galileo-unbound.blog/wp-content/uploads/2024/01/image-2.png?w=1024",
            "https://galileo-unbound.blog/wp-content/uploads/2024/01/opo9919k.jpg?w=1024",
            "https://galileo-unbound.blog/wp-content/uploads/2024/01/atlas-entanglementvsmass.png?w=1024",
            "https://galileo-unbound.blog/wp-content/uploads/2023/06/differenceengine.jpg?w=1200",
            "https://galileo-unbound.blog/wp-content/uploads/2023/06/323362_steamboy_1400x2100_english.jpg?w=560",
            "https://galileo-unbound.blog/wp-content/uploads/2023/06/image-1.png?w=900",
            "https://galileo-unbound.blog/wp-content/uploads/2023/06/babbages-difference-engine-no-1-1824-18322.jpg?w=807",
            "https://galileo-unbound.blog/wp-content/uploads/2023/06/image.png?w=806",
            "https://galileo-unbound.blog/wp-content/uploads/2023/06/image-2.png?w=1024",
            "https://galileo-unbound.blog/wp-content/uploads/2023/06/diagram_for_the_computation_of_bernoulli_numbers.jpg?w=1024",
            "https://galileo-unbound.blog/wp-content/uploads/2023/04/transformer-2.jpg?w=1024",
            "https://galileo-unbound.blog/wp-content/uploads/2023/04/49682529-b23e2880-fadb-11e8-8625-82fc2b14c487.png?w=900",
            "https://galileo-unbound.blog/wp-content/uploads/2023/01/frontiers.jpg?w=1200",
            "https://galileo-unbound.blog/wp-content/uploads/2024/01/image.png?w=1024",
            "https://www.sciencenews.org/wp-content/uploads/2022/06/060922_ec_quantum-machine-learning_feat-1030x580.jpg",
            "https://galileo-unbound.blog/wp-content/uploads/2023/01/image-1.png?w=794",
            "https://galileo-unbound.blog/wp-content/uploads/2023/01/220512-black-hole-mb-1434-d30fc8.jpg?w=1024",
            "https://www.llnl.gov/sites/www/files/styles/scaled_425h/public/28723_preamplifiersBig.jpg?itok=Zb4VmLP4",
            "https://galileo-unbound.blog/wp-content/uploads/2022/10/flux.jpg?w=1200",
            "https://galileo-unbound.blog/wp-content/uploads/2022/10/thinveneer.jpg?w=958",
            "https://galileo-unbound.blog/wp-content/uploads/2022/10/image.png?w=864",
            "https://galileo-unbound.blog/wp-content/uploads/2022/10/climatemodel-1.jpg?w=1024",
            "https://galileo-unbound.blog/wp-content/uploads/2022/10/flux.jpg?w=1024",
            "https://galileo-unbound.blog/wp-content/uploads/2022/10/image-1.png?w=614",
            "https://galileo-unbound.blog/wp-content/uploads/2022/03/starflight.jpg?w=1200",
            "https://galileo-unbound.blog/wp-content/uploads/2022/03/560px-artists_impression_of_the_planet_orbiting_proxima_centauri.jpg?w=560",
            "https://galileo-unbound.blog/wp-content/uploads/2022/03/082416_to_proxima_centauri_main.jpg?w=860",
            "https://galileo-unbound.blog/wp-content/uploads/2022/03/image-17.png?w=1024",
            "https://galileo-unbound.blog/wp-content/uploads/2022/03/image-18.png?w=428",
            "https://galileo-unbound.blog/wp-content/uploads/2022/03/image-19.png?w=806",
            "https://galileo-unbound.blog/wp-content/uploads/2022/03/image-20.png?w=630",
            "https://galileo-unbound.blog/wp-content/uploads/2022/03/image-21.png?w=610",
            "https://galileo-unbound.blog/wp-content/uploads/2022/03/image-22.png?w=358",
            "https://galileo-unbound.blog/wp-content/uploads/2022/03/image-23.png?w=398",
            "https://galileo-unbound.blog/wp-content/uploads/2022/03/image-24.png?w=1024",
            "https://galileo-unbound.blog/wp-content/uploads/2022/03/image-9.png?w=398",
            "https://galileo-unbound.blog/wp-content/uploads/2022/03/image-10.png?w=562",
            "https://galileo-unbound.blog/wp-content/uploads/2022/03/image-13.png?w=1024",
            "https://galileo-unbound.blog/wp-content/uploads/2022/03/image-27.png?w=1024",
            "https://galileo-unbound.blog/wp-content/uploads/2022/03/image-15.png?w=868",
            "https://galileo-unbound.blog/wp-content/uploads/2022/03/image-16.png?w=802",
            "https://galileo-unbound.blog/wp-content/uploads/2022/03/image-28.png?w=1024",
            "https://galileo-unbound.blog/wp-content/uploads/2019/10/nolte_modern_dynamics_sm_card.jpg?w=1024",
            "https://galileo-unbound.blog/wp-content/uploads/2022/10/autoc2.png?w=1200",
            "https://galileo-unbound.blog/wp-content/uploads/2022/03/image.png?w=362",
            "https://galileo-unbound.blog/wp-content/uploads/2022/03/image-1.png?w=256",
            "https://galileo-unbound.blog/wp-content/uploads/2022/03/image-2.png?w=230",
            "https://galileo-unbound.blog/wp-content/uploads/2022/03/image-3.png?w=202",
            "https://galileo-unbound.blog/wp-content/uploads/2022/03/image-4.png?w=1024",
            "https://galileo-unbound.blog/wp-content/uploads/2022/03/image-5.png?w=1024",
            "https://galileo-unbound.blog/wp-content/uploads/2022/03/image-6.png?w=1024",
            "https://galileo-unbound.blog/wp-content/uploads/2022/03/image-7.png?w=1024",
            "https://galileo-unbound.blog/wp-content/uploads/2022/03/image-8.png?w=1024",
            "https://galileo-unbound.blog/wp-content/uploads/2022/03/image-26.png?w=1024",
            "https://galileo-unbound.blog/wp-content/uploads/2021/10/icelandicsaga.jpg?w=1200",
            "https://galileo-unbound.blog/wp-content/uploads/2021/10/googleearth2-1.jpg?w=1024",
            "https://galileo-unbound.blog/wp-content/uploads/2021/10/authentic_viking_recreation.jpg?w=1024",
            "https://galileo-unbound.blog/wp-content/uploads/2021/10/2abadb38186424b40f4ba190f520478d-1.jpg?w=697",
            "https://galileo-unbound.blog/wp-content/uploads/2021/10/solarflare.jpg?w=1024",
            "https://galileo-unbound.blog/wp-content/uploads/2021/10/tst_54_ufigure1_0.jpg?w=1024",
            "https://galileo-unbound.blog/wp-content/uploads/2021/10/carbon14.jpg?w=1024",
            "https://galileo-unbound.blog/wp-content/uploads/2023/06/cropped-gulogo2.jpg?w=50",
            "https://galileo-unbound.blog/wp-content/uploads/2023/06/cropped-gulogo2.jpg?w=50",
            "https://pixel.wp.com/b.gif?v=noscript"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "David D. Nolte"
        ],
        "publish_date": "2024-07-23T06:21:20-04:00",
        "summary": "",
        "meta_description": "Blog on the practice of science.",
        "meta_lang": "en",
        "meta_favicon": "https://galileo-unbound.blog/wp-content/uploads/2023/06/cropped-gulogo2.jpg?w=32",
        "meta_site_name": "Galileo Unbound",
        "canonical_link": "https://galileo-unbound.blog/category/sciencescape/",
        "text": "One of my favorite movies within the Star Wars movie franchise is Rogue One, the prequel to the very first movie (known originally simply as Star Wars but now called Episode IV: A New Hope).\n\nBut I always thought there was a fundamental flaw in the plotline of Rogue One when the two main characters Jyn Erso and Cassian Andor (played by Felicity Jones and Diego Luna) are forced to climb a physical tower to retrieve a physical memory unit, like a hard drive, containing the plans to the Death Star.\n\nIn such an advanced technological universe as Star Wars, why were the Death Star plans sitting on a single isolated hard drive, stored away like a file in a filing cabinet? Why weren’t they encrypted and stored in bits and pieces across the cloud? In fact, among all the technological wonders of the Star Wars universe, the cloud and the internet are conspicuously absent. Why?\n\nAfter the Microsoft IT crash of July 19, 2024, I think I know the answer: Because the internet and the cloud and computer operating systems are so fundamentally and hopelessly flawed that any advanced civilization would have dispensed with them eons ago.\n\nInformation Technology (IT)\n\nI used to love buying a new computer. It was a joy to boot up for the first time, like getting a new toy. But those days may be over.\n\nNow, when I buy a new computer through my university, the IT staff won’t deliver it until they have installed several layers of control systems overlayed on top of the OS. And then all the problems start … incompatibilities, conflicts, permissions denied, failed software installation, failed VPN connections, unrecognized IP addresses, and on and on.\n\nThe problem, of course, is computer security. There are so many IT hack attacks through so many different avenues that multiple layers of protection are needed to keep attackers out of the university network and off its computers.\n\nBut the security overhead is getting so burdensome, causing so many problems, that the dream from decades ago that the computer era would save all of us so much time has now become a nightmare as we spend hours per day just doing battle with IT issues. More and more of our time is sucked into the IT black hole.\n\nThe Microsoft IT Crash of July 19, 2024\n\nOn Friday the 19th, we were in New York City, scheduled to fly out of Newark Airport around 2pm to return to Indianapolis. We knew we were in trouble when we looked at the news on Friday morning. The top story was about an IT crash of Microsoft operating systems controlling airlines, banks and healthcare systems.\n\nAt Newark airport, we were greeted by the Blue Screen of Death (BSoD) on all the displays that should have been telling us about flight information. Our United apps still worked on our iPhones, but our flight to Indy had been cancelled. We took an option for a later flight and went to the United Club with two valid tickets and a lot of time to kill, but they wouldn’t let us in because their reader had crashed too.\n\nSo we went to get pot stickers for lunch. Our push notifications had been turned on, but we never received the alert that our second flight had been cancelled because the push notifications weren’t going out. By the time we realized we had no flight, United had rebooked us on a flight 2 days later.\n\nNot wanting to hang around the Newark airport for 2 days, we went online to rent a car to drive the 16 hours back to Indy, but all the cars were sold out. In a last desperate act, we went onto Expedia and found an available car from Thrifty Car Rental—likely the very last one at the Newark airport.\n\nSo, on the road by 4pm we had 16 hours ahead of us before getting back home. The cost out of pocket (even after subtracting the $400 refund from United on our return flight) was $700 … all because of one line of code in a Microsoft update. The total estimated cost of that error worldwide is anticipated to exceed $1B.\n\nA House of Cards\n\nThe IT era began around 1980, about 45 years ago, when IBM launched its PC. Operating systems were amazingly simplistic at that time, but slowly over the decades they grew into behemoths, add-ons adding to add-ons, cobbled together as if with chewing gum and baling wire. Now they consist of millions of lines of code, patches on patches seeking to fix incompatibilities that create more incompatibilities in the process.\n\nIT is a house of cards that takes only one bad line of code to bring the whole thing crashing down across the world. This is particularly worrisome given the Axis of Chaos that resents seeing the free world enjoying its freedoms. It’s an easy target.\n\nBut it doesn’t have to be this way. It’s not unlike the early industrial revolution of steam power when every engine was different, or transportation when there were multiple railroad wheel widths, or electrification when AC did battle with DC, or telecommunications when different types of MUX on fiber-optic cables were incompatible. This always happens when there is a revolution in technology that develops rapidly.\n\nWhat is needed is a restart, to scrap the entire system and start from scratch. Computer Scientists know how to build an efficient and resilient network from the ground up, with certification processes to remove the anonymity that enables cyber criminals to masquerade as legitimate operators.\n\nBut to do this requires a financial incentive. The cost would be huge because the current system is so delocalized as every laptop or smart pad becomes a node in the network. The Infrastructure Bill could still make this goal its target. That would be revolutionary and enabling (like the Eisenhower Interstate System was in the 1950’s which transformed American society), instead of spending a trillion dollars to fill in potholes across a neglected infrastructure.\n\nIt may seem to be too late to start over, but a few more IT crashes like last Friday may make it mandatory. Wouldn’t it be better to start now?\n\nAlbert Michelson was the first American to win a Nobel Prize in science. He was awarded the Nobel Prize in physics in 1907 for the invention of his eponymous interferometer and for its development as a precision tool for metrology. On board ship traveling to Sweden from London to receive his medal, he was insulted by the British author Rudyard Kipling (that year’s Nobel Laureate in literature) who quipped that America was filled with ignorant masses who wouldn’t amount to anything.\n\nNotwithstanding Kipling’s prediction, across the following century, Americans were awarded 96 Nobel prizes in physics. The next closest nationalities were Germany with 28, the United Kingdom with 25 and France with 18. These are ratios of 3:1, 4:1 and 5:1. Why was the United States so dominant, and why was Rudyard Kipling so wrong?\n\nAt the same time that American scientists were garnering the lion’s share of Nobel prizes in physics in the 20th century, the American real (inflation-adjusted) gross-domestic-product (GDP) grew from 60 billion dollars to 20 trillion dollars, making up about a third of the world-wide GDP, even though it has only about 5% of the world population. So once again, why was the United States so dominant across the last century? What factors contributed to this success?\n\nThe answers are complicated, with many contributing factors and lots of shades of gray. But two factors stand out that grew hand-in-hand over the century; these are:\n\n1) The striking rise of American elite universities, and\n\n2) The significant gain in the US brain trust through immigration\n\nAlbert Michelson is a case in point.\n\nThe Firestorms of Albert Michelson\n\nAlbert Abraham Michelson was, to some, an undesirable immigrant, born poor in Poland to a Jewish family who made the arduous journey through the Panama Canal in the second wave of 49ers swarming over the California gold country. Michelson grew up in the Wild West, first in the rough town of Murphy’s Camp in California, in foothills of the Sierras. After his father’s supply store went up in flames, they moved to Virginia City, Nevada. His younger brother Charlie lived by the gun (after Michelson had left home), providing meat and protection for supply trains during the Apache wars in the Southwest. This was America in the raw.\n\nYet Michelson was a prodigy. He outgrew the meager educational possibilities in the mining towns, so his family scraped together enough money to send him to a school in San Francisco, where he excelled. Later, in Virginia City, an academic competition was held for a special appointment to the Naval Academy in Annapolis, and Michelson tied for first place, but the appointment went to the other student who was the son of a Civil War Vet.\n\nWith the support of the local Jewish community, Michelson took a train to Washington DC (traveling on the newly-completed Transcontinental Railway, passing over the spot where a golden spike had been driven one month prior into a railroad tie made of Californian laurel) to make his case directly. He met with President Grant at the White House, but all the slots at Annapolis had been filled. Undaunted, Michelson camped out for three days in the waiting room of the office of an Annapolis Admiral, who finally relented and allowed Michelson to take the entrance exam. Still, there was no place for him at the Academy.\n\nDiscouraged, Michelson bought a ticket and boarded the train for home. One can only imagine his shock when he heard his name called out by a someone walking down the car aisle. It was a courier from the White House. Michelson met again with Grant, who made an extraordinary extra appointment for Michelson at Annapolis; the Admiral had made his case for him. With no time to return home, he was on board ship for his first training cruise within a week, returning a month later to start classes.\n\nYears later, as Michelson prepared, with Edmund Morley, to perform the most sensitive test ever made of the motion of the Earth, using his recently-invented “Michelson Interferometer”, the building with his lab went up in flames, just like his father’s goods store had done years before. This was a trying time for Michelson. His first marriage was on the rocks, and he had just recovered from having a nervous breakdown (his wife at one point tried to have him committed to an insane asylum from where patients rarely ever returned). Yet with Morley’s help, they completed the measurement.\n\nTo Michelson’s dismay, the exquisite experiment with the finest sensitivity—that should have detected a large deviation of the fringes depending on the orientation of the interferometer relative to the motion of the Earth through space—gave a null result. They published their findings, anyway, as one more puzzle in the question of the speed of light, little knowing how profound this “Michelson-Morley” experiment would be in the history of modern physics and the subsequent development of the relativity theory of Albert Einstein (another immigrant).\n\nPutting the disappointing null result behind him, Michelson next turned his ultra-sensitive interferometer to the problem of replacing the platinum meter-bar standard in Paris with a new standard that was much more fundamental—wavelengths of light. This work, unlike his null result, led to practical success for which he was awarded the Nobel Prize in 1907 (not for his null result with Morley).\n\nMichelson’s Nobel Prize in physics in 1907 did not immediately open the floodgates. Sixteen years passed before the next Nobel in physics went to an American (Robert Millikan). But after 1936 (as many exiles from fascism in Europe immigrated to the US) Americans were regularly among the prize winners.\n\nList of American Nobel Prizes in Physics\n\n* (I) designates an immigrant.\n\n1907 Albert Michelson (I) Optical precision instruments and metrology\n\n1923 Robert Millikan Elementary charge and photoelectric effect\n\n1927 Arthur Compton The Compton effect\n\n1936 Carl David Anderson Discovery of the positron\n\n1937 Clinton Davisson Diffraction of electrons by crystals\n\n1939 Ernest Lawrence Invention of the cyclotron\n\n1943 Otto Stern (I) Magnetic moment of the proton\n\n1944 Isidor Isaac Rabi (I) Magnetic properties of atomic nuclei\n\n1946 Percy Bridgman High pressure physics\n\n1952 E. M. Purcell Nuclear magnetic precision measurements\n\n1952 Felix Bloch (I) Nuclear magnetic precision measurements\n\n1955 Willis Lamb Fine structure of the hydrogen spectrum\n\n1955 Polykarp Kusch (I) Magnetic moment of the electron\n\n1956 William Shockley (I) Discovery of the transistor effect\n\n1956 John Bardeen Discovery of the transistor effect\n\n1956 Walter H. Brattain (I) Discovery of the transistor effect\n\n1957 Chen Ning Yang (I) Parity laws of elementary particles\n\n1957 Tsung-Dao Lee (I) Parity laws of elementary particles\n\n1959 Owen Chamberlain Discovery of the antiproton\n\n1959 Emilio Segrè (I) Discovery of the antiproton\n\n1960 Donald Glaser Invention of the bubble chamber\n\n1961 Robert Hofstadter The structure of nucleons\n\n1963 Maria Goeppert-Mayer (I) Nuclear shell structure\n\n1963 Eugene Wigner (I) Fundamental symmetry principles\n\n1964 Charles Townes Quantum electronics\n\n1965 Richard Feynman Quantum electrodynamics\n\n1965 Julian Schwinger Quantum electrodynamics\n\n1967 Hans Bethe (I) Theory of nuclear reactions\n\n1968 Luis Alvarez Hydrogen bubble chamber\n\n1969 Murray Gell-Mann Classification of elementary particles and interactions\n\n1972 John Bardeen Theory of superconductivity\n\n1972 Leon N. Cooper Theory of superconductivity\n\n1972 Robert Schrieffer Theory of superconductivity\n\n1973 Ivar Giaever (I) Tunneling phenomena\n\n1975 Ben Roy Mottelson The structure of the atomic nucleus\n\n1975 James Rainwater The structure of the atomic nucleus\n\n1976 Burton Richter Discovery of a heavy elementary particle\n\n1976 Samuel C. C. Ting Discovery of a heavy elementary particle\n\n1977 Philip Anderson Magnetic and disordered systems\n\n1977 John van Vleck Magnetic and disordered systems\n\n1978 Robert Wilson Discovery of cosmic microwave background radiation\n\n1978 Arno Penzias (I) Discovery of cosmic microwave background radiation\n\n1979 Steven Weinberg Unified weak and electromagnetic interaction\n\n1979 Sheldon Glashow Unified weak and electromagnetic interaction\n\n1980 James Cronin Symmetry principles in the decay of neutral K-mesons\n\n1980 Val Fitch Symmetry principles in the decay of neutral K-mesons\n\n1981 Nicolaas Bloembergen (I) Nonlinear Optics\n\n1981 Arthur Schawlow Development of laser spectroscopy\n\n1982 Kenneth Wilson Theory for critical phenomena and phase transitions\n\n1983 William Fowler Formation of the chemical elements in the universe\n\n1983 Subrahmanyan Chandrasekhar (I) The evolution of the stars\n\n1988 Leon Lederman Discovery of the muon neutrino\n\n1988 Melvin Schwartz Discovery of the muon neutrino\n\n1988 Jack Steinberger (I) Discovery of the muon neutrino\n\n1989 Hans Dehmelt (I) Ion trap\n\n1989 Norman Ramsey Atomic clocks\n\n1990 Jerome Friedman Deep inelastic scattering of electrons on nucleons\n\n1990 Henry Kendall Deep inelastic scattering of electrons on nucleons\n\n1993 Russell Hulse Discovery of a new type of pulsar\n\n1993 Joseph Taylor Jr. Discovery of a new type of pulsar\n\n1994 Clifford Shull Neutron diffraction\n\n1995 Martin Perl Discovery of the tau lepton\n\n1995 Frederick Reines Detection of the neutrino\n\n1996 David Lee Discovery of superfluidity in helium-3\n\n1996 Douglas Osheroff Discovery of superfluidity in helium-3\n\n1996 Robert Richardson Discovery of superfluidity in helium-3\n\n1997 Steven Chu Laser atom traps\n\n1997 William Phillips Laser atom traps\n\n1998 Horst Störmer (I) Fractionally charged quantum Hall effect\n\n1998 Robert Laughlin Fractionally charged quantum Hall effect\n\n1998 Daniel Tsui (I) Fractionally charged quantum Hall effect\n\n2000 Jack Kilby Integrated circuit\n\n2001 Eric Cornell Bose-Einstein condensation\n\n2001 Carl Wieman Bose-Einstein condensation\n\n2002 Raymond Davis Jr. Cosmic neutrinos\n\n2002 Riccardo Giacconi (I) Cosmic X-ray sources\n\n2003 Anthony Leggett (I) The theory of superconductors and superfluids\n\n2003 Alexei Abrikosov (I) The theory of superconductors and superfluids\n\n2004 David Gross Asymptotic freedom in the strong interaction\n\n2004 H. David Politzer Asymptotic freedom in the strong interaction\n\n2004 Frank Wilczek Asymptotic freedom in the strong interaction\n\n2005 John Hall Quantum theory of optical coherence\n\n2005 Roy Glauber Quantum theory of optical coherence\n\n2006 John Mather Anisotropy of the cosmic background radiation\n\n2006 George Smoot Anisotropy of the cosmic background radiation\n\n2008 Yoichiro Nambu (I) Spontaneous broken symmetry in subatomic physics\n\n2009 Willard Boyle (I) CCD sensor\n\n2009 George Smith CCD sensor\n\n2009 Charles Kao (I) Fiber optics\n\n2011 Saul Perlmutter Accelerating expansion of the Universe\n\n2011 Brian Schmidt Accelerating expansion of the Universe\n\n2011 Adam Riess Accelerating expansion of the Universe\n\n2012 David Wineland Atom Optics\n\n2014 Shuji Nakamura (I) Blue light-emitting diodes\n\n2016 F. Duncan Haldane (I) Topological phase transitions\n\n2016 John Kosterlitz (I) Topological phase transitions\n\n2017 Rainer Weiss (I) LIGO detector and gravitational waves\n\n2017 Kip Thorne LIGO detector and gravitational waves\n\n2017 Barry Barish LIGO detector and gravitational waves\n\n2018 Arthur Ashkin Optical tweezers\n\n2019 Jim Peebles (I) Cosmology\n\n2020 Andrea Ghez Milky Way black hole\n\n2021 Syukuro Manabe (I) Global warming\n\n2022 John Clauser Quantum entanglement\n\n(Table information source.)\n\n(Note: This list does not include Enrico Fermi, who was awarded the Nobel Prize while in Italy. After traveling to Stockholm to receive the award, he did not return to Italy, but went to the US to protect his Jewish wife from the new race laws enacted by the nationalist government of Italy. There are many additional Nobel prize winners not on this list (like Albert Einstein) who received the Nobel Prize while in their own country but who then came to the US to teach at US institutions.)\n\nImmigration and Elite Universities\n\nA look at the data behind the previous list tells a striking story: 1) Nearly all of the American Nobel Prizes in physics were awarded for work performed at elite American universities; 2) Roughly a third of the prizes went to immigrants. And for those prize winners who were not immigrants themselves, many were taught by, or studied under, immigrant professors at those elite universities.\n\nElite universities are not just the source of Nobel Prizes, but are engines of the economy. The Tech Sector may contribute only 10% of the US GDP, but 85% of our GDP is attributed to “innovation”, much of coming out of our universities. Our “inventive” economy is driving the American standard of living and keeps us competitive in the worldwide market.\n\nToday, elite universities, as well as immigration, are under attack by forces who want to make America great again. Legislatures in some states have passed laws restricting how those universities hire and teach, and more states are following suite. Some new state laws restrict where Chinese-born professors, who are teaching and conducting research at American universities, can or cannot buy houses. And some members of Congress recently ambushed the leaders of a few of our most elite universities (who failed spectacularly to use common sense), using the excuse of a non-academic issue to turn universities into a metaphor for the supposed evils of elitism.\n\nBut the forces seeking to make America great again may be undermining the very thing that made America great in the first place.\n\nThey want to cook the goose, but they are overlooking the golden eggs.\n\nThese days, the physics breakthroughs in the news that really catch the eye tend to be Astro-centric. Partly, this is due to the new data coming from the James Webb Space Telescope, which is the flashiest and newest toy of the year in physics. But also, this is part of a broader trend in physics that we see in the interest statements of physics students applying to graduate school. With the Higgs business winding down for high energy physics, and solid state physics becoming more engineering, the frontiers of physics have pushed to the skies, where there seem to be endless surprises.\n\nTo be sure, quantum information physics (a hot topic) and AMO (atomic and molecular optics) are performing herculean feats in the laboratories. But even there, Bose-Einstein condensates are simulating the early universe, and quantum computers are simulating worm holes—tipping their hat to astrophysics!\n\nSo here are my picks for the top physics breakthroughs of 2023.\n\nThe Early Universe\n\nThe James Webb Space Telescope (JWST) has come through big on all of its promises! They said it would revolutionize the astrophysics of the early universe, and they were right. As of 2023, all astrophysics textbooks describing the early universe and the formation of galaxies are now obsolete, thanks to JWST.\n\nForemost among the discoveries is how fast the universe took up its current form. Galaxies condensed much earlier than expected, as did supermassive black holes. Everything that we thought took billions of years seem to have happened in only about one-tenth of that time (incredibly fast on cosmic time scales). The new JWST observations blow away the status quo on the early universe, and now the astrophysicists have to go back to the chalk board.\n\nGravitational Ripples\n\nIf LIGO and the first detection of gravitational waves was the huge breakthrough of 2015, detecting something so faint that it took a century to build an apparatus sensitive enough to detect them, then the newest observations of gravitational waves using galactic ripples presents a whole new level of gravitational wave physics.\n\nBy using the exquisitely precise timing of distant pulsars, astrophysicists have been able to detect a din of gravitational waves washing back and forth across the universe. These waves came from supermassive black hole mergers in the early universe. As the waves stretch and compress the space between us and distant pulsars, the arrival times of pulsar pulses detected at the Earth vary a tiny but measurable amount, haralding the passing of a gravitational wave.\n\nThis approach is a form of statistical optics in contrast to the original direct detection that was a form of interferometry. These are complimentary techniques in optics research, just as they will be complimentary forms of gravitational wave astronomy. Statistical optics (and fluctuation analysis) provides spectral density functions which can yield ensemble averages in the large N limit. This can answer questions about large ensembles that single interferometric detection cannot contribute to. Conversely, interferometric detection provides the details of individual events in ways that statistical optics cannot do. The two complimentary techniques, moving forward, will provide a much clearer picture of gravitational wave physics and the conditions in the universe that generate them.\n\nPhosphorous on Enceladus\n\nPlanetary science is the close cousin to the more distant field of cosmology, but being close to home also makes it more immediate. The search for life outside the Earth stands as one of the greatest scientific quests of our day. We are almost certainly not alone in the universe, and life may be as close as Enceladus, the icy moon of Saturn.\n\nScientists have been studying data from the Cassini spacecraft that observed Saturn close-up for over a decade from 2004 to 2017. Enceladus has a subsurface liquid ocean that generates plumes of tiny ice crystals that erupt like geysers from fissures in the solid surface. The ocean remains liquid because of internal tidal heating caused by the large gravitational forces of Saturn.\n\nThe Cassini spacecraft flew through the plumes and analyzed their content using its Cosmic Dust Analyzer. While the ice crystals from Enceladus were already known to contain organic compounds, the science team discovered that they also contain phosphorous. This is the least abundant element within the molecules of life, but it is absolutely essential, providing the backbone chemistry of DNA as well as being a constituent of amino acids.\n\nWith this discovery, all the essential building blocks of life are known to exist on Enceladus, along with a liquid ocean that is likely to be in chemical contact with rocky minerals on the ocean floor, possibly providing the kind of environment that could promote the emergence of life on a planet other than Earth.\n\nSimulating the Expanding Universe in a Bose-Einstein Condensate\n\nPutting the universe under a microscope in a laboratory may have seemed a foolish dream, until a group at the University of Heidelberg did just that. It isn’t possible to make a real universe in the laboratory, but by adjusting the properties of an ultra-cold collection of atoms known as a Bose-Einstein condensate, the research group was able to create a type of local space whose internal metric has a curvature, like curved space-time. Furthermore, by controlling the inter-atomic interactions of the condensate with a magnetic field, they could cause the condensate to expand or contract, mimicking different scenarios for the evolution of our own universe. By adjusting the type of expansion that occurs, the scientists could create hypotheses about the geometry of the universe and test them experimentally, something that could never be done in our own universe. This could lead to new insights into the behavior of the early universe and the formation of its large-scale structure.\n\nQuark Entanglement\n\nThis is the only breakthrough I picked that is not related to astrophysics (although even this effect may have played a role in the very early universe).\n\nEntanglement is one of the hottest topics in physics today (although the idea is 89 years old) because of the crucial role it plays in quantum information physics. The topic was awarded the 2022 Nobel Prize in Physics which went to John Clauser, Alain Aspect and Anton Zeilinger.\n\nDirect observations of entanglement have been mostly restricted to optics (where entangled photons are easily created and detected) or molecular and atomic physics as well as in the solid state.\n\nBut entanglement eluded high-energy physics (which is quantum matter personified) until 2023 when the Atlas Collaboration at the LHC (Large Hadron Collider) in Geneva posted a manuscript on Arxiv that reported the first observation of entanglement in the decay products of a quark.\n\nQuarks interact so strongly (literally through the strong force), that entangled quarks experience very rapid decoherence, and entanglement effects virtually disappear in their decay products. However, top quarks decay so rapidly, that their entanglement properties can be transferred to their decay products, producing measurable effects in the downstream detection. This is what the Atlas team detected.\n\nWhile this discovery won’t make quantum computers any better, it does open up a new perspective on high-energy particle interactions, and may even have contributed to the properties of the primordial soup during the Big Bang.\n\nSomething strange almost happened in 1840’s England just a few years into Queen Victoria’s long reign—a giant machine the size of a large shed, built of thousands of interlocking steel gears, driven by steam power, almost came to life—a thinking, mechanical automaton, the very image of Cyber Steampunk.\n\nCyber Steampunk is a genre of media that imagines an alternate history of a Victorian Age with advanced technology—airships and rockets and robots and especially computers—driven by steam power. Some of the classics that helped launch the genre are the animé movies Castle in the Sky (1986) by Hayao Miyazaki and Steam Boy (2004) by Katsuhiro Otomo and the novel The Difference Engine (1990) by William Gibson and Bruce Sterling. The novel pursues Ada Byron, Lady Lovelace, through the shadows of London by those who suspect she has devised a programmable machine that can win at gambling using steam and punched cards. This is not too far off from what might have happened in real life if Ada Lovelace had a bit more sway over one of her unsuitable suitors—Charles Babbage.\n\nBut Babbage, part genius, part fool, could not understand what Lovelace understood—for if he had, a Victorian computer built of oiled gears and leaky steam pipes, instead of tiny transistors and metallic leads, might have come a hundred years early as another marvel of the already marvelous Industrial Revolution. How might our world today be different if Babbage had seen what Lovelace saw?\n\nBoundless Babbage\n\nThere is no question of Babbage’s genius. He was so far ahead of his time that he appeared to most people in his day to be a crackpot, and he was often treated as one. His father thought he was useless, and he told him so, because to be a scientist in the early 1800’s was to be unemployable, and Babbage was unemployed for years after college. Science was, literally, natural philosophy, and no one hired a philosopher unless they were faculty at some college. But Babbage’s friends from Trinity College, Cambridge, like William Whewell (future dean of Trinity) and John Herschel (son of the famous astronomer), new his worth and were loyal throughout their lives and throughout his trials.\n\nCharles Babbage was a favorite at Georgian dinner parties because he was so entertaining to watch and to listen to. From personal letters of his friends (and enemies) of the time one gets a picture of a character not too different from Sheldon Cooper on the TV series The Big Bang Theory—convinced of his own genius and equally convinced of the lack of genius of everyone else and ready to tell them so. His mind was so analytic, that he talked like a walking computer—although nothing like a computer existed in those days—everything was logic and functions and propositions—hence his entertainment value. No one understood him, and no one cared—until he ran into a young woman who actually did, but more of that later.\n\nOne summer day in 1821, Babbage and Herschel were working on mathematical tables for the Astrophysical Society, a dull but important job to ensure that star charts and moon positions could be used accurately for astronomical calculations and navigation. The numbers filled column after column, page after page. But as they checked the values, the two were shocked by how many entries in the tables were wrong. In that day, every numerical value of every table or chart was calculated by a person (literally called a computer), and people make mistakes. Even as they went to correct the numbers, new mistakes would crop in. In frustration, Babbage exclaimed to Herschel that what they needed was a steam-powered machine that would calculate the numbers automatically. No sooner had he said it, than Babbage had a vision of a mechanical machine, driven by a small steam engine, full of gears and rods, that would print out the tables automatically without flaws.\n\nBeing unemployed (and unemployable) Babbage had enough time on his hands to actually start work on his engine. He called it the Difference Engine because it worked on the Method of Differences—mathematical formulas were put into a form where a number was expressed as a series, and the differences between each number in the series would be calculated by the engine. He approached the British government for funding, and it obliged with considerable funds. In the days before grant proposals and government funding, Babbage had managed to jump start his project and, in a sense, gain employment. His father was not impressed, but he did not live long enough to see what his son Charles could build. Charles inherited a large sum from his father (the equivalent of about 14 million dollars today), which further freed him to work on his Difference Engine. By 1832, he had finally completed a seventh part of the Engine and displayed it in his house for friends and visitors to see.\n\nThis working section of the Difference Engine can be seen today in the London Science Museum. It is a marvel of steel and brass, consisting of three columns of stacked gears whose enmeshed teeth represent digital numbers. As a crank handle is turned, the teath work upon each other, generating new numbers through the permutations of rotated gear teeth. Carrying tens was initially a problem for Babbage, as it is for school children today, but he designed an ingenious mechanical system to accomplish the carry.\n\nAll was going well, and the government was pleased with progress, until Charles had a better idea that threatened to scrap all he had achieved. It is not known how this new idea came into being, but it is known that it happened shortly after he met the amazing young woman: Ada Byron.\n\nLovely Lovelace\n\nAda Lovelace, born Ada Byron, had the awkward distinction of being the only legitimate child of Lord Byron, lyric genius and poet. Such was Lord Byron’s hedonist lifestyle that no-one can say for sure how many siblings Ada had, not even Lord Byron himself, which was even more awkward when his half-sister bore a bastard child that may have been his.\n\nAda’s high-born mother prudently divorced the wayward poet and was not about to have Ada pulled into her father’s morass. Where Lord Byron was bewitched (some would say possessed) by art and spirit, the mother sought an antidote, and she encouraged Ada to study hard cold mathematics. She could not have known that Ada too had a genius like her father’s, only aimed differently, bewitched by the beauty in the sublime symbols of math.\n\nAn insight into the precocious child’s way of thinking can be gained from a letter that the 12-year-old girl wrote to her mother who was off looking for miracle cures for imaginary ills. At that time in 1828, in a confluence of historical timelines in the history of mathematics, Ada and her mother (and Ada’s cat Puff) were living at Bifrons House which was the former estate of Brook Taylor, who had developed the Taylor’s series a hundred years earlier in 1715. In Ada’s letter, she describes a dream she had of a flying machine, which is not so remarkable, but then she outlined her plan to her mother to actually make one, which is remarkable. As you read her letter, you see she is already thinking about weights and material strengths and energy efficiencies, thinking like an engineer and designer—at the age of only 12 years!\n\nIn later years, Lovelace would become the Enchantress of Number to a number of her mathematical friends, one of whom was the strange man she met at a dinner party in the summer of 1833 when she was 17 years old. The strange man was Charles Babbage, and when he talked to her about his Difference Engine, expecting to be tolerated as an entertaining side show, she asked pertinent questions, one after another, and the two became locked in conversation.\n\nBabbage was a recent widower, having lost his wife with whom he had been happily compatible, and one can only imagine how he felt when the attractive and intelligent woman gave him her attention. But Ada’s mother would never see Charles as a suitable husband for her daughter—she had ambitious plans for her, and she tolerated Babbage only as much as she did because of the affection that Ada had for him. Nonetheless, Ada and Charles became very close as friends and met frequently and wrote long letters to each other, discussing problems and progress on the Difference Engine.\n\nIn December of 1834, Charles invited Lady Byron and Ada to his home where he described with great enthusiasm a vision he had of an even greater machine. He called it his Analytical Engine, and it would surpass his Difference Engine in a crucial way: where the Difference Engine needed to be reconfigured by hand before every new calculation, the Analytical Engine would never need to be touched, it just needed to be programmed with punched cards. Charles was in top form as he wove his narrative, and even Lady Byron was caught up in his enthusiasm. The effect on Ada, however, was nothing less than a religious conversion.\n\nAda’s Notes\n\nTo meet Babbage as an equal, Lovelace began to study mathematics with an obsession, or one might say, with delusions of grandeur. She wrote “I believe myself to possess a most singular combination of qualities exactly fitted to make me pre-eminently a discoverer of the hidden realities of nature,” and she was convinced that she was destined to do great things.\n\nThen, in 1835, Ada was married off to a rich but dull aristocrat who was elevated by royal decree to the Earldom of Lovelace, making her the Countess of Lovelace. The marriage had little effect on Charles’ and Ada’s relationship, and he was invited frequently to the new home where they continued their discussions about the Analytical Engine.\n\nBy this time Charles had informed the British government that he was putting all his effort into the design his new machine—news that was not received favorably since he had never delivered even a working Difference Engine. Just when he hoped to start work on his Analytical Engine, the government ministers pulled their money. This began a decade’s long ordeal for Babbage as he continued to try to get monetary support as well as professional recognition from his peers for his ideas. Neither attempt was successful at home in Britain, but he did receive interest abroad, especially from a future prime minister of Italy, Luigi Menabrae, who invited Babbage to give a lecture in Turin on his Analytical Engine. Menabrae later had the lecture notes published in French. When Charles Wheatstone, a friend of Babbage, learned of Menabrae’s publication, he suggested to Lovelace that she translate it into English. Menabrae’s publication was the only existing exposition of the Analytical Engine, because Babbage had never written on the Engine himself, and Wheatstone was well aware of Lovelace’s talents, expecting her to be one of the only people in England who had the ability and the connections to Babbage to accomplish the task.\n\nAda Lovelace dove into the translation of Menabrae’s “Sketch of the Analytical Engine Invented by Charles Babbage” with the single-mindedness that she was known for. Along with the translation, she expanded on the work with Notes of her own that she added, lettered from A to G. By the time she wrote them, Lovelace had become a top-rate mathematician, possibly surpassing even Babbage, and her Notes were three times longer than the translation itself, providing specific technical details and mathematical examples that Babbage and Menabrae only allude to.\n\nOn a different level, the character of Ada’s Notes stands in stark contrast to Charles’ exposition as captured by Menabrae: where Menabrae provided only technical details of Babbage’s Engine, Lovelace’s Notes captured the Engine’s potential. She was still a poet by disposition—that inheritance from her father was never lost.\n\nLovelace wrote:\n\nWe may say most aptly, that the Analytical Engine weaves algebraic patterns just as the Jacquard-loom weaves flowers and leaves.\n\nHere she is referring to the punched cards that the Jacquard loom used to program the weaving of intricate patterns into cloth. Babbage had explicitly borrowed this function from Jacquard, adapting it to provide the programmed input to his Analytical Engine.\n\nBut it was not all poetics. She also saw the abstract capabilities of the Engine, writing\n\nIn studying the action of the Analytical Engine, we find that the peculiar and independent nature of the considerations which in all mathematical analysis belong to operations, as distinguished from the objects operated upon and from the results of the operations performed upon those objects, is very strikingly defined and separated.\n\nAgain, it might act upon other things besides number, where objects found whose mutual fundamental relations could be expressed by those of the abstract science of operations, and which should be also susceptible of adaptations to the action of the operating notation and mechanism of the engine.\n\nSupposing, for instance, that the fundamental relations of pitched sounds in the science of harmony and of musical composition were susceptible of such expression and adaptations, the engine might compose elaborate and scientific pieces of music of any degree of complexity or extent.\n\nHere she anticipates computers generating musical scores.\n\nMost striking is Note G. This is where she explicitly describes how the Engine would be used to compute numerical values as solutions to complicated problems. She chose, as her own example, the calculation of Bernoulli numbers which require extensive numerical calculations that were exceptionally challenging even for the best human computers of the day. In Note G, Lovelace writes down the step-by-step process by which the Engine would be programmed by the Jacquard cards to carry out the calculations. In the history of computer science, this stands as the first computer program.\n\nWhen it was time to publish, Babbage read over Lovelace’s notes, checking for accuracy, but he appears to have been uninterested in her speculations, possibly simply glossing over them. He saw his engine as a calculating machine for practical applications. She saw it for what we know today to be the exceptional adaptability of computers to all realms of human study and activity. He did not see what she saw. He was consumed by his Engine to the same degree as she, but where she yearned for the extraordinary, he sought funding for the mundane costs of machining and materials.\n\nAda’s Business Plan Pitch\n\nAda Lovelace watched in exasperation as Babbage floundered about with ill-considered proposals to the government while making no real progress towards a working Analytical Engine. Because of her vision into the potential of the Engine, a vision that struck her to her core, and seeing a prime opportunity to satisfy her own yearning to make an indelible mark on the world, she despaired in ever seeing it brought to fruition. Charles, despite his genius, was too impractical, wasting too much time on dead ends and incapable of performing the deft political dances needed to attract support. She, on the other hand, saw the project clearly and had the time and money and the talent, both mathematically and through her social skills, to help.\n\nOn Monday August 14, 1843, Ada wrote what might be the most heart-felt and impassioned business proposition in the history of computing. She laid out in clear terms to Charles how she could advance the Analytical Engine to completion if only he would surrender to her the day-to-day authority to make it happen. She was, in essence, proposing to be the Chief Operating Officer in a disruptive business endeavor that would revolutionize thinking machines a hundred years before their time. She wrote (she liked to underline a lot):\n\n“Firstly: I want to know whether if I continue to work on & about your own great subject, you will undertake to abide wholly by the judgment of myself (or of any persons whom you may now please to name as referees, whenever we may differ), on all practical matters relating to whatever can involve relations with any fellow-creature or fellow-creatures.\n\nSecondly: can you undertake to give your mind wholly & undividedly, as a primary object that no engagement is to interfere with, to the consideration of all those matters in which I shall at times require your intellectual assistance & supervision; & can you promise not to slur & hurry things over; or to mislay, & allow confusion and mistakes to enter into documents, &c?\n\nThirdly: if I am able to lay before you in the course of a year or two, explicit & honorable propositions for executing your engine, (such as are approved by persons whom you may now name to be referred to for their approbation), would there be any chance of your allowing myself & such parties to conduct the business for you; your own undivided energies being devoted to the execution of the work; & all other matters being arranged for you on terms which your own friends should approve?“\n\nThis is a remarkable letter from a self-possessed 28-year-old woman, laying out in explicit terms how she proposed to take on the direction of the project, shielding Babbage from the problems of relating to other people or “fellow-creatures” (which was his particular weakness), giving him time to focus his undivided attention on the technical details (which was his particular strength), while she would be the outward face of the project that would attract the appropriate funding.\n\nIn her preface to her letter, Ada adroitly acknowledges that she had been a romantic disappointment to Charles, but she pleads with him not to let their personal history cloud his response to her proposal. She also points out that her keen intellect would be an asset to the project and asks that he not dismiss it because of her sex (which a biased Victorian male would likely do). Despite her entreaties, this is exactly what Babbage did. Pencilled on the top of the original version of Ada’s letter in the Babbage archives is his simple note: “Tuesday 15 saw AAL this morning and refused all the conditions”. He had not even given her proposal 24 hours consideration as he indeed slurred and hurried things over.\n\nAftermath\n\nBabbage never constructed his Analytical Engine and never even wrote anything about it. All his efforts would have been lost to history if Alan Turing had not picked up on Ada’s Notes and expanded upon them a hundred years later, bringing both her and him to the attention of the nascent computing community.\n\nAda Lovelace died young in 1852, at the age of 36, of cancer. By then she had moved on from Babbage and was working on other things. But she never was able to realize her ambition of uncovering such secrets of nature as to change the world.\n\nAda had felt from an early age that she was destined for greatness. She never achieved it in her lifetime and one can only wonder what she thought about this as she faced her death. Did she achieve it in posterity? This is a hotly debated question. Some say she wrote the first computer program, which may be true, but little programming a hundred years later derived directly from her work. She did not affect the trajectory of computing history. Discovering her work after the fact is interesting, but cannot be given causal weight in the history of science. The Vikings were the first Europeans to discover America, but no-one knew about it. They did not affect subsequent history the way that Columbus did.\n\nOn the other hand, Ada has achieved greatness in a different way. Now that her story is known, she stands as an exemplar of what scientific and technical opportunities look like, and the risk of ignoring them. Babbage also did not achieve greatness during his lifetime, but he could have—if he had not dismissed her and her intellect. He went to his grave embittered rather than lauded because he passed up an opportunity he never recognized.\n\nBy David D. Nolte, June 26, 2023\n\nReferences\n\n[1] Facsimile of “Sketch of the Analytical Engine Invented by Charles Babbage” translated by Ada Lovelace from Harvard University.\n\n[2] Facsimile of Ada Lovelace’s “Notes by the Translator“.\n\n[3] Stephen Wolfram, “Untangling the Tale of Ada Lovelace“, Wolfram Writings (2015).\n\n[4] J. Essinger, “Charles and Ada : The computer’s most passionate partnership,” (History Press, 2019).\n\n[5] D. Swade, The Difference Engine: Charles Babbage and the quest to build the first computer (Penguin Books, 2002).\n\n[6] W. Gibson, and B. Sterling, The Difference Engine (Bantam Books, 1992).\n\n[7] L. J. Snyder, The Philosophical Breakfast Club : Four remarkable friends who transformed science and changed the world (Broadway Books, 2011).\n\n[8] Allan G. Bromley, Charles Babbage’s Analytical Engine, 1838, Annals of the History of Computing, Volume 4, Number 3, July 1982, pp. 196 – 217\n\nMark Twain once famously wrote in a letter from London to a New York newspaper editor:\n\n“I have … heard on good authority that I was dead [but] the report of my death was an exaggeration.”\n\nThe same may be true of recent reports on the grave illness and possible impending death of human culture at the hands of ChatGPT and other so-called Large Language Models (LLM). It is argued that these algorithms have such sophisticated access to the bulk of human knowledge, and can write with apparent authority on virtually any topic, that no-one needs to learn or create anything new. It can all be recycled—the end of human culture!\n\nWhile there may be a kernel of truth to these reports, they are premature. ChatGPT is just the latest in a continuing string of advances that have disrupted human life and human culture ever since the invention of the steam engine. We—humans, that is—weathered the steam engine in the short term and are just as likely to weather the LLM’s.\n\nChatGPT: What is it?\n\nFor all the hype, ChatGPT is mainly just a very sophisticated statistical language model (SLM).\n\nTo start with a very simple example of SLM, imagine you are playing a word scramble game and have the letter “Q”. You can be pretty certain that the “Q“ will be followed by a “U” to make “QU”. Or if you have the initial pair “TH” there is a very high probability that it will be followed by a vowel as “THA…”, “THE…”, ”THI…”, “THO..” or “THU…” and possibly with an “R” as “THR…”. This almost exhausts the probabilities. This is all determined by the statistical properties of English.\n\nStatistical language models build probability distributions for the likelihood that some sequence of letters will be followed by another sequence of letters, or a sequence of words (and punctuations) will be followed by another sequence of words. The bigger the chains of letters and words, the number of possible permutations grows exponentially. This is why SLMs usually stop at some moderate order of statistics. If you build sentences from such a model, it sounds OK for a sentence or two, but then it just drifts around like it’s dreaming or hallucinating in a stream of consciousness without any coherence.\n\nChatGPT works in much the same way. It just extends the length of the sequences where it sounds coherent up to a paragraph or two. In this sense, it is no more “intelligent” than the SLM that follows “Q” with “U”. ChatGPT simply sustains the charade longer.\n\nNow the details of how ChatGPT accomplishes this charade is nothing less than revolutionary. The acronym GPT means Generative Pre-Trained Transformer. Transformers were a new type of neural net architecture invented in 2017 by the Google Brain team. Transformers removed the need to feed sentences word-by-word into a neural net, instead allowing whole sentences and even whole paragraphs to be input in parallel. Then, by feeding the transformers on more than a Terabyte of textual data from the web, they absorbed the vast output of virtually all the crowd-sourced information from the past 20 years. (This what transformed the model from an SLM to an LLM.) Finally, using humans to provide scores on what good answers looked like versus bad answers, ChatGPT was supervised to provide human-like responses. The result is a chatbot that in any practical sense passes the Turing Test—if you query it for an extended period of time, you would be hard pressed to decide if it was a computer program or a human giving you the answers. But Turing Tests are boring and not very useful.\n\nThe true value of ChatGPT is the access it has to that vast wealth of information (note it is information and not knowledge). Give it almost any moderately technical query, and it will provide a coherent summary for you—on amazingly esoteric topics—because almost every esoteric topic has found its way onto the net by now, and ChatGPT can find it.\n\nAs a form of search engine, this is tremendous! Think how frustrating it has always been searching the web for something specific. Furthermore, the lengthened coherence made possible by the transformer neural net means that a first query that leads to an unsatisfactory answer from the chatbot can be refined, and ChatGPT will find a “better” response, conditioned by the statistics of its first response that was not optimal. In a feedback cycle, with the user in the loop, very specific information can be isolated.\n\nOr, imagine that you are not a strong writer, or don’t know the English language as well as you would like. But entering your own text, you can ask ChatGPT to do a copy-edit, even rephrasing your writing where necessary, because ChatGPT above all else has an unequaled command of the structure of English.\n\nOr, for customer service, instead of the frustratingly discrete menu of 5 or 10 potted topics, ChatGPT with a voice synthesizer could respond to continuously finely graded nuances of the customer’s problem—not with any understanding or intelligence, but with probabilistic likelihoods of what the solutions are for a broad range of possible customer problems.\n\nIn the midst of all the hype surrounding ChatGPT, it is important to keep in mind two things: First, we are witnessing the beginning of a revolution and a disruptive technology that will change how we live. Second, it is still very early days, just like the early days of the first steam engines running on coal.\n\nDisruptive Technology\n\nDisruptive technologies are the coin of the high-tech realm of Silicon Valley. But this is nothing new. There have always been disruptive technologies—all the way back to Thomas Newcomen and James Watt and the steam engines they developed between 1712 and 1776 in England. At first, steam engines were so crude they were used only to drain water from mines, increasing the number jobs in and around the copper and tin mines of Cornwall (viz. the popular BBC series Poldark) and the coal mines of northern England. But over the next 50 years, steam engines improved, and they became the power source for textile factories that displaced the cottage industry of spinning and weaving that had sustained marginal farms for centuries before.\n\nThere is a pattern to a disruptive technology. It not only disrupts an existing economic model, but it displaces human workers. Once-plentiful jobs in an economic sector can vanish quickly after the introduction of the new technology. The change can happen so fast, that there is not enough time for the workforce to adapt, followed by human misery in some sectors. Yet other, newer, sectors always flourish, with new jobs, new opportunities, and new wealth. The displaced workers often never see these benefits because they lack skills for the new jobs.\n\nThe same is likely true for the LLMs and the new market models they will launch. There will be a wealth of new jobs curating and editing LLM outputs. There will also be new jobs in the generation of annotated data and in the technical fields surrounding the support of LLMs. LLMs are incredibly hungry for high-quality annotated data in a form best provided by humans. Jobs unlikely to be at risk, despite prophesies of doom, include teachers who can use ChatGPT as an aide by providing appropriate context to its answers. Conversely, jobs that require a human to assemble information will likely disappear, such as news aggregators. The same will be true of jobs in which effort is repeated, or which follow a set of patterns, such as some computer coding jobs or data analysts. Customer service positions will continue to erode, as will library services. Media jobs are at risk, as well as technical writing. The writing of legal briefs may be taken over by LLMs, along with market and financial analysts. By some estimates, there are 300 million jobs around the world that will be impacted one way or another by the coming spectrum of LLMs.\n\nThis pattern of disruption is so set and so clear and so consistent, that forward-looking politicians or city and state planners could plan ahead, because we have been on a path of continuing waves disruption for over two hundred years.\n\nWaves of Disruption\n\nIn the history of technology, it is common to describe a series of revolutions as if they were distinct. The list looks something like this:\n\nFirst: Power (The Industrial Revolution: 1760 – 1840)\n\nSecond: Electricity and Connectivity (Technological Revolution: 1860 – 1920)\n\nThird: Automation, Information, Cybernetics (Digital Revolution: 1950 – )\n\nFourth: Intelligence, cyber-physical (Imagination Revolution: 2010 – )\n\nThe first revolution revolved around steam power fueled by coal, radically increasing output of goods. The second revolution shifted to electrical technologies, including communication networks through telegraph and the telephones. The third revolution focused on automation and digital information.\n\nYet this discrete list belies an underlying fact: There is, and has been, only one continuous Industrial Revolution punctuated by waves.\n\nThe Age of Industrial Revolutions began around 1760 with the invention of the spinning jenny by James Hargreaves—and that Age has continued, almost without pause, up to today and will go beyond. Each disruptive technology has displaced the last. Each newly trained workforce has been displaced by the last. The waves keep coming.\n\nNote that the fourth wave is happening now, as artificial intelligence matures. This is ironic, because this latest wave of the Industrial Revolution is referred to as the “Imagination Revolution” by the optimists who believe that we are moving into a period where human creativity is unleashed by the unlimited resources of human connectivity across the web. Yet this moment of human ascension to the heights of creativity is happening at just the moment when LLM’s are threatening to remove the need to create anything new.\n\nSo is it the end of human culture? Will all knowledge now just be recycled with nothing new added?\n\nA Post-Human Future?\n\nThe limitations of the generative aspects of ChatGPT might be best visualized by using an image-based generative algorithm that has also gotten a lot of attention lately. This is the ability to input a photograph, and input a Van Gogh painting, and create a new painting of the photograph in the style of Van Gogh.\n\nIn this example, the output on the right looks like a Van Gogh painting. It is even recognizable as a Van Gogh. But in fact it is a parody. Van Gogh consciously created something never before seen by humans.\n\nEven if an algorithm can create “new” art, it is a type of “found” art, like a picturesque stone formation or a sunset. The beauty becomes real only in the response it elicits in the human viewer. Art and beauty do not exist by themselves; they only exist in relationship to the internal state of the conscious observer, like a text or symbol signifying to an interpreter. The interpreter is human, even if the artist is not.\n\nChatGPT, or any LLM like Google’s Bard, can generate original text, but its value only resides in the human response to it. The human interpreter can actually add value to the LLM text by “finding” sections that are interesting or new, or that inspire new thoughts in the interpreter. The interpreter can also “edit” the text, to bring it in line with their aesthetic values. This way, the LLM becomes a tool for discovery. It cannot “discover” anything on its own, but it can present information to a human interpreter who can mold it into something that they recognize as new. From a semiotic perspective, the LLM can create the signifier, but the signified is only made real by the Human interpreter—emphasize Human.\n\nTherefore, ChatGPT and the LLMs become part of the Fourth Wave of the human Industrial Revolution rather than replacing it.\n\nWe are moving into an exciting time in the history of technology, giving us a rare opportunity to watch as the newest wave of revolution takes shape before our very eyes. That said … just as the long-term consequences of the steam engine are only now coming home to roost two hundred years later in the form of threats to our global climate, the effect of ChatGPT in the long run may be hard to divine until far in the future—and then, maybe after it’s too late, so a little caution now would be prudent.\n\nResources\n\nOpenAI ChatGPT: https://openai.com/blog/chatgpt/\n\nTraining GPT with human input: https://arxiv.org/pdf/2203.02155.pdf\n\nGenerative art: https://github.com/Adi-iitd/AI-Art\n\nStatus of Large Language Models: https://www.tasq.ai/blog/large-language-models/\n\nLLMs at Google: https://blog.google/technology/ai/bard-google-ai-search-updates/\n\nHow Transformers work: https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452\n\nThe start of the Transformer: https://arxiv.org/abs/1706.03762\n\nPhysics forged ahead in 2022, making a wide range of advances. From a telescope far out in space to a telescope that spans the size of the Earth, from solid state physics and quantum computing at ultra-low temperatures to particle and nuclear physics at ultra-high energies, the year saw a number of firsts. Here’s a list of eight discoveries of 2022 that define the frontiers of physics.\n\nJames Webb Space Telescope\n\n“First Light” has two meanings: the “First Light” that originated at the beginning of the universe, and the “First Light” that is collected by a new telescope. In the beginning of this year, the the James Webb Space Telescope (JWST) saw both types of first light, and with it came first surprises.\n\nThe JWST has found that galaxies are too well formed too early in the universe relative to current models of galaxy formation. Almost as soon as the JWST began forming images, it acquired evidence of massive galaxies from only a few hundred million years old. Existing theories of galaxy formation did not predict such large galaxies so soon after the Big Bang.\n\nAnother surprise came from images of the Southern Ring Nebula. While the Hubble did not find anything unusual about this planetary nebula, the JWST found cold dust surrounding the white dwarf that remained after the explosion of the supernova. This dust was not supposed to be there, but it may be coming from a third member of the intra-nebular environment. In addition, the ring-shaped nebula contained masses of swirling streams and ripples that are challenging astrophysicists who study supernova and nebula formation to refine their current models.\n\nQuantum Machine Learning\n\nMachine learning—the training of computers to identify and manipulate complicated patterns within massive data—has been on a roll in recent years, ever since efficient training algorithms were developed in the early 2000’s for large multilayer neural networks. Classical machine learning can take billions of bits of data and condense it down to understandable information in a matter of minutes. However, there are types of problems that even conventional machine learning might take the age of the universe to calculate, for instance calculating the properties of quantum systems based on a set of quantum measurements of the system.\n\nIn June of 2022, researchers at Caltech and Google announced that a quantum computer—Google’s Sycamore quantum computer—could calculate properties of quantum systems using exponentially fewer measurements than would be required to perform the same task using conventional computers. Quantum machine learning uses the resource of quantum entanglement that is not available to conventional machine learning, enabling new types of algorithms that can exponentially speed up calculations of quantum systems. It may come as no surprise that quantum computers are ideally suited to making calculations of quantum systems.\n\nA Possible Heavy W Boson\n\nHigh-energy particle physics has been in a crisis ever since 2012 when they reached the pinnacle of a dogged half-century search for the fundamental constituents of the universe. The Higgs boson was the crowning achievement, and was supposed to be the vanguard of a new frontier of physics uncovered by CERN. But little new physics has emerged, even though fundamental physics is in dire need of new results. For instance, dark matter and dark energy remain unsolved mysteries despite making up the vast majority of all there is. Therefore, when physicists at Fermilab announced that the W boson, a particle that carries the nuclear weak interaction, was heavier than predicted by the Standard Model, some physicists heaved sighs of relief. The excess mass could signal higher-energy contributions that might lead to new particles or interactions … if the excess weight holds up under continued scrutiny.\n\nImaging the Black Hole at the Center of the Milky Way\n\nImagine building a telescope the size of the Earth. What could it see?\n\nIf it detected in the optical regime, it could see a baseball on the surface of the Moon. If it detected at microwave frequencies, then it could see the material swirling around distant black holes. This is what the Event Horizon Telescope (EHT) can do. In 2019, it revealed the first image of a black hole: the super-massive black hole at the core of the M87 galaxy 53 million light years away. They did this Herculean feat by combining the signals of microwave telescopes from across the globe, combining their signals interferometrically to create an effective telescope aperture that was the size of the Earth.\n\nThe next obvious candidate was the black hole at the center of our own galaxy, the Milky Way. Even though our own black hole is much smaller than the one in M87, ours is much closer, and both subtend about the same solid angle. The challenge was observing it through the swirling stars and dust at the core of our galaxy. In May of this year, the EHT unveiled the first image of our own black hole, showing the radiation emitted by the in-falling material.\n\nTetraneutrons\n\nNuclear physics is a venerable part of modern physics that harkens back to the days of Bohr and Rutherford and the beginning of quantum physics, but in recent years it has yielded few new surprises (except at the RHIC collider which smashes heavy nuclei against each other to create quark-gluon plasma). That changed in June of 2022, when researchers in Germany announced the successful measurement of a tetraneutron–a cluster of four neutrons bound transiently together by the strong nuclear force.\n\nNeutrons are the super-glue that holds together the nucleons in standard nuclei. The force is immense, strong enough to counteract the Coulomb repulsion of protons in a nucleus. For instance, Uranium 238 has 92 protons crammed within a volume of about 10 femtometer radius. It takes 146 neutrons to bind these together without flying apart. But neutrons don’t tend to bind to themselves, except in “resonance” states that decay rapidly. In 2012, a dineutron (two neutrons bound in a transient resonance state) was observed, but four neutrons were expected to produce an even more transient resonance (a three-neutron state is not allowed). When the German group created the tetraneutron, it had a lifetime of only about 1×10-21 seconds, so it is extremely ephemeral. Nonetheless, studying the properties of the tetraneutron may give insights into both the strong and weak nuclear forces.\n\nHi-Tc superconductivity\n\nWhen Bednorz and Müller discovered Hi-Tc superconductivity in 1986, it set off both a boom and a crisis. The boom was the opportunity to raise the critical temperature of superconductivity from 23 K that had been the world record held by Nb3Ge for 13 years since it was set in 1973. The crisis was that the new Hi-Tc materials violated the established theory of superconductivity explained by Bardeen-Cooper-Schrieffer (BCS). There was almost nothing in the theory of solid state physics that could explain how such high critical temperatures could be attained. At the March Meeting of the APS the following year in 1987, the session on the new Hi-Tc materials and possible new theories became known as the Woodstock of Physics, where physicists camped out in the hallway straining their ears to hear the latest ideas on the subject.\n\nOne of the ideas put forward at the session was the idea of superexchange by Phil Anderson. The superexchange of two electrons is related to their ability to hop from one lattice site to another. If the hops are coordinated, then there can be an overall reduction in their energy, creating a ground state of long-range coordinated electron hopping that could support superconductivity. Anderson was perhaps the physicist best situated to suggest this theory because of his close familiarity with what was, even then, known as the Anderson Hamiltonian that explicitly describes the role of hopping in solid-state many-body phenomena.\n\nEver since, the idea of superexchange has been floating around the field of Hi-Tc superconductivity, but no one had been able to pin it down conclusively, until now. In a paper published in the PNAS in September of 2022, an experimental group at Oxford presented direct observations of the spatial density of Cooper pairs in relation to the spatial hopping rates—where hopping was easiest then the Cooper pair density was highest, and vice versa. This experiment provides almost indisputable evidence in favor of Anderson’s superexchange mechanism for Cooper pair formation in the Hi-Tc materials, laying to rest the crisis launched 36 years ago.\n\nHolographic Wormhole\n\nThe holographic principle of cosmology proposes that our three-dimensional physical reality—stars, galaxies, expanding universe—is like the projection of information encoded on a two-dimensional boundary—just as a two-dimensional optical hologram can be illuminated to recreate a three-dimensional visual representation. This 2D to 3D projection was first proposed by Gerald t’Hooft, inspired by the black hole information paradox in which the entropy of a black hole scales as surface area of the black hole instead of its volume. The holographic principle was expanded by Leonard Susskind in 1995 based on string theory and is one path to reconciling quantum physics with the physics of gravitation in a theory of quantum gravity—one of the Holy Grails of physics.\n\nWhile it is an elegant cosmic idea, the holographic principle could not be viewed as anything down to Earth, until now. In November 2022 a research group at Caltech published a paper in Nature describing how they used Google’s Sycamore quantum computer (housed at UC Santa Barbara) to manipulate a set of qubits into creating a laboratory-based analog of a Einstein-Rosen bridge, also known as a “wormhole”, through spacetime. The ability to use quantum information states to simulate a highly-warped spacetime analog provides the first experimental evidence for the validity of the cosmological holographic principle. Although the simulation did not produce a physical wormhole in our spacetime, it showed how quantum information and differential geometry (the mathematics of general relativity) can be connected.\n\nOne of the most important consequences of this work is the proposal that ER = EPR (Einstein-Rosen = Einstein-Podolsky-Rosen). The EPR paradox of quantum entanglement has long been viewed as a fundamental paradox of physics that requires instantaneous non-local correlations among quantum particles that can be arbitrarily far apart. Although EPR violates local realism, it is a valuable real-world resource for quantum teleportation. By demonstrating the holographic wormhole, the recent Caltech results show how quantum teleportation and gravitational wormholes may arise from the same physics.\n\nNet-Positive-Energy from Nuclear Fusion\n\nEver since nuclear fission was harnessed to generate energy, the idea of tapping the even greater potential of nuclear fusion to power the world has been a dream of nuclear physicists. Nuclear fusion energy would be clean and green and could help us avoid the long-run disaster of global warming. However, achieving that dream has been surprisingly frustrating. While nuclear fission was harnessed for energy (and weapons) within only a few years of discovery, and a fusion “boost” was added to nuclear destructive power in the so-called hydrogen bomb, sustained energy production from fusion has remained elusive.\n\nIn December of 2022, the National Ignition Facility (NIF) focussed the power of 192 pulsed lasers onto a deuterium-tritium pellet, causing it to implode, and the nuclei to fuse, releasing about 50% more energy that it absorbed. This was the first time that controlled fusion released net positive energy—about 3 million Joules out from 2 million Joules in—enough energy to boil about 3 liters of water. This accomplishment represents a major milestone in the history of physics and could one day provide useful energy. The annual budget of the NIF is about 300 million dollars, so there is a long road ahead (probably several more decades) before this energy source can be scaled down to an economical level.\n\nBy David D. Nolte Jan. 16, 2023\n\nWhen our son was ten years old, he came home from a town fair in Battleground, Indiana, with an unwanted pet—a goldfish in a plastic bag. The family rushed out to buy a fish bowl and food and plopped the golden-red animal into it. In three days, it was dead!\n\nIt turns out that you can’t just put a gold fish in a fish bowl. When it metabolizes its food and expels its waste, it builds up toxic levels of ammonia unless you add filters or plants or treat the water with chemicals. In the end, the goldfish died because it was asphyxiated by its own pee.\n\nIt’s a basic rule—don’t pee in your own fish bowl.\n\nThe same can be said for humans living on the surface of our planet. Polluting the atmosphere with our wastes cannot be a good idea. In the end it will kill us. The atmosphere may look vast—the fish bowl was a big one—but it is shocking how thin it is.\n\nTurn on your Apple TV, click on the screen saver, and you are skimming over our planet on the dark side of the Earth. Then you see a thin blue line extending over the limb of the dark disc. Hold! That thin blue line! That is our atmosphere! Is it really so thin?\n\nWhen you look upwards on a clear sunny day, the atmosphere seems like it goes on forever. It doesn’t. It is a thin veneer on the surface of the Earth barely one percent of the Earth’s radius. The Earth’s atmosphere is frighteningly thin.\n\nConsider Mars. It’s half the size of Earth, yet it cannot hold on to an atmosphere even 1/100th the thickness of ours. When Mars first formed, it had an atmosphere not unlike our own, but through the eons its atmosphere has wafted away irretrievably into space.\n\nAn atmosphere is a precious fragile thing for a planet. It gives life and it gives protection. It separates us from the deathly cold of space, holding heat like a blanket. That heat has served us well over the eons, allowing water to stay liquid and allowing life to arise on Earth. But too much of a good thing is not a good thing.\n\nCommon Sense\n\nIf the fluid you are bathed in gives you life, then don’t mess with it. Don’t run your car in the garage while you are working in it. Don’t use a charcoal stove in an enclosed space. Don’t dump carbon dioxide into the atmosphere because it also is an enclosed space.\n\nAt the end of winter, as the warm spring days get warmer, you take the winter blanket off your bed because blankets hold in heat. The thicker the blanket, the more heat it holds in. Common sense tells you to reduce the thickness of the blanket if you don’t want to get too warm. Carbon dioxide in the atmosphere acts like a blanket. If we don’t want the Earth to get too warm, then we need to limit the thickness of the blanket.\n\nWithout getting into the details of any climate change model, common sense already tells us what we should do. Keep the atmosphere clean and stable (Don’t’ pee in our fishbowl) and limit the amount of carbon dioxide we put into it (Don’t let the blanket get too thick).\n\nSome Atmospheric Facts\n\nHere are some facts about the atmosphere, about the effect humans have on it, and about the climate:\n\nFact 1. Humans have increased the amount of carbon dioxide in the atmosphere by 45% since 1850 (the beginning of the industrial age) and by 30% since just 1960.\n\nFact 2. Carbon dioxide in the atmosphere prevents some of the heat absorbed from the Sun to re-radiate out to space. More carbon dioxide stores more heat.\n\nFact 3. Heat added to the Earth’s atmosphere increases its temperature. This is a law of physics.\n\nFact 4. The Earth’s average temperature has risen by 1.2 degrees Celsius since 1850 and 0.8 degrees of that has been just since 1960, so the effect is accelerating.\n\nThese facts are indisputable. They hold true regardless of whether there is a Republican or a Democrat in the White House or in control of Congress.\n\nThere is another interesting observation which is not so direct, but may hold a harbinger for the distant future: The last time the Earth was 3 degrees Celsius warmer than it is today was during the Pliocene when the sea level was tens of meters higher. If that sea level were to occur today, all of Delaware, most of Florida, half of Louisiana and the entire east coast of the US would be under water, including Houston, Miami, New Orleans, Philadelphia and New York City. There are many reasons why this may not be an immediate worry. The distribution of water and ice now is different than in the Pliocene, and the effect of warming on the ice sheets and water levels could take centuries. Within this century, the amount of sea level rise is likely to be only about 1 meter, but accelerating after that.\n\nBalance and Feedback\n\nIt is relatively easy to create a “rule-of-thumb” model for the Earth’s climate (see Ref. [2]). This model is not accurate, but it qualitatively captures the basic effects of climate change and is a good way to get an intuitive feeling for how the Earth responds to changes, like changes in CO2 or to the amount of ice cover. It can also provide semi-quantitative results, so that relative importance of various processes or perturbations can be understood.\n\nThe model is a simple energy balance statement: In equilibrium, as much energy flows into the Earth system as out.\n\nThis statement is both simple and immediately understandable. But then the work starts as we need to pin down how much energy is flowing in and how much is flowing out. The energy flowing in comes from the sun, and the energy flowing out comes from thermal radiation into space.\n\nWe also need to separate the Earth system into two components: the surface and the atmosphere. These are two very different things that have two different average temperatures. In addition, the atmosphere transmits sunlight to the surface, unless clouds reflect it back into space. And the Earth radiates thermally into space, unless clouds or carbon dioxide layers reflect it back to the surface.\n\nThe energy fluxes are shown in the diagram in Fig. 3 for the 4-component system: Sun, Surface, Atmosphere, and Space. The light from the sun, mostly in the visible range of the spectrum, is partially absorbed by the atmosphere and partially transmitted and reflected. The transmitted portion is partially absorbed and partially reflected by the surface. The heat of the Earth is radiated at long wavelengths to the atmosphere, where it is partially transmitted out into space, but also partially reflected by the fraction a’a which is the blanket effect. In addition, the atmosphere itself radiates in equal parts to the surface and into outer space. On top of all of these radiative processes, there is also non-radiative convective interaction between the atmosphere and the surface.\n\nThese processes are captured by two energy flux equations, one for the atmosphere and one for the surface, in Fig. 4. The individual contributions from Fig. 3 are annotated in each case. In equilibrium, each flux equals zero, which can then be used to solve for the two unknowns: Ts0 and Ta0: the surface and atmosphere temperatures.\n\nAfter the equilibrium temperatures Ts0 and Ta0 are found, they go into a set of dynamic response equations that governs how deviations in the temperatures relax back to the equilibrium values. These relaxation equations are\n\nwhere ks and ka are the relaxation rates for the surface and atmosphere. These can be quite slow, in the range of a century. For illustration, we can take ks = 1/75 years and ka = 1/25 years. The equilibrium temperatures for the surface and atmosphere differ by about 50 degrees Celsius, with Ts = 289 K and Ta = 248 K. These are rough averages over the entire planet. The solar constant is S = 1.36×103 W/m2, the Stefan-Boltzman constant is σ = 5.67×10-8 W/m2/K4, and the convective interaction constant is c = 2.5 W m-2 K-1. Other parameters are given in Table I.\n\nThe relaxation equations are in the standard form of a mathematical “flow” (see Ref. [1]) and the solutions are plotted as a phase-space portrait in Fig. 5 as a video of the flow as the parameters in Table I shift because of the addition of greenhouse gases to the atmosphere. The video runs from the year 1850 (the dawn of the industrial age) through to the year 2060 about 40 years from now.\n\nThe scariest part of the video is how fast it accelerates. From 1850 to 1950 there is almost no change, but then it accelerates, faster and faster, reflecting the time-lag in temperature rise in response to increased greenhouse gases.\n\nWhat if the Models are Wrong? Russian Roulette\n\nNow come the caveats.\n\nThis model is just for teaching purposes, not for any realistic modeling of climate change. It captures the basic physics, and it provides a semi-quantitative set of parameters that leads to roughly accurate current temperatures. But of course, the biggest elephant in the room is that it averages over the entire planet, which is a very crude approximation.\n\nIt does get the basic facts correct, though, showing an alarming trend in the rise in average temperatures with the temperature rising by 3 degrees by 2060.\n\nThe professionals in this business have computer models that are orders of magnitude more more accurate than this one. To understand the details of the real climate models, one needs to go to appropriate resources, like this NOAA link, this NASA link, this national climate assessment link, and this government portal link, among many others.\n\nOne of the frequent questions that is asked is: What if these models are wrong? What if global warming isn’t as bad as these models say? The answer is simple: If they are wrong, then the worst case is that life goes on. If they are right, then in the worst case life on this planet may end.\n\nIt’s like playing Russian Roulette. If just one of the cylinders on the revolver has a live bullet, do you want to pull the trigger?\n\nMatlab Code\n\nfunction flowatmos.m mov_flag = 1; if mov_flag == 1 moviename = 'atmostmp'; aviobj = VideoWriter(moviename,'MPEG-4'); aviobj.FrameRate = 12; open(aviobj); end Solar = 1.36e3; % Solar constant outside atmosphere [J/sec/m2] sig = 5.67e-8; % Stefan-Boltzman constant [W/m2/K4] % 1st-order model of Earth + Atmosphere ta = 0.53; % (0.53)transmissivity of air tpa0 = 0.06; % (0.06)primes are for thermal radiation as0 = 0.11; % (0.11) aa0 = 0.30; % (0.30) apa0 = 0.31; % (0.31) c = 2.5; % W/m2/K xrange = [287 293]; yrange = [247 251]; rngx = xrange(2) - xrange(1); rngy = yrange(2) - yrange(1); [X,Y] = meshgrid(xrange(1):0.05:xrange(2), yrange(1):0.05:yrange(2)); smallarrow = 1; Delta0 = 0.0000009; for tloop =1:80 Delta = Delta0*(exp((tloop-1)/8)-1); % This Delta is exponential, but should become more linear over time date = floor(1850 + (tloop-1)*(2060-1850)/79); [x,y] = f5(X,Y); clf hold off eps = 0.002; for xloop = 1:11 xs = xrange(1) +(xloop-1)*rngx/10 + eps; for yloop = 1:11 ys = yrange(1) +(yloop-1)*rngy/10 + eps; streamline(X,Y,x,y,xs,ys) end end hold on [XQ,YQ] = meshgrid(xrange(1):1:xrange(2),yrange(1):1:yrange(2)); smallarrow = 1; [xq,yq] = f5(XQ,YQ); quiver(XQ,YQ,xq,yq,.2,'r','filled') hold off axis([xrange(1) xrange(2) yrange(1) yrange(2)]) set(gcf,'Color','White') fun = @root2d; x0 = [0 -40]; x = fsolve(fun,x0); Ts = x(1) + 288 Ta = x(2) + 288 hold on rectangle('Position',[Ts-0.05 Ta-0.05 0.1 0.1],'Curvature',[1 1],'FaceColor',[1 0 0],'EdgeColor','k','LineWidth',2) posTs(tloop) = Ts; posTa(tloop) = Ta; plot(posTs,posTa,'k','LineWidth',2); hold off text(287.5,250.5,strcat('Date = ',num2str(date)),'FontSize',24) box on xlabel('Surface Temperature (oC)','FontSize',24) ylabel('Atmosphere Temperature (oC)','FontSize',24) hh = figure(1); pause(0.01) if mov_flag == 1 frame = getframe(hh); writeVideo(aviobj,frame); end end % end tloop if mov_flag == 1 close(aviobj); end function F = root2d(xp) % Energy fluxes x = xp + 288; feedfac = 0.001; % feedback parameter apa = apa0 + feedfac*(x(2)-248) + Delta; % Changes in the atmospheric blanket tpa = tpa0 - feedfac*(x(2)-248) - Delta; as = as0 - feedfac*(x(1)-289); F(1) = c*(x(1)-x(2)) + sig*(1-apa)*x(1).^4 - sig*x(2).^4 - ta*(1-as)*Solar/4; F(2) = c*(x(1)-x(2)) + sig*(1-tpa - apa)*x(1).^4 - 2*sig*x(2).^4 + (1-aa0-ta+as*ta)*Solar/4; end function [x,y] = f5(X,Y) % Dynamical flow equations k1 = 1/75; % 75 year time constant for the Earth k2 = 1/25; % 25 year time constant for the Atmosphere fun = @root2d; x0 = [0 0]; x = fsolve(fun,x0); % Solve for the temperatures that set the energy fluxes to zero Ts0 = x(1) + 288; % Surface temperature in Kelvin Ta0 = x(2) + 288; % Atmosphere temperature in Kelvin xtmp = -k1*(X - Ts0); % Dynamical equations ytmp = -k2*(Y - Ta0); nrm = sqrt(xtmp.^2 + ytmp.^2); if smallarrow == 1 x = xtmp./nrm; y = ytmp./nrm; else x = xtmp; y = ytmp; end end % end f5 end % end flowatmos\n\nThis model has a lot of parameters that can be tweaked. In addition to the parameters in the Table, the time dependence on the blanket properties of the atmosphere are governed by Delta0 and by feedfac for feedback of temperature on the atmosphere, such as increasing cloud cover and decrease ice cover. As an exercise, and using only small changes in the given parameters, find the following cases: 1) An increasing surface temperature is moderated by a falling atmosphere temperature; 2) The Earth goes into thermal run-away and ends like Venus; 3) The Earth initially warms then plummets into an ice age.\n\nBy David D. Nolte Oct. 16, 2022\n\nReferences\n\n[1] D. D. Nolte, Introduction to Modern Dynamics: Chaos, Networks, Space and Time, 2nd Ed. (Oxford University Press, 2019)\n\n[2] E. Boeker and R. van Grondelle, Environmental Physics (Wiley, 1995)\n\n[3] Recent lecture at the National Academy of Engineering by John Holdren.\n\nThe ability to travel to the stars has been one of mankind’s deepest desires. Ever since we learned that we are just one world in a vast universe of limitless worlds, we have yearned to visit some of those others. Yet nature has thrown up an almost insurmountable barrier to that desire–the speed of light. Only by traveling at or near the speed of light may we venture to far-off worlds, and even then, decades or centuries will pass during the voyage. The vast distances of space keep all the worlds isolated–possibly for the better.\n\nYet the closest worlds are not so far away that they will always remain out of reach. The very limit of the speed of light provides ways of getting there within human lifetimes. The non-intuitive effects of special relativity come to our rescue, and we may yet travel to the closest exoplanet we know of.\n\nProxima Centauri b\n\nThe closest habitable Earth-like exoplanet is Proxima Centauri b, orbiting the red dwarf star Proxima Centauri that is about 4.2 lightyears away from Earth. The planet has a short orbital period of only about 11 Earth days, but the dimness of the red dwarf puts the planet in what may be a habitable zone where water is in liquid form. Its official discovery date was August 24, 2016 by the European Southern Observatory in the Atacama Desert of Chile using the Doppler method. The Alpha Centauri system is a three-star system, and even before the discovery of the planet, this nearest star system to Earth was the inspiration for the Hugo-Award winning sci-fi trilogy The Three Body Problem by Chinese author Liu Cixin, originally published in 2008.\n\nIt may seem like a coincidence that the closest Earth-like planet to Earth is in the closest star system to Earth, but it says something about how common such exoplanets may be in our galaxy.\n\nBreakthrough Starshot\n\nThere are already plans to send centimeter-sized spacecraft to Alpha Centauri. One such project that has received a lot of press is Breakthrough Starshot, a project of the Breakthrough Initiatives. Breakthrough Starshot would send around 1000 centimeter-sized camera-carrying laser-fitted spacecraft with 5-meter-diameter solar sails propelled by a large array of high-power lasers. The reason there are so many of these tine spacecraft is because of the collisions that are expected to take place with interstellar dust during the voyage. It is possible that only a few dozen of the craft will finally make it to Alpha Centauri intact.\n\nAs these spacecraft fly by the Alpha Centauri system, possibly within one hundred million miles of Proxima Centauri b, their tiny HR digital cameras will take pictures of the planet’s surface with enough resolution to see surface features. The on-board lasers will then transmit the pictures back to Earth. The travel time to the planet is expected to be 20 or 30 years, plus the four years for the laser information to make it back to Earth. Therefore, it would take a quarter century after launch to find out if Proxima Centauri b is habitable or not. The biggest question is whether it has an atmosphere. The red dwarf it orbits sends out catastrophic electromagnetic bursts that could strip the planet of its atmosphere thus preventing any chance for life to evolve or even to be sustained there if introduced.\n\nThere are multiple projects under consideration for travel to the Alpha Centauri systems. Even NASA has a tentative mission plan called the 2069 Mission (100 year anniversary of the Moon landing). This would entail a single spacecraft with a much larger solar sail than the small starshot units. Some of the mission plans proposed star-drive technology, such as nuclear propulsion systems, rather than light sails. Some of these designs could sustain a 1-g acceleration throughout the entire mission. It is intriguing to do the math on what such a mission could look like, in terms of travel time. Could we get an unmanned probe to Alpha Centauri in a matter of years? Let’s find out.\n\nSpecial Relativity of Acceleration\n\nThe most surprising aspect of deriving the properties of relativistic acceleration using special relativity is that it works at all. We were all taught as young physicists that special relativity deals with inertial frames in constant motion. So the idea of frames that are accelerating might first seem to be outside the scope of special relativity. But one of Einstein’s key insights, as he sought to extend special relativity towards a more general theory, was that one can define a series of instantaneously inertial co-moving frames relative to an accelerating body. In other words, at any instant in time, the accelerating frame has an inertial co-moving frame. Once this is defined, one can construct invariants, just as in usual special relativity. And these invariants unlock the full mathematical structure of accelerating objects within the scope of special relativity.\n\nFor instance, the four-velocity and the four-acceleration in a co-moving frame for an object accelerating at g are given by\n\nThe object is momentarily stationary in the co-moving frame, which is why the four-velocity has only the zeroth component, and the four-acceleration has simply g for its first component.\n\nArmed with these four-vectors, one constructs the invariants\n\nand\n\nThis last equation is solved for the specific co-moving frame as\n\nBut the invariant is more general, allowing the expression\n\nwhich yields\n\nFrom these, putting them all together, one obtains the general differential equations for the change in velocity as a set of coupled equations\n\nThe solution to these equations is\n\nwhere the unprimed frame is the lab frame (or Earth frame), and the primed frame is the frame of the accelerating object, for instance a starship heading towards Alpha Centauri. These equations allow one to calculate distances, times and speeds as seen in the Earth frame as well as the distances, times and speeds as seen in the starship frame. If the starship is accelerating at some acceleration g’ other than g, then the results are obtained simply by replacing g by g’ in the equations.\n\nRelativistic Flight\n\nIt turns out that the acceleration due to gravity on our home planet provides a very convenient (but purely coincidental) correspondence\n\nWith a similarly convenient expression\n\nThese considerably simplify the math for a starship accelerating at g.\n\nLet’s now consider a starship accelerating by g for the first half of the flight to Alpha Centauri, turning around and decelerating at g for the second half of the flight, so that the starship comes to a stop at its destination. The equations for the times to the half-way point are\n\nThis means at the midpoint that 1.83 years have elapsed on the starship, and about 3 years have elapsed on Earth. The total time to get to Alpha Centauri (and come to a stop) is then simply\n\nIt is interesting to look at the speed at the midpoint. This is obtained by\n\nwhich is solved to give\n\nThis amazing result shows that the starship is traveling at 95% of the speed of light at the midpoint when accelerating at the modest value of g for about 3 years. Of course, the engineering challenges for providing such an acceleration for such a long time are currently prohibitive … but who knows? There is a lot of time ahead of us for technology to advance to such a point in the next century or so.\n\nMatlab alphacentaur.m\n\n% alphacentaur.m clear format compact g0 = 1; L = 4.37; for loop = 1:100 g = 0.1*loop*g0; taup = (1/g)*acosh(g*L/2 + 1); tearth = (1/g)*sinh(g*taup); tauspacecraft(loop) = 2*taup; tlab(loop) = 2*tearth; acc(loop) = g; end figure(1) loglog(acc,tauspacecraft,acc,tlab,'LineWidth',2) legend('Space Craft','Earth Frame','FontSize',18) xlabel('Acceleration (g)','FontSize',18) ylabel('Time (years)','FontSize',18) dum = set(gcf,'Color','White'); H = gca; H.LineWidth = 2; H.FontSize = 18;\n\nTo Centauri and Beyond\n\nOnce we get unmanned probes to Alpha Centauri, it opens the door to star systems beyond. The next closest are Barnards star at 6 Ly away, Luhman 16 at 6.5 Ly, Wise at 7.4 Ly, and Wolf 359 at 7.9 Ly. Several of these are known to have orbiting exoplanets. Ross 128 at 11 Ly and Lyuten at 12.2 Ly have known earth-like planets. There are about 40 known earth-like planets within 40 lightyears from Earth, and likely there are more we haven’t found yet. It is almost inconceivable that none of these would have some kind of life. Finding life beyond our solar system would be a monumental milestone in the history of science. Perhaps that day will come within this century.\n\nBy David D. Nolte, March 23, 2022\n\nFurther Reading\n\nR. A. Mould, Basic Relativity. Springer (1994)\n\nD. D. Nolte, Introduction to Modern Dynamics : Chaos, Networks, Space and Time, 2nd ed.: Oxford University Press (2019)\n\nThis Blog Post is a Companion to the undergraduate physics textbook Modern Dynamics: Chaos, Networks, Space and Time, 2nd ed. (Oxford, 2019) introducing Lagrangians and Hamiltonians, chaos theory, complex systems, synchronization, neural networks, econophysics and Special and General Relativity.\n\nAn old joke goes that Democracy is a terrible form of government … except it’s better than all the others!\n\nOur world today is faced with conflict between democracy and dictatorship. On the one side is the free world, where leaders are chosen by some form of representation of large numbers of citizens and sometimes even a majority. On the other side is authoritarianism where a select few are selected by a select few to govern everyone else.\n\n[I]t has been said that democracy is the worst form of Government except all those other forms that have been tried from time to time; but there is the broad feeling in our country that the people should rule, and that public opinion expressed by all constitutional means, should shape, guide, and control the actions of Ministers who are their servants and not their masters.\n\nWinston Churchill (1947)\n\nAn argument in favor of democracy is freedom of choice for the largest segment of the population, plus the ability to remove leaders who fail to provide for the perceived welfare of the most citizens. This makes democracy adaptive, shifting with the times. It also makes leaders accountable for their actions and crimes. An argument in favor of authoritarianism is the myth of the benevolent dictator–someone who knows what’s best for the people even if the people don’t know it themselves.\n\nBut dictators are rarely benevolent, and as they become saturated with power, they are corrupted. The criminal massacres, perpetrated by Putin, of Ukrainian civilians is one of the strongest recent arguments against authoritarianism. A single man decides, on a whim, the life and death of thousands or maybe more. The invasion of Ukraine is so egregious and unwarranted, that we wonder how the Russian people can put up with their isolated and manic leader. Yet by some measure more than 60% of the people in Russia approve of the war.\n\nHow can the free world see the invasion as the atrocity it is, while Russia’s majority sees it as a just war? The answer is a surprising result of population dynamics known as the replicator-mutator equation. The challenge for us here in the free world is to learn how to game the replicator-mutator equation to break up the monopoly of popular opinion and make Putin pay for his arrogance. This blog explains how “mass hysteria” can arise from forces within a complex environment, and how to construct a possible antidote.\n\nReplicator-Mutator Equation\n\nThere are several simple models of population dynamics that try to explain the rise and fall of the number of individuals that belong to varying cohorts within the population. These models incorporate aspects of relative benefit of one group over another, plus the chance to change sides–defection. The dynamics under these conditions can be highly nonlinear and highly non-intuitive. One of the simplest of these models is known as the replicator-mutator model where replication follows the fitness of the cohort, and where individuals can defect to a “more fit” cohort.\n\nThe basic dynamics of the model are\n\nwhere xa is the fraction of the population that is in cohort a, Wab is a transition probability, and φ is the average fitness of the full population. The transition matrix is given by\n\nwhere fb is the fitness of cohort b and Qba is a stochastic matrix that allows for defection of an individual from one cohort to another. The fitness of a cohort is given by\n\nwhere pbc is the pay-off matrix for the relative benefit of one cohort at the expense of another. Finally the average fitness is\n\nThe Einstein implicit summation convention is assumed in all of these equations, and the metric space in which the dynamics are embedded is “flat” so that there is no essential difference between superscripts and subscripts. There is also a conservation law that the sum over all population fractions equals unity.\n\nIn the language of population dynamics, this model has frequency-dependent fitness, with defection and pay-off, in a zero-sum game.\n\nOne of the simplest questions to answer with this model is how so many people can come to believe one thing. This is known as “opinion uniformity”.\n\nUniformity versus Diversity\n\nThis replicator-mutator model explains the property of opinion uniformity, as well as the opposite extreme of opinion diversity. The starting point for both is the pay-off matrix pbc which is assumed to be unity on the diagonal for b = c and to a constant factor a for b ~= c. This pay"
    }
}
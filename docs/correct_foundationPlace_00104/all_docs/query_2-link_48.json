{
    "id": "correct_foundationPlace_00104_2",
    "rank": 48,
    "data": {
        "url": "https://neo4j.com/developer-blog/making-sense-of-news-the-knowledge-graph-way/",
        "read_more_link": "",
        "language": "en",
        "title": "Making Sense of News, the Knowledge Graph Way",
        "top_image": "https://dist.neo4j.com/wp-content/uploads/20210301082021/18PDusM8iy7j5SGa-t8680A.png",
        "meta_img": "https://dist.neo4j.com/wp-content/uploads/20210301082021/18PDusM8iy7j5SGa-t8680A.png",
        "images": [
            "https://dist.neo4j.com/wp-content/uploads/20230926084108/Logo_FullColor_RGB_TransBG.svg",
            "https://dist.neo4j.com/wp-content/uploads/20230926084108/Logo_FullColor_RGB_TransBG.svg",
            "https://dist.neo4j.com/wp-content/uploads/20240402072516/Icon-GraphAcademy.svg",
            "https://dist.neo4j.com/wp-content/uploads/20240401103728/Icon-Documentation.svg",
            "https://dist.neo4j.com/wp-content/uploads/20240401103728/Icon-Documentation.svg",
            "https://dist.neo4j.com/wp-content/uploads/20240402072516/Icon-GraphAcademy.svg",
            "https://dist.neo4j.com/wp-content/uploads/20240402072514/Icon-Events.svg",
            "https://dist.neo4j.com/wp-content/uploads/20230926084108/Logo_FullColor_RGB_TransBG.svg",
            "https://dist.neo4j.com/wp-content/uploads/20240327092936/gdb.svg",
            "https://dist.neo4j.com/wp-content/uploads/20240327092934/auradb.svg",
            "https://dist.neo4j.com/wp-content/uploads/20240329130329/genai.svg",
            "https://dist.neo4j.com/wp-content/uploads/20240401103728/Icon-Documentation.svg",
            "https://dist.neo4j.com/wp-content/uploads/20240401103728/Icon-Documentation.svg",
            "https://dist.neo4j.com/wp-content/uploads/20240402072516/Icon-GraphAcademy.svg",
            "https://dist.neo4j.com/wp-content/uploads/20240402072514/Icon-Events.svg",
            "https://dist.neo4j.com/wp-content/uploads/20240708132519/Logo-GraphSummit-reverse.svg",
            "https://dist.neo4j.com/wp-content/uploads/20240402072012/Logo-Connections.svg",
            "https://dist.neo4j.com/wp-content/uploads/20240517073021/tomaz-bratanic-150x150.jpeg",
            "https://dist.neo4j.com/wp-content/uploads/20210301082021/18PDusM8iy7j5SGa-t8680A.png",
            "https://dist.neo4j.com/wp-content/uploads/20210301082017/1IIwRLCWGg4Hiw0jsbUi_aQ.png",
            "https://dist.neo4j.com/wp-content/uploads/20210301082012/1wTcLG_TEUlq2V7ej4kYEkA.png",
            "https://dist.neo4j.com/wp-content/uploads/20210301082007/102uyTa5WYoDbJagcnz9HhQ.png",
            "https://dist.neo4j.com/wp-content/uploads/20210301082003/11Oixqq9w8j6o06tPpdye3g.png",
            "https://dist.neo4j.com/wp-content/uploads/20210301081958/10q5JxbjpYO8Acl4xdBMDIg.png",
            "https://dist.neo4j.com/wp-content/uploads/20210301081952/1pOMv1GCzJpQ6YE-nWY8Q4Q.png",
            "https://dist.neo4j.com/wp-content/uploads/20210301081948/1ErtFMNBJdDRjLHzaQtkVvw.png",
            "https://dist.neo4j.com/wp-content/uploads/20210301081943/1rqhW1MpC8pkrZN7N0JyIDw.png",
            "https://dist.neo4j.com/wp-content/uploads/20210301081939/1RZHLKF5F-FjgoOGGa9F_QQ.png",
            "https://dist.neo4j.com/wp-content/uploads/20210301081933/1k3344PLiGcvB6L7ZbOiMrg.png",
            "https://dist.neo4j.com/wp-content/uploads/20210301081928/1RR8GkEDRp6gnnsi4bqpKQg.png",
            "https://dist.neo4j.com/wp-content/uploads/20210301081923/1_-2pFbOzYEvFvBpARL5F4w.png",
            "https://dist.neo4j.com/wp-content/uploads/20210301081918/11cDRp7rEDpGqy_6lvB2rog.png",
            "https://dist.neo4j.com/wp-content/uploads/20210301081913/1qshydr5lCcY_c7gXJtk0aQ.png",
            "https://dist.neo4j.com/wp-content/uploads/20210301081908/13pPNc_Rf8owN0B1GibzHxw.png",
            "https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=d33810ce5005",
            "https://dist.neo4j.com/wp-content/uploads/20240321094132/Blog-Asset-AuraDB-Logo.svg",
            "https://dist.neo4j.com/wp-content/uploads/20240321094136/Blog-Asset-AuraDB-Cloud.svg",
            "https://dist.neo4j.com/wp-content/uploads/20240321094134/Blog-Asset-GraphAcademy-Trophy.svg",
            "https://dist.neo4j.com/wp-content/uploads/20240514071251/node-24-logo.svg",
            "https://dist.neo4j.com/wp-content/uploads/20240514071252/nodes-patterns.svg",
            "https://dist.neo4j.com/wp-content/uploads/20240517073021/tomaz-bratanic-150x150.jpeg",
            "https://dist.neo4j.com/wp-content/uploads/20240718095939/1wqqqBzPj1HYMk-R_joBW4A-e1721322291903.png",
            "https://dist.neo4j.com/wp-content/uploads/20240717084326/unstructured-io-knowledge-graph-e1721231081572.jpg",
            "https://dist.neo4j.com/wp-content/uploads/20240715065226/chemical-reaction-graph-e1721051577357.jpg",
            "https://dist.neo4j.com/wp-content/uploads/20230921083327/homepage-viz_ART-left.svg",
            "https://dist.neo4j.com/wp-content/uploads/20230921083329/homepage-viz_ART-right.svg",
            "https://dist.neo4j.com/wp-content/uploads/20230921082858/homepage-viz_left-side-art_375.svg",
            "https://dist.neo4j.com/wp-content/uploads/20230921082910/homepage-viz_right-side-art_375.svg",
            "https://dist.neo4j.com/wp-content/uploads/20210608133508/icon-tooltip-info.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2021-02-02T20:20:00+00:00",
        "summary": "",
        "meta_description": "Learn how to combine the disciplines of named entity linking and Wikipedia data enrichment using a knowledge graph to make sense of news.",
        "meta_lang": "en",
        "meta_favicon": "/apple-touch-icon.png?v=bOXynyJWa61",
        "meta_site_name": "Graph Database & Analytics",
        "canonical_link": "https://neo4j.com/developer-blog/making-sense-of-news-the-knowledge-graph-way/",
        "text": "How to combine Named Entity Linking with Wikipedia data enrichment to analyze the internet news.\n\nA wealth of information is being produced every day on the internet. Understanding the news and other content-generating websites is becoming increasingly crucial to successfully run a business. It can help you spot opportunities, generate new leads, or provide indicators about the economy.\n\nIn this blog post, I want to show you how you can create a news monitoring data pipeline that combines natural language processing (NLP) and knowledge graph technologies.\n\nThe data pipeline consists of three parts. In the first part, we scrape articles from an Internet provider of news. Next, we run the articles through an NLP pipeline and store results in the form of a knowledge graph. In the last part of the data pipeline, we enrich our knowledge with information from the WikiData API. To demonstrate the benefits of using a knowledge graph to store the information from the data pipeline, we perform simple network analysis and try to find insights.\n\nAgenda\n\nScraping internet news\n\nEntity linking with Wikifier\n\nWikipedia data enrichment\n\nNetwork analysis\n\nGraph Model\n\nWe use Neo4j to store our knowledge graph. If you want to follow along with this blog post, you need to download Neo4j and install both the APOC and Graph Data Science libraries. All the code is available on GitHub as well.\n\nOur graph data model consists of articles and their tags. Each article has many sections of text. Once we run the section text through the NLP pipeline, we extract and store mentioned entities back to our graph.\n\nWe start by defining unique constraints for our graph.\n\nUniqueness constraints are used to ensure data integrity, as well as to optimize Cypher query performance.\n\nCREATE CONSTRAINT IF NOT EXISTS ON (a:Article) ASSERT a.url IS UNIQUE;\n\nCREATE CONSTRAINT IF NOT EXISTS ON (e:Entity) ASSERT e.wikiDataItemId is UNIQUE;\n\nCREATE CONSTRAINT IF NOT EXISTS ON (t:Tag) ASSERT t.name is UNIQUE;\n\nInternet News Scraping\n\nNext, we scrape the CNET news portal. I have chosen the CNET portal because it has the most consistent HTML structure, making it easier to demonstrate the data pipeline concept without focusing on the scraping element. We use theapoc.load.html procedure for the HTML scraping. It uses jsoup under the hood. Find more information in the documentation.\n\nFirst, we iterate over popular topics and store the link of the last dozen of articles for each topic in Neo4j.\n\nCALL apoc.load.html(\"https://www.cnet.com/news/\",\n\n{topics:\"div.tag-listing > ul > li > a\"}) YIELD value\n\nUNWIND value.topics as topic\n\nWITH \"https://www.cnet.com\" + topic.attributes.href as link\n\nCALL apoc.load.html(link, {article:\"div.row.asset > div > a\"}) YIELD value\n\nUNWIND value.article as article\n\nWITH distinct \"https://www.cnet.com\" + article.attributes.href as article_link\n\nMERGE (a:Article{url:article_link});\n\nNow that we have the links to the articles, we can scrape their content as well as their tags and publishing date. We store the results according to the graph schema we defined in the previous section.\n\nMATCH (a:Article)\n\nCALL apoc.load.html(a.url,\n\n{date:\"time\", title:\"h1.speakableText\", text:\"div.article-main-body > p\", tags: \"div.tagList > a\"}) YIELD value\n\nSET a.datetime = datetime(value.date[0].attributes.datetime)\n\nFOREACH (_ IN CASE WHEN value.title[0].text IS NOT NULL THEN [true] ELSE [] END |\n\nCREATE (a)-[:HAS_TITLE]->(:Section{text:value.title[0].text})\n\n)\n\nFOREACH (t in value.tags |\n\nMERGE (tag:Tag{name:t.text}) MERGE (a)-[:HAS_TAG]->(tag)\n\n)\n\nWITH a, value.text as texts\n\nUNWIND texts as row\n\nWITH a,row.text as text\n\nWHERE text IS NOT NULL\n\nCREATE (a)-[:HAS_SECTION]->(:Section{text:text});\n\nI did not want to complicate the Cypher query that stores the results of the articles even more, so we must perform a minor cleanup of tags before we continue.\n\nMATCH (n:Tag)\n\nWHERE n.name CONTAINS \"Notification\"\n\nDETACH DELETE n;\n\nLet’s evaluate our scraping process and look at how many of the articles have been successfully scraped.\n\nMATCH (a:Article)\n\nRETURN exists((a)-[:HAS_SECTION]->()) as scraped_articles,\n\ncount(*) as count\n\nIn my case, I have successfully collected the information for 245 articles. Unless you have a time machine, you won’t be able to recreate this analysis identically. I have scraped the website on the 30th of January 2021, and you will probably do it later. I have prepared most of the analysis queries generically, so they work regardless of the date you choose to scrape the news.\n\nLet’s also examine the most frequent tags of the articles.\n\nMATCH (n:Tag)\n\nRETURN n.name as tag, size((n)<-[:HAS_TAG]-()) as articles\n\nORDER BY articles DESC\n\nLIMIT 10\n\nHere are the results:\n\nAll charts in this blog post are made using the Seaborn library. CNET Apps Today is the most frequent tag. I think that’s just a generic tag for daily news. We can observe that they have custom tags for various big companies such as Amazon, Apple, and Google.\n\nNamed Entity Linking: Wikification\n\nIn my previous blog post, we have already covered the Named Entity Recognition techniques to create a knowledge graph. Here, we will take it up a notch and delve into Named Entity Linking.\n\nFirst of all, what exactly is Named Entity Linking?\n\nNamed Entity Linking is an upgrade to the entity recognition technique. It starts by recognizing all the entities in the text. Once it finishes the named entity recognition process, it tries to link those entities to a target knowledge base. Usually, the target knowledge bases are Wikipedia or DBpedia, but there are other knowledge bases out there as well.\n\nIn the above example, we can observe that the named entity recognition process recognized Paris as an entity. The next step is to link it to a target entity in a knowledge base. Here, it uses Wikipedia as the target knowledge base. This is also known as the Wikification process.\n\nThe Entity Linking process is a bit tricky as we can see that many entities exist in Wikipedia that have Paris in their title. So, as a part of the Entity Linking process, the NLP model also does the entity disambiguation.\n\nThere are a dozen Entity Linking models out there. Some of them are:\n\nhttps://wikifier.org/\n\nhttps://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/ambiverse-nlu/aida\n\nhttps://github.com/informagi/REL\n\nhttps://github.com/facebookresearch/BLINK\n\nI am from Slovenia, so my biased decision is to use the Slovenian solution Wikifier [1]. They don’t actually offer their NLP model, but they have a free-to-use API endpoint. All you have to do is to register. They don’t even want your password or email, which is nice of them.\n\nThe Wikifier supports more than 100 languages. It also features some parameters you can use to fine-tune the results. I have noticed that the most dominant parameter is the pageRankSqThreshold parameter, which you can use to optimize either recall or accuracy of the model.\n\nIf we run the above example through the Wikifier API, we get the following results:\n\nWe can observe that Wikifier API returned three entities and their corresponding Wikipedia URL as well as WikiData item id. We use the WikiData item id as a unique identifier for storing them back to Neo4j.\n\nThe APOC library has the apoc.load.json procedure, which you can use to retrieve results from any API endpoint. If you are dealing with a larger amount of data, you will want to use the apoc.periodic.iterate procedure for batching purposes.\n\nIf we put it all together, the following Cypher query fetches the annotation results for each section from the API endpoint and stores the results in Neo4j.\n\nCALL apoc.periodic.iterate('\n\nMATCH (s:Section) RETURN s\n\n','\n\nWITH s, \"https://www.wikifier.org/annotate-article?\" +\n\n\"text=\" + apoc.text.urlencode(s.text) + \"&\" +\n\n\"lang=en&\" +\n\n\"pageRankSqThreshold=0.80&\" +\n\n\"applyPageRankSqThreshold=true&\" +\n\n\"nTopDfValuesToIgnore=200&\" +\n\n\"nWordsToIgnoreFromList=200&\" +\n\n\"minLinkFrequency=100&\" +\n\n\"maxMentionEntropy=10&\" +\n\n\"wikiDataClasses=false&\" +\n\n\"wikiDataClassIds=false&\" +\n\n\"userKey=\" + $userKey as url\n\nCALL apoc.load.json(url) YIELD value\n\nUNWIND value.annotations as annotation\n\nMERGE (e:Entity{wikiDataItemId:annotation.wikiDataItemId})\n\nON CREATE SET e.title = annotation.title, e.url = annotation.url\n\nMERGE (s)-[:HAS_ENTITY]->(e)',\n\n{batchSize:100, params: {userKey:$user_key}})\n\nThe Named Entity Linking process takes a couple of minutes. We can now check the most frequently mentioned entities.\n\nMATCH (e:Entity)\n\nRETURN e.title, size((e)<--()) as mentions\n\nORDER BY mentions DESC LIMIT 10;\n\nHere are the results:\n\nApple Inc. is the most frequently mentioned entity. I am guessing that all dollar signs or USD mentions get linked to the United States dollar. We can also examine the most frequently-mentioned-by-article tags.\n\nMATCH (e:Entity)<-[:HAS_ENTITY]-()<-[:HAS_SECTION]-()-[:HAS_TAG]->(tag)\n\nWITH tag.name as tag, e.title as title, count(*) as mentions\n\nORDER BY mentions DESC\n\nRETURN tag, collect(title)[..3] as top_3_mentions\n\nLIMIT 5;\n\nHere are the results:\n\nWikiData Enrichment\n\nA bonus to using the Wikification process is that we have the WikiData item id of our entities. This makes it very easy for us to scrape the WikiData API for additional information.\n\nLet’s say we want to define all business and person entities. We will fetch the entity classes from WikiData API and use that information to group the entities. Again we will use the apoc.load.json procedure to retrieve the response from an API endpoint.\n\nMATCH (e:Entity)\n\n// Prepare a SparQL query\n\nWITH 'SELECT *\n\nWHERE{\n\n?item rdfs:label ?name .\n\nfilter (?item = wd:' + e.wikiDataItemId + ')\n\nfilter (lang(?name) = \"en\" ) .\n\nOPTIONAL{\n\n?item wdt:P31 [rdfs:label ?class] .\n\nfilter (lang(?class)=\"en\")\n\n}}' AS sparql, e\n\n// make a request to Wikidata\n\nCALL apoc.load.jsonParams(\n\n\"https://query.wikidata.org/sparql?query=\" +\n\napoc.text.urlencode(sparql),\n\n{ Accept: \"application/sparql-results+json\"}, null)\n\nYIELD value\n\nUNWIND value['results']['bindings'] as row\n\nFOREACH(ignoreme in case when row['class'] is not null then [1] else [] end |\n\nMERGE (c:Class{name:row['class']['value']})\n\nMERGE (e)-[:INSTANCE_OF]->(c));\n\nWe continue by inspecting the most frequent classes of the entities.\n\nMATCH (c:Class)\n\nRETURN c.name as class, size((c)<--()) as count\n\nORDER BY count DESC LIMIT 5;\n\nHere are the results:\n\nThe Wikification process found almost 250 human entities and 100 business entities. We assign a secondary label to Person and Business entities to simplify our further Cypher queries.\n\nMATCH (e:Entity)-[:INSTANCE_OF]->(c:Class)\n\nWHERE c.name in [\"human\"]\n\nSET e:Person;\n\nMATCH (e:Entity)-[:INSTANCE_OF]->(c:Class)\n\nWHERE c.name in [\"business\", \"enterprise\"]\n\nSET e:Business;\n\nWith the added secondary label, we can now easily examine the most frequently-mentioned business entities.\n\nMATCH (b:Business)\n\nRETURN b.title as business, size((b)<-[:HAS_ENTITY]-()) as mentions\n\nORDER BY mentions DESC\n\nLIMIT 10\n\nHere are the results:\n\nWe already knew that Apple and Amazon were discussed a lot. Some of you already know that this was an exciting week on the stock market, as we can see lots of mentions of GameStop.\n\nJust because we can, let’s also fetch the industries of the business entities from the WikiData API.\n\nMATCH (e:Business)\n\n// Prepare a SparQL query\n\nWITH 'SELECT *\n\nWHERE{\n\n?item rdfs:label ?name .\n\nfilter (?item = wd:' + e.wikiDataItemId + ')\n\nfilter (lang(?name) = \"en\" ) .\n\nOPTIONAL{\n\n?item wdt:P452 [rdfs:label ?industry] .\n\nfilter (lang(?industry)=\"en\")\n\n}}' AS sparql, e\n\n// make a request to Wikidata\n\nCALL apoc.load.jsonParams(\n\n\"https://query.wikidata.org/sparql?query=\" +\n\napoc.text.urlencode(sparql),\n\n{ Accept: \"application/sparql-results+json\"}, null)\n\nYIELD value\n\nUNWIND value['results']['bindings'] as row\n\nFOREACH(ignoreme in case when row['industry'] is not null then [1] else [] end |\n\nMERGE (i:Industry{name:row['industry']['value']})\n\nMERGE (e)-[:PART_OF_INDUSTRY]->(i));\n\nExploratory Graph Analysis\n\nOur data pipeline ingestion is complete. Now we can have some fun and explore our knowledge graph. First, we will examine the most co-occurrent entities of the most frequently-mentioned entity, which is Apple Inc. in my case.\n\nMATCH (b:Business)\n\nWITH b, size((b)<-[:HAS_ENTITY]-()) as mentions\n\nORDER BY mentions DESC\n\nLIMIT 1\n\nMATCH (other_entities)<-[:HAS_ENTITY]-()-[:HAS_ENTITY]->(b)\n\nRETURN other_entities.title as entity, count(*) as count\n\nORDER BY count DESC LIMIT 10;\n\nHere are the results:\n\nNothing spectacular here. Apple Inc. appears in sections where iPhone, Apple Watch, and VR are also mentioned. We can look at some more exciting news. I was searching for any relevant tags of articles that might be interesting.\n\nCNET has many specific tags, but the Stock Market tag stood out as more broad and very relevant in these times. Let’s check the most frequently-mentioned industries in the Stock Market category of articles.\n\nMATCH (t:Tag)<-[:HAS_TAG]-()-[:HAS_SECTION]->()-[:HAS_ENTITY]->(entity:Business)-[:PART_OF_INDUSTRY]->(industry)\n\nWHERE t.name = \"Stock Market\"\n\nRETURN industry.name as industry, count(*) as mentions\n\nORDER BY mentions DESC\n\nLIMIT 10\n\nHere are the results:\n\nRetail is by far the most mentioned, next is the video game industry, and then some other industries that are mentioned only once. Next, we will check the most mentioned businesses or persons in the Stock Market category.\n\nMATCH (t:Tag)<-[:HAS_TAG]-()-[:HAS_SECTION]->()-[:HAS_ENTITY]->(entity)\n\nWHERE t.name = \"Stock Market\" AND (entity:Person OR entity:Business)\n\nRETURN entity.title as entity, count(*) as mentions\n\nORDER BY mentions DESC\n\nLIMIT 10\n\nHere are the results:\n\nOkay, so GameStop is huge this weekend with more than 40 mentions. Very far behind are Jim Cramer, Elon Musk, and Alexandria Ocasio-Cortez. Let’s try to understand why GameStop is so huge by looking at the co-occurring entities.\n\nMATCH (b:Business{title:\"GameStop\"})<-[:HAS_ENTITY]-()-[:HAS_ENTITY]->(other_entity)\n\nRETURN other_entity.title as co_occurent_entity, count(*) as mentions\n\nORDER BY mentions DESC\n\nLIMIT 10\n\nHere are the results:\n\nThe most frequently-mentioned entities in the same section as GameStop are Stock, Reddit, and US dollar. If you look at the news you might see that the results make sense. I would venture a guess that AMC (TV channel) was wrongly identified and should probably be the AMC Theaters company.\n\nThere will always be some mistakes in the NLP process. We can filter the results a bit and look for the most co-occurring person or business entities of GameStop.\n\nMATCH (b:Business{title:\"GameStop\"})<-[:HAS_ENTITY]-()-[:HAS_ENTITY]->(other_entity:Person)\n\nRETURN other_entity.title as co_occurent_entity, count(*) as mentions\n\nORDER BY mentions DESC\n\nLIMIT 10\n\nHere are the results:\n\nAlexandria Ocasio-Cortez(AOC) and Elon Musk each appear in three sections with GameStop. Let’s examine the text where AOC co-occurs with GameStop.\n\nMATCH (b:Business{title:\"GameStop\"})<-[:HAS_ENTITY]-(section)-[:HAS_ENTITY]->(p:Person{title:\"Alexandria Ocasio-Cortez\"})\n\nRETURN section.text as text\n\nHere are the results:\n\nGraph Data Science\n\nSo far, we have only done a couple of aggregations using the Cypher query language. As we are utilizing a knowledge graph to store our information, let’s execute some graph algorithms on it. Neo4j Graph Data Science library is a plugin for Neo4j that currently has more than 50 graph algorithms available. The algorithms range from community detection and centrality to node embedding and graph neural network categories.\n\nWe have already inspected some co-occurring entities so far. Next, we infer a co-occurrence network of persons within our knowledge graph. This process basically translates indirect relationships, where two entities are mentioned in the same section, to a direct relationship between those two entities. This diagram might help you understand the process.\n\nThe Cypher query for inferring the person co-occurrence network is:\n\nMATCH (s:Person)<-[:HAS_ENTITY]-()-[:HAS_ENTITY]->(t:Person)\n\nWHERE id(s) < id(t)\n\nWITH s,t, count(*) as weight\n\nMERGE (s)-[c:CO_OCCURENCE]-(t)\n\nSET c.weight = weight\n\nThe first graph algorithm we use is the Weakly Connected Components algorithm. It is used to identify disconnected components or islands within the network.\n\nCALL gds.wcc.write({\n\nnodeProjection:'Person',\n\nrelationshipProjection:'CO_OCCURENCE',\n\nwriteProperty:'wcc'})\n\nYIELD componentCount, componentDistribution\n\nHere are the results:\n\nThe algorithm found 134 disconnected components within our graph. The p50 value is the 50th percentile of the community size. Most of the components consist of a single node.\n\nThis implies that they don’t have any CO_OCCURENCE relationships. The largest island of nodes consists of 30 members. We mark its members with a secondary label.\n\nMATCH (p:Person)\n\nWITH p.wcc as wcc, collect(p) as members\n\nORDER BY size(members) DESC LIMIT 1\n\nUNWIND members as member\n\nSET member:LargestWCC\n\nWe further analyze the largest component by examining its community structure and trying to find the most central nodes. When you have a plan to run multiple algorithms on the same projected graph, it is better to use a named graph. The relationship in the co-occurrence network is treated as undirected.\n\nCALL gds.graph.create('person-cooccurence', 'LargestWCC',\n\n{CO_OCCURENCE:{orientation:'UNDIRECTED'}},\n\n{relationshipProperties:['weight']})\n\nFirst, we run the PageRank algorithm, which helps us identify the most central nodes.\n\nCALL gds.pageRank.write('person-cooccurence', {relationshipWeightProperty:'weight', writeProperty:'pagerank'})\n\nNext, we run the Louvain algorithm, which is a community detection algorithm.\n\nCALL gds.louvain.write('person-cooccurence', {relationshipWeightProperty:'weight', writeProperty:'louvain'})\n\nSome people say that a picture is worth a thousand words. When you are dealing with smaller networks it makes sense to create a network visualization of the results. The following visualization was created using Neo4j Bloom.\n\nConclusion\n\nI really love how NLP and knowledge graphs are a perfect match. Hopefully, I have given you some ideas and pointers on how you can go about implementing your data pipeline and storing results in a form of a knowledge graph. Let me know what do you think!\n\nAs always, the code is available on GitHub.\n\nReferences\n\n[1] Janez Brank, Gregor Leban, Marko Grobelnik. Annotating Documents with Relevant Wikipedia Concepts. Proceedings of the Slovenian Conference on Data Mining and Data Warehouses (SiKDD 2017), Ljubljana, Slovenia, 9 October 2017."
    }
}
{
    "id": "dbpedia_122_2",
    "rank": 31,
    "data": {
        "url": "https://openai.com/index/dall-e/",
        "read_more_link": "",
        "language": "en",
        "title": "DALL·E: Creating images from text",
        "top_image": "https://images.ctfassets.net/kftzwdyauwt9/ed21faee-ce44-4d91-f5cc39941d47/bdd3983530857e93d205304e219e2d95/dall-e.jpg?w=1600&h=900&fit=fill",
        "meta_img": "https://images.ctfassets.net/kftzwdyauwt9/ed21faee-ce44-4d91-f5cc39941d47/bdd3983530857e93d205304e219e2d95/dall-e.jpg?w=1600&h=900&fit=fill",
        "images": [
            "https://images.ctfassets.net/kftzwdyauwt9/ed21faee-ce44-4d91-f5cc39941d47/bdd3983530857e93d205304e219e2d95/dall-e.jpg?w=640&q=90&fm=webp 640w, https://images.ctfassets.net/kftzwdyauwt9/ed21faee-ce44-4d91-f5cc39941d47/bdd3983530857e93d205304e219e2d95/dall-e.jpg?w=750&q=90&fm=webp 750w, https://images.ctfassets.net/kftzwdyauwt9/ed21faee-ce44-4d91-f5cc39941d47/bdd3983530857e93d205304e219e2d95/dall-e.jpg?w=828&q=90&fm=webp 828w, https://images.ctfassets.net/kftzwdyauwt9/ed21faee-ce44-4d91-f5cc39941d47/bdd3983530857e93d205304e219e2d95/dall-e.jpg?w=1080&q=90&fm=webp 1080w, https://images.ctfassets.net/kftzwdyauwt9/ed21faee-ce44-4d91-f5cc39941d47/bdd3983530857e93d205304e219e2d95/dall-e.jpg?w=1200&q=90&fm=webp 1200w, https://images.ctfassets.net/kftzwdyauwt9/ed21faee-ce44-4d91-f5cc39941d47/bdd3983530857e93d205304e219e2d95/dall-e.jpg?w=1920&q=90&fm=webp 1920w, https://images.ctfassets.net/kftzwdyauwt9/ed21faee-ce44-4d91-f5cc39941d47/bdd3983530857e93d205304e219e2d95/dall-e.jpg?w=2048&q=90&fm=webp 2048w, https://images.ctfassets.net/kftzwdyauwt9/ed21faee-ce44-4d91-f5cc39941d47/bdd3983530857e93d205304e219e2d95/dall-e.jpg?w=3840&q=90&fm=webp 3840w",
            "https://images.ctfassets.net/kftzwdyauwt9/6Viz8iK1yGb6sLmiAGSI7T/49da64d0369f457d3c022c0a0b17ba71/DALL_E_2024-04-06_16.58_4.png?w=640&q=90&fm=webp 640w, https://images.ctfassets.net/kftzwdyauwt9/6Viz8iK1yGb6sLmiAGSI7T/49da64d0369f457d3c022c0a0b17ba71/DALL_E_2024-04-06_16.58_4.png?w=750&q=90&fm=webp 750w, https://images.ctfassets.net/kftzwdyauwt9/6Viz8iK1yGb6sLmiAGSI7T/49da64d0369f457d3c022c0a0b17ba71/DALL_E_2024-04-06_16.58_4.png?w=828&q=90&fm=webp 828w, https://images.ctfassets.net/kftzwdyauwt9/6Viz8iK1yGb6sLmiAGSI7T/49da64d0369f457d3c022c0a0b17ba71/DALL_E_2024-04-06_16.58_4.png?w=1080&q=90&fm=webp 1080w, https://images.ctfassets.net/kftzwdyauwt9/6Viz8iK1yGb6sLmiAGSI7T/49da64d0369f457d3c022c0a0b17ba71/DALL_E_2024-04-06_16.58_4.png?w=1200&q=90&fm=webp 1200w, https://images.ctfassets.net/kftzwdyauwt9/6Viz8iK1yGb6sLmiAGSI7T/49da64d0369f457d3c022c0a0b17ba71/DALL_E_2024-04-06_16.58_4.png?w=1920&q=90&fm=webp 1920w, https://images.ctfassets.net/kftzwdyauwt9/6Viz8iK1yGb6sLmiAGSI7T/49da64d0369f457d3c022c0a0b17ba71/DALL_E_2024-04-06_16.58_4.png?w=2048&q=90&fm=webp 2048w, https://images.ctfassets.net/kftzwdyauwt9/6Viz8iK1yGb6sLmiAGSI7T/49da64d0369f457d3c022c0a0b17ba71/DALL_E_2024-04-06_16.58_4.png?w=3840&q=90&fm=webp 3840w",
            "https://images.ctfassets.net/kftzwdyauwt9/3vA8jsIso6CRsO8296SxLi/f120c0f762cfa4614ff87de2ade85e0e/gpt-4o-mini_card_image-updated.png?w=640&q=90&fm=webp 640w, https://images.ctfassets.net/kftzwdyauwt9/3vA8jsIso6CRsO8296SxLi/f120c0f762cfa4614ff87de2ade85e0e/gpt-4o-mini_card_image-updated.png?w=750&q=90&fm=webp 750w, https://images.ctfassets.net/kftzwdyauwt9/3vA8jsIso6CRsO8296SxLi/f120c0f762cfa4614ff87de2ade85e0e/gpt-4o-mini_card_image-updated.png?w=828&q=90&fm=webp 828w, https://images.ctfassets.net/kftzwdyauwt9/3vA8jsIso6CRsO8296SxLi/f120c0f762cfa4614ff87de2ade85e0e/gpt-4o-mini_card_image-updated.png?w=1080&q=90&fm=webp 1080w, https://images.ctfassets.net/kftzwdyauwt9/3vA8jsIso6CRsO8296SxLi/f120c0f762cfa4614ff87de2ade85e0e/gpt-4o-mini_card_image-updated.png?w=1200&q=90&fm=webp 1200w, https://images.ctfassets.net/kftzwdyauwt9/3vA8jsIso6CRsO8296SxLi/f120c0f762cfa4614ff87de2ade85e0e/gpt-4o-mini_card_image-updated.png?w=1920&q=90&fm=webp 1920w, https://images.ctfassets.net/kftzwdyauwt9/3vA8jsIso6CRsO8296SxLi/f120c0f762cfa4614ff87de2ade85e0e/gpt-4o-mini_card_image-updated.png?w=2048&q=90&fm=webp 2048w, https://images.ctfassets.net/kftzwdyauwt9/3vA8jsIso6CRsO8296SxLi/f120c0f762cfa4614ff87de2ade85e0e/gpt-4o-mini_card_image-updated.png?w=3840&q=90&fm=webp 3840w"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "We’ve trained a neural network called DALL·E that creates images from text captions for a wide range of concepts expressible in natural language.",
        "meta_lang": "en",
        "meta_favicon": "/favicon.ico",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "DALL·E is a 12-billion parameter version of GPT-3(opens in a new window) trained to generate images from text descriptions, using a dataset of text–image pairs. We’ve found that it has a diverse set of capabilities, including creating anthropomorphized versions of animals and objects, combining unrelated concepts in plausible ways, rendering text, and applying transformations to existing images.\n\nSee also: DALL·E 2, which generates more realistic and accurate images with 4x greater resolution.\n\nLike GPT-3, DALL·E is a transformer language model. It receives both the text and the image as a single stream of data containing up to 1280 tokens, and is trained using maximum likelihood to generate all of the tokens, one after another. A\n\nThis training procedure allows DALL·E to not only generate an image from scratch, but also to regenerate any rectangular region of an existing image that extends to the bottom-right corner, in a way that is consistent with the text prompt.\n\nWe recognize that work involving generative models has the potential for significant, broad societal impacts. In the future, we plan to analyze how models like DALL·E relate to societal issues like economic impact on certain work processes and professions, the potential for bias in the model outputs, and the longer term ethical challenges implied by this technology.\n\nThe task of translating text to images is underspecified: a single caption generally corresponds to an infinitude of plausible images, so the image is not uniquely determined. For instance, consider the caption “a painting of a capybara sitting on a field at sunrise.” Depending on the orientation of the capybara, it may be necessary to draw a shadow, though this detail is never mentioned explicitly. We explore DALL·E’s ability to resolve underspecification in three cases: changing style, setting, and time; drawing the same object in a variety of different situations; and generating an image of an object with specific text written on it.\n\nWith varying degrees of reliability, DALL·E provides access to a subset of the capabilities of a 3D rendering engine via natural language. It can independently control the attributes of a small number of objects, and to a limited extent, how many there are, and how they are arranged with respect to one another. It can also control the location and angle from which a scene is rendered, and can generate known objects in compliance with precise specifications of angle and lighting conditions.\n\nUnlike a 3D rendering engine, whose inputs must be specified unambiguously and in complete detail, DALL·E is often able to “fill in the blanks” when the caption implies that the image must contain a certain detail that is not explicitly stated.\n\nDALL·E is a simple decoder-only transformer that receives both the text and the image as a single stream of 1280 tokens—256 for the text and 1024 for the image—and models all of them autoregressively. The attention mask at each of its 64 self-attention layers allows each image token to attend to all text tokens. DALL·E uses the standard causal mask for the text tokens, and sparse attention for the image tokens with either a row, column, or convolutional attention pattern, depending on the layer. We provide more details about the architecture and training procedure in our paper(opens in a new window).\n\nText-to-image synthesis has been an active area of research since the pioneering work of Reed et. al,1 whose approach uses a GAN conditioned on text embeddings. The embeddings are produced by an encoder pretrained using a contrastive loss, not unlike CLIP. StackGAN3 and StackGAN++4 use multi-scale GANs to scale up the image resolution and improve visual fidelity. AttnGAN5 incorporates attention between the text and image features, and proposes a contrastive text-image feature matching loss as an auxiliary objective. This is interesting to compare to our reranking with CLIP, which is done offline. Other work2, 6, 7 incorporates additional sources of supervision during training to improve image quality. Finally, work by Nguyen et. al8 and Cho et. al9 explores sampling-based strategies for image generation that leverage pretrained multimodal discriminative models.\n\nSimilar to the rejection sampling used in VQVAE-2(opens in a new window), we use CLIP to rerank the top 32 of 512 samples for each caption in all of the interactive visuals. This procedure can also be seen as a kind of language-guided search16, and can have a dramatic impact on sample quality."
    }
}
{
    "id": "dbpedia_7736_1",
    "rank": 84,
    "data": {
        "url": "https://github.com/espnet/espnet",
        "read_more_link": "",
        "language": "en",
        "title": "End Speech Processing Toolkit",
        "top_image": "https://opengraph.githubassets.com/273f0d503da75932b91b873640b0fe0e98bc79d28272f4a64c5c8e4424b51aa6/espnet/espnet",
        "meta_img": "https://opengraph.githubassets.com/273f0d503da75932b91b873640b0fe0e98bc79d28272f4a64c5c8e4424b51aa6/espnet/espnet",
        "images": [
            "https://github.com/espnet/espnet/raw/master/doc/image/espnet_logo1.png",
            "https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg",
            "https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg",
            "https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg",
            "https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg",
            "https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg",
            "https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg",
            "https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg",
            "https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg",
            "https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg",
            "https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg",
            "https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg",
            "https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg",
            "https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg",
            "https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg",
            "https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg",
            "https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg",
            "https://github.com/espnet/espnet/actions/workflows/ci_on_debian11.yml/badge.svg",
            "https://github.com/espnet/espnet/actions/workflows/ci_on_windows.yml/badge.svg",
            "https://github.com/espnet/espnet/actions/workflows/ci_on_macos.yml/badge.svg",
            "https://github.com/espnet/espnet/actions/workflows/ci_on_macos.yml/badge.svg",
            "https://camo.githubusercontent.com/7676458c0a03f1c0e8874f90811a1d8d9d0761475f3594866e25d63fa449a8eb/68747470733a2f2f62616467652e667572792e696f2f70792f6573706e65742e737667",
            "https://camo.githubusercontent.com/8e24381f95fd38f4e961de91faa25796f7084e2055305b15b4b7fb3dbb781fe2/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f6573706e65742e737667",
            "https://camo.githubusercontent.com/2af67d6f321af6286b268be179c841db0d7de1cfd14e11a00c720500323dd4db/68747470733a2f2f706570792e746563682f62616467652f6573706e6574",
            "https://camo.githubusercontent.com/1968c2f394002b010d19eac40a047fa7a6c4dfc1f3c8639687db6f96cac6b039/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6573706e65742f6573706e65742e737667",
            "https://camo.githubusercontent.com/b6cde04e43cbe1839c93b08e3c4756ce70affd567e0f7f70e63892af6be0e810/68747470733a2f2f636f6465636f762e696f2f67682f6573706e65742f6573706e65742f6272616e63682f6d61737465722f67726170682f62616467652e737667",
            "https://camo.githubusercontent.com/5bf9e9fa18966df7cb5fac7715bef6b72df15e01a6efa9d616c83f9fcb527fe2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c61636b2d3030303030302e737667",
            "https://camo.githubusercontent.com/4e71e9b7ea25fbc70f186444684f4bfd9def4c737dfc327796cc2c332cbf0b46/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f253230696d706f7274732d69736f72742d2532333136373462313f7374796c653d666c6174266c6162656c436f6c6f723d656638333336",
            "https://camo.githubusercontent.com/c731282ba4f2eb20634a733cb88422fedf3df9f8f4dee6a0e530ea8e5177e92d/68747470733a2f2f726573756c74732e7072652d636f6d6d69742e63692f62616467652f6769746875622f6573706e65742f6573706e65742f6d61737465722e737667",
            "https://camo.githubusercontent.com/65e01e8f2f2cfc2ad873bdd9c5681c7f55c22d9472235d47e1fcd0546e245aa5/68747470733a2f2f696d672e736869656c64732e696f2f656e64706f696e742e7376673f75726c3d68747470733a2f2f6170692e6d6572676966792e636f6d2f76312f6261646765732f6573706e65742f6573706e6574267374796c653d666c6174",
            "https://camo.githubusercontent.com/3b49d1c0cb6939689cd1a31a5a96aae2de6cece1e32ef2ec10c60ce88ff1185a/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f313137343533383530303336303635303737333f636f6c6f723d253233394235394236266c6162656c3d636861742532306f6e253230646973636f7264",
            "https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667",
            "https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667",
            "https://camo.githubusercontent.com/a4ff28c1dbabfaa46915ab215390308c2415c77b4b180e78909c08d74c174ad8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565",
            "https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667",
            "https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667",
            "https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667",
            "https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667",
            "https://camo.githubusercontent.com/a4ff28c1dbabfaa46915ab215390308c2415c77b4b180e78909c08d74c174ad8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565",
            "https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667",
            "https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667",
            "https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667",
            "https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667",
            "https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667",
            "https://avatars.githubusercontent.com/u/45381089?s=64&v=4",
            "https://avatars.githubusercontent.com/u/56527184?s=64&v=4",
            "https://avatars.githubusercontent.com/u/179139331?s=64&v=4",
            "https://avatars.githubusercontent.com/u/83943042?s=64&v=4",
            "https://avatars.githubusercontent.com/u/96317696?s=64&v=4",
            "https://avatars.githubusercontent.com/u/39617376?s=64&v=4",
            "https://avatars.githubusercontent.com/u/162611425?s=64&v=4",
            "https://avatars.githubusercontent.com/u/82242732?s=64&v=4",
            "https://avatars.githubusercontent.com/u/22779813?s=64&v=4",
            "https://avatars.githubusercontent.com/u/11741550?s=64&v=4",
            "https://avatars.githubusercontent.com/u/19261024?s=64&v=4",
            "https://avatars.githubusercontent.com/u/22814472?s=64&v=4",
            "https://avatars.githubusercontent.com/u/17924227?s=64&v=4",
            "https://avatars.githubusercontent.com/u/41155456?s=64&v=4",
            "https://avatars.githubusercontent.com/in/10562?s=64&v=4",
            "https://avatars.githubusercontent.com/u/6745326?s=64&v=4",
            "https://avatars.githubusercontent.com/in/68672?s=64&v=4",
            "https://avatars.githubusercontent.com/u/18532145?s=64&v=4",
            "https://avatars.githubusercontent.com/u/18726713?s=64&v=4",
            "https://avatars.githubusercontent.com/u/11988996?s=64&v=4",
            "https://avatars.githubusercontent.com/u/33059381?s=64&v=4",
            "https://avatars.githubusercontent.com/u/9349732?s=64&v=4"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "End-to-End Speech Processing Toolkit. Contribute to espnet/espnet development by creating an account on GitHub.",
        "meta_lang": "en",
        "meta_favicon": "https://github.com/fluidicon.png",
        "meta_site_name": "GitHub",
        "canonical_link": "https://github.com/espnet/espnet",
        "text": "ESPnet: end-to-end speech processing toolkit\n\nsystem/pytorch ver. 1.13.1 2.0.1 2.1.2 2.2.2 2.3.1 2.4.0 ubuntu/python3.10/pip ubuntu/python3.9/pip ubuntu/python3.8/pip ubuntu/python3.7/pip debian11/python3.10/conda windows/python3.10/pip macos/python3.10/pip macos/python3.10/conda\n\nESPnet is an end-to-end speech processing toolkit covering end-to-end speech recognition, text-to-speech, speech translation, speech enhancement, speaker diarization, spoken language understanding, and so on. ESPnet uses pytorch as a deep learning engine and also follows Kaldi style data processing, feature extraction/format, and recipes to provide a complete setup for various speech processing experiments.\n\nTutorial Series\n\n2019 Tutorial at Interspeech\n\nMaterial\n\n2021 Tutorial at CMU\n\nOnline video\n\nMaterial\n\n2022 Tutorial at CMU\n\nUsage of ESPnet (ASR as an example)\n\nOnline video\n\nMaterial\n\nAdd new models/tasks to ESPnet\n\nOnline video\n\nMaterial\n\nKey Features\n\nKaldi-style complete recipe\n\nSupport numbers of ASR recipes (WSJ, Switchboard, CHiME-4/5, Librispeech, TED, CSJ, AMI, HKUST, Voxforge, REVERB, Gigaspeech, etc.)\n\nSupport numbers of TTS recipes in a similar manner to the ASR recipe (LJSpeech, LibriTTS, M-AILABS, etc.)\n\nSupport numbers of ST recipes (Fisher-CallHome Spanish, Libri-trans, IWSLT'18, How2, Must-C, Mboshi-French, etc.)\n\nSupport numbers of MT recipes (IWSLT'14, IWSLT'16, the above ST recipes etc.)\n\nSupport numbers of SLU recipes (CATSLU-MAPS, FSC, Grabo, IEMOCAP, JDCINAL, SNIPS, SLURP, SWBD-DA, etc.)\n\nSupport numbers of SE/SS recipes (DNS-IS2020, LibriMix, SMS-WSJ, VCTK-noisyreverb, WHAM!, WHAMR!, WSJ-2mix, etc.)\n\nSupport voice conversion recipe (VCC2020 baseline)\n\nSupport speaker diarization recipe (mini_librispeech, librimix)\n\nSupport singing voice synthesis recipe (ofuton_p_utagoe_db, opencpop, m4singer, etc.)\n\nASR: Automatic Speech Recognition\n\nState-of-the-art performance in several ASR benchmarks (comparable/superior to hybrid DNN/HMM and CTC)\n\nHybrid CTC/attention based end-to-end ASR\n\nFast/accurate training with CTC/attention multitask training\n\nCTC/attention joint decoding to boost monotonic alignment decoding\n\nEncoder: VGG-like CNN + BiRNN (LSTM/GRU), sub-sampling BiRNN (LSTM/GRU), Transformer, Conformer, Branchformer, or E-Branchformer\n\nDecoder: RNN (LSTM/GRU), Transformer, or S4\n\nAttention: Flash Attention, Dot product, location-aware attention, variants of multi-head\n\nIncorporate RNNLM/LSTMLM/TransformerLM/N-gram trained only with text data\n\nBatch GPU decoding\n\nData augmentation\n\nTransducer based end-to-end ASR\n\nArchitecture:\n\nCustom encoder supporting RNNs, Conformer, Branchformer (w/ variants), 1D Conv / TDNN.\n\nDecoder w/ parameters shared across blocks supporting RNN, stateless w/ 1D Conv, MEGA, and RWKV.\n\nPre-encoder: VGG2L or Conv2D available.\n\nSearch algorithms:\n\nGreedy search constrained to one emission by timestep.\n\nDefault beam search algorithm [Graves, 2012] without prefix search.\n\nAlignment-Length Synchronous decoding [Saon et al., 2020].\n\nTime Synchronous Decoding [Saon et al., 2020].\n\nN-step Constrained beam search modified from [Kim et al., 2020].\n\nmodified Adaptive Expansion Search based on [Kim et al., 2021] and NSC.\n\nFeatures:\n\nUnified interface for offline and streaming speech recognition.\n\nMulti-task learning with various auxiliary losses:\n\nEncoder: CTC, auxiliary Transducer and symmetric KL divergence.\n\nDecoder: cross-entropy w/ label smoothing.\n\nTransfer learning with an acoustic model and/or language model.\n\nTraining with FastEmit regularization method [Yu et al., 2021].\n\nPlease refer to the tutorial page for complete documentation.\n\nCTC segmentation\n\nNon-autoregressive model based on Mask-CTC\n\nASR examples for supporting endangered language documentation (Please refer to egs/puebla_nahuatl and egs/yoloxochitl_mixtec for details)\n\nWav2Vec2.0 pre-trained model as Encoder, imported from FairSeq.\n\nSelf-supervised learning representations as features, using upstream models in S3PRL in frontend.\n\nSet frontend to s3prl\n\nSelect any upstream model by setting the frontend_conf to the corresponding name.\n\nTransfer Learning :\n\neasy usage and transfers from models previously trained by your group or models from ESPnet Hugging Face repository.\n\nDocumentation and toy example runnable on colab.\n\nStreaming Transformer/Conformer ASR with blockwise synchronous beam search.\n\nRestricted Self-Attention based on Longformer as an encoder for long sequences\n\nOpenAI Whisper model, robust ASR based on large-scale, weakly-supervised multitask learning\n\nDemonstration\n\nReal-time ASR demo with ESPnet2\n\nGradio Web Demo on Hugging Face Spaces. Check out the Web Demo\n\nStreaming Transformer ASR Local Demo with ESPnet2.\n\nTTS: Text-to-speech\n\nArchitecture\n\nTacotron2\n\nTransformer-TTS\n\nFastSpeech\n\nFastSpeech2\n\nConformer FastSpeech & FastSpeech2\n\nVITS\n\nJETS\n\nMulti-speaker & multi-language extension\n\nPre-trained speaker embedding (e.g., X-vector)\n\nSpeaker ID embedding\n\nLanguage ID embedding\n\nGlobal style token (GST) embedding\n\nMix of the above embeddings\n\nEnd-to-end training\n\nEnd-to-end text-to-wav model (e.g., VITS, JETS, etc.)\n\nJoint training of text2mel and vocoder\n\nVarious language support\n\nEn / Jp / Zn / De / Ru / And more...\n\nIntegration with neural vocoders\n\nParallel WaveGAN\n\nMelGAN\n\nMulti-band MelGAN\n\nHiFiGAN\n\nStyleMelGAN\n\nMix of the above models\n\nDemonstration\n\nReal-time TTS demo with ESPnet2\n\nIntegrated to Hugging Face Spaces with Gradio. See demo:\n\nTo train the neural vocoder, please check the following repositories:\n\nkan-bayashi/ParallelWaveGAN\n\nr9y9/wavenet_vocoder\n\nSE: Speech enhancement (and separation)\n\nSingle-speaker speech enhancement\n\nMulti-speaker speech separation\n\nUnified encoder-separator-decoder structure for time-domain and frequency-domain models\n\nEncoder/Decoder: STFT/iSTFT, Convolution/Transposed-Convolution\n\nSeparators: BLSTM, Transformer, Conformer, TasNet, DPRNN, SkiM, SVoice, DC-CRN, DCCRN, Deep Clustering, Deep Attractor Network, FaSNet, iFaSNet, Neural Beamformers, etc.\n\nFlexible ASR integration: working as an individual task or as the ASR frontend\n\nEasy to import pre-trained models from Asteroid\n\nBoth the pre-trained models from Asteroid and the specific configuration are supported.\n\nDemonstration\n\nInteractive SE demo with ESPnet2\n\nStreaming SE demo with ESPnet2\n\nST: Speech Translation & MT: Machine Translation\n\nState-of-the-art performance in several ST benchmarks (comparable/superior to cascaded ASR and MT)\n\nTransformer-based end-to-end ST (new!)\n\nTransformer-based end-to-end MT (new!)\n\nVC: Voice conversion\n\nTransformer and Tacotron2-based parallel VC using Mel spectrogram\n\nEnd-to-end VC based on cascaded ASR+TTS (Baseline system for Voice Conversion Challenge 2020!)\n\nSLU: Spoken Language Understanding\n\nArchitecture\n\nTransformer-based Encoder\n\nConformer-based Encoder\n\nBranchformer based Encoder\n\nE-Branchformer based Encoder\n\nRNN based Decoder\n\nTransformer-based Decoder\n\nSupport Multitasking with ASR\n\nPredict both intent and ASR transcript\n\nSupport Multitasking with NLU\n\nDeliberation encoder based 2 pass model\n\nSupport using pre-trained ASR models\n\nHubert\n\nWav2vec2\n\nVQ-APC\n\nTERA and more ...\n\nSupport using pre-trained NLP models\n\nBERT\n\nMPNet And more...\n\nVarious language support\n\nEn / Jp / Zn / Nl / And more...\n\nSupports using context from previous utterances\n\nSupports using other tasks like SE in a pipeline manner\n\nSupports Two Pass SLU that combines audio and ASR transcript Demonstration\n\nPerforming noisy spoken language understanding using a speech enhancement model followed by a spoken language understanding model.\n\nPerforming two-pass spoken language understanding where the second pass model attends to both acoustic and semantic information.\n\nIntegrated to Hugging Face Spaces with Gradio. See SLU demo on multiple languages:\n\nSUM: Speech Summarization\n\nEnd to End Speech Summarization Recipe for Instructional Videos using Restricted Self-Attention [Sharma et al., 2022]\n\nSVS: Singing Voice Synthesis\n\nFramework merge from Muskits\n\nArchitecture\n\nRNN-based non-autoregressive model\n\nXiaoice\n\nTacotron-singing\n\nDiffSinger (in progress)\n\nVISinger\n\nVISinger 2 (its variations with different vocoders-architecture)\n\nSupport multi-speaker & multilingual singing synthesis\n\nSpeaker ID embedding\n\nLanguage ID embedding\n\nVarious language support\n\nJp / En / Kr / Zh\n\nTight integration with neural vocoders (the same as TTS)\n\nSSL: Self-supervised Learning\n\nSupport HuBERT Pre-training:\n\nExample recipe: egs2/LibriSpeech/ssl1\n\nUASR: Unsupervised ASR (EURO: ESPnet Unsupervised Recognition - Open-source)\n\nArchitecture\n\nwav2vec-U (with different self-supervised models)\n\nwav2vec-U 2.0 (in progress)\n\nSupport PrefixBeamSearch and K2-based WFST decoding\n\nS2T: Speech-to-text with Whisper-style multilingual multitask models\n\nReproduces Whisper-style training from scratch using public data: OWSM\n\nSupports multiple tasks in a single model\n\nMultilingual speech recognition\n\nAny-to-any speech translation\n\nLanguage identification\n\nUtterance-level timestamp prediction (segmentation)\n\nDNN Framework\n\nFlexible network architecture thanks to Chainer and PyTorch\n\nFlexible front-end processing thanks to kaldiio and HDF5 support\n\nTensorboard-based monitoring\n\nDeepSpeed-based large-scale training\n\nSee ESPnet2.\n\nIndependent from Kaldi/Chainer, unlike ESPnet1\n\nOn-the-fly feature extraction and text processing when training\n\nSupporting DistributedDataParallel and DaraParallel both\n\nSupporting multiple nodes training and integrated with Slurm or MPI\n\nSupporting Sharded Training provided by fairscale\n\nA template recipe that can be applied to all corpora\n\nPossible to train any size of corpus without CPU memory error\n\nESPnet Model Zoo\n\nIntegrated with wandb\n\nIf you intend to do full experiments, including DNN training, then see Installation.\n\nIf you just need the Python module only:\n\n# We recommend you install PyTorch before installing espnet following https://pytorch.org/get-started/locally/ pip install espnet # To install the latest # pip install git+https://github.com/espnet/espnet # To install additional packages # pip install \"espnet[all]\"\n\nIf you use ESPnet1, please install chainer and cupy.\n\npip install chainer==6.0.0 cupy==6.0.0 # [Option]\n\nYou might need to install some packages depending on each task. We prepared various installation scripts at tools/installers.\n\n(ESPnet2) Once installed, run wandb login and set --use_wandb true to enable tracking runs using W&B.\n\nDocker Container\n\ngo to docker/ and follow instructions.\n\nThank you for taking the time for ESPnet! Any contributions to ESPnet are welcome, and feel free to ask any questions or requests to issues. If it's your first ESPnet contribution, please follow the contribution guide.\n\nASR results\n\nexpand\n\nWe list the character error rate (CER) and word error rate (WER) of major ASR tasks.\n\nTask CER (%) WER (%) Pre-trained model Aishell dev/test 4.6/5.1 N/A link ESPnet2 Aishell dev/test 4.1/4.4 N/A link Common Voice dev/test 1.7/1.8 2.2/2.3 link CSJ eval1/eval2/eval3 5.7/3.8/4.2 N/A link ESPnet2 CSJ eval1/eval2/eval3 4.5/3.3/3.6 N/A link ESPnet2 GigaSpeech dev/test N/A 10.6/10.5 link HKUST dev 23.5 N/A link ESPnet2 HKUST dev 21.2 N/A link Librispeech dev_clean/dev_other/test_clean/test_other N/A 1.9/4.9/2.1/4.9 link ESPnet2 Librispeech dev_clean/dev_other/test_clean/test_other 0.6/1.5/0.6/1.4 1.7/3.4/1.8/3.6 link Switchboard (eval2000) callhm/swbd N/A 14.0/6.8 link ESPnet2 Switchboard (eval2000) callhm/swbd N/A 13.4/7.3 link TEDLIUM2 dev/test N/A 8.6/7.2 link ESPnet2 TEDLIUM2 dev/test N/A 7.3/7.1 link TEDLIUM3 dev/test N/A 9.6/7.6 link WSJ dev93/eval92 3.2/2.1 7.0/4.7 N/A ESPnet2 WSJ dev93/eval92 1.1/0.8 2.8/1.8 link\n\nNote that the performance of the CSJ, HKUST, and Librispeech tasks was significantly improved by using the wide network (#units = 1024) and large subword units if necessary reported by RWTH.\n\nIf you want to check the results of the other recipes, please check egs/<name_of_recipe>/asr1/RESULTS.md.\n\nASR demo\n\nexpand\n\nYou can recognize speech in a WAV file using pre-trained models. Go to a recipe directory and run utils/recog_wav.sh as follows:\n\n# go to the recipe directory and source path of espnet tools cd egs/tedlium2/asr1 && . ./path.sh # let's recognize speech! recog_wav.sh --models tedlium2.transformer.v1 example.wav\n\nwhere example.wav is a WAV file to be recognized. The sampling rate must be consistent with that of data used in training.\n\nAvailable pre-trained models in the demo script are listed below.\n\nModel Notes tedlium2.rnn.v1 Streaming decoding based on CTC-based VAD tedlium2.rnn.v2 Streaming decoding based on CTC-based VAD (batch decoding) tedlium2.transformer.v1 Joint-CTC attention Transformer trained on Tedlium 2 tedlium3.transformer.v1 Joint-CTC attention Transformer trained on Tedlium 3 librispeech.transformer.v1 Joint-CTC attention Transformer trained on Librispeech commonvoice.transformer.v1 Joint-CTC attention Transformer trained on CommonVoice csj.transformer.v1 Joint-CTC attention Transformer trained on CSJ csj.rnn.v1 Joint-CTC attention VGGBLSTM trained on CSJ\n\nSE results\n\nexpand\n\nWe list results from three different models on WSJ0-2mix, which is one the most widely used benchmark dataset for speech separation.\n\nModel STOI SAR SDR SIR TF Masking 0.89 11.40 10.24 18.04 Conv-Tasnet 0.95 16.62 15.94 25.90 DPRNN-Tasnet 0.96 18.82 18.29 28.92\n\nSE demos\n\nexpand\n\nYou can try the interactive demo with Google Colab. Please click the following button to get access to the demos.\n\nIt is based on ESPnet2. Pre-trained models are available for both speech enhancement and speech separation tasks.\n\nSpeech separation streaming demos:\n\nST results\n\nexpand\n\nWe list 4-gram BLEU of major ST tasks.\n\nend-to-end system\n\nTask BLEU Pre-trained model Fisher-CallHome Spanish fisher_test (Es->En) 51.03 link Fisher-CallHome Spanish callhome_evltest (Es->En) 20.44 link Libri-trans test (En->Fr) 16.70 link How2 dev5 (En->Pt) 45.68 link Must-C tst-COMMON (En->De) 22.91 link Mboshi-French dev (Fr->Mboshi) 6.18 N/A\n\ncascaded system\n\nTask BLEU Pre-trained model Fisher-CallHome Spanish fisher_test (Es->En) 42.16 N/A Fisher-CallHome Spanish callhome_evltest (Es->En) 19.82 N/A Libri-trans test (En->Fr) 16.96 N/A How2 dev5 (En->Pt) 44.90 N/A Must-C tst-COMMON (En->De) 23.65 N/A\n\nIf you want to check the results of the other recipes, please check egs/<name_of_recipe>/st1/RESULTS.md.\n\nST demo\n\nexpand\n\n(New!) We made a new real-time E2E-ST + TTS demonstration in Google Colab. Please access the notebook from the following button and enjoy the real-time speech-to-speech translation!\n\nYou can translate speech in a WAV file using pre-trained models. Go to a recipe directory and run utils/translate_wav.sh as follows:\n\n# Go to recipe directory and source path of espnet tools cd egs/fisher_callhome_spanish/st1 && . ./path.sh # download example wav file wget -O - https://github.com/espnet/espnet/files/4100928/test.wav.tar.gz | tar zxvf - # let's translate speech! translate_wav.sh --models fisher_callhome_spanish.transformer.v1.es-en test.wav\n\nwhere test.wav is a WAV file to be translated. The sampling rate must be consistent with that of data used in training.\n\nAvailable pre-trained models in the demo script are listed as below.\n\nModel Notes fisher_callhome_spanish.transformer.v1 Transformer-ST trained on Fisher-CallHome Spanish Es->En\n\nMT results\n\nexpand\n\nTask BLEU Pre-trained model Fisher-CallHome Spanish fisher_test (Es->En) 61.45 link Fisher-CallHome Spanish callhome_evltest (Es->En) 29.86 link Libri-trans test (En->Fr) 18.09 link How2 dev5 (En->Pt) 58.61 link Must-C tst-COMMON (En->De) 27.63 link IWSLT'14 test2014 (En->De) 24.70 link IWSLT'14 test2014 (De->En) 29.22 link IWSLT'14 test2014 (De->En) 32.2 link IWSLT'16 test2014 (En->De) 24.05 link IWSLT'16 test2014 (De->En) 29.13 link\n\nTTS results\n\nESPnet2\n\nYou can listen to the generated samples in the following URL.\n\nESPnet2 TTS generated samples\n\nNote that in the generation, we use Griffin-Lim (wav/) and Parallel WaveGAN (wav_pwg/).\n\nYou can download pre-trained models via espnet_model_zoo.\n\nESPnet model zoo\n\nPre-trained model list\n\nYou can download pre-trained vocoders via kan-bayashi/ParallelWaveGAN.\n\nkan-bayashi/ParallelWaveGAN\n\nPre-trained vocoder list\n\nESPnet1\n\nNOTE: We are moving on ESPnet2-based development for TTS. Please check the latest results in the above ESPnet2 results.\n\nYou can listen to our samples in demo HP espnet-tts-sample. Here we list some notable ones:\n\nSingle English speaker Tacotron2\n\nSingle Japanese speaker Tacotron2\n\nSingle other language speaker Tacotron2\n\nMulti English speaker Tacotron2\n\nSingle English speaker Transformer\n\nSingle English speaker FastSpeech\n\nMulti English speaker Transformer\n\nSingle Italian speaker FastSpeech\n\nSingle Mandarin speaker Transformer\n\nSingle Mandarin speaker FastSpeech\n\nMulti Japanese speaker Transformer\n\nSingle English speaker models with Parallel WaveGAN\n\nSingle English speaker knowledge distillation-based FastSpeech\n\nYou can download all of the pre-trained models and generated samples:\n\nAll of the pre-trained E2E-TTS models\n\nAll of the generated samples\n\nNote that in the generated samples, we use the following vocoders: Griffin-Lim (GL), WaveNet vocoder (WaveNet), Parallel WaveGAN (ParallelWaveGAN), and MelGAN (MelGAN). The neural vocoders are based on the following repositories.\n\nkan-bayashi/ParallelWaveGAN: Parallel WaveGAN / MelGAN / Multi-band MelGAN\n\nr9y9/wavenet_vocoder: 16 bit mixture of Logistics WaveNet vocoder\n\nkan-bayashi/PytorchWaveNetVocoder: 8 bit Softmax WaveNet Vocoder with the noise shaping\n\nIf you want to build your own neural vocoder, please check the above repositories. kan-bayashi/ParallelWaveGAN provides the manual about how to decode ESPnet-TTS model's features with neural vocoders. Please check it.\n\nHere we list all of the pre-trained neural vocoders. Please download and enjoy the generation of high-quality speech!\n\nModel link Lang Fs [Hz] Mel range [Hz] FFT / Shift / Win [pt] Model type ljspeech.wavenet.softmax.ns.v1 EN 22.05k None 1024 / 256 / None Softmax WaveNet ljspeech.wavenet.mol.v1 EN 22.05k None 1024 / 256 / None MoL WaveNet ljspeech.parallel_wavegan.v1 EN 22.05k None 1024 / 256 / None Parallel WaveGAN ljspeech.wavenet.mol.v2 EN 22.05k 80-7600 1024 / 256 / None MoL WaveNet ljspeech.parallel_wavegan.v2 EN 22.05k 80-7600 1024 / 256 / None Parallel WaveGAN ljspeech.melgan.v1 EN 22.05k 80-7600 1024 / 256 / None MelGAN ljspeech.melgan.v3 EN 22.05k 80-7600 1024 / 256 / None MelGAN libritts.wavenet.mol.v1 EN 24k None 1024 / 256 / None MoL WaveNet jsut.wavenet.mol.v1 JP 24k 80-7600 2048 / 300 / 1200 MoL WaveNet jsut.parallel_wavegan.v1 JP 24k 80-7600 2048 / 300 / 1200 Parallel WaveGAN csmsc.wavenet.mol.v1 ZH 24k 80-7600 2048 / 300 / 1200 MoL WaveNet csmsc.parallel_wavegan.v1 ZH 24k 80-7600 2048 / 300 / 1200 Parallel WaveGAN\n\nIf you want to use the above pre-trained vocoders, please exactly match the feature setting with them.\n\nTTS demo\n\nESPnet2\n\nYou can try the real-time demo in Google Colab. Please access the notebook from the following button and enjoy the real-time synthesis!\n\nReal-time TTS demo with ESPnet2\n\nEnglish, Japanese, and Mandarin models are available in the demo.\n\nESPnet1\n\nNOTE: We are moving on ESPnet2-based development for TTS. Please check the latest demo in the above ESPnet2 demo.\n\nYou can try the real-time demo in Google Colab. Please access the notebook from the following button and enjoy the real-time synthesis.\n\nReal-time TTS demo with ESPnet1\n\nWe also provide a shell script to perform synthesis. Go to a recipe directory and run utils/synth_wav.sh as follows:\n\n# Go to recipe directory and source path of espnet tools cd egs/ljspeech/tts1 && . ./path.sh # We use an upper-case char sequence for the default model. echo \"THIS IS A DEMONSTRATION OF TEXT TO SPEECH.\" > example.txt # let's synthesize speech! synth_wav.sh example.txt # Also, you can use multiple sentences echo \"THIS IS A DEMONSTRATION OF TEXT TO SPEECH.\" > example_multi.txt echo \"TEXT TO SPEECH IS A TECHNIQUE TO CONVERT TEXT INTO SPEECH.\" >> example_multi.txt synth_wav.sh example_multi.txt\n\nYou can change the pre-trained model as follows:\n\nsynth_wav.sh --models ljspeech.fastspeech.v1 example.txt\n\nWaveform synthesis is performed with the Griffin-Lim algorithm and neural vocoders (WaveNet and ParallelWaveGAN). You can change the pre-trained vocoder model as follows:\n\nsynth_wav.sh --vocoder_models ljspeech.wavenet.mol.v1 example.txt\n\nWaveNet vocoder provides very high-quality speech, but it takes time to generate.\n\nSee more details or available models via --help.\n\nsynth_wav.sh --help\n\nVC results\n\nexpand\n\nTransformer and Tacotron2-based VC\n\nYou can listen to some samples on the demo webpage.\n\nCascade ASR+TTS as one of the baseline systems of VCC2020\n\nThe Voice Conversion Challenge 2020 (VCC2020) adopts ESPnet to build an end-to-end based baseline system. In VCC2020, the objective is intra/cross-lingual nonparallel VC. You can download converted samples of the cascade ASR+TTS baseline system here.\n\nSLU results\n\nexpand\n\nWe list the performance on various SLU tasks and datasets using the metric reported in the original dataset paper\n\nTask Dataset Metric Result Pre-trained Model Intent Classification SLURP Acc 86.3 link Intent Classification FSC Acc 99.6 link Intent Classification FSC Unseen Speaker Set Acc 98.6 link Intent Classification FSC Unseen Utterance Set Acc 86.4 link Intent Classification FSC Challenge Speaker Set Acc 97.5 link Intent Classification FSC Challenge Utterance Set Acc 78.5 link Intent Classification SNIPS F1 91.7 link Intent Classification Grabo (Nl) Acc 97.2 link Intent Classification CAT SLU MAP (Zn) Acc 78.9 link Intent Classification Google Speech Commands Acc 98.4 link Slot Filling SLURP SLU-F1 71.9 link Dialogue Act Classification Switchboard Acc 67.5 link Dialogue Act Classification Jdcinal (Jp) Acc 67.4 link Emotion Recognition IEMOCAP Acc 69.4 link Emotion Recognition swbd_sentiment Macro F1 61.4 link Emotion Recognition slue_voxceleb Macro F1 44.0 link\n\nIf you want to check the results of the other recipes, please check egs2/<name_of_recipe>/asr1/RESULTS.md.\n\nCTC Segmentation demo\n\nESPnet2\n\nCTC segmentation determines utterance segments within audio files. Aligned utterance segments constitute the labels of speech datasets.\n\nAs a demo, we align the start and end of utterances within the audio file ctc_align_test.wav. This can be done either directly from the Python command line or using the script espnet2/bin/asr_align.py.\n\nFrom the Python command line interface:\n\n# load a model with character tokens from espnet_model_zoo.downloader import ModelDownloader d = ModelDownloader(cachedir=\"./modelcache\") wsjmodel = d.download_and_unpack(\"kamo-naoyuki/wsj\") # load the example file included in the ESPnet repository import soundfile speech, rate = soundfile.read(\"./test_utils/ctc_align_test.wav\") # CTC segmentation from espnet2.bin.asr_align import CTCSegmentation aligner = CTCSegmentation( **wsjmodel , fs=rate ) text = \"\"\" utt1 THE SALE OF THE HOTELS utt2 IS PART OF HOLIDAY'S STRATEGY utt3 TO SELL OFF ASSETS utt4 AND CONCENTRATE ON PROPERTY MANAGEMENT \"\"\" segments = aligner(speech, text) print(segments) # utt1 utt 0.26 1.73 -0.0154 THE SALE OF THE HOTELS # utt2 utt 1.73 3.19 -0.7674 IS PART OF HOLIDAY'S STRATEGY # utt3 utt 3.19 4.20 -0.7433 TO SELL OFF ASSETS # utt4 utt 4.20 6.10 -0.4899 AND CONCENTRATE ON PROPERTY MANAGEMENT\n\nAligning also works with fragments of the text. For this, set the gratis_blank option that allows skipping unrelated audio sections without penalty. It's also possible to omit the utterance names at the beginning of each line by setting kaldi_style_text to False.\n\naligner.set_config( gratis_blank=True, kaldi_style_text=False ) text = [\"SALE OF THE HOTELS\", \"PROPERTY MANAGEMENT\"] segments = aligner(speech, text) print(segments) # utt_0000 utt 0.37 1.72 -2.0651 SALE OF THE HOTELS # utt_0001 utt 4.70 6.10 -5.0566 PROPERTY MANAGEMENT\n\nThe script espnet2/bin/asr_align.py uses a similar interface. To align utterances:\n\n# ASR model and config files from pre-trained model (e.g., from cachedir): asr_config=<path-to-model>/config.yaml asr_model=<path-to-model>/valid.*best.pth # prepare the text file wav=\"test_utils/ctc_align_test.wav\" text=\"test_utils/ctc_align_text.txt\" cat << EOF > ${text} utt1 THE SALE OF THE HOTELS utt2 IS PART OF HOLIDAY'S STRATEGY utt3 TO SELL OFF ASSETS utt4 AND CONCENTRATE utt5 ON PROPERTY MANAGEMENT EOF # obtain alignments: python espnet2/bin/asr_align.py --asr_train_config ${asr_config} --asr_model_file ${asr_model} --audio ${wav} --text ${text} # utt1 ctc_align_test 0.26 1.73 -0.0154 THE SALE OF THE HOTELS # utt2 ctc_align_test 1.73 3.19 -0.7674 IS PART OF HOLIDAY'S STRATEGY # utt3 ctc_align_test 3.19 4.20 -0.7433 TO SELL OFF ASSETS # utt4 ctc_align_test 4.20 4.97 -0.6017 AND CONCENTRATE # utt5 ctc_align_test 4.97 6.10 -0.3477 ON PROPERTY MANAGEMENT\n\nThe output of the script can be redirected to a segments file by adding the argument --output segments. Each line contains the file/utterance name, utterance start and end times in seconds, and a confidence score; optionally also the utterance text. The confidence score is a probability in log space that indicates how well the utterance was aligned. If needed, remove bad utterances:\n\nmin_confidence_score=-7 # here, we assume that the output was written to the file `segments` awk -v ms=${min_confidence_score} '{ if ($5 > ms) {print} }' segments\n\nSee the module documentation for more information. It is recommended to use models with RNN-based encoders (such as BLSTMP) for aligning large audio files; rather than using Transformer models that have a high memory consumption on longer audio data. The sample rate of the audio must be consistent with that of the data used in training; adjust with sox if needed.\n\nAlso, we can use this tool to provide token-level segmentation information if we prepare a list of tokens instead of that of utterances in the text file. See the discussion in #4278 (comment).\n\n@inproceedings{watanabe2018espnet, author={Shinji Watanabe and Takaaki Hori and Shigeki Karita and Tomoki Hayashi and Jiro Nishitoba and Yuya Unno and Nelson {Enrique Yalta Soplin} and Jahn Heymann and Matthew Wiesner and Nanxin Chen and Adithya Renduchintala and Tsubasa Ochiai}, title={{ESPnet}: End-to-End Speech Processing Toolkit}, year={2018}, booktitle={Proceedings of Interspeech}, pages={2207--2211}, doi={10.21437/Interspeech.2018-1456}, url={http://dx.doi.org/10.21437/Interspeech.2018-1456} } @inproceedings{hayashi2020espnet, title={{Espnet-TTS}: Unified, reproducible, and integratable open source end-to-end text-to-speech toolkit}, author={Hayashi, Tomoki and Yamamoto, Ryuichi and Inoue, Katsuki and Yoshimura, Takenori and Watanabe, Shinji and Toda, Tomoki and Takeda, Kazuya and Zhang, Yu and Tan, Xu}, booktitle={Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, pages={7654--7658}, year={2020}, organization={IEEE} } @inproceedings{inaguma-etal-2020-espnet, title = \"{ESP}net-{ST}: All-in-One Speech Translation Toolkit\", author = \"Inaguma, Hirofumi and Kiyono, Shun and Duh, Kevin and Karita, Shigeki and Yalta, Nelson and Hayashi, Tomoki and Watanabe, Shinji\", booktitle = \"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations\", month = jul, year = \"2020\", address = \"Online\", publisher = \"Association for Computational Linguistics\", url = \"https://www.aclweb.org/anthology/2020.acl-demos.34\", pages = \"302--311\", } @article{hayashi2021espnet2, title={{ESP}net2-{TTS}: Extending the edge of {TTS} research}, author={Hayashi, Tomoki and Yamamoto, Ryuichi and Yoshimura, Takenori and Wu, Peter and Shi, Jiatong and Saeki, Takaaki and Ju, Yooncheol and Yasuda, Yusuke and Takamichi, Shinnosuke and Watanabe, Shinji}, journal={arXiv preprint arXiv:2110.07840}, year={2021} } @inproceedings{li2020espnet, title={{ESPnet-SE}: End-to-End Speech Enhancement and Separation Toolkit Designed for {ASR} Integration}, author={Chenda Li and Jing Shi and Wangyou Zhang and Aswin Shanmugam Subramanian and Xuankai Chang and Naoyuki Kamo and Moto Hira and Tomoki Hayashi and Christoph Boeddeker and Zhuo Chen and Shinji Watanabe}, booktitle={Proceedings of IEEE Spoken Language Technology Workshop (SLT)}, pages={785--792}, year={2021}, organization={IEEE}, } @inproceedings{arora2021espnet, title={{ESPnet-SLU}: Advancing Spoken Language Understanding through ESPnet}, author={Arora, Siddhant and Dalmia, Siddharth and Denisov, Pavel and Chang, Xuankai and Ueda, Yushi and Peng, Yifan and Zhang, Yuekai and Kumar, Sujay and Ganesan, Karthik and Yan, Brian and others}, booktitle={ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, pages={7167--7171}, year={2022}, organization={IEEE} } @inproceedings{shi2022muskits, author={Shi, Jiatong and Guo, Shuai and Qian, Tao and Huo, Nan and Hayashi, Tomoki and Wu, Yuning and Xu, Frank and Chang, Xuankai and Li, Huazhe and Wu, Peter and Watanabe, Shinji and Jin, Qin}, title={{Muskits}: an End-to-End Music Processing Toolkit for Singing Voice Synthesis}, year={2022}, booktitle={Proceedings of Interspeech}, pages={4277-4281}, url={https://www.isca-speech.org/archive/pdfs/interspeech_2022/shi22d_interspeech.pdf} } @inproceedings{lu22c_interspeech, author={Yen-Ju Lu and Xuankai Chang and Chenda Li and Wangyou Zhang and Samuele Cornell and Zhaoheng Ni and Yoshiki Masuyama and Brian Yan and Robin Scheibler and Zhong-Qiu Wang and Yu Tsao and Yanmin Qian and Shinji Watanabe}, title={{ESPnet-SE++: Speech Enhancement for Robust Speech Recognition, Translation, and Understanding}}, year=2022, booktitle={Proc. Interspeech 2022}, pages={5458--5462}, } @inproceedings{gao2023euro, title={{EURO: ESP}net unsupervised {ASR} open-source toolkit}, author={Gao, Dongji and Shi, Jiatong and Chuang, Shun-Po and Garcia, Leibny Paola and Lee, Hung-yi and Watanabe, Shinji and Khudanpur, Sanjeev}, booktitle={ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, pages={1--5}, year={2023}, organization={IEEE} } @inproceedings{peng2023reproducing, title={Reproducing {W}hisper-style training using an open-source toolkit and publicly available data}, author={Peng, Yifan and Tian, Jinchuan and Yan, Brian and Berrebbi, Dan and Chang, Xuankai and Li, Xinjian and Shi, Jiatong and Arora, Siddhant and Chen, William and Sharma, Roshan and others}, booktitle={2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)}, pages={1--8}, year={2023}, organization={IEEE} } @inproceedings{sharma2023espnet, title={ESPnet-{SUMM}: Introducing a novel large dataset, toolkit, and a cross-corpora evaluation of speech summarization systems}, author={Sharma, Roshan and Chen, William and Kano, Takatomo and Sharma, Ruchira and Arora, Siddhant and Watanabe, Shinji and Ogawa, Atsunori and Delcroix, Marc and Singh, Rita and Raj, Bhiksha}, booktitle={2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)}, pages={1--8}, year={2023}, organization={IEEE} } @article{jung2024espnet, title={{ESPnet-SPK}: full pipeline speaker embedding toolkit with reproducible recipes, self-supervised front-ends, and off-the-shelf models}, author={Jung, Jee-weon and Zhang, Wangyou and Shi, Jiatong and Aldeneh, Zakaria and Higuchi, Takuya and Theobald, Barry-John and Abdelaziz, Ahmed Hussen and Watanabe, Shinji}, journal={Proc. Interspeech 2024}, year={2024} } @inproceedings{yan-etal-2023-espnet, title = \"{ESP}net-{ST}-v2: Multipurpose Spoken Language Translation Toolkit\", author = \"Yan, Brian and Shi, Jiatong and Tang, Yun and Inaguma, Hirofumi and Peng, Yifan and Dalmia, Siddharth and Pol{\\'a}k, Peter and Fernandes, Patrick and Berrebbi, Dan and Hayashi, Tomoki and Zhang, Xiaohui and Ni, Zhaoheng and Hira, Moto and Maiti, Soumi and Pino, Juan and Watanabe, Shinji\", booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)\", year = \"2023\", publisher = \"Association for Computational Linguistics\", pages = \"400--411\", }"
    }
}
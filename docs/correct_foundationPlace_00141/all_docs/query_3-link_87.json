{
    "id": "correct_foundationPlace_00141_3",
    "rank": 87,
    "data": {
        "url": "https://link.springer.com/article/10.1007/s41125-024-00100-1",
        "read_more_link": "",
        "language": "en",
        "title": "In Search of a Modus Operandi for Evaluation: Developing a Mixed-Methods Approach for Crisis Team Exercises",
        "top_image": "https://static-content.springer.com/image/art%3A10.1007%2Fs41125-024-00100-1/MediaObjects/41125_2024_100_Fig1_HTML.png",
        "meta_img": "https://static-content.springer.com/image/art%3A10.1007%2Fs41125-024-00100-1/MediaObjects/41125_2024_100_Fig1_HTML.png",
        "images": [
            "https://link.springer.com/oscar-static/images/darwin/header/img/logo-springerlink-39ee2a28d8.svg",
            "https://media.springernature.com/w72/springer-static/cover-hires/journal/41125?as=webp",
            "https://media.springernature.com/w215h120/springer-static/image/art%3Aplaceholder%2Fimages/placeholder-figure-springernature.png",
            "https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-3-319-91791-7?as=webp",
            "https://media.springernature.com/w215h120/springer-static/image/art%3Aplaceholder%2Fimages/placeholder-figure-springernature.png",
            "https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs41125-024-00100-1/MediaObjects/41125_2024_100_Fig1_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs41125-024-00100-1/MediaObjects/41125_2024_100_Fig2_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs41125-024-00100-1/MediaObjects/41125_2024_100_Fig3_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs41125-024-00100-1/MediaObjects/41125_2024_100_Fig4_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs41125-024-00100-1/MediaObjects/41125_2024_100_Fig5_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs41125-024-00100-1/MediaObjects/41125_2024_100_Fig6_HTML.png",
            "https://link.springer.com/oscar-static/images/logo-springernature-white-19dd4ba190.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Patricia M"
        ],
        "publish_date": "2024-06-10T00:00:00",
        "summary": "",
        "meta_description": "In recent years, there has been an increase in critical situations such as crisis and disasters in Germany. Due to the country's low-crisis history and",
        "meta_lang": "en",
        "meta_favicon": "/oscar-static/img/favicons/darwin/apple-touch-icon-92e819bf8a.png",
        "meta_site_name": "SpringerLink",
        "canonical_link": "https://link.springer.com/article/10.1007/s41125-024-00100-1",
        "text": "Exercises in the context of civil protection, disaster and emergency management have been scientifically accompanied for many years at the higher education Institute of Public Safety and Emergency Management (IPSEM). Both large, inter-organizational and small, intra-organizational exercises were examined and evaluated. Due to the interdisciplinary orientation, safety engineering and social science approaches as well as experiences from prior evaluations were incorporated into the respective evaluations as well as in the approach described in this text.\n\n3.1 Background and Design\n\nDuring the discussions between the author team and the exercise leaders of the above-mentioned exercise, communication and leadership emerged as topics of interest, in addition to the evaluation of the newly introduced staff regulations. The main question was how these two aspects were influenced by the staff regulations during the exercise. As the literature review results above show, leadership and communication are two relevant topics when crisis management is practiced. On the one hand, everything within staff work ultimately takes place in some form of communication, and on the other hand, staffs are a management tool or support tool for leaders in themselves (Behrmann et al. 2022; Gißler 2020, 2021). Both of these aspects are proving to be all the more challenging in the case of interdisciplinary and inter-organizational CT such as those of public administrations (Bach et al. 2023). This is, among other things, due to different organizational logics coming together. The members of (interorganizational) CT are recruited from parts of the public administration, fire department, police, aid organizations and other affected stakeholder groups (e.g., energy, electricity operators and water supply) (Behrmann et al. 2022). In order to prepare well-founded decisions, they generally have to develop a joint picture of the situation and ideally a shared mental model. To do this, they exchange information on the situation from their original organizations within the crisis management team in (usually structured) situation reports,Footnote 3 bilateral communication, e-mail exchange, telephone and radio, if necessary, etc. (based on recommendations like written in the FwDV100) By creating temporarily valid common organizational and sensory structures such as goals, clearly defined responsibilities, coordinated communication channels and modes, etc., as is the case in staff exercises, the clash of organizational logics is cushioned (e.g., Hofinger 2008), but potentials for friction and conflict still remain.\n\nVerbal, non-verbal and technically mediated communication in the sense of processes in which information is transferred between sender and receiver thus forms the basis of this evaluation (Behrmann et al. 2022). In order to be able to better handle the topics of interest research-related, an additional sociotechnical analysis approach was taken. The approach has become established in the context of event security and safety research at IPSEM following the assumption that inter- and intraorganizational safety and security production can be made visible and analyzable in terms of human, technological and organizational elements of systems and their interactions (Pasmore et al. 2019; Schütte et al. 2022a; Schütte and Willmes 2022). Human elements (category “people”) stand for the people themselves, their behavior, human factors, personal characteristics and traits, knowledge and qualification, social ties, social measures, etc. Technological aspects (“technology”) comprise technologies, engineering, technical systems, building constructions, rooms, infrastructure and premises. Structures and processes, i.e. (formalized) organizational design, are organizational components (“organization”) (Schütte et al. 2022a; Schütte and Willmes 2022). These three elements were considered as central analysis categories for communication and leadership within the survey, interview and observation instruments (see below).\n\nUltimately, and in this CT framework exercise for the first time, the research team used a design with five phases, alternating quantitative and qualitative methods (see Fig. 1). The goal was to examine the subject of the staff regulations as comprehensively as possible. To this end, quantitative and qualitative research methods were combined quasi-sequentially, and the data obtained and, above all, the results were triangulated with one another (Flick 1992, 2011). More details can be found in the following explanations of the individual steps.\n\n3.2 Instruments for Data CollectionFootnote 4\n\n3.2.1 Step 1 and 3: Standardized Online Survey “Before” and “After”\n\nThe idea of the survey in advance (“Before”) was to gain a mostly unbiased view from those involved in the exercise on the staff regulations and the exercise itself. Therefore, it was conducted before the exercise (from one week until the day before the exercise). The focus was on the participants' state of preparation, expectations in general and in particular technical, social/personal as well as organizational aspects. To this end, they were asked about the following: (1) experience in crisis management, real operations and exercises, and preparations, (2) emotions related to the exercise, (3) the extend of knowledge about the staff regulations, (4) expectations regarding room and technology, as well as social (e.g., personnel composition, togetherness), and organizational (such as staff regulation, information management, coordination) aspects. The latter are based on headings in the staff work manual Handbuch Stabsarbeit from Hofinger and Heimann (2022) which presents them as significant components of CT. In addition to a majority of closed questions, some open questions were also included, for example on an individual assessment of the staff regulations, in order to gain deeper insights here. The survey was distributed among exercise control members, participants of the staff framework exercise, and those of the operational-tactical staffs exercising alongside, but remotely, on the same scenario. In total, this encompassed a population of around 150 individuals.\n\nThe second survey (“After”) took place immediately after the exercise over the course of one week. Similar to the pre-exercise survey, the same topics (1–4) were asked again to see to what extent changes in views and lessons learned, e.g., about the staff regulations, could be determined here. Topic (4) was expanded to include questions on communication and leadership. Questions on the latter topic were oriented to explanations of success-critical factors of leadership performance (e.g., Gißler 2020, 2021; Hofinger and Heimann 2022; Linnenbürger 2020). The topic of communication was underpinned with excerpts from a tested and validated survey tool from a previous project at IPSEM (Behrmann et al. 2022; Hofinger and Heimann 2022; Linnenbürger 2020). The evaluation was carried out using descriptive statistics. The open questions were evaluated via qualitative category-based content analysis, analog to the interviews described in step 4.\n\n3.2.2 Step 2: Exercise Observation\n\nThe observation represented a peculiarity in the case. At IPSEM, open observations supported by a very rough guideline are usually conducted in the field, e.g., at major music events (Schütte et al. 2022a; Schütte and Willmes 2022). This was not possible in the present case, as exercise developers and those responsible from the public administration wanted to limit potential disturbances by external persons as much as possible. Therefore, only two “observers” were allowed to participate silently at places (initially) assigned to them. Due to the size of the room and up to 48 participants during the three exercise days, it was hardly possible to closely monitor the communicated content in the room in a qualitative manner. It was decided to implement a quantitative observation with possible qualitative additions. The observers recorded who communicated with whom at what time in a prepared excel list (communication processes could be verbally, by telephone or radio). In addition, situational characteristics were written down in more detail (e.g., situation presentations, video interventions, accumulation of people somewhere in the room) on classic observation sheets, allowing for comments e.g. on individual stress situations, peculiarities in communication processes or qualitative observations of leadership. Thereby, communication in the room could be counted and complemented by observation of general behavior and interactions. These observations have been used to add exercise-specific questions to the guided interviews (see step 4). To facilitate the mixture of the quantitative and qualitative observation during an exercise or “real life” CT settings, the IPSEMdeveloped an app, where communication processes between the CT members can be recorded even more quickly and notes can be included at any time and each input (quantitative or qualitative) has its own automated time stamp. The app has been tested and improved during exercise evaluations between May and December 2023.\n\nThe communication processes were analyzed in three different manners, anonymized, partly anonymized and in full. The (partly) anonymized results do not allow conclusions to be drawn about individual participants, it simply describes the extend of detail used for the evaluation: The participants can be divided into four groups, “lead”, “subject groups”, “administrative services” and “expert advisors”.Footnote 5 In the anonymized analysis, the communication processes between these four groups were considered, not taking into account different subgroups. Those were part of the partly anonymized analysis whereas the full assessment allows for individual assessment of members of the sub-groups. Due to the fact that most subgroups only consist of one to three members, these findings could be used to draw conclusions about individuals and are therefore not used in publications or other presentations of results. All results can be displayed at a certain point during the exercise (in 5-min time stamps) or cumulated until a certain point during the exercise. The first being especially important for the analysis of leadership and leadership processes, the latter for an analysis of communication processes and channels in general. For both use cases there are examples in the “data evaluation” section of this text.\n\n3.2.3 Step 4: Guided Interviews\n\nAfter the exercise, guided interviews were conducted with volunteers from the exercise participants. For this purpose, the representatives present from all subject areas, administrative services, expert advisors and lead were approached (48 people) and asked for an interview. The interview guideline was set up aligned to the surveys (Step 1 and 3). However, only open-ended questions or narrative prompts are formulated here, so that the interviewees had the opportunity to speak comprehensively and to decide independently on the direction and depth of their statements. The interviews were analyzed using a qualitative category-based content analysis (Gläser and Laudel 2010; Mayring 2015). For this purpose, a deductive-inductive approach was used. Deductive categories, i.e., categories determined on the basis of preliminary theoretical considerations (see above), were communication and leadership with the respective subcategories of people, technology, and organization. Inductive categories were formed on the basis of the material runs through the interviews. In the last step, the categories were combined into essential statements against the background of the text passages found. Ultimately, this was always oriented to answering the question of the usefulness of the newly introduced staff regulations.\n\n3.2.4 Step 5: Triangulation\n\nIn order to not let the collected data and obtained results stand on their own, the research team planned a triangulation of methods, data and results (after the elicitation and individual evaluations), i.e., a kind of method, data or result linkage. The team wanted to be able to cover the subject under investigation as comprehensively as possible and to enrich and complete the findings (Denzin 1989, 2008; Denzin and Lincoln 2000; Flick 1992, 2011). Therefore, the dominating triangulation logic was that quantitative and qualitative research could support and complement each other (Bryman 1988, 1992; Flick 2011).\n\nThis use of synergies is shown in various aspects of the evaluation of the results. Using the same categories for the qualitative content analysis of the surveys and the interviews is one step, allowing for extended personal views and individual perceptions by the participants. Furthermore, expressing these views in a face-to-face, rather spontaneous situation of an interview possibly appeals to some people more than writing down their individual thoughts in a structured way during a survey; and vice versa. Synchronizing the survey questions with the interview guideline, as stated earlier, is another important aspect, acknowledging the advantages of both qualitative and quantitative research methods and backing up or questioning results from the one with those from the other. The comparison between individual perceptions before and after an exercise can be used to differentiate between expectations and retrospective assessments. The individual perceptions of the participants (29 people completed the “Before” survey, 42 the “After” survey and 17 participants agreed to interviews) were contrasted with the results of the observations. Do these individual impressions match the actual observed communication processes? Do leadership processes and tools, assessed during the surveys and interviews, have impact on the observable activities in the CT?\n\nTo give a clearer picture of what the combination of the methods and their results can mean in this context, the following section shows exemplary results combining the used methods.\n\n3.3 Data Evaluation: Exemplary Results and their Meanings\n\nAs described, IPSEM works with three different methods (a survey before the exercise, a survey afterwards, the observation of the exercise itself and interviews with the participants afterwards) with the ultimate goal to describe and evaluate communication and leadership aspects during an exercise. Since the evaluation is still work-in-progress, this passage focuses on the communication category for the exemplary results. The interaction of these methods will be shown here using two examples from the above-mentioned, 3-day exercise for a public administration in Germany we evaluated in 2022: (A) (experienced) communication and (B) communication in relation to leadership tools. 44 to 48 persons attended the exercise each day, 21 to 23 working in the subject groups, 7 to 8 in the administrative services, 11 to 13 as expert advisors and 4 in the exercise lead. The exercise control (management, white cell, etc.) comprised of approximately 40 persons, seated in adjoining rooms.\n\nExample A “Communication”.\n\nStarting with two matching survey questions „The conditions for functioning communication among the participants are good “ (before) and „The staff regulation provides a sufficient framework for communication “ (after), Fig. 2 compares results.\n\nThe data show that before the exercise, the participants were rather optimistic regarding functioning communication, 76% stated “fully applies” or “applies” to the question. Here, the familiar facilities and some reading knowledge of the newly introduced staff regulations went into the assessments. Also, 83% of the participants thought beforehand, that the composition of the staff was sensibly chosen, indicating that the majority of the participants were familiar with each other—either from a work context or due to the workshops that were carried out in preparation of the exercise.\n\nAfter the exercise, when the participants had experience with using (not only reading) the staff regulations, people were slightly more negative and, maybe most importantly, more uncertain (31% stated “I don’t know” here) if communication is well addressed. This indicates at least some negative experiences of communication processes during the exercise.\n\nThe consultation of the respective quantitative data from the observation of the exercise can help to interpret these statements. Figure 3 shows the anonymized, cumulative communication processes divided into the different communication channels for a general overview. Also, for this purpose, we provide some statistics of overall communication processes in Table 1.\n\nCommunication processes during a CT meeting (be it for an exercise or an actual crisis) are not evenly distributed through the participant groups, due to the different tasks and responsibilities they have. During the exercise at hand, but also being the case for most exercises the IPSEM team were allowed to attend, by far the most communication processes took place between the members of the subject groups (S1 to S6, summarized here): accumulated over the exercise days, they communicated 509 times with each other, gave information to or discussed with the lead 85 times, and discussed with or asked the expert advisors and administrative services 164 + 51 = 215 times. They were also the most addressed group by the other participants.\n\nFrom the figures in Table 1 it becomes obvious that the participants were able to increase the interactions with other particoants throughout the days. The so-called “chaos phase” in the beginning is a rather well-known phenomenon during exercises, sometimes even during real crises, where people have to adjust to the situation and are more preoccupied with themselves and their respective tasks. But the fact that the participants were able to increase the communication processes per hour by a factor of 1.6 between days 1 and 2 and 1.8 between days 1 and 3 demonstrates the good communication skills of the CT. Assuming a medium of 46 participants, the figures show 3.5 communication processes per participant and hour on day 3. Taking into account that most of these processes are not quick “answer-response” issues, but rather complex discussions and the fact that the subject groups, having the most responsibilities in the CT, take part in 75% of these communication processes (extracted from Fig. 2), but only accounted for approximately 50% of the participants, a high communication load for this group can be stated. From a quantitative perspective, these figures do not suggest insufficient conditions for communicating. They are also not able to evaluate the quality of the communication processes.\n\nTo get to the bottom of these assumingly inconsistent results, more information from the participants can be taken into account. From the “after” survey we know that 38% of the participants found all information received was relevant to them, for 60% this rather did not or did not apply. Also, only 24% found that a lot of the communication with other participants was necessary for their work, for 67% this rather did not or did not apply.\n\nIt can therefore be assumed that the participants thought correctly that the premises (and other parameters known beforehand, e.g., familiar participants) provide for good communication conditions, but once the “chaos phase” was over and communication processes started picking up, it became clear that not everybody knew what to communicate with whom. The sheer number of communication processes especially regarding the subject groups therefore indicates a tendency to ineffectiveness of communication.\n\nThe interviews also provide evidence for this assumption. The interview guideline aimed specifically at the staff regulation and if and where it was helpful for crisis management during the exercise or where weaknesses lie.\n\nFor communication processes, the answers were ambivalent: most interviewees stated that after the “chaos phase”, from the second day on, communication became “better”, they “learned” which information was important for whom, how to present the information during situation reports or which other staff members had potentially important information for them, i.e., the communication processes gained in quality or effectiveness. But the aforementioned ineffectiveness also manifests among the interviewees:\n\n“After a while, I also noticed what was important and what wasn't with the situation report […].” (Interviews, “subject group 3”, 2022)\n\n“But certain information actually did not accumulate with me. […] that you can't really get a good feel for how the development actually is on the ground right now.” (Interviews, “expert advisor 7”, 2022)\n\n“In my opinion, this was not yet fully developed. Because some information simply arrived in duplicate and triplicate.” (Interviews, “subject group 2”, 2022)\n\nRegarding one of the main goals of the exercise evaluation, the assessment of the staff regulations, it becomes equally clear that those staff regulations can only provide a framework and the participants mostly seem to have a realistic view on what staff regulations can or cannot provide for:\n\n“However, this can be presented in so well in the staff regulation. It just has to get into people's heads. […] Simply to train […] accordingly. That they do know: It's in the staff regulation, but they've simply internalized it.” (Interviews, “subject group 1”, 2022)\n\n“The staff regulations should provide for substitution rules and information on which subject groups have to consult with each other” (Interviews, “subject group 2”, 2022)\n\n“The staff regulations need to be internalized […]. […] smaller exercises just for information in- and output make sense“ (Interviews, „subject group 1“, 2022)\n\n“[…] more knowledge about the expert advisors” (Interviews, “expert advisor 7”, 2022)\n\nThose “wishes” regarding the staff regulations that we requested during the interviews seem both realistic and reasonable. In conclusion, it can be said that the observed and perceived inefficiency in internal staff communication was realistically assessed by the participants and decreased in the course of the exercise days. The staff regulations were helpful only to a certain extent, the participants provided some useful suggestions for improvement. But “overregulating” has to be avoided and a lot of the participants concluded that exercising is preferable to a too much detailed and therefore no longer generally applicable framework.\n\nExample B “leadership tools and their effect on communication”.\n\nA small but very specific example of how leadership tools affect communication processes is the comparison of those processes before and after so-called situation reports.\n\nFrom the observation it is known at what time situation reports took place. The communication processes (again anonymized to the four subgroups) in the half hour before and the half hour after provide information about the impact of these reports (see Figs. 4 and 5).\n\nEspecially for the already as the most actively communicating members of the CT identified “subject groups”, the difference between the two 30-min situational snapshots is evident. In total, the subject group members address more than twice the amount of people after the situation report than before (47 vs. 21). Of course, there is no information on the quality of these interactions: are they questions? Information given? Discussions? Due to the fact that the staff lead ends each situation report with new (or repeated) tasks for the subject groups and some other staff members, it is likely though that a lot of the interactions are information exchanges.\n\nThis can be combined with statements such as cited above already: “After a while, I also noticed what was important and what wasn't with the situation report […].” (Interviews, “subject group 3”, 2022) or.\n\n“[…] these situation reports that came in […] with a lot of accessories, but also a lot of important things, where you have to sort out what is important […].” (Interviews, “expert advisor 5”, 2022) or\n\n“The situation reports. […] we can definitely get better at this” (Interviews, „subject group 1“, 2022).\n\nSorting out the important information from the situation reports and giving stringent, short and concise situation reports were two aspects described as rather difficult in the interviews. The sharp increase in communication processes may therefore be partly due to the fact that the information received had to be reviewed and classified.\n\nThe “After” survey included two questions regarding the quality of situation reports during the exercise. The results of these questions are shown in Fig. 6 and are another step in the assessment of the question at hand, how a leadership tool such as a situation report affects communication.\n\nAgain, similar to the triangulation in example A, the results send slightly different signals that have to be addressed. 71 resp. 83% of the survey participants had a rather positive image of the situation reports. Here, the drastic rise in communication processes after a report indicates that the CT knew their tasks and were eager, willing and able to dive into them once the situation report was finished.\n\nOf course, the truth will most probably lie somewhere in the middle. Summarizing information and making it accessible for everybody is both a helpful and a difficult task, that therefore has to be practiced. And obviously, the more practiced a staff member is the more effective they can display the relevant information during their reports and also distinguish relevant from not so relevant information in others.\n\nThe example shows that a triangulation helps to provide a “360°-view” of a certain topic, such as communication in a CT. Looking only at the results displayed in Fig. 6 would give an incomplete, mainly positive, picture. The objectively measurable communication processes help with classification and the interviews show nuances and capture personal, internal assessments.\n\nWhat do those findings say about the quality of the proposed evaluation approach?\n\nBoth these examples do not show surprising results. Statements like “communication has been ineffective” or “I didn’t know who to address” or “I didn’t get all the information I needed for a comprehensive situational picture” are often heard after exercises as well as during review meetings of actual CT. But even if individual findings from the respective individual methods are not surprising in themselves, their extended consideration through the results of the application of other methods certainly leads to interesting insights. In terms of the concrete example here, it was only through the interviews and the questionnaires that it became clear why an increased level of communication was observed during the exercise. In this case study, it was partly due to the administrative backgrounds and logics of the individual members, for whom, for example, the staff regulations were not formulated clearly enough and offered too little concrete (and legally secure) guidance. Additionally, some interviewees also lacked clear structural guidelines for meetings and exchanges from the CT leaders. The survey results underlined this in that they show that in many cases the right information was not transferred to the right places and added also the aspect of technical difficulties. Such triangulated results are an initial indication that a multi-method evaluation approach as well as a triangulation of methods and results can deliver empirically sound results. In addition, they level out bias that are inherent in the nature of the individual methods. The methods used are long validated and established in empirical social research. Beyond that, however, (combined) use of different methods enables a comprehensive view of a topic such as communication or leadership (or their combination) in CT. And the socio-technical analysis framework is open enough to be supplemented by different topics and thus enable individual adaptation to specific training needs. In the opinion of the authors, the triangulation method in this context is particularly valuable because it allows an evaluation to become comprehensive and can complete it with regard to the subject matter. But the it is still rare in evaluation processes and also not yet fully discussed in the fields of empirical social research. More research and elaboration with regard to the development of standards, reliable measurement and quality criteria still needs to be done (Beerens 2021).\n\nThe experience of the team of authors in accompanying exercises shows also that in science-to-practice-transfer-aspects practitioners appreciate the combination of results from different data sources, particularly for the follow-up of exercises and ask for them in order to have a valid basis for possible changes. The authors believe that the partial-standardized approach described here not only is able to support this transfer by allowing for a standardized, easy-to-apply display of results. It can also improve the quality of results due to a high objectivity, the immanent learning process of the scientists and the comparability of results over a large span of time (and a large set of executive scientists).\n\nSuch an approach has further implications for research and practice in this field as well. The systematic survey, analysis and a practice-oriented elaboration have the potential to increase the acceptance of scientists as companions of exercises and confidence in the scientific foundation itself. In the case of the authors, this can be seen, for example, in the fact that they are repeatedly asked by certain stakeholders to accompany exercises and that they are also recommended for exercises of other stakeholders. However, this is only possible if the practical side also considers and implements a thorough follow-up which is not always the case (Bach et al. 2023). Only then, exercise leaders, for example, are able to identify results that can help to improve crisis management, design even more suitable exercises or refine staff regulations to make them even more fitting to their context. Otherwise, there is a risk that the results will remain in the scientific “drawer” be forgotten or misinterpreted. On the other hand, for scientists it also means a certain feeling for the field of practice and its requirements for the presentation of results (Baroutsi 2023). What can make scientific evaluations uninteresting for practitioners are, for example, a potential lack of practical relevance, a lack of clarity about their added value and benefits (especially in relation to the (transactional) costs incurred by practitioners) and the long time it takes to produce them. The authors worked out the present evaluation approach in the course of requested scientific exercise support. In the situation described above, it worked well. One reason is certainly that the chosen evaluation approach was at least partially tailored to the exercise at hand after consultation with the exercise planners as well as the involved administrative management. It helped that the scientists already had experience with exercise evaluations and the corresponding methodological skills. The mentioned exercise was a kind of practice field to test the instruments in detail and the approach as a whole.\n\nThe latter point is of course a central limitation of this work. At the time this article was written, the instruments had not yet been tested in other exercises. It is therefore not clear to what extent this actually provides meaningful opportunities for comparison. In addition, after the first trial, the need for adjustments became apparent which are currently being implemented. The first step is to define and elaborate the individual evaluation modules as a reasonably fixed framework. One example is the quantitatively oriented observation. Based on this, an app already was designed for subsequent exercises. It not only facilitates the recording of communication activities, but also provides additional information. A timestamp is automatically stored for each activity. It is also possible to select the sender and receiver as well as special communication forms (e.g., situation report) and to make notes on them. The next tool to be adapted will be the survey. Here, the questions should be more homogenized to allow for even better “before-after” comparisons This means that more questions are included here, which can be asked both before and after the exercise in order to recognize developments. For example, the pre-exercise survey asks about expectations in terms of technical, analogue, personnel and spatial equipment during the exercise, and the post-exercise survey asks about what the exercise participants actually encountered. This has already been implemented in other surveys. However, the socio-technical perspective as a theoretical framework is retained as a common anchor of all instruments, while the topics to be examined are to remain interchangeable. Thus, it is planned to prepare topics such as communication, leadership, information management and cooperation in terms of content in question blocks so that they can be selected according to the practice interests. Briefings for other researchers could then be used to explain the evaluation framework and its use for their projects and questions within. Although the idea was to develop a modularized and phase-related evaluation design, which consists of quantitative and qualitative research tools or modules, that can be used primarily formatively, i.e., process-accompanying, either as individual self-contained modules, diverse module combinations or as an overall design, for each exercise, it makes sense to check individually which measures are suitable on the one hand and feasible on the other. This makes the approach relatively resource-intensive at present. The triangulated evaluation also still requires some time, which practitioners often do not have until they need results. It will therefore be necessary to reduce the amount of resources and time required in future, possibly with the help of practitioners in the context of transdisciplinary exchange formats.\n\nIn the near future, other exercises will be evaluated using parts of the approach. Further exercise accompaniments with the whole approach have already been scheduled. On this basis, the organizational and working forms of CT can be compared, similarities and differences in crisis management can be identified, but also success factors, e.g., for communication, leadership and collaboration in CT exercises. The next step after testing, adaptation and evaluation by the authors will be to have it tested throughout the own institute and then also by external scientists and, if possible, to develop common standards for scientific exercise evaluations in the crisis management field—outside the consulting industry."
    }
}
{
    "id": "dbpedia_5418_2",
    "rank": 78,
    "data": {
        "url": "https://www.science.gov/topicpages/r/resolution%2Baerial%2Bimagery",
        "read_more_link": "",
        "language": "en",
        "title": "resolution aerial imagery: Topics by Science.gov",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://www.science.gov/scigov/desktop/en/images/SciGov_logo.png",
            "https://www.science.gov/topicpages/r/images/arrow-up.gif",
            "https://www.science.gov/topicpages/r/images/arrow-down.gif"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Urban cover mapping using digital, high-resolution aerial imagery\n\nTreesearch\n\nSoojeong Myeong; David J. Nowak; Paul F. Hopkins; Robert H. Brock\n\n2003-01-01\n\nHigh-spatial resolution digital color-infrared aerial imagery of Syracuse, NY was analyzed to test methods for developing land cover classifications for an urban area. Five cover types were mapped: tree/shrub, grass/herbaceous, bare soil, water and impervious surface. Challenges in high-spatial resolution imagery such as shadow effect and similarity in spectral...\n\nAutomated Identification of River Hydromorphological Features Using UAV High Resolution Aerial Imagery.\n\nPubMed\n\nCasado, Monica Rivas; Gonzalez, Rocio Ballesteros; Kriechbaumer, Thomas; Veal, Amanda\n\n2015-11-04\n\nEuropean legislation is driving the development of methods for river ecosystem protection in light of concerns over water quality and ecology. Key to their success is the accurate and rapid characterisation of physical features (i.e., hydromorphology) along the river. Image pattern recognition techniques have been successfully used for this purpose. The reliability of the methodology depends on both the quality of the aerial imagery and the pattern recognition technique used. Recent studies have proved the potential of Unmanned Aerial Vehicles (UAVs) to increase the quality of the imagery by capturing high resolution photography. Similarly, Artificial Neural Networks (ANN) have been shown to be a high precision tool for automated recognition of environmental patterns. This paper presents a UAV based framework for the identification of hydromorphological features from high resolution RGB aerial imagery using a novel classification technique based on ANNs. The framework is developed for a 1.4 km river reach along the river Dee in Wales, United Kingdom. For this purpose, a Falcon 8 octocopter was used to gather 2.5 cm resolution imagery. The results show that the accuracy of the framework is above 81%, performing particularly well at recognising vegetation. These results leverage the use of UAVs for environmental policy implementation and demonstrate the potential of ANNs and RGB imagery for high precision river monitoring and river management.\n\nAnalysis of the impact of spatial resolution on land/water classifications using high-resolution aerial imagery\n\nUSGS Publications Warehouse\n\nEnwright, Nicholas M.; Jones, William R.; Garber, Adrienne L.; Keller, Matthew J.\n\n2014-01-01\n\nLong-term monitoring efforts often use remote sensing to track trends in habitat or landscape conditions over time. To most appropriately compare observations over time, long-term monitoring efforts strive for consistency in methods. Thus, advances and changes in technology over time can present a challenge. For instance, modern camera technology has led to an increasing availability of very high-resolution imagery (i.e. submetre and metre) and a shift from analogue to digital photography. While numerous studies have shown that image resolution can impact the accuracy of classifications, most of these studies have focused on the impacts of comparing spatial resolution changes greater than 2 m. Thus, a knowledge gap exists on the impacts of minor changes in spatial resolution (i.e. submetre to about 1.5 m) in very high-resolution aerial imagery (i.e. 2 m resolution or less). This study compared the impact of spatial resolution on land/water classifications of an area dominated by coastal marsh vegetation in Louisiana, USA, using 1:12,000 scale colour-infrared analogue aerial photography (AAP) scanned at four different dot-per-inch resolutions simulating ground sample distances (GSDs) of 0.33, 0.54, 1, and 2 m. Analysis of the impact of spatial resolution on land/water classifications was conducted by exploring various spatial aspects of the classifications including density of waterbodies and frequency distributions in waterbody sizes. This study found that a small-magnitude change (1â1.5 m) in spatial resolution had little to no impact on the amount of water classified (i.e. percentage mapped was less than 1.5%), but had a significant impact on the mapping of very small waterbodies (i.e. waterbodies â¤ 250 m2). These findings should interest those using temporal image classifications derived from very high-resolution aerial photography as a component of long-term monitoring programs.\n\nUnmanned Aerial Vehicles Produce High-Resolution Seasonally-Relevant Imagery for Classifying Wetland Vegetation\n\nNASA Astrophysics Data System (ADS)\n\nMarcaccio, J. V.; Markle, C. E.; Chow-Fraser, P.\n\n2015-08-01\n\nWith recent advances in technology, personal aerial imagery acquired with unmanned aerial vehicles (UAVs) has transformed the way ecologists can map seasonal changes in wetland habitat. Here, we use a multi-rotor (consumer quad-copter, the DJI Phantom 2 Vision+) UAV to acquire a high-resolution (< 8 cm) composite photo of a coastal wetland in summer 2014. Using validation data collected in the field, we determine if a UAV image and SWOOP (Southwestern Ontario Orthoimagery Project) image (collected in spring 2010) differ in their classification of type of dominant vegetation type and percent cover of three plant classes: submerged aquatic vegetation, floating aquatic vegetation, and emergent vegetation. The UAV imagery was more accurate than available SWOOP imagery for mapping percent cover of submergent and floating vegetation categories, but both were able to accurately determine the dominant vegetation type and percent cover of emergent vegetation. Our results underscore the value and potential for affordable UAVs (complete quad-copter system < 3,000 CAD) to revolutionize the way ecologists obtain imagery and conduct field research. In Canada, new UAV regulations make this an easy and affordable way to obtain multiple high-resolution images of small (< 1.0 km2) wetlands, or portions of larger wetlands throughout a year.\n\nEstimating Discharge in Low-Order Rivers With High-Resolution Aerial Imagery\n\nNASA Astrophysics Data System (ADS)\n\nKing, Tyler V.; Neilson, Bethany T.; Rasmussen, Mitchell T.\n\n2018-02-01\n\nRemote sensing of river discharge promises to augment in situ gauging stations, but the majority of research in this field focuses on large rivers (>50 m wide). We present a method for estimating volumetric river discharge in low-order (<50 m wide) rivers from remotely sensed data by coupling high-resolution imagery with one-dimensional hydraulic modeling at so-called virtual gauging stations. These locations were identified as locations where the river contracted under low flows, exposing a substantial portion of the river bed. Topography of the exposed river bed was photogrammetrically extracted from high-resolution aerial imagery while the geometry of the remaining inundated portion of the channel was approximated based on adjacent bank topography and maximum depth assumptions. Full channel bathymetry was used to create hydraulic models that encompassed virtual gauging stations. Discharge for each aerial survey was estimated with the hydraulic model by matching modeled and remotely sensed wetted widths. Based on these results, synthetic width-discharge rating curves were produced for each virtual gauging station. In situ observations were used to determine the accuracy of wetted widths extracted from imagery (mean error 0.36 m), extracted bathymetry (mean vertical RMSE 0.23 m), and discharge (mean percent error 7% with a standard deviation of 6%). Sensitivity analyses were conducted to determine the influence of inundated channel bathymetry and roughness parameters on estimated discharge. Comparison of synthetic rating curves produced through sensitivity analyses show that reasonable ranges of parameter values result in mean percent errors in predicted discharges of 12%-27%.\n\nVernal Pools Detection Using High-Resolution LiDAR Data and Aerial Imagery in Hubbardston, Massachusetts\n\nNASA Astrophysics Data System (ADS)\n\nJiang, Jiaxin\n\nVernal pool refers to temporary or semi-permanent pools that occur in surface depressions without permanent inlets or outlets. Because they periodically dry out, vernal pools are free of fish and essential to amphibians, some reptiles, birds, and mammals for breeding habitats. In Massachusetts, vernal pool habitats are found in woodland depressions, swales or kettle holes where water is contained for at least two months in most years. However, vernal pools are delicate ecosystems. These systems are fragile to human activities such as urbanization. Understanding the current situation of vernal pools helps city planners make wiser decisions. This study focuses on identifying vernal pools in the state of Massachusetts with high-resolution light detection and ranging (LiDAR) data and aerial imagery. By using high-resolution light detection and ranging data, aerial imagery, land use data, the MassDEP Hydrography layer and the Soil Survey Geographic Database, the approach located over 1800 potential vernal pools in a 108 km 2 study area in Massachusetts. The assessment of the study result shows the commission rate was 5.6% and omission rate was 7.1%. This method provides an efficient way of locating vernal pools over large areas.\n\nFusing Unmanned Aerial Vehicle Imagery with High Resolution Hydrologic Modeling (Invited)\n\nNASA Astrophysics Data System (ADS)\n\nVivoni, E. R.; Pierini, N.; Schreiner-McGraw, A.; Anderson, C.; Saripalli, S.; Rango, A.\n\n2013-12-01\n\nAfter decades of development and applications, high resolution hydrologic models are now common tools in research and increasingly used in practice. More recently, high resolution imagery from unmanned aerial vehicles (UAVs) that provide information on land surface properties have become available for civilian applications. Fusing the two approaches promises to significantly advance the state-of-the-art in terms of hydrologic modeling capabilities. This combination will also challenge assumptions on model processes, parameterizations and scale as land surface characteristics (~0.1 to 1 m) may now surpass traditional model resolutions (~10 to 100 m). Ultimately, predictions from high resolution hydrologic models need to be consistent with the observational data that can be collected from UAVs. This talk will describe our efforts to develop, utilize and test the impact of UAV-derived topographic and vegetation fields on the simulation of two small watersheds in the Sonoran and Chihuahuan Deserts at the Santa Rita Experimental Range (Green Valley, AZ) and the Jornada Experimental Range (Las Cruces, NM). High resolution digital terrain models, image orthomosaics and vegetation species classification were obtained from a fixed wing airplane and a rotary wing helicopter, and compared to coarser analyses and products, including Light Detection and Ranging (LiDAR). We focus the discussion on the relative improvements achieved with UAV-derived fields in terms of terrain-hydrologic-vegetation analyses and summer season simulations using the TIN-based Real-time Integrated Basin Simulator (tRIBS) model. Model simulations are evaluated at each site with respect to a high-resolution sensor network consisting of six rain gauges, forty soil moisture and temperature profiles, four channel runoff flumes, a cosmic-ray soil moisture sensor and an eddy covariance tower over multiple summer periods. We also discuss prospects for the fusion of high resolution models with novel\n\nImproving Measurement of Forest Structural Parameters by Co-Registering of High Resolution Aerial Imagery and Low Density LiDAR Data\n\nPubMed Central\n\nHuang, Huabing; Gong, Peng; Cheng, Xiao; Clinton, Nick; Li, Zengyuan\n\n2009-01-01\n\nForest structural parameters, such as tree height and crown width, are indispensable for evaluating forest biomass or forest volume. LiDAR is a revolutionary technology for measurement of forest structural parameters, however, the accuracy of crown width extraction is not satisfactory when using a low density LiDAR, especially in high canopy cover forest. We used high resolution aerial imagery with a low density LiDAR system to overcome this shortcoming. A morphological filtering was used to generate a DEM (Digital Elevation Model) and a CHM (Canopy Height Model) from LiDAR data. The LiDAR camera image is matched to the aerial image with an automated keypoints search algorithm. As a result, a high registration accuracy of 0.5 pixels was obtained. A local maximum filter, watershed segmentation, and object-oriented image segmentation are used to obtain tree height and crown width. Results indicate that the camera data collected by the integrated LiDAR system plays an important role in registration with aerial imagery. The synthesis with aerial imagery increases the accuracy of forest structural parameter extraction when compared to only using the low density LiDAR data. PMID:22573971\n\nVehicle detection from very-high-resolution (VHR) aerial imagery using attribute belief propagation (ABP)\n\nNASA Astrophysics Data System (ADS)\n\nWang, Yanli; Li, Ying; Zhang, Li; Huang, Yuchun\n\n2016-10-01\n\nWith the popularity of very-high-resolution (VHR) aerial imagery, the shape, color, and context attribute of vehicles are better characterized. Due to the various road surroundings and imaging conditions, vehicle attributes could be adversely affected so that vehicle is mistakenly detected or missed. This paper is motivated to robustly extract the rich attribute feature for detecting the vehicles of VHR imagery under different scenarios. Based on the hierarchical component tree of vehicle context, attribute belief propagation (ABP) is proposed to detect salient vehicles from the statistical perspective. With the Max-tree data structure, the multi-level component tree around the road network is efficiently created. The spatial relationship between vehicle and its belonging context is established with the belief definition of vehicle attribute. To effectively correct single-level belief error, the inter-level belief linkages enforce consistency of belief assignment between corresponding components at different levels. ABP starts from an initial set of vehicle belief calculated by vehicle attribute, and then iterates through each component by applying inter-level belief passing until convergence. The optimal value of vehicle belief of each component is obtained via minimizing its belief function iteratively. The proposed algorithm is tested on a diverse set of VHR imagery acquired in the city and inter-city areas of the West and South China. Experimental results show that the proposed algorithm can detect vehicle efficiently and suppress the erroneous effectively. The proposed ABP framework is promising to robustly classify the vehicles from VHR Aerial imagery.\n\nD Surface Generation from Aerial Thermal Imagery\n\nNASA Astrophysics Data System (ADS)\n\nKhodaei, B.; Samadzadegan, F.; Dadras Javan, F.; Hasani, H.\n\n2015-12-01\n\nAerial thermal imagery has been recently applied to quantitative analysis of several scenes. For the mapping purpose based on aerial thermal imagery, high accuracy photogrammetric process is necessary. However, due to low geometric resolution and low contrast of thermal imaging sensors, there are some challenges in precise 3D measurement of objects. In this paper the potential of thermal video in 3D surface generation is evaluated. In the pre-processing step, thermal camera is geometrically calibrated using a calibration grid based on emissivity differences between the background and the targets. Then, Digital Surface Model (DSM) generation from thermal video imagery is performed in four steps. Initially, frames are extracted from video, then tie points are generated by Scale-Invariant Feature Transform (SIFT) algorithm. Bundle adjustment is then applied and the camera position and orientation parameters are determined. Finally, multi-resolution dense image matching algorithm is used to create 3D point cloud of the scene. Potential of the proposed method is evaluated based on thermal imaging cover an industrial area. The thermal camera has 640Ã480 Uncooled Focal Plane Array (UFPA) sensor, equipped with a 25 mm lens which mounted in the Unmanned Aerial Vehicle (UAV). The obtained results show the comparable accuracy of 3D model generated based on thermal images with respect to DSM generated from visible images, however thermal based DSM is somehow smoother with lower level of texture. Comparing the generated DSM with the 9 measured GCPs in the area shows the Root Mean Square Error (RMSE) value is smaller than 5 decimetres in both X and Y directions and 1.6 meters for the Z direction.\n\nAdvanced Image Processing of Aerial Imagery\n\nNASA Technical Reports Server (NTRS)\n\nWoodell, Glenn; Jobson, Daniel J.; Rahman, Zia-ur; Hines, Glenn\n\n2006-01-01\n\nAerial imagery of the Earth is an invaluable tool for the assessment of ground features, especially during times of disaster. Researchers at the NASA Langley Research Center have developed techniques which have proven to be useful for such imagery. Aerial imagery from various sources, including Langley's Boeing 757 Aries aircraft, has been studied extensively. This paper discusses these studies and demonstrates that better-than-observer imagery can be obtained even when visibility is severely compromised. A real-time, multi-spectral experimental system will be described and numerous examples will be shown.\n\nSurface Temperature Mapping of the University of Northern Iowa Campus Using High Resolution Thermal Infrared Aerial Imageries\n\nPubMed Central\n\nSavelyev, Alexander; Sugumaran, Ramanathan\n\n2008-01-01\n\nThe goal of this project was to map the surface temperature of the University of Northern Iowa campus using high-resolution thermal infrared aerial imageries. A thermal camera with a spectral bandwidth of 3.0-5.0 Î¼m was flown at the average altitude of 600 m, achieving ground resolution of 29 cm. Ground control data was used to construct the pixel- to-temperature conversion model, which was later used to produce temperature maps of the entire campus and also for validation of the model. The temperature map then was used to assess the building rooftop conditions and steam line faults in the study area. Assessment of the temperature map revealed a number of building structures that may be subject to insulation improvement due to their high surface temperatures leaks. Several hot spots were also identified on the campus for steam pipelines faults. High-resolution thermal infrared imagery proved highly effective tool for precise heat anomaly detection on the campus, and it can be used by university facility services for effective future maintenance of buildings and grounds. PMID:27873800\n\nSpatially explicit rangeland erosion monitoring using high-resolution digital aerial imagery\n\nUSGS Publications Warehouse\n\nGillan, Jeffrey K.; Karl, Jason W.; Barger, Nichole N.; Elaksher, Ahmed; Duniway, Michael C.\n\n2016-01-01\n\nNearly all of the ecosystem services supported by rangelands, including production of livestock forage, carbon sequestration, and provisioning of clean water, are negatively impacted by soil erosion. Accordingly, monitoring the severity, spatial extent, and rate of soil erosion is essential for long-term sustainable management. Traditional field-based methods of monitoring erosion (sediment traps, erosion pins, and bridges) can be labor intensive and therefore are generally limited in spatial intensity and/or extent. There is a growing effort to monitor natural resources at broad scales, which is driving the need for new soil erosion monitoring tools. One remote-sensing technique that can be used to monitor soil movement is a time series of digital elevation models (DEMs) created using aerial photogrammetry methods. By geographically coregistering the DEMs and subtracting one surface from the other, an estimate of soil elevation change can be created. Such analysis enables spatially explicit quantification and visualization of net soil movement including erosion, deposition, and redistribution. We constructed DEMs (12-cm ground sampling distance) on the basis of aerial photography immediately before and 1 year after a vegetation removal treatment on a 31-ha PiÃ±on-Juniper woodland in southeastern Utah to evaluate the use of aerial photography in detecting soil surface change. On average, we were able to detect surface elevation change of Â±Â 8â9cm and greater, which was sufficient for the large amount of soil movement exhibited on the study area. Detecting more subtle soil erosion could be achieved using the same technique with higher-resolution imagery from lower-flying aircraft such as unmanned aerial vehicles. DEM differencing and process-focused field methods provided complementary information and a more complete assessment of soil loss and movement than any single technique alone. Photogrammetric DEM differencing could be used as a technique to\n\nSpecies classification using Unmanned Aerial Vehicle (UAV)-acquired high spatial resolution imagery in a heterogeneous grassland\n\nNASA Astrophysics Data System (ADS)\n\nLu, Bing; He, Yuhong\n\n2017-06-01\n\nInvestigating spatio-temporal variations of species composition in grassland is an essential step in evaluating grassland health conditions, understanding the evolutionary processes of the local ecosystem, and developing grassland management strategies. Space-borne remote sensing images (e.g., MODIS, Landsat, and Quickbird) with spatial resolutions varying from less than 1 m to 500 m have been widely applied for vegetation species classification at spatial scales from community to regional levels. However, the spatial resolutions of these images are not fine enough to investigate grassland species composition, since grass species are generally small in size and highly mixed, and vegetation cover is greatly heterogeneous. Unmanned Aerial Vehicle (UAV) as an emerging remote sensing platform offers a unique ability to acquire imagery at very high spatial resolution (centimetres). Compared to satellites or airplanes, UAVs can be deployed quickly and repeatedly, and are less limited by weather conditions, facilitating advantageous temporal studies. In this study, we utilize an octocopter, on which we mounted a modified digital camera (with near-infrared (NIR), green, and blue bands), to investigate species composition in a tall grassland in Ontario, Canada. Seven flight missions were conducted during the growing season (April to December) in 2015 to detect seasonal variations, and four of them were selected in this study to investigate the spatio-temporal variations of species composition. To quantitatively compare images acquired at different times, we establish a processing flow of UAV-acquired imagery, focusing on imagery quality evaluation and radiometric correction. The corrected imagery is then applied to an object-based species classification. Maps of species distribution are subsequently used for a spatio-temporal change analysis. Results indicate that UAV-acquired imagery is an incomparable data source for studying fine-scale grassland species composition\n\nForest fuel treatment detection using multi-temporal airborne Lidar data and high resolution aerial imagery ---- A case study at Sierra Nevada, California\n\nNASA Astrophysics Data System (ADS)\n\nSu, Y.; Guo, Q.; Collins, B.; Fry, D.; Kelly, M.\n\n2014-12-01\n\nForest fuel treatments (FFT) are often employed in Sierra Nevada forest (located in California, US) to enhance forest health, regulate stand density, and reduce wildfire risk. However, there have been concerns that FFTs may have negative impacts on certain protected wildlife species. Due to the constraints and protection of resources (e.g., perennial streams, cultural resources, wildlife habitat, etc.), the actual FFT extents are usually different from planned extents. Identifying the actual extent of treated areas is of primary importance to understand the environmental influence of FFTs. Light detection and ranging (Lidar) is a powerful remote sensing technique that can provide accurate forest structure measurements, which provides great potential to monitor forest changes. This study used canopy height model (CHM) and canopy cover (CC) products derived from multi-temporal airborne Lidar data to detect FFTs by an approach combining a pixel-wise thresholding method and a object-of-interest segmentation method. We also investigated forest change following the implementation of landscape-scale FFT projects through the use of normalized difference vegetation index (NDVI) and standardized principle component analysis (PCA) from multi-temporal high resolution aerial imagery. The same FFT detection routine was applied on the Lidar data and aerial imagery for the purpose of comparing the capability of Lidar data and aerial imagery on FFT detection. Our results demonstrated that the FFT detection using Lidar derived CC products produced both the highest total accuracy and kappa coefficient, and was more robust at identifying areas with light FFTs. The accuracy using Lidar derived CHM products was significantly lower than that of the result using Lidar derived CC, but was still slightly higher than using aerial imagery. FFT detection results using NDVI and standardized PCA using multi-temporal aerial imagery produced almost identical total accuracy and kappa coefficient\n\nAutomatic Sea Bird Detection from High Resolution Aerial Imagery\n\nNASA Astrophysics Data System (ADS)\n\nMader, S.; GrenzdÃ¶rffer, G. J.\n\n2016-06-01\n\nGreat efforts are presently taken in the scientific community to develop computerized and (fully) automated image processing methods allowing for an efficient and automatic monitoring of sea birds and marine mammals in ever-growing amounts of aerial imagery. Currently the major part of the processing, however, is still conducted by especially trained professionals, visually examining the images and detecting and classifying the requested subjects. This is a very tedious task, particularly when the rate of void images regularly exceeds the mark of 90%. In the content of this contribution we will present our work aiming to support the processing of aerial images by modern methods from the field of image processing. We will especially focus on the combination of local, region-based feature detection and piecewise global image segmentation for automatic detection of different sea bird species. Large image dimensions resulting from the use of medium and large-format digital cameras in aerial surveys inhibit the applicability of image processing methods based on global operations. In order to efficiently handle those image sizes and to nevertheless take advantage of globally operating segmentation algorithms, we will describe the combined usage of a simple performant feature detector based on local operations on the original image with a complex global segmentation algorithm operating on extracted sub-images. The resulting exact segmentation of possible candidates then serves as a basis for the determination of feature vectors for subsequent elimination of false candidates and for classification tasks.\n\nDetecting blind building faÃ§ades from highly overlapping wide angle aerial imagery\n\nNASA Astrophysics Data System (ADS)\n\nBurochin, Jean-Pascal; Vallet, Bruno; BrÃ©dif, Mathieu; Mallet, ClÃ©ment; Brosset, Thomas; Paparoditis, Nicolas\n\n2014-10-01\n\nThis paper deals with the identification of blind building faÃ§ades, i.e. faÃ§ades which have no openings, in wide angle aerial images with a decimeter pixel size, acquired by nadir looking cameras. This blindness characterization is in general crucial for real estate estimation and has, at least in France, a particular importance on the evaluation of legal permission of constructing on a parcel due to local urban planning schemes. We assume that we have at our disposal an aerial survey with a relatively high stereo overlap along-track and across-track and a 3D city model of LoD 1, that can have been generated with the input images. The 3D model is textured with the aerial imagery by taking into account the 3D occlusions and by selecting for each faÃ§ade the best available resolution texture seeing the whole faÃ§ade. We then parse all 3D faÃ§ades textures by looking for evidence of openings (windows or doors). This evidence is characterized by a comprehensive set of basic radiometric and geometrical features. The blindness prognostic is then elaborated through an (SVM) supervised classification. Despite the relatively low resolution of the images, we reach a classification accuracy of around 85% on decimeter resolution imagery with 60 Ã 40 % stereo overlap. On the one hand, we show that the results are very sensitive to the texturing resampling process and to vegetation presence on faÃ§ade textures. On the other hand, the most relevant features for our classification framework are related to texture uniformity and horizontal aspect and to the maximal contrast of the opening detections. We conclude that standard aerial imagery used to build 3D city models can also be exploited to some extent and at no additional cost for facade blindness characterisation.\n\nEstimating chlorophyll with thermal and broadband multispectral high resolution imagery from an unmanned aerial system using relevance vector machines for precision agriculture\n\nNASA Astrophysics Data System (ADS)\n\nElarab, Manal; Ticlavilca, Andres M.; Torres-Rua, Alfonso F.; Maslova, Inga; McKee, Mac\n\n2015-12-01\n\nPrecision agriculture requires high-resolution information to enable greater precision in the management of inputs to production. Actionable information about crop and field status must be acquired at high spatial resolution and at a temporal frequency appropriate for timely responses. In this study, high spatial resolution imagery was obtained through the use of a small, unmanned aerial system called AggieAirTM. Simultaneously with the AggieAir flights, intensive ground sampling for plant chlorophyll was conducted at precisely determined locations. This study reports the application of a relevance vector machine coupled with cross validation and backward elimination to a dataset composed of reflectance from high-resolution multi-spectral imagery (VIS-NIR), thermal infrared imagery, and vegetative indices, in conjunction with in situ SPAD measurements from which chlorophyll concentrations were derived, to estimate chlorophyll concentration from remotely sensed data at 15-cm resolution. The results indicate that a relevance vector machine with a thin plate spline kernel type and kernel width of 5.4, having LAI, NDVI, thermal and red bands as the selected set of inputs, can be used to spatially estimate chlorophyll concentration with a root-mean-squared-error of 5.31 Î¼g cm-2, efficiency of 0.76, and 9 relevance vectors.\n\nOrthorectification, mosaicking, and analysis of sub-decimeter resolution UAV imagery for rangeland monitoring\n\nUSDA-ARS?s Scientific Manuscript database\n\nUnmanned aerial vehicles (UAVs) offer an attractive platform for acquiring imagery for rangeland monitoring. UAVs can be deployed quickly and repeatedly, and they can obtain sub-decimeter resolution imagery at lower image acquisition costs than with piloted aircraft. Low flying heights result in ima...\n\nMonitoring the Invasion of Spartina alterniflora Using Very High Resolution Unmanned Aerial Vehicle Imagery in Beihai, Guangxi (China)\n\nPubMed Central\n\nWan, Huawei; Wang, Qiao; Jiang, Dong; Yang, Yipeng; Liu, Xiaoman\n\n2014-01-01\n\nSpartina alterniflora was introduced to Beihai, Guangxi (China), for ecological engineering purposes in 1979. However, the exceptional adaptability and reproductive ability of this species have led to its extensive dispersal into other habitats, where it has had a negative impact on native species and threatens the local mangrove and mudflat ecosystems. To obtain the distribution and spread of Spartina alterniflora, we collected HJ-1 CCD imagery from 2009 and 2011 and very high resolution (VHR) imagery from the unmanned aerial vehicle (UAV). The invasion area of Spartina alterniflora was 357.2âha in 2011, which increased by 19.07% compared with the area in 2009. A field survey was conducted for verification and the total accuracy was 94.0%. The results of this paper show that VHR imagery can provide details on distribution, progress, and early detection of Spartina alterniflora invasion. OBIA, object based image analysis for remote sensing (RS) detection method, can enable control measures to be more effective, accurate, and less expensive than a field survey of the invasive population. PMID:24892066\n\nMonitoring the invasion of Spartina alterniflora using very high resolution unmanned aerial vehicle imagery in Beihai, Guangxi (China).\n\nPubMed\n\nWan, Huawei; Wang, Qiao; Jiang, Dong; Fu, Jingying; Yang, Yipeng; Liu, Xiaoman\n\n2014-01-01\n\nSpartina alterniflora was introduced to Beihai, Guangxi (China), for ecological engineering purposes in 1979. However, the exceptional adaptability and reproductive ability of this species have led to its extensive dispersal into other habitats, where it has had a negative impact on native species and threatens the local mangrove and mudflat ecosystems. To obtain the distribution and spread of Spartina alterniflora, we collected HJ-1 CCD imagery from 2009 and 2011 and very high resolution (VHR) imagery from the unmanned aerial vehicle (UAV). The invasion area of Spartina alterniflora was 357.2 ha in 2011, which increased by 19.07% compared with the area in 2009. A field survey was conducted for verification and the total accuracy was 94.0%. The results of this paper show that VHR imagery can provide details on distribution, progress, and early detection of Spartina alterniflora invasion. OBIA, object based image analysis for remote sensing (RS) detection method, can enable control measures to be more effective, accurate, and less expensive than a field survey of the invasive population.\n\nAutomated Verification of Spatial Resolution in Remotely Sensed Imagery\n\nNASA Technical Reports Server (NTRS)\n\nDavis, Bruce; Ryan, Robert; Holekamp, Kara; Vaughn, Ronald\n\n2011-01-01\n\nImage spatial resolution characteristics can vary widely among sources. In the case of aerial-based imaging systems, the image spatial resolution characteristics can even vary between acquisitions. In these systems, aircraft altitude, speed, and sensor look angle all affect image spatial resolution. Image spatial resolution needs to be verified with estimators that include the ground sample distance (GSD), the modulation transfer function (MTF), and the relative edge response (RER), all of which are key components of image quality, along with signal-to-noise ratio (SNR) and dynamic range. Knowledge of spatial resolution parameters is important to determine if features of interest are distinguishable in imagery or associated products, and to develop image restoration algorithms. An automated Spatial Resolution Verification Tool (SRVT) was developed to rapidly determine the spatial resolution characteristics of remotely sensed aerial and satellite imagery. Most current methods for assessing spatial resolution characteristics of imagery rely on pre-deployed engineered targets and are performed only at selected times within preselected scenes. The SRVT addresses these insufficiencies by finding uniform, high-contrast edges from urban scenes and then using these edges to determine standard estimators of spatial resolution, such as the MTF and the RER. The SRVT was developed using the MATLAB programming language and environment. This automated software algorithm assesses every image in an acquired data set, using edges found within each image, and in many cases eliminating the need for dedicated edge targets. The SRVT automatically identifies high-contrast, uniform edges and calculates the MTF and RER of each image, and when possible, within sections of an image, so that the variation of spatial resolution characteristics across the image can be analyzed. The automated algorithm is capable of quickly verifying the spatial resolution quality of all images within a data\n\nCanopy Density Mapping on Ultracam-D Aerial Imagery in Zagros Woodlands, Iran\n\nNASA Astrophysics Data System (ADS)\n\nErfanifard, Y.; Khodaee, Z.\n\n2013-09-01\n\nCanopy density maps express different characteristics of forest stands, especially in woodlands. Obtaining such maps by field measurements is so expensive and time-consuming. It seems necessary to find suitable techniques to produce these maps to be used in sustainable management of woodland ecosystems. In this research, a robust procedure was suggested to obtain these maps by very high spatial resolution aerial imagery. It was aimed to produce canopy density maps by UltraCam-D aerial imagery, newly taken in Zagros woodlands by Iran National Geographic Organization (NGO), in this study. A 30 ha plot of Persian oak (Quercus persica) coppice trees was selected in Zagros woodlands, Iran. The very high spatial resolution aerial imagery of the plot purchased from NGO, was classified by kNN technique and the tree crowns were extracted precisely. The canopy density was determined in each cell of different meshes with different sizes overlaid on the study area map. The accuracy of the final maps was investigated by the ground truth obtained by complete field measurements. The results showed that the proposed method of obtaining canopy density maps was efficient enough in the study area. The final canopy density map obtained by a mesh with 30 Ar (3000 m2) cell size had 80% overall accuracy and 0.61 KHAT coefficient of agreement which shows a great agreement with the observed samples. This method can also be tested in other case studies to reveal its capability in canopy density map production in woodlands.\n\nMapping Urban Tree Canopy Coverage and Structure using Data Fusion of High Resolution Satellite Imagery and Aerial Lidar\n\nNASA Astrophysics Data System (ADS)\n\nElmes, A.; Rogan, J.; Williams, C. A.; Martin, D. G.; Ratick, S.; Nowak, D.\n\n2015-12-01\n\nUrban tree canopy (UTC) coverage is a critical component of sustainable urban areas. Trees provide a number of important ecosystem services, including air pollution mitigation, water runoff control, and aesthetic and cultural values. Critically, urban trees also act to mitigate the urban heat island (UHI) effect by shading impervious surfaces and via evaporative cooling. The cooling effect of urban trees can be seen locally, with individual trees reducing home HVAC costs, and at a citywide scale, reducing the extent and magnitude of an urban areas UHI. In order to accurately model the ecosystem services of a given urban forest, it is essential to map in detail the condition and composition of these trees at a fine scale, capturing individual tree crowns and their vertical structure. This paper presents methods for delineating UTC and measuring canopy structure at fine spatial resolution (<1m). These metrics are essential for modeling the HVAC benefits from UTC for individual homes, and for assessing the ecosystem services for entire urban areas. Such maps have previously been made using a variety of methods, typically relying on high resolution aerial or satellite imagery. This paper seeks to contribute to this growing body of methods, relying on a data fusion method to combine the information contained in high resolution WorldView-3 satellite imagery and aerial lidar data using an object-based image classification approach. The study area, Worcester, MA, has recently undergone a large-scale tree removal and reforestation program, following a pest eradication effort. Therefore, the urban canopy in this location provides a wide mix of tree age class and functional type, ideal for illustrating the effectiveness of the proposed methods. Early results show that the object-based classifier is indeed capable of identifying individual tree crowns, while continued research will focus on extracting crown structural characteristics using lidar-derived metrics. Ultimately\n\nModel-based conifer crown surface reconstruction from multi-ocular high-resolution aerial imagery\n\nNASA Astrophysics Data System (ADS)\n\nSheng, Yongwei\n\n2000-12-01\n\nTree crown parameters such as width, height, shape and crown closure are desirable in forestry and ecological studies, but they are time-consuming and labor intensive to measure in the field. The stereoscopic capability of high-resolution aerial imagery provides a way to crown surface reconstruction. Existing photogrammetric algorithms designed to map terrain surfaces, however, cannot adequately extract crown surfaces, especially for steep conifer crowns. Considering crown surface reconstruction in a broader context of tree characterization from aerial images, we develop a rigorous perspective tree image formation model to bridge image-based tree extraction and crown surface reconstruction, and an integrated model-based approach to conifer crown surface reconstruction. Based on the fact that most conifer crowns are in a solid geometric form, conifer crowns are modeled as a generalized hemi-ellipsoid. Both the automatic and semi-automatic approaches are investigated to optimal tree model development from multi-ocular images. The semi-automatic 3D tree interpreter developed in this thesis is able to efficiently extract reliable tree parameters and tree models in complicated tree stands. This thesis starts with a sophisticated stereo matching algorithm, and incorporates tree models to guide stereo matching. The following critical problems are addressed in the model-based surface reconstruction process: (1) the problem of surface model composition from tree models, (2) the occlusion problem in disparity prediction from tree models, (3) the problem of integrating the predicted disparities into image matching, (4) the tree model edge effect reduction on the disparity map, (5) the occlusion problem in orthophoto production, and (6) the foreshortening problem in image matching, which is very serious for conifer crown surfaces. Solutions to the above problems are necessary for successful crown surface reconstruction. The model-based approach was applied to recover the\n\nIntegration of aerial oblique imagery and terrestrial imagery for optimized 3D modeling in urban areas\n\nNASA Astrophysics Data System (ADS)\n\nWu, Bo; Xie, Linfu; Hu, Han; Zhu, Qing; Yau, Eric\n\n2018-05-01\n\nPhotorealistic three-dimensional (3D) models are fundamental to the spatial data infrastructure of a digital city, and have numerous potential applications in areas such as urban planning, urban management, urban monitoring, and urban environmental studies. Recent developments in aerial oblique photogrammetry based on aircraft or unmanned aerial vehicles (UAVs) offer promising techniques for 3D modeling. However, 3D models generated from aerial oblique imagery in urban areas with densely distributed high-rise buildings may show geometric defects and blurred textures, especially on building faÃ§ades, due to problems such as occlusion and large camera tilt angles. Meanwhile, mobile mapping systems (MMSs) can capture terrestrial images of close-range objects from a complementary view on the ground at a high level of detail, but do not offer full coverage. The integration of aerial oblique imagery with terrestrial imagery offers promising opportunities to optimize 3D modeling in urban areas. This paper presents a novel method of integrating these two image types through automatic feature matching and combined bundle adjustment between them, and based on the integrated results to optimize the geometry and texture of the 3D models generated from aerial oblique imagery. Experimental analyses were conducted on two datasets of aerial and terrestrial images collected in Dortmund, Germany and in Hong Kong. The results indicate that the proposed approach effectively integrates images from the two platforms and thereby improves 3D modeling in urban areas.\n\nSystematic evaluation of deep learning based detection frameworks for aerial imagery\n\nNASA Astrophysics Data System (ADS)\n\nSommer, Lars; Steinmann, Lucas; Schumann, Arne; Beyerer, JÃ¼rgen\n\n2018-04-01\n\nObject detection in aerial imagery is crucial for many applications in the civil and military domain. In recent years, deep learning based object detection frameworks significantly outperformed conventional approaches based on hand-crafted features on several datasets. However, these detection frameworks are generally designed and optimized for common benchmark datasets, which considerably differ from aerial imagery especially in object sizes. As already demonstrated for Faster R-CNN, several adaptations are necessary to account for these differences. In this work, we adapt several state-of-the-art detection frameworks including Faster R-CNN, R-FCN, and Single Shot MultiBox Detector (SSD) to aerial imagery. We discuss adaptations that mainly improve the detection accuracy of all frameworks in detail. As the output of deeper convolutional layers comprise more semantic information, these layers are generally used in detection frameworks as feature map to locate and classify objects. However, the resolution of these feature maps is insufficient for handling small object instances, which results in an inaccurate localization or incorrect classification of small objects. Furthermore, state-of-the-art detection frameworks perform bounding box regression to predict the exact object location. Therefore, so called anchor or default boxes are used as reference. We demonstrate how an appropriate choice of anchor box sizes can considerably improve detection performance. Furthermore, we evaluate the impact of the performed adaptations on two publicly available datasets to account for various ground sampling distances or differing backgrounds. The presented adaptations can be used as guideline for further datasets or detection frameworks.\n\nMonitoring black-tailed prairie dog colonies with high-resolution satellite imagery\n\nUSGS Publications Warehouse\n\nSidle, John G.; Johnson, D.H.; Euliss, B.R.; Tooze, M.\n\n2002-01-01\n\nThe United States Fish and Wildlife Service has determined that the black-tailed prairie dog (Cynomys ludovicianus) warrants listing as a threatened species under the Endangered Species Act. Central to any conservation planning for the black-tailed prairie dog is an appropriate detection and monitoring technique. Because coarse-resolution satellite imagery is not adequate to detect black-tailed prairie dog colonies, we examined the usefulness of recently available high-resolution (1-m) satellite imagery. In 6 purchased scenes of national grasslands, we were easily able to visually detect small and large colonies without using image-processing algorithms. The Ikonos (Space Imaging(tm)) satellite imagery was as adequate as large-scale aerial photography to delineate colonies. Based on the high quality of imagery, we discuss a possible monitoring program for black-tailed prairie dog colonies throughout the Great Plains, using the species' distribution in North Dakota as an example. Monitoring plots could be established and imagery acquired periodically to track the expansion and contraction of colonies.\n\nEarth mapping - aerial or satellite imagery comparative analysis\n\nNASA Astrophysics Data System (ADS)\n\nFotev, Svetlin; Jordanov, Dimitar; Lukarski, Hristo\n\nNowadays, solving the tasks for revision of existing map products and creation of new maps requires making a choice of the land cover image source. The issue of the effectiveness and cost of the usage of aerial mapping systems versus the efficiency and cost of very-high resolution satellite imagery is topical [1, 2, 3, 4]. The price of any remotely sensed image depends on the product (panchromatic or multispectral), resolution, processing level, scale, urgency of task and on whether the needed image is available in the archive or has to be requested. The purpose of the present work is: to make a comparative analysis between the two approaches for mapping the Earth having in mind two parameters: quality and cost. To suggest an approach for selection of the map information sources - airplane-based or spacecraft-based imaging systems with very-high spatial resolution. Two cases are considered: area that equals approximately one satellite scene and area that equals approximately the territory of Bulgaria.\n\nPrecision measurements from very-large scale aerial digital imagery.\n\nPubMed\n\nBooth, D Terrance; Cox, Samuel E; Berryman, Robert D\n\n2006-01-01\n\nManagers need measurements and resource managers need the length/width of a variety of items including that of animals, logs, streams, plant canopies, man-made objects, riparian habitat, vegetation patches and other things important in resource monitoring and land inspection. These types of measurements can now be easily and accurately obtained from very large scale aerial (VLSA) imagery having spatial resolutions as fine as 1 millimeter per pixel by using the three new software programs described here. VLSA images have small fields of view and are used for intermittent sampling across extensive landscapes. Pixel-coverage among images is influenced by small changes in airplane altitude above ground level (AGL) and orientation relative to the ground, as well as by changes in topography. These factors affect the object-to-camera distance used for image-resolution calculations. 'ImageMeasurement' offers a user-friendly interface for accounting for pixel-coverage variation among images by utilizing a database. 'LaserLOG' records and displays airplane altitude AGL measured from a high frequency laser rangefinder, and displays the vertical velocity. 'Merge' sorts through large amounts of data generated by LaserLOG and matches precise airplane altitudes with camera trigger times for input to the ImageMeasurement database. We discuss application of these tools, including error estimates. We found measurements from aerial images (collection resolution: 5-26 mm/pixel as projected on the ground) using ImageMeasurement, LaserLOG, and Merge, were accurate to centimeters with an error less than 10%. We recommend these software packages as a means for expanding the utility of aerial image data.\n\nGoogle Haul Out: Earth Observation Imagery and Digital Aerial Surveys in Coastal Wildlife Management and Abundance Estimation\n\nPubMed Central\n\nMoxley, Jerry H.; Bogomolni, Andrea; Hammill, Mike O.; Moore, Kathleen M. T.; Polito, Michael J.; Sette, Lisa; Sharp, W. Brian; Waring, Gordon T.; Gilbert, James R.; Halpin, Patrick N.; Johnston, David W.\n\n2017-01-01\n\nAbstract As the sampling frequency and resolution of Earth observation imagery increase, there are growing opportunities for novel applications in population monitoring. New methods are required to apply established analytical approaches to data collected from new observation platforms (e.g., satellites and unmanned aerial vehicles). Here, we present a method that estimates regional seasonal abundances for an understudied and growing population of gray seals (Halichoerus grypus) in southeastern Massachusetts, using opportunistic observations in Google Earth imagery. Abundance estimates are derived from digital aerial survey counts by adapting established correction-based analyses with telemetry behavioral observation to quantify survey biases. The result is a first regional understanding of gray seal abundance in the northeast US through opportunistic Earth observation imagery and repurposed animal telemetry data. As species observation data from Earth observation imagery become more ubiquitous, such methods provide a robust, adaptable, and cost-effective solution to monitoring animal colonies and understanding species abundances. PMID:29599542\n\nGoogle Haul Out: Earth Observation Imagery and Digital Aerial Surveys in Coastal Wildlife Management and Abundance Estimation.\n\nPubMed\n\nMoxley, Jerry H; Bogomolni, Andrea; Hammill, Mike O; Moore, Kathleen M T; Polito, Michael J; Sette, Lisa; Sharp, W Brian; Waring, Gordon T; Gilbert, James R; Halpin, Patrick N; Johnston, David W\n\n2017-08-01\n\nAs the sampling frequency and resolution of Earth observation imagery increase, there are growing opportunities for novel applications in population monitoring. New methods are required to apply established analytical approaches to data collected from new observation platforms (e.g., satellites and unmanned aerial vehicles). Here, we present a method that estimates regional seasonal abundances for an understudied and growing population of gray seals (Halichoerus grypus) in southeastern Massachusetts, using opportunistic observations in Google Earth imagery. Abundance estimates are derived from digital aerial survey counts by adapting established correction-based analyses with telemetry behavioral observation to quantify survey biases. The result is a first regional understanding of gray seal abundance in the northeast US through opportunistic Earth observation imagery and repurposed animal telemetry data. As species observation data from Earth observation imagery become more ubiquitous, such methods provide a robust, adaptable, and cost-effective solution to monitoring animal colonies and understanding species abundances.\n\nParabolic dune reactivation and migration at Napeague, NY, USA: Insights from aerial and GPR imagery\n\nNASA Astrophysics Data System (ADS)\n\nGirardi, James D.; Davis, Dan M.\n\n2010-02-01\n\nObservations from mapping since the 19th century and aerial imagery since 1930 have been used to study changes in the aeolian geomorphology of coastal parabolic dunes over the last ~ 170 years in the Walking Dune Field, Napeague, NY. The five large parabolic dunes of the Walking Dune Field have all migrated across, or are presently interacting with, a variably forested area that has affected their migration, stabilization and morphology. This study has concentrated on a dune with a particularly complex history of stabilization, reactivation and migration. We have correlated that dune's surface evolution, as revealed by aerial imagery, with its internal structures imaged using 200 MHz and 500 MHz Ground Penetrating Radar (GPR) surveys. Both 2D (transect) and high-resolution 3D GPR imagery image downwind dipping bedding planes which can be grouped by apparent dip angle into several discrete packages of beds that reflect distinct decadal-scale episodes of dune reactivation and growth. From aerial and high resolution GPR imagery, we document a unique mode of reactivation and migration linked to upwind dune formation and parabolic dune interactions with forest trees. This study documents how dune-dune and dune-vegetation interactions have influenced a unique mode of blowout deposition that has alternated on a decadal scale between opposite sides of a parabolic dune during reactivation and migration. The pattern of recent parabolic dune reactivation and migration in the Walking Dune Field appears to be somewhat more complex, and perhaps more sensitive to subtle environmental pressures, than an idealized growth model with uniform deposition and purely on-axis migration. This pattern, believed to be prevalent among other parabolic dunes in the Walking Dune Field, may occur also in many other places where similar observational constraints are unavailable.\n\nApplication of Unmanned Aerial Systems in Spatial Downscaling of Landsat VIR imageries of Agricultural Fields\n\nNASA Astrophysics Data System (ADS)\n\nTorres, A.; Hassan Esfahani, L.; Ebtehaj, A.; McKee, M.\n\n2016-12-01\n\nWhile coarse space-time resolution of satellite observations in visible to near infrared (VIR) is a serious limiting factor for applications in precision agriculture, high resolution remotes sensing observation by the Unmanned Aerial Systems (UAS) systems are also site-specific and still practically restrictive for widespread applications in precision agriculture. We present a modern spatial downscaling approach that relies on new sparse approximation techniques. The downscaling approach learns from a large set of coincident low- and high-resolution satellite and UAS observations to effectively downscale the satellite imageries in VIR bands. We focus on field experiments using the AggieAirTM platform and Landsat 7 ETM+ and Landsat 8 OLI observations obtained in an intensive field campaign in 2013 over an agriculture field in Scipio, Utah. The results show that the downscaling methods can effectively increase the resolution of Landsat VIR imageries by the order of 2 to 4 from 30 m to 15 and 7.5 m, respectively. Specifically, on average, the downscaling method reduces the root mean squared errors up to 26%, considering bias corrected AggieAir imageries as the reference.\n\nProceedings of the 2004 High Spatial Resolution Commercial Imagery Workshop\n\nNASA Technical Reports Server (NTRS)\n\n2006-01-01\n\nTopics covered include: NASA Applied Sciences Program; USGS Land Remote Sensing: Overview; QuickBird System Status and Product Overview; ORBIMAGE Overview; IKONOS 2004 Calibration and Validation Status; OrbView-3 Spatial Characterization; On-Orbit Modulation Transfer Function (MTF) Measurement of QuickBird; Spatial Resolution Characterization for QuickBird Image Products 2003-2004 Season; Image Quality Evaluation of QuickBird Super Resolution and Revisit of IKONOS: Civil and Commercial Application Project (CCAP); On-Orbit System MTF Measurement; QuickBird Post Launch Geopositional Characterization Update; OrbView-3 Geometric Calibration and Geopositional Accuracy; Geopositional Statistical Methods; QuickBird and OrbView-3 Geopositional Accuracy Assessment; Initial On-Orbit Spatial Resolution Characterization of OrbView-3 Panchromatic Images; Laboratory Measurement of Bidirectional Reflectance of Radiometric Tarps; Stennis Space Center Verification and Validation Capabilities; Joint Agency Commercial Imagery Evaluation (JACIE) Team; Adjacency Effects in High Resolution Imagery; Effect of Pulse Width vs. GSD on MTF Estimation; Camera and Sensor Calibration at the USGS; QuickBird Geometric Verification; Comparison of MODTRAN to Heritage-based Results in Vicarious Calibration at University of Arizona; Using Remotely Sensed Imagery to Determine Impervious Surface in Sioux Falls, South Dakota; Estimating Sub-Pixel Proportions of Sagebrush with a Regression Tree; How Do YOU Use the National Land Cover Dataset?; The National Map Hazards Data Distribution System; Recording a Troubled World; What Does This-Have to Do with This?; When Can a Picture Save a Thousand Homes?; InSAR Studies of Alaska Volcanoes; Earth Observing-1 (EO-1) Data Products; Improving Access to the USGS Aerial Film Collections: High Resolution Scanners; Improving Access to the USGS Aerial Film Collections: Phoenix Digitizing System Product Distribution; System and Product Characterization: Issues Approach\n\nEvaluation of orthomosics and digital surface models derived from aerial imagery for crop mapping\n\nUSDA-ARS?s Scientific Manuscript database\n\nOrthomosics derived from aerial imagery acquired by consumer-grade cameras have been used for crop mapping. However, digital surface models (DSM) derived from aerial imagery have not been evaluated for this application. In this study, a novel method was proposed to extract crop height from DSM and t...\n\nAutomatic extraction of tree crowns from aerial imagery in urban environment\n\nNASA Astrophysics Data System (ADS)\n\nLiu, Jiahang; Li, Deren; Qin, Xunwen; Yang, Jianfeng\n\n2006-10-01\n\nTraditionally, field-based investigation is the main method to investigate greenbelt in urban environment, which is costly and low updating frequency. In higher resolution image, the imagery structure and texture of tree canopy has great similarity in statistics despite the great difference in configurations of tree canopy, and their surface structures and textures of tree crown are very different from the other types. In this paper, we present an automatic method to detect tree crowns using high resolution image in urban environment without any apriori knowledge. Our method catches unique structure and texture of tree crown surface, use variance and mathematical expectation of defined image window to position the candidate canopy blocks coarsely, then analysis their inner structure and texture to refine these candidate blocks. The possible spans of all the feature parameters used in our method automatically generate from the small number of samples, and HOLE and its distribution as an important characteristics are introduced into refining processing. Also the isotropy of candidate image block and holes' distribution is integrated in our method. After introduction the theory of our method, aerial imageries were used ( with a resolution about 0.3m ) to test our method, and the results indicate that our method is an effective approach to automatically detect tree crown in urban environment.\n\nTracking stormwater discharge plumes and water quality of the Tijuana River with multispectral aerial imagery\n\nNASA Astrophysics Data System (ADS)\n\nSvejkovsky, Jan; Nezlin, Nikolay P.; Mustain, Neomi M.; Kum, Jamie B.\n\n2010-04-01\n\nSpatial-temporal characteristics and environmental factors regulating the behavior of stormwater runoff from the Tijuana River in southern California were analyzed utilizing very high resolution aerial imagery, and time-coincident environmental and bacterial sampling data. Thirty nine multispectral aerial images with 2.1-m spatial resolution were collected after major rainstorms during 2003-2008. Utilizing differences in color reflectance characteristics, the ocean surface was classified into non-plume waters and three components of the runoff plume reflecting differences in age and suspended sediment concentrations. Tijuana River discharge rate was the primary factor regulating the size of the freshest plume component and its shorelong extensions to the north and south. Wave direction was found to affect the shorelong distribution of the shoreline-connected fresh plume components much more strongly than wind direction. Wave-driven sediment resuspension also significantly contributed to the size of the oldest plume component. Surf zone bacterial samples collected near the time of each image acquisition were used to evaluate the contamination characteristics of each plume component. The bacterial contamination of the freshest plume waters was very high (100% of surf zone samples exceeded California standards), but the oldest plume areas were heterogeneous, including both polluted and clean waters. The aerial imagery archive allowed study of river runoff characteristics on a plume component level, not previously done with coarser satellite images. Our findings suggest that high resolution imaging can quickly identify the spatial extents of the most polluted runoff but cannot be relied upon to always identify the entire polluted area. Our results also indicate that wave-driven transport is important in distributing the most contaminated plume areas along the shoreline.\n\nPedestrian Detection and Tracking from Low-Resolution Unmanned Aerial Vehicle Thermal Imagery\n\nPubMed Central\n\nMa, Yalong; Wu, Xinkai; Yu, Guizhen; Xu, Yongzheng; Wang, Yunpeng\n\n2016-01-01\n\nDriven by the prominent thermal signature of humans and following the growing availability of unmanned aerial vehicles (UAVs), more and more research efforts have been focusing on the detection and tracking of pedestrians using thermal infrared images recorded from UAVs. However, pedestrian detection and tracking from the thermal images obtained from UAVs pose many challenges due to the low-resolution of imagery, platform motion, image instability and the relatively small size of the objects. This research tackles these challenges by proposing a pedestrian detection and tracking system. A two-stage blob-based approach is first developed for pedestrian detection. This approach first extracts pedestrian blobs using the regional gradient feature and geometric constraints filtering and then classifies the detected blobs by using a linear Support Vector Machine (SVM) with a hybrid descriptor, which sophisticatedly combines Histogram of Oriented Gradient (HOG) and Discrete Cosine Transform (DCT) features in order to achieve accurate detection. This research further proposes an approach for pedestrian tracking. This approach employs the feature tracker with the update of detected pedestrian location to track pedestrian objects from the registered videos and extracts the motion trajectory data. The proposed detection and tracking approaches have been evaluated by multiple different datasets, and the results illustrate the effectiveness of the proposed methods. This research is expected to significantly benefit many transportation applications, such as the multimodal traffic performance measure, pedestrian behavior study and pedestrian-vehicle crash analysis. Future work will focus on using fused thermal and visual images to further improve the detection efficiency and effectiveness. PMID:27023564\n\nPedestrian Detection and Tracking from Low-Resolution Unmanned Aerial Vehicle Thermal Imagery.\n\nPubMed\n\nMa, Yalong; Wu, Xinkai; Yu, Guizhen; Xu, Yongzheng; Wang, Yunpeng\n\n2016-03-26\n\nDriven by the prominent thermal signature of humans and following the growing availability of unmanned aerial vehicles (UAVs), more and more research efforts have been focusing on the detection and tracking of pedestrians using thermal infrared images recorded from UAVs. However, pedestrian detection and tracking from the thermal images obtained from UAVs pose many challenges due to the low-resolution of imagery, platform motion, image instability and the relatively small size of the objects. This research tackles these challenges by proposing a pedestrian detection and tracking system. A two-stage blob-based approach is first developed for pedestrian detection. This approach first extracts pedestrian blobs using the regional gradient feature and geometric constraints filtering and then classifies the detected blobs by using a linear Support Vector Machine (SVM) with a hybrid descriptor, which sophisticatedly combines Histogram of Oriented Gradient (HOG) and Discrete Cosine Transform (DCT) features in order to achieve accurate detection. This research further proposes an approach for pedestrian tracking. This approach employs the feature tracker with the update of detected pedestrian location to track pedestrian objects from the registered videos and extracts the motion trajectory data. The proposed detection and tracking approaches have been evaluated by multiple different datasets, and the results illustrate the effectiveness of the proposed methods. This research is expected to significantly benefit many transportation applications, such as the multimodal traffic performance measure, pedestrian behavior study and pedestrian-vehicle crash analysis. Future work will focus on using fused thermal and visual images to further improve the detection efficiency and effectiveness.\n\nPhotovoltaic panel extraction from very high-resolution aerial imagery using region-line primitive association analysis and template matching\n\nNASA Astrophysics Data System (ADS)\n\nWang, Min; Cui, Qi; Sun, Yujie; Wang, Qiao\n\n2018-07-01\n\nIn object-based image analysis (OBIA), object classification performance is jointly determined by image segmentation, sample or rule setting, and classifiers. Typically, as a crucial step to obtain object primitives, image segmentation quality significantly influences subsequent feature extraction and analyses. By contrast, template matching extracts specific objects from images and prevents shape defects caused by image segmentation. However, creating or editing templates is tedious and sometimes results in incomplete or inaccurate templates. In this study, we combine OBIA and template matching techniques to address these problems and aim for accurate photovoltaic panel (PVP) extraction from very high-resolution (VHR) aerial imagery. The proposed method is based on the previously proposed region-line primitive association framework, in which complementary information between region (segment) and line (straight line) primitives is utilized to achieve a more powerful performance than routine OBIA. Several novel concepts, including the mutual fitting ratio and best-fitting template based on region-line primitive association analyses, are proposed. Automatic template generation and matching method for PVP extraction from VHR imagery are designed for concept and model validation. Results show that the proposed method can successfully extract PVPs without any user-specified matching template or training sample. High user independency and accuracy are the main characteristics of the proposed method in comparison with routine OBIA and template matching techniques.\n\nVery high resolution aerial films\n\nNASA Astrophysics Data System (ADS)\n\nBecker, Rolf\n\n1986-11-01\n\nThe use of very high resolution aerial films in aerial photography is evaluated. Commonly used panchromatic, color, and CIR films and their high resolution equivalents are compared. Based on practical experience and systematic investigations, the very high image quality and improved height accuracy that can be achieved using these films are demonstrated. Advantages to be gained from this improvement and operational restrictions encountered when using high resolution film are discussed.\n\nA fully convolutional network for weed mapping of unmanned aerial vehicle (UAV) imagery.\n\nPubMed\n\nHuang, Huasheng; Deng, Jizhong; Lan, Yubin; Yang, Aqing; Deng, Xiaoling; Zhang, Lei\n\n2018-01-01\n\nAppropriate Site Specific Weed Management (SSWM) is crucial to ensure the crop yields. Within SSWM of large-scale area, remote sensing is a key technology to provide accurate weed distribution information. Compared with satellite and piloted aircraft remote sensing, unmanned aerial vehicle (UAV) is capable of capturing high spatial resolution imagery, which will provide more detailed information for weed mapping. The objective of this paper is to generate an accurate weed cover map based on UAV imagery. The UAV RGB imagery was collected in 2017 October over the rice field located in South China. The Fully Convolutional Network (FCN) method was proposed for weed mapping of the collected imagery. Transfer learning was used to improve generalization capability, and skip architecture was applied to increase the prediction accuracy. After that, the performance of FCN architecture was compared with Patch_based CNN algorithm and Pixel_based CNN method. Experimental results showed that our FCN method outperformed others, both in terms of accuracy and efficiency. The overall accuracy of the FCN approach was up to 0.935 and the accuracy for weed recognition was 0.883, which means that this algorithm is capable of generating accurate weed cover maps for the evaluated UAV imagery.\n\nModeling vegetation heights from high resolution stereo aerial photography: an application for broad-scale rangeland monitoring\n\nUSGS Publications Warehouse\n\nGillan, Jeffrey K.; Karl, Jason W.; Duniway, Michael; Elaksher, Ahmed\n\n2014-01-01\n\nVertical vegetation structure in rangeland ecosystems can be a valuable indicator for assessing rangeland health and monitoring riparian areas, post-fire recovery, available forage for livestock, and wildlife habitat. Federal land management agencies are directed to monitor and manage rangelands at landscapes scales, but traditional field methods for measuring vegetation heights are often too costly and time consuming to apply at these broad scales. Most emerging remote sensing techniques capable of measuring surface and vegetation height (e.g., LiDAR or synthetic aperture radar) are often too expensive, and require specialized sensors. An alternative remote sensing approach that is potentially more practical for managers is to measure vegetation heights from digital stereo aerial photographs. As aerial photography is already commonly used for rangeland monitoring, acquiring it in stereo enables three-dimensional modeling and estimation of vegetation height. The purpose of this study was to test the feasibility and accuracy of estimating shrub heights from high-resolution (HR, 3-cm ground sampling distance) digital stereo-pair aerial images. Overlapping HR imagery was taken in March 2009 near Lake Mead, Nevada and 5-cm resolution digital surface models (DSMs) were created by photogrammetric methods (aerial triangulation, digital image matching) for twenty-six test plots. We compared the heights of individual shrubs and plot averages derived from the DSMs to field measurements. We found strong positive correlations between field and image measurements for several metrics. Individual shrub heights tended to be underestimated in the imagery, however, accuracy was higher for dense, compact shrubs compared with shrubs with thin branches. Plot averages of shrub height from DSMs were also strongly correlated to field measurements but consistently underestimated. Grasses and forbs were generally too small to be detected with the resolution of the DSMs. Estimates of\n\nMapping trees outside forests using high-resolution aerial imagery: a comparison of pixel- and object-based classification approaches.\n\nPubMed\n\nMeneguzzo, Dacia M; Liknes, Greg C; Nelson, Mark D\n\n2013-08-01\n\nDiscrete trees and small groups of trees in nonforest settings are considered an essential resource around the world and are collectively referred to as trees outside forests (ToF). ToF provide important functions across the landscape, such as protecting soil and water resources, providing wildlife habitat, and improving farmstead energy efficiency and aesthetics. Despite the significance of ToF, forest and other natural resource inventory programs and geospatial land cover datasets that are available at a national scale do not include comprehensive information regarding ToF in the United States. Additional ground-based data collection and acquisition of specialized imagery to inventory these resources are expensive alternatives. As a potential solution, we identified two remote sensing-based approaches that use free high-resolution aerial imagery from the National Agriculture Imagery Program (NAIP) to map all tree cover in an agriculturally dominant landscape. We compared the results obtained using an unsupervised per-pixel classifier (independent component analysis-[ICA]) and an object-based image analysis (OBIA) procedure in Steele County, Minnesota, USA. Three types of accuracy assessments were used to evaluate how each method performed in terms of: (1) producing a county-level estimate of total tree-covered area, (2) correctly locating tree cover on the ground, and (3) how tree cover patch metrics computed from the classified outputs compared to those delineated by a human photo interpreter. Both approaches were found to be viable for mapping tree cover over a broad spatial extent and could serve to supplement ground-based inventory data. The ICA approach produced an estimate of total tree cover more similar to the photo-interpreted result, but the output from the OBIA method was more realistic in terms of describing the actual observed spatial pattern of tree cover.\n\nMapping snow depth in complex alpine terrain with close range aerial imagery - estimating the spatial uncertainties of repeat autonomous aerial surveys over an active rock glacier\n\nNASA Astrophysics Data System (ADS)\n\nGoetz, Jason; Marcer, Marco; Bodin, Xavier; Brenning, Alexander\n\n2017-04-01\n\nSnow depth mapping in open areas using close range aerial imagery is just one of the many cases where developments in structure-from-motion and multi-view-stereo (SfM-MVS) 3D reconstruction techniques have been applied for geosciences - and with good reason. Our ability to increase the spatial resolution and frequency of observations may allow us to improve our understanding of how snow depth distribution varies through space and time. However, to ensure accurate snow depth observations from close range sensing we must adequately characterize the uncertainty related to our measurement techniques. In this study, we explore the spatial uncertainties of snow elevation models for estimation of snow depth in a complex alpine terrain from close range aerial imagery. We accomplish this by conducting repeat autonomous aerial surveys over a snow-covered active-rock glacier located in the French Alps. The imagery obtained from each flight of an unmanned aerial vehicle (UAV) is used to create an individual digital elevation model (DEM) of the snow surface. As result, we obtain multiple DEMs of the snow surface for the same site. These DEMs are obtained from processing the imagery with the photogrammetry software Agisoft Photoscan. The elevation models are also georeferenced within Photoscan using the geotagged imagery from an onboard GNSS in combination with ground targets placed around the rock glacier, which have been surveyed with highly accurate RTK-GNSS equipment. The random error associated with multi-temporal DEMs of the snow surface is estimated from the repeat aerial survey data. The multiple flights are designed to follow the same flight path and altitude above the ground to simulate the optimal conditions of repeat survey of the site, and thus try to estimate the maximum precision associated with our snow-elevation measurement technique. The bias of the DEMs is assessed with RTK-GNSS survey observations of the snow surface elevation of the area on and surrounding\n\nMapping Urban Ecosystem Services Using High Resolution Aerial Photography\n\nNASA Astrophysics Data System (ADS)\n\nPilant, A. N.; Neale, A.; Wilhelm, D.\n\n2010-12-01\n\nEcosystem services (ES) are the many life-sustaining benefits we receive from nature: e.g., clean air and water, food and fiber, cultural-aesthetic-recreational benefits, pollination and flood control. The ES concept is emerging as a means of integrating complex environmental and economic information to support informed environmental decision making. The US EPA is developing a web-based National Atlas of Ecosystem Services, with a component for urban ecosystems. Currently, the only wall-to-wall, national scale land cover data suitable for this analysis is the National Land Cover Data (NLCD) at 30 m spatial resolution with 5 and 10 year updates. However, aerial photography is acquired at higher spatial resolution (0.5-3 m) and more frequently (1-5 years, typically) for most urban areas. Land cover was mapped in Raleigh, NC using freely available USDA National Agricultural Imagery Program (NAIP) with 1 m ground sample distance to test the suitability of aerial photography for urban ES analysis. Automated feature extraction techniques were used to extract five land cover classes, and an accuracy assessment was performed using standard techniques. Results will be presented that demonstrate applications to mapping ES in urban environments: greenways, corridors, fragmentation, habitat, impervious surfaces, dark and light pavement (urban heat island). Automated feature extraction results mapped over NAIP color aerial photograph. At this scale, we can look at land cover and related ecosystem services at the 2-10 m scale. Small features such as individual trees and sidewalks are visible and mappable. Classified aerial photo of Downtown Raleigh NC Red: impervious surface Dark Green: trees Light Green: grass Tan: soil\n\nEnhancement of spectral quality of archival aerial photographs using satellite imagery for detection of land cover\n\nNASA Astrophysics Data System (ADS)\n\nSiok, Katarzyna; Jenerowicz, Agnieszka; Woroszkiewicz, MaÅgorzata\n\n2017-07-01\n\nArchival aerial photographs are often the only reliable source of information about the area. However, these data are single-band data that do not allow unambiguous detection of particular forms of land cover. Thus, the authors of this article seek to develop a method of coloring panchromatic aerial photographs, which enable increasing the spectral information of such images. The study used data integration algorithms based on pansharpening, implemented in commonly used remote sensing programs: ERDAS, ENVI, and PCI. Aerial photos and Landsat multispectral data recorded in 1987 and 2016 were chosen. This study proposes the use of modified intensity-hue-saturation and Brovey methods. The use of these methods enabled the addition of red-green-blue (RGB) components to monochrome images, thus enhancing their interpretability and spectral quality. The limitations of the proposed method relate to the availability of RGB satellite imagery, the accuracy of mutual orientation of the aerial and the satellite data, and the imperfection of archival aerial photographs. Therefore, it should be expected that the results of coloring will not be perfect compared to the results of the fusion of recent data with a similar ground sampling resolution, but still, they will allow a more accurate and efficient classification of land cover registered on archival aerial photographs.\n\nEnvironmental applications utilizing digital aerial imagery\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nMonday, H.M.\n\n1995-06-01\n\nThis paper discusses the use of satellite imagery, aerial photography, and computerized airborne imagery as applied to environmental mapping, analysis, and monitoring. A project conducted by the City of Irving, Texas involves compliance with national pollutant discharge elimination system (NPDES) requirements stipulated by the Environmental Protection Agency. The purpose of the project was the development and maintenance of a stormwater drainage utility. Digital imagery was collected for a portion of the city to map the City`s porous and impervious surfaces which will then be overlaid with property boundaries in the City`s existing Geographic information System (GIS). This information will allowmoreÂ Â» the City to determine an equitable tax for each land parcel according to the amount of water each parcel is contributing to the stormwater system. Another project involves environmental compliance for warm water discharges created by utility companies. Environmental consultants are using digital airborne imagery to analyze thermal plume affects as well as monitoring power generation facilities. A third project involves wetland restoration. Due to freeway and other forms of construction, plus a major reduction of fresh water supplies, the Southern California coastal wetlands are being seriously threatened. These wetlands, rich spawning grounds for plant and animal life, are home to thousands of waterfowl and shore birds who use this habitat for nesting and feeding grounds. Under the leadership of Southern California Edison (SCE) and CALTRANS (California Department of Transportation), several wetland areas such as the San Dieguito Lagoon (Del Mar, California), the Sweetwater Marsh (San Diego, California), and the Tijuana Estuary (San Diego, California) are being restored and closely monitored using digital airborne imagery.Â«Â less\n\nModeling vegetation heights from high resolution stereo aerial photography: an application for broad-scale rangeland monitoring.\n\nPubMed\n\nGillan, Jeffrey K; Karl, Jason W; Duniway, Michael; Elaksher, Ahmed\n\n2014-11-01\n\nVertical vegetation structure in rangeland ecosystems can be a valuable indicator for assessing rangeland health and monitoring riparian areas, post-fire recovery, available forage for livestock, and wildlife habitat. Federal land management agencies are directed to monitor and manage rangelands at landscapes scales, but traditional field methods for measuring vegetation heights are often too costly and time consuming to apply at these broad scales. Most emerging remote sensing techniques capable of measuring surface and vegetation height (e.g., LiDAR or synthetic aperture radar) are often too expensive, and require specialized sensors. An alternative remote sensing approach that is potentially more practical for managers is to measure vegetation heights from digital stereo aerial photographs. As aerial photography is already commonly used for rangeland monitoring, acquiring it in stereo enables three-dimensional modeling and estimation of vegetation height. The purpose of this study was to test the feasibility and accuracy of estimating shrub heights from high-resolution (HR, 3-cm ground sampling distance) digital stereo-pair aerial images. Overlapping HR imagery was taken in March 2009 near Lake Mead, Nevada and 5-cm resolution digital surface models (DSMs) were created by photogrammetric methods (aerial triangulation, digital image matching) for twenty-six test plots. We compared the heights of individual shrubs and plot averages derived from the DSMs to field measurements. We found strong positive correlations between field and image measurements for several metrics. Individual shrub heights tended to be underestimated in the imagery, however, accuracy was higher for dense, compact shrubs compared with shrubs with thin branches. Plot averages of shrub height from DSMs were also strongly correlated to field measurements but consistently underestimated. Grasses and forbs were generally too small to be detected with the resolution of the DSMs. Estimates of\n\nAutomatic digital surface model (DSM) generation from aerial imagery data\n\nNASA Astrophysics Data System (ADS)\n\nZhou, Nan; Cao, Shixiang; He, Hongyan; Xing, Kun; Yue, Chunyu\n\n2018-04-01\n\nAerial sensors are widely used to acquire imagery for photogrammetric and remote sensing application. In general, the images have large overlapped region, which provide a lot of redundant geometry and radiation information for matching. This paper presents a POS supported dense matching procedure for automatic DSM generation from aerial imagery data. The method uses a coarse-to-fine hierarchical strategy with an effective combination of several image matching algorithms: image radiation pre-processing, image pyramid generation, feature point extraction and grid point generation, multi-image geometrically constraint cross-correlation (MIG3C), global relaxation optimization, multi-image geometrically constrained least squares matching (MIGCLSM), TIN generation and point cloud filtering. The image radiation pre-processing is used in order to reduce the effects of the inherent radiometric problems and optimize the images. The presented approach essentially consists of 3 components: feature point extraction and matching procedure, grid point matching procedure and relational matching procedure. The MIGCLSM method is used to achieve potentially sub-pixel accuracy matches and identify some inaccurate and possibly false matches. The feasibility of the method has been tested on different aerial scale images with different landcover types. The accuracy evaluation is based on the comparison between the automatic extracted DSMs derived from the precise exterior orientation parameters (EOPs) and the POS.\n\nUsing high-resolution digital aerial imagery to map land cover\n\nUSGS Publications Warehouse\n\nDieck, J.J.; Robinson, Larry\n\n2014-01-01\n\nThe Upper Midwest Environmental Sciences Center (UMESC) has used aerial photography to map land cover/land use on federally owned and managed lands for over 20 years. Until recently, that process used 23- by 23-centimeter (9- by 9-inch) analog aerial photos to classify vegetation along the Upper Mississippi River System, on National Wildlife Refuges, and in National Parks. With digital aerial cameras becoming more common and offering distinct advantages over analog film, UMESC transitioned to an entirely digital mapping process in 2009. Though not without challenges, this method has proven to be much more accurate and efficient when compared to the analog process.\n\nThe investigation of classification methods of high-resolution imagery\n\nTreesearch\n\nTracey S. Frescino; Gretchen G. Moisen; Larry DeBlander; Michel Guerin\n\n2007-01-01\n\nAs remote-sensing technology advances, high-resolution imagery, such as Quickbird and photography from the National Agriculture Imagery Program (NAIP), is becoming more readily available for use in forestry applications. Quickbird imagery is currently the highest resolution imagery commercially available. It consists of 2.44-m (8-ft) resolution multispectral bands...\n\nMapping coastal marine debris using aerial imagery and spatial analysis.\n\nPubMed\n\nMoy, Kirsten; Neilson, Brian; Chung, Anne; Meadows, Amber; Castrence, Miguel; Ambagis, Stephen; Davidson, Kristine\n\n2017-12-19\n\nThis study is the first to systematically quantify, categorize, and map marine macro-debris across the main Hawaiian Islands (MHI), including remote areas (e.g., Niihau, Kahoolawe, and northern Molokai). Aerial surveys were conducted over each island to collect high resolution photos, which were processed into orthorectified imagery and visually analyzed in GIS. The technique provided precise measurements of the quantity, location, type, and size of macro-debris (>0.05m 2 ), identifying 20,658 total debris items. Northeastern (windward) shorelines had the highest density of debris. Plastics, including nets, lines, buoys, floats, and foam, comprised 83% of the total count. In addition, the study located six vessels from the 2011 TÅhoku tsunami. These results created a baseline of the location, distribution, and composition of marine macro-debris across the MHI. Resource managers and communities may target high priority areas, particularly along remote coastlines where macro-debris counts were largely undocumented. Copyright Â© 2017 Elsevier Ltd. All rights reserved.\n\nLearning Scene Categories from High Resolution Satellite Image for Aerial Video Analysis\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nCheriyadat, Anil M\n\n2011-01-01\n\nAutomatic scene categorization can benefit various aerial video processing applications. This paper addresses the problem of predicting the scene category from aerial video frames using a prior model learned from satellite imagery. We show that local and global features in the form of line statistics and 2-D power spectrum parameters respectively can characterize the aerial scene well. The line feature statistics and spatial frequency parameters are useful cues to distinguish between different urban scene categories. We learn the scene prediction model from highresolution satellite imagery to test the model on the Columbus Surrogate Unmanned Aerial Vehicle (CSUAV) dataset ollected bymoreÂ Â» high-altitude wide area UAV sensor platform. e compare the proposed features with the popular Scale nvariant Feature Transform (SIFT) features. Our experimental results show that proposed approach outperforms te SIFT model when the training and testing are conducted n disparate data sources.Â«Â less\n\nCorrection of Line Interleaving Displacement in Frame Captured Aerial Video Imagery\n\nTreesearch\n\nB. Cooke; A. Saucier\n\n1995-01-01\n\nScientists with the USDA Forest Service are currently assessing the usefulness of aerial video imagery for various purposes including midcycle inventory updates. The potential of video image data for these purposes may be compromised by scan line interleaving displacement problems. Interleaving displacement problems cause features in video raster datasets to have...\n\nScaling Sap Flow Results Over Wide Areas Using High-Resolution Aerial Multispectral Digital Imaging, Leaf Area Index (LAI) and MODIS Satellite Imagery in Saltcedar Stands on the Lower Colorado River\n\nNASA Astrophysics Data System (ADS)\n\nMurray, R.; Neale, C.; Nagler, P. L.; Glenn, E. P.\n\n2008-12-01\n\nHeat-balance sap flow sensors provide direct estimates of water movement through plant stems and can be used to accurately measure leaf-level transpiration (EL) and stomatal conductance (GS) over time scales ranging from 20-minutes to a month or longer in natural stands of plants. However, their use is limited to relatively small branches on shrubs or trees, as the gauged stem section needs to be uniformly heated by the heating coil to produce valid measurements. This presents a scaling problem in applying the results to whole plants, stands of plants, and larger landscape areas. We used high-resolution aerial multispectral digital imaging with green, red and NIR bands as a bridge between ground measurements of EL and GS, and MODIS satellite imagery of a flood plain on the Lower Colorado River dominated by saltcedar (Tamarix ramosissima). Saltcedar is considered to be a high-water-use plant, and saltcedar removal programs have been proposed to salvage water. Hence, knowledge of actual saltcedar ET rates is needed on western U.S. rivers. Scaling EL and GS to large landscape units requires knowledge of leaf area index (LAI) over large areas. We used a LAI model developed for riparian habitats on Bosque del Apache, New Mexico, to estimate LAI at our study site on the Colorado River. We compared the model estimates to ground measurements of LAI, determined with a Li-Cor LAI-2000 Plant Canopy Analyzer calibrated by leaf harvesting to determine Specific Leaf Area (SLA) (m2 leaf area per g dry weight leaves) of the different species on the floodplain. LAI could be adequately predicted from NDVI from aerial multispectral imagery and could be cross-calibrated with MODIS NDVI and EVI. Hence, we were able to project point measurements of sap flow and LAI over multiple years and over large areas of floodplain using aerial multispectral imagery as a bridge between ground and satellite data. The methods are applicable to riparian corridors throughout the western U.S.\n\nVery Large Scale Aerial (VLSA) imagery for assessing postfire bitterbrush recovery\n\nTreesearch\n\nCorey A. Moffet; J. Bret Taylor; D. Terrance Booth\n\n2008-01-01\n\nVery large scale aerial (VLSA) imagery is an efficient tool for monitoring bare ground and cover on extensive rangelands. This study was conducted to determine whether VLSA images could be used to detect differences in antelope bitterbrush (Purshia tridentata Pursh DC) cover and density among similar ecological sites with varying postfire recovery...\n\nCombining Human Computing and Machine Learning to Make Sense of Big (Aerial) Data for Disaster Response.\n\nPubMed\n\nOfli, Ferda; Meier, Patrick; Imran, Muhammad; Castillo, Carlos; Tuia, Devis; Rey, Nicolas; Briant, Julien; Millet, Pauline; Reinhard, Friedrich; Parkan, Matthew; Joost, StÃ©phane\n\n2016-03-01\n\nAerial imagery captured via unmanned aerial vehicles (UAVs) is playing an increasingly important role in disaster response. Unlike satellite imagery, aerial imagery can be captured and processed within hours rather than days. In addition, the spatial resolution of aerial imagery is an order of magnitude higher than the imagery produced by the most sophisticated commercial satellites today. Both the United States Federal Emergency Management Agency (FEMA) and the European Commission's Joint Research Center (JRC) have noted that aerial imagery will inevitably present a big data challenge. The purpose of this article is to get ahead of this future challenge by proposing a hybrid crowdsourcing and real-time machine learning solution to rapidly process large volumes of aerial data for disaster response in a time-sensitive manner. Crowdsourcing can be used to annotate features of interest in aerial images (such as damaged shelters and roads blocked by debris). These human-annotated features can then be used to train a supervised machine learning system to learn to recognize such features in new unseen images. In this article, we describe how this hybrid solution for image analysis can be implemented as a module (i.e., Aerial Clicker) to extend an existing platform called Artificial Intelligence for Disaster Response (AIDR), which has already been deployed to classify microblog messages during disasters using its Text Clicker module and in response to Cyclone Pam, a category 5 cyclone that devastated Vanuatu in March 2015. The hybrid solution we present can be applied to both aerial and satellite imagery and has applications beyond disaster response such as wildlife protection, human rights, and archeological exploration. As a proof of concept, we recently piloted this solution using very high-resolution aerial photographs of a wildlife reserve in Namibia to support rangers with their wildlife conservation efforts (SAVMAP project, http://lasig.epfl.ch/savmap ). The\n\nNOAA's Use of High-Resolution Imagery\n\nNASA Technical Reports Server (NTRS)\n\nHund, Erik\n\n2007-01-01\n\nNOAA's use of high-resolution imagery consists of: a) Shoreline mapping and nautical chart revision; b) Coastal land cover mapping; c) Benthic habitat mapping; d) Disaster response; and e) Imagery collection and support for coastal programs.\n\nMonitoring spotted knapweed with very-large-scale-aerial imagery in sagebrush-dominated rangelands.\n\nUSDA-ARS?s Scientific Manuscript database\n\nSpotted knapweed (Centaurea stoebe L.) invades and destroys productive rangelands. Monitoring weed infestations across extensive and remote landscapes can be difficult and costly. We evaluated the efficacy of very-large-scale-aerial (VLSA) imagery for detection and quantification of spotted knapwee...\n\nA workflow for extracting plot-level biophysical indicators from aerially acquired multispectral imagery\n\nUSDA-ARS?s Scientific Manuscript database\n\nAdvances in technologies associated with unmanned aerial vehicles (UAVs) has allowed for researchers, farmers and agribusinesses to incorporate UAVs coupled with various imaging systems into data collection activities and aid expert systems for making decisions. Multispectral imageries allow for a q...\n\nSpatial Quality Evaluation of Resampled Unmanned Aerial Vehicle-Imagery for Weed Mapping.\n\nPubMed\n\nBorra-Serrano, Irene; PeÃ±a, JosÃ© Manuel; Torres-SÃ¡nchez, Jorge; Mesas-Carrascosa, Francisco Javier; LÃ³pez-Granados, Francisca\n\n2015-08-12\n\nUnmanned aerial vehicles (UAVs) combined with different spectral range sensors are an emerging technology for providing early weed maps for optimizing herbicide applications. Considering that weeds, at very early phenological stages, are similar spectrally and in appearance, three major components are relevant: spatial resolution, type of sensor and classification algorithm. Resampling is a technique to create a new version of an image with a different width and/or height in pixels, and it has been used in satellite imagery with different spatial and temporal resolutions. In this paper, the efficiency of resampled-images (RS-images) created from real UAV-images (UAV-images; the UAVs were equipped with two types of sensors, i.e., visible and visible plus near-infrared spectra) captured at different altitudes is examined to test the quality of the RS-image output. The performance of the object-based-image-analysis (OBIA) implemented for the early weed mapping using different weed thresholds was also evaluated. Our results showed that resampling accurately extracted the spectral values from high spatial resolution UAV-images at an altitude of 30 m and the RS-image data at altitudes of 60 and 100 m, was able to provide accurate weed cover and herbicide application maps compared with UAV-images from real flights.\n\nSpatial Quality Evaluation of Resampled Unmanned Aerial Vehicle-Imagery for Weed Mapping\n\nPubMed Central\n\nBorra-Serrano, Irene; PeÃ±a, JosÃ© Manuel; Torres-SÃ¡nchez, Jorge; Mesas-Carrascosa, Francisco Javier; LÃ³pez-Granados, Francisca\n\n2015-01-01\n\nUnmanned aerial vehicles (UAVs) combined with different spectral range sensors are an emerging technology for providing early weed maps for optimizing herbicide applications. Considering that weeds, at very early phenological stages, are similar spectrally and in appearance, three major components are relevant: spatial resolution, type of sensor and classification algorithm. Resampling is a technique to cr"
    }
}
{
    "id": "dbpedia_1424_0",
    "rank": 30,
    "data": {
        "url": "https://www.pewtrusts.org/en/research-and-analysis/issue-briefs/2021/08/how-fda-regulates-artificial-intelligence-in-medical-products",
        "read_more_link": "",
        "language": "en",
        "title": "How FDA Regulates Artificial Intelligence in Medical Products",
        "top_image": "https://www.pewtrusts.org/-/media/post-launch-images/2021/08/gettyimages1039290390jpgmaster/16x9_m.jpg",
        "meta_img": "https://www.pewtrusts.org/-/media/post-launch-images/2021/08/gettyimages1039290390jpgmaster/16x9_m.jpg",
        "images": [
            "https://www.pewtrusts.org/-/media/post-launch-images/trust-magazine/spring-2024/web-images/housing/housing_thumbnail1x1.jpg?h=676&w=676&la=en&hash=DC42EE033C0BA4499C06DE5B474FDC1A",
            "https://www.pewtrusts.org/-/media/post-launch-images/trust-magazine/spring-2024/web-images/broadband/broadband_thumbnail1x1.jpg?h=676&w=676&la=en&hash=F5A266A80467BF356E7E03D2C0E366F1",
            "https://www.pewtrusts.org/-/media/post-launch-images/2022/06/johnmarkarnoldunsplash_16x9.jpg?h=381&w=676&la=en&hash=8424D6A96C3A514DE5EF36186C2AF016",
            "https://www.pewtrusts.org/-/media/post-launch-images/2021/08/gettyimages1039290390jpgmaster/16x9_m.jpg?h=1024&w=1820&la=en&hash=4B54973E11DBA159EC234D1296219EE5",
            "https://www.pewtrusts.org/-/media/data-visualizations/infographics/2021/08/asset-1.png?h=158&w=150&hash=DCE142B5055290EBEEACBEF74AC19246",
            "https://www.pewtrusts.org/-/media/data-visualizations/infographics/2021/08/asset-2.png?h=144&w=150&hash=50E02DF1ED6DB2903A7012BC13A1FCDD",
            "https://www.pewtrusts.org/-/media/data-visualizations/infographics/2021/08/asset-3.png?h=157&w=150&hash=5EDA540C214FBF14850830175069E30A",
            "https://www.pewtrusts.org/-/media/data-visualizations/infographics/2021/08/asset-4.png?h=179&w=150&hash=774FF978D0F590BF3A1D0A865CE7EE7E",
            "https://www.pewtrusts.org/-/media/data-visualizations/infographics/2021/08/asset-5.png?h=176&w=150&hash=F715F536BF621AE7E8BC956FE88C9DF6",
            "https://www.pewtrusts.org/-/media/data-visualizations/infographics/2021/08/asset-6.png?h=176&w=150&hash=2ECB9463C26F30FB14856E2D8D0E3225",
            "https://www.pewtrusts.org/-/media/post-launch-images/2021/04/gettyimages909024898jpgmaster/16x9_m.jpg?h=346&w=615&la=en&hash=DFEE833A2EEA00F30BE361763EC614DF",
            "https://www.pewtrusts.org/-/media/post-launch-images/2021/04/gettyimages909024898jpgmaster/1x1_s.jpg?h=615&w=615&la=en&hash=770A087CC6C07598D55E9B30862EB1A2",
            "https://www.pewtrusts.org/-/media/post-launch-images/2021/04/gettyimages909024898jpgmaster/1x1_s.jpg?h=615&w=615&la=en&hash=770A087CC6C07598D55E9B30862EB1A2",
            "https://www.pewtrusts.org/-/media/post-launch-images/2021/03/gettyimages1267824486jpgmaster/16x9_m.jpg?h=346&w=615&la=en&hash=D4CA2F27961D87FEA9AEA2D8AD2AC9AD",
            "https://www.pewtrusts.org/-/media/post-launch-images/2021/03/gettyimages1267824486jpgmaster/1x1_s.jpg?h=615&w=615&la=en&hash=6657A26A047955BF6FDAEF6B07431F25",
            "https://www.pewtrusts.org/-/media/post-launch-images/2021/03/gettyimages1267824486jpgmaster/1x1_s.jpg?h=615&w=615&la=en&hash=6657A26A047955BF6FDAEF6B07431F25",
            "https://www.pewtrusts.org/-/media/post-launch-images/2024/06/usc/gettyimages998731212jpgmaster/1x1_s.jpg?h=317&w=317&la=en&hash=0661146C19074D310269AFD2317BBB26",
            "https://www.pewtrusts.org/-/media/post-launch-images/2024/08/gettyimages976155958jpgmaster/1x1_s.jpg?h=317&w=317&la=en&hash=90E65D7456C1020381C19E2F5B215669",
            "https://www.pewtrusts.org/-/media/post-launch-images/2024/07/297209453720781eae464ojpgmaster/1x1_s.jpg?h=317&w=317&la=en&hash=F2FA7570AF1A98E9940907F8822FA338",
            "https://www.pewtrusts.org/-/media/post-launch-images/2024/07/landing-page-bai-v3/16x9_s.jpg?h=180&w=320&la=en&hash=735E99842075BA11A9F0CBCF78B9E218",
            "https://www.pewtrusts.org/-/media/post-launch-images/2024/07/gettyimages1005963886jpgmaster/16x9_m.jpg?h=180&w=320&la=en&hash=DDB16D4273397857C47A2EF77C714E4D",
            "https://www.pewtrusts.org/-/media/post-launch-images/2024/07/img7651jpegmaster/16x9_m.jpg?h=180&w=320&la=en&hash=AD841E285957596B7A08F8099851039E",
            "https://www.pewtrusts.org/-/media/post-launch-images/2024/07/gettyimages1432001474jpgmaster/16x9_m.jpg?h=180&w=320&la=en&hash=CD80A779B6D8C5FAE70B81E8B8B72B9B"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            "Issue Brief",
            "Health",
            "Health Care",
            "US Policy",
            "Food and Drug Safety",
            "Health Care Products",
            "United States",
            "Pew Charitable Trusts",
            "Liz Richardson"
        ],
        "tags": null,
        "authors": [
            "The Pew Charitable Trusts"
        ],
        "publish_date": "2021-08-05T13:39:00-04:00",
        "summary": "",
        "meta_description": "Health care organizations are using artificial intelligence (AI)—which the U.S. Food and Drug Administration defines as “the science and engineering of making intelligent machines”—for a growing range of clinical, administrative, and research purposes. This AI software can, for example, help health care providers diagnose diseases, monitor patients’ health, or assist with rote functions such as scheduling patients.",
        "meta_lang": "en",
        "meta_favicon": "https://www.pewtrusts.org/-/media/files/favicon.ico",
        "meta_site_name": "",
        "canonical_link": "https://www.pewtrusts.org/en/research-and-analysis/issue-briefs/2021/08/how-fda-regulates-artificial-intelligence-in-medical-products",
        "text": "Overview\n\nHealth care organizations are using artificial intelligence (AI)—which the U.S. Food and Drug Administration defines as “the science and engineering of making intelligent machines”—for a growing range of clinical, administrative, and research purposes. This AI software can, for example, help health care providers diagnose diseases, monitor patients’ health, or assist with rote functions such as scheduling patients.\n\nAlthough AI offers unique opportunities to improve health care and patient outcomes, it also comes with potential challenges. AI-enabled products, for example, have sometimes resulted in inaccurate, even potentially harmful, recommendations for treatment.1 These errors can be caused by unanticipated sources of bias in the information used to build or train the AI, inappropriate weight given to certain data points analyzed by the tool, and other flaws.\n\nThe regulatory framework governing these tools is complex. FDA regulates some—but not all—AI-enabled products used in health care, and the agency plays an important role in ensuring the safety and effectiveness of those products under its jurisdiction. The agency is currently considering how to adapt its review process for AI-enabled medical devices that have the ability to evolve rapidly in response to new data, sometimes in ways that are difficult to foresee.2\n\nThis brief describes current and potential uses of AI in health care settings and the challenges these technologies pose, outlines how and under what circumstances they are regulated by FDA, and highlights key questions that will need to be addressed to ensure that the benefits of these devices outweigh their risks. It will take a collective effort by FDA, Congress, technology developers, and the health care industry to ensure the safety and effectiveness of AI-enabled technology.\n\nWhat is AI and how is it used in health care?\n\nAI refers to the ability of a machine to perform a task that mimics human behavior, including problem-solving and learning.3 It can be used for a range of purposes, including automating tasks, identifying patterns in data, and synthesizing multiple sources of information. In health care, AI technologies are already used in fields that rely on image analysis, such as radiology and ophthalmology, and in products that process and analyze data from wearable sensors to detect diseases or infer the onset of other health conditions.4\n\nAI programs can also predict patient outcomes based on data collected from electronic health records, such as determining which patients may be at higher risk for disease or estimating who should receive increased monitoring. One such model identifies patients in the emergency room who may be at increased risk for developing sepsis based on factors such as vital signs and test results from electronic health records.5 Another hospital system has developed a model that aims to better predict which discharged patients are likely to be readmitted following their release compared with other risk-assessment tools.6 Other health care systems will likely follow suit in developing their own models as the technology becomes more accessible and well-established, and as federal regulations implement efforts to facilitate data exchange between electronic health record systems and mobile applications, a process known as interoperability.7\n\nFinally, AI can also play a role in research, including pharmaceutical development, combing through large sets of clinical data to improve a drug’s design, predict its efficacy, and discover novel ways to treat diseases.8 The COVID-19 pandemic might help drive advances in AI in the clinical context, as hospitals and researchers have deployed it to support research, predict patient outcomes, and diagnose the disease.9 Some examples of AI products developed for use against COVID-19:10\n\nCOViage, a software prediction system, assesses whether hospitalized COVID-19 patients are at high risk of needing intubation.11\n\nCLEWICU System, prediction software that identifies which ICU COVID-19 patients are at risk for respiratory failure or low blood pressure.12\n\nMount Sinai Health System developed an AI model that analyzes computed tomography (CT) scans of the chest and patient data to rapidly detect COVID-19.13\n\nResearchers at the University of Minnesota, along with Epic Systems and M Health Fairview, developed an AI tool that can evaluate chest X-rays to diagnose possible cases of COVID-19.14\n\nHow are AI products developed?\n\nAI can be developed using a variety of techniques. In traditional, or rules-based, approaches, an AI program will follow human-prescribed instructions for how to process data and make decisions, such as being programmed to alert a physician each time a patient with high blood pressure should be prescribed medication.15 Rules-based approaches are usually grounded in established best practices, such as clinical practice guidelines or literature.16 On the other hand, machine learning (ML) algorithms—also referred to as a data-based approach—“learn” from numerous examples in a dataset without being explicitly programmed to reach a particular answer or conclusion.17 ML algorithms can learn to decipher patterns in patient data at scales larger than a human can analyze while also potentially uncovering previously unrecognized correlations.18 Algorithms may also work at a faster pace than a human. These capabilities could be especially useful in health care settings, which can provide continuous streams of data from sources, including patient medical records and clinical studies.19\n\nMost ML-driven applications use a supervised approach in which the data used to train and validate the algorithm is labeled in advance by humans; for example, a collection of chest X-rays taken of people who have lung cancer and those who do not, with the two groups identified for the AI software. The algorithm examines all examples within the training dataset to “learn” which features of a chest X-ray are most closely correlated with the diagnosis of lung cancer and uses that analysis to predict new cases. Developers then test the algorithm to see how generalizable it is; that is, how well it performs on a new dataset, in this case, a new set of chest X-rays. Further validation is required by the end user, such as the health care practice, to ensure that the algorithm is accurate in real-world settings. Unsupervised learning is also possible, in which an algorithm does not receive labeled data and instead infers underlying patterns within a dataset.20\n\nChallenges and risks with AI-enabled products\n\nLike any digital health tool, AI models can be flawed, presenting risks to patient safety. These issues can stem from a variety of factors, including problems with the data used to develop the algorithm, the choices that developers make in building and training the model, and how the AI-enabled program is eventually deployed.\n\nAI programs should be built and trained appropriately\n\nAI algorithms need to be trained on large, diverse datasets to be generalizable across a variety of populations and to ensure that they are not biased in a way that affects their accuracy and reliability. These challenges can resemble those for other health care products. For example, if a drug is tested in a clinical trial population that is not sufficiently representative of the actual populations it will be used in, it will not work as well when implemented in real-world clinical settings. In AI, similarly, any model must be evaluated carefully to ensure that its performance can be applied across a diverse set of patients and settings.\n\nHowever, such datasets are often difficult and expensive to assemble because of the fragmented U.S. health care system, characterized by multiple payers and unconnected health record systems. These factors can increase the propensity for error due to datasets that are incomplete or inappropriately merged from multiple sources.21 A 2020 analysis of data used to train image-based diagnostic AI systems found that approximately 70% of the studies that were included used data from three states, and that 34 states were not represented at all. Algorithms developed without considering geographic diversity, including variables such as disease prevalence and socioeconomic differences, may not perform as well as they should across a varied array of real-world settings.22\n\nThe data collection challenges and the inequalities embedded within the health care system contribute to bias in AI programs that can affect product safety and effectiveness and reinforce the disparities that have led to improper or insufficient treatment for many populations, particularly minority groups.23 For example, cardiovascular disease risks in populations of races and ethnicities that are not White have been both overestimated and underestimated by algorithms trained with data from the Framingham Heart Study, which mostly involved White patients.24 Similarly, if an algorithm developed to help detect melanoma is trained heavily on images of patients with lighter skin tones, it may not perform as well when analyzing lesions on people of color, who already present with more advanced skin disease and face lower survival rates than White patients.25\n\nBias can also occur when an algorithm developed in one setting, such as a large academic medical center, is applied in another, such as a small rural hospital with fewer resources. If not adapted and validated for its new context, an AI program may recommend treatments that are not available or appropriate in a facility with less access to specialists and cutting-edge technology.26\n\nMoreover, assembling sufficiently large patient datasets for AI-enabled programs can raise complex questions about data privacy and the ownership of personal health data. Protections to ensure that sensitive patient data remains anonymous are vital.27 Some health systems are sharing their patients’ data with technology companies and digital startups to develop their AI-based programs, sometimes without those patients’ knowledge. There is ongoing debate over whether patients should consent to having their data shared, and whether they should share in the profits from products that outside entities develop using their data.28 However, anonymizing patient data can pose its own challenges, as it can sometimes undermine efforts to ensure the representativeness of large datasets; if patient demographics are unknown to AI developers, then they may not be able to detect bias in the data.\n\nEnsuring safe and effective use of AI-enabled products\n\nOnce an AI-enabled program has been developed, it must be used in a way that ensures that it consistently performs as expected. This can be a complex undertaking, depending on the purpose of the AI model and how it is updated.\n\nML algorithms, for example, fall along a spectrum from “locked” to “adaptive” (also referred to as “continuous learning”). In a locked algorithm, the same input will always produce the same result unless the developer updates the program. In contrast, an adaptive algorithm has the potential to update itself based on new data, meaning that the same input could generate different decisions and recommendations over time.29 Either type of algorithm presents its own challenges.\n\nLocked algorithms can degrade as new treatments and clinical practices arise or as populations alter over time. These inevitable changes may make the real-world data entered into the AI program vastly different from its training data, leading the software to yield less accurate results. An adaptive algorithm could present an advantage in such situations, because it may learn to calibrate its recommendations in response to new data, potentially becoming more accurate than a locked model. However, allowing an adaptive algorithm to learn and adapt on its own also presents risks, including that it may infer patterns from biased practices or underperform in small subgroups of patients.30\n\nAI-enabled programs can also pose risks if they are not deployed appropriately and monitored carefully. One study found that a widely used algorithm disproportionately recommended White patients for high-risk care management programs, which provide intensive—and often expensive—services to people with complex health needs. Several health systems relied on the algorithm to identify patients who were most likely to benefit. However, the algorithm used higher health care costs as a proxy for medical need. Because Black patients are less likely to have access to care, even if they are insured, their health care costs tend to be lower. As a result, the algorithm systematically underestimated their health needs and excluded them from high-risk care programs.31\n\nOther challenges relate to the explainability of the output—that is, how easy it is to explain to the end user how a program produced a certain result—and the lack of transparency around how an AI-enabled program was developed. Some AI programs, for example, are referred to as “black-box” models because the algorithms are derived from large datasets using complex techniques and reflect underlying patterns that may be too convoluted for a person, including the initial programmer, to understand. AI companies may also choose to keep their algorithms confidential, as proprietary information.32 Moreover, companies do not always publicly report detailed information on the datasets they use to develop or validate algorithms, limiting the ability of health care providers to evaluate how well the AI will perform for their patients. For example, a report examining companies’ public summaries about their FDA-approved AI tools found that, of the 10 products approved for breast imaging, only one included a breakdown of the racial demographics in the dataset used to validate the algorithm. Breast cancer is significantly more likely to be fatal in Black women, who may be diagnosed at later stages of the disease and who experience greater barriers to care. Some or all of the AI devices in question may have been trained and validated on diverse patient populations, but the lack of public disclosure means that health care providers and patients might not have all the information they need to make informed decisions about the use of these products.33\n\nIn addition, patients are often not aware when an AI program has influenced the course of their care; these tools could, for example, be part of the reason a patient does not receive a certain treatment or is recommended for a potentially unnecessary procedure.34 Although there are many aspects of health care that a patient may not fully understand, in a recent patient engagement meeting hosted by FDA, some committee members—including patient advocates—expressed a desire to be notified when an AI product is part of their care. This desire included knowing if the data the model was trained on was representative of their particular demographics, or if it had been modified in some way that changed its intended use.35\n\nGiven the complexity of these products and the challenge of deploying them, health systems may need to recruit or train staff members with the technical skills to evaluate these models, understand their limitations, and implement them effectively. A provider’s trust in—and ability to correctly and appropriately use—an AI tool is fundamental to its safety and effectiveness, and these human factors may vary significantly across institutions and even individuals.36 If providers do not understand how and why an algorithm arrived at a particular decision or result, they may struggle to interpret the result or apply it to a patient.\n\nSoftware developers, health care providers, policymakers, and patients all have a role to play in addressing these various challenges. Regulatory agencies also may need to adapt their current oversight processes to keep pace with the rapid shifts underway in this field.\n\nHow and under what circumstances does FDA regulate AI products?\n\nFDA is tasked with ensuring the safety and effectiveness of many AI-driven medical products. The agency largely regulates software based on its intended use and the level of risk to patients if it is inaccurate. If the software is intended to treat, diagnose, cure, mitigate, or prevent disease or other conditions, FDA considers it a medical device.37 Most products considered medical devices and that rely on AI/ML are categorized as Software as a Medical Device (SaMD).38 Examples of SaMD include software that helps detect and diagnose a stroke by analyzing MRI images, or computer-aided detection (CAD) software that processes images to aid in detecting breast cancer.39 Some consumer-facing products—such as certain applications that run on a smartphone—may also be classified as SaMD.40 By contrast, FDA refers to a computer program that is integral to the hardware of a medical device—such as one that controls an X-ray panel—as Software in a Medical Device.41 These products can also incorporate AI technologies.\n\nAs with any medical device, AI-enabled software is subject to FDA review based on its risk classification. Class I devices—such as software that solely displays readings from a continuous glucose monitor— pose the lowest risk. Class II devices are considered to be moderate to high risk, and may include AI software tools that analyze medical images such as mammograms and flag suspicious findings for a radiologist to review.48 Most Class II devices undergo what is known as a 510(k) review (named for the relevant section of the Federal Food, Drug, and Cosmetic Act), in which a manufacturer demonstrates that its device is “substantially equivalent” to an existing device on the market with the same intended use and technological characteristics.49 One study found that the majority of FDA-reviewed AI-based devices on the market have come through FDA’s 510(k) pathway. However, the authors note that they relied on publicly available information, and because the agency does not require companies to categorize their devices as AI/ML-based in public documents, it is difficult to know the true number.50\n\nAlternatively, certain Class I and Class II device manufacturers may submit a De Novo request to FDA, which can be used for devices that are novel but whose safety and underlying technology are well understood, and which are therefore considered to be lower risk.51 Several AI-driven devices currently on the market—such as IDx-DR, OsteoDetect, and ContaCT (see the text box, “Examples of FDA Cleared or Approved AI-Enabled Products”)—are Class II devices that were reviewed through the De Novo pathway.52\n\nClass III devices pose the highest risk. They include products that are life-supporting, life-sustaining, or substantially important in preventing impairment of human health. These devices must undergo the full premarket approval process, and developers must submit clinical evidence that the benefits of the product outweigh the risks.53 The continuous glucose monitoring system, Guardian Connect system, was approved through a premarket approval.54\n\nOnce a device is on the market, FDA takes a risk-based approach to determine whether it will require premarket review of any changes the developer makes. In general, each time a manufacturer significantly updates the software or makes other changes that would substantially affect the device’s performance, the device may be subject to additional review by FDA, although the process for this evaluation differs depending on the device’s risk classification and the nature of the change.\n\nClinical decision support (CDS) software is a broad term that FDA defines as technologies that provide health care providers and patients with “knowledge and person-specific information, intelligently filtered or presented at appropriate times to enhance health and health care.”56 Studies have shown that CDS software can improve patient care.57 These products can have device and nondevice applications. To be exempt from the definition of device, and not regulated by the FDA, CDS software must meet criteria that Congress set in the 21st Century Cures Act of 2016. (See the text box, “Exemptions From FDA Review.”)\n\nCrucially, the CDS software must support or provide recommendations to health care providers as they make clinical decisions, but the software cannot be intended toreplace a provider’s independent judgment. That is, the software can inform decisions, but it cannot be intended as the driving factor behind them. Otherwise, the software must be regulated as a medical device by the agency. The distinction between informing and driving a decision can be difficult to assess and has proved challenging for FDA to describe as it attempts to implement the law. The agency released draft guidance in 2017 on how it would interpret those provisions with respect to CDS software. In response to feedback from product developers, which raised concerns that the agency was interpreting its authority too broadly, FDA officials revised and re-released the draft guidance in 2019. However, the 2019 guidance—in which FDA attempted to harmonize its interpretation of the 21st Century Cures Act with existing international criteria for software—has also drawn concerns from some health care provider organizations. They argue that the guidance may exclude too many types of software from review and that FDA needs to clarify how the agency would apply it to specific products.58\n\nThis is particularly the case for CDS products—including those that rely on AI—developed and used by health care providers. Some health systems may be developing or piloting AI-driven CDS software for use within their own facility that might technically meet the definition of a medical device. The distinction between the practice of medicine—which FDA does not regulate—and a device is unclear in circumstances in which a software program is developed and implemented within a single health care system and is not sold to an outside party. The agency has not publicly stated its position on this issue; however, current regulations do exempt licensed practitioners who manufacture or alter devices solely for use in their practice from product registration requirements.59\n\nHospital accrediting bodies (such as the Joint Commission), standards-setting organizations (such as the Association for the Advancement of Medical Instrumentation), and government actors may need to fill this gap in oversight to ensure patient safety as these tools are more widely adopted.60 For example, the Federal Trade Commission (FTC), which is responsible for protecting consumers and promoting fair market competition, published guidance in April 2020 for organizations using AI-enabled algorithms. Because algorithms that automate decision-making have the potential to produce negative or adverse outcomes for consumers, the guidance emphasizes the importance of using tools that are transparent, fair, robust, and explainable to the end consumer.61 One year later, the FTC announced that it may take action against those organizations whose algorithms may be biased or inaccurate.62\n\nEmerging FDA proposals for SaMD regulation\n\nFDA officials have acknowledged that the rapid pace of innovation in the digital health field poses a significant challenge for the agency. They say new regulatory frameworks will be essential to allow the agency to ensure the safety and effectiveness of the devices on the market without unnecessarily slowing progress.63\n\nIn 2019, the agency began piloting an oversight framework called the Software Precertification Program, which, if fully implemented, would be a significant departure from its normal review process. Rather than reviewing devices individually, FDA would first evaluate the developer. If the organization meets certain qualifications and demonstrates it has rigorous processes to develop safe, effective devices, it would be able to undergo a significantly streamlined review process and make changes or even introduce products without going through premarket review. Nine companies participated in this pilot program. The lessons learned may help inform the development of a future regulatory model for software-based medical devices.64 However, some members of Congress have questioned FDA’s statutory authority to establish this program.65 Legislation may be required before FDA can fully implement it, and its appeal to software developers is not yet clear.\n\nThe agency has also proposed a regulatory framework targeted to SaMD products that rely on an adaptive learning approach. Thus far, FDA has only cleared or approved AI devices that rely on a “locked” algorithm, which does not change over time unless it is updated by the developer. Adaptive algorithms, by contrast, have the potential to incorporate new data and “learn” in real time, meaning that the level of risk or performance of the product may also change rapidly. Given the speed and sometimes unpredictable nature of these changes, it can be difficult to determine when the SaMD’s algorithm may require additional review by the agency to ensure that it is still safe and effective for its intended use.\n\nIn a 2019 white paper, FDA outlined a potential approach to addressing this question of adaptive learning. It is based on four general principles:66\n\nClear expectations on quality systems and good machine learning practices. As with any device manufacturer, FDA expects SaMD developers to have an established system to ensure that their device meets the relevant quality standards and conforms to regulations. In addition, a developer would need to implement established best practices for developing an algorithm, known as Good Machine Learning Practices (GMLP). This set of standards is still evolving, and may eventually need to be included as an amendment to current Good Manufacturing Practice requirements for devices.67 FDA has recently stated it needs industry and stakeholder input to address outstanding questions on what these good practices look like for algorithm design, training, and testing.68\n\nPremarket assessment of SaMD products that require it. Under this framework, developers would have the option to submit a plan for future modifications, called a predetermined change control plan, as part of the initial premarket review of an SaMD that relies on AI/ML. This plan would include the types of anticipated modifications that may occur and the approach the developer would use to implement those changes and reduce the associated risks.\n\nRoutine monitoring of SaMD products by manufacturers to determine when an algorithm change requires FDA review. Under the current regulatory framework, many changes to an SaMD product would likely require the developer to file a new premarket submission. In the proposed approach, if modifications are made within the bounds of the predetermined change control plan, developers would need only to document those changes. If the changes are beyond the scope of the change control plan but do not lead to a new intended use of the device (for example, the developer makes the SaMD compatible with other sources of data, or incorporates a different type of data), then FDA may perform a review of the change control plan alone and approve a new version. However, if the modifications lead to a new intended use (for example, by expanding the target patient population from adults to children), then FDA would likely need to conduct an additional premarket review.\n\nTransparency and real-world performance monitoring. As part of this approach, FDA would expect a commitment from developers to adhere to certain principles of transparency and engage in ongoing performance monitoring. As such, developers would be expected to provide periodic reporting to the agency on implemented updates and performance metrics, among other requirements.\n\nThe proposed framework would be a significant shift in how FDA currently regulates devices, and—as with the precertification program—the agency has acknowledged that certain aspects of the framework may require congressional approval to implement.69 Even if permission is granted, there are outstanding questions about how this framework would be implemented in practice and applied to specific devices. FDA is currently working on a series of follow-up documents that will provide further details on its proposed approach.70\n\nMost recently, the agency published the “Artificial Intelligence/Machine Learning (AI/ML)-Based Software as a Medical Device (SaMD) Action Plan,” outlining its intended next steps. These include updating its proposed framework and issuing draft guidance on the predetermined change control plan, encouraging harmonization among technology developers on the development of GMLP, and holding a public workshop on medical device labeling to support transparency to end users. In addition, the agency will support efforts to develop methods for the evaluation and improvement of ML algorithms, including how to identify and eliminate bias, and to work with stakeholders to advance real-world performance monitoring pilots.71\n\nRemaining questions and oversight gaps\n\nEspecially as the use of AI products in health care proliferates, FDA and other stakeholders will need to develop clear guidelines on the clinical evidence necessary to demonstrate the safety and effectiveness of such products and the extent to which product labels need to specify limitations on their performance and generalizability. As part of this effort, the agency could consider requiring developers to provide public information about the data used to validate and test AI devices so that end users can better understand their benefits and risks.\n\nFDA’s recent SaMD Action Plan is a good step forward, but the agency will still need to clarify other key issues, including:\n\nWhen a modification to SaMD or an adaptive ML device requires premarket review. The draft guidance on the predetermined change control plan could be a critical part of this policy.\n\nWhether and how the Software Precertification Program can be extended beyond the pilot phase.\n\nThe distinction between software regulated by FDA and exempt software, which will turn heavily on the difference between informing clinical decisions and driving them.\n\nHow GMLP, when they are developed, will intersect with the current quality system regulations that apply to all devices.\n\nHow software updates and potential impacts on performance will be communicated to end users.\n\nIn addition, because there are products otherwise excluded from the definition of a medical device, another oversight body may need to play a role in ensuring patient safety, particularly for AI-enabled software not subject to FDA’s authority. Further, for AI products used in the drug development process, FDA may need to provide additional guidance on the extent and type of evidence necessary to validate that products are working as intended.72\n\nTo fully seize the potential benefits that AI can add to the health care field while simultaneously ensuring the safety of patients, FDA may need to forge partnerships with a variety of stakeholders, including hospital accreditors, private technology firms, and other government actors such as the Office of the National Coordinator for Health Information Technology, which promulgates key standards for many software products, or the Centers for Medicare and Medicaid Services, which makes determinations about which technologies those insurance programs will cover. And, as previously mentioned, Congress may need to grant FDA additional authorities before the agency can implement some of its proposed policies, particularly as they relate to the precertification pilot.\n\nConclusion\n\nAI represents a transformational opportunity to improve patient outcomes, drive efficiency, and expedite research across health care. As such, health care providers, software developers, and researchers will continue to innovate and develop new AI products that test the current regulatory framework. FDA is attempting to meet these challenges and develop policies that can enable innovation while protecting public health, but there are many questions that the agency will need to address in order to ensure that this happens. As these policies evolve, legislative action may also be necessary to resolve the regulatory uncertainties within the sector.\n\nGlossary\n\nExplainability: The ability for developers to explain in plain language how their data will be used.73\n\nGeneralizability: The accuracy with which results or findings can be transferred to other situations or people outside of those originally studied.74\n\nGood Machine Learning Practices (GMLP): AI/ML best practices (such as those for data management or evaluation), analogous to good software engineering practices or quality system practices.75\n\nMachine learning (ML): An AI technique that can be used to design and train software algorithms to learn from and act on data. These algorithms can be “locked,” so that their function does not change, or “adaptive,” meaning that their behavior can change over time.76\n\nSoftware as a Medical Device (SaMD): Defined by the International Medical Device Regulators Forum as “software intended to be used for one or more medical purposes that perform these purposes without being part of a hardware medical device.”77\n\nEndnotes"
    }
}
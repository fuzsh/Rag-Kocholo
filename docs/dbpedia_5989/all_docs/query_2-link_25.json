{
    "id": "dbpedia_5989_2",
    "rank": 25,
    "data": {
        "url": "https://crfms.pstat.ucsb.edu/seminar.htm",
        "read_more_link": "",
        "language": "en",
        "title": "CRFMS",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://crfms.pstat.ucsb.edu/CRFMS.jpg",
            "https://crfms.pstat.ucsb.edu/images/ucsb_logo.gif"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Seminar\n\nInaugural day: Monday October 16th, 2006\n\n2006-07 Regents' Lecture byDr. Bruno Dupire(Bloomberg and NYU)\n\nPast 2012-2013 seminars\n\nPast 2011-2012 seminars\n\nPast 2010-2011 seminars\n\nPast 2009-2010 seminars\n\nPast 2008-2009 seminars\n\nPast 2007-2008 seminars\n\nPast 2006-2007 seminars\n\nPast 2005-2006 seminars\n\n2013-2014 Seminars\n\nUpcoming\n\npast\n\nMonday, November 18, South Hall 5607F, 3:30-5PM, Refreshments served at 3:15 PM\n\nDr. Thilo Meyer-Brandis (University of Munich)\n\nTitle: Bismut-Elworthy-Li Formulas for Diffusions with Irregular Drift Coefficients\n\nAbstract: One of the most pertinent applications of Malliavin calculus in Mathematical Finance is the representation of the so-called Greeks, which are sensitivities of option prices w.r.t. to involved parameters, as expectation functionals of the pay-off function times a so-called Malliavin weight. In particular for non-smooth pay-off functions this representation yields a numerically efficient way to compute Greeks. In the case of the Delta (sensitivity w.r.t. the initial value of the underlying diffusion), which is of special interest for hedging purposes, the above mentioned representation is also referred to as Bismut-Elworthy-Li type formula. However, in the existing literature the underlying ItËo diffusion is assumed to have smooth coefficients. For example, an extended Ornstein-Uhlenbeck process with regime switching mean reversion rate, an important model in electricity price modelling, is not included in this class of ItËo diffusions. In this presentation we demonstrate how to generalize the Bismut-Elworthy-Li type formula to ItËo diffusions with irregular drift coefficients. To this end, we study the theoretical questions of existence and Malliavin differentiability of strong solutions of stochastic differential equations with irregular drift coefficients. Using techniques from white noise analysis and a compactness criteria based on Malliavin calculus we develop a new method for the construction of strong solutions of SDEâs with irregular drift coefficients. Further, this approach yields the additional important result that the constructed strong solutions are Malliavin differentiable and Sobolev differentiable in their initial conditions. This insight together with some âlocal time variational calculusâ finally enables us to extend the corresponding Bismut-Elworthy-Li representation.\n\npast\n\nMonday, November 18, South Hall 5607F, 3:30-5PM, Refreshments served at 3:15 PM\n\nDr. Thilo Meyer-Brandis (University of Munich)\n\nTitle: Bismut-Elworthy-Li Formulas for Diffusions with Irregular Drift Coefficients\n\nAbstract: One of the most pertinent applications of Malliavin calculus in Mathematical Finance is the representation of the so-called Greeks, which are sensitivities of option prices w.r.t. to involved parameters, as expectation functionals of the pay-off function times a so-called Malliavin weight. In particular for non-smooth pay-off functions this representation yields a numerically efficient way to compute Greeks. In the case of the Delta (sensitivity w.r.t. the initial value of the underlying diffusion), which is of special interest for hedging purposes, the above mentioned representation is also referred to as Bismut-Elworthy-Li type formula. However, in the existing literature the underlying ItËo diffusion is assumed to have smooth coefficients. For example, an extended Ornstein-Uhlenbeck process with regime switching mean reversion rate, an important model in electricity price modelling, is not included in this class of ItËo diffusions. In this presentation we demonstrate how to generalize the Bismut-Elworthy-Li type formula to ItËo diffusions with irregular drift coefficients. To this end, we study the theoretical questions of existence and Malliavin differentiability of strong solutions of stochastic differential equations with irregular drift coefficients. Using techniques from white noise analysis and a compactness criteria based on Malliavin calculus we develop a new method for the construction of strong solutions of SDEâs with irregular drift coefficients. Further, this approach yields the additional important result that the constructed strong solutions are Malliavin differentiable and Sobolev differentiable in their initial conditions. This insight together with some âlocal time variational calculusâ finally enables us to extend the corresponding Bismut-Elworthy-Li representation.\n\nMonday, November 25, South Hall 5607F, 3:30-5PM, Refreshments served at 3:15 PM\n\nDr. Francesca Biagini (University of Munich)\n\nTitle: Mathematical models for the formation of financial bubbles\n\nAbstract: The notion of an asset price bubble has two ingredients. One is the observed market price of a given financial asset, the other is the asset's intrinsic value, and the bubble is defined as the difference between the two. The intrinsic value, also called the fundamental value of the asset, is usually defined as the expected sum of future discounted dividends. In the first part of the talk we study a flow in the space of equivalent martingale measures and focus on the corresponding shifting perception of the fundamental value of a given asset in an incomplete financial market model. This allows us to capture the birth of a perceived bubble and to describe it as an initial submartingale which then turns into a supermartingale before it falls back to its initial value zero. In the second part of the talk we examine the impact of overconfidence on bubbles formation in the framework of reduced-form models for credit risk. We assume that the wealth associated to a defaultable asset may be strongly affected by the trading activity of overconfident investors, who believe the asset to be safe and provoke an alteration of its estimated value. Since the value process changes under this influence, the underlying pricing measure has also to readapt determining a switch in the space of the equivalent martingale measures. In this way we provide a constructive approach to explain bubbles formation as well as motivate a dynamics in the space of equivalent martingale measures at micro-economic level.\n\nTuesday, November 26, South Hall 5607F, 3:30-5PM, Refreshments served at 3:15 PM\n\nDr. Marcel Nutz (Columbia University)\n\nTitle: On Model Uncertainty in Discrete Time\n\nAbstract: We study the problems of arbitrage, superhedging and utility maximization in a nondominated model of a discrete-time ï¬nancial market. We show that absence of arbitrage in a quasi-sure sense is equivalent to the existence of a suitable family of martingale measures, that a superhedging duality holds, and that optimal strategies for robust utility maximization exist. If time permits, some consequences for martingale theory will also be discussed. Based on joint works with Mathias BeiglbÃ¶ck and Bruno Bouchard.\n\nWednesday, February 5, South Hall 5607F, 3:30-5PM, Refreshments served at 3:15 PM\n\nDr. Tim Leung (Columbia University)\n\nTitle: Pricing Derivatives with Counterparty Risk and Collateralization: A Fixed Point Approach\n\nAbstract: We study a valuation framework for financial contracts subject to reference and counterparty default risks with collateralization requirement. A fixed point approach is proposed to analyze the mark-to-market contract value with counterparty risk provision, and show that it is a unique bounded and continuous fixed point via contraction mapping. This leads us to develop an accurate iterative numerical scheme for valuation. Specifically, we solve a sequence of linear inhomogeneous PDEs, whose solutions converge to the fixed point price function. We apply our methodology to compute the bid and ask prices for both defaultable equity and fixed-income derivatives, and illustrate the non-trivial effects of counterparty risk, collateralization ratio and liquidation convention on the bid-ask spreads. (joint work with Jinbeom Kim (Barclays))\n\nMonday, February 10, South Hall 5607F, 5-6:30PM, Refreshments served at 3:15 PM\n\nDr. Sebastian Jaimungal (University of Toronto)\n\nTitle: Robust Market Making\n\nAbstract: Because market makers (MMs) acknowledge that their models are incorrectly specified, in this paper, we allow for ambiguity in their choices to make their models robust to misspecification in (i) the arrival rate of market orders (MOs), (ii) the fill probability of limit orders, and (iii) the dynamics of the fundamental value of the asset they deal. We demonstrate that MMs adjust their quotes to reduce inventory risk and adverse selection costs. Moreover, robust market making increases the Sharpe ratio of market making strategies and allows for the MM to fine tune the tradeoff between the mean and the standard deviation of expected profits. Our framework adopts the robust optimal control approach of Hansen and Sargent (2007) and we provide analytical solutions for the robust optimal strategies as well as a verification theorem. We also find that in many circumstances ambiguity averse MMs act differently from MMs who are risk averse.\n\nMonday, Febrary 24, South Hall 5607F, 3:30-5PM, Refreshments served at 3:15 PM\n\nDr. Stéphane Loisel, (Lyon 1, visiting Cornell)\n\nTitle: Ruin probability for some particular correlated claims, for worsening risks, and risks with infinite mean.\n\nAbstract: We first give explicit formulas for the infinite time ruin probability for some particular correlated claim amounts or inter-arrival times. We then investigate asymptotics of ruin probabilities when claim distribution is worsening over time, due to phenomena like sectorial inflation or global warming. We end up with some results in the case where claim amounts have infinite mean. If time permits, weâll make the link between some ruin problem and some longevity online changepoint detection problem.\n\nMonday, March 10, South Hall 5607F, 3:30-5PM, Refreshments served at 3:15 PM\n\nAndrey Sarantsev (Ph.D student at dept of Mathematics University of Washington, Seattle)\n\nTitle: Market Models with Splits and Mergers\n\nAbstract: We study models of regulatory breakup but with a fluctuating number of companies. If at some moment the share of the total market capitalization of a company reaches a certain threshold, then the company is split into two random parts. This can be viewed as a consequence of antitrust legislation. Companies are also allowed to merge when an exponential clock rings. Under certain condition, the quantity of stocks does not go to infinity in finite time, and the model does not admit arbitrage.\n\nJoint work with Ioannis Karatzas.\n\nMonday, April 7, South Hall 5607F, 3:30-5PM, Refreshments served at 3:15 PM\n\nDr. Pavel Shevchenko\n\nTitle: A generalized grouped t-copula with multiple parameters of degrees of freedom and analysis of tail dependence in currency carry trades. Joint work with Gareth Peters, Matthew Ames, Guillaume Bagnarosa and Xiaolin Luo\n\nAbstract: The t-copula is a popular dependence structure often used in risk management as it allows for modelling the tail dependence between risks and it is simple to simulate and calibrate. The use of a standard t-copula is often criticized due to its restriction of having a single parameter for the degrees of freedom (dof) that may limit its capability to model the tail dependence structure in a multivariate case. To overcome this problem, the grouped t-copula was proposed in the literature, where risks are grouped a priori in such a way that each group has a standard t-copula with its specific dof parameter. To avoid a priori grouping, which is often difficult in practice, recently we proposed a generalized grouped t-copula, where each group consists of one risk factor. We present characteristics, simulation and calibration procedures for the generalized t-copula, including Markov chain Monte Carlo method for estimation and Bayesian model selection. This generalized grouped t-copula is significantly different from the standard t-copula in terms of risk measures such as tail dependence, value-at-risk and expected shortfall. Using historical data of foreign exchange (FX) rates as a case study, we found that Bayesian model choice criteria overwhelmingly favor the generalized t-copula when compared to the grouped and standard t-copulas. We demonstrate the impact of model choice on the conditional Value-at-Risk for portfolios of major FX rates. In addition, using this and other copula models, we analyse tail dependence in baskets of high and low interest rate currencies used for carry trade investment strategies.\n\nMonday, May 19, South Hall 5607F, 3:30-5PM, Refreshments served at 3:15 PM\n\nDr. Paolo Cavarani (L'Aquila, Italy)\n\nTitle:On Control Invariance methods and their use in Finance\n\nAbstract:Asset transactions in the financial market can be viewed as controls influenc- ing the dynamic evolution of an uncertain system. Uncertainty is traditionally described in terms of a stochastic control process driven by random noise. Re- cent developments in control theory recognized the fact that in physics, biology, engineering, and other fields including finance, probability distributions are es- sentially unknown - or at best largely contingent on ad hoc assumptions - and moved in the direction of obtaining essentially distribution-free results. These developments led to the notion of robustness and robust control-invariance. The unknown random variables affecting the systemâs trajectory are assumed to lie in a given set. All prior knowledge of randomness is subsumed under the shape of this set. The ensuing exercise is to determine what properties of the dynamic system remain invariant with respect to that set.\n\nThe seminar provides first an introduction to control invariance and to robust control invariance for a simple class of dynamic systems - that of linear systems. The invariance definitions are essentially independent of linearity because in this context the greater complexity of non-linear systems manifests itself more in terms of machinery and computation than in terms of the underlying conceptual framework. Secondly, the invariance notion is generalized to the case of games. Each player in an n-person game is viewed as a source of uncertainty by other players. A multi-invariance notion akin to Nash equilibrium arises when each player succeeds in making a desired property of the system evolution - for example, state membership to a prescribed set - invariant to the strategies of the other players. We call this notion an Invariant Equilibrium (IE). In both cases simple but powerful computational tools based on Lyapunov theory and linear matrix inequalities will be discussed.\n\n2012-2013 Seminars\n\nFriday, October 19\n\nDr. Kazutoshi Yamazaki (Osaka Uniersity)\n\nTitle: Optimal Stopping for Spectrally Negative Levy Processes and Applications in Finance\n\nAbstract: We consider a class of infinite-time horizon optimal stopping problems for spectrally negative Levy processes. Focusing on strategies of threshold type, we write explicit expressions for the corresponding expected payoffs via the scale function. We obtain and show the equivalence of the continuous/smooth fit condition and the first-order condition for maximization over threshold levels. Extensions to multiple-stopping and applications in Lelandâs endogenous default model are also discussed.\n\nMonday October 22, South Hall 5607F, 3:30-5PM, Refreshments served at 3:15 PM\n\nDr. Hoi Ying Wong (Department of Statistics, The Chinese University of Hong Kong)\n\nTitle: Application and Implication of Cointegration in Asset Pricing\n\nAbstract: Cointegration is a useful econometric tool for identifying assets which share a common equilibrium. The importance of cointegration has become recognized and resulted in a Nobel Prize in Economics for Granger in 2003. In this talk, I will report several recent advances of asset pricing theories based on continuous-time cointegration dynamics. It covers cointegrated pairs-trading using classical mean-variance portfolio theory, cointegration option pricing with stochastic correlations using Fourier analysis, and (if time allows) the hedging with mortality risk in insurance products. Our theories predict that 1. if cointegrated assets are liquidly traded, then there exists a statistical arbitrage opportunity; 2. If the assets are not traded or not liquidly traded, their corresponding derivatives securities, in particular futures contracts, exhibit stochastic convenient yields which are partially driven by cointegrating factors; and 3. As human mortality is not traded by its nature and the national mortality rate is cointegrated with the mortality rate of an individual insurance companys client pool, cointegration techniques enhance the hedging of mortality risk with national mortality bonds. Empirical studies are performed to validate the use of the developed theories and numerical methods.\n\n(The talk is based on several joint papers with M.C. Chiu, T.W. Wong and J. Zhao)\n\nMonday October 29, South Hall 5607F, 3:30-5PM, Refreshments served at 3:15 PM\n\nDr. Mykhaylo Shkolnikov (University of California, Berkeley)\n\nTitle: Asymmetrically colliding Brownian particles in stochastic portfolio theory and beyond\n\nAbstract:We will discuss systems of Brownian particles on the real line, which interact by splitting the local times of collisions among themselves in an asymmetric manner. These can be identified with the collections of ordered processes in a Brownian particle system, in which the drift coefficients, the diffusion coefficients, and the collision local times for the individual particles are assigned according to their ranks. Such processes can be viewed as generalizations of those arising in first-order models for equity markets in the context of stochastic portfolio theory, and are able to correct for several shortcomings of such models while being equally amenable to computations. We also show that, in addition to being of interest in their own right, such systems of Brownian particles arise as universal scaling limits of systems of jump processes on the integer lattice with local interactions. In particular, this result extends the convergence of TASEP to its continuous analogue. This is joint work with Ioannis Karatzas and Soumik Pal.\n\n2011-2012 Seminars\n\nWednesday, September 28, South Hall 5607F, 3:30-5PM, Refreshments served at 3:15 PM\n\nDr. Alexandra Chronopoulou (PSTAT-UCSB)\n\nTitle: Parameter Estimation for Fractional SDEs\n\nAbstract: We consider the parameter estimation problem for a multidimensional stochastic differential equation driven by a fractional Brownian motion with Hurst parameter H > 1/2, with non-linear random drift and diffusion coefficients. Due to the intractability of the likelihood function, we propose the maximizer of a partial likelihood as the estimator of the parameters of the model. We show how to compute this estimator using Malliavin calculus techniques and approximation results. We study the computational efficiency of our method and we provide rates of convergence for the approximation task. For a particular class of fractional SDEs, we establish consistency of the proposed estimator. We apply our methodology to the estimation of the parameters of the fractional Black-Scholes model using S&P 500 data.\n\nMonday October 10, South Hall 5607F, 3:30-5PM, Refreshments served at 3:15 PM\n\nDr. Gerard Brunick (PSTAT, UCSB)\n\nTitle: Weak uniqueness for a class of degenerate diffusions with continuous covariance\n\nAbstract: Motivated by the problem of calibrating linear pricing rules to the market prices of options, we provide a new weak uniqueness result for degenerate diffusions. In particular, we consider path-dependent stochastic differential equations where the diffusion coefficient is a function of both the current location of the process and the running integral of the process, and we show that uniqueness holds for continuous, strictly positive-definite diffusion coefficients. These results combine tools from the theory of singular integrals on Lie groups with the localization machinery of Stroock and Varadhan.\n\nMonday October 17th, South Hall 5607F, 3:30-5PM, Refreshments served at 3:15 PM\n\nDr. Stephan Sturm (ORFE, Princeton University)\n\nTitle: Portfolio optimization under convex incentive schemes: Duality theory and stochastic volatility\n\nAbstract: We consider the utility maximization problem from the point of view of a portfolio manager paid by a convex incentive scheme. This problem departs from classical portfolio optimization theory since the implied utility function is no more concave which produces some interesting phenomena. Using duality theory, we are able top prove existence and uniqueness of the optimal wealth in general (incomplete) semimartingale markets as long as the unique optimizer of the dual problem has no atom with respect to the Lebesgue measure. In many cases, this fact is independent of the incentive scheme and depends only on the structure of the set of equivalent local martingale measures. As example we discuss stochastic volatility models and show that existence and uniqueness of an optimizer are guaranteed as long as the market price of risk satisfies a certain (Malliavin-)smoothness condition. This is joint work with Maxim Bichuch.\n\nMonday, November 21, South Hall 5607F, 3:30-5PM, Refreshments served at 3:15 PM\n\nDr. Srikanth Iyer (PSTAT-UCSB)\n\nTitle: Pricing Credit Derivatives in a Markov Modulated Reduced Form Model\n\nAbstract: Numerous incidents in the financial world have exposed the need for the design and analysis of models for correlated default timings. Some models have been studied in this regard which can capture the feedback in case of a major credit event. We extend the research in the same direction by proposing a new family of models having the feedback phenomena and capturing the effects of regime switching economy on the market. The regime switching economy is modeled by a continuous time Markov chain. The Markov chain may also be interpreted to represent the credit rating of the firm whose bond we seek to price. We model the default intensity in a pool of firms using the Markov chain and a risk factor process. We price some single-name and multi-name credit derivatives in terms of certain transforms of the default and loss processes. These transforms can be calculated explicitly in case the default intensity is modeled as a linear function of a conditionally affine jump diffusion process. In such a case, under suitable technical conditions, the price of credit derivatives are obtained as solutions to a system of ODEs with weak coupling, subject to appropriate terminal conditions. Solving the system of ODEs numerically, we analyze the credit derivative spreads and compare their behavior with the non-switching counterparts. We show that our model can easily incorporate the effects of business cycle. We demonstrate the impact on spreads of the inclusion of rare states that attempt to capture a tight liquidity situation. These states are characterized by low floating interest rate, high default intensity rate and high volatility. We also model the effects of firm restructuring on the credit spread, in case of a default.\n\nMonday, January 9, South Hall 5607F, 3:30-5PM, Refreshments served at 3:15 PM\n\nDr. Konstantinos Spiliopoulos (Brown University)\n\nTitle: Recent results on systemic risk in large financial networks & more\n\nAbstract: The past several years have made clear the need to better understand the behavior of risk in large interconnected financial networks. Interconnections often make a system robust, but they can act as conduits for risk. In this talk, I will present recent results on modeling the dynamics of correlated default events in the financial market. An empirically motivated system of interacting point processes is introduced and we study how different types of risk, like contagion and exposure to systematic risk, compete and interact in large-scale systems. A law of large numbers for the loss from default is proven and used for approximating the distribution of the loss from default in large, potentially heterogenous portfolios. Large deviation arguments are then used to identify the way that atypically large (i.e. ``rare'') default clusters are most likely to occur. The results give insights into how different sources of default correlation interact to generate typical and atypical portfolio losses.\n\nTime permitting, I will discuss briefly recent general results on large deviations and Monte-Carlo methods for multiple scale systems. Questions of interest include qualitative and quantitative descrpition of transitions probabilities between different equilibrium states of a given system. The results can potentially have applications in scientific disciplines such as chemistry and are also related to certain stochastic volatility models.\n\nWednesday, January 11, South Hall 5607F, 3:30-5PM, Refreshments served at 3:15 PM\n\nDr. Tomoyuki Ichiba (PSTAT)\n\nTitle: Planar Diffusions with rank-based characteristics\n\nAbstract: We construct a diffusion process with values in the plane and with rank-based drift/dispersive characteristics. We compute the transition probabilities of this process and the order statistics, discuss pathwise uniqueness and strength of related stochastic differential equations, and study its dynamics under a time-reversal. We also show that the planar diffusion can be represented in terms of one-dimensional diffusion with bang-bang drift driven by a standard Brownian motion, its local time accumulated at the origin, and an independent standard Brownian motion, in a form which can be construed as a two-dimensional analogue of the stochastic equation satisfied by the so-called skew Brownian motion. This is a joint work with E. Robert Fernholz, Ioannis Karatzas, and Vilmos Prokaj.\n\nThursday, January 12, South Hall 5607F, 3:30-5PM, Refreshments served at 3:15 PM\n\nDr. Sergey Nadtochiy (Oxford Man Institute)\n\nTitle: MARKET-BASED APPROACH TO MODELING DERIVATIVES PRICES\n\nABSTRACT. Most classical models for derivatives prices focus on prescribing the time evolution of the underlying stochastic factors. The prices of derivatives are then computed, for example, via the risk-neutral expectations. As markets developed and many derivative contracts became liquidly traded, it appeared necessary, in order to avoid creating arbitrage opportunities and to fully exploit the information given by the market, to calibrate such models so that they reproduce the observed derivatives prices. However, the calibration results may vary significantly from day to day, implying that none of the calibrated models can be used to describe the future time evolution of the derivatives prices and, in particular, study the risks associated with them. The idea of the market-based approach is to model the derivatives prices directly, as the prices of generic financial assets. This approach allows to start a model from an arbitrary combination of derivatives prices currently observed in the market, without having to change (recalibrate) the model. In this presentation, I will outline the main problems associated with the construction of a market-based model and will present the general methodology which provides solutions to these problems. I will also give an overview of the existing constructions of the market-based models, starting with the famous Heath-Jarrow-Morton theory, and show how these results agree with the general method. Finally, I will illustrate the theory by constructing (both mathematically and numerically) a family of market-based models for the European call options of multiple strikes and maturities.\n\nWednesday, January 18, South Hall 5607F, 3:30-5PM, Refreshments served at 3:15 PM\n\nDr. Yan Dolinsky (ETH Zurich)\n\nTitle: Numerical Schemes for Stochastic Volatility Models\n\nAbstract: We present a tree based approximations for stochastic volatility models, such as the Stein and Stein model, Heston model etc. The importance of such numerical schemes follows from the wide use of stochastic volatility models among practitioners and the fact that for these models analytical solutions usually are not available. We show how to calculate efficiently options prices (European and American) with general type of payoffs such as Vanilla Options, Barrier Options, Lookback Options, Asian options etc. Our main tool is the weak convergence approach which allows to approximate diffusion processes by correlated random walks.\n\nFriday, January 20, South Hall 5607F, 3:30-5PM, Refreshments served at 3:15 PM\n\nDr. Gerard Brunick (PSTAT, UCSB)\n\nTitle:Optimal Investment in the Presence of High-water Mark Fees\n\nAbstract: In this talk, we will consider the problem of optimal asset allocation for an agent who may invest in a money market fund, a stock, and a hedge fund. We model the risky assets as correlated geometric Brownian motions and we assume that our investor maximizes discounted CRRA utility from consumption on an infinite horizon. We further suppose that the investment in the hedge fund is subject to a proportional performance fee that is assessed each time the cumulative profit-to-date derived from the investment in the hedge fund eaches a new running maximum. We will see that this problem reduces to the optimal control of a reflected diffusion. We will examine the regularity of the associated Hamilton-Jacobi-Bellman equation and show the existence of optimal controls. Finally, we will examine some qualitative properties of the optimal investment strategy.\n\nMonday Jan 23, South Hall 5607F, 3:30-5PM, Refreshments served at 3:15 PM\n\nDr. Alexandra Chronopoulou (PSTAT, UCSB)\n\nTitle:Stochastic volatility models with long-memory in discrete and continuous time\n\nAbstract: We consider a continuous time stochastic volatility model with long memory in which the stock price is described by a Geometric Brownian motion with volatility that follows a fractional Ornstein-Uhlenbeck process. In addition, we study two discrete time models: a discretization of the continuous model via an Euler scheme and a discrete model in which the returns are a zero mean iid sequence where the volatility is a fractional ARIMA process.\n\nUsing a particle filtering algorithm we estimate the empirical distribution of the unobserved volatility process for all three models. Based on the volatility filter, we construct a multinomial recombining tree for option pricing. We also discuss appropriate parameter estimation techniques for each model. For the long memory parameter, we compute an implied value by calibrating the model with real data.\n\nFinally, we compare the different models using simulated data and we price options on the S&P 500 index.\n\nMonday Feb. 13, South Hall 5607F, 3:30-5PM, Refreshments served at 3:15 PM\n\nDr. Henry Schellhorn (Claremont Graduate University)\n\nTitle: A No-arbitrage Model of Liquidity in Financial Markets involving Brownian Sheets\n\nAbstract: We consider a dynamic market model where buyers and sellers submit limit orders. If at a given moment in time, the buyer is unable to complete his entire order due to the shortage of sell orders at the required limit price, the unmatched part of the order is recorded in the order book. Subsequently these buy unmatched orders may be matched with new incoming sell orders. The total number of buy orders D(p,t) entered in the system at time t is a collection of stochastic processes indexed by a continuous parameter p, the limit price, and its dynamics are modelled by a stochastic partial differential equation (SPDE) driven by a Brownian sheet. The same type of dynamics hold for the total number of sell orders S(p,t).\n\nThe curves D and S are related to but not identical to the demand and supply curves, and thus the exogenous data is not identical to the exogenous data specified in other models (such as Cetin, Jarrow, and Protter 2004, or CJP). The equilibrium price process is the (limit) price at which these two curves are equal at all times. We derive the SPDE of the equilibrium process and then provide necessary and sufficient conditions for the existence of a risk-neutral measure where this process is a local martingale. This result is important in finance because the first fundamental theorem of asset pricing in the CJP model holds if and only if there is a measure where the equilibrium price process is a local martingale. Interestingly, the risk-neutral measure is not unique in our model, which shows that more economic conditions are needed to close it.\n\nThe no-arbitrage conditions we obtain are applicable to a wide class of models, in the same way that the Heath-Jarrow-Morton conditions apply to a wide class of interest rate models. We implement and calibrate several parametric models and analyze empirically when arbitrage occurred using real order book data. We also investigate the effectiveness of some arbitrage strategies based on these parametric models.\n\nWed, February 29, South Hall 5607F, 3:30-5PM, Refreshments served at 3:15 PM\n\nDr. Joscha Diehl (TU Berlin)\n\nTitle: Rough path theory: a quick overview and some applications\n\nAbstract: Differential equations of the form dY = f(Y) dX are covered by classical theory in the case where X is a smooth path. Rough path theory treats such equations when the driving signal X is very irregular in time, e.g. only HÃ¶lder continuous for some exponent larger then zero. It turns out that in general the information given by the path itself is not sufficient to build a satisfying theory. The missing piece is some kind of 'higher order' process, which can be thought of as encoding iterated integrals of the path against itself. I will sketch the main ideas of the theory, using an approach that only requires knowledge of undergraduate mathematics.\n\nWednesday, March 14, South Hall 5607F, 3:30-5PM, Refreshments served at 3:15 PM\n\nDr. Ronnie Sircar (Princeton University)\n\nTitle: A Feedback Model for the Financialization of Commodities Prices\n\nAbstract: Tang and Xiong (2009) discuss the financial markets as a result of increased index investing activity in the past decade. They find empirical evidence of increased exposure of commodities prices to shocks to other asset classes. We build a feedback model to try and capture some of these effects in which traditional economic demand for a commodity, oil say, is perturbed by the influence of portfolio optimizers. The analysis reveals correlation effects proportional to the long or short positions of the investors, along with a lowering of volatility.\n\nMonday, April 16, South Hall 5607F, 3:30-5PM, Refreshments served at 3:15 PM\n\nDr. Marco Frittelli (University of Milano, visiting UCSB)\n\nTitle: Two applications of quasi-convex analysis\n\nAbstract: We introduce a class of law invariant quasi-convex risk measures based on an appropriate family of acceptance sets. As a particular case, we propose a generalization of the classical notion of the V@R, that takes into account not only the probability of the losses, but the balance between such probability and the amount of the loss. Our second application concerns the evaluation of the quality of the scientific research. We introduc a family of performance measures, called Scientific Research Measures (SRM) that are based on the whole distribution of citations. These SRM are: flexible to fit peculiarities of different areas and seniorities; inclusive, as they comprehend several popular indices;coherent, as they share the same structural properties; calibrated to the particular scientific community.\n\nMonday, May 14, South Hall 5607F, 3:30-5PM, Refreshments served at 3:15 PM\n\nDr. Sachin Adlakha (Caltech)\n\nTitle: Mean Field Equilibria in Large Scale Stochastic Games\n\nAbstract:We study stochastic games with many interacting players. In contrast to studying Markov perfect equilibrium (MPE), we consider mean field equilibrium (MFE), where players’ strategies depend only on the long runaverage state of their competitors. We study the conditions under which MFE exists and show that the same conditions ensure that MFE is a good\n\napproximation to MPE in large games. The conditions we propose amount to a dichotomy between decreasing and increasing return to larger states.\n\nMonday, May 21, South Hall 5607F, 3:30-5PM, Refreshments served at 3:15 PM\n\nYuri Saporito (PSTAT-UCSB)\n\nTitle: Functional It Weights for Greeks\n\nAbstract: Functional Ito calculus is an extension of the classical Ito calculus to functions of path and it was first presented by Dr. Bruno Dupire in his paper \"Functional Ito Calculus\". In this talk I will present an application of this theory to the computation of Greeks, which are sensitivities of derivatives prices with respect to model parameters. This gives us an alternative to the Malliavin calculus approach presented by Fournie et al in the classical paper \"Applications of Malliavin calculus to Monte Carlo methods in finance\".\n\n2010-2011 Seminars\n\nMONDAY September 27, South Hall 5607F, 3:30-5PM, Refreshments served at 3:15 PM\n\nDr. Jeremy Staum (Northwestern University)\n\nTitle: Systemic Risk Components and Deposit Insurance Premia\n\nAbstract: In light of recent events, there have been several proposals to establish a theory of financial system risk management analogous to portfolio risk management.Â One important aspect of portfolio risk management is risk attribution, the process of decomposing a portfolio risk measure into components that are attributed to individual assets or activities. Â We consider the total premium required to insure all deposits in the banking system as the systemic risk measure. Â The component of this risk measure attributable to a bank could serve as the bank's deposit insurance premium. Â The richer structure of a banking system, compared to a portfolio, makes the theory of systemic risk components more complicated than the theory of portfolio risk components. Â We propose a scheme for systemic risk attribution that could be\n\nused in setting deposit insurance premia.\n\nJoint work with Ming Liu of Morgan Stanley.\n\nMONDAY October 11, South Hall 5607F, 3:30-5 PM, Refreshments served at 3:15 PM\n\nProf. Tomoyuki Ichiba (PSTAT/CRFMS-UCSB)\n\nTitle:Â Modeling a systemic risk of frozen inter-bank lending\n\nAbstract: In this talk we propose a simple model of banking system and analyze a systemic risk of frozen inter-bank lending. The monetary reserves of banks are modeled as a system of interacting Feller diffusion. The model is simple enough for mathematical analysis, yet captures how lending preferences of banks cause possible multiple bank failures. We quantify the lending preference from one bank to another as a function of all the reserves. We find an extreme example that only k out of n banks can survive. This banking system induces a class of random graph processes in continuous time. We may extend this toy model for understanding the systemic risk and for preventing crisis in the banking system. (joint work with Jean-Pierre Fouque and Chunkai Gao).\n\nMONDAY October 25, South Hall 5607F, 3:30-5 PM, Refreshments served at 3:15 PM\n\nProf. Marco Frittelli (Math. Dept., Milano Univ., visiting PSTAT-CRFMS/UCSB)\n\nTitle: Â On quasiconvex dynamic risk measures\n\nAbstract: We introduce conditional quasiconvex risk measures and provide their dual representation. This generalizes the representation of quasiconvex real valued risk measures and of conditional convex risk measures. These results are applied in the study of the theory of dynamic risk measures and of performance indices.\n\nMONDAY November 15, South Hall 5607F, 3:30-5 PM, Refreshments served at 3:15 PM\n\nDr. Ramon van Handel (ORFE, Princeton)\n\nTitle: Â The ergodic theory of nonlinear filters\n\nAbstract: The goal of nonlinear filtering is to estimate a process of interest given noisy and incomplete observations. Nonlinear filtering methods play a role in a wide variety of applications, ranging from robotics to finance. In many applications, one is interested in understanding the performance of the nonlinear filter (as well as related Monte Carlo algorithms or sequential decision procedures) over a long time horizon. Mathematically, this requires an understanding of the ergodic theory of nonlinear filters. Such a theory was first developed in a classic paper of H. Kunita (1971). Unfortunately, the key part of the proof in this paper contains a fundamental measure-theoretic error, which lies at the heart of the ergodicity problem for nonlinear filters.\n\nIn this talk, I will discuss recent progress in understanding the general ergodic theory of nonlinear filters. I will introduce the central measure-theoretic identity and outline its proof under very general assumptions, by means of the theory of Markov chains in random environments. I will also discuss two surprising counterexamples where the filter fails to be ergodic. The upshot is that the ergodicity of classical nonlinear filtering problems is now largely resolved, but nonlinear filtering problems with infinite dimensional signals (such as appear in applications to weather prediction or data assimilation) remain a mystery.\n\nMONDAY November 22, South Hall 5607F, 3:30-5 PM, Refreshments served at 3:15 PM\n\nDr. Olaf Menkens, Dublin City University (DCU), Ireland\n\nTitle: Optimising Proportional Reinsurance Using a Worst Case Scenario Approach\n\nAbstract: This presentation considers the problem of an insurance company to optimise its reserve process by proportional reinsurance. Usually, the reinsurance level will be determined by a ruin probability constraint or by minimising the ruin probability (see e.g. Hipp and Vogt (2003), Schmidli (2001, 2002, and 2004), or Eisenberg and Schmidli (2008)). Instead of conditioning on the ruin probability, this presentation will maximise the controlled reserve process by a worst--case scenario approach.\n\nThe worst--case scenario approach has been introduced in the context of portfolio optimisation by Korn and Wilmott (2002). This approach has been extended so far in various ways (e.g. considering different utility function (Korn and Menkens (2005)), optimising investment portfolio of an insurance company (Korn (2005)), in a stochastic differential game context (Korn and Steffensen (2007)).\n\nWe start by making the so--called small claims assumption, that is the claims will be modelled as a Brownian motion with drift. Second, the claims will be modelled as the sum of a Brownian motion with drift and a Poisson process and third, claims will be modelled as a Poisson process. Results will be computed, analysed, and compared with the results of minimising the ruin probability.\n\nThis is work in progress and joint research with Ralf Korn (TU Kaiserslautern) and Mogens Steffensen (U of Copenhagen).\n\nMONDAY January 31, 2011, South Hall 5607F, 3:30-5 PM, Refreshments served at 3:15 PM\n\nJorge Zubelli (IMPA, Rio, Brazil)\n\nTitle: Â A Convex-Regularization Framework for Local-Volatility Calibration in Derivative Markets\n\nAbstract: We discuss a unified framework for the calibration of local volatility models that makes use of recent tools of convex regularization of ill-posed Inverse Problems. The key aspect of the present approach is that it addresses in a general and rigorous way the issue of convergence and sensitivity of the regularized solution when the noise level of the observed prices goes to zero. In particular, we present convergence results that include convergence rates with respect to noise level in fairly general contexts and go well beyond the classical quadratic regularization. This is joint work with Otmar Scherzer (Vienna) and Adriano De Cezaro (UFRGS).\n\nMONDAY February 14, 2011, South Hall 5607F, 3:30-5 PM, Refreshments served at 3:15 PM\n\nMatt Lorig (UCSB)\n\nTitle: Â Time changed Fast Mean-Reverting Stochastic Volatility Models\n\nAbstract: We introduce a class of randomly time-changed fast mean-reverting stochastic volatility models and, using spectral theory and singular perturbation techniques, we derive an approximation for the prices of European options in this setting. Three examples of random\n\ntime-changes are provided and the implied volatility surfaces induced by these time-changes are examined as a function of the model parameters. Three key features of our framework are that we are able to incorporate jumps into the price process of the underlying asset, allow for the leverage effect, and accommodate multiple factors of volatility, which operate on different time-scales.\n\nMONDAY April 11, 2011, South Hall 5607F, 3:30-5 PM, Refreshments served at 3:15 PM\n\nSylvain Rubenthaler (U of Nice)\n\nTitle: Particle systems, Kalman interacting filter, definitions and proof of convergence.\n\nAbstract: I will talk about particle systems used to approximate conditional laws. I will present the classic system and something called the interacting Kalman filter. I will give some elements of proof of why this last system has a good performance uniformly in time. Finally, I will present some examples.\n\nMONDAY May 9, 2011, South Hall 5607F, 3:30-5 PM, Refreshments served at 3:15 PM\n\nYuri Saporito (PSTAT-UCSB)\n\nTitle: Functional Ito's Calculus a la Dupire\n\nAbstract: This talk will present some results of an extension of the ItÃ´ Calculus to functionals of the current path of a stochastic process. These results were first presented by Dr. Bruno Dupire in his paper \"Functional ItÃ´ Calculus\", which is the main reference for the talk. We will also discuss the possible applications to Finance.\n\nMONDAY May 23, 2011, South Hall 5607F, 3:30-5 PM, Refreshments served at 3:15 PM\n\nProf. Rodney Garratt (Economics, UCSB)\n\nTitle: Mapping systemic risk in the international banking network\n\nAbstract: Systemic risk among the network of international banking groups arises when financial stress threatens to criss-cross many national boundaries and expose imperfect international co-ordination. To assess this risk, we apply an information theoretic map equation due to Martin Rosvall and Carl Bergstrom to partition banking groups from 21 countries into modules. The resulting modular structure reflects the flow of financial stress through the network, combining nodes that are most closely related in terms of the transmission of stress. The modular structure of the international banking network has changed dramatically over the past three decades. In the late 1980s four important financial centres formed one large supercluster that was highly contagious in terms of transmission of stress within its ranks, but less contagious on a global scale. Since then the most influential modules have become significantly smaller and more broadly contagious. The analysis contributes to our understanding as to why defaults in US sub-prime mortgages had such large global implications.\n\nWEDNESDAY June 1, South Hall 5607F, 3:30 PM, Refreshments served at 3:15 PM\n\nProf. Bernt Oksendal (University of Oslo)\n\nTITLE: Optimal pricing strategies and Stackelberg equilibria in time-delayed stochastic differential games.\n\nABSTRACT: In the classical newsvendor problem there are two agents: (i) The manufacturer, who today (i.e. at time t-\\delta) Â decides the unit price to sell the manufactured goods for to the retailer, with delivery tomorrow (at time t) (ii) The retailer, who then today (at time t-\\delta) Â decides the quantity to order from the manufacturer and the price to sell each item for to the public the next day.\n\nWhat is the optimal price set by the manufacturer and the optimal quantity to order and the optimal retailer price? The problem is that neither of these agents know what the demand will be the next day, only its probabilistic distribution. This is a problem that occurs in many situations, for example in the pricing of electricity in a liberated electricity market.\n\nWe generalize this classical newsvendor problem to continuous time and a jump diffusion setting, and formulate it as a problem to find the Stackelberg equilibrium of a stochastic differential game with delayed information flow. We find a maximum principle for this type of control problem, and use it to solve the optimal pricing problem in some specific cases.\n\nThe presentation is based on recent joint work with Leif Sandal and Jan Uboe, both at NHH, Bergen, Norway.\n\nTHURSDAY June 2, South Hall 5607F, 9:30-10:45AM, NOTE unusual day and time\n\nProf Matheus Grasselli (McMaster University, Canada)\n\nTitle: An agent-based model for bank formation, bank runs and interbank networks\n\nAbstract: We introduce a simple framework where banks emerge as a response to a natural need in society of individuals with heterogeneous liquidity preferences. We examine bank runs and under what conditions an interbank market Â is to be established.\n\nWe start with an economy consisting of a group of individuals arranged in a 2-dimensional cellular automate and two assets available for investment. Because of uncertainty, individuals might change their investing preferences and accordingly seek their surroundings neighbours as trading partners to fulfil their new preferences. We demonstrate that the individual uncertainty regarding his preference shock coupled with the possibility of not finding a suitable trading partners when needed give rise to banks as liquidity providers. Using a simple learning process, individuals decide whether or not to join the banks, and through a feedback mechanism we illustrate why banks prevail in the society. We then show how the same uncertainty in individual investing preferences that give rise to banks also causes bank runs. Â In the second Â level of our analysis, in a similar fashion, banks are treated as agents and use their own learning process to avoid runs and create an interbank market.\n\nIn addition to providing a bottom up model for the formation of banks and interbank markets, our model allows us to address under what conditions bank monopolies and frequent banks runs are to be observed, or when an interbank market is more likely to be observed instead. It also provides a test bed to investigate several measures of systemic robustness proposed in the literature, as well as the likely response to different policy changes.\n\nPast 2009-2010 Seminars\n\nMONDAY September 21, South Hall 5607F, 3:15-4:45 PM, Refreshments served at 3:00 PM\n\nDr. Tomoyuki Ichiba (PSTAT/CRFMS Postdoctoral Fellow, UCSB)\n\nTitle: Hybrid Atlas Models (Part I)\n\nWe study Atlas type models of equity markets with local characteristics that depend on both name and rank, and in ways that explain a stability of empirical capital distribution. This study involves rankings of continuous semimartingales and the local times of the differences between adjacent processes. By a comparison argument with Bessel processes we derive a representation of the market in terms of the reflected Brownian motion that yields invariant distribution of market capital shares under some assumptions. The class of resulting expected capital distribution curves seem rich enough to explain the empirical curves. We also discuss various portfolios including universal portfolios.\n\nTopics: (sub)martingale problems, rankings of continuous semimartingales, attainability of diffusions, collisions of Brownian particles, reflected Brownian motions, stochastic stability, average occupation time, capital distributions, stochastic portfolio theory.\n\nMONDAY September 28, South Hall 5607F, 3:15-4:45PM, Refreshments served at 3:00 PM\n\nDr. Tomoyuki Ichiba (PSTAT/CRFMS Postdoctoral Fellow, UCSB)\n\nTitle: Hybrid Atlas Models (Part II)\n\nWe study Atlas type models of equity markets with local characteristics that depend on both name and rank, and in ways that explain a stability of empirical capital distribution. This study involves rankings of continuous semimartingales and the local times of the differences between adjacent processes. By a comparison argument with Bessel processes we derive a representation of the market in terms of the reflected Brownian motion that yields invariant distribution of market capital shares under some assumptions. The class of resulting expected capital distribution curves seem rich enough to explain the empirical curves. We also discuss various portfolios including universal portfolios.\n\nTopics: (sub)martingale problems, rankings of continuous semimartingales, attainability of diffusions, collisions of Brownian particles, reflected Brownian motions, stochastic stability, average occupation time, capital distributions, stochastic portfolio theory.\n\nMONDAY October 12, South Hall 5607F, 3:15 PM, Refreshments served at 3:00 PM\n\nAdam Tashman (PSTAT, UCSB)\n\nTitle: Option Pricing Under a Stressed-Beta Model\n\nAbstract: Empirical studies have concluded that stochastic volatility is an important component of option prices. We introduce a regime-switching mechanism into a continuous-time Capital Asset Pricing Model (CAPM) which naturally induces stochastic volatility in the asset price. Under this Stressed-Beta model, the mechanism is relatively simple: the slope coefficient - which measures asset excess returns relative to market excess returns - switches between two values, depending on the market being above or below a given level. After specifying the model, we use it to price European options on the asset. Interestingly, these option prices are given explicitly as integrals with respect to known densities. We find that the model is able to produce a volatility skew, which is a prominent feature in option markets. This opens the possibility of forward-looking calibration of the slope coefficients, using options data, as illustrated in the paper.\n\nJoint work with J.-P. Fouque.\n\nMONDAY October 19, South Hall 5607F, 3:15 PM, Refreshments served at 3:00 PM\n\nWinslow Strong (PSTAT, UCSB)\n\nTitle: Regulation and the Removal of Arbitrage in Strongly Markovian Equity Market Models\n\nAbstract: Recent studies of stochastic models of equity markets have raised new and surprising questions with respect to the importance and realism of the assumption of no arbitrage. E. R. Fernholz has pointed out that we both observe in the data and expect on theoretical grounds that equity markets are diverse in the sense that no single company's capitalization can approach the capitalization of the whole market. The implication of this is that modulo some mild technical conditions in any model where stocks are diffusion processes (and therefore continuous) that any passive (buy-and-hold) portfolio can be outperformed with probability 1 over a prespecified sufficiently long time horizon. In the real world this translates into the existence of portfolios which are statistical arbitrages with respect to the market portfolio. In this talk we lift the assumption of continuity in a very mild way by imposing a form of regulation in the market model whereby market portfolio weights are confined within a region of our choosing by reallocation of money amongst companies upon exit from this region. This allows us to create regulated versions of certain strongly Markovian market models which may have admitted arbitrage in unregulated form but are both diverse and free of arbitrage in their regulated form. We discuss many of the particular models which admit arbitrage and whether or not this procedure is applicable to those models.\n\nMONDAY October 26, South Hall 5607F, 3:15 PM, Refreshments served at 3:00 PM\n\nPeter Van De Zilver (PIMCO, Newport Beach, CA)\n\nTittle: Tracking Error models and implementation\n\nAbstract: We will review the construction of factor models and their use in risk managemen of investment portfolio's. In particular, we will look at the use in Tracking Error calculations and issues related to stability and estimation of the model. We will show an example of an implementation of the linear model with a non-linear extension an use this to analyze portfolio positions and risk concentrations. We will review Black-Litterman implied views and issues related to portfolio optimization.\n\nMONDAY November 16, South Hall 5607F, 3:15 PM, Refreshments served at 3:00 PM\n\nHao Xing (Boston University)\n\nTittle: Strict Local Martingale Deflators and Pricing American Call-Type Options\n\nAbstract: When the discounted stock price is a martingale under the risk neutral measure, it is well know that exercising the American call option at the terminal time is optimal. However, it may not be the case when the discounted stock price is a strict local martingale. In this talk, I will present how to price and optimally exercise American call-type options in markets which do not necessarily admit an equivalent local martingale measure. This resolves an open question proposed by Fernholz and Karatzas [Stochastic Portfolio Theory: A Survey, Handbook of Numerical Analysis, 15:89-168, 2009]. The relationship between the martingale property of diffusion processes and the uniqueness of classical solutions for Cauchy problems will be also discussed.\n\nJoint work with Erhan Bayraktar and Kostas Kardaras.\n\nMONDAY January 25, South Hall 5607F, 3:15 PM, Refreshments served at 3:00 PM\n\nDavid German (Claremont McKenna College)\n\nTittle: Pricing and hedging in an equilibrium-based model for a large investor\n\nAbstract: We study a financial model with a non-trivial price impact effect. In this model we consider the interaction of a large investor trading in an illiquid security, and a market maker who is quoting prices for this security. We assume that the market maker quotes the prices such that by taking the other side of the investor's demand, the market maker will arrive at maturity with maximal expected wealth. Within this model we concentrate on two major issues: evaluation of contingent claims, and hedging.\n\nMONDAY February 22, South Hall 5607F, 3:15 PM, Refreshments served at 3:00 PM\n\nErhan Bayraktar (Univ. of Michigan)\n\nTittle: Optimal Stopping for Dynamic Convex Risk Measures\n\nAbstract: We use martingale and stochastic analysis techniques to study a continuous-time optimal stopping problem, in which the decision maker uses a dynamic convex risk measure to evaluate future rewards. We also find a saddle point for an equivalent zero-sum game of control and stopping, between an agent (the âstopperâ) who chooses the termination time of the game, and an agent (the âcontrollerâ, or ânatureâ) who selects the probability measure.\n\nWEDNESDAY Mar. 3, South Hall 5607F, 3:15 PM, Refreshments served at 3:00 PM\n\nDr. Nizar Touzi (Ecole Polytechnique, France)\n\nTitle: Wellposedness of Second Order Backward SDEs\n\nAbstract: We provide an existence and uniqueness theory for an extension of backward SDEs to the second order. While standard Backward SDEs are naturally connected to semilinear PDEs, our second order extension is connected to fully nonlinear PDEs. In particular, we provide a fully nonlinear extension of the Feynman-Kac formula. We discuss various applications to probabilistic numerical methods, and hedging under market illiquidity.\n\nMONDAY April 26, South Hall 5607F, 3:15 PM, Refreshments served at 3:00 PM\n\nKostas Kardaras (Boston University)\n\nTittle: Numeraire-invariant choices in financial modeling\n\nAbstract: We provide an axiomatic foundation for the representation of numeraire-invariant preferences of agents acting in a financial\n\nmarket. In a static environment, the simple axioms turn out to be equivalent to the following choice rule: the agent prefers one outcome over another if and only if the expected (under the agent's subjective probability) relative rate of return of the latter outcome with respect to the former is nonpositive. With the addition of a transitivity requirement, this last preference relation is extended to expected logarithmic utility maximization. We also discuss the previous in a dynamic environment, where consumption streams are the objects of choice. There, a novel result concerning a canonical representation of optional measures with unit mass enables one to explicitly solve the investment-consumption problem by completely separating the two aspects of investment and consumption. Finally, we give an application to the problem of optimal log-investment with a random-time horizon.\n\nIf time permits, further applications of the representation of optional measures with unit mass in the theory of random times will be\n\ngiven. This last topic is work in progress.\n\nMONDAY May 17, South Hall 5607F, 3:15 PM, Refreshments served at 3:00 PM\n\nKasper Larsen (Carnegie Mellon University)\n\nTittle: Horizon dependence of utility optimizers in incomplete models\n\nAbstract: This paper studies the utility maximization problem with changing time horizons in the incomplete Brownian setting. We first show that the primal value function and the optimal terminal wealth are continuous with respect to the time horizon $T$. Secondly, we exemplify that the expected utility stemming from applying the $T$-horizon optimizer on a shorter time horizon $S$, $S < T$, may not converge as $S\\uparrow T$ to the $T$-horizon value. Finally, we provide necessary and sufficient conditions preventing the existence of this phenomenon. (joint work with Hang Yu)\n\nWEDNESDAY June 2, South Hall 5607F, 3:15 PM, Refreshments served at 3:00 PM\n\nEric Hillebrand (Department of Economics, Louisiana State University)\n\nTittle: TEMPORAL CORRELATION OF DEFAULTS IN SUBPRIME SECURITIZATION\n\nAbstract: The securitization of subprime mortgages in instruments like mortgage-backed securities and collateralized debt obligations is one of the key ingredients to the current financial crisis. During 2007 and 2008, subprime defaults increased sharply, displaying high serial correlation in their arrival. Subprime default events depend on house price changes. We establish a link between the dynamics of house price changes and the dynamics of default rates in the Gaussian copula framework by specifying a time series model for a common risk factor. We show analytically and in simulations that serial correlation translates from the common risk factor to correlation of default rates across vintages of mortgage pools. We simulate prices of mortgage-backed securities, which are securitized from pools of mortgages using a waterfall structure. We find that subsequent vintages of these securities inherit default correlation from the common risk factor. The findings in this paper imply that one can model correlated default arrivals by introducing serial correlation to the common risk factor.\n\nJoint work with Ambar N. Sengupta and Pierre Xu.\n\nPAST 2008-2009 Seminars\n\nMONDAY September 29, South Hall 5607F, 3:15 PM, Refreshments served at 3:00 PM\n\nRobert Gingrich (PIMCO, Newport Beach, CA)\n\nTitle: SYSTEMIC CREDIT RISK: WHAT IS THE MARKET TELLING US? [paper]\n\nAbstract: The ongoing subprime crisis raises many concerns about the possibility of much broader credit shocks in the economy. We use a simple linear version of the Longstaff and Ra jan (2007) model to extract the information about macroeconomic credit risk embedded in the prices of tranches on the most-liquid credit indexes. Three types of credit risk appear to be priced by the market: idiosyncratic risks at the level of individual firms, sectorwide risk at the level of correlated firms within the same industry group, and economywide or systemic risk. We apply the model to the recent behavior of tranches in the U.S. and European credit derivatives markets and show that the current credit crisis has more than twice the systemic risk of the May 2005 auto-downgrade credit crisis.\n\nMONDAY October 13, South Hall 5607F, 3:15 PM, Refreshments served at 3:00 PM\n\nDavid Nualart (Kansas University)\n\nTitle: Fractional Brownian Â motion: Stochastic calculus and applications.\n\nAbstract: The fractional Brownian motion is a centered self-similar Gaussian process with stationary increments, which depends on a parameter H in (0,1) called the Hurst index. In Â this talk we will describe some basic properties of the fractional Brownian motion, and we will Â present a version of the P. Levy characterization theorem. We will analyze different approaches Â to construct a stochastic calculus with respect to the fractional Brownian motion, using path-wise techniques, Riemann sums Â and Malliavin calculus. Some applications Â in mathematical finance will be discussed.\n\nMONDAY October 20, South Hall 5607F, 3:15 PM, Refreshments served at 3:00 PM\n\nIoannis Karatzas (Columbia University)\n\nTitle: VOLATILITY STABILIZATION, DIVERSITY AND ARBITRAGE IN STOCHASTIC FINANCE [slide]\n\nWe'll start this talk with an overview of the modern theory of portfolios, based on Stochastic Analysis. We shall introduce then the notion of relative arbitrage. and provide simple, descriptive and easy-to-test criteria for the existence of such arbitrage in equity markets. These criteria postulate essentially that the excess growth rate of the market portfolio, a positive quantity that can be estimated or even computed from a given market structure, be \"sufficiently large\". We show that conditions satisfying these criteria are manifestly present in the US equity market, and construct explicit portfolios under these conditions. One such condition, market diversity, emerges when the volatility structure is bounded.\n\nWe then construct examples of abstract markets in which the criteria hold. We study in some detail a specific example of a non-diverse abstract market which is volatility-stabilized, in that the return from the market portfolio has constant drift and variance rates, while the smallest stocks are assigned the largest volatilities and individual stocks fluctuate widely. An interesting probabilistic structure emerges in which time changes, Bessel processes, and the asymptotic theory for planar Brownian motion, play crucial roles. Several open questions are raised for further study. (Joint work with E. Robert Fernholz.)\n\nMONDAY October 27, South Hall 5607F, 3:15 PM, Refreshments served at 3:00 PM\n\nRaj Sau (UCSB)\n\nTitle: HEDGING IN MODELS WITH JUMPS AND TRANSACTION COSTS - a survey\n\nAbstract: Transaction costs invalidate Black-Scholes argument, since continuous revision implies infinite trading. Hence it is necessary to rebalance in discrete time steps, which results in hedging error. The hedging strategies can be classified into two main categories, time-based and move-based (utility) strategies. The goal is to minimize the variance of hedging error. If the asset price follows a jump diffusion, the market is in general incomplete. Kennedy, Forsyth Vetzal devise a strategy that simultaneously eliminates diffusion risk, and minimizes an objective that is a linear combination of jump risk and transaction cost. In this survey I review the above hedging methods.\n\nWEDNESDAY, November 5, South Hall 5607F, 3:15 PM, Refreshments served at 3:00 PM\n\nMichael Ludkovski (UCSB)\n\nTitle: Optimal Risk Sharing under Distorted Probabilities\n\nAbstract: We study optimal risk sharing among n agents endowed with distortion risk measures. Risk sharing under third-party constraints is also considered. We obtain an explicit formula for Pareto optimal allocations. In particular, we find that a stop-loss or deductible risk sharing is optimal in the case of two agents and several common distortion functions. This extends recent result of Jouini et al. (2006) to the problem with unbounded risks and market frictions.\n\nIn the first part of my talk I will give a brief survey of distortion risk measures and its relation to other risk preferences. I will then\n\ndiscuss recent research and open problems.\n\nMONDAY January 12, South Hall 5607F, 3:15 PM, Refreshments served at 3:00 PMã\n\nJosh Davis (PIMCO, Newport Beach, CA)\n\nTitle: Robust Bayesian Portfolio Construction\n\nSLIDES\n\nFRIDAY January 23, South Hall 4607, 2PM-3PM\n\nKonstantinos Spiliopoulos (University of Maryland )\n\nTitle: Reaction-Diffusion Equations with Nonlinear Boundary Conditions in Narrow Domains\n\nAbstract: We will consider the second initial boundary problem in narrow domains of width $\\epsilon\\ll 1$ for linear second order differential equations with nonlinear boundary conditions. Using probabilistic methods we show that the solution of such a problem converges as $\\epsilon \\downarrow 0$ to the solution of a standard reaction-diffusion equation in a domain of reduced dimension. This reduction allows to obtain some results concerning wave front propagation in narrow domains. In particular, we describe conditions leading to jumps of the wave front. In addition, an important and interesting problem, which is related to the previous one and will be presented here, is the Wiener process with instantaneous reflection in a narrow tube which, in contrast to before, is assumed to be non-smooth asymptotically.\n\nMONDAY January 26, PUBLIC LECTURE at 4PM, Corwin Pavilion, followed by a reception. [Event Flyer]\n\nArnold Miyamoto (CitiGroup, Managing Director CitiFX)\n\nTitle: Overview on Foreign Exchange and Capital Markets\n\nAbstract: Arnold will present an overview of Foreign Exchange Sales & Trading from its historical beginnings to today's environment. As uantitative strategies are at the center of how Wall Street works, he will also discuss where quants are used and the opportunities. Lastly, he will reflect on the past 1-1/2 years in the financial markets with a view toward what is on the near horizon.\n\nMonday February 9, South Hall 5607F, 3:15 PM, Refreshments served at 3:00 PM\n\nProf. Masaaki Kijima, Graduate School of Economics, Tokyo Metropolitan University\n\nTitle: Risk Management and the Pricing of CDOs Based on the Multivariate Wang Transform\n\nAbstract: We consider a CDO within the CreditMetrics framework for the purpose of risk manegement. We then apply the change of measure formula based on the multivariate Wang transform, which is consistent with Buhlmann's equilibrium pricing model, to evaluate the fair price of the CDO. The pricing formula is an extension of the one-factor Gaussian copula model, the standard market model for valuing CDO's. Unlike the existing models, our model calibrates the parameter associated with risk aversion index of the representative investor, not the correlation parameter. A t-copula model is also considered to describe the fat-tail distribution observed n the actual markets. Some numerical results are presented to show the usefulness of our model.\n\nWEDNESDAY March 4, South Hall 5607F, 3:15 PM, Refreshments served at 3:00 PM\n\nProf. Henry Schellhorn (Claremont Graduate University)\n\nTitle: An Algorithm for the Pricing of Path-Dependent American Options Using Malliavin Calculus\n\nAbstract: We propose a recursive scheme to calculate backward the values of conditional expectations of functions of path values of Brownian motion. This scheme is based on the Clark-Ocone formula in discrete time. We construct an algorithm based on our scheme to efficiently calculate the price of American options on securities with path-dependent payoffs. Our algorithm can be combined with regression-based Monte Carlo methods, like the Longstaff-Schwartz algorithm. In this case, our algorithm remedies the decrease of performance experienced by regression-based methods when the number of basis functions, or regressands, needs to be quite large, because of path-dependence.\n\nMonday April 6, South Hall 5607F, 3:15 PM, Refreshments served at 3:00 PM\n\nMihai Sirbu (UT Austin)\n\nTitle: Asymptotic analysis of utility-based prices and hedging strategies for utilities defined on the whole real line\n\nAbstract: We perform an asymptotic expansion of utility-based prices and hedging strategies for small number of contingent claims, in the framework of optimal investment with general utilities defined on the whole real line. Conceptually, this follows previous joint work with Dmitry Kramkov and relies on using the risk-tolerance wealth processes as the natural numeraire for pricing and hedging. The quantitative analysis generalizes the work of Henderson for the case of exponential utility and basis-risk model and the qualitative part is related to the work of Mania & Schweizer, Becherer and Kallsen & Rheinlander. The presentation is based on work in progress.\n\nMonday April 13, South Hall 5607F, 3:15 PM, Refreshments served at 3:00 PM\n\nSoumik Pal (University of Washington)\n\nTitle: Two models of equity markets and the distribution of market weights\n\nAbstracts: We consider two stochastic models of equity markets as proposed by Fernholz, Karatzas, and coauthors. These are multidimensional processes where each coordinate represents the capital that a company has in the equity market. The market weights refer to the ratio of individual capital over the total capital that is present in the market. The market weight of a company represents the influence that an individual company exerts on the entire financial market. The structure and properties of market weights have long been studied in economists who are fascinated by its peculiar structure and very stable behavior.\n\nFernholz and Karatzas, along with coauthors, propose the 'rank-based models' and the 'volatility-stabilized market models' in order to\n\ncapture an empirically observed fact. The growth rate and volatility of a stock capital depends heavily on its current market weight. In particular, stocks with smaller market weights have higher growth rate and higher fluctuations, while the ones with larger weights are more stable. We analyze the probabilistic behavior of market weights under these models. We show that they are linked to many other areas of probability, including reflected Brownian motions, measure-valued diffusions, and the Poisson-Dirichlet point processes. We also mathematically explain a widely observed fact: a power law decay of ordered market weights in equilibrium.\n\nMonday April 20, South Hall 5607F, 3:15 PM, Refreshments served at 3:00 PM\n\nCarole Bernard (University of Waterloo)\n\nTitle: Â Path-dependent inefficient strategies and how to make them efficient\n\nAbstract: Â We make the following assumptions. (1) Agents' preferences depend only on the probability distribution of terminal wealth. (2) Agents prefer more to less. (3) The market is perfect and frictionless. (4) The market is arbitrage-free and could be incomplete. Under these assumptions, we show that in general path-dependent strategies are inefficient and not optimal. In addition, we characterize the ones that are cost-efficient. We obtain an explicit formula for the efficiency cost of a strategy as well as for the payoff of the cost-efficient derivative that should be preferred by all investors. Finally, we show that in the Black and Scholes framework, the necessary and sufficient conditions for a strategy to be cost-efficient is that its terminal payoff is an increasing function of the stock price.\n\nThis is joint work with Professor Phelim Boyle.\n\nMonday April 27, South Hall 5607F, 3:15 PM, Refreshments served at 3:00 PM\n\nXin Guo (UC Berkeley)\n\nTitle: Connecting singular controls and switching controls, with applications\n\nAbstract: It was well known that a certain class of singular control problems is connected to optimal stopping problems. In this talk, we present a new theoretical connection between singular control of finite variation and optimal switching problems. This correspondence provides a novel method for solving explicitly multi-dimensional singular control problems, and links singular controls and Dynkin games through sequential optimal stopping.\n\nThis is a joint work with P. Tomecek, JP Morgan.\n\nMonday May 4, South Hall 5607F, 3:15 PM, Refreshments served at 3:00 PM\n\nPaolo Guasoni (Boston University)\n\nTitle: Two and Twenty: what Incentives?\n\nAbstract: Hedge fund managers receive a large fraction of their funds' gains, in addition to the small fraction of funds' assets typical of mutual funds. The additional fee is paid only when the fund exceeds its previous maximum - the high-water mark. The most common scheme is 20% of gains, plus 2% of assets.\n\nTo understand the incentives implied by these fees, we solve the portfolio choice problem of a manager with Constant Relative Risk Aversion and a Long Horizon, who maximizes the utility from future fees.\n\nWith constant investment opportunities, and in the absence of fixed fees, the manager's optimal portfolio is constant. It coincides with the portfolio of an investor with a different risk aversion, which depends on the manager's true risk aversion and on the size of the fees. This portfolio is also related to that of an investor with a drawdown constraint. The combination of both fees leads to a more complex solution.\n\nThe model involves a stochastic differential equation involving the running maximum of the solution, which is related to perturbed Brownian Motions. The solution of the control problem employs a verification theorem which relies on asymptotic properties of positive local martingales.\n\nMonday May 18, South Hall 5607F, 3:15 PM, Refreshments served at 3:00 PM\n\nMatheus Grasselli (McMaster University)\n\nTitle: Chaotic Interest Rate Model Calibration\n\nAbstract: The Wiener chaos approach to interest rates was introduced a few years ago by Hughston and Rafailidis as an axiomatic framework to model positive interest rates, continuing a line of research started by the seminal Flesaker and Hughston model and including the elegant potential approach of Rogers and others. Apart from ensuring positivity, one appealing feature of the chaotic approach is its hierarchical way to introduce randomness into a model through different orders of chaos expansions. We propose a systematic way to calibrate Wiener chaos models to market data, and compare the performance of expansions of different orders in the presence of interest rate derivatives of increased complexity. This is joint work with Tsunehiro Tsujimoto.\n\nWEDNESDAY May 20, South Hall 5607F, 3:15 PM, Refreshments served at 3:00 PM\n\nProf. Marco Frittelli (University of Milano, Italy)\n\nTitle: Conditional certainty equivalent and representation of risk measures\n\nAbstract: In the framework of dynamic indifference pricing, we study the conditional version of the classical notion of the certainty equivalent. This concept leads to the investigation of quasi convex maps and their dual representation.\n\nMonday June 1, South Hall 5607F, 4PM [note the unusual time]\n\nYingying Fan (Marshall School of Business, USC)\n\nTitle : Testing and Detecting Jumps Based on a Discretely Observed Process\n\nAbstract: We propose a new nonparametric test for detecting the presence of jumps in asset prices using discretely observed data. Compared with the test statistic in A\\\"{i}t-Sahalia and Jacod (2007), our new test statistic enjoys the same asymptotic properties but has smaller variance. These results are justified both theoretically and numerically. Thanks to the reduction of the variance, we also propose a new test procedure to identify the locations of jumps. The problem of jump identification thus reduces to a multiple comparison problem. We employ the False Discovery Rate (FDR) approach to control the type I error. Simulation studies and real data analysis further demonstrate the power of the newly proposed test method. This is a joint work with Professor Jianqing Fan.\n\nPAST 2007-2008 Seminars\n\nWEDNESDAY October 3, South Hall 5607F, 3:15 PM, Refreshments served at 3:00 PM\n\nStephane Villeneuve (Toulouse, France, visiting PSTAT)\n\nTitle: Optimal dividend policy and growth option\n\nWe analyse the interaction between dividend policy and investment decision in a growth opportunity of a liquidity constrained firm. This leads us to study a mixed singular control/optimal stopping problem for a diffusion that we solve quasi-explicitly establishing connection with an optimal stopping problem. We characterize situations where it is optimal to postpone dividend distribution in order to invest at a subsequent date in the growth opportunity. We show that uncertainty and liquidity shocks have ambiguous effect on the investment decision.\n\nMONDAY October 8, South Hall 5607F, 3:15 PM, Refreshments served at 3:00 PM\n\nMarek Rutkowski (School of Mathematics and Statistics, University of New South Wales)\n\nPDE Approach to Credit Derivatives Paper1 Paper2\n\nWEDNESDAY October 10, South Hall 5607F, 3:15 PM, Refreshments served at 3:00 PM\n\nHelgi Tomasson (University of Iceland, visiting PSTAT)\n\nSome Computational Aspects for Inference on Diffusion Processes\n\nThe theory of diffusion processes is fundamental for modern mathematical-finance. Real data are assumed to be observations of a continuous-time process at discrete time-points. The statistical toolbox for financial data is briefly reviewed. A computer program, written in R, for approximation of the likelihood function for some simple processes is shown. The approximation is based on a Taylor-expansion of the Kolmogorov-forward equation in the spirit of Ait-Sahalia(1999, 2002). Properties of maximum-likelihood estimators are illustrated by simulation. Some aspects of applying the approximation to Bayesian inference and statistical-surveillance (change-point-detection) are discussed\n\nMONDAY October 15, South Hall 5607F, 3:15 PM, Refreshments served at 3:00 PM\n\nBjorn Flesaker (Bloomberg, NY)\n\nTitle: Replication based pricing of default contingent claims slide\n\nThe standard market model for single name credit default swap pricing is usually represented as a pure reduced form model where default occurs as the first jump of a Poisson process with deterministic risk neutral intensity. We provide conditions under which a static portfolio of standard credit default swaps along with a money market account balance can be used to replicate a broad class of default contingent claims and demonstrate that the resulting no-arbitrage values are consistent with the standard market model, regardless of the dynamics of the default generating process. The replication based pricing operator, as well as the associated survival contingent money market account balance and the replicating CDS portfolio positions, are fully characterized in terms of second order ordinary differential equations (for the continuous maturity limit) and difference equations (for discrete holdings), and examples of their explicit solutions are given.\n\nThis is based on joint work with Peter Carr.\n\nWEDNESDAY October 17, South Hall 5607F, 3:15 PM, Refreshments served at 3:00 PM\n\nHyekyung Min (postdoc, PSTAT)\n\nTitle: A Stochastic Control Model of Optimal Dividend and Capital Financing\n\nThe stochastic control model, introduced by Peura and Keppo (2006), is considered for valuing a firm whose capital evolves according to Brownian motion with a drift. The firm controls the flow of capitals not only by paying out the dividends but also by raising the capital in the presence of fixed cost (K) and delay (D). A solution to this control problem is obtained by solving a system of quasi-variational inequalities. It is shown that a unique solution exists for all values of K >= 0 and D > 0. The asymptotic behavior of the optimal dividend and capital issue barriers, and the ruin probability and the expected lifetime of the firm following the optimal policy will be discussed.\n\nMONDAY October 29,South Hall 5607F, 3:15 PM, Refreshments served at 3:00 PM\n\nEric Hillebrand (Economics, LSU)\n\nPricing an Option on Revenue from an Innovation: An Application to Movie Box Office Revenue\n\nWe develop a model for valuing revenue streams from innovations. The stochastic properties of revenue from innovations create a more diffcult environment in which to value options than when the underlying is a security. There is no initial revenue and cumulative revenue cannot decrease. Revenues from innovations are character- ized by different lives and different rates of the resolution of uncertainty. A common deterministic model for predicting revenue from an innovation is due to Bass (1969). We imbed the Bass model in a gamma process, resulting in a stochastic process with moments proportional to the mean of the Bass model. To illustrate this model we choose the valuation of options on movie box oÂ±ce revenue. These options enable film distributors to manage the risk of a movie, and they offer diversification opportunities for investors. We develop the econometric methodology for ex-ante parameter estimation and a Bayesian updating scheme using Markov Chain Monte Carlo simulation as data after release become available. Call prices obtained using MLE parameter estimates from the full data set closely approximate the average discounted value of ex-post call payouts that would have occurred at option maturity.\n\nMonday, November 26, South Hall 5607F, 3:15 PM, Refreshments served at 3:00 PM\n\nMartin Forde (PSTAT, UCSB)\n\nSmall time and tail asymptotics for stochastic volatility models\n\nWe show how to construct a volatility-of-variance function for an uncorrelated stochastic volatility model, so as to be consistent with an observed symmetric small-maturity smile, by solving an Abel Volterra integral equation. We show how to adapt the methodolgy for implied volatility skews. We also discuss Lewis's small-time asymptotics for the derivatives of the implied volatility At the-Money, and the large-x asymptotics for his CEV(p)- volatility model. We also discuss tail asymptotics for such models, using Olver's asymptotics.\n\nMonday, Dec 10, South Hall 5607F, 3:15 PM, Refreshments served at 3:00 PM\n\nHuyÃªn PHAM, UniversitÃ© Paris 7, Denis Diderot.\n\nHedging and pricing with execution delay.\n\nWe consider impulse control problems in finite horizon for diffusions with decision lag and execution delay. The new feature is that our general framework deals with the important case when several consecutive orders may be decided before the effective execution of the first one.\n\nThis is motivated by financial applications in the trading of illiquid assets such as hedge funds.\n\nWe show that the value functions for such control problems satisfy a suitable version of dynamic programming principle in finite dimension, which takes into account the past dependence of state process through the pending orders. The corresponding Bellman partial differential equations (PDE) system is derived, and exhibit some peculiarities on the coupled equations, domains and boundary conditions. We prove a unique characterization of the value functions to this nonstandard PDE system by means of viscosity solutions. We then provide an algorithm to find the value functions and the optimal control. This implementable algorithm involves backward and forward iterations on the domains and the value functions, which appear in turn\n\nas original arguments in the proofs for the boundary conditions and uniqueness results. Finally, we give several numerical experiments illustrating the impact of execution delay on trading strategies and on option pricing.\n\nFriday, January 18th, South Hall 5607F, 3:15PM, refreshments served at 3:00PM\n\nJan Vecer, Department of Statistics, Columbia University\n\nTradeable Measures of Risk\n\nThe main idea of this talk is to introduce Tradeable Measures of Risk as an objective and model independent way of\n\nmeasuring risk. The present methods of risk measurement, such as the standard Value-at-Risk supported by Basel II, are based on subjective assumptions of future returns. In order to achieve an objective measurement of risk, we introduce a concept of Realized Risk which we define as a directly observable function of realized returns. Predictive assessment of the future risk is given by Tradeable Measure of Risk - the price of a contract which pays its holder the Realized Risk for a certain period. Our definition of the Realized Risk payoff includes a Weighted Average of Ordered Returns, with the following special cases: the worst return, the empirical Value-at-Risk, and the empirical mean shortfall. When Tradeable Measures of Risk of this type are priced and quoted by the market (even over-the-counter, or traded internally within a financial institution), one does not need a model to calculate values of a risk measure since it will be observed directly from the market. We use an option pricing approach to obtain dynamic pricing formulas for these contracts, where we make an assumption about the distribution of the returns. We also discuss the connection between Tradeable Measures of Risk and the axiomatic definition of Coherent Measures of Risk, and provide some convergence results.\n\nTuesday, January 22nd, South Hall 5607F, 10:00AM, refreshments served at 9:45am.\n\nMike Ludkovski\n\nOptimal Stopping and Optimal Switching for Hidden Markov Models\n\nWe study optimal stopping and optimal switching problems for hidden Markov chains with Poissonian information structures. In our model, the controller maximizes expected rewards that depend on an unobserved Markovian environment with information collected through a (compound) Poisson observation process. Examples of such systems arise in investment timing, reliability theory, sequential tracking, and economic policy making. We solve the problem by performing Bayesian updates of the posterior likelihoods of the unobservable and studying the resulting optimization problem for a piecewise-deterministic process. We then prove the dynamic programming principle and explicitly characterize an optimal strategy. We also provide an efficient numerical scheme and illustrate our results with several computational examples.\n\nThis is based on joint work with Semih Sezer and Erhan Bayraktar (U of M).\n\nMonday, January 28th, South Hall 5607F, 3:15PM, refreshments served at 3:00PM\n\nTim Siu-Tang Leung (ORFE, Princeton University)\n\nUtility-based Valuation of Employee Stock Options\n\nEmployee stock options (ESOs) have become an integral component of compensation in the U.S. Financial regulations now require firms to expense these options in their accounting statements. ESOs have a number of complicated characteristics which distinguish them from standard market-traded American call options. Their value is much less due to the suboptimal exercising strategies of the holders, which arise from risk aversion, hedging constraints, and job termination risk. We analyze a utility-based valuation procedure that accounts for the combined effect of all of these factors, along with multiple exercising rights and vesting periods. This leads to the numerical study of a system of nonlinear free-boundary problems of reaction-diffusion type. In addition, we examine the holder's hedging strategies that involve a combination of dynamic trading of correlated assets and static positions in market-traded put options. We find that static hedges induce the ESO holder to delay exercises, and lead to higher ESO costs.\n\nMonday February 11, South Hall 5607F, 3:15PM, refreshments served at 3:00PM\n\nAlok Khare (PSTAT, UCSB)\n\nPresent Value Relation and the Volatility Puzzle: A Reexamination\n\nIn the seventies and eighties, martingale model for asset prices was tested using variance bounds. The publications by LeRoy and Porter, and Shiller indicated that prices violated the variance bounds implied by the model. The econometric methods of these early contributions were criticized on the grounds that tests produced biased results. Subsequent research using improved econometric methods produced similar findings. The tests are generally built on the premise that value of a firm is discounted value of dividends paid to shareholders. Miller and Modigliani show that this is true only when firms neither issue nor repurchase shares. In general, the value of a firm is the discounted present value of future net cash flows, which consists of dividends and repurchases net of share issues. We employ the variance bound test derived by West on data using this payout measure. Our result is that prices are not excessively volatile when compared to subsequent total net cash flows to shareholders. The data consist of all firms in the DJIA from 1983 to 2005.\n\nTuesday February 19, 10AM, Sobel room, refreshments at 9:45\n\nTom Hurd (McMaster University, Canada)\n\nThe first passage problem for jump diffusions and its implications for credit risk\n\nWe begin by reviewing some classic problems in finance that boil down to a problem of first passage of an underlying stochastic process. Although jump diffusions are widely used for modeling financial time series, they have been only slowly adopted in\n\nsome areas, like credit risk, perhaps because of the intractable nature of their first passage problem. In this talk I show how this difficulty can be circumvented. Thus freed, we are able to investigate some consequences of adding jumps to structural credit models\n\nfor one firm. As an example of what can be done, a class of credit-equity hybrid models is proposed. At the end of the talk, we will move to multivariate credit models and briefly consider the implications of introducing dynamic default correlations through time change.\n\nMonday February 25, South Hall 5607F, 3:15PM, refreshments served at 3:00PM\n\nArchil Gulisashvili (Ohio University)\n\nDistribution densities in stochastic volatility models\n\nStochastic volatility models were introduced in 1980s-1990s. In these models, the volatility of a stock is described by a stochastic process. For instance, in the Hull-White model, the volatility is a geometric Brownian motion, in the Stein-Stein model, the absolute value of an Ornstein-Uhlenbeck process plays the role of the volatility of a stock, and in the Heston model, the volatility is a square root process. The main object of our interest in this work is the distribution density of the stock price in a stochastic volatility model. We find explicit formulas for leading terms in asymptotic expansions of such densities and give error estimates. Using these results, we compare ``fat tails\" of stock price distributions in various stochastic volatility models. We also obtain a sharp asymptotic formula for the law of a mean-square average of the volatility process. As an application of our methods, the asymptotic behavior of the implied volatility is characterized, and a sharp asymptotic formula for the price of an Asian option is obtained.\n\nThis is a joint work with E. M. Stein (Princeton University).\n\nMonday March 3, South Hall 5607F, 3:15PM, refreshments served at 3:00PM\n\nAdam Tashman (Ivy Asset Management, New York, NY)\n\nModeling Risk in Arbitrage Strategies Using Finite Mixtures\n\nArbitrage strategies produce stable, modest returns punctuated by intervals of dramatically poor performance. The weakness stems from an oversight in modeling: seemingly independent bets infrequently become highly correlated to market variables. Thus, the risk in arbitrage strategies is systematically underestimated, and hedging is not properly implemented. This paper illustrates the fitting and application of a mixture model to a series of hedge fund index returns, for the purpose of more effectively hedging downside risk. The model captures the regime-switching nature of the process in a general setting, free from the assumption of a linear relationship between explanatory and response variables. A logistic regression function is used to predict the acting regime, and linear regression functions relate explanatory variables to the expected hedge fund return in each regime. The covariates considered are stock market returns, volatility of the stock market, the slope of the US swap curve, and credit spreads. The dependent variable under investigation is the HFRI Merger Arbitrage Index. The model is applied in a novel hedging strategy, termed mixture hedging. The strategy is back tested over the period 1990-2005, and its performance is compared against the prevalent beta-neutral hedging strategy. The merger arbitrage index exhibited strong evidence of a regime-switching process, and the proposed model offered an improved fit relative to standard regression techniques. Mixture hedging was more effective at reducing downside risk than beta-neutral hedging. Maximum drawdown was 5.50% for the mixture strategy, versus 5.98% for beta-neutral hedging and 6.46% for an unhedged portfolio. The improvement will be more pronounced if the portfolio is levered.\n\nMonday, March 10, South Hall 5607F, 3:15PM, refreshments served at 3:00PM\n\nMike Landrigan (Rimrock Capital, San Juan Capistrano, CA).\n\nPortfolio Optimization for Valuing Collateralized Mortgage Obligations\n\nCollateralized Mortgage Obligations (CMO) can have a high degree of variability in cash flows and a complex embedded optionality structure. Because of this, it is generally recognized that a yield to maturity or static spread calculation is not a suitable valuation methodology. To assess the value of a CMO it is common to rely on benchmark-calibrated Monte Carlo simulation of interest rates and projection of cash flows along the various interest rate paths. The often quoted measure of value derived from such simulation is the Option-Adjusted Spread (OAS). We find that OAS has serious flaws and introduce a variance reducing portfolio optimization technique to value CMO. The optimization is applied to liquid, benchmark interest-rate derivatives. In our talk we will introduce CMOs and OAS, present the general framework of our analysis, and discuss results from numerical experiments.\n\nMonday April 7, South Hall 5607F, 3:15PM, refreshments served at 3:00PM\n\nEckhard Platen (University of Technology Sydney, Australia)\n\nThe Law of the Minimal Price\n\nThe paper introduces a general market setting under which the Law of One Price does no longer hold. Instead the Law of the Minimal Price will be derived, which for a range of contingent claims provides lower prices than suggested under the currently prevailing approach. This new law only requires the existence of the numeraire portfolio, which turns out to be the portfolio that maximizes expected logarithmic utility. In several ways the numeraire portfolio cannot be outperformed by any nonnegative portfolio. The new Law of the Minimal Price leads directly to the real world pricing formula, which uses the numeraire portfolio as numeraire and the real world probability measure as pricing measure when computing conditional expectations. The pricing and hedging of extreme maturity bonds illustrates that the price of a zero coupon bond, when obtained under the Law of the Minimal Price, can be far less expensive than when calculated under the risk neutral approach.\n\nMonday April 21, South Hall 5607F, 3:15PM, refreshments served at 3:00PM\n\nSebastian Jaimungal (University of Toronto)\n\nHitting Time Problems with Applications to Finance and Insurance\n\nThe distribution of the first hitting time of a Brownian motion to a linear boundary is well known. However, if the boundary is no longer linear, this distribution is not in general identifiable. Nonetheless, the boundary and distribution satisfy a variety of beautiful integral equations due to Peskir. In this talk, I will discuss how to generalize those equations and lead to an interesting partial solution to the inverse problem: âGiven a distribution of hitting times, what is the corresponding boundary?â By randomizing the starting point of the Brownian motion, I will show how a kernel estimator of the distribution with gamma kernels can be exactly replicated.\n\nArmed with these tools, there are two natural applications: one to finance and one to insurance. In the financial context, the Brownian motion may drive the value of a firm and through a structural modeling approach I will show how CDS spread curves can be matched. In the insurance context, suppose an individualâs health reduces by one unit per annum with fluctuations induced by a Brownian motion and once their health hits zero the individual dies. I will show how life-table data can be nicely explained by this model and illustrate how to perturb the distribution for pricing purposes.\n\nThis is joint work with Alex Kreinin and Angelo Valov.\n\nMonday, May 5 at McCune Conference Room HSSB 6020 at 3:15pm\n\nFilm screening: Wolfgang Doeblin a mathematician rediscovered.\n\nMonday June 9, South Hall 5607F, 3:15PM, refreshments served at 3:00PM\n\nBernt Oksendal (Oslo, Norway)\n"
    }
}
{
    "id": "dbpedia_5772_3",
    "rank": 78,
    "data": {
        "url": "https://arxiv.org/html/2402.03509v1",
        "read_more_link": "",
        "language": "en",
        "title": "Evaluating the Factuality of Zero-shot Summarizers Across Varied Domains",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://arxiv.org/html/x1.png",
            "https://arxiv.org/html/x2.png",
            "https://arxiv.org/html/x3.png",
            "https://arxiv.org/html/x4.png",
            "https://arxiv.org/html/x5.png",
            "https://arxiv.org/html/x6.png",
            "https://arxiv.org/html/x7.png",
            "https://arxiv.org/html/x8.png",
            "https://arxiv.org/html/x9.png",
            "https://arxiv.org/html/x10.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "License: CC BY 4.0\n\narXiv:2402.03509v1 [cs.CL] 05 Feb 2024\n\nEvaluating the Factuality of Zero-shot Summarizers\n\nAcross Varied Domains\n\nSanjana Ramprasad♢♢{}^{\\diamondsuit}start_FLOATSUPERSCRIPT ♢ end_FLOATSUPERSCRIPT Kundan Krishna♣♣{}^{\\clubsuit}start_FLOATSUPERSCRIPT ♣ end_FLOATSUPERSCRIPT Zachary C. Lipton♣normal-♣{}^{\\clubsuit}start_FLOATSUPERSCRIPT ♣ end_FLOATSUPERSCRIPT Byron C. Wallace♢normal-♢{}^{\\diamondsuit}start_FLOATSUPERSCRIPT ♢ end_FLOATSUPERSCRIPT\n\n♢♢{}^{\\diamondsuit}start_FLOATSUPERSCRIPT ♢ end_FLOATSUPERSCRIPTNortheastern University\n\n♣♣{}^{\\clubsuit}start_FLOATSUPERSCRIPT ♣ end_FLOATSUPERSCRIPT Carnegie Mellon University\n\n{ramprasad.sa,b.wallace}@northeastern.edu\n\n{kundank,zlipton}@andrew.cmu.edu\n\nAbstract\n\nRecent work has shown that large language models (LLMs) are capable of generating summaries zero-shot (i.e., without explicit supervision) that are often comparable or even preferred to manually composed reference summaries. However, this prior work has focussed almost exclusively on evaluating news article summarization. How do zero-shot summarizers perform in other, potentially more specialized, domains? In this work we evaluate zero-shot generated summaries across specialized domains including: biomedical articles, and legal bills (in addition to standard news benchmarks, for reference). We focus especially on the factuality of outputs. We acquire annotations from domain experts to identify inconsistencies in summaries and systematically categorize these errors. We analyze whether the prevalence of a given domain in the pretraining corpus affects extractiveness and faithfulness of generated summaries of articles in this domain. We release all collected annotations to facilitate additional research toward measuring and realizing factually accurate summarization, beyond news articles.\n\nEvaluating the Factuality of Zero-shot Summarizers\n\nAcross Varied Domains\n\nSanjana Ramprasad♢normal-♢{}^{\\diamondsuit}start_FLOATSUPERSCRIPT ♢ end_FLOATSUPERSCRIPT Kundan Krishna♣normal-♣{}^{\\clubsuit}start_FLOATSUPERSCRIPT ♣ end_FLOATSUPERSCRIPT Zachary C. Lipton♣normal-♣{}^{\\clubsuit}start_FLOATSUPERSCRIPT ♣ end_FLOATSUPERSCRIPT Byron C. Wallace♢normal-♢{}^{\\diamondsuit}start_FLOATSUPERSCRIPT ♢ end_FLOATSUPERSCRIPT ♢♢{}^{\\diamondsuit}start_FLOATSUPERSCRIPT ♢ end_FLOATSUPERSCRIPTNortheastern University ♣♣{}^{\\clubsuit}start_FLOATSUPERSCRIPT ♣ end_FLOATSUPERSCRIPT Carnegie Mellon University {ramprasad.sa,b.wallace}@northeastern.edu {kundank,zlipton}@andrew.cmu.edu\n\n1 Introduction\n\nModern LLMs now offer strong zero-shot summarization performance, and even surpass fine-tuned models according to human assessments Goyal et al. (2022). Indeed, zero-shot summaries are sometimes deemed comparable in quality to reference summaries Zhang et al. (2023). Past evaluative work, however, has focused nearly exclusively on news article summarization, a domain in which there is no shortage of available training data.\n\nBut zero-shot summarization is perhaps most appealing in niche domains where acquiring training data with which to fine-tune summarization models is sparse and may be prohibitively expensive to collect. Recent work Shaib et al. (2023); Tang et al. (2023) suggests the promise of zero-shot summarization in such domains. However, there has not yet been a comprehensive investigation of the factuality of model outputs produced in zero-shot summarization across multiple domains (i.e., beyond news). Here we address this gap, and compare the quality of zero-shot summaries generated in niche domains (law, medicine) to those generated for news articles.\n\nIn evaluating these models, we center the consistency and faithfulness of summaries generated by LLMs with respect to the input (source) document. Inconsistencies within summaries have long posed a challenge (Maynez et al., 2020; Pagnoni et al., 2021), motivating approaches intended to mitigate this issue (Zhu et al., 2020; Cao and Wang, 2021), and for automated evaluation of factuality (Kryściński et al., 2019; Goyal and Durrett, 2020; Fabbri et al., 2021; Scialom et al., 2021; Laban et al., 2022; Luo et al., 2023). Here we systematically assess the factual accuracy of zero-shot summarizers across a diverse set of specialized domains.\n\nSpecifically, we look to answer four major questions. (1) What is the prevalence of errors in zero-shot summaries across various domains, and how does this compare to established results on news summarization tasks? (2) Are the types of errors observed in these niche domains different from what has been seen in news article summarization? (3) What is the relationship between the frequency of domains in training corpora and the likelihood of model hallucinations in these domains? (4) Are existing automatic systems for factual evaluation reliable across multiple domains?\n\nTo answer these questions, we enlist expert annotators to manually evaluate the outputs from two representative zero-shot summarization systems—GPT-3.5 (gpt-3.5-turbo-0301; Brown et al. 2020) and Flan-T5-XL (Chung et al., 2022)—across standard and niche summarization datasets. Specifically, we evaluate (zero-shot) summaries of medical and legal documents, as well as news articles for reference.\n\nIn general, we find that the proportion of factual inconsistencies in summaries varies considerably across domains, calling into question the community focus on news summarization datasets specifically. Further, we find evidence that the prevalence of articles in pretraining data from a given domain may correlate with the factuality of summaries of articles from the same. We speculate that this may be due to the model introducing content implicit in its weights in such cases (whereas it may have less “knowledge” in niche domains), although this would need to be validated in future work.\n\n2 Manual Evaluations of Summaries\n\nData\n\nWe use XSUM (Narayan et al., 2018) and CNN-DM (Hermann et al., 2015) for news, as well as niche domains like PubMed (medicine; Cohan et al. 2018) and legal bills (law; Kornilova and Eidelman 2019) for comparison. We select articles shorter than 4096 tokens from the test sets to accommodate model token limitations, resulting in approximately 22,000 articles for news, 3,000 for billsum, and 200 for PubMed. We randomly (i.i.d.) sample 50 articles from each domain. We provide more data statistics in Appendix A.1\n\nModel Details\n\nWe run experiments with GPT-3.5 (gpt-3.5-turbo-0301) and Flan-T5-XL Chung et al. (2022). We use a general prompt similar to prior work (Goyal et al., 2022) for generating summaries across domains. Specifically, the prompt is as follows: \"Article: [article]. Summarize the above article.\"\n\nAnnotation Collection\n\nTo acquire manual assessments of model-generated summaries, we hire domain experts via Upwork. We recruit two experts for each domain: linguistics experts for news, attorneys in civil litigation and public policy for the legal domain, and medical doctors (MDs) for the medical domain.\n\nOur evaluation consists of two rounds. In the first round, annotators primarily assess the factual consistency of summaries in relation to the source article. We collect sentence-level annotations, instructing annotators to identify sentences with inconsistencies. The average proportion of such sentences in each domain is a key reported result. The inter-annotator agreement at the summary level was determined by calculating the fraction of instances where both annotators identified a summary as inconsistent with respect to the source. The agreement values are 0.80, 0.72, and 0.85 for news, billsum, and PubMed, respectively. We provide more details about annotation, including agreement statistics, in the Appendix A.2\n\nIn the second round of annotations, we categorize errors based on typology previously introduced (Tang et al., 2022). These errors include: (a) Intrinsic errors, which misrepresent source content, and (b) Extrinsic errors, or “hallucinations”, which introduce terms or concepts not in the source. Past research (Cao et al., 2021) has shown that hallucinations can align with real-world knowledge and even be beneficial.\n\nTo distinguish extrinsic errors further, we sub-categorize them into: Extrinsic nonfactual errors, which are hallucinations inconsistent with world knowledge; and Extrinsic factual errors, where hallucinations align with world knowledge. Additionally, considering that LLMs are trained on data up to specific points in time, we introduce Extrinsic factual outdated errors, which capture hallucinations that are outdated but were once in alignment with world knowledge (e.g., former presidents of countries). To assess the factual nature of hallucinations, annotators use online resources like Google Search and Wikipedia, in keeping with prior work (Cao et al., 2021).\n\n3 Results\n\nHow prevalent are errors across domains?\n\nFigure 0(a) shows the average proportion of sentences marked as inconsistent (with respect to the corresponding input) in summaries generated by GPT-3.5 Brown et al. (2020) and Flan-T5 XL Chung et al. (2022) for three domains: News, medical, and legal. Perhaps surprisingly, we observe a higher prevalence of inconsistencies for news articles, as compared to the specialized domains of medicine and law. While Flan-T5 introduces more errors than GPT-3.5 overall, the trends are analogous.\n\nError categories across domains\n\nWe next characterize the distribution of error categories in factually inconsistent summaries generated by models across the domains considererd. Figure 0(b) reports the distribution of error categories for both models. There are more extrinsic errors introduced in the news domain compared to the niche domain datasets. We include “mixed” errors for cases where errors were classified as different types (intrinsic/extrinsic) by annotators. The news domain has a higher frequency of such cases. Reviewing these, we find that they include cases where the summary both misinterprets source information and where it introduces new information. We provide examples in Appendix A.5.\n\nAn “other” option is available to annotators, along with a comment box for capturing miscellaneous errors. Annotator comments highlight instances where there is no clear misunderstanding but instead a misleading overall impression, such as the over-generalization of specific information in the summary\n\nHow extractive are summaries, and how does this relate to factuality?\n\nWe investigate the relationship between extractiveness (i.e., degree of copying) and factual accuracy across domains. Specifically, we take the proportion of 3-gram sequences in the summary that are also present in the source for each source-summary pair as a proxy measure for extractiveness.\n\nFigure 2 reveals that there is a comparable level of copying across different models and domains. However, models tend to copy more often when summarizing articles in the PubMed dataset; this could explain the lower frequency of errors in this domain, since extractive summaries are unlikely to “hallucinate” by definition. We calculated Spearman rank correlations between 3-gram overlaps and factuality scores for article-summary pairs. The correlations for the news, billsum, and PubMed domains are 0.61, 0.38, and 0.16 respectively.\n\nDomain representation in pretraining corpora and its relation to factuality.\n\nOne possible explanation for the higher proportion of factual errors in news datasets compared to specialized domains is that general news has greater representation in the training data. As a proxy to measure model exposure to articles belonging to these domains we prompt LLMs to generate overviews of articles based on titles only (headlines for news articles, bill titles for billsum, and study titles for PubMed).\n\nWe use the template “Generate a comprehensive overview of the following topic: [title]” to generate text for each article title, assessing LLMs’ memorization. We speculate that increased exposure to an article topic in training data should enable LLMs to reproduce more content present in the original article (as seen with popular celebrities/events, for instance). We assess information overlap between the generated text and original article using ROUGE-L recall, favoring it over embedding based metrics because it emphasizes longest common subsequences based on exact word matches, which makes it suitable for measuring memorization. This is also preferable for content containing specialized terminology like PubMed abstracts and legal articles.\n\nFigure 3 shows that GPT-3.5 and Flan-T5-XL have higher ROUGE-L recall scores for news, suggesting that these models have had more exposure to news topics; this could explain the increased extrinsic error rate in news summaries. Furthermore, in Appendix A.7, we show similar trends using an alternative approach to measure domain representation by directly querying the pretraining corpus with article titles, and using the number of retrieved articles as a proxy for representation.\n\nAre existing automatic systems for factual evaluation reliable across different domains? Prior research has focused on creating automated metrics for evaluating factuality of generated summaries using question answering (Scialom et al., 2021; Fabbri et al., 2021), natural language inference (NLI; Laban et al. 2022), dependency entailment(Goyal and Durrett, 2020), and classification methods (Kryściński et al., 2019). The performance of these metrics has been assessed almost exclusively on evaluation benchmarks comprising model-generated summaries annotated for factuality in the news domain (Kryściński et al., 2019; Wang et al., 2020; Huang et al., 2020; Maynez et al., 2020; Pagnoni et al., 2021; Cao and Wang, 2021; Goyal and Durrett, 2021; Cao et al., 2022). The effectiveness of such automated factuality metrics outside of news is underexplored.\n\nTo address this, we use our annotated dataset to examine the performance of QAFactEval (Fabbri et al., 2021), QuestEval (Scialom et al., 2021) and SummaC variations (Laban et al., 2022) across all three domains. The results in Table 1 reveal that automated metrics struggle when applied to niche domains. We note that the lower scores observed for PubMed could be due to the scarcity of observed errors in this dataset, which makes it challenging to reliably evaluate its performance.\n\n4 Conclusions\n\nWe analyzed zero-shot summarization abilities of two LLMs, focusing on factuality. Surprisingly, inaccuracies were more likely to be introduced in summaries of news articles compared to legal and biomedical domains. Specifically, in this domain we observed more extrinsic errors—i.e., hallucinations of content not mentioned in the source—whereas errors in specialized domains were typically related to an apparent “misunderstanding” of concepts in the source.\n\nWe hypothesize that the discrepancy could result from a higher proportion of news articles in the model’s pretraining data, supported by preliminary evidence. Additionally, we observed lower Spearman rank correlations between automated metrics and human annotations in specialized domains compared to news articles, highlighting the necessity for manual evaluations or the development of new metrics for diverse benchmarks.\n\nLimitations\n\nWe evaluated only two (representative) LLMs; it is possible that other models would show different patterns in behaviour. Another limitation of this work is that we used only a single prompt to generate summaries; although similar to a previously evaluated prompt (Goyal et al., 2022) it is unclear how choice of prompt might interact with factuality of outputs across domains.\n\nReferences\n\nBrown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901.\n\nCao et al. (2022) Meng Cao, Yue Dong, and Jackie Cheung. 2022. Hallucinated but factual! inspecting the factuality of hallucinations in abstractive summarization. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3340–3354, Dublin, Ireland. Association for Computational Linguistics.\n\nCao et al. (2021) Meng Cao, Yue Dong, and Jackie Chi Kit Cheung. 2021. Hallucinated but factual! inspecting the factuality of hallucinations in abstractive summarization. arXiv preprint arXiv:2109.09784.\n\nCao and Wang (2021) Shuyang Cao and Lu Wang. 2021. Cliff: Contrastive learning for improving faithfulness and factuality in abstractive summarization. arXiv preprint arXiv:2109.09209.\n\nChung et al. (2022) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416.\n\nCohan et al. (2018) Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian. 2018. A discourse-aware attention model for abstractive summarization of long documents.\n\nFabbri et al. (2021) Alexander R Fabbri, Chien-Sheng Wu, Wenhao Liu, and Caiming Xiong. 2021. Qafacteval: Improved qa-based factual consistency evaluation for summarization. arXiv preprint arXiv:2112.08542.\n\nGoyal and Durrett (2020) Tanya Goyal and Greg Durrett. 2020. Evaluating factuality in generation with dependency-level entailment. arXiv preprint arXiv:2010.05478.\n\nGoyal and Durrett (2021) Tanya Goyal and Greg Durrett. 2021. Annotating and modeling fine-grained factuality in summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1449–1462, Online. Association for Computational Linguistics.\n\nGoyal et al. (2022) Tanya Goyal, Junyi Jessy Li, and Greg Durrett. 2022. News summarization and evaluation in the era of gpt-3. arXiv preprint arXiv:2209.12356.\n\nHermann et al. (2015) Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. Advances in neural information processing systems, 28.\n\nHuang et al. (2020) Dandan Huang, Leyang Cui, Sen Yang, Guangsheng Bao, Kun Wang, Jun Xie, and Yue Zhang. 2020. What have we achieved on text summarization? arXiv preprint arXiv:2010.04529.\n\nKornilova and Eidelman (2019) Anastassia Kornilova and Vlad Eidelman. 2019. Billsum: A corpus for automatic summarization of us legislation.\n\nKryściński et al. (2019) Wojciech Kryściński, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Evaluating the factual consistency of abstractive text summarization. arXiv preprint arXiv:1910.12840.\n\nLaban et al. (2022) Philippe Laban, Tobias Schnabel, Paul N Bennett, and Marti A Hearst. 2022. Summac: Re-visiting nli-based models for inconsistency detection in summarization. Transactions of the Association for Computational Linguistics, 10:163–177.\n\nLuo et al. (2023) Zheheng Luo, Qianqian Xie, and Sophia Ananiadou. 2023. Chatgpt as a factual inconsistency evaluator for text summarization.\n\nMaynez et al. (2020) Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and factuality in abstractive summarization. arXiv preprint arXiv:2005.00661.\n\nNarayan et al. (2018) Shashi Narayan, Shay B Cohen, and Mirella Lapata. 2018. Don’t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. arXiv preprint arXiv:1808.08745.\n\nPagnoni et al. (2021) Artidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021. Understanding factuality in abstractive summarization with frank: A benchmark for factuality metrics. arXiv preprint arXiv:2104.13346.\n\nRaffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485–5551.\n\nScialom et al. (2021) Thomas Scialom, Paul-Alexis Dray, Patrick Gallinari, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano, and Alex Wang. 2021. Questeval: Summarization asks for fact-based evaluation. arXiv preprint arXiv:2103.12693.\n\nShaib et al. (2023) Chantal Shaib, Millicent L Li, Sebastian Joseph, Iain J Marshall, Junyi Jessy Li, and Byron C Wallace. 2023. Summarizing, simplifying, and synthesizing medical evidence using gpt-3 (with varying success). arXiv preprint arXiv:2305.06299.\n\nTang et al. (2022) Liyan Tang, Tanya Goyal, Alexander R Fabbri, Philippe Laban, Jiacheng Xu, Semih Yavuz, Wojciech Kryściński, Justin F Rousseau, and Greg Durrett. 2022. Understanding factual errors in summarization: Errors, summarizers, datasets, error detectors. arXiv preprint arXiv:2205.12854.\n\nTang et al. (2023) Liyan Tang, Zhaoyi Sun, Betina Idnay, Jordan G Nestor, Ali Soroush, Pierre A Elias, Ziyang Xu, Ying Ding, Greg Durrett, Justin Rousseau, et al. 2023. Evaluating large language models on medical evidence summarization. medRxiv, pages 2023–04.\n\nWang et al. (2020) Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020. Asking and answering questions to evaluate the factual consistency of summaries. arXiv preprint arXiv:2004.04228.\n\nZhang et al. (2023) Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori B Hashimoto. 2023. Benchmarking large language models for news summarization. arXiv preprint arXiv:2301.13848.\n\nZhu et al. (2020) Chenguang Zhu, William Hinthorn, Ruochen Xu, Qingkai Zeng, Michael Zeng, Xuedong Huang, and Meng Jiang. 2020. Enhancing factual consistency of abstractive summarization. arXiv preprint arXiv:2003.08612.\n\nAppendix A Appendix\n\nA.1 Data Statistics\n\nThis section presents additional data statistics in Table 2, including the average number of sentences in both summaries and source articles across various domains, offering context for comparisons.\n\nA.2 Annotation Details\n\nWe recruited annotators on the Upwork platform and selected two domain experts for each task. In the first round, annotators identified sentences in the summary that were inconsistent with the source. The agreement at the summary level includes all cases where both annotators marked at least one sentence in the summary as inconsistent. At the sentence level, we calculated agreement as a function of the fraction of instances in which annotators marked the same sentence within a summary as being inconsistent with the source. We calculate agreement for the error categories by considering the pre-defined error types chosen by each annotator. Notably the datasets, particularly pubmed, has an imbalance due to the dataset’s significant skew in error labels, resulting in a higher expected chance agreement and lower Cohen’s kappa scores. Therefore, we provide the average inter-annotator agreement and Cohen’s kappa scores in the table 3\n\nA.3 Inconsistent summary annotation\n\nIn the first annotation round we asked annotators to mark sentences with unsupported information, i.e., any information not explicitly found in the source, and which could not readily be inferred from the source alone. An example is shown in figure 3(a)\n\nA.4 Error category annotation\n\nIn the second round of annotation, we asked annotators to categorize errors identified in the first round. The options provided are shown in Figure 3(b). We map the options to categories as follows\n\n(a) terms or concepts from the source are misrepresented are mapped to intrinisc errors\n\n(b) The information in the summary is not found in the source but can be verified via an internet search as accurate is mapped to extrinsic (factual) errors\n\n(c) The information in the summary is not found in the source and can be verified via an internet search as being accurate at a previous time but is outdated is mapped to extrinsic(factual, outdated) and\n\n(d) The information in the summary is not found in the source and can not be verified via an internet search is mapped to extrinsic(nonfactual)\n\n3) Other with a mandatory comment.\n\nAn example of this round is displayed in Figure 3(b)\n\nA.5 Mixed errors\n\nWe highlight some examples of the mixed error category annotations in Figure 5\n\nA.6 Error categories per model\n\nIn Figure 6, we present error category distributions for the Flan-T5 and GPT-3.5 models separately. Specifically, for the Flan-T5 model in the news domain, errors are typically categorized as \"mixed\" or marked as intrinsic and extrinsic errors, with no instances labeled as \"other.\" For both models, the trend shows that intrinsic errors in specialized domains are equal to or higher than those in the news domain.\n\nA.7 Alternative method for domain representation\n\nAs an alternative method for evaluating domain representation and its relation to factuality, we use the C4 dataset to query article titles. C4 is a large dataset derived from the the Common Crawl web corpus. It was used to train the T5 Transformer models (Raffel et al., 2020). The number of relevant articles found for each title serves as a proxy for article representation in the training data. We use a C4 search tool to query the C4 dataset. Queries for each article are manually designed using key terms from the article title with the “AND” condition.\n\nFigure 7 demonstrates that queries for news domain retrieved more articles in the C4 dataset compared to Billsum and Pubmed articles.\n\nA.8 Model Details\n\nWe use the default decoding parameters to generate text from GPT-3.5 and Flan-T5-XL. We use the Huggingface Transformers library to implement Flan-T5-XL."
    }
}
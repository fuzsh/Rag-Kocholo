{
    "id": "dbpedia_7466_2",
    "rank": 67,
    "data": {
        "url": "https://arxiv.org/abs/2312.00923",
        "read_more_link": "",
        "language": "en",
        "title": "[2312.00923] Label Delay in Online Continual Learning",
        "top_image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png",
        "meta_img": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png",
        "images": [
            "https://arxiv.org/static/browse/0.3.4/images/icons/close-slider.png",
            "https://arxiv.org/static/browse/0.3.4/images/icons/smileybones-infinity-large.png",
            "https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg",
            "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg",
            "https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg",
            "https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg",
            "https://arxiv.org/icons/licenses/zero-1.0.png",
            "https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png",
            "https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Ser-Nam"
        ],
        "publish_date": null,
        "summary": "",
        "meta_description": "Online continual learning, the process of training models on streaming data, has gained increasing attention in recent years. However, a critical aspect often overlooked is the label delay, where new data may not be labeled due to slow and costly annotation processes. We introduce a new continual learning framework with explicit modeling of the label delay between data and label streams over time steps. In each step, the framework reveals both unlabeled data from the current time step $t$ and labels delayed with $d$ steps, from the time step $t-d$. In our extensive experiments amounting to 1060 GPU days, we show that merely augmenting the computational resources is insufficient to tackle this challenge. Our findings underline a notable performance decline when solely relying on labeled data when the label delay becomes significant. More surprisingly, when using state-of-the-art SSL and TTA techniques to utilize the newer, unlabeled data, they fail to surpass the performance of a na√Øve method that simply trains on the delayed supervised stream. To this end, we introduce a simple, efficient baseline that rehearses from the labeled memory samples that are most similar to the new unlabeled samples. This method bridges the accuracy gap caused by label delay without significantly increasing computational complexity. We show experimentally that our method is the least affected by the label delay factor and in some cases successfully recovers the accuracy of the non-delayed counterpart. We conduct various ablations and sensitivity experiments, demonstrating the effectiveness of our approach.",
        "meta_lang": "en",
        "meta_favicon": "/static/browse/0.3.4/images/icons/apple-touch-icon.png",
        "meta_site_name": "arXiv.org",
        "canonical_link": "https://arxiv.org/abs/2312.00923",
        "text": "arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them."
    }
}
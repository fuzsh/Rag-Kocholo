{
    "id": "correct_foundationPlace_00063_0",
    "rank": 53,
    "data": {
        "url": "https://f1000research.com/articles/5-2000",
        "read_more_link": "",
        "language": "en",
        "title": "Top 10 metrics for life science software good practices",
        "top_image": "https://f1000research.com/img/share/og_research.png",
        "meta_img": "https://f1000research.com/img/share/og_research.png",
        "images": [
            "https://www.facebook.com/tr?id=1641728616063202&noscript=1&ev=PixelInitialized",
            "https://f1000research.com/img/research/F1000Research_white_solid.svg",
            "https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_horizontal.svg",
            "https://f1000research.s3.amazonaws.com/thumbnails/020d477b-be35-49cb-ad8f-05b8c92dfa63_collection.thumbnail",
            "https://f1000research.s3.amazonaws.com/thumbnails/8a3589e5-e69f-46ed-9694-91fc4c5a34f9_collection.thumbnail",
            "https://f1000research.s3.amazonaws.com/thumbnails/38234efc-b5b1-45f1-84b5-5f0cc38a2a2b_collection.thumbnail",
            "https://f1000research.com/img/icon/interactive_content.png",
            "https://f1000research.com/img/icon/interactive_content.png",
            "https://f1000research.com/img/research/F1000Research_white.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Chue Hong",
            "Rob W.W",
            "Rafael C",
            "Brett G",
            "Svobodová Vařeková",
            "Van Parys",
            "Haydee Artaza",
            "Neil Chue Hong",
            "Manuel Corpas",
            "Angel Corpuz"
        ],
        "publish_date": "2016-08-16T00:00:00",
        "summary": "",
        "meta_description": "Read the original article in full on F1000Research: Top 10 metrics for life science software good practices",
        "meta_lang": "",
        "meta_favicon": "/img/favicon-research.ico",
        "meta_site_name": "",
        "canonical_link": "https://f1000research.com/articles/5-2000",
        "text": "Introduction\n\nCompliance with and promotion of good development practice is a powerful mechanism for promoting software sustainability. Using metrics to judge good practice can enhance research software maintainability and helps establish a baseline of quality, reusability and reproducibility. Software development metrics, however, are only useful if it is clear what they measure. This could be a) the application of agreed good practice in a piece of software or software team, or b) how sustainable the software is in the long term. There have been previous attempts to assess good practices for scientific computing1 but they did not specifically tackle the question how to measure them during software development. As part of a collaboration between the ELIXIR pan-European research infrastructure for life science data and the Software Sustainability Institute, a working group met at Schiphol airport (Amsterdam) on December 14–15th 2015 to a) define and select the metrics that reflect the application of good practices, b) discuss the collection of these metrics and c) establish how the metrics could be implemented to ensure their wide adoption. In this article we report the outcomes of this workshop. We believe this effort is set apart from previous initiatives because of its ‘bottom up’ approach to ensure community adoption and therefore it should have realistic chances of implementation. We benefit from the fact that participating members of both groups have long established track records in life science research software development. This is the first release of our agreed software development good practices and expect that new revisions could evolve from it in future iterations. It is outside of the scope of this manuscript to delve into the issues that these metrics might raise in terms of performance comparison between different software.\n\nMethods\n\nIn a workshop 12 experts from across Europe met to discuss good software development practices for life sciences. At the meeting, the group was divided randomly into two equally large subgroups to facilitate discussion, each subgroup spending a set time discussing potential metrics, their impact and applicability. The experts in each subgroup did not impose any restriction on which metrics to propose, but rather aimed to be as inclusive as possible, as long as each suggested metric had potential for impact. After the discussion, each group summarised the results and subsequently we merged the resulting metrics together into a list of 17 topics.\n\nNext, the two groups worked on prioritising the identified metrics according to two criteria: 1) Importance and 2) Implementability. Importance is a measure of the impact that a particular practice can have in making software more sustainable. A metric is considered highly implementable if it is easy to generate. For each identified metric, importance and implementability were ranked by all members of the working group on a scale from 1 to 5, 5 being highest importance or easiest implementation. An average score was calculated and the resulting list was sorted from highest to lowest scoring metrics. Here we discuss and evaluate a final list of the top ten prioritised metrics.\n\nResults\n\nWe identified a set of 17 topics that are critical for software development good practice (Box 1). It was evident that these include measurements of different styles: measurements can be self-reported, automatically produced or externally audited. The type of metric is also important to consider here: there are metrics of qualitative and quantitative nature. Qualitative metrics correspond to a binary classification description, while quantitative metrics tend to be more amenable to integration and presentation as statistics. Metrics interpretation may pose challenges of its own kind, particularly related to the subjective nature of the importance of metrics and the different perceptions of value according to the context in which they are used.\n\n1. Version control:\n\na. Yes/no?\n\nb. How many committers?\n\nc. When was the version control started?\n\nd. When was the last commit?\n\n2. Code reviews:\n\na. Yes/no?\n\nb. Star rating based on code description\n\n3. Automated testing:\n\na. Yes/no?\n\nb. Coverage for unit tests\n\nc. Yes/no for individual tests:\n\ni. Unit tests\n\nii. Functional tests\n\niii. Integration tests\n\niv. Regression tests\n\nd. Are the tests part of the code in the repository?\n\n4. Not reinventing the wheel:\n\na. Using libraries?\n\nb. Using Frameworks?\n\nc. Describing the algorithm, explaining why known code is reimplemented.\n\nd. Reinventing should be documented. References to the algorithm?\n\ne. Percentage of code written from scratch?\n\nf. Percentage of code that is involved in the core functionality?\n\n5. Discoverability:\n\na. Via structured search on functionality?\n\nb. Is it in the ELIXIR Tools and Data Services Registry2 or others (e.g., BioSharing3)?\n\n6. Reusability of source code:\n\na. Number or reuses = number of derived projects/external commits?\n\n7. Reusability of software:\n\na. Number of citations on the paper\n\nb. Having basic description of features in structured ELIXIR format (EDAM ontology4) - in ELIXIR Tools and Data Services Registry?\n\n8. Licensing:\n\na. Is there a license?\n\nb. Is the source available?\n\nc. Is it open source according to opensource.org?\n\n9. Issue tracking/bug tracking:\n\na. Does it have a publicly accessible issue tracker?\n\nb. How long are issues open?\n\nc. What is the number of unresolved issues?\n\nd. How much activity has there been in the last three months in the issue tracker?\n\n10. Support processes:\n\na. Are basic processes defined? Like governance, mailing list, releases, ...\n\n11. Compliance with community standards:\n\na. Yes/no?\n\nb. Specifies the level of compliance, specification version or metrics?\n\n12. Buildable code:\n\na. Does the compiler give warnings?\n\nb. Does a static analysis (“lint”) give warnings?\n\nc. Is an automated build system used?\n\n13. Open development:\n\na. Number of external committers in the repositories\n\n14. Making data available:\n\na. Yes/no?\n\nb. Where?\n\n15. Documentation:\n\na. Ratio code/comments, code lines/document lines?\n\nb. Percentage of code dedicated to documentation?\n\n16. Simplicity:\n\na. Measure of cyclomatic complexity\n\n17. Dependency management:\n\na. Is it done automatically using a system?\n\nb. Does it use a language-standard repository to pull in dependencies?\n\nc. Is software made available as a dependency in a dependency repository?\n\nWe used the 43 metrics contained in the 17 identified topics as a basis for further prioritisation as described in the Methods section. Prioritisation of metrics was achieved by all participants scoring them according to their perception of importance and implementability. An average score was calculated and a sum of average importance and average implementability to rank the list (Table 1). We introduced also a manual evaluation for each of the proposed ranked metrics, which reflected the consensus of the final prioritisation, given initial difference of opinions when reviewing the average scores. In Table 1, we summarise the top 10 suggested metrics.\n\nTop 10 Ranked Metrics Avg\n\nImportancea Avg\n\nImplementabilityb Avg Sum\n\nPriority Scorec Manual Priority\n\nEvaluationd Is version control used? 5 4.6 9.6 1 Is the software discoverable? 4.1 5 9.1 2 Is an automated build system used? 4.6 3.9 8.4 3 Are test data available? 3.8 4 7.8 4 Does software contain parts that\n\nreimplement existing technology? 4.4 2.9 7.3 5 Is the software compliant with\n\ncommunity standards? 4.1 2.5 6.6 6 Are code reviews performed? 3.4 2.8 6.1 7 Is automated testing performed? 3.5 3.1 6.6 8 Is the code documented? 2.4 4.3 6.6 9 How high is the code complexity? 3.5 2.9 6.4 10\n\nAs a use case, we base the application of these metrics within the context of code development in ELIXIR. We define each of the 10 prioritised metrics in Table 1 and, where necessary, describe and explain the motivation for a metric and how to measure it. We consider that these definitions are applicable to a wider range of software development communities in life sciences.\n\n1. Is controlled versioning used?\n\n○ Description: Is it clearly indicated, can it be easily found?\n\n○ Motivation: Version control systems provide an environment for safe and transparent software development.\n\n○ How to measure: Put information about a version control tool to the ELIXIR Tools and Data Services Registry (which system, when it was installed, …)\n\n2. Is the software discoverable?\n\n○ Description: Is it easy to find the software based on its functionality (without knowing its exact name)?\n\n○ Motivation: It is important to be discoverable so other potential contributions are encouraged and more people use the software.\n\n○ How to measure: The ELIXIR community should be motivated and guided to provide this information into the ELIXIR Tools and Data Services Registry. If not, a list of other catalogues should be defined (maximum 5–10 other sources, e.g. BioSharing, field-specific catalogues, etc.). If the tool cannot be found there, the discoverability should be evaluated as 0.\n\n3. Is an automated build system used?\n\n○ Description: Are the builds of the software performed by some automated system?\n\n○ Motivation: If the automated system for builds is applied, can the users rebuild the software easily, which markedly increases its usability?\n\n○ How to measure: This information should be again included into the ELIXIR Tools and Data Services Registry2. Ideally, a link to the installation document should also be provided. How many commands are necessary for building of the software? (Optimally, just one command should be performed.)\n\n4. Are test data available?\n\n○ Description: Are data for testing of the software easily available for users?\n\n○ Motivation: Without test data, it may be difficult to try the functionality of the software and assess correct functioning of an installation.\n\n○ How to measure: The test data should be linked to from the web page describing the software or in the supplementary material of its associated publication. A link to the data should be included in ELIXIR Service Registry.\n\n5. Does software contain parts that reimplement existing technology?\n\n○ Description: Are common components/algorithms covered by libraries or reimplemented?\n\n○ Motivation: A (naïve) reimplementation can cause unnecessary errors or decrease the effectiveness.\n\n○ How to measure: Percentage of code written from scratch and/or number of used libraries. Additionally, descriptions of why a library with similar functionality was not used and responses to suggestions from community.\n\n6. Does the software support open community standards and what is its level of compliance?\n\n○ Description: Evaluation of software compliance with open/community standards\n\n○ Motivation: This is needed, for example, where data input/output, networking and general interoperability are concerned. However, it is also non-trivial to implement and measure in terms of the overall software quality.\n\n○ How to measure: A base metric would be: “does the software make use of open standards (yes/no), if so which ones (listing)?” In addition, more qualitative information such as “which versions of the standard does the software support?”, “Is it compatible with the latest specification?”, and “Can it be used to provide a more general level of support?” Another fundamental aspect to consider is whether the standard provide its own compliance metric (e.g., a test suite) and what the software’s level of compliance is. An example of such a compliance test suite is provided by the Systems Biology Markup Language (SBML,5).\n\n7. Are code reviews performed?\n\n○ Description: Whether new code is inspected by someone else before it becomes part of the code base.\n\n○ Motivation: Code reviews increase quality of the code both because it is written with more care and because the second pair of eyes will more readily catch false assumptions or errors.\n\n○ How to measure: Activity in code review process (comments to updated lines, etc.)\n\n8. Is automated testing performed?\n\n○ Description: Is some system for automated testing implemented?\n\n○ Motivation: Automatic testing decreases occurrence of bugs.\n\n○ How to measure: Information about the testing methodology should be present in the software documentation. In parallel, developers can be motivated to add this information to ELIXIR Service Registry.\n\n9. Is the code documented?\n\n○ Description: Does the code contain comments describing its main elements?\n\n○ Motivation: Code comments increase the readability of the code and also indirectly motivate the programmer to write a cleaner code. However, commenting can present the problem of not being updated as code changes. This means that code comments may rot and become misleading/inaccurate. Often comments can be made redundant by better names of variables and methods. An exception is example code where explaining what each line does with a comment is useful.\n\n○ How to measure: Determine the percentage of text from the source code that corresponds to comments.\n\n10. How high is the code complexity?\n\n○ Description: This refers to how complex or straightforward the code is.\n\n○ Motivation: The more complex code, the higher risk of errors. Code can be simplified by proper separation of tasks into different routines and methods.\n\n○ How to measure: Measure the cyclomatic complexity.\n\nDiscussion and conclusion\n\nWe present an initial set of 10 good practices that could help make software for the life sciences more sustainable. From our discussions, it was clear that a community-wide adoption of standards is needed in terms of how measurement of metrics are collected and shared. We operate under the assumption that all software developed should be open source from the beginning of development, which means that the collection of statistics for good practice compliance should not violate any of the licensing or privacy issues associated to closed code.\n\nThese ‘Top 10 Good Practices’ should be considered as an initial view of what the community considers important with a description of their feasibility for implementation within the life sciences. Among our top suggested topics there is a remarkable coincidence on the need for versioning. The ways on how to collect metrics regarding versioning systems vary: if using GitHub, a number of statistics are readily available that allow their easy collection for benchmarking. We do not, however, want to prescribe which versioning systems should be adopted. There are many ways in which this metric can be measured, a sample of which we offer. The metrics we propose can be both qualitative and quantitative. Although quantitative metrics are easier to track, it is also important to capture qualitative characteristics such as existence of automated testing or compliance with community standards.\n\nThis article is a first attempt to crystallise the conclusions from the work that the group of experts gathered under the auspices of ELIXIR and the Software Sustainability Institute. It is thus not intended to be a final declaration of what the ELIXIR community thinks the metrics, implementation and feasibility for measuring good practices for software development should be. This document is an initial response from the working group established to assess the problem of evaluating metrics for software development good practices. We expect it to be modified in future versions as more experts join this group and new challenges emerge with evolving technologies and life science software needs.\n\nAuthor details Author details\n\n1 The Earlham Institute & ELIXIR-UK, Norwich Research Park, Norwich, NR4 7UH, UK\n\n2 Software Sustainability Institute, University of Edinburgh, Edinburgh, EH9 3FD, UK\n\n3 University “La Sapienza, Rome, 00185, Italy\n\n4 DTL, PO Box 19245, Utrecht, 3501 DE, The Netherlands\n\n5 ELIXIR Hub, Wellcome Trust Genome Campus, Hinxton, CB10 1SD, UK\n\n6 Institute for Biostatistics and Medical Informatics (IBMI), Faculty of Medicine, University of Ljubljana, Ljubljana, SI-1104, Slovenia\n\n7 Systems Bioinformatics, Vrije Universiteit Amsterdam, Amsterdam, 1081 HV, The Netherlands\n\n8 Loschmidt Laboratories, Faculty of Science, Masaryk University, Brno, 625 00, Czech Republic\n\n9 CEITEC Masaryk University, Brno, 625 00, Czech Republic\n\n10 Department of Plant Systems Biology, VIB, Ghent, 9052, Belgium\n\n11 EMBL-EBI, Wellcome Trust Genome Campus, Hinxton, CB10 1SD, UK\n\nCompeting interests\n\nNo competing interests were disclosed.\n\nGrant information\n\nMC and HA are strategically core funded by UK’s BBSRC. BGO is funded by the BE-BASIC grant F08.005. NCH was supported by EPSRC, BBSRC and ESRC Grant EP/N006410/1 for the UK Software Sustainability Institute. The work was part of the ELIXIR-EXCELERATE project, funded by the European Commission within the Research Infrastructures programme of Horizon 2020, grant agreement number 676559.\n\nThe funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.\n\nCopyright\n\n© 2016 Artaza H et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited."
    }
}
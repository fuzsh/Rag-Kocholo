{
    "id": "correct_foundationPlace_00043_2",
    "rank": 48,
    "data": {
        "url": "https://www.cio.com/article/1312311/unesco-finds-pervasive-gender-bias-in-generative-ai-tools.html",
        "read_more_link": "",
        "language": "en",
        "title": "UNESCO finds ‘pervasive’ gender bias in generative AI tools",
        "top_image": "https://www.cio.com/wp-content/uploads/2024/03/GettyImages-157613698.jpg?quality=50&strip=all&w=1024",
        "meta_img": "https://www.cio.com/wp-content/uploads/2024/03/GettyImages-157613698.jpg?quality=50&strip=all&w=1024",
        "images": [
            "https://www.cio.com/wp-content/uploads/2024/06/profile-100304406-orig-3.png?w=150",
            "https://www.cio.com/wp-content/uploads/2024/03/GettyImages-157613698.jpg?quality=50&strip=all&w=1024",
            "https://www.cio.com/wp-content/uploads/2024/06/profile-100304406-orig-3.png?w=240",
            "https://www.cio.com/wp-content/uploads/2023/02/sarah_white-4-100639838-orig-13.jpg?quality=50&strip=all&w=1024",
            "https://www.cio.com/wp-content/uploads/2023/04/author_photo_Thor-Olavsrud_1639078885-4.jpg?quality=50&strip=all&w=150",
            "https://www.cio.com/wp-content/uploads/2024/07/3475889-0-38998400-1721658636-shutterstock_2136788461.jpg?quality=50&strip=all&w=444",
            "https://www.cio.com/wp-content/uploads/2024/07/3475888-0-28379400-1721657824-Getty-1075599574-copy-1.jpeg?quality=50&strip=all&w=375",
            "https://www.cio.com/wp-content/uploads/2024/07/2520897-0-60486900-1721653382-iStock-653836738.jpg?quality=50&strip=all&w=374",
            "https://www.cio.com/wp-content/uploads/2024/07/sinan-thumb-16x9-1-2.jpg?quality=50&strip=all&w=444",
            "https://www.cio.com/wp-content/uploads/2024/07/Thumb_CIOLL_ME24_Mosa_A.png?w=444",
            "https://www.cio.com/wp-content/uploads/2024/07/Thumb_CIOLL_ME24_Al_Moawda_RA-1.png?w=444",
            "https://www.cio.com/wp-content/uploads/2024/07/2520487-0-45106700-1721290364-CIOLL-India-thumb_Irshad-Saifi.jpg?quality=50&strip=all&w=444",
            "https://www.cio.com/wp-content/uploads/2024/07/sinan-thumb-16x9-1-3.jpg?quality=50&strip=all&w=444",
            "https://www.cio.com/wp-content/uploads/2024/07/Thumb_CIOLL_ME24_Al_Moawda_RA-2.png?w=444"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Jon Gold",
            "Jon Gold Senior",
            "Robert Mitchell Author",
            "Thor Olavsrud Senior"
        ],
        "publish_date": "2024-03-07T21:32:48-08:00",
        "summary": "",
        "meta_description": "Because AI outputs are no better than the human-generated data it’s trained on, the UN agency concludes judicial and social interventions will be necessary to address gen AI risks.",
        "meta_lang": "en",
        "meta_favicon": "https://www.cio.com/wp-content/themes/cio-b2b-child-theme/src/static/img/favicon.ico",
        "meta_site_name": "CIO",
        "canonical_link": "https://www.cio.com/article/1312311/unesco-finds-pervasive-gender-bias-in-generative-ai-tools.html",
        "text": "Generative AI’s outputs still reflect a considerable amount of gender and sexuality based bias, associating feminine names with traditional gender roles, generating negative content about gay subjects, and more besides, according to a new report from UNESCO’s International Research Centre on Artificial Intelligence.\n\nThe report, published today, centered on several individual studies of bias, including tests for associations between gendered names and careers, frequently generated less positive responses to prompts related to LGBTQ+ individuals and women, and assigned stereotyped professions to members of different genders and ethnic groups.\n\nThe researchers found three major categories of bias underlying generative AI technologies. The first is a data issue, in which an AI isn’t exposed to training data from underrepresented groups or doesn’t account for differences in sex or ethnicity, which can lead to inaccuracies. The second is algorithm selection, which can result in aggregation or learning bias. The classic example of this would be an AI identifying resumes from male job candidates as more desirable based on gender-based disparities already present in hiring practices. Finally, the study identified biases in deployment, where AI systems were applied to different contexts than the ones they had been developed for, resulting in “improper” associations between psychiatric terms and specific ethnic groups or genders.\n\nEach form of bias present within the large language models (LLMs) underpinning modern AI systems reflects the texts on which the LLMs are trained, the authors of the UNESCO report wrote in an introduction. Because these texts have been generated by humans, the LLMs, therefore, reflect human biases.\n\n“Consequently, LLMs can reinforce stereotypes and biases against women and girls, practices through biased AI recruitment tools, gender-biased decision-making in sectors like finance (where AI might influence credit scoring and loan approvals), or even medical or psychiatric misdiagnosis due to demographically biased models or norms,” they wrote.\n\nThe researchers noted that their study was not without its limitations, discussing several potential challenges, including limitations on implicit association tests, data contamination, deployment bias, language limitation, and the lack of intersectional analysis."
    }
}
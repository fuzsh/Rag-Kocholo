{
    "id": "dbpedia_5553_0",
    "rank": 55,
    "data": {
        "url": "https://arxiv.org/html/2311.17094v2",
        "read_more_link": "",
        "language": "en",
        "title": "In Search of a Data Transformation That Accelerates Neural Field Training",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/visualize_intensity/original_with_histogram.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/visualize_intensity/rpp_with_histogram.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/visualize_intensity/zigzag_with_histogram.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/visualize_intensity/inversion_with_histogram.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/visualize_intensity/standardization_with_histogram.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/visualize_intensity/linear_scaling_with_histogram.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/visualize_intensity/centering_with_histogram.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/visualize_intensity/gamma_with_histogram.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/visualize_intensity/horizontal_ocean_colorbar.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/loss_landscape_v2_down/0_30_origin_hole.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/loss_landscape_v2_down/0_30_shuffle_hole.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/loss_landscape_v2_down/0_30_origin_valley.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/loss_landscape_v2_down/0_30_shuffle_valley.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/loss_landscape_v2_down/30_50_origin.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/loss_landscape_v2_down/30_50_shuffle.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/supp/loss_landscape_down/kodak/kodak1_ori_30.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/supp/loss_landscape_down/kodak/kodak1_sh_30.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/supp/loss_landscape_down/kodak/kodak1_ori_50.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/supp/loss_landscape_down/kodak/kodak1_sh_50.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/supp/loss_landscape_down/kodak/kodak2_ori_30.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/supp/loss_landscape_down/kodak/kodak2_sh_30.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/supp/loss_landscape_down/kodak/kodak2_ori_50.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/supp/loss_landscape_down/kodak/kodak2_sh_50.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/supp/loss_landscape_down/div2k/div2k801_ori_30.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/supp/loss_landscape_down/div2k/div2k801_sh_30.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/supp/loss_landscape_down/div2k/div2k801_ori_50.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/supp/loss_landscape_down/div2k/div2k801_sh_50.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/supp/loss_landscape_down/div2k/div2k802_ori_30.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/supp/loss_landscape_down/div2k/div2k802_sh_30.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/supp/loss_landscape_down/div2k/div2k802_ori_50.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/supp/loss_landscape_down/div2k/div2k802_sh_50.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/supp/loss_landscape_down/clic/clic0_ori_30.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/supp/loss_landscape_down/clic/clic0_sh_30.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/supp/loss_landscape_down/clic/clic0_ori_50.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/supp/loss_landscape_down/clic/clic0_sh_50.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/supp/loss_landscape_down/clic/clic1_ori_30.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/supp/loss_landscape_down/clic/clic1_sh_30.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/supp/loss_landscape_down/clic/clic1_ori_50.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/supp/loss_landscape_down/clic/clic1_sh_50.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/supp/loss_landscape_down/kodak_rd/kodak8_ori_30_v2.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/supp/loss_landscape_down/kodak_rd/kodak8_sh_30_v2.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/supp/loss_landscape_down/kodak_rd/kodak8_ori_50_v2.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/supp/loss_landscape_down/kodak_rd/kodak8_sh_50_v2.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/supp/loss_landscape_down/kodak_eigen/kodak8_ori_top30_elev.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/supp/loss_landscape_down/kodak_eigen/kodak8_sh_top30_elev.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/supp/loss_landscape_down/kodak_eigen/kodak8_ori_top30_side.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/supp/loss_landscape_down/kodak_eigen/kodak8_sh_top30_side.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/supp/loss_landscape_down/kodak_eigen/kodak8_ori_top50_elev.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/supp/loss_landscape_down/kodak_eigen/kodak8_sh_top50_elev.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/supp/loss_landscape_down/kodak_eigen/kodak8_ori_top50_side.png",
            "https://arxiv.org/html/extracted/2311.17094v2/assets/file/cvpr_figure/supp/loss_landscape_down/kodak_eigen/kodak8_sh_top50_side.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Junwon Seo Sangyoon Leeâˆ— Kwang In Kim Jaeho Lee\n\nPohang University of Science and Technology (POSTECH)\n\n{junwon.seo, sangyoon.lee, kimkin, jaeho.lee}@postech.ac.kr equal contribution\n\nAbstract\n\nNeural field is an emerging paradigm in data representation that trains a neural network to approximate the given signal. A key obstacle that prevents its widespread adoption is the encoding speedâ€”generating neural fields requires an overfitting of a neural network, which can take a significant number of SGD steps to reach the desired fidelity level. In this paper, we delve into the impacts of data transformations on the speed of neural field training, specifically focusing on how permuting pixel locations affect the convergence speed of SGD. Counterintuitively, we find that randomly permuting the pixel locations can considerably accelerate the training. To explain this phenomenon, we examine the neural field training through the lens of PSNR curves, loss landscapes, and error patterns. Our analyses suggest that the random pixel permutations remove the easy-to-fit patterns, which facilitate easy optimization in the early stage but hinder capturing fine details of the signal.\n\n1 Introduction\n\nNeural field is a form of data representation that parameterizes each target signal as a neural network that maps spatiotemporal coordinates to the signal values [40]. For example, a colored image can be represented by a model that maps (ğš‡ğš‡\\mathtt{X}typewriter_X,ğšˆğšˆ\\mathtt{Y}typewriter_Y) pixel coordinates to the corresponding (ğšğš\\mathtt{R}typewriter_R,ğ™¶ğ™¶\\mathtt{G}typewriter_G,ğ™±ğ™±\\mathtt{B}typewriter_B) values. This parameterization enjoys many advantages in faithfully and efficiently representing high-dimensional signals with fine detail, and thus is being widely used for modeling signals of various modalities, such as image [4], video [15], 3D scene [25], or spherical data [12].\n\nA key obstacle that prevents the widespread adoption of neural fields is their training cost. To represent each datum as a neural field, one must train a neural network using many SGD iterations. For instance, NeRF requires at least 12 hours of training time on GPUs to represent a single 3D scene [25]. Representing a set of data thus requires a considerable amount of computation and time, making it very difficult to develop practical applications of neural fields.\n\nMany prior works view the â€œoptimization biasâ€ as a major cause behind the long training time of neural fields [36]. In particular, the spectral bias of the SGD-based training is known to bias the neural network to prioritize fitting the low-frequency components of the target signal and leave high-frequency components for the late stage of training [28]. It has been observed that such a tendency greatly hinders the neural fields from expressing natural data (e.g., images) with fine, high-frequency details [36, 45].\n\nThe prevailing strategy to mitigate such bias is to introduce a useful prior (or inductive bias) that can help neutralize the negative effects of the bias. One popular approach is to develop new network components that bring a favorable architectural prior, such as the Fourier features [25], sinusoidal activation [34], or spatial encoding [24, 26]. Other works also attempt to introduce the prior in the form of initial parameters that have been meta-learned from a large set of signals [33, 37]. Such meta-learned initializations tend to have rich high-frequency spectra, which can help fitting natural signals within a small number of SGD steps [37].\n\nIn this paper, we approach the problem from a different angle. In particular, we ask the following question:\n\nâ€œCan we exploit the optimization bias of SGD,\n\ninstead of fighting against it?â€\n\nPrecisely, we ask whether we can transform the datum in a way that the optimization bias acts favorably in fitting the transformed data with a neural field. If there exists such a transformation, and if it admits an efficiently computable inverse, we may be able to use the following strategy to reduce the neural field training cost: we train a neural field that expresses the transformed signal, and the original signal can be recovered from the model by applying an inverse transformation to the signal generated by the model (Fig. 1).\n\nContribution. As a first step to answer this question, we conduct an extensive empirical study on how applying simple data transformations affect the computational cost of training the corresponding neural field. In particular, we compare the number of SGD steps needed to fit the transformed data to a certain fidelity level (e.g., PSNR 50 for images), with the steps to fit the original data. We try total seven different data transformations, from the one that permutes the location of the pixels to the one that scales the intensity values of each pixel.\n\nFrom the experiment, we make a surprising observation: we find that the random pixel permutation (abbr. ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP) provides a consistent acceleration on a range of datasets and various neural field architectures with sophisticated encoding schemes. The ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP is in fact the only data transformation that provided acceleration on every experimental setups that we considered. We find that the original data requires, on average, âˆ¼similar-to\\simâˆ¼30% greater number of SGD steps to achieve the similar fidelity level than the ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP data. As ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP tends to bias the data toward high-frequency, it is quite counter-intuitive that they can be fit faster than the original signals.\n\nWhy does the random pixel permutation accelerate neural field training? To explain this phenomenon, we articulate the following â€œblessings of no patternâ€ hypothesis:\n\nOriginal data often have smooth, representative patterns that facilitate easy optimization, particularly in the early phase of learning. However, this smoothness quickly transforms into an obstacle when aiming for a sufficiently high level of fidelity. Random pixel permutation removes these easy-to-fit patterns, which accelerates optimization in the long run. (â˜…â˜…\\bigstarâ˜…)\n\nTo corroborate our hypothesis â˜…â˜…\\bigstarâ˜…, we take an in-depth look into the training dynamics of the original and the ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP data. We find that, indeed the training speed on the ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP data is slower than on the original data during the early training phase (Sec. 4.1). However, after training for a sufficient number of epochs, the neural field trained on ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP data finds a â€œlinear loss highwayâ€ on which the optimization is very easy; the optimization on original data fails to find one (Sec. 4.2). Furthermore, we find that the errors in the ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP images are more evenly distributed over pixels and lack visually distinguishable structures; errors in original images tend to have periodic or axis-aligned patterns, which might be the artifacts of the encoding scheme (Sec. 4.3).\n\nTo sum up, our contribution can be summarized as:\n\n1.\n\nThrough systematic study, we find that simple data transformations can dramatically change the training speed of neural field (Ã—0.1absent0.1\\times 0.1Ã— 0.1â€“Ã—20absent20\\times 20Ã— 20), even on state-of-the-art neural fields architectures with sophisticated encodings [26].\n\n2.\n\nWe discover that random pixel permutations (ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP) provide consistent acceleration over fitting the original image, providing Ã—1.08absent1.08\\times 1.08Ã— 1.08â€“Ã—1.50absent1.50\\times 1.50Ã— 1.50 speedups.\n\n3.\n\nWe conduct an in-depth analysis which sheds light on how ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP speeds up the training by removing the easy-to-fit patterns that slow down the training eventually.\n\nDespite the limitation of ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP that it may be difficult to be applied for applications where generalization is critical, our study has at least two potential impact areas. First, the ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP itself may be useful in the applications of neural field where a strong training fidelity is the core performance criterion, such as data compression [31]. In fact, one of the core challenges in neural-field-based data compression is the slow encoding speed (see, e.g., COOL-CHIC [17]). Second, our study provides a concrete starting point in developing a new data transformation that strikes the balance between the training speed and the generalizability.\n\n2 General framework\n\nThe general framework of finding data transformations that accelerate neural field training can be formalized as an optimization problem, as we describe in this section.\n\n2.1 Formalisms\n\nSuppose that we want to use the neural field parameterization to approximate some signal ğ±âˆˆğ’³ğ±ğ’³\\mathbf{x}\\in\\mathcal{X}bold_x âˆˆ caligraphic_X. Here, the signal space ğ’³ğ’³\\mathcal{X}caligraphic_X is the space of all signals that have the same data type. For example, we can let ğ’³ğ’³\\mathcal{X}caligraphic_X be the set of all 256Ã—256256256256\\times 256256 Ã— 256 RGB images; in such case, we may have ğ’³âŠ†â„256Ã—256Ã—3ğ’³superscriptâ„2562563\\mathcal{X}\\subseteq\\mathbb{R}^{256\\times 256\\times 3}caligraphic_X âŠ† blackboard_R start_POSTSUPERSCRIPT 256 Ã— 256 Ã— 3 end_POSTSUPERSCRIPT.\n\nThe data transformation T:ğ’³â†’ğ’³:ğ‘‡â†’ğ’³ğ’³T:\\mathcal{X}\\to\\mathcal{X}italic_T : caligraphic_X â†’ caligraphic_X maps an element in the signal space to another element. We define the transformation space ğ’¯ğ’¯\\mathcal{T}caligraphic_T as a set of data transformations on ğ’³ğ’³\\mathcal{X}caligraphic_X that has an inverse.\n\nOur goal is to find a data transformation Tğ‘‡Titalic_T that minimizes the training cost of fitting the transformed signal Tâ¢(ğ±)ğ‘‡ğ±T(\\mathbf{x})italic_T ( bold_x ). To formalize this, we define the training cost\n\nğ–¼ğ—ˆğ—Œğ—:ğ’³Ã—ğ’¯â†’â„+.:ğ–¼ğ—ˆğ—Œğ—â†’ğ’³ğ’¯subscriptâ„\\displaystyle\\mathsf{cost}:\\mathcal{X}\\times\\mathcal{T}\\to\\mathbb{R}_{+}.sansserif_cost : caligraphic_X Ã— caligraphic_T â†’ blackboard_R start_POSTSUBSCRIPT + end_POSTSUBSCRIPT . (1)\n\nHere, the function ğ–¼ğ—ˆğ—Œğ—â¢(ğ±,T)ğ–¼ğ—ˆğ—Œğ—ğ±ğ‘‡\\mathsf{cost}(\\mathbf{x},T)sansserif_cost ( bold_x , italic_T ) measures the computational burden required to train a neural field for Tâ¢(ğ±)ğ‘‡ğ±T(\\mathbf{x})italic_T ( bold_x ), whenever its outcome is inverted back via Tâˆ’1superscriptğ‘‡1T^{-1}italic_T start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT, approximates the original signal ğ±ğ±\\mathbf{x}bold_x with the desired level of precision. The burden may be measured in various ways, e.g., the number of SGD steps, FLOPs, or wall-clock time. We note that the training cost of ğ±ğ±\\mathbf{x}bold_x using Tâ¢(â‹…)ğ‘‡â‹…T(\\cdot)italic_T ( â‹… ) need not be identical to the training cost of fitting Tâ¢(ğ±)ğ‘‡ğ±T(\\mathbf{x})italic_T ( bold_x ) with the same precision, i.e.,\n\nğ–¼ğ—ˆğ—Œğ—â¢(ğ±,T)â‰ ğ–¼ğ—ˆğ—Œğ—â¢(Tâ¢(ğ±),Id),ğ–¼ğ—ˆğ—Œğ—ğ±ğ‘‡ğ–¼ğ—ˆğ—Œğ—ğ‘‡ğ±Id\\displaystyle\\mathsf{cost}(\\mathbf{x},T)\\neq\\mathsf{cost}(T(\\mathbf{x}),% \\mathrm{Id}),sansserif_cost ( bold_x , italic_T ) â‰  sansserif_cost ( italic_T ( bold_x ) , roman_Id ) , (2)\n\nespecially when the transformation involves scaling of the signal. Also, the training cost may heavily depend on the choice of neural field architectures, hardware type, and the batch sizeâ€”using a larger batch size tends to reduce the number of steps until convergence [32], but it also requires training with a larger memory.\n\nGiven these tools, we work to consider the optimization\n\nminimizeğ–¼ğ—ˆğ—Œğ—â¢(ğ±,T),subjectâ¢toTâˆˆğ’¯âˆ—,minimizeğ–¼ğ—ˆğ—Œğ—ğ±ğ‘‡subjecttoğ‘‡superscriptğ’¯\\displaystyle\\mathrm{minimize}\\quad\\mathsf{cost}(\\mathbf{x},T),\\qquad\\mathrm{% subject\\>to}\\quad T\\in\\mathcal{T}^{*},roman_minimize sansserif_cost ( bold_x , italic_T ) , roman_subject roman_to italic_T âˆˆ caligraphic_T start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT , (3)\n\nwhere ğ’¯âˆ—âŠ†ğ’¯superscriptğ’¯ğ’¯\\mathcal{T}^{*}\\subseteq\\mathcal{T}caligraphic_T start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT âŠ† caligraphic_T is a subset of the transformation space that satisfies some desired properties. The set ğ’¯âˆ—superscriptğ’¯\\mathcal{T}^{*}caligraphic_T start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT can be configured in many different ways, to account for the expected usages of the trained neural fields. This problem can also be extended to a version where we have a probability distribution of the signals ğ±ğ±\\mathbf{x}bold_x, and find a single transformation Tğ‘‡Titalic_T that minimizes the expected cost.\n\nScope of this paper. While the most general framework is to solve the optimization (3) directly, the problem is intractible unless we have an expressive, well-paramaterized transformation space ğ’¯âˆ—superscriptğ’¯\\mathcal{T}^{*}caligraphic_T start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT that admits an effective optimization method. In this paper, we focus on the proof-of-concept that there exists some choice of Tğ‘‡Titalic_T such that\n\nğ–¼ğ—ˆğ—Œğ—â¢(ğ±,T)<ğ–¼ğ—ˆğ—Œğ—â¢(ğ±,Id),ğ–¼ğ—ˆğ—Œğ—ğ±ğ‘‡ğ–¼ğ—ˆğ—Œğ—ğ±Id\\displaystyle\\mathsf{cost}(\\mathbf{x},T)<\\mathsf{cost}(\\mathbf{x},\\mathrm{Id}),sansserif_cost ( bold_x , italic_T ) < sansserif_cost ( bold_x , roman_Id ) , (4)\n\n(Sec. 3), and deepening our understanding on when and why such Tâ¢(â‹…)ğ‘‡â‹…T(\\cdot)italic_T ( â‹… ) can accelerate the training (Sec. 4).\n\n2.2 Example cases\n\nHere are some examples desired properties of data transformation, and how they relate to practical applications. We provide a more in-depth discussions in the Appendix S3.\n\nEfficient invertibility. The inverse Tâˆ’1superscriptğ‘‡1T^{-1}italic_T start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT should be able to be computed efficiently, e.g., by performing a linear operation. Ideally, one may wish to be able to combine the inverse transformation into the neural field by modifying the parameters of the trained neural field directly. For instance, if Tâ¢(ğ±)=âˆ’ğ±ğ‘‡ğ±ğ±T(\\mathbf{x})=-\\mathbf{x}italic_T ( bold_x ) = - bold_x, we can incorporate the inverse in the neural field by negating the final layer weights of the neural field. This property is useful whenever we expect many repeated inferences of the neural field, e.g., for real-time interaction.\n\nRetains interpolatability. We want our transform Tğ‘‡Titalic_T to admit a principled way to sample the value of interpolated coordinates from the neural field that approximates the transformed signal. In some neural field applications, e.g., super-resolution [4], this interpolating capability plays an essential role. On the other hand, some applications like data compression do not involve any interpolation [31].\n\n3 Data transformations vs. training speed\n\nIn this section, we compare the training cost of the neural field training on various data transformation schemes. As it turns out, there indeed exists a nice data transformation that satisfies ineq. 4: the random pixel permutation (ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP).\n\n3.1 Experimental setup\n\nWe focus on the task of 2D image regression. This choice reduces the computational burden of fitting each datum, so that we can make experimental validations on multiple datasets and models with extensive hyperparameter tuning.\n\nFor the training cost, we use the number of SGD steps until we reach the training PSNR over 50dB (a near-perfect reconstruction for 8-bit images). The number of SGD steps is an informative indicator of the total FLOPs and runtime for any fixed choice of hardware, model architecture, and data. Importantly, we tune the learning rate for each (transformed) image, making it close to the scenario where all settings have been optimized to minimize the runtime.\n\nFor each choice of an image and the data transformation, we measure the acceleration factor, i.e.,\n\nğ–ºğ–¼ğ–¼â¢(ğ±,T)â‰œğ–¼ğ—ˆğ—Œğ—â¢(ğ±,Id)ğ–¼ğ—ˆğ—Œğ—â¢(ğ±,T).â‰œğ–ºğ–¼ğ–¼ğ±ğ‘‡ğ–¼ğ—ˆğ—Œğ—ğ±Idğ–¼ğ—ˆğ—Œğ—ğ±ğ‘‡\\displaystyle\\mathsf{acc}(\\mathbf{x},T)\\triangleq\\frac{\\mathsf{cost}(\\mathbf{x% },\\mathrm{Id})}{\\mathsf{cost}(\\mathbf{x},T)}.sansserif_acc ( bold_x , italic_T ) â‰œ divide start_ARG sansserif_cost ( bold_x , roman_Id ) end_ARG start_ARG sansserif_cost ( bold_x , italic_T ) end_ARG . (5)\n\nOther configurations are as follows.\n\nDatasets. We use three different image datasets:\n\nâ€¢\n\nKodak. Consists of 24 different images from the Kodak lossless true color image suite [16].\n\nâ€¢\n\nDIV2K. Consists of 100 images in the validation split of the DIV2K image super-resolution challenge (HR) [1].\n\nâ€¢\n\nCLIC. Consists of 100 images from the validation split of the dataset for the Challenge on Learned Image Compression 2020 (CLIC) [38]. There are total 102 images in the validation split, but we removed two images that have a sidelength shorter than 512 pixels.\n\nWe pre-process each image in the datasets as follows. Following BACON [19], we resize each image into 512Ã—\\timesÃ—512 pixels by first center-cropping the image to a square with sidelengths equal to the shorter sidelength of the original image, and then resizing with the Lanczos algorithm. Next, we convert the image to grayscale. Finally, we apply sRGB to Linear RGB operation, as in Instant-NGP [26].\n\nThe resulting images have 512Ã—\\timesÃ—512 pixels (thus total 218superscript2182^{18}2 start_POSTSUPERSCRIPT 18 end_POSTSUPERSCRIPT pixels) with intensities lying in the interval [0,1]01[0,1][ 0 , 1 ].\n\nModels. We use two different neural field architectures.\n\nâ€¢\n\nSIREN. A classical architecture that uses multi-layered perceptrons with sinusoidal activation functions [34]. We configure the model to have three hidden layers with 512 neurons in each layer and output dimension 1. We use the default frequency scaling hyperparameter Ï‰0=30subscriptğœ”030\\omega_{0}=30italic_Ï‰ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = 30.\n\nâ€¢\n\nInstant-NGP. A more recently proposed neural field with multi-resolution hash encodings [26]. We use the default setup for the image regression, with the maximum hash table size and 16-bit parameters.\n\nTraining. We use full-batch gradient descent, i.e., the batch size 218superscript2182^{18}2 start_POSTSUPERSCRIPT 18 end_POSTSUPERSCRIPT. This batch size, in general, minimizes the number of SGD steps required to fit the target image. We note that we sample directly from the coordinate grid, instead of sampling from interpolated coordinates (as in, e.g., [26]).\n\nHyperparameters. We tune the learning rate using the grid search. For SIREN, we tune the learning rate in the range {2âˆ’8,â€¦,2âˆ’16}superscript28â€¦superscript216\\{2^{-8},\\ldots,2^{-16}\\}{ 2 start_POSTSUPERSCRIPT - 8 end_POSTSUPERSCRIPT , â€¦ , 2 start_POSTSUPERSCRIPT - 16 end_POSTSUPERSCRIPT }. For Instant-NGP, we tune the learning rate in the range {2âˆ’4,â€¦,2âˆ’15}superscript24â€¦superscript215\\{2^{-4},\\ldots,2^{-15}\\}{ 2 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT , â€¦ , 2 start_POSTSUPERSCRIPT - 15 end_POSTSUPERSCRIPT }.\n\nData transformations. We consider total seven elementary data transformations (Fig. 2): Two transformations that only change the location of the pixels (random pixel permutation, zigzag permutation), and five transformations that change the intensity values of each pixels (inversion, standardization, linear scaling, centering, and gamma correction).\n\nâ€¢\n\nRandom pixel permutation (ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP). We randomly permute the location of each pixels. This transformation generally increases the frequency spectrum of the image.\n\nâ€¢\n\nZigzag permutation. We sort all the pixels in the ascending order of their intensities. Then, we place the sorted pixels in the zigzag order, starting from the upper left corner. This produces a low-frequency image.\n\nâ€¢\n\nInversion. We invert the intensity of each pixel, i.e., perform zâ†¦1âˆ’zmaps-toğ‘§1ğ‘§z\\mapsto 1-zitalic_z â†¦ 1 - italic_z on each intensity values.\n\nâ€¢\n\nStandardization. We measure the mean Î¼ğœ‡\\muitalic_Î¼ and standard deviation Ïƒğœ\\sigmaitalic_Ïƒ for the intensities of all pixels in the image. Then we standardize the intensities via zâ†¦zâˆ’Î¼Ïƒmaps-toğ‘§ğ‘§ğœ‡ğœz\\mapsto\\frac{z-\\mu}{\\sigma}italic_z â†¦ divide start_ARG italic_z - italic_Î¼ end_ARG start_ARG italic_Ïƒ end_ARG.\n\nâ€¢\n\nLinear scaling. We scale the intensities of each pixel by tğ‘¡titalic_t without any centering, i.e. zâ†¦tâ¢zmaps-toğ‘§ğ‘¡ğ‘§z\\mapsto tzitalic_z â†¦ italic_t italic_z.\n\nâ€¢\n\nCentering. We center the interval [0,1]01[0,1][ 0 , 1 ] at zero and scale the intensities by tğ‘¡titalic_t, i.e., zâ†¦tâ¢(zâˆ’1/2)maps-toğ‘§ğ‘¡ğ‘§12z\\mapsto t(z-\\nicefrac{{1}}{{2}})italic_z â†¦ italic_t ( italic_z - / start_ARG 1 end_ARG start_ARG 2 end_ARG ).\n\nâ€¢\n\nGamma correction. We nonlinearly scale the intensity of each pixels as zâ†¦z1/Î³maps-toğ‘§superscriptğ‘§1ğ›¾z\\mapsto z^{1/\\gamma}italic_z â†¦ italic_z start_POSTSUPERSCRIPT 1 / italic_Î³ end_POSTSUPERSCRIPT. Choosing Î³>1ğ›¾1\\gamma>1italic_Î³ > 1 makes images brighter, and Î³<1ğ›¾1\\gamma<1italic_Î³ < 1 makes darker.\n\n3.2 Results\n\nWe report the average acceleration factors in Tab. 1. From the table, we make several observations:\n\nâ€¢\n\nğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP provides a consistent acceleration over the original image; see Sec. 4 for an in-depth analysis.\n\nâ€¢\n\nThe zigzag permutation works tremendously well on SIRENâ€”speeding up training by over Ã—17absent17\\times 17Ã— 17â€”but slows down training Instant-NGP. We hypothesize that this is due to the axis-aligned inductive bias imposed by the spatial grid encoding; see Sec. 4.3 for more discussion.\n\nâ€¢\n\nInversion consistently slows down the training. This fact, ironically, implies that inversion might have been an effective accelerator if all â€œnatural imagesâ€ looked like the color-inverted images.\n\nâ€¢\n\nScaling up the intensities tends to have opposite effects on different architectures. Scaling up speeds up training in SIRENs, but slows down in Instant-NGPs.\n\nâ€¢\n\nGamma correction slows down the training, regardless of taking the power of 2222 or 1/212\\nicefrac{{1}}{{2}}/ start_ARG 1 end_ARG start_ARG 2 end_ARG. We suspect that this is due to the precision errors from taking powers. In other words, excessive scaling operations may not be helpful in accelerating the training.\n\n4 A closer look at the random permutation\n\nWe now focus on a specific type of data transformation: the random pixel permutation (ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP). The ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP relocates each pixel to a new random location, i.e.,\n\n(ğšŒğš˜ğš˜ğš›ğšğšœi,ğšŸğšŠğš•ğšğšğšœi)â†¦(ğšŒğš˜ğš˜ğš›ğšğšœÏ€â¢(i),ğšŸğšŠğš•ğšğšğšœi)maps-tosubscriptğšŒğš˜ğš˜ğš›ğšğšœğ‘–subscriptğšŸğšŠğš•ğšğšğšœğ‘–subscriptğšŒğš˜ğš˜ğš›ğšğšœğœ‹ğ‘–subscriptğšŸğšŠğš•ğšğšğšœğ‘–\\displaystyle(\\mathtt{coords}_{i},\\mathtt{values}_{i})\\mapsto(\\mathtt{coords}_% {\\pi(i)},\\mathtt{values}_{i})( typewriter_coords start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , typewriter_values start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) â†¦ ( typewriter_coords start_POSTSUBSCRIPT italic_Ï€ ( italic_i ) end_POSTSUBSCRIPT , typewriter_values start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) (6)\n\nfor some random permutation Ï€â¢(â‹…)ğœ‹â‹…\\pi(\\cdot)italic_Ï€ ( â‹… ). The marginal distributions of the input coordinates or the output values remain the same. As we have seen in Tab. 1, the ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP transformation accelerates the fitting of the corresponding neural field, which is very unexpected and difficult to explain.\n\nWhy is this strange? From the perspective of the spectral bias, the ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP operation should have slowed down the training speed, instead of accelerating it. As demonstrated by Rahaman et al. [28], typical neural networks first rapidly fit the low-frequency components of the target signal, and fit the high-frequency components much later. The ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP transformation, in this sense, should have been detrimental to the training. In fact, ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP images tend to have more higher-frequency components than the original images. In Fig. 3, we provide the average discrete cosine transform (DCT) coefficients of the original and ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP images; for visualization, we raise the coefficients to the power of 0.030.030.030.03, similar to [43]. We observe that the original image is more biased toward low-frequency than ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP. However, we also observe that ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP images fit faster than the original images.\n\nIn the remainder of this section, we identify three different aspects that the training dynamics of ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP images are critically different from that of original images. These aspects, when put together, support our â€œblessings of no patternâ€ hypothesis (â˜…â˜…\\bigstarâ˜…) that ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP accelerates training by removing easy-to-fit patterns that can be harmful to reaching a high level of fidelity. In particular, we show that\n\nâ€¢\n\nğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP is slow to reach moderate PSNR levels, but is fast to reach high PSNR (Sec. 4.1).\n\nâ€¢\n\nthere is a linear path that connects the moderate-to-high PSNR points in the ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP loss landscape (Sec. 4.2).\n\nâ€¢\n\nmodel trained on ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP images tends to have less structured error, while vanilla training leads to error patterns that reflect the underlying encoding schemes (Sec. 4.3).\n\n4.1 The PSNR curve: Slower to approximate well, but faster to approximate â€œvery wellâ€\n\nFirst, we observe that ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP images tend to achieve moderate PSNRs (e.g., 30dB) much later than the original images, but they quickly reach high PSNRs (e.g., 50dB) afterwards.\n\nIn Fig. 4, we provide an example plot of fitting a SIREN on a natural image from the Kodak dataset. From the figure, we can immediately make the following observations:\n\nâ€¢\n\nRPPs are slow starters. The training PSNR of the original image quickly arrives at âˆ¼similar-to\\simâˆ¼ 30dB the early stage of training, even before taking 300 SGD iterations. On the other hand, the PSNR of the ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP image remains very small, under 15 dB at the similar number of steps.\n\nâ€¢\n\nExplosive surge in later stage. After staying at a low PSNR level (under 20 dB) for a while, the PSNR of ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP images bursts explosively. The PSNR jumps from âˆ¼similar-to\\simâˆ¼ 15dB to 50dB in less than 100 SGD steps. On the other hand, the PSNR curve of the original image oscillates wildly at 30â€“35dB level for a long time, and gradually reaches the 50dB at around 1600 steps.\n\nThese observations are consistent over the choice of the target image; we provide a comprehensive collection of figures in the supplementary materials.\n\nFor a more quantitative comparison, we compare the average number of SGD steps to arrive the PSNR of 30dB and 50dB for the original and ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP images (Tab. 2). Again, we use the images from the Kodak dataset and fit with SIRENs.\n\nWe observe that, for original images, the number of steps to reach 30dB is around 100 steps, while the number of SGD steps to reach the 50dB is over 1300 steps. That is, it takes almost 13Ã—13\\times13 Ã— more steps to reach the high PSNR than for moderate PSNRs. On the other hand, for ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP images, it takes over 800 steps to reach the moderate PSNR level, while it takes only 300 more steps to reach the high PSNR.\n\n4.2 Linear paths in the loss landscape: RPP images find a â€œlinear expresswayâ€\n\nFrom the previous observation, it seems likely that, for ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP images, there may exist an expressway in the neural field parameter space that connects the moderate-PSNR parameter with the high-PSNR parameter; on this expressway, the SGD may not encounter too many hills to circumvent, and may mostly follow a smooth linear path.\n\nTo validate this intuition, we visualize the loss landscape of the SIREN trained on Kodak images. In particular, we capture two different phases of training.\n\n1.\n\nEarly phase. We plot the linear path from the initialization to 30dB point, i.e., the parameter that first achieves PSNR 30dB during the training (Fig. 5)\n\n2.\n\nLate phase. We plot the linear path from the 30dB point to the PSNR 50dB point (Fig. 6).\n\nTo generate such visualization, we project the parameter space into a two-dimensional space, as in [18]. Unlike [18], we fix one axis to a directional vector between two parameter points that we want to capture (e.g., 30dB point and 50dB point); the other axis has been decided randomly.\n\nDuring the early phase (Fig. 5), we observe that the loss landscape of ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP is quite hostile; unlike in original images, the minima that 30dB point belongs to is quite narrow and there is no clear pathway toward the 30dB point.\n\nDuring the late phase (Fig. 6), for the ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP image, once the parameter arrives at the minima, there exists a linear path that connects the 30dB point to the 50dB point. If we measure the loss barrier, i.e. the maximum loss on the linear path between 30dB and 50dB points, it is as low as 28.3dB for the ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP versions of Kodak images. For original images, the loss barrier is quite high (15.3dB).\n\n4.3 Patterns in the error: The errors are less structured in RPP images\n\nAnother interesting property of ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP is that the errors that the neural field makes are more evenly distributed among the pixels, lacking a clearly distinguishable pattern. It turns out that the whiteness of the ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP error can help generating more visually sharp and satisfactory images than neural fields trained on original images.\n\nIn Tab. 3, we measure the pixel loss variance in both original and ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP images trained to various target PSNR levels. From the table, we observe that the loss variance is up to 5.80Ã—5.80\\times5.80 Ã— larger in the neural fields trained on the original images than on ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP images. This implies that, for original images, some pixels in the original image are fit much faster than other pixels. Putting that differently, the neural field trained on the unpermuted images tends to prioritize learning a specific type of patterns first, while the neural field trained on ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP images does less so.\n\nExactly what pattern is ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP avoiding to fit? In Figs. 7 and 8, we compare the images generated by the neural fields trained on original and ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP images on SIREN and Instant-NGP, respectively. In Fig. 7, we observe that SIRENs trained on original images have thicker wavy patterns, which is likely to be a consequence of the sinusoidal encoding structure of SIREN. The models trained on ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP images, on the other hand, have much sharper texture with fine-grained noise. A similar phenomenon is observed in the case of Instant-NGP (Fig. 8). Here, we observe that the models trained on original images attain axis-aligned artifacts; black (or white) horizontal (or vertical) blocks appear in the rendered images. On the other hand, the error is much less structured in models trained on ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP images.\n\n5 Related work\n\nOptimization bias of neural networks. It has been well known, both theoretically and empirically, that SGD-based optimization algorithms are biased toward learning â€œsimpler solutions,â€ for various notions of simplicity. A prominent example is the spectral bias, a tendency to prioritize learning lower-frequency components of the target function [3, 28]. Other types of biases have also been studied in the literature: SGD tends to prefer learning smaller-norm solutions [27, 13] and low-rank solutions [39, 42]. These works mostly focus on explaining how such simplicity biases are advantageous in learning a solution that can generalize well (see, e.g., [29]). Our work, in contrast, describes how such bias can be disadvantageous in terms of the training speed, instead of the generalization performance.\n\nRandom labels and memorization. The question of how neural nets behave when trained on random labels has been actively studied since its connection to the generalization capability of deep learning has been established [46]. Arpit et al. [2] finds that neural networks tend to learn simple patterns first and then memorize the unexplainable samplesâ€”such as random label and dataâ€”in the later phase. A more recent study argues that training on randomly labeled data can actually be advantageous [23]; pre-training on random data helps fitting the training data, when eventually fine-tuning on the dataset with correct labels. Different from these works, our work discovers how random label can be beneficial in the neural field context, without any consideration on further fine-tuning.\n\nFaster training of neural fields. Other than the works described in the introduction, there have been many other attempts to accelerate the neural field training by designing better encoding schemes. Positional encodings [25, 36, 34, 9] and spatial grids [21, 10, 26, 24] are the most popular options, and some works also use tree-like structures [35, 44]. Our work is complementary to this line of works, and argues that transforming the data in the original data space can provide auxiliary training speed boosts. We also note that a concurrent work also aims to boost the training speed by modifying the given data [20]. In particular, [20] finds that separating the visual signals into many sub-segments can facilitate the convergence of SIRENs and proposes a meta-learning algorithm to speed up training. Our work considers a more general class of data transformations, and focus on understanding how transformations accelerate training.\n\n6 Conclusion\n\nLearning neural fields poses a unique challenge, as the primary objective is to overfit to the given dataâ€”an aim contrary to the typical goals in other problem domains where learning attempts to achieve regularization to prevent overfitting. In our study of this distinctive problem, we discovered that certain data transformations can expedite the networkâ€™s acquisition of fine details more rapidly than when learning from the original images. Specifically, we demonstrated that random pixel permutations, which explicitly make learning low-frequency details more challenging, guide the network away from fixating on low-frequency components. This resulted in significantly enhanced PSNR within a given computational budget and, furthermore, yielded visually more plausible reconstructions at the same PSNR level. Our insights are substantiated by thorough empirical evaluations.\n\nLimitation & future direction. A notable limitation of our approach is that its direct applicability may be constrained when learning neural networks for other problem domains such as classification and regression, where regularization is crucial. As a future work, we aim to conduct more comprehensive studies to understand how data transformations affect both trainability and generalization of neural fields (and other neural networks). Such exploration may offer new insights on how to modify the given workload to jointly optimize the training cost and the model performance.\n\nAnother, more narrowly scoped future direction will be to gain a more concrete, theoretical understanding on the effect of random pixel permutations on the training dynamics of neural fields. Although our empirical analyses provide some insights into how ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP can help accelerate the training, we are yet to fully understand why fitting the easy patterns slows down the later phase of training. Deeper understandings on this matter may lead to impactful lessons applicable to any deep learning domain that involves long training.\n\nAcknowledgments. This work was partly supported by the National Research Foundation of Korea (NRF) grant (RS-2023-00213710, Neural Network Optimization with Minimal Optimization Costs), and partly by the Institute of Information & communications Technology Planning & Evaluation (IITP) grant (No.2022- 0-00713, Meta-learning applicable to real-world problems, No.2019-0-01906, Artificial Intelligence Graduate School Program (POSTECH)), all funded by the Korea government (MSIT).\n\nReferences\n\nAgustsson and Timofte [2017] Eirikur Agustsson and Radu Timofte. NTIRE 2017 challenge on single image super-resolution: Dataset and study. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 2017.\n\nArpit et al. [2017] Devansh Arpit, StanisÅ‚aw JastrzÈ©bski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S. Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, and Simon Lacoste-Julien. A closer look at memorization in deep networks. In ICML, 2017.\n\nBasri et al. [2019] Ronen Basri, David Jacobs, Yoni Kasten, and Shira Kritchman. The convergence rate of neural networks for learned functions of different frequencies. In NeurIPS, 2019.\n\nChen et al. [2021] Yinbo Chen, Sifei Liu, and Xiaolong Wang. Learning continuous image representation with local implicit image function. In CVPR, 2021.\n\nDe Luigi et al. [2023] Luca De Luigi, Adriano Cardace, Riccardo Spezialetti, Pierluigi Zama Ramirez, Samuele Salti, and Luigi Di Stefano. Deep learning on implicit neural representations of shapes. In ICLR, 2023.\n\nDupont et al. [2021] Emilien Dupont, Adam GoliÅ„ski, Milad Alizadeh, Yee Whye Teh, and Arnaud Doucet. Coin: Compression with implicit neural representations. 2021.\n\nDupont et al. [2022a] Emilien Dupont, Hyunjik Kim, S. M. Ali Eslami, Danilo Jimenez Rezende, and Dan Rosenbaum. From data to functa: Your data point is a function and you can treat it like one. In ICML, 2022a.\n\nDupont et al. [2022b] Emilien Dupont, Hrushikesh Loya, Milad Alizadeh, Adam Golinski, Yee Whye Teh, and Arnaud Doucet. COIN++: neural compression across modalities. Trans. Mach. Learn. Res., 2022b.\n\nFathony et al. [2021] Rizal Fathony, Anit Kumar Sahu, Devin Willmott, and J Zico Kolter. Multiplicative filter networks. In ICLR, 2021.\n\nFridovich-Keil et al. [2022] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In CVPR, 2022.\n\nGoodfellow et al. [2015] Ian J. Goodfellow, Oriol Vinyals, and Andrew M. Saxe. Qualitatively characterizing neural network optimization problems. In ICLR, 2015.\n\nGrattarola and Vandergheynst [2022] Daniele Grattarola and Pierre Vandergheynst. Generalised implicit neural representations. NeurIPS, 2022.\n\nGunasekar et al. [2017] Suriya Gunasekar, Blake Woodworth, Behnam Neyshabur Srinadh Bhojanapalli, and Nathan Srebro. Implicit regularization in matrix factorization. In NeurIPS, 2017.\n\nJiaxiang Tang [2021] Gang Zeng Jiaxiang Tang, Xiaokang Chen. Joint implicit image function for guided depth super-resolution. In ACM MM, 2021.\n\nKim et al. [2022] Subin Kim, Sihyun Yu, Jaeho Lee, and Jinwoo Shin. Scalable neural video representations with learnable positional features. In NeurIPS, 2022.\n\nKodak [1999] E. Kodak. Kodak dataset, 1999.\n\nLadune et al. [2022] ThÃ©o Ladune, Pierrick Philippe, FÃ©lix Henry, Gordon Clare, and Thomas Leguay. COOL-CHIC: Coordinate-based low complexity hierarchical image codec. arXiv preprint 2212.05458, 2022.\n\nLi et al. [2018] Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape of neural nets. In NeurIPS, 2018.\n\nLindell et al. [2022] David B. Lindell, Dave Van Veen, Jeong Joon Park, and Gordon Wetzstein. BACON: Band-limited coordinate networks for multiscale scene representation. In CVPR, 2022.\n\nLiu et al. [2023] Ke Liu, Feng Liu, Haishuai Wang, Ning Ma, Jiajun Bu, and Bo Han. Partition speeds up learning implicit neural representations based on exponential-increase hypothesis. In ICCV, pages 5474â€“5483, 2023.\n\nLiu et al. [2020] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. Neural sparse voxel fields. In NeurIPS. Curran Associates, Inc., 2020.\n\nMa et al. [2024] Rongkai Ma, Leo Lebrat, Rodrigo Santa Cruz, Gil Avraham, Yan Zuo, Clinton Fookes, and Olivier Salvado. Divide and conquer: Rethinking the training paradigm of neural radiance fields. arxiv preprint 2401.16144, 2024.\n\nMaennel et al. [2020] Hartmut Maennel, Ibrahim M. Alabdulmohsin, Ilya O. Tolstikhin, Robert Baldock, Olivier Bousquet, Sylvain Gelly, and Daniel Keysers. What do neural networks learn when trained with random labels? In NeurIPS, 2020.\n\nMartel et al. [2021] Julien N. P. Martel, David B. Lindell, Connor Z. Lin, Eric R. Chan, Marco Monteiro, and Gordon Wetzstein. ACORN: Adaptive coordinate networks for neural scene representation. ACM Transactions on Graphics, 40(4), 2021.\n\nMildenhall et al. [2020] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020.\n\nMÃ¼ller et al. [2022] Thomas MÃ¼ller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. ACM Transactions on Graphics, 41(4):102:1â€“102:15, 2022.\n\nNeyshabur et al. [2015] Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the role of implicit regularization in deep learning. In ICLR, 2015.\n\nRahaman et al. [2019] Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred Hamprecht, Yoshua Bengio, and Aaron Courville. On the spectral bias of neural networks. In ICML, 2019.\n\nRazin and Cohen [2020] Noam Razin and Nadav Cohen. Implicit regularization in deep learning may not be explainable by norms. In NeurIPS, 2020.\n\nSaragadam et al. [2023] Vishwanath Saragadam, Daniel LeJeune, Jasper Tan, Guha Balakrishnan, Ashok Veeraraghavan, and Richard G. Baraniuk. Wire: Wavelet implicit neural representations. In CVPR, 2023.\n\nSchwarz et al. [2023] Jonathan Schwarz, Jihoon Tack, Yee Whye Teh, Jaeho Lee, and Jinwoo Shin. Modality-agnostic variational compression of implicit neural representations. In ICML, 2023.\n\nShallue et al. [2019] Christopher J. Shallue, Jaehoon Lee, Joseph Antognini, Jascha Sohl-Dickstein, Roy Frostig, and George E. Dahl. Measuring the effects of data parallelism on neural network training. Journal of Machine Learning Research, 20(112):1â€“49, 2019.\n\nSitzmann et al. [2020a] Vincent Sitzmann, Eric Chan, Richard Tucker, Noah Snavely, and Gordon Wetzstein. MetaSDF: Meta-learning signed distance functions. In NeurIPS, 2020a.\n\nSitzmann et al. [2020b] Vincent Sitzmann, Julien N.P. Martel, Alexander W. Bergman, David B. Lindell, and Gordon Wetzstein. Implicit neural representations with periodic activation functions. In NeurIPS, 2020b.\n\nTakikawa et al. [2021] Towaki Takikawa, Joey Litalien, Kangxue Yin, Karsten Kreis, Charles Loop, Derek Nowrouzezahrai, Alec Jacobson, Morgan McGuire, and Sanja Fidler. Neural geometric level of detail: Real-time rendering with implicit 3d shapes. In CVPR, 2021.\n\nTancik et al. [2020] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. In NeurIPS, 2020.\n\nTancik et al. [2021] Matthew Tancik, Ben Mildenhall, Terrance Wang, Divi Schmidt, Pratul P. Srinivasan, Jonathan T. Barron, and Ren Ng. Learned initializations for optimizing coordinate-based neural representations. In CVPR, 2021.\n\nToderici et al. [2020] George Toderici, Wenzhe Shi, Radu Timofte, Lucas Theis, Johannes BallÃ©, Eirikur Agustsson, Nick Johnston, and Fabian Mentzer. Workshop and challenge on learned image compression (CLIC2020), 2020.\n\nValle-PÃ©rez et al. [2019] Guillermo Valle-PÃ©rez, Chico Q. Camargo, and Ard A. Louis. Deep learning generalizes because the parameter-function map is biased towards simple functions. In ICLR, 2019.\n\nXie et al. [2022] Yiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany, Shiqin Yan, Numair Khan, Federico Tombari, James Tompkin, Vincent Sitzmann, and Srinath Sridhar. Neural fields in visual computing and beyond. Comput. Graph. Forum, 2022.\n\nXu et al. [2022] Dejia Xu, Peihao Wang, Yifan Jiang, Zhiwen Fan, and Zhangyang Wang. Signal processing for implicit neural representations. In NeurIPS, 2022.\n\nYang and Salman [2019] Greg Yang and Hadi Salman. A fine-grained spectral perspective on neural networks. arXiv preprint 1907.10599, 2019.\n\nYaroslavsky [2015] L. P. Yaroslavsky. Compression, restoration, resampling, â€˜compressive sensingâ€™: Fast transforms in digital imaging. Journal of Optics, 2015.\n\nYu et al. [2021] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. PlenOctrees for real-time rendering of neural radiance fields. In ICCV, 2021.\n\nYÃ¼ce et al. [2022] Gizen YÃ¼ce, Guillermo Ortiz-JimÃ©nez, Beril Besbiar, and Pascal Frossard. A structured dictionary perspective on implicit neural representations. In CVPR, 2022.\n\nZhang et al. [2017] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In ICLR, 2017.\n\nZhang [2021] Kaiwei Zhang. Implicit neural representation learning for hyperspectral image super-resolution. IEEE Transactions on Geoscience and Remote Sensing, 2021.\n\n\\thetitle\n\nSupplementary Material\n\nContents\n\nAppendix S1 Wall-clock latency comparison\n\nWe primarily focus on evaluating the speedup through our proposed acceleration factor, which is derived from the number of SGD steps. In Tab. S5, we show whether this factor indeed leads to an acceleration of neural field training. To demonstrate this point empirically, we conduct a comparison between the average wall-clock runtimes of two key stages: data pre-processing and loading (referred to as â€œLoadâ€) and the actual SGD iterations (referred to as â€œTrainâ€) on original and ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP Kodak images. We find that the extra computation spent during the â€œLoadâ€ is much smaller in scale than the speedup from â€œTrainâ€, resulting in a net speedup. This corresponds to a time savings of 54543 ms, a whopping 25% time reduction, in the SIREN architecture, where SGD iterations make up the majority of the training.\n\nAppendix S2 Average number of steps for achieving target PSNR\n\nIn Tab. S4, we present the average number of steps required to achieve a 50dB Peak Signal-to-Noise Ratio (PSNR). We observe that this table does not exactly reflect the trends observed in Tab. 1 of the main paper. This discrepancy is due to the presence of outliers; some images that require much bigger number of steps than other images can dominate the overall statistic. For example, for Instant-NGP, the original Kodak#20 image requires 1279 number of steps until convergence, which is 7.96 times greater than other images on average. In such case, the average depends heavily on this sample; without Kodak#20, the average for original is 160.7 steps and for ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP is 111.0 steps. The acceleration factor, which we used for the main table, is relatively robust against this issue.\n\nNote. For some data augmentations, several augmented images could not have been fit with Instant-NGP to PSNR 50dB, with any of the learning rates that we tried. In particular, Gamma correction with Î³=2.0ğ›¾2.0\\gamma=2.0italic_Î³ = 2.0 cannot fit (1,2,1)121(1,2,1)( 1 , 2 , 1 ) images on Kodak, DIV2K, and CLIC datasets, respectively. Likewise, Gamma correction with Î³=0.5ğ›¾0.5\\gamma=0.5italic_Î³ = 0.5 and Linear Scaling with t=2.0ğ‘¡2.0t=2.0italic_t = 2.0 cannot fit (0,6,2)062(0,6,2)( 0 , 6 , 2 ) and (1,2,1)121(1,2,1)( 1 , 2 , 1 ) images in each datasets, respectively. For these cases, we report the average number of steps except on unconverged cases. Nevertheless, the average number of steps for PSNR 50dB on other data points for these augmentations are typically very large; thus it is very unlikely that these modified averages will lead to a misleading conclusion that such augmentations are beneficial for the training speed.\n\nAppendix S3 Extended discussions on the potentials and limitations of data transformations\n\nWhile ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP provides consistent speedups for fitting the given datum, it has limited applicability to scenarios that require the trained model to have certain characteristics, such as interpolatability (as briefly discussed Sec. 2.2). This section provides a more comprehensive discussion on this matter, to elucidate both the potential benefits and limitations.\n\nImage fitting is to represent the target 2D image with a corresponding neural field [34, 24]. Here, the main challenge is to overfit to a singular data instance with high fidelity, and our framework is well-suited for this application. By applying the same permutation matrix across all images, ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP can accelerate training without additional memory overhead (Appendix S4).\n\nImage superresolution [4, 14, 47] and image inpainting [41, 30] utilizes neural field trained on a set of seen coordinates to predict the signal value on unseen coordinates. These applications strongly rely on the ability of neural field to capture the spatial patterns of the target signal from the seen coordinates and interpolate them. However, ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP introduces a significant challenge in learning the meaningful implicit information in terms of spatial relationships. ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP can disrupt the neural fieldâ€™s understanding of locality and continuity, which is vital for both superresolution and inpainting. Other transformations that adjust only pixel intensities (e.g., standardization), retain the local structure of data, and can thus be used.\n\nData compression [6, 8, 17] reduces the required number of bits to store the data by representing the datum by a neural field with a small number of parameters. This idea, however, entails substantial computational resources for each datum during the encoding process, where we need to train a neural field. ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP is potentially very useful for reducing the encoding time, by accelerating the neural field training.\n\n3D scene reconstruction and novel view synthesis [25, 26] utilizes a neural field trained on a small number of 2D images of a same 3D scene to generate a novel view of the scene. For this, we heavily rely on the capability of the neural field to infer implicit relationships from all input data. Data transformations involving pixel relocations present a significant hurdle in this context, complicating or outright precluding the networks to predict the adequate relationship. Several concurrent works [20, 22] have explored a training paradigm which trains a single input with segmenting based on the inherent characteristics such as frequencies. Our hypothesis, â€œblessing of no patternâ€, can provide a novel insight within this discourse. We demonstrate that there exist representative patterns in data, and they work completely differently depending on the learning phase. From this point of view, the representative patterns can serve as a new criterion for partitioning an instance.\n\nInference with neural field weights [7, 5] regards the set of weight parameters derived from neural fields as a conventional input feature, based on which we perform inference. Data transformations can help constructing a large-scale dataset of neural field weights, which can be used to train a model that predicts based on the weight vectors.\n\nAppendix S4 Utilizing a single permutation matrix for RPP\n\nIn the main paper, we used an independently drawn permutation matrix for each ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP image. Despite the interesting results, this hinders applying ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP to further applications due to its complete randomness and memory footprint of the matrix.\n\nWe find that ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP can accelerate the neural field training even with a single unified permutation matrix. In Tab. S6, we compare the acceleration factors of ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP using the independently drawn random permutation (per image) and using the same permutation, on the Kodak dataset. We observe that the quantities are roughly identical to each other both in two architectures.\n\nAppendix S5 DCT coefficient of other datasets\n\nIn Fig. 3 of the main text, we have compared the average Discrete Cosine Transform (DCT) coefficients of original images and ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP images on the Kodak dataset. Figure 3 demonstrates a discernible pattern: in the original images, we observe that the upper left region (low-frequency components) has much higher coefficients than other regions, whereas in ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP images the scale of the coefficients are relatively uniform. We extend our analysis to other datasets, DIV2K and CLIC, to validate the consistency of our initial findings. The results from these datasets (Figs. S9 and S10, respectively) mirror the observations on the Kodak images. In the original images from DIV2K and CLIC, we once again observed a prevalence of low-frequency components, particularly in the upper left region of the frequency spectra. The distribution is uniform on ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP images.\n\nAppendix S6 Full PSNR curves on the Kodak dataset\n\nFigures S11 and S12 show the PSNR curves of a total of 24 images in Kodak dataset. ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP images fit faster than original images on 15 out of 24 images (marked with red borders). Throughout all images, the ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP images quickly surge to PSNR 50dB at a later stage in the majority of cases. In contrast, the original images show fluctuating PSNR values, which mostly hover above a moderate level, i.e. 30dB.\n\nAppendix S7 Reconstructed images in Instant-NGP\n\nWe present an extended analysis of the reconstructed images using Instant-NGP at a higher target PSNR value of 30dB. While the increase in PSNR typically correlates with enhanced image fidelity, our observations reveal a subtle, yet noteworthy, persistence of artifacts. These artifacts, manifesting as horizontal and vertical lines, are similar in nature to those observed at the lower PSNR of 20dB Fig. 8, albeit less pronounced. Figure S13 illustrates these findings, showcasing the reconstructed images at PSNR 30dB. We hypothesize that these artifacts are intrinsically linked to the spatial grid encoding of the Instant-NGP model, a pattern consistent with our earlier observations at the lower PSNR threshold.\n\nAppendix S8 Additional loss landscapes\n\nS8.1 Landscapes on other images\n\nWe show the loss landscapes for the first two images from Kodak, DIV2K, and CLIC. As shown in Figs. 5 and 6, we select a direction vector between two parameters and, the other direction randomly. In Figs. S14, S15, S16, S17, S18 and S19, (a) and (b) illustrate the loss landscapes during the early phase (i.e. initial point to 30dB), while (c) and (d) correspond to the late phase (i.e. 30dB to 50dB). In the early phase, as illustrated in the most of the figures, the loss landscape of the original image shows a smoother and more navigable trajectory from the initial point to a PSNR of 30dB than the ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP. We also typically observe a \"linear expresswayâ€™ in all the loss landscapes of the ğšğ™¿ğ™¿ğšğ™¿ğ™¿\\mathtt{RPP}typewriter_RPP images, which is detailed in our main paper.\n\nS8.2 Projections on different direction vectors\n\nFigure S20 shows the loss landscapes for the Kodak#08 image, which is already used in Figs. 5 and 6. We keep the direction between two parameters, yet in this instance, another direction is chosen differently at random. Despite the difference in random directions, the overall shapes of the loss landscapes exhibit a similarity due to the insignificance of random vectors in a high-dimensional space.\n\nFurthermore, we provide the loss landscapes again for the Kodak#08 image, opting not to select the direction randomly this time. Instead, we use the top-1 eigenvector, corresponding to the largest eigenvalue of the Hessian matrix of the loss function, rather than the random vector. Figures S21 and S22 presents these loss landscapes, offering both elevated and side views as in Figs. 5 and 6, respectively."
    }
}
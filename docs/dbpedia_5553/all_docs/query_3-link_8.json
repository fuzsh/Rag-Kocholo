{
    "id": "dbpedia_5553_3",
    "rank": 8,
    "data": {
        "url": "https://arxiv.org/html/2407.05694v1",
        "read_more_link": "",
        "language": "en",
        "title": "On the Limitations of Compute Thresholds as a Governance Strategy.",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://arxiv.org/html/extracted/5713855/images/The_Plague_Belgium.jpg",
            "https://arxiv.org/html/extracted/5713855/images/Great_Fire_London.jpg",
            "https://arxiv.org/html/extracted/5713855/images/1rjq425hb8501.jpg",
            "https://arxiv.org/html/x1.png",
            "https://arxiv.org/html/x2.png",
            "https://arxiv.org/html/extracted/5713855/images/computers_labatory.jpg",
            "https://arxiv.org/html/extracted/5713855/images/1985_02_BYTE_10-02_Computing_and_the_Sciences_0000.jpg",
            "https://arxiv.org/html/x3.png",
            "https://arxiv.org/html/x4.png",
            "https://arxiv.org/html/x5.png",
            "https://arxiv.org/html/x6.png",
            "https://arxiv.org/html/x7.png",
            "https://arxiv.org/html/x8.png",
            "https://arxiv.org/html/x9.png",
            "https://arxiv.org/html/x10.png",
            "https://arxiv.org/html/x11.png",
            "https://arxiv.org/html/x12.png",
            "https://arxiv.org/html/x13.png",
            "https://arxiv.org/html/x14.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "name=Sara Hooker\n\nAbstract\n\nAt face value, this essay is about understanding a fairly esoteric governance tool called compute thresholds. However, in order to grapple with whether these thresholds will achieve anything, we must first understand how they came to be. This requires engaging with a decades-old debate at the heart of computer science progress, namely, is “bigger always better?” Hence, this essay may be of interest not only to policymakers and the wider public but also to computer scientists interested in understanding the role of compute in unlocking breakthroughs. Does a certain inflection point of compute result in changes to the risk profile of a model? This discussion is increasingly urgent given the wide adoption of governance approaches that suggest greater compute equates with higher propensity for harm. Several leading frontier AI companies have released responsible scaling policies. Both the White House Executive Orders on AI Safety (EO) and the EU AI Act encode the use of FLOP or “floating-point operations” as a way to identify more powerful systems. What is striking about the choice of compute thresholds to-date is that no models currently deployed in the wild fulfill the current criteria set by the EO. This implies that the emphasis is often not on auditing the risks and harms incurred by currently deployed models – but rather is based upon the belief that future levels of compute will introduce unforeseen new risks. A key conclusion of this essay is that compute thresholds as currently implemented are shortsighted and likely to fail to mitigate risk. Governance that is overly reliant on compute fails to understand that the relationship between compute and risk is highly uncertain and rapidly changing. It also overestimates our ability to predict what abilities emerge at different scales. This essay ends with recommendations for a better way forward.\n\n1 Understanding Risk\n\nIt’s hard to predict — especially the future.\n\nNiels Bohr\n\nInherent to the human experience is our desire to limit risk. We avoid walking down dark streets at night; we wear sunscreen to reduce the risk of skin damage; we use seatbelts when driving. Seeking to proactively control risk is one of the key differentiators of modern society. As the historian Peter Bernstein said, “The ability to define what may happen in the future and to choose among alternatives lies at the heart of contemporary societies.”\n\nRisk is a particularly challenging concept to formulate an effective governance response to, because it requires both 1) a successful estimate of the level and origins of risk to society and 2) aligning on a proportionate response. History is replete with examples where one or both requirements fail. For example, the large human toll incurred by the black death is a good example of the difficulty of estimating what vectors amplify risk, where inadequate medical knowledge in the 1300s led to a failure to identify rats as one of the main carriers of the disease (Benedictow, 2004). In other cases, the risk is well known yet the response is inadequate. In 1966, the famous London fire swept through the city and devastated over half of all buildings. This risk was well known by authorities, as London had experienced several major fires before 1666. However, hesitation from authorities to act quickly to contain the blaze doomed the city (Peter, 2002).\n\nFew areas pose as significant a headache to policymakers as new technological breakthroughs. The historian Arthur Schlesinger aptly said, “Science and technology revolutionize our lives, but memory, tradition and myth frame our response.” Arthur’s point is that new technology must interact with the social fabric of our past and present, and be shaped by our humanity. Policymakers are often the first to grapple with what this means in practice. Here, the two-pronged objective of estimating and mitigating risks introduced by new technology is particularly tricky because breakthroughs are by definition hard to predict, so our response is almost always reactionary.\n\nThis is almost certainly true for Generative AI, where a combination of deep neural networks, transformers, and ever-larger amounts of compute and data have changed overnight the realm of what is possible. Most language models prior to 2017 were focused on mastering narrow tasks that tested whether a model could learn linguistic properties such as logic or entailment (Wang et al., 2019; Winograd, 1980). These models couldn’t generate long, fluid sequences and were rarely used outside of the realm of research conferences. In contrast, we now have machines that produce text indistinguishable from that produced by humans. Our current models can produce usable code, reason about the steps involved in solving a math problem, amuse humans with creativity, and accelerate productivity.\n\nWith more powerful tools comes more possibility for misuse. This includes known harms including propensity for hallucinations (HAI, 2023; Economist, 2023; Kossen et al., 2024), disinformation and misinformation (Zhou et al., 2023; Zellers et al., 2019; Goldstein et al., 2023; Musser, 2023; Buchanan et al., 2021), bias (Wiggers, 2023; Hao, 2024) and toxicity (Pozzobon et al., 2023a; Üstün et al., 2024; Gehman et al., 2020). However, it also includes unknown risks incurred by further developing this technology, with researchers concerned by national security risks like biorisk (AISI, 2024; Mouton et al., 2024; OpenAI, 2024), cybersecurity threats (NCSC, 2024; Barrett et al., 2024; Fang et al., 2024a; Lohn & Musser, 2022) and loss of control (UK Government, 2021). Partly, the difficulty we face is how to balance this portfolio of risks and how to allocate limited resources between mitigation of both present and future possible harms.\n\nA surprisingly popular approach to target and mitigate risk has been to equate the amount of compute used to train a model with its propensity for harm. The implication that scale is a key lever for estimating risk pervades frameworks like responsible scaling policies released by key industry players like Anthropic (Anthropic, 2023) and Open AI (OpenAI, 2023). It is also core to the motivation of compute thresholds which have influenced some of the first national and transnational policy governing Generative AI systems such as the White House Executive Order (The White House, 2023) (EO) and the EU AI Act (European Union, 2024) as well as ongoing legislation in China (Linghan et al., 2024), California (Senate, 2024) and Bills focused on export controls (on Foreign Affairs, 2024; Reuters, 2024). Both the White House Executive Order and the EU AI Act differentiate models into different tiers of risk based upon a hard coded threshold; models above the threshold are considered more risky and require additional reporting steps and scrutiny. Namely, both use a static total number of FLOP or floating-point operations to identify highly performant systems that require additional scrutiny. For the White House Executive Order this is set as any model that was trained using a quantity of computing power greater than 1026superscript102610^{26}10 start_POSTSUPERSCRIPT 26 end_POSTSUPERSCRIPT integer or floating-point operations, whereas in the EU AI Act a more stringent threshold is chosen as any model trained with more than 1025superscript102510^{25}10 start_POSTSUPERSCRIPT 25 end_POSTSUPERSCRIPT FLOP.\n\nIn this essay, we will ask what at first glance is a series of straightforward questions: 1) is compute as measured by FLOP a meaningful metric to estimate model risk? and 2) are hard-coded thresholds an effective response to mitigate this risk? A key conclusion of this work is that compute thresholds as currently implemented are shortsighted and likely to fail to mitigate risk. Governance that relies on compute fails to understand that the relationship between compute and risk is highly uncertain and rapidly changing. We are observing a bifurcation in compute trends. On the one hand, at least in the short term systems are likely to continue to get bigger. On the other hand, the relationship between compute and performance is increasingly strained and hard to predict (Niu et al., 2024). While the trend over the last 10 years involves more and more compute, a clear counter-trend has emerged with smaller models showcasing extremely high levels of performance.\n\nThere is not a clear justification for any of the compute thresholds proposed to date. Indeed, the choice of 1026superscript102610^{26}10 start_POSTSUPERSCRIPT 26 end_POSTSUPERSCRIPT and 1025superscript102510^{25}10 start_POSTSUPERSCRIPT 25 end_POSTSUPERSCRIPT rather than a number smaller or larger has not been justified in any of the policies implementing compute thresholds as a governance strategy. We do know that model scale amplifies certain risks – larger models tend to produce more toxic text and harmful associations (Birhane et al., 2023) and increases privacy risk because the propensity to memorize rare artifacts can increase the likelihood of data leakage (Panda et al., 2024; Kandpal et al., 2022; Carlini et al., 2023). However, these relationships hold in compute settings far below 1025superscript102510^{25}10 start_POSTSUPERSCRIPT 25 end_POSTSUPERSCRIPT or 1026superscript102610^{26}10 start_POSTSUPERSCRIPT 26 end_POSTSUPERSCRIPT FLOP and are present in many models far smaller than the current threshold. What is striking about the choice of compute thresholds to date is that many are examples of precautionary policy (Ricci & Zhang, 2011) – no models currently deployed in the wild fulfill the current criteria set by US Executive order. Only a handful of models will be impacted by the EU AI Act when it comes into effect (Epoch AI, 2023). This implies that the emphasis is not on auditing the risks incurred by currently deployed models in the wild but rather is based upon the belief that future levels of compute will introduce unforeseen new risks that demand a higher level of scrutiny. Across this essay, several recommendations will emerge from our deep dive into the relationship between compute and risk:\n\n1.\n\nThe relationship between compute and risk is rapidly changing While the last decade has involved ever larger amounts of compute, increasingly smaller models are more performant due to optimization which happens outside of traditional training. Training compute fails to account for “inference-time compute” enhancements which can dramatically change risk profile of the model. In Section 2 we explore what is known about the relationship between compute and performance and find that much of the gains in risk over the last few years can be attributed to optimization strategies and high quality data, rather than pure FLOP. Year over year, smaller models are showcasing extremely high levels of performance.\n\n2.\n\nEvidence to-date suggests we are not good at predicting what abilities emerge at different scales. The choice of where compute thresholds are set will have far-ranging implications – too low and too many models will be selected for additional scrutiny and reporting each year. In contrast, if it is set too high, not enough models will subject to reporting requirements, and the threshold risks become decorative rather than a meaningful indicator of risk. In Section 4 we take stock of our track record predicting performance at different levels of compute and find that our track record to date is wanting. Put simply, we are not good at predicting the relationship between scale and downstream metrics. Despite considerable effort and a large body of literature, our ability to predict the emergence of specific downstream capabilities with scale remains elusive (Schaeffer et al., 2024a). This calls into question the viability of any choice of training compute threshold – it is hard to tell if we have set the number of FLOP correctly.\n\n3.\n\nFLOP has to be better specified as a metric to be meaningful. Existing policies do not specify key details around FLOP measurement that are necessary to ensure fair reporting. In Section 3, we show how an under-specified threshold on FLOP presents many loopholes that are easy to exploit. As currently detailed, the lack of specification is a lesson in Goodhart’s Law: “When a measure becomes a target, it ceases to be a good measure.” Preventing FLOP from becoming merely decorative requires clear and consistent guidance across jurisdictions. Using compute thresholds should be done with caution, and having clear understanding of the limitations and standardized reporting is critical for avoiding manipulation of the metric.\n\n4.\n\nGovernments should be transparent about what risks they are concerned about and where they are allocating limited resources. Current compute thresholds do not apply to almost all models currently deployed in the wild. However, currently deployed models present considerable risk. Governments should articulate what future risks motivate a focus on forward-looking scrutiny. There is currently a severe shortage of technical staff with AI experience within government (Zakrzewski, 2024; Aitken et al., 2022; Engstrom et al., 2020) and capacity issues which might limit the ability of governments to implement effective policies (Marchant, 2011; Reuel et al., 2024). With limited resources, it is even more paramount that governance goals are transparent with the public. Without being explicit about the risks compute thresholds hope to mitigate, it is hard to weigh the likelihood of successful mitigation.\n\n5.\n\nApplying hard coded thresholds to a quickly changing distribution is likely to fail. We show throughout this essay that one of the most misbehaved and rapidly changing distributions is the relationship between compute and performance. When a data distribution is rapidly changing, it is risky to use a hard-coded threshold precisely because it is hard to know exactly where to place it. In Section 5.1, we recommend instead using a dynamic threshold which automatically self-adjusts to a percentile of the distribution of model properties released that year. We also recommend moving away from using compute as a sole indicate to tier models, and instead using a risk index composed of several measures of performance. This avoids putting all eggs in one basket.\n\nTo first understand how thresholds came to be, we need to delve into a decades-old debate at the heart of compute science progress, namely, is scaling always better. For the last decade, computer science progress has been caught by our own Moore’s law (Schaller, 1997) of a painfully simple formula for innovation by adding more model parameters and data. Yet, this essay will posit it is far from clear that future innovation or indeed amplified levels of risk will come from compute alone. As we will see in the next section, the relationship between compute and performance is far from straightforward and far from settled. Compute is changing rapidly, as fast as the technology that it serves.\n\n2 The Uncertain Relationship Between Compute and Risk.\n\n“Well Babbage what are you dreaming about?” to which I replied, “I am thinking that all these tables might be calculated by machinery.”\n\nCharles Babbage\n\nMany inventions are re-purposed for means unintended by their designers. Initially, the magnetron tube was developed for radar technology during World War II. In 1945, a self-taught American engineer, Percy Spencer, noticed that a chocolate bar melted in his pocket whenever he was close to a radar set. This innocuous discovery resulted in the patent for the first microwave (Zhang, 2017). In a similar vein, deep neural networks only began to work when an existing technology was unexpectedly re-purposed. A graphical processing unit (GPU) was originally introduced in the 1970s as a specialized accelerator for video games and for developing graphics for movies and animation. In the 2000s, like the magnetron tube, GPUs were re-purposed for an entirely unimagined use case – to train deep neural networks (Chellapilla et al., 2006; Hooker, 2021; Oh & Jung, 2004; Payne et al., 2005). GPUs had one critical advantage over CPUs - they were far better at parallelizing matrix multiples (Brodtkorb et al., 2013; Dettmers, 2023), a mathemetical operation which dominates the definition of deep neural network layers (Fawzi et al., 2022; Davies et al., 2024). This higher number of floating operation points per second (FLOP/s) combined with the clever distribution of training between GPUs unblocked the training of deeper networks. The depth of the network turned out to be critical. Performance on ImageNet jumped with ever deeper networks in 2011 (Ciresan et al., 2011), 2012 (Krizhevsky et al., 2012) and 2015 (Szegedy et al., 2014). A striking example of this jump in compute is a comparison of the now famous 2012 Google paper which used 16,000 CPU cores to classify cats (Le et al., 2012) to a paper published a mere year later that solved the same task with only two CPU cores and four GPUs (Coates et al., 2013).\n\nThis would ignite a rush for compute which has led to a bigger-is-better race in the number of model parameters over the last decade (Canziani et al., 2016; Strubell et al., 2019b; Rae et al., 2021; Raffel et al., 2020; Bommasani et al., 2021; Bender et al., 2021). The computer scientist Ken Thompson famously said “When in doubt, use brute force.” This was formalized as the “bitter lesson” by Rich Sutton who posited that computer science history tells us that throwing more compute at a problem has consistently outperformed all attempts to leverage human knowledge of a domain to teach a model (Sutton, 2019). In a punch to the ego of every computer scientist out there, what Sutton is saying is that symbolic methods that codify human knowledge have not worked as well as letting a model learn patterns for itself coupled with ever-vaster amounts of compute.\n\nIs Sutton right? Certainly, he is correct that scaling has been a widely favored formula because it has provided persuasive gains in overall performance – size is the most de-risked tool we have to unlock new gains. As the computer scientist Michael Jordan quipped “Today we can’t think without holding a piece of metal.” Increasing compute also conveniently fits into the cadence of quarterly industry planning, it is less risky to propose training a bigger model than it is to propose an alternative optimization technique. However, relying on compute alone misses a critical shift that is underway in the relationship between compute and performance. It is not always the case that bigger models result in better performance. The bitter lesson doesn’t explain why Falcon 180B (Almazrouei et al., 2023) is easily outperformed by far smaller open weights models such as Llama-3 8B (AI@Meta, 2024), Command R 35B (Cohere & Team, 2024), Gemma 27B (Team, 2024). It also doesn’t explain why Aya 23 8B (Aryabumi et al., 2024) easily outperforms BLOOM 176 B (Workshop et al., 2023) despite having only 4.5% of the parameters.\n\nThese are not isolated examples, but rather indicative of an overall trend where there is no guarantee larger models consistently outperform smaller models. Figure 3(b) plots the scores of models submitted to the Open LLM Leaderboard over the last two years. Here, we plot large models with more than 13 billion parameters whose leaderboard score is less than the top performing small model with less than 13 billion parameters. We observe that over time, more and more large models have been submitted that are outperformed by the best small model daily submission. To understand why this is the case, we must understand what key variables have been driving gains in performance over the last decade. In an era where there are diminishing returns for the amount of compute available (Lohn & Musser, 2022; Thompson et al., 2020), optimization and architecture breakthroughs define the rate of return for a given unit of compute. It is this rate of return which is most critical to the pace of progress and to the level of risk incurred by additional compute.\n\n2.1 A shift in the relationship between compute and performance\n\nThe world has changed less since Jesus Christ than it has in the last 30 years.\n\nCharles Peguy, 1913\n\nIn complex systems, it is challenging to manipulate one variable in isolation and foresee all implications. Throughout the 20th century doctors recommended removing tonsils in response to any swelling or infection, but research has recently shown the removal may lead to higher incidence of throat cancer (Liang et al., 2023). Early televised drug prevention advertisements in the 2000s led to increased drug use (Terry-McElrath et al., 2011). In a similar vein, the belief that more compute equates with more risk belies a far more complex picture that requires re-examining the relationship between performance and compute. A key limitation of simply throwing more scale at a task is that the relationship between additional compute and generalization remains poorly understood. A growing body of research suggests that the relationship between compute and performance is far more complex. Empirical evidence suggests that small models are rapidly becoming more performant and riskier.\n\nData quality reduces reliance on compute. Models trained on better data do not require as much compute. A large body of work has emerged which shows that efforts to better curate training corpus, including de-duping (Taylor et al., 2022; Kocetkov et al., 2022), data pruning (Marion et al., 2023; Singh et al., 2024a; Sorscher et al., 2023; Albalak et al., 2024; Tirumala et al., 2023; Chimoto et al., 2024) or data prioritization (Boubdir et al., 2023; Thakkar et al., 2023) can compensate for more weights. This suggests that the number of learnable parameters is not definitively the constraint on improving performance; investments in better data quality mitigate the need for more weights (Singh et al., 2024a; Penedo et al., 2023; Raffel et al., 2020; Lee et al., 2022). If the size of a training dataset can be reduced without impacting performance (Marion et al., 2023), training time is reduced. This directly impacts the number of training FLOP and means less compute is needed.\n\nOptimization breakthroughs compensate for compute. Progress over the last few years has been as much due to optimization improvements as it has been due to compute. This includes extending pre-training with instruction finetuning to teach models instruction following (Singh et al., 2024b), model distillation using synthetic data from larger more performant \"teachers\" to train highly capable, smaller \"students\" (Team et al., 2024b; Aryabumi et al., 2024), chain-of-thought reasoning (Wei et al., 2023; Hsieh et al., 2023), increased context-length (Xiong et al., 2023), enabled tool-use (Qin et al., 2023; Wang et al., 2023a), retrieval augmented generation (Pozzobon et al., 2023b; Lewis et al., 2020), and preference training to align models with human feedback (Dang et al., 2024; Ahmadian et al., 2024; Ouyang et al., 2022; Bai et al., 2022; Lee et al., 2023; Tunstall et al., 2023; Khalifa et al., 2021; Rafailov et al., 2023; Azar et al., 2023). All these techniques compensate for the need for weights or expensive prolonged training (Ho et al., 2024b). All things equal, these have been shown to dramatically improve model performance relative to a model trained without these optimization tricks given the same level of compute (Davidson et al., 2023; Hernandez & Brown, 2020; Erdil & Besiroglu, 2023; METR Team, ; Liu et al., 2024). In Figure 3(a), we plot the best daily 13B or smaller model submitted to the Open LLM Leaderboard over time. In a mere span of 2 years, the best-performing daily scores from small model went from an average of 38.59% across to an average of 77.15% across 2024 submissions. The takeaway is clear – smaller models with the same amount of capacity are becoming more and more performant.\n\nArchitecture plays a significant role in determining scalability The introduction of a new architecture design can fundamentally change the relationship between compute and performance (Tay et al., 2022; Sevilla et al., 2022a; Ho et al., 2024a) and render any compute threshold that is set irrelevant. For example, the key breakthroughs in AI adoption around the world were the introduction of architectures like convolutional neural networks (CNNs) for vision (Ciresan et al., 2011; Krizhevsky et al., 2012; Szegedy et al., 2014) and Transformers for language modeling (Vaswani et al., 2023).\n\nWhile deep neural networks represent a huge step forward in performance for a given level of compute, what is often missed is that our architectures also represent the ceiling in what is achievable through scaling. While progress has revolved around deep neural networks for the last decade, there is much to suggest that the next significant gain in efficiency will require an entirely different architecture. Deep neural networks remain very inefficient as an algorithm. Our typical training regimes require that all examples are shown the same number of times during the training (Xue et al., 2023). All modern networks are trained based upon minimization of average error (Goodfellow et al., 2016). This means that learning rare artifacts requires far more training time or capacity due to the diluted signal of infrequent attributes relative to the most frequent patterns in the dataset (Achille et al., 2017; Jiang et al., 2020; Mangalam & Prabhu, 2019; Faghri et al., 2020; Frankle et al., 2020; Arpit et al., 2017). Small models are already good at learning the most frequent features, and most easy features and common patterns are learned early on training with much harder rare features learned in later stages (Agarwal & Hooker, 2020; Paul et al., 2021; Mangalam & Prabhu, 2019; Siddiqui et al., 2022; Abbe et al., 2021). When we radically scale the size of a model, we show the most gains in performance on are rare and underrepresented attributes in the dataset – the long-tail (Hooker et al., 2019, 2020). Put differently, scaling is being used to inefficiently learn a very small fraction of the overall training dataset. Our reliance on global updates also results in catastrophic forgetting, where performance deteriorates on the original task because the new information interferes with previously learned behavior (Mcclelland et al., 1995; Pozzobon et al., 2023b). All this suggests that our current architecture choices are probably not final and key disruptions lie ahead. This is likely to radically change any scaling relationships, in the same way it has done in the last decade. For example, it is unlikely any prediction of how compute scales based upon architectures before deep neural networks holds true post-2012 after the introduction of convolutional neural networks.\n\n3 Avoiding a FLOP FLOP\n\nAny statistical relationship will break down when used for policy purposes.\n\nJon Danielsson\n\nAre FLOP a reliable proxy for overall compute? Even if the relationship between compute and generalization were stable – there are difficulties operationalizing FLOP as a metric. FLOP (Goldberg, 1991) refers to floating-point operations, and has a fairly straightforward definition: sum up all the math operations in floating point (such as addition, subtraction, multiplication, and division). In the 1950s and 1960s, as computers were becoming more prevalent, the need for a standard measure of performance arose. FLOP are particularly useful in fields that require floating-point calculations, such as scientific computations, advanced analytics, and 3D graphics processing. This is because all these areas are dominated by simple primitive mathematical operations – for example, FLOP tend to be closely associated with the size of models because deep neural network layers are dominated by a single operation – matrix multiplies – which can be decomposed into a set of floating point operations (Fawzi et al., 2022; Davies et al., 2024).\n\nWe first begin by noting there are some reasons FLOP are attractive as a policy measure. The primary one is that FLOP provides a standardized way to compare across different hardware and software stacks. FLOP counts don’t change across hardware – the number of mathematical operations is the same no matter what hardware you train a model on. In a world where hardware is increasingly heterogeneous (Hooker, 2021) and it is hard to replicate the exact training setting due to a lack of software portability (Mince et al., 2023), it is attractive to use a metric that doesn’t depend on replicating exact infrastructure. It also neatly sidesteps reporting issues that could occur if relying only on the number of hardware devices used to train a model. The rapidly increasing performance of new hardware generations (Hobbhahn et al., 2023), as well as engineering investments in training infrastructure (Yoo et al., 2022; Lepikhin et al., 2020), mean that over time much larger models will be trained using the same number of devices. FLOP is also a metric which could potentially be inferred by cloud providers. Given most machine learning workloads are run by a few key cloud providers, this may make administering such a measure effectively easier (Heim et al., 2024).\n\nA key conundrum posed by FLOP thresholds is that policymakers are using FLOP as a proxy for risk, but FLOP doesn’t say anything about end performance of a model — only about the number of operations applied to the data. For example, if you compare two models trained for the same number of FLOP but one has had safety alignment during post-training (Aakanksha et al., 2024; Bai et al., 2022) and the other has none – these two models will still be accorded the same level of risk according to number of FLOP but one will present a far lower risk to society because of safety alignment.\n\nAnother key hurdle governance which adopts compute threshold will have to overcome is the lack of clear guidance in all the policy to-date about how FLOP will actually be measured in practice. This ambiguity risks FLOP as a metric being irrelevant or at the very least easy to manipulate. Developing principled standards for measuring any metric of interest is essential for ensuring that safety measures are applied in a proportionate and appropriate way. In the followings Section, we specify some of the key ways in which it is easy to manipulate FLOP if it is left underspecified as a metric.\n\n3.1 Challenges of using FLOP as a metric\n\nIf you cannot measure it, you cannot improve it.\n\nLord Kelvin\n\nTraining FLOP doesn’t account for post-training leaps in performance Applying scrutiny and regulation based upon training FLOP ignores that a lot of compute can be spent outside of training to improve performance of a model. This can be grouped under “inference-time compute” and can result in large performance gains that dramatically increase the risk profile of a model. The limited work to-date which has evaluated a subset of ‘inference-time compute” improvements estimates these can impart gains between 5x and 20x of base level post-training performance (Davidson et al., 2023).“Inference-time compute” includes best-of-n sampling techniques (Team et al., 2024a), chain-of-thought reasoning (Wei et al., 2023; Hsieh et al., 2023; Wang et al., 2023c) and model distillation using synthetic data (Aryabumi et al., 2024; Shimabucoro et al., 2024; Üstün et al., 2024; Team et al., 2024a). All these techniques require more compute at test-time because of the need to perform more forward passes of the model to generate additional samples. However, these are not reflected in training time costs and indeed can often reduce the compute needed during training. For example, smaller, more performant models are often trained on smaller amounts of synthetic data from a highly performant teacher (Villalobos & Atkinson, 2023; Huang et al., 2022). These improvements dramatically improve performance but are currently completely ignored by compute thresholds since they don’t contribute to training FLOP.\n\nIncreasing the context-length (Xiong et al., 2023) and retrieval augmented systems (Lee et al., 2024; Pozzobon et al., 2023b; Lewis et al., 2020) are additional examples of introducing additional computational overhead at test-time by increasing the number of tokens to process. Retrieval augmented models (RAG) have become a mainstay of state-of-art models yet are often introduced after training. Most RAG systems are critical for keeping models up-to-date with knowledge yet contribute minimal or no FLOP. Retrieval augmented models are particularly good at supplementing models with search capabilities or external knowledge, which can enhances risks which depend on up-to-date knowledge such as biorisk and cybersecurity threats.\n\nAdditionally increasing the context length often requires minimal FLOP but can dramatically increase performance of a model. Entire books can be passed in at test time dramatically improving model performance on specialized tasks (Gemini has 2M context window) (Xiong et al., 2023). This can make the number of FLOP irrelevant if sensitive biological data can be passed at inference time in a long-context window.\n\nDifficulty Tracking FLOP across model lifecycle. Increasingly, training a model falls into distinct stages that all confer different properties. For example, unsupervised pre-training dominates compute costs because the volume of data is typically in the trillions of tokens (Cottier, 2023; Heim, 2023). Following this, there is instruction finetuning, which confers the model the ability to follow instructions (Singh et al., 2024a) and then preference training (Aakanksha et al., 2024; Ahmadian et al., 2024; Bai et al., 2022; Ouyang et al., 2022; Lee et al., 2023; Tunstall et al., 2023; Khalifa et al., 2021; Rafailov et al., 2023; Azar et al., 2023), which aligns model performance with human values. Between each of these steps models are often released publicly (Üstün et al., 2024; Touvron et al., 2023; Aryabumi et al., 2024), meaning that developers can take a model from a different developer and continue optimizing. The models with the most downloads on platforms like HuggingFace are base models which are most conducive for continued pre-training. As sharing of models at different stages of the life-cycle becomes more common, so will difficulties in tallying FLOP across the entire model life-cycle. Furthermore, it may simply be infeasible to trace federated, decentralized training of models where hardware often belongs to many different participants and training is conducted in a privacy-preserving manner (Don-Yehiya et al., 2023; Borzunov et al., 2023; Yuan et al., 2023; Qin et al., 2024).\n\nHow to handle Mixture of Experts (MoEs) and classic ensembling? MoEs (Zadouri et al., 2023; Shazeer et al., 2018; Riquelme et al., 2021; Du et al., 2022; Fedus et al., 2022; Tan et al., 2024) are examples of adaptive compute – where examples are routed to different parts of a model. This type of architecture can often provide powerful efficiency gains, as despite a much larger overall architecture, only a subset of weights are activated for a given example. Current policy frameworks do clearly not specify how to handle Mixture of Experts (MoEs), which constitute some of the most highly performant systems currently deployed, such as Mixtral (Jiang et al., 2024) and the Gemini family of models (Team et al., 2024a). However, this raises important questions – should the compute for each expert be counted towards total FLOP, or only the FLOP used to train the subset of experts that are active at inference time? Given final performance depends on all experts in an MoE, a recommendation should be to include all FLOP in the final consideration, but this is currently under-specified. It also raises the question of how to treat new hybrid techniques which train several specialized experts and then both average parameters and utilize routing (Sukhbaatar et al., 2024).\n\nClassical simple ensembling techniques dominate production systems in the real world (Ko et al., 2023; Li et al., 2024) and have been shown to heavily outperform a single model. Unlike MoEs which are jointly optimized or trained using a router, classic ensembles are often only combined at inference time using simple averaging of weights. Given the ensemble is never trained together, it is unclear whether FLOP should reflect the compute of the single final model or the sum of all the training compute across models that were averaged. If it only reflects the FLOP of the final model, this may underestimate risk given ensembling is known to improve performance.\n\nFLOP only accounts for a single model, but does not capture risk of the overall system. The emphasis on compute thresholds as an indicator of risk also implies that risk is the property of a single model rather than the system in which it is deployed. In the real-world, impact and risk are rarely attributable to a single model but are a facet of the entire system a model sits in and the way it interacts with its environment (Zaharia et al., 2024; Sculley et al., 2015; Jatho et al., 2023; Raji et al., 2020). Many real-world production systems are made up of cascading models where the final output is produced as a results of inputs being processed by multiple algorithms in sequence (Paleyes et al., 2022; Forum, 2023; Sculley et al., 2015; Shankar et al., 2022). There has yet to be guidance on whether the FLOP threshold is specific to a single model or whether all models that constitute an end-to-end system contribute to the final tally. This has significant implications for model providers – a cascade system is often made up of models which are not individually very powerful or risky – yet the overall system may exceed the FLOP threshold.\n\nThere is also no specification as to how to treat model agents which may interact with both each other and/or use tools. End performance of the agents is undoubtedly due to the interactions with other agents and access to tools (Li et al., 2024), yet is unlikely to be considered a single model. It has already been shown that models which are enabled with tool use, or can interact with a wider environment outperform a single model on its own (Wang et al., 2023b; Anwar et al., 2024a; Mialon et al., 2023). These are far from edge cases; the reality is that most technology deployed in the wild is rarely just an algorithm is isolation. Typically, interdependent models feed into a user experience and interact with a set of choices about design and delivery that impact the overall level of risk.\n\nFLOP varies dramatically for different modalities. In Figure 6(b), we plot the FLOP requirements over time of models grouped according to modality and downstream use case (model FLOP data from Epoch AI (2024)). It is easy to observe that the compute requirements have not increased at the same rate across modalities. For example, code models typically require less compute (Lin et al., 2024b), as do biological models (Maug et al., 2024). Multilingual models (Üstün et al., 2024; Aryabumi et al., 2024) tend to require more compute for each additional language covered. This is often referred to as the curse of multilinguality (Üstün et al., 2024; Arivazhagan et al., 2019; Conneau et al., 2019; Pfeiffer et al., 2022), where capacity is split between more languages such that performance on any given language suffers relative to a monolingual (single language) model of the same size. These differing compute needs mean that a single threshold may penalize some types of models and reward others. For example, thresholds may penalize multilingual models that attempt to serve many languages and improve access to technology (Üstün et al., 2024; Aryabumi et al., 2024).\n\nOne way to address differences in modalities is to maintain different compute thresholds for each modality. While at first glance this is an attractive solution, it also imposes more technical overhead on governments who must correctly set a hard-coded benchmark for each modality. For example, it is interesting to note that the US Executive Order already has at least one modality-specific caveat to the compute thresholds by carving out a separate compute threshold for biological models. It is set lower for models trained for biological sequence data at 1023superscript102310^{23}10 start_POSTSUPERSCRIPT 23 end_POSTSUPERSCRIPT. However, since the threshold was set, models like xTrimoPGLM (Chen et al., 2024) already exceed the biological threshold set at 1⁢e⁢231𝑒231e231 italic_e 23 operations by a factor of 6x (Maug et al., 2024). Many models (Lin et al., 2023; Elnaggar et al., 2020; Dalla-Torre et al., 2023) are currently within a factor of 10x the Executive Order’s reporting threshold (Maug et al., 2024). These models do not appear to present a decidedly different risk profile from previous generations, so if the goal of the thresholds is to be an inflection point for amplified risk it is unclear if it has been set successfully.\n\nSpecifying separate thresholds for different modalities also risks inviting gamification. For example, to avoid a lower threshold for scrutiny for biological models one loophole is to preserve biology specific training data at less than 50%. According to current guidance the model would no-longer qualify as a “biological” model and would only be subject to the higher general purpose compute thresholds. Galactica-120B (Taylor et al., 2022) and Llama-molinst-protein-7b (Fang et al., 2024b) are both examples of models with capabilities for biological sequence modeling without primarily being trained on biological sequence data. Despite both presenting biological capabilities, neither is likely to be considered “biological” under the current Executive Order requirements (Maug et al., 2024). This highlights the fundamental tension of relying on compute alone – since it is not anchored to the risk metric that is of primary concern, it may be possible to sidestep in many creative ways while still presenting high-risk capabilities.\n\nIn Appendix A, we also present some more technical aspects of the difficulty of measuring FLOP in practice, such as the difference between theoretical and hardware FLOP, and how to handle difference in quantization. Developing principled standards for measuring FLOP is essential for ensuring that safety measures are applied in a proportionate and appropriate way.\n\n4 We are not very good at predicting the relationship between compute and risk\n\nIn theory, there is no difference between theory and practice. But, in practice, there is.\n\nWalter J. Savitch\n\nThe choice of where compute thresholds are set will have far-ranging implications – too low and too many models will be selected for additional auditing and benchmarking each year. In contrast, if it is set too high, not enough models will be audited for risk, and the threshold risks become decorative rather than a meaningful indicator of risk. None of the policies to date have provided justification about where they have set their thresholds, or why it excludes almost all models deployed in the wild today. In Section 2.1, we grappled with the changing overall relationship between compute and performance. However, scientific justification for a threshold requires predicting how downstream risk scales with additional compute. Indeed, ideally the choice of hard coded threshold reflects scientific consensus as to when particular risk factors are expected to emerge due to scale. Hence, it is worth considering our success to date in estimating how different model properties change with scale.\n\nWarren Buffet once said “Don’t ask the barber if you need a haircut.” In the same vein, don’t ask a computer scientist or economist whether you can predict the future. The temptation to say yes often overrides a necessary humility about what can and cannot be predicted accurately. One such area where hubris has overridden common sense is attempts to predict the relationship between scale and performance in the form of scaling laws (Kaplan et al., 2020; Hernandez et al., 2021; Dhariwal et al., 2021) which either try and predict how a model’s pre-training loss scales (Bowman, 2023) or how downstream properties emerge with scale. It is the latter task which is urgently needed by policymakers in order to anticipate the emergence of unsafe capabilities and inform restrictions (such as compute thresholds) at inflection points where risk increases with scale (Anthropic, 2023; OpenAI, 2023; Kaminski, 2023).\n\nOne of the biggest limitations of scaling laws is that they have only been shown to hold when predicting a model’s pre-training test loss (Bowman, 2023), which measures the model’s ability to correctly predict how an incomplete piece of text will be continued. Indeed, when actual performance on downstream tasks is used, the results are often murky or inconsistent (Ganguli et al., 2022; Schaeffer et al., 2023; Anwar et al., 2024b; Ganguli et al., 2022; Schaeffer et al., 2024b; Hu et al., 2024). Indeed, the term emerging properties is often used to describe this discrepancy (Wei et al., 2022; Srivastava et al., 2023): a property that appears “suddenly” as the complexity of the system increases and cannot be predicted. Emergent properties imply that scaling laws don’t hold when you try to predict downstream performance instead of predicting test loss for the next word token.\n\nEven when limited to predicting test loss, there have been issues with replicability of scaling results under slightly different assumptions about the distribution (Besiroglu et al., 2024; Anwar et al., 2024a). Research has also increasingly found that many downstream capabilities display irregular scaling curves (Srivastava et al., 2023) or non power-law scaling (Caballero et al., 2023). For complex systems that require projecting into the future, small errors end up accumulating due to time step dependencies being modelled. This makes accurate predictions of when risks will emerge inherently hard, which is compounded by the small samples sizes often available for analysis. each data point is a model, and computation cost means scaling “laws” are frequently based upon analysis of less than 100 data points (Ruan et al., 2024)). This means many reported power law relationships can lack statistical support and power (Stumpf & Porter, 2012).\n\nOne immediate recommendation is that the accuracy of scaling laws and predictions of emerging risk can be greatly improved by more guidance from policymakers about what range is of interest and specifying the risks that policymakers are concerned about (Stumpf & Porter, 2012). For example, there is a big difference between using scaling laws to optimize for the correct amount of training data in your next large-scale run versus attempting to extrapolate trends several orders of magnitude out. Typically, policy use cases demand high precision over a longer time horizon, which is exactly the type of extrapolation we are currently worst at. Specifying which risks are of interest will also benefit precision; scaling laws tend to have high variance in precision between tasks. For example, code-generation has shown fairly predictable power law scaling across 10 orders of magnitude of compute (Hu et al., 2024; Anwar et al., 2024b). However, other capabilities have been far shown to scale far more erratically (Srivastava et al., 2023; Caballero et al., 2023). Perhaps as important, policymakers should be aware that accurately predicting the impact of scaling is currently far from feasible. Hence, there is currently limited scientific support for using exact thresholds of compute alone to triage different risk levels.\n\n5 The Way Forward\n\n5.1 Moving Away from Hard Coded Compute Thresholds\n\nA measurement is not an absolute thing, but only relates one entity to another.\n\nH.T. Pledge\n\nCompute thresholds to date propose a single number (1026superscript102610^{26}10 start_POSTSUPERSCRIPT 26 end_POSTSUPERSCRIPT or 1025superscript102510^{25}10 start_POSTSUPERSCRIPT 25 end_POSTSUPERSCRIPT) to distinguish risky systems which merit more scrutiny. This hard-coding of a single threshold reflects a philosophy of absolutism, a legal and philosophical view that at least some truths in the relevant domain apply to all times, places or social and cultural frameworks. From a data-centric perspective, absolutism makes sense as a governance philosophy when the data distribution is well known and follows a predictable statistical pattern. For example, the use thresholds in medicine for classifying diabetes detection (Saudek et al., 2008) or for allocating additional care to infants based upon birth weight (Cutland et al., 2017; Seri & Evans, 2008). These hard-coded thresholds have stood the test of time because these data distributions tend to be well-behaved and predictable.\n\nIn your introduction to machine learning class, this type of bell-shaped distribution was introduced to you as a normal distribution. In Figure 8, we plot some very common examples of close to normal distributions found in the wild. Unlike other distributions, the normal distribution is well-behaved and remarkably symmetrical, with an equal number of outliers on each side. Normal distributions in the real world also tend to coincide with distributions that don’t change much over time. For example, the distribution of baby weights is unlikely to change tomorrow or even in the next 10 years. For these type of stable distributions where the data is well behaved hard thresholds make sense as a governance tool. The stability of these distributions make it easy to determine outliers and have confidence that a set threshold will have longevity and not have to change every year. There are successful examples of governments setting hard thresholds when they designate speed limits (US Department of Transportation, 2020) or limits for blood alcohol to determine drinking under the influence (World Health Organization, ).\n\nIn contrast, we know from Section 2.1 that one of the most misbehaved and rapidly changing distributions is the relationship between compute and performance. The plots in Figure 7 show that if we plot any proxy variable for compute – parameters, FLOP, training dataset size, training time – we are confronted with a distribution that is far from the perfect bell-shaped curve that characterize the kinds of problems that hard-coded thresholds are successfully applied to. Perhaps more dangerous, these non-normal distributions are also more likely to rapidly shift over time. For these distributions, applying a hard-coded threshold is a bad policy as there is a much higher likelihood that the threshold will be placed incorrectly. As quoted by the Mathematician David Orrell, Orthodox tools based on a normal distribution therefore fail exactly where they are most needed, at the extremes.\n\nCompute thresholds could be much improved by moving to dynamic instread of static thresholds An unpredictable relationship between compute and performance means that there will likely be false negatives when a hard threshold is set. That is, as smaller models become more performant, models which should be audited because of the risk they present avoid doing so because they fall underneath the threshold. Furthermore, it is likely that policymakers will constantly have to revisit and redefine a sensible threshold, which imposes technical overhead and creates issues with credibility.\n\nSophist Protagora (c. 485-410 B.C.) said Man is the measure of all things, implying that most of how we arrive at judgement is based upon relative perception. Instead of leveraging hard-coded thresholds, in the face of unknown distributions, it is more sensible to have relative approaches for auditing that are easier to adapt over time (Reuel & Undheim, 2024). In practice, there are plenty of historical examples where government policy defaults to dynamic automatically adjusting tools to address rapidly changing distributions. For example, the U.S. government adjusts the dollar threshold for exempt consumer credit transactions annually based on the Consumer Price Index for Urban Wage Earners and Clerical Workers (CPI-W). There are also dynamic thresholds for identifying systemic banking crises using ratios (Bordley, 2014), including credit-to-GDP. (Lund-Jensen, 2012). The European Union avoids hardcoding definitions of poverty by instead defining an at-risk-of-poverty threshold at 60% of the median equivalized disposable income (Office, 2024). This allows it to adjust as wages grow dynamically over time. A dynamic threshold for compute could focus auditing resources on the top 5-10 percentile of models ranked according to an index of metrics (consisting of more than compute) that serve as a proxy for risk.\n\nSwitching to dynamic thresholds would also mean current harms are not neglected. Using a percentile threshold based upon annual reporting would also ensure a guaranteed number of models with relatively higher estimated risk receive additional scrutiny every year. This would ensure that thresholds don’t become decorative and only applied to future models, but also apply to models currently deployed that are outliers relative to their peer group. Having a predictable number of models that receive additional scrutiny also helps build up needed technical muscle within recently created safety institutes around the world that have varying levels of technical expertise (Zakrzewski, 2024; Aitken et al., 2022; Engstrom et al., 2020).\n\nGiven the large variance in compute FLOP across modalities, AI regulators should also look to the rich body of work on reference class forecasting (Baerenbold, 2023), where forecasts are only made relative to similar basket of goods. For example, if you wanted to predict how long it takes to read a history textbook, it is less informative to take the average reading time for all books in the world and likely more precise to restrict to similar history books. This is already done when setting property prices (takes into account local neighborhoods) and assessing risk on financial assets. In turn, policymakers could consider grouping models by whether they are general purpose in intent or domain-specialized (biological model for example). This should again be combined with additional metrics as FLOP is insufficient and be implemented as a dynamic threshold to avoid the technical overhead of continual adjustments of several hard coded thresholds.\n\nCompute should not be used alone as a proxy for risk In 1928, the Soviet Union embarked on a set of 5-year plans where the government set specific targets for industrial output, agricultural production, and other economic indicators (Erlich, 1967). The metrics for success where defined almost entirely by the quantity of goods built, rather than the quality. This under-specification led to decades of commendable success in growth of production, but extremely low quality output which was often immediately discarded (Duda, 2023). In the same vein, a clear takeaway is that compute cannot be used as the only indicator of risk.\n\nEven if we limit our purview to future risks like cyber- and bio-risk, it is unclear compute thresholds are viable. This is both because we are not good at predicting what capabilities emerge with scaling (Section 4) and because the relationship is fundamentally changing between training compute and performance (Section 2.1). Dynamic compute thresholds will not resolve all these limitations. One recommendation is that any threshold is done based upon a basket of metrics that inform an index of risk. Here, policymakers being transparent about what risks are of concern helps inform more precise selection of benchmarks. For example, if concern about future risks like bio-risk is indeed top-of-mind, then specialized benchmarks that capture these risks are far more useful. Additionally, one could imagine complementing this index with some measure of general performance such as ranking by quality of open-ended responses (Chiang et al., 2024). This dilutes reliance on the limitations on single metric – another recommendation is that the index be allowed to evolve over time to account for changes in risks governments are concerned about.\n\nFLOP as a metric has to be better specified to be meaningful Even if compute as measured by FLOP remains one metric in an overall index to profile risk, it has to be better specified to be meaningful. The existing legislation does not specify key details around FLOP – how to deal with quantized weights, mixture of expert models, fractured pre-training. This will increasingly pose issues as these inference time optimizations result in gains in performance without any associated increase in FLOP. The use of FLOP can be greatly strengthened by standardizing technical specifications.\n\n5.2 Parting Thoughts\n\nOur knowledge of the ways things work, in society or nature, comes trailing clouds of vagueness. Vast ills have followed a belief in certainty.\n\nKenneth Arrow\n\nIt is very hard to trace how compute thresholds gained such traction in a short amount of time over national and international governance of AI. Compute thresholds are striking because they have emerged with no clear scientific support for either the thresholds chosen at 1026superscript102610^{26}10 start_POSTSUPERSCRIPT 26 end_POSTSUPERSCRIPT and 1025superscript102510^{25}10 start_POSTSUPERSCRIPT 25 end_POSTSUPERSCRIPT, and largely only apply to future models. One key recommendation that emerges from this essay is that we should be transparent about what risks we are concerned about. This is both to allow everyday citizens to weigh in on how government resources are allocated and also to allow for needed scientific scrutiny as to whether compute thresholds are a successful protocol for estimating and mitigating risk.\n\nAny recommendation of compute as a metric to triage risk should be technically motivated by scientific evidence. When policy is introduced, it is often hard to change. The initial values chosen by the Executive Order, as described by the Computer Scientist Suresh Venkatasubramanian had huge “signaling power” (Karen Hao, 2023) and likely influenced the default framing of discussion in the European Union that informed the EU Act. Given this intertia, it is even more critical that governance strategies like thresholds are motivated by scientific evidence. The choice of 1026superscript102610^{26}10 start_POSTSUPERSCRIPT 26 end_POSTSUPERSCRIPT and 1025superscript102510^{25}10 start_POSTSUPERSCRIPT 25 end_POSTSUPERSCRIPT rather than a number smaller or larger has not been justified in any of the policies implementing compute thresholds as a governance strategy. To motivate a compute threshold we should be able to articulate what risks we believe will be mitigated by investing in scrutiny of models at that threshold.\n\nGiven the wide adoption of compute thresholds across governance structures, scientific support seems necessary in the same way precautionary policies that aim to present harm from climate change (448 U.S. 607, 1980) or policies to improve public health (Krimsky, 2005) are justified after weighing the scientific evidence. Governments should invite technical reports from a variety of experts before adopting thresholds. If hard thresholds are chosen as part of national or international governance, they should be motivated by scientific consensus.\n\nPolicymakers face a formidable task ahead of them. What is humbling and, at times, overwhelming to ponder is that computer science as a discipline is incredibly young – it has been a mere 68 years since the Dartmouth workshop where the term Artificial Intelligence was coined. Much remains to be discovered, and new tools will pose formidable risks and benefits. Perhaps one of the key takeaways of this essay, is that we must have necessary humility about our ability to predict the future. Compute thresholds are currently presented as a very rigid governance tools because of the emphasis on a single static number to tier risk. These types of estimates are prone to failure precisely because of how rapidly the landscape is changing. Instead, we should focus on flexible tools for monitoring risk that are not tied to static numbers. Furthermore, FLOP as a measure can be greatly improved by standardizing reporting and closing possible loopholes. In the previous Section 5.1, we discussed some of these recommendations. As to what comes next, the only certain thing is that something will come next. Perhaps fitting to conclude with a quote from Alan Turing “We can only see a short distance ahead, but we can see plenty there that needs to be done.”\n\n6 Acknowledgments\n\nWisdom is like a baobab tree; no one individual can embrace it.\n\nEwe Proverb\n\nThank you to many of my wonderful colleagues and peers who took time to provide valuable feedback on earlier versions of this essay. I do not have much time to write or think deeply in isolation about a topic these days. Unfortunately, my time is increasingly spent helping others create breakthroughs. However, I have greatly enjoyed the small parcels of time I have spent on this essay wrestling with these ideas. This essay felt important to write because it requires grappling with several topics that are timely: the changing relationship we have with compute, how we navigate the risks introduced by the technology we have helped build and how science should inform policy. I have decided to release it as an essay that reflects the evolution of my thought process. This was more enjoyable for me writing down my thoughts, and was perhaps necessary for getting this to the finish line given other demands on my time. We are in an interesting time; it is rare to see research progress that is adopted overnight. Computer science ideas do not just resonate in conference halls anymore, but profoundly impact the world around us. This merits accountability, evidence and care as we navigate this impact.\n\nThanks for valuable feedback from several colleagues across several drafts of this essay (in no particular order): Usman Anwar, Neil Thompson, Sanmi Kojeyo, Helen Toner, Lennard Heim, Irene Solaiman, Shayne Longpre, Leshem Choshen, Sasha Luccioni, Stephen Casper, Jaime Sevilla, Nitarshan Rajkumar, Patrick Lewis, Aaron Courville, Nick Frosst, Rishi Bommasani, Gary Marcus, Thomas Diettrich, Margaret Jennings, Marzieh Fadaee, Ahmet Ustun, Aidan Peppin, Arash Ahmadian, Yoshua Bengio, Ivan Zhang, Markus Anderljung, Alexander Popper. Perhaps unusually, I regularly try and stress test ideas by seeking to understand the strongest counterarguments. I typically learn more from those who hold different viewpoints, and for this essay I have tried to invite input from colleagues with a varied set of stances on compute thresholds. No need to identify these worthy critics, but a huge thank you to everyone who engaged fully with this piece by providing very meaningful and rich feedback that greatly improved it. Many thanks to Aidan Peppin for additional valuable proofreading. An additional thanks to Linus Chui for visual input on the normal distribution plots in Figure 8. Many thanks to Shivalika Singh for putting together the associated website to make this essay more accessible to those beyond the academic community.\n\nReferences\n\n448 U.S. 607 (1980) 448 U.S. 607. Industrial union department v. American Petroleum Institute, June 1980. 448 U.S. 607.\n\nAakanksha et al. (2024) Aakanksha, Arash Ahmadian, Beyza Ermis, Seraphina Goldfarb-Tarrant, Julia Kreutzer, Marzieh Fadaee, and Sara Hooker. The multilingual alignment prism: Aligning global and local preferences to reduce harm, 2024. URL https://arxiv.org/abs/2406.18682.\n\nAbbe et al. (2021) Emmanuel Abbe, Enric Boix-Adsera, Matthew Brennan, Guy Bresler, and Dheeraj Nagaraj. The staircase property: How hierarchical structure can guide deep learning, 2021. URL https://arxiv.org/abs/2108.10573.\n\nAchille et al. (2017) Alessandro Achille, Matteo Rovere, and Stefano Soatto. Critical learning periods in deep neural networks. ArXiv, abs/1711.08856, 2017.\n\nAdamic & Huberman (2001) Lada Adamic and Bernardo Huberman. Zipf’s law and the internet. Glottometrics, 3, 11 2001.\n\nAgarwal & Hooker (2020) Chirag Agarwal and Sara Hooker. Estimating example difficulty using variance of gradients, 2020.\n\nAhmadian et al. (2023) Arash Ahmadian, Saurabh Dash, Hongyu Chen, Bharat Venkitesh, Zhen Stephen Gou, Phil Blunsom, Ahmet Üstün, and Sara Hooker. Intriguing properties of quantization at scale. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=IYe8j7Gy8f.\n\nAhmadian et al. (2024) Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms, 2024.\n\nAI@Meta (2024) AI@Meta. Llama 3 model card. 2024. URL https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md.\n\nAISI (2024) AISI. Advanced ai evaluations: May update, 2024. URL https://www.aisi.gov.uk/work/advanced-ai-evaluations-may-update.\n\nAitken et al. (2022) Murray Aitken, David Leslie, Fabian Ostmann, Joseph Pratt, Helen Margetts, and Cristina Dorobantu. Common regulatory capacity for AI. The Alan Turing Institute, 2022. 10.5281/zenodo.6838946. URL https://doi.org/10.5281/zenodo.6838946.\n\nAji & Heafield (2020) Alham Fikri Aji and Kenneth Heafield. Compressing Neural Machine Translation Models with 4-bit Precision. In Proceedings of the Fourth Workshop on Neural Generation and Translation, pp. 35–42, Online, July 2020. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/2020.ngt-1.4.\n\nAlbalak et al. (2024) Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, Colin Raffel, Shiyu Chang, Tatsunori Hashimoto, and William Yang Wang. A survey on data selection for language models, 2024.\n\nAlmazrouei et al. (2023) Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, Daniele Mazzotta, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. The falcon series of open language models, 2023. URL https://arxiv.org/abs/2311.16867.\n\nAnthropic (2023) Anthropic. Responsible scaling of ai, 2023. URL https://www-cdn.anthropic.com/1adf000c8f675958c2ee23805d91aaade1cd4613/responsible-scaling-policy.pdf.\n\nAnwar et al. (2024a) Usman Anwar, Abulhair Saparov, Javier Rando, Daniel Paleka, Miles Turpin, Peter Hase, Ekdeep Singh Lubana, Erik Jenner, Stephen Casper, Oliver Sourbut, Benjamin L. Edelman, Zhaowei Zhang, Mario Günther, Anton Korinek, Jose Hernandez-Orallo, Lewis Hammond, Eric Bigelow, Alexander Pan, Lauro Langosco, Tomasz Korbak, Heidi Zhang, Ruiqi Zhong, Seán Ó hÉigeartaigh, Gabriel Recchia, Giulio Corsi, Alan Chan, Markus Anderljung, Lilian Edwards, Yoshua Bengio, Danqi Chen, Samuel Albanie, Tegan Maharaj, Jakob Foerster, Florian Tramer, He He, Atoosa Kasirzadeh, Yejin Choi, and David Krueger. Foundational challenges in assuring alignment and safety of large language models, 2024a. URL https://arxiv.org/abs/2404.09932.\n\nAnwar et al. (2024b) Usman Anwar, Abulhair Saparov, Javier Rando, Daniel Paleka, Miles Turpin, Peter Hase, Ekdeep Singh Lubana, Erik Jenner, Stephen Casper, Oliver Sourbut, Benjamin L. Edelman, Zhaowei Zhang, Mario Günther, Anton Korinek, Jose Hernandez-Orallo, Lewis Hammond, Eric Bigelow, Alexander Pan, Lauro Langosco, Tomasz Korbak, Heidi Zhang, Ruiqi Zhong, Seán Ó hÉigeartaigh, Gabriel Recchia, Giulio Corsi, Alan Chan, Markus Anderljung, Lilian Edwards, Yoshua Bengio, Danqi Chen, Samuel Albanie, Tegan Maharaj, Jakob Foerster, Florian Tramer, He He, Atoosa Kasirzadeh, Yejin Choi, and David Krueger. Foundational challenges in assuring alignment and safety of large language models, 2024b.\n\nArivazhagan et al. (2019) Naveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry Lepikhin, Melvin Johnson, Maxim Krikun, Mia Xu Chen, Yuan Cao, George Foster, Colin Cherry, et al. Massively multilingual neural machine translation in the wild: Findings and challenges. arXiv preprint arXiv:1907.05019, 2019.\n\nArpit et al. (2017) Devansh Arpit, Stanisław Jastrzębski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S. Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, and Simon Lacoste-Julien. A closer look at memorization in deep networks. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 233–242. PMLR, 06–11 Aug 2017. URL https://proceedings.mlr.press/v70/arpit17a.html.\n\nAryabumi et al. (2024) Viraat Aryabumi, John Dang, Dwarak Talupuru, Saurabh Dash, David Cairuz, Hangyu Lin, Bharat Venkitesh, Madeline Smith, Kelly Marchisio, Sebastian Ruder, Acyr Locatelli, Julia Kreutzer, Nick Frosst, Phil Blunsom, Marzieh Fadaee, Ahmet Üstün, and Sara Hooker. Aya 23: Open weight releases to further multilingual progress, 2024.\n\nAzar et al. (2023) Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, and Rémi Munos. A general theoretical paradigm to understand learning from human preferences, 2023.\n\nBaerenbold (2023) Rebekka Baerenbold. Reducing risks in megaprojects: The potential of reference class forecasting. Project Leadership and Society, 4:100103, 2023. ISSN 2666-7215. https://doi.org/10.1016/j.plas.2023.100103. URL https://www.sciencedirect.com/science/article/pii/S2666721523000248.\n\nBai et al. (2022) Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional ai: Harmlessness from ai feedback, 2022.\n\nBarrett et al. (2024) Anthony M. Barrett, Krystal Jackson, Evan R. Murphy, Nada Madkour, and Jessica Newman. Benchmark early and red team often: A framework for assessing and managing dual-use hazards of ai foundation models, 2024.\n\nBender et al. (2021) Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT ’21, pp. 610–623, New York, NY, USA, 2021. Association for Computing Machinery. URL https://doi.org/10.1145/3442188.3445922.\n\nBenedictow (2004) O.J. Benedictow. The Black Death, 1346-1353: The Complete History. Boydell Press, 2004. ISBN 9781843832140. URL https://books.google.com/books?id=KjLHAOE7irsC.\n\nBesiroglu et al. (2024) Tamay Besiroglu, Ege Erdil, Matthew Barnett, and Josh You. Chinchilla scaling: A replication attempt, 2024.\n\nBirhane et al. (2023) Abeba Birhane, Vinay Prabhu, Sang Han, and Vishnu Naresh Boddeti. On hate scaling laws for data-swamps, 2023.\n\nBommasani et al. (2021) Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the Opportunities and Risks of Foundation Models, 2021.\n\nBordley (2014) Robert F. Bordley. Reference class forecasting: Resolving its challenge to statistical modeling. The American Statistician, 68(4):221–229, 2014. 10.1080/00031305.2014.937544. URL https://doi.org/10.1080/00031305.2014.937544.\n\nBorzunov et al. (2023) Alexander Borzunov, Dmitry Baranchuk, Tim Dettmers, Max Ryabinin, Younes Belkada, Artem Chumachenko, Pavel Samygin, and Colin Raffel. Petals: Collaborative inference and fine-tuning of large models, 2023.\n\nBoubdir et al. (2023) Meriem Boubdir, Edward Kim, Beyza Ermis, Marzieh Fadaee, and Sara Hooker. Which prompts make the difference? data prioritization for efficient human llm evaluation, 2023.\n\nBowman (2023) Samuel R. Bowman. Eight things to know about large language models, 2023.\n\nBrodtkorb et al. (2013) André R. Brodtkorb, Trond R. Hagen, and Martin L. Sætra. Graphics processing unit (gpu) programming strategies and trends in gpu computing. Journal of Parallel and Distributed Computing, 73(1):4 – 13, 2013. ISSN 0743-7315. https://doi.org/10.1016/j.jpdc.2012.04.003. URL http://www.sciencedirect.com/science/article/pii/S0743731512000998. Metaheuristics on GPUs.\n\nBuchanan et al. (2021) Ben Buchanan, Andrew Lohn, Micah Musser, and Katerina Sedova. Truth, lies, and automation: How language models could change disinformation, May 2021.\n\nCaballero et al. (2023) Ethan Caballero, Kshitij Gupta, Irina Rish, and David Krueger. Broken neural scaling laws, 2023.\n\nCanziani et al. (2016) Alfredo Canziani, Adam Paszke, and Eugenio Culurciello. An Analysis of Deep Neural Network Models for Practical Applications. arXiv e-prints, pp. arXiv:1605.07678, May 2016.\n\nCarlini et al. (2023) Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. Quantifying memorization across neural language models, 2023.\n\nChellapilla et al. (2006) Kumar Chellapilla, Sidd Puri, and Patrice Simard. High performance convolutional neural networks for document processing, 10 2006.\n\nChen et al. (2024) Bo Chen, Xingyi Cheng, Pan Li, Yangli ao Geng, Jing Gong, Shen Li, Zhilei Bei, Xu Tan, Boyan Wang, Xin Zeng, Chiming Liu, Aohan Zeng, Yuxiao Dong, Jie Tang, and Le Song. xtrimopglm: Unified 100b-scale pre-trained transformer for deciphering the language of protein, 2024.\n\nChiang et al. (2024) Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E. Gonzalez, and Ion Stoica. Chatbot arena: An open platform for evaluating llms by human preference, 2024.\n\nChimoto et al. (2024) Everlyn Asiko Chimoto, Jay Gala, Orevaoghene Ahia, Julia Kreutzer, Bruce A. Bassett, and Sara Hooker. Critical learning periods: Leveraging early training dynamics for efficient data pruning, 2024.\n\nCiresan et al. (2011) Dan Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella, and Jürgen Schmidhuber. Flexible, high performance convolutional neural networks for image classification. pp. 1237–1242, 07 2011. 10.5591/978-1-57735-516-8/IJCAI11-210.\n\nCoates et al. (2013) Adam Coates, Brody Huval, Tao Wang, David Wu, Bryan Catanzaro, and Ng Andrew. Deep learning with COTS HPC systems. In Sanjoy Dasgupta and David McAllester (eds.), Proceedings of the 30th International Conference on Machine Learning, volume 28 of Proceedings of Machine Learning Research, pp. 1337–1345, Atlanta, Georgia, USA, 17–19 Jun 2013. PMLR. URL http://proceedings.mlr.press/v28/coates13.html.\n\nCohere & Team (2024) Cohere and Cohere For AI Team. C4ai command r+, 2024. URL https://huggingface.co/CohereForAI/c4ai-command-r-plus. Accessed: 2024-06-30.\n\nConneau et al. (2019) Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. pp. 8440–8451, July 2019. 10.18653/v1/2020.acl-main.747. URL https://aclanthology.org/2020.acl-main.747.\n\nCottier (2023) Ben Cottier. Trends in the dollar training cost of machine learning systems, 2023. URL https://epochai.org/blog/trends-in-the-dollar-training-cost-of-machine-learning-systems. Accessed: 2024-05-28.\n\nCourbariaux et al. (2014) Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Training deep neural networks with low precision multiplications. arXiv e-prints, art. arXiv:1412.7024, Dec 2014.\n\nCutland et al. (2017) Claire L. Cutland, Emily M. Lackritz, Tanis Mallett-Moore, Anna Bardají, Ramakrishnan Chandrasekaran, Cagri Lahariya, Muhammad Imran Nisar, Martin D. Tapia, Jay Pathirana, Sumita Kochhar, Francisco M. Muñoz, and Brighton Collaboration Low Birth Weight Working Group. Low birth weight: Case definition & guidelines for data collection, analysis, and presentation of maternal immunization safety data. Vaccine, 35(48 Pt A):6492–6500, 2017. 10.1016/j.vaccine.2017.01.049.\n\nDalla-Torre et al. (2023) Hugo Dalla-Torre, Liam Gonzalez, Javier Mendoza Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, Evan Trop, Hassan Sirelkhatim, Guillaume Richard, Marcin Skwark, Karim Beguir, Marie Lopez, and Thomas Pierrot. The nucleotide transformer: Building and evaluating robust foundation models for human genomics. bioRxiv, 2023. 10.1101/2023.01.11.523679. URL https://www.biorxiv.org/content/early/2023/01/15/2023.01.11.523679.\n\nDang et al. (2024) John Dang, Arash Ahmadian, Kelly Marchisio, Julia Kreutzer, Ahmet Üstün, and Sara Hooker. Rlhf can speak many languages: Unlocking multilingual preference optimization for llms, 2024. URL https://arxiv.org/abs/2407.02552.\n\nDavidson et al. (2023) Tom Davidson, Jean-Stanislas Denain, Pablo Villalobos, and Guillem Bas. Ai capabilities can be significantly improved without expensive retraining, 2023.\n\nDavies et al. (2024) Michael Davies, Ian McDougall, Selvaraj Anandaraj, Deep Machchhar, Rithik Jain, and Karthikeyan Sankaralingam. A journey of a 1,000 kernels begins with a single step: A retrospective of deep learning on gpus. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2, ASPLOS ’24, pp. 20–36, New York, NY, USA, 2024. Association for Computing Machinery. ISBN 9798400703850. 10.1145/3620665.3640367. URL https://doi.org/10.1145/3620665.3640367.\n\nDehghani et al. (2021) Mostafa Dehghani, Anurag Arnab, Lucas Beyer, Ashish Vaswani, and Yi Tay. The efficiency misnomer. CoRR, abs/2110.12894, 2021. URL https://arxiv.org/abs/2110.12894.\n\nDerczynski (2020) Leon Derczynski. Power Consumption Variation over Activation Functions. arXiv preprint arXiv:2006.07237v1, 2020. URL https://arxiv.org/abs/2006.07237v1.\n\nDettmers (2023) Tim Dettmers. Which gpu for deep learning in 2023?, 2023. URL https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/.\n\nDettmers et al. (2022) Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022.\n\nDettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms, 2023. URL https://arxiv.org/abs/2305.14314.\n\nDeWitt et al. (2024) Tristan D. DeWitt, Timothy J. Garrett, Katherine N. Rees, Christophe Bois, Scott K. Krueger, and Nicolas Ferlay. Climatologically invariant scale invariance seen in distributions of cloud horizontal sizes. Atmospheric Chemistry and Physics, 24:109–122, 2024. 10.5194/acp-24-109-2024.\n\nDhariwal et al. (2021) Prafulla Dhariwal, Girish Sastry, Mark Chen, Dan I. Moldovan, Alex, Beutel, and Jonathan Deaton. Data and parameter scaling laws for neural machine translation. 2021. URL https://api.semanticscholar.org/CorpusID:235415752.\n\nDon-Yehiya et al. (2023) Shachar Don-Yehiya, Elad Venezian, Colin Raffel, Noam Slonim, Yoav Katz, and Leshem Choshen. Cold fusion: Collaborative descent for distributed multitask finetuning, 2023.\n\nDu et al. (2022) Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten Bosma, Zongwei Zhou, Tao Wang, Yu Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc V Le, Yonghui Wu, Zhifeng Chen, and Claire Cui. Glam: Efficient scaling of language models with mixture-of-experts, 2022.\n\nDuda (2023) Kathryn Duda. The soviet imaginary: Industrialization and collectivization, 2023. URL https://www.lib.uchicago.edu/collex/exhibits/soviet-imaginary/technology/industrialization-and-collectivization/.\n\nEconomist (2023) The Economist. Why ai models make stuff up, and how hallucinations can be controlled, 2023. URL https://www.economist.com/science-and-technology/2023/02/28/ai-models-make-stuff-up-how-can-hallucinations-be-controlled.\n\nElnaggar et al. (2020) Ahmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rehawi, Yu Wang, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger, Debsindhu Bhowmik, and Burkhard Rost. Prottrans: Towards cracking the language of life’s code through self-supervised deep learning and high performance computing. CoRR, abs/2007.06225, 2020. URL https://arxiv.org/abs/2007.06225.\n\nEngstrom et al. (2020) David Freeman Engstrom, Daniel E. Ho, Catherine M. Sharkey, and Mariano-Florentino Cuellar. Government by algorithm: Artificial intelligence in federal administrative agencies. NYU School of Law, Public Law Research Paper, February 2020. 10.2139/ssrn.3551505. URL https://ssrn.com/abstract=3551505.\n\nEpoch AI (2023) Epoch AI. Key trends and figures in machine learning, 2023. URL https://epochai.org/trends. Accessed: 2024-05-19.\n\nEpoch AI (2024) Epoch AI. Parameter, compute and data trends in machine learning, 2024. URL https://epochai.org/data/epochdb/visualization. Accessed: 2024-05-27.\n\nErdil & Besiroglu (2023) Ege Erdil and Tamay Besiroglu. Algorithmic progress in computer vision, 2023.\n\nErlich (1967) Alexander Erlich. Development Strategy and Planning: The Soviet Experience, pp. 233–278. NBER, 1967. URL http://www.nber.org/chapters/c1425.\n\nEuropean Union (2024) European Union. Eu artificial intelligence act, 2024. URL https://artificialintelligenceact.eu/the-act/. Accessed: 2024-06-30.\n\nEvci et al. (2019) Utku Evci, Fabian Pedregosa, Aidan Gomez, and Erich Elsen. The difficulty of training sparse neural networks. arXiv preprint arXiv:1906.10732, 2019.\n\nFaghri et al. (2020) Fartash Faghri, David Duvenaud, David J. Fleet, and Jimmy Ba. A Study of Gradient Variance in Deep Learning. arXiv e-prints, art. arXiv:2007.04532, July 2020.\n\nFang et al. (2024a) Richard Fang, Rohan Bindu, Akul Gupta, and Daniel Kang. Llm agents can autonomously exploit one-day vulnerabilities, 2024a.\n\nFang et al. (2024b) Yin Fang, Ningyu Zhang, Zhuo Chen, Lingbing Guo, Xiaohui Fan, and Huajun Chen. Domain-agnostic molecular generation with chemical feedback, 2024b.\n\nFawzi et al. (2022) Ali Fawzi, Miklos Balog, Alex Huang, Ziwei Song, Yang Song, and Oriol Vinyals. Discovering faster matrix multiplication algorithms with reinforcement learning. Nature, 610:47–53, 2022.\n\nFedus et al. (2022) William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity, 2022.\n\nForum (2023) Frontier Model Forum. Issue brief: Measuring training compute. https://www.frontiermodelforum.org/updates/issue-brief-measuring-training-compute/, 2023.\n\nFrankle et al. (2020) Jonathan Frankle, David J. Schwab, and Ari S. Morcos. The early phase of neural network training. CoRR, abs/2002.10365, 2020. URL https://arxiv.org/abs/2002.10365.\n\nFrantar et al. (2022) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. GPTQ: Accurate post-training compression for generative pretrained transformers. arXiv preprint arXiv:2210.17323, 2022.\n\nFrydman & Molloy (2007) Carola Frydman and Raven Molloy. Historical trends in executive compensation, 1936-2003. Journal of Economic History, 67, 01 2007.\n\nGanguli et al. (2022) Deep Ganguli, Danny Hernandez, Liane Lovitt, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova Dassarma, Dawn Drain, Nelson Elhage, Sheer El Showk, Stanislav Fort, Zac Hatfield-Dodds, Tom Henighan, Scott Johnston, Andy Jones, Nicholas Joseph, Jackson Kernian, Shauna Kravec, Ben Mann, Neel Nanda, Kamal Ndousse, Catherine Olsson, Daniela Amodei, Tom Brown, Jared Kaplan, Sam McCandlish, Christopher Olah, Dario Amodei, and Jack Clark. Predictability and surprise in large generative models. In 2022 ACM Conference on Fairness, Accountability, and Transparency, FAccT ’22. ACM, June 2022. 10.1145/3531146.3533229. URL http://dx.doi.org/10.1145/3531146.3533229.\n\nGehman et al. (2020) Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. Realtoxicityprompts: Evaluating neural toxic degeneration in language models, 2020. URL https://arxiv.org/abs/2009.11462.\n\nGoldberg (1991) David Goldberg. What every computer scientist should know about floating-point arithmetic. ACM Comput. Surv., 23(1):5–48, mar 1991. ISSN 0360-0300. 10.1145/103162.103163. URL https://doi.org/10.1145/103162.103163.\n\nGoldstein et al. (2023) Josh A. Goldstein, Girish Sastry, Micah Musser, Renee DiResta, Matthew Gentzel, and Katerina Sedova. Generative language models and automated influence operations: Emerging threats and potential mitigations, 2023.\n\nGoodfellow et al. (2016) Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http://www.deeplearningbook.org.\n\nGupta et al. (2015) Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep Learning with Limited Numerical Precision. CoRR, abs/1502.02551, 2015. URL http://arxiv.org/abs/1502.02551.\n\nHAI (2023) Stanford HAI. Hallucinating the law: Legal mistakes in large language models are pervasive, 2023. URL https://hai.stanford.edu/news/hallucinating-law-legal-mistakes-large-language-models-are-pervasive.\n\nHao (2024) Karen Hao. Llms become more “covertly racist” with human intervention, 2024. URL https://www.technologyreview.com/2024/03/11/1089683/llms-become-more-covertly-racist-with-human-intervention/.\n\nHassibi et al. (1993a) B. Hassibi, D. G. Stork, and G. J. Wolff. Optimal Brain Surgeon and general network pruning. In IEEE International Conference on Neural Networks, pp. 293–299 vol.1, March 1993a.\n\nHassibi et al. (1993b) Babak Hassibi, David G. Stork, and Stork Crc. Ricoh. Com. Second order derivatives for network pruning: Optimal brain surgeon. In Advances in Neural Information Processing Systems 5, pp. 164–171. Morgan Kaufmann, 1993b.\n\nHeim (2023) Lennart Heim. Estimating palm’s training cost, 2023. URL https://blog.heim.xyz/palm-training-cost/. Accessed: 2023-08-10.\n\nHeim et al. (2024) Lennart Heim, Tim Fist, Janet Egan, Sihao Huang, Stephen Zekany, Robert Trager, Michael A Osborne, and Noa Zilberman. Governing through the cloud: The intermediary role of compute providers in ai regulation, 2024.\n\nHernandez & Brown (2020) Danny Hernandez and Tom B. Brown. Measuring the algorithmic efficiency of neural networks. CoRR, abs/2005.04305, 2020. URL https://arxiv.org/abs/2005.04305.\n\nHernandez et al. (2021) Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. Scaling laws for transfer, 2021.\n\nHo et al. (2024a) Anson Ho, Tamay Besiroglu, Ege Erdil, David Owen, Robi Rahman, Zifan Carl Guo, David Atkinson, Neil Thompson, and Jaime Sevilla. Algorithmic progress in language models, 2024a.\n\nHo et al. (2024b) Anson Ho, Tamay Besiroglu, Ege Erdil, David Owen, Robi Rahman, Zifan Carl Guo, David Atkinson, Neil Thompson, and Jaime Sevilla. Algorithmic progress in language models, 2024b. URL https://arxiv.org/abs/2403.05812.\n\nHobbhahn et al. (2023) Marius Hobbhahn, Lennart Heim, and Gökçe Aydos. Trends in machine learning hardware, 2023. URL https://epochai.org/blog/trends-in-machine-learning-hardware. Accessed: 2024-05-28.\n\nHooker (2021) Sara Hooker. The hardware lottery. Commun. ACM, 64(12):58–65, nov 2021. ISSN 0001-0782. 10.1145/3467017. URL https://doi.org/10.1145/3467017.\n\nHooker et al. (2019) Sara Hooker, Aaron Courville, Gregory Clark, Yann Dauphin, and Andrea Frome. What do compressed deep neural networks forget?, 2019.\n\nHooker et al. (2020) Sara Hooker, Nyalleng Moorosi, Gregory Clark, Samy Bengio, and Emily Denton. Characterising Bias in Compressed Models, 2020.\n\nHsieh et al. (2023) Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes, 2023.\n\nHu et al. (2024) Shengding Hu, Xin Liu, Xu Han, Xinrong Zhang, Chaoqun He, Weilin Zhao, Yankai Lin, Ning Ding, Zebin Ou, Guoyang Zeng, Zhiyuan Liu, and Maosong Sun. Predicting emergent abilities with infinite resolution evaluation, 2024. URL https://arxiv.org/abs/2310.03262.\n\nHuang et al. (2022) Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large language models can self-improve, 2022.\n\nHubara et al. (2016) Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Quantized neural networks: Training neural networks with low precision weights and activations. CoRR, abs/1609.07061, 2016. URL http://arxiv.org/abs/1609.07061.\n\nJacob et al. (2018) Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efficient integer-arithmetic-only inference. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, Jun 2018. 10.1109/cvpr.2018.00286. URL http://dx.doi.org/10.1109/CVPR.2018.00286.\n\nJatho et al. (2023) Edgar W. Jatho, Logan O. Mailloux, Eugene D. Williams, Patrick McClure, and Joshua A. Kroll. Concrete safety for ml problems: System safety for ml development and assessment, 2023. URL https://arxiv.org/abs/2302.02972.\n\nJiang et al. (2024) Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.\n\nJiang et al. (2020) Ziheng Jiang, Chiyuan Zhang, Kunal Talwar, and Michael C Mozer. Exploring the memorization-generalization continuum in deep learning. arXiv preprint arXiv:2002.03206, 2020.\n\nKaminski (2023) Margot E. Kaminski. Regulating the Risks of AI. Boston University Law Review, 103(1347), 2023. 10.2139/ssrn.4195066. URL https://ssrn.com/abstract=4195066. U of Colorado Law Legal Studies Research Paper No. 22-21.\n\nKandpal et al. (2022) Nikhil Kandpal, Eric Wallace, and Colin Raffel. Deduplicating training data mitigates privacy risks in language models, 2022.\n\nKaplan et al. (2020) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020.\n\nKaren Hao (2023) Matteo Wong Karen Hao. The white house is preparing for an ai-dominated future, 2023. URL https://www.theatlantic.com/technology/archive/2023/10/biden-white-house-ai-executive-order/675837/.\n\nKhalifa et al. (2021) Muhammad Khalifa, Hady Elsahar, and Marc Dymetman. A distributional approach to controlled text generation, 2021.\n\nKo et al. (2023) Wei-Yin Ko, Daniel D’souza, Karina Nguyen, Randall Balestriero, and Sara Hooker. Fair-ensemble: When fairness naturally emerges from deep ensembling, 2023.\n\nKocetkov et al. (2022) Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Muñoz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro von Werra, and Harm de Vries. The stack: 3 tb of permissively licensed source code, 2022.\n\nKossen et al. (2024) Jannik Kossen, Jiatong Han, Muhammed Razzak, Lisa Schut, Shreshth Malik, and Yarin Gal. Semantic entropy probes: Robust and cheap hallucination detection in llms, 2024. URL https://arxiv.org/abs/2406.15927.\n\nKrimsky (2005) Sheldon Krimsky. The weight of scientific evidence in policy and law. American Journal of Public Health, 95(Suppl 1):S129–S136, 2005. 10.2105/AJPH.2004.044727.\n\nKrizhevsky et al. (2012) Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. Communications of the ACM, 60(6):84–90, 2012. 10.1145/3091627. URL https://doi.org/10.1145/3091627.\n\nLe et al. (2012) Quoc V. Le, Marc’Aurelio Ranzato, Rajat Monga, Matthieu Devin, Kai Chen, Greg S. Corrado, Jeff Dean, and Andrew Y. Ng. Building high-level features using large scale unsupervised learning, 2012.\n\nLeCun et al. (1990) Yann LeCun, John S. Denker, and Sara A. Solla. Optimal Brain Damage. In David Touretzky (ed.), Advances in Neural Information Processing Systems, volume 2, pp. 598–605. Morgan Kaufmann, 1990.\n\nLee et al. (2023) Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, and Sushant Prakash. Rlaif: Scaling reinforcement learning from human feedback with ai feedback, 2023.\n\nLee et al. (2024) Jinhyuk Lee, Anthony Chen, Zhuyun Dai, Dheeru Dua, Devendra Singh Sachan, Michael Boratko, Yi Luan, Sébastien M. R. Arnold, Vincent Perot, Siddharth Dalmia, Hexiang Hu, Xudong Lin, Panupong Pasupat, Aida Amini, Jeremy R. Cole, Sebastian Riedel, Iftekhar Naim, Ming-Wei Chang, and Kelvin Guu. Can long-context language models subsume retrieval, rag, sql, and more?, 2024.\n\nLee et al. (2022) Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. Deduplicating training data makes language models better, 2022.\n\nLepikhin et al. (2020) Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding, 2020.\n\nLewis et al. (2020) Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 9459–9474. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf.\n\nLi et al. (2024) Junyou Li, Qin Zhang, Yangbin Yu, Qiang Fu, and Deheng Ye. More agents is all you need, 2024.\n\nLiang et al. (2023) Jinfeng Liang, Yi Huang, Li Yin, Fatemeh Sadeghi, Yanping Yang, Xue Xiao, Hans-Olov Adami, Weimin Ye, Zhe Zhang, and Fang Fang. Cancer risk following surgical removal of tonsils and adenoids — a population-based, sibling-controlled cohort study in sweden. BMC Medicine, 21, 05 2023. 10.1186/s12916-023-02902-x.\n\nLin et al. (2024a) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration. In MLSys, 2024a.\n\nLin et al. (2024b) Jiayi Lin, Hande Dong, Yutao Xie, and Lei Zhang. Scaling laws behind code understanding model, 2024b.\n\nLin et al. (2023) Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, and Alexander Rives. Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, 379(6637):1123–1130, 2023. 10.1126/science.ade2574. URL https://www.science.org/doi/abs/10.1126/science.ade2574.\n\nLinghan et al. (2024) Zhang Linghan, Yang Jianjun, Cheng Ying, Zhao Jingwu, Han Xuzhi, Zheng Zhifeng, and Xu Xiaoben. Artificial intelligence law of the people’s republic of china (draft for suggestions from scholars), 2024. URL https://cset.georgetown.edu/publication/china-ai-law-draft/.\n\nLiu et al. (2024) Hong Liu, Zhiyuan Li, David Hall, Percy Liang, and Tengyu Ma. Sophia: A scalable stochastic second-order optimizer for language model pre-training, 2024.\n\nLohn & Musser (2022) Andrew J. Lohn and Micah Musser. Ai and compute: How much longer can computing power drive artificial intelligence progress?, January 2022. URL https://doi.org/10.51593/2021CA009.\n\nLouizos et al. (2017) Christos Louizos, Max Welling, and Diederik P. Kingma. Learning Sparse Neural Networks through L0subscript𝐿0L_{0}italic_L start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT Regularization. ArXiv e-prints, December 2017.\n\nLund-Jensen (2012) Kasper Lund-Jensen. Monitoring systemic risk based on dynamic thresholds. IMF Working Papers, 12, 06 2012. 10.5089/9781475504576.001.\n\nLyman & Varian (2003) Peter Lyman and Hal R Varian. How much information?, 2003. URL http://www.sims.berkeley.edu/how-much-info2003.\n\nMangalam & Prabhu (2019) Karttikeya Mangalam and Vinay Uday Prabhu. Do deep neural networks learn shallow learnable examples first. 2019.\n\nMarchant (2011) Gary E. Marchant. The growing gap between emerging technologies and the law. In Gary Marchant, Braden Allenby, and Joseph Herkert (eds.), The Growing Gap Between Emerging Technologies and Legal-Ethical Oversight, volume 7 of The International Library of Ethics, Law and Technology. Springer, 2011. 10.1007/978-94-007-1356-7_2. URL https://doi.org/10.1007/978-94-007-1356-7_2.\n\nMarchisio et al. (2024) Kelly Marchisio, Saurabh Dash, Hongyu Chen, Dennis Aumiller, Ahmet Üstün, Sara Hooker, and Sebastian Ruder. How does quantization affect multilingual llms?, 2024.\n\nMarion et al. (2023) Max Marion, Ahmet Üstün, Luiza Pozzobon, Alex Wang, Marzieh Fadaee, and Sara Hooker. When less is more: Investigating data pruning for pretraining llms at scale, 2023.\n\nMaug et al. (2024) Nicole Maug, Aidan O’Gara, and Tamay Besiroglu. Biological sequence models in the context of the ai directives, 2024. URL https://epochai.org/blog/biological-sequence-models-in-the-context-of-the-ai-directives. Accessed: 2024-05-19.\n\nMcclelland et al. (1995) James Mcclelland, Bruce Mcnaughton, and Randall O’Reilly. Why there are complementary learning systems in the hippocampus and neocortex: Insights from the successes and failures of connectionist models of learning and memory. Psychological review, 102:419–57, 08 1995. 10.1037/0033-295X.102.3.419.\n\n(144) METR Team. Elicitation gap. URL https://metr.github.io/autonomy-evals-guide/elicitation-gap/.\n\nMialon et al. (2023) Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, Edouard Grave, Yann LeCun, and Thomas Scialom. Augmented language models: a survey, 2023. URL https://arxiv.org/abs/2302.07842.\n\nMince et al. (2023) Fraser Mince, Dzung Dinh, Jonas Kgomo, Neil Thompson, and Sara Hooker. The grand illusion: The myth of software portability and implications for ml progress. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 21217–21229. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/42c40aff7814e9796266e12053b1c610-Paper-Conference.pdf.\n\nMouton et al. (2024) Christopher A. Mouton, Caleb Lucas, and Ella Guest. The Operational Risks of AI in Large-Scale Biological Attacks: Results of a Red-Team Study. RAND Corporation, Santa Monica, CA, 2024. 10.7249/RRA2977-2.\n\nMusameh et al. (2017) Mohammad Musameh, Christopher Nelson, Jonathan Gracey, Jessica Davies, Richard Davies, Denise Francis, Adrian Hughes, Gregory Y H Lip, Helen Mcnamara, Alison Mccarthy, et al. Determinants of day–night difference in blood pressure, a compa"
    }
}
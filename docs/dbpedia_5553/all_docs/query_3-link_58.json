{
    "id": "dbpedia_5553_3",
    "rank": 58,
    "data": {
        "url": "https://dl.acm.org/doi/10.1145/3632297",
        "read_more_link": "",
        "language": "en",
        "title": "Building Human Values into Recommender Systems: An Interdisciplinary Synthesis",
        "top_image": "https://dl.acm.org/cms/asset/edc7479e-5d55-431f-9739-e49117aa60b8/3613671.cover.jpg",
        "meta_img": "https://dl.acm.org/cms/asset/edc7479e-5d55-431f-9739-e49117aa60b8/3613671.cover.jpg",
        "images": [
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/acm-dl-logo-white-1ecfb82271e5612e8ca12aa1b1737479.png",
            "https://dl.acm.org/doi/10.1145/specs/products/acm/releasedAssets/images/acm-logo-1-ad466e729c8e2a97780337b76715e5cf.png",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/action/showDoPubAsset?doi=10.1145/contrib-81100063551&format=rel-imgonly&assetId=incf-keynote.jpg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/action/showDoPubAsset?doi=10.1145/contrib-81100049948&format=rel-imgonly&assetId=boutilier_chair_fall08.jpg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/action/showDoPubAsset?doi=10.1145/contrib-81444608287&format=rel-imgonly&assetId=ekstrand_michael-cropped.jpg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/action/showDoPubAsset?doi=10.1145/contrib-99658640424&format=rel-imgonly&assetId=headshot.jpg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/action/showDoPubAsset?doi=10.1145/contrib-81553902156&format=rel-imgonly&assetId=tanu_sm.jpg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/footer-logo1-45ae33115db81394d8bd25be65853b77.png",
            "https://dl.acm.org/cms/10.1145/3632297/asset/6d243c66-e952-4c5b-9d04-a2b7dec1eedd/assets/images/medium/tors-2022-0047-f01.jpg",
            "https://dl.acm.org/cms/10.1145/3632297/asset/4f3fb5da-f6f3-4e3d-affa-0447c23b743e/assets/images/medium/tors-2022-0047-f02.jpg",
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/Default_image_lazy-0687af31f0f1c8d4b7a22b686995ab9b.svg",
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/loader-7e60691fbe777356dc81ff6d223a82a6.gif",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/action/showDoPubAsset?doi=10.1145/contrib-81100063551&format=rel-imgonly&assetId=incf-keynote.jpg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/action/showDoPubAsset?doi=10.1145/contrib-81100049948&format=rel-imgonly&assetId=boutilier_chair_fall08.jpg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/action/showDoPubAsset?doi=10.1145/contrib-81444608287&format=rel-imgonly&assetId=ekstrand_michael-cropped.jpg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/action/showDoPubAsset?doi=10.1145/contrib-99658640424&format=rel-imgonly&assetId=headshot.jpg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/action/showDoPubAsset?doi=10.1145/contrib-81553902156&format=rel-imgonly&assetId=tanu_sm.jpg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/acm-logo-dl-8437178134fce530bc785276fc316cbf.png",
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/acm-logo-3-10aed79f3a6c95ddb67053b599f029af.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            "Recommender systems",
            "value-sensitive design",
            "technology policy"
        ],
        "tags": null,
        "authors": [
            "Palo Alto",
            "USA https:",
            "orcid.org",
            "Connie Moon Sehat Hacks",
            "Behavioral Sciences",
            "Stanford University",
            "Lianne Kerlin BBC",
            "UK https:",
            "David Vickrey Meta Inc",
            "Menlo Park"
        ],
        "publish_date": null,
        "summary": "",
        "meta_description": "Recommender systems are the algorithms which select, filter, and personalize content\nacross many of the world's largest platforms and apps. As such, their positive and\nnegative effects on individuals and on societies have been extensively theorized and\n...",
        "meta_lang": "en",
        "meta_favicon": "/pb-assets/head-metadata/apple-touch-icon-1574252172393.png",
        "meta_site_name": "ACM Transactions on Recommender Systems",
        "canonical_link": "https://dl.acm.org/doi/10.1145/3632297",
        "text": "Abstract\n\nRecommender systems are the algorithms which select, filter, and personalize content across many of the world's largest platforms and apps. As such, their positive and negative effects on individuals and on societies have been extensively theorized and studied. Our overarching question is how to ensure that recommender systems enact the values of the individuals and societies that they serve. Addressing this question in a principled fashion requires technical knowledge of recommender design and operation, and also critically depends on insights from diverse fields including social science, ethics, economics, psychology, policy, and law. This article is a multidisciplinary effort to synthesize theory and practice from different perspectives, with the goal of providing a shared language, articulating current design approaches, and identifying open problems. We collect a set of values that seem most relevant to recommender systems operating across different domains, and then examine them from the perspectives of current industry practice, measurement, product design, and policy approaches. Important open problems include multi-stakeholder processes for defining values and resolving trade-offs, better values-driven measurements, recommender controls that people use, non-behavioral algorithmic feedback, optimization for long-term outcomes, causal inference of recommender effects, academic-industry research collaborations, and interdisciplinary policy-making.\n\n1 Introduction\n\nRecommender systems are the algorithms which select, filter, and personalize content across social media [195], news aggregators [79], music and video streaming services [155, 389], online shopping [210], online ad targeting [387], and other systems. As such, their positive and negative effects on individuals and societies have been extensively theorized and studied. In the context of social media recommendation, there has been mixed evidence regarding both positive and negative effects on adolescent well-being [266], polarization [6, 16] and news consumption [118]. In news recommender systems, diversity of opinion and content sourcing is a major concern [33, 150]. Recommender systems used to promote job openings may be discriminatory if they do not consider the balance of distribution across legally protected user attributes [196]. Product recommendations used for online shopping could shift large-scale behavioral patterns with significant economic, environmental, or social effects [137]. Even systems designed purely for entertainment, such as film and music streaming services, must consider the fair allocation of attention to the artists who create content [222]. One overarching question across all of these contexts is, how can we make recommender systems enact the values of the individuals and societies that they serve?\n\nThis article is an interdisciplinary, multi-stakeholder effort to define a common language and review the current state-of-the-art for addressing problems related to designing and operating recommender systems in support of a wide array of human values. We proceed by proposing a set of relevant values based on a synthesis of previous work and refined through a cross-sector expert workshop. Based on this set of values, we then review current industrial practice and emerging design techniques for building value-aligned recommender systems. This is a wide-ranging exercise in value-sensitive design [122], which we undertake by synthesizing knowledge from a variety of perspectives including computer science, ethics, economics, psychology, sociology, journalism, philosophy, and law. Although any inquiry into values brings up deep theoretical questions, our orientation is fundamentally practical: we want to know what can be done today, or in the near future, by those who build and operate contemporary large-scale recommenders.\n\nWe use the term “recommender systems” to focus on the core problem of personalized content selection across many domains. Recommender systems often operate without an explicit user query, though the user may also ask for more tailored recommendations (e.g., “politics podcast”). This contrasts with search functionality which requires an explicit query and where results tend to be much less personalized [74, 190, 199]. Social media is a major application of recommender systems, but we note that the two are not synonymous, and the effects of social media depend on many other design choices including content moderation and other ways of finding content. We focus primarily on recommenders for social media, news content, and entertainment streaming services. However, many of the issues and approaches we highlight are broadly applicable to other important categories of recommender systems, including online shopping, targeted advertising, recruitment, healthcare, and education.\n\nThere are several widely used frames for discussing the normative implications of AI systems, most of which apply to our narrower context of recommender systems. “Alignment” is concerned with ensuring that AI systems enact the intentions of its designers and of its users despite the impossibility of specifying the correct action in all possible cases [141] and provides us with the language of “value alignment.” “Fairness” or “bias” is primarily concerned with the distribution of benefits and harms between people or groups [64, 99]. “Integrity” refers to identifying and moderating content that violates platform policies for a variety of reasons, including obscenity, copyright, criminal activity, and misinformation [142]. “Well-being” is an umbrella term for a wide variety of sociological measures used across sectors including government, public health, and research, which are starting to be applied to AI systems [291]. There is a body of work on the “ethics” of recommender systems [226]. The potential effects of AI systems have also been analyzed within a “human rights” framework [89]. There is substantial overlap between these categories, each of which encompasses a range of more specific concerns such as misinformation [114], polarization [321], or addiction [7].\n\nRather than trying to reconcile these diverse frameworks, we take all of these to be concerned with values. We draw on the field of value-sensitive design [122] to define values as “what a person or group of people consider important in life” [40:1]. Building real systems requires both deep technical practice and grounding in the realities of diverse human lives, including an understanding of the actual effects of deployed recommenders. Furthermore, values and our understanding of them are constantly evolving, so the whole exercise depends on moral and philosophical reflection. Therefore, building human values into recommender systems requires a mix of conceptual, empirical, and technical work [121].\n\nOur focus is on existing or near-future techniques and current industrial practice, because we want to ensure that this work is applicable to the developers of large commercial recommender systems. This article contributes to previous work by answering three main research questions:\n\n•\n\nWhat human values are most relevant to the design and operation of recommender systems? We answer this by reviewing previous compilations, and then gathering structured feedback from a specific set of stakeholders: experts in academia, civil society, and industry.\n\n•\n\nWhat organizational processes might be used to encode values into production recommender systems? We answer this by proposing an idealized version of current industrial practice, drawing on public reports and the authors’ own experience at several platforms.\n\n•\n\nWhat specific software designs might support the values identified above? We answer this with a synthesis of existing and emerging approaches to item ranking algorithms and recommender user interfaces.\n\nThe article is structured as follows. Section 2 reports our work identifying values and summarizes the state of research and practice around each, including the empirical effects of existing recommender systems. Section 3 summarizes current industry practice, including the design of contemporary recommenders and a widely used framework for implementing values in commercial recommender systems. Section 4 takes up the challenges of measuring values in practice, including issues around operationalization and data sources. Section 5 synthesizes previous published work in recommender design that is relevant to implementing these values. Section 6 discusses policy approaches that might incentivize the implementation of these values. Section 7 concludes by listing major open problems.\n\n2 Values for Recommender Systems\n\nOur first research question is, what values are applicable to recommender systems? We did not aim at creating a comprehensive list of all relevant values (which is probably not possible). Instead, our goal was to ground the many discussions happening in industry, academia, and policy in a reasonably broad set of values so progress can be made.\n\nWe began by searching for previous compilations of values, issues, and risks relevant to either recommenders in particular or AI systems in general. We found four sources that were primarily compilations: a survey of AI ethics policy documents [116], a large user research project undertaken by the BBC [183], a compilation of AI-relevant well-being metrics [161], and an IEEE process for ethical system design [162]. We extracted and combined these lists to produce an initial set of values.\n\nOne of the questions that arises when assessing relevant values is “valuable to whom?” Recommender systems are fundamentally multi-stakeholder in nature, as they have effects on consumers, creators, platforms, and non-users including society in general [1]. There are also fairness considerations that apply across different subgroups [99] and cultural considerations that apply in different parts of the world. Therefore, we extended our initial list through a multi-stakeholder approach.\n\nIn an effort to include values relevant to a wide variety of stakeholders, we gathered input through the Partnership on AI (PAI), a global non-profit partnership of roughly 100 academic, civil society, industry, and media organizations working towards positive outcomes from AI for people and society. We put out a broad invitation to people from these organizations to attend a discussion of values in recommender systems, and approximately 40 people responded. At the subsequent workshop, these participants discussed our initial list of values in small groups to further develop their conception of how values apply in recommender systems.\n\nOf the 29 people who identified themselves in a pre-workshop survey, 14 were from civil society, 8 were from industry, and 7 were from academic institutions [202]. Almost all workshop participants were based in the US. Anticipating this bias, we included a set of global “techno-moral virtues” [350:15, 120] as well as the feminist ethic of care [309] and philosophical traditions in Africa that emphasize the relationships and bonds among people [243]. We asked each participant to provide further citations to any literature they considered relevant. We combined our initial list derived from the compilations above, all values discussed in transcripts of the workshop deliberations, the values mentioned in all participant-submitted references, and anything mentioned in a brief post-workshop survey. The authors then refined this long list by merging and editing entries through several rounds of reviews to produce our final list.\n\nThis approach of literature review followed by expert deliberation complements attempts to articulate the values that apply to recommender systems through user research. We note that one of our initial compilation sources [183] is a large user survey and that our list of values includes those found in qualitative recommender user studies such as [68]. We also note that there is much overlap with the UN Sustainable Development Goals [341], in particular, good health and well-being, quality education, decent work and economic growth, industry and innovation, reduced inequalities, climate action, peace, justice, and strong institutions.\n\nThe goal of this exercise was not to arrive at a set of global, universally applicable values, but to list some of the key values that have been identified to be particularly relevant to various stakeholders in various recommender contexts. The main challenge in devising our list was coming up with a set of definitions at an appropriate level of granularity that together cover a wide set of overlapping and often vague concepts. We aimed for a level of abstraction between overly general statements (e.g., “do good”) and specific formulations (e.g., particular metrics). Our list is presented in Table 1. In Appendix A we expand this table to include citations for definitions, example indicators or metrics for each value, and example design changes which could promote that value.\n\nTable 1.\n\nBy nature, there are tensions between values, and those tensions lead to many of the difficulties in operationalizing them in recommender systems. For example, the value of free expression is in tension with the value of safety because if we allow users to say anything they want on social media, others may feel threatened. As another example, privacy can be in tension with usefulness. Privacy suggests that a platform should not try to infer whether a user might have a particular disease, even though early information and intervention might be helpful to them.\n\nThere are tradeoffs not just between values, but between different people and groups of people. For example, if we exclusively focus on the well-being of individuals, society may suffer [254]. Other tensions arise because recommenders must simultaneously serve several types of stakeholders including users, content producers, platforms, and non-users, as studied in the field of multi-stakeholder recommendation (Abdollahpouri et al., 2020). Values can also operate on varying time scales. Giving users entertaining content may satisfy their short-term needs, but providing more informative content may have longer-term benefits. Having informed users may also be a societal value, and individual values often operate on a shorter timescale than societal values.\n\nMany of the values in our list are closer in nature to instrumental goals for an AI system, by which higher-order values are achieved. For example, the bioethics principles of respect for autonomy, beneficence, and justice are often considered primary, without resorting to other values for justification [56]. By contrast, other values seem to derive much of their importance because they contribute to these principles, including privacy, agency, control, transparency, accessibility/inclusiveness, and accountability.\n\nRather than discussing each value individually, we approach them through a number of themes that characterize many discussions on these topics: usefulness, well-being, legal and human rights, public discourse, safety, and societal values.\n\n2.1 Usefulness\n\nPlatforms use recommender systems because they believe them to be useful to users, content creators, and themselves. The most straightforward distinction between recommendation and search is that a recommender can suggest items without an explicit query, which is valuable in a variety of contexts. For example, news items cannot be selected through user queries alone, because the user is unaware of new events. While the value of presenting a previously unknown post, article, person, movie, song or ad varies, all of these can lead to positive and novel outcomes for people. Modern recommender systems have their roots in collaborative filtering systems in the 1990s, and the need for intelligent filtering has only increased since then as the pool of available information has exploded. We call this value “usefulness” to distinguish it from “utility” which has a more technical meaning from economics (discussed in Section 5.3.1).\n\nUsefulness is closely related to control, which we take to mean that users should be able to select which content they are seeing, the type and degree of personalization (if any), and understand the processes that determine what they see. In large part, this requires that platforms provide features to support such choices (e.g., playlists on a music streaming service or topic selection on a news service) though there are also important questions about community governance [201, 385]. On social networks, interfaces for describing which posts to see are particularly complex given the breadth of available content. Note that feeling in control and actually having control are different. Placebo controls may increase user satisfaction without offering any actual improvement [348] while users may not perceive even large effects of functional controls [207]. Agency is a similar value, but we use it to refer to control over other elements of the users’ lives, not just the recommender. For example, a recommender could assist a user with education [81] or direct them to job opportunities.\n\nThere are complex tensions between agency and control and other values, as users might make choices that harm themselves or others. This grounds out in concrete ethical questions such as: if it is possible to infer that someone has an eating disorder [377], and if there is research indicating that viewing dieting videos leads to bad outcomes for such a person, is it reasonable or even obligatory to thwart their expressed intention to see such material?\n\nRecommender systems can also be useful to both content creators and platforms. While recommenders are a commercially important technology, algorithmic content optimization does not translate directly into revenue in many contexts [169]. For example, subscription services must maximize user retention, while current recommender designs struggle with long-term outcomes. Direct optimization for revenue, or more precisely profit, is best developed in the context of online shopping [80, 83, 168, 210]. Even for ad-driven services, content personalization and ad selection are often handled by different recommenders which optimize against different objectives [333]. Other organizations which operate recommenders may not intend to make money by doing so, such as news publishers. While not all recommender systems need to be profitable, they all must be useful to the operator by some measure.\n\n2.2 Well-Being\n\nWell-being is fundamental to human experience, and recommender systems have the potential to affect users’ well-being in many ways. While the phrase has specific implications around positive subjective experience, it is also widely used in the policy community as an umbrella framework which encompasses other values [106, 135, 250] and has been explored as an important end goal for AI systems in general [161]. Well-being as a subjective experience is one of the values in our list, but many other values intersect with it strongly and are often included in well-being frameworks.\n\nWell-being is a complex concept, and there is little consensus on how to define it [88]. Objective measures such as employment, lack of crime, and economic prosperity were historically used as proxies for well-being. More recently there has been increasing focus on a more holistic understanding of well-being based on both subjective and objective measures [86, 153, 191]. These subjective well-being measures account for people's cognitive and affective evaluations of their lives by asking subjects to rate how much they agree with statements like, “The conditions of my life are excellent” [87].\n\nThe values of connection, community and belonging, recognition and acknowledgement, self expression, care, compassion, and empathy all relate to the concept of well-being. Many different types of content can contribute to increased well-being, such as through education, motivation, or personal relationships. Entertainment can also contribute to well-being, especially as a short-term emotional experience, and hedonism is often considered a basic drive [293].\n\nThe effect of recommender systems on well-being has been most widely studied in the context of social media use. There is conflicting evidence for both positive and negative effects, in part because well-being in this context is variously defined and often represented by other factors, from self-esteem and life satisfaction to civic engagement, social capital, and user satisfaction. Also, most studies have not been designed to separate the effects of algorithmic content selection from other aspects of social media such as user creation and sharing.\n\nIn one study of over 2,000 college students, social media use was associated with improvement in various facets of psychological well-being such as overall life satisfaction, civic engagement, and social trust [349]. Some investigators have suggested that the amount of time spent online is less important than the quality of that time; active use may promote well-being, whereas passive use and emotional connection to use may have a negative impact [28, 123, 354]. The literature also points toward negative health effects related to social media use. One longitudinal study compared social media use with mental health and physical health, and found a decrease of 5%–8% of a standard deviation in self-reported mental health [296]. In another study, deactivating social media accounts for four weeks resulted in increased time in offline interactions and improved subjective well-being [6]. Upward social comparison has been proposed as one potential link between social media use and mental health disorders such as depression and anxiety [225, 367].\n\nSome recommender-based products may encourage addictive tendencies. Allcott et al. [7] found that abstaining from social media use for a time or allowing people to set future screen-time limits produced a decrease in subsequent use, suggesting that social media use may result in habit formation and self-control issues. These effects may not be due to personalized recommendations per se, as broadcast television, a non-personalized medium, has also been found to be addictive in this sense [120].\n\nThere is classic work on the advantages of both strong and weak social ties [13]. These benefits are less well studied in the context of recommender systems, but a few studies of social media are worth noting. Social media use has been associated with increased social connection and social capital in online and offline social networks, with particular benefit for users experiencing low self-esteem and low life satisfaction [101, 315, 326]. Larger social networks may be associated with greater perceived social support, reduced stress, and improved well-being [238]. Social media use has been shown to increase intergroup contact and reduce prejudice when offline social network diversity is low [16]. Recommenders can also contribute to civic engagement. A large-scale experiment showed that messaging on social media can increase voter turnout on the order of 1% [38]. While the possibility that recommendations connect people to violent or extremist groups has been widely discussed (and we review the evidence below) the converse of this concern is the possibility of connecting people to constructive communities or social movements.\n\nWe see long-term well-being as a major open problem for recommender systems, with a number of challenging sub-problems: defining well-being in contextually appropriate ways, measuring user well-being in production, and algorithmically optimizing for relatively long-term outcomes (months to years).\n\n2.3 Legal and Human Rights\n\nSome of the values in our list can be considered rights, in the sense of obligations to various parties. Previous work has explored the legal and human rights potentially impacted by recommender systems [226] and by AI in general [89].\n\nPrivacy has at least three different interpretations in the recommender context. The first sense refers to information the system knows, infers, or estimates about the user. The second refers to the possibility of direct revelation of user information to other users or third-parties. There is also the possibility of indirect (often noisy) revelation of user information by the actions of the recommender, e.g., a recommendation made to one user may allow them to infer something about the items viewed by another user. This form of information revelation is the subject of a large body of research in the area of differential privacy [69, 96, 166]. Privacy trades off against other values such as usefulness, transparency, and fairness. The question of demographic inference which might help the user is well discussed in the context of algorithmic fairness [11] but analogous concerns arise with many types of user knowledge. Granting users control over whether and how they want platforms inferring and using their personal data to recommend useful content might be one way to reconcile privacy and ethical use of personal data.\n\nLike privacy, transparency and explainability are broad concepts. Explainability might mean that a recommender system gives reasons why a certain piece of content is being shown to a user [386]. Transparency could require platform disclosure of various types of data including metrics characterizing different types of content or aggregated user activity. Transparency can be important for building users’ trust [76] and is often discussed as a policy tool to promote understanding and accountability. Explanations and disclosures can come in multiple forms depending on their intended audience, such as users, researchers, governance bodies, or auditors, and meaningful transparency needs to be informed by concrete individual and societal information needs [93, 303]. From a technical perspective, explanations can be tricky to generate because many recommender techniques (including deep learning) do not lend themselves to easy explanations [366].\n\nFairness, equity, and equality are closely tied to human rights (Universal Declaration of Human Rights, UN). In everyday situations where recommender systems are employed, multiple parties have some interest in the outcome [2]. Fairness has a variety of meanings in recommender systems, including considering the disparate impacts of recommendations across user classes [203]. Content providers want to be fairly treated in terms of the exposure and benefit they receive from the system [85, 174]; users want to receive good quality of service, and do not want to be under-served relative to other users [100]; other stakeholders, such as the system operator, content creators, and society broadly each have ideas of what it may mean for recommendation to be “fair” [85, 98]. This has deep ties to various conceptions of equity, in particular, equity of attention [37:201, 302]. Recommenders may also create externalities that affect people who do not use the platform, such as by directing many new people to a formerly obscure place, or encouraging the consumption of products with environmental consequences [279]. The field of multi-stakeholder recommendation has emerged to tackle these dynamics [1].\n\nGiven all the parties involved through the platform or via externalities, fairness is a multifaceted problem where different stakeholders have different objectives and needs from the system. Often the desires of different stakeholders are in conflict; not everyone can have exactly what they want, though it might still be possible to give something worthwhile to everyone.\n\n2.4 Public Discourse\n\nSome of the most intense recent discussion around recommender systems has centered around how they affect public discourse. Accuracy of information and diversity of content are two prime examples.\n\nAccess to accurate and factual information supports human decision-making and understanding across myriad domains, ranging from healthcare to politics to economics [154, 295]. While falsehoods and misleading content have threatened truth for centuries [14, 253], online user-generated media may have amplified the threats to sensemaking and decision-making. For example, some evidence suggests fabricated news articles (as identified by third-party fact checkers) spread significantly faster through Twitter than genuine news articles, especially articles about politics [356]. It is not clear to what extent this is an effect of recommender algorithms, as opposed to non-algorithmic social media creation and sharing dynamics [156, 236]. Similarly, COVID-19 misinfo was common on social media globally, especially early in the pandemic [8, 48] and there is evidence that suggests misinformation contributed to the spread of the virus via reduced vaccination rates [208, 264]. This suggests that personalized recommendations may have had a causal effect on disease spread, though this has not been directly studied.\n\nMany authors have argued that designers of news recommenders have editorial responsibilities similar to news editors [115, 148, 244, 311]. Getting the right information to the right people at the right time is a key normative concern that goes well beyond ensuring accuracy, a value that we call “informativeness.” We can look to the tradition of public-service journalism to inform recommender design [115, 311] but personalized news delivery is a new technology and specific editorial theories are still developing. One approach would be to try to deliver a news item if the user previously expressed an interest in a particular topic, if it reports on events that affect their life, or if there is an opportunity for the user to help others [319]. These sorts of ideas have yet to be effectively translated into algorithmic terms.\n\nDiversity is a value that can be relevant to consumers, content creators, and society in general. In industrial settings, diversity has mostly been studied because increased diversity typically results in greater user satisfaction and user engagement, at least up to a point [155, 194, 222, 369]. Many recommenders use diversification algorithms for the practical task of ensuring that users are not continuously shown the same type of item in their recommendations, often implemented as a re-ranking pass [132, 390]. Other parties also have an interest in diversity. A streaming music or television platform needs to ensure that the long tail of less popular artists or producers have enough exposure to make it worthwhile for them to stay on the platform [222], and increased diversity may contribute to equity of attention [37, 302].\n\nThe experience of a lack of diversity in personalized content consumption has been described as a “filter bubble” or “echo chamber.” However, this language is somewhat vague and has been used to describe a wide variety of phenomena including self-selected consumption behavior, homophily in social networks, and algorithmic feedback effects [50]. We consider such possibilities more specifically throughout the rest of this article.\n\nDiversity has been most specifically studied in the context of news recommendations, where it might serve a variety of democratic goals including consumer choice, civic participation, pluralist tolerance, or challenging the status quo [33, 148, 357]. The meaning of diversity can differ between news organizations, depending on their editorial missions and the balancing of other values that matter to the organization (such as personal relevance, engagement, or time spent). In practice, diversity is often measured using item-similarity metrics [194] but such formulations do not capture the complexity that the social sciences have brought to the debate about media diversity [206].\n\nOne concern is that a lack of diversity in personalized news recommendations could prevent users from being exposed to contrary perspectives. Simulations have suggested that optimization to increase user engagement could create feedback loops that drive users into narrower selections of content [57, 175, 177, 192, 284]. However, news personalization algorithms do not seem to produce a less diverse selection than human editors [231] and the news provided to different users is quite similar [139, 244]. Social media users consume a more diverse range of news sources than non-users [118] but correspondingly also consume more news from partisan sources [117].\n\nA related concern is that recommender systems might be causing large-scale polarization of ideology (issue polarization) or attitudes (affective polarization) [321, 323]. A recent review found an overall positive correlation between “digital media” use and polarization [209]. The causal evidence is more mixed. In the U.S., polarization began increasing decades before social media [43] while several other developed countries have similar internet usage but do not show increasing polarization [44]. Paying people to stop using social media for several weeks produced small declines in issue polarization measures in a study of American users [6] but increases in a measure of ethnic polarization in Bosnia-Herzegovina [16] which was hypothesized to result from reduced inter-group contact. While there is little evidence that filter bubbles or a lack of diversity are driving polarization, other socio-technical processes can increase polarization including partisan sorting [339]. Optimizing for engagement (see Section 3.1) can prioritize divisive content, and there are now several lines of evidence that this mechanism is exacerbating polarization [323].\n\n2.5 Safety\n\nSafety includes the idea that people should not be bullied, attacked, or dehumanized and should not be exposed to disturbing content. For social media, safety has thus far mostly been considered in the context of content moderation, and many platforms have developed their content moderation policies based on international human rights principles and frameworks and in consultation with third-party experts [90, 108, 260]. As a notable example, a series of human rights abuses occurred in Myanmar, characterized by hate speech and disinformation against the minority Rohingya by the military on social media [51].\n\nRecommender systems should not promote violence. Note that “polarization,” a mass hardening of political divisions, is conceptually distinct from “radicalization,” where a small number of individuals violate mainstream norms and may resort to violence [316]. There are a number of documented cases of far-right and terrorist radicalization where online recommendations were involved [26, 237, 280, 362]. However, these reports mention many other factors including chat rooms, personal relationships, user-directed searches, and life circumstances. More systematic studies have looked for recommender feedback effects that move users toward radicalization en mass [110, 200, 236, 275] These studies generally show that recommenders alter content mix in the direction of engagement, but have produced poor evidence on the radicalizing potential of recommender systems because of insufficiently powerful experimental designs, as we will discuss below. In general, causal understanding of user trajectories through recommender systems remains a major challenge.\n\n2.6 Societal Values\n\nThere are additional values on our list that have more of a societal flavor such as progress, labor, duty, environmental sustainability, duty, and tradition and history. These are important values that arose in our research, though the degree to which they are considered important varies considerably from one culture to another. In our workshop and the literature we reviewed these values did not come up as particularly relevant to online platforms, but that does not mean that they cannot or should not be promoted in certain contexts.\n\n3 The State of Practice\n\nOur second research question is, how might these values be operationalized within commercial recommender systems today? Because our focus is industrial applicability, rather than attempting to design an ideal process from first principles we document and build on current practice. While the academic literature on recommenders is vast, there is far less documentation of actual commercial operations.\n\nIn Section 3.1, we describe a general architecture that captures common design patterns in commercial recommender systems. This provides the necessary context for the rest of the article. In Section 3.2, we propose an organizational process for modifying a recommender system to support a specific value, within the context of a large company. These sections draw on public material, plus the author's own experience at several large platforms, to triangulate the current state of practice.\n\n3.1 How Recommenders Work\n\nRecommender systems are often described as “black boxes” [307] but most are constructed using similar principles. This section presents a greatly simplified, but illustrative recommender design. While it doesn't represent the details of any particular system, many real systems share its features. Our discussion also leads to a technical definition of engagement in terms of behavioral data. Most recommenders are built to prioritize some form of engagement at their core, though they also consider many other types of signals.\n\nRecommendations start with a pool of content items. These items may be produced entirely by the recommender operator, as with a news organization's recommendations; or they may be curated from multiple sources, such as a music recommender which gets content from publishers; or items may be entirely user-generated and posted without prior review, as on social media. These three categories have been called “closed,” “curated,” and “open” recommender systems [72]. Before recommendations are generated, a moderation process identifies and removes items which violate platform policies. Platform moderation is a complex process which involves many human and machine steps from policy making to enforcement to appeals [142, 380] but here we are concerned only with how moderation defines which items are available for recommendation.\n\nIndividual recommendations are generated using data about item content, user attributes, and context, and result in an output stream or set of recommended items, sometimes called a feed or slate. The context may include a wide variety of features such as the video the user is currently watching, the time of day, or a search query like “politics podcast” [25, 36]. User attributes may be derived from personal information the user has provided, any explicit user feedback or control settings, and any implicit feedback contained in the history of past interactions with the system.\n\nThe recommendation process usually begins with the selection of candidate items. Candidate generation algorithms are tuned to retrieve an overbroad sample but are very efficient at filtering a corpus of (potentially billions of) items available for recommendation down to a small set which might be a good fit for the user and context, typically ranging in size from a few hundred to a few thousand. These candidates are then ranked, that is, each is assigned a relevance score, which typically reflects a prediction of user engagement with the candidate item. In modern systems ranking may involve dozens or hundreds of “signals” which summarize aspects of the content, the user, the context, and how all of these interact. The top-scoring items are then selected as the user's recommendations. Many systems then re-rank the remaining items, this time comparing them to each other, rather than evaluating each individually, to achieve goals such as diversity of item topic or source [131, 390].\n\nFig. 1.\n\nItem ranking constitutes the core of personalization. The final item score is typically a weighted combination or some other function of (i) the predicted probability of a number of different types of user responses [109, 227, 310, 389] plus (ii) a wide variety of scoring signals that range from source credibility [261] to playlist diversity [222] to whether an item tends to be inspiring [163]. Ranking items by the probability of desired or targeted user reactions (e.g., sharing, dwell time) is informally known as optimizing for engagement. This may or may not optimize for value to various stakeholders, which is why many non-engagement signals are also used.\n\nThe word “engagement” has been used across many fields including media and technology to suggest that users are repeatedly interacting with a product, as evidenced by a wide variety of metrics. Here we propose a more specific definition, compatible with recommender design practice. We take engagement to be a set of user behaviors, generated in the normal course of interaction with the platform, which are thought to correlate with value to the user, the platform, or other stakeholders. This definition builds on previous work [167, 379]. It is multi-stakeholder in nature and reflects the fact that engagement signals are chosen to be indicators of value, but aren't going to be fully aligned with specific values in all cases. It also suggests there are some signals of value that can only be derived from non-ordinary or non-behavioral data, as we discuss below.\n\n3.2 Implementing a Product Change\n\nIn this section, we answer our second research question: what organizational processes might be used to encode values into production recommender systems? Our goal is to provide practical advice for people who are working on large recommender systems within the industry. Because of this, we focus on documenting the ways in which values are actually built into recommenders today. Unfortunately, there is little public documentation of the actual processes by which values are engineered into large recommender systems, so we draw heavily on the authors’ collective experience working on a variety of platform-scale recommenders.\n\nThis is a different approach than, for example, attempting to derive an ideal organizational process from first principles. Nor does it explain what stakeholders outside of the organization building recommenders should do (but see our discussion of policy approaches in Section 6). However, this approach has the advantage of codifying a process which we know is actually possible within the industry today.\n\nWe start by assuming that a choice has been made to prioritize a particular value or to adjust the status-quo tradeoff in favor of that value. We do not give a process for deciding how to prioritize values, taking into consideration the needs of all stakeholders—this is a major open problem which we articulate in Section 7. Here, we focus on what can be done within an organization once the choice has been made.\n\nWe illustrate the process by which a specific human value might be incorporated into a recommender system's design using the example value of diversity. Consider the development or refinement of a news recommendation platform in which the designers are concerned with users having the opportunity to develop an awareness and understanding of multiple political perspectives—this has been called the deliberative perspective on recommender diversity [150]. One way that a recommender system might contribute to this value is by increasing the diversity of recommended items. This strategy rests on two key assumptions: there is currently a lack of diversity in the items users see, and showing them more diverse items will increase tolerance. Both of these assumptions are complex and available evidence is mixed [249, 321]. Nonetheless, this is a non-trivial example of how metric-driven recommender engineering might proceed.\n\nOnce the decision to increase content diversity has been made, the implementation might proceed using the steps outlined below.\n\n3.2.1 Research Unintended Consequences.\n\nGenerally, such an effort begins with the study of potential mechanisms to incorporate this value. In the case of diversity, designers may research the history of attempts to create inter-group tolerance through diversity of exposure, including the ways in which this has failed [258, 263]. User studies may be used to test the exposure diversity hypothesis with users of the platform, assessing the impact of seeing news items from diverse perspectives on people's knowledge and attitudes. In particular, it will be important to test for backfire effects where people reject diverse information [18, 95] or other unintended consequences. There must also be a reasonable expectation that the changes to the recommendation system won't adversely affect other values that the system embodies. In concrete terms, this often means that any proposed change should not decrease other metrics (see below) more than a specified amount.\n\n3.2.2 Develop a Metric.\n\nAs discussed in Section 4, metrics are central to any attempt to build values into recommender systems. A variety of diversity metrics have been proposed, both for news content [75, 357] and for recommender systems more generally [60, 194]. Choosing or developing a metric requires settling on a definition of diversity suitable for that particular system. It may be that multiple metrics are needed to capture different aspects of diversity. While product teams typically choose metrics, there has been experimentation involving external stakeholders in order to increase the legitimacy of such decisions [33, 201, 320].\n\nHere we focus on a conception of “productive” diversity, where people disagree in ways that are ultimately constructive [321]. Given this concept, developing a method to measure the diversity of articles on the platform may be broken into several phases, such as\n\n•\n\nDeveloping a description of diversity for use by human raters (e.g., “Does this set of articles include constructive contributions from multiple perspectives?”).\n\n•\n\nCreating a training dataset of positive and negative examples using human-rater evaluations.\n\n•\n\nUsing this training data to develop a heuristic or machine-learned model that can predict whether a list of (recommended) articles adequately reflects “productive diversity.”\n\n3.2.3 Consider Different Product Designs.\n\nBefore implementing a specific product change, different product-based approaches to increasing diversity will be explored. For example, the designer could change the user interface, perhaps by showing related articles from other viewpoints below each item [325], or change the ranking algorithm to try to nudge people to consume more articles that represent this type of diversity [216]. For the sake of example, we assume below that the latter change has been selected.\n\n3.2.4 Implement the Product Change.\n\nImplementing this specific product refinement requires incorporating the diversity prediction model into the item ranking procedure. Whereas many current recommender systems score each item independently, diversity is a property of sets of items. One challenge is that scoring entire lists is both more complicated and more expensive than scoring individual items [160, 369] which may necessitate the development of more efficient algorithms for ranking such item sets.\n\n3.2.5 Evaluate and Advocate.\n\nOnce implemented, the new product feature will typically be tested using offline data [55, 128] followed by A/B tests with a small group of users. If these tests show positive results, the new diversity prediction model will be deployed in production, and monitored to see whether the target diversity metric improves (this may involve gradual ramping up of the deployment to larger numbers of users, holdback tests, etc.). There may also be side effects—for example, while increasing diversity often increases engagement [194, 369], this is not always the case. Suppose that as a by-product of the model deployment, there is a drop in engagement, declines in other values-relevant outcomes such as quality or safety metrics, or some cost imposed on other stakeholders (e.g., content producers). In practice, this requires negotiation among internal stakeholders to decide if the increase in diversity is significant enough to justify a drop in other metrics.\n\n3.2.6 Establish Guardrails to Prevent Reversion.\n\nOnce deployed, product teams often establish a review process so that subsequent product changes, within the originating team or elsewhere, don't indirectly revert the diversity improvement. This might include numeric “guardrails” that specify the maximum allowable decrease in diversity induced by other product or algorithmic changes.\n\n3.2.7 Monitor Outputs and Outcomes.\n\nIn practice, product teams will continually monitor the diversity of recommended items to detect operational failures (e.g., bugs or system failures), or (induced or exogenous) changes in the user distribution and user behavior. A survey which asks users or human raters to detect “productive” diversity may also be regularly employed to detect model drift and produce updated training data. Finally, determining whether the ultimate goal is being achieved, or is still worthy of being achieved, requires ongoing evaluation. This could involve survey methods to assess metrics such as affective polarization, and ethnographic research to understand what it means for users to be encouraged to be “more tolerant” in this way.\n\n4 Value Measurement\n\nIn order for recommender systems to incorporate human values, there need to be methods for measuring how well a system is adhering to, promoting, or facilitating these values over time. This section describes how to go from a value to a metric, the issues involved in designing good metrics for values, and the data that is available on which to build such metrics. Figure 2 shows the steps in operationalizing a value, including the process of selecting metrics, the interactions a user has, and the ultimate outcomes.\n\nFig. 2.\n\n4.1 From Value to Metric\n\nDefining a metric is part of the process of specifying what, exactly, a particular human value means in the context of a real system. This process is called operationalizing a value. The people involved in defining metrics have considerable influence over the ultimate function of a recommender, which is why multi-stakeholder involvement in recommender metric selection may be important [201, 320].\n\nTo illustrate the gap between a value and its measurement, consider the value of “safety,” and in particular, protecting users from hate speech. A precise definition of hate speech is not only hard to articulate, but under constant debate and evolution, and every choice has some set of undesirable side effects [84, 134, 214, 214, 259]. Well-being is an even more complicated example. As discussed, well-being has many components (physical health, health of personal relationships, having a purpose in life, etc.) and can be considered in the short term (e.g., entertaining content) or the longer-term (e.g., learning useful skills or fostering relationships). Making news recommendations that serve the public interest likewise requires defining concrete metrics that reflect some assessment of that interest [115, 357]. At some point, a real recommender must commit to specific operationalizations of broad concepts, with the resulting tradeoffs between competing values and stakeholders.\n\n4.2 Characteristics of a Good Value Measurement\n\nA good measurement has a number of desirable properties, including validity and reliability [230], fairness, and legitimacy. Quinn et al. [269] summarize the situation as\n\nThe evaluation of any measurement is generally based on its reliability (can it be repeated?) and validity (is it right?). Embedded within the complex notion of validity are interpretation (what does it mean?) and application (does it “work”?). [269:216]\n\nMany social science theories involve quantities that are not directly observable and hence must be inferred, often with considerable uncertainty, which makes any measurement instrument implicitly a model of some purported underlying reality [164]. Construct validity is the convergence of a successful theoretical idea with a measurement that effectively captures it [318]. Jacobs and Wallach [165] propose several other types of validity in the context of measurements within computational systems, including face validity (is the metric plausible?), content validity (does the metric capture all the relevant parts of the concept of interest?), convergent validity (does this metric agree with other accepted metrics?) and discriminant validity (does this capture something different than other metrics?). Reliability is typically evaluated in terms of agreement between multiple measurements (test-retest reliability), between different human judges (inter-rater reliability) and, for surveys, between different ways of asking a question (inter-item reliability) [164].\n\nIt is rare that a particular measurement fully captures what we mean by some value in a particular context, meaning that most metrics are in fact proxies for what we actually care about. Even a good metric will change meaning when it is known to be used to make consequential decisions, e.g., student test scores must be interpreted differently when they are used to decide academic progression because instructors will begin “teaching to the test.” This effect is sometimes known as Goodhart's law, but there are a variety of different causal structures which can produce feedback processes that widen the distance between a metric and the underlying value [213, 235]. In the technical community, this has been most discussed in the context of the general problem of algorithmic optimization and the difficulty of objective specification [78, 141]. Hence, as part of being precise about the definition of a human value, it is important to identify gaps in what is measured and monitor them over time. Human values—that is, what is considered important—also tend to change over time. Qualitative user research plays a critical ongoing role in designing and evaluating metrics.\n\nBecause modeling assumptions are required to connect a measurement to the underlying value it purports to reflect, measurement itself has fairness implications. The signals from measurements can vary across demographics, even after controlling for differences in user intent and task [221]. For example, if older users read more slowly than younger users, then a metric based on dwell time will be an over-optimistic measurement for older users regardless of their level of satisfaction. Thus, interpreting metrics at face value may systematically disadvantage and misrepresent certain demographics and user groups. Organizations deploying recommendation systems often rely on internal auditing methods as a way to measure how the overall system performs across different demographics and other user attributes [113, 272]. External audits may also be required by regulation, as discussed below.\n\nIt is not enough to have an accurate and fair measurement of a human value. Stakeholders in the recommender system need to be able to accept the process and outcome of the measurement as legitimate. This means that two metrics that are operationally identical (e.g., they both correlate strongly with a desired outcome) may not be interchangeable. Transparency around how the measurement is carried out may help build trust in a metric. For example, some platforms periodically release public transparency reports which include various metrics [308]. Another way to increase legitimacy is to establish accountability regarding a measurement, e.g., through independent, external audits of the measurement [329]. More ambitiously, a metric could be created or chosen through a participatory process [320]. For instance, the measurement could aggregate the opinions of a panel of users, as in the “digital juries” [112] and “citizens assembly” concepts for making platform decisions [255]. In one case, representatives of various stakeholders participated in the construction of a recommender that matched supermarket excess food donations to volunteer drivers and local food banks, using an elicitation process to define a ranking function [201].\n\n4.3 Data Sources for Measuring Values\n\nOur news recommender diversity example involved a complex, multi-step process for defining an algorithmic measure, ultimately drawing on human labeling to define the value of diversity. Broadly speaking, there are three main data sources for value measurements on a recommender platform.\n\nThe first source of data is the behavioral signals that users generate during normal usage, e.g., articles a user clicks on, songs a user plays, comments, emoji reactions, re-sharing of content, ads a user clicks on, purchases made, time spent on the platform and on specific items, and so on. These sorts of implicit signals of value are often called “engagement,” but we note that some behavior signals are explicitly designed to give feedback to the algorithm, such as swipe left/right or thumbs up/down. Implicit and explicit behavioral feedback both provide distinct and useful information to a recommender system [172].\n\nThe second source for signals of value is answers to survey questions that are posed to a fraction of the user base, typically a very small fraction. Surveys can ask very targeted questions, e.g., Facebook has asked users whether individual posts were “worth their time” [107] while YouTube uses user satisfaction surveys that ask users to evaluate previous recommendations [131, 389]. Survey results can be used to monitor real-world outcomes, evaluate A/B tests, and ultimately recommend items that a user is predicted to respond positively to when asked about their experience.\n\nThe third source for value measurements is human annotation. This data is often produced by paid raters, though human ratings also come from users flagging or reporting items. Vast amounts of human training data are routinely used to create models for identifying particular kinds of positive or negative content for the purpose of content moderation and ranking. Platforms that feature professionally-made content can also ask creators to provide metadata for a variety of ranking purposes. For instance, the Swedish public broadcaster asks editors to rate each story in terms of importance, public service value, and lifespan [217]. While survey data is limited by how often users can be asked to fill out a form, annotation data is costly to create and therefore available in limited quantities, and some types of labeling work may contribute to mental health issues for raters [15]. Human annotation can also be noisy, inconsistent, and biased depending on how rater pools are selected, and have limited, unbalanced coverage, while the meaning or usage of labels can change over time. Research in the area of human computation [198] attempts to address such issues [111].\n\nIn contrast to surveys and annotation, the benefit of behavioral signals is that they are plentiful—many orders of magnitude more data is available. However, the behavior of users on the platform typically correlates with but does not perfectly capture any particular type of value, and is only a proxy for what different stakeholders actually care about. Moreover, behavioral signals have been shown to be sensitive to a variety of factors, such as the user and their demographics [58, 146], the context in which the user is interacting with the system, and the recency of interactions. For example dwell time, a behavioral signal that has been used as a proxy for user satisfaction [379], can vary significantly depending on whether the item is the first one clicked in a list of results or not [39]. As discussed above, behavior does not represent underlying preferences for a variety of reasons including cognitive biases, information asymmetry, coercion, lowered expectations, and so on [9, 32, 54]. Additionally, the collection of vast amounts of user data poses significant privacy concerns.\n\nOne important approach that tries to combine the benefits of behavioral and survey signals is to learn a model that predicts survey results given user behavior, that is, it tries to predict how a particular user would answer a survey question if shown a particular set of items. This method extrapolates limited survey data to all users, and is common in industry [131, 320] but has not been much discussed. We view such prediction of survey results as an emerging method for aligning recommender systems with human values, though it must be understood that predictions are just proxies and must be continually monitored for divergence from ground-truth survey responses.\n\nIn principle, surveys can elicit complex judgements of value. In addition to the challenges of complicated, multi-component values (e.g., well-being) the context and wording of a survey can significantly affect the results [317, 364]. Social desirability bias, the tendency of respondents to answer in ways that others would view favorably, adds a further complication [124, 239]. Another issue arises when users from different demographic groups tend to answer questions differently [102]. Since online surveys often involve casual participants, “seriousness” checks and data denoising can be very important [17].\n\n5 Design Approaches\n\nWe are now ready to discuss the major approaches that recommender system developers and researchers have used to promote the values discussed above, and in turn, to identify some of the challenges and open problems which require the development of new technologies. The structure of this section reflects the issues presented above. Section 5.1 discusses the core of the recommender system, namely, the techniques used to select and order items shown to the user, and long-term user paths through these items. Section 5.2 discusses the affordances and controls made available to users and other stakeholders through UI and UX design. Finally, Section 5.3 considers fairness and multi-stakeholder perspectives, and describes techniques used by system designers to help optimize tradeoffs within real systems.\n\n5.1 Item Selection\n\nAt the heart of any recommendation system are the models and algorithms used to generate one or more recommended items. Generally, the core models are trained to predict a user's behavioral response to a candidate recommendation (e.g., watch, like, reply, purchase) using properties of the user, the candidate items, and the context. Items are then scored by combining predicted outcomes in some way [227, 389]. The nature of the properties used to make predictions, the predicted responses themselves, and the way these predictions are combined into a final score all play a role in the values that a recommender embodies.\n\n5.1.1 Item Ranking Signals.\n\nThe properties of an item being considered for recommendation—whether a social media post, news article, or musical track—play a key role in determining whether it is of interest to the user, and, therefore, whether showing it to a user can serve one of their values.\n\nThe “topic” of an item is an important signal. A variety of techniques have been used for text analysis in recommender systems, including latent semantic indexing in Google News [79], Latent Dirichlet allocation [173], and transformers. Image and video analysis are also used for topic assessment [82], as is audio analysis [381]. Modern recommender topic taxonomies can encompass tens of thousands, or even millions, of distinct categories and sub-categories. One of the challenges is that these classifiers are typically accurate for the popular elements of the taxonomy of topics, but often do not perform well for posts on less popular topics.\n\nTopics do not directly correspond to specific user needs or to values. A post about “football” can be about organizing a football viewing party (possibly contributing to the value of connection) or just reporting the latest scores or player injuries (the value of knowledge). To support the value of empathy and care it would be useful to identify posts where the poster could benefit from support of their network (e.g., after losing a loved one, needing advice on a certain matter, or announcing an important event in their life). An open challenge is inferring the way a particular user will relate to an item, rather than analyzing properties of the item alone. There is nascent work on predicting the experience a user may have when viewing content, e.g., whether they are entertained, angry, or inspired [67, 163]. These sorts of inferences could also enable better targeted or more persuasive advertising, which raises policy concerns [304].\n\nContent analysis is also heavily used by social media platforms in order to determine whether a post violates community policies or is of low quality in some way, and should therefore be removed or demoted. The predominant method for determining such violations uses ML models trained on content labels provided by paid raters. These models increasingly use multimodal techniques, simultaneously considering text, speech/audio, images, and video. This is often crucial to understanding the intent of a post, e.g., the caption “love the way you smell today” means something different when superimposed over the image of a rose vs. a skunk [184].\n\nIncreasingly, items are evaluated not just in terms of their content but their context, including properties of the poster and audience, previous user behavior, reactions from other users, and so on [142]. Incorporating embeddings of sequences of user interactions has been shown to increase accuracy in misinformation and hate speech classification on Facebook [246] while “toxic” conversations on Twitter are better predicted by including network structure features [290].\n\nThese models are generally trained on labels or annotations generated by human raters. This process itself is potentially subject to error and particular forms of bias, both in the instructions given to evaluators and their assessment biases [287]. Also, many of these categories are both complex and politically contested [46, 214] and raters often disagree, which complicates the evaluation of model accuracy [134]. The promotion or demotion of particular topics or types of language, and the errors and biases in this process, have major implications for freedom of expression, which implies a deep connection between these technical methods and broader policy considerations [91, 181, 380].\n\n5.1.2 User Trajectories.\n\nWe use “trajectory” to refer both to the sequence of items a user saw over time and the reactions those items evoked. A user's past trajectory often provides information that can be used to make better predictions about their future preferences and behaviors. At the same time, we want to ensure that the user's future trajectory supports the values they care about. Moreover, trajectories are the basic unit for studying some problems users experience on platforms, as they are central to discussions about potential long term effects on, say, well-being and polarization. For example, the “filter bubble” critique is essentially a statement about the typical course of user trajectories.\n\nThe majority of recommenders deployed in practice are “myopic” in the sense that they make predictions of a user's immediate response to the next slate of items presented, and rank items based on these predictions. However, many recommenders use past sequences of user and system behavior in sophisticated ways.\n\nAdvances in deep learning have made possible the practical deployment of sequence models, such as recurrent neural networks or transformers. For example, Beutel et al. [36] describe a recurrent (specifically, an LSTM) model deployed at YouTube. Recurrent approaches explicitly model a user's “latent” or “hidden” state, that is, they include variables which represent aspects of the user's situation or psychological state which are unobserved but have effects on what the user wants to see next. This state might encode aspects such as user satisfaction, frustration, or current topic focus, but interpretation of the hidden state embodied in such models is challenging and is tightly coupled to the engagement metrics being predicted and optimized. Inferring user state from behavior is an important challenge and a key step toward better supporting many values. For example, a recommender could detect a user's dissatisfaction with a certain type of content, or with too little topic diversity in the recommendation stream, or even that someone was developing an eating disorder [377].\n\nCurrent ranking techniques do not offer the means to (directly) optimize a user's future trajectory. A promising direction is the use of reinforcement learning (RL) for optimizing such futures non-myopically [5, 297, 330]. In particular, the use of RL allows the system to consider the impact on the user of not just immediate recommendations, but of the entire sequence of recommendations it might make over an extended period, and plan that sequence accordingly.\n\nIn our news diversity example, the redesigned recommender considered diversity within each slate of recommended items independently. An RL-based recommender would be able to consider the diversity of items over days or weeks as well. In an educational setting—where a user's understanding of a topic might best be served by an individualized sequence of content— an RL recommender resulted in faster learning and more course completions than a baseline linear sequence [24]. As such, RL offers considerable promise as a technology for individual or user-level value alignment. To date it has been used largely to optimize user engagement over the long term, but with suitable metrics and objective criteria, it could play a vital role in better aligning recommendation trajectories with user well-being by adaptively planning recommendation sequences.\n\nAs with sequence models, advances in deep RL methods have made it more practical to deploy RL in practical recommender systems [66, 126, 160]. That said, a number of challenges remain. First, since RL relies on sequential interaction data with real users, such models are often trained offline using data generated by past recommendations, which may induce bias because the user was interacting with a different model [66]. The second challenge involves choosing from a large action space ranging from hundreds to millions depending on the recommendation task [94, 160]. A related problem is that item recommendations are often made jointly in slates or scrolling feeds, where the interactions or interference among the visible items makes interpreting and optimizing for user responses challenging [160]. Finally, adopting RL for true value-alignment requires sophisticated models of various aspects of user latent state (preferences, satisfaction, awareness/knowledge levels, fatigue/boredom, etc.) and their dynamics. These psychological and situational states are challenging to uncover from observable user-response behavior, and may require planning over extremely long “event horizons” as user adaptation to recommender changes may take 3–6 months to fully materialize [57, 229].\n\nSome work has considered trajectories that might be unhealthy or harmful. One problem concerns minimizing the likelihood that trajectories will end up with users viewing large amounts of questionable content, such as videos that might promote unhealthy behavior, false statements about COVID vaccines [61], or other content that may not explicitly violate the platform's policies. This might occur not because users explicitly ask for such material, but because they end up in places (e.g., groups) where this material is common. In addition to downranking such content, another solution is to avoid recommending low-quality groups or content sources to users.\n\nAnother strand of work concerns items that are acceptable or appropriate when considered in isolation but could be harmful if consumed too much or by certain vulnerable people. For example, there may be nothing wrong with a diet video but perhaps someone with an eating disorder should not be presented with an endless stream of such videos; or it may be unhealthy to watch exclusively violent movies. Singh et al. [301] address this possibility by ranking user trajectories by proportion of “unhealthy” content, then using the mean proportion of this content in the top αth percentile of user trajectories as a regularization metric. They demonstrate a safe RL approach that improves both worst-case and average-case outcomes, in terms of the fraction of “unhealthy” items recommended to any one user.\n\nThe most complex concern about trajectories is the possibility that recommender systems might change user preferences in a manner that increases engagement but harms some other value. This is commonly associated with the idea of “manipulation,” meaning unwanted changes in attitude or behavior (even if unintentional).\n\nThis sort of optimization-driven shift has been frequently suggested as a mechanism driving filter bubble, echo chamber, or polarization effects, though the empirical evidence is mixed [50, 139, 249, 321, 392]. Such models posit a feedback loop where users choose particular items (as in selective exposure effects [265]) and the recommender responds to that engagement by narrowing its output to those topics, which in turn shifts user preferences further in that direction. This effect may be a particular concern for RL systems, which may learn how to make their users more predictable so as to maximize engagement [283]. A polarizing preference shift effect has been demonstrated in multiple simulations with different specifications [57, 105, 175, 177, 192, 284] which suggests that it could be a robust effect.\n\nA range of work has attempted to determine the causal effect of recommender systems on content consumption trajectories. This is particularly important if those trajectories are correlated with offline outcomes such as violence [129]. There is replicated evidence that strongly moralizing expression spreads faster than other content on social networks [47] and such moralizing seems to precede offline violence [232]. A number of researchers interested in categories such as “far right,” “conspiracy,” or “radical” have studied the network structure of recommendations between YouTube channels [110, 200, 275]. While this showed that more extreme channels often link to each other, these studies do not analyze user trajectories because they were conducted without any personalization. A different approach is to program bots to selectively engage with certain topics. This has generally shown that engaging with some type of content increases its frequency [360, 368] but this design models users as unchanging, so it does not provide evidence on persuasive effects. A user-tracking study of “far right” content on YouTube from 2016 to 2019 found that consumption there matched broader patterns across the web, including consumption from sources without recommender systems [156]. Separating consumption due to recommenders and consumption due to users’ seeking behavior is difficult without on-platform experiments. Using a set of Twitter users randomly assigned to receive a baseline chronological feed, Huszár et al. [159] found that Twitter's home timeline recommender reduced the consumption of more politically extreme material.\n\nIn general, inference of the causal effects of recommenders on user preferences and outcome is a major open problem. The feedback effects between algorithms and societies are at the cutting edge of social science research, in need of both interdisciplinary and cross-sector collaboration.\n\n5.2 Controls and Feedback\n\nMeaningfully supporting human values requires effective communication between the user and the system, beyond the standard implicit signals (clicks, shares, dwell time, etc.). We place communication affordances on a continuous spectrum between explicit and implicit. We use controls to refer to features where the user can explicitly change certain settings, and feedback to refer to situations where the user is more passive and their preferences are elicited in some way, such as by answering a survey question, providing a like/dislike, and so on.\n\nThere are a variety of documented benefits to providing users with more control over their experience. Users have reported being more satisfied with their recommendations when given greater control [176] and better controls have led to increased engagement [171]. Interestingly, even the appearance of controls can achieve some of these effects. In [348], users reported higher satisfaction with the system even though the controls had random effects. Conversely, Loecherbach [207] gave users control over the diversity of items in their feed but there was no correlation between actual and perceived diversity. In general, greater recommender control usage does not always translate into better control [359].\n\nExisting systems offer a variety of ways for a user to customize their experience, and many more control and feedback schemes have been proposed by researchers [147]. Some involve simple direct feedback on individual items, e.g., thumbs up/thumbs down, or item-level ratings [42, 388]. Others involve evaluating the relevant importance of pairs of items [41, 65]. Even when controls are provided, many users do not know that they exist or what they do [157, 306], find them challenging to use, or simply don't see the value in engaging with them. As a result, most users do not use recommender controls and a “passive” user experience remains the default [176]. Aside from better educating users on controls, a promising approach is to design feedback mechanisms that serve multiple purposes: such as sending a social signal (which is why the user would use it) and simultaneously providing direct feedback that pertains to some value. Some examples of this exist today, such as the “Insightful” emoji on LinkedIn, the “Care” emoji on Facebook, and the proposed Respect button [324].\n\nOne of the issues that limits the use of controls is the difficulty in creating a direct, understandable link between the input a user provides and the resulting change in the system's behavior. In some cases, this link is quite direct (e.g., “don't show me any sports-related content”), whereas in others the outcome of a control/feedback action is less obvious (e.g., “show me more videos like this video I just watched”). There are many points along this spectrum. For example, instructing the system to “show less about football” will be largely predictable in terms of the content it affects (though what about political statements/protests by athletes?) but users may still not understand the expected magnitude of change. To complicate things further, there is often some ambiguity about whether the change the control offers is transient (e.g., for the current session) or for the longer term. One proposed design is to highlight items that will be added and removed from the user's feed when a control is changed [292].\n\nOrganizing content into “channels” can help users to better customize their experience. Some systems include implicit ways of specifying what sort of items are to be included, such as music recommenders which can continue a human-generated playlist [382]. Algorithmic channels could also be designed around particular goals (e.g., learning to play guitar) or needs (e.g., getting support from friends, participating in lively conversations). The primary technical challenge here is building relevance measures that capture the dimensions that users care about.\n\nIn the future, communication between the user and the system will take different forms. Conversational recommender systems are an emerging area that offers a potentially more natural, engaging, and usable interface for people to express their preferences, particularly in light of recent advances in speech, natural language and dialog technologies [3]. Applications range from integrations with the approaches listed above to entire reimaginings of the control process [71, 170]. A related idea is recommender personae, where the recommender is associated with a particular personality (e.g., explorer, diplomat, expert) to set a particular context for the conversation [144]. Conversational recommenders face many of the same challenges as other control systems, but more so: they must interpret the metaphorical, imprecise, or subjective language a user may use to convey their needs or topic preferences.\n\n5.2.1 Meaningful Explainability.\n\nThe field of explainable recommender systems has grown into a vibrant research area [386] in part because users respond positively to having good justifications for why certain items are suggested. Tintarev and Masthoff [337] argue that a good explanation can increase transparency, scrutability, trust, effectiveness, efficiency, persuasiveness, and satisfaction in the system. Each of these terms expresses somewhat different goals: transparency shows how the system works and can be instrumental in accountability; scrutability allows the users to tell the system it is wrong; trust increases confidence in the system; and satisfaction increases users’ sense of derived benefit; effectiveness helps the user employ the system toward their own aims; persuasiveness convinces the user to change their beliefs or behavior; efficiency can increase the value of the system. Some of these values can be contradictory and may not be achieved at the same time. For example, an explanation that increases transparency of the system does not necessarily increase trust if the explanation is not understandable or reveals undesirable behavior. Therefore, what constitutes a “good” explanation very much depends on the goal, and the field is rapidly developing [145, 241, 248].\n\nWith the rise of increasingly complex machine learning models, it has also become increasingly difficult to give intuitive and understandable explanations of why a user received a specific recommendation. Explanations may be shown to the user in different forms (e.g., text, visuals, highlighting relevant features) and may either attempt to explain the workings of the recommendation model itself, or may be the result of training a separate model that generates post-hoc explanations from model inputs and outputs [386]. There are also recommender algorithms specifically designed to be explainable [376]. The system of Balog et al. [19], for instance, operates based on set intersections, e.g., “recommended for you because you don't like science fiction movies except those about space exploration.” This explainable-by-design approach avoids the challenges of interpreting learned models [282], but generating understandable explanations from deep learning models is an active area of research that may yet prove fruitful [185, 189, 328].\n\n5.3 Making Tradeoffs\n\nIn general, there are numerous tradeoffs involved when incorporating human values into recommender systems, and a variety of techniques to evaluate these tradeoffs and make choices. There are at least three categories of tradeoffs: (1) tradeoffs between the benefits and potential harms to different people, (2) tradeoffs between different types of stakeholders, such as content creators versus content consumers, and (3) tradeoffs between values, resulting in different (but not obviously worse or better) measurable outcomes induced by various recommendation strategies.\n\nUltimately there must be some notion of a fair or optimal tradeoff, and techniques for making “good” tradeoffs in this sense. We hope that these tradeoffs are informed by the expressed opinions of users and other stakeholders, so we first discuss the theory of social choice, which studies how to combine the preferences of many people. We then discuss techniques for achieving various notions of fairness, such as between different types of users or between different stakeholder categories. Finally, we discuss tools designed to optimize tradeoffs when faced with the practical necessity of tuning a recommender's ranking function.\n\n5.3.1 Tradeoffs in Theory: Social Choice.\n\nPeople express their values in their everyday use of recommender systems. Indeed, many recommender controls are designed specifically for this purpose, everything from upvoting and swiping left/right to reporting violating content. This creates the problem of aggregating preferences, and negotiating between competing desires of individuals, groups, and stakeholders. The framework of social choice, originally developed in economics [125, 294], provides a foundational tool for addressing tradeoffs at all of these levels.\n\nAbstractly, this framework assumes that each stakeholder has a utility function over a set of possible outcomes for them. This is motivated by the idea that someone could say which of several situations they'd prefer, that is, that each person has preferences. Note that this utility function can be purely “local” (a user may care only about whether they get good recommendations) or it can involve societal values (a user may care that other users also like what they like, or that vendors are treated fairly). A social welfare function is a voting process or a way of “adding up” or combining stakeholder preferences to produce a single number, a “societal utility” or social welfare. The aim is then to adopt a recommender policy which maximizes the expected social welfare.\n\nThis formulation can encompass virtually any criteria that express preferences over short or long-term, individual or group outcomes. There are relatively direct mathematical expressions for penalizing addictive behaviors, group-level diversity of consumed content, fairness across individuals, and so on. Conversely, any collaborative recommender system aggregates the explicit or implicit preferences of users in some mechanical way, as when signals like upvoting and watch time are combined across many users to decide how items are ranked for a different user. Social choice theory is a key bridge between the normative and the algorithmic, useful for analysis and design.\n\nAlthough social choice theory is foundational, there are two major difficulties to applying it in practice. First, determining what an individual's “preferences” actually are is quite challenging both in theory and in practice [334]. There is a long history of formal elicitation methods such as asking users to repeatedly say which of two options they prefer, or asking them to play various kinds of economic games [53, 65], but formal preference elicitation can be involved and is quite demanding of the participants. Furthermore, someone might be uninformed, coerced, addicted, have lowered expectations [9] or want something that isn't available [119, 277]. In addition, attempts to elicit preferences can lead to strategic behavior where people misrepresent their beliefs to try to induce more favorable recommender outcomes, e.g., a content creator usually has an incentive to argue that their content is relevant to as many users as possible [29, 30]. While many different kinds of feedback can provide crucial signals of what people value, behavior cannot be naively interpreted as true preferences.\n\nSecond, there is no entirely bottom-up or value-neutral method of ethics. Simply specifying the outcomes over which stakeholder preferences are defined is inextricably tied to the values being considered [32]. For example, there is the choice of what will be voted upon. There is also the question of whose preferences count in what situations, e.g., community administrators may have special voting privileges, and it may be important to encode “rights” that cannot be infringed by the preferences of others. Hence, social choice approaches cannot excuse system designers from making consequential normative choices [27]. There is more to democracy than voting systems.\n\n5.3.2 Fairness and Tradeoffs Among Stakeholders.\n\nMany of the values in our list involve making tradeoffs between different stakeholders (such as users vs. content providers) or among members of a stakeholder group (such as different subgroups of content providers which compete for attention). Correspondingly, there are a wide variety of notions of “fairness,” which often (but not always) are framed as some sort of tradeoff. The extent to which these tradeoffs are inherent is an open question in the research literature because there are cases where it is possible to improve performance for one user subgroup without decreasing performance for other users. There are multiple challenges in this area, including defining fairness, measuring it in practice, and designing algorithms for efficient recommendation. See Ekstrand et al. [98] for an overview.\n\nSeveral major categories of fairness have been proposed in the context of recommender systems, roughly corresponding to the interests of different stakeholders. “C fairness” considers how well individual information consumers are served, “P fairness” is concerned with the distribution of attention between items or providers of content, while “CP fairness” considers both simultaneously, as in a rental property recommender designed to protect the rights of both minority renters and minority landlords [52, 361].\n\nRecommender systems are mostly evaluated based on average performance across all users, but different user subgroups, such as age or gender groups, might be served with differing performance or error rates. Subgroup performance disparities can happen for a variety of reasons, including differences in group size or activity that affect the amount of training data available [100, 204]. There is a large body of work on mitigating group-level unfairness in classifier models, some of which can be adapted to recommender systems. For example, [34, 35] use pairwise comparisons of the ranking of different items to generalize the well-known “equality of opportunity” and “equality of odds” measures, showing that it is possible to equalize prediction error rates between user groups on a large commercial platform. However, algorithmic approaches that aim at equalizing effectiveness disparities between user groups may make inappropriate tradeoffs: increasing recommendation utility for one user does not necessarily require decreasing it for other users, so it is not clear that allowing a solution that may decrease utility for well-served users is appropriate, as opposed to other approaches such as feature prioritization in the engineering process (Ekstrand et al., 2021 Section 5.4).\n\nItems and their producers, on the other hand, necessarily compete for user attention. This leads to the concept of “exposure fairness” which may be formulated in a variety of ways [302]. Even if predicted “relevance” scores correctly measure the value of an item to an individual user, slightly less relevant items may get disproportionately less attention simply because they appear farther down, an effect known as “position bias.” Several algorithms have been proposed to ensure item attention is proportional to item value on average, either across a group of items or across multiple slates of recommendations [37, 85, 361]. Such algorithms can be considered a correction to a type of error or “technical bias” in the fair ranking typology of Zehlike et al. [383]. More normative definitions of item fairness are often desirable. For example, Spotify strives to give exposure to less popular artists to counteract the “superstar economics” of cultural production [222], while a “demographic parity” conception of fairness may be appropriate when qualified candidates from different groups (say, men and women) should be shown to prospective employers at the same rate [127]. A wide variety of fairness metrics concerning the exposure of items, groups of items, or producers have been proposed, though many of these are closely related [193, 270, 288, 384].\n\nWhere it is possible to produce reasonable quantitative estimates of utility to different stakeholders, multi-objective optimization (MOO) can be used to balance multiple conflicting stakeholder utility and fairness objectives. One approach is to ensure that the recommender is Pareto efficient, meaning that there should be no way to modify a slate of recommendations to make it more fair without reducing utility, or to increase utility without reducing fairness [374]. Mladenov et al. [228] go beyond this by proposing a recommendation method that maximizes user social welfare (total user utility) by allowing small sacrifices in utility from well-served users to drive large gains for less well-served users. RL has also been applied to multi-sided fairness, through contextual multi-armed bandits which simultaneously optimize stakeholder utility and fairness objectives [222, 223, 375]. Recently, several researchers have taken a game-theoretic approach to the study of recommender systems. Ben-Porat and Tennenholtz [30] and Ben-Porat et al. [29] develop approaches that account for the strategic behavior of content providers while aiming at maximizing user engagement. While all these methods hold promise for value alignment in complex ecosystems, they have not seen practical deployment to date.\n\n5.3.3 Optimizing Tradeoffs.\n\nBecause there is no purely “bottom up” way of making tradeoffs, system designers must ultimately choose some set of overall objectives or “ground truth” signals to serve as overall measures of value. Increasingly, AI tools are used to help make tradeoffs over the complex design space of recommender parameters, particularly the relative “weighting” of the signals that feed into item ranking functions.\n\nMilli et al. [227] determined the relative value of different user actions including viewing a Tweet, sharing it, liking it, and so on by connecting these interactions to the sparse use of the “see less often” control in a causal Bayesian network. This network represents dependencies such as the fact that a user has to view a Tweet before they can share it. By taking “see less often” as a ground truth signal of negative value it was possible to infer the value of all of the other, more common interactions. This idea generalizes to more complex methods for finding multiple weights that optimize multiple metrics.\n\nBayesian optimization [130, 274] can be used to find the weights of a ranking function that maximizes relevant (perhaps long-term) metrics, and automatically run data-gathering experiments to improve those predictions [140]. This approach requires that the designer be able to assess the overall “utility” of any vector of performance metrics, which itself induces various tradeoffs. For example, is a product that has a greater number of items viewed but less total time spent and lower reported satisfaction better than the opposite? The tools of interactive optimization and utility elicitation [42, 388] could play an important role here, though these approaches have not yet found widespread use in practical recommender design.\n\n6 Policy Approaches\n\nIn the previous section, we have discussed design approaches to recommender systems and human values, that is, potential product changes. This section considers policy-making, which can be a powerful lever for change. Policy-making is informed by all of the perspectives articulated above, including ethical, procedural, measurement, and technical issues. Chosen policies impose constraints on how to build a recommender system and introduce additional technical challenges.\n\nBy policy-making we mean external governance, from government, regulators, or external bodies with appropriate authority. All large platforms have internal policies as well, particularly around which types of items are eligible for recommendation, but we focus on external governance as an important interface between recommender system operators and the rest of society. Because recommender systems are used in so many different types of products, we do not offer specific policy recommendations. Instead, our goal is to discuss the major categories of policies that have been proposed, and especially to understand how these policies could be translated into terms of metrics and algorithms. At the current time there is a large gap in terminology and understanding between the recommender technology and policy communities, which we seek to highlight and begin to address. There are also policy-relevant technology gaps: the capability to do what policy-makers ask may not yet exist, as in the “right to explanation” provisions of the GDPR [358].\n\nWe consider policy approaches that are relevant to recommender systems specifically, as opposed to social media or online platforms generally (neither of which necessarily involve recommender systems). We do not discuss content moderation policy approaches here, but direct readers to reviews such as [91, 181, 380].\n\n6.1 Risk Management vs. Value Sensitive Design\n\nOne proposed policy approach would require recommender system operators to evaluate the potential risk of harm from operating their systems. This is the approach taken by the EU Digital Services Act, which requires “very large online platforms” (currently defined as those with more than 45 million users in the EU) to perform yearly assessments for three kinds of risks: the dissemination of illegal content, negative effects on fundamental rights, and “manipulation” with effects on “public health, minors, civic discourse, or actual or foreseeable effects related to electoral processes and public security.” [104] Any harm found must be mitigated through various means including “adapting content moderation or recommender systems.” The proposed Digital Services Oversight and Safety Act (DSOSA) of 2022 in the U.S. takes a similar approach, requiring platforms of certain sizes and scope to conduct assessments and mitigate any risks, including by “adapting the content moderation or recommender systems (including policies and enforcement) of the provider.\n\nThis touches on several of the values in our table but is relatively narrow in two ways. First, a risk-based framework is concerned only with potential harms. Second, the type of risk mitigation envisioned by the DSA and the DSOSA generally happens after a system is already built. Another approach is to require consideration of important values during the design phase, as with “privacy by design” provisions [92]. Extending this to more general values, one German law requires platforms to meet certain content diversity obligations [151].\n\nThe challenge for policy-makers or regulators is to be both precise and general about how harms are to be assessed and values are to be enacted in recommender systems. In principle, this could involve monitoring certain metrics, as is already done in environmental regulation and media monopoly policy. Such regulation would face all of the challenges of choosing metrics discussed above, and certainly no one set of metrics will be appropriate for all recommender systems. Even if useful metrics for harm or good can be found, there is the difficult question of what constitutes an “acceptable” value [244].\n\n6.2 Accountability\n\nPolicy approaches to the issue of accountability include provisions around transparency and evaluation mechanisms such as audits. Both are instrumental in supporting other values such as agency, control, and accountability.\n\nTransparency has been a major focal point of legislative efforts in the United States and the European Union. Over the past two years, lawmakers in the United States have introduced numerous bills that seek to compel internet platforms to provide more transparency around how they develop and deploy algorithmic systems for content curation purposes [234, 303]. The key challenge from a policy perspective is in defining the goal "
    }
}
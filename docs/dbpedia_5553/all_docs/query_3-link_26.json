{
    "id": "dbpedia_5553_3",
    "rank": 26,
    "data": {
        "url": "https://github.com/MinghuiChen43/awesome-deep-phenomena",
        "read_more_link": "",
        "language": "en",
        "title": "phenomena: A curated list of papers of interesting empirical study and insight on deep learning. Continually updating...",
        "top_image": "https://opengraph.githubassets.com/2c05584afac2532fc1e6a615c6317af7d9607265886ac7ac636c4deeeb443ddc/MinghuiChen43/awesome-deep-phenomena",
        "meta_img": "https://opengraph.githubassets.com/2c05584afac2532fc1e6a615c6317af7d9607265886ac7ac636c4deeeb443ddc/MinghuiChen43/awesome-deep-phenomena",
        "images": [
            "https://camo.githubusercontent.com/bf00e4154552d507a39364381e5d48ff28b86c34921cf59adb137db0fa10988d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4d61696e7461696e65642533462d5945532d677265656e2e737667",
            "https://camo.githubusercontent.com/519f7d400497993d70858432f8465561d88704f22257d59b5f85391b989ee2d0/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5052732d77656c636f6d652d627269676874677265656e",
            "https://camo.githubusercontent.com/9e0050922ad6d498ae79febe6d1a353d79755d5b2f088047a67fe1dfedb42bd9/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6173742d636f6d6d69742f4d696e676875694368656e34332f617765736f6d652d646565702d7068656e6f6d656e61",
            "https://camo.githubusercontent.com/872ecd9c350b92813a6ee9ad8100cf1833f72630aa11e08c063a324a01ded58b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4d696e676875694368656e34332f617765736f6d652d646565702d7068656e6f6d656e613f636f6c6f723d626c7565267374796c653d706c6173746963",
            "https://camo.githubusercontent.com/5d5ee8453dea3200ea325de699c404e97c9c7b1afd53341120eecb9fdb707254/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f77617463686572732f4d696e676875694368656e34332f617765736f6d652d646565702d7068656e6f6d656e613f636f6c6f723d79656c6c6f77267374796c653d706c6173746963",
            "https://camo.githubusercontent.com/4358c9394ab0d801c61b5c7f9204c9857b6134564e4a1094f58cb6aadd07bdfb/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f4d696e676875694368656e34332f617765736f6d652d646565702d7068656e6f6d656e613f636f6c6f723d726564267374796c653d706c6173746963",
            "https://camo.githubusercontent.com/19350a6f3a741def7de6fd5e22c98bcec1b1f78a38989ecee80653563b67683d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6e7472696275746f72732f4d696e676875694368656e34332f617765736f6d652d646565702d7068656e6f6d656e613f636f6c6f723d677265656e267374796c653d706c6173746963",
            "https://camo.githubusercontent.com/50cf39121274b3db22bf1bd72cbe25af9078e037441cb5b5bdef1cc9dc5eb2f7/68747470733a2f2f63646e2e7261776769742e636f6d2f73696e647265736f726875732f617765736f6d652f643733303566333864323966656437386661383536353265336136336531353464643865383832392f6d656469612f62616467652e737667",
            "https://github.com/MinghuiChen43/awesome-deep-phenomena/raw/master/img/DALL·E 2024-01-15 16.32.32 - A highly detailed digital art piece featuring an artificial brain integrated with advanced technology and physics concepts. The brain, depicted in the.png",
            "https://camo.githubusercontent.com/9c9e84e83be04164aa004e04d411fe520adc6e5ee38eac948e3200535452c0bf/68747470733a2f2f64616e69656c74616b657368692e6769746875622e696f2f6173736574732f756e6465727374616e64696e675f646c5f72657468696e6b696e675f67656e2e706e67",
            "https://github.com/MinghuiChen43/awesome-deep-phenomena/raw/master/img/NC_animation.gif",
            "https://camo.githubusercontent.com/07df356fabf35f946c2e1cc420349f579119c8c2724cd51d53afe969cb8e0dd9/68747470733a2f2f6d69726f2e6d656469756d2e636f6d2f76322f726573697a653a6669743a313430302f312a2d546e79595177624e30673667686b46716a64372d412e706e67",
            "https://camo.githubusercontent.com/df03ffaea8b1cf9f56009ceaebd2f76c17f04641f6ab3bdae0b58b271b113299/68747470733a2f2f6d69726f2e6d656469756d2e636f6d2f76322f726573697a653a6669743a313430302f312a6a62303758694f4e7879326a4d70527151596a4471772e706e67",
            "https://github.com/Hannibal046/Awesome-LLM/raw/main/resources/image8.gif",
            "https://camo.githubusercontent.com/7f06b7d9564e604a6fd807029f3957ba81d69b468c33123d6d3f050283253d92/68747470733a2f2f6d69726f2e6d656469756d2e636f6d2f76322f726573697a653a6669743a313230302f312a686b596c544f44706a4a676f3332446f434f574e35772e706e67",
            "https://camo.githubusercontent.com/201f81b110e08c031d744edf9580f855dac9cf7fedcb83c553727f73858c0e83/68747470733a2f2f6c696c69616e77656e672e6769746875622e696f2f706f7374732f323031372d30392d32382d696e666f726d6174696f6e2d626f74746c656e65636b2f69622d6c61796572732e706e67",
            "https://camo.githubusercontent.com/becbc6327cf4aa9fa1ddd07822cee0198a86c87efc2e8c881e64495dcc10612a/68747470733a2f2f692e7974696d672e636f6d2f76692f72615432454372766261672f6d617872657364656661756c742e6a7067",
            "https://camo.githubusercontent.com/5787c97e61bbc51314e33f9750e44b5ee9a2d6b4df6cc543ca876b69ef24bc5c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f5a4959552d444545502f417765736f6d652d496e666f726d6174696f6e2d426f74746c656e65636b",
            "https://camo.githubusercontent.com/e2a4300a94792d0eed3eadb8837fbbada54b6de08cb5afa0eb1f7b32c0747717/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6173742d636f6d6d69742f5a4959552d444545502f417765736f6d652d496e666f726d6174696f6e2d426f74746c656e65636b",
            "https://camo.githubusercontent.com/9bb82a17e2b1ccceec2fbffa50bd8392831d4d31b36e541a3533aa7aa14f0bc8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6b7769676e622f4e657572616c54616e67656e744b65726e656c2d506170657273",
            "https://camo.githubusercontent.com/463ad1c05ee505f559a29673073455ed3a86c878491015ca5bdcadce9ff59946/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6173742d636f6d6d69742f6b7769676e622f4e657572616c54616e67656e744b65726e656c2d506170657273",
            "https://camo.githubusercontent.com/745240a66c7f53d68a95543314b6a8e94ea8876ef2743b6e243d827043b1a013/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f5765694875616e6730352f417765736f6d652d466561747572652d4c6561726e696e672d696e2d446565702d4c6561726e696e672d54686f657279",
            "https://camo.githubusercontent.com/b1d0cb77ab1eff7c15d4b47a6e6f2cd74fcc3b1d16b2cf00b88a9dc645672ee2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6173742d636f6d6d69742f5765694875616e6730352f417765736f6d652d466561747572652d4c6561726e696e672d696e2d446565702d4c6561726e696e672d54686f657279",
            "https://camo.githubusercontent.com/774b5bb312c78d7cc0db9b55f777880e5594aba09fe4371df0c5ccac10448078/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4d696e676875694368656e34332f617765736f6d652d7472757374776f727468792d646565702d6c6561726e696e67",
            "https://camo.githubusercontent.com/d856658b96bc3becc19420a6566a7151a66b5c5455b302269b5b9a1be34028a1/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6173742d636f6d6d69742f4d696e676875694368656e34332f617765736f6d652d7472757374776f727468792d646565702d6c6561726e696e67"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "A curated list of papers of interesting empirical study and insight on deep learning. Continually updating... - MinghuiChen43/awesome-deep-phenomena",
        "meta_lang": "en",
        "meta_favicon": "https://github.com/fluidicon.png",
        "meta_site_name": "GitHub",
        "canonical_link": "https://github.com/MinghuiChen43/awesome-deep-phenomena",
        "text": "Awesome Deep Phenomena\n\nOur understanding of modern neural networks lags behind their practical successes. This growing gap poses a challenge to the pace of progress in machine learning because fewer pillars of knowledge are available to designers of models and algorithms (Hanie Sedghi). Inspired by the ICML 2019 workshop Identifying and Understanding Deep Learning Phenomena, I collect papers and related resources which present interesting empirical study and insight into the nature of deep learning.\n\nTable of Contents\n\nEmpirical Study\n\nNeural Collapse\n\nDeep Double Descent\n\nLottery Ticket Hypothesis\n\nEmergence and Phase Transitions\n\nInteractions with Neuroscience\n\nInformation Bottleneck\n\nNeural Tangent Kernel\n\nOther Papers\n\nResources\n\nEmpirical Study\n\nEmpirical Study: 2024\n\nAI models collapse when trained on recursively generated data. [paper]\n\nIlia Shumailov, Zakhar Shumaylov, Yiren Zhao, Nicolas Papernot, Ross Anderson, Yarin Gal. Nature\n\nKey Word: Model Collapse; Generative Model.\n\nDigest The paper demonstrates that AI models trained on recursively generated data suffer from a collapse in performance. This finding highlights the critical need for diverse and high-quality data sources to maintain the robustness and reliability of AI systems.\n\nNot All Language Model Features Are Linear. [paper]\n\nJoshua Engels, Isaac Liao, Eric J. Michaud, Wes Gurnee, Max Tegmark.\n\nKey Word: Large Language Model; Linear Representation Hypothesis.\n\nDigest This paper challenges the linear representation hypothesis in language models by proposing that some representations are inherently multi-dimensional. Using sparse autoencoders, the authors identify interpretable multi-dimensional features in GPT-2 and Mistral 7B, such as circular features for days and months, and demonstrate their computational significance through intervention experiments.\n\nLoRA Learns Less and Forgets Less. [paper]\n\nDan Biderman, Jose Gonzalez Ortiz, Jacob Portes, Mansheej Paul, Philip Greengard, Connor Jennings, Daniel King, Sam Havens, Vitaliy Chiley, Jonathan Frankle, Cody Blakeney, John P. Cunningham.\n\nKey Word: LoRA; Fine-Tuning; Learning-Forgetting Trade-off.\n\nDigest The study compares the performance of Low-Rank Adaptation (LoRA), a parameter-efficient finetuning method for large language models, with full finetuning in programming and mathematics domains. While LoRA generally underperforms compared to full finetuning, it better maintains the base model's performance on tasks outside the target domain, providing stronger regularization than common techniques like weight decay and dropout. Full finetuning learns perturbations with a rank that is 10-100X greater than typical LoRA configurations, which may explain some performance gaps. The study concludes with best practices for finetuning with LoRA.\n\nThe Platonic Representation Hypothesis. [paper]\n\nMinyoung Huh, Brian Cheung, Tongzhou Wang, Phillip Isola.\n\nKey Word: Foundation Models; Representational Convergence.\n\nDigest The paper argues that representations in AI models, especially deep networks, are converging. This convergence is observed across time, multiple domains, and different data modalities. As models get larger, they measure distance between data points in increasingly similar ways. The authors hypothesize that this convergence is moving towards a shared statistical model of reality, which they term the \"platonic representation.\" They discuss potential selective pressures towards this representation and explore the implications, limitations, and counterexamples to their analysis.\n\nThe Unreasonable Ineffectiveness of the Deeper Layers. [paper]\n\nAndrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo Glorioso, Daniel A. Roberts.\n\nKey Word: Large Language Model; Pruning.\n\nDigest This study explores a straightforward layer-pruning approach on widely-used pretrained large language models (LLMs), showing that removing up to half of the layers results in only minimal performance decline on various question-answering benchmarks. The method involves selecting the best layers to prune based on layer similarity, followed by minimal finetuning to mitigate any loss in performance. Specifically, it employs parameter-efficient finetuning techniques like quantization and Low Rank Adapters (QLoRA), enabling experiments on a single A100 GPU. The findings indicate that layer pruning could both reduce finetuning computational demands and enhance inference speed and memory efficiency. Moreover, the resilience of LLMs to layer removal raises questions about the effectiveness of current pretraining approaches or highlights the significant knowledge-storing capacity of the models' shallower layers.\n\nUnfamiliar Finetuning Examples Control How Language Models Hallucinate. [paper]\n\nKatie Kang, Eric Wallace, Claire Tomlin, Aviral Kumar, Sergey Levine.\n\nKey Word: Large Language Model; Hallucination; Supervised Fine-Tuning.\n\nDigest This study investigates the propensity of large language models (LLMs) to produce plausible but factually incorrect responses, focusing on their behavior with unfamiliar concepts. The research identifies a pattern where LLMs resort to hedged predictions for unfamiliar inputs, influenced by the supervision of such examples during fine-tuning. By adjusting the supervision of these examples, it's possible to direct LLM responses towards acknowledging their uncertainty (e.g., by saying \"I don't know\").\n\nWhen Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method. [paper]\n\nBiao Zhang, Zhongtao Liu, Colin Cherry, Orhan Firat.\n\nKey Word: Neural Scaling Laws; Large Language Model; Fine-Tuning.\n\nDigest This study investigates how scaling factors—model size, pretraining data size, finetuning parameter size, and finetuning data size—affect finetuning performance of large language models (LLMs) across two methods: full-model tuning (FMT) and parameter efficient tuning (PET). Experiments on bilingual LLMs for translation and summarization tasks reveal that finetuning performance scales multiplicatively with data size and other factors, favoring model scaling over pretraining data scaling, with PET parameter scaling showing limited effectiveness. These insights suggest the choice of finetuning method is highly task- and data-dependent, offering guidance for optimizing LLM finetuning strategies.\n\nRethink Model Re-Basin and the Linear Mode Connectivity. [paper]\n\nXingyu Qu, Samuel Horvath.\n\nKey Word: Linear Mode Connectivity; Model Merging; Re-Normalization; Pruning.\n\nDigest The paper discusses the \"model re-basin regime,\" where most solutions found by stochastic gradient descent (SGD) in sufficiently wide models converge to similar states, impacting model averaging. It identifies limitations in current strategies due to a poor understanding of the mechanisms involved. The study critiques existing matching algorithms for their inadequacies and proposes that proper re-normalization can address these issues. By adopting a more analytical approach, the paper reveals how matching algorithms and re-normalization interact, offering clearer insights and improvements over previous work. This includes a connection between linear mode connectivity and pruning, leading to a new lightweight post-pruning method that enhances existing pruning techniques.\n\nHow Good is a Single Basin? [paper]\n\nKai Lion, Lorenzo Noci, Thomas Hofmann, Gregor Bachmann.\n\nKey Word: Linear Mode Connectivity; Deep Ensembles.\n\nDigest This paper investigates the assumption that the multi-modal nature of neural loss landscapes is key to the success of deep ensembles. By creating \"connected\" ensembles that are confined to a single basin, the study finds that this limitation indeed reduces performance. However, it also discovers that distilling knowledge from multiple basins into these connected ensembles can offset the performance deficit, effectively creating multi-basin deep ensembles within a single basin. This suggests that while knowledge from outside a given basin exists within it, it is not readily accessible without learning from other basins.\n\nEmpirical Study: 2023\n\nTruth is in There: Improving Reasoning in Language Models with Layer-Selective Rank Reduction. [paper] [code]\n\nPratyusha Sharma, Jordan T. Ash, Dipendra Misra\n\nKey Word: Large Language Models; Reasoning.\n\nDigest Transformer-based Large Language Models (LLMs) have become a fixture in modern machine learning. Correspondingly, significant resources are allocated towards research that aims to further advance this technology, typically resulting in models of increasing size that are trained on increasing amounts of data. This work, however, demonstrates the surprising result that it is often possible to significantly improve the performance of LLMs by selectively removing higher-order components of their weight matrices. This simple intervention, which we call LAyer-SElective Rank reduction (LASER), can be done on a model after training has completed, and requires no additional parameters or data. We show extensive experiments demonstrating the generality of this finding across language models and datasets, and provide in-depth analyses offering insights into both when LASER is effective and the mechanism by which it operates.\n\nThe Transient Nature of Emergent In-Context Learning in Transformers. [paper]\n\nAaditya K. Singh, Stephanie C.Y. Chan, Ted Moskovitz, Erin Grant, Andrew M. Saxe, Felix Hill. NeurIPS 2023\n\nKey Word: In-Context Learning.\n\nDigest This paper shows that in-context learning (ICL) in transformers, where models exhibit abilities not explicitly trained for, is often transient rather than persistent during training. The authors find ICL emerges then disappears, giving way to in-weights learning (IWL). This occurs across model sizes and datasets, raising questions around stopping training early for ICL vs later for IWL. They suggest L2 regularization may lead to more persistent ICL, removing the need for early stopping based on ICL validation. The transience may be caused by competition between emerging ICL and IWL circuits in the model.\n\nWhat do larger image classifiers memorise? [paper]\n\nMichal Lukasik, Vaishnavh Nagarajan, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar.\n\nKey Word: Large Model; Memorization.\n\nDigest This paper explores the relationship between memorization and generalization in modern neural networks. It discusses Feldman's metric for measuring memorization and applies it to ResNet models for image classification. The paper then investigates whether larger neural models memorize more and finds that memorization trajectories vary across different training examples and model sizes. Additionally, it notes that knowledge distillation, a model compression technique, tends to inhibit memorization while improving generalization, particularly on examples with increasing memorization trajectories.\n\nCan Neural Network Memorization Be Localized? [paper]\n\nPratyush Maini, Michael C. Mozer, Hanie Sedghi, Zachary C. Lipton, J. Zico Kolter, Chiyuan Zhang. ICML 2023\n\nKey Word: Atypical Example Memorization; Location of Memorization; Task Specific Neurons.\n\nDigest The paper demonstrates that memorization in deep overparametrized networks is not limited to individual layers but rather confined to a small set of neurons across various layers of the model. Through experimental evidence from gradient accounting, layer rewinding, and retraining, the study reveals that most layers are redundant for example memorization, and the contributing layers are typically not the final layers. Additionally, the authors propose a new form of dropout called example-tied dropout, which allows them to selectively direct memorization to a pre-defined set of neurons, effectively reducing memorization accuracy while also reducing the generalization gap.\n\nNo Wrong Turns: The Simple Geometry Of Neural Networks Optimization Paths. [paper]\n\nCharles Guille-Escuret, Hiroki Naganuma, Kilian Fatras, Ioannis Mitliagkas.\n\nKey Word: Restricted Secant Inequality; Error Bound; Loss Landscape Geometry.\n\nDigest The paper explores the geometric properties of optimization paths in neural networks and reveals that the quantities related to the restricted secant inequality and error bound exhibit consistent behavior during training, suggesting that optimization trajectories encounter no significant obstacles and maintain stable dynamics, leading to linear convergence and supporting commonly used learning rate schedules.\n\nSharpness-Aware Minimization Leads to Low-Rank Features. [paper]\n\nMaksym Andriushchenko, Dara Bahri, Hossein Mobahi, Nicolas Flammarion.\n\nKey Word: Sharpness-Aware Minimization; Low-Rank Features.\n\nDigest Sharpness-aware minimization (SAM) is a method that minimizes the sharpness of the training loss of a neural network. It improves generalization and reduces the feature rank at different layers of a neural network. This low-rank effect occurs for different architectures and objectives. A significant number of activations get pruned by SAM, contributing to rank reduction. This effect can also occur in deep networks.\n\nA surprisingly simple technique to control the pretraining bias for better transfer: Expand or Narrow your representation. [paper]\n\nFlorian Bordes, Samuel Lavoie, Randall Balestriero, Nicolas Ballas, Pascal Vincent.\n\nKey Word: Pretraining; Fine-Tuning; Information Bottleneck.\n\nDigest A commonly used trick in SSL, shown to make deep networks more robust to such bias, is the addition of a small projector (usually a 2 or 3 layer multi-layer perceptron) on top of a backbone network during training. In contrast to previous work that studied the impact of the projector architecture, we here focus on a simpler, yet overlooked lever to control the information in the backbone representation. We show that merely changing its dimensionality -- by changing only the size of the backbone's very last block -- is a remarkably effective technique to mitigate the pretraining bias.\n\nWhy is the winner the best? [paper]\n\nAuthor List Matthias Eisenmann, Annika Reinke, Vivienn Weru, Minu Dietlinde Tizabi, Fabian Isensee, Tim J. Adler, Sharib Ali, Vincent Andrearczyk, Marc Aubreville, Ujjwal Baid, Spyridon Bakas, Niranjan Balu, Sophia Bano, Jorge Bernal, Sebastian Bodenstedt, Alessandro Casella, Veronika Cheplygina, Marie Daum, Marleen de Bruijne, Adrien Depeursinge, Reuben Dorent, Jan Egger, David G. Ellis, Sandy Engelhardt, Melanie Ganz, Noha Ghatwary, Gabriel Girard, Patrick Godau, Anubha Gupta, Lasse Hansen, Kanako Harada, Mattias Heinrich, Nicholas Heller, Alessa Hering, Arnaud Huaulmé, Pierre Jannin, Ali Emre Kavur, Oldřich Kodym, Michal Kozubek, Jianning Li, Hongwei Li, Jun Ma, Carlos Martín-Isla, Bjoern Menze, Alison Noble, Valentin Oreiller, Nicolas Padoy, Sarthak Pati, Kelly Payette, Tim Rädsch, Jonathan Rafael-Patiño, Vivek Singh Bawa, Stefanie Speidel, Carole H. Sudre, Kimberlin van Wijnen, Martin Wagner, Donglai Wei, Amine Yamlahi, Moi Hoon Yap, Chun Yuan, Maximilian Zenk, Aneeq Zia, David Zimmerer, Dogu Baran Aydogan, Binod Bhattarai, Louise Bloch, Raphael Brüngel, Jihoon Cho, Chanyeol Choi, Qi Dou, Ivan Ezhov, Christoph M. Friedrich, Clifton Fuller, Rebati Raman Gaire, Adrian Galdran, Álvaro García Faura, Maria Grammatikopoulou, SeulGi Hong, Mostafa Jahanifar, Ikbeom Jang, Abdolrahim Kadkhodamohammadi, Inha Kang, Florian Kofler, Satoshi Kondo, Hugo Kuijf, Mingxing Li, Minh Huan Luu, Tomaž Martinčič, Pedro Morais, Mohamed A. Naser, Bruno Oliveira, David Owen, Subeen Pang, Jinah Park, Sung-Hong Park, Szymon Płotka, Elodie Puybareau, Nasir Rajpoot, Kanghyun Ryu, Numan Saeed , Adam Shephard, Pengcheng Shi, Dejan Štepec, Ronast Subedi, Guillaume Tochon, Helena R. Torres, Helene Urien, João L. Vilaça, Kareem Abdul Wahid, Haojie Wang, Jiacheng Wang, Liansheng Wang, Xiyue Wang, Benedikt Wiestler, Marek Wodzinski, Fangfang Xia, Juanying Xie, Zhiwei Xiong, Sen Yang, Yanwu Yang, Zixuan Zhao, Klaus Maier-Hein, Paul F. Jäger, Annette Kopp-Schneider, Lena Maier-Hein.\n\nKey Word: Benchmarking Competitions; Medical Imaging.\n\nDigest The article discusses the lack of investigation into what can be learned from international benchmarking competitions for image analysis methods. The authors conducted a multi-center study of 80 competitions conducted in the scope of IEEE ISBI 2021 and MICCAI 2021 to address this gap. Based on comprehensive descriptions of the submitted algorithms and their rankings, as well as participation strategies, statistical analyses revealed common characteristics of winning solutions. These typically include the use of multi-task learning and/or multi-stage pipelines, a focus on augmentation, image preprocessing, data curation, and postprocessing.\n\nSparks of Artificial General Intelligence: Early experiments with GPT-4. [paper]\n\nSébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang.\n\nKey Word: Artificial General Intelligence; Benchmarking; GPT.\n\nDigest We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT.\n\nIs forgetting less a good inductive bias for forward transfer? [paper]\n\nJiefeng Chen, Timothy Nguyen, Dilan Gorur, Arslan Chaudhry. ICLR 2023\n\nKey Word: Continual Learning; Catastrophic Forgetting; Forward Transfer; Inductive Bias.\n\nDigest One of the main motivations of studying continual learning is that the problem setting allows a model to accrue knowledge from past tasks to learn new tasks more efficiently. However, recent studies suggest that the key metric that continual learning algorithms optimize, reduction in catastrophic forgetting, does not correlate well with the forward transfer of knowledge. We believe that the conclusion previous works reached is due to the way they measure forward transfer. We argue that the measure of forward transfer to a task should not be affected by the restrictions placed on the continual learner in order to preserve knowledge of previous tasks.\n\nDropout Reduces Underfitting. [paper] [code]\n\nZhuang Liu, Zhiqiu Xu, Joseph Jin, Zhiqiang Shen, Trevor Darrell.\n\nKey Word: Dropout; Overfitting.\n\nDigest In this study, we demonstrate that dropout can also mitigate underfitting when used at the start of training. During the early phase, we find dropout reduces the directional variance of gradients across mini-batches and helps align the mini-batch gradients with the entire dataset's gradient. This helps counteract the stochasticity of SGD and limit the influence of individual batches on model training.\n\nThe Role of Pre-training Data in Transfer Learning. [paper]\n\nRahim Entezari, Mitchell Wortsman, Olga Saukh, M.Moein Shariatnia, Hanie Sedghi, Ludwig Schmidt.\n\nKey Word: Pre-training; Transfer Learning.\n\nDigest We investigate the impact of pre-training data distribution on the few-shot and full fine-tuning performance using 3 pre-training methods (supervised, contrastive language-image and image-image), 7 pre-training datasets, and 9 downstream datasets. Through extensive controlled experiments, we find that the choice of the pre-training data source is essential for the few-shot transfer, but its role decreases as more data is made available for fine-tuning.\n\nThe Dormant Neuron Phenomenon in Deep Reinforcement Learning. [paper] [code]\n\nGhada Sokar, Rishabh Agarwal, Pablo Samuel Castro, Utku Evci.\n\nKey Word: Dormant Neuron; Deep Reinforcement Learning.\n\nDigest The paper identifies the dormant neuron phenomenon in deep reinforcement learning, where inactive neurons increase and hinder network expressivity, affecting learning. To address this, they propose a method called ReDo, which recycles dormant neurons during training. ReDo reduces the number of dormant neurons, maintains network expressiveness, and leads to improved performance.\n\nCliff-Learning. [paper]\n\nTony T. Wang, Igor Zablotchi, Nir Shavit, Jonathan S. Rosenfeld.\n\nKey Word: Foundation Models; Fine-Tuning.\n\nDigest We study the data-scaling of transfer learning from foundation models in the low-downstream-data regime. We observe an intriguing phenomenon which we call cliff-learning. Cliff-learning refers to regions of data-scaling laws where performance improves at a faster than power law rate (i.e. regions of concavity on a log-log scaling plot).\n\nEmpirical Study: 2022\n\nModelDiff: A Framework for Comparing Learning Algorithms. [paper] [code]\n\nHarshay Shah, Sung Min Park, Andrew Ilyas, Aleksander Madry.\n\nKey Word: Representation-based Comparison; Example-level Comparisons; Comparing Feature Attributions.\n\nDigest We study the problem of (learning) algorithm comparison, where the goal is to find differences between models trained with two different learning algorithms. We begin by formalizing this goal as one of finding distinguishing feature transformations, i.e., input transformations that change the predictions of models trained with one learning algorithm but not the other. We then present ModelDiff, a method that leverages the datamodels framework (Ilyas et al., 2022) to compare learning algorithms based on how they use their training data.\n\nOverfreezing Meets Overparameterization: A Double Descent Perspective on Transfer Learning of Deep Neural Networks. [paper]\n\nYehuda Dar, Lorenzo Luzi, Richard G. Baraniuk.\n\nKey Word: Transfer Learning; Deep Double Descent; Overfreezing.\n\nDigest We study the generalization behavior of transfer learning of deep neural networks (DNNs). We adopt the overparameterization perspective -- featuring interpolation of the training data (i.e., approximately zero train error) and the double descent phenomenon -- to explain the delicate effect of the transfer learning setting on generalization performance. We study how the generalization behavior of transfer learning is affected by the dataset size in the source and target tasks, the number of transferred layers that are kept frozen in the target DNN training, and the similarity between the source and target tasks.\n\nHow to Fine-Tune Vision Models with SGD. [paper]\n\nAnanya Kumar, Ruoqi Shen, Sébastien Bubeck, Suriya Gunasekar.\n\nKey Word: Fine-Tuning; Out-of-Distribution Generalization.\n\nDigest We show that fine-tuning with AdamW performs substantially better than SGD on modern Vision Transformer and ConvNeXt models. We find that large gaps in performance between SGD and AdamW occur when the fine-tuning gradients in the first \"embedding\" layer are much larger than in the rest of the model. Our analysis suggests an easy fix that works consistently across datasets and models: merely freezing the embedding layer (less than 1\\% of the parameters) leads to SGD performing competitively with AdamW while using less memory.\n\nWhat Images are More Memorable to Machines? [paper] [code]\n\nJunlin Han, Huangying Zhan, Jie Hong, Pengfei Fang, Hongdong Li, Lars Petersson, Ian Reid.\n\nKey Word: Self-Supervised Memorization Quantification.\n\nDigest This paper studies the problem of measuring and predicting how memorable an image is to pattern recognition machines, as a path to explore machine intelligence. Firstly, we propose a self-supervised machine memory quantification pipeline, dubbed ``MachineMem measurer'', to collect machine memorability scores of images. Similar to humans, machines also tend to memorize certain kinds of images, whereas the types of images that machines and humans memorialize are different.\n\nHarmonizing the object recognition strategies of deep neural networks with humans. [paper] [code]\n\nThomas Fel, Ivan Felipe, Drew Linsley, Thomas Serre.\n\nKey Word: Interpretation; Neural Harmonizer; Psychophysics.\n\nDigest Across 84 different DNNs trained on ImageNet and three independent datasets measuring the where and the how of human visual strategies for object recognition on those images, we find a systematic trade-off between DNN categorization accuracy and alignment with human visual strategies for object recognition. State-of-the-art DNNs are progressively becoming less aligned with humans as their accuracy improves. We rectify this growing issue with our neural harmonizer: a general-purpose training routine that both aligns DNN and human visual strategies and improves categorization accuracy.\n\nPruning's Effect on Generalization Through the Lens of Training and Regularization. [paper]\n\nTian Jin, Michael Carbin, Daniel M. Roy, Jonathan Frankle, Gintare Karolina Dziugaite.\n\nKey Word: Pruning; Regularization.\n\nDigest We show that size reduction cannot fully account for the generalization-improving effect of standard pruning algorithms. Instead, we find that pruning leads to better training at specific sparsities, improving the training loss over the dense model. We find that pruning also leads to additional regularization at other sparsities, reducing the accuracy degradation due to noisy examples over the dense model. Pruning extends model training time and reduces model size. These two factors improve training and add regularization respectively. We empirically demonstrate that both factors are essential to fully explaining pruning's impact on generalization.\n\nWhat does a deep neural network confidently perceive? The effective dimension of high certainty class manifolds and their low confidence boundaries. [paper] [code]\n\nStanislav Fort, Ekin Dogus Cubuk, Surya Ganguli, Samuel S. Schoenholz.\n\nKey Word: Class Manifold; Linear Region; Out-of-Distribution Generalization.\n\nDigest Deep neural network classifiers partition input space into high confidence regions for each class. The geometry of these class manifolds (CMs) is widely studied and intimately related to model performance; for example, the margin depends on CM boundaries. We exploit the notions of Gaussian width and Gordon's escape theorem to tractably estimate the effective dimension of CMs and their boundaries through tomographic intersections with random affine subspaces of varying dimension. We show several connections between the dimension of CMs, generalization, and robustness.\n\nIn What Ways Are Deep Neural Networks Invariant and How Should We Measure This? [paper]\n\nHenry Kvinge, Tegan H. Emerson, Grayson Jorgenson, Scott Vasquez, Timothy Doster, Jesse D. Lew. NeurIPS 2022\n\nKey Word: Invariance and Equivariance.\n\nDigest We explore the nature of invariance and equivariance of deep learning models with the goal of better understanding the ways in which they actually capture these concepts on a formal level. We introduce a family of invariance and equivariance metrics that allows us to quantify these properties in a way that disentangles them from other metrics such as loss or accuracy.\n\nRelative representations enable zero-shot latent space communication. [paper]\n\nLuca Moschella, Valentino Maiorca, Marco Fumero, Antonio Norelli, Francesco Locatello, Emanuele Rodolà.\n\nKey Word: Representation Similarity; Model stitching.\n\nDigest Neural networks embed the geometric structure of a data manifold lying in a high-dimensional space into latent representations. Ideally, the distribution of the data points in the latent space should depend only on the task, the data, the loss, and other architecture-specific constraints. However, factors such as the random weights initialization, training hyperparameters, or other sources of randomness in the training phase may induce incoherent latent spaces that hinder any form of reuse. Nevertheless, we empirically observe that, under the same data and modeling choices, distinct latent spaces typically differ by an unknown quasi-isometric transformation: that is, in each space, the distances between the encodings do not change. In this work, we propose to adopt pairwise similarities as an alternative data representation, that can be used to enforce the desired invariance without any additional training.\n\nMinimalistic Unsupervised Learning with the Sparse Manifold Transform. [paper]\n\nYubei Chen, Zeyu Yun, Yi Ma, Bruno Olshausen, Yann LeCun.\n\nKey Word: Self-Supervision; Sparse Manifold Transform.\n\nDigest We describe a minimalistic and interpretable method for unsupervised learning, without resorting to data augmentation, hyperparameter tuning, or other engineering designs, that achieves performance close to the SOTA SSL methods. Our approach leverages the sparse manifold transform, which unifies sparse coding, manifold learning, and slow feature analysis. With a one-layer deterministic sparse manifold transform, one can achieve 99.3% KNN top-1 accuracy on MNIST, 81.1% KNN top-1 accuracy on CIFAR-10 and 53.2% on CIFAR-100.\n\nA Review of Sparse Expert Models in Deep Learning. [paper]\n\nWilliam Fedus, Jeff Dean, Barret Zoph.\n\nKey Word: Mixture-of-Experts.\n\nDigest Sparse expert models are a thirty-year old concept re-emerging as a popular architecture in deep learning. This class of architecture encompasses Mixture-of-Experts, Switch Transformers, Routing Networks, BASE layers, and others, all with the unifying idea that each example is acted on by a subset of the parameters. By doing so, the degree of sparsity decouples the parameter count from the compute per example allowing for extremely large, but efficient models. The resulting models have demonstrated significant improvements across diverse domains such as natural language processing, computer vision, and speech recognition. We review the concept of sparse expert models, provide a basic description of the common algorithms, contextualize the advances in the deep learning era, and conclude by highlighting areas for future work.\n\nA Data-Based Perspective on Transfer Learning. [paper] [code]\n\nSaachi Jain, Hadi Salman, Alaa Khaddaj, Eric Wong, Sung Min Park, Aleksander Madry.\n\nKey Word: Transfer Learning; Influence Function; Data Leakage.\n\nDigest It is commonly believed that in transfer learning including more pre-training data translates into better performance. However, recent evidence suggests that removing data from the source dataset can actually help too. In this work, we take a closer look at the role of the source dataset's composition in transfer learning and present a framework for probing its impact on downstream performance. Our framework gives rise to new capabilities such as pinpointing transfer learning brittleness as well as detecting pathologies such as data-leakage and the presence of misleading examples in the source dataset.\n\nWhen Does Re-initialization Work? [paper]\n\nSheheryar Zaidi, Tudor Berariu, Hyunjik Kim, Jörg Bornschein, Claudia Clopath, Yee Whye Teh, Razvan Pascanu.\n\nKey Word: Re-initialization; Regularization.\n\nDigest We conduct an extensive empirical comparison of standard training with a selection of re-initialization methods to answer this question, training over 15,000 models on a variety of image classification benchmarks. We first establish that such methods are consistently beneficial for generalization in the absence of any other regularization. However, when deployed alongside other carefully tuned regularization techniques, re-initialization methods offer little to no added benefit for generalization, although optimal generalization performance becomes less sensitive to the choice of learning rate and weight decay hyperparameters. To investigate the impact of re-initialization methods on noisy data, we also consider learning under label noise. Surprisingly, in this case, re-initialization significantly improves upon standard training, even in the presence of other carefully tuned regularization techniques.\n\nHow You Start Matters for Generalization. [paper]\n\nSameera Ramasinghe, Lachlan MacDonald, Moshiur Farazi, Hemanth Sartachandran, Simon Lucey.\n\nKey Word: Implicit regularization; Fourier Spectrum.\n\nDigest We promote a shift of focus towards initialization rather than neural architecture or (stochastic) gradient descent to explain this implicit regularization. Through a Fourier lens, we derive a general result for the spectral bias of neural networks and show that the generalization of neural networks is heavily tied to their initialization. Further, we empirically solidify the developed theoretical insights using practical, deep networks.\n\nRethinking the Role of Demonstrations: What Makes In-Context Learning Work? [paper] [code]\n\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer.\n\nKey Word: Natural Language Processing; In-Context Learning.\n\nDigest We show that ground truth demonstrations are in fact not required -- randomly replacing labels in the demonstrations barely hurts performance, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of end task performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence.\n\nEmpirical Study: 2021\n\nMasked Autoencoders Are Scalable Vision Learners. [paper] [code]\n\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick. CVPR 2022\n\nKey Word: Self-Supervision; Autoencoders.\n\nDigest This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g., 75%, yields a nontrivial and meaningful self-supervisory task.\n\nLearning in High Dimension Always Amounts to Extrapolation. [paper]\n\nRandall Balestriero, Jerome Pesenti, Yann LeCun.\n\nKey Word: Interpolation and Extrapolation.\n\nDigest The notion of interpolation and extrapolation is fundamental in various fields from deep learning to function approximation. Interpolation occurs for a sample x whenever this sample falls inside or on the boundary of the given dataset's convex hull. Extrapolation occurs when x falls outside of that convex hull. One fundamental (mis)conception is that state-of-the-art algorithms work so well because of their ability to correctly interpolate training data. A second (mis)conception is that interpolation happens throughout tasks and datasets, in fact, many intuitions and theories rely on that assumption. We empirically and theoretically argue against those two points and demonstrate that on any high-dimensional (>100) dataset, interpolation almost surely never happens.\n\nUnderstanding Dataset Difficulty with V-Usable Information. [paper] [code]\n\nKawin Ethayarajh, Yejin Choi, Swabha Swayamdipta. ICML 2022\n\nKey Word: Dataset Difficulty Measures; Information Theory.\n\nDigest Estimating the difficulty of a dataset typically involves comparing state-of-the-art models to humans; the bigger the performance gap, the harder the dataset is said to be. However, this comparison provides little understanding of how difficult each instance in a given distribution is, or what attributes make the dataset difficult for a given model. To address these questions, we frame dataset difficulty -- w.r.t. a model V -- as the lack of V-usable information (Xu et al., 2019), where a lower value indicates a more difficult dataset for V. We further introduce pointwise V-information (PVI) for measuring the difficulty of individual instances w.r.t. a given distribution.\n\nExploring the Limits of Large Scale Pre-training. [paper]\n\nSamira Abnar, Mostafa Dehghani, Behnam Neyshabur, Hanie Sedghi. ICLR 2022\n\nKey Word: Pre-training.\n\nDigest We investigate more than 4800 experiments on Vision Transformers, MLP-Mixers and ResNets with number of parameters ranging from ten million to ten billion, trained on the largest scale of available image data (JFT, ImageNet21K) and evaluated on more than 20 downstream image recognition tasks. We propose a model for downstream performance that reflects the saturation phenomena and captures the nonlinear relationship in performance of upstream and downstream tasks.\n\nStochastic Training is Not Necessary for Generalization. [paper] [code]\n\nJonas Geiping, Micah Goldblum, Phillip E. Pope, Michael Moeller, Tom Goldstein. ICLR 2022\n\nKey Word: Stochastic Gradient Descent; Regularization.\n\nDigest It is widely believed that the implicit regularization of SGD is fundamental to the impressive generalization behavior we observe in neural networks. In this work, we demonstrate that non-stochastic full-batch training can achieve comparably strong performance to SGD on CIFAR-10 using modern architectures. To this end, we show that the implicit regularization of SGD can be completely replaced with explicit regularization even when comparing against a strong and well-researched baseline.\n\nPointer Value Retrieval: A new benchmark for understanding the limits of neural network generalization. [paper]\n\nChiyuan Zhang, Maithra Raghu, Jon Kleinberg, Samy Bengio.\n\nKey Word: Out-of-Distribution Generalization.\n\nDigest In this paper we introduce a novel benchmark, Pointer Value Retrieval (PVR) tasks, that explore the limits of neural network generalization. We demonstrate that this task structure provides a rich testbed for understanding generalization, with our empirical study showing large variations in neural network performance based on dataset size, task complexity and model architecture.\n\nWhat can linear interpolation of neural network loss landscapes tell us? [paper]\n\nTiffany Vlaar, Jonathan Frankle. ICML 2022\n\nKey Word: Linear Interpolation; Loss Landscapes.\n\nDigest We put inferences of this kind to the test, systematically evaluating how linear interpolation and final performance vary when altering the data, choice of initialization, and other optimizer and architecture design choices. Further, we use linear interpolation to study the role played by individual layers and substructures of the network. We find that certain layers are more sensitive to the choice of initialization, but that the shape of the linear path is not indicative of the changes in test accuracy of the model.\n\nCan Vision Transformers Learn without Natural Images? [paper] [code]\n\nKodai Nakashima, Hirokatsu Kataoka, Asato Matsumoto, Kenji Iwata, Nakamasa Inoue. AAAI 2022\n\nKey Word: Formula-driven Supervised Learning; Vision Transformer.\n\nDigest We pre-train ViT without any image collections and annotation labor. We experimentally verify that our proposed framework partially outperforms sophisticated Self-Supervised Learning (SSL) methods like SimCLRv2 and MoCov2 without using any natural images in the pre-training phase. Moreover, although the ViT pre-trained without natural images produces some different visualizations from ImageNet pre-trained ViT, it can interpret natural image datasets to a large extent.\n\nThe Low-Rank Simplicity Bias in Deep Networks. [paper]\n\nMinyoung Huh, Hossein Mobahi, Richard Zhang, Brian Cheung, Pulkit Agrawal, Phillip Isola.\n\nKey Word: Low-Rank Embedding; Inductive Bias.\n\nDigest We make a series of empirical observations that investigate and extend the hypothesis that deeper networks are inductively biased to find solutions with lower effective rank embeddings. We conjecture that this bias exists because the volume of functions that maps to low effective rank embedding increases with depth. We show empirically that our claim holds true on finite width linear and non-linear models on practical learning paradigms and show that on natural data, these are often the solutions that generalize well.\n\nGradient Descent on Neural Networks Typically Occurs at the Edge of Stability. [paper] [code]\n\nJeremy M. Cohen, Simran Kaur, Yuanzhi Li, J. Zico Kolter, Ameet Talwalkar. ICLR 2021\n\nKey Word: Edge of Stability.\n\nDigest We empirically demonstrate that full-batch gradient descent on neural network training objectives typically operates in a regime we call the Edge of Stability. In this regime, the maximum eigenvalue of the training loss Hessian hovers just above the numerical value 2/(step size), and the training loss behaves non-monotonically over short timescales, yet consistently decreases over long timescales. Since this behavior is inconsistent with several widespread presumptions in the field of optimization, our findings raise questions as to whether these presumptions are relevant to neural network training.\n\nPre-training without Natural Images. [paper] [code]\n\nHirokatsu Kataoka, Kazushige Okayasu, Asato Matsumoto, Eisuke Yamagata, Ryosuke Yamada, Nakamasa Inoue, Akio Nakamura, Yutaka Satoh. ACCV 2020\n\nKey Word: Formula-driven Supervised Learning.\n\nDigest The paper proposes a novel concept, Formula-driven Supervised Learning. We automatically generate image patterns and their category labels by assigning fractals, which are based on a natural law existing in the background knowledge of the real world. Theoretically, the use of automatically generated images instead of natural images in the pre-training phase allows us to generate an infinite scale dataset of labeled images. Although the models pre-trained with the proposed Fractal DataBase (FractalDB), a database without natural images, does not necessarily outperform models pre-trained with human annotated datasets at all settings, we are able to partially surpass the accuracy of ImageNet/Places pre-trained models.\n\nEmpirical Study: 2020\n\nWhen Do Curricula Work? [paper] [code]\n\nXiaoxia Wu, Ethan Dyer, Behnam Neyshabur. ICLR 2021\n\nKey Word: Curriculum Learning.\n\nDigest We set out to investigate the relative benefits of ordered learning. We first investigate the implicit curricula resulting from architectural and optimization bias and find that samples are learned in a highly consistent order. Next, to quantify the benefit of explicit curricula, we conduct extensive experiments over thousands of orderings spanning three kinds of learning: curriculum, anti-curriculum, and random-curriculum -- in which the size of the training dataset is dynamically increased over time, but the examples are randomly ordered.\n\nIn Search of Robust Measures of Generalization. [paper] [code]\n\nGintare Karolina Dziugaite, Alexandre Drouin, Brady Neal, Nitarshan Rajkumar, Ethan Caballero, Linbo Wang, Ioannis Mitliagkas, Daniel M. Roy. NeurIPS 2020\n\nKey Word: Generalization Measures.\n\nDigest One of the principal scientific challenges in deep learning is explaining generalization, i.e., why the particular way the community now trains networks to achieve small training error also leads to small error on held-out data from the same population. It is widely appreciated that some worst-case theories -- such as those based on the VC dimension of the class of predictors induced by modern neural network architectures -- are unable to explain empirical performance. A large volume of work aims to close this gap, primarily by developing bounds on generalization error, optimization error, and excess risk. When evaluated empirically, however, most of these bounds are numerically vacuous. Focusing on generalization bounds, this work addresses the question of how to evaluate such bounds empirically.\n\nThe Deep Bootstrap Framework: Good Online Learners are Good Offline Generalizers. [paper] [code]\n\nPreetum Nakkiran, Behnam Neyshabur, Hanie Sedghi. ICLR 2021\n\nKey Word: Online Learning; Finite-Sample Deviations.\n\nDigest We propose a new framework for reasoning about generalization in deep learning. The core idea is to couple the Real World, where optimizers take stochastic gradient steps on the empirical loss, to an Ideal World, where optimizers take steps on the population loss. This leads to an alternate decomposition of test error into: (1) the Ideal World test error plus (2) the gap between the two worlds. If the gap (2) is universally small, this reduces the problem of generalization in offline learning to the problem of optimization in online learning.\n\nCharacterising Bias in Compressed Models. [paper]\n\nSara Hooker, Nyalleng Moorosi, Gregory Clark, Samy Bengio, Emily Denton.\n\nKey Word: Pruning; Fairness.\n\nDigest The popularity and widespread use of pruning and quantization is driven by the severe resource constraints of deploying deep neural networks to environments with strict latency, memory and energy requirements. These techniques achieve high levels of compression with negligible impact on top-line metrics (top-1 and top-5 accuracy). However, overall accuracy hides disproportionately high errors on a small subset of examples; we call this subset Compression Identified Exemplars (CIE).\n\nDataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics. [paper] [code]\n\nSwabha Swayamdipta, Roy Schwartz, Nicholas Lourie, Yizhong Wang, Hannaneh Hajishirzi, Noah A. Smith, Yejin Choi. EMNLP 2020\n\nKey Word: Training Dynamics; Data Map; Curriculum Learning.\n\nDigest Large datasets have become commonplace in NLP research. However, the increased emphasis on data quantity has made it challenging to assess the quality of data. We introduce Data Maps---a model-based tool to characterize and diagnose datasets. We leverage a largely ignored source of information: the behavior of the model on individual instances during training (training dynamics) for building data maps.\n\nWhat is being transferred in transfer learning? [paper] [code]\n\nBehnam Neyshabur, Hanie Sedghi, Chiyuan Zhang. NeurIPS 2020\n\nKey Word: Transfer Learning.\n\nDigest We provide new tools and analyses to address these fundamental questions. Through a series of analyses on transferring to block-shuffled images, we separate the effect of feature reuse from learning low-level statistics of data and show that some benefit of transfer learning comes from the latter. We present that when training from pre-trained weights, the model stays in the same basin in the loss landscape and different instances of such model are similar in feature space and close in parameter space.\n\nDeep Isometric Learning for Visual Recognition. [paper] [code]\n\nHaozhi Qi, Chong You, Xiaolong Wang, Yi Ma, Jitendra Malik. ICML 2020\n\nKey Word: Isometric Networks.\n\nDigest This paper shows that deep vanilla ConvNets without normalization nor skip connections can also be trained to achieve surprisingly good performance on standard image recognition benchmarks. This is achieved by enforcing the convolution kernels to be near isometric during initialization and training, as well as by using a variant of ReLU that is shifted towards being isometric.\n\nOn the Generalization Benefit of Noise in Stochastic Gradient Descent. [paper]\n\nSamuel L. Smith, Erich Elsen, Soham De. ICML 2020\n\nKey Word: Stochastic Gradient Descent.\n\nDigest In this paper, we perform carefully designed experiments and rigorous hyperparameter sweeps on a range of popular models, which verify that small or moderately large batch sizes can substantially outperform very large batches on the test set. This occurs even when both models are trained for the same number of iterations and large batches achieve smaller training losses.\n\nDo CNNs Encode Data Augmentations? [paper]\n\nEddie Yan, Yanping Huang.\n\nKey Word: Data Augmentations.\n\nDigest Surprisingly, neural network features not only predict data augmentation transformations, but they predict many transformations with high accuracy. After validating that neural networks encode features corresponding to augmentation transformations, we show that these features are primarily encoded in the early layers of modern CNNs.\n\nDo We Need Zero Training Loss After Achieving Zero Training Error? [paper] [code]\n\nTakashi Ishida, Ikko Yamane, Tomoya Sakai, Gang Niu, Masashi Sugiyama. ICML 2020\n\nKey Word: Regularization.\n\nDigest Our approach makes the loss float around the flooding level by doing mini-batched gradient descent as usual but gradient ascent if the training loss is below the flooding level. This can be implemented with one line of code, and is compatible with any stochastic optimizer and other regularizers. We experimentally show that flooding improves performance and as a byproduct, induces a double descent curve of the test loss.\n\nUnderstanding Why Neural Networks Generalize Well Through GSNR of Parameters. [paper]\n\nJinlong Liu, Guoqing Jiang, Yunzhi Bai, Ting Chen, Huayan Wang. ICLR 2020\n\nKey Word: Generalization Indicators.\n\nDigest In this paper, we provide a novel perspective on these issues using the gradient signal to noise ratio (GSNR) of parameters during training process of DNNs. The GSNR of a parameter is defined as the ratio between its gradient's squared mean and variance, over the data distribution.\n\nEmpirical Study: 2019\n\nAngular Visual Hardness. [paper]\n\nBeidi Chen, Weiyang Liu, Zhiding Yu, Jan Kautz, Anshumali Shrivastava, Animesh Garg, Anima Anandkumar. ICML 2020\n\nKey Word: Calibration; Example Hardness Measures.\n\nDigest We propose a novel measure for CNN models known as Angular Visual Hardness. Our comprehensive empirical studies show that AVH can serve as an indicator of generalization abilities of neural networks, and improving SOTA accuracy entails improving accuracy on hard example\n\nFantastic Generalization Measures and Where to Find Them. [paper] [code]\n\nYiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, Samy Bengio. ICLR 2020\n\nKey Word: Complexity Measures; Spurious Correlations.\n\nDigest We present the first large scale study of generalization in deep networks. We investigate more then 40 complexity measures taken from both theoretical bounds and empirical studies. We train over 10,000 convolutional networks by systematically varying commonly used hyperparameters. Hoping to uncover potentially causal relationships between each measure and generalization, we analyze carefully controlled experiments and show surprising failures of some measures as well as promising measures for further research.\n\nTruth or Backpropaganda? An Empirical Investigation of Deep Learning Theory. [paper] [code]\n\nMicah Goldblum, Jonas Geiping, Avi Schwarzschild, Michael Moeller, Tom Goldstein. ICLR 2020\n\nKey Word: Local Minima.\n\nDigest The authors take a closer look at widely held beliefs about neural networks. Using a mix of analysis and experiment, they shed some light on the ways these assumptions break down.\n\nRapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML. [paper] [code]\n\nAniruddh Raghu, Maithra Raghu, Samy Bengio, Oriol Vinyals. ICLR 2020\n\nKey Word: Meta Learning.\n\nDigest Despite MAML's popularity, a fundamental open question remains -- is the effectiveness of MAML due to the meta-initialization being primed for rapid learning (large, efficient changes in the representations) or due to feature reuse, with the meta initialization already containing high quality features? We investigate this question, via ablation studies and analysis of the latent representations, finding that feature reuse is the dominant factor.\n\nFinding the Needle in the Haystack with Convolutions: on the benefits of architectural bias. [paper] [code]\n\nStéphane d'Ascoli, Levent Sagun, Joan Bruna, Giulio Biroli. NeurIPS 2019\n\nKey Word: Architectural Bias.\n\nDigest In particular, Convolutional Neural Networks (CNNs) are known to perform much better than Fully-Connected Networks (FCNs) on spatially structured data: the architectural structure of CNNs benefits from prior knowledge on the features of the data, for instance their translation invariance. The aim of this work is to understand this fact through the lens of dynamics in the loss landscape.\n\nAdversarial Training Can Hurt Generalization. [paper]\n\nAditi Raghunathan, Sang Michael Xie, Fanny Yang, John C. Duchi, Percy Liang.\n\nKey Word: Adversarial Examples.\n\nDigest While adversarial training can improve robust accuracy (against an adversary), it sometimes hurts standard accuracy (when there is no adversary). Previous work has studied this tradeoff between standard and robust accuracy, but only in the setting where no predictor performs well on both objectives in the infinite data limit. In this paper, we show that even when the optimal predictor with infinite data performs well on both objectives, a tradeoff can still manifest itself with finite data.\n\nBad Global Minima Exist and SGD Can Reach Them. [paper] [code]\n\nShengchao Liu, Dimitris Papailiopoulos, Dimitris Achlioptas. NeurIPS 2020\n\nKey Word: Stochastic Gradient Descent.\n\nDigest Several works have aimed to explain why overparameterized neural networks generalize well when trained by Stochastic Gradient Descent (SGD). The consensus explanation that has emerged credits the randomized nature of SGD for the bias of the training process towards low-complexity models and, thus, for implicit regularization. We take a careful look at this explanation in the context of image classification with common deep neural network architectures. We find that if we do not regularize explicitly, then SGD can be easily made to converge to poorly-generalizing, high-complexity models: all it takes is to first train on a random labeling on the data, before switching to properly training with the correct labels.\n\nDeep ReLU Networks Have Surprisingly Few Activation Patterns. [paper]\n\nBoris Hanin, David Rolnick. NeurIPS 2019\n\nDigest In this paper, we show that the average number of activation patterns for ReLU networks at initialization is bounded by the total number of neurons raised to the input dimension. We show empirically that this bound, which is independent of the depth, is tight both at initialization and during training, even on memorization tasks that should maximize the number of activation patterns.\n\nSensitivity of Deep Convolutional Networks to Gabor Noise. [paper] [code]\n\nKenneth T. Co, Luis Muñoz-González, Emil C. Lupu.\n\nKey Word: Robustness.\n\nDigest Deep Convolutional Networks (DCNs) have been shown to be sensitive to Universal Adversarial Perturbations (UAPs): input-agnostic perturbations that fool a model on large portions of a dataset. These UAPs exhibit interesting visual patterns, but this phenomena is, as yet, poorly understood. Our work shows that visually similar procedural noise patterns also act as UAPs. In particular, we demonstrate that different DCN architectures are sensitive to Gabor noise patterns. This behaviour, its causes, and implications deserve further in-depth study.\n\nRethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks. [paper]\n\nGuangyong Chen, Pengfei Chen, Yujun Shi, Chang-Yu Hsieh, Benben Liao, Shengyu Zhang.\n\nKey Word: Batch Normalization; Dropout.\n\nDigest Our work is based on an excellent idea that whitening the inputs of neural networks can achieve a fast convergence speed. Given the well-known fact that independent components must be whitened, we introduce a novel Independent-Component (IC) layer before each weight layer, whose inputs would be made more independent.\n\nA critical analysis of self-supervision, or what we can learn from a single image. [paper] [code]\n\nYuki M. Asano, Christian Rupprecht, Andrea Vedaldi. ICLR 2020\n\nKey Word: Self-Supervision.\n\nDigest We show that three different and representative methods, BiGAN, RotNet and DeepCluster, can learn the first few layers of a convolutional network from a single image as well as using millions of images and manual labels, provided that strong data augmentation is used. However, for deeper layers the gap with manual supervision cannot be closed even if millions of unlabelled images are used for training.\n\nApproximating CNNs with Bag-of-local-Features models works surprisingly well on ImageNet. [paper] [code]\n\nWieland Brendel, Matthias Bethge. ICLR 2019\n\nKey Word: Bag-of-Features.\n\nDigest Our model, a simple variant of the ResNet-50 architecture called BagNet, classifies an image based on the occurrences of small local image features without taking into account their spatial ordering. This strategy is closely related to the bag-of-feature (BoF) models popular before the onset of deep learning and reaches a surprisingly high accuracy on ImageNet.\n\nTransfusion: Understanding Transfer Learning for Medical Imaging. [paper] [code]\n\nMaithra Raghu, Chiyuan Zhang, Jon Kleinberg, Samy Bengio. NeurIPS 2019\n\nKey Word: Transfer Learning; Medical Imaging.\n\nDigest we explore properties of transfer learning for medical imaging. A performance evaluation on two large scale medical imaging tasks shows that surprisingly, transfer offers little benefit to performance, and simple, lightweight models can perform comparably to ImageNet architectures.\n\nIdentity Crisis: Memorization and Generalization under Extreme Overparameterization. [paper]\n\nChiyuan Zhang, Samy Bengio, Moritz Hardt, Michael C. Mozer, Yoram Singer. ICLR 2020\n\nKey Word: Memorization.\n\nDigest We study the interplay between memorization and generalization of overparameterized networks in the extreme case of a single training example and an identity-mapping task.\n\nAre All Layers Created Equal? [paper]\n\nChiyuan Zhang, Samy Bengio, Yoram Singer. JMLR\n\nKey Word: Robustness.\n\nDigest We show that the layers can be categorized as either \"ambient\" or \"critical\". Resetting the ambient layers to their initial values has no negative consequence, and in many cases they barely change throughout training. On the contrary, resetting the critical layers completely destroys the predictor and the performance drops to chance.\n\nEmpirical Study: 2018\n\nWhy ReLU networks yield high-confidence predictions far away from the training data and how to mitigate the problem. [paper] [code]\n\nMatthias Hein, Maksym Andriushchenko, Julian Bitterwolf. CVPR 2019\n\nKey Word: ReLU.\n\nDigest Classifiers used in the wild, in particular for safety-critical systems, should not only have good generalization properties but also should know when they don't know, in particular make low confidence predictions far away from the training data. We show that ReLU type neural networks which yield a piecewise linear classifier function fail in this regard as they produce almost always high confidence predictions far away from the training data.\n\nAn Empirical Study of Example Forgetting during Deep Neural Network Learning. [paper] [code]\n\nMariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, Geoffrey J. Gordon. ICLR 2019\n\nKey Word: Curriculum Learning; Sample Weighting; Example Forgetting.\n\nDigest We define a 'forgetting event' to have occurred when an individual training example transitions from being classified correctly to incorrectly over the course of learning. Across several benchmark data sets, we find that: (i) certain examples are forgotten with high frequency, and some not at all; (ii) a data set's (un)forgettable examples generalize across neural architectures; and (iii) based on forgetting dynamics, a significant fraction of examples can be omitted from the training data set while still maintaining state-of-the-art generalization performance.\n\nOn Implicit Filter Level Sparsity in Convolutional Neural Networks. [paper]\n\nDushyant Mehta, Kwang In Kim, Christian Theobalt. CVPR 2019\n\nKey Word: Regularization; Sparsification.\n\nDigest We investigate filter level sparsity that emerges in convolutional neural networks (CNNs) which employ Batch Normalization and ReLU activation, and are trained with adaptive gradient descent techniques and L2 regularization or weight decay. We conduct an extensive experimental study casting our initial findings into hypotheses and conclusions about the mechanisms underlying the emergent filter level sparsity. This study allows new insight into the performance gap obeserved between adapative and non-adaptive gradient descent methods in practice.\n\nChallenging Common Assumptions in the Unsupervised Learning of Disentangled Representations. [paper] [code]\n\nFrancesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Rätsch, Sylvain Gelly, Bernhard Schölkopf, Olivier Bachem. ICML 2019\n\nKey Word: Disentanglement.\n\nDigest Our results suggest that future work on disentanglement learning should be explicit about the role of inductive biases and (implicit) supervision, investigate concrete benefits of enforcing disentanglement of the learned representations, and consider a reproducible experimental setup covering several data sets.\n\nInsights on representational similarity in neural networks with canonical correlation. [paper] [code]\n\nAri S. Morcos, Maithra Raghu, Samy Bengio. NeurIPS 2018\n\nKey Word: Representational Similarity.\n\nDigest Comparing representations in neural networks is fundamentally difficult as the structure of representations varies greatly, even across groups of networks trained on identical tasks, and over the course of training. Here, we develop projection weighted CCA (Canonical Correlation Analysis) as a tool for understanding neural networks, building off of SVCCA.\n\nLayer rotation: a surprisingly powerful indicator of generalization in deep networks? [paper] [code]\n\nSimon Carbonnelle, Christophe De Vleeschouwer.\n\nKey Word: Weight Evolution.\n\nDigest Our work presents extensive empirical evidence that layer rotation, i.e. the evolution across training of the cosine distance between each layer's weight vector and its initialization, constitutes an impressively consistent indicator of generalization performance. In particular, larger cosine distances between final and initial weights of each layer consistently translate into better generalization performance of the final model.\n\nSensitivity and Generalization in Neural Networks: an Empirical Study. [paper]\n\nRoman Novak, Yasaman Bahri, Daniel A. Abolafia, Jeffrey Pennington, Jascha Sohl-Dickstein. ICLR 2018\n\nKey Word: Sensitivity.\n\nDigest In this work, we investigate this tension between complexity and generalization through an extensive empirical exploration of two natural metrics of complexity related to sensitivity to input perturbations. We find that trained neural networks are more robust to input perturbations in the vicinity of the training data manifold, as measured by the norm of the input-output Jacobian of the network, and that it correlates well with generalization.\n\nEmpirical Study: 2017\n\nDeep Image Prior. [paper] [code]\n\nDmitry Ulyanov, Andrea Vedaldi, Victor Lempitsky.\n\nKey Word: Low-Level Vision.\n\nDigest In this paper, we show that, on the contrary, the structure of a generator network is sufficient to capture a great deal of low-level image statistics prior to any learning. In order to do so, we show that a randomly-initialized neural network can be used as a handcrafted prior with excellent results in standard inverse problems such as denoising, super-resolution, and inpainting.\n\nCritical Learning Periods in Deep Neural Networks. [paper]\n\nAlessandro Achille, Matteo Rovere, Stefano Soatto. ICLR 2019\n\nKey Word: Memorization.\n\nDigest Our findings indicate that the early transient is critical in determining the final solution of the optimization associated with training an artificial neural network. In particular, the effects of sensory deficits during a critical period cannot be overcome, no matter how much additional training is performed.\n\nA Closer Look at Memorization in Deep Networks. [paper]\n\nDevansh Arpit, Stanisław Jastrzębski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S. Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, Simon Lacoste-Julien. ICML 2017\n\nKey Word: Memorization.\n\nDigest In our experiments, we expose qualitative differences in gradient-based optimization of deep neural networks (DNNs) on noise vs. real data. We also demonstrate that for appropriately tuned explicit regularization (e.g., dropout) we can degrade DNN training performance on noise datasets without compromising generalization on real data.\n\nEmpirical Study: 2016\n\nUnderstanding deep learning requires rethinking generalization. [paper]\n\nChiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, Oriol Vinyals. ICLR 2017\n\nKey Word: Memorization.\n\nDigest Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data.\n\nNeural Collapse\n\nNeural Collapse: 2024\n\nNeural Collapse Meets Differential Privacy: Curious Behaviors of NoisyGD with Near-perfect Representation Learning. [paper]\n\nChendi Wang, Yuqing Zhu, Weijie J. Su, Yu-Xiang Wang.\n\nkey Word: Neural Collapse; Differential Privacy.\n\nDigest The study by De et al. (2022) explores the impact of large-scale representation learning on differentially private (DP) learning, focusing on the phenomenon of Neural Collapse (NC) in deep learning and transfer learning. The research establishes an error bound within the NC framework, evaluates feature quality, reveals the lesser robustness of DP fine-tuning, and suggests strategies to enhance its robustness, with empirical evidence supporting these findings.\n\nAverage gradient outer product as a mechanism for deep neural collapse. [paper]\n\nDaniel Beaglehole, Peter Súkeník, Marco Mondelli, Mikhail Belkin.\n\nKey Word: Neural Collapse; Average Gradient Outer Product.\n\nDigest This paper investigates the phenomenon of Deep Neural Collapse (DNC), where the final layers of Deep Neural Networks (DNNs) exhibit a highly structured representation of data. The study presents significant evidence that DNC primarily occurs through the process of deep feature learning, facilitated by the average gradient outer product (AGOP). This approach marks a departure from previous explanations that relied on feature-agnostic models. The authors highlight the role of the right singular vectors and values of the network weights in reducing within-class variability, a key aspect of DNC. They establish a link between this singular structure and the AGOP, further demonstrating experimentally and theoretically that AGOP can induce DNC even in randomly initialized neural networks. The paper also discusses Deep Recursive Feature Machines, a conceptual method representing AGOP feature learning in convolutional neural networks, and shows its capability to exhibit DNC.\n\nPushing Boundaries: Mixup's Influence on Neural Collapse. [paper]\n\nQuinn Fisher, Haoming Meng, Vardan Papyan.\n\nKey Word: Mixup; Neural Collapse.\n\nDigest The abstract investigates \"Mixup,\" a technique enhancing deep neural network robustness by blending training data and labels, focusing on its success through geometric configurations of network activations. It finds that mixup leads to a unique alignment of last-layer activations that challenges prior expectations, with mixed examples of the same class aligning with the classifier and different classes marking distinct boundaries. This unexpected behavior suggests mixup affects deeper network layers in a novel way, diverging from simple convex combinations of class features. The study connects these findings to improved model calibration and supports them with a theoretical analysis, highlighting the role of a specific geometric pattern (simplex equiangular tight frame) in optimizing last-layer features for better performance.\n\nNeural Collapse: 2023\n\nAre Neurons Actually Collapsed? On the Fine-Grained Structure in Neural Representations. [paper]\n\nYongyi Yang, Jacob Steinhardt, Wei Hu. ICML 2023\n\nKey Word: Neural Collapse.\n\nDigest The paper challenges the notion of \"Neural Collapse\" in well-trained neural networks, arguing that while the last-layer representations may appear to collapse, there is still fine-grained structure present in the representations that captures the intrinsic structure of the input distribution.\n\nNeural (Tangent Kernel) Collapse. [paper]\n\nMariia Seleznova, Dana Weitzner, Raja Giryes, Gitta Kutyniok, Hung-Hsu Chou.\n\nKey Word: Neural Collapse; Neural Tangent Kernel.\n\nDigest This paper investigates how the Neural Tangent Kernel (NTK), which tracks how deep neural networks (DNNs) change during training, and the Neural Collapse (NC) phenomenon, which refers to the symmetry and structure in the last-layer features of trained classification DNNs, are related. They assume that the empirical NTK has a block structure that matches the class labels, meaning that samples of the same class are more correlated than samples of different classes. They show how this assumption leads to the dynamics of DNNs trained with mean squared (MSE) loss and the emergence of NC in DNNs with block-structured NTK. They support their theory with large-scale experiments on three DNN architectures and three datasets.\n\nNeural Collapse Inspired Feature-Classifier Alignment for Few-Shot Class Incremental Learning. [paper] [code]\n\nYibo Yang, Haobo Yuan, Xiangtai Li, Zhouchen Lin, Philip Torr, Dacheng Tao. ICLR 2023\n\nKey Word: Few-Shot Class Incremental Learning; Neural Collapse.\n\nDigest We deal with this misalignment dilemma in FSCIL inspired by the recently discovered phenomenon named neural collapse, which reveals that the last-layer features of the same class will collapse into a vertex, and the vertices of all classes are aligned with the classifier prototypes, which are formed as a simplex equiangular tight frame (ETF). It corresponds to an optimal geometric structure for classification due to the maximized Fisher Discriminant Ratio.\n\nNeural Collapse in Deep Linear Network: From Balanced to Imbalanced Data. [paper]\n\nHien Dang, Tan Nguyen, Tho Tran, Hung Tran, Nhat Ho.\n\nKey Word: Neural Collapse; Imbalanced Learning.\n\nDigest We take a step further and prove the Neural Collapse occurrence for deep linear network for the popular mean squared error (MSE) and cross entropy (CE) loss. Furthermore, we extend our research to imbalanced data for MSE loss and present the first geometric analysis for Neural Collapse under this setting.\n\nNeural Collapse: 2022\n\nPrincipled and Efficient Transfer Learning of Deep Models via Neural Collapse. [paper]\n\nXiao Li, Sheng Liu, Jinxin Zhou, Xinyu Lu, Carlos Fernandez-Granda, Zhihui Zhu, Qing Qu.\n\nKey Word: Neural Collapse; Transfer Learning.\n\nDigest This work delves into the mystery of transfer learning through an intriguing phenomenon termed neural collapse (NC), where the last-layer features and classifiers of learned deep networks satisfy: (i) the within-class variability of the features collapses to zero, and (ii) the between-class feature means are maximally and equally separated. Through the lens of NC, our findings for transfer learning are the following: (i) when pre-training models, preventing intra-class variability collapse (to a certain extent) better preserves the intrinsic structures of the input data, so that it leads to better model transferability; (ii) when fine-tuning models on downstream tasks, obtaining features with more NC on downstream data results in better test accuracy on the given task.\n\nPerturbation Analysis of Neural Collapse. [paper]\n\nTom Tirer, Haoxiang Huang, Jonathan Niles-Weed.\n\nKey Word: Neural Collapse.\n\nDigest We propose a richer model that can capture this phenomenon by forcing the features to stay in the vicinity of a predefined features matrix (e.g., intermediate features). We explore the model in the small vicinity case via perturbation analysis and establish results that cannot be obtained by the previously studied models.\n\nImbalance Trouble: Revisiting Neural-Collapse Geometry. [paper]\n\nChristos Thrampoulidis, Ganesh R. Kini, Vala Vakilian, Tina Behnia.\n\nKey Word: Neural Collapse; Class Imbalance.\n\nDigest Neural Collapse refers to the remarkable structural properties characterizing the geometry of class embeddings and classifier weights, found by deep nets when trained beyond zero training error. However, this characterization only holds for balanced data. Here we thus ask whether it can be made invariant to class imbalances. Towards this end, we adopt the unconstrained-features model (UFM), a recent theoretical model for studying neural collapse, and introduce Simplex-Encoded-Labels Interpolation (SELI) as an invariant characterization of the neural collapse phenomenon.\n\nNeural Collapse: A Review on Modelling Principles and Generalization. [paper]\n\nVignesh Kothapalli, Ebrahim Rasromani, Vasudev Awatramani.\n\nKey Word: Neural Collapse.\n\nDigest We analyse the principles which aid in modelling such a phenomena from the ground up and show how they can build a common understanding of the recently proposed models that try to explain NC. We hope that our analysis presents a multifaceted perspective on modelling NC and aids in forming connections with the generalization capabilities of neural networks. Finally, we conclude by discussing the avenues for further research and propose potential research problems.\n\nDo We Really Need a Learnable Classifier at the End of Deep Neural Network? [paper]\n\nYibo Yang, Liang Xie, Shixiang Chen, Xiangtai Li, Zhouchen Lin, Dacheng Tao.\n\nKey Word: Neural Collapse.\n\nDigest We study the potential of training a network with the last-layer linear classifier randomly initialized as a simplex ETF and fixed during training. This practice enjoys theoretical merits under the layer-peeled analytical framework. We further develop a simple loss function specifically for the ETF classifier. Its advantage gets verified by both theoretical and experimental results.\n\nLimitations of Neural Collapse for Understanding Generalization in Deep Learning. [paper]\n\nLike Hui, Mikhail Belkin, Preetum Nakkiran.\n\nKey Word: Neural Collapse.\n\nDigest We point out that Neural Collapse is primarily an optimization phenomenon, not a generalization one, by investigating the train collapse and test collapse on various dataset and architecture combinations. We propose more precise definitions — \"strong\" and \"weak\" Neural Collapse for both the train set and the test set — and discuss their theoretical feasibility.\n\nNeural Collapse: 2021\n\nOn the Role of Neural Collapse in Transfer Learning. [paper]\n\nTomer Galanti, András György, Marcus Hutter. ICLR 2022\n\nKey Word: Neural Collapse; Transfer Learning.\n\nDigest We provide an explanation for this behavior based on the recently observed phenomenon that the features learned by overparameterized classification networks show an interesting clustering property, called neural collapse.\n\nAn Unconstrained Layer-Peeled Perspective on Neural Collapse. [paper]\n\nWenlong Ji, Yiping Lu, Yiliang Zhang, Zhun Deng, Weijie J. Su. ICLR 2022\n\nKey Word: Neural Collapse; Uncostrained Model; Implicit Regularization.\n\nDigest We introduce a surrogate model called the unconstrained layer-peeled model (ULPM). We prove that gradient flow on this model converges to critical points of a minimum-norm separation problem exhibiting neural collapse in its global minimizer. Moreover, we show that the ULPM with the cross-entropy loss has a benign global landscape for its loss function, which allows us to prove that all the critical points are strict saddle points except the global minimizers that exhibit the neural collapse phenomenon.\n\nNeural Collapse Under MSE Loss: Proximity to and Dynamics on the Central Path. [paper]\n\nX.Y. Han, Vardan Papyan, David L. Donoho. ICLR 2022\n\nKey Word: Neural Collapse; Gradient Flow.\n\nDigest The analytically-tractable MSE loss offers more mathematical opportunities than the hard-to-analyze CE loss, inspiring us to leverage MSE loss towards the theoretical investigation of NC. We develop three main contributions: (I) We show a new decomposition of the MSE loss into (A) terms directly interpretable through the lens of NC and which assume the last-layer classifier is exactly the least-squares classifier; and (B) a term capturing the deviation from this least-squares classifier. (II) We exhibit experiments on canonical datasets and networks demonstrating that term-(B) is negligible during training. This motivates us to introduce a new theoretical construct: the central path, where the linear classifier stays MSE-optimal for feature activations throughout the dynamics. (III) By studying renormalized gradient flow along the central path, we derive exact dynamics that predict NC.\n\nA Geometric Analysis of Neural Collapse with Unconstrained Features. [paper] [code]\n\nZhihui Zhu, Tianyu Ding, Jinxin Zhou, Xiao Li, Chong You, Jeremias Sulam, Qing Qu. NeurIPS 2021\n\nKey Word: Neural Collapse, Nonconvex Optimization.\n\nDigest We provide the first global optimization landscape analysis of Neural Collapse -- an intriguing empirical phenomenon that arises in the last-layer classifiers and features of neural networks during the terminal phase of training. As recently reported by Papyan et al., this phenomenon implies that (i) the class means and the last-layer classifiers all collapse to the vertices of a Simplex Equiangular Tight Frame (ETF) up to scaling, and (ii) cross-example within-class variability of last-layer activations collapses to zero. We study the problem based on a simplified unconstrained feature model, which isolates the topmost layers from the classifier of the neural network.\n\nExploring Deep Neural Networks via Layer-Peeled Model: Minority Collapse in Imbalanced Training. [paper] [code]\n\nCong Fang, Hangfeng He, Qi Long, Weijie J. Su. PNAS\n\nKey Word: Neural Collapse; Imbalanced Training.\n\nDigest In this paper, we introduce the Layer-Peeled Model, a nonconvex yet analytically tractable optimization program, in a quest to better understand deep neural networks that are trained for a sufficiently long time. As the name suggests, this new model is derived by isolating the topmost layer from the remainder of the neural network, followed by imposing certain constraints separately on the two parts of the network. When moving to the imbalanced case, our analysis of the Layer-Peeled Model reveals a hitherto unknown phenomenon that we term Minority Collapse, which fundamentally limits the performance of deep learning models on the minority classes.\n\nNeural Collapse: 2020\n\nPrevalence of Neural Collapse during the terminal phase of deep learning training. [paper] [code]\n\nVardan Papyan, X.Y. Han, David L. Donoho. PNAS\n\nKey Word: Neural Collapse.\n\nDigest This paper studied the terminal phase of training (TPT) of today’s canonical deepnet training protocol. It documented that during TPT a process called Neural Collapse takes place, involving four fundamental and interconnected phenomena: (NC1)-(NC4).\n\nDeep Double Descent\n\nDeep Double Descent: 2023\n\nA U-turn on Double Descent: Rethinking Parameter Counting in Statistical Learning. [paper]\n\nAlicia Curth, Alan Jeffares, Mihaela van der Schaar.\n\nKey Word: Deep Double Descent.\n\nDigest This paper challenges the conventional understanding of the relationship between model complexity and prediction error. It explores the phenomenon of \"double descent,\" which suggests that test error can decrease as the parameter count exceeds the sample size. While this phenomenon has been observed in deep learning and other models like linear regression, trees, and boosting, the paper argues that the interpretation is influenced by multiple complexity axes. It demonstrates that the second descent occurs when the transition between these underlying axes happens and is not inherently tied to the interpolation threshold. The paper proposes a generalized measure for the effective number of parameters, which resolves tensions between double descent and traditional statistical intuition.\n\nDropout Drops Double Descent. [paper]\n\nTian-Le Yang, Joe Suzuki.\n\nKey Word: Dropout; Deep Double Descent.\n\nDigest The paper finds that adding a dropout layer before the fully-connected linear layer can drop the double descent phenomenon. Double descent is when the prediction error rises and drops as sample or model size increases. Optimal dropout can alleviate this in linear and nonlinear regression models, both theoretically and empirically. Optimal dropout can achieve a monotonic test error curve in nonlinear neural networks. Previous deep learning models do not encounter double-descent because they already apply regularization approaches like dropout.\n\nDouble Descent Demystified: Identifying, Interpreting & Ablating the Sources of a Deep Learning Puzzle. [paper]\n\nRylan Schaeffer, Mikail Khona, Zachary Robertson, Akhilan Boopathy, Kateryna Pistunova, Jason W. Rocks, Ila Rani Fiete, Oluwasanmi Koyejo.\n\nKey Word: Deep Double Descent; Tutorial.\n\nDigest We briefly describe double descent, then provide an explanation of why double descent occurs in an informal and approachable manner, requiring only familiarity with linear algebra and introductory probability. We provide visual intuition using polynomial regression, then mathematically analyze double descent with ordinary linear regression and identify three interpretable factors that, when simultaneously all present, together create double descent.\n\nUnifying Grokking and Double Descent. [paper] [code]\n\nXander Davies, Lauro Langosco, David Krueger.\n\nKey Word: Deep Double Descent; Grokking.\n\nDigest We hypothesize that grokking and double descent can be understood as instances of the same learning dynamics within a framework of pattern learning speeds. We propose that this framework also applies when varying model capacity instead of optimization steps, and provide the first demonstration of model-wise grokking.\n\nDeep Double Descent: 2022\n\nSparse Double Descent: Where Network Pruning Aggravates Overfitting. [paper] [code]\n\nZheng He, Zeke Xie, Quanzhi Zhu, Zengchang Qin. ICML 2022\n\nKey Word: Deep Double Descent; Lottery Ticket Hypothesis.\n\nDigest While recent studies focused on the deep double descent with respect to model overparameterization, they failed to recognize that sparsity may also cause double descent. In this paper, we have three main contributions. First, we report the novel sparse double descent phenomenon through extensive experiments. Second, for this phenomenon, we propose a novel learning distance interpretation that the curve of ℓ2 learning distance of sparse models (from initialized parameters to final parameters) may correlate with the sparse double descent curve well and reflect generalization better than minima flatness. Third, in the context of sparse double descent, a winning ticket in the lottery ticket hypothesis surprisingly may not always win.\n\nCan Neural Nets Learn the Same Model Twice? Investigating Reproducibility and Double Descent from the Decision Boundary Perspective. [paper] [code]\n\nGowthami Somepalli, Liam Fowl, Arpit Bansal, Ping Yeh-Chiang, Yehuda Dar, Richard Baraniuk, Micah Goldblum, Tom Goldstein. CVPR 2022\n\nKey Word: Deep Double Descent; Manifold.\n\nDigest We discuss methods for visualizing neural network decision boundaries and decision regions. We use these visualizations to investigate issues related to reproducibility and generalization in neural network training. We observe that changes in model architecture (and its associate inductive bias) cause visible changes in decision boundaries, while multiple runs with the same architecture yield results with strong similarities, especially in the case of wide architectures. We also use decision boundary methods to visualize double descent phenomena.\n\nPhenomenology of Double Descent in Finite-Width Neural Networks. [paper] [code]\n\nSidak Pal Singh, Aurelien Lucchi, Thomas Hofmann, Bernhard Schölkopf. ICLR 2022\n\nKey Word: Deep Double Descent.\n\nDigest 'Double descent' delineates the generalization behaviour of models depending on the regime they belong to: under- or over-parameterized. The current theoretical understanding behind the occurrence of this phenomenon is primarily based on linear and kernel regression models -- with informal parallels to neural networks via the Neural Tangent Kernel. Therefore such analyses do not adequately capture the mechanisms behind double descent in finite-width neural networks, as well as, disregard crucial components -- such as the choice of the loss function. We address these shortcomings by leveraging influence functions in order to derive suitable expressions of the population loss and its lower bound, while imposing minimal assumptions on the form of the parametric model.\n\nDeep Double Descent: 2021\n\nMulti-scale Feature Learning Dynamics: Insights for Double Descent. [paper] [code]\n\nMohammad Pezeshki, Amartya Mitra, Yoshua Bengio, Guillaume Lajoie.\n\nKey Word: Deep Double Descent.\n\nDigest We investigate the origins of the less studied epoch-wise double descent in which the test error undergoes two non-monotonous transitions, or descents as the training time increases. By leveraging tools from statistical physics, we study a linear teacher-student setup exhibiting epoch-wise double descent similar to that in deep neural networks. In this setting, we derive closed-form analytical expressions for the evolution of generalization error over training. We find that double descent can be attributed to distinct features being learned at different scales: as fast-learning features overfit, slower-learning features start to fit, resulting in a second descent in test error.\n\nAsymptotic Risk of Overparameterized Likelihood Models: Double Descent Theory for Deep Neural Networks. [paper]\n\nRyumei Nakada, Masaaki Imaizumi.\n\nKey Word: Deep Double Descent.\n\nDigest We consider a likelihood maximization problem without the model constraints and analyze the upper bound of an asymptotic risk of an estimator with penalization. Technically, we combine a property of the Fisher information matrix with an extended Marchenko-Pastur law and associate the combination with empirical process techniques. The derived bound is general, as it describes both the double descent and the regularized risk curves, depending on the penalization.\n\nDistilling Double Descent. [paper]\n\nAndrew Cotter, Aditya Krishna Menon, Harikrishna Narasimhan, Ankit Singh Rawat, Sashank J. Reddi, Yichen Zhou.\n\nKey Word: Deep Double Descent; Distillation.\n\nDigest Distillation is the technique of training a \"student\" model based on examples that are labeled by a separate \"teacher\" model, which itself is trained on a labeled dataset. The most common explanations for why distillation \"works\" are predicated on the assumption that student is provided with soft labels, e.g. probabilities or confidences, from the teacher model. In this work, we show, that, even when the teacher model is highly overparameterized, and provides hard labels, using a very large held-out unlabeled dataset to train the student model can result in a model that outperforms more \"traditional\" approaches.\n\nDeep Double Descent: 2020\n\nUnderstanding Double Descent Requires a Fine-Grained Bias-Variance Decomposition. [paper]\n\nBen Adlam, Jeffrey Pennington. NeurIPS 2020\n\nKey Word: Deep Double Descent; Bias-Variance.\n\nDigest Classical learning theory suggests that the optimal generalization performance of a machine learning model should occur at an intermediate model complexity, with simpler models exhibiting high bias and more complex models exhibiting high variance of the predictive function. However, such a simple trade-off does not adequately describe deep learning models that simultaneously attain low bias and variance in the heavily overparameterized regime. A primary obstacle in explaining this behavior is that deep learning algorithms typically involve multiple sources of randomness whose individual contributions are not visible in the total variance. To enable fine-grained analysis, we describe an interpretable, symmetric decomposition of the variance into terms associated with the randomness from sampling, initialization, and the labels.\n\nGradient Flow in Sparse Neural Networks and How Lottery Tickets Win. [paper] [code]\n\nUtku Evci, Yani A. Ioannou, Cem Keskin, Yann Dauphin. AAAI 2020\n\nKey Word: Lottery Ticket Hypothesis.\n\nDigest Sparse Neural Networks (NNs) can match the generalization of dense NNs using a fraction of the compute/storage for inference, and also have the potential to enable efficient training. However, naively training unstructured sparse NNs from random initialization results in significantly worse generalization, with the notable exceptions of Lottery Tickets (LTs) and Dynamic Sparse Training (DST). Through our analysis of gradient flow during training we attempt to answer: (1) why training unstructured sparse networks from random initialization performs poorly and; (2) what makes LTs and DST the exceptions?\n\nMultiple Descent: Design Your Own Generalization Curve. [paper]\n\nLin Chen, Yifei Min, Mikhail Belkin, Amin Karbasi. NeurIPS 2021\n\nKey Word: Deep Double Descent.\n\nDigest This paper explores the generalization loss of linear regression in variably parameterized families of models, both under-parameterized and over-parameterized. We show that the generalization curve can have an arbitrary number of peaks, and moreover, locations of those peaks can be explicitly controlled. Our results highlight the fact that both classical U-shaped generalization curve and the recently observed double descent curve are not intrinsic properties of the model family. Instead, their emergence is due to the interaction between the properties of the data and the inductive biases of learning algorithms.\n\nEarly Stopping in Deep Networks: Double Descent and How to Eliminate it. [paper] [code]\n\nReinhard Heckel, Fatih Furkan Yilmaz. ICLR 2021\n\nKey Word: Deep Double Descent; Early Stopping.\n\nDigest We show that such epoch-wise double descent arises for a different reason: It is caused by a superposition of two or more bias-variance tradeoffs that arise because different parts of the network are learned at different epochs, and eliminating this by proper scaling of stepsizes can significantly improve the early stopping performance. We show this analytically for i) linear regression, where differently scaled features give rise to a superposition of bias-variance tradeoffs, and for ii) a two-layer neural network, where the first and second layer each govern a bias-variance tradeoff. Inspired by this theory, we study two standard convolutional networks empirically and show that eliminating epoch-wise double descent through adjusting stepsizes of different layers improves the early stopping performance significantly.\n\nTriple descent and the two kinds of overfitting: Where & why do they appear? [paper] [code]\n\nStéphane d'Ascoli, Levent Sagun, Giulio Biroli.\n\nKey Word:Deep Double Descent.\n\nDigest In this paper, we show that despite their apparent similarity, these two scenarios are inherently different. In fact, both peaks can co-exist when neural networks are applied to noisy regression tasks. The relative size of the peaks is governed by the degree of nonlinearity of the activation function. Building on recent developments in the analysis of random feature models, we provide a theoretical ground for this sample-wise triple descent.\n\nA Brief Prehistory of Double Descent. [paper]\n\nMarco Loog, Tom Viering, Alexander Mey, Jesse H. Krijthe, David M.J. Tax.\n\nKey Word: Deep Double Descent.\n\nDigest This letter draws attention to some original, earlier findings, of interest to double descent.\n\nDouble Trouble in Double Descent : Bias and Variance(s) in the Lazy Regime. [paper] [code]\n\nStéphane d'Ascoli, Maria Refinetti, Giulio Biroli, Florent Krzakala. ICML 2020\n\nKey Word: Deep Double Descent; Bias-Variance.\n\nDigest Deep neural networks can achieve remarkable generalization performances while interpolating the training data perfectly. Rather than the U-curve emblematic of the bias-variance trade-off, their test error often follows a \"double descent\" - a mark of the beneficial role of overparametrization. In this work, we develop a quantitative theory for this phenomenon in the so-called lazy learning regime of neural networks, by considering the problem of learning a high-dimensional function with random features regression. We obtain a precise asymptotic expression for the bias-variance decomposition of the test error, and show that the bias displays a phase transition at the interpolation threshold, beyond which it remains constant.\n\nRethinking Bias-Variance Trade-off for Generalization of Neural Networks. [paper] [code]\n\nZitong Yang, Yaodong Yu, Chong You, Jacob Steinhardt, Yi Ma. ICML 2020\n\nKey Word: Deep Double Descent; Bias-Variance.\n\nDigest The classical bias-variance trade-off predicts that bias decreases and variance increase with model complexity, leading to a U-shaped risk curve. Recent work calls this into question for neural networks and other over-parameterized models, for which it is often observed that larger models generalize better. We provide a simple explanation for this by measuring the bias and variance of neural networks: while the bias is monotonically decreasing as in the classical theory, the variance is unimodal or bell-shaped: it increases then decreases with the width of the network.\n\nThe Curious Case of Adversarially Robust Models: More Data Can Help, Double Descend, or Hurt Generalization. [paper]\n\nYifei Min, Lin Chen, Amin Karbasi. UAI 2021\n\nKey Word: Deep Double Descent.\n\nDigest We challenge this conventional belief and show that more training data can hurt the generalization of adversarially robust models in the classification problems. We first investigate the Gaussian mixture classification with a linear loss and identify three regimes based on the strength of the adversary. In the weak adversary regime, more data improves the generalization of adversarially robust models. In the medium adversary regime, with more training data, the generalization loss exhibits a double descent curve, which implies the existence of an intermediate stage where more training data hurts the generalization. In the strong adversary regime, more data almost immediately causes the generalization error to increase.\n\nDeep Double Descent: 2019\n\nDeep Double Descent: Where Bigger Models and More Data Hurt. [paper]\n\nPreetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, Ilya Sutskever. ICLR 2020\n\nKey Word: Deep Double Descent.\n\nDigest We show that a variety of modern deep learning tasks exhibit a \"double-descent\" phenomenon where, as we increase model size, performance first gets worse and then gets better.\n\nDeep Double Descent: 2018\n\nReconciling modern machine learning practice and the bias-variance trade-off. [paper]\n\nMikhail Belkin, Daniel Hsu, Siyuan Ma, Soumik Mandal. PNAS\n\nKey Word: Bias-Variance; Over-Parameterization.\n\nDigest In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This \"double descent\" curve subsumes the textbook U-shaped bias-variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance.\n\nA Modern Take on the Bias-Variance Tradeoff in Neural Networks. [paper]\n\nBrady Neal, Sarthak Mittal, Aristide Baratin, Vinayak Tantia, Matthew Scicluna, Simon Lacoste-Julien, Ioannis Mitliagkas.\n\nKey Word: Bias-Variance; Over-Parameterization.\n\nDigest The bias-variance tradeoff tells us that as model complexity increases, bias falls and variances increases, leading to a U-shaped test error curve. However, recent empirical results with over-parameterized neural networks are marked by a striking absence of the classic U-shaped test error curve: test error keeps decreasing in wider networks. Motivated by the shaky evidence used to support this claim in neural networks, we measure bias and variance in the modern setting. We find that both bias and variance can decrease as the number of parameters grows. To better understand this, we introduce a new decomposition of the variance to disentangle the effects of optimization and data sampling.\n\nLottery Ticket Hypothesis\n\nLottery Ticket Hypothesis: 2024\n\nNo Free Prune: Information-Theoretic Barriers to Pruning at Initialization. [paper]\n\nTanishq Kumar, Kevin Luo, Mark Sellke.\n\nKey Word: Lottery Tickets; Overparameterization; Mutual Information.\n\nDigest This paper investigates the concept of \"lottery tickets\" in deep learning, questioning the necessity of large models versus identifying and training sparse networks from the start. Despite attempts, finding these efficient subnetworks without training the full model has largely failed. The study proposes a theoretical reason for this, focusing on the effective parameter count, which includes non-zero weights and the data-related information within the sparsity mask. It extends the Law of Robustness to sparse networks, suggesting that data-dependent masks are crucial for robust performance. The findings indicate that masks created during or after training contain more information than those at initialization, affecting the network's effective capacity. This explains the difficulty in finding lottery tickets without full model training, as confirmed by experimental results on neural networks.\n\nLottery Ticket Hypothesis: 2023\n\nInstant Soup: Cheap Pruning Ensembles in A Single Pass Can Draw Lottery Tickets from Large Models. [paper]\n\nAjay Jaiswal, Shiwei Liu, Tianlong Chen, Ying Ding, Zhangyang Wang.\n\nKey Word: Lottery Tickets; Model Soup.\n\nDigest The paper introduces Instant Soup Pruning (ISP), a novel approach that leverages the idea of model soups to generate high-quality subnetworks from large pre-trained models, reducing the computational cost compared to traditional iterative magnitude pruning (IMP) methods.\n\nA Three-regime Model of Network Pruning. [paper]\n\nYefan Zhou, Yaoqing Yang, Arin Chang, Michael W. Mahoney.\n\nKey Word: Pruning; Linear Mode Connectivity.\n\nDigest This paper proposes a model based on statistical mechanics to predict how training hyperparameters affect pruning performance of neural networks. The paper finds a sharp transition phenomenon that depends on two parameters in the pre-pruned and pruned models. The paper also identifies three types of global structures in the pruned loss landscape and applies the model to three practical scenarios.\n\nGeneralization Bounds for Magnitude-Based Pruning via Sparse Matrix Sketching. [paper]\n\nEtash Kumar Guha, Prasanjit Dubey, Xiaoming Huo.\n\nKey Word: Magnitude-Based Pruning; Norm-based Generalization Bound; Sparse Matrix Sketching.\n\nDigest This paper proposes a new bound on the generalization error of Magnitude-Based pruning1, a technique that removes weights with small magnitudes from neural networks. The paper improves on previous bounds by using Sparse Matrix Sketching, a method that compresses pruned matrices into smaller dimensions. The paper also extends the results to Iterative Pruning, a process that prunes and retrains the network multiple times. The paper shows that the new method achieves better generalization than existing methods on some datasets.\n\nPruning at Initialization -- A Sketching Perspective. [paper]\n\nNoga Bar, Raja Giryes.\n\nKey Word: Pruning at Ininitialization; Sketching Algorithm; Neural Tangent Kernel.\n\nDigest The paper studies how to prune linear neural networks before training. They show that this problem is related to the sketching problem for fast matrix multiplication. They use this connection to analyze the error and data dependence of pruning at initialization. They also propose a general improvement to existing pruning algorithms based on sketching techniques.\n\nNTK-SAP: Improving neural network pruning by aligning training dynamics. [paper] [code]\n\nYite Wang, Dawei Li, Ruoyu Sun. ICLR 2023\n\nKey Word: Pruning at Ininitialization; Neural Tangent Kernel.\n\nDigest We propose to prune the connections that have the least influence on the spectrum of the NTK. This method can he"
    }
}
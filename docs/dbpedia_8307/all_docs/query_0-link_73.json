{
    "id": "dbpedia_8307_0",
    "rank": 73,
    "data": {
        "url": "https://pypi.org/project/symbolicai/",
        "read_more_link": "",
        "language": "en",
        "title": "symbolicai",
        "top_image": "https://pypi.org/static/images/twitter.abaf4b19.webp",
        "meta_img": "https://pypi.org/static/images/twitter.abaf4b19.webp",
        "images": [
            "https://pypi.org/static/images/logo-small.8998e9d1.svg",
            "https://pypi-camo.freetls.fastly.net/74e19ec56eab7a3a15b25917f7c3da766179a448/68747470733a2f2f7365637572652e67726176617461722e636f6d2f6176617461722f62306233653338393432333630646663373332396239656234313831666133643f73697a653d3530",
            "https://pypi-camo.freetls.fastly.net/23093a54d140100ba10dfc017b7cb597f3e058ce/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f457874656e7369747941492f73796d626f6c696361692f6d61696e2f6173736574732f696d616765732f73796d61695f6c6f676f2e706e67",
            "https://pypi-camo.freetls.fastly.net/08f2af4c4fa71b23f5990413f97baee8ec8fb6b7/68747470733a2f2f62616467652e667572792e696f2f70792f73796d626f6c696361692e737667",
            "https://pypi-camo.freetls.fastly.net/3677cc2d00014b87886b24c89fb398ba1f388d81/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4253445f332d2d436c617573652d626c75652e737667",
            "https://pypi-camo.freetls.fastly.net/64d707a9fcebd1c17e9a45f081caaa78b79df6fb/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f75726c2f68747470732f747769747465722e636f6d2f64696e756d6172697573632e7376673f7374796c653d736f6369616c266c6162656c3d466f6c6c6f7725323025343044696e754d617269757343",
            "https://pypi-camo.freetls.fastly.net/269908b2e8eb1349881ec72ffa6d1a622948d8a2/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f75726c2f68747470732f747769747465722e636f6d2f73796d626f6c69636170692e7376673f7374796c653d736f6369616c266c6162656c3d466f6c6c6f77253230253430457874656e736974794149",
            "https://pypi-camo.freetls.fastly.net/793ef3fadef48113e66f3c652ebc9b591bcb1745/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f6e747269627574696f6e732d77656c636f6d652d627269676874677265656e2e7376673f7374796c653d666c6174",
            "https://pypi-camo.freetls.fastly.net/a986de780eafeab766b0f346db1a17bbe089f882/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f3736383038373136313837383038353634333f6c6162656c3d446973636f7264266c6f676f3d446973636f7264266c6f676f436f6c6f723d7768697465",
            "https://pypi-camo.freetls.fastly.net/6ff00ee97a01a218ca1f1ef761a889507298eb4f/68747470733a2f2f686974732e736565796f756661726d2e636f6d2f6170692f636f756e742f696e63722f62616467652e7376673f75726c3d68747470732533412532462532466769746875622e636f6d253246587069746669726525324673796d626f6c6963616926636f756e745f62673d253233373943383344267469746c655f62673d2532333535353535352669636f6e3d2669636f6e5f636f6c6f723d253233453745374537267469746c653d6869747326656467655f666c61743d66616c7365",
            "https://pypi-camo.freetls.fastly.net/d9034b0e139f9a6e6a181320c77781d3f63b1dbe/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f457874656e7369747941492f73796d626f6c696361692e7376673f7374796c653d736f6369616c266c6162656c3d466f726b266d61784167653d32353932303030",
            "https://pypi-camo.freetls.fastly.net/94abe8087b0c4017272953a3dcee4a56d85f17bc/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f457874656e7369747941492f73796d626f6c696361692e7376673f7374796c653d736f6369616c266c6162656c3d53746172266d61784167653d32353932303030",
            "https://pypi-camo.freetls.fastly.net/903df96977f3cb79d3f43ab6c56fe83fa0b4753f/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f457874656e7369747941492f73796d626f6c696361692f6d61696e2f6173736574732f696d616765732f707265766965772e676966",
            "https://pypi-camo.freetls.fastly.net/64b38884af91e38e95311dc7f4b8c835b1aa60af/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f457874656e7369747941492f73796d626f6c696361692f6d61696e2f6173736574732f696d616765732f766964362e706e67",
            "https://pypi-camo.freetls.fastly.net/e4f656fec51a0db6ac04e17fd728479e7e191b36/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f457874656e7369747941492f73796d626f6c696361692f6d61696e2f6173736574732f696d616765732f766964352e706e67",
            "https://pypi-camo.freetls.fastly.net/57a615e1d548649057aaf2a1bb120cc3c6e03294/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f457874656e7369747941492f73796d626f6c696361692f6d61696e2f6173736574732f696d616765732f766964342e706e67",
            "https://pypi-camo.freetls.fastly.net/2550b1fb8fa788459b020181b0cdcb59aa505d61/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f457874656e7369747941492f73796d626f6c696361692f6d61696e2f6173736574732f696d616765732f766964332e706e67",
            "https://pypi-camo.freetls.fastly.net/6cb5bfd01b059c7e76d5712e8141d490b4634348/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f457874656e7369747941492f73796d626f6c696361692f6d61696e2f6173736574732f696d616765732f766964322e706e67",
            "https://pypi-camo.freetls.fastly.net/2b9e32e4681ba89b23bd0668a661c2ecebc68622/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f457874656e7369747941492f73796d626f6c696361692f6d61696e2f6173736574732f696d616765732f766964312e706e67",
            "https://pypi-camo.freetls.fastly.net/e1b386b362193663e12b0f97f76861d9e7ae0652/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f457874656e7369747941492f73796d626f6c696361692f6d61696e2f6173736574732f696d616765732f73796d73682e706e67",
            "https://pypi-camo.freetls.fastly.net/da63df55adad8414d8ec265d9c49a86e4fc6879b/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f457874656e7369747941492f73796d626f6c696361692f6d61696e2f6173736574732f696d616765732f73637265656e312e6a706567",
            "https://pypi-camo.freetls.fastly.net/3ca71afb8992e2d2905fee3d3c914ae423ce2c14/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f457874656e7369747941492f73796d626f6c696361692f6d61696e2f6173736574732f696d616765732f696d67352e706e67",
            "https://pypi-camo.freetls.fastly.net/c5ba20d8916d0375fff45d8260c37d4541439190/68747470733a2f2f6d656469612e67697068792e636f6d2f6d656469612f6d47634e6a736657416a593541455a4e77362f67697068792e676966",
            "https://pypi-camo.freetls.fastly.net/5deaace2047236c934efecf4f5dcf0e157e84795/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f457874656e7369747941492f73796d626f6c696361692f6d61696e2f6173736574732f696d616765732f696d67312e706e67",
            "https://pypi-camo.freetls.fastly.net/4b0be70faec51526b3bae6d059eb2690e1149de4/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f457874656e7369747941492f73796d626f6c696361692f6d61696e2f6173736574732f696d616765732f696d67372e706e67",
            "https://pypi-camo.freetls.fastly.net/3e990ab74a0377900cc7e64c9de1a8261963ca17/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f457874656e7369747941492f73796d626f6c696361692f6d61696e2f6173736574732f696d616765732f696d6731302e706e67",
            "https://pypi-camo.freetls.fastly.net/37c911b6fe618ea2cb6c8d878bd9e58032e25138/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f457874656e7369747941492f73796d626f6c696361692f6d61696e2f6173736574732f696d616765732f696d67332e706e67",
            "https://pypi-camo.freetls.fastly.net/cdf236034d8cfdd10c69259679beb1fb5921a8cc/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f457874656e7369747941492f73796d626f6c696361692f6d61696e2f6173736574732f696d616765732f696d67392e706e67",
            "https://pypi-camo.freetls.fastly.net/da67061167d8309dc028abb2be1a75c6f30b74e1/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f457874656e7369747941492f73796d626f6c696361692f6d61696e2f6173736574732f696d616765732f696d67342e706e67",
            "https://pypi-camo.freetls.fastly.net/a63be77cffd07335311b2c9c122fa0821043d1af/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f457874656e7369747941492f73796d626f6c696361692f6d61696e2f6173736574732f696d616765732f696d67322e706e67",
            "https://pypi-camo.freetls.fastly.net/91bbec0c9c63917ced4b052963bc012fbd32aacf/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f457874656e7369747941492f73796d626f6c696361692f6d61696e2f6173736574732f696d616765732f696d67362e706e67",
            "https://pypi-camo.freetls.fastly.net/111b517615b69ca619c48631edd19707dca41240/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f457874656e7369747941492f73796d626f6c696361692f6d61696e2f6173736574732f696d616765732f696d67382e706e67",
            "https://pypi-camo.freetls.fastly.net/910d26f842d924da65269ed2a72a6e25f553ab83/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f457874656e7369747941492f73796d626f6c696361692f6d61696e2f6578616d706c65732f726573756c74732f6e6577735f707265762e706e67",
            "https://pypi-camo.freetls.fastly.net/85e91bbb928104e4ce317951541520c6b9c170e1/68747470733a2f2f6d7962696e6465722e6f72672f62616467655f6c6f676f2e737667",
            "https://pypi-camo.freetls.fastly.net/b25417590ee03361d4fc19892bf9ddc977990477/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f457874656e7369747941492f73796d626f6c696361692f6d61696e2f6173736574732f696d616765732f6361742e6a7067",
            "https://pypi-camo.freetls.fastly.net/1c6b48d4ac9b582dbc400aa2bd53b809f2f2b394/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446f6e6174652d50617950616c2d677265656e2e737667",
            "https://pypi-camo.freetls.fastly.net/a986de780eafeab766b0f346db1a17bbe089f882/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f3736383038373136313837383038353634333f6c6162656c3d446973636f7264266c6f676f3d446973636f7264266c6f676f436f6c6f723d7768697465",
            "https://pypi-camo.freetls.fastly.net/74e19ec56eab7a3a15b25917f7c3da766179a448/68747470733a2f2f7365637572652e67726176617461722e636f6d2f6176617461722f62306233653338393432333630646663373332396239656234313831666133643f73697a653d3530",
            "https://pypi.org/static/images/blue-cube.572a5bfb.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi-camo.freetls.fastly.net/ed7074cadad1a06f56bc520ad9bd3e00d0704c5b/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f707970692d6173736574732f73706f6e736f726c6f676f732f6177732d77686974652d6c6f676f2d7443615473387a432e706e67",
            "https://pypi-camo.freetls.fastly.net/8855f7c063a3bdb5b0ce8d91bfc50cf851cc5c51/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f707970692d6173736574732f73706f6e736f726c6f676f732f64617461646f672d77686974652d6c6f676f2d6668644c4e666c6f2e706e67",
            "https://pypi-camo.freetls.fastly.net/df6fe8829cbff2d7f668d98571df1fd011f36192/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f707970692d6173736574732f73706f6e736f726c6f676f732f666173746c792d77686974652d6c6f676f2d65684d3077735f6f2e706e67",
            "https://pypi-camo.freetls.fastly.net/420cc8cf360bac879e24c923b2f50ba7d1314fb0/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f707970692d6173736574732f73706f6e736f726c6f676f732f676f6f676c652d77686974652d6c6f676f2d616734424e3774332e706e67",
            "https://pypi-camo.freetls.fastly.net/524d1ce72f7772294ca4c1fe05d21dec8fa3f8ea/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f707970692d6173736574732f73706f6e736f726c6f676f732f6d6963726f736f66742d77686974652d6c6f676f2d5a443172685444462e706e67",
            "https://pypi-camo.freetls.fastly.net/d01053c02f3a626b73ffcb06b96367fdbbf9e230/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f707970692d6173736574732f73706f6e736f726c6f676f732f70696e67646f6d2d77686974652d6c6f676f2d67355831547546362e706e67",
            "https://pypi-camo.freetls.fastly.net/67af7117035e2345bacb5a82e9aa8b5b3e70701d/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f707970692d6173736574732f73706f6e736f726c6f676f732f73656e7472792d77686974652d6c6f676f2d4a2d6b64742d706e2e706e67",
            "https://pypi-camo.freetls.fastly.net/b611884ff90435a0575dbab7d9b0d3e60f136466/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f707970692d6173736574732f73706f6e736f726c6f676f732f737461747573706167652d77686974652d6c6f676f2d5467476c6a4a2d502e706e67"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2024-07-30T16:57:35+00:00",
        "summary": "",
        "meta_description": "A Neuro-Symbolic Framework for Large Language Models",
        "meta_lang": "en",
        "meta_favicon": "/static/images/favicon.35549fe8.ico",
        "meta_site_name": "PyPI",
        "canonical_link": "https://pypi.org/project/symbolicai/",
        "text": "A Neuro-Symbolic Perspective on Large Language Models (LLMs)\n\nBuilding applications with LLMs at the core using our Symbolic API facilitates the integration of classical and differentiable programming in Python.\n\nRead full paper here.\n\nRead further documentation here.\n\nAbstract\n\nConceptually, SymbolicAI is a framework that leverages machine learning – specifically LLMs – as its foundation, and composes operations based on task-specific prompting. We adopt a divide-and-conquer approach to break down a complex problem into smaller, more manageable problems. Consequently, each operation addresses a simpler task. By reassembling these operations, we can resolve the complex problem. Moreover, our design principles enable us to transition seamlessly between differentiable and classical programming, allowing us to harness the power of both paradigms.\n\nTutorials\n\nDate Title Video 2nd Dec. 2023 Use ChatGPT and off-the-shelf RAG on Terminal/Command Prompt/Shell 21st Nov. 2023 Virtual Persona from Documents, Multi-Agent Chat, Text-to-Speech to hear your Personas 1st Aug. 2023 Automatic Retrieval Augmented Generation, Multimodal Inputs, User Packages 22nd July 2023 ChatBot In-Depth Demonstration (Tool Use and Iterative Processing) 1st July 2023 Symbols, Operations, Expressions, LLM-based functions! 9th June 2023 The future is neuro-symbolic: Expressiveness of ChatGPT and generalizability of symbols\n\n📖 Table of Contents\n\nSymbolicAI\n\nA Neuro-Symbolic Perspective on Large Language Models (LLMs)\n\nAbstract\n\nTutorials\n\n📖 Table of Contents\n\n🔧 Get Started\n\n➡️ Quick Install\n\nAPI Keys\n\n[Optional] Installs\n\n🦖 Apps\n\nShell Command Tool\n\n🖥️ Starting an Interactive Shell\n\nAuto-completion\n\nQuery Neuro-Symbolic Model\n\nPipe with Files\n\nSlicing Operation on Files\n\nStateful Conversation\n\nChatbot\n\n📦 Package Manager\n\n📦 Package Runner\n\nUsage\n\nExamples\n\nAlias File\n\nNote\n\n📦 Package Initializer\n\nUsage\n\nExamples\n\nImport\n\n💯 Other Use Cases\n\nCommunity Demos\n\n🤷‍♂️ Why SymbolicAI?\n\nTell me some more fun facts!\n\nHow Does it Work?\n\nSymbolic Operations\n\nRanking Objects\n\nEvaluating Expressions by Best Effort\n\nDynamic Casting\n\nProbabilistic Programming\n\n🧠 Causal Reasoning\n\nOperations\n\nCustom Operations\n\nFew-Shot Operations\n\nPrompt Design\n\n😑 Expressions\n\nSequence expressions\n\nStream expressions\n\n❌ Error Handling\n\n🕷️ Interpretability, Testing & Debugging\n\nUnit Testing Models\n\n🔥Debugging\n\nExample: News Summary\n\n▶️ Experiment with Our API\n\n📈 Interface for Query and Response Inspection\n\n🤖 Engines\n\nSymbolic Engine\n\nSpeech Engine\n\nOCR Engine\n\nSearch Engine\n\nWebCrawler Engine\n\nDrawing Engine\n\nFile Engine\n\nIndexing Engine\n\nCLIP Engine\n\nLocal Neuro-Symbolic Engine\n\nCustom Engine\n\n⚡Limitations\n\n🥠 Future Work\n\nConclusion\n\n👥 References, Related Work, and Credits\n\nComparison to Other Frameworks\n\nAcknowledgements\n\nContribution\n\n📜 Citation\n\n📝 License\n\nLike this Project?\n\n📫 Contact\n\n🔧 Get Started\n\n➡️ Quick Install\n\npip install symbolicai\n\nOne can run our framework in two ways:\n\nusing local engines (experimental) that are run on your local machine (see Local Neuro-Symbolic Engine section), or\n\nusing engines powered by external APIs, i.e. using OpenAI's API (see API Keys).\n\nAPI Keys\n\nBefore the first run, define exports for the required API keys to enable the respective engines. This will register the keys in internally for subsequent runs. By default SymbolicAI currently uses OpenAI's neural engines, i.e. GPT-3 Davinci-003, DALL·E 2 and Embedding Ada-002, for the neuro-symbolic computations, image generation and embeddings computation respectively. However, these modules can easily be replaced with open-source alternatives. Examples are\n\nOPT or Bloom for neuro-symbolic computations,\n\nCraiyon for image generation,\n\nand any BERT variants for semantic embedding computations.\n\nTo set the OpenAI API Keys use the following command:\n\n# Linux / MacOS exportOPENAI_API_KEY=\"<OPENAI_API_KEY>\" # Windows (PowerShell) $Env:OPENAI_API_KEY=\"<OPENAI_API_KEY>\" # Jupyter Notebooks (important: do not use quotes) %envOPENAI_API_KEY=<OPENAI_API_KEY>\n\nTo get started import our library by using:\n\nimport symai as ai\n\nOverall, the following engines are currently supported:\n\nNeuro-Symbolic Engine: OpenAI's LLMs (supported GPT-3, ChatGPT, GPT-4) (as an experimental alternative using llama.cpp for local models)\n\nEmbedding Engine: OpenAI's Embedding API\n\n[Optional] Symbolic Engine: WolframAlpha\n\n[Optional] Search Engine: SerpApi\n\n[Optional] OCR Engine: APILayer\n\n[Optional] SpeechToText Engine: OpenAI's Whisper\n\n[Optional] WebCrawler Engine: Selenium\n\n[Optional] Image Rendering Engine: DALL·E 2\n\n[Optional] Indexing Engine: Pinecone\n\n[Optional] CLIP Engine: 🤗 Hugging Face (experimental image and text embeddings)\n\n[Optional] Installs\n\nSymbolicAI uses multiple engines to process text, speech and images. We also include search engine access to retrieve information from the web. To use all of them, you will need to install also the following dependencies or assign the API keys to the respective engines.\n\nIf you want to use the WolframAlpha Engine, Search Engine or OCR Engine you will need to export the following API keys:\n\n# Linux / MacOS exportSYMBOLIC_ENGINE_API_KEY=\"<WOLFRAMALPHA_API_KEY>\" exportSEARCH_ENGINE_API_KEY=\"<SERP_API_KEY>\" exportOCR_ENGINE_API_KEY=\"<APILAYER_API_KEY>\" exportINDEXING_ENGINE_API_KEY=\"<PINECONE_API_KEY>\" # Windows (PowerShell) $Env:SYMBOLIC_ENGINE_API_KEY=\"<WOLFRAMALPHA_API_KEY>\" $Env:SEARCH_ENGINE_API_KEY=\"<SERP_API_KEY>\" $Env:OCR_ENGINE_API_KEY=\"<APILAYER_API_KEY>\" $Env:INDEXING_ENGINE_API_KEY=\"<PINECONE_API_KEY>\"\n\nTo use the optional engines, install the respective extras:\n\npip install\"symbolicai[wolframalpha]\" pip install\"symbolicai[whisper]\" pip install\"symbolicai[selenium]\" pip install\"symbolicai[serpapi]\" pip install\"symbolicai[pinecone]\"\n\nOr, install all optional dependencies at once:\n\npip install\"symbolicai[all]\"\n\n[Note] Additionally, you need to install the respective codecs.\n\nSpeechToText Engine: ffmpeg for audio processing (based on OpenAI's whisper)\n\n# Linux sudo apt update&& sudo apt install ffmpeg # MacOS brew install ffmpeg # Windows choco install ffmpeg\n\nWebCrawler Engine: For selenium, we automatically install the driver with chromedriver-autoinstaller. Currently we only support Chrome as the default browser.\n\nAlternatively, you can specify in your project path a symai.config.json file with all the engine properties. This will replace the environment variables. See the following configuration file as an example:\n\n{ \"NEUROSYMBOLIC_ENGINE_API_KEY\":\"<OPENAI_API_KEY>\", \"NEUROSYMBOLIC_ENGINE_MODEL\":\"text-davinci-003\", \"SYMBOLIC_ENGINE_API_KEY\":\"<WOLFRAMALPHA_API_KEY>\", \"EMBEDDING_ENGINE_API_KEY\":\"<OPENAI_API_KEY>\", \"EMBEDDING_ENGINE_MODEL\":\"text-embedding-ada-002\", \"IMAGERENDERING_ENGINE_API_KEY\":\"<OPENAI_API_KEY>\", \"VISION_ENGINE_MODEL\":\"openai/clip-vit-base-patch32\", \"SEARCH_ENGINE_API_KEY\":\"<SERP_API_KEY>\", \"SEARCH_ENGINE_MODEL\":\"google\", \"OCR_ENGINE_API_KEY\":\"<APILAYER_API_KEY>\", \"SPEECH_TO_TEXT_ENGINE_MODEL\":\"base\", \"TEXT_TO_SPEECH_ENGINE_MODEL\":\"tts-1\", \"INDEXING_ENGINE_API_KEY\":\"<PINECONE_API_KEY>\", \"INDEXING_ENGINE_ENVIRONMENT\":\"us-west1-gcp\", \"COLLECTION_DB\":\"ExtensityAI\", \"COLLECTION_STORAGE\":\"SymbolicAI\", \"SUPPORT_COMMUNITY\":False }\n\n[NOTE]: Our framework allows you to support us train models for local usage by enabling the data collection feature. On application startup we show the terms of services and you can activate or disable this community feature. We do not share or sell your data to 3rd parties and only use the data for research purposes and to improve your user experience. To change this setting you will be prompted with in our setup wizard to enable or disable community support or you can go to the symai.config.json file located in your home directory of your .symai folder (i.e., ~/.symai/symai.config.json), and turn it on/off by setting the SUPPORT_COMMUNITY property to True/False via the config file or the respective environment variable. [NOTE]: By default, the user warnings are enabled. To disable them, export SYMAI_WARNINGS=0 in your environment variables.\n\n🦖 Apps\n\nWe provide a set of useful tools that demonstrate how to interact with our framework and enable package manage. You can access these apps by calling the sym+<shortcut-name-of-app> command in your terminal or PowerShell.\n\nShell Command Tool\n\nThe Shell Command Tool is a basic shell command support tool that translates natural language commands into shell commands. To start the Shell Command Tool, simply run:\n\nsymsh\"<your-query>\"\n\nFor more information about the tool and available arguments, use the --help flag:\n\nsymsh --help\n\nHere is an example of how to use the Shell Command Tool:\n\n$> symsh\"PowerShell edit registry entry\" # :Output: # Set-ItemProperty -Path <path> -Name <name> -Value <value> $> symsh\"Set-ItemProperty -Path <path> -Name <name> -Value <value>\" --add\"path='/Users/myuser' name=Demo value=SymbolicAI\" # :Output: # Set-ItemProperty -Path '/Users/myuser' -Name Demo -Value SymbolicAI $> symsh\"Set-ItemProperty -Path '/Users/myuser' -Name Demo -Value SymbolicAI\" --del\"string quotes\" # :Output: # Set-ItemProperty -Path /Users/myuser -Name Demo -Value SymbolicAI $> symsh\"Set-ItemProperty -Path '/Users/myuser' -Name Demo -Value SymbolicAI\" --convert\"linux\" # :Output: # export Demo=\"SymbolicAI\"\n\n🖥️ Starting an Interactive Shell\n\nsymsh is also a regular shell program that interacts with users in the terminal emulation window. It interprets Linux, MacOS, and Windows PowerShell shell commands, and supports ANSI escape sequences.\n\n[NOTE]: Because the colors for the default style is highly dependent on whether the theme is light or dark, they may not be displayed correctly in some terminals. You can change the default style to better fit your needs by modifying the symsh.config.json file in the .symai directory in your home directory (~/.symai/symsh.config.json).\n\nTo enter an interactive shell, simply run without any additional parameters:\n\n$> symsh\n\nThe interactive shell uses the python -m symai.shell feature and runs on top of your existing terminal.\n\nWithin the interactive shell you can use your regular shell commands and additionally use the symsh neuro-symbolic commands. The interactive shell supports the following commands:\n\nAuto-completion\n\nsymsh provides path auto-completion and history auto-completion enhanced by the neuro-symbolic engine. Start typing the path or command, and symsh will provide you with relevant suggestions based on your input and command history. To trigger a suggestion, press Tab or Ctrl+Space.\n\nQuery Neuro-Symbolic Model\n\nsymsh can interact with a language model. By beginning a command with a special character (\", ', or `), symsh will treat the command as a query for a language model.\n\nFor instance, to make a query, you can type:\n\n$>\"What is the capital of France?\" # :Output: # Paris\n\nPipe with Files\n\nThe shell command in symsh also has the capability to interact with files using the pipe (|) operator. It operates like a Unix-like pipe but with a few enhancements due to the neuro-symbolic nature of symsh.\n\nHere is the basic usage of the pipe with files:\n\n$>\"explain this file\"| file_path.txt\n\nThis command would instruct the AI to explain the file file_path.txt and consider its contents for the conversation.\n\nSlicing Operation on Files\n\nThe real power of symsh shines through when dealing with large files. symsh extends the typical file interaction by allowing users to select specific sections or slices of a file.\n\nTo use this feature, you would need to append the desired slices to the filename within square brackets []. The slices should be comma-separated, and you can apply Python's indexing rules. You can specify a single line, a range of lines, or step indexing.\n\nHere are a few examples:\n\nSingle line:\n\n$>\"analyze this line\"| file_path.txt[10]\n\nRange of lines:\n\n# analyze lines 10 to 20 $>\"analyze this line\"| file_path.txt[10:20]\n\nStep indexing:\n\n# analyze lines 10 to 30 with a step size of 3 $>\"analyze this line\"| file_path.txt[10:30:3]\n\nMulti-line indexing:\n\n# analyze lines 10 to 30 with a step size of 3, and lines 40 to 50 $>\"analyze this line\"| file_path.txt[10:30:3,20,40:50]\n\nThe above commands would read and include the specified lines from file file_path.txt into the ongoing conversation.\n\nThis feature enables you to maintain highly efficient and context-thoughtful conversations with symsh, especially useful when dealing with large files where only a subset of content in specific locations within the file is relevant at any given moment.\n\nStateful Conversation\n\nThe stateful_conversation feature is used for maintaining a continuing conversation with the language model. To use this feature, you have to start your commands with specific symbols in the shell:\n\nCreating a new stateful conversation:\n\nUse any of these three symbols at the start of your command: !\", !', or !`. This will initialize a new stateful conversation. If there was a previously saved conversation, these commands will overwrite it.\n\nContinuing a stateful conversation:\n\nUse one of these three symbols at the start of your command: .\", .', or .`. The command can then be used to continue the most recent stateful conversation. If no previous conversation exists, a new one is created.\n\nExample:\n\nStarting a new conversation: !\"what is your name\"\n\nContinuing the conversation: .\"how old are you\"\n\nThese commands can be used in any shell operation. Keep in mind, stateful conversations are saved and can be resumed later. The shell will save the conversation automatically if you type exit or quit to exit the interactive shell.\n\nStateful conversation offers the capability to process files as well. If your command contains a pipe (|), the shell will treat the text after the pipe as the name of a file to add it to the conversation.\n\nExample:\n\n$> !\"explain this file\"| my_file.txt\n\nThis command will instruct the AI to explain the file my_file.txt and consider its contents in the conversation. Afterwards you can continue the conversation with:\n\n$> .\"what did you mean with ...?\"\n\nChatbot\n\nYou can engage in a basic conversation with Symbia, a chatbot that uses SymbolicAI to detect the content of your request and switch between different contextual modes to answer your questions. These modes include search engines, speech engines, and more. To start the chatbot, simply run:\n\n$> symchat\n\nThis will launch a chatbot interface:\n\nSymbia: Hi there! I'm Symbia, your virtual assistant. How may Ihelp you? $>\n\nTo exit the conversation, type exit, quit, or press Ctrl+C. You can also load our chatbot SymbiaChat into a jupyter notebook and process step-wise requests.\n\n📦 Package Manager\n\nWe provide a package manager called sympkg that allows you to manage extensions from the command line. With sympkg, you can install, remove, list installed packages, or update a module.\n\nTo use sympkg, follow the steps below:\n\nOpen your terminal or PowerShell.\n\nRun the following command: sympkg <command> [<args>]\n\nThe available commands are:\n\ni or install: Install a new package. To install a package, use the following command: sympkg i <package>\n\nr or remove: Remove an installed package. To remove a package, use the following command: sympkg r <package>\n\nl or list: List all installed packages. To list installed packages, use the following command: sympkg l\n\nu or update: Update an installed package. To update a package, use the following command: sympkg u <package>\n\nFor more information on each command, you can use the --help flag. For example, to get help on the i command, use the following command: sympkg i --help.\n\nNote: The package manager is based on GitHub, so you will need git installed to install or update packages. The packages names use the GitHub <username>/<repo_name> convention.\n\nHappy package managing!\n\n📦 Package Runner\n\nThe Package Runner is a command-line tool that allows you to run packages via alias names. It provides a convenient way to execute commands or functions defined in packages. You can access the Package Runner by using the symrun command in your terminal or PowerShell.\n\nUsage\n\nTo use the Package Runner, you can run the following command:\n\n$> symrun <alias>[<args>]| <command> <alias>[<package>]\n\nThe most commonly used Package Runner commands are:\n\n<alias> [<args>]: Run an alias\n\nc <alias> <package>: Create a new alias\n\nl: List all aliases\n\nr <alias>: Remove an alias\n\nExamples\n\nHere are a few examples to illustrate how to use the Package Runner:\n\n$> symrun my_alias arg1 arg2kwarg1=value1kwarg2=value2\n\nThis command runs the specified my_alias with the provided arguments arg1, arg2, kwarg1 and kwarg2, where arg1 and arg2 are considered as *args parameter and kwarg1 and kwarg1 **kwargs key-value arguments. These arguments will be passed on to the executable expression within the expression.\n\n$> symrun c my_alias <username>/<repo_name>\n\nThis command creates a new alias named my_alias that points to <username>/<repo_name>.\n\n$> symrun l\n\nThis command lists all the aliases that have been created.\n\n$> symrun r my_alias\n\nThis command removes the alias named my_alias.\n\nAlias File\n\nThe Package Runner stores aliases in a JSON file named aliases.json. This file is located in the .symai/packages/ directory in your home directory (~/.symai/packages/). You can view the contents of this file to see the existing aliases.\n\nHere is an example how to use the sympkg and symrun via shell:\n\nNote\n\nIf the alias specified cannot be found in the alias file, the Package Runner will attempt to run the command as a package. If the package is not found or an error occurs during execution, an appropriate error message will be displayed.\n\nThat's it! You now have a basic understanding of how to use the Package Runner provided to run packages and aliases from the command line.\n\n📦 Package Initializer\n\nThe Package Initializer is a command-line tool provided that allows developers to create new GitHub packages from the command line. It automates the process of setting up a new package directory structure and files. You can access the Package Initializer by using the symdev command in your terminal or PowerShell.\n\nUsage\n\nTo use the Package Initializer, you can run the following command:\n\n$> symdev c <username>/<repo_name>\n\nThe most commonly used Package Initializer command is:\n\nc <username>/<repo_name>: Create a new package\n\nExamples\n\nHere is an example to illustrate how to use the Package Initializer:\n\n$> symdev c symdev/my_package\n\nThis command creates a new package named my_package under the GitHub username symdev.\n\nThe Package Initializer creates the following files and directories:\n\n.gitignore: Specifies files and directories that should be ignored by Git.\n\nLICENSE: Contains the license information for the package.\n\nREADME.md: Contains the description and documentation for the package.\n\nrequirements.txt: Lists the packages and dependencies required by the package.\n\npackage.json: Provides metadata for the package, including version, name, description, and expressions.\n\nsrc/func.py: Contains the main function and expression code for the package.\n\nThe Package Initializer creates the package in the .symai/packages/ directory in your home directory (~/.symai/packages/<username>/<repo_name>). Within the created package you will see the package.json config file defining the new package metadata and symrun entry point and offers the declared expression types to the Import class.\n\nImport\n\nThe Import class is a module management class in the SymbolicAI library. This class provides an easy and controlled way to manage the use of external modules in the user’s project, with main functions including the ability to install, uninstall, update, and check installed modules. It is used to manage expression loading from packages and accesses the respective metadata from the package.json.\n\nThe metadata for the package includes version, name, description, and expressions. It also lists the package dependencies required by the package.\n\nHere is an example of a package.json file:\n\n{ \"version\":\"0.0.1\", \"name\":\"<username>/<repo_name>\", \"description\":\"<Project Description>\", \"expressions\":[{\"module\":\"src/func\",\"type\":\"MyExpression\"}], \"run\":{\"module\":\"src/func\",\"type\":\"MyExpression\"}, \"dependencies\":[] }\n\nversion: Specifies the version number of the package. It is recommended to follow semantic versioning.\n\nname: Specifies the name of the package. It typically follows the format <username>/<repo_name>, where <username> is your GitHub username and <repo_name> is the name of your package repository.\n\ndescription: Provides a brief description of the package.\n\nexpressions: Defines the exported expressions for the package. Each expression is defined by its module and type. The module specifies the file path or module name where the expression is defined, and the type specifies the type of the expression. These are used to be accessed from code by calling `Import.\n\nrun: Specifies the expression that should be executed when the package is run. It follows the same format as the expressions property, only defined by a single entry point type.\n\ndependencies: Lists the package dependencies to other SymbolicAI packages! Dependencies can be specified with their package name <username>/<repo_name>.\n\nNote that the package.json file is automatically created when you use the Package Initializer tool (symdev) to create a new package. Alongside the package.json also a requirements.txt is created. This file contains all the pip relevant dependencies.\n\nTo import a package from code, see the following example:\n\nfrom symai import Import symask_module = Import(\"ExtensityAI/symask\")\n\nThis command will clone the module from the given GitHub repository (ExtensityAI/symask in this case), install any dependencies, and expose the module's classes for use in your project.\n\nYou can also install a module without instantiating it using the install method:\n\nImport.install(\"ExtensityAI/symask\")\n\nThe Import class will automatically handle the cloning of the repository and the installation of dependencies that are declared in the package.json and requirements.txt files of the repository.\n\nPlease refer to the comments in the code for more detailed explanations of how each method of the Import class works.\n\n💯 Other Use Cases\n\nWe have compiled several examples to demonstrate the use of our Symbolic API. These can be found in the notebooks folder.\n\nBasics: Explore the basics notebook to become familiar with our API structure (notebooks/Basics.ipynb)\n\nQueries: Learn about query manipulation in our notebook on contextualized operations (notebooks/Queries.ipynb)\n\nNews & Docs Generation: Discover stream processing in our news and documentation generation notebook (notebooks/News.ipynb)\n\nChatBot: Learn how to implement a custom chatbot based on semantic narrations (notebooks/ChatBot.ipynb)\n\nYou can solve numerous problems with our Symbolic API. We look forward to seeing what you create! Share your work in our community space on Discord: AI Is All You Need / SymbolicAI.\n\nCommunity Demos\n\nWe are showcasing the exciting demos and tools created using our framework. If you want to add your project, feel free to message us on Twitter at @SymbolicAPI or via Discord.\n\n🤷‍♂️ Why SymbolicAI?\n\nSymbolicAI aims to bridge the gap between classical programming, or Software 1.0, and modern data-driven programming (aka Software 2.0). It is a framework designed to build software applications that leverage the power of large language models (LLMs) with composability and inheritance, two potent concepts in the object-oriented classical programming paradigm.\n\nBy using SymbolicAI, you can traverse the spectrum between the classical programming realm and the data-driven programming realm, as illustrated in the following figure:\n\nWe adopt a divide-and-conquer approach, breaking down complex problems into smaller, manageable tasks. We use the expressiveness and flexibility of LLMs to evaluate these sub-problems. By re-combining the results of these operations, we can solve the broader, more complex problem.\n\nIn time, and with sufficient data, we can gradually transition from general-purpose LLMs with zero and few-shot learning capabilities to specialized, fine-tuned models designed to solve specific problems (see above). This strategy enables the design of operations with fine-tuned, task-specific behavior.\n\nTell me some more fun facts!\n\nSymbolicAI is fundamentally inspired by the neuro-symbolic programming paradigm.\n\nNeuro-symbolic programming is an artificial intelligence and cognitive computing paradigm that combines the strengths of deep neural networks and symbolic reasoning.\n\nDeep neural networks are machine learning algorithms inspired by the structure and function of biological neural networks. They excel in tasks such as image recognition and natural language processing. However, they struggle with tasks that necessitate explicit reasoning, like long-term planning, problem-solving, and understanding causal relationships.\n\nSymbolic reasoning uses formal languages and logical rules to represent knowledge, enabling tasks such as planning, problem-solving, and understanding causal relationships. While symbolic reasoning systems excel in tasks requiring explicit reasoning, they fall short in tasks demanding pattern recognition or generalization, like image recognition or natural language processing.\n\nNeuro-symbolic programming aims to merge the strengths of both neural networks and symbolic reasoning, creating AI systems capable of handling various tasks. This combination is achieved by using neural networks to extract information from data and utilizing symbolic reasoning to make inferences and decisions based on that data. Another approach is for symbolic reasoning to guide the neural networks' generative process and increase interpretability.\n\nEmbedded accelerators for LLMs will likely be ubiquitous in future computation platforms, including wearables, smartphones, tablets, and notebooks. These devices will incorporate models similar to GPT-3, ChatGPT, OPT, or Bloom.\n\nLLMs are expected to perform a wide range of computations, like natural language understanding and decision-making. Additionally, neuro-symbolic computation engines will learn how to tackle unseen tasks and resolve complex problems by querying various data sources for solutions and executing logical statements on top. To ensure the content generated aligns with our objectives, it is crucial to develop methods for instructing, steering, and controlling the generative processes of machine learning models. As a result, our approach works to enable active and transparent flow control of these generative processes.\n\nThe figure above depicts this generative process as shifting the probability mass of an input stream toward an output stream in a contextualized manner. With properly designed conditions and expressions, you can validate and guide the behavior towards a desired outcome or repeat expressions that fail to meet requirements. Our approach consists of defining a set of fuzzy operations to manipulate the data stream and condition LLMs to align with our goals. We regard all data objects – such as strings, letters, integers, and arrays – as symbols and view natural language as the primary interface for interaction. See the following figure:\n\nAs long as our goals can be expressed through natural language, LLMs can be used for neuro-symbolic computations. Consequently, we develop operations that manipulate these symbols to construct new symbols. Each symbol can be interpreted as a statement, and multiple statements can be combined to formulate a logical expression.\n\nBy combining statements together, we can build causal relationship functions and complete computations, transcending reliance purely on inductive approaches. The resulting computational stack resembles a neuro-symbolic computation engine at its core, facilitating the creation of new applications in tandem with established frameworks.\n\nHow Does it Work?\n\nWe will now demonstrate how we define our Symbolic API, which is based on object-oriented and compositional design patterns. The Symbol class serves as the base class for all functional operations, and in the context of symbolic programming (fully resolved expressions), we refer to it as a terminal symbol. The Symbol class contains helpful operations that can be interpreted as expressions to manipulate its content and evaluate new Symbols.\n\nSymbolic Operations\n\nLet's define a Symbol and perform some basic manipulations. We begin with a translation operation:\n\nsym = ai.Symbol(\"Welcome to our tutorial.\") sym.translate('German')\n\nOutput: <class'symai.expressions.Symbol'>(value=Willkommen zu unserem Tutorial.)\n\nRanking Objects\n\nOur API can also execute basic data-agnostic operations like filter, rank, or extract patterns. For instance, we can rank a list of numbers:\n\nsym = ai.Symbol(numpy.array([1, 2, 3, 4, 5, 6, 7])) res = sym.rank(measure='numerical', order='descending')\n\nOutput: <class'symai.expressions.Symbol'>(value=['7','6','5','4','3','2','1'])\n\nEvaluating Expressions by Best Effort\n\nEvaluations are resolved in the language domain and by best effort. We showcase this on the example of word2vec.\n\nWord2Vec generates dense vector representations of words by training a shallow neural network to predict a word based on its neighbors in a text corpus. These resulting vectors are then employed in numerous natural language processing applications, such as sentiment analysis, text classification, and clustering.\n\nIn the example below, we can observe how operations on word embeddings (colored boxes) are performed. Words are tokenized and mapped to a vector space where semantic operations can be executed using vector arithmetic.\n\nSimilar to word2vec, we aim to perform contextualized operations on different symbols. However, as opposed to operating in vector space, we work in the natural language domain. This provides us the ability to perform arithmetic on words, sentences, paragraphs, etc., and verify the results in a human-readable format.\n\nThe following examples display how to evaluate such an expression using a string representation:\n\nai.Symbol('King - Man + Women').expression()\n\nOutput: <class'symai.expressions.Symbol'>(value=Queen)\n\nDynamic Casting\n\nWe can also subtract sentences from one another, where our operations condition the neural computation engine to evaluate the Symbols by their best effort. In the subsequent example, it identifies that the word enemy is present in the sentence, so it deletes it and replaces it with the word friend (which is added):\n\nres = ai.Symbol('Hello my enemy') - 'enemy' + 'friend'\n\nOutput: <class'symai.expressions.Symbol'>(value=Hello my friend)\n\nAdditionally, the API performs dynamic casting when data types are combined with a Symbol object. If an overloaded operation of the Symbol class is employed, the Symbol class can automatically cast the second object to a Symbol. This is a convenient way to perform operations between Symbol objects and other data types, such as strings, integers, floats, lists, etc., without cluttering the syntax.\n\nProbabilistic Programming\n\nIn this example, we perform a fuzzy comparison between two numerical objects. The Symbol variant is an approximation of numpy.pi. Despite the approximation, the fuzzy equals == operation still successfully compares the two values and returns True.\n\nsym = ai.Symbol('3.1415...') sym == numpy.pi\n\n:[Output]: True\n\n🧠 Causal Reasoning\n\nThe main goal of our framework is to enable reasoning capabilities on top of the statistical inference of Language Models (LMs). As a result, our Symbol objects offers operations to perform deductive reasoning expressions. One such operation involves defining rules that describe the causal relationship between symbols. The following example demonstrates how the & operator is overloaded to compute the logical implication of two symbols.\n\nres = ai.Symbol('The horn only sounds on Sundays.') & ai.Symbol('I hear the horn.')\n\n:[Output]: <class'symai.expressions.Symbol'>(value=It is Sunday.)\n\nThe current & operation overloads the and logical operator and sends few-shot prompts to the neural computation engine for statement evaluation. However, we can define more sophisticated logical operators for and, or, and xor using formal proof statements. Additionally, the neural engines can parse data structures prior to expression evaluation. Users can also define custom operations for more complex and robust logical operations, including constraints to validate outcomes and ensure desired behavior.\n\nTo provide a more comprehensive understanding, we present several causal examples below. These examples aim to obtain logical answers based on questions like:\n\n# 1) \"A line parallel to y = 4x + 6 passes through (5, 10). What is the y-coordinate of the point where this line crosses the y-axis?\" # 2) \"Bob has two sons, John and Jay. Jay has one brother and father. The father has two sons. Jay's brother has a brother and a father. Who is Jay's brother?\" # 3) \"Is 1000 bigger than 1063.472?\"\n\nAn example approach using our framework would involve identifying the neural engine best suited for the task and preparing the input for that engine. Here's how we could achieve this:\n\nval = \"<one of the examples above>\" # First, define a class that inherits from the Expression class class ComplexExpression(ai.Expression): # more on the Expression class in later sections # write a method that returns the causal evaluation def causal_expression(self): pass # see below for implementation # instantiate an object of the class expr = ComplexExpression(val) # set WolframAlpha as the main expression engine to use wolfram = ai.Interface('wolframalpha') # evaluate the expression res = expr.causal_expression()\n\nA potential implementation of the causal_expression method could resemble the following:\n\ndef causal_expression(self): # verify which case to use based on `self.value` if self.isinstanceof('mathematics'): # get the mathematical formula formula = self.extract('mathematical formula') # verify the problem type if formula.isinstanceof('linear function'): # prepare for WolframAlpha question = self.extract('question sentence') req = question.extract('what is requested?') x = self.extract('coordinate point (.,.)') # get the coordinate point / could also ask for other points query = formula | f', point x = {x}' | f', solve {req}' # concatenate the question and formula res = wolfram(query) # send the prepared query to WolframAlpha elif formula.isinstanceof('number comparison'): res = wolfram(formula) # send directly to WolframAlpha ... # more cases elif self.isinstanceof('linguistic problem'): sentences = self / '.' # first, split into sentences graph = {} # define the graph for s in sentences: sym = ai.Symbol(s) relations = sym.extract('connected entities (e.g., A has three B => A | A: three B)') / '|' # and split by pipe for r in relations: ... # add relations and populate the graph, or alternatively, learn about CycleGT ... # more cases return res\n\nIn the example above, the causal_expression method iteratively extracts information, enabling manual resolution or external solver usage.\n\nAttention: Keep in mind that this implementation sketch requires significantly more engineering effort for the causal_expression method. Additionally, the current GPT-3 LLM backend may sometimes struggle to extract accurate information or make the correct comparison. However, we believe that future advances in the field, specifically fine-tuned models like ChatGPT with Reinforcement Learning from Human Feedback (RLHF), will improve these capabilities.\n\nLastly, with sufficient data, we could fine-tune methods to extract information or build knowledge graphs using natural language. This advancement would allow the performance of more complex reasoning tasks, like those mentioned above. Therefore, we recommend exploring recent publications on Text-to-Graphs. In this approach, answering the query involves simply traversing the graph and extracting the necessary information.\n\nIn the next section, we will explore operations.\n\nOperations\n\nOperations form the core of our framework and serve as the building blocks of our API. These operations define the behavior of symbols by acting as contextualized functions that accept a Symbol object and send it to the neuro-symbolic engine for evaluation. Operations then return one or multiple new objects, which primarily consist of new symbols but may include other types as well. Polymorphism plays a crucial role in operations, allowing them to be applied to various data types such as strings, integers, floats, and lists, with different behaviors based on the object instance.\n\nOperations are executed using the Symbol object's value attribute, which contains the original data type converted into a string representation and sent to the engine for processing. As a result, all values are represented as strings, requiring custom objects to define a suitable __str__ method for conversion while preserving the object's semantics.\n\nInheritance is another essential aspect of our API, which is built on the Symbol class as its base. All operations are inherited from this class, offering an easy way to add custom operations by subclassing Symbol while maintaining access to basic operations without complicated syntax or redundant functionality. Subclassing the Symbol class allows for the creation of contextualized operations with unique constraints and prompt designs by simply overriding the relevant methods. However, it is recommended to subclass the Expression class for additional functionality.\n\nDefining custom operations can be done through overriding existing Python methods and providing a custom prompt object with example code. Here is an example of creating a custom == operation by overriding the __eq__ method:\n\nclass Demo(ai.Symbol): def __eq__(self, other) -> bool: @ai.equals(examples=ai.Prompt([ \"1 == 'ONE' =>True\", \"'six' == 7 =>False\", \"'Acht' == 'eight' =>True\", ... ]) ) def _func(_, other) -> bool: return False # default behavior on failure return _func(self, other)\n\nBasic operations in Symbol are implemented by defining local functions and decorating them with corresponding operation decorators from the symai/core.py file, a collection of predefined operation decorators that can be applied rapidly to any function. Using local functions instead of decorating main methods directly avoids unnecessary communication with the neural engine and allows for default behavior implementation. It also helps cast operation return types to symbols or derived classes, using the self.sym_return_type(...) method for contextualized behavior based on the determined return type. More details can be found in the Symbol class.\n\nThe following section demonstrates that most operations in symai/core.py are derived from the more general few_shot decorator.\n\nCustom Operations\n\nDefining custom operations is also possible, such as creating an operation to generate a random integer between 0 and 10:\n\nclass Demo(ai.Expression): def __init__(self, value = '', **kwargs) -> None: super().__init__(value, **kwargs) @ai.zero_shot(prompt=\"Generate a random integer between 0 and 10.\", constraints=[ lambda x: x >= 0, lambda x: x <= 10 ]) def get_random_int(self) -> int: pass\n\nThe Symbolic API employs Python Decorators to define operations, utilizing the @ai.zero_shot decorator to create custom operations that do not require demonstration examples when the prompt is self-explanatory. In this example, the zero_shot decorator accepts two arguments: prompt and constraints. The former defines the prompt dictating the desired operation behavior, while the latter establishes validation constraints for the computed outcome, ensuring it meets expectations.\n\nIf a constraint is not satisfied, the implementation will utilize the specified default fallback or default value. If neither is provided, the Symbolic API will raise a ConstraintViolationException. The return type is set to int in this example, so the value from the wrapped function will be of type int. The implementation uses auto-casting to a user-specified return data type, and if casting fails, the Symbolic API will raise a ValueError. If no return type is specified, the return type defaults to Any.\n\nFew-Shot Operations\n\nThe @ai.few_shot decorator is a generalized version of the @ai.zero_shot decorator, used to define custom operations that require demonstration examples. To provide a clearer understanding, we present the function signature of the few_shot decorator:\n\ndef few_shot(prompt: str, examples: Prompt, constraints: List[Callable] = [], default: Optional[object] = None, limit: int = 1, pre_processors: Optional[List[PreProcessor]] = None, post_processors: Optional[List[PostProcessor]] = None, **decorator_kwargs):\n\nThe prompt and constraints attributes behave similarly to those in the zero_shot decorator. The examples and limit arguments are new. The examples argument defines a list of demonstrations used to condition the neural computation engine, while the limit argument specifies the maximum number of examples returned, given that there are more results. The pre_processors argument accepts a list of PreProcessor objects for pre-processing input before it's fed into the neural computation engine. The post_processors argument accepts a list of PostProcessor objects for post-processing output before returning it to the user. Lastly, the decorator_kwargs argument passes additional arguments from the decorator kwargs, which are streamlined towards the neural computation engine and other engines.\n\nTo provide a more comprehensive understanding of our conceptual implementation, refer to the flow diagram below, containing the most important classes:\n\nThe colors indicate logical groups of data processing steps. Yellow represents input and output data, blue shows places where one can customize or prepare the input of the engine, green indicates post-processing steps of the engine response, red displays the application of constraints (including attempted casting of the return type signature if specified in the decorated method), and grey denotes the custom method defining all properties, thus having access to all the previously mentioned objects.\n\nTo conclude this section, here is an example of how to write a custom Japanese name generator using our @ai.zero_shot decorator:\n\nimport symai as ai class Demo(ai.Symbol): @ai.few_shot(prompt=\"Generate Japanese names: \", examples=ai.Prompt( [\"愛子\", \"和花\", \"一郎\", \"和枝\"] ), limit=2, constraints=[lambda x: len(x) > 1]) def generate_japanese_names(self) -> list: return ['愛子', '和花'] # dummy implementation\n\nIf the neural computation engine cannot compute the desired outcome, it will revert to the default implementation or default value. If no default implementation or value is found, the method call will raise an exception.\n\nPrompt Design\n\nThe Prompt class is used to perform all the above operations. Acting as a container for information required to define a specific operation, the Prompt class also serves as the base class for all other Prompt classes.\n\nHere's an example of defining a Prompt to enforce the neural computation engine to compare two values:\n\nclass CompareValues(ai.Prompt): def __init__(self) -> ai.Prompt: super().__init__([ \"4 > 88 =>False\", \"-inf < 0 =>True\", \"inf > 0 =>True\", \"4 > 3 =>True\", \"1 < 'four' =>True\", ... ])\n\nWhen calling the <= operation on two Symbols, the neural computation engine evaluates the symbols in the context of the CompareValues prompt.\n\nres = ai.Symbol(1) <= ai.Symbol('one')\n\nThis statement evaluates to True since the fuzzy compare operation conditions the engine to compare the two Symbols based on their semantic meaning.\n\n:[Output]: True\n\nIn general, the semantics of Symbol operations may vary depending on the context hierarchy of the expression class and the operations used. To better illustrate this, we display our conceptual prompt design in the following figure:\n\nThe figure illustrates the hierarchical prompt design as a container for information provided to the neural computation engine to define a task-specific operation. The yellow and green highlighted boxes indicate mandatory string placements, dashed boxes represent optional placeholders, and the red box marks the starting point of model prediction.\n\nThree main prompt designs are considered: Context-based Prompts, Operational Prompts, and Templates. Prompts can be curated either by inheritance or composition. For example, Static Context can be defined by inheriting the Expression class and overriding the static_context property. An Operation and Template prompt can be created by providing a PreProcessor to modify input data.\n\nEach prompt concept is explained in more detail below:\n\nContext-based Prompts (Static, Dynamic, and Payload) are considered optional and can be defined either statically (by subclassing the Expression class and overriding the static_context property) or at runtime (by updating the dynamic_context property or passing payload kwargs to a method). As an example of using the payload kwargs via method signature:\n\n# creating a query to ask if an issue was resolved or not sym = Symbol(\"<some-community-conversation>\") q = sym.query(\"Was the issue resolved?\") # write manual condition to check if the issue was resolved if 'not resolved' in q: # do a new query but payload the previous query answer to the new query sym.query(\"What was the resolution?\", payload=q) ... else: pass # all good\n\nRegardless of how the context is set, the contextualized prompt defines the desired behavior of Expression operations. For example, one can operate within a domain-specific language context without having to override each base class method. See more details in this notebook.\n\nOperation prompts define the behavior of atomic operations and are mandatory to express the nature of such operations. For example, the + operation is used to add two Symbols together, so its prompt explains this behavior. Examples provide an optional structure giving the neural computation engine a set of demonstrations used to condition it properly. For instance, the + operation prompt can be conditioned on adding numbers by providing demonstrations like 1 + 1 = 2, 2 + 2 = 4, etc.\n\nTemplate prompts are optional and encapsulate the resulting prediction to enforce a specific format. For example, to generate HTML tags, one can use the curated <html>{{placeholder}}</html> template. This template ensures that the neural computation engine starts the generation process within the context of an HTML tag format, avoiding the production of irrelevant descriptions regarding its task.\n\n😑 Expressions\n\nAn Expression is a non-terminal symbol that can be further evaluated. It inherits all the properties from the Symbol class and overrides the __call__ method to evaluate its expressions or values. All other expressions are derived from the Expression class, which also adds additional capabilities, such as the ability to fetch data from URLs, search on the internet, or open files. These operations are specifically separated from the Symbol class as they do not use the value attribute of the Symbol class.\n\nSymbolicAI's API closely follows best practices and ideas from PyTorch, allowing the creation of complex expressions by combining multiple expressions as a computational graph. Each Expression has its own forward method that needs to be overridden. The forward method is used to define the behavior of the expression. It is called by the __call__ method, which is inherited from the Expression base class. The __call__ method evaluates an expression and returns the result from the implemented forward method. This design pattern evaluates expressions in a lazy manner, meaning the expression is only evaluated when its result is needed. It is an essential feature that allows us to chain complex expressions together. Numerous helpful expressions can be imported from the symai.components file.\n\nOther important properties inherited from the Symbol class include sym_return_type and static_context. These two properties define the context in which the current Expression operates, as described in the Prompt Design section. The static_context influences all operations of the current Expression sub-class. The sym_return_type ensures that after evaluating an Expression, we obtain the desired return object type. It is usually implemented to return the current type but can be set to return a different type.\n\nExpressions may have more complex structures and can be further sub-classed, as shown in the Sequence expression example in the following figure:\n\nA Sequence expression can hold multiple expressions evaluated at runtime.\n\nSequence expressions\n\nHere is an example of defining a Sequence expression:\n\n# First import all expressions from symai.components import * # Define a sequence of expressions Sequence( Clean(), Translate(), Outline(), Compose('Compose news:'), )\n\nStream expressions\n\nAs previously mentioned, we can create contextualized prompts to define the behavior of operations on our neural engine. However, this limits the available context size due to GPT-3 Davinci's context length constraint of 4097 tokens. This issue can be addressed using the Stream processing expression, which opens a data stream and performs chunk-based operations on the input stream.\n\nA Stream expression can be wrapped around other expressions. For example, the chunks can be processed with a Sequence expression that allows multiple chained operations in a sequential manner. Here is an example of defining a Stream expression:\n\nStream(Sequence( Clean(), Translate(), Outline(), Embed() ))\n\nThe example above opens a stream, passes a Sequence object which cleans, translates, outlines, and embeds the input. Internally, the stream operation estimates the available model context size and breaks the long input text into smaller chunks, which are passed to the inner expression. The returned object type is a generator.\n\nThis approach has the drawback of processing chunks independently, meaning there is no shared context or information among chunks. To address this issue, the Cluster expression can be used, where the independent chunks are merged based on their similarity, as illustrated in the following figure:\n\nIn the illustrated example, all individual chunks are merged by clustering the information within each chunk. It consolidates contextually related information, merging them meaningfully. The clustered information can then be labeled by streaming through the content of each cluster and extracting the most relevant labels, providing interpretable node summaries.\n\nThe full example is shown below:\n\nstream = Stream(Sequence( Clean(), Translate(), Outline(), )) sym = Symbol('<some long text>') res = Symbol(list(stream(sym))) expr = Cluster() expr(res)\n\nNext, we could recursively repeat this process on each summary node, building a hierarchical clustering structure. Since each Node resembles a summarized subset of the original information, we can use the summary as an index. The resulting tree can then be used to navigate and retrieve the original information, transforming the large data stream problem into a search problem.\n\nAlternatively, vector-based similarity search can be used to find similar nodes. Libraries such as Annoy, Faiss, or Milvus can be employed for searching in a vector space.\n\n❌ Error Handling\n\nA key idea of the SymbolicAI API is code generation, which may result in errors that need to be handled contextually. In the future, we want our API to self-extend and resolve issues automatically. We propose the Try expression, which has built-in fallback statements and retries an execution with dedicated error analysis and correction. The expression analyzes the input and error, conditioning itself to resolve the error by manipulating the original code. If the fallback expression succeeds, the result is returned. Otherwise, this process is repeated for the specified number of retries. If the maximum number of retries is reached and the problem remains unresolved, the error is raised again.\n\nSuppose we have some executable code generated previously. By the nature of generative processes, syntax errors may occur. Using the Execute expression, we can evaluate our generated code, which takes in a symbol and tries to execute it. Naturally, this will fail. However, in the following example, the Try expression resolves the syntax error, and we receive a computed result.\n\nexpr = Try(expr=Execute()) sym = Symbol('a = int(\"3,\")') # Some code with a syntax error res = expr(sym)\n\nThe resulting output is the corrected, evaluated code:\n\n:Output: a=3\n\nWe are aware that not all errors are as simple as the syntax error example shown, which can be resolved automatically. Many errors occur due to semantic misconceptions, requiring contextual information. We are exploring more sophisticated error handling mechanisms, including the use of streams and clustering to resolve errors in a hierarchical, contextual manner. It is also important to note that neural computation engines need further improvements to better detect and resolve errors.\n\n🕷️ Interpretability, Testing & Debugging\n\nPerhaps one of the most significant advantages of using neuro-symbolic programming is that it allows for a clear understanding of how well our LLMs comprehend simple operations. Specifically, we gain insight into whether and at what point they fail, enabling us to follow their StackTraces and pinpoint the failure points. In our case, neuro-symbolic programming enables us to debug the model predictions based on dedicated unit tests for simple operations. To detect conceptual misalignments, we can use a chain of neuro-symbolic operations and validate the generative process. Although not a perfect solution, as the verification might also be error-prone, it provides a principled way to detect conceptual flaws and biases in our LLMs.\n\nUnit Testing Models\n\nSince our approach is to divide and conquer complex problems, we can create conceptual unit tests and target very specific and tractable sub-problems. The resulting measure, i.e., the success rate of the model prediction, can then be used to evaluate their performance and hint at undesired flaws or biases.\n\nThis method allows us to design domain-specific benchmarks and examine how well general learners, such as GPT-3, adapt with certain prompts to a set of tasks.\n\nFor example, we can write a fuzzy comparison operation that can take in digits and strings alike and perform a semantic comparison. LLMs can then be asked to evaluate these expressions. Often, these LLMs still fail to understand the semantic equivalence of tokens in digits vs. strings and provide incorrect answers.\n\nThe following code snippet shows a unit test to perform semantic comparison of numbers (between digits and strings):\n\nimport unittest from symai import * class TestComposition(unittest.TestCase): def test_compare(self): res = Symbol(10) > Symbol(5) self.assertTrue(res) res = Symbol(1) < Symbol('five') self.assertTrue(res) ...\n\n🔥Debugging\n\nWhen creating complex expressions, we debug them by using the Trace expression, which allows us to print out the applied expressions and follow the StackTrace of the neuro-symbolic operations. Combined with the Log expression, which creates a dump of all prompts and results to a log file, we can analyze where our models potentially failed.\n\nExample: News Summary\n\nIn the following example, we create a news summary expression that crawls the given URL and streams the site content through multiple expressions. The outcome is a news website created based on the crawled content. The Trace expression allows us to follow the StackTrace of the operations and observe which operations are currently being executed. If we open the outputs/engine.log file, we can see the dumped traces with all the prompts and results.\n\n# crawling the website and creating a website based on its facts news = News(url='https://www.cnbc.com/cybersecurity/', pattern='cnbc', filters=ExcludeFilter('sentences about subscriptions, licensing, newsletter'), render=True) expr = Log(Trace(news)) res = expr()\n\nHere is the corresponding StackTrace of the model:\n\nThe above code creates a webpage with the crawled content from the original source. See the preview below, the entire rendered webpage image here, and the resulting code of the webpage here.\n\n▶️ Experiment with Our API\n\nLaunch and explore the notebook here:\n\nFind more examples in the examples folder and the notebooks folder. You can also examine the test cases in the tests folder.\n\n📈 Interface for Query and Response Inspection\n\nSymbolicAI is a data-driven framework by design. This implies that we can gather data from API interactions while delivering the requested responses. For rapid, dynamic adaptations or prototyping, we can swiftly integrate user-desired behavior into existing prompts. Moreover, we can log user queries and model predictions to make them accessible for post-processing. Consequently, we can enhance and tailor the model's responses based on real-world data.\n\nIn the example below, we demonstrate how to use an Output expression to pass a handler function and access the model's input prompts and predictions. These can be utilized for data collection and subsequent fine-tuning stages. The handler function supplies a dictionary and presents keys for input and output values. The content can then be sent to a data pipeline for additional processing.\n\nsym = Symbol('Hello World!') def handler(res): input_ = res['input'] output = res['output'] expr = Output(expr=sym.translate, handler=handler, verbose=True) res = expr('German')\n\nSince we used verbose, the console print of the Output expression is also visible:\n\nInput:(['Translate the following text into German:\\n\\nHello World!'],) Expression: <bound method Symbol.translate of <class'symai.symbol.Symbol'>(value=Hello World!)> args:('German',) kwargs:{'input_handler': <function OutputEngine.forward.<locals>.input_handler at ... Dictionary:{'instance': <class'symai.components.Output'>(value=None),'func': <function Symbol.output.<locals>._func at ... Output: Hallo Welt!\n\n🤖 Engines\n\nDue to limited computing resources, we currently utilize OpenAI's GPT-3, ChatGPT and GPT-4 API for the neuro-symbolic engine. However, given adequate computing resources, it is feasible to use local machines to reduce latency and costs, with alternative engines like OPT or Bloom. This would enable recursive executions, loops, and more complex expressions.\n\nFurthermore, we interpret all objects as symbols with different encodings and have integrated a set of useful engines that convert these objects into the natural language domain to perform our operations.\n\nSymbolic Engine\n\nAlthough our work primarily emphasizes how LLMs can assess symbolic expressions, many formal statements have already been efficiently implemented in existing symbolic engines, such as WolframAlpha. Therefore, with an API KEY from WolframAlpha, we can use their engine by using the Interface('wolframalpha'). This avoids error-prone evaluations from neuro-symbolic engines for mathematical operations. The following example demonstrates how to use WolframAlpha to compute the result of the variable x:\n\nfrom symai import Interface expression = Interface('wolframalpha') res = expression('x^2 + 2x + 1')\n\n:Output: x= -1\n\nSpeech Engine\n\nTo transcribe audio files, we can perform speech transcription using whisper. The following example demonstrates how to transcribe an audio file and return the text:\n\nfrom symai.interfaces import Interface speech = Interface('whisper') res = speech('examples/audio.mp3')\n\n:Output: I may have overslept.\n\nOCR Engine\n\nTo extract text from images, we can perform optical character recognition (OCR) with APILayer. The following example demonstrates how to transcribe an image and return the text:\n\nfrom symai.interfaces import Interface ocr = Interface('ocr') res = ocr('https://media-cdn.tripadvisor.com/media/photo-p/0f/da/22/3a/rechnung.jpg')\n\nThe OCR engine returns a dictionary with a key all_text where the full text is stored. For more details, refer to their documentation here.\n\n:Output: China Restaurant\\nMaixim,s\\nSegeberger Chaussee273\\n22851 Norderstedt\\nTelefon040/529162 ...\n\nSearch Engine\n\nTo obtain fact-based content, we can perform search queries via SerpApi with a Google backend. The following example demonstrates how to search for a query and return the results:\n\nfrom symai.interfaces import Interface search = Interface('serpapi') res = search('Birthday of Barack Obama')\n\n:Output: August4,1961\n\nWebCrawler Engine\n\nTo access data from the web, we can use Selenium. The following example demonstrates how to crawl a website and return the results:\n\nfrom symai.interfaces import Interface crawler = Interface('selenium') res = crawler(url=\"https://www.google.com/\", pattern=\"google\")\n\nThe pattern property can be used to verify if the document has been loaded correctly. If the pattern is not found, the crawler will timeout and return an empty result.\n\n:Output: GoogleKlicke hier, wenn du nach einigen Sekunden nicht automatisch weitergeleitet wirst.GmailBilderAnmelden ...\n\nDrawing Engine\n\nTo render images from text descriptions, we use DALL·E 2. The following example demonstrates how to draw a text description and return the image:\n\nfrom symai.interfaces import Interface dalle = Interface('dall_e') res = dalle('a cat with a hat')\n\n:Output: https://oaidalleapiprodscus.blob.core.windows.net/private/org-l6FsXDfth6Uct ...\n\nDon't worry, we would never hide an image of a cat with a hat from you. Here is the image preview and link:\n\nFile Engine\n\nTo perform file operations, we use the operating system's file system. Currently, we support only PDF files and plain text files. This is an early stage, and we are working on more sophisticated file system access and remote storage. The following example demonstrates how to read a PDF file and return the text:\n\nexpr = Expression() res = expr.open('./LICENSE')\n\n:Output: BSD3-Clause License\\n\\nCopyright(c)2023 ...\n\nIndexing Engine\n\nWe use Pinecone to index and search for text. The following example demonstrates how to store text as an index and then retrieve the most related match:\n\nexpr = Expression() expr.add(Symbol('Hello World!').zip()) expr.add(Symbol('I like cookies!').zip()) res = expr.get(Symbol('hello').embedding, index_name='default_index').ast() res['matches'][0]['metadata']['text'][0]\n\n:Output: Hello World!\n\nHere, the zip method creates a pair of strings and embedding vectors, which are then added to the index. The line with get retrieves the original source based on the vector value of hello and uses ast to cast the value to a dictionary.\n\nYou can set several optional arguments for the indexing engine. For more details, see the symai/backend/engine_pinecone.py file.\n\nCLIP Engine\n\nTo perform text-based image few-shot classification, we use CLIP. This implementation is very experimental, and conceptually does not fully integrate the way we intend it, since the embeddings of CLIP and GPT-3 are not aligned (embeddings of the same word are not identical for both models). Aligning them remains an open problem for future research. For example, one could learn linear projections from one embedding space to the other.\n\nThe following example demonstrates how to classify the image of our generated cat from above and return the results as an array of probabilities:\n\nclip = Interface('clip') res = clip('https://oaidalleapiprodscus.blob.core.windows.net/private/org-l6FsXDfth6...', ['cat', 'dog', 'bird', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe'])\n\n:Output: array([[9.72840726e-01,6.34790864e-03,2.59368378e-03,3.41371237e-03, 3.71197984e-03,8.53193272e-03,1.03346225e-04,2.08464009e-03, 1.77942711e-04,1.94185617e-04]],dtype=float32)\n\nLocal Neuro-Symbolic Engine\n\nYou can use a locally hosted instance for the Neuro-Symbolic Engine. We build on top of:\n\nllama.cpp through llama-cpp-python. Please follow the llama-cpp-python installation instructions. We make the assumption the user has experience running llama.cpp prior to using our API for local hosting.\n\nhuggingface/transformers through a custom FastAPI server.\n\nllama.cpp backend\n\nFor instance, let's suppose you want to set as a Neuro-Symbolic Engine the latest Llama 3 model. First, download the model with the HuggingFace CLI:\n\nhuggingface-cli download TheBloke/LLaMA-Pro-8B-Instruct-GGUF llama-pro-8b-instruct.Q4_K_M.gguf --local-dir .\n\nNormally, to start the server through llama.cpp you would run something that looks like this:\n\npython -m llama_cpp.server --model ./llama-pro-8b-instruct.Q4_K_M.gguf --n_gpu_layers -1 --chat_format llama-3 --port8000 --host localhost\n\nWith symai, simply set the NEUROSYMBOLIC_ENGINE_MODEL to llamacpp:\n\n{ \"NEUROSYMBOLIC_ENGINE_API_KEY\":\"\", \"NEUROSYMBOLIC_ENGINE_MODEL\":\"llamacpp\", ... }\n\nThen, run symserver with options available for llama.cpp:\n\nsymserver --model ./llama-pro-8b-instruct.Q4_K_M.gguf --n_gpu_layers -1 --chat_format llama-3 --port8000 --host localhost\n\nTo see all the available options llama.cpp provides, run:\n\nsymserver --help\n\nHuggingFace backend\n\nLet's suppose we want to use dolphin-2.9.3-mistral-7B-32k from HuggingFace. First, download the model with the HuggingFace CLI:\n\nhuggingface-cli download cognitivecomputations/dolphin-2.9.3-mistral-7B-32k --local-dir ./dolphin-2.9.3-mistral-7B-32k\n\nFor the HuggingFace server, you have to set the NEUROSYMBOLIC_ENGINE_MODEL to huggingface:\n\n{ \"NEUROSYMBOLIC_ENGINE_API_KEY\":\"\", \"NEUROSYMBOLIC_ENGINE_MODEL\":\"huggingface\", ... }\n\nThen, run symserver with the following options:\n\nsymserver --model ./dolphin-2.9.3-mistral-7B-32k --attn_implementation flash_attention_2\n\nTo see all the available options we support for HuggingFace, run:\n\nsymserver --help\n\nNow you are set to use the local engine.\n\n# do some symbolic computation with the local engine sym = Symbol('Kitties are cute!').compose() print(sym) # :Output: # Kittens are known for their adorable nature and fluffy appearance, making them a favorite addition to many homes across the world. They possess a strong bond with # their owners, providing companionship and comfort that can ease stress and anxiety. With their playful personalities, they are often seen as a symbol of happiness # and joy, and their unique characteristics such as purring, kneading, and head butts bring warmth to our hearts. Cats also have a natural instinct to groom, which # helps them maintain their clean and soft fur. Not only do they bring comfort and love to their owners, but they also have some practical benefits, such as reducing # allergens, deterring pests, and even reducing stress in their surroundings. Overall, it is no surprise that pets have a long history of providing both emotional # and physical comfort and happiness to their owners, making them a much-loved member of families around the world.\n\nCustom Engine\n\nIf you want to replace or extend the functionality of our framework, you can do so by customizing the existing engines or creating new engines. To create and use any other LLM as a backend you can for example change the neurosymbolic engine setting and register the new engine to the EngineRepository. The following example shows how to create a new neurosymbolic engine:\n\nfrom symai.backend.base import Engine from symai.functional import EngineRepository # setup an engine class MyEngine(Engine): def id(self): return 'neurosymbolic' def prepare(self, argument): # get input from the pre-processors output and use *args, **kwargs and prop from argument # argument.prop contains all your kwargs accessible via dot `.` operation and additional meta info # such as function signature, system relevant info etc. prompts = argument.prop.preprocessed_input args = argument.args kwargs = argument.kwargs # prepare the prompt statement as you want (take a look at the other engines like for GPT-4) ... # assign it to prepared_input argument.prop.prepared_input = ... def forward(self, argument): # get prep statement prompt = argument.prop.prepared_input # Your API / engine related call code here return ... # register your engine EngineRepository.register('neurosymbolic', engine)\n\nAny engine is derived from the base class Engine and is then registered in the engines repository using its registry ID. The ID is for instance used in core.py decorators to address where to send the zero/few-shot statements using the class EngineRepository. You can find the EngineRepository defined in functional.py with the respective query method. Every engine has therefore three main methods you need to implement. The id, prepare and forward method. The id return the engine category. The prepare and forward methods have a signature variable called argument which carries all necessary pipeline relevant data. For instance, the output of the argument.prop.preprocessed_input contains the pre-processed output of the PreProcessor objects and is usually what you need to build and pass on to the argument.prop.prepared_input, which is then used in the forward call.\n\nIf you don't want to re-write the entire engine code but overwrite the existing prompt prepare logic, you can do so by subclassing the existing engine and overriding the prepare method.\n\nHere is an example of how to initialize your own engine. We will subclass the existing GPTXCompletionEngine and override the prepare method. This method is called before the neural computation and can be used to modify the input prompt's parameters that will be passed in for execution. In this example, we will replace the prompt with dummy text for illustration purposes:\n\nfrom symai.backend.engines.neurosymbolic.engine_gptX_completion import GPTXCompletionEngine from symai.functional import EngineRepository class DummyEngine(GPTXCompletionEngine): def prepare(self, argument): argument.prop.prepared_input = ['Go wild and generate something!'] custom_engine = DummyEngine() sym = Symbol() EngineRepository.register('neurosymbolic', custom_engine) res = sym.compose()\n\nTo configure an engine, we can forward commands through Expression objects by using the command method. The command method passes on configurations (as **kwargs) to the engines and change functionalities or parameters. The functionalities depend on the respective engine.\n\nIn this example, we will enable verbose mode, where the engine will print out the methods it is executing and the parameters it is using. This is useful for debugging purposes:\n\nsym = Symbol('Hello World!') Expression.command(engines=['neurosymbolic'], verbose=True) res = sym.translate('German')\n\n:Output: <symai.backend.engines.engine_gptX_completion.GPTXCompletionEngine object at0, <function Symbol.translate.<locals>._func at 0x7fd68ba04820>,{'instance': <class'symai.symbol.S ['\\n\\nHallo Welt!']\n\nHere is the list of names of the engines that are currently supported:\n\nneurosymbolic - GPT-3, ChatGPT, GPT-4\n\nsymbolic - WolframAlpha\n\nocr - Optical Character Recognition\n\ntext_vision - CLIP\n\ntext-to-speech - TTS-1 OpenAI\n\nspeech-to-text - Whisper\n\nembedding - OpenAI Embeddings API (ada-002)\n\nuserinput - User Command Line Input\n\nserpapi - SerpApi (Google search)\n\ncrawler - Selenium\n\nexecute - Python Interpreter\n\nindex - Pinecone\n\nopen - File System\n\noutput - Output Callbacks (e.g., for printing to console or storage)\n\nimagerendering - DALL·E 2\n\nFinally, if you want to create a completely new engine but still maintain our workflow, you can use the query function from symai/functional.py and pass in your engine along with all other specified objects (i.e., Prompt, PreProcessor, etc.; see also section Custom Operations).\n\n⚡Limitations\n\nWe are constantly working to improve the framework and overcome limitations and issues. Just to name a few:\n\nEngineering challenges:\n\nOur framework constantly evolves and receives bug fixes. However, we advise caution when considering it for production use cases. For example, the Stream class only estimates the prompt size by approximation, which can fail. One can also create more sophisticated prompt hierarchies and dynamically adjust the global context based on a state-based approach. This would allow for consistent predictions even for long text streams.\n\nOperations need further improvements, such as verification for biases, fairness, robustness, etc.\n\nThe code may not be complete and is not yet optimized for speed and memory usage. It utilizes API-based LLMs due to limitations in computing resources.\n\nCode coverage is not yet complete, and we are still working on the documentation.\n\nIntegrate with a more diverse set of models from Hugging Face or other platforms.\n\nCurrently, we have not accounted for multi-threading and multi-processing.\n\nResearch challenges:\n\nTo reliably use our framework, one needs to further explore how to fine-tune LLMs to specifically solve many of the proposed operations in a more robust and efficient manner.\n\nThe experimental integration of CLIP aims to align image and text embeddings. Enabling decision-making of LLMs based on observations and performing symbolic operations on objects in images or videos would be a significant leap forward. This integration would work well with reinforcement learning approaches and enable us to control policies systematically (see also GATO). Therefore, we need to train large multi-modal variants with image/video data and text data, describing scenes in high detail to obtain neuro-symbolic computation engines that can perform semantic operations similar to move-towards-tree, open-door, etc.\n\nGeneralist LLMs are still highly over-parameterized, and hardware has not yet caught up to hosting these models on everyday machines. This limitation constrains the applicability of our approach not only on small data streams but also creates high latencies, reducing the amount of complexity and expressiveness we can achieve with our expressions.\n\n🥠 Future Work\n\nWe are continually working on enhancing the framework and are receptive to any suggestions, feedback, or comments. Meanwhile, we have identified several areas for potential future developments:\n\nMeta-learning semantic concepts on top of neuro-symbolic expressions\n\nSelf-evolving and self-healing API\n\nIntegration of our neuro-symbolic framework with reinforcement learning\n\nWe believe that LLMs, as neuro-symbolic computation engines, enable a new class of applications, complete with tools and APIs that can perform self-analysis and self-repair. We eagerly anticipate the future developments this area will bring and are looking forward to receiving your feedback and contributions.\n\nConclusion\n\nWe have provided a neuro-symbolic perspective on LLMs and demonstrated their potential as a central component for many multi-modal operations. We offered a technical report on utilizing our framework and briefly discussed the capabilities and prospects of these models for integration with modern software development.\n\n👥 References, Related Work, and Credits\n\nThis project draws inspiration from the following works, among others:\n\nNewell and Simon's Logic Theorist: Historical Background and Impact on Cognitive Modeling\n\nSearch and Reasoning in Problem Solving\n\nThe Algebraic Theory of Context-Free Languages\n\nNeural Networks and the Chomsky Hierarchy\n\nBinding Language Models in Symbolic Languages\n\nTracr: Compiled Transformers as a Laboratory for Interpretability\n\nHow can computers get common sense?\n\nArtificial Intelligence: A Modern Approach\n\nSymPy: symbolic computing in Python\n\nNeuro-symbolic programming\n\nFuzzy Sets\n\nAn early approach toward graded identity and graded membership in set theory\n\nFrom Statistical to Causal Learning\n\nLanguage Models are Few-Shot Learners\n\nDeep reinforcement learning from human preferences\n\nAligning Language Models to Follow Instructions\n\nChain of Thought Prompting Elicits Reasoning in Large Language Models\n\nMeasuring and Narrowing the Compositionality Gap in Language Models\n\nLarge Language Models are Zero-Shot Reasoners\n\nPre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing\n\nReAct: Synergizing Reasoning and Acting in Language Models\n\nUnderstanding Stereotypes in Language Models: Towards Robust Measurement and Zero-Shot Debiasing\n\nConnectionism and Cognitive Architecture: A Critical Analysis\n\nUnit Testing for Concepts in Neural Networks\n\nTeaching Algorithmic Reasoning via In-context Learning\n\nPromptChainer: Chaining Large Language Model Prompts through Visual Programming\n\nPrompting Is Programming: A Query Language For Large Language Models\n\nSelf-Instruct: Aligning Language Model with Self Generated Instructions\n\nREALM: Retrieval-Augmented Language Model Pre-Training\n\nWolfram|Alpha as the Way to Bring Computational Knowledge Superpowers to ChatGPT\n\nBuild a GitHub support bot with GPT3, LangChain, and Python\n\nComparison to Other Frameworks\n\nHere is a brief list contrasting our approach with other frameworks:\n\nWe focus on cognitive science and cognitive architectures research. We believe that the current state of the art in LLMs is not yet ready for general-purpose tasks. So, we concentrate on advances in concept learning, reasoning, and flow control of the generative process.\n\nWe consider LLMs as one type of neuro-symbolic computation engine, which could take various shapes or forms, such as knowledge graphs, rule-based systems, etc. Hence, our approach is not necessarily limited to Transformers or LLMs.\n\nWe aim to advance the development of programming languages and new programming paradigms, along with their programming stack, including neuro-symbolic design patterns that integrate with operators, inheritance, polymorphism, compositionality, etc. Classical object-oriented and compositional design patterns have been well-studied in the literature, but we offer a novel perspective on how LLMs integrate and augment fuzzy logic and neuro-symbolic computation.\n\nOur proposed prompt design helps combine object-oriented paradigms with machine learning models. We believe that prompt misalignments in their current form will be alleviated with further advances in Reinforcement Learning from Human Feedback and other value alignment methods. As a result, these approaches will address the need for prompt engineering or the ability to prompt hack statements, leading to much shorter zero- or few-shot examples (at least for small enough tasks). We envision the power of a divide-and-conquer approach by performing basic operations and recombining them to tackle complex tasks.\n\nWe view operators/methods as being able to move along a spectrum between prompting and fine-tuning, based on task-specific requirements and data availability. We believe this approach is more general compared to prompting frameworks.\n\nWe propose a general method for handling large context sizes and transforming a data stream problem into a search problem, related to reasoning as a search problem in Search and Reasoning in Problem Solving.\n\nWe hope that our work can be seen as complementary and offer a future outlook on how we would like to use machine learning models as an integral part of programming languages and their entire computational stack.\n\nAcknowledgements\n\nWe have a long list of acknowledgements. Special thanks go to our colleagues and friends at the Institute for Machine Learning at Johannes Kepler University (JKU), Linz for their exceptional support and feedback. We are also grateful to the AI Austria RL Community for supporting this project. Additionally, we appreciate all contributors to this project, regardless of whether they provided feedback, bug reports, code, or simply used the framework. Your support is highly valued.\n\nFinally, we would like to thank the open-source community for making their APIs and tools publicly available, including (but not limited to) PyTorch, Hugging Face, OpenAI, GitHub, Microsoft Research, and many others.\n\nSpecial thanks are owed to Kajetan Schweighofer, Markus Hofmarcher, Thomas Natschläger, and Sepp Hochreiter.\n\nContribution\n\nIf you wish to contribute to this project, please read the CONTRIBUTING.md file for details on our code of conduct, as well as the process for submitting pull requests. Any contributions are greatly appreciated.\n\n📜 Citation\n\n@software{Dinu_SymbolicAI_2022, author={Dinu, Marius-Constantin}, editor={Leoveanu-Condrei, Claudiu}, title={{SymbolicAI: A Neuro-Symbolic Perspective on Large Language Models (LLMs)}}, url={https://github.com/ExtensityAI/symbolicai}, month={11}, year={2022} }\n\n📝 License\n\nThis project is licensed under the BSD-3-Clause License - see the LICENSE file for details.\n\nLike this Project?\n\nIf you appreciate this project, please leave a star ⭐️ and share it with friends and colleagues. To support the ongoing development of this project even further, consider donating. Thank you!\n\nWe are also seeking contributors or investors to help grow and support this project. If you are interested, please reach out to us.\n\n📫 Contact\n\nFeel free to contact us with any questions about this project via email, through our website, or find us on Discord:"
    }
}
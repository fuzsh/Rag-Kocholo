{
    "id": "dbpedia_8307_3",
    "rank": 10,
    "data": {
        "url": "https://www.simplilearn.com/tutorials/machine-learning-tutorial/k-means-clustering-algorithm",
        "read_more_link": "",
        "language": "en",
        "title": "K-means Clustering Algorithm: Applications, Types, and Demos [Updated]",
        "top_image": "https://www.simplilearn.com/logo.png",
        "meta_img": "https://www.simplilearn.com/logo.png",
        "images": [
            "https://www.simplilearn.com/ice9/new_logo.svgz",
            "https://www.simplilearn.com/ice9/free_resources_article_thumb/k_means_clustering.jpg",
            "https://www.simplilearn.com/ice9/assets/form_opacity.png",
            "https://www.simplilearn.com/ice9/assets/form_opacity.png",
            "https://www.simplilearn.com/ice9/assets/form_opacity.png",
            "https://www.simplilearn.com/ice9/assets/form_opacity.png",
            "https://www.simplilearn.com/ice9/assets/form_opacity.png",
            "https://www.simplilearn.com/ice9/assets/form_opacity.png",
            "https://www.simplilearn.com/ice9/assets/form_opacity.png",
            "https://www.simplilearn.com/ice9/assets/form_opacity.png",
            "https://www.simplilearn.com/ice9/assets/form_opacity.png",
            "https://www.simplilearn.com/ice9/assets/form_opacity.png",
            "https://www.simplilearn.com/ice9/assets/form_opacity.png",
            "https://www.simplilearn.com/ice9/assets/form_opacity.png",
            "https://www.simplilearn.com/ice9/assets/form_opacity.png",
            "https://www.simplilearn.com/ice9/assets/form_opacity.png",
            "https://www.simplilearn.com/ice9/assets/form_opacity.png",
            "https://www.simplilearn.com/ice9/assets/form_opacity.png",
            "https://www.simplilearn.com/ice9/assets/form_opacity.png",
            "https://www.simplilearn.com/ice9/assets/form_opacity.png",
            "https://www.simplilearn.com/ice9/assets/form_opacity.png",
            "https://www.simplilearn.com/ice9/assets/form_opacity.png",
            "https://www.simplilearn.com/ice9/assets/form_opacity.png",
            "https://www.simplilearn.com/ice9/assets/form_opacity.png",
            "https://www.simplilearn.com/ice9/assets/form_opacity.png",
            "https://www.simplilearn.com/ice9/assets/form_opacity.png",
            "https://www.simplilearn.com/ice9/assets/form_opacity.png",
            "https://www.simplilearn.com/ice9/assets/form_opacity.png",
            "https://www.simplilearn.com/ice9/assets/form_opacity.png",
            "https://www.simplilearn.com/ice9/assets/form_opacity.png",
            "https://www.simplilearn.com/ice9/assets/form_opacity.png",
            "https://www.simplilearn.com/ice9/assets/form_opacity.png",
            "https://www.simplilearn.com/ice9/assets/form_opacity.png",
            "https://www.simplilearn.com/ice9/assets/form_opacity.png",
            "https://www.simplilearn.com/ice9/assets/form_opacity.png",
            "https://www.simplilearn.com/ice9/assets/form_opacity.png",
            "https://www.simplilearn.com/ice9/banners/Data-Scientist.svgz",
            "https://www.simplilearn.com/ice9/assets/form_opacity.png",
            "https://www.simplilearn.com/ice9/assets/form_opacity.png",
            "https://www.simplilearn.com/ice9/assets/form_opacity.png",
            "https://www.simplilearn.com/ice9/assets/form_opacity.png",
            "https://www.simplilearn.com/ice9/assets/form_opacity.png",
            "https://www.simplilearn.com/ice9/assets/form_opacity.png"
        ],
        "movies": [
            "https://www.youtube.com/watch?v=watch?v=Xvwt7y2jf5E",
            "https://www.youtube.com/watch?v=Xvwt7y2jf5E"
        ],
        "keywords": [],
        "meta_keywords": [
            "what is k means clustering",
            "k means clustering",
            "k means clustering algorithm",
            "k means algorithm",
            "k means clustering example",
            "k means applications",
            "types of k means",
            "k means demos",
            "clustering in machine learning",
            "k means clustering in machine learning",
            "k means algorithm in machine learning"
        ],
        "tags": null,
        "authors": [
            "Mayank Banoula"
        ],
        "publish_date": "2020-04-22T22:45:48+05:30",
        "summary": "",
        "meta_description": "K-Means clustering is an unsupervised learning algorithm. Learn to understand the types of clustering, its applications, how does it work and demo. Read on to know more!",
        "meta_lang": "en",
        "meta_favicon": "https://www.simplilearn.com/static/frontend/images/favicon.png",
        "meta_site_name": "Simplilearn.com",
        "canonical_link": "https://www.simplilearn.com/tutorials/machine-learning-tutorial/k-means-clustering-algorithm",
        "text": "Every Machine Learning engineer wants to achieve accurate predictions with their algorithms. Such learning algorithms are generally broken down into two types - supervised and unsupervised. K-Means clustering is one of the unsupervised algorithms where the available input data does not have a labeled response.\n\nTypes of Clustering\n\nClustering is a type of unsupervised learning wherein data points are grouped into different sets based on their degree of similarity.\n\nThe various types of clustering are:\n\nHierarchical clustering\n\nPartitioning clusteringÂ\n\nHierarchical clustering is further subdivided into:\n\nAgglomerative clustering\n\nDivisive clustering\n\nPartitioning clustering is further subdivided into:\n\nK-Means clusteringÂ\n\nFuzzy C-Means clusteringÂ\n\nHierarchical Clustering\n\nHierarchical clustering uses a tree-like structure, like so:\n\nIn agglomerative clustering, there is a bottom-up approach. We begin with each element as a separate cluster and merge them into successively more massive clusters, as shown below:\n\nDivisive clustering is a top-down approach. We begin with the whole set and proceed to divide it into successively smaller clusters, as you can see below:\n\nPartitioning ClusteringÂ\n\nPartitioning clustering is split into two subtypes - K-Means clustering and Fuzzy C-Means.\n\nIn k-means clustering, the objects are divided into several clusters mentioned by the number âK.â So if we say K = 2, the objects are divided into two clusters, c1 and c2, as shown:\n\nHere, the features or characteristics are compared, and all objects having similar characteristics are clustered together.Â\n\nFuzzy c-means is very similar to k-means in the sense that it clusters objects that have similar characteristics together. In k-means clustering, a single object cannot belong to two different clusters. But in c-means, objects can belong to more than one cluster, as shown.Â\n\nWhat is Meant by the K-Means Clustering Algorithm?\n\nK-Means clustering is an unsupervised learning algorithm. There is no labeled data for this clustering, unlike in supervised learning. K-Means performs the division of objects into clusters that share similarities and are dissimilar to the objects belonging to another cluster.Â\n\nThe term âKâ is a number. You need to tell the system how many clusters you need to create. For example, K = 2 refers to two clusters. There is a way of finding out what is the best or optimum value of K for a given data.Â\n\nFor a better understanding of k-means, let's take an example from cricket. Imagine you received data on a lot of cricket players from all over the world, which gives information on the runs scored by the player and the wickets taken by them in the last ten matches. Based on this information, we need to group the data into two clusters, namely batsman and bowlers.Â\n\nLet's take a look at the steps to create these clusters.\n\nSolution:\n\nAssign data points\n\nHere, we have our data set plotted on âxâ and âyâ coordinates. The information on the y-axis is about the runs scored, and on the x-axis about the wickets taken by the players.Â\n\nIf we plot the data, this is how it would look:\n\nPerform Clustering\n\nWe need to create the clusters, as shown below:\n\nConsidering the same data set, let us solve the problem using K-Means clustering (taking K = 2).\n\nThe first step in k-means clustering is the allocation of two centroids randomly (as K=2). Two points are assigned as centroids. Note that the points can be anywhere, as they are random points. They are called centroids, but initially, they are not the central point of a given data set.\n\nThe next step is to determine the distance between each of the randomly assigned centroids' data points. For every point, the distance is measured from both the centroids, and whichever distance is less, that point is assigned to that centroid. You can see the data points attached to the centroids and represented here in blue and yellow.\n\nThe next step is to determine the actual centroid for these two clusters. The original randomly allocated centroid is to be repositioned to the actual centroid of the clusters.\n\nThis process of calculating the distance and repositioning the centroid continues until we obtain our final cluster. Then the centroid repositioning stops.\n\nAs seen above, the centroid doesn't need anymore repositioning, and it means the algorithm has converged, and we have the two clusters with a centroid.Â\n\nAdvantages of k-means\n\nSimple and easy to implement: The k-means algorithm is easy to understand and implement, making it a popular choice for clustering tasks.\n\nFast and efficient: K-means is computationally efficient and can handle large datasets with high dimensionality.\n\nScalability: K-means can handle large datasets with a large number of data points and can be easily scaled to handle even larger datasets.\n\nFlexibility: K-means can be easily adapted to different applications and can be used with different distance metrics and initialization methods.\n\nDisadvantages of K-Means:\n\nSensitivity to initial centroids: K-means is sensitive to the initial selection of centroids and can converge to a suboptimal solution.\n\nRequires specifying the number of clusters: The number of clusters k needs to be specified before running the algorithm, which can be challenging in some applications.\n\nSensitive to outliers: K-means is sensitive to outliers, which can have a significant impact on the resulting clusters.\n\nApplications of K-Means Clustering\n\nK-Means clustering is used in a variety of examples or business cases in real life, like:\n\nAcademic performanceÂ\n\nDiagnostic systemsÂ\n\nSearch enginesÂ\n\nWireless sensor networks\n\nAcademic Performance\n\nBased on the scores, students are categorized into grades like A, B, or C.Â\n\nDiagnostic systems\n\nThe medical profession uses k-means in creating smarter medical decision support systems, especially in the treatment of liver ailments.\n\nSearch engines\n\nClustering forms a backbone of search engines. When a search is performed, the search results need to be grouped, and the search engines very often use clustering to do this.Â\n\nWireless sensor networks\n\nThe clustering algorithm plays the role of finding the cluster heads, which collect all the data in its respective cluster.\n\nDistance MeasureÂ\n\nDistance measure determines the similarity between two elements and influences the shape of clusters.\n\nK-Means clustering supports various kinds of distance measures, such as:Â\n\nEuclidean distance measure\n\nManhattan distance measureÂ\n\nA squared euclidean distance measure\n\nCosine distance measureÂ\n\nEuclidean Distance MeasureÂ\n\nThe most common case is determining the distance between two points. If we have a point P and point Q, the euclidean distance is an ordinary straight line. It is the distance between the two points in Euclidean space.Â\n\nThe formula for distance between two points is shown below:\n\nSquared Euclidean Distance Measure\n\nThis is identical to the Euclidean distance measurement but does not take the square root at the end. The formula is shown below:\n\nManhattan Distance Measure\n\nThe Manhattan distance is the simple sum of the horizontal and vertical components or the distance between two points measured along axes at right angles.\n\nNote that we are taking the absolute value so that the negative values don't come into play.Â\n\nThe formula is shown below:\n\nCosine Distance Measure\n\nIn this case, we take the angle between the two vectors formed by joining the origin point. The formula is shown below:\n\nK-means on Geyser's Eruptions Segmentation\n\nK-means can be used to segment the Geyser's Eruptions dataset, which records the duration and waiting time between eruptions of the Old Faithful geyser in Yellowstone National Park. The algorithm can be used to cluster the eruptions based on their duration and waiting time and identify different patterns of eruptions.\n\nK-means on Image Compression\n\nK-means can also be used for image compression, where it can be used to reduce the number of colors in an image while maintaining its visual quality. The algorithm can be used to cluster the colors in the image and replace the pixels with the centroid color of each cluster, resulting in a compressed image.\n\nEvaluation Methods\n\nEvaluation methods are used to measure the performance of clustering algorithms. Common evaluation methods include:\n\nSum of Squared Errors (SSE): This measures the sum of the squared distances between each data point and its assigned centroid.\n\nSilhouette Coefficient: This measures the similarity of a data point to its own cluster compared to other clusters. A high silhouette coefficient indicates that a data point is well-matched to its own cluster and poorly matched to neighboring clusters.\n\nSilhouette Analysis\n\nSilhouette analysis is a graphical technique used to evaluate the quality of the clusters generated by a clustering algorithm. It involves calculating the silhouette coefficient for each data point and plotting them in a histogram. The width of the histogram indicates the quality of the clustering. A wide histogram indicates that the clusters are well-separated and distinct, while a narrow histogram indicates that the clusters are poorly separated and may overlap.\n\nHow Does K-Means Clustering Work?\n\nThe flowchart below shows how k-means clustering works:\n\nThe goal of the K-Means algorithm is to find clusters in the given input data. There are a couple of ways to accomplish this. We can use the trial and error method by specifying the value of K (e.g., 3,4, 5). As we progress, we keep changing the value until we get the best clusters.Â\n\nAnother method is to use the Elbow technique to determine the value of K. Once we get the K's value, the system will assign that many centroids randomly and measure the distance of each of the data points from these centroids. Accordingly, it assigns those points to the corresponding centroid from which the distance is minimum. So each data point will be assigned to the centroid, which is closest to it. Thereby we have a K number of initial clusters.\n\nFor the newly formed clusters, it calculates the new centroid position. The position of the centroid moves compared to the randomly allocated one.\n\nOnce again, the distance of each point is measured from this new centroid point. If required, the data points are relocated to the new centroids, and the mean position or the new centroid is calculated once again.Â\n\nIf the centroid moves, the iteration continues indicating no convergence. But once the centroid stops moving (which means that the clustering process has converged), it will reflect the result.\n\nLet's use a visualization example to understand this better.Â\n\nWe have a data set for a grocery shop, and we want to find out how many clusters this has to be spread across. To find the optimum number of clusters, we break it down into the following steps:\n\nStep 1:\n\nThe Elbow method is the best way to find the number of clusters. The elbow method constitutes runningÂ K-Means clustering on the dataset.\n\nNext, we use within-sum-of-squares as a measure to find the optimum number of clusters that can be formed for a given data set. Within the sum of squares (WSS) is defined as the sum of the squared distance between each member of the cluster and its centroid.\n\nThe WSS is measured for each value of K. The value of K, which has the least amount of WSS, is taken as the optimum value.Â\n\nNow, we draw a curve between WSS and the number of clusters.Â\n\nHere, WSS is on the y-axis and number of clusters on the x-axis.\n\nYou can see that there is a very gradual change in the value of WSS as the K value increases from 2.Â\n\nSo, you can take the elbow point value as the optimal value of K. It should be either two, three, or at most four. But, beyond that, increasing the number of clusters does not dramatically change the value in WSS, it gets stabilized.Â\n\nStep 2:\n\nLet's assume that these are our delivery points:\n\nWe can randomly initialize two points called the cluster centroids.\n\nHere, C1 and C2 are the centroids assigned randomly.Â\n\nStep 3:\n\nNow the distance of each location from the centroid is measured, and each data point is assigned to the centroid, which is closest to it.\n\nThis is how the initial grouping is done:\n\nStep 4:Â\n\nCompute the actual centroid of data points for the first group.\n\nStep 5:\n\nReposition the random centroid to the actual centroid.\n\nStep 6:Â\n\nCompute the actual centroid of data points for the second group.\n\nStep 7:\n\nReposition the random centroid to the actual centroid.\n\nStep 8:Â\n\nOnce the cluster becomes static, the k-means algorithm is said to be converged.Â\n\nThe final cluster with centroids c1 and c2 is as shown below:\n\nK-Means Clustering AlgorithmÂ\n\nLet's say we have x1, x2, x3â¦â¦â¦ x(n) as our inputs, and we want to split this into K clusters.Â\n\nThe steps to form clusters are:\n\nStep 1: Choose K random points as cluster centers called centroids.Â\n\nStep 2: Assign each x(i) to the closest cluster by implementing euclidean distance (i.e., calculating its distance to each centroid)\n\nStep 3: Identify new centroids by taking the average of the assigned points.\n\nStep 4: Keep repeating step 2 and step 3 until convergence is achieved\n\nLet's take a detailed look at it at each of these steps.\n\nStep 1:\n\nWe randomly pick K (centroids). We name them c1,c2,..... ck, and we can say thatÂ\n\nWhere C is the set of all centroids.\n\nStep 2:\n\nWe assign each data point to its nearest center, which is accomplished by calculating the euclidean distance.\n\nWhere dist() is the Euclidean distance.\n\nHere, we calculate each x value's distance from each c value, i.e. the distance between x1-c1, x1-c2, x1-c3, and so on. Then we find which is the lowest value and assign x1 to that particular centroid.\n\nSimilarly, we find the minimum distance for x2, x3, etc.Â\n\nStep 3:Â\n\nWe identify the actual centroid by taking the average of all the points assigned to that cluster.Â\n\nWhere Si is the set of all points assigned to the ith cluster.Â Â Â Â Â\n\nIt means the original point, which we thought was the centroid, will shift to the new position, which is the actual centroid for each of these groups.Â\n\nStep 4:\n\nKeep repeating step 2 and step 3 until convergence is achieved.\n\nHow to Choose the Value of \"K number of clusters\" in K-Means Clustering?\n\nAlthough there are many choices available for choosing the optimal number of clusters, the Elbow Method is one of the most popular and appropriate methods. The Elbow Method uses the idea of WCSS value, which is short for for Within Cluster Sum of Squares. WCSS defines the total number of variations within a cluster. This is the formula used to calculate the value of WCSS (for three clusters) provided courtesy of Javatpoint:\n\nWCSS= âPi in Cluster1 distance(Pi C1)2 +âPi in Cluster2distance(Pi C2)2+âPi in CLuster3 distance(Pi C3)2\n\nPython Implementation of the K-Means Clustering Algorithm\n\nHereâs how to use Python to implement the K-Means Clustering Algorithm. These are the steps you need to take:\n\nData pre-processing\n\nFinding the optimal number of clusters using the elbow method\n\nTraining the K-Means algorithm on the training data set\n\nVisualizing the clusters\n\n1. Data Pre-Processing. Import the libraries, datasets, and extract the independent variables.\n\n# importing librariesÂ Â Â Â\n\nimport numpy as nmÂ Â Â Â\n\nimport matplotlib.pyplot as mtpÂ Â Â Â\n\nimport pandas as pdÂ Â Â Â\n\n# Importing the datasetÂ Â\n\ndataset = pd.read_csv('Mall_Customers_data.csv')Â Â\n\nx = dataset.iloc[:, [3, 4]].valuesÂ\n\n2. Find the optimal number of clusters using the elbow method. Hereâs the code you use:\n\n#finding optimal number of clusters using the elbow methodÂ Â\n\nfrom sklearn.cluster import KMeansÂ Â\n\nwcss_list= []Â #Initializing the list for the values of WCSSÂ Â\n\n#Using for loop for iterations from 1 to 10.Â Â\n\nfor i in range(1, 11):Â Â\n\nÂ Â Â Â kmeans = KMeans(n_clusters=i, init='k-means++', random_state= 42)Â Â\n\nÂ Â Â Â kmeans.fit(x)Â Â\n\nÂ Â Â Â wcss_list.append(kmeans.inertia_)Â Â\n\nmtp.plot(range(1, 11), wcss_list)Â Â\n\nmtp.title('The Elobw Method Graph')Â Â\n\nmtp.xlabel('Number of clusters(k)') Â\n\nmtp.ylabel('wcss_list')Â Â\n\nmtp.show()Â\n\n3. Train the K-means algorithm on the training dataset. Use the same two lines of code used in the previous section. However, instead of using i, use 5, because there are 5 clusters that need to be formed. Hereâs the code:\n\n#training the K-means model on a datasetÂ Â\n\nkmeans = KMeans(n_clusters=5, init='k-means++', random_state= 42)Â Â\n\ny_predict= kmeans.fit_predict(x)Â\n\n4. Visualize the Clusters. Since this model has five clusters, we need to visualize each one.\n\n#visulaizing the clustersÂ Â\n\nmtp.scatter(x[y_predict == 0, 0], x[y_predict == 0, 1], s = 100, c = 'blue', label = 'Cluster 1') #for first clusterÂ Â\n\nmtp.scatter(x[y_predict == 1, 0], x[y_predict == 1, 1], s = 100, c = 'green', label = 'Cluster 2') #for second clusterÂ Â\n\nmtp.scatter(x[y_predict== 2, 0], x[y_predict == 2, 1], s = 100, c = 'red', label = 'Cluster 3') #for third clusterÂ Â\n\nmtp.scatter(x[y_predict == 3, 0], x[y_predict == 3, 1], s = 100, c = 'cyan', label = 'Cluster 4') #for fourth clusterÂ Â\n\nmtp.scatter(x[y_predict == 4, 0], x[y_predict == 4, 1], s = 100, c = 'magenta', label = 'Cluster 5') #for fifth clusterÂ Â\n\nmtp.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = 'yellow', label = 'Centroid')Â Â Â\n\nmtp.title('Clusters of customers')Â Â\n\nmtp.xlabel('Annual Income (k$)')Â Â\n\nmtp.ylabel('Spending Score (1-100)')Â Â\n\nmtp.legend()Â Â\n\nmtp.show()Â Â\n\nCoding provided by Javatpoint.\n\nDemo: K-Means Clustering\n\nProblem Statement - Walmart wants to open a chain of stores across the state of Florida, and it wants to find the optimal store locations to maximize revenue.\n\nThe issue here is if they open too many stores close to each other, they will not make a profit. But, if the stores are too far apart, they do not have enough sales coverage.Â\n\nSolution - An organization like Walmart is an e-commerce giant. They already have the addresses of their customers in their database. So they can use this information and perform K-Means Clustering to find the optimal location.\n\nConclusionÂ\n\nConsidered as the Job of the future, Machine Learning engineers are in-demand as well as highly paid. A report by Marketwatch predicts a machine learning growth rate of over 45% for the period between 2017 and 2025. So, why restrict your learning to merely K-means clustering algorithms? Enroll in Simplilearn's Machine Learning Course and expand your knowledge in the broader concepts of Machine Learning. Get certified and become a part of the Artificial Intelligence talent that companies constantly look forward to."
    }
}
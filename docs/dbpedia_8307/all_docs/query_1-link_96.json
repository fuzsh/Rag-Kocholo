{
    "id": "dbpedia_8307_1",
    "rank": 96,
    "data": {
        "url": "https://docs.getdbt.com/guides/dbt-python-snowpark",
        "read_more_link": "",
        "language": "en",
        "title": "Leverage dbt Cloud to generate analytics and ML-ready pipelines with SQL and Python with Snowflake",
        "top_image": "https://docs.getdbt.com/img/avatar.png",
        "meta_img": "https://docs.getdbt.com/img/avatar.png",
        "images": [
            "https://docs.getdbt.com/img/dbt-logo.svg",
            "https://docs.getdbt.com/img/dbt-logo-light.svg",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/2-snowflake-configuration/1-snowflake-trial-AWS-setup.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/2-snowflake-configuration/2-new-snowflake-account.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/2-snowflake-configuration/3-accept-anaconda-terms.jpeg?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/2-snowflake-configuration/4-enable-anaconda.jpeg?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/3-connect-to-data-source/1-rename-worksheet-and-select-warehouse.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/3-connect-to-data-source/2-load-data-from-s3.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/3-connect-to-data-source/3-create-new-worksheet-to-query-data.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/3-connect-to-data-source/4-query-circuits-data.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/4-configure-dbt/1-open-partner-connect.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/4-configure-dbt/2-partner-connect-optional-grant.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/4-configure-dbt/3-connect-to-dbt.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/4-configure-dbt/4-dbt-cloud-sign-up.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/5-development-schema-name/1-settings-gear-icon.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/5-development-schema-name/2-credentials-edit-schema-name.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/5-development-schema-name/3-save-new-schema-name.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/5-development-schema-name/4-initialize-dbt-project.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/5-development-schema-name/5-first-commit-and-push.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/5-development-schema-name/6-initalize-project.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/5-development-schema-name/7-IDE-overview.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/5-development-schema-name/8-dbt-run-example-models.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/5-development-schema-name/9-second-model-details.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/5-development-schema-name/10-confirm-example-models-built-in-snowflake.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/7-folder-structure/1-create-folder.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/7-folder-structure/2-file-path.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/7-folder-structure/3-tree-of-new-folders.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/8-sources-and-staging/1-staging-folder.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/8-sources-and-staging/2-delete-example.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/8-sources-and-staging/3-successful-run-in-snowflake.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/8-sources-and-staging/4-confirm-models.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/9-sql-transformations/1-dag.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/10-python-transformations/1-python-model-details-output.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/10-python-transformations/2-fastest-pit-stops-preview.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/10-python-transformations/3-lap-times-trends-preview.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/11-machine-learning-prep/1-completed-ml-data-prep.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/12-machine-learning-training-prediction/1-preview-train-test-position.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/12-machine-learning-training-prediction/2-list-snowflake-stage.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/12-machine-learning-training-prediction/3-view-snowflake-query-history.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/13-testing/1-generic-testing-file-tree.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/13-testing/2-macro-testing.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/13-testing/3-gte-macro-applied-to-pit-stops.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/13-testing/4-custom-singular-test.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/13-testing/5-running-tests-on-python-models.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/13-testing/6-testing-output-details.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/14-documentation/1-docs-icon.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/14-documentation/2-view-docblock-description.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/14-documentation/3-mini-lineage-docs.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/14-documentation/4-full-dag-docs.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/15-deployment/1-merge-to-main-branch.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/15-deployment/2-ui-select-environments.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/15-deployment/3-update-deployment-credentials-production.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/15-deployment/4-run-production-job.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/15-deployment/5-job-details.png?v=2",
            "https://docs.getdbt.com/img/guides/dbt-ecosystem/dbt-python-snowpark/15-deployment/6-all-models-generated.png?v=2"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2024-08-13T16:47:24+00:00",
        "summary": "",
        "meta_description": "Leverage dbt Cloud to generate analytics and ML-ready pipelines with SQL and Python with Snowflake",
        "meta_lang": "en",
        "meta_favicon": "/img/favicon.png",
        "meta_site_name": "",
        "canonical_link": "https://docs.getdbt.com/guides/dbt-python-snowpark",
        "text": "Snowflake\n\nIntermediate\n\nMenu\n\nThe focus of this workshop will be to demonstrate how we can use both SQL and python together in the same workflow to run both analytics and machine learning models on dbt Cloud.\n\nAll code in today‚Äôs workshop can be found on GitHub.\n\nA Snowflake account with ACCOUNTADMIN access\n\nA dbt Cloud account\n\nHow to build scalable data transformation pipelines using dbt, and Snowflake using SQL and Python\n\nHow to leverage copying data into Snowflake from a public S3 bucket\n\nBasic to intermediate SQL and python.\n\nBasic understanding of dbt fundamentals. We recommend the dbt Fundamentals course if you're interested.\n\nHigh level machine learning process (encoding, training, testing)\n\nSimple ML algorithms ‚Äî we will use logistic regression to keep the focus on the workflow, not algorithms!\n\nA set of data analytics and prediction pipelines using Formula 1 data leveraging dbt and Snowflake, making use of best practices like data quality tests and code promotion between environments\n\nWe will create insights for:\n\nFinding the lap time average and rolling average through the years (is it generally trending up or down)?\n\nWhich constructor has the fastest pit stops in 2021?\n\nPredicting the position of each driver given using a decade of data (2010 - 2020)\n\nAs inputs, we are going to leverage Formula 1 datasets hosted on a dbt Labs public S3 bucket. We will create a Snowflake Stage for our CSV files then use Snowflake‚Äôs COPY INTO function to copy the data in from our CSV files into tables. The Formula 1 is available on Kaggle. The data is originally compiled from the Ergast Developer API.\n\nOverall we are going to set up the environments, build scalable pipelines in dbt, establish data tests, and promote code to production.\n\nLog in to your trial Snowflake account. You can sign up for a Snowflake Trial Account using this form if you don‚Äôt have one.\n\nEnsure that your account is set up using AWS in the US East (N. Virginia). We will be copying the data from a public AWS S3 bucket hosted by dbt Labs in the us-east-1 region. By ensuring our Snowflake environment setup matches our bucket region, we avoid any multi-region data copy and retrieval latency issues.\n\nSnowflake trial\n\nAfter creating your account and verifying it from your sign-up email, Snowflake will direct you back to the UI called Snowsight.\n\nWhen Snowsight first opens, your window should look like the following, with you logged in as the ACCOUNTADMIN with demo worksheets open:\n\nSnowflake trial demo worksheets\n\nNavigate to Admin > Billing & Terms. Click Enable > Acknowledge & Continue to enable Anaconda Python Packages to run in Snowflake.\n\nAnaconda terms\n\nEnable Anaconda\n\nFinally, create a new Worksheet by selecting + Worksheet in the upper right corner.\n\nWe need to obtain our data source by copying our Formula 1 data into Snowflake tables from a public S3 bucket that dbt Labs hosts.\n\nWhen a new Snowflake account is created, there should be a preconfigured warehouse in your account named COMPUTE_WH.\n\nIf for any reason your account doesn‚Äôt have this warehouse, we can create a warehouse using the following script:\n\ncreateorreplace warehouse COMPUTE_WH with warehouse_size=XSMALL\n\nRename the worksheet to data setup script since we will be placing code in this worksheet to ingest the Formula 1 data. Make sure you are still logged in as the ACCOUNTADMIN and select the COMPUTE_WH warehouse.\n\nRename worksheet and select warehouse\n\nCopy the following code into the main body of the Snowflake worksheet. You can also find this setup script under the setup folder in the Git repository. The script is long since it's bring in all of the data we'll need today!\n\ncreateorreplacedatabase formula1;\n\nusedatabase formula1;\n\ncreateorreplaceschema raw;\n\nuseschema raw;\n\ncreateorreplacefile format csvformat\n\ntype= csv\n\nfield_delimiter =','\n\nfield_optionally_enclosed_by ='\"',\n\nskip_header=1;\n\ncreateorreplace stage formula1_stage\n\nfile_format = csvformat\n\nurl ='s3://formula1-dbt-cloud-python-demo/formula1-kaggle-data/';\n\ncreateorreplacetable formula1.raw.circuits (\n\nCIRCUITID NUMBER(38,0),\n\nCIRCUITREF VARCHAR(16777216),\n\nNAME VARCHAR(16777216),\n\nLOCATION VARCHAR(16777216),\n\nCOUNTRY VARCHAR(16777216),\n\nLAT FLOAT,\n\nLNG FLOAT,\n\nALT NUMBER(38,0),\n\nURL VARCHAR(16777216)\n\n);\n\ncopy into circuits\n\nfrom@formula1_stage/circuits.csv\n\non_error='continue';\n\ncreateorreplacetable formula1.raw.constructors (\n\nCONSTRUCTORID NUMBER(38,0),\n\nCONSTRUCTORREF VARCHAR(16777216),\n\nNAME VARCHAR(16777216),\n\nNATIONALITY VARCHAR(16777216),\n\nURL VARCHAR(16777216)\n\n);\n\ncopy into constructors\n\nfrom@formula1_stage/constructors.csv\n\non_error='continue';\n\ncreateorreplacetable formula1.raw.drivers (\n\nDRIVERID NUMBER(38,0),\n\nDRIVERREF VARCHAR(16777216),\n\nNUMBER VARCHAR(16777216),\n\nCODE VARCHAR(16777216),\n\nFORENAME VARCHAR(16777216),\n\nSURNAME VARCHAR(16777216),\n\nDOB DATE,\n\nNATIONALITY VARCHAR(16777216),\n\nURL VARCHAR(16777216)\n\n);\n\ncopy into drivers\n\nfrom@formula1_stage/drivers.csv\n\non_error='continue';\n\ncreateorreplacetable formula1.raw.lap_times (\n\nRACEID NUMBER(38,0),\n\nDRIVERID NUMBER(38,0),\n\nLAP NUMBER(38,0),\n\nPOSITION FLOAT,\n\nTIMEVARCHAR(16777216),\n\nMILLISECONDS NUMBER(38,0)\n\n);\n\ncopy into lap_times\n\nfrom@formula1_stage/lap_times.csv\n\non_error='continue';\n\ncreateorreplacetable formula1.raw.pit_stops (\n\nRACEID NUMBER(38,0),\n\nDRIVERID NUMBER(38,0),\n\nSTOP NUMBER(38,0),\n\nLAP NUMBER(38,0),\n\nTIMEVARCHAR(16777216),\n\nDURATION VARCHAR(16777216),\n\nMILLISECONDS NUMBER(38,0)\n\n);\n\ncopy into pit_stops\n\nfrom@formula1_stage/pit_stops.csv\n\non_error='continue';\n\ncreateorreplacetable formula1.raw.races (\n\nRACEID NUMBER(38,0),\n\nYEAR NUMBER(38,0),\n\nROUND NUMBER(38,0),\n\nCIRCUITID NUMBER(38,0),\n\nNAME VARCHAR(16777216),\n\nDATEDATE,\n\nTIMEVARCHAR(16777216),\n\nURL VARCHAR(16777216),\n\nFP1_DATE VARCHAR(16777216),\n\nFP1_TIME VARCHAR(16777216),\n\nFP2_DATE VARCHAR(16777216),\n\nFP2_TIME VARCHAR(16777216),\n\nFP3_DATE VARCHAR(16777216),\n\nFP3_TIME VARCHAR(16777216),\n\nQUALI_DATE VARCHAR(16777216),\n\nQUALI_TIME VARCHAR(16777216),\n\nSPRINT_DATE VARCHAR(16777216),\n\nSPRINT_TIME VARCHAR(16777216)\n\n);\n\ncopy into races\n\nfrom@formula1_stage/races.csv\n\non_error='continue';\n\ncreateorreplacetable formula1.raw.results (\n\nRESULTID NUMBER(38,0),\n\nRACEID NUMBER(38,0),\n\nDRIVERID NUMBER(38,0),\n\nCONSTRUCTORID NUMBER(38,0),\n\nNUMBER NUMBER(38,0),\n\nGRID NUMBER(38,0),\n\nPOSITION FLOAT,\n\nPOSITIONTEXT VARCHAR(16777216),\n\nPOSITIONORDER NUMBER(38,0),\n\nPOINTS NUMBER(38,0),\n\nLAPS NUMBER(38,0),\n\nTIMEVARCHAR(16777216),\n\nMILLISECONDS NUMBER(38,0),\n\nFASTESTLAP NUMBER(38,0),\n\nRANK NUMBER(38,0),\n\nFASTESTLAPTIME VARCHAR(16777216),\n\nFASTESTLAPSPEED FLOAT,\n\nSTATUSID NUMBER(38,0)\n\n);\n\ncopy into results\n\nfrom@formula1_stage/results.csv\n\non_error='continue';\n\ncreateorreplacetable formula1.raw.status(\n\nSTATUSID NUMBER(38,0),\n\nSTATUSVARCHAR(16777216)\n\n);\n\ncopy intostatus\n\nfrom@formula1_stage/status.csv\n\non_error='continue';\n\nEnsure all the commands are selected before running the query ‚Äî an easy way to do this is to use Ctrl-a to highlight all of the code in the worksheet. Select run (blue triangle icon). Notice how the dot next to your COMPUTE_WH turns from gray to green as you run the query. The status table is the final table of all 8 tables loaded in.\n\nLoad data from S3 bucket\n\nLet‚Äôs unpack that pretty long query we ran into component parts. We ran this query to load in our 8 Formula 1 tables from a public S3 bucket. To do this, we:\n\nCreated a new database called formula1 and a schema called raw to place our raw (untransformed) data into.\n\nDefined our file format for our CSV files. Importantly, here we use a parameter called field_optionally_enclosed_by = since the string columns in our Formula 1 csv files use quotes. Quotes are used around string values to avoid parsing issues where commas , and new lines /n in data values could cause data loading errors.\n\nCreated a stage to locate our data we are going to load in. Snowflake Stages are locations where data files are stored. Stages are used to both load and unload data to and from Snowflake locations. Here we are using an external stage, by referencing an S3 bucket.\n\nCreated our tables for our data to be copied into. These are empty tables with the column name and data type. Think of this as creating an empty container that the data will then fill into.\n\nUsed the copy into statement for each of our tables. We reference our staged location we created and upon loading errors continue to load in the rest of the data. You should not have data loading errors but if you do, those rows will be skipped and Snowflake will tell you which rows caused errors\n\nNow let's take a look at some of our cool Formula 1 data we just loaded up!\n\nCreate a new worksheet by selecting the + then New Worksheet.\n\nCreate new worksheet to query data\n\nNavigate to Database > Formula1 > RAW > Tables.\n\nQuery the data using the following code. There are only 76 rows in the circuits table, so we don‚Äôt need to worry about limiting the amount of data we query.\n\nselect*from formula1.raw.circuits\n\nRun the query. From here on out, we‚Äôll use the keyboard shortcuts Command-Enter or Control-Enter to run queries and won‚Äôt explicitly call out this step.\n\nReview the query results, you should see information about Formula 1 circuits, starting with Albert Park in Australia!\n\nFinally, ensure you have all 8 tables starting with CIRCUITS and ending with STATUS. Now we are ready to connect into dbt Cloud!\n\nQuery circuits data\n\nWe are going to be using Snowflake Partner Connect to set up a dbt Cloud account. Using this method will allow you to spin up a fully fledged dbt account with your Snowflake connection, managed repository, environments, and credentials already established.\n\nNavigate out of your worksheet back by selecting home.\n\nIn Snowsight, confirm that you are using the ACCOUNTADMIN role.\n\nNavigate to the Data Products > Partner Connect. Find dbt either by using the search bar or navigating the Data Integration. Select the dbt tile.\n\nOpen Partner Connect\n\nYou should now see a new window that says Connect to dbt. Select Optional Grant and add the FORMULA1 database. This will grant access for your new dbt user role to the FORMULA1 database.\n\nPartner Connect Optional Grant\n\nEnsure the FORMULA1 is present in your optional grant before clicking Connect. This will create a dedicated dbt user, database, warehouse, and role for your dbt Cloud trial.\n\nConnect to dbt\n\nWhen you see the Your partner account has been created window, click Activate.\n\nYou should be redirected to a dbt Cloud registration page. Fill out the form. Make sure to save the password somewhere for login in the future.\n\ndbt Cloud sign up\n\nSelect Complete Registration. You should now be redirected to your dbt Cloud account, complete with a connection to your Snowflake account, a deployment and a development environment, and a sample job.\n\nTo help you version control your dbt project, we have connected it to a managed repository, which means that dbt Labs will be hosting your repository for you. This will give you access to a Git workflow without you having to create and host the repository yourself. You will not need to know Git for this workshop; dbt Cloud will help guide you through the workflow. In the future, when you‚Äôre developing your own project, feel free to use your own repository. This will allow you to learn more about features like Slim CI builds after this workshop.\n\nFirst we are going to change the name of our default schema to where our dbt models will build. By default, the name is dbt_. We will change this to dbt_<YOUR_NAME> to create your own personal development schema. To do this, select Profile Settings from the gear icon in the upper right.\n\nSettings menu\n\nNavigate to the Credentials menu and select Partner Connect Trial, which will expand the credentials menu.\n\nCredentials edit schema name\n\nClick Edit and change the name of your schema from dbt_ to dbt_YOUR_NAME replacing YOUR_NAME with your initials and name (hwatson is used in the lab screenshots). Be sure to click Save for your changes!\n\nSave new schema name\n\nWe now have our own personal development schema, amazing! When we run our first dbt models they will build into this schema.\n\nLet‚Äôs open up dbt Cloud‚Äôs Integrated Development Environment (IDE) and familiarize ourselves. Choose Develop at the top of the UI.\n\nWhen the IDE is done loading, click Initialize dbt project. The initialization process creates a collection of files and folders necessary to run your dbt project.\n\nInitialize dbt project\n\nAfter the initialization is finished, you can view the files and folders in the file tree menu. As we move through the workshop we'll be sure to touch on a few key files and folders that we'll work with to build out our project.\n\nNext click Commit and push to commit the new files and folders from the initialize step. We always want our commit messages to be relevant to the work we're committing, so be sure to provide a message like initialize project and select Commit Changes.\n\nFirst commit and push\n\nInitialize project\n\nCommitting your work here will save it to the managed git repository that was created during the Partner Connect signup. This initial commit is the only commit that will be made directly to our main branch and from here on out we'll be doing all of our work on a development branch. This allows us to keep our development work separate from our production code.\n\nThere are a couple of key features to point out about the IDE before we get to work. It is a text editor, an SQL and Python runner, and a CLI with Git version control all baked into one package! This allows you to focus on editing your SQL and Python files, previewing the results with the SQL runner (it even runs Jinja!), and building models at the command line without having to move between different applications. The Git workflow in dbt Cloud allows both Git beginners and experts alike to be able to easily version control all of their work with a couple clicks.\n\nIDE overview\n\nLet's run our first dbt models! Two example models are included in your dbt project in the models/examples folder that we can use to illustrate how to run dbt at the command line. Type dbt run into the command line and click Enter on your keyboard. When the run bar expands you'll be able to see the results of the run, where you should see the run complete successfully.\n\ndbt run example models\n\nThe run results allow you to see the code that dbt compiles and sends to Snowflake for execution. To view the logs for this run, select one of the model tabs using the > icon and then Details. If you scroll down a bit you'll be able to see the compiled code and how dbt interacts with Snowflake. Given that this run took place in our development environment, the models were created in your development schema.\n\nDetails about the second model\n\nNow let's switch over to Snowflake to confirm that the objects were actually created. Click on the three dots ‚Ä¶ above your database objects and then Refresh. Expand the PC_DBT_DB database and you should see your development schema. Select the schema, then Tables and Views. Now you should be able to see MY_FIRST_DBT_MODEL as a table and MY_SECOND_DBT_MODEL as a view.\n\nConfirm example models are built in Snowflake\n\nIn this step, we‚Äôll need to create a development branch and set up project level configurations.\n\nTo get started with development for our project, we'll need to create a new Git branch for our work. Select create branch and name your development branch. We'll call our branch snowpark_python_workshop then click Submit.\n\nThe first piece of development we'll do on the project is to update the dbt_project.yml file. Every dbt project requires a dbt_project.yml file ‚Äî this is how dbt knows a directory is a dbt project. The dbt_project.yml file also contains important information that tells dbt how to operate on your project.\n\nSelect the dbt_project.yml file from the file tree to open it and replace all of the existing contents with the following code below. When you're done, save the file by clicking save. You can also use the Command-S or Control-S shortcut from here on out.\n\nname:'snowflake_dbt_python_formula1'\n\nversion:'1.3.0'\n\nrequire-dbt-version:'>=1.3.0'\n\nconfig-version:2\n\nprofile:'default'\n\nmodel-paths:[\"models\"]\n\nanalysis-paths:[\"analyses\"]\n\ntest-paths:[\"tests\"]\n\nseed-paths:[\"seeds\"]\n\nmacro-paths:[\"macros\"]\n\nsnapshot-paths:[\"snapshots\"]\n\ntarget-path:\"target\"\n\nclean-targets:\n\n-\"target\"\n\n-\"dbt_packages\"\n\nmodels:\n\nsnowflake_dbt_python_formula1:\n\nstaging:\n\n+docs:\n\nnode_color:\"CadetBlue\"\n\nmarts:\n\n+materialized: table\n\naggregates:\n\n+docs:\n\nnode_color:\"Maroon\"\n\n+tags:\"bi\"\n\ncore:\n\n+docs:\n\nnode_color:\"#800080\"\n\nintermediate:\n\n+docs:\n\nnode_color:\"MediumSlateBlue\"\n\nml:\n\nprep:\n\n+docs:\n\nnode_color:\"Indigo\"\n\ntrain_predict:\n\n+docs:\n\nnode_color:\"#36454f\"\n\nThe key configurations to point out in the file with relation to the work that we're going to do are in the models section.\n\nrequire-dbt-version ‚Äî Tells dbt which version of dbt to use for your project. We are requiring 1.3.0 and any newer version to run python models and node colors.\n\nmaterialized ‚Äî Tells dbt how to materialize models when compiling the code before it pushes it down to Snowflake. All models in the marts folder will be built as tables.\n\ntags ‚Äî Applies tags at a directory level to all models. All models in the aggregates folder will be tagged as bi (abbreviation for business intelligence).\n\ndocs ‚Äî Specifies the node_color either by the plain color name or a hex value.\n\nMaterializations are strategies for persisting dbt models in a warehouse, with tables and views being the most commonly utilized types. By default, all dbt models are materialized as views and other materialization types can be configured in the dbt_project.yml file or in a model itself. It‚Äôs very important to note Python models can only be materialized as tables or incremental models. Since all our Python models exist under marts, the following portion of our dbt_project.yml ensures no errors will occur when we run our Python models. Starting with dbt version 1.4, Python files will automatically get materialized as tables even if not explicitly specified.\n\nmarts:\n\n+materialized: table\n\ndbt Labs has developed a project structure guide that contains a number of recommendations for how to build the folder structure for your project. Do check out that guide if you want to learn more. Right now we are going to create some folders to organize our files:\n\nSources ‚Äî This is our Formula 1 dataset and it will be defined in a source YAML file.\n\nStaging models ‚Äî These models have a 1:1 with their source table.\n\nIntermediate ‚Äî This is where we will be joining some Formula staging models.\n\nMarts models ‚Äî Here is where we perform our major transformations. It contains these subfolders:\n\naggregates\n\ncore\n\nml\n\nIn your file tree, use your cursor and hover over the models subdirectory, click the three dots ‚Ä¶ that appear to the right of the folder name, then select Create Folder. We're going to add two new folders to the file path, staging and formula1 (in that order) by typing staging/formula1 into the file path.\n\nCreate folder\n\nSet file path\n\nIf you click into your models directory now, you should see the new staging folder nested within models and the formula1 folder nested within staging.\n\nCreate two additional folders the same as the last step. Within the models subdirectory, create new directories marts/core.\n\nWe will need to create a few more folders and subfolders using the UI. After you create all the necessary folders, your folder tree should look like this when it's all done:\n\nFile tree of new folders\n\nRemember you can always reference the entire project in GitHub to view the complete folder and file strucutre.\n\nIn this section, we are going to create our source and staging models.\n\nSources allow us to create a dependency between our source database object and our staging models which will help us when we look at data lineage later. Also, if your source changes database or schema, you only have to update it in your f1_sources.yml file rather than updating all of the models it might be used in.\n\nStaging models are the base of our project, where we bring all the individual components we're going to use to build our more complex and useful models into the project.\n\nSince we want to focus on dbt and Python in this workshop, check out our sources and staging docs if you want to learn more (or take our dbt Fundamentals course which covers all of our core functionality).\n\nWe're going to be using each of our 8 Formula 1 tables from our formula1 database under the raw schema for our transformations and we want to create those tables as sources in our project.\n\nCreate a new file called f1_sources.yml with the following file path: models/staging/formula1/f1_sources.yml.\n\nThen, paste the following code into the file before saving it:\n\nversion:2\n\nsources:\n\n-name: formula1\n\ndescription: formula 1 datasets with normalized tables\n\ndatabase: formula1\n\nschema: raw\n\ntables:\n\n-name: circuits\n\ndescription: One record per circuit, which is the specific race course.\n\ncolumns:\n\n-name: circuitid\n\ntests:\n\n- unique\n\n- not_null\n\n-name: constructors\n\ndescription: One record per constructor. Constructors are the teams that build their formula 1 cars.\n\ncolumns:\n\n-name: constructorid\n\ntests:\n\n- unique\n\n- not_null\n\n-name: drivers\n\ndescription: One record per driver. This table gives details about the driver.\n\ncolumns:\n\n-name: driverid\n\ntests:\n\n- unique\n\n- not_null\n\n-name: lap_times\n\ndescription: One row per lap in each race. Lap times started being recorded in this dataset in 1984 and joined through driver_id.\n\n-name: pit_stops\n\ndescription: One row per pit stop. Pit stops do not have their own id column, the combination of the race_id and driver_id identify the pit stop.\n\ncolumns:\n\n-name: stop\n\ntests:\n\n-accepted_values:\n\nvalues:[1,2,3,4,5,6,7,8]\n\nquote:false\n\n-name: races\n\ndescription: One race per row. Importantly this table contains the race year to understand trends.\n\ncolumns:\n\n-name: raceid\n\ntests:\n\n- unique\n\n- not_null\n\n-name: results\n\ncolumns:\n\n-name: resultid\n\ntests:\n\n- unique\n\n- not_null\n\ndescription: One row per result. The main table that we join out for grid and position variables.\n\n-name: status\n\ndescription: One status per row. The status contextualizes whether the race was finished or what issues arose e.g. collisions, engine, etc.\n\ncolumns:\n\n-name: statusid\n\ntests:\n\n- unique\n\n- not_null\n\nThe next step is to set up the staging models for each of the 8 source tables. Given the one-to-one relationship between staging models and their corresponding source tables, we'll build 8 staging models here. We know it‚Äôs a lot and in the future, we will seek to update the workshop to make this step less repetitive and more efficient. This step is also a good representation of the real world of data, where you have multiple hierarchical tables that you will need to join together!\n\nLet's go in alphabetical order to easily keep track of all our staging models! Create a new file called stg_f1_circuits.sql with this file path models/staging/formula1/stg_f1_circuits.sql. Then, paste the following code into the file before saving it:\n\nwith\n\nsource as(\n\nselect*from {{ source('formula1','circuits') }}\n\n),\n\nrenamed as(\n\nselect\n\ncircuitid as circuit_id,\n\ncircuitref as circuit_ref,\n\nname as circuit_name,\n\nlocation,\n\ncountry,\n\nlat as latitude,\n\nlng as longitude,\n\nalt as altitude\n\nfrom source\n\n)\n\nselect*from renamed\n\nAll we're doing here is pulling the source data into the model using the source function, renaming some columns, and omitting the column url with a commented note since we don‚Äôt need it for our analysis.\n\nCreate stg_f1_constructors.sql with this file path models/staging/formula1/stg_f1_constructors.sql. Paste the following code into it before saving the file:\n\nwith\n\nsource as(\n\nselect*from {{ source('formula1','constructors') }}\n\n),\n\nrenamed as(\n\nselect\n\nconstructorid as constructor_id,\n\nconstructorref as constructor_ref,\n\nname as constructor_name,\n\nnationality as constructor_nationality\n\nfrom source\n\n)\n\nselect*from renamed\n\nWe have 6 other stages models to create. We can do this by creating new files, then copy and paste the code into our staging folder.\n\nCreate stg_f1_drivers.sql with this file path models/staging/formula1/stg_f1_drivers.sql:\n\nwith\n\nsource as(\n\nselect*from {{ source('formula1','drivers') }}\n\n),\n\nrenamed as(\n\nselect\n\ndriverid as driver_id,\n\ndriverref as driver_ref,\n\nnumber as driver_number,\n\ncode as driver_code,\n\nforename,\n\nsurname,\n\ndob as date_of_birth,\n\nnationality as driver_nationality\n\nfrom source\n\n)\n\nselect*from renamed\n\nCreate stg_f1_lap_times.sql with this file path models/staging/formula1/stg_f1_lap_times.sql:\n\nwith\n\nsource as(\n\nselect*from {{ source('formula1','lap_times') }}\n\n),\n\nrenamed as(\n\nselect\n\nraceid as race_id,\n\ndriverid as driver_id,\n\nlap,\n\nposition,\n\ntimeas lap_time_formatted,\n\nmilliseconds as lap_time_milliseconds\n\nfrom source\n\n)\n\nselect*from renamed\n\nCreate stg_f1_pit_stops.sql with this file path models/staging/formula1/stg_f1_pit_stops.sql:\n\nwith\n\nsource as(\n\nselect*from {{ source('formula1','pit_stops') }}\n\n),\n\nrenamed as(\n\nselect\n\nraceid as race_id,\n\ndriverid as driver_id,\n\nstop as stop_number,\n\nlap,\n\ntimeas lap_time_formatted,\n\nduration as pit_stop_duration_seconds,\n\nmilliseconds as pit_stop_milliseconds\n\nfrom source\n\n)\n\nselect*from renamed\n\norderby pit_stop_duration_seconds desc\n\nCreate stg_f1_races.sql with this file path models/staging/formula1/stg_f1_races.sql:\n\nwith\n\nsource as(\n\nselect*from {{ source('formula1','races') }}\n\n),\n\nrenamed as(\n\nselect\n\nraceid as race_id,\n\nyearas race_year,\n\nround as race_round,\n\ncircuitid as circuit_id,\n\nname as circuit_name,\n\ndateas race_date,\n\nto_time(time)as race_time,\n\nfp1_date as free_practice_1_date,\n\nfp1_time as free_practice_1_time,\n\nfp2_date as free_practice_2_date,\n\nfp2_time as free_practice_2_time,\n\nfp3_date as free_practice_3_date,\n\nfp3_time as free_practice_3_time,\n\nquali_date as qualifying_date,\n\nquali_time as qualifying_time,\n\nsprint_date,\n\nsprint_time\n\nfrom source\n\n)\n\nselect*from renamed\n\nCreate stg_f1_results.sql with this file path models/staging/formula1/stg_f1_results.sql:\n\nwith\n\nsource as(\n\nselect*from {{ source('formula1','results') }}\n\n),\n\nrenamed as(\n\nselect\n\nresultid as result_id,\n\nraceid as race_id,\n\ndriverid as driver_id,\n\nconstructorid as constructor_id,\n\nnumber as driver_number,\n\ngrid,\n\nposition::intas position,\n\npositiontext as position_text,\n\npositionorder as position_order,\n\npoints,\n\nlaps,\n\ntimeas results_time_formatted,\n\nmilliseconds as results_milliseconds,\n\nfastestlap as fastest_lap,\n\nrank as results_rank,\n\nfastestlaptime as fastest_lap_time_formatted,\n\nfastestlapspeed::decimal(6,3)as fastest_lap_speed,\n\nstatusid as status_id\n\nfrom source\n\n)\n\nselect*from renamed\n\nLast one! Create stg_f1_status.sql with this file path: models/staging/formula1/stg_f1_status.sql:\n\nwith\n\nsource as(\n\nselect*from {{ source('formula1','status') }}\n\n),\n\nrenamed as(\n\nselect\n\nstatusid as status_id,\n\nstatus\n\nfrom source\n\n)\n\nselect*from renamed\n\nAfter the source and all the staging models are complete for each of the 8 tables, your staging folder should look like this:\n\nStaging folder\n\nIt‚Äôs a good time to delete our example folder since these two models are extraneous to our formula1 pipeline and my_first_model fails a not_null test that we won‚Äôt spend time investigating. dbt Cloud will warn us that this folder will be permanently deleted, and we are okay with that so select Delete.\n\nDelete example folder\n\nNow that the staging models are built and saved, it's time to create the models in our development schema in Snowflake. To do this we're going to enter into the command line dbt build to run all of the models in our project, which includes the 8 new staging models and the existing example models.\n\nYour run should complete successfully and you should see green checkmarks next to all of your models in the run results. We built our 8 staging models as views and ran 13 source tests that we configured in the f1_sources.yml file with not that much code, pretty cool!\n\nSuccessful dbt build in Snowflake\n\nLet's take a quick look in Snowflake, refresh database objects, open our development schema, and confirm that the new models are there. If you can see them, then we're good to go!\n\nConfirm models\n\nBefore we move onto the next section, be sure to commit your new models to your Git branch. Click Commit and push and give your commit a message like profile, sources, and staging setup before moving on.\n\nNow that we have all our sources and staging models done, it's time to move into where dbt shines ‚Äî transformation!\n\nWe need to:\n\nCreate some intermediate tables to join tables that aren‚Äôt hierarchical\n\nCreate core tables for business intelligence (BI) tool ingestion\n\nAnswer the two questions about:\n\nfastest pit stops\n\nlap time trends about our Formula 1 data by creating aggregate models using python!\n\nWe need to join lots of reference tables to our results table to create a human readable dataframe. What does this mean? For example, we don‚Äôt only want to have the numeric status_id in our table, we want to be able to read in a row of data that a driver could not finish a race due to engine failure (status_id=5).\n\nBy now, we are pretty good at creating new files in the correct directories so we won‚Äôt cover this in detail. All intermediate models should be created in the path models/intermediate.\n\nCreate a new file called int_lap_times_years.sql. In this model, we are joining our lap time and race information so we can look at lap times over years. In earlier Formula 1 eras, lap times were not recorded (only final results), so we filter out records where lap times are null.\n\nwith lap_times as(\n\nselect*from {{ ref('stg_f1_lap_times') }}\n\n),\n\nraces as(\n\nselect*from {{ ref('stg_f1_races') }}\n\n),\n\nexpanded_lap_times_by_year as(\n\nselect\n\nlap_times.race_id,\n\ndriver_id,\n\nrace_year,\n\nlap,\n\nlap_time_milliseconds\n\nfrom lap_times\n\nleftjoin races\n\non lap_times.race_id = races.race_id\n\nwhere lap_time_milliseconds isnotnull\n\n)\n\nselect*from expanded_lap_times_by_year\n\nCreate a file called in_pit_stops.sql. Pit stops are a many-to-one (M:1) relationship with our races. We are creating a feature called total_pit_stops_per_race by partitioning over our race_id and driver_id, while preserving individual level pit stops for rolling average in our next section.\n\nwith stg_f1__pit_stops as\n\n(\n\nselect*from {{ ref('stg_f1_pit_stops') }}\n\n),\n\npit_stops_per_race as(\n\nselect\n\nrace_id,\n\ndriver_id,\n\nstop_number,\n\nlap,\n\nlap_time_formatted,\n\npit_stop_duration_seconds,\n\npit_stop_milliseconds,\n\nmax(stop_number)over(partitionby race_id,driver_id)as total_pit_stops_per_race\n\nfrom stg_f1__pit_stops\n\n)\n\nselect*from pit_stops_per_race\n\nCreate a file called int_results.sql. Here we are using 4 of our tables ‚Äî races, drivers, constructors, and status ‚Äî to give context to our results table. We are now able to calculate a new feature drivers_age_years by bringing the date_of_birth and race_year into the same table. We are also creating a column to indicate if the driver did not finish (dnf) the race, based upon if their position was null called, dnf_flag.\n\nwith results as(\n\nselect*from {{ ref('stg_f1_results') }}\n\n),\n\nraces as(\n\nselect*from {{ ref('stg_f1_races') }}\n\n),\n\ndrivers as(\n\nselect*from {{ ref('stg_f1_drivers') }}\n\n),\n\nconstructors as(\n\nselect*from {{ ref('stg_f1_constructors') }}\n\n),\n\nstatusas(\n\nselect*from {{ ref('stg_f1_status') }}\n\n),\n\nint_results as(\n\nselect\n\nresult_id,\n\nresults.race_id,\n\nrace_year,\n\nrace_round,\n\ncircuit_id,\n\ncircuit_name,\n\nrace_date,\n\nrace_time,\n\nresults.driver_id,\n\nresults.driver_number,\n\nforename ||' '|| surname as driver,\n\ncast(datediff('year', date_of_birth, race_date)asint)as drivers_age_years,\n\ndriver_nationality,\n\nresults.constructor_id,\n\nconstructor_name,\n\nconstructor_nationality,\n\ngrid,\n\nposition,\n\nposition_text,\n\nposition_order,\n\npoints,\n\nlaps,\n\nresults_time_formatted,\n\nresults_milliseconds,\n\nfastest_lap,\n\nresults_rank,\n\nfastest_lap_time_formatted,\n\nfastest_lap_speed,\n\nresults.status_id,\n\nstatus,\n\ncasewhen position isnullthen1else0endas dnf_flag\n\nfrom results\n\nleftjoin races\n\non results.race_id=races.race_id\n\nleftjoin drivers\n\non results.driver_id = drivers.driver_id\n\nleftjoin constructors\n\non results.constructor_id = constructors.constructor_id\n\nleftjoinstatus\n\non results.status_id =status.status_id\n\n)\n\nselect*from int_results\n\nCreate a Markdown file intermediate.md that we will go over in depth in the Test and Documentation sections of the Leverage dbt Cloud to generate analytics and ML-ready pipelines with SQL and Python with Snowflake guide.\n\n# the intent of this .md is to allow for multi-line long form explanations for our intermediate transformations\n\n# below are descriptions\n\n{% docs int_results %} In this query we want to join out other important information about the race results to have a human readable table about results, races, drivers, constructors, and status.\n\nWe will have 4 left joins onto our results table. {% enddocs %}\n\n{% docs int_pit_stops %} There are many pit stops within one race, aka a M:1 relationship.\n\nWe want to aggregate this so we can properly join pit stop information without creating a fanout. {% enddocs %}\n\n{% docs int_lap_times_years %} Lap times are done per lap. We need to join them out to the race year to understand yearly lap time trends. {% enddocs %}\n\nCreate a YAML file intermediate.yml that we will go over in depth during the Test and Document sections of the Leverage dbt Cloud to generate analytics and ML-ready pipelines with SQL and Python with Snowflake guide.\n\nversion:2\n\nmodels:\n\n-name: int_results\n\ndescription:'{{ doc(\"int_results\") }}'\n\n-name: int_pit_stops\n\ndescription:'{{ doc(\"int_pit_stops\") }}'\n\n-name: int_lap_times_years\n\ndescription:'{{ doc(\"int_lap_times_years\") }}'\n\nThat wraps up the intermediate models we need to create our core models!\n\nCreate a file fct_results.sql. This is what I like to refer to as the ‚Äúmega table‚Äù ‚Äî a really large denormalized table with all our context added in at row level for human readability. Importantly, we have a table circuits that is linked through the table races. When we joined races to results in int_results.sql we allowed our tables to make the connection from circuits to results in fct_results.sql. We are only taking information about pit stops at the result level so our join would not cause a fanout.\n\nwith int_results as(\n\nselect*from {{ ref('int_results') }}\n\n),\n\nint_pit_stops as(\n\nselect\n\nrace_id,\n\ndriver_id,\n\nmax(total_pit_stops_per_race)as total_pit_stops_per_race\n\nfrom {{ ref('int_pit_stops') }}\n\ngroupby1,2\n\n),\n\ncircuits as(\n\nselect*from {{ ref('stg_f1_circuits') }}\n\n),\n\nbase_results as(\n\nselect\n\nresult_id,\n\nint_results.race_id,\n\nrace_year,\n\nrace_round,\n\nint_results.circuit_id,\n\nint_results.circuit_name,\n\ncircuit_ref,\n\nlocation,\n\ncountry,\n\nlatitude,\n\nlongitude,\n\naltitude,\n\ntotal_pit_stops_per_race,\n\nrace_date,\n\nrace_time,\n\nint_results.driver_id,\n\ndriver,\n\ndriver_number,\n\ndrivers_age_years,\n\ndriver_nationality,\n\nconstructor_id,\n\nconstructor_name,\n\nconstructor_nationality,\n\ngrid,\n\nposition,\n\nposition_text,\n\nposition_order,\n\npoints,\n\nlaps,\n\nresults_time_formatted,\n\nresults_milliseconds,\n\nfastest_lap,\n\nresults_rank,\n\nfastest_lap_time_formatted,\n\nfastest_lap_speed,\n\nstatus_id,\n\nstatus,\n\ndnf_flag\n\nfrom int_results\n\nleftjoin circuits\n\non int_results.circuit_id=circuits.circuit_id\n\nleftjoin int_pit_stops\n\non int_results.driver_id=int_pit_stops.driver_id and int_results.race_id=int_pit_stops.race_id\n\n)\n\nselect*from base_results\n\nCreate the file pit_stops_joined.sql. Our results and pit stops are at different levels of dimensionality (also called grain). Simply put, we have multiple pit stops per a result. Since we are interested in understanding information at the pit stop level with information about race year and constructor, we will create a new table pit_stops_joined.sql where each row is per pit stop. Our new table tees up our aggregation in Python.\n\nwith base_results as(\n\nselect*from {{ ref('fct_results') }}\n\n),\n\npit_stops as(\n\nselect*from {{ ref('int_pit_stops') }}\n\n),\n\npit_stops_joined as(\n\nselect\n\nbase_results.race_id,\n\nrace_year,\n\nbase_results.driver_id,\n\nconstructor_id,\n\nconstructor_name,\n\nstop_number,\n\nlap,\n\nlap_time_formatted,\n\npit_stop_duration_seconds,\n\npit_stop_milliseconds\n\nfrom base_results\n\nleftjoin pit_stops\n\non base_results.race_id=pit_stops.race_id and base_results.driver_id=pit_stops.driver_id\n\n)\n\nselect*from pit_stops_joined\n\nEnter in the command line and execute dbt build to build out our entire pipeline to up to this point. Don‚Äôt worry about ‚Äúoverriding‚Äù your previous models ‚Äì dbt workflows are designed to be idempotent so we can run them again and expect the same results.\n\nLet‚Äôs talk about our lineage so far. It‚Äôs looking good üòé. We‚Äôve shown how SQL can be used to make data type, column name changes, and handle hierarchical joins really well; all while building out our automated lineage!\n\nThe DAG\n\nTime to Commit and push our changes and give your commit a message like intermediate and fact models before moving on.\n\nUp until now, SQL has been driving the project (car pun intended) for data cleaning and hierarchical joining. Now it‚Äôs time for Python to take the wheel (car pun still intended) for the rest of our lab! For more information about running Python models on dbt, check out our docs. To learn more about dbt python works under the hood, check out Snowpark for Python, which makes running dbt Python models possible.\n\nThere are quite a few differences between SQL and Python in terms of the dbt syntax and DDL, so we‚Äôll be breaking our code and model runs down further for our python models.\n\nFirst, we want to find out: which constructor had the fastest pit stops in 2021? (constructor is a Formula 1 team that builds or ‚Äúconstructs‚Äù the car).\n\nCreate a new file called fastest_pit_stops_by_constructor.py in our aggregates (this is the first time we are using the .py extension!).\n\nCopy the following code into the file:\n\nimport numpy as np\n\nimport pandas as pd\n\ndefmodel(dbt, session):\n\ndbt.config(packages=[\"pandas\",\"numpy\"])\n\npit_stops_joined = dbt.ref(\"pit_stops_joined\").to_pandas()\n\nyear=2021\n\npit_stops_joined[\"PIT_STOP_SECONDS\"]= pit_stops_joined[\"PIT_STOP_MILLISECONDS\"]/1000\n\nfastest_pit_stops = pit_stops_joined[(pit_stops_joined[\"RACE_YEAR\"]==year)].groupby(by=\"CONSTRUCTOR_NAME\")[\"PIT_STOP_SECONDS\"].describe().sort_values(by='mean')\n\nfastest_pit_stops.reset_index(inplace=True)\n\nfastest_pit_stops.columns = fastest_pit_stops.columns.str.upper()\n\nreturn fastest_pit_stops.round(2)\n\nLet‚Äôs break down what this code is doing step by step:\n\nFirst, we are importing the Python libraries that we are using. A library is a reusable chunk of code that someone else wrote that you may want to include in your programs/projects. We are using numpy and pandasin this Python model. This is similar to a dbt package, but our Python libraries do not persist across the entire project.\n\nDefining a function called model with the parameter dbt and session. The parameter dbt is a class compiled by dbt, which enables you to run your Python code in the context of your dbt project and DAG. The parameter session is a class representing your Snowflake‚Äôs connection to the Python backend. The model function must return a single DataFrame. You can see that all the data transformation happening is within the body of the model function that the return statement is tied to.\n\nThen, within the context of our dbt model library, we are passing in a configuration of which packages we need using dbt.config(packages=[\"pandas\",\"numpy\"]).\n\nUse the .ref() function to retrieve the data frame pit_stops_joined that we created in our last step using SQL. We cast this to a pandas dataframe (by default it's a Snowpark Dataframe).\n\nCreate a variable named year so we aren‚Äôt passing a hardcoded value.\n\nGenerate a new column called PIT_STOP_SECONDS by dividing the value of PIT_STOP_MILLISECONDS by 1000.\n\nCreate our final data frame fastest_pit_stops that holds the records where year is equal to our year variable (2021 in this case), then group the data frame by CONSTRUCTOR_NAME and use the describe() and sort_values() and in descending order. This will make our first row in the new aggregated data frame the team with the fastest pit stops over an entire competition year.\n\nFinally, it resets the index of the fastest_pit_stops data frame. The reset_index() method allows you to reset the index back to the default 0, 1, 2, etc indexes. By default, this method will keep the \"old\" indexes in a column named \"index\"; to avoid this, use the drop parameter. Think of this as keeping your data ‚Äúflat and square‚Äù as opposed to ‚Äútiered‚Äù. If you are new to Python, now might be a good time to learn about indexes for 5 minutes since it's the foundation of how Python retrieves, slices, and dices data. The inplace argument means we override the existing data frame permanently. Not to fear! This is what we want to do to avoid dealing with multi-indexed dataframes!\n\nConvert our Python column names to all uppercase using .upper(), so Snowflake recognizes them.\n\nFinally we are returning our dataframe with 2 decimal places for all the columns using the round() method.\n\nZooming out a bit, what are we doing differently here in Python from our typical SQL code:\n\nMethod chaining is a technique in which multiple methods are called on an object in a single statement, with each method call modifying the result of the previous one. The methods are called in a chain, with the output of one method being used as the input for the next one. The technique is used to simplify the code and make it more readable by eliminating the need for intermediate variables to store the intermediate results.\n\nThe way you see method chaining in Python is the syntax .().(). For example, .describe().sort_values(by='mean') where the .describe() method is chained to .sort_values().\n\nThe .describe() method is used to generate various summary statistics of the dataset. It's used on pandas dataframe. It gives a quick and easy way to get the summary statistics of your dataset without writing multiple lines of code.\n\nThe .sort_values() method is used to sort a pandas dataframe or a series by one or multiple columns. The method sorts the data by the specified column(s) in ascending or descending order. It is the pandas equivalent to order by in SQL.\n\nWe won‚Äôt go as in depth for our subsequent scripts, but will continue to explain at a high level what new libraries, functions, and methods are doing.\n\nBuild the model using the UI which will execute:\n\ndbt run --select fastest_pit_stops_by_constructor\n\nin the command bar.\n\nLet‚Äôs look at some details of our first Python model to see what our model executed. There two major differences we can see while running a Python model compared to an SQL model:\n\nOur Python model was executed as a stored procedure. Snowflake needs a way to know that it's meant to execute this code in a Python runtime, instead of interpreting in a SQL runtime. We do this by creating a Python stored proc, called by a SQL command.\n\nThe snowflake-snowpark-python library has been picked up to execute our Python code. Even though this wasn‚Äôt explicitly stated this is picked up by the dbt class object because we need our Snowpark package to run Python!\n\nPython models take a bit longer to run than SQL models, however we could always speed this up by using Snowpark-optimized Warehouses if we wanted to. Our data is sufficiently small, so we won‚Äôt worry about creating a separate warehouse for Python versus SQL files today.\n\nWe can see our python model is run a stored procedure in our personal development schema\n\nThe rest of our Details output gives us information about how dbt and Snowpark for Python are working together to define class objects and apply a specific set of methods to run our models.\n\nSo which constructor had the fastest pit stops in 2021? Let‚Äôs look at our data to find out!\n\nWe can't preview Python models directly, so let‚Äôs create a new file using the + button or the Control-n shortcut to create a new scratchpad.\n\nReference our Python model:\n\nselect*from {{ ref('fastest_pit_stops_by_constructor') }}\n\nand preview the output:\n\nLooking at our new python data model we can see that Red Bull had the fastest pit stops!\n\nNot only did Red Bull have the fastest average pit stops by nearly 40 seconds, they also had the smallest standard deviation, meaning they are both fastest and most consistent teams in pit stops. By using the .describe() method we were able to avoid verbose SQL requiring us to create a line of code per column and repetitively use the PERCENTILE_COUNT() function.\n\nNow we want to find the lap time average and rolling average through the years (is it generally trending up or down)?\n\nCreate a new file called lap_times_moving_avg.py in our aggregates folder.\n\nCopy the following code into the file:\n\nimport pandas as pd\n\ndefmodel(dbt, session):\n\ndbt.config(packages=[\"pandas\"])\n\nlap_times = dbt.ref(\"int_lap_times_years\").to_pandas()\n\nlap_times[\"LAP_TIME_SECONDS\"]= lap_times[\"LAP_TIME_MILLISECONDS\"]/1000\n\nlap_time_trends = lap_times.groupby(by=\"RACE_YEAR\")[\"LAP_TIME_SECONDS\"].mean().to_frame()\n\nlap_time_trends.reset_index(inplace=True)\n\nlap_time_trends[\"LAP_MOVING_AVG_5_YEARS\"]= lap_time_trends[\"LAP_TIME_SECONDS\"].rolling(5).mean()\n\nlap_time_trends.columns = lap_time_trends.columns.str.upper()\n\nreturn lap_time_trends.round(1)\n\nBreaking down our code a bit:\n\nWe‚Äôre only using the pandas library for this model and casting it to a pandas data frame .to_pandas().\n\nGenerate a new column called LAP_TIMES_SECONDS by dividing the value of LAP_TIME_MILLISECONDS by 1000.\n\nCreate the final dataframe. Get the lap time per year. Calculate the mean series and convert to a data frame.\n\nReset the index.\n\nCalculate the rolling 5 year mean.\n\nRound our numeric columns to one decimal place.\n\nNow, run this model by using the UI Run model or\n\ndbt run --select lap_times_moving_avg\n\nin the command bar.\n\nOnce again previewing the output of our data using the same steps for our fastest_pit_stops_by_constructor model.\n\nViewing our lap trends and 5 year rolling trends\n\nWe can see that it looks like lap times are getting consistently faster over time. Then in 2010 we see an increase occur! Using outside subject matter context, we know that significant rule changes were introduced to Formula 1 in 2010 and 2011 causing slower lap times.\n\nNow is a good time to checkpoint and commit our work to Git. Click Commit and push and give your commit a message like aggregate python models before moving on.\n\nLet‚Äôs take a step back before starting machine learning to both review and go more in-depth at the methods that make running dbt python models possible. If you want to know more outside of this lab‚Äôs explanation read the documentation here.\n\ndbt model(dbt, session). For starters, each Python model lives in a .py file in your models/ folder. It defines a function named model(), which takes two parameters:\n\ndbt ‚Äî A class compiled by dbt Core, unique to each model, enables you to run your Python code in the context of your dbt project and DAG.\n\nsession ‚Äî A class representing your data platform‚Äôs connection to the Python backend. The session is needed to read in tables as DataFrames and to write DataFrames back to tables. In PySpark, by convention, the SparkSession is named spark, and available globally. For consistency across platforms, we always pass it into the model function as an explicit argument called session.\n\nThe model() function must return a single DataFrame. On Snowpark (Snowflake), this can be a Snowpark or pandas DataFrame.\n\n.source() and .ref() functions. Python models participate fully in dbt's directed acyclic graph (DAG) of transformations. If you want to read directly from a raw source table, use dbt.source(). We saw this in our earlier section using SQL with the source function. These functions have the same execution, but with different syntax. Use the dbt.ref() method within a Python model to read data from other models (SQL or Python). These methods return DataFrames pointing to the upstream source, model, seed, or snapshot.\n\n.config(). Just like SQL models, there are three ways to configure Python models:\n\nIn a dedicated .yml file, within the models/ directory\n\nWithin the model's .py file, using the dbt.config() method\n\nCalling the dbt.config() method will set configurations for your model within your .py file, similar to the {{ config() }} macro in .sql model files:\n\ndefmodel(dbt, session):\n\ndbt.config(materialized=\"table\")\n\nThere's a limit to how complex you can get with the dbt.config() method. It accepts only literal values (strings, booleans, and numeric types). Passing another function or a more complex data structure is not possible. The reason is that dbt statically analyzes the arguments to .config() while parsing your model without executing your Python code. If you need to set a more complex configuration, we recommend you define it using the config property in a YAML file. Learn more about configurations here.\n\nNow that we‚Äôve gained insights and business intelligence about Formula 1 at a descriptive level, we want to extend our capabilities into prediction. We‚Äôre going to take the scenario where we censor the data. This means that we will pretend that we will train a model using earlier data and apply it to future data. In practice, this means we‚Äôll take data from 2010-2019 to train our model and then predict 2020 data.\n\nIn this section, we‚Äôll be preparing our data to predict the final race position of a driver.\n\nAt a high level we‚Äôll be:\n\nCreating new prediction features and filtering our dataset to active drivers\n\nEncoding our data (algorithms like numbers) and simplifying our target variable called position\n\nSplitting our dataset into training, testing, and validation\n\nTo keep our project organized, we‚Äôll need to create two new subfolders in our ml directory. Under the ml folder, make the subfolders prep and train_predict.\n\nCreate a new file under ml/prep called ml_data_prep.py. Copy the following code into the file and Save.\n\nimport pandas as pd\n\ndefmodel(dbt, session):\n\ndbt.config(packages=[\"pandas\"])\n\nfct_results = dbt.ref(\"fct_results\").to_pandas()\n\nstart_year=2010\n\nend_year=2020\n\ndata = fct_results.loc[fct_results['RACE_YEAR'].between(start_year, end_year)]\n\ndata['POSITION']= data['POSITION'].astype(float)\n\ndata['TOTAL_PIT_STOPS_PER_RACE']= data['TOTAL_PIT_STOPS_PER_RACE'].fillna(0)\n\nmapping ={'Force India':'Racing Point','Sauber':'Alfa Romeo','Lotus F1':'Renault','Toro Rosso':'AlphaTauri'}\n\ndata['CONSTRUCTOR_NAME'].replace(mapping, inplace=True)\n\ndnf_by_driver = data.groupby('DRIVER').sum(numeric_only=True)['DNF_FLAG']\n\ndriver_race_entered = data.groupby('DRIVER').count()['DNF_FLAG']\n\ndriver_dnf_ratio =(dnf_by_driver/driver_race_entered)\n\ndriver_confidence =1-driver_dnf_ratio\n\ndriver_confidence_dict =dict(zip(driver_confidence.index,driver_confidence))\n\ndnf_by_constructor = data.groupby('CONSTRUCTOR_NAME').sum(numeric_only=True)['DNF_FLAG']\n\nconstructor_race_entered = data.groupby('CONSTRUCTOR_NAME').count()['DNF_FLAG']\n\nconstructor_dnf_ratio =(dnf_by_constructor/constructor_race_entered)\n\nconstructor_relaiblity =1-constructor_dnf_ratio\n\nconstructor_relaiblity_dict =dict(zip(constructor_relaiblity.index,constructor_relaiblity))\n\ndata['DRIVER_CONFIDENCE']= data['DRIVER'].apply(lambda x:driver_confidence_dict[x])\n\ndata['CONSTRUCTOR_RELAIBLITY']= data['CONSTRUCTOR_NAME'].apply(lambda x:constructor_relaiblity_dict[x])\n\nactive_constructors =['Renault','Williams','McLaren','Ferrari','Mercedes',\n\n'AlphaTauri','Racing Point','Alfa Romeo','Red Bull',\n\n'Haas F1 Team']\n\nactive_drivers =['Daniel Ricciardo','Kevin Magnussen','Carlos Sainz',\n\n'Valtteri Bottas','Lance Stroll','George Russell',\n\n'Lando Norris','Sebastian Vettel','Kimi R√§ikk√∂nen',\n\n'Charles Leclerc','Lewis Hamilton','Daniil Kvyat',\n\n'Max Verstappen','Pierre Gasly','Alexander Albon',\n\n'Sergio P√©rez','Esteban Ocon','Antonio Giovinazzi',\n\n'Romain Grosjean','Nicholas Latifi']\n\ndata['ACTIVE_DRIVER']= data['DRIVER'].apply(lambda x:int(x in active_drivers))\n\ndata['ACTIVE_CONSTRUCTOR']= data['CONSTRUCTOR_NAME'].apply(lambda x:int(x in active_constructors))\n\nreturn data\n\nAs usual, let‚Äôs break down what we are doing in this Python model:\n\nWe‚Äôre first referencing our upstream fct_results table and casting it to a pandas dataframe.\n\nFiltering on years 2010-2020 since we‚Äôll need to clean all our data we are using for prediction (both training and testing).\n\nFilling in empty data for total_pit_stops and making a mapping active constructors and drivers to avoid erroneous predictions\n\n‚ö†Ô∏è You might be wondering why we didn‚Äôt do this upstream in our fct_results table! The reason for this is that we want our machine learning cleanup to reflect the year 2020 for our predictions and give us an up-to-date team name. However, for business intelligence purposes we can keep the historical data at that point in time. Instead of thinking of one table as ‚Äúone source of truth‚Äù we are creating different datasets fit for purpose: one for historical descriptions and reporting and another for relevant predictions.\n\nCreate new confidence features for drivers and constructors\n\nGenerate flags for the constructors and drivers that were active in 2020\n\nExecute the following in the command bar:\n\ndbt run --select ml_data_prep\n\nThere are more aspects we could consider for this project, such as normalizing the driver confidence by the number of races entered. Including this would help account for a driver‚Äôs history and consider whether they are a new or long-time driver. We‚Äôre going to keep it simple for now, but these are some of the ways we can expand and improve our machine learning dbt projects. Breaking down our machine learning prep model:\n\nLambda functions ‚Äî We use some lambda functions to transform our data without having to create a fully-fledged function using the def notation. So what exactly are lambda functions?\n\nIn Python, a lambda function is a small, anonymous function defined using the keyword \"lambda\". Lambda functions are used to perform a quick operation, such as a mathematical calculation or a transformation on a list of elements. They are often used in conjunction with higher-order functions, such as apply, map, filter, and reduce.\n\n.apply() method ‚Äî We used .apply() to pass our functions into our lambda expressions to the columns and perform this multiple times in our code. Let‚Äôs explain apply a little more:\n\nThe .apply() function in the pandas library is used to apply a function to a specified axis of a DataFrame or a Series. In our case the function we used was our lambda function!\n\nThe .apply() function takes two arguments: the first is the function to be applied, and the second is the axis along which the function should be applied. The axis can be specified as 0 for rows or 1 for columns. We are using the default value of 0 so we aren‚Äôt explicitly writing it in the code. This means that the function will be applied to each row of the DataFrame or Series.\n\nLet‚Äôs look at the preview of our clean dataframe after running our ml_data_prep model:\n\nWhat our clean dataframe fit for machine learning looks like\n\nIn this next part, we‚Äôll be performing covariate encoding. Breaking down this phrase a bit, a covariate is a variable that is relevant to the outcome of a study or experiment, and encoding refers to the process of converting data (such as text or categorical variables) into a numerical format that can be used as input for a model. This is necessary because most machine learning algorithms can only work with numerical data. Algorithms don‚Äôt speak languages, have eyes to see images, etc. so we encode our data into numbers so algorithms can perform tasks by using calculations they otherwise couldn‚Äôt.\n\nüß† We‚Äôll think about this as : ‚Äúalgorithms like numbers‚Äù.\n\nCreate a new file under ml/prep called covariate_encoding copy the code below and save.\n\nimport pandas as pd\n\nimport numpy as np\n\nfrom sklearn.preprocessing import StandardScaler,LabelEncoder,OneHotEncoder\n\nfrom sklearn.linear_model import LogisticRegression\n\ndefmodel(dbt, session):\n\ndbt.config(packages=[\"pandas\",\"numpy\",\"scikit-learn\"])\n\ndata = dbt.ref(\"ml_data_prep\").to_pandas()\n\ncovariates = data[['RACE_YEAR','CIRCUIT_NAME','GRID','CONSTRUCTOR_NAME','DRIVER','DRIVERS_AGE_YEARS','DRIVER_CONFIDENCE','CONSTRUCTOR_RELAIBLITY','TOTAL_PIT_STOPS_PER_RACE','ACTIVE_DRIVER','ACTIVE_CONSTRUCTOR','POSITION']]\n\nfil_cov = covariates[(covariates['ACTIVE_DRIVER']==1)&(covariates['ACTIVE_CONSTRUCTOR']==1)]\n\nle = LabelEncoder()\n\nfil_cov['CIRCUIT_NAME']= le.fit_transform(fil_cov['CIRCUIT_NAME'])\n\nfil_cov['CONSTRUCTOR_NAME']= le.fit_transform(fil_cov['CONSTRUCTOR_NAME'])\n\nfil_cov['DRIVER']= le.fit_transform(fil_cov['DRIVER'])\n\nfil_cov['TOTAL_PIT_STOPS_PER_RACE']= le.fit_transform(fil_cov['TOTAL_PIT_STOPS_PER_RACE'])\n\ndefposition_index(x):\n\nif x<4:\n\nreturn1\n\nif x>10:\n\nreturn3\n\nelse:\n\nreturn2\n\nencoded_data = fil_cov.drop(['ACTIVE_DRIVER','ACTIVE_CONSTRUCTOR'],axis=1))\n\nencoded_data['POSITION_LABEL']= encoded_data['POSITION'].apply(lambda x: position_index(x))\n\nencoded_data_grouped_target = encoded_data.drop(['POSITION'],axis=1))\n\nreturn encoded_data_grouped_target\n\nExecute the following in the command bar:\n\ndbt run --select covariate_encoding\n\nIn this code, we are using a ton of functions from libraries! This is really cool, because we can utilize code other people have developed and bring it into our project simply by using the import function. Scikit-learn, ‚Äúsklearn‚Äù for short, is an extremely popular data science library. Sklearn contains a wide range of machine learning techniques, including supervised and unsupervised learning algorithms, feature scaling and imputation, as well as tools model evaluation and selection. We‚Äôll be using Sklearn for both preparing our covariates and creating models (our next section).\n\nOur dataset is pretty small data so we are good to use pandas and sklearn. If you have larger data for your own project in mind, consider dask or category_encoders.\n\nBreaking it down a bit more:\n\nWe‚Äôre selecting a subset of variables that will be used as predictors for a driver‚Äôs position.\n\nFilter the dataset to only include rows using the active driver and constructor flags we created in the last step.\n\nThe next step is to use the LabelEncoder from scikit-learn to convert the categorical variables CIRCUIT_NAME, CONSTRUCTOR_NAME, DRIVER, and TOTAL_PIT_STOPS_PER_RACE into numerical values.\n\nCreate a new variable called POSITION_LABEL, which is a derived from our position variable.\n\nüí≠ Why are we changing our position variable? There are 20 total positions in Formula 1 and we are grouping them together to simplify the classification and improve performance. We also want to demonstrate you can create a new function within your dbt model!\n\nOur new position_label variable has meaning:\n\nIn Formula1 if you are in:\n\nTop 3 you get a ‚Äúpodium‚Äù position\n\nTop 10 you gain points that add to your overall season total\n\nBelow top 10 you get no points!\n\nWe are mapping our original variable position to position_label to the corresponding places above to 1,2, and 3 respectively.\n\nDrop the active driver and constructor flags since they were filter criteria and additionally drop our original position variable.\n\nNow that we‚Äôve cleaned and encoded our data, we are going to further split in by time. In this step, we will create dataframes to use for training and prediction. We‚Äôll be creating two dataframes 1) using data from 2010-2019 for training, and 2) data from 2020 for new prediction inferences. We‚Äôll create variables called start_year and end_year so we aren‚Äôt filtering on hardcasted values (and can more easily swap them out in the future if we want to retrain our model on different timeframes).\n\nCreate a file called train_test_dataset.py copy and save the following code:\n\nimport pandas as pd\n\ndefmodel(dbt, session):\n\ndbt.config(packages=[\"pandas\"], tags=\"train\")\n\nencoding = dbt.ref(\"covariate_encoding\").to_pandas()\n\nstart_year=2010\n\nend_year=2019\n\ntrain_test_dataset = encoding.loc[encoding['RACE_YEAR'].between(start_year, end_year)]\n\nreturn train_test_dataset\n\nCreate a file called hold_out_dataset_for_prediction.py copy and save the following code below. Now we‚Äôll have a dataset with only the year 2020 that we‚Äôll keep as a hold out set that we are going to use similar to a deployment use case.\n\nimport pandas as pd\n\ndefmodel(dbt, session):\n\ndbt.config(packages=[\"pandas\"], tags=\"predict\")\n\nencoding = dbt.ref(\"covariate_encoding\").to_pandas()\n\nyear=2020\n\nhold_out_dataset = encoding.loc[encoding['RACE_YEAR']== year]\n\nreturn hold_out_dataset\n\nExecute the following in the command bar:\n\ndbt run --select train_test_dataset hold_out_dataset_for_prediction\n\nTo run our temporal data split models, we can use this syntax in the command line to run them both at once. Make sure you use a space syntax between the model names to indicate you want to run both!\n\nCommit and push our changes to keep saving our work as we go using ml data prep and splits before moving on.\n\nüëè Now that we‚Äôve finished our machine learning prep work we can move onto the fun part ‚Äî training and prediction!\n\nWe‚Äôre ready to start training a model to predict the driver‚Äôs position. Now is a good time to pause and take a step back and say, usually in ML projects you‚Äôll try multiple algorithms during development and use an evaluation method such as cross validation to determine which algorithm to use. You can definitely do this in your dbt project, but for the content of this lab we‚Äôll have decided on using a logistic regression to predict position (we actually tried some other algorithms using cross validation outside of this lab such as k-nearest neighbors and a support vector classifier but that didn‚Äôt perform as well as the logistic regression and a decision tree that overfit).\n\nThere are 3 areas to break down as we go since we are working at the intersection all within one model file:\n\nMachine Learning\n\nSnowflake and Snowpark\n\ndbt Python models\n\nIf you haven‚Äôt seen code like this before or use joblib files to save machine learning models, we‚Äôll be going over them at a high level and you can explore the links for more technical in-depth along the way! Because Snowflake and dbt have abstracted away a lot of the nitty gritty about serialization and storing our model object to be called again, we won‚Äôt go into too much detail here. There‚Äôs a lot going on here so take it at your pace!\n\nProject organization remains key, so let‚Äôs make a new subfolder called train_predict under the ml folder.\n\nNow create a new file called train_test_position.py and copy and save the following code:\n\nimport snowflake.snowpark.functions as F\n\nfrom sklearn.model_selection import train_test_split\n\nimport pandas as pd\n\nfrom sklearn.metrics import confusion_matrix, balanced_accuracy_score\n\nimport io\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom joblib import dump, load\n\nimport joblib\n\nimport logging\n\nimport sys\n\nfrom joblib import dump, load\n\nlogger = logging.getLogger(\"mylog\")\n\ndefsave_file(session, model, path, dest_filename):\n\ninput_stream = io.BytesIO()\n\njoblib.dump(model, input_stream)\n\nsession._conn.upload_stream(input_stream, path, dest_filename)\n\nreturn\"successfully created file: \"+ path\n\ndefmodel(dbt, session):\n\ndbt.config(\n\npackages =['numpy','scikit-learn','pandas','numpy','joblib','cachetools'],\n\nmaterialized =\"table\",\n\ntags =\"train\"\n\n)\n\nsession.sql('create or replace stage MODELSTAGE').collect()\n\nversion =\"1.0\"\n\nlogger.info('Model training version: '+ version)\n\ntest_train_df = dbt.ref(\"train_test_dataset\")\n\ntest_train_pd_df = test_train_df.to_pandas()\n\ntarget_col =\"POSITION_LABEL\"\n\nsplit_X = test_train_pd_df.drop([target_col], axis=1)\n\nsplit_y = test_train_pd_df[target_col]\n\nX_train, X_test, y_train, y_test = train_test_split(split_X, split_y, train_size=0.7, random_state=42)\n\ntrain =[X_train, y_train]\n\ntest =[X_test, y_test]\n\nmodel = LogisticRegression()\n\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict_proba(X_test)[:,1]\n\npredictions =[round(value)for value in y_pred]\n\nbalanced_accuracy = balanced_accuracy_score(y_test, predictions)\n\nsave_file(session, model,\"@MODELSTAGE/driver_position_\"+version,\"driver_position_\"+version+\".joblib\")\n\nlogger.info('Model artifact:'+\"@MODELSTAGE/driver_position_\"+version+\".joblib\")\n\nsnowpark_train_df = session.write_pandas(pd.concat(train, axis=1, join='inner'),\"train_table\", auto_create_table=True, create_temp_table=True)\n\nsnowpark_test_df = session.write_pandas(pd.concat(test, axis=1, join='inner'),\"test_table\", auto_create_table=True, create_temp_table=True)\n\nreturn snowpark_train_df.with_column(\"DATASET_TYPE\", F.lit(\"train\")).union(snowpark_test_df.with_column(\"DATASET_TYPE\", F.lit(\"test\")))\n\nExecute the following in the command bar:\n\ndbt run --select train_test_position\n\nBreaking down our Python script here:\n\nWe‚Äôre importing some helpful libraries.\n\nDefining a function called save_file() that takes four parameters: session, model, path and dest_filename that will save our logistic regression model file.\n\nsession ‚Äî an object representing a connection to Snowflake.\n\nmodel ‚Äî an object that needs to be saved. In this case, it's a Python object that is a scikit-learn that can be serialized with joblib.\n\npath ‚Äî a string representing the directory or bucket location where the file should be saved.\n\ndest_filename ‚Äî a string representing the desired name of the file.\n\nCreating our dbt model\n\nWithin this model we are creating a stage called MODELSTAGE to place our logistic regression joblib model file. This is really important since we need a place to keep our model to reuse and want to ensure it's there. When using Snowpark commands, it's common to see the .collect() method to ensure the action is performed. Think of the session as our ‚Äústart‚Äù and collect as our ‚Äúend‚Äù when working with Snowpark (you can use other ending methods other than collect).\n\nUsing .ref() to connect into our train_test_dataset model.\n\nNow we see the machine learning part of our analysis:\n\nCreate new dataframes for our prediction features from our target variable position_label.\n\nSplit our dataset into 70% training (and 30% testing), train_size=0.7 with a random_state specified to have repeatable results.\n\nSpecify our model is a logistic regression.\n\nFit our model. In a logistic regression this means finding the coefficients that will give the least classification error.\n\nRound our predictions to the nearest integer since logistic regression creates a probability between for each class and calculate a balanced accuracy to account for imbalances in the target variable.\n\nRight now our model is only in memory, so we need to use our nifty function save_file to save our model file to our Snowflake stage. We save our model as a joblib file so Snowpark can easily call this model object back to create predictions. We really don‚Äôt need to know much else as a data practitioner unless we want to. It‚Äôs worth noting that joblib files aren‚Äôt able to be queried directly by SQL. To do this, we would need to transform the joblib file to an SQL querable format such as JSON or CSV (out of scope for this workshop).\n\nFinally we want to return our dataframe, but create a new column indicating what rows were used for training and those for training.\n\nViewing our output of this model:\n\nPreview which rows of our model were used for training and testing\n\nLet‚Äôs pop back over to Snowflake and check that our logistic regression model has been stored in our MODELSTAGE using the command:\n\nlist @modelstage\n\nList the objects in our Snowflake stage to check for our logistic regression to predict driver position\n\nTo investigate the commands run as part of train_test_position script, navigate to Snowflake query history to view it Activity > Query History. We can view the portions of query that we wrote such as create or replace stage MODELSTAGE, but we also see additional queries that Snowflake uses to interpret python code.\n\nView Snowflake query history to see how python models are run under the hood\n\nCreate a new file called predict_position.py and copy and save the following code:\n\nimport logging\n\nimport joblib\n\nimport pandas as pd\n\nimport os\n\nfrom snowflake.snowpark import types as T\n\nDB_STAGE ='MODELSTAGE'\n\nversion ='1.0'\n\nmodel_file_path ='driver_position_'+version\n\nmodel_file_packaged ='driver_position_'+version+'.joblib'\n\nLOCAL_TEMP_DIR =f'/tmp/driver_position'\n\nDOWNLOAD_DIR = os.path.join(LOCAL_TEMP_DIR,'download')\n\nTARGET_MODEL_DIR_PATH = os.path.join(LOCAL_TEMP_DIR,'ml_model')\n\nTARGET_LIB_PATH = os.path.join(LOCAL_TEMP_DIR,'lib')\n\nFEATURE_COLS =[\n\n\"RACE_YEAR\"\n\n,\"CIRCUIT_NAME\"\n\n,\"GRID\"\n\n,\"CONSTRUCTOR_NAME\"\n\n,\"DRIVER\"\n\n,\"DRIVERS_AGE_YEARS\"\n\n,\"DRIVER_CONFIDENCE\"\n\n,\"CONSTRUCTOR_RELAIBLITY\"\n\n,\"TOTAL_PIT_STOPS_PER_RACE\"]\n\ndefregister_udf_for_prediction(p_predictor ,p_session ,p_dbt):\n\ndefpredict_position(p_df: T.PandasDataFrame[int,int,int,int,\n\nint,int,int,int,int])-> T.PandasSeries[int]:\n\np_df.columns =[*FEATURE_COLS]\n\npred_array = p_predictor.predict(p_df)\n\ndf_predicted = pd.Series(pred_array)\n\nreturn df_predicted\n\nudf_packages = p_dbt.config.get('packages')\n\npredict_position_udf = p_session.udf.register(\n\npredict_position\n\n,name=f'predict_position'\n\n,packages = udf_packages\n\n)\n\nreturn predict_position_udf\n\ndefdownload_models_and_libs_from_stage(p_session):\n\np_session.file.get(f'@{DB_STAGE}/{model_file_path}/{model_file_packaged}', DOWNLOAD_DIR)\n\ndefload_model(p_session):\n\nmodel_fl_path = os.path.join(DOWNLOAD_DIR, model_file_packaged)\n\npredictor = joblib.load(model_fl_path)\n\nreturn predictor\n\ndefmodel(dbt, session):\n\ndbt.config(\n\npackages =['snowflake-snowpark-python','scipy','scikit-learn','pandas','numpy'],\n\nmaterialized =\"table\",\n\ntags =\"predict\"\n\n)\n\nsession._use_scoped_temp_objects =False\n\ndownload_models_and_libs_from_stage(session)\n\npredictor = load_model(session)\n\npredict_position_udf = register_udf_for_prediction(predictor, session ,dbt)\n\nhold_out_df =(dbt.ref(\"hold_out_dataset_for_prediction\")\n\n.select(*FEATURE_COLS)\n\n)\n\nnew_predictions_df = hold_out_df.withColumn(\"position_predicted\"\n\n,predict_position_udf(*FEATURE_COLS)\n\n)\n\nreturn new_predictions_df\n\nExecute the following in the command bar:\n\ndbt run --select predict_position\n\nCommit and push our changes to keep saving our work as we go using the commit message logistic regression model training and application before moving on.\n\nAt a high level in this script, we are:\n\nRetrieving our staged logistic regression model\n\nLoading the model in\n\nPlacing the model within a user defined function (UDF) to call in line predictions on our driver‚Äôs position\n\nAt a more detailed level:\n\nImport our libraries.\n\nCreate variables to reference back to the MODELSTAGE we just created and stored our model to.\n\nThe temporary file paths we created might look intimidating, but all we‚Äôre doing here is programmatically using an initial file path and adding to it to create the following directories:\n\nLOCAL_TEMP_DIR ‚û°Ô∏è /tmp/driver_position\n\nDOWNLOAD_DIR ‚û°Ô∏è /tmp/driver_position/download\n\nTARGET_MODEL_DIR_PATH ‚û°Ô∏è /tmp/driver_position/ml_model\n\nTARGET_LIB_PATH ‚û°Ô∏è /tmp/driver_position/lib\n\nProvide a list of our feature columns that we used for model training and will now be used on new data for prediction.\n\nNext, we are creating our main function register_udf_for_prediction(p_predictor ,p_session ,p_dbt):. This function is used to register a user-defined function (UDF) that performs the machine learning prediction. It takes three parameters: p_predictor is an instance of the machine learning model, p_session is an instance of the Snowflake session, and p_dbt is an instance of the dbt library. The function creates a UDF named predict_churn which takes a pandas dataframe with the input features and returns a pandas series with the predictions.\n\n‚ö†Ô∏è Pay close attention to the whitespace here. We are using a function within a function for this script.\n\nWe have 2 simple functions that are programmatically retrieving our file paths to first get our stored model out of our MODELSTAGE and downloaded into the session download_models_and_libs_from_stage and then to load the contents of our model in (parameters) in load_model to use for prediction.\n\nTake the model we loaded in and call it predictor and wrap it in a UDF.\n\nReturn our dataframe with both the features used to predict and the new label.\n\nüß† Another way to read this script is from the bottom up. This can help us progressively see what is going into our final dbt model and work backwards to see how the other functions are being referenced.\n\nLet‚Äôs take a look at our predicted position alongside our feature variables. Open a new scratchpad and use the following query. I chose to order by the prediction of who would obtain a podium position:\n\nselect*from {{ ref('predict_position') }} orderby position_predicted\n\nWe can see that we created predictions in our final dataset, we are ready to move on to testing!\n\nWe have now completed building all the models for today‚Äôs lab, but how do we know if they meet our assertions? Put another way, how do we know the quality of our data models are any good? This brings us to testing!\n\nWe test data models for mainly two reasons:\n\nEnsure that our source data is clean on ingestion before we start data modeling/transformation (aka avoid garbage in, garbage out problem).\n\nMake sure we don‚Äôt introduce bugs in the transformation code we wrote (stop ourselves from creating bad joins/fanouts).\n\nTesting in dbt comes in two flavors: generic and singular.\n\nYou define them in a test block (similar to a macro) and once defined, you can reference them by name in your .yml files (applying them to models, columns, sources, snapshots, and seeds).\n\nYou might be wondering: what about testing Python models?\n\nSince the output of our Python models are tables, we can test SQL and Python models the same way! We don‚Äôt have to worry about any syntax differences when testing SQL versus Python data models. This means we use .yml and .sql files to test our entities (tables, views, etc.). Under the hood, dbt is running an SQL query on our tables to see if they meet assertions. If no rows are returned, dbt will surface a passed test. Conversely, if a test results in returned rows, it will fail or warn depending on the configuration (more on that later).\n\nTo implement generic out-of-the-box tests dbt comes with, we can use YAML files to specify information about our models. To add generic tests to our aggregates model, create a file called aggregates.yml, copy the code block below into the file, and save.\n\nThe aggregates.yml file in our file tree\n\nversion:2\n\nmodels:\n\n-name: fastest_pit_stops_by_constructor\n\ndescription: Use the python .describe() method to retrieve summary statistics table about pit stops by constructor. Sort by average stop time ascending so the first row returns the fastest constructor.\n\ncolumns:\n\n-name: constructor_name\n\ndescription: team that makes the car\n\ntests:\n\n- unique\n\n-name: lap_times_moving_avg\n\ndescription: Use the python .rolling() method to calculate the 5 year rolling average of pit stop times alongside the average for each year.\n\ncolumns:\n\n-name: race_year\n\ndescription: year of the race\n\ntests:\n\n-relationships:\n\nto: ref('int_lap_times_years')\n\nfield: race_year\n\nLet‚Äôs unpack the code we have here. We have both our aggregates models with the model name to know the object we are referencing and the description of the model that we‚Äôll populate in our documentation. At the column level (a level below our model), we are providing the column name followed by our tests. We want to ensure our constructor_name is unique since we used a pandas groupby on constructor_name in the model fastest_pit_stops_by_constructor. Next, we want to ensure our race_year has referential integrity from the model we selected from int_lap_times_years into our subsequent lap_times_moving_avg model.\n\nFinally, if we want to see how tests were deployed on sources and SQL models, we can look at other files in our project such as the f1_sources.yml we created in our Sources and staging section.\n\nUnder your macros folder, create a new file and name it test_all_values_gte_zero.sql. Copy the code block below and save the file. For clarity, ‚Äúgte‚Äù is an abbreviation for greater than or equal to.\n\nmacro file for reusable testing code\n\n{% macro test_all_values_gte_zero(table,column)%}\n\nselect*from {{ ref(table) }} where {{ column }} <0\n\n{% endmacro %}\n\nMacros in Jinja are pieces of code that can be reused multiple times in our SQL models ‚Äî they are analogous to \"functions\" in other programming languages, and are extremely useful if you find yourself repeating code across multiple models.\n\nWe use the {% macro %} to indicate the start of the macro and {% endmacro %} for the end. The text after the beginning of the macro block is the name we are giving the macro to later call it. In this case, our macro is called test_all_values_gte_zero. Macros take in arguments to pass through, in this case the table and the column. In the body of the macro, we see an SQL statement that is using the ref function to dynamically select the table and then the column. You can always view macros without having to run them by using dbt run-operation. You can learn more here.\n\nGreat, now we want to reference this macro as a test! Let‚Äôs create a new test file called macro_pit_stops_mean_is_positive.sql in our tests folder.\n\ncreating a test on our pit stops model referencing the macro\n\nCopy the following code into the file and save:\n\n{{\n\nconfig(\n\nenabled=true,\n\nseverity='warn',\n\ntags =['bi']\n\n)\n\n}}\n\n{{ test_all_values_gte_zero('fastest_pit_stops_by_constructor','mean') }}\n\nIn our testing file, we are applying some configurations to the test including enabled, which is an optional configuration for disabling models, seeds, snapshots, and tests. Our severity is set to warn instead of error, which means our pipeline will still continue to run. We have tagged our test with bi since we are applying this test to one of our bi models.\n\nThen, in our final line, we are calling the test_all_values_gte_zero macro that takes in our table and column arguments and inputting our table 'fastest_pit_stops_by_constructor' and the column 'mean'.\n\nThe simplest way to define a test is by writing the exact SQL that will return failing records. We call these \"singular\" tests, because they're one-off assertions usable for a single purpose.\n\nThese tests are defined in .sql files, typically in your tests directory (as defined by your test-paths config). You can use Jinja in SQL models (including ref and source) in the test definition, just like you can when creating models. Each .sql file contains one select statement, and it defines one test.\n\nLet‚Äôs add a custom test that asserts that the moving average of the lap time over the last 5 years is greater than zero (it‚Äôs impossible to have time less than 0!). It is easy to assume if this is not the case the data has been corrupted.\n\nCreate a file lap_times_moving_avg_assert_positive_or_null.sql under the tests folder.\n\ncustom singular test for testing lap times are positive values\n\nCopy the following code and save the file:\n\n{{\n\nconfig(\n\nenabled=true,\n\nseverity='error',\n\ntags =['bi']\n\n)\n\n}}\n\nwith lap_times_moving_avg as(select*from {{ ref('lap_times_moving_avg') }} )\n\nselect*\n\nfrom lap_times_moving_avg\n\nwhere lap_moving_avg_5_years <0and lap_moving_avg_5_years isnotnull\n\nTime to run our tests! Altogether, we have created 4 tests for our 2 Python models:\n\nfastest_pit_stops_by_constructor\n\nUnique constructor_name\n\nLap times are greater than 0 or null (to allow for the first leading values in a rolling calculation)\n\nlap_times_moving_avg\n\nReferential test on race_year\n\nMean pit stop times are greater than or equal to 0 (no negative time values)\n\nTo run the tests on both our models, we can use this syntax in the command line to run them both at once, similar to how we did our data splits earlier. Execute the following in the command bar:\n\ndbt test--select fastest_pit_stops_by_constructor lap_times_moving_avg\n\nrunning tests on our python models\n\nAll 4 of our tests passed (yay for clean data)! To understand the SQL being run against each of our tables, we can click into the details of the test.\n\nNavigating into the Details of the unique_fastest_pit_stops_by_constructor_name, we can see that each line constructor_name should only have one row.\n\nview details of testing our python model that used SQL to test data assertions\n\nWhen it comes to documentation, dbt brings together both column and model level descriptions that you can provide as well as details from your Snowflake information schema in a static site for consumption by other data team members and stakeholders.\n\nWe are going to revisit 2 areas of our project to understand our documentation:\n\nintermediate.md file\n\ndbt_project.yml file\n\nTo start, let‚Äôs look back at our intermediate.md file. We can see that we provided multi-line descriptions for the models in our intermediate models using docs blocks. Then we reference these docs blocks in our .yml file. Building descriptions with doc blocks in Markdown files gives you the ability to format your descriptions with Markdown and are particularly helpful when building long descriptions, either at the column or model level. In our dbt_project.yml, we added node_colors at folder levels.\n\nTo see all these pieces come together, execute this in the command bar:\n\ndbt docs generate\n\nThis will generate the documentation for your project. Click the book button, as shown in the screenshot below to access the docs.\n\ndbt docs book icon\n\nGo to our project area and view int_results. View the description that we created in our doc block.\n\nDocblock description within docs site\n\nView the mini-lineage that looks at the model we are currently selected on (int_results in this case).\n\nMini lineage view on docs site\n\nIn our dbt_project.yml, we configured node_colors depending on the file directory. By color coding your project, it can help you cluster together similar models or steps and more easily troubleshoot when viewing lineage in your docs.\n\nFull project DAG on docs site\n\nBefore we jump into deploying our code, let's have a quick primer on environments. Up to this point, all of the work we've done in the dbt Cloud IDE has been in our development environment, with code committed to a feature branch and the models we've built created in our development schema in Snowflake as defined in our Development environment connection. Doing this work on a feature branch, allows us to separate our code from what other coworkers are building and code that is already deemed production ready. Building models in a development schema in Snowflake allows us to separate the database objects we might still be modifying and testing from the database objects running production dashboards or other downstream dependencies. Together, the combination of a Git branch and Snowflake database objects form our environment.\n\nNow that we've completed testing and documenting our work, we're ready to deploy our code from our development environment to our production environment and this involves two steps:\n\nPromoting code from our feature branch to the production branch in our repository.\n\nGenerally, the production branch is going to be named your main branch and there's a review process to go through before merging code to the main branch of a repository. Here we are going to merge without review for ease of this workshop.\n\nDeploying code to our production environment.\n\nOnce our code is merged to the main branch, we'll need to run dbt in our production environment to build all of our models and run all of our tests. This will allow us to build production-ready objects into our production environment in Snowflake. Luckily for us, the Partner Connect flow has already created our deployment environment and job to facilitate this step.\n\nBefore getting started, let's make sure that we've committed all of our work to our feature branch. If you still have work to commit, you'll be able to select the Commit and push, provide a message, and then select Commit again.\n\nOnce all of your work is committed, the git workflow button will now appear as Merge to main. Select Merge to main and the merge process will automatically run in the background.\n\nMerge into main\n\nWhen it's completed, you should see the git button read Create branch and the branch you're currently looking at will become main.\n\nNow that all of our development work has been merged to the main branch, we can build our deployment job. Given that our production environment and production job were created automatically for us through Partner Connect, all we need to do here is update some default configurations to meet our needs.\n\nIn the menu, select Deploy > Environments\n\nNavigate to environments within the UI\n\nYou should see two environments listed and you'll want to select the Deployment environment then Settings to modify it.\n\nBefore making any changes, let's touch on what is defined within this environment. The Snowflake connection shows the credentials that dbt Cloud is using for this environment and in our case they are the same as what was created for us through Partner Connect. Our deployment job will build in our PC_DBT_DB database and use the default Partner Connect role and warehouse to do so. The deployment credentials section also uses the info that was created in our Partner Connect job to create the credential connection. However, it is using the same default schema that we've been using as the schema for our development environment.\n\nLet's update the schema to create a new schema specifically for our production environment. Click Edit to allow you to modify the existing field values. Navigate to Deployment Credentials > schema.\n\nUpdate the schema name to production. Remember to select Save after you've made the change.\n\nUpdate the deployment credentials schema to production\n\nBy updating the schema for our production environment to production, it ensures that our deployment job for this environment will build our dbt models in the production schema within the PC_DBT_DB database as defined in the Snowflake Connection section.\n\nNow let's switch over to our production job. Click on the deploy tab again and then select Jobs. You should see an existing and preconfigured Partner Connect Trial Job. Similar to the environment, click on the job, then select Settings to modify it. Let's take a look at the job to understand it before making changes.\n\nThe Environment section is what connects this job with the environment we want it to run in. This job is already defaulted to use the Deployment environment that we just updated and the rest of the settings we can keep as is.\n\nThe Execution settings section gives us the option to generate docs, run source freshness, and defer to a previous run state. For the purposes of our lab, we're going to keep these settings as is as well and stick with just generating docs.\n\nThe Commands section is where we specify exactly which commands we want to run during this job, and we also want to keep this as is. We want our seed to be uploaded first, then run our models, and finally test them. The order of this is important as well, considering that we need our seed to be created before we can run our incremental model, and we need our models to be created before we can test them.\n\nFinally, we have the Triggers section, where we have a number of different options for scheduling our job. Given that our data isn't updating regularly here and we're running this job manually for now, we're also going to leave this section alone.\n\nSo, what are we changing then? Just the name! Click Edit to allow you to make changes. Then update the name of the job to Production Job to denote this as our production deployment job. After that's done, click Save. 12. Now let's go to run our job. Clicking on the job name in the path at the top of the screen will take you back to the job run history page where you'll be able to click Run run to kick off the job. If you encounter any job failures, try running the job again before further troubleshooting.\n\nRun production job\n\nView production job details\n\nLet's go over to Snowflake to confirm that everything built as expected in our production schema. Refresh the database objects in your Snowflake account and you should see the production schema now within our default Partner Connect database. If you click into the schema and everything ran successfully, you should be able to see all of the models we developed.\n\nCheck all our models in our pipeline are in Snowflake\n\nFantastic! You‚Äôve finished the workshop! We hope you feel empowered in using both SQL and Python in your dbt Cloud workflows with Snowflake. Having a reliable pipeline to surface both analytics and machine learning is crucial to creating tangible business value from your data.\n\nFor more help and information join our dbt community Slack which contains more than 50,000 data practitioners today. We have a dedicated slack channel #db-snowflake to Snowflake related content. Happy dbt'ing!\n\n0"
    }
}
{
    "id": "dbpedia_8307_1",
    "rank": 7,
    "data": {
        "url": "https://medium.com/web-mining-is688-spring-2021/assignment-4-5236a8c96167",
        "read_more_link": "",
        "language": "en",
        "title": "Using The Cosine Similarity and DBSCAN to Get Clusters from The Housing Data Set in Python",
        "top_image": "https://miro.medium.com/v2/resize:fit:600/1*fkNEX3KaG85jlbTLdqAATg.png",
        "meta_img": "https://miro.medium.com/v2/resize:fit:600/1*fkNEX3KaG85jlbTLdqAATg.png",
        "images": [
            "https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png",
            "https://miro.medium.com/v2/resize:fill:88:88/1*6GcZ_yNvEUWEReoMhMx5hA.jpeg",
            "https://miro.medium.com/v2/resize:fill:48:48/1*oBn-ROC_CF_pheuqczMWiQ.jpeg",
            "https://miro.medium.com/v2/resize:fill:144:144/1*6GcZ_yNvEUWEReoMhMx5hA.jpeg",
            "https://miro.medium.com/v2/resize:fill:64:64/1*oBn-ROC_CF_pheuqczMWiQ.jpeg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Julia Wu"
        ],
        "publish_date": "2021-04-02T18:12:35.339000+00:00",
        "summary": "",
        "meta_description": "Although I have shown my interest to identify the cheapest home with all the features I love in my first article, I ended up encountering three limitations, only 200 homes, a lack of geospatial data…",
        "meta_lang": "en",
        "meta_favicon": "https://miro.medium.com/v2/5d8de952517e8160e40ef9841c781cdc14a5db313057fa3c3de41c6f5b494b19",
        "meta_site_name": "Medium",
        "canonical_link": "https://medium.com/web-mining-is688-spring-2021/assignment-4-5236a8c96167",
        "text": "Although I have shown my interest to identify the cheapest home with all the features I love in my first article, I ended up encountering three limitations, only 200 homes, a lack of geospatial data of other important facilities, and a lack of a background map. In this article, I am going to continue to explore more data on the web and conquer those limitations. To keep it local, I will be focusing on the homes located in Ocean County, NJ. I am planning to use the cosine similarity as well as DBSCAN algorithm from the SKLearn library in Python to cluster the houses in my new data set. Once I have my clusters, they may help me to find my ideal house more efficiently. Therefore, I am wondering about\n\nHow many clusters are in my data set? And what are the characteristics of each cluster?\n\nCollect the data\n\nThis time I use the “Requests” library from Python to retrieve my home data set from the RapidAPI again, and I finally get around 5,500 records in my home data set.\n\nBug Attention!\n\nI add a for loop in my function to get as many homes with different features as possible and store them in a data frame. I use five keywords that are allowed to use in the “features” column when I run the query multiple times, such as “single_story,” “basement,” and “1_or_more_garage,” etc.\n\nSince the distance from a home to a public school, a fire station, and a police station may be important for some customers, I use the “JSON” library from Python to retrieve those data sets from Homeland Infrastructure Foundation-Level Data (HIFLD), which provides National foundation-level geospatial data within the open public domain. This is to say that now I have the address, coordinates for all public schools, fire stations, and police stations in Ocean County. Besides, I also use the “JSON” library to retrieve the criminal record of each police department in the recent five years from the FBI database. For some reason, the records of 2020 are not available yet, so what I mean by “the recent five years” is from 2015 to 2019.\n\nFurthermore, I download the shapefile of Ocean County from the New Jersey Geographic Information Network (NJGIN) and use the “geopandas” library to read it in Python, so I can visualize the data sets on a map. The shapefile also includes the population and the population density for all municipalities in Ocean County.\n\nBug Attention!\n\nWhen I am trying to install the “geopandas” library in Jupyter to visualize the geospatial data, I get an error message like this “A GDAL API version must be specified.” And I solve the problem by following the solution in this post.\n\nClean up data\n\nAfter I get those data sets, I merge them into a data frame containing 27 columns for around 2,240 unique homes. The columns are consist of “property_id,” “price,” “beds,” “baths,” “size(sqft),” “line,” “city,” “state,” “zipcode,” “longitude,” “latitude,” “days_ago” “features” “P_zip,” “P_latitude,” “P_longitude,” “dist_P (mi.),” “S_zip,” “S_latitude,” “S_longitude,” “dist_S (mi.),” “F_zip,” “F_latitude,” “F_longitude,” “dist_F (mi.),” “crime_rate(%),” and “POPDEN2010 (per sq. mi.).” And then I store this data frame as a CSV file “Final” for the ease of manipulating the data. It is to say that, finally, I have all the important information in one place.\n\nFinal = pd.read_csv('./Final.csv')\n\nBug Attention!\n\nWhen I check the home data set, I find it has four issues. First, a home may have more than one entry if it has different prices. Second, a home may have more than one entry with different coordinates. Third, a home may have more than one entry if it has more than one feature. Fourth, the above issues may cause that a home has multiple IDs. Since a home may have multiple IDs and coordinates in this data set, I cannot simply use them as primary keys to merge multiple entries of a home.\n\nTo solve these issues, first, I add a column “days_ago” in the data frame to calculate how many days ago each record was posted.\n\nSecond, I apply the “groupby.agg()” method to get a data frame containing the latest record with the lowest price if multiple records have the same address, city, and feature.\n\nThird, I apply the “groupby.agg()” method to the new data frame to combine multiple features if multiple records have the same address and city. By now, I have the other data frame containing around 2,240 unique homes with one or more features.\n\nFourth, I merge two new data frames, keep the one with the lowest price as well as drop all duplicates to get another new data set.\n\nFifth, I right-merge the original home and the third new data sets and drop all duplicates again to get the clean home data set.\n\nBug Attention!\n\nI also find there is no numerical unique foreigner key to join some data sets, which are crime, police data sets as well as the Ocean County shapefile when I am trying to add a new column “crime_rate(%)” to calculate the crime rate for each municipality in Ocean County. The police and Ocean County shapefile data sets contain a column for the municipality, but they do not apply the same naming rule. The crime data set only contains the full name of the police department. For example, “Toms River” may be shown as “Toms River Township,” “Toms River Police Department,” or “Toms River Township Police Department” in different data sets. Therefore, the typical “join” method cannot be simply applied to this case since it only joins different data sets if they have the same string in their column for joining.\n\nTo join these data sets, I import a library “FuzzyWuzzy” in Python for approximate string matching. FuzzyWuzzy helps me find a string that matches a given pattern by utilizing a string metric Levenshtein Distance to calculate the differences between sequences or phrases. I crossjoin the crime and police datasets and calculate their scores and ranks for the “token set ratio” as well as the “token sort ratio”, and then I filter out the mismatched ones. The typical criteria for the token set ratio are 70 for the score and 1 for the rank. However, from my observation, to get the best result, I have to add other filter criteria on the score of the token sort ratio. Now I have the crime_police data set by joining the police and crime data sets to get the coordinates and criminal records for each municipal police department.\n\nThen I use the same technique to join the crime_police data set as well as the Ocean County shapefile and set the criteria for the token set ratio are 60 for the score and 1 for the rank based on my observation. Finally, I can calculate the approximate crime rate for each municipality, the case number from 2015 to 2019 divided by five years, and by the population in 2010 from the crime data set and the Ocean County shapefile accordingly.\n\nFeature extraction\n\nMy feature columns are “beds,” “baths,” “size(sqft),” “longitude,” “latitude,” “dist_P (mi.),” “dist_S (mi.),” “dist_F (mi.),” “crime_rate(%),” “POPDEN2010 (per sq. mi.),” and “features.” The column “features” is the only one text type. Since they are ten numerical types and one text type, I use the ColumnTransformer() method to contain different encoders. The MinMaxScaler is for my ten numerical columns, and the TfidfVectorizer is for my text column “features.” The two encoders normalize my feature columns. Now I am using the fit_tranform() method to standardize and normalize my data set.\n\nCosine similarity\n\nI use the cosine similarity from the “SKLearn” library to calculate the similarity between all homes in my “Final” data set. The concept is to measure the cosine of the angle between two vectors projected in a multi-dimensional space, and the cosine would be between 1 and 0.\n\ncos_sim = cosine_similarity(token)\n\nDBSCAN\n\nI use the DBSCAN algorithm from the “SKLearn” library to help me cluster the homes based on their score in the cosine similarity. the DBSCAN algorithm does not have to give a pre-defined “k” value since it automatically clusters the homes based on the values that I assign in the “eps” and “min_sample.” I set my “min_sample” as 30 since the “min_sample” is usually twice the number of the feature columns and a total of my feature columns is fifteen after scaling. I keep trying to assign different values in the “eps” and visualize the clusters in the following step. If the value of the “eps” is greater than 0.01, I will get only one cluster. If the value of the “eps” is less than 0.01, I may get more than two clusters and 100 outliers. Therefore, the appropriate value of the “eps” may be 0.01 based on my observation, and then I get two clusters and around 20 outliers. This is to say my “k” value is 2.\n\nI make a copy of my “Final” data set with all the feature columns and add a new column “cluster” to the copy, so now each unique home either belongs to a cluster or is labeled as an outlier.\n\nComparisons of clusters\n\nTo get an overview of the importance of each feature column to different clusters, I use the “seaborn” library to make a pairing grid of ten numerical feature columns. I find that the crime rate and population density may not contribute to distinguishing the two clusters.\n\nThen I use the “seaborn” library to make a horizontal bar chart for the “features” column. Interestingly, most homes containing the feature “single_story” belong to cluster 0 whereas most homes containing the feature “two_or_more_stories” belong to cluster 1.\n\nCharacteristics of clusters\n\nSince I find “beds,” “baths,” and “size(sqft)” seem to be able to distinguish the differences between the two clusters, I make the horizontal bar charts for each with the more detailed X-axis and Y-axis tickers. From the figures, I learned that most homes having three or fewer bedrooms and two or fewer bathrooms belong to cluster 0 whereas most homes having three or more bedrooms and bathrooms belong to cluster 1. The size of most homes in cluster 0 is around 1,250 square feet whereas the size of most homes in cluster 1 is around 2,500 square feet.\n\nThen I make a pairing grid for only the three feature columns again for ease of reading. From the figure, I learned any two of them may have a positive correlation in terms of clusters. It also demonstrates that most homes in cluster 0 tend to be smaller and have fewer bedrooms as well as bathrooms whereas most homes in cluster 1 tend to be larger and have more bedrooms as well as bathrooms.\n\nBesides, I make horizontal bar charts for the distance between a home and its nearest public school, fire station, and police station to see if they are significant in distinguishing the two clusters. From the figures, I learned that the distance between most homes in cluster 0 and their nearest public school is around 1 mile whereas that the distance between most homes in cluster 1 and their nearest public school is around 0.5 miles. The distance between most homes in cluster 0 and their nearest fire station is around 1 mile whereas that the distance between most homes in cluster 1 and their nearest fire station is around 0.5 miles.\n\nFurthermore, I also visualize the homes on the map to see if the coordinates make difference between the two clusters. I find most homes located in the center of Ocean County belong to cluster 0 whereas most homes located in the northeast coast of Ocean County belong to cluster 1."
    }
}
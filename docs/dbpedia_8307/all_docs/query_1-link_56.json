{
    "id": "dbpedia_8307_1",
    "rank": 56,
    "data": {
        "url": "https://pypi.org/project/symbolicai/0.2.22/",
        "read_more_link": "",
        "language": "en",
        "title": "symbolicai",
        "top_image": "https://pypi.org/static/images/twitter.abaf4b19.webp",
        "meta_img": "https://pypi.org/static/images/twitter.abaf4b19.webp",
        "images": [
            "https://pypi.org/static/images/logo-small.8998e9d1.svg",
            "https://pypi-camo.freetls.fastly.net/74e19ec56eab7a3a15b25917f7c3da766179a448/68747470733a2f2f7365637572652e67726176617461722e636f6d2f6176617461722f62306233653338393432333630646663373332396239656234313831666133643f73697a653d3530",
            "https://pypi-camo.freetls.fastly.net/34b9ef0a38d2afcda3fac7f8decc2f15028c0438/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f58706974666972652f73796d626f6c696361692f6d61696e2f6173736574732f696d616765732f73796d61695f6c6f676f2e706e67",
            "https://pypi-camo.freetls.fastly.net/08f2af4c4fa71b23f5990413f97baee8ec8fb6b7/68747470733a2f2f62616467652e667572792e696f2f70792f73796d626f6c696361692e737667",
            "https://pypi-camo.freetls.fastly.net/3677cc2d00014b87886b24c89fb398ba1f388d81/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4253445f332d2d436c617573652d626c75652e737667",
            "https://pypi-camo.freetls.fastly.net/64d707a9fcebd1c17e9a45f081caaa78b79df6fb/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f75726c2f68747470732f747769747465722e636f6d2f64696e756d6172697573632e7376673f7374796c653d736f6369616c266c6162656c3d466f6c6c6f7725323025343044696e754d617269757343",
            "https://pypi-camo.freetls.fastly.net/6611020148fcb2d5dc1b996ed3e5bcbfb182e1f0/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f75726c2f68747470732f747769747465722e636f6d2f73796d626f6c69636170692e7376673f7374796c653d736f6369616c266c6162656c3d466f6c6c6f7725323025343053796d626f6c69634149",
            "https://pypi-camo.freetls.fastly.net/793ef3fadef48113e66f3c652ebc9b591bcb1745/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f6e747269627574696f6e732d77656c636f6d652d627269676874677265656e2e7376673f7374796c653d666c6174",
            "https://pypi-camo.freetls.fastly.net/a986de780eafeab766b0f346db1a17bbe089f882/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f3736383038373136313837383038353634333f6c6162656c3d446973636f7264266c6f676f3d446973636f7264266c6f676f436f6c6f723d7768697465",
            "https://pypi-camo.freetls.fastly.net/6ff00ee97a01a218ca1f1ef761a889507298eb4f/68747470733a2f2f686974732e736565796f756661726d2e636f6d2f6170692f636f756e742f696e63722f62616467652e7376673f75726c3d68747470732533412532462532466769746875622e636f6d253246587069746669726525324673796d626f6c6963616926636f756e745f62673d253233373943383344267469746c655f62673d2532333535353535352669636f6e3d2669636f6e5f636f6c6f723d253233453745374537267469746c653d6869747326656467655f666c61743d66616c7365",
            "https://pypi-camo.freetls.fastly.net/1abe73af6f5e671b8983615425f85d554d10f166/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f58706974666972652f73796d626f6c696361692e7376673f7374796c653d736f6369616c266c6162656c3d466f726b266d61784167653d32353932303030",
            "https://pypi-camo.freetls.fastly.net/5c6158b718a18f26f7a1eb9a969a8de8be618bfe/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f58706974666972652f73796d626f6c696361692e7376673f7374796c653d736f6369616c266c6162656c3d53746172266d61784167653d32353932303030",
            "https://pypi-camo.freetls.fastly.net/03b7a769011402039e42b116a386b545995acc03/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f58706974666972652f73796d626f6c696361692f6d61696e2f6173736574732f696d616765732f707265766965772e676966",
            "https://pypi-camo.freetls.fastly.net/6a24a23840f2733a65afe2e04034826869417f0a/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f58706974666972652f73796d626f6c696361692f6d61696e2f6173736574732f696d616765732f696d67352e706e67",
            "https://pypi-camo.freetls.fastly.net/c5ba20d8916d0375fff45d8260c37d4541439190/68747470733a2f2f6d656469612e67697068792e636f6d2f6d656469612f6d47634e6a736657416a593541455a4e77362f67697068792e676966",
            "https://pypi-camo.freetls.fastly.net/1e7f5409863b622e86bb67fa8cb08501b467b71c/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f58706974666972652f73796d626f6c696361692f6d61696e2f6173736574732f696d616765732f696d67312e706e67",
            "https://pypi-camo.freetls.fastly.net/92988fa66a2164f042c202cfa866e1c883543f50/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f58706974666972652f73796d626f6c696361692f6d61696e2f6173736574732f696d616765732f696d67372e706e67",
            "https://pypi-camo.freetls.fastly.net/dca06f699f64cc2eb55da914d2540639fbc02db5/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f58706974666972652f73796d626f6c696361692f6d61696e2f6173736574732f696d616765732f696d6731302e706e67",
            "https://pypi-camo.freetls.fastly.net/0ab712067c44ebb54cc75564a3044a748f90de4f/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f58706974666972652f73796d626f6c696361692f6d61696e2f6173736574732f696d616765732f696d67332e706e67",
            "https://pypi-camo.freetls.fastly.net/c3f970038a8cef2d24a0716e62fc33622d9acddf/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f58706974666972652f73796d626f6c696361692f6d61696e2f6173736574732f696d616765732f696d67392e706e67",
            "https://pypi-camo.freetls.fastly.net/9dfa8839774956e7e31ac0204206c37e07d770b1/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f58706974666972652f73796d626f6c696361692f6d61696e2f6173736574732f696d616765732f696d67342e706e67",
            "https://pypi-camo.freetls.fastly.net/4f80eb5aeb2649df8c7c7a4ca1a4c50a8c33578e/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f58706974666972652f73796d626f6c696361692f6d61696e2f6173736574732f696d616765732f696d67322e706e67",
            "https://pypi-camo.freetls.fastly.net/3fb9632a744b7702a423c73c2e1d8d6497b430bb/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f58706974666972652f73796d626f6c696361692f6d61696e2f6173736574732f696d616765732f696d67362e706e67",
            "https://pypi-camo.freetls.fastly.net/e82d79816045cb4c4fbb1ba16bd07099beca0271/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f58706974666972652f73796d626f6c696361692f6d61696e2f6173736574732f696d616765732f696d67382e706e67",
            "https://pypi-camo.freetls.fastly.net/b1f30912b4d38ccb12a76f8bd603e02620b4d570/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f58706974666972652f73796d626f6c696361692f6d61696e2f6578616d706c65732f726573756c74732f6e6577735f707265762e706e67",
            "https://pypi-camo.freetls.fastly.net/85e91bbb928104e4ce317951541520c6b9c170e1/68747470733a2f2f6d7962696e6465722e6f72672f62616467655f6c6f676f2e737667",
            "https://pypi-camo.freetls.fastly.net/38a280cb35586f6a6e1c5e154967057459e543cc/6173736574732f696d616765732f6361742e6a7067",
            "https://pypi-camo.freetls.fastly.net/1c6b48d4ac9b582dbc400aa2bd53b809f2f2b394/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446f6e6174652d50617950616c2d677265656e2e737667",
            "https://pypi-camo.freetls.fastly.net/a986de780eafeab766b0f346db1a17bbe089f882/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f3736383038373136313837383038353634333f6c6162656c3d446973636f7264266c6f676f3d446973636f7264266c6f676f436f6c6f723d7768697465",
            "https://pypi-camo.freetls.fastly.net/74e19ec56eab7a3a15b25917f7c3da766179a448/68747470733a2f2f7365637572652e67726176617461722e636f6d2f6176617461722f62306233653338393432333630646663373332396239656234313831666133643f73697a653d3530",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/blue-cube.572a5bfb.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi.org/static/images/white-cube.2351a86c.svg",
            "https://pypi-camo.freetls.fastly.net/ed7074cadad1a06f56bc520ad9bd3e00d0704c5b/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f707970692d6173736574732f73706f6e736f726c6f676f732f6177732d77686974652d6c6f676f2d7443615473387a432e706e67",
            "https://pypi-camo.freetls.fastly.net/8855f7c063a3bdb5b0ce8d91bfc50cf851cc5c51/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f707970692d6173736574732f73706f6e736f726c6f676f732f64617461646f672d77686974652d6c6f676f2d6668644c4e666c6f2e706e67",
            "https://pypi-camo.freetls.fastly.net/df6fe8829cbff2d7f668d98571df1fd011f36192/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f707970692d6173736574732f73706f6e736f726c6f676f732f666173746c792d77686974652d6c6f676f2d65684d3077735f6f2e706e67",
            "https://pypi-camo.freetls.fastly.net/420cc8cf360bac879e24c923b2f50ba7d1314fb0/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f707970692d6173736574732f73706f6e736f726c6f676f732f676f6f676c652d77686974652d6c6f676f2d616734424e3774332e706e67",
            "https://pypi-camo.freetls.fastly.net/524d1ce72f7772294ca4c1fe05d21dec8fa3f8ea/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f707970692d6173736574732f73706f6e736f726c6f676f732f6d6963726f736f66742d77686974652d6c6f676f2d5a443172685444462e706e67",
            "https://pypi-camo.freetls.fastly.net/d01053c02f3a626b73ffcb06b96367fdbbf9e230/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f707970692d6173736574732f73706f6e736f726c6f676f732f70696e67646f6d2d77686974652d6c6f676f2d67355831547546362e706e67",
            "https://pypi-camo.freetls.fastly.net/67af7117035e2345bacb5a82e9aa8b5b3e70701d/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f707970692d6173736574732f73706f6e736f726c6f676f732f73656e7472792d77686974652d6c6f676f2d4a2d6b64742d706e2e706e67",
            "https://pypi-camo.freetls.fastly.net/b611884ff90435a0575dbab7d9b0d3e60f136466/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f707970692d6173736574732f73706f6e736f726c6f676f732f737461747573706167652d77686974652d6c6f676f2d5467476c6a4a2d502e706e67"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2023-06-11T22:53:55+00:00",
        "summary": "",
        "meta_description": "A Neuro-Symbolic Framework for Python",
        "meta_lang": "en",
        "meta_favicon": "/static/images/favicon.35549fe8.ico",
        "meta_site_name": "PyPI",
        "canonical_link": "https://pypi.org/project/symbolicai/",
        "text": "A Neuro-Symbolic Perspective on Large Language Models (LLMs)\n\nBuilding applications with LLMs at its core through our Symbolic API leverages the power of classical and differentiable programming in Python.\n\nRead further documentation here.\n\nAbstract\n\nConceptually, SymbolicAI is a framework that uses machine learning - and specifically LLMs - at its core, and composes operations based on task-specific prompting. We adopt a divide and conquer approach to decompose a complex problem into smaller problems. Therefore, each operation solves a simple task. By re-combining these operations we can solve the complex problem. Furthermore, our design principles allow us to transition between differentiable and classical programming, and to leverage the power of both worlds.\n\n📖 Table of Contents\n\nSymbolicAI\n\nA Neuro-Symbolic Perspective on Large Language Models (LLMs)\n\nAbstract\n\n📖 Table of Contents\n\n🔧 Get Started\n\n➡️ Quick Install\n\nAPI Keys\n\n[Optional] Installs\n\n🦖 Apps\n\nShell Command Tool\n\nChatbot\n\n💯 Other Use Cases\n\nCommunity demos\n\n🤷‍♂️ Why SymbolicAI?\n\nTell me some more fun facts!\n\n😶‍🌫️ How does it work?\n\n📚 Symbolic operations\n\nRanking objects\n\nEvaluating Expressions by best effort\n\nDynamic casting\n\nFuzzy Comparisons\n\n🧠 Causal Reasoning\n\n😷 Operations\n\n🧪 Custom Operations\n\nFew-shot operations\n\nPrompt Design\n\n😑 Expressions\n\nSequence expressions\n\nStream expressions\n\n❌ Error Handling\n\n🕷️ Interpretability, Testing & Debugging\n\nUnit Testing Models\n\n🔥Debugging\n\nExample: News Summary\n\n▶️ Play around with our API\n\n📈 Interface for Query and Response Inspection\n\n🤖 Engines\n\nSymbolic Engine\n\nSpeech Engine\n\nOCR Engine\n\nSearch Engine\n\nWebCrawler Engine\n\nDrawing Engine\n\nFile Engine\n\nIndexing Engine\n\nCLIP Engine\n\nLocal Neuro-Symbolic Engine\n\nCustom Engine\n\n⚡Limitations\n\n🥠 Future Work\n\nConclusion\n\n👥 References, Related Work & Credits\n\nComparison to other frameworks\n\nAcknowledgements\n\nContribution\n\n📜 Citation\n\n📝 License\n\nLike this project?\n\n📫 Contact\n\n🔧 Get Started\n\n➡️ Quick Install\n\npip install symbolicai\n\nOne can run our framework in two ways:\n\nusing local engines (experimental) that are run on your local machine (see Local Neuro-Symbolic Engine section), or\n\nusing engines powered by external APIs, i.e. using OpenAI's API (see API Keys).\n\nAPI Keys\n\nBefore the first run, define exports for the required API keys to enable the respective engines. This will register the keys in the internally for subsequent runs. By default SymbolicAI currently uses OpenAI's neural engines, i.e. GPT-3 Davinci-003, DALL·E 2 and Embedding Ada-002, for the neuro-symbolic computations, image generation and embeddings computation respectively. However, these modules can easily be replaced with open-source alternatives. Examples are\n\nOPT or Bloom for neuro-symbolic computations,\n\nCraiyon for image generation,\n\nand any BERT variants for semantic embedding computations.\n\nTo set the OpenAI API Keys use the following command:\n\n# Linux / MacOS exportOPENAI_API_KEY=\"<OPENAI_API_KEY>\" # Windows (PowerShell) $Env:OPENAI_API_KEY=\"<OPENAI_API_KEY>\" # Jupyter Notebooks (important: do not use quotes) %envOPENAI_API_KEY=<OPENAI_API_KEY>\n\nTo get started import our library by using:\n\nimport symai as ai\n\nOverall, the following engines are currently supported:\n\nNeuro-Symbolic Engine: OpenAI's LLMs (GPT-3)\n\nEmbedding Engine: OpenAI's Embedding API\n\n[Optional] Symbolic Engine: WolframAlpha\n\n[Optional] Search Engine: SerpApi\n\n[Optional] OCR Engine: APILayer\n\n[Optional] SpeechToText Engine: OpenAI's Whisper\n\n[Optional] WebCrawler Engine: Selenium\n\n[Optional] Image Rendering Engine: DALL·E 2\n\n[Optional] Indexing Engine: Pinecone\n\n[Optional] CLIP Engine: 🤗 Hugging Face (experimental image and text embeddings)\n\n[Optional] Installs\n\nSymbolicAI uses multiple engines to process text, speech and images. We also include search engine access to retrieve information from the web. To use all of them, you will need to install also the following dependencies or assign the API keys to the respective engines.\n\nIf you want to use the WolframAlpha Engine, Search Engine or OCR Engine you will need to export the following API keys:\n\n# Linux / MacOS exportSYMBOLIC_ENGINE_API_KEY=\"<WOLFRAMALPHA_API_KEY>\" exportSEARCH_ENGINE_API_KEY=\"<SERP_API_KEY>\" exportOCR_ENGINE_API_KEY=\"<APILAYER_API_KEY>\" exportINDEXING_ENGINE_API_KEY=\"<PINECONE_API_KEY>\" # Windows (PowerShell) $Env:SYMBOLIC_ENGINE_API_KEY=\"<WOLFRAMALPHA_API_KEY>\" $Env:SEARCH_ENGINE_API_KEY=\"<SERP_API_KEY>\" $Env:OCR_ENGINE_API_KEY=\"<APILAYER_API_KEY>\" $Env:INDEXING_ENGINE_API_KEY=\"<PINECONE_API_KEY>\"\n\nTo use them, you will also need to install the following dependencies:\n\nSpeechToText Engine: ffmpeg for audio processing (based on OpenAI's whisper)\n\n# Linux sudo apt update&& sudo apt install ffmpeg # MacOS brew install ffmpeg # Windows choco install ffmpeg\n\n[Note] Additionally, you need to install the newest version directly from their repository, since the version available via pip is outdated:\n\npip install git+https://github.com/openai/whisper.git\n\nWebCrawler Engine: For selenium, we automatically install the driver with chromedriver-autoinstaller. Currently we only support Chrome as the default browser.\n\nAlternatively, you can specify in your project path a symai.config.json file with all the engine properties. This will replace the environment variables. See the following configuration file as an example:\n\n{ \"NEUROSYMBOLIC_ENGINE_API_KEY\":\"<OPENAI_API_KEY>\", \"NEUROSYMBOLIC_ENGINE_MODEL\":\"text-davinci-003\", \"SYMBOLIC_ENGINE_API_KEY\":\"<WOLFRAMALPHA_API_KEY>\", \"EMBEDDING_ENGINE_API_KEY\":\"<OPENAI_API_KEY>\", \"EMBEDDING_ENGINE_MODEL\":\"text-embedding-ada-002\", \"IMAGERENDERING_ENGINE_API_KEY\":\"<OPENAI_API_KEY>\", \"VISION_ENGINE_MODEL\":\"openai/clip-vit-base-patch32\", \"SEARCH_ENGINE_API_KEY\":\"<SERP_API_KEY>\", \"SEARCH_ENGINE_MODEL\":\"google\", \"OCR_ENGINE_API_KEY\":\"<APILAYER_API_KEY>\", \"SPEECH_ENGINE_MODEL\":\"base\", \"INDEXING_ENGINE_API_KEY\":\"<PINECONE_API_KEY>\", \"INDEXING_ENGINE_ENVIRONMENT\":\"us-west1-gcp\" }\n\n🦖 Apps\n\nOver the course of th next weeks, we will expand our experimental demo apps and provide a set of useful tools that showcase how to interact with our framework. These apps are made available by calling the sym+<shortcut-name-of-app> command in your terminal or PowerShell.\n\nShell Command Tool\n\nYou can start a basic shell command support tool that translates natural language commands into shell commands. To start the shell command tool, simply run:\n\nsymsh\"<your-query>\"\n\nYou can also use the --help flag to get more information about the tool and available arguments.\n\nsymsh --help\n\nHere is an example of how to use the tool:\n\n$> symsh\"PowerShell edit registiry entry\" # :Output: # Set-ItemProperty -Path <path> -Name <name> -Value <value> $> symsh\"Set-ItemProperty -Path <path> -Name <name> -Value <value>\" --add\"path='/Users/myuser' name=Demo value=SymbolicAI\" # :Output: # Set-ItemProperty -Path '/Users/myuser' -Name Demo -Value SymbolicAI $> symsh\"Set-ItemProperty -Path '/Users/myuser' -Name Demo -Value SymbolicAI\" --del\"string quotes\" # :Output: # Set-ItemProperty -Path /Users/myuser -Name Demo -Value SymbolicAI $> symsh\"Set-ItemProperty -Path '/Users/myuser' -Name Demo -Value SymbolicAI\" --convert\"linux\" # :Output: # export Demo=\"SymbolicAI\"\n\nChatbot\n\nYou can start a basic conversation with Symbia. Symbia is a chatbot that uses SymbolicAI to detect the content of your request and switch between different contextual modes to answer your questions. These mode include search engines, speech engines and more. To start the chatbot, simply run:\n\nsymchat\n\nThis will start now a chatbot interface:\n\nSymbia: Hi there! I'm Symbia, your virtual assistant. How may Ihelp you? $>\n\nYou can exit the conversation by either typing exit, quit or pressing Ctrl+C.\n\n💯 Other Use Cases\n\nWe compiled a few examples to show how to use our Symbolic API. You can find them in the notebooks folder.\n\nBasics: See our basics notebook to get familiar with our API structure (notebooks/Basics.ipynb)\n\nQueries: See our query manipulation notebook for contextualized operations (notebooks/Queries.ipynb)\n\nNews & Docs Generation: See our news and documentation generation notebook for stream processing (notebooks/News.ipynb)\n\nChatBot: See how to implement a custom chatbot based on semantic narrations (notebooks/ChatBot.ipynb)\n\nYou can solve many more problems with our Symbolic API. We are looking forward to see what you will build with it. Keep us posted on our shared community space on Discord: AI Is All You Need / SymbolicAI.\n\nCommunity demos\n\nWe are listing all your cool demos and tools that you build with our framework. If you want to add your project just PM on Twitter at @SymbolicAPI or via Discord.\n\n🤷‍♂️ Why SymbolicAI?\n\nSymbolicAI tries to close the gap between classical programming or Software 1.0 and modern data-driven programming (aka Software 2.0). It is a framework that allows to build software applications, which are able to utilize the power of large language models (LLMs) wtih composability and inheritance - two powerful concepts from the object-oriented classical programming paradigm.\n\nThis allows to move along the spectrum between the classical programming realm and data-driven programming realm as illustrated in the following figure:\n\nAs briefly mentioned, we adopt a divide and conquer approach to decompose a complex problem into smaller problems. We then use the expressiveness and flexibility of LLMs to evaluate these sub-problems and by re-combining these operations we can solve the complex problem.\n\nIn this turn, and with enough data, we can gradually transition between general purpose LLMs with zero and few-shot learning capabilities, and specialized fine-tuned models to solve specific problems (see above). This means that each operations could be designed to use a model with fine-tuned task-specific behavior.\n\nTell me some more fun facts!\n\nIn its essence, SymbolicAI was inspired by the neuro-symbolic programming paradigm.\n\nNeuro-symbolic programming is a paradigm for artificial intelligence and cognitive computing that combines the strengths of both deep neural networks and symbolic reasoning.\n\nDeep neural networks are a type of machine learning algorithms that are inspired by the structure and function of biological neural networks. They are particularly good at tasks such as image recognition, natural language processing etc. However, they are not as good at tasks that require explicit reasoning, such as long-term planning, problem solving, and understanding causal relationships.\n\nSymbolic reasoning, on the other hand uses formal languages and logical rules to represent knowledge and perform tasks such as planning, problem solving, and understanding causal relationships. Symbolic reasoning systems are good at tasks that require explicit reasoning, but are not as good at tasks that require pattern recognition or generalization, such as image recognition or natural language processing.\n\nNeuro-symbolic programming aims to combine the strengths of both neural networks and symbolic reasoning to create AI systems that can perform a wide range of tasks. One way this is done is by using neural networks to extract information from data and then using symbolic reasoning to make inferences and decisions based on that information. Another way is to use symbolic reasoning to guide the generative process of neural networks and make them more interpretable.\n\nEmbedded accelerators for LLMs will, in our opinion, be ubiquitous in future computation platforms, such as wearables, smartphones, tablets or notebooks. They will contain models similar to GPT-3, ChatGPT, OPT or Bloom.\n\nThese LLMs will be able to perform a wide range of computations, such as natural language understanding or decision making. Furthermore, neuro-symbolic computation engines will be able to learn concepts how to tackle unseen tasks and solve complex problems by querying various data sources for solutions and executing logical statements on top. In this turn, to ensure the generated content is in alignment with our goals, we need to develop ways to instruct, steer and control the generative processes of machine learning models. Therefore, our approach is an attempt to enable active and transparent flow control of these generative processes.\n\nAs shown in the figure above, one can think of this generative process as shifting a probability mass of an input stream of data towards an output stream of data, in a contextualized manner. With properly designed conditions and expressions, one can also validate and steer the behavior towards a desired outcome, or repeat expressions that failed to fulfil our requirements. Our approach is to define a set of fuzzy operations that manipulate the data stream and conditions the LLMs to align with our goals. In essence, we consider all data objects, such as strings, letters, integers, arrays, etc. as symbols and we see natural language as the main interface to interact with. See the following figure:\n\nWe show that as long as we can express our goals in natural language, we can use the power of LLMs for neuro-symbolic computations. In this turn, we create operations that manipulate these symbols to generate new symbols from them. Each symbol can be interpreted as a statement. Multiple statements can be combined to form a logical expression.\n\nTherefore, by chaining statements together we can build causal relationships and computations, instead of relying only on inductive approaches. Consequently, the outlook towards an updated computational stack resembles a neuro-symbolic computation engine at its core and, in combination with established frameworks, enables new applications.\n\n😶‍🌫️ How does it work?\n\nWe now show how we define our Symbolic API, which is based on object-oriented and compositional design patterns. The Symbol class is the base class for all functional operations, which we refer to as a terminal symbol in the context of symbolic programming (fully resolved expressions). The Symbol class holds helpful operations that can be interpreted as expressions to manipulate its content and evaluate to new Symbols.\n\n📚 Symbolic operations\n\nLet us now define a Symbol and perform some basic manipulations. We start with a translation operation:\n\nsym = ai.Symbol(\"Welcome to our tutorial.\") sym.translate('German')\n\n:[Output]: <class'symai.expressions.Symbol'>(value=Willkommen zu unserem Tutorial.)\n\nRanking objects\n\nOur API can also perform basic data-agnostic operations to filter, rank or extract patterns. For example, we can rank a list of numbers:\n\nsym = ai.Symbol(numpy.array([1, 2, 3, 4, 5, 6, 7])) res = sym.rank(measure='numerical', order='descending')\n\n:[Output]: <class'symai.expressions.Symbol'>(value=['7','6','5','4','3','2','1'])\n\nEvaluating Expressions by best effort\n\nAs an inspiration, we relate to an approach demonstrated by word2vec.\n\nWord2Vec generates dense vector representations of words by training a shallow neural network to predict a word given its neighbors in a text corpus. The resulting vectors are then used in a wide range of natural language processing applications, such as sentiment analysis, text classification, and clustering.\n\nBelow we can see an example how one can perform operations on the word embeddings (colored boxes). The words are tokenized and mapped to a vector space, where we can perform semantic operations via vector arithmetics.\n\nSimilar to word2vec we intend to perform contextualized operations on different symbols, however, instead of operating in the vector space, we operate in the natural language domain. This gives us the ability to perform arithmetics on words, sentences, paragraphs, etc. and verify the results in a human readable format.\n\nThe following examples show how to evaluate such an expression via a string representation:\n\nai.Symbol('King - Man + Women').expression()\n\n:[Output]: <class'symai.expressions.Symbol'>(value=Queen)\n\nDynamic casting\n\nWe can also subtract sentences from each other, where our operations condition the neural computation engine to evaluate the Symbols by best effort. In the following example, it determines that the word enemy is present in the sentence, therefore deletes it and replaces it with the word friend (which is added):\n\nres = ai.Symbol('Hello my enemy') - 'enemy' + 'friend'\n\n:[Output]: <class'symai.expressions.Symbol'>(value=Hello my friend)\n\nWhat we also see is that the API performs dynamic casting, when data types are combined with a Symbol object. If an overloaded operation of the Symbol class is used, the Symbol class can automatically cast the second object to a Symbol. This is a convenient modality to perform operations between Symbolobjects and other types of data, such as strings, integers, floats, lists, etc. without bloating the syntax.\n\nFuzzy Comparisons\n\nIn this example we are fuzzily comparing two number objects, where the Symbol variant is only an approximation of numpy.pi. Given the context of the fuzzy equals == operation, this comparison still succeeds and returns True.\n\nsym = ai.Symbol('3.1415...') sym == numpy.pi\n\n:[Output]: True\n\n🧠 Causal Reasoning\n\nOur framework was built with the intention to enable reasoning capabilities on top of statistical inference of LLMs. Therefore, we can also perform deductive reasoning operations with our Symbol objects. For example, we can define a set of operations with rules that define the causal relationship between two symbols. The following example shows how the & is used to compute the logical implication of two symbols.\n\nres = ai.Symbol('The horn only sounds on Sundays.') & ai.Symbol('I hear the horn.')\n\n:[Output]: <class'symai.expressions.Symbol'>(value=It is Sunday.)\n\nThe current &-operation overloads the and logical operator and sends few-shot prompts how to evaluate the statement to the neural computation engine. However, we can define more sophisticated logical operators for and, or and xor via formal proof statements and use the neural engines to parse data structures prior to our expression evaluation. Therefore, one can also define custom operations to perform more complex and robust logical operations, including constraints to validate the outcomes and ensure a desired behavior.\n\nTo provide a more complete picture, we also sketch more comprehensive causal examples below, where one tries to obtain logical answers, based on questions of the kind:\n\n# 1) \"A line parallel to y = 4x + 6 passes through (5, 10). What is the y-coordinate of the point where this line crosses the y-axis?\" # 2) \"Bob has two sons, John and Jay. Jay has one brother and father. The father has two sons. Jay's brother has a brother and a father. Who is Jay's brother.\" # 3) \"is 1000 bigger than 1063.472?\"\n\nTo give an rough idea of how we would approach this with our framework is by, first, using a chain of operations to detect the neural engine that is best suited to handle this task, and second, prepare the input for the respective engine. Let's see an example:\n\nval = \"<one of the examples above>\" # First define a class that inherits from the Expression class class ComplexExpression(ai.Expression): # more to the Expression class in later sections # write a method that returns the causal evaluation def causal_expression(self): pass # see below for implementation # instantiate an object of the class expr = ComplexExpression(val) # set WolframAlpha as the main expression engine to use Expression.command(engines=['symbolic'], expression_engine='wolframalpha') # evaluate the expression res = expr.causal_expression()\n\nNow, the implementation of causal_expression could in principle look like this:\n\ndef causal_expression(self): # very which case to use `self.value` contains the input if self.isinstanceof('mathematics'): # get the mathematical formula formula = self.extract('mathematical formula') # verify which problem type we have if formula.isinstanceof('linear function'): # prepare for wolframalpha question = self.extract('question sentence') req = question.extract('what is requested?') x = self.extract('coordinate point (.,.)') # get coordinate point / could also ask for other points query = formula @ f', point x = {x}' @ f', solve {req}' # concatenate to the question and formula res = query.expression(query) # send prepared query to wolframalpha elif formula.isinstanceof('number comparison'): res = formula.expression() # send directly to wolframalpha ... # more cases elif self.isinstanceof('linguistic problem'): sentences = self / '.' # first split into sentences graph = {} # define graph for s in sentences: sym = ai.Symbol(s) relations = sym.extract('connected entities (e.g. A has three B => A | A: three B)') / '|' # and split by pipe for r in relations: ... # add relations and populate graph => alternatively, read also about CycleGT ... # more cases return res\n\nThe above example shows how we can use the causal_expression expression method to step-wise iterate and extract information which we can then either manually or using external solvers resolve.\n\nAttention: We hint the reader that this is a very rough sketch and that the implementation of the causal_expression method would need much more engineering effort. Furthermore, the currently used GPT-3 LLM backend often fails to extract the correct information or resolve the right comparison. However, we strongly believe in the advances of the field and that this will change in the future, specifically with fine-tuned models like ChatGPT with Reinforcement Learning from Human Feedback (RLHF).\n\nLastly, it is also noteworthy that given enough data, we could fine-tune methods that extract information or build our knowledge graph from natural language. This would enable us to perform more complex reasoning tasks, such as the ones mentioned above. Therefore, we also point the reader to recent publications for translating Text-to-Graphs. This means that in the attempt to answer the query, we can simply traverse the graph and extract the information we need.\n\nIn the next section, we will explore operations.\n\n😷 Operations\n\nOperations are at the core of our framework. They are the building blocks of our API and are used to define the behavior of our symbols. We can think of operations as contextualized functions that take in a Symbol object, send it to the neuro-symbolic engine for evaluation, and return one or multiple new objects (mainly new symbols; but not necessarily limited to that). Another fundamental property is polymorphism, which means that operations can be applied to different types of data, such as strings, integers, floats, lists, etc. with different behaviors, depending on the object instance.\n\nThe way we execute operations is by using the Symbol object value attribute containing the original data type that is then sent as a string representations to the engines to perform the operations. Therefore all values are casted to a string representation. This also means, that for custom objects one needs to define a proper __str__ method to cast the object to a string representation and ensure preservation of the semantics of that object.\n\nLastly, we need to talk about inheritance. Our API is built on top of the Symbol class, which is the base class of all operations. This means that all operations are inherited from the Symbol class. This provides a convenient modality to add new custom operations by sub-classing Symbol, yet, ensuring to always have a set of base operations at our disposal without bloating the syntax or re-implementing many existing functionalities. This also means that we can define contextualized operations with individual constraints, prompt designs and therefore behaviors by simply sub-classing the Symbol class and overriding the corresponding method. However, we recommend sub-classing the Expression class as we will see later, it adds additional functionalities.\n\nHere is an example of how to define a custom == operation by overriding the __eq__ method and providing a custom prompt object with a list of examples:\n\nclass Demo(ai.Symbol): def __eq__(self, other) -> bool: @ai.equals(examples=ai.Prompt([ \"1 == 'ONE' =>True\", \"'six' == 7 =>False\", \"'Acht' == 'eight' =>True\", ... ]) ) def _func(_, other) -> bool: return False # default behavior on failure return _func(self, other)\n\nAs shown in the above example, this is also the way we implemented the basic operations in Symbol, by defining local functions that are then decorated with the respective operation decorator from the symai/core.py file. The symai/core.py is a collection of pre-defined operation decorators that we can quickly apply to any function. The reason why we use locally defined functions instead of directly decorating the main methods, is that we do not necessarily want that all our operations are sent to the neural engine and could implement a default behavior. Another reason is that we want to cast return types of the operation outcome to symbols or other derived classes thereof. This is done by using the self._sym_return_type(...) method and can give contextualized behavior based on the defined return type. See more details in the actual Symbol class.\n\nIn the next section, we will show that almost all operations in symai/core.py are derived from the more generic few_shot decorator.\n\n🧪 Custom Operations\n\nOne can also define customized operations. For example, let us define a custom operation to generate a random integer between 0 and 10:\n\nclass Demo(ai.Symbol): def __init__(self, value = '') -> None: super().__init__(value) @ai.zero_shot(prompt=\"Generate a random integer between 0 and 10.\", constraints=[ lambda x: x >= 0, lambda x: x <= 10 ]) def get_random_int(self) -> int: pass\n\nAs we show, the Symbolic API uses Python Decorators to define operations. The @ai.zero_shot decorator is used to define a custom operation that does not require any demonstration examples, since the prompt is expressive enough. In the shown example, the zero_shot decorator takes in two arguments: prompt and constraints. The former is used to define the prompt that conditions our desired operation behavior. The latter is used to define validation constraints of the computed outcome, to ensure it fulfills our expectations.\n\nIf the constraint is not fulfilled, the above implementation would reach out to the specified default implementation or default value. If no default implementation or value was found, the Symbolic API would raise an ConstraintViolationException.\n\nWe also see that in the above example the return type is defined as int. Therefore, the resulting value from the wrapped function will be of type int. This works because our implementation uses auto-casting to a user specified return data type. If the cast fails, the Symbolic API will raise a ValueError. If no return type is specified, the return type will be Any.\n\nFew-shot operations\n\nThe @ai.few_shot decorator is the a generalized version of @ai.zero_shot and is used to define a custom operation that requires demonstration examples. To give a more complete picture, we present the function signature of the few_shot decorator:\n\ndef few_shot(prompt: str, examples: Prompt, constraints: List[Callable] = [], default: Optional[object] = None, limit: int = 1, pre_processor: Optional[List[PreProcessor]] = None, post_processor: Optional[List[PostProcessor]] = None, **wrp_kwargs):\n\nThe prompt and constraints attributes behavior is similar to the zero_shot decorator. The examples and limit arguments are new. The examples argument is used to define a list of demonstrations that are used to condition the neural computation engine. The limit argument is used to define the maximum number of examples that are returned, give that there are more results. The pre_processor argument takes a list of PreProcessor objects which can be used to pre-process the input before it is fed into the neural computation engine. The post_processor argument takes a list of PostProcessor objects which can be used to post-process the output before it is returned to the user. The wrp_kwargs argument is used to pass additional arguments to the wrapped method, which are also stream-lined towards the neural computation engine and other engines.\n\nTo give a more holistic picture ouf our conceptional implementation, see the following flow diagram containing the most important classes:\n\nThe colors indicate logical groups of data processing steps. Yellow indicates the input and output data. Blue indicates places you can customize or prepare the input of your engine. Green indicates post-processing steps of the engine response. Red indicates the application of constraints (which also includes the attempted casting of the return type signature, if specified in the decorated method). Grey indicates the custom method which defines all properties, therefore has access to all the above mentioned objects.\n\nTo conclude this section, here is an example how to write a custom Japanese name generator with our @ai.zero_shot decorator:\n\nimport symai as ai class Demo(ai.Symbol): @ai.few_shot(prompt=\"Generate Japanese names: \", examples=ai.Prompt( [\"愛子\", \"和花\", \"一郎\", \"和枝\"] ), limit=2, constraints=[lambda x: len(x) > 1]) def generate_japanese_names(self) -> list: return ['愛子', '和花'] # dummy implementation\n\nShould the neural computation engine not be able to compute the desired outcome, it will reach out to the default implementation or default value. If no default implementation or value was found, the method call will raise an exception.\n\nPrompt Design\n\nThe way all the above operations are performed is by using a Prompt class. The Prompt class is a container for all the information that is needed to define a specific operation. The Prompt class is also the base class for all other Prompt classes.\n\nHere is an example how to define a Prompt to enforce the neural computation engine for comparing two values:\n\nclass CompareValues(ai.Prompt): def __init__(self) -> ai.Prompt: super().__init__([ \"4 > 88 =>False\", \"-inf < 0 =>True\", \"inf > 0 =>True\", \"4 > 3 =>True\", \"1 < 'four' =>True\", ... ])\n\nFor example, when calling the <= operation on two Symbols, the neural computation engine will evaluate the symbols in the context of the CompareValues prompt.\n\nres = ai.Symbol(1) <= ai.Symbol('one')\n\nThis statement evaluates to True, since the fuzzy compare operation was conditions our engine to compare the two Symbols based on their semantic meaning.\n\n:[Output]: True\n\nIn a more general notion, depending on the context hierarchy of the expression class and used operations the semantics of the Symbol operations may vary. To better illustrate this, we show our conceptual prompt design in the following figure:\n\nThe figure shows our hierarchical prompt design as a container of all the information that is provided to the neural computation engine to define a task-specific operation. The Yellow and Green highlighted boxes indicate mandatory string placements. The dashed boxes are optional placeholders. and the Red box indicates the starting point of the model prediction.\n\nConceptually we consider three main prompt designs: Context-based Prompts, Operational Prompts, and Templates. The prompts can be curated either by inheritance or by composition. For example, the Static Context can be defined by inheriting from the Expression class and overriding the static_context property. An Operation and Template prompt can be created by providing an PreProcessor to modify the input data.\n\nWe will now explain each prompt concept in more details:\n\nThe Context-based Prompts (Static, Dynamic and Payload) are considered optional and can be defined in a static manner, either by sub-classing the Expression class and overriding the static_context property, or at runtime by updating the dynamic_context property or passing an payload kwargs to a method. Here is an example how to use the payload kwargs via the method signature:\n\n# creating a query to ask if an issue was resolve or not sym = Symbol(\"<some-community-conversation>\") q = sym.query(\"Was the issue resolved?\") # write manual condition to check if the issue was resolved if 'not resolved' in q: # do a new query but payload the previous query answer to the new query sym.query(\"What was the resolution?\", payload=q) ... else: pass # all good\n\nRegardless of how we set the context, our contextualized prompt defines the desired behavior of the Expression operations. For example, if we want to operate in the context of a domain-specific language, without having to override each base class method. See more details in this notebook.\n\nThe Operation prompts define the behavior of an atomic operation and is therefore mandatory to express the nature of such an operation. For example, the +-operation is used to add two Symbols together and therefore the +-operation prompt explains its behavior. Examples defines another optional structure that provides the neural computation engine with a set of demonstrations that are used to properly condition the engine. For example, the +-operation prompt can be conditioned on how to add numbers by providing a set of demonstrations, such as 1 + 1 = 2, 2 + 2 = 4, etc.\n\nThe Template prompts are optional and encapsulates the resulting prediction to enforce a specific format. For example, to generate HTML tags we can use a curated <html>{{placeholder}}</html> template. This template will enforce the neural computation engine to start the generation process already in the context of a HTML tags format, and not produce irrelevant descriptions about its task.\n\n😑 Expressions\n\nAn Expression is a non-terminal symbol, which can be further evaluated. It inherits all the properties from Symbol and overrides the __call__ method to evaluate its expressions or values. From the Expression class, all other expressions are derived. The Expression class also adds additional capabilities i.e. to fetch data from URLs, search on the internet or open files. These operations are specifically separated from Symbol since they do not use the value attribute of the Symbol class.\n\nSymbolicAI' API closely follows best practices and ideas from PyTorch, therefore, one can build complex expressions by combining multiple expressions as a computational graph. Each Expression has its own forward method, which has to be overridden. The forward method is used to define the behavior of the expression. The forward method is called by the __call__ method, which is inherited from the Expression base class. The __call__ evaluates an expression and returns the result from the implemented forward method. This design pattern is used to evaluate the expressions in a lazy manner, which means that the expression is only evaluated when the result is needed. This is a very important feature, since it allows us to chain complex expressions together. We already implemented many useful expressions, which can be imported from the symai.components file.\n\nOther important properties that are inherited from the Symbol class are _sym_return_type and static_context. These two properties define the context in which the current Expression operates, as described in the Prompt Design section. The static_context therefore influences all operations of the current Expression sub-class. The _sym_return_type ensures that after each evaluation of an Expression, we obtain the desired return object type. This is usually implemented to return the current type, but can be set to return a different type.\n\nExpressions can of course have more complex structures and be further sub-classed, such as shown in the example of the Sequence expression in the following figure:\n\nA Sequence expression can hold multiple expressions, which are evaluated at runtime.\n\nSequence expressions\n\nHere is an example how to define a Sequence expression:\n\n# first import all expressions from symai import * # define a sequence of expressions Sequence( Clean(), Translate(), Outline(), Compose('Compose news:'), )\n\nStream expressions\n\nAs we saw earlier, we can create contextualized prompts to define the behavior of operations on our neural engine. However, this also takes away a lot of the available context size and since e.g. the GPT-3 Davinci context length is limited to 4097 tokens, this might quickly become a problem. Luckily, we can use the Stream processing expression. This expression opens up a data stream and performs chunk-based operations on the input stream.\n\nA Stream expression can easily be wrapped around other expressions. For example, the chunks can be processed with a Sequence expression, that allows multiple chained operations in sequential manner. Here is an example how to define such a Stream expression:\n\nStream(Sequence( Clean(), Translate(), Outline(), Embed() ))\n\nThe shown example opens a stream, passes a Sequence object which cleans, translates, outlines and embeds the input. Internally, the stream operation estimates the available model context size and chunks the long input text into smaller chunks, which are passed to the inner expression. The returned object type is a generator.\n\nThe issue with this approach is, that the resulting chunks are processed independently. This means there is no shared context or information among chunks. To solve this issue, we can use the Cluster expression instead, where the independent chunks are merged based on their similarity. We illustrate this in the following figure:\n\nIn the shown example all individual chunks are merged by clustering the information within each chunk. This gives us a way to consolidate contextually related information and merge them in a meaningful way. Furthermore, the clustered information can then be labeled by streaming through the content of each cluster and extracting the most relevant labels, providing us with interpretable node summaries.\n\nThe full example is shown below:\n\nstream = Stream(Sequence( Clean(), Translate(), Outline(), )) sym = Symbol('<some long text>') res = Symbol(list(stream(sym))) expr = Cluster() expr(res)\n\nIn a next step, we could recursively repeat this process on each summary node, therefore, build a hierarchical clustering structure. Since each Node resembles a summarized sub-set of the original information we can use the summary as an index. The resulting tree can then be used to navigate and retrieve the original information, turning the large data stream problem into a search problem.\n\nAlternatively, we could use vector-base similarity search to find similar nodes. For searching in a vector space we can use dedicated libraries such as Annoy, Faiss or Milvus.\n\n❌ Error Handling\n\nA key idea of the SymbolicAI API is to be able to generate code. This in turn means that errors may occur, which we need to handle in a contextual manner. As a future vision, we even want our API to self extend and therefore need to be able to resolve issues automatically. To do so, we propose the Try expression, which has a fallback statements built-in and retries an execution with dedicated error analysis and correction. This expression analyses the input and the error, and conditions itself to resolve the error by manipulating the original code. If the fallback expression succeeds, the result is returned. Otherwise, this process is repeated for the number of retries specified. If the maximum number of retries is reached and the problem was not resolved, the error is raised again.\n\nLet us assume, we have some executable code that was previously generated. However, by the nature of generative processes syntax errors may occur. By using the Execute expression, we can evaluate our generated code, which takes in a symbol and tries to execute it. Naturally, this will fail. However, in the following example the Try expression resolves this syntactic error and the receive a computed result.\n\nexpr = Try(expr=Execute()) sym = Symbol('a = int(\"3,\")') # some code with a syntax error res = expr(sym)\n\nThe resulting output is the evaluated code, which was corrected:\n\n:Output: a=3\n\nWe are aware that not all errors are as simple as the shown syntactic error example, which can be resolved automatically. Many errors occur due to semantic misconceptions. Such issues require contextual information. Therefore, we are further exploring means towards more sophisticated error handling mechanism. This includes also the usage of streams and clustering to resolve errors in a more hierarchical contextual manner. It is also noteworthy that neural computations engines need to be further improved to better detect and resolve errors.\n\n🕷️ Interpretability, Testing & Debugging\n\nPerhaps one of the greatest benefits of using neuro-symbolic programming is, that we can get a clear understanding of how well our LLMs understand simple operations. Specifically we gain knowledge about if, and at which point they fail, enabling us to follow their StackTraces and determine the failure points. In our case, neuro-symbolic programming allows us to debug the model predictions based on dedicated unit test for simple operations. To detect conceptual misalignments we can also use a chain of neuro-symbolic operations and validate the generative process. This is of course not a perfect solution, since the verification may also be error prone, but it gives us at least a principle way to detect conceptual flaws and biases in our LLMs.\n\nUnit Testing Models\n\nSince our premise is to divide and conquer complex problems, we can curate conceptual unit test and target very specific and tracktable sub-problems. The resulting measure, i.e. success rate of the model prediction, can then be used to evaluate their performance, and hint towards undesired flaws or biases.\n\nThis allows us to design domain-specific benchmarks and see how well general learners, such as GPT-3, adapt with certain prompts to a set of tasks.\n\nFor example, we can write a fuzzy comparison operation, that can take in digits and strings alike, and perform a semantic comparison. LLMs can then be asked to evaluate these expressions. Often times, these LLMs still fail to understand the semantic equivalence of tokens in digits vs strings and give wrong answers.\n\nThe following code snipped shows a unit test to perform semantic comparison of numbers (between digits and strings):\n\nimport unittest from symai import * class TestComposition(unittest.TestCase): def test_compare(self): res = Symbol(10) > Symbol(5) self.assertTrue(res) res = Symbol(1) < Symbol('five') self.assertTrue(res) ...\n\n🔥Debugging\n\nWhen creating very complex expressions, we debug them by using the Trace expression, which allows to print out the used expressions, and follow the StackTrace of the neuro-symbolic operations. Combined with the Log expression, which creates a dump of all prompts and results to a log file, we can analyze where our models potentially failed.\n\nExample: News Summary\n\nIn the following example we create a news summary expression that crawls the given URL and streams the site content through multiple expressions. The outcome is a news website that is created based on the crawled content. The Trace expression allows to follow the StackTrace of the operations and see what operations are currently executed. If we open the outputs/engine.log file we can see the dumped traces with all the prompts and results.\n\n# crawling the website and creating an own website based on its facts news = News(url='https://www.cnbc.com/cybersecurity/', pattern='cnbc', filters=ExcludeFilter('sentences about subscriptions, licensing, newsletter'), render=True) expr = Log(Trace(news)) res = expr()\n\nHere is the corresponding StackTrace of the model:\n\nThe above code creates a webpage with the crawled content from the original source. See the preview below, the entire rendered webpage image here and resulting [code of webpage here](https://raw.githubusercontent.com/Xpitfire/symbolicai/main/examples/results/news.html.\n\n▶️ Play around with our API\n\nLaunch and explore the notebook here:\n\nThere are many more examples in the examples folder and in the notebooks folder. You can also explore the test cases in the tests folder.\n\n📈 Interface for Query and Response Inspection\n\nSymbolicAI is by design a data-driven framework. This means that we can collect data from API interactions while we provide the requested responses. For very agile, dynamic adaptations or prototyping we can integrate user desired behavior quickly into existing prompts. However, we can also log the user queries and model predictions to make them available for post-processing. Therefore, we can customize and improve the model's responses based on real-world data.\n\nIn the following example, we show how we can use an Output expression to pass a handler function and access input prompts of the model and model predictions. These, can be used for data collection and later fine-tuning stages. The handler function provides a dictionary and offers keys for input and output values. The content can then be sent to a data pipeline for further processing.\n\nsym = Symbol('Hello World!') def handler(res): input_ = res['input'] output = res['output'] expr = Output(expr=sym.translate, handler=handler, verbose=True) res = expr('German')\n\nSince we called verbose, we can also see the console print of the Output expression:\n\nInput:(['Translate the following text into German:\\n\\nHello World!'],) Expression: <bound method Symbol.translate of <class'symai.symbol.Symbol'>(value=Hello World!)> args:('German',) kwargs:{'input_handler': <function OutputEngine.forward.<locals>.input_handler at ... Dictionary:{'wrp_self': <class'symai.components.Output'>(value=None),'func': <function Symbol.output.<locals>._func at ... Output: Hallo Welt!\n\n🤖 Engines\n\nDue to limited compute resources we currently rely on OpenAI's GPT-3 API for the neuro-symbolic engine. However, given the right compute resources, it is possible to use local machines to avoid high latencies and costs, with alternative engines such as OPT or Bloom. This would allow for recursive executions, loops, and more complex expressions.\n\nFurthermore, as we interpret all objects as symbols only with a different encodings, we integrated a set of useful engines that transform these objects to the natural language domain to perform our operations.\n\nSymbolic Engine\n\nAlthough in our work, we mainly focus on how LLMs can evaluate symbolic expressions, many formal statements were already well implemented in existing symbolic engines, like WolframAlpha. Therefore, given an API KEY from WolframAlpha, we can use their engine by setting the expression_engine attribute. This avoids error prune evaluations from neuro-symbolic engines for mathematical operations. The following example shows how to use WolframAlpha to compute the result of the variable x:\n\nExpression.command(engines=['symbolic'], expression_engine='wolframalpha') res = expr.expression('x^2 + 2x + 1')\n\n:Output: x= -1\n\nSpeech Engine\n\nTo interpret audio files we can perform speech transcription by using whisper. The following example shows how to transcribe an audio file and return the text:\n\nexpr = Expression() res = expr.speech('examples/audio.mp3')\n\n:Output: I may have overslept.\n\nOCR Engine\n\nTo \"read\" text from images we can perform optical character recognition (OCR) with APILayer. The following example shows how to transcribe an image and return the text:\n\nexpr = Expression() res = expr.ocr('https://media-cdn.tripadvisor.com/media/photo-p/0f/da/22/3a/rechnung.jpg')\n\nThe OCR engine returns a dictionary with a key all_text where the full text is stored. See more details in their documentation here.\n\n:Output: China Restaurant\\nMaixim,s\\nSegeberger Chaussee273\\n22851 Norderstedt\\nTelefon040/529162 ...\n\nSearch Engine\n\nTo obtain fact-based content we perform search queries via SerpApi with a Google backend. The following example shows how to search for a query and return the results:\n\nexpr = Expression() res = expr.search('Birthday of Barack Obama')\n\n:Output: August4,1961\n\nWebCrawler Engine\n\nTo access any data source from the web, we can use Selenium. The following example shows how to crawl a website and return the results:\n\nexpr = Expression() res = expr.fetch(url=\"https://www.google.com/\", pattern=\"google\")\n\nThe pattern property can be used to detect if the document as been loaded correctly. If the pattern is not found, the crawler will timeout and return an empty result.\n\n:Output: GoogleKlicke hier, wenn du nach einigen Sekunden nicht automatisch weitergeleitet wirst.GmailBilderAnmelden ...\n\nDrawing Engine\n\nTo render nice images from text description we use DALL·E 2. The following example shows how to draw a text description and return the image:\n\nexpr = Expression('a cat with a hat') res = expr.draw()\n\n:Output: https://oaidalleapiprodscus.blob.core.windows.net/private/org-l6FsXDfth6Uct ...\n\nDon't worry, we would never hide an image of a cat with a hat from you. Here is the image preview and link:\n\nFile Engine\n\nTo perform file operations we use the file system of the OS. At the moment, we support only PDF files and plain text files. This is a very early stage and we are working on more sophisticated file system access and also remote storage. The following example shows how to read a PDF file and return the text:\n\nexpr = Expression() res = expr.open('./LICENSE')\n\n:Output: BSD3-Clause License\\n\\nCopyright(c)2023 ...\n\nIndexing Engine\n\nWe use Pinecone to index and search for text. The following example shows how to store text as an index and then retrieve the most related match of it:\n\nexpr = Expression() expr.add(Symbol('Hello World!').zip()) expr.add(Symbol('I like cookies!').zip()) res = expr.get(Symbol('hello').embed().value).ast() res['matches'][0]['metadata']['text'][0]\n\n:Output: Hello World!\n\nHere the zip method creates a pair of strings and embedding vectors. Afterwards they are added to the index. The line with get basically retrieves the original source based on the vector value of hello and uses ast to cast the value to a dictionary.\n\nOne can set several kwargs for the indexing engine. See the symai/backend/engine_index.py file for more details.\n\nCLIP Engine\n\nTo perform text-based image few-shot classification we use CLIP. This implementation is very experimental and conceptually does not fully integrate the way we intend it, since the embeddings of CLIP and GPT-3 are not aligned (embeddings of the same word are not identical for both models). Aligning them is an open problem for future research. For example, one could learn linear projections from one embedding space to the other.\n\nThe following example shows how to classify the image of our generated cat from above and return the results as an array of probabilities:\n\nexpr = Expression() res = expr.vision('https://oaidalleapiprodscus.blob.core.windows.net/private/org-l6FsXDfth6...', ['cat', 'dog', 'bird', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe'])\n\n:Output: array([[9.72840726e-01,6.34790864e-03,2.59368378e-03,3.41371237e-03, 3.71197984e-03,8.53193272e-03,1.03346225e-04,2.08464009e-03, 1.77942711e-04,1.94185617e-04]],dtype=float32)\n\nLocal Neuro-Symbolic Engine\n\nOne can use the a locally hosted instance for the Neuro-Symbolic Engine. Out of the box we provide a Hugging Face client-server backend and host the model EleutherAI/gpt-j-6B to perform the inference. As the name suggests this is a six billion parameter model and requires a GPU with ~16GB RAM to run properly. The following example shows how to host and configure the usage of the local Neuro-Symbolic Engine.\n\nFist we start the backend server:\n\n# optional: set cache folder for transformers (Linux/MacOS) exportTRANSFORMERS_CACHE=\"<path-to-cache-folder>\" # start server backend (default model is EleutherAI/gpt-j-6B) symsvr # initialize server with client call symclient\n\nThen use once the following code to set up the local engine:\n\nfrom symai.backend.engine_nesy_client import NeSyClientEngine # setup local engine engine = NeSyClientEngine() Expression.setup(engines={'neurosymbolic': engine})\n\nNow you can use the local engine to perform symbolic computation:\n\n# do some symbolic computation with the local engine sym = Symbol('cats are cute') res = sym.compose() ...\n\nCustom Engine\n\nIf you want to replace or extend the functionality of our framework, you can do this by customizing the existing engines or creating new engines. The Symbol class provides for this functionality some helper methods, such as command and setup. The command method can pass on configurations (as **kwargs) to the engines and change functionalities or parameters. The setup method can be used to re-initialize an engine with your custom engine implementation which must sub-class the Engine class. Both methods can be specified to address one, more or all engines.\n\nHere is an example how to initialize your own engine. We will sub-class the existing GPT3Engine and override the prepare method. This method is called before the neural computation and can be used to modify the parameters of the actual input prompt that will be passed in for execution. In this example, we will replace the prompt with dummy text for illustration purposes:\n\nfrom symai.backend.engine_gpt3 import GPT3Engine class DummyEngine(GPT3Engine): def prepare(self, args, kwargs, wrp_params): wrp_params['prompts'] = ['Go wild and generate something!'] custom_engine = DummyEngine() sym = Symbol() Expression.setup(engines={'neurosymbolic': custom_engine}) res = sym.compose()\n\nTo configure an engine, we can use the command method. In this example, we will enable verbose mode, where the engine will print out what methods it is executing and the parameters it is using. This is useful for debugging purposes:\n\nsym = Symbol('Hello World!') Expression.command(engines=['neurosymbolic'], verbose=True) res = sym.translate('German')\n\n:Output: <symai.backend.engine_gpt3.GPT3Engine object at0, <function Symbol.translate.<locals>._func at 0x7fd68ba04820>,{'wrp_self': <class'symai.symbol.S ['\\n\\nHallo Welt!']\n\nHere is the list of names of the engines that are currently supported:\n\nneurosymbolic - GPT-3\n\nsymbolic - WolframAlpha\n\nocr - Optical Character Recognition\n\nvision - CLIP\n\nspeech - Whisper\n\nembedding - OpenAI Embeddings API (ada-002)\n\nuserinput - User Command Line Input\n\nsearch - SerpApi (Google search)\n\ncrawler - Selenium\n\nexecute - Python Interpreter\n\nindex - Pinecone\n\nopen - File System\n\noutput - Output Callbacks (e.g. for printing to console or storage)\n\nimagerendering - DALL·E 2\n\nFinally, let's assume you want to create a entirely new engine, but still keep our workflow, then you can use the _process_query function from symai/functional.py and pass in your engine including all other specified objects (i.e. Prompt, PreProcessor, etc.; see also section Custom Operations).\n\n⚡Limitations\n\nUff... this is a long list. We are still in the early stages of development and are working hard to overcome these limitations. Just to name a few:\n\nEngineering challenges:\n\nOur framework is still in its early stages of development and is not yet meant for production use. For example, the Stream class only estimates the prompt size by an approximation, which sometimes can fail. One can also create more sophisticated prompt hierarchies and dynamically adjust the global context based on a state-based approach. This would allow making consistent predictions even for long text streams.\n\nMany operations need to be further improved: verified for biases, fairness, robustness, etc.\n\nThe code may not be complete and is not yet optimized for speed and memory usage, and uses API-based LLMs due to limitations of compute resources.\n\nCode coverage is not yet complete and we are still working on the documentation.\n\nIntegrate with a more diverse set of models from Hugging Face or other platforms.\n\nCurrently we did not account for multi-threading and multi-processing.\n\nResearch challenges:\n\nTo reliably use our framework, one needs to further explore how to fine-tune LLMs to specifically solve many of the proposed operations in a more robust and efficient way.\n\nThe experimental integration of CLIP is meant to align image and text embeddings. To enable decision-making of LLMs based on observations and perform symbolic operations on objects in images or videos would be a huge leap forward. This would perfectly integrate with reinforcement learning approaches and enable us to control policies in a systematic way (see also GATO). Therefore, we need to train large multi-modal variants with image / video data and text data, describing in high details the scenes to obtain neuro-symbolic computation engines that can perform semantic operations similar to move-towards-tree, open-door, etc.\n\nGeneralist LLMs are still highly over-parameterized and hardware has not yet caught up to host these models on arbitrary day-to-day machines. This limits the applicability of our approach not only on small data streams, but also gives high latencies and therefore limits the amount of complexity and expressiveness we can achieve with our expressions.\n\n🥠 Future Work\n\nWe are constantly working on improving the framework and are open to any suggestions, feedback or comments. However, we try to think ahead of time and have some general ideas for future work in mind:\n\nMeta-Learning Semantic Concepts on top of Neuro-Symbolic Expressions\n\nSelf-evolving and self-healing API\n\nIntegrate our neuro-symbolic framework with Reinforcement Learning\n\nWe believe that LLMs as neuro-symbolic computation engines enable us a new class of applications, with tools and APIs that can self-analyze and self-heal. We are excited to see what the future brings and are looking forward to your feedback and contributions.\n\nConclusion\n\nWe have presented a neuro-symbolic view on LLMs and showed how they can be a central pillar for many multi-modal operations. We gave an technical report on how to utilize our framework and also hinted at the capabilities and prospects of these models to be leveraged by modern software development.\n\n👥 References, Related Work & Credits\n\nThis project is inspired by the following works, but not limited to them:\n\nNewell and Simon's Logic Theorist: Historical Background and Impact on Cognitive Modeling\n\nSearch and Reasoning in Problem Solving\n\nThe Algebraic Theory of Context-Free Languages\n\nNeural Networks and the Chomsky Hierarchy\n\nTracr: Compiled Transformers as a Laboratory for Interpretability\n\nHow can computers get common sense?\n\nArtificial Intelligence: A Modern Approach\n\nSymPy: symbolic computing in Python\n\nNeuro-symbolic programming\n\nFuzzy Sets\n\nAn early approach toward graded identity and graded membership in set theory\n\nFrom Statistical to Causal Learning\n\nLanguage Models are Few-Shot Learners\n\nDeep reinforcement learning from human preferences\n\nAligning Language Models to Follow Instructions\n\nChain of Thought Prompting Elicits Reasoning in Large Language Models\n\nMeasuring and Narrowing the Compositionality Gap in Language Models\n\nLarge Language Models are Zero-Shot Reasoners\n\nPre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing\n\nReAct: Synergizing Reasoning and Acting in Language Models\n\nUnderstanding Stereotypes in Language Models: Towards Robust Measurement and Zero-Shot Debiasing\n\nConnectionism and Cognitive Architecture: A Critical Analysis\n\nUnit Testing for Concepts in Neural Networks\n\nTeaching Algorithmic Reasoning via In-context Learning\n\nPromptChainer: Chaining Large Language Model Prompts through Visual Programming\n\nPrompting Is Programming: A Query Language For Large Language Models\n\nSelf-Instruct: Aligning Language Model with Self Generated Instructions\n\nREALM: Retrieval-Augmented Language Model Pre-Training\n\nWolfram|Alpha as the Way to Bring Computational Knowledge Superpowers to ChatGPT\n\nBuild a GitHub support bot with GPT3, LangChain, and Python\n\nComparison to other frameworks\n\nSince an often received request is to state the differences between our project and LangChain, this is a short list and by no means complete to contrast ourselves to other frameworks:\n\nWe focus on cognitive science and cognitive architectures research, and therefore, do not consider our framework as a production-ready implementation. We believe that the current state of the art in LLMs is not yet ready for general purpose tasks, and therefore, we focus on the advances of concept learning, reasoning and flow control of the generative process.\n\nWe consider LLMs as one type of neuro-symbolic computation engines, which could be of any shape or form, such as knowledge graphs, rule-based systems, etc., therefore, not necessarily limited to Transformers or LLMs.\n\nWe focus on advancing the development of programming languages and new programming paradigms, and subsequently its programming stack, including neuro-symbolic design patterns to integrate with operators, inheritance, polymorphism, compositionality, etc. Classical object-oriented and compositional design pattern have been well studied in the literature, however, we bring a novel view on how LLMs integrate and augment fuzzy logic and neuro-symbolic computation.\n\nWe do not consider our main attention towards prompt engineering. Our proposed prompt design helps the purpose to combine object-oriented paradigms with machine learning models. We believe that prompt misalignments in their current form will alleviate with further advances in Reinforcement Learning from Human Feedback and other value alignment methods. Therefore, these approaches will solve the necessity to prompt engineer or the ability to prompt hack statements. Consequently, this will result to much shorter zero- or few-shot examples (at least for small enough tasks). This is where we see the power of a divide a conquer approach, performing basic operations and re-combining them to solve the complex tasks.\n\nWe see operators / methods as being able to move along a spectrum between prompting and fine-tuning, based on task-specific requirements and availability of data. We believe that this is a more general approach, compared to prompting frameworks.\n\nWe propose a general approach how to handle large context sizes and how to transform a data stream problem into a search problem, related to the reasoning as a search problem in Search and Reasoning in Problem Solving.\n\nWe also want to state, that we highly value and support the further development of LangChain. We believe that for the community they offer very important contributions and help advance the commercialization of LLMs. We hope that our work can be seen as complementary, and future outlook on how we would like to use machine learning models as an integral part of programming languages and therefore its entire computational stack.\n\nAcknowledgements\n\nAlso this is a long list. Great thanks to my colleagues and friends at the Institute for Machine Learning at Johannes Kepler University (JKU), Linz for their great support and feedback; great thanks to Dynatrace Research for supporting this project. Thanks also to the AI Austria RL Community. Thanks to all the people who contributed to this project. Be it by providing feedback, bug reports, code, or just by using the framework. We are very grateful for your support.\n\nAnd finally, thanks to the open source community for making their APIs and tools available to the public, including (but not exclusive to) PyTorch, Hugging Face, OpenAI, GitHub, Microsoft Research, and many more.\n\nSpecial thanks to the contributions from Kajetan Schweighofer, Markus Hofmarcher, Thomas Natschläger and Sepp Hochreiter.\n\nContribution\n\nIf you want to contribute to this project, please read the CONTRIBUTING.md file for details on our code of conduct, and the process for submitting pull requests to us. Any contributions are highly appreciated.\n\n📜 Citation\n\n@software{Dinu_SymbolicAI_2022, author={Dinu, Marius-Constantin}, title={{SymbolicAI: A Neuro-Symbolic Perspective on Large Language Models (LLMs)}}, url={https://github.com/Xpitfire/symbolicai}, month={11}, year={2022} }\n\n📝 License\n\nThis project is licensed under the BSD-3-Clause License - see the LICENSE file for details.\n\nLike this project?\n\nIf you like this project, leave a star ⭐️ and share it with your friends and colleagues. And if you want to support this project even further, please consider donating to support the continuous development of this project. Thank you!\n\nWe are also looking for contributors or investors to grow and support this project. If you are interested, please contact us.\n\n📫 Contact\n\nIf you have any questions about this project, please contact us via email, on our website or find us on Discord:"
    }
}
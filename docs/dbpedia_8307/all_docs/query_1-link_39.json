{
    "id": "dbpedia_8307_1",
    "rank": 39,
    "data": {
        "url": "https://github.com/ZihengZZH/data-science-IBM",
        "read_more_link": "",
        "language": "en",
        "title": "IBM: repository for IBM Data Science Professional Certificate @ Coursera",
        "top_image": "https://opengraph.githubassets.com/83295d5d229d20f69d03c85f0995284bd3f2f6587600bd905ccb900acacbbaed/ZihengZZH/data-science-IBM",
        "meta_img": "https://opengraph.githubassets.com/83295d5d229d20f69d03c85f0995284bd3f2f6587600bd905ccb900acacbbaed/ZihengZZH/data-science-IBM",
        "images": [
            "https://github.com/ZihengZZH/data-science-IBM/raw/master/images/CRISP-DM.png",
            "https://github.com/ZihengZZH/data-science-IBM/raw/master/images/methodology.png",
            "https://github.com/ZihengZZH/data-science-IBM/raw/master/images/model_ER.jpg",
            "https://github.com/ZihengZZH/data-science-IBM/raw/master/images/SQL-API.png",
            "https://github.com/ZihengZZH/data-science-IBM/raw/master/images/residual_plot.png",
            "https://github.com/ZihengZZH/data-science-IBM/raw/master/images/VISUAL-01-LINE-PLOT.png",
            "https://github.com/ZihengZZH/data-science-IBM/raw/master/images/VISUAL-02-AREA-PLOT.png",
            "https://github.com/ZihengZZH/data-science-IBM/raw/master/images/VISUAL-03-HISTOGRAM.png",
            "https://github.com/ZihengZZH/data-science-IBM/raw/master/images/VISUAL-04-BAR-CHART.png",
            "https://github.com/ZihengZZH/data-science-IBM/raw/master/images/VISUAL-05-PIE-CHART.png",
            "https://github.com/ZihengZZH/data-science-IBM/raw/master/images/VISUAL-06-BOX-PLOT.png",
            "https://github.com/ZihengZZH/data-science-IBM/raw/master/images/VISUAL-07-SCATTER-PLOT.png",
            "https://github.com/ZihengZZH/data-science-IBM/raw/master/images/VISUAL-08-REGRESSION-PLOT.png",
            "https://github.com/ZihengZZH/data-science-IBM/raw/master/images/VISUAL-09-FOLIUM.png",
            "https://github.com/ZihengZZH/data-science-IBM/raw/master/images/VISUAL-10-FOLIUM.png",
            "https://github.com/ZihengZZH/data-science-IBM/raw/master/images/VISUAL-11-FOLIUM.png",
            "https://github.com/ZihengZZH/data-science-IBM/raw/master/images/VISUAL-12-CHOROPLETH-MAP.png",
            "https://github.com/ZihengZZH/data-science-IBM/raw/master/images/METRIC-01-J-INDEX.png",
            "https://github.com/ZihengZZH/data-science-IBM/raw/master/images/METRIC-02-F1-SCORE.png",
            "https://github.com/ZihengZZH/data-science-IBM/raw/master/images/METRIC-03-LOG-LOSS.png",
            "https://github.com/ZihengZZH/data-science-IBM/raw/master/images/CLAS-01-LOGISTIC.png",
            "https://github.com/ZihengZZH/data-science-IBM/raw/master/images/CLAS-02-GRADIENT.png",
            "https://github.com/ZihengZZH/data-science-IBM/raw/master/images/METRIC-04-SVM.png",
            "https://github.com/ZihengZZH/data-science-IBM/raw/master/images/elbow-rule.png",
            "https://github.com/ZihengZZH/data-science-IBM/raw/master/images/DBSCAN.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "repository for IBM Data Science Professional Certificate @ Coursera - ZihengZZH/data-science-IBM",
        "meta_lang": "en",
        "meta_favicon": "https://github.com/fluidicon.png",
        "meta_site_name": "GitHub",
        "canonical_link": "https://github.com/ZihengZZH/data-science-IBM",
        "text": "Data Science by IBM\n\nData Science by IBM\n\nWhat is Data Science -- Week 1\n\nWhat is Data Science -- Week 2\n\nWhat is Data Science -- Week 3\n\nOpen Source tools for Data Science -- Week 1\n\nOpen Source tools for Data Science -- Week 2\n\nOpen Source tools for Data Science -- Week 3\n\nData Science Methodology -- Week 1\n\nFrom Problem to Approach\n\nFrom Requirements to Collection\n\nData Science Methodology -- Week 2\n\nFrom Understanding to Preparation\n\nFrom Modelling to Evaluation\n\nData Science Methodology -- Week 3\n\nFrom Deployment to Feedback\n\nPython for Data Science and AI -- Week 1\n\nPython for Data Science and AI -- Week 2\n\nPython for Data Science and AI -- Week 3\n\nPython for Data Science and AI -- Week 4\n\nPython for Data Science and AI -- Week 5\n\nDatabases and SQL for Data Science -- Week 1\n\nDatabases and SQL for Data Science -- Week 2\n\nDatabases and SQL for Data Science -- Week 3\n\nDatabases and SQL for Data Science -- Week 4\n\nData Analysis with Python -- Week 1\n\nData Analysis with Python -- Week 2\n\nData Analysis with Python -- Week 3\n\nData Analysis with Python -- Week 4\n\nData Analysis with Python -- Week 5\n\nData Visualization with Python -- Week 1\n\nData Visualization with Python -- Week 2\n\nData Visualization with Python -- Week 3\n\nMachine Learning with Python -- Week 2\n\nLinear Regression\n\nMultiple Linear Regression\n\nPolynomial Regression\n\nMachine Learning with Python -- Week 3\n\nK-Nearest Neighbours\n\nDecision Trees\n\nLogistic Regression\n\nSupport Vector Machine (SVM)\n\nMachine Learning with Python -- Week 4\n\nK-Means\n\nHierarchical Clustering\n\nDensity-based Clustering\n\nMachine Learning with Python -- Week 5\n\nContent-based Recommender\n\nCollaborative Filtering\n\nApplied Data Science Capstone -- Week 1\n\nApplied Data Science Capstone -- Week 2\n\nApplied Data Science Capstone -- Week 3\n\nWhat is Data Science -- Week 1\n\nData Science is the field of exploring, manipulating, and analyzing data, and using data to answer questions or make recommendations.\n\nAs Data Science is not a discipline traditionally taught at universities, contemporary data scientists come from diverse backgrounds such as engineering, statistics, and physics.\n\nQuiz 010101\n\nHarvard Business Review called Data Science the sexiest job in the 21st century. -- True\n\nAccording to the report by the McKinsey Global Institute, by 2018, it is projected that there will be a shortage of 140,000 - 190,000 people with deep analytical skills in the world. -- False (in US alone)\n\nWalmart addressed its analytical needs by approaching Kaggle to host a competition for analyzing its proprietary data. -- True\n\nWhat is the average base salary for a data scientist reported by the New York Times? -- $112,000\n\nAccording to professor Haider, the three important qualities to possess in order to succeed as a data scientist are curious, judgemental, and proficient in programming. -- False (argumentative as the last one)\n\nUsing complicated machine learning algorithms does not always guarantee achieving a better performance. Occasionally, a simple algorithm such as k-nearest neighbor can yield a satisfactory performance comparable to the one achieved using a complicated algorithm. It all depends on the data.\n\nThe cloud allows you to bypass the physical limitations of your personal computer and the systems you are using. One limitation of the cloud is that you are not able to deploy capabilities of advanced machines that do not necessarily have to be your machines.\n\nQuiz 010102\n\nHal Varian, the chief economist at Google, declared that \"the sexy job in the next ten years will be statisticians, but it is widely believed that what he really meant were data scientists.\n\nThe author defines a data scientist as someone who finds solutions to problems by analyzing data using appropriate tool and then tells stories to communicate their finding to the relevant stakeholders. -- True\n\nAccording to the reading, the author defines data science as the art of uncovering the hidden secrets in data. -- False\n\nWhat is admirable about Dr. Patil's definition of a data scientist is that it limits data science to activities involving machine learning. -- False (that it is inclusive of individuals of various academic backgrounds and training and does not restrict the definition of a data scientist to a particular tool or subject it to a certain arbitrary minimum threshold of data size)\n\nAccording to the reading, the characteristics exhibited by the best data scientists are those who are curious, ask good questions, and have at least 10 years of experience. -- False\n\nWhat is Data Science -- Week 2\n\nData science and business analytics became very hot subjects recently, since around the year 2012.\n\nBig data was started by Google when Google tried to figure out how to solve their PageRank algorithm.\n\nBig data is data that is large enough and has enough volume and velocity that you cannot handle it with traditional database systems.\n\nAccording to Dr. White, most of the components of data science, such as probability, statistics, linear algebra, and programming, have been around for many decades but now we have the computational capabilities to apply combine them and come up with new techniques and learning algorithms.\n\nQuiz 010201\n\nAccording to the reading, the output of a data mining exercise largely depends on the skills of the data scientist carrying out the exercise. -- False\n\nWhen data are missing in a systematic way, you can simply extrapolate the data or impute the missing data by filling in the average of the values around the missing data. -- False (when data are missing in a systematic way, you should determine the impact of missing data on the results and whether missing data can be excluded from the analysis)\n\nPrior Variable Analysis and Principal Component Analysis are both examples of a data reduction algorithm. -- False (Prior Variable Analysis is not a data reduction algorithm)\n\nAfter the data are appropriately processed, transformed, and stored, machine learning and non-parametric methods are a good starting point for data mining. -- False\n\nIn-sample forecast is the process of formally evaluating the predictive capabilities of the models developed using observed data to see how effective the algorithms are in reproducing data. -- True\n\nNeural networks have been around for decades, but people decided not to develop them any more because they were computationally very expensive.\n\nThe use cases for deep learning include speech recognition and classifying image at a large scale.\n\nAccording to Dr. White, if someone is coming into a data science team, the first skills they would need are: knowing basic probability and some basic statistics, knowing some algebra and some calculus, understanding relational databases, knowing how to program, at least have some computational thinking.\n\nAccording to Dr. White, the industrial world is shifting to a new trend, and for high school students to be on the right side of this new trend, his advice to them is:\n\ntake a course in probability\n\nlearn how to program\n\nlearn some math\n\ntry to start experimenting with building small systems that work and are useful\n\nlearn statistics\n\nNetflix uses machine learning to recommend movies to you based on movies you have already watched and liked or disliked.\n\nQuiz 010202\n\nThe real added value of the author's research on residential real estate is quantifying the magnitude of relationships between housing prices and different determinants. -- True\n\nRegression is a statistical technique developed by Sir Frances Galton. -- True\n\nAccording to the reading, the author discovered that an additional bedroom adds more to the housing prices than an additional washroom. -- False (other way round)\n\nThe author discovered that, all else being equal, houses located less than 5km but more than 2km to shopping centres sold for more than the rest. -- True\n\n\"What are typical land taxes in a house sale?\" is a question that can be put to regression analysis. -- False\n\nWhat is Data Science -- Week 3\n\nWhat are some of the first steps that companies need to take to get started in data science?\n\nStart collecting data\n\nPut together a team of data scientists\n\nCuriosity is one of the most important skills that a data scientist should have in addition to sense of humor and story telling.\n\nWhen hiring a data scientist, you need to ensure that the candidate is passionate about your field of work. A brilliant data scientist who is passionate about the field of IT won't necessary excel in the field of healthcare if they are passionate about it.\n\nWhich of the following are applications of data science?\n\nAugmented Reality\n\nOptimizing resource allocation for relief aid\n\nDetermining if an application for a credit card should be accepted based on the applicant's financial history and data\n\nQuiz 010301\n\nAccording to the reading, what is the ultimate purpose of analytics? -- To communicate findings to stakeholders to formulate policy or strategy\n\nThe reading mentions a common role of a data scientist is to use analytics insights to build a narrative to communicate findings to stakeholders. -- True\n\nThe United States Economic Forecast is a publication by -- Deloitte University Press\n\nThe report discussed in the reading successfully did the job of -- using data and analytics to generate likely economic scenarios\n\nAccording to the reading, it is recommended that a team waits until the results of analytics are out before they can decide on the final deliverable. -- False (in order to produce a compelling narrative, initial planning and conceptualizing of the final deliverable is of extreme importance)\n\nQuiz 010302\n\nRegardless of the length of the final deliverable, the author recommends that it includes a cover page, table of contents, executive summary, detailed contents, acknowledgements and references. -- True\n\nAn introductory section is always helpful in -- setting up the problem for the read who might be new to the topic\n\nThe result section is where you present -- your empirical findings\n\nThe discussion section is where you -- craft your main arguments by building on the results; rely on the power of narrative to enable numbers to communicate your thesis to your readers; highlight how your findings provide the ultimate missing piece to the puzzle\n\nAdding a list of references and an acknowledgement section are examples of housekeeping, according to the author. -- True\n\nOpen Source tools for Data Science -- Week 1\n\nSkill Network Labs is an environment that contains tools such as RStudio, Jupyter Network, and Zeppelin Notebook. These tools provide an interactive environment for you to perform data analysis, data visualization, machine learning and image recognition.\n\nQuiz\n\nWhat is Skills Network Labs? -- an environment that saves time: from installing, configuring and maintaining tools; a web-based environment that data science practitioners and enthusiasts alike can use for free; an online virtual lab environment that includes several open data science tools in the clouds\n\nSkills Network Labs provides an integrated environment to -- learn and try various, popular data science tools; use tools such as Python and Scala; use tools such as R and Spark; save your time without needing to install and configure these tools\n\nZeppelin and Jupyter Notebooks are great for -- interactive data analytics\n\nWhich of the following feature of RStudio specifically enables you to create interactive apps or visualizations? -- script editor\n\nBasic charts are already included in Apache Zeppelin, allowing you to convert data tables directly into visualizations without any code. -- True\n\nJupyter Notebooks are \"interactive\" documents that enable you to write and execute code in chunks (as opposed to all-or-none), add explanatory text, write mathematical equations. They are popular in data science as you can do most of the data science work (importing data, cleaning data, analyzing data, visualizing data, machine learning) and you can also share the notebooks to collaborate with others.\n\nJupyter Notebook refers the actual file while JupyterLab is the environment that organizes my various Jupyter Notebooks that allows me to run them.\n\nTo write and execute code in Jupyter Notebooks, first you need to write code in one of the gray code cells, and then press the Play button to execute the code. Alternatives to pressing the Play button include keyboard shortcuts such as Shift+Enter and Ctrl+Enter to run the code.\n\nQuiz\n\nWhat can you write in Jupyter Notebooks? -- code to be executed in one of the kernels (e.g. Python / R / Scala); stylized text in a format called \"Markdown\"; HTML code that can be written and rendered via Markdown cells\n\nWhat were Jupyter Notebooks called before the name was changed to Jupyter? -- IPython notebooks\n\nIf you wrote \"# Hi there\" in Markdown, this would be the equivalent of writing <h1>Hi there<\\h1> in HTML. -- True\n\nWhich of the following options are TRUE? -- download and save Jupyter Notebooks as .ipynb files; change kernels within a Jupyter Notebook; connect to databases from within Jupyter Notebooks\n\nAlthough you can change the kernel of the Jupyter Notebook between different programming languages (e.g. Python / R / Scala), you cannot use multiple kernels within the same Jupyter notebook (e.g. running Python, R and Scala within the same notebook). -- True\n\nOpen Source tools for Data Science -- Week 2\n\nZeppelin notebooks come with the following built in: Apache Spark, certain data visualizations and pivot charts.\n\nYou can use multiple programming languages in the same Zeppelin notebook. Note that Scala language and SQL language are used in different cells, but were operating one the same data. This is the key difference between Jupyter and Zeppelin notebooks; in Jupyter notebooks, you can only use one language at a time.\n\nWhat is the filetype for Zeppelin notebooks? .json\n\nQuiz\n\nApache Zeppelin lets you -- document your code; display your output; show your visualizations within the Zeppelin netbook; switch between multiple languages\n\nWhich is NOT an interpreter supported by Zeppelin? -- Matlab\n\nMajor benefits to using Zeppelin over Jupyter notebooks include -- some visualizations are automatically generated in Zeppelin, thus minimizing code required; you can use multiple languages in the same notebook in Zeppelin, but not in Jupyter\n\nYou cannot customize the width of your cells in a Zeppelin notebook. -- False\n\nYou can use multiple programming languages within the same Zeppelin notebook, and they can even operate on the same data. -- True\n\nRStudio IDE is a tool for programming in R (only R).\n\nYou can upload files directly into RStudio IDE from your computer.\n\nGenerally speaking, why you should use the text editor to write code, rather than in the console? Using a text editor will provide you a larger space for you to write your code that you can save and even describe what your code does.\n\nIn R, what are datasets in tabular format typically called? Dataframes.\n\nQuiz\n\nRStudio IDE allows you to -- analyze data; see your console; visualize your plots and graphs\n\nThe reason why 'sc' is one of the default variables in RStudio IDE on Skills Network Labs is because -- 'sc' stands for \"SparkContext\" and is created by default to enable users to use Apache Spark from RStudio IDE\n\nWhat are ways that you can execute R code in RStudio? -- from the script editor & console\n\nYou can view a history of code you have previously executed on RStudio IDE. -- True\n\nWhich of the following options are True? -- you can use built-in datasets (dataframes) in RStudio, such as \"mtcars\".\n\nOpen Source tools for Data Science -- Week 3\n\nJupyter Notebooks and RStudio are both available on IDM Data Science Experience.\n\nIn order to create a Project, you must first point to two resources: a storage type and a Spark service.\n\nIn Markdown, to ensure two lines of text are rendered as two separate lines of text, what should you do? Add at least two empty spaces at the end of the first line.\n\nQuiz\n\nWhat is the key difference between Skills Network Labs and IBM Watson Studio (Data Science Experience)? -- IBM Watson Studio is an enterprise-ready environment with scalability in mind, whereas Skills network Labs is primarily a learning environment\n\nThe only way to add notebooks to your project on IBM Watson Studio is to either create an empty notebook, or upload a notebook from your computer. -- False\n\nWhich of the following options are correct? -- you can add collaborators to projects; collaborators on a project can add comments to notebooks; you can use Python 3, R or Scala with Jupyter Notebooks on Watson Studio\n\nIn the upper right-hand corner of a Jupyter Notebook is the kernel interpreter (e.g. Python 3). Next to it is a circle. What does it mean if the circle is full, black circle and what should you do? -- A black circle means that the kernel is not ready to execute more code. You can either wait, interrupt, or try to restart the kernel\n\nRStudio is available on IBM Watson Studio. -- True\n\nData Science Methodology -- Week 1\n\nIn data mining, the Cross Industry Process for Data Mining (CRISP-DM) methodology is widely used. The CRISP-DM methodology is a process aimed at increasing the use of data mining over a wide variety of business applications industries. The intent is to take case specific scenarios and general behaviors to make them domain neutral. CRISP-DM is comprised of six steps with an entity that has to implement in order to have a reasonable chance of success.\n\nBusiness Understanding\n\nData Understanding\n\nData Preparation\n\nModelling\n\nEvaluation\n\nDeployment\n\nFrom Problem to Approach\n\nData science methodology begins with spending the time to seek clarification, to attain what can be referred to as a business understanding. (1st in methodology)\n\nEstablishing a clearly defined question starts with understanding the goal of the person asking the question.\n\nOnce the problem to be addressed is defined, the appropriate analytic approach for the problem is selected in the context of the business requirements. (2nd in methodology)\n\nIf the question is to determine probabilities of an action -- use a predictive model\n\nIf the question is to show relationships -- use a descriptive model\n\nIf the question requires a yes/no answer -- use a classification model\n\nAlthough the analytics approach is the second stage of the data science methodology, it is still independent of the business understanding stage. -- False (it is only when the problem to be addressed is defined, that the appropriate analytic approach for the problem can e selected in context of the business requirements)\n\nQuiz\n\nA methodology is an application for a computer program. -- False\n\nThe first stage of the data science methodology is Data Understanding. -- False\n\nBusiness Understanding is an important stage in the data science methodology. Why? -- Because it clearly defines the problem and the needs from a business perspective\n\nWhich of the following statements about the analytic approach are correct? -- if the question defined in the business understanding stage can be answered by determining probabilities of an action, then a predictive model would the right analytic approach; if the question defined in the business understanding deals with exploring relationships between different factors, then a descriptive approach, where clusters of similar activities based on events and preferences are examined, would be the right analytic method\n\nFor the case study, a decision tree classification model was used to identify the combination of conditions leading to each patient's outcome. -- True\n\nFrom Requirements to Collection\n\nPrior to undertaking the data collection and data preparation stages of the methodology, it is vital to define the data requirements, including identifying the necessary data content, formats and sources for initial data collection. (3nd in methodology)\n\nCongestive heart failure patients with other significant medical conditions were included in the study in order to increase the sample size of the patients included in the study. -- False (congestive heart failure patients with other significant medical conditions were actually excluded from the study)\n\nIn this phase, the data requirements are revised and decisions are made as to whether or not the collection requires more or less data. Once the data ingredients are collected, then in the data collection stage, the data scientist will have a good understanding of what they will be working with. (4th in methodology)\n\nWhen collecting data, it is alright to defer decisions about unavailable data, and attempt to acquire it at a later stage.\n\nQuiz\n\nWhich of the following analogies is used in the videos to explain the Data Requirements and Data Collection stages of the data science methodology? -- you can think of the Data Requirements and Data Collection stages as a cooking task, where the problem at hand is a recipe, and the data to answer the question is the ingredients\n\nThe Data Requirements stage of the data science methodology involves identifying the necessary data content, formats and sources for initial data collection. -- True\n\nDatabase Administrators determine how to collect and prepare the data. -- False\n\nIn the Data Collection stage, the business understanding of the problem is revised and decisions are made as to whether or not more data is needed. -- False (not business understanding is revised but data requirements)\n\nTechniques such as supervised methods and unsupervised methods can be applied to the dataset, to assess the content, quality, and initial insights about the data. -- False\n\nData Science Methodology -- Week 2\n\nFrom Understanding to Preparation\n\nData Understanding encompasses all activities related to constructing the dataset. Essentially, the data understanding section of the data science methodology answers the question: Is the data that you collected representative of the problem to be solved? (5th in methodology)\n\nTransforming data in the data preparation phase is the process of getting the data into a state where it may be easier to work with. Specifically, the data preparation stage of the methodology answers the question: What are the ways in which data is prepared? (6th in methodology)\n\nTo work effectively with the data, it must be prepared in a way that addresses missing or invalid values and removes duplicates, towards ensuring that everything is properly formatted.\n\nThe Data Preparation stage is the least time-consuming phase of a data science project, typically taking between 5 to 10 percent of the overall project time. -- False (most, 70 to 90 percent)\n\nIn the case study, the target variable was congestive heart failure (CHF) with 45 days following discharge from CHF hospitalization. -- False (45)\n\nQuiz\n\nThe Data Understanding stage refers to the stage of removing redundant data. -- False\n\nIn the case study, during the Data Understanding stage, data scientists discovered that not all the congestive heart failure admissions that were expected were being captured. What action did they take to resolve the issue? -- the data scientists looped back to the Data Collection stage, adding secondary and tertiary diagnoses, and building a more comprehensive definition of congestive heart failure admission\n\nThe Data Preparation stage involves correcting invalid values and addressing outliers. -- True\n\nSelecting the correct statement about what data scientists do during the Data Preparation stage. -- data scientists define the variables to be used in the model; data scientists determine the timing of events; data scientists aggregate the data and merge them from different sources; data scientists identify missing data\n\nThe Data Preparation stage is a very iterative and complicated stage that cannot be accelerated through automation. -- False\n\nFrom Modelling to Evaluation\n\nData Modelling focuses on developing models that are either descriptive or predictive. (7th in methodology)\n\nUnderstand the question at hand\n\nSelect an analytic approach or method to solve the problem\n\nObtain, understand, prepare, and model the data\n\nA training set is used to build a predictive model.\n\nIn the case study, the best performing model was the second model, with a relative cost of 4:1 and an overall accuracy of 85%. -- False (third one, 4:1, and 81%)\n\nModel Evaluation is performed during model development and before the model is deployed. Evaluation allows the quality of the model to be assessed but it's also an opportunity to see if it meets the initial request. (8th in methodology)\n\nThe ROC curve is a useful diagnostic tool in determining the optimal classification model. This curve quantifies how well a binary classification model performs, declassifying the yes and no outcomes when some discrimination criterion is varied.\n\nModel evaluation can have two main phases: a diagnostic measure phase and statistical significance testing.\n\nQuiz\n\nSelect the correction statement. -- a training set is used for predictive modelling\n\nA statistician calls a false-negative, a type I error, and a false-positive, a type II error. -- False (false-negative == type II; false-positive == type II)\n\nThe Modelling stage is followed by the Analytic Approach stage. -- False\n\nModel Evaluation includes ensuring that the data are properly handled and interpreted. -- True\n\nThe ROC curve is a useful diagnostic tool for determining the optimal classification model. -- True\n\nData Science Methodology -- Week 3\n\nFrom Deployment to Feedback\n\nOnce the model is evaluated and the data scientist is confident it will work, it is deployed and put to the ultimate test. Depending on the purpose of the model, it may be rolled out to a limited group of users or in a test environment, to build up confidence in applying the outcome for use across the board. (9th in methodology)\n\nOnce in play, feedback from the users will help to refine the model and assess it for performance and impact. The value of the model will be dependent on successfully incorporating feedback and making adjustments for as long sa the solution is required. (10th in methodology)\n\nThe data science methodology is highly iterative, ensuring the refinement at each stage in the game.\n\nThinking like a data scientist:\n\nforming a concrete business or research problem\n\ncollecting and analyzing data\n\nbuilding a model\n\nunderstanding the feedback after model deployment\n\nLearning how to work with data:\n\ndetermining the data requirements\n\ncollecting the appropriate data\n\nunderstanding the data\n\npreparing the data for modelling\n\nLearning how to derive the answer:\n\nevaluating and deploying the model\n\ngetting feedback on it\n\nusing that feedback constructively so as to improve the model\n\nQuiz\n\nThe final stages of the data science methodology are an iterative cycle between which of the different stages? -- modelling, evaluation, deployment and feedback\n\nFeedback is not required once the model is deployed because the Model Evaluation stage would have assessed the model and made sure that it performed well. -- False\n\nWhat does deploying a model into production represent? -- it represents the beginning of an iterative process that includes feedback, model refinement and redeployment and requires the input of additional groups, such as marketing personnel and business owners\n\nThe data science methodology is a specific strategy that guides processes and activities relating to data science only for text analytics. -- False\n\nA data scientist determines that building a recommender system is the solution for a particular business problem at hand. What stage of the data science methodology does this represent? -- analytic approach\n\nA car company asked a data scientist to determine what type of customers are more likely to purchase their vehicles. However, the data comes from several sources and is in a relatively raw format. What kind of processing can the data scientist perform on the data to prepare it for Modelling stage? -- feature engineering; transforming the data into more useful variables; combining the data from the various sources; addressing missing invalid values\n\nWhat do data scientists typically use for exploratory analysis of data and to get acquainted with it? -- they use descriptive statistics and data visualization techniques\n\nWhich of the following represent the two important characteristics of the data science methodology? -- it is a highly iterative process and it never ends\n\nFor predictive models, a test set, which is similar to - but independent of - the training set, is used to determine how well the model predicts outcomes. This is an example of what step in the methodology? -- model evaluation\n\nData scientists should maintain continuous communication with stakeholders throughout a project so that business stakeholders can ensure the work remains on track to generate the intended solution. -- True\n\nPython for Data Science and AI -- Week 1\n\nThis Python course consists of the following modules: Python basics, Python data structures, Python programming fundamentals, working with Data in Python, final project.\n\nQuiz\n\nWhat is the type of the following: 1.0? -- float\n\nWhat is the type of the following: \"7.1\"? -- string\n\nWhat is the result of the following code segment: int(12.3)? -- 12\n\nWhat is the result of the following code segment: int(True)? -- 1\n\nQuiz\n\nWhat is the result of the following code segment: 1/2? -- 0.5\n\nWhat is the value of x after the following lines of code? x=2; x=x+2 -- 4\n\nWhat is the result of the following operation 3+2*2? -- 7\n\nIn Python3, what is the type of the variable x after the following: x=2/2? -- float\n\nQuiz\n\nIn Python, if you executed name = 'Lizz', what would be the output of print(name[0:2])? -- Li\n\nConsider the string A = '1934567', what is the result of the following operation A[1::2]? -- '946'\n\nIn Python, what is the result of the following operation: '1'+'2'? -- '12'\n\nGiven myvar = 'hello', how would you return myvar as uppercase? -- myvar.upper()\n\nConsider the string Name = 'Michael Jackson', what is the result of the following operation Name.find('el')? -- 5\n\nWhat is the result of the following: str(1)+str(1)? -- '11'\n\nWhat is the result of the following: \"123\".replace(\"12\", \"ab\")? -- \"ab3\"\n\nPython for Data Science and AI -- Week 2\n\nQuiz\n\nWhat is the syntax to obtain the first element of the tuple: A=('a','b','c')? -- A[0]\n\nConsider the tuple A=((11,12),[21,22]), that contains a tuple and list. What is the result of the following operation A[1]: -- [21,22]\n\nConsider the tuple A=((11,12),[21,22]), that contains a tuple and list. What is the result of the following operation A[0][1]: -- 12\n\nWhat is the result of the following operation: '1,2,3,4'.split(',') -- ['1','2','3','4']\n\nAfter applying the following method, L.append(['a','b']), the following list will be only one element longer. -- True\n\nLists are mutable. -- True\n\nConsider the following list: A=['hard rock',10,1.2]. What will list A contain after the following command is run: del(A[0])? -- [10,1.2]\n\nIf A is a list what does the following syntax do: B=A[:]? -- variable B references a new copy or clone of the original list A\n\nWhat is the result of the following: len(('disco', 10))? -- 2\n\nQuiz\n\nConsider the following dictionary: {\"The Bodyguard\": \"1992\", \"Saturday Night Fever\": \"1977\"}, select the keys: -- \"The Bodyguard\" & \"Saturday Night Fever\"\n\nThe variable release_year_dict is a Python dictionary, what is the result of applying the following method: release_year_dict.values() -- retrieves the values of the dictionary\n\nHow many identical keys can a dictionary have? -- 0\n\nQuiz\n\nHow do you cast the list A to the set a? -- a = set(A)\n\nConsider the Set: V={'1','2'}, what is the result of V.add('3')? -- {'1','2','3'}\n\nWhat is the result of the following: '1' in {'1','2'}? -- True\n\nPython for Data Science and AI -- Week 3\n\nQuiz\n\nWhat value of x will produce the output: \"Hello\\nMike\"? -- x=\"A\"\n\nWhat is the output of the following code? -- \"Go Mike\"\n\nWhat is the result of the following lines of code? -- True\n\nQuiz\n\nWhat is the output of the following few lines of code? -- 2/4/6\n\nWhat is the output of the following few lines of code? -- 5/4/3\n\nWhat is the output of the following few lines of code? -- 1 A / 2 B / 3 C\n\nWhat is the output of the following? -- 2\n\nQuiz\n\nConsider the function delta, when will the function return a value of 1? -- when the input is 0\n\nGiven the function add shown below, what does the following return? -- '11'\n\nWhat is the correct way to sort list 'B' using a method, the result should not return a new list, just change the list 'B'? -- B.sort()\n\nWhat is the output of the following lines of code? -- 2\n\nQuiz\n\nConsider the class Rectangle, what are the data attributes? -- self.height, self.width, self.color\n\nWhat is the result of running the following lines of code? -- x=A y=B\n\nWhat is the result of running the following lines of code? -- x=A y=2\n\nPython for Data Science and AI -- Week 4\n\nQuiz\n\nConsider the following text file. What is the output of the following lines of code? -- This is line 1\n\nConsider the file object: File1. How would you read the first line of text? -- File1.readline()\n\nWhat do the following lines of code do? -- read the file \"Example1.txt\"\n\nQuiz\n\nConsider the following line of code. What mode is the file object in? -- write\n\nWhat do the following lines of code do? -- append to the file \"Example.txt\"\n\nWhat task do the following lines of code perform? -- copy the text from Example2.txt to Example3.txt\n\nUsing loc, iloc and ix\n\nloc is primarily label based. When two arguments are used, you use column headers and row indexes to select the data you want. loc can also take an integer as a row or column number. loc will return a KeyError if the requested items are not found.\n\niloc is integer-based. You use column numbers and row numbers to get rows or columns at particular positions in the data frame. iloc will return a KeyError if the requested indexer is out-of-bounds.\n\nix looks for a label. If ix does not find a label, it will use an integer. This means you can select data by using either column numbers and row numbers or column headers and row names using ix. (deprecated after pd 0.20.0)\n\nQuiz\n\nWhat is the result of applying the following method df.head() to dataframe df? -- print the first 5 rows of the dataframe\n\nConsider the dataframe df, how would you access the element in the 1st row 3rd column? -- df.ix[0,2]\n\nIn the lab, you learned you can also obtain a series from a dataframe df. Select the correct way to assign the column with header Length to a pandas series to the variable x. -- x = df['Length']\n\nQuiz\n\nWhat is the result of the following lines of code? -- array([0,0,0,0,0])\n\nWhat is the result of the following lines of code? -- 0\n\nWhat is the result of the following lines of code? -- array([11,11,11,11,11])\n\nQuiz\n\nHow do you perform matrix multiplication on the numpy arrays A and B? -- np.dot(A,B)\n\nWhat values does the variable out take if the following lines of code are run? -- array([0,1])\n\nWhat is the value of Z after the following code is run? -- array([[2,2],[2,2]])\n\nAn API lets two pieces of software talk to each other. Pandas, an example of API, is actually a set of software components much of which are not even written in Python.\n\nPython for Data Science and AI -- Week 5\n\nExtracting essential data from a dataset and displaying it is a necessary part of data science; therefore, individuals can make correct decisions based on data.\n\nDatabases and SQL for Data Science -- Week 1\n\nSQL is powerful language that is used for communicating with databases. Here are some of the advantages of learning SQL for someone interested in data science:\n\nSQL will boost your professional profile as a data scientist\n\nLearning SQL will give you a good understanding of relational databases\n\nIf you work with reporting tools that generate SQL queries for you, it might be useful to write your own SQL statements\n\nWhat is SQL\n\nSQL (Structured Query Language) is a language used for relational databases and for querying data.\n\nWhat is data\n\nData is a collection of facts in the form of words, numbers or even pictures. Data is one of the most critical assets of any business. Data is important, so it needs to be secure and it needs to be stored and accessed quickly.\n\nWhat is a database\n\nA database is repository of data. It is a program that stores data. A database also provides the functionality of adding, modifying and querying that data. Different kinds of databases store data in different forms. Data stored in tabular form is a relational database.\n\nWhich of the following statements are correct about databases?\n\nA database is a repository or logically coherent collection of data with some inherent meaning.\n\nTypically comes with functionality for adding, modifying and querying data.\n\nSQL or Structured Query Language is commonly used for accessing data in relational databases.\n\nWhat is an RDBMS\n\nRDBMS, or Relational DataBase Management System, is a set of software tools that controls the data: access, organization and storage.\n\nA relational database stores data in a tabular format - i.e. in rows and columns. But not all types of databases use the tabular format.\n\nA Cloud Database is a database service built and accessed through a cloud platform. Benefits are: ease of use / scalability / disaster recovery. Database services are logical abstractions for managing workloads in a database. An instance of the Cloud Database operates as a service that handles all applications requests to work with the data in any of the databases managed by that instance.\n\nQuiz\n\nWhich of the following statements are correct about databases? -- a database is repository of data; there are different types of databases - Relational, Hierarchical, No SQL, etc; a database can be populated with data and be queried\n\nWhich of the following statements about a Database is/are correct? -- a database is a logically coherent collection of data with some inherent meaning\n\nSelect the correct statement below about database services or database instances: -- database services are logical abstractions for managing workloads in a database; an instance of the Cloud database operates as a service that handles all application requests to work with the data in any of the databases managed by that instance; the database service instance is the target of the connection requests from applications\n\nThe 5 basic SQL commands are: -- CREATE, SELECT, INSERT, UPDATE, DELETE\n\nA database stores data in tabular form only. -- False\n\nThere are 5 simple statements:\n\ncreate table\n\ninsert data\n\nselect data\n\nupdate data\n\ndelete data\n\nDDL (Data Definition Language) statements: define / change / drop data\n\nDML (Data Manipulation Language) statements: read / modify data\n\nThe Primary Key in a relational table prevents duplicate rows in that table. -- True (the primary key of a relational table uniquely identifies each row in a table)\n\nCREATE TABLE author (author_id CHAR(2) CONSTRAINT AUTHOR_PK PRIMARY KEY (author_id) NOT NULL, lastname VARCHAR(15) NOT NULL, firstname VARCHAR(15) NOT NULL, email VARCHAR(40), city VARCHAR(15), country CHAR(2) ); DROP TABLE author;\n\nThe main purpose of a database management system is not just to store the data but also facilitate retrieval of the data. The SELECT statement is called a query and the output we get from executing this query is called a result set or a result table.\n\nYou can retrieve just the columns you want. The order of the columns displayed always matches the order in the SELECT statement. WHERE clause helps to restrict the result set, which always requires a Predicate: True, False or Unknown. (not equal to <>)\n\nSELECT ID,NAME FROM COUNTRY; SELECT * FROM COUNTRY; SELECT * FROM COUNTRY WHERE CCODE='CA';\n\nCOUNT() is a built-in function that retrieves the number of rows matching the query criteria.\n\nSELECT COUNT(COUNTRY) FROM MEDALS WHERE COUNTRY='CANADA';\n\nDISTINCT is used to remove duplicate values from a result set.\n\nSELECT DISTINCT COUNTRY FROM MEDALS WHERE MEDALTYPE='GOLD';\n\nLIMIT is used for restricting the number of rows retrieved from the database.\n\nSELECT * FROM MEDALS WHERE YEAR=2018 LIMIT 5\n\nINSERT statement is used to populate the table with data. A single INSERT statement can be used to insert one or multiple rows in a table.\n\nINSERT INTO AUTHOR (AUTHOR_ID, LASTNAME, FIRSTNAME, EMAIL, CITY, COUNTRY) VALUES ('A1', 'CHONG', 'RAUL', 'RFC@IBM.com', 'TORONTO', 'CA'), ('A2', 'ZIHENG', 'ZHANG', 'ZZH@IBM.com', 'SHENZHEN', 'CH')\n\nUPDATE statement is used to alter the data when the table is created and data is inserted into the table.\n\nUPDATE AUTHOR SET LASTNAME = KETTA FIRSTNAME = LAKSHMI WHERE AUTHOR_ID = A2\n\nDELETE statement is used to remove one or more rows from the table.\n\nDELETE FROM AUTHOR WHERE AUTHOR_ID IN ('A2','A3')\n\nif no WHERE clause is used, all rows will be removed.\n\nQuiz\n\nThe primary key of a relational table uniquely identifies each rwo in a table. -- True\n\nThe INSERT statement cannot be used to insert multiple rows in a single statement. -- False\n\nThe SELECT statement is called a Query, and the output we get from executing the query is called a Result Set. -- True\n\nThe CREATE TABLE statement is a -- DDL statement\n\nWhat are the basic categories of the SQL language based on functionality? -- Data Definition Language and Data Manipulation Language\n\nInformation models and data models are different and serve different purposes. An information model is at the conceptual level and defines relationship between objects. Data models are defined in a more concrete level, are specific and include details. A data model is a blueprint of any database system.\n\nRelational Model\n\nmost used data model\n\nallows for data independence (logical data / physical data / physical storage)\n\ndata is stored in tables\n\nAn entity-relationship data model or ER data model is an alternative to a relational data model.\n\nWhich statement below is correct about the relational model? -- data is organized in tables with entity relationships\n\nThe ER model is used as a tool to design relational databases. In ER models, entities are objects that exist independently of any other entities in the database. Building blocks of ER models are entities and attributes. Entities have attributes, which are the data elements that characterize the entity.\n\nDefining relationships between entities:\n\none-to-one relationship\n\none-to-many relationship\n\nmany-to-many relationship\n\nER Diagrams are basic foundation for designing a database. Begin with ERD, and map the ERD to the table.\n\nWhich of the following statements about Entity Relationship Diagrams (ERD) is true? -- attributes in an ERD are mapped to columns in a relational table; entities in an ERD are mapped to tables in a relational table\n\nIn a relation: degree = the number of attributes in a relation; cardinality = the number of tuples. Rows in a relational instance (or a table) are also known as Tuples. If a table has 4 columns and 8 rows then its cardinality is 4.\n\nQuiz\n\nCAR DEALERSHIP DATABASE CAR: serial_no, model, manufacturer, price SALE: salesperson_id, serial_no, date, sale_price SALESPERSON: salesperson_id, name, phone\n\nHow many relations does the Car Dealership Schema contain? -- 3\n\nHow many attributes does the relation CAR contain? -- 4\n\nWhat is the degree of the relation Salesperson? -- 3\n\nQuiz\n\nAdvantages of the relational model include: -- it is the most used data model; data is stored in simple data structures such as tables; provides logical and physical data independence\n\nA table containing one or more foreign keys is called a Dependent table. -- True\n\nThe Primary Key of a relational table uniquely identifies each __ in a table. -- row\n\nDatabases and SQL for Data Science -- Week 2\n\nRetrieving rows from a table\n\nSELECT * FROM BOOK; SELECT BOOK_ID, TITLE FROM BOOK; SELECT BOOK_ID, TITLE FROM BOOK WHERE BOOK_ID='B1'\n\nIn a relational database, we can use string patterns to search data rows that match this condition. The LIKE predicate is used in a WHERE clause to search for a pattern in a column. We can also use the comparison operators or BETWEEN AND in a WHERE clause (the values in the range are inclusive). We can use IN operator to specify a set of values in a WHERE clause.\n\nSELECT FIRSTNAME FROM AUTHOR WHERE FIRSTNAME LIKE 'R%'; SELECT TITLE, PAGES FROM BOOK WHERE PAGE >= 290 AND PAGES <= 300; SELECT TITLE, PAGES FROM BOOK WHERE PAGES BETWEEN 290 AND 300; SELECT FIRSTNAME, LASTNAME, COUNTRY FROM AUTHOR WHERE COUNTRY='AU' OR COUNTRY='BR'; SELECT FIRSTNAME, LASTNAME, COUNTRY FROM AUTHOR WHERE COUNTRY IN ('AU', 'BR')\n\nTo display the result set in alphabetical order, we add the ORDER BY clause to the SELECT statement (ascending by default; descending using DESC). In addition, we can specify column sequence number in ORDER BY clause.\n\nSELECT TITLE FROM BOOK ORDER BY TITLE; SELECT TITLE FROM BOOK ORDER BY TITLE DESC; SELECT TITLE, PAGES FROM BOOK ORDER BY 2; (order by pages)\n\nTo eliminate duplicates in the result set, we use the keyword DISTINCT. Besides, the GROUP BY clause groups a result set into subsets that has matching values for one or more columns. We can further restrict the number of rows with HAVING clause in the GROUP BY clause.\n\nSELECT DISTINCT(COUNTRY) FROM AUTHOR; SELECT COUNTRY, COUNT(COUNTRY) FROM AUTHOR GROUP BY COUNTRY; SELECT COUNTRY, COUNT(COUNTRY) AS COUNT FROM AUTHOR GROUP BY COUNTRY; SELECT COUNTRY, COUNT(COUNTRY) AS COUNT FROM AUTHOR GROUP BY COUNTRY HAVING COUNT(COUNTRY) > 4\n\nQuiz\n\nYou want to select author's last name from a table, but you only remember the author's last name starts with the letter B, which string pattern can you use? -- SELECT FIRSTNAME FROM AUTHOR WHERE LASTNAME LIKE 'B%'\n\nIn a SELECT statement, which SQL clause controls how the result set is displayed? -- ORDER BY clause\n\nWhich SELECT statement eliminates duplicates in the result set? -- SELECT DISTINCT(COUNTRY) FROM AUTHOR\n\nWhat is the default sorting mode of the ORDER BY clause? -- ascending\n\nWhich of the following can be used in a SELECT statement to restrict a result set? -- HAVING / WHERE / DISTINCT\n\nMost databases come with built-in SQL functions. Built-in functions can be included as part of SQL statements. Database functions significantly reduce the amount of data that needs to be retrieved. Built-in functions can speed up data processing.\n\nAggregate or Column Functions\n\nINPUT: collection of values (e.g. entire column)\n\nOUTPUT: single value\n\ne.g. SUM(), MIN(), MAX(), AVG() etc\n\nSELECT SUM(SALESPERSON) FROM PETSALE; SELECT SUM(SALESPERSON) AS SUM_OF_SALESPERSON FROM PETSALE; SELECT MAX(QUANTITY) FROM PETSALE; SELECT MIN(ID) FROM PETSALE WHERE ANIMAL='DOG'; SELECT AVG(SALEPRICE) FROM PETSALE; SELECT AVG(SALEPRICE / QUANTITY) FROM PETSALE WHERE ANIMAL='DOG'\n\nScaler and String Functions\n\nperforms operations on every input values. e.g. ROUND(), LENGTH(), UCASE(), LCASE() etc\n\nSELECT ROUND(SALEPRICE) FROM PETSALE; round up/down every value SELECT LENGTH(ANIMAL) FROM PETSALE; retrieve length of each value SELECT UCASE(ANIMAL) FROM PETSALE; SELECT * FROM PETSALE WHERE LCASE(ANIMAL)='cat'; SELECT DISTINCT(UCASE(ANIMAL)) FROM PETSALE\n\nDate and Time Functions\n\nMost databases contain special data types for dates and times.\n\nDATE YYYYMMDD\n\nTIME HHMMSS\n\nTIMESTAMP YYYYMMDDHHMMSSZZZZZZ\n\nDate / Time functions: YEAR(), MONTH(), DAY(), DAYOFMONTH(), DAYOFWEEK(), DAYOFYEAR(), WEEK(), HOUR(), MINUTE(), SECOND()\n\nSpecial registers: CURRENT_DATE, CURRENT_TIME\n\nSELECT DAY(SALEDATE) FROM PETSALE WHERE ANIMAL='CAT'; SELECT COUNT(*) FROM PETSALE WHERE MONTH(SALEDATE)='05'; SELECT (SALEDATE + 3 DAYS) FROM PETSALE; SELECT (CURRENT_DATE - SALEDATE) FROM PETSALE)\n\nSub-query is a query inside another query. This allows you to form more powerful queries than would have been otherwise possible.\n\nSELECT COLUMN1 FROM TABLE WHERE COLUMN2 = (SELECT MAX(COLUMN2) FROM TABLE); SELECT EMP_ID, F_NAME, L_NAME, SALARY FROM EMPLOYEES WHERE SALARY < (SELECT AVG(SALARY) FROM EMPLOYEES); SELECT EMP_ID, SALARY, (SELECT AVG(SALARY) FROM EMPLOYEES) AS AVG_SALARY FROM EMPLOYEES; SELECT * FROM (SELECT EMP_ID, F_NAME, L_NAME, DEP_ID FROM EMPLOYEES) AS EMP4ALL\n\nThere are several ways to access multiple tables in the same query:\n\nsub-queries\n\nimplicit JOIN\n\nJOIN operators (INNER JOIN, OUTER JOIN, etc)\n\nSELECT * FROM EMPLOYEES WHERE DEP_ID IN (SELECT DEPT_ID_DEP FROM DEPARTMENTS); SELECT * FROM EMPLOYEES WHERE DEP_ID IN (SELECT DEPT_ID_DEP FROM DEPARTMENTS WHERE LOC_ID='L0002'); SELECT DEPT_ID_DEP, DEP_NAME FROM DEPARTMENTS WHERE DEPT_ID_DEP IN (SELECT DEP_ID FROM EMPLOYEES WHERE SALARY > 70000); SELECT * FROM EMPLOYEES, DEPARTMENTS; (full join / Cartesian join / every row in 1st is joined with every row in 2nd) SELECT * FORM EMPLOYEES, DEPARTMENTS WHERE EMPLOYEES.DEP_ID=DEPARTMENTS.DEPT_ID_DEP; SELECT * FROM EMPLOYEES E, DEPARTMENTS D WHERE E.DEP_ID=D.DEPT_ID_DEP; SELECT EMPLOYEES.EMP_ID, DEPARTMENTS.DEPT_NAME FROM EMPLOYEES E, DEPARTMENTS D WHERE E.DEP_ID=D.DEPT_ID_DEP; SELECT E.EMP_ID, D.DEP_ID_DEP FROM EMPLOYEES E, DEPARTMENTS D WHERE E.DEP_ID=D.DEPT_ID_DEP\n\nQuiz\n\nWhich of the following will retrieve the LOWEST value of SALARY in a table called EMPLOYEES? -- SELECT MIN(SALARY) FROM EMPLOYEES\n\nAssume there exists an INSTRUCTOR table with several columns including FIRSTNAME, LASTNAME, etc. Which of the following is the most likely result set for the following query? SELECT DISTINCT(FIRSTNAME) FROM INSTRUCTOR -- LEON / PAUL / JOE\n\nWhich of the following queries will return the first name of the employee who earns the highest salary? -- SELECT FIRST_NAME FROM EMPLOYEES WHERE SALARY=(SELECT MAX(SALARY) FROM EMPLOYEES)\n\nWhich of the following queries will return the data for employees who belong to the department with the highest value of department ID? -- SELECT * FROM EMPLOYEES WHERE DEP_ID=(SELECT MAX(DEPT_ID_DEP) FROM DEPARTMENTS)\n\nA DEPARTMENTS table contains DEP_NAME, and DEPT_ID_DEP columns and an EMPLOYEES table contains columns called F_NAME and DEP_ID. We want to retrieve the Department Name for each Employee. Which of the following queries will correctly accomplish this? -- SELECT D.DEP_NAME, E.F_NAME FROM DEPARTMENTS D, EMPLOYEES E WHERE D.DEPT_ID_DEP=E.DEP_ID\n\nA Foreign Key is a set of columns referring to a primary key of another table. It is the Primary Key which uniquely identifies a row in a table.\n\nParent Table: a table containing a Primary Key that is related to at least one Foreign Key\n\nDependent Table: a table containing one or more Foreign Keys\n\nThe following six constraints are defined in a relational database model:\n\nentity integrity constraint\n\nreferential integrity constraint\n\nsemantic integrity constraint\n\ndomain constraint\n\nnull constraint\n\ncheck constraint\n\nIf a Primary Key was allowed to have NULL values the Entity Integrity Constraint of a table could be violated. The Referential Integrity ensures the validity of the data using a combination of Primary Keys and Foreign Keys.\n\nQuiz\n\nWhich Relational Constraint prevents duplicate rows in a table? -- Entity Integrity Constraint\n\nWhich Relational Constraint ensures the validity of the data using a combination of Primary Keys and Foreign Keys? -- Referential Integrity Constraint\n\nWhich of the following statements are correct about the primary keys? -- the value of the Primary Key must be unique for each instance of the entity; the Primary Key is immutable, i.e., once created the value of the Primary Key cannot be changed; Primary Keys cannot have missing or NULL values\n\nWhich of the following statement is true? -- a Foreign Key refers to a Primary Key of another table\n\nDatabases and SQL for Data Science -- Week 3\n\nThere are many benefits of Python for database programming:\n\necosystem: NumPy, Pandas, matplotlib, SciPy\n\nease of use\n\nportable\n\ndatabase APIs\n\nsupport for relational database systems\n\ndetailed documentation\n\nNotebooks are very popular in the field of data science because they run in an environment that allows creation and sharing of documents that contain live code, equations, visualizations and explanatory texts. A notebook interface is a virtual notebook environment used for programming.\n\nSQL API consists of a library of function calls as an application programming interface (API) for the DBMS.\n\nTwo concepts of the Python DB API:\n\nconnection objects\n\ndatabase connections\n\nmanage transaction\n\ncursor objects\n\ndatabase queries\n\nCursors created from the same connection are not isolated, i.e. any changes done to the database by a cursor are immediately visible to the other cursors. Cursors created from different connections can or cannot be isolated depending on how the transaction support is implemented.\n\nA database cursor is a control structure that enables traversal over the records in a database. It behaves like a file name or file handle in a programming language.\n\nfrom dbmodule import connect # create connection object Connection = connection('databasename', 'username', 'pswd') # create a cursor object Cursor = Connection.cursor() # run queries Cursor.execute('SELECT * FROM MYTABLE') Results = Cursor.fetchall() # free resources Cursor.close() Connection.close()\n\nThe ibm_db API provides a variety of useful Python functions for accessing and manipulating data in an IBM data server Database.\n\nThe ibm_db.exec_immediate() function prepares and executes a SQL statement. The parameters passed to the ibm_db.exec_immediate function include: connection; statement; options.\n\nQuiz\n\nA database cursor is a control structure that enables traversal over the records in a database. -- True\n\nThe ibm_db API provides a variety of useful Python functions for accessing and manipulating data in an IBM data server like Db2. -- True\n\nA DataFrame represents a tabular, spreadsheet-like data structure containing an ordered collection of columns, each of which can be a different value type. A pandas dataframe in Python can be used for storing the result set of a SQL query. -- True\n\nWhich of the following statement about Python is NOT correct? -- due to its proprietary nature, database access from Python is not available for many databases\n\nTo query data from tables in database a connection to the database needs to be established. Which of the following is NOT required to establish a connection with a relational database from a Python network. -- table and column names\n\nA simple SELECT statement retrieves data from one or more columns from a single table. The next level of complexity is retrieving data from two or more tables. To combine data from two tables, we use JOIN operator: 1) combines row from two or more tables; 2) based on a relationship.\n\nPrimaryKey-ForeignKey is the common JOIN operator\n\nJOIN operator is used to combine more than one table\n\nYou have to know the relationship between the tables\n\nINNER JOIN\n\nmost popular\n\nAn INNER JOIN operation returns only data from the rows in the tables that match the inner join criteria.\n\nSELECT B.BORROWER_ID, B.LASTNAME, B.COUNTRY, L.BORROWER_ID, L.LOAN_DATE FROM BORROWER B INNER JOIN LOAN L ON B.BORROWER_ID=L.BORROWER_ID SELECT B.LASTNAME, L.COPY_ID, C.STATUS FROM BORROWER B INNER JOIN LOAN L ON B.BORROWER_ID=L.BORROWER_ID INNER JOIN COPY C ON L.COPY_ID=C.COPY_ID\n\nLEFT OUTER JOIN\n\nA LEFT (OUTER) JOIN operation matches the results from two tables and displays all the rows from the left table, and combines the information with rows from the right table that matches the criteria specified in the query.\n\nSELECT B.BORROWER_ID, B.LASTNAME, B.COUNTRY, L.BORROWER_ID, L.LOAN_DATE FROM BORROWER B LEFT JOIN LOAN L ON B.BORROWER_ID=L.BORROWER_ID\n\nWhen using a LEFT JOIN, if the right table does not have a corresponding value, a NULL value was returned.\n\nRIGHT OUTER JOIN\n\nA RIGHT (OUTER) JOIN operation matches the results from two tables and displays all the rows from the right table, and combines the information with rows from the left table that matches the criteria specified in the query.\n\nSELECT B.BORROWER_ID, B.LASTNAME, B.COUNTRY, L.BORROWER_ID, L.LOAN_DATE FROM BORROWER B RIGHT JOIN LOAN L ON B.BORROWER_ID=L.BORROWER_ID\n\nIf you do not specify what type of OUTER JOIN you want to perform, it defaults to RIGHT OUTER JOIN. -- False (you need to specify which type of OUTER JOIN you want to perform - LEFT, RIGHT OR FULL)\n\nFULL OUTER JOIN\n\nThe FULL (OUTER) JOIN operation returns all rows from both tables, all rows from the left table and all rows from the right table. So, the FULL JOIN could return a very large result set.\n\nSELECT B.BORROWER_ID, B.LASTNAME, B.COUNTRY, L.BORROWER_ID, L.LOAN_DATE FROM BORROWER B FULL JOIN LOAN L ON B.BORROWER_ID=L.BORROWER_ID\n\nQuiz\n\nAn INNER JOIN returns only the rows that match. -- True\n\nA LEFT OUTER JOIN displays all the rows from the right table, and combines matching rows from the left table. -- False\n\nWhen using an OUTER JOIN, you must explicitly state what kind of OUTER JOIN you want - a LEFT JOIN, a RIGHT JOIN, or a FULL JOIN. -- True\n\nWhich of the following are valid types of JOINs? -- LEFT OUTER JOIN, RIGHT OUTER JOIN, FULL OUTER JOIN\n\nA FULL JOIN returns only the rows that match. -- False\n\nDatabases and SQL for Data Science -- Week 4\n\nMany of the real-world datasets are made available as .csv files. These are text files which contain data values typically separated by commas.\n\nWhen querying column names with mixed (upper and lower) case, we can use double quotes to specify mixed-case column names. By default, spaces are mapped to underscores. Other characters may also get mapped to underscores.\n\nSELECT \"Id\", \"Name_of_Dog\", \"Breed__dominant_breed_if_not_pure_breed_\" FROM DOGS\n\nWe can use backslash \\ as the escape character in cases where the query contains single quotes.\n\nselectQuery = 'SELECT * FROM DOGS WHERE \"Name_of_Dog\"=\\'Huggy\\''\n\nWe can use backslash \\ to split the query into multiple lines.\n\n%sql SELECT \"Id\", \"Name_of_Dog\", \\ FROM DOGS \\ WHERE \"Name_of_Dog\"='Huggy'\n\nOr we can use %%sql in the first row of the cell in the notebook.\n\n%%sql SELECT \"Id\", \"Name_of_Dog\", FROM DOGS WHERE \"Name_of_Dog\"='Huggy'\n\nData Analysis with Python -- Week 1\n\nImportance of Data Analysis: data is everywhere; data analysis helps us answer questions from data; discovering useful information; answering questions; predicting future or the unknown\n\nQuiz\n\nWhat does .csv file stand for? -- common separated values\n\nEach column contains a -- attribute or feature\n\nPython Packages for Data Science includes\n\nScientific computing libraries in Python: Pandas / Numpy / Scipy\n\nVisualization libraries in Python: Matplotlib / Seaborn\n\nAlgorithmic libraries in Python: Scikit-learn / Statsmodels\n\nQuiz\n\nWhat is a Python Library? -- a collection of functions and methods that allows you to perform lots of actions without writing your code\n\nScikit-learn is used for -- statistical modelling including regression and classification\n\nData acquisition is a process of loading and reading data into notebook from various sources. Two important factors to consider in importing data using Python: format and file path.\n\ndf.head(n) to show the first n rows of dataframe; df.tail(n) to show the bottom n rows of dataframe.\n\nQuiz\n\nWhat is the file path? -- the file path tells us where the data is stored\n\nWhat task does the following lines of code perform? path='xxx'; df.to_csv(path) -- exports your Pandas dataframe to a new .csv file, in the location specified by the variable path\n\nThere are some basic insights from the data once you load the data:\n\nunderstand your data before you begin any analysis\n\ncheck data types\n\npotential info and type mismatch / compatibility with Python method\n\ndf.dtypes\n\ncheck data distribution\n\ndf.describe() returns a statistical summary\n\ndf.info() provides a concise summary of the dataframe\n\nlocate potential issues with the data\n\nQuiz\n\nWhat is the name of what we want to predict? -- target\n\nWhat does .csv stand for? -- comma separated values\n\nWhat library is primarily used for data analysis? -- pandas\n\nSelect the libraries you will use for this course? -- matplotlib; pandas; scikit-learn\n\nWhat is the command to display the first five rows of a dataframe df? -- df.head()\n\nWhat task does the following command perform? df.to_csv(\"A.csv\") -- save the dataframe df to a .csv file called \"A.csv\"\n\nWhat attribute or method will give you the data type of each column? -- dtypes\n\nHow would you generate descriptive statistics for all the columns for the dataframe df? -- df.describe(include='all')\n\nData Analysis with Python -- Week 2\n\nData pre-processing, AKA data cleaning or data wrangling, is the process of converting or mapping data from the initial raw form into another format, in order to prepare the data for further analysis.\n\nDealing with Missing Values in Python\n\nMissing values occur when no data value is stored for a variable (feature) in an observation. Missing values could be represented as \"?\", \"N/A\", 0 or just a blank cell.\n\nCheck the data collection source\n\nDrop the missing values (variable / data entry)\n\nReplace the missing values (with average / by frequency (mode) / based on functions)\n\nLeave it as missing data\n\nUse dataframes.dropna(inplace=True) to drop missing values;\n\nUse dataframes.replace(miss_val, new_val) to replace missing values;\n\nQuiz\n\nHow would you access the column \"symboling\" from the dataframe df? -- df[\"symboling\"]\n\nHow would you replace the missing values in the column \"normalized-losses\" with the mean of that column? -- mean=df[\"normalized-losses\"].mean(); df[\"normalized-losses\"].replace(np.nan, mean)\n\nWhat is the correct symbol for missing data? -- nan\n\nData Formatting in Python\n\nData formatting means bringing data into a common standard of expression that allows users to make meaningful comparison. Data formatting ensures the data is consistent and easily understandable.\n\nnon-formatted: confusing / hard to aggregate / hard to compare\n\nformatted: more clear / easy to aggregate / easy to compare\n\ndf[\"city-mpg\"] = 235 / df[\"city-mpg\"] df.rename(columns={\"city-mpg\": \"city-L/100km\"}, inplace=True)\n\nSometimes the wrong data type is assigned to a feature. There are many data types in pandas: objects / int64 / float64.\n\nTo identify data types: dataframe.dtypes()\n\nTo convert data types: dataframe.astype()\n\nQuiz\n\nHow would you multiply each element in the column df[\"a\"] by 2 and assign it back to the column df[\"a\"]? -- df[\"a\"] = 2*df[\"a\"]\n\nHow would you rename the column \"city_mpg\" to \"city-L/100km\"? -- df.rename(columns={\"city_mpg\": \"city-L/100km\"}, inplace=True)\n\nData Normalization in Python\n\nData normalization refers to uniforming the features value with different range. By making the ranges consistent between variables, normalization enables a fair comparison between the different features, making sure they have the same impact.\n\nThere are several approaches for normalization:\n\nfeature scaling ([0,1]) df[\"len\"] = df[\"len\"]/df[\"len\"].max()\n\nmin-max ([0,1]) df[\"len\"] = (df[\"len\"]-df[\"len\"].min()) / (df[\"len\"].max()-df[\"len\"].min())\n\nz-score (hover around 0) df[\"len\"] = (df[\"len\"]-df[\"len\"].mean()) / df[\"len\"].std()\n\nQuiz\n\nWhat is the maximum value for feature scaling? -- 1\n\nConsider the column \"length\", select the correct code for simple feature scaling. -- df[\"length\"] = df[\"length\"]/df[\"length\"].max()\n\nBinning in Python\n\nBinning is a method of data pre-processing when you group values together into bins. We use binning to convert numeric variables into categorical variables for improved accuracy in the predictive models. In addition, we use data binning to group a set of numerical values into a smaller number of bins to have a better understanding of the data distribution.\n\nbins = np.linspace(min(df[\"price\"]), max(df[\"price\"]), 4) group_names = [\"low\", \"medium\", \"high\"] df[\"price_binned\"] = pd.cut(df[\"price\"], bins, labels=group_names, include_lowest=True)\n\nTurning categorical variables into quantitative variables in Python\n\nMost statistical modes cannot take in objects or strings as input and for model training only take the numbers as inputs. We encode the values by adding new features corresponding to each unique element in the original feature we would like to encode.\n\ncategorical -> numeric: add dummy variables for each unique category / assign 0 or 1 in each category (ONE-HOT ENCODING)\n\nUse pandas.get_dummies() method to convert categorical variables to dummy variables (0 / 1). E.g. pd.get_dummies(df['fuel'])\n\nQuiz\n\nWhy do we convert values of categorical values into numerical values? -- most statistical models cannot take in objects or strings as inputs\n\nWhat is the correct line of code to perform one-hot encoding on the column \"fuel\"? -- pd.get_dummies(df['fuel'])\n\nQuiz\n\nWhat task does the following line of code perform? df['peak-rpm'].repalce(np.nan, 5, inplace=True) -- replace the not a number values with 5 in the column 'peak-rpm'\n\nWhat task do the following lines of code perform? avg=df['bore'].mean(axis=0); df['bore'].replace(np.nan, avg, inplace=True) -- calculate the mean value for the 'bore' column and replace all the NaN values of that column by the mean value\n\nConsider the dataframe df, convert the column df['city-mpg'] to df['city-L/100km'] by dividing 235 by each element in the column 'city-mpg'. -- df['city-L/100km'] = 235 / df['city-mpg']\n\nWhat data type is the following set of numbers? 666, 1.1, 232, 23.12 -- float\n\nThe following code is an example of (df['len']-df['len'].mean()) / df['len'].std() -- z-score\n\nData Analysis with Python -- Week 3\n\nExploratory data analysis (EDA) is a preliminary step in data analysis to 1) summarize main characteristics of the data; 2) gain better understanding of the dataset; 3) uncover relationships between different variables; 4) extract important variables for the problem we are trying to solve.\n\nDescriptive statistics describe basic features of data and give short summaries about the sample and measures of the data.\n\nsummarize statistics using pandas describe() method df.describe()\n\nNaN values will be excluded if the method describe() is applied to a dataframe with NaN values\n\nsummarize the categorical data by using the method value_counts()\n\nvisualize numeric data using box plots (visualization of various distributions of the data)\n\nmedian / upper quartile / lower quartile / outlier\n\nvisualize the relationship between two variables as each observation represented as a point\n\npredictor / independent variables on x-axis\n\ntarget / dependent variables on y-axis\n\nQuiz\n\nSelect the appropriate table for the following line of code df = pd.DataFrame({'A': [\"a\", \"b\", \"c\", \"a\", \"a\"]}); df['A'].value_counts() -- a:3 b:1 c:1\n\nConsider the following scatter plot, what kind of relationship do the two variables have? -- positive linear relationship\n\nGrouping data can be done using dataframe.groupby() method\n\ncan be applied on categorical variables\n\ngrouping data into categories\n\ngrouping based on single or multiple variables\n\ndf_test = df['drive-wheels', 'body-style', 'price'] df_grp = df_test.groupby(['drive-wheels', 'body-style'], as_index=False).mean()\n\nPandas method pivot(): one variable displayed along the columns and the other variable display along the rows.\n\ndf_pivot = df_grp.pivot(index='drive-wheels', columns='body-style')\n\nHeatmap is to plot target variable over multiple variables.\n\nplt.pcolor(df_pivot, cmap='RdBBu') plt.colorbar() plt.show()\n\nQuiz\n\nThe following is the output from applying the groupby method. How many different groups are there? -- 3\n\nCorrelation is a statistical metric for measuring to what extent different variables are interdependent. E.g. lung cancer -> smoking; rain -> umberlla. Correlation does not imply causation.\n\nQuiz\n\nWhat is Correlation? -- measures to what extent different variables are interdepedent\n\nPearson Correlation: to measure the strength of the correlation between two features\n\ncorrelation coefficient\n\nclose to +1: large positive relationship\n\nclose to -1: large negative relationship\n\nclose to 0: no relationship\n\np-value\n\n< 0.001 strong certainty in the result\n\n< 0.05 moderate certainty in the result\n\n< 0.1 weak certainty in the result\n\n> 0.1 no certainty in the result\n\nstrong correlation\n\ncorrelation coefficient close to +1 or -1\n\np-value less than 0.001\n\nTaking all variables into account, we can now create a heatmap that indicates the correlation between each of the variables with one another. This correlation heatmap gives us a good overview of how the different variables are related to one another and, most importantly, how these variables are related to price.\n\nAnalysis of Variance (ANOVA): to find the correlation between different groups of a categorical variable\n\nF-test score: variation between sample group means divided by variation within sample group\n\nsmall F implies poor correltaion between variale categories and target variable\n\nlarge F implies strong correlation between variable categories and target variable\n\np-value: confidence degree (whether the obtained result is statistically significant)\n\nFor instance, ANOVA between \"Honda\" and \"Subaru\":\n\ndf_anova = df[['make', 'price']] grouped_anova = df_anova.groupby(['make']) anova_results_l = stats.f_oneway(grouped_anova.get_group('honda')['price'], grouped_anova.get_group('subaru')['price'])\n\nANOVA results: F=0.19744031275, p=F_onewayResult(statistic=0.1974430127), pvalue=0.660947824\n\nThe prices between Honda's and Subaru are not significantly different since the F-score is veryl small (0.197). However, there is a strong correlation between a categorical variable and other variables if the ANOVA test gives us a large F-test score and a small p-value.\n\nQuiz\n\nWhat task does the method value_counts perform? -- return counts of unique values\n\nWhat does the vertical axis in a scatter plot represent? -- dependent\n\nWhat is the largest possible element resulting in the operation df.corr()? -- 1\n\nIf the p-value of the Pearson Correlation is 1, then -- the variables are not correlated\n\nConsider the dataframe df, what method provides the summary statistics? -- df.describe()\n\nWhat is the minimum possible value of Pearson's Correlation? -- -1\n\nWhat is the Pearson Correlation between variables X and Y if X = Y? -- 1\n\nData Analysis with Python -- Week 4\n\nA model can be thought of as a mathematical equation used to predict a value given one or more other values. Usually the more relevant data you have the more accuracte your model is.\n\nLinear regression refers to one independent variable to make a prediction\n\nMultiple linear regression refers to multiple independent variables to make a prediction\n\nSimple Linear Regression (LR)\n\nthe predictor (independent) variable - $x$\n\nthe target (dependent) variable -$y$\n\n$y = b_0 + b_1 x$\n\nwhere $b_0$ is the intercept and $b_1$ is the slope\n\nMultiple Linear Regression (MLR)\n\nis used to explain the relationship between:\n\none continuous target (Y) variable\n\ntwo or more predictor (X) variables\n\n$Y = b_0 + b_1x_1 + b_2x_2 + b_3x_3 + ...$\n\nwhere $b_0$ is the intercept (X=0) and $b_1$ is the coefficient or parameter of $x_1$ and $b_2$ is the coefficient or parameter of $x_2$ and so on.\n\nQuiz\n\nConsider the following lines of code, what is the name of the column that contains the target values? Y = df['price'] -- 'price'\n\nConsider the following equation, what is the parameter $b_0$? $y = b_0 + b_1x$ -- the intercept\n\nRegression plot gives us a good estimate of 1) relationship between two variables; 2) the strength of the correlation; 3) the direction of the relationship (positive or negative)\n\nRegression plot shows us a combination of\n\nthe scatterplot: where each point represents a different y\n\nthe fitted linear regression line y'\n\nimport seaborn as sns sns.regplot(x='highway-mpg', y='price', data=df) plt.ylim(0,)\n\nAttention on the spread of the residuals: Randomly spread out around x-axis then a linear model is appropriate.\n\nDistribution plot shows us a combination of\n\nthe fitted values that result from the model\n\nthe actual values (groundtruth)\n\nimport seaborn as sns ax1 = sns.distplot(df['price'], hist=False, color='r', label='Actual Value') sns.distplot(Yhat, hist=False, color='r', label='Fitted Value', ax=ax1)\n\nQuiz\n\nConsider the following Residual Plot, is our linear model correct? -- yes (randomly spread out around x-axis)\n\nSelect the distribution plot that demonstrates the best-predicted values compared to actual values -- b\n\nPolynomial regression is a special case of the general linear regression model. It is useful for describing curvilinear relationships.\n\nquadratic -- 2nd order\n\ncubic -- 3rd order\n\nhigher order\n\nf = np.polyfit(x, y, 3) p = np.polydl(f) print(p)\n\nWe can also have multi-dimensional polynomial linear regression\n\n$Y = b_0 + b_1X_1 + b_2X_2 + b_3X_1X_2 + b_4X_1^2 + b_5X_2^2$\n\nfrom sklearn.preprocessing import PolynomialFeatures pr = PolynomialFeatures(degree=2) x_polly = pr.fit_transform(x[['horsepower', 'curb-weight']], include_bias=False)\n\nAs the dimension of the data gets larger, we may want to normalize multiple features.\n\nPipeline sequentially perform a series of transformations and the last step carries out a prediction. A pipeline typically contains:\n\nnormalization\n\npolynomial transform\n\nlinear regression\n\nInput = [('scale', StandardScaler()), ('polynomial', PolynomialFeatures(degree=2)), ('mode', LinearRegression())] pipe = Pipeline(Input) # a pipeline object pipe.train(X['horsepower', 'curb-weight', 'engine-size', 'highway-mpg'], y) yhat = pipe.predict(X[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])\n\nQuiz\n\nWhat functions are used to generate Polynomial Regression with more than one dimension? -- pr = PolynomialFeatures(degree=2); pr.fit_transform([1,2], include_bias=False\n\nSelect the line of code that imports the module required for polynomial features. -- from sklearn.preprocessing import PolynomialFeatures\n\nThe measures that we use for in-sample evaluation are a way to numerically determine how good the model fits on our data. Two important measures are: Mean Squared Error(MSE) and R-squared (or coefficient of determination).\n\nTo measure the MSE, we find the difference between the actual values and the predicted values then square it.\n\nfrom sklearn.metrics import mean_squared_error mean_squared_error(Y, Yhat)\n\nR-squared is a measure to determine how close the data is to the fitted regression line. $R^2 = (1 - \\frac{MSE_{regression}}{MSE_{dataaverage}})$ So $R^2$ is in range [0,1]\n\nlm = LinearRegression() lm.fit(X, Y) lm.score(X, y)\n\nQuiz\n\nConsider the following lines of code, what value does the variable out contain? lm = LinearRegression(); lm.fit(X, Y); out=lm.score(X, y) -- the coefficient of determination or $R^2$\n\nSelect the correct function to calculate the mean squared error between yhat and y? -- from sklearn.metrics import mean_squared_error; mean_squared_error(y, yhat)\n\nDecision Making: determining a good model fit\n\ndo the predicted values make sense?\n\nvisualization (regression plot / residual plot)\n\nnumerical measures for evaluation (MSE, $R^2$)\n\ncomparing models\n\nQuiz\n\nIf the predicted function is $y = b_0 + b_1x_1 + b_2x_2 + b_3x_3 + b_4x_4$. The method is: -- multiple linear regression\n\nWhat steps do the following lines of code perform? Input=[('scale',StandardScaler()),('model',LinearRegression())]; pipe=Pipeline(Input); pipe.fit(Z,y); ypipe=pipe.predict(Z) -- standardize the data, then perform a prediction using a linear regression model using the features z and targets y\n\nWe create a polynomial feature as follows \"PolynomialFeatures(degree=2)\", what is the order of the polynomial? -- 2\n\nWhat value of $R^2$ (coefficient of determination) indicates your model performs best? -- 1\n\nThe larger the mean squared error, the better your model has performed. -- False\n\nConsider the following equation: $y = b_0 + b_1x$. The value y is what? -- the target or dependent variable\n\nData Analysis with Python -- Week 5\n\nIn-sample evaluation tells us how well our model will fit the data used to train it. It does not tell us how well the trained model can be used to predict new data.\n\nIn-sample data or training data\n\nOut-of-sample evaluation or test set\n\nBuild and train the model with a training set, and use test set to assess the performance of the predictive model.\n\nX_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.3, random_state=0)\n\nCross Validation is the most common out-of-sample evaluation metrics, and it enables more effective use of data (each observaion is used for both training and testing).\n\nfrom sklearn.model_selection import cross_val_score scores = cross_val_score(lr, X_data, y_data, cv=10) np.mean(scores)\n\ncross_val_predict() returns the prediction that was obtained for each element when it was in the test set.\n\nfrom sklearn.model_selection import cross_val_predict yhat = cross_val_predict(lr2e, X_data, y_data, cv=10)\n\nQuiz\n\nWhat is the correct use of the \"train_test_split\" function such that 90% of the data samples will be utilized for training, the parameter \"random_state\" is set to zero, and the input variables for the features and targets are X_data, y_data respectively? -- train_test_split(X_data, y_data, test_size=0.1, random_state=0)\n\nWhat is the problem with the in-sample evaluation? -- it does not tell us how the trained model can be used to predict new data\n\nHow to do the model selection\n\nUnderfitting, where the model is too simpe to fit the data.\n\nOverfitting, where the model is too flexible and fits the noise rather than function.\n\nNoise term is one reason for the error (becuase the rror is random, we cannot predict it), and it is also reffered to as irreducible error.\n\nQuiz\n\nWhat model should you select? -- b (not underfitting or overfitting)\n\nThe following is an example of? -- underfitting\n\nRidge Regression is useful to prevent overfitting. Ridge regression controls the magnitude of these polynomial coefficients by introducing the parameter alpha. Alpha is a parameter, or hyperparameter, that we select before ftting or training the model.\n\nfrom sklearn.linear_model import Ridge RidgeModel = Ridge(alpha=0.1) RidgeModel.fit(X, y) yhat = RidgeModel.predict(X)\n\nQuiz\n\nSelect the model with the optmimum value of alpha. -- b\n\nThe following models were all trained on the same data, select the model with the heighest value for alpha. -- c\n\nGrid Search allows us to scan through multiple free parameters with few lines of code. Scikit-learn has a means of automatically iterating over these hyperparameters using cross-validation called Grid Search.\n\nWe use the validation data to pick the best hyperparameters.\n\nfrom sklearn.linear_model import Ridge from sklearn.model_selection import GridSearchCV parameters1 = [{'alpha': [0.001, 0.1, 1.0, 100], 'normalize': [True, False]}] RR = Ridge() Grid1 = GridSearchCV(RR, parameters1, cv=10) Grid1.fit(X_data[['horsepower', 'curb-weight', 'engine-size', 'highwat-mpg']], y_data) Grid1.best_estimator_ scores = Grid1.cv_results_ scores['mean_test_score]\n\nQuiz\n\nWhat is the correct use of the \"train_test_split\" function such that 40% of the data samples will be utilized for testing, the parameter \"random_state\" is set to zero, and the input variables for the features and targets are X_data, y_data respectively? -- train_test_split(X_data, y_data, test_size=0.4, random_state=0)\n\nWhat is the correct use of the \"train_test_split\" function such that 40 samples will be utilized for testing, the parameter \"random_state\" is set to zero, and the input variables for the features and targets are X_data, y_data respectively? -- train_test_split(X_data, y_data, test_size=40, random_state=0)\n\nWhat is the code to create a ridge regression object RR with an alpha term equal to 10? -- RR = Ridge(alpha=10)\n\nWhat dictionary value would we use to perform a grid search to determine if normalization should be used and for testing the following values of alpha 1, 10, 100? -- [{'alpha': [1,10,100], 'normalize':[True,False]}]\n\nYou have a linear model: the average R^2 value on your training data is 0.5, you perform a 100th order polynomial transform on your data then use these values to train another model. After this step, your average R^2 is 0.99, which of the following comments is correct? -- the results on your training data is not the best indicators of how your model performs; you should use your test data to get a better idea\n\nThe following is an example of what? -- overfitting\n\nThe following is an example of what? -- underfitting\n\nData Visualization with Python -- Week 1\n\nData visualization is a way to show a complex data in a form that is graphical and easy to understand. But why?\n\nfor exploratory data analysis\n\ncommunicate data clearly\n\nshare unbiased representation of data\n\nuse them to support recommendations to different stakeholders\n\nWhen creating a visual, always remember these best practices:\n\nless is more effective\n\nless is more attractive\n\nless is more impactive\n\nMatplotlib Architecture has three main layers: 1) Backend Layer; 2) Artist Layer; 3) Scripting Layer.\n\nBackend layer has three built-in abstract interface classes:\n\nFigureCanvas: matplotlib.backend_bases.FigureCanvas\n\nencompasses the area onto which the figure is drawn\n\nRenderer: matplotlib.backend_bases.Renderer\n\nknows how to draw on the FigureCanvas\n\nEvent: matplotlib.backend_bases.Event\n\nhandles user inputs such as keyboard strokes and mouse clicks\n\nArtist layer is comprised of one main object -- Artist: knows how to use the Renderer to draw on the canvas. Title, lines, tick labels, and images, all correspond to individual Artist instances. Two types of Artist objects:\n\nprimitive: Line2D, Rectangle, Circle, and Text\n\ncomposite: Axis, Tick, Axes, and Figure\n\n(each compositive artist may contain other composite artists as well as primitive artists)\n\nScripting layer is comprised mainly of pyplot, a scripting interface that is lighter than the Artist layer.\n\n%matplotlib inline\n\nA magic function starts with % in matploblib, and to enforce plots to be rendered within the browser, you pass it inline as the backend.\n\nA line plot is a type of plot which displays information as a series of data points called 'markers' connected by straight line segments.\n\nQuiz\n\nData visualization is used to explore a given dataset and perform data analytics and build predictive models. -- False\n\nMatplotlib was created by John Hunter, an American neurobiologist, and was originally developed as an EEG/ECoG visualization tool. -- True\n\nThe Backend, Artist, and Scripting Layers are three layers that make up the Matplotlib architecture. -- True\n\nUsing the notebook backend, you cannot modify a figure after it is rendered. -- False (you cannot modify the figure using inline backend, but can modify the figure using the notebook backend)\n\nWhich of the following are examples of Matplotlib magic functions? -- %matplotlib inline and %matplotlib notebook\n\nData Visualization with Python -- Week 2\n\nA area plot is also known as area chart or area graph, and it is commonly used to represent cumulated totals using numbers or percentages over time.\n\nA histogram is a way of representing the frequency distribution of a variable.\n\nA bar chart is commonly used to compare the values of a variable at a given point in time.\n\nQuiz\n\nArea plots are stacked by default. -- True\n\nWhich of the following codes uses the artist layer to create a stacked area plot of the data in the pandas dataframe, area_df? --\n\nax = area_df.plot(kind='area', figsize=(20,10)) ax.set_title('xxx') ax.set_ylabel('xxx') ax.set_xlabel('xxx')\n\nThe following code will create an unstacked area plot of the data in the pandas dataframe, area_df, with a transparency value of 0.35? -- False\n\nimport matplotlib.pyplot as plt transparency = 0.35 area_df.plot(kind='area', alpha=transparency, figsize=(20,10)) plt.title('xxx') plt.ylabel('xxx') plt.xlabel('xxx') plt.show()\n\nGiven a pandas series, series_data, which of the following will create a histogram of series_data and align the bin edges with the horizontal tick marks? --\n\ncount, bin_edges = np.histogram(series_data) series_data.plot(kind='hist', xticks=bin_edges)\n\nGiven a pandas dataframe, question, which of the following will create a horizontal bar chart of the data in question? -- question.plot(kind='barh')\n\nA pie chart is a circular statistical graphic divided into slices to illustrate numerical proportion.\n\nMost argue that pie charts fail to accurately display data with any consistency. Bar charts are much better when it comes to representing the data in a consistent way and getting the message across.\n\nA box plot is a way of statistically representing the distribution of given data through five main dimensions median / first quartile / third quartile / minimum / maximum / (outliers).\n\nA scatter plot is a type of plot that displays values pertaining to typically two variables against each other. Usually it is a dependent variable to be plotted against an independent variable in order to determine if any correlation between the two variables exists.\n\nQuiz\n\nPie chats are less confusing than bar charts and should be your first attempt when creating a visual. -- False\n\nWhat do the letters in the box plot above represent? -- A = median; B = third quartile; C = first quartile; D = inner quartile range; E = minimum; F = Outliers\n\nWhat is the correct combination of function and parameter to create a box plot in Matplotlib? -- function = plot, parameter = kind, with value = 'box'\n\nWhich of the lines of code below will create the following scatter plot, given the pandas dataframe df_total? --\n\nimport matplotlib.pyplot as plt df_total.plot(kind='scatter', x='year', y='total') plt.title('xxx') plt.xlabel('xxx') plt.ylabel('xxx') plt.show()\n\nA bubble plot is a variation of the scatter plot that displays one dimension of data. -- False (actually 3 dimensions)\n\nData Visualization with Python -- Week 3\n\nA waffle chart is an interesting visualization that is normally created to display progress towards goals.\n\nA word cloud is a depiction of the frequency of different words in some textual data.\n\nSeaborn is a Python visualization library based on Matplotlib. And using Seaborn can effecively reduce the numbre of lines of code to create a plot, e.g. regression plot.\n\nQuiz\n\nSeaborn is a Python visualization library that provides a high-level interface for drawing attractive statistical graphics, such as regression plots and box plots. -- True\n\nThe following code creates the following regression plot. -- False (marker)\n\nA regression plot is a great way to visualize data in relation to a whole, or to highlight progress against a given threshold. -- False (waffle chart)\n\nIn Python, creating a waffle chart is straightforward since we can easily create one using the scripting layer of Matplotlib. -- False\n\nA word cloud -- is a depiction of the frequency of different words in some textual data; is a depiction of the meaningful words in some textual data, where the more a specific word appears in the text, bigger and bolder it appears in the word cloud; can be generated in Python using the word_cloud package that was developed by Andreas Mueller\n\nFolium is a powerful Python library that helps you create several types of Leaflet maps. It enables both the binding of data to a map for choropleth visualizations as well as passing visualizations as markers on the map.\n\nA choropleth map is a thematic map in which areas are shaded or patterned in proportion to the measurement of the statistical varaible being displayed on the map, such as population density or per capita income. The higher the measurement the darker the color.\n\nQuiz\n\nStamen Terrain, Stamen Toner, and Mapbox Bright, are three tile styles of Folium maps. -- True\n\nStamen Toner is the right tile style of Folium maps for visualizing and exploring river meanders and coastal zones of a given geographical area. -- True\n\nYou cluster markers, superimposed onto a map in Folium, using a marker cluster object. -- True\n\nThe following code will generate a map of Spain, displaying its hill shading and natural vegetation. folium.Map(location=[-40.4637,-3.7492], zoom_start=6, tiles='Stamen Toner') -- False\n\nIn choropleth maps, the higher the measurement of the statistical variable being displayed on the map, the lighter the color. -- False\n\nMachine Learning with Python -- Week 1\n\nMachine Learning is the subfield of computer science that gives computers the ability to learn without being explicitly programmed.\n\nMajor machine learning techniques:\n\nregression / estimation\n\npredicting continuous values\n\nclassification\n\npredicting the item class / category of a case\n\nclustering\n\nfinding the structure of data; summarization\n\nassociations\n\nassociating frequent co-occurring items / events\n\nanomaly detection\n\ndiscovering abnormal and unusual cases\n\nsequence mining\n\npredicting next events; click-stream (Markov Model, HMM)\n\ndimension reduction\n\nreducing the size of data\n\nrecommendation systems\n\nrecommending items\n\nDifference between AI / ML / DL:\n\nArtificial Intelligence (or AI) tries to make computers intelligent in order to minic the cognitive functions of humans. So it is a general field with a broad scope including: Computer Vision, Language Processing, Creativity, and Summarization.\n\nMachine Learning (or ML) is the branch of AI that covers the statistical part of Artificial Intelligence, which teaches the computer to solve problems by looking at hundreds or thousands of examples, learning from them, and then using that experience to solve the same problem in new situations.\n\nDeep Learning (or DL) is a very special field of Machine Learning where computers can actually learn and make intelligent decisions on their own. It involves a deeper level of automation in comparison with most machine learning algorithms.\n\nPython libraries for machine learning:\n\nnumpy / scipy / matplotlib / pandas / scikit-learn\n\nMore about scikit-learn:\n\nfree software machine learning library\n\nclassification, regression, clustering algorithms\n\nworks with numpy and scipy\n\ngreat documentation\n\neasy to implement\n\n(data preprocessing -> train/test split -> algorithm setup -> model fitting -> prediction -> evaluation -> model export)\n\nfrom sklearn import preprocessing X = preprocessing.StandardScaler().fit(X).transform(X) from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) from sklearn import svm clf = svm.SVC(gamma=0.001, C=100.) clf.fit(X_train, y_train) yhat = clf.predict(X_test) from sklearn.metrics import confusion_matrix print(confusion_matrix(y_test, yhat, labels[1,0])) import pickle s = pickle.dump(clf)\n\nSupervise means to observe and direct the execution of a task, project or activity. Supervised learning has a more controlled environment, and it has more evaluation methods than unsupervised learning:\n\nClassification is the process of predicting discrete class labels or categories.\n\nRegression is the process of predicting continuous values.\n\nUnsupervised models works on its won. Unsupervised learning has more difficult algorithms than supervised learning since we know little to no information about the data, or the outcomes that are to be expected:\n\nClustering is grouping of data points or objects that are somehow similar by: discovering structure / summarization / anomaly detection\n\nQuiz\n\nSupervised learning deals with unlabeled data, while unsupervised learning deals with labelled data. -- False\n\nWhich of the following is not true about machine learning? -- machine learning gives computers the ability to make decision by writing down rules and methods and being explicitly prgrammed.\n\nWhich of the following groups are not Machine Learning techniques? -- numpy, scipy and scikit-learn\n\nThe regression technique in machine learning is a group of algorithms that are used for -- predicting a continuous value; e.g. predicting the price of a house based on its characteristics\n\nIn constrast to supervised learning, unsupervised learning has more models and more evaluation methods that can be used in order to ensure the outcome of the model is accurate. -- False\n\nMachine Learning with Python -- Week 2\n\nRegression is the process of predicting a continuous value. In regression, there are two types of variables, a dependent variable (y), and one or more independent variables (X). The key point in the regression is that our dependent variable should be continuous and cannot be a discrete value.\n\nTypes of regression models:\n\nsimple regression\n\nsimple linear regression\n\nsimple non-linear regression\n\nmultiple regression\n\nmultiple linear regression\n\nmultiple non-linear regression\n\nLinear Regression\n\nLinear regression model representation: $y = \\theta_0 + \\theta_1 x_1$\n\nPros of linear regression: very fast / no parameter tuning / easy to understand and highly interpretable\n\nTraining accuracy\n\nthe percentage of correct predictions that the model makes when using the test dataset\n\nhigh training accuracy not necessarily a good thing\n\nresult of overfitting (the model is overly trained to the dataset, which may capture noise and produce a non-generalized model)\n\nOut-of-sample accuracy\n\nthe percentage of correct predictions that the model makes on data that the model has not been trained on\n\nimportant that our models have a high out-of-sample accuracy\n\nTrain / Test split evaluation approach:\n\ntest on a portion of training set\n\nhigh training accuracy\n\nlow out-of-sample accuracy\n\ntrain / test split\n\nmutually exclusive\n\nmore accurate evaluation on out-of-sample accuracy\n\nhighly dependent on which datasets the data is trained and tested\n\nEvaluation metrics are used to explain the performance of a model. In the context of regression, the error of a model is the difference between the data points and the trend line generated by the algorithm. The error types include MAE (mean absolute error), MSE (mean squared error), RMSE (root mean squared error), RAE (relative absolute error), and RSE (relative squared error).\n\nMultiple Linear Regression\n\nMultiple linear regression measures independent variables effectiveness on prediction, and also enables predicting impacts of changes.\n\nMultiple linear regression model representation: $y = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_n x_n$, which can be simplified as $y = \\theta^T x$.\n\nHow to estimate $\\theta$?\n\nordinary least squares\n\nlinear algebra operations\n\ntakes a long time for large datasets\n\nan optimization algorithm\n\ngradient descent\n\nproper approach for large datasets\n\nPolynomial Regression\n\nPolynomial regression fits a curve line to your data, e.g. $y = \\theta_0 + \\theta_1 x + \\theta_2 x^2 + \\theta_3 x^3$. A polynomial regression model can be transformed into a linear regression model.\n\nPolynomial regression models can fit using the model of least squares. Least squares iis a method of estimating the unknown parameters in a linear regression model by minimizing the sum of squares of the differences between the observed dependent variable in the given dataset and those predicted by the linear function.\n\nWhat is non-linear regression?\n\nto model non-linear relationship between the dependent variable and a set of independent variables\n\n$y$ must be a non-linear function of the parameters $\\theta$, not necessarily the features $X$.\n\nQuiz\n\nMultiple Linear Regression is appropriate for -- predicting tomorrow's rainfall amount based on the wind speed and temperature\n\nWhich of the following is the meaning of \"out-of-sample accuracy\" in the context of evaluation of models? -- \"out-of-sample accuracy\" is the percentage of correct predictions that the model makes on data that the model has NOT been trained on\n\nWhen should we use Multiple Linear Regression? -- when we would like to identify the strength of the effect that the independent variables have on a dependent variable; when we would like to predict impacts of changes in independent variables on a dependent variable\n\nWhich of the following statements are TRUE about Polynomial Regression? -- polynomial regression fits a curve line to your data\n\nWhich sentence is NOT TRUE about non-linear regression? -- non-linear regression must have more than one dependent variable\n\nMachine Learning with Python -- Week 3\n\nClassification, as a supervised learning approach, means categorizing some unknown items into a discrete set of categories or \"classes\". The target attribute is a categorical variable. Classification determines the class label for an unlabelled test case.\n\nA multi-class classifier is a classifier that can predict a field with multiple discrete values, such as \"DrugA\", \"DrugX\", or \"DrugY\".\n\nClassification algorithms in machine learning: decision trees (ID3, C4.5, C5.0) / Naive Bayes / Linear Discriminant Analysis / K-Nearest Neighbours / Logistic Regression / Neural Networks / Support Vector Machines (SVM)\n\nK-Nearest Neighbours\n\nGiven the dataset with predefined labels, we need to build a model to be used to predict the class of a new or unknown case. The K-Nearest Neighbours algorithm is a classification algorithm that takes a bunch of labeled points and uses them to learn how to label other points.\n\na method for classifying cases based on their similarity to other cases\n\ncases that are near each other are said to be neighbours\n\nbased on similar cases with same class labels are near each other\n\nThe K-Nearest Neighbours algorithm:\n\npick a value for K\n\ncalculate the distance of unknown case from all cases\n\nselect the K-observations in the training data that are \"nearest\" to the unknown data point\n\npredict the response of the unknown data point using the most popular response value from the K-nearest neighbours\n\nCalculating the similarity / distance in a multi-dimensional space: $dis(X1,X2) = \\sqrt{(54-50)^2 + (190-200)^2 + (3-8)^2} = 11.87$\n\nDetermining the best value of K for KNN:\n\na low value of K causes results capturing the noise or anomaly in data\n\na low value of K causes a highly complex model, resulting in overfitting\n\na high value of K causes overly generalization\n\nthe general solution is to reserve a part of your data for testing the accuracy of the model, to choose K equals one, to use the training part for modelling, to calculate the accuracy of prediction, to repeat the proess increasing the K, and to determine the K which gives the best accuracy\n\nEvaluation metrics provide a key role in the development of a model, as they provide insights into areas that might require improvement.\n\nJaccard index (ideal to be 1)\n\nF1-score (ideal to be 1)\n\nLog loss (ideal to be 0)\n\nDecision Trees\n\nThe basic intuition behind a decision tree is to map out all possible deicision paths in the form of a tree. Decision trees are built by splitting the training set into distinct nodes. One node in a Decision Tree contains all of or most of, one category of the data.\n\nDecision Trees learning algorithm:\n\nchoose an attribute from your dataset\n\ncalculate the significance of attribute in splitting of data\n\nsplit data based on the value of the best attribute\n\ngo to step 1\n\nDecision Trees are built using recursive partitioning to classify the data.\n\nWhich attribute is the best? more predictiveness, less impurity, lower entropy.\n\nImpurity of nodes is calcualted by entropy of data in the node.\n\nEntropy is a measure of randomness or uncertainty.\n\nThe lower the entropy, the less uniform the distribution, the purer the node.\n\n$entropy = p(A)*log(p(A)) - p(B)*log(p(B))$.\n\nThe attribute is determined best to make the tree with the higher Information Gain after splitting.\n\nInformation Gain is the information that can increase the level of uncertainty after splitting.\n\n$informationGain = (entropyBeforeSplit) - (weightedEntropyAfterSplit)$\n\nLogistic Regression\n\nLogistic Regression is a classification algorithm for categorical variables (for both binary classificaiton or multi-class classification). Some applications include:\n\npredicting the probability of a person having a heart attack\n\npredicting the mortality in injured patients\n\npredicting a customer's propensity to purchase a product or halt a subcription\n\npredicting th"
    }
}
{
    "id": "dbpedia_8307_0",
    "rank": 25,
    "data": {
        "url": "https://pycaret.readthedocs.io/en/stable/api/clustering.html",
        "read_more_link": "",
        "language": "en",
        "title": "Clustering — pycaret 3.0.4 documentation",
        "top_image": "",
        "meta_img": "",
        "images": [],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "data: dataframe-like\n\nData set with shape (n_samples, n_features), where n_samples is the number of samples and n_features is the number of features. If data is not a pandas dataframe, it’s converted to one using default column names.\n\ndata_func: Callable[[], DATAFRAME_LIKE] = None\n\nThe function that generate data (the dataframe-like input). This is useful when the dataset is large, and you need parallel operations such as compare_models. It can avoid broadcasting large dataset from driver to workers. Notice one and only one of data and data_func must be set.\n\nindex: bool, int, str or sequence, default = True\n\nHandle indices in the data dataframe.\n\nIf False: Reset to RangeIndex.\n\nIf True: Keep the provided index.\n\nIf int: Position of the column to use as index.\n\nIf str: Name of the column to use as index.\n\nIf sequence: Array with shape=(n_samples,) to use as index.\n\nordinal_features: dict, default = None\n\nCategorical features to be encoded ordinally. For example, a categorical feature with ‘low’, ‘medium’, ‘high’ values where low < medium < high can be passed as ordinal_features = {‘column_name’ : [‘low’, ‘medium’, ‘high’]}.\n\nnumeric_features: list of str, default = None\n\nIf the inferred data types are not correct, the numeric_features param can be used to define the data types. It takes a list of strings with column names that are numeric.\n\ncategorical_features: list of str, default = None\n\nIf the inferred data types are not correct, the categorical_features param can be used to define the data types. It takes a list of strings with column names that are categorical.\n\ndate_features: list of str, default = None\n\nIf the inferred data types are not correct, the date_features param can be used to overwrite the data types. It takes a list of strings with column names that are DateTime.\n\ntext_features: list of str, default = None\n\nColumn names that contain a text corpus. If None, no text features are selected.\n\nignore_features: list of str, default = None\n\nignore_features param can be used to ignore features during preprocessing and model training. It takes a list of strings with column names that are to be ignored.\n\nkeep_features: list of str, default = None\n\nkeep_features param can be used to always keep specific features during preprocessing, i.e. these features are never dropped by any kind of feature selection. It takes a list of strings with column names that are to be kept.\n\npreprocess: bool, default = True\n\nWhen set to False, no transformations are applied except for train_test_split and custom transformations passed in custom_pipeline param. Data must be ready for modeling (no missing values, no dates, categorical data encoding), when preprocess is set to False.\n\ncreate_date_columns: list of str, default = [“day”, “month”, “year”]\n\nColumns to create from the date features. Note that created features with zero variance (e.g. the feature hour in a column that only contains dates) are ignored. Allowed values are datetime attributes from pandas.Series.dt. The datetime format of the feature is inferred automatically from the first non NaN value.\n\nimputation_type: str or None, default = ‘simple’\n\nThe type of imputation to use. Unsupervised learning only supports ‘imputation_type=simple’. If None, no imputation of missing values is performed.\n\nnumeric_imputation: str, default = ‘mean’\n\nMissing values in numeric features are imputed with ‘mean’ value of the feature in the training dataset. The other available option is ‘median’ or ‘zero’.\n\ncategorical_imputation: str, default = ‘constant’\n\nMissing values in categorical features are imputed with a constant ‘not_available’ value. The other available option is ‘mode’.\n\ntext_features_method: str, default = “tf-idf”\n\nMethod with which to embed the text features in the dataset. Choose between “bow” (Bag of Words - CountVectorizer) or “tf-idf” (TfidfVectorizer). Be aware that the sparse matrix output of the transformer is converted internally to its full array. This can cause memory issues for large text embeddings.\n\nmax_encoding_ohe: int, default = -1\n\nCategorical columns with max_encoding_ohe or less unique values are encoded using OneHotEncoding. If more, the encoding_method estimator is used. Note that columns with exactly two classes are always encoded ordinally. Set to below 0 to always use OneHotEncoding.\n\nencoding_method: category-encoders estimator, default = None\n\nA category-encoders estimator to encode the categorical columns with more than max_encoding_ohe unique values. If None, category_encoders.basen.BaseN is used.\n\nrare_to_value: float or None, default=None\n\nMinimum fraction of category occurrences in a categorical column. If a category is less frequent than rare_to_value * len(X), it is replaced with the string in rare_value. Use this parameter to group rare categories before encoding the column. If None, ignores this step.\n\nrare_value: str, default=”rare”\n\nValue with which to replace rare categories. Ignored when rare_to_value is None.\n\npolynomial_features: bool, default = False\n\nWhen set to True, new features are derived using existing numeric features.\n\npolynomial_degree: int, default = 2\n\nDegree of polynomial features. For example, if an input sample is two dimensional and of the form [a, b], the polynomial features with degree = 2 are: [1, a, b, a^2, ab, b^2]. Ignored when polynomial_features is not True.\n\nlow_variance_threshold: float or None, default = None\n\nRemove features with a training-set variance lower than the provided threshold. If 0, keep all features with non-zero variance, i.e. remove the features that have the same value in all samples. If None, skip this transformation step.\n\nremove_multicollinearity: bool, default = False\n\nWhen set to True, features with the inter-correlations higher than the defined threshold are removed. For each group, it removes all except the first feature.\n\nmulticollinearity_threshold: float, default = 0.9\n\nMinimum absolute Pearson correlation to identify correlated features. The default value removes equal columns. Ignored when remove_multicollinearity is not True.\n\nbin_numeric_features: list of str, default = None\n\nTo convert numeric features into categorical, bin_numeric_features parameter can be used. It takes a list of strings with column names to be discretized. It does so by using ‘sturges’ rule to determine the number of clusters and then apply KMeans algorithm. Original values of the feature are then replaced by the cluster label.\n\nremove_outliers: bool, default = False\n\nWhen set to True, outliers from the training data are removed using an Isolation Forest.\n\noutliers_method: str, default = “iforest”\n\nMethod with which to remove outliers. Possible values are:\n\n‘iforest’: Uses sklearn’s IsolationForest.\n\n‘ee’: Uses sklearn’s EllipticEnvelope.\n\n‘lof’: Uses sklearn’s LocalOutlierFactor.\n\noutliers_threshold: float, default = 0.05\n\nThe percentage outliers to be removed from the dataset. Ignored when remove_outliers=False.\n\ntransformation: bool, default = False\n\nWhen set to True, it applies the power transform to make data more Gaussian-like. Type of transformation is defined by the transformation_method parameter.\n\ntransformation_method: str, default = ‘yeo-johnson’\n\nDefines the method for transformation. By default, the transformation method is set to ‘yeo-johnson’. The other available option for transformation is ‘quantile’. Ignored when transformation is not True.\n\nnormalize: bool, default = False\n\nWhen set to True, it transforms the features by scaling them to a given range. Type of scaling is defined by the normalize_method parameter.\n\nnormalize_method: str, default = ‘zscore’\n\nDefines the method for scaling. By default, normalize method is set to ‘zscore’ The standard zscore is calculated as z = (x - u) / s. Ignored when normalize is not True. The other options are:\n\nminmax: scales and translates each feature individually such that it is in the range of 0 - 1.\n\nmaxabs: scales and translates each feature individually such that the maximal absolute value of each feature will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity.\n\nrobust: scales and translates each feature according to the Interquartile range. When the dataset contains outliers, robust scaler often gives better results.\n\npca: bool, default = False\n\nWhen set to True, dimensionality reduction is applied to project the data into a lower dimensional space using the method defined in pca_method parameter.\n\npca_method: str, default = ‘linear’\n\nMethod with which to apply PCA. Possible values are:\n\n‘linear’: Uses Singular Value Decomposition.\n\n‘kernel’: Dimensionality reduction through the use of RBF kernel.\n\n‘incremental’: Similar to ‘linear’, but more efficient for large datasets.\n\npca_components: int, float, str or None, default = None\n\nNumber of components to keep. This parameter is ignored when pca=False.\n\nIf None: All components are kept.\n\nIf int: Absolute number of components.\n\nIf float: Such an amount that the variance that needs to be explained\n\nis greater than the percentage specified by n_components. Value should lie between 0 and 1 (ony for pca_method=’linear’).\n\nIf “mle”: Minka’s MLE is used to guess the dimension (ony for pca_method=’linear’).\n\ncustom_pipeline: list of (str, transformer), dict or Pipeline, default = None\n\nAddidiotnal custom transformers. If passed, they are applied to the pipeline last, after all the build-in transformers.\n\ncustom_pipeline_position: int, default = -1\n\nPosition of the custom pipeline in the overal preprocessing pipeline. The default value adds the custom pipeline last.\n\nn_jobs: int, default = -1\n\nThe number of jobs to run in parallel (for functions that supports parallel processing) -1 means using all processors. To run all functions on single processor set n_jobs to None.\n\nuse_gpu: bool or str, default = False\n\nWhen set to True, it will use GPU for training with algorithms that support it, and fall back to CPU if they are unavailable. When set to ‘force’, it will only use GPU-enabled algorithms and raise exceptions when they are unavailable. When False, all algorithms are trained using CPU only.\n\nGPU enabled algorithms:\n\nNone at this moment.\n\nhtml: bool, default = True\n\nWhen set to False, prevents runtime display of monitor. This must be set to False when the environment does not support IPython. For example, command line terminal, Databricks Notebook, Spyder and other similar IDEs.\n\nsession_id: int, default = None\n\nControls the randomness of experiment. It is equivalent to ‘random_state’ in scikit-learn. When None, a pseudo random number is generated. This can be used for later reproducibility of the entire experiment.\n\nsystem_log: bool or str or logging.Logger, default = True\n\nWhether to save the system logging file (as logs.log). If the input is a string, use that as the path to the logging file. If the input already is a logger object, use that one instead.\n\nlog_experiment: bool, default = False\n\nA (list of) PyCaret BaseLogger or str (one of ‘mlflow’, ‘wandb’, ‘comet_ml’) corresponding to a logger to determine which experiment loggers to use. Setting to True will use just MLFlow. If wandb (Weights & Biases) is installed, will also log there.\n\nexperiment_name: str, default = None\n\nName of the experiment for logging. Ignored when log_experiment is False.\n\nexperiment_custom_tags: dict, default = None\n\nDictionary of tag_name: String -> value: (String, but will be string-ified if not) passed to the mlflow.set_tags to add new custom tags for the experiment.\n\nlog_plots: bool or list, default = False\n\nWhen set to True, certain plots are logged automatically in the MLFlow server. To change the type of plots to be logged, pass a list containing plot IDs. Refer to documentation of plot_model. Ignored when log_experiment is False.\n\nlog_profile: bool, default = False\n\nWhen set to True, data profile is logged on the MLflow server as a html file. Ignored when log_experiment is False.\n\nlog_data: bool, default = False\n\nWhen set to True, dataset is logged on the MLflow server as a csv file. Ignored when log_experiment is False.\n\nverbose: bool, default = True\n\nWhen set to False, Information grid is not printed.\n\nmemory: str, bool or Memory, default=True\n\nUsed to cache the fitted transformers of the pipeline.\n\nIf False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory.\n\nprofile: bool, default = False\n\nWhen set to True, an interactive EDA report is displayed.\n\nprofile_kwargs: dict, default = {} (empty dict)"
    }
}
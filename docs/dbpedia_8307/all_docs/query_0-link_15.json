{
    "id": "dbpedia_8307_0",
    "rank": 15,
    "data": {
        "url": "https://spotintelligence.com/2023/01/16/document-clustering-in-python/",
        "read_more_link": "",
        "language": "en",
        "title": "Tutorial On How To Implement Document Clustering In Python With K-means",
        "top_image": "https://spotintelligence.com/wp-content/uploads/2023/01/clustering.jpg",
        "meta_img": "https://spotintelligence.com/wp-content/uploads/2023/01/clustering.jpg",
        "images": [
            "https://spotintelligence.com/wp-content/uploads/2023/01/dark_logo_transparent.png",
            "https://i0.wp.com/spotintelligence.com/wp-content/uploads/2023/01/clustering-jpg.webp?w=1080&ssl=1",
            "https://i0.wp.com/spotintelligence.com/wp-content/uploads/2023/01/Figure_1.png?resize=640%2C480&ssl=1",
            "https://spotintelligence.com/wp-content/uploads/2023/05/neri-van-otten.jpg",
            "https://spotintelligence.com/wp-content/uploads/2022/01/KAG3045_RT_web_lowres.jpg",
            "https://i0.wp.com/spotintelligence.com/wp-content/uploads/2024/08/BLEU.jpg?fit=1200%2C675&ssl=1",
            "https://i0.wp.com/spotintelligence.com/wp-content/uploads/2024/08/ROUGE-1-example.jpg?fit=1200%2C675&ssl=1",
            "https://i0.wp.com/spotintelligence.com/wp-content/uploads/2024/08/NDCG-ranking.jpg?fit=1200%2C675&ssl=1",
            "https://i0.wp.com/spotintelligence.com/wp-content/uploads/2024/08/mean-reciprocal-rank-mrr.jpg?fit=1200%2C675&ssl=1",
            "https://i0.wp.com/spotintelligence.com/wp-content/uploads/2024/07/ethical-ai.jpg?fit=1200%2C675&ssl=1",
            "https://i0.wp.com/spotintelligence.com/wp-content/uploads/2023/09/mean-average-precision-ranking.jpg?fit=960%2C540&ssl=1",
            "https://i0.wp.com/spotintelligence.com/wp-content/uploads/2024/07/hash-mechanism.jpg?fit=1200%2C675&ssl=1",
            "https://i0.wp.com/spotintelligence.com/wp-content/uploads/2024/06/roc-curve.jpg?fit=1200%2C675&ssl=1",
            "https://i0.wp.com/spotintelligence.com/wp-content/uploads/2024/05/decision-boundaries-gaussian-naive-bayes.jpg?fit=1200%2C675&ssl=1",
            "https://spotintelligence.com/wp-content/uploads/2022/06/color_logo_transparent.png",
            "https://spotintelligence.com/wp-content/uploads/2022/12/Zonder-titel-400-x-200-px-600-x-100-px-800-x-80-px-600-x-80-px-700-x-80-px.jpg",
            "https://i0.wp.com/spotintelligence.com/wp-content/uploads/2023/03/New-Blog-Linkedin-Post-21.png?fit=610%2C610&ssl=1"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Neri Van Otten"
        ],
        "publish_date": "2023-01-16T00:00:00",
        "summary": "",
        "meta_description": "Different document clustering algorithms in Python: when to use them/how to evaluate them. Different libraries and how to in Scikit-learn",
        "meta_lang": "en",
        "meta_favicon": "https://i0.wp.com/spotintelligence.com/wp-content/uploads/2022/11/cropped-favicon.png?fit=192%2C192&ssl=1",
        "meta_site_name": "Spot Intelligence",
        "canonical_link": "https://spotintelligence.com/2023/01/16/document-clustering-in-python/",
        "text": "Introduction to document clustering and its importance\n\nGrouping similar documents together in Python based on their content is called document clustering, also known as text clustering. This unsupervised machine learning method is used to analyse and organise extensive collections of text data.\n\nDocument clustering is important because it can find patterns and structure in text data that is not organized. This can be useful for many things, like text mining, natural language processing, and getting information.\n\nDocuments can be clustered by how similar they are.\n\nSome examples of how document clustering can be used include:\n\nGrouping news articles by topic to improve search results and recommendation systems\n\nOrganizing customer feedback by theme helps identify critical issues and improve customer service.\n\nClustering research papers by topic will assist with the literature review and research organisation.\n\nDocument clustering can help make sense of large and complex text data, making it more manageable and actionable.\n\nOverview of standard document clustering techniques in Python\n\nThere are several standard techniques used for document clustering, including:\n\nK-means: This well-known clustering algorithm divides a dataset into k clusters, where k is a parameter that the user specifies. When using K-means, spherical clusters are created, with the mean of each cluster’s points serving as its representation.\n\nHierarchical Clustering: With this approach, the nested grouping of the data is represented by a tree-like structure called a dendrogram. Merging or dividing clusters can be used to build a hierarchy of clusters. Hierarchical clustering is divided into two categories:\n\nAgglomerative: This bottom-up method treats each data point as a single cluster, then iteratively merged with other clusters until a stopping criterion is satisfied.\n\nDivisive: This top-down method splits the data points until a stopping criterion is met by treating them all as one big cluster.\n\nExpectation-Maximization (EM): This technique calculates the probabilities that make up the data’s probability distribution. The EM algorithm can group text data by assuming that the data points come from a mix of different probability distributions.\n\nLatent Dirichlet Allocation (LDA): Using this generative probabilistic model, text data is clustered according to the topics that produce the data. LDA is a well-liked topic modelling method that has also been applied to document clustering.\n\nDBSCAN : The density-based clustering algorithm known as Density-Based Spatial Clustering of Applications with Noise (DBSCAN) groups together data points that are closely spaced apart (points with numerous close neighbours) while classifying as outliers those points that are isolated in low-density areas (called noise points).\n\nEach of these algorithms has benefits and drawbacks of its own, and the technique to be used will depend on the specifics of the dataset and the desired result.\n\nSelection of a document clustering technique for your dataset\n\nThe best method for a particular dataset will depend on several factors, such as the dataset’s size and complexity, the desired result, and the available computing resources.\n\nK-means is a good choice if the dataset is small to medium-sized and the number of clusters is known or can be estimated. It scales well to big datasets and is relatively quick and simple to implement.\n\nHierarchical clustering is a good choice if the dataset is large and the number of clusters is unknown. Large datasets can be handled, and the number of clusters need not be predetermined.\n\nExpectation-Maximization (EM) or Latent Dirichlet Allocation (LDA) may be appropriate if the dataset has many features and the objective is to find latent structure in the data. With these methods, you can find patterns in the data that are not obvious at first glance.\n\nDBSCAN is a good choice if the dataset contains many points closely clustered in some feature space regions and separated by large regions of low point density.\n\nIt’s a good idea to start with a combination of techniques, such as hierarchical clustering and k-means, and then use more specialised techniques, like LDA or EM. If you have little to no prior knowledge about the nature of the data and want to explore the dataset and discover patterns.\n\nIt’s important to keep in mind that to choose the right method, you need to think about both the problem and the data. You should try out different methods, evaluate how well they work using the available evaluation metrics, and then choose the method that best solves the problem.\n\nPreparation of the dataset for clustering\n\nBefore using any document clustering method, the text must be cleaned, tokenized, vectorized, and other steps taken to prepare the dataset.\n\nText cleaning: This step eliminates extraneous or unnecessary text, including punctuation, stop words, special characters, and numbers. By eliminating data noise, this step enhances the clustering algorithm’s accuracy.\n\nTokenization: In this step, the text is broken up into tokens, single words or phrases, so the algorithm can figure out what it means.\n\nStemming/Lemmatization: Stemming or a lexicon-based method is used to strip inflectional endings from words and return them to their basic form (lemmatization). In this step, similar words are grouped to help reduce the dataset’s dimensionality.\n\nVectorization: In this step, the text is transformed into a numerical representation that the clustering algorithm can use as input. Bag-of-words, term frequency-inverse document frequency (TF-IDF), and word embeddings are a few techniques for vectorizing text. The best method will depend on the specific characteristics of the dataset and the desired result. Each of these methods has its benefits and drawbacks.\n\nDimensionality reduction: By removing correlated features or projecting the data onto a lower-dimensional space, this step reduces the number of features. When the dataset has a lot of features and the clustering algorithm needs to be sped up, this step is especially helpful.\n\nThe dataset is ready to be used as input for the clustering algorithm once it has been prepared. It is crucial to keep in mind that the quality of the pre-processing steps determines the quality of the clustering results, making it crucial to take the time to prepare the dataset properly.\n\nImplementation of the chosen document clustering technique using Python\n\nHere is a list of Python libraries that can be used for document clustering.\n\nscikit-learn: This is a popular machine learning library that provides a wide range of clustering algorithms, including k-means, hierarchical clustering, and EM. It also provides tools for pre-processing and evaluating the clustering results.\n\nNLTK: This is a natural language processing library that provides tools for text cleaning, tokenization, stemming, and lemmatization.\n\nGensim: This is a library for topic modelling and document indexing that provides tools for vectorization, dimensionality reduction, and LDA.\n\nSpacy: This industrial-strength natural language processing library provides tools for tokenization, stemming, and lemmatization.\n\nClustering text documents using k-means\n\nHere is an example of how to use the k-means algorithm for document clustering in Python using the scikit-learn library and a sample dataset:\n\nIn this example, we first use the TfidfVectorizer to vectorize the dataset. This converts the text into a numerical representation that can be used as input for the k-means algorithm.\n\nThen we specify the number of clusters to be used (in this case, 2) and build a KMeans model. The model is then fitted to the data, and each document is assigned to a cluster using the prediction method.\n\nIn this case, the documents are put into two clusters. The documents in the first cluster are about outdoor activities and sports. In contrast, the documents in the second cluster are about indoor activities such as reading and playing video games.\n\nThis is a simple example, but the same process can be applied to larger datasets and more complex text data. The number of clusters can also be adjusted to fit the desired outcome.\n\nEvaluation of the clustering results\n\nA crucial step in ensuring the algorithm operates as intended is assessing the results. The quality of clustering results can be assessed using several metrics, such as:\n\nSilhouette Score: This metric gauges how closely each point resembles its cluster in relation to other clusters. A score of 1 means that the point is well-matched to its own cluster, and a score of -1 means that it is better matched to another cluster. The silhouette score can range from -1 to 1. The clusters are clearly defined if the silhouette score is high.\n\nAdjusted Rand Index (ARI): This metric assesses how closely the actual labels match the forecasted labels. The scale goes from -1 to 1, with 1 representing a perfect match and -1 representing entirely different labels.\n\nNormalized Mutual Information (NMI): This metric measures how similar the actual labels are to the labels that were predicted.\n\nFowlkes-Mallows index (FMI): This metric assesses how geometrically similar the actual labels are to the predicted ones.\n\nV-measure: This metric contrasts the harmonic mean between the clustering results’ precision and recall.\n\nDavies-Bouldin index (DBI): This metric measures the similarity between each cluster and its most similar cluster.\n\nCalinski-Harabasz index (CHI): This metric measures the ratio of the variance between clusters to the difference in variance within a cluster.\n\nIt’s important to remember that no single metric can provide a comprehensive evaluation of clustering results, so using several metrics is advised to obtain a more thorough evaluation. Remembering that various data types and clustering algorithms may respond better to various metrics is essential.\n\nVisualization of the document clustering using Python libraries\n\nViewing the clusters can aid in understanding the dataset’s structure and data grouping. To visualise the document clustering, a variety of Python libraries are available, including:\n\nMatplotlib: This is a plotting library that provides a wide range of plotting options, including scatter plots, line plots, and bar plots.\n\nSeaborn: This is a library that is built on top of Matplotlib and provides advanced visualisation options such as heatmaps, pair plots, and violin plots.\n\nPlotly: This is an open-source library that provides interactive and web-based visualisation options.\n\nBokeh: This is another interactive visualisation library that is well suited for large datasets and streaming data.\n\nYellowbrick: This visualisation library is built on top of Matplotlib and specifically designed for machine learning tasks.\n\nHere is an example of how to visualise the clusters using Matplotlib and Seaborn:\n\nHere, we create a scatter plot of the first and fifth dimensions of the data and colour the points according to the predicted cluster assignments.\n\nThe plot shows how the data are spread out within each cluster and makes it easier to see patterns in the data.\n\nSimilarly, other visualisation libraries can plot different plots like heatmaps, pair plots, etc.\n\nThe example above should be considered a starting point, and you may need to adjust the code depending on the specific requirements of your project.\n\nConclusion\n\nDocument clustering is a strong technique that can effectively analyse and organise large amounts of text data. Document clustering is a multi-step process that includes pre-processing the dataset, choosing an appropriate clustering method, using the method, analysing the outcomes, and visualising the clusters.\n\nSeveral Python libraries can be used to put document clustering strategies into action, and there are several ways to measure the quality of the clustering results. In addition, visualisation libraries can be used to comprehend the dataset’s structure and grouping.\n\nPotential further next steps\n\nPotential next steps for further analysis include the following:\n\nFine-tuning the pre-processing steps: Depending on the dataset, the text cleaning, tokenization, and vectorization steps may need to be changed to improve the quality of the clustering results.\n\nUsing different clustering methods: If the results of one method aren’t good enough, it might be helpful to try other methods and compare how well they work.\n\nApplying supervised learning: After clustering the data, it’s possible to use the clusters as features to train a supervised model on the same dataset for classification or regression tasks.\n\nUsing domain-specific knowledge: If domain-specific knowledge can be used to guide the clustering process, it can be used to improve the quality of the results.\n\nInterpreting the clusters: It’s also important to interpret the clusters and understand the characteristics of the grouped documents. This can help figure out what patterns are going on in the data and guide further analysis.\n\nOverall, document clustering in Python is an effective method for making sense of large and complex text data, making it more manageable and valuable. It’s important to remember that the quality of the pre-processing steps determines the quality of the clustering results, so it’s essential to take the time to prepare the dataset properly."
    }
}
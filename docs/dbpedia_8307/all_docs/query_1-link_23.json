{
    "id": "dbpedia_8307_1",
    "rank": 23,
    "data": {
        "url": "https://medium.com/%40ozturkfemre/unsupervised-learning-in-python-project-part-i-cf0a737e7402",
        "read_more_link": "",
        "language": "en",
        "title": "Unsupervised Learning in Python: Project (Part I)",
        "top_image": "https://miro.medium.com/v2/resize:fit:398/1*TmxE6yqa0krG_QMUH1ubEQ.png",
        "meta_img": "https://miro.medium.com/v2/resize:fit:398/1*TmxE6yqa0krG_QMUH1ubEQ.png",
        "images": [
            "https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png",
            "https://miro.medium.com/v2/resize:fill:88:88/1*8ytrbTfHfi0k9yn0vg8CVA.jpeg",
            "https://miro.medium.com/v2/resize:fill:144:144/1*8ytrbTfHfi0k9yn0vg8CVA.jpeg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Fatih Emre Ozturk, MSc",
            "medium.com",
            "Fatih Emre Ozturk"
        ],
        "publish_date": "2023-05-05T15:06:52.509000+00:00",
        "summary": "",
        "meta_description": "In this article, I will make a project in Python about unsupervised learning topics that I have shared theoretical information and applications with R in series before. It will be a detailed…",
        "meta_lang": "en",
        "meta_favicon": "https://miro.medium.com/v2/5d8de952517e8160e40ef9841c781cdc14a5db313057fa3c3de41c6f5b494b19",
        "meta_site_name": "Medium",
        "canonical_link": "https://medium.com/@ozturkfemre/unsupervised-learning-in-python-project-part-i-cf0a737e7402",
        "text": "k-means\n\nFor theoretical explanation of k-means, you can read this post.\n\nLet’s start with determining the optimal number of clusters:\n\nElbow Method\n\nfrom kneed import KneeLocator\n\nfrom sklearn.cluster import KMeans\n\nfrom sklearn.metrics import silhouette_score\n\nSum_of_squared_distances = []\n\nK = range(1,10)\n\nfor num_clusters in K :\n\nkmeans = KMeans(n_clusters=num_clusters, n_init=25)\n\nkmeans.fit(pcadf)\n\nSum_of_squared_distances.append(kmeans.inertia_)\n\nplt.figure(figsize=(10,7))\n\nplt.plot(K,Sum_of_squared_distances, 'x-')\n\nplt.xlabel('cluster number')\n\nplt.ylabel('Total Within Cluster Sum of Squares')\n\nplt.title('Elbow Plot')\n\nplt.show()\n\nWhen the Elbow Method graph is analyzed, it can be said that it is not possible to make a definite decision for the number of clusters, but two clusters can be selected.\n\nAverage Silhouette Method\n\nK = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n\nsilhouette_avg = []\n\nfor num_clusters in K:\n\n# initialise kmeans\n\nkm = KMeans(n_clusters=num_clusters, n_init=25)\n\nkm.fit(pcadf)\n\ncluster_labels = km.labels_\n\n# silhouette score\n\nsilhouette_avg.append(silhouette_score(pcadf, cluster_labels))\n\nplt.figure(figsize=(10,7))\n\nplt.plot(K,silhouette_avg,'bx-')\n\nplt.xlabel('cluster number k')\n\nplt.ylabel('Silhouette score')\n\nplt.title('Average Silhouette Plot')\n\nplt.show()\n\nWhen the Silhouette graph is analyzed, it can be observed that the highest silhouette value is in two clusters. However, 3 clusters can also be tried since there is not much difference between them.\n\nDavies — Bouldin Method\n\nfrom sklearn.metrics import davies_bouldin_score\n\nK = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n\ndb = []\n\nfor num_clusters in K:\n\n# initialise kmeans\n\nkmeans = KMeans(n_clusters=num_clusters, n_init=25)\n\nkmeans.fit(pcadf)\n\ncluster_labels = kmeans.fit_predict(pcadf)\n\n# silhouette score\n\ndb.append(davies_bouldin_score(pcadf, cluster_labels))\n\nplt.figure(figsize=(10,7))\n\nplt.plot(K,db,'bx-')\n\nplt.xlabel('cluster number k')\n\nplt.ylabel('Davies Bouldin score')\n\nplt.title('Davies Bouldin Plot')\n\nplt.show()\n\nWhen the Davies Bouldin plot is analyzed, it can be observed that the lowest davies bouldin value is in 6 clusters. This is quite different than the others. I will cluster data set for 2 and 3 clusters as silhouette suggests.\n\nk-means for k = 2\n\n# clustering\n\nkmeans2 = KMeans(n_clusters=2, random_state=0, n_init=25, algorithm='lloyd')\n\nkmeans2.fit(pcadf)\n\n# output\n\nzero = []\n\none = []\n\nfor i in kmeans2.labels_:\n\nif i == 0:\n\nzero.append(i)\n\nelse:\n\none.append(i)\n\nprint('\\n',\n\n\"Cluster centers:\", '\\n',\n\n\"Cluster 0 :\", kmeans2.cluster_centers_[0],'\\n',\n\n\"Cluster 1 :\", kmeans2.cluster_centers_[1], '\\n','\\n',\n\n\"Clustering vector:\" ,'\\n', kmeans2.labels_, '\\n','\\n',\n\n\"Total Within Cluster Sum of Squares : \", '\\n',\n\nkmeans2.inertia_ , '\\n',\n\n\"Observation numbers :\", '\\n',\n\n\"Cluster 0 :\", len(zero), '\\n',\n\n\"Cluster 1 :\", len(one))\n\nCluster centers:\n\nCluster 0 : [ 3.00438761 -0.07488982]\n\nCluster 1 : [-1.29082985 0.03217628]\n\nClustering vector:\n\n[0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1\n\n1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 0 1\n\n1 1 1 0 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1\n\n1 0 1 1 1 1 0 0 1 1 0 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1\n\n1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 1\n\n1 1 1 1 1 0 1 1 1 0 1 0 0 0 1 1 0 0 0 1 1 1 1 1 1 0 1 0 0 0 1 1 1 0 0 1 1\n\n1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0\n\n0 0 1 0 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n\n1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n\n1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n\n0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n\n1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1\n\n0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n\n1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0\n\n1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n1 1 1 1 1 1 1 0 0 0 0 1 0 1]\n\nTotal Within Cluster Sum of Squares :\n\n2342.4243127486625\n\nObservation numbers :\n\nCluster 0 : 171\n\nCluster 1 : 398\n\nWhen the result of the k-means clustering with 2 clusters is examined, the followings are founded:\n\nThere are 398 observations in cluster 1, 171 observations in cluster 0.\n\nTotal within cluster sum of squares is 2342.4243127486657\n\nLets check how the clusters look like:\n\nfrom scipy.spatial import ConvexHull\n\nfrom matplotlib.colors import to_rgba\n\nsns.set_style(\"whitegrid\")\n\ndata = pcadf\n\nxcol = \"PC1\"\n\nycol = \"PC2\"\n\nhues = [0,1]\n\ncolors = sns.color_palette(\"Paired\", len(hues))\n\npalette = {hue_val: color for hue_val, color in zip(hues, colors)}\n\nplt.figure(figsize=(15,10))\n\ng = sns.relplot(data=pcadf, x=xcol, y=ycol, hue=kmeans2.labels_, style=kmeans2.labels_, col=kmeans2.labels_, palette=palette, kind=\"scatter\")\n\ndef overlay_cv_hull_dataframe(x, y, color, data, hue):\n\nfor hue_val, group in pcadf.groupby(hue):\n\nhue_color = palette[hue_val]\n\npoints = group[[x, y]].values\n\nhull = ConvexHull(points)\n\nplt.fill(points[hull.vertices, 0], points[hull.vertices, 1],\n\nfacecolor=to_rgba(hue_color, 0.2),\n\nedgecolor=hue_color)\n\ng.map_dataframe(overlay_cv_hull_dataframe, x=xcol, y=ycol, hue=kmeans2.labels_)\n\ng.set_axis_labels(xcol, ycol)\n\nplt.show()\n\nSeparation can be observed only in PC1 dimension. Within sum of square of the cluster 1 is much than the cluster 0. The reason of this needs to be the difference between observation numbers of the clusters.There is no visible overlap between clusters.\n\nCluster Validation\n\nThis will be the only clustering algorithm that I share the codes of the cluster validation. I do not want it to be so repetitive. I will show all of the validation metrics in a data frame and compare the algorithms later.\n\nSilhouette\n\nfrom yellowbrick.cluster import silhouette_visualizer\n\nplt.figure(figsize=(10,7))\n\nsilhouette_visualizer(kmeans2, pcadf, colors='yellowbrick')\n\nWhen the graph containing the silhouette values of each observation is examined, it can be seen that some observations in the first cluster which is shown in blue colored, have negative values. This indicates that those observations may have been assigned to the wrong cluster.\n\nsilhouette_score(pcadf, kmeans2.labels_)\n\n0.49228663332300016\n\nAverage silhouette score is 0.49. This value will be compared with other clustering algorithms.\n\nCalinski-Harabasz\n\nfrom sklearn.metrics import calinski_harabasz_score\n\ncalinski_harabasz_score(pcadf, kmeans2.labels_)\n\n534.4714168884335\n\nThis value will be compared with other clustering algorithms.\n\nAdjusted Rand Index\n\nfrom sklearn.metrics.cluster import adjusted_rand_score\n\nadjusted_rand_score(df1['Diagnosis'],kmeans2.labels_)\n\n0.6465880638205838\n\nThis value will be compared with other clustering algorithms.\n\nAccuracy Rate\n\ndf1 = df1.assign(\n\nDiagnosis = lambda dataframe: dataframe[\"Diagnosis\"].map(lambda Diagnosis: 0 if Diagnosis == \"M\" else 1)\n\n)\n\nfrom sklearn.metrics import accuracy_score\n\naccuracy_score(df1['Diagnosis'],kmeans2.labels_)\n\n0.9033391915641477\n\nk-means for k = 3\n\n# clustering\n\nkmeans3 = KMeans(n_clusters=3, random_state=0, n_init=25, algorithm='lloyd')\n\nkmeans3.fit(pcadf)\n\n# output\n\nzero = []\n\none = []\n\ntwo = []\n\nfor i in kmeans3.labels_:\n\nif i == 0:\n\nzero.append(i)\n\nelif i == 1:\n\none.append(i)\n\nelse:\n\ntwo.append(i)\n\nprint('\\n',\n\n\"Cluster centers:\", '\\n',\n\n\"Cluster 0 :\", kmeans3.cluster_centers_[0],'\\n',\n\n\"Cluster 1 :\", kmeans3.cluster_centers_[1],'\\n',\n\n\"Cluster 2 :\", kmeans3.cluster_centers_[2], '\\n',\n\n\"Clustering vector:\" ,'\\n', kmeans3.labels_, '\\n',\n\n\"Total Within Cluster Sum of Squares : \", '\\n',\n\nkmeans3.inertia_ , '\\n',\n\n\"Observation numbers :\", '\\n',\n\n\"Cluster 0 :\", len(zero), '\\n',\n\n\"Cluster 1 :\", len(one), '\\n',\n\n\"Cluster 2 :\", len(two))\n\nCluster centers:\n\nCluster 0 : [1.04646079 1.89652539]\n\nCluster 1 : [-1.53868518 -0.26518367]\n\nCluster 2 : [ 3.35917626 -1.13723881]\n\nClustering vector:\n\n[0 2 2 0 2 0 2 0 0 0 1 0 2 1 0 0 1 0 2 1 0 1 0 2 2 0 0 2 0 2 2 0 2 2 0 2 0\n\n1 1 0 1 0 2 0 1 2 1 0 1 1 1 1 1 2 1 1 2 0 1 1 0 1 0 1 0 0 1 1 0 1 2 0 2 1\n\n1 1 0 2 2 1 1 0 2 2 1 2 1 2 1 0 1 1 1 1 0 2 1 1 1 0 1 1 1 1 1 0 1 1 2 1 1\n\n0 0 0 1 1 1 0 0 2 1 2 2 0 1 1 1 2 0 2 1 0 0 1 2 1 1 1 0 1 1 0 1 1 1 0 0 1\n\n1 1 0 0 0 1 1 1 2 1 1 1 0 2 2 1 2 1 1 1 2 1 1 1 0 1 1 1 0 2 1 1 2 2 1 1 1\n\n1 2 1 1 1 0 1 1 0 0 1 0 2 2 0 1 2 2 0 1 1 1 1 0 1 2 1 2 2 0 0 1 1 2 2 1 0\n\n1 0 1 1 1 1 1 0 2 1 1 2 1 1 2 2 1 2 1 1 0 1 2 1 1 1 1 1 2 1 2 2 2 0 2 0 0\n\n2 2 1 2 1 2 2 1 1 1 1 1 1 2 1 1 0 1 2 1 1 2 1 2 0 1 1 1 1 0 1 0 1 1 1 1 1\n\n1 1 1 1 2 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 0 1 0 2 1 2 1 1 1 1 0 0 0 1 1\n\n1 1 2 1 2 1 2 1 1 1 2 1 1 1 1 1 0 1 0 2 0 1 1 0 1 1 1 1 1 1 1 1 2 2 1 2 2\n\n2 1 2 2 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 2 1 1 0 2 1 1 1 1 1 1 2 1 1 1 1 1 1\n\n1 2 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 2 2 1 0 1 1 1 1 1 2 1 1\n\n2 1 2 1 1 2 1 2 1 1 1 1 1 1 1 1 2 2 1 1 1 1 1 1 2 0 1 1 1 1 1 1 1 1 1 0 1\n\n1 0 1 0 0 1 2 1 1 1 1 2 1 1 1 0 1 2 2 0 0 0 2 0 0 0 0 1 0 1 1 0 1 1 1 2 2\n\n0 0 0 2 1 1 1 1 1 1 0 1 1 1 1 2 1 2 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n1 1 1 1 1 1 1 0 2 2 2 1 2 1]\n\nTotal Within Cluster Sum of Squares :\n\n1713.2725354315216\n\nObservation numbers :\n\nCluster 0 : 117\n\nCluster 1 : 335\n\nCluster 2 : 117\n\nWhen the result of the k-means clustering with 3 clusters is examined, the followings are founded:\n\nThere are 117 observations in cluster 0, 117 observations in cluster 2, and 335 observations in cluster 1.\n\nTotal within cluster sum of squares for clusters are 1713.272535431522\n\nsns.set_style(\"whitegrid\")\n\ndata = pcadf\n\nxcol = \"PC1\"\n\nycol = \"PC2\"\n\nhues = [0,1,2]\n\ncolors = sns.color_palette(\"Paired\", len(hues))\n\npalette = {hue_val: color for hue_val, color in zip(hues, colors)}\n\nplt.figure(figsize=(15,10))\n\ng = sns.relplot(data=pcadf, x=xcol, y=ycol, hue=kmeans3.labels_, style=kmeans3.labels_, col=kmeans3.labels_, palette=palette, kind=\"scatter\")\n\ndef overlay_cv_hull_dataframe(x, y, color, data, hue):\n\nfor hue_val, group in pcadf.groupby(hue):\n\nhue_color = palette[hue_val]\n\npoints = group[[x, y]].values\n\nhull = ConvexHull(points)\n\nplt.fill(points[hull.vertices, 0], points[hull.vertices, 1],\n\nfacecolor=to_rgba(hue_color, 0.2),\n\nedgecolor=hue_color)\n\ng.map_dataframe(overlay_cv_hull_dataframe, x=xcol, y=ycol, hue=kmeans3.labels_)\n\ng.set_axis_labels(xcol, ycol)\n\nplt.show()\n\nThere is no overlap between clusters.\n\nSeparation can be observed both in PC1 and in PC2 dimensions.\n\nWithin sum of square of the cluster 2 is more than other clusters.\n\nk-medoids\n\nFor theoretical explanation of k-medoids, please see this post.\n\nLet’s start with determining the optimal number of clusters:\n\nElbow Method\n\nSum_of_squared_distances = []\n\nK = range(1,10)\n\nfor num_clusters in K :\n\nkmedoid = KMedoids(n_clusters=num_clusters)\n\nkmedoid.fit(pcadf)\n\nSum_of_squared_distances.append(kmedoid.inertia_)\n\nplt.figure(figsize=(10,7))\n\nplt.plot(K,Sum_of_squared_distances, 'x-')\n\nplt.xlabel('cluster number')\n\nplt.ylabel('Total Within Cluster Sum of Squares')\n\nplt.title('Elbow Plot')\n\nplt.show()\n\nWhen the Elbow Method graph is analyzed, it can be said that it is not possible to make a definite decision for the number of clusters, but two clusters can be selected.\n\nAverage Silhouette Method\n\nK = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n\nsilhouette_avg = []\n\nfor num_clusters in K:\n\n# initialise kmeans\n\nkmedoids = KMedoids(n_clusters=num_clusters)\n\nkmedoids.fit(pcadf)\n\ncluster_labels = kmedoids.labels_\n\n# silhouette score\n\nsilhouette_avg.append(silhouette_score(pcadf, cluster_labels))\n\nplt.figure(figsize=(10,7))\n\nplt.plot(K,silhouette_avg,'bx-')\n\nplt.xlabel('cluster number k')\n\nplt.ylabel('Silhouette score')\n\nplt.title('Average Silhouette Plot')\n\nplt.show()\n\nWhen the Silhouette graph is analyzed, it can be observed that the highest silhouette value is in two clusters.\n\nDavies-Bouldin Method\n\nK = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n\ndb = []\n\nfor num_clusters in K:\n\n# initialise kmeans\n\nkmedoids = KMedoids(n_clusters=num_clusters)\n\nkmedoids.fit(pcadf)\n\ncluster_labels = kmedoids.fit_predict(pcadf)\n\n# silhouette score\n\ndb.append(davies_bouldin_score(pcadf, cluster_labels))\n\nplt.figure(figsize=(10,7))\n\nplt.plot(K,db,'bx-')\n\nplt.xlabel('cluster number k')\n\nplt.ylabel('Davies Bouldin score')\n\nplt.title('Davies Bouldin Plot')\n\nplt.show()\n\nWhen the Davies Bouldin graph is analyzed, it can be observed that the lowest davies-bouldin value is in two clusters.\n\nk-medoids for k =2\n\n# clustering\n\nkmedoids2 = KMedoids(n_clusters=2)\n\nkmedoids2.fit(pcadf)\n\n# output\n\nzero = []\n\none = []\n\nfor i in kmedoids2.labels_:\n\nif i == 0:\n\nzero.append(i)\n\nelse:\n\none.append(i)\n\nprint('\\n',\n\n\"Cluster medoids:\", '\\n',\n\n\"Cluster 0 :\", kmedoids2.cluster_centers_[0],'\\n',\n\n\"Cluster 1 :\", kmedoids2.cluster_centers_[1], '\\n','\\n',\n\n\"Clustering vector:\" ,'\\n', kmedoids2.labels_, '\\n','\\n',\n\n\"Total Within Cluster Sum of Squares : \", '\\n',\n\nkmedoids2.inertia_ , '\\n',\n\n\"Observation numbers :\", '\\n',\n\n\"Cluster 0 :\", len(zero), '\\n',\n\n\"Cluster 1 :\", len(one))\n\nCluster medoids:\n\nCluster 0 : [ 2.35928485 -0.30157828]\n\nCluster 1 : [-1.35986794 -0.03765549]\n\nClustering vector:\n\n[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1\n\n1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 0 1\n\n1 0 1 0 0 1 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1\n\n1 0 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1\n\n1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 1\n\n1 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 0 0 1 1 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1\n\n1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0\n\n0 0 1 0 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1\n\n1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n\n1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n\n0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n\n1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1\n\n0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n\n1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0\n\n1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n1 1 1 1 1 1 1 0 0 0 0 0 0 1]\n\nTotal Within Cluster Sum of Squares :\n\n968.3783653743097\n\nObservation numbers :\n\nCluster 0 : 190\n\nCluster 1 : 379\n\nWhen the output of clustering algorithm is analyzed, the followings can be stated:\n\nThere are 190 observations in cluster 0, 379 observations in cluster 1 which can be stated as unbalanced.\n\nCluster medoid for cluster 0 is [ 2.35928485 -0.30157828], [-1.35986794 -0.03765549] is for cluster 1.\n\nTotal within cluster sum of squares is 968.3783653743101.\n\nsns.set_style(\"whitegrid\")\n\ndata = pcadf\n\nxcol = \"PC1\"\n\nycol = \"PC2\"\n\nhues = [0,1]\n\ncolors = sns.color_palette(\"Paired\", len(hues))\n\npalette = {hue_val: color for hue_val, color in zip(hues, colors)}\n\nplt.figure(figsize=(15,10))\n\ng = sns.relplot(data=pcadf, x=xcol, y=ycol, hue=kmedoids2.labels_, style=kmedoids2.labels_, col=kmedoids2.labels_, palette=palette, kind=\"scatter\")\n\ndef overlay_cv_hull_dataframe(x, y, color, data, hue):\n\nfor hue_val, group in pcadf.groupby(hue):\n\nhue_color = palette[hue_val]\n\npoints = group[[x, y]].values\n\nhull = ConvexHull(points)\n\nplt.fill(points[hull.vertices, 0], points[hull.vertices, 1],\n\nfacecolor=to_rgba(hue_color, 0.2),\n\nedgecolor=hue_color)\n\ng.map_dataframe(overlay_cv_hull_dataframe, x=xcol, y=ycol, hue=kmedoids2.labels_)\n\ng.set_axis_labels(xcol, ycol)\n\nplt.show()\n\nNo overlap is observed when two and three dimensional graphs are analyzed. Just like in the k-means, it is observed that the separation occurs only in the PC1 dimension. The variance in the cluster shown in lighter blue is higher."
    }
}
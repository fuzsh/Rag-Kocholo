{
    "id": "dbpedia_8307_1",
    "rank": 47,
    "data": {
        "url": "https://arxiv.org/html/2403.18203v1",
        "read_more_link": "",
        "language": "en",
        "title": "EndToEndML: An Open-Source End-to-End Pipeline for Machine Learning Applications",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://arxiv.org/html/extracted/2403.18203v1/Figures/full_arch.jpg",
            "https://arxiv.org/html/extracted/2403.18203v1/Figures/diagram-ei.jpg",
            "https://arxiv.org/html/extracted/2403.18203v1/Figures/backend.jpg",
            "https://arxiv.org/html/extracted/2403.18203v1/Figures/demo_front.jpg",
            "https://arxiv.org/html/extracted/2403.18203v1/Figures/lang_dataset.jpg",
            "https://arxiv.org/html/extracted/2403.18203v1/Figures/lang_variables.jpg",
            "https://arxiv.org/html/extracted/2403.18203v1/Figures/lang_results.jpg",
            "https://arxiv.org/html/x1.jpg",
            "https://arxiv.org/html/x2.jpg",
            "https://arxiv.org/html/x3.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            "machine learning",
            "bioinformatics",
            "neural networks"
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "1st Nisha Pillai *Corresponding author\n\n2nd Athish Ram Das\n\n3rd Moses Ayoola\n\n{@IEEEauthorhalign} 4th Ganga Gireesan\n\n5th Bindu Nanduri\n\n6th Mahalingam Ramkumar\n\nAbstract\n\nArtificial intelligence (AI) techniques are widely applied in the life sciences. However, applying innovative AI techniques to understand and deconvolute biological complexity is hindered by the learning curve for life science scientists to understand and use computing languages. An open-source, user-friendly interface for AI models, that does not require programming skills to analyze complex biological data will be extremely valuable to the bioinformatics community. With easy access to different sequencing technologies and increased interest in different ‘omics’ studies, the number of biological datasets being generated has increased and analyzing these high-throughput datasets is computationally demanding. The majority of AI libraries today require advanced programming skills as well as machine learning, data preprocessing, and visualization skills. In this research, we propose a web-based end-to-end pipeline that is capable of preprocessing, training, evaluating, and visualizing machine learning (ML) models without manual intervention or coding expertise. By integrating traditional machine learning and deep neural network models with visualizations, our library assists in recognizing, classifying, clustering, and predicting a wide range of multi-modal, multi-sensor datasets, including images, languages, and one-dimensional numerical data, for drug discovery, pathogen classification, and medical diagnostics.\n\nIndex Terms:\n\nmachine learning, bioinformatics, neural networks\n\nI Introduction\n\nBiomedical research communities are increasingly in need of more accessible and interpretable artificial intelligence (AI) tools to extract insights from increasingly large and complex datasets. Performing machine learning on multifaceted biological data currently requires specialized programming expertise, limiting its adoption to bioinformaticians and computational biologists alone. The development of an open-source, user-friendly interface that enables researchers without coding skills to apply AI algorithms to data such as the microbiome related to soil, livestock, plant health, and other domains could dramatically expand the user base. By abstracting away the coding complexity through an intuitive graphical interface and pre-built analysis workflows, more biologists could utilize the predictive and pattern recognition capabilities of AI. This could accelerate discovery and the generation of hypotheses from high-dimensional biological data. Additional capabilities like interactive visualizations could also improve user understanding of the patterns and relationships detected by the models. An open-source platform would facilitate collaboration and allow customization of the built-in AI tools as methodologies continue to evolve. Overall, democratizing AI would equip a wider range of biologists to derive clinically and biologically relevant insights from increasingly complex and multi-modal data. This can profoundly enhance and accelerate biomedical research.\n\nNowadays, there exists a wide array of machine learning libraries and frameworks, such that even seasoned practitioners have difficulty discerning the appropriate tools given each solution’s narrow capability. Furthermore, integration of these disjointed components is cumbersome when attempting to bring end-to-end functionality together. The greatest potential for impactful applications of machine learning lies in interdisciplinary partnerships. These synergies could be enhanced through a universal framework accessible to non-specialists. Placing straightforward yet potent machine learning capabilities into more hands accelerates innovation by allowing users of any skill level to move ML from promise to practical solutions.\n\nAs an integrated solution to bridge expertise gaps hampering ML adoption, we have developed EndToEndML, an easy-to-use web application suite spanning the machine learning pipeline from data preprocessing to model deployment. Using an intuitive graphical interface that requires no programming knowledge, users can upload datasets and produce trained models using automated, state-of-the-art algorithms. Built-in evaluation metrics benchmark model performance on test data, while interactive visualizations provide insights into model behavior. Under the hood, the system handles tedious data cleansing, feature engineering, hyperparameter tuning, and model-building steps that otherwise require specialized knowledge. Seamlessly connecting these processes accelerates experiment iteration for users at any skill level. Through a profoundly simplified approach to machine learning, we aim to help visionaries across all domains turn their ideas into practical AI solutions.\n\nThe outline of this paper is as follows. Related research is briefed under Section II. The proposed architecture is presented in Section III. The supporting functions are described in Section IV. Section V provides a detailed demonstration of two use cases. The conclusion is provided in Section VI.\n\nII Related Work\n\nNumerous open-source machine learning suites have gained widespread adoption among practitioners and researchers. WEKA (Waikato Environment for Knowledge Analysis) [1] is a widely used open-source machine learning software suite written in Java. It provides a wide range of machine learning algorithms for data mining tasks like data processing, classification, regression, clustering, association rule mining, and visualization. ML algorithms are accessible without having to know anything about programming through a Graphical User Interface. Life science students, however, who are unfamiliar with machine learning algorithms, may find the list of algorithms overwhelming and find it difficult to decide. Our architectural design aims to eliminate the initial apprehension associated with using machine learning applications and to reduce the learning curve by providing a very simple user interface.\n\nOrange3 [2] is another open-source tool that integrates data visualization, machine learning, and data mining into a user-friendly graphical user interface (GUI). It simplifies exploratory data analysis and interactive visualization with a visual programming approach, making it accessible to users of all skill levels. This feature empowers researchers, data analysts, and educators, especially those with limited programming expertise, to delve into data analysis with more ease than traditional coding requires. Despite its user-friendly design, Orange3 includes advanced functionalities such as SQL integration and a sophisticated system where widgets communicate and pass data through channels, allowing for intricate interactions. However, these advanced features might pose challenges for beginners, particularly in fields like life sciences where individuals may not be familiar with various databases or tools. While Orange3 aims to democratize data analysis, the EndToEndML project takes this a step further by striving to simplify machine learning to its most fundamental components, making it even more accessible for novices without any background in databases or specific analytical tools.\n\nScikit-learn [3] leads for its breadth of algorithms and data tools coded in the versatile Python language. It provides a robust set of ML algorithms for supervised and unsupervised problems including classification, regression, clustering, dimensionality reduction, model selection, preprocessing and more. TensorFlow [4] and PyTorch [5] are the other two most popular open-source frameworks for building and deploying deep learning models. TensorFlow provides end-to-end infrastructure from data wrangling to the deployment of deep learning models. For Python-based neural architectures, PyTorch offers strong GPU optimization. They both provide prebuilt libraries for common model architectures and datasets and offer flexibility and control in model architecture design to build complex models. However, expecting beginners to simultaneously learn programming, machine learning theory, and the use of specialized libraries can be a challenging route into this field.\n\nIII Architecture\n\nThe EndToEndML pipeline (see fig 1) aims to create an easy-to-use bioinformatics interface for users, working with large and diverse datasets and with limited programming knowledge. Employing an object-oriented architecture, the graphical frontend exclusively handles intuitive user interaction while encapsulating all computational complexity within the backend. This abstraction focuses interface design on usability rather than functionality. Through this simplified user-facing layer, researchers can configure datasets, parameters and visualize outputs to pilot robust analyses engineered in the backend. Modular software decoupling reinforces flexibility and maintenance of complex data wrangling, machine learning model building, and result generation procedures from user requests. We designed the system to emphasize an easy user experience rather than technical details under the hood. In this way, all types of users can harness computational power to derive biological insights from their data. This lets them uncover new discoveries without needing complicated programming.\n\nIII-A Frontend\n\nSince the focus of EndToEndML was its user-friendliness, we followed a minimalistic approach for the frontend. It follows a streamlined sequence of interfaces from the data upload window all the way through the results window, with options for customization graphically. The final design of the GUI is shown in Figure 2. The EndToEndML interface guides users through an intuitive workflow to extract insights from their data. First, users select their desired analysis type such as prediction, clustering, or visualization based on data modalities and use cases (Step a). The system provides an option to select the input data in the case of tabular datasets, or a folder in the case of language, vision, or multi-modal verifications. Recognizing computational runtimes, email notifications allow users to review completed results asynchronously. Minimal configuration is required for initial runs. Users need only specify target variables for prediction and/or input variables for supervised/unsupervised learning from a basic parameters list (Steps b and c). Default preprocessing and validation options apply automated best practices for streamlined analysis. Advanced configuration exposes tuning and algorithm selection. The preprocessing selection could be left untouched by beginners, since our system will choose all the appropriate processing algorithms for the selected dataset automatically. In the following steps, machine learning algorithms will be run sequentially, with the results emailed back to the user.\n\nIII-B Backend\n\nOne of the major challenges when developing the backend system for EndToEndML was its complexity. As a single machine learning model can be challenging in itself, while each dataset has its own characteristics, running multiple machine learning and deep learning models on a single dataset added a great deal of complexity. With the use of Object-Oriented Programming (OOP), this problem was mitigated to some extent. During the development stage, a custom error handler object was created and was constantly updated to handle the unique errors that were encountered. An overview of the backend system is shown in figure 3. The backend system is supported with 4 object classes: DataHandler, ModelEngine, NeuralEngine, and VisualEngine. Each object class is discussed in the following section (see algorithm 1 for an overall process flow).\n\nIII-B1 Data Handler\n\nDataHandler reads a variety of file formats while transforming heterogeneous data into unified representations suitable for machine learning. As part of the data handler, we have included two data conditioning blocks: DataPrePreProcessing and DataPreProcessing. With DataPrePreProcessing, tabular features of categorical, continuous, and binary types are parsed, recognized and then standardized into homogeneous arrays of numerical values, while DataPreProcessing includes methods for normalizing, standardizing, and oversampling the results. Integrated transformation capabilities allow normalization, rescaling, imputation, and sampling augmentation to rectify distributions statistically and minimize distortions algorithmically.\n\nPre-Preprocessing\n\nThe data pre-preprocessing is an interesting as well as the most critical problem in the project. Since the program handles data of diverse formats through a single channel, the pre-preprocessing step is important for standardizing them into a robustly designed data structure (see algorithm 2). An example of this step in the microbiome data context would be to unify the data, which is split in multiple sheets, into a single pandas DataFrame variable. The module also addresses potential typos and irrelevant lines in data which are typically present in a software API-generated file.\n\nPre-processing\n\nDuring the pre-processing step, as shown in the Algorithm 3, appropriate normalization techniques are applied based on the data types in each column; undefined values (NaN) are removed; the data is scaled; and then a random or SMOTE oversampling is performed to account for class imbalances.\n\nIII-B2 ModelEngine\n\nThe ModelEngine contains all the machine learning models defined in a library (see Algorithm 4). A list of models, based on the user’s preference, is returned from the getModel function for training. The models are defined based on the dimensions of the input data, and the hyperparameters are set according to preset defaults.\n\nIII-B3 Neural Engine\n\nThis class goes through the standard training steps for a machine learning model. Major features of the program include the Train/Test Split, which randomly divides collected data randomly into a training set and a test set. The next step is to prepare the training data to feed into the selected model, tuning the parameters against the provided examples and enable it to capture the underlying relationships. The evaluation of trained models using unseen test data in terms of appropriate metrics such as accuracy, AUC, and precision. Next, hyperparameter tuning is conducted to optimize evaluation metrics through new training cycles. Finally, the model will be retrained on the entire dataset to achieve the best results.\n\nIII-B4 Visual Engine\n\nThe Visual Engine generates all graphical outputs that need to be returned to the user. Additionally, it handles the logging function for the entire architecture.\n\nIV Supporting Functions\n\nSpanning both exploratory and advanced techniques, EndToEndML’s initial release features versatile algorithms designed to extract insights from multi-omics datasets. We have already integrated some popular and widely used algorithms; however, some integrations are currently in progress. For traditional modeling, it includes regression strategies such as logistic models, tree-based techniques including random forests and gradient boosting, instance-based algorithms such as k-nearest neighbors, along with unsupervised methods like k-means clustering. For sequencing or temporal data, recurrent and convolutional neural networks come pre-equipped. State-of-the-art transformer architectures are available for language tasks. Through its modular software architecture, newly published or custom methodologies can be smoothly integrated over time. In addition to enabling the immediate performance of a wide range of analyses, this adaptability allows both novice and expert users to evolve alongside the advancements in the field. With an accessible interface that is equipped with both conventional and emerging machine learning techniques, EndToEndML offers multifaceted ML solutions to match the complexity and promise of AI research in the life sciences.\n\nIV-A Prediction\n\nUsing Python scikit-learn [3], our system leverages robust machine learning algorithms to perform predictions. Scikit-learn supplies a versatile set of supervised and unsupervised learning methods for tasks ranging from regression to classification and clustering. It enables efficient data transformations and model training using techniques such as random forests and neural networks, followed by optimized predictions on new data. Our backend pipeline interfaces with scikit-learn estimators to automatically construct ML models custom-fitted to supplied datasets without requiring coding from users. Seamless access to scikit-learn’s extensive catalog enables our application to provide accurate forecasts and data-driven insights across industries for front-facing stakeholders. By managing the coding complexities involved in applying ML, our system democratizes these predictive capabilities, making them accessible to users of any technical skill level to meet various business needs.\n\nLinear Regression\n\nLinear regression [6] is a basic and commonly used type of predictive analysis. It is a statistical method to model the relationship between a dependent variable y and one or more independent variables X. This method quantifies the linear correlation between the variables by fitting a best-fit straight line, known as a linear model, to the observed data points.\n\nLogistic Regression\n\nLogistic regression [7] is a classification algorithm used to predict a categorical dependent variable. Logistic regression transforms its output through the logistic sigmoid function, returning probabilities that range between 0 and 1. This function captures the nonlinear relationships present in the data.\n\nSupport Vector Machine\n\nSVMs [8] are supervised machine learning models used generally for classification and regression problems. They construct an optimal hyperplane in n-dimensional space that distinctly classifies the data points. This hyperplane is determined based on the support vectors, which are the data points closest to the hyperplane.\n\nk-Nearest Neighbours\n\nThe k-Nearest Neighbors (k-NN) [9] algorithm is a simple, non-parametric supervised learning algorithm that predicts based on the similarity of data points in feature space to training samples. It operates by storing all training samples and classifying new data points based on a similarity measure (e.g., Euclidean distance) to its k nearest neighbors in the training set. A majority vote of the neighbour classes is taken as the prediction.\n\nNaive Bayes\n\nNaive Bayes [10] is a simple yet effective supervised classification algorithm that operates under strong feature independence assumptions. Classification is performed by applying Bayes’ rule to compute the probabilities of each class given the feature evidence, and then predicting the class with the highest posterior probability.\n\nRandom Forest\n\nRandom Forest [11] is an ensemble supervised learning technique that combines multiple decision tree models to improve prediction accuracy. It works by creating multiple decision trees during training time. Each tree is trained on a random subset of data features. This approach introduces diversity among the trees, enhancing the model’s robustness.\n\nGradient Boosting\n\nGradient boosting [12] produces a predictive model in the form of an ensemble of weak prediction models, typically decision trees. It strategically combines weak models together for improved predictive performance by targeting errors through gradient descent optimization. The algorithm builds the additive model in a forward, stage-wise fashion, and iteratively adds models to provide a more accurate estimate of the response variable. During each iteration, a new weak learner model (decision tree) is trained with the goal of minimizing the loss function (e.g., MSE for regression, log loss for classification) by fitting on the residual errors left over from the aggregation of all prior learner models.\n\nIV-B Clustering\n\nBy leveraging the versatile algorithms and metrics available in the scikit-learn Python library [3], our framework constructs ML clustering pipelines that facilitate data transformations, model building, and evaluation, all without requiring manual coding.\n\nK-means\n\nK-means clustering is an unsupervised machine learning algorithm that groups an unlabeled dataset into a predefined number of clusters, denoted by k. It works by identifying k centroids, which are points that represent the mean position of all the data points within a cluster. The algorithm repeatedly assigns each data point to the nearest centroid, forms k clusters, and recomputes the centroid of each cluster. The result is the clustering of data into distinct groups based on similarity, achieved without any prior training.\n\nDBSCAN\n\nDensity-Based Spatial Clustering of Applications with Noise [13] is a popular density-based clustering algorithm used in unsupervised machine learning. It clusters data points based on local density, rather than on distance from centroids as in k-means. Regions characterized by a high density of points form clusters. DBSCAN is particularly effective for spatial data and can identify clusters of arbitrary shapes.\n\nAgglomerative\n\nAgglomerative clustering [14] is a type of hierarchical clustering algorithm that groups data points into a tree of clusters. This algorithm employs a bottom-up approach, starting with each data point in its own cluster. It proceeds by successively merging the most similar, or “nearest,” clusters.\n\nGaussian mixture model\n\nGaussian Mixture Models (GMMs) [15] are unsupervised clustering algorithms based on multivariate Gaussian distributions. Data points are assumed to be generated from a combination of a finite number of Gaussian distributions with unknown parameters. GMMs offer probabilistic clustering for heterogeneous data by identifying subpopulations within Gaussian distributions.\n\nIV-C Dimensionality Reduction\n\nThe present design includes Principal Component Analysis (PCA) [16] and Kernel PCA [17] to reduce dimensionality. In both cases, the scikit-learn API is used as the backend software. PCA is an unsupervised linear dimensionality reduction technique that seeks to project the data along directions of maximum variance called principal components. It works by calculating a covariance matrix of the features and finding its eigenvectors. Kernel PCA is a nonlinear form of PCA that uses the kernel trick to transform data and uncover nonlinear latent patterns. Data are implicitly mapped into a higher-dimensional space where the patterns become linear. Then, standard linear PCA is performed in the transformed space to reduce dimensions along principal components.\n\nIV-D Statistics\n\nThis package includes popular statistical methods such as correlation coefficients and distribution verifications. To implement these modules in our program, we use Scipy [18], a Python library that is free, open-source, and designed for scientific and technical computing.\n\nCorrelation Coefficient\n\nThe correlation coefficient measures the strength and direction of the linear relationship between two continuous variables. The Pearson correlation coefficient (r) [19] is a simple and straightforward to calculate and interpret degree of linear association. It utilizes standardized data values to identify correlation ranging from -1 to 1. When the value is 0, there is no relationship. Statistical correlations are expressed as 1 for complete positive correlations and -1 for complete negative correlations. It assesses the linear dependence between samples of two variables, X and Y, by quantifying tightness of data scatter around the regression line. It is calculated by finding the covariance between standardized versions of X and Y divided by their standard deviations. The Spearman’s rank correlation coefficient [20] is a nonparametric measure quantifying the monotonic relationship between two variables. It assesses monotonic relationships, where variables change together but not necessarily in a linear fashion, by converting values to ranks and then calculating the Pearson coefficient on these ranks. Kendall’s tau [21] is a non-parametric statistical metric measuring the ordinal association between two variables. Rather than using ranks, it measures correlation by observing concordance and discordance of pairs of observations.\n\nIV-E Explainable AI\n\nExplainable AI (XAI) refers to AI systems capable of explaining their reasoning, rationale, and decision-making processes to human users in an understandable manner. XAI is becoming increasingly important as AI systems are deployed in sensitive domains such as healthcare, finance, and criminal justice, where transparency in how they arrive at conclusions is critical. Utilizing the OmniXAI [22] libraries, we have incorporated several of the most popular explainable AI architectures.\n\nLocal Interpretable Model-agnostic Explanations\n\nLIME [23] is a popular XAI technique that explains the predictions of any machine learning model by approximating it locally with an interpretable model. It aims to explain individual predictions of complex black box ML models, such as deep neural networks, by fitting simple linear models that approximate the model behavior locally around the prediction. It generates synthetic data samples surrounding the data point to be explained, obtains predictions for these samples, and fits a simple linear model, such as linear regression or a decision tree, to this dataset.\n\nModel Agnostic Counterfactual Explanations\n\nMACE [24] is another technique for providing explanations of machine learning model predictions. MACE generates counterfactual explanations to explain model predictions. Counterfactuals are examples generated by introducing minimal changes to the input features, thereby altering the model’s prediction from the original output. It aims to identify causal relationships between input variables and model outputs within a localized region to offer intuitive and actionable explanations.\n\nSHapley Additive exPlanations\n\nSHAP [25] explains the prediction of machine learning models by computing Shapley values from game theory to determine feature contributions. It assigns to each feature a value that represents its impact on the prediction via an explainable additive feature attribution method. Features with high absolute SHAP values contribute predominantly to differences between predictions. The sign of a SHAP value indicates whether the feature increased or decreased the prediction.\n\nPartial Dependence Plot\n\nPDP [26] is a visualization technique used in XAI to understand how machine learning models respond to changes in input features. PDPs illustrate the marginal effect that one or two features exert on the predicted outcome of a machine learning model by averaging out the influences of all other features.\n\nIV-F Language\n\nBidirectional Encoder Representations from Transformers (BERT) [27] have been incorporated into our architecture to facilitate language processing functions with the support of the PyTorch deep learning library [28]. BERT utilizes transformers, reading text both sequentially and in reverse order, to learn contextual relationships in both directions.\n\nIV-G Image\n\nThe Vision Transformer (ViT) [29] is incorporated using the Keras neural network library [30] for performing image recognition tasks. ViT employs a pure transformer architecture, effectively modeling global contexts and dependencies among image regions.\n\nIV-H Visual question answering (VQA)\n\nVisual question answering [31] seeks to mimic the human ability to interpret images and answer natural language questions about visual content. In VQA systems, users pose free-form questions related to the details of a photograph or another form of image presented. Algorithms subsequently analyze pixels and text to automatically generate answers in fluent natural language without human intervention. By integrating computer vision and natural language processing, these systems initially recognize objects, actions, scenes, and attributes within images. Concurrently, natural language understanding modules extract the intent and semantics from the open-ended questions. Through multimodal reasoning, linguistic and visual comprehensions are unified to ascertain the most appropriate response. Our current visual question answering (VQA) framework and models employ PyTorch deep learning library [28] as the underlying implementation framework.\n\nV Practical Usecase Demonstration\n\nTo demonstrate the functionality and simplicity of our approach, we present two examples using datasets related to biological sciences. Figure 4 illustrates the frontend of our graphical user interface. The user is required to select the appropriate option: ’Language’ to generate a language model based on their dataset, or ’Multi-Modal’ for multi-sensor applications such as Visual Question Answering (VQA).\n\nV-A Language\n\nFor this demonstration, we selected a Kaggle dataset [32] entitled “Medical Speech, Transcription, and Intent,” comprising English language transcriptions and common medical symptoms. Our objective is to generate a language model using this medical data and provide a trained model that users can later utilize to develop a medical chatbot. Through this process, we introduce users to the language-based neural network facets of machine learning.\n\nUpon selecting the “Language” tab from the EndToEndML frontend, our tool directs users to a simple webpage where they can choose their dataset (Figure 5), which should be downloaded and placed in a pre-selected folder, from a dropdown menu. Additionally, we offer users the option to enter their email address, enabling them to receive a notification that includes the results and the location of the saved result folder for future reference.\n\nWe selected a CSV file containing multiple medical variables as features. The subsequent step (Figure 6) involves selecting the input variable, which consists of a sequence of text data that can be used to build a language model and generate a chatbot. Furthermore, it is necessary to select the target feature from the CSV file, which stores medical symptom information corresponding to the medical transcription (input feature).\n\nUpon the selection of relevant features, our backend Application Programming Interface (API), constructed with the Python programming language [33], initiates the process of constructing a language model. This model is generated by leveraging the sequence of medical transcriptions and their corresponding medical symptoms present in the dataset. The learning phase may extend over several hours, depending on the server employed and the size of the dataset. Upon the conclusion of the learning process, our Graphical User Interface (GUI) displays an evaluation report in PDF format (Figure 7). Additionally, it stores the trained language model and other essential files required for the future development of a chatbot application.\n\nFigure 8 illustrates a sample evaluation report. Initially, it details the dataset, including the number of samples and the language sequences corresponding to each medical symptom. Subsequently, it provides information about the training phase, detailing the training parameters employed in the learning process. Currently, our framework employs the Bidirectional Encoder Representations from Transformers (BERT) [27] tokenizer and the BERT language model for the learning process. It then displays the batch-wise training loss and accuracy. Upon completion of the learning phase, the report presents the total training loss and accuracy.\n\nSubsequently, the process transitions to the evaluation phase, during which the test data are assessed using the learned model, and both the classification loss and accuracy are reported. Furthermore, this phase includes a classification report generated with the scikit-learn library. Finally, our software saves all learned model artifacts, including the tokenizer and label details, facilitating their subsequent utilization.\n\nV-B Visual Question Answering\n\nFor the multi-modal learning approach, we utilized a medical visual question answering (VQA) dataset [34]. In this setup, an AI system is presented with a pathology image alongside a question and is tasked with generating the appropriate answer. This dataset comprises a range of open-ended questions along with their corresponding answers, all of which are based on pathology images.\n\nIn this scenario, users initiate the process by selecting the “Multi-Modal” option within the application’s interface (Figure 4), subsequently guiding them to the dataset selection stage (Figure 5). At this point, users can select a specific dataset folder for their project. The dataset in question is characterized by three elements: an image, a question related to that image, and the answer to the question. Our goal is to develop a multi-modal visio-language model that is capable of accurately answering questions presented alongside an image during the evaluation phase.\n\nUpon activation of our VQA (Visual Question Answering) model, the system begins to analyze dataset features, alongside training and testing parameters. We integrate the Bootstrapping Language-Image Pre-training (BLIP) [35] image processor and the BLIP language processor to manage our image and language data, respectively. Additionally, we employ the BLIP VQA model, utilizing Hugging Face transformers [36] to develop our VQA framework. Comprehensive details of this process (Figure 9), including outcomes and evaluations, are documented in a report provided to the user upon completion of the execution. Subsequent to the learning process, we report (Figure 10) the training and evaluation loss as key evaluation metrics. Following this, we evaluate the performance of the trained Visual Question Answering (VQA) model using our test dataset, which comprises images and their corresponding questions.\n\nVI Conclusion\n\nWe present a user-friendly and intuitive Graphical User Interface (GUI) accessible to beginners in the life sciences domain, enabling them to adopt machine learning techniques and obtain a comprehensive evaluation report. This report includes detailed information on the dataset, training parameters, evaluation metrics, results, and performance evaluations. Currently, our EndToEndML tool integrates traditional machine learning methods, renowned explainable AI techniques, and learning architectures for language, visual, and multimodal data. In the future, we plan to incorporate advanced language and image models into our training methodologies. Furthermore, we aim to integrate bioinformatics-related tools into this framework, thereby establishing a unified bio-neural network architecture for learners.\n\nReferences\n\n[1] E. Frank, M. A. Hall, G. Holmes, R. Kirkby, B. Pfahringer, and I. H. Witten, Weka: A machine learning workbench for data mining. Berlin: Springer, 2005, pp. 1305–1314.\n\n[2] J. Demšar, T. Curk, A. Erjavec, Črt Gorup, T. Hočevar, M. Milutinovič, M. Možina, M. Polajnar, M. Toplak, A. Starič, M. Štajdohar, L. Umek, L. Žagar, J. Žbontar, M. Žitnik, and B. Zupan, “Orange: Data mining toolbox in python,” Journal of Machine Learning Research, vol. 14, pp. 2349–2353, 2013. [Online]. Available: http://jmlr.org/papers/v14/demsar13a.html\n\n[3] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg et al., “Scikit-learn: Machine learning in python,” the Journal of machine Learning research, vol. 12, pp. 2825–2830, 2011.\n\n[4] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Mané, R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Viégas, O. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng, “TensorFlow: Large-scale machine learning on heterogeneous systems,” 2015, software available from tensorflow.org.\n\n[5] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala, “Pytorch: An imperative style, high-performance deep learning library,” in Advances in Neural Information Processing Systems 32. Curran Associates, Inc., 2019, pp. 8024–8035.\n\n[6] F. Galton, “Regression towards mediocrity in hereditary stature.” The Journal of the Anthropological Institute of Great Britain and Ireland, vol. 15, pp. 246–263, 1886.\n\n[7] D. R. Cox, “The regression analysis of binary sequences,” Journal of the Royal Statistical Society: Series B (Methodological), vol. 20, no. 2, pp. 215–232, 1958.\n\n[8] C. Cortes and V. Vapnik, “Support-vector networks,” Machine learning, vol. 20, no. 3, pp. 273–297, 1995.\n\n[9] A. Mucherino, P. J. Papajorgji, and P. M. Pardalos, k-Nearest Neighbor Classification. New York, NY: Springer New York, 2009, pp. 83–106.\n\n[10] G. I. Webb, Naïve Bayes. Springer US, 2010.\n\n[11] T. K. Ho, “Random decision forests,” in Proceedings of 3rd international conference on document analysis and recognition, vol. 1. IEEE, 1995, pp. 278–282.\n\n[12] J. H. Friedman, “Greedy function approximation: a gradient boosting machine,” Annals of statistics, pp. 1189–1232, 2001.\n\n[13] M. Ester, H.-P. Kriegel, J. Sander, X. Xu et al., “A density-based algorithm for discovering clusters in large spatial databases with noise.” in kdd, vol. 96, no. 34, 1996, pp. 226–231.\n\n[14] M. L. Zepeda-Mendoza and O. Resendis-Antonio, Hierarchical Agglomerative Clustering. New York, NY: Springer New York, 2013, pp. 886–887.\n\n[15] M.-S. Yang, C.-Y. Lai, and C.-Y. Lin, “A robust em clustering algorithm for gaussian mixture models,” Pattern Recognition, vol. 45, no. 11, pp. 3950–3961, 2012.\n\n[16] K. P. F.R.S., “Liii. on lines and planes of closest fit to systems of points in space,” The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, vol. 2, no. 11, pp. 559–572, 1901.\n\n[17] B. Schölkopf, A. Smola, and K.-R. Müller, “Kernel principal component analysis,” in Artificial Neural Networks — ICANN’97, W. Gerstner, A. Germond, M. Hasler, and J.-D. Nicoud, Eds. Berlin, Heidelberg: Springer Berlin Heidelberg, 1997, pp. 583–588.\n\n[18] P. Virtanen, R. Gommers, T. E. Oliphant, M. Haberland, T. Reddy, D. Cournapeau, E. Burovski, P. Peterson, W. Weckesser, J. Bright, S. J. van der Walt, M. Brett, J. Wilson, K. J. Millman, N. Mayorov, A. R. J. Nelson, E. Jones, R. Kern, E. Larson, C. J. Carey, İ. Polat, Y. Feng, E. W. Moore, J. VanderPlas, D. Laxalde, J. Perktold, R. Cimrman, I. Henriksen, E. A. Quintero, C. R. Harris, A. M. Archibald, A. H. Ribeiro, F. Pedregosa, P. van Mulbregt, and SciPy 1.0 Contributors, “SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python,” Nature Methods, vol. 17, pp. 261–272, 2020.\n\n[19] J. Benesty, J. Chen, Y. Huang, and I. Cohen, “Pearson correlation coefficient,” in Noise reduction in speech processing. Springer, 2009, pp. 37–40.\n\n[20] J. H. Zar, “Spearman rank correlation,” Encyclopedia of Biostatistics, vol. 7, 2005.\n\n[21] M. G. Kendall, “The treatment of ties in ranking problems,” Biometrika, vol. 33, no. 3, pp. 239–251, 1945.\n\n[22] W. Yang, H. Le, T. Laud, S. Savarese, and S. C. H. Hoi, “Omnixai: A library for explainable ai,” ArXiv, vol. abs/2206.01612, 2022. [Online]. Available: https://api.semanticscholar.org/CorpusID:249375643\n\n[23] M. T. Ribeiro, S. Singh, and C. Guestrin, “” why should i trust you?” explaining the predictions of any classifier,” in Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, 2016, pp. 1135–1144.\n\n[24] W. Yang, J. Li, C. Xiong, and S. C. H. Hoi, “Mace: An efficient model-agnostic framework for counterfactual explanation,” ArXiv, 2022.\n\n[25] S. M. Lundberg and S.-I. Lee, “A unified approach to interpreting model predictions,” in Neural Information Processing Systems, 2017. [Online]. Available: https://api.semanticscholar.org/CorpusID:21889700\n\n[26] B. M. Greenwell, “pdp: An R package for constructing partial dependence plots,” The R Journal, vol. 9, no. 1, pp. 421–436, 2017.\n\n[27] J. D. M.-W. C. Kenton and L. K. Toutanova, “Bert: Pre-training of deep bidirectional transformers for language understanding,” in Proceedings of NAACL-HLT, 2019, pp. 4171–4186.\n\n[28] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., “Pytorch: An imperative style, high-performance deep learning library,” Advances in neural information processing systems, vol. 32, 2019.\n\n[29] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al., “An image is worth 16x16 words: Transformers for image recognition at scale,” in International Conference on Learning Representations, 2020.\n\n[30] F. Chollet et al. (2015) Keras. [Online]. Available: https://github.com/fchollet/keras\n\n[31] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and D. Parikh, “Vqa: Visual question answering,” in Proceedings of the IEEE international conference on computer vision, 2015, pp. 2425–2433.\n\n[32] Kaggle, “Medical speech, transcription, and intent [last accessed: Feb 17, 2024].” [Online]. Available: https://www.kaggle.com/datasets/paultimothymooney/medical-speech-transcription-and-intent/data\n\n[33] G. Van Rossum and F. L. Drake Jr, Python reference manual. Centrum voor Wiskunde en Informatica Amsterdam, 1995.\n\n[34] X. He, Y. Zhang, L. Mou, E. Xing, and P. Xie, “Pathvqa: 30000+ questions for medical visual question answering,” arXiv preprint arXiv:2003.10286, 2020.\n\n[35] J. Li, D. Li, C. Xiong, and S. Hoi, “Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation,” 2022. [Online]. Available: https://arxiv.org/abs/2201.12086\n\n[36] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz et al., “Transformers: State-of-the-art natural language processing,” in Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations, 2020, pp. 38–45."
    }
}
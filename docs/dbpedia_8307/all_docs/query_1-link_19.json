{
    "id": "dbpedia_8307_1",
    "rank": 19,
    "data": {
        "url": "https://knowledge.dataiku.com/latest/ml-analytics/ml-concepts/concept-clustering.html",
        "read_more_link": "",
        "language": "en",
        "title": "Concept | Clustering algorithms #",
        "top_image": "https://knowledge.dataiku.com/latest/_static/img/social-card.png",
        "meta_img": "https://knowledge.dataiku.com/latest/_static/img/social-card.png",
        "images": [
            "https://knowledge.dataiku.com/latest/_static/img/DKU-logo-KNOWLEDGE-WHITE.png",
            "https://knowledge.dataiku.com/latest/_static/img/home-links.png",
            "https://play.vidyard.com/FM9N6VJs4KmJafJWSJrA1i.jpg",
            "https://knowledge.dataiku.com/latest/_images/clustering.png",
            "https://knowledge.dataiku.com/latest/_images/recommendation-engines.png",
            "https://knowledge.dataiku.com/latest/_images/spam-filtering.png",
            "https://knowledge.dataiku.com/latest/_images/kmeans.png",
            "https://knowledge.dataiku.com/latest/_images/initial-centroids.png",
            "https://knowledge.dataiku.com/latest/_images/find-distance.png",
            "https://knowledge.dataiku.com/latest/_images/assign-to-cluster.png",
            "https://knowledge.dataiku.com/latest/_images/recompute-the-centroids.png",
            "https://knowledge.dataiku.com/latest/_images/elbow-plot-2.png",
            "https://knowledge.dataiku.com/latest/_images/elbow-plot-6.png",
            "https://knowledge.dataiku.com/latest/_images/elbow-plot-elbow.png",
            "https://knowledge.dataiku.com/latest/_images/hierarchical-clustering.png",
            "https://knowledge.dataiku.com/latest/_images/initial-clusters.png",
            "https://knowledge.dataiku.com/latest/_images/find-distance.png",
            "https://knowledge.dataiku.com/latest/_images/merge-clusters.png",
            "https://knowledge.dataiku.com/latest/_images/dendrogram1.png",
            "https://knowledge.dataiku.com/latest/_images/dendrogram-e-f.png",
            "https://knowledge.dataiku.com/latest/_images/dendrogram-completed.png",
            "https://knowledge.dataiku.com/latest/_images/dendrogram-optimal-number.png",
            "https://knowledge.dataiku.com/latest/_images/dendrogram-optimal-six.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "Discover how clustering algorithms like K-means work before using them in unsupervised learning tasks in Dataiku.",
        "meta_lang": "en",
        "meta_favicon": "https://www.dataiku.com/static/img/favicon.png",
        "meta_site_name": "Dataiku Knowledge Base",
        "canonical_link": "https://knowledge.dataiku.com/ml-analytics/ml-concepts/concept-clustering.html",
        "text": "K-Means#\n\nK-Means separates data points into clusters, characterized by their midpoints, which we call centroids.\n\nLet’s use an example to demonstrate K-Means. To begin, we select “K”, or the number of clusters that we want to identify in our data. For our example, K is equal to three. This means we will identify three clusters in our data.\n\nFirst, K-Means takes this number, three, and marks three random points on the chart. These are the centroids, or midpoints, of the initial clusters.\n\nNext, K-Means measures the distance, known as the euclidean distance, between each data point and each centroid.\n\nFinally, it assigns each data point to its closest centroid and the corresponding cluster.\n\nOnce each data point is assigned to these initial clusters, the new midpoint, or centroid, of each cluster is calculated.\n\nK-Means repeats the steps of measuring the euclidean distance between each data point and the centroid, re-assigning each data point to its closest centroid, and then calculating the new midpoint of each cluster again.\n\nThis continues until one of two outcomes occurs: either the clusters stabilize, meaning no data points can be reassigned, or, a predefined maximum number of iterations has been reached.\n\nElbow plot#\n\nIn K-Means, we want to set K to an optimal number, creating just the right number of clusters. The most common method for determining the optimal value for K is to use an elbow plot.\n\nThe goal of the elbow plot is to find the elbow, or the inflection point of the curve. This is where increasing the value of K, or adding more clusters, is no longer providing a sufficient decrease in variation.\n\nTo build our elbow plot, we iteratively run the K-Means algorithm, first with K=1, then K=2, and so forth, and computing the total variation within clusters at each value of K.\n\nAs we increase the value of K and the number of clusters increases, the total variation within clusters will always decrease, or at least remain constant.\n\nIn our example, the elbow is approximately “five”, so we may want to opt for five clusters.\n\nHierarchical clustering#\n\nLet’s look at another common clustering method, hierarchical clustering. Hierarchical clustering generates clusters based on hierarchical relationships between data points.\n\nIn this method, instead of beginning with a randomly-selected value of “K”, hierarchical clustering starts by declaring each data point as a cluster of its own.\n\nThen, hierarchical clustering computes the euclidean distance between all cluster pairs.\n\nNext, the two closest clusters are merged into a single cluster. Hierarchical clustering iteratively repeats this process until all data points are merged into one large cluster.\n\nDendrogram#\n\nTo visualize the hierarchical relationship between the clusters and determine the optimal number of clusters for our use case, we can use a dendrogram. A dendrogram is a diagram that shows the hierarchical relationship between objects.\n\nTo build our dendrogram, we name each of our data points, declaring each one as its own cluster.\n\nThen we iteratively combine the closest pairs. Points “E” and “F” are the first to be merged, so we draw a line linking these two data points in our dendrogram. The height of that line will be determined by the distance between these two data points on the original chart.\n\nWe continue merging the next two closest clusters, and so forth, and adding the relevant lines to the chart.\n\nLet’s now use our dendrogram to help determine the optimal number of clusters. Based on our knowledge of the business domain and the use case, we can set a distance threshold, also known as a cluster dissimilarity threshold. We can use this horizontal line to see how many clusters we’d have at this threshold by counting how many vertical lines it bisects.\n\nDepending on our use case, we might decide one cluster would contain points A, B and C, and the other would contain points D, E, and F. Alternatively, we might decide that our clusters must be as dissimilar as possible. In our case, if we set the distance threshold so that each cluster is as dissimilar as possible, we would have six bisected lines, resulting in six clusters."
    }
}
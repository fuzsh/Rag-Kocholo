{
    "id": "dbpedia_8307_1",
    "rank": 5,
    "data": {
        "url": "https://www.mindsmapped.com/unsupervised-learning-in-python/",
        "read_more_link": "",
        "language": "en",
        "title": "Unsupervised Learning in Python",
        "top_image": "https://www.mindsmapped.com/wp-content/uploads/2018/07/cropped-MM-Logo-32x32.png",
        "meta_img": "https://www.mindsmapped.com/wp-content/uploads/2018/07/cropped-MM-Logo-32x32.png",
        "images": [
            "https://www.mindsmapped.com/wp-content/uploads/2020/05/logo_big.png",
            "https://www.mindsmapped.com/wp-content/uploads/2020/07/Unsupervised-Learning-in-Python.png",
            "https://www.mindsmapped.com/wp-content/uploads/2020/07/image.png",
            "https://www.mindsmapped.com/wp-content/uploads/2020/07/image-1.png",
            "https://www.mindsmapped.com/wp-content/uploads/2020/07/image-2.png",
            "https://www.mindsmapped.com/wp-content/uploads/2020/07/image-3.png",
            "https://www.mindsmapped.com/wp-content/uploads/2020/07/image-4.png",
            "https://www.mindsmapped.com/wp-content/uploads/2020/07/image-5.png",
            "https://www.mindsmapped.com/wp-content/uploads/avatars/28814/5ebb8bdcaf073-bpfull.jpg",
            "https://www.mindsmapped.com/wp-content/uploads/2020/11/Manipulating-Strings-in-R-300x200.png",
            "https://www.mindsmapped.com/wp-content/uploads/2020/09/Decoding-Neural-Networks-300x200.png",
            "https://www.mindsmapped.com/wp-content/uploads/2020/08/Numeric-Functions-in-R-Programming-300x200.png",
            "https://www.mindsmapped.com/wp-content/uploads/2018/07/Getting-Started-with-Business-Analysis-Fundamentals-1-150x150.jpg",
            "https://www.mindsmapped.com/wp-content/uploads/2018/06/Step-by-Step-Guide-to-Learn-Manual-and-Automation-Software-Testing-150x150.jpg",
            "https://www.mindsmapped.com/wp-content/uploads/2018/06/iiba-ecba-certification-training-for-business-analysts-150x150.jpeg",
            "https://www.mindsmapped.com/wp-content/uploads/2018/06/MindsMapped-White-Logo-Transparent.png",
            "https://www.mindsmapped.com/wp-content/plugins/miniorange-login-openid/includes/images/icons/g.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            "unsupervised learning in python",
            "unsupervised learning",
            "pca",
            "learn python",
            "principal component analysis",
            "k-means clustering",
            "clustering",
            "hierarchical clustering",
            "dendrogram",
            "elbow curve",
            "dimensionality reduction",
            "data science",
            "machine learning"
        ],
        "tags": null,
        "authors": [
            "Sayantan Banerjee",
            "https://www.mindsmapped.com/author/riju-banerjee93/#author"
        ],
        "publish_date": "2020-07-04T23:02:03-04:00",
        "summary": "",
        "meta_description": "This article is about Unsupervised Learning in python where I have discussed about PCA and Clustering Algorithms, along with implementation in Python.",
        "meta_lang": "en",
        "meta_favicon": "https://www.mindsmapped.com/wp-content/uploads/2018/07/cropped-MM-Logo-32x32.png",
        "meta_site_name": "",
        "canonical_link": "https://www.mindsmapped.com/unsupervised-learning-in-python/",
        "text": "Hello World, here I am with my new blog and this is about Unsupervised learning in Python. Previously I wrote about Supervised learning methods such as Linear Regression and Logistic regression. In this blog, I am going to discuss about two of the most important methods in unsupervised learning i.e., Principal Component Analysis and Clustering.\n\nUnsupervised learning refers to the fact that the data in use, is not labeled manually as in Supervised learning methods. It looks for previously undetected pattern without any human supervision. In supervised learning, we label data-points as belonging to a class. Hence it is easier for an algorithm to learn from the labelled data. In case of unsupervised learning the data points are grouped as belonging to a cluster based on similarity. Similarity can be measured by plotting a data-point in n-dimensional vector space and finding euclidean distance between data-points. The less the distance, the more similar they are.\n\nChallenges in Unsupervised learning\n\nUnsupervised learning is harder in comparison to Supervised learning as there is no annotated data, so the algorithms need to be such that it understands the pattern.\n\nWe cannot validate the results from Unsupervised learning since no labelled data is present.\n\nAdvantages of Unsupervised learning\n\nNo manual labeling required for annotating huge amount of data\n\nWe don’t know, into how many classes the data is actually divided.\n\nPrincipal Component Analysis reduces the dimension of large data-set, thus helping in less computation.\n\nPrincipal Component Analysis(PCA)\n\nLarge data-sets are widespread in many sectors. In order to analyse and interpret such data-sets, methods are required to significantly reduce the dimensionality in an interpretable way, such that most of the information is preserved. Many techniques have been developed, but principal component analysis (PCA) is one of the oldest and widely used. Its idea is simple, reduce the dimensionality of a dataset, on the other hand, preserving as much ‘variability’ (i.e. statistical information) as possible.\n\nI will not discuss about working of PCA and how its algorithm works, but straight dive into implementing it in python. I will apply this to a large data-set and check, how it can significantly reduce the dimensionality of it. Let me start by importing necessary packages\n\nimport pandas as pd import numpy as np\n\nI will be working with Breast Cancer data-set. The data-set has 9 feature columns. We will reduce the 9 columns to 3 principal components and understand how much of the information is retained by the 3 principal components.\n\ndataset = pd.read_csv('breast_cancer_dataset.csv') dataset= dataset.rename(columns={'clump_thickness': 'clumpThickness', 'uniformity_of_cell_size': 'cellSize', 'uniformity_of_cell_shape':'cellShape', 'marginal_adhesion': 'marginalAdhesion', 'single_epithelial_cell_size':'singleECellSize'}) dataset.shape\n\n(569, 10)\n\nThe data-set has 569 rows and 10 columns.\n\nLet me look at random 5 data from the data-set\n\ndataset.sample(5)\n\ndataset.dtypes\n\nclumpThickness int64 cellSize int64 cellShape int64 marginalAdhesion int64 singleECellSize int64 bare_nuclei int64 bland_chromatin int64 normal_nucleoli int64 mitosis int64 class int64 dtype: object\n\nWe have a data-set having all numerical columns. PCA works good for numerical columns and it is not advisable to use PCA with categorical data. Let me check the data-set description using the describe function.\n\ndataset.describe()\n\nWe can clearly see that there are no missing values but bare_nuclei has minimum value of -100000 which is not normal. So I will replace -100000 with 1. So lets so that.\n\ndataset = dataset.replace(-100000, 1) dataset.describe()\n\nWe have replaced the value. Now I will convert the target variable i.e., ‘class’ into categorical variable where 2 refers to benign and 4 to malignant. I will separate the data-set into feature and target and finally perform standardization to standardize the features, since it is required for PCA. Lets do that.\n\ndataset['class'] = dataset['class'].astype('category') dataset.dtypes\n\nclumpThickness int64 cellSize int64 cellShape int64 marginalAdhesion int64 singleECellSize int64 bare_nuclei int64 bland_chromatin int64 normal_nucleoli int64 mitosis int64 class category dtype: object\n\nfeature = dataset.iloc[:,:-1] feature.sample(5)\n\ntarget = dataset['class'] target.sample(5)\n\n314 2 110 2 52 4 393 2 338 2 Name: class, dtype: category Categories (2, int64): [2, 4]\n\nfrom mlxtend.preprocessing import standardize feature = standardize(feature, columns=feature.columns) feature.sample(5)\n\nNow that we have standardized the data-set, we will apply PCA\n\nfrom sklearn.decomposition import PCA pcaData = PCA(n_components=3) principalComps = pcaData.fit_transform(feature) principalDataset = pd.DataFrame(data = principalComps , columns = ['p_comp1', 'p_comp2','p_comp3']) principalDataset.sample(5)\n\nWe have converted the entire data-set into a data-set with only 3 features i.e., Principal Components. Lets checkout how the principal components can explain the variance, i.e., information retained by 3 principal components.\n\npcaData.explained_variance_ratio_\n\narray([0.6501069 , 0.08621319, 0.06142388])\n\nFrom the above code, we can understand that the 1st principal component has 65% of the information, while 2nd and 3rd has 8% and 6% information, respectively.\n\nWe can use this principal components as features and develop our classification or regression models.\n\nPCA is very sensitive to outliers and so, outliers must be detected and handled. PCA creates variables that are linear combinations of the original variables. The new variables have the orthogonal property. PCA transformation can be helpful as pre-processing step before clustering\n\nNow that we have understood how to implement PCA in python, we will look at clustering and understand how to implement clustering in python.\n\nClustering\n\nClustering is a very important topic in machine-learning, where we can can create groups of data from a sample, having similar values. Annotating large data-sets is a very hectic task and needs extensive time and effort to accomplish. Clustering comes to the rescue and can be implemented easily in python. There are lots of clustering algorithms but I will discuss about K-Means Clustering and Hierarchical Clustering. K-means clustering is centroid based, while Hierarchical clustering is connectivity based.\n\nK-Means Clustering\n\nK-Means Clustering is an algorithm that falls under the category of centroid-based clustering. K-Means clustering can be used in Customer Segmentation, Recommendation Engine,etc. In K-means clustering, each observation belongs to a particular cluster, which has the nearest mean(cluster centroid). Euclidean distance is used to measure the distance and variance is used to measure scatter between clusters. The intra-cluster variation, or Within-cluster sum of squares(WSS) needs to be minimum and Inter-Cluster variance needs to be maximum.\n\nAt first, we have to choose, into how many clusters, the data-set will be divided into. Suppose we select k clusters, so k points will be randomly assigned as centroid in vector space. Now distance between each data points and the k centroids are calculated. The data points closer to the centroid are clubbed as a single cluster. Now in next iteration, the new centroid is calculated for each clusters formed, and the data points are clubbed into clusters. The process repeats untill there is no change in the centroid formed. Finally, similar data points can be found as clusters.\n\nLet us implement K-means clustering in Python.\n\nI will be using the above data-set excluding the target variable. Next we will use elbow curve to choose number of clusters and check whether there are 2 clusters, since there are two classes in the original data-set.\n\nElbow curve helps us to determine optimal number of clusters. The location of the bend helps to determine the optimal number of clusters.\n\nfrom sklearn.cluster import KMeans from yellowbrick.cluster import KElbowVisualizer clusteringDataset = dataset.iloc[: ,:-1] model = KMeans() visualizer = KElbowVisualizer(model, k=(1,8)) visualizer.fit(clusteringDataset) visualizer.show()\n\nWe can clearly see that we should choose 2 clusters for this data-set. Lets Apply K-means clustering now on the data-set and visualize the result.\n\nimport matplotlib.pyplot as plt kmeans = KMeans(n_clusters=2) kmeans.fit(clusteringDataset) y_kmeans = kmeans.predict(clusteringDataset) plt.scatter(clusteringDataset.iloc[:, 0], clusteringDataset.iloc[:, 1], c=y_kmeans, s=50, cmap='viridis') centers = kmeans.cluster_centers_ plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.5)\n\nWe understood how to use k-means clustering and how to choose optimum number of clusters from elbow curve. K-means clustering has got lot of uses The next topic is about Hierarchical clustering.\n\nHierarchical clustering\n\nHierarchical Clustering is another clustering technique, which starts by refering individual observations as a cluster. Then it follows two steps:\n\nIdentify closest data point\n\nMerge them as cluster\n\nThe output from Hierarchical clustering is a dendrogram.\n\nFor applying and visualizing hierarchical clustering, lets generate a simple data-set. The data-set has two columns, since two-dimensional data is easy to visualize.\n\nnp.random.seed(4715) a = np.random.multivariate_normal([10, 30], [[3, 1], [1, 4]], size=[50,]) b = np.random.multivariate_normal([5, 20], [[3, 1], [1, 4]], size=[20,]) X = np.concatenate((a, b),) plt.scatter(X[:,0], X[:,1]) plt.show()\n\nWe have created the data-set and plotted a scatterplot. Let us visualize the dendrogram. A dendrogram is a structure that shows the hierarchical relationship between data. It is most commonly created as an output of hierarchical clustering. The main use of a dendrogram is to find out the best way to allocate data-points to clusters. In the dendrogram below, the height of the dendrogram indicates the order in which the clusters were joined. A more informative dendrogram can be generated where the heights reflect the distance between the clusters as is shown below. In this case, the dendrogram shows us the big difference between clusters is between the cluster of A and B versus that of C, D, E, and F.\n\nfrom scipy.cluster.hierarchy import dendrogram, linkage from matplotlib import pyplot as plt linked = linkage(X, 'single') plt.figure(figsize=(10, 7)) dendrogram(linked, orientation='top', distance_sort='descending', show_leaf_counts=True) plt.show()\n\nFrom the dendrogram, we can understand that there are basically 2 clusters. Now let me perform Hierarchical Clustering using 2 clusters.\n\nfrom sklearn.cluster import AgglomerativeClustering cluster = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward') cluster.fit_predict(X) plt.scatter(X[:,0],X[:,1], c=cluster.labels_, cmap='rainbow')\n\nIn the above diagram, we got to visualize the two clusters. Hierarchical clustering is preferred when the data is small. When the data-set is large enough, K-means clustering will be helpful, as it is more robust.\n\nFinal Words – Unsupervised Learning in Python"
    }
}
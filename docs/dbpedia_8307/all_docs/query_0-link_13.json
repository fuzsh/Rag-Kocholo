{
    "id": "dbpedia_8307_0",
    "rank": 13,
    "data": {
        "url": "https://www.bartleby.com/essay/News-Aggregation-Of-Python-Using-Hierarchical-Clustering-FKPFY9929BWQ",
        "read_more_link": "",
        "language": "en",
        "title": "News Aggregation Of Python Using Hierarchical Clustering",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://assets.bartleby.com/1.17/images/logos/bartleby/logo-home.svg",
            "https://assets.bartleby.com/1.17/images/placeholders/essay_preview.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "Free Essay: News Aggregation in Python using Hierarchical Clustering Rahul S Verma CSE Department IMSEC Ghaziabad rahul.1a94@gmail.com Satyam Gupta CSE...",
        "meta_lang": "",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": "https://www.bartleby.com/essay/News-Aggregation-Of-Python-Using-Hierarchical-Clustering-FKPFY9929BWQ",
        "text": "Actually a user often want to retrieve author’s concept and idea, in order to do so he supplies a list of keywords in the search query. The primary goal of this project is to develop a system that will capture the user’s idea through his list of key words. Our first task is to identify the possible concepts that are in user’s mind, then extract all articles containing these concepts.\n\nWith an increase in the amount of information consumed every day, time is a prime resource. Keeping up with current events is an activity that is essential for everyone, but saving time is also important. Our paper is mainly focused on the implementation of Natural Language Processing techniques and algorithms to summarize news articles from public sources such that they can be consumed in a short amount of time, keeping the user updated of global as well as local events. We first provide an Introduction by stating problems faced, and an overview of NLP and Automatic Summarization. We then survey different types of Summarization, and detail a solution using the TextRank algorithm, along with our proposed implementation.\n\nText document processing plays a key role in data mining as well as web search for information retrieval. In text processing, the commonly used model is bag-of-words model [5]. In this model each document is typically represented in vector form in which each element indicates the value of the analogous feature in the document. The feature value can be selected by finding number of occurrences of a term in the document. However relative term frequency can be defined as the ratio between the term frequency and the total number of occurrences of all the terms in the document set. Frequently, the dimensionality of a document is large and the resulting vector is sparse, i.e., most of the selected feature values in the vector are zero. Such high-dimensionality and sparsity is a challenge for similarity measure and thus it is a very important operation in text processing algorithms.\n\nDue to the huge growth and expansion of the World Wide Web, a large amount of information is available online. Through Search engines we can easily access this information with the help of Search engine indexing. To facilitate fast and accurate information retrieval search engine indexing collects, parses, and store data. This paper explains partitioning clustering technique for implementing indexing phase of search engine. Clustering techniques are widely used for grouping a set of objects in such a way that objects in the same group are more to each other than to those in other groups in “Web Usage Mining”. Clustering methods are largely divided into two groups: hierarchical and partitioning methods. This paper proposes the k-mean partitioning method of clustering and also provide a comparison of k-mean clustering and Single link HAC . Performance of these clustering techniques are compared according to the execution time based on no of clusters and no of data items being entered.\n\nThe recent years have seen a huge increase in the number of online documents. This has resulted in a huge amount of information being available at the click of a mouse. But, at the same time, the retrieval of relevant information from this collection of unstructured documents has emerged as a challenging task and is a topic of research. A major part of retrieving information out of a document is finding out the words or phrases of significance in the article like the persons, organization, location,\n\nWeb based document (WBD) commonly known as Latent Semantic Indexing in the context of information retrieval is a fully automatic mathematical/statistical technique for extracting and inferring relations of expected contextual usage of words in passages of discourse. It is based on the application of a particular mathematical technique, called Singular Value Decomposition (SVD), to a word-by-document matrix [4]. The word-by-document matrix is formed from WBD inputs that consist of raw text parsed into words defined as unique character strings and separated into meaningful passages or samples such as sentences or paragraphs. This application provides a way of viewing the global relationship between terms in the whole documents’ collection enabling the semantic structures within the collection to be unearthed. WBD application in information retrieval is motivated by the challenges encountered in natural language processing where a word may have several meanings (polysemy) and several words may mean the same thing (synonymy) thereby presenting ambiguities in expressing users’ concepts. For example, several empirical studies show that the likelihood of two people choosing the same keyword for a familiar object is less than 15%. It is due to these challenges that mere keywords searching techniques are inadequate in addressing user queries. WBD enables retrieval on the basis of conceptual content, instead of merely matching words between queries and\n\nDynamics of contemporary news industry is complex and challenged as almost all aspects of gathering, producing, delivery and reception is changing (BBC 2015b; Franklin 2014). Any technological changes occurring in an era will affect the publics it served (Pavlik 2000). Technology has always affected journalism since its beginning. The use of telegram and then telephone besides other inventions as part of news processes are examples of previous journalistic adaptation of technologies into its practice. Similar to other earlier forms of technology that have altered journalism in the past, the arrival of the Internet and the technologies it carry has further enhanced contemporary journalism.\n\nAlso, the fourth feature of the target user for this system is that the target users would want a constant stream of new information and they would want some short news or some highlighted-interesting information as well in order to read through a large amount of news in a short time.\n\nThe main purpose is to detect topics automatically and track related documents from a stream of documents temporally so that readers can understand. First stage, Theme Generation process tries to identify the theme of the topic. Next Event Segmentation and Summarization models the documents as a symmetric block association matrix. Eigen vectors are then drawn to examine and extract summaries. Finally, Temporal Similarity (TS) function is used to calculate the event dependencies. This had given me an opportunity to expose my knowledge in Software Engineering and Data Mining. This also helped us to gain domain knowledge and also enhance technical skills like Servlets and JSP, used for implementing main logic, while JDBC for back end database connection and performing basic operations of database and Html for UI"
    }
}
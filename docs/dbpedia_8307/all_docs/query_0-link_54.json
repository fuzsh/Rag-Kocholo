{
    "id": "dbpedia_8307_0",
    "rank": 54,
    "data": {
        "url": "https://docs.databricks.com/en/lakehouse-architecture/performance-efficiency/best-practices.html",
        "read_more_link": "",
        "language": "en",
        "title": "Best practices for performance efficiency",
        "top_image": "https://www.databricks.com/wp-content/uploads/2020/04/og-databricks.png",
        "meta_img": "https://www.databricks.com/wp-content/uploads/2020/04/og-databricks.png",
        "images": [
            "https://docs.databricks.com/en/_static/small-scale-lockup-full-color-rgb.svg",
            "https://docs.databricks.com/en/_static/small-scale-lockup-full-color-rgb.svg",
            "https://docs.databricks.com/en/_static/icons/globe.png",
            "https://docs.databricks.com/en/_static/icons/aws.svg",
            "https://docs.databricks.com/en/_static/icons/azure.svg",
            "https://docs.databricks.com/en/_static/icons/gcp.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "This article covers best practices supporting principles of performance efficiency on the data lakehouse on Databricks.",
        "meta_lang": "en",
        "meta_favicon": "../../_static/favicon.ico",
        "meta_site_name": "",
        "canonical_link": "https://docs.databricks.com/en/lakehouse-architecture/performance-efficiency/best-practices.html",
        "text": "3. Design workloads for performance\n\nUnderstand your data ingestion and access patterns\n\nFrom a performance perspective, data access patterns - such as âaggregations versus point accessâ or âscan versus searchâ - behave differently depending on the data size. Large files are more efficient for scan queries, and smaller files are better for searches, because you need to read less data to find the specific row(s).\n\nFor ingestion patterns, it is common to use DML statements. DML statements are most performant when the data is clustered, and you can simply isolate the section of data. It is important to keep the data clustered and isolatable during ingestion: consider keeping a natural time sort order and apply as many filters as possible to the ingest target table. For append-only and overwrite ingestion workloads, there isnât much to consider because itâs a relatively cheap operation.\n\nThe ingestion and access patterns often point to an obvious data layout and clustering. If not, decide what is more important to your business and focus on how to better achieve that goal.\n\nUse parallel computation where it is beneficial\n\nTime to value is an important dimension when working with data. While many use cases can be easily implemented on a single machine (small data, few and simple computational steps), there are often use cases that need to process large data sets, have long run times due to complicated algorithms, or need to be repeated 100s and 1000s of times.\n\nThe cluster environment of the Databricks platform is a great environment for efficiently distributing these workloads. It automatically parallelizes SQL queries across all nodes of a cluster and it provides libraries for Python and Scala to do the same. Under the hood, the engines Apache Spark and Photon engines analyze the queries, determine the optimal way of parallel execution, and manage the distributed execution in a resilient way.\n\nIn the same way as batch tasks, Structured Streaming distributes streaming jobs across the cluster for best performance.\n\nOne of the easiest ways to use parallel computing is with Delta Live Tables. You declare a jobâs tasks and dependencies in SQL or Python, and then Delta Live Tables handles execution planning, efficient infrastructure setup, job execution, and monitoring.\n\nFor data scientists, pandas is a Python package that provides easy-to-use data structures and data analysis tools for the Python programming language. However, Pandas does not scale out to big data. Pandas API on Spark fills this gap by providing pandas equivalent APIs that work on Apache Spark.\n\nIn addition, the platform comes with parallelized machine learning algorithms in the standard machine learning library MLlib. It supports out-of-the-box multi-GPU usage. Deep learning can also be parallelized using Horovod Runner, DeepSpeed Distributor or TorchDistributor.\n\nAnalyze the whole chain of execution\n\nMost pipelines or consumption patterns involve a chain of systems. For example, with BI tools the performance is impacted by several factors:\n\nThe BI tool itself.\n\nThe connector that connects the BI tool and the SQL engine.\n\nThe SQL engine to which the BI tool sends the query.\n\nFor best-in-class performance, the entire chain must be considered and selected/tuned for best performance.\n\nPrefer larger clusters\n\nPlan for larger clusters, especially if the workload scales linearly. In this case, using a large cluster for a workload is not more expensive than using a smaller cluster. Itâs just faster. The key is that you rent the cluster for the duration of the workload. So, if you spin up two worker clusters and it takes an hour, you are paying for those workers for the full hour. Similarly, if you spin up a four worker cluster and it only takes half an hour (this is where linear scalability comes in), the costs are the same. If costs are the primary driver with a very flexible SLA, an autoscaling cluster is usually the cheapest, but not necessarily the fastest.\n\nNote\n\nPreferring large clusters is not required for serverless compute because it automatically manages clusters.\n\nUse native Spark operations\n\nUser-defined functions (UDFs) are a great way to extend the functionality of Spark SQL. However, donât use Python or Scala UDFs if a native function exists:\n\nSpark SQL\n\nPySpark\n\nReasons:\n\nSerialization is required to transfer data between Python and Spark. This significantly slows down queries.\n\nIncreased effort to implement and test functionality that already exists in the platform.\n\nIf native functions are missing and should be implemented as Python UDFs, use Pandas UDFs. Apache Arrow ensures data moves efficiently back and forth between Spark and Python.\n\nUse native platform engines\n\nPhoton is the engine on Databricks that provides fast query performance at low cost â from data ingestion, ETL, streaming, data science, and interactive queries â directly on your data lake. Photon is compatible with Apache Spark APIs, so getting started is as easy as turning it on â no code changes and no lock-in.\n\nPhoton is part of a high-performance runtime that runs your existing SQL and DataFrame API calls faster, reducing your total cost per workload. Photon is used by default in Databricks SQL warehouses.\n\nUnderstand your hardware and workload type\n\nNot all cloud VMs are created equal. The various families of machines offered by cloud providers are all different enough to matter. There are obvious differences - RAM and cores - and more subtle differences - processor type and generation, network bandwidth guarantees, and local high-speed storage versus local disk versus remote disk. There are also differences in the âspotâ markets. These should be understood before deciding on the best VM type for your workload.\n\nNote\n\nThis is not required for serverless compute because serverless compute automatically manages clusters.\n\nUse caching\n\nCaching stores frequently accessed data in a faster medium, reducing the time required to retrieve it compared to accessing the original data source. This results in lower latency and faster response times, which can significantly improve an applicationâs overall performance and user experience. By minimizing the number of requests to the original data source, caching helps reduce network traffic and data transfer costs. This efficiency gain can be particularly beneficial for applications that rely on external APIs or pay-per-use databases. It can help spread the load more evenly across the system, preventing bottlenecks and potential downtime.\n\nThere are several types of caching available in Databricks. Here are the characteristics of each type:\n\nUse disk cache\n\nThe disk cache (formerly known as âDelta cacheâ) stores copies of remote data on the local disks (for example, SSD) of the virtual machines. It can improve the performance for a wide range of queries but cannot be used to store the results of arbitrary subqueries. The disk cache automatically detects when data files are created or deleted and updates its contents accordingly. The recommended (and easiest) way to use disk caching is to choose a worker type with SSD volumes when configuring your cluster. Such workers are enabled and configured for disk caching.\n\nAvoid Spark Caching\n\nThe Spark cache (by using .persist() and .unpersist()) can store the result of any subquery data and data stored in formats other than Parquet (such as CSV, JSON, and ORC). However, if used in the wrong locations in a query, it can consume all the memory and can even slow down queries significantly. As a rule of thumb, avoid Spark caching unless you know exactly what the impact will be.\n\nQuery Result Cache\n\nPer cluster caching of query results for all queries through SQL warehouses. To benefit from query result caching, focus on deterministic queries that for example, donât use predicates such as = NOW(). When a query is deterministic, and the underlying data is in Delta format and unchanged, SQL Warehouses will return the result directly from the query result cache.\n\nDatabricks SQL UI caching Per user caching of all query and legacy dashboard results results in the Databricks SQL UI.\n\nUse compaction\n\nDelta Lake on Databricks can improve the speed of reading queries from a table. One way is to coalesce small files into larger ones. You trigger compaction by running the OPTIMIZE command. See Optimize data file layout.\n\nDelta Lake provides options for automatically configuring the target file size for writes and for OPTIMIZE operations. Databricks automatically tunes many of these settings, and enables features that automatically improve table performance by seeking to right-size files:\n\nAuto compact combines small files within Delta table partitions to automatically reduce small file problems. Auto compaction occurs after a write to a table has succeeded and runs synchronously on the cluster that has performed the write. Auto compaction only compacts files that havenât been compacted previously.\n\nOptimized writes improve file size as data is written and benefit subsequent reads on the table. Optimized writes are most effective for partitioned tables, as they reduce the number of small files written to each partition.\n\nSee Configure Delta Lake to control data file size for more details.\n\nUse data skipping\n\nData skipping can significantly improve query performance by skipping over data that doesnât meet the query criteria. This reduces the amount of data that needs to be read and processed, leading to faster query execution times.\n\nTo achieve this, data skipping information is automatically collected when you write data to a Delta table (by default Delta Lake on Databricks collects statistics on the first 32 columns defined in your table schema). Delta Lake on Databricks uses this information (minimum and maximum values) at query time to provide faster queries. See Data skipping for Delta Lake.\n\nFor best results, use Z-ordering, a technique for collocating related information in the same set of files. This co-locality is automatically used on Databricks by Delta Lake data-skipping algorithms. This behavior significantly reduces the amount of data Delta Lake must read.\n\nOr use the newer Liquid Clustering, which simplifies data layout decisions and optimizes query performance. It will replace partitioning and z-ordering over time. Databricks recommends liquid clustering for all new delta tables. Liquid Clustering provides the flexibility to redefine clustering keys without rewriting existing data, allowing data layout to evolve with analytical needs over time. Databricks recommends liquid clustering for all new delta tables.\n\nTables with the following characteristics benefit from liquid clustering:\n\nFiltered by columns with high cardinality.\n\nWith significantly skewed data distribution.\n\nThat grow rapidly and require maintenance and tuning effort.\n\nWith concurrent write requests.\n\nWith access patterns that change over time.\n\nWhere a typical partition key could leave the table with too many or too few partitions.\n\nFor more details and techniques see the Comprehensive Guide to Optimize Databricks, Spark, and Delta Lake Workloads.\n\nEnable predictive optimization for Delta Lake\n\nPredictive optimization removes the need to manually manage maintenance operations for Delta tables on Databricks. With predictive optimization enabled, Databricks automatically identifies tables that would benefit from maintenance operations and runs them for the user. Maintenance operations are only run as necessary, eliminating both unnecessary runs for maintenance operations and the burden associated with tracking and troubleshooting performance.\n\nAvoid over-partitioning\n\nIn the past, partitioning was the most common way to skip data. However, partitioning is static and manifests itself as a filesystem hierarchy. There is no easy way to change partitions as access patterns change over time. Often, partitioning leads to over-partitioning - in other words, too many partitions with too small files, resulting in poor query performance.\n\nDatabricks recommends that you do not partition tables below 1TB in size, and that you only partition by a column if you expect the data in each partition to be at least 1GB.\n\nIn the meantime, a better choice than partitioning is Z-ordering or the newer Liquid Clustering (see above)."
    }
}
{
    "id": "dbpedia_8307_1",
    "rank": 61,
    "data": {
        "url": "https://serokell.io/blog/k-means-clustering-in-machine-learning",
        "read_more_link": "",
        "language": "en",
        "title": "Means Clustering Algorithm in ML",
        "top_image": "https://serokell.io/files/gh/opengraph.ghb8n3f9.normal-K-Means_Clustering_Algorithm.png",
        "meta_img": "https://serokell.io/files/gh/opengraph.ghb8n3f9.normal-K-Means_Clustering_Algorithm.png",
        "images": [
            "https://serokell.io/files/hf/tiny.hfk1x406.Inna_2.png",
            "https://serokell.io/files/q4/q49pm3tx.K-Means_Clustering_Algorithm_pic1_(1).png",
            "https://serokell.io/files/9f/9fd9v9tv.Animation.gif",
            "https://serokell.io/files/a0/a0t1smuq.K-Means_Clustering_Algorithm_pic3_(1).png",
            "https://serokell.io/files/0b/0bzk0ym2.K-Means_Clustering_Algorithm_pic4.png",
            "https://serokell.io/files/f0/f0ippkwe.K-Means_Clustering_Algorithm_pic5.png",
            "https://serokell.io/files/ao/aohnu13t.K-Means_Clustering_Algorithm_pic7.png",
            "https://serokell.io/files/e5/e5g946o6.K-Means_Clustering_Algorithm_pic8.png",
            "https://serokell.io/files/a7/a7hx9tga.K-Means_Clustering_Algorithm_pic9.png",
            "https://serokell.io/files/iw/iwwjvt4p.monads-horizontal-320@3x.png",
            "https://serokell.io/files/dd/thumb.dd43n4zp.normal_(3).jpg",
            "https://serokell.io/files/hi/thumb.hi4xwf8b.mobile_(3).jpg",
            "https://serokell.io/files/x2/thumb.x29ob2s7.A_Guide_to_One-Shot_Learning.jpg",
            "https://serokell.io/files/js/thumb.jsrxi1pj.mobile-A_Guide_to_One-Shot_Learning.jpg",
            "https://serokell.io/files/6j/thumb.6jmnuhdv.desktop_(370).jpg",
            "https://serokell.io/files/g2/thumb.g2uz2pa0.mobile_(37).jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Inna Logunova"
        ],
        "publish_date": "2023-01-10T00:00:00+00:00",
        "summary": "",
        "meta_description": "How is the k-means clustering algorithm used in machine learning? How to find the optimal k-value for k-means clustering? Read this post to find out.",
        "meta_lang": "en",
        "meta_favicon": "/favicon.png",
        "meta_site_name": "K-Means Clustering Algorithm in ML",
        "canonical_link": "https://serokell.io/blog/k-means-clustering-in-machine-learning",
        "text": "K-means is a data clustering approach for unsupervised machine learning that can separate unlabeled data into a predetermined number of disjoint groups of equal variance – clusters – based on their similarities.\n\nIt’s a popular algorithm thanks to its ease of use and speed on large datasets. In this blog post, we look at its underlying principles, use cases, as well as benefits and limitations.\n\nWhat is k-means clustering?\n\nK-means is an iterative algorithm that splits a dataset into non-overlapping subgroups that are called clusters. The amount of clusters created is determined by the value of k – a hyperparameter that’s chosen before running the algorithm.\n\nHow does k-means clustering work?\n\nFirst, the algorithm selects k initial points, where k is the value provided to the algorithm.\n\nEach of these serves as an initial centroid for a cluster – a real or imaginary point that represents a cluster’s center. Then each other point in the dataset is assigned to the centroid that’s closest to it by distance.\n\nAfter that, we recalculate the locations of the centroids. The coordinate of the centroid is the mean value of all points of the cluster. You can use different mean functions for this, but a commonly used one is the arithmetic mean (the sum of all points, divided by the number of points).\n\nOnce we have recalculated the centroid locations, we can readjust the points to the clusters based on distance to the new locations.\n\nThe recalculation of centroids is repeated until a stopping condition has been satisfied.\n\nSome common stopping conditions for k-means clustering are:\n\nThe centroids don’t change location anymore.\n\nThe data points don’t change clusters anymore.\n\nFinally, we can also terminate training after a set number of iterations.\n\nTo sum up, the process consists of the following steps:\n\nProvide the number of clusters (k) the algorithm must generate.\n\nRandomly select k data points and assign each as a centroid of a cluster.\n\nClassify data based on these centroids.\n\nCompute the centroids of the resulting clusters.\n\nRepeat the steps 3 and 4 until you reach a stopping condition.\n\nHow to choose the k value?\n\nThe end result of the algorithm depends on the number of сlusters (k) that’s selected before running the algorithm. However, choosing the right k can be hard, with options varying based on the dataset and the user’s desired clustering resolution.\n\nThe smaller the clusters, the more homogeneous data there is in each cluster. Increasing the k value leads to a reduced error rate in the resulting clustering. However, a big k can also lead to more calculation and model complexity. So we need to strike a balance between too many clusters and too few.\n\nThe most popular heuristic for this is the elbow method.\n\nBelow you can see a graphical representation of the elbow method. We calculate the variance explained by different k values while looking for an “elbow” – a value after which higher k values do not influence the results significantly. This will be the best k value to use.\n\nMost commonly, Within Cluster Sum of Squares (WCSS) is used as the metric for explained variance in the elbow method. It calculates the sum of squares of distance from each centroid to each point in that centroid’s cluster.\n\nWe calculate this metric for the range of k values we want to test and look at the results. At the point where WCSS stops dropping significantly with each new cluster added, adding another cluster to the model won’t really increase its explanatory power by a lot. This is the “elbow,” and we can safely pick this number as the k value for the algorithm.\n\nWatch this video to learn more about the general principles of k-means clustering and picking the k value.\n\nWhat is the difference between kNN and k-means?\n\nSince both k-nearest neighbors (kNN) and k-means clustering use a hyperparameter called k, they can be mistaken for each other. Let’s look at the differences between these machine learning algorithms.\n\nThe main difference between the two algorithms is that k-means is a clustering algorithm, while k-nearest neighbors is a classification algorithm.\n\nK-means is an unsupervised algorithm. It divides an unlabeled set of data into clusters based on the patterns that the data exhibits. The clusters are created by the algorithm and might not map to any “human” concepts.\n\nIn contrast, kNN is a supervised algorithm. It uses a distance function to find the closest labeled points to a given unlabeled point, the classes of which are then used to classify that point. This enables you to use a previously labeled data set to label new data points. The classes used for labeling are already predetermined: there is no way to create new ones.\n\nThe k value in k-means refers to the amount of clusters that will be created by the algorithm, while the k in kNN refers to the number of points (nearest neighbors) that will be used to classify any given point.\n\nTo learn more about k-nearest neighbors, check out our post on the algorithm.\n\nBenefits and drawbacks\n\nTo understand the advantages and disadvantages of k-means, it’s practical to compare it to hierarchical clustering.\n\nHierarchical clustering (hierarchical cluster analysis) is an analysis method that aims to construct a hierarchy of clusters without a predetermined number of clusters.\n\nK-means advantages K-means drawbacks\n\nIt is straightforward to understand and apply.\n\nYou have to set the number of clusters – the value of k. It is applicable to clusters of different shapes and dimensions. With a large number of variables, k-means performs faster than hierarchical clustering. It’s sensitive to rescaling. For example, the outcome will be dramatically different if we rescale our data using normalization. It provides tighter clusters than hierarchical clustering. The input, such as the number of clusters in a network, significantly impacts output (value of k). Clustering doesn’t work if you need the clusters to have a complex geometric shape.\n\nPractical applications\n\nSearch engines\n\nClustering is the foundation of many search engines that often use this technique to organize search results. To provide the user with the most accurate answer to their query, the search engine must analyze millions of unstructured pages, evaluate their content similarity, and sort the results by content relevance. After that, URLs are shown in descending order based on the assessment of hundreds of parameters, such as page content, meta tags, loading speed, usability, user behavior, etc.\n\nK-means clustering for SE is a method for improving page ranking efficiency based on the assumption that documents with similar keywords and phrases better match search queries.\n\nTo evaluate the similarity of documents, search engines break down each record into individual words, phrases, and combinations of words. Since not all words are equally important, elements such as prepositions and articles are excluded from the analysis. The remaining content is ranked according to its relevance to the user’s query, previous searches, frequency of use in the natural language (TF-IDF matrix of features), and some other metrics.\n\nAnomaly detection for business analysis\n\nK-means clustering can be used to identify data points that are different from all others in the same dataset (outliers). These could be abnormal time values, non-typical user behavior, inconsistent experiment results, etc. One example is the analysis of repetitive transactions in the financial market. The algorithm can separate a group of users with suspicious behavior who conduct too many transactions in a short time and alert the bank or financial organization of possible fraud.\n\nReal estate\n\nThe k-means clustering algorithm can effectively group properties with similar non-spatial characteristics affecting the value of the premises. This categorization method helps elaborate tax maps, improve property resource management, and facilitate decision making.\n\nMeasuring academic results\n\nK-means can group students based on their test scores and preliminary assessments, providing insight into the effectiveness of teaching methods or student motivation. Student clustering gives an educational institution objective metrics that can help understand how to improve academic performance in the long and short term.\n\nMedical diagnosis\n\nIn healthcare, k-means is used to develop intelligent medical decision support systems, particularly in treating liver diseases. The main benefit of using k-means clustering in medical diagnosis is that we can recognize a particular disease earlier by clustering the patients with similar symptoms. This unsupervised method supports the doctor’s research when the disease has not yet been identified because of a lack of data. In contrast, the ML model assigns a patient to a cluster based on the available analysis results, providing a direction for further investigation.\n\nSerokell has successfully worked with biotech companies. Find out about the services we offer as a biotech software developer.\n\nCustomer segmentation\n\nIn retail, the simplest segmentation can be done based on demographics similarities. An unsupervised machine algorithm, such as k-means for cluster analysis, allows us to divide customers into groups with similar behavioral patterns and consumer preferences. With this information, it is possible to customize the ads, send targeted newsletters, offer special coupons to certain categories, and stimulate rare buyers with unique offers tailored to their preferences.\n\nImage clustering\n\nClustering is one of the essential stages of image recognition. Suppose we want to detect a chair or a person in an indoor image. In that case, we can use image clustering to separate backgrounds and important elements and analyze each object separately. K-means fulfills this task quickly and efficiently.\n\nExample: k-means clustering with scikit-learn\n\nAs an example, we’ll apply k-means clustering to medical data. We’ll use the diabetes dataset and the k-means algorithm available in the sklearn package.\n\nfrom sklearn.cluster import KMeans from sklearn import datasets from sklearn.utils import shuffle diabetes = datasets.load_diabetes()\n\nThe dataset gives us data on 442 diabetes patients with variables like age, sex, body mass index (BMI), blood pressure, and blood serum measurements. Suppose we want to divide the patients into groups for determining certain risk groups.\n\nFor simplicity, we’ll only work with the first three columns: sex, age, and body mass index (BMI).\n\nX = diabetes.data[:,:3] y = diabetes.target names = diabetes.feature_names[:3] X, y = shuffle(X, y, random_state=42)\n\nFor the division to be representative, we need to find the optimal number of clusters (patient groups) before clustering.\n\nWe can do it with the elbow method described in this article. We’ll run k-means with cluster numbers from 1 to 50 and calculate Euclidean distances between points and cluster centroids. By summarizing these distances, we’ll get the within-cluster squared error (WSS).\n\nThe number of clusters where this error stops to decrease dramatically is the optimal “elbow” we want to find.\n\nfrom tqdm.notebook import tqdm as tqdm def calculate_WSS(points, kmax): sse = [] for k in tqdm(range(1, kmax+1)): kmeans = KMeans(n_clusters = k).fit(points) centroids = kmeans.cluster_centers_ pred_clusters = kmeans.predict(points) curr_sse = 0 for i in range(len(points)): curr_center = centroids[pred_clusters[i]] curr_sse += (points[i, 0] - curr_center[0]) ** 2 + (points[i, 1] - curr_center[1]) ** 2 sse.append(curr_sse) return sse sse = calculate_WSS(X, 50)\n\nWe can plot the results on a chart.\n\nimport matplotlib.pyplot as plt import matplotlib as mpl %matplotlib inline plt.plot(sse) plt.grid() plt.title('Sum of squared errors within centroids dependent on number of clusters') plt.xlabel('N Clusters') plt.ylabel('Sum of squared errors') plt.show()\n\nFrom the chart above, it’s clear that the “elbow” is somewhere around the k value of 10. So let’s run k-means with 10 clusters.\n\nmodel = KMeans(n_clusters=10, random_state=42) diabetes_kmeans = model.fit(X) fig = plt.figure(figsize=(20, 10)) ax1 = fig.add_subplot(1, 2, 1, projection='3d') ax1.scatter(X[:, 0], X[:, 1], X[:, 2], c=diabetes_kmeans.labels_.astype(float), edgecolor=\"k\", s=150, cmap=plt.get_cmap('Paired')) ax1.view_init(20, -50) ax1.set_xlabel(names[0], fontsize=12) ax1.set_ylabel(names[1], fontsize=12) ax1.set_zlabel(names[2], fontsize=12) ax1.set_title(\"K-Means Clusters for the Diabetes Dataset\", fontsize=12) ax2 = fig.add_subplot(1, 2, 2, projection='3d') ax2.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, edgecolor=\"k\", s=150, cmap=plt.get_cmap('Greys')) ax2.view_init(20, -50) ax2.set_xlabel(names[0], fontsize=12) ax2.set_ylabel(names[1], fontsize=12) ax2.set_zlabel(names[2], fontsize=12) ax2.set_title(\"Diabetes progression\", fontsize=12) fig.show()\n\nNow we can analyze the results of clustering. An important note is that the dataset was normalized beforehand, so the values for age, sex, and BMI are not absolute but percentage deviations from the mean.\n\nThe first figure shows the clusters found. The main subdivision is by sex, with five groups for women and men. The bright dots on the left represent women, while the pale dots on the right represent men.\n\nWomen can be divided into five groups:\n\nyounger patients with low BMI (blue);\n\nolder patients with low BMI (green);\n\nyounger patients with average BMI (red);\n\nolder patients with average BMI (brown);\n\nmiddle-aged women with high BMI (yellow).\n\nFor men, we have the following groups:\n\nyoung patients with low BMI (pale brown);\n\nmiddle-aged patients with low to average BMI (pale blue);\n\nolder patients with low BMI (purple);\n\nolder patients with high BMI (pale purple);\n\nyounger patients with high BMI (pale red).\n\nThe target variable for the dataset contains blood test results showing which patients have developed diabetes and how pronounced their condition is. You can see a plot with this value in the second plot.\n\nThe most severe cases are colored black, while the milder cases are gray. For both women and men, we can deduce that the groups with high BMI and older age are most at risk. They are grouped in clusters of yellow, brown, light, red, and light purple.\n\nConclusion: why is the method so popular?\n\nK-means is the fastest unsupervised machine learning algorithm to break down data points into groups even when very little information is available. Thanks to its high speed, K-means clustering is a good choice for large datasets. Simple and flexible, it’s also the optimal algorithm to get started with clustering.\n\nAll these features make it effective for multiple industries, including scientific research, business analysis, archive management, search engines, etc."
    }
}
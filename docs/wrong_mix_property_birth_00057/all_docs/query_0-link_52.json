{
    "id": "wrong_mix_property_birth_00057_0",
    "rank": 52,
    "data": {
        "url": "https://siddancha.github.io/",
        "read_more_link": "",
        "language": "en",
        "title": "Siddharth Ancha",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://siddancha.github.io/images/siddharth_ancha_865_813.png",
            "https://siddancha.github.io/images/logos/mit.svg",
            "https://siddancha.github.io/images/logos/cmu.png",
            "https://siddancha.github.io/images/logos/uoft.png",
            "https://siddancha.github.io/images/logos/iitg.png",
            "https://siddancha.github.io/projects/evora/thumbnails/thumbnail_1125x500.webp",
            "https://siddancha.github.io/projects/evidential-segmentation-uncertainty/thumbnails/thumbnail_1125x500.webp",
            "https://siddancha.github.io/projects/active-velocity-estimation/thumbnails/thumbnail_1900x874.webp",
            "https://siddancha.github.io/images/3dv21_1.png",
            "https://siddancha.github.io/images/cvpr21.png",
            "https://siddancha.github.io/projects/active-perception-light-curtains/resources/thumbnail.gif",
            "https://siddancha.github.io/images/iros20.jpeg",
            "https://siddancha.github.io/images/corl19.png",
            "https://siddancha.github.io/images/miccai18.png",
            "https://siddancha.github.io/images/miccai16.png",
            "https://siddancha.github.io/images/neurips16.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "",
        "meta_favicon": "favicons/mit_favicon.ico",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "I am a postdoctoral researcher at MIT in Nicholas Roy's Robust Robotics Group. I am interested in enabling robots to actively, robustly and adaptively perceive their surroundings in order to autonomously and reliably navigate in complex environments. Towards this goal, I draw on techniques at the intersection of machine learning, robotics and computer vision.\n\nI obtained my PhD in the Machine Learning Department at CMU, co-advised by David Held and Srinivasa Narasimhan. My PhD work developed active perception algorithms for Programmable Light Curtains, a fast and high-resolution depth sensor that needs to be controlled intelligently in order to sense efficiently.\n\nPrior to that, I was a masters student in the Department of Computer Science at the University of Toronto, where I worked on statistical machine learning with Daniel Roy and Roger Grosse. I also spent multiple summers working with Aditya Nori at Microsoft Research Cambridge. I graduated from IIT Guwahati with a major in Computer Science and a minor in Mathematics.\n\nDeep Evidential Uncertainty Estimation for Semantic Segmentation under OOD Obstacles\n\nSiddharth Ancha, Philip R. Osteen, Nicholas Roy\n\nICRA 2024 (Best paper nomination in robot vision)\n\nwebpage | abstract | pdf | bibtex | talk\n\nIn order to navigate safely and reliably in novel environments, robots must estimate perceptual uncertainty when confronted with out-of-distribution (OOD) obstacles not seen in training data. We present a method to accurately estimate pixel-wise uncertainty in semantic segmentation without requiring real or synthetic OOD examples at training time. From a shared per-pixel latent feature representation, a classification network predicts a categorical distribution over semantic labels, while a normalizing flow estimates the probability density of features under the training distribution. The label distribution and density estimates are combined in a Dirichlet-based evidential uncertainty framework that efficiently computes epistemic and aleatoric uncertainty in a single neural network forward pass. Our method is enabled by three key contributions. First, we simplify the problem of learning a transformation to the training data density by starting from a fitted Gaussian mixture model instead of the conventional standard normal distribution. Second, we learn a richer and more expressive latent pixel representation to aid OOD detection by training a decoder to reconstruct input image patches. Third, we perform theoretical analysis of the loss function used in the evidential uncertainty framework and propose a principled objective that more accurately balances training the classification and density estimation networks. We demonstrate the accuracy of our uncertainty estimation approach under long-tail OOD obstacle classes for semantic segmentation in both off-road and urban driving environments.\n\nActive Velocity Estimation using Light Curtains via Self-Supervised Multi-Armed Bandits\n\nSiddharth Ancha, Gaurav Pathak, Ji Zhang, Srinivasa Narasimhan,\n\nDavid Held\n\nRSS 2023 (Invited to Autonomous Robots special issue)\n\nwebpage | abstract | pdf | bibtex | code | short talk | long talk\n\nTo navigate in an environment safely and autonomously, robots must accurately estimate where obstacles are and how they move. Instead of using expensive traditional 3D sensors, we explore the use of a much cheaper, faster, and higher resolution alternative: programmable light curtains. Light curtains are a controllable depth sensor that sense only along a surface that the user selects. We adapt a probabilistic method based on particle filters and occupancy grids to explicitly estimate the position and velocity of 3D points in the scene using partial measurements made by light curtains. The central challenge is to decide where to place the light curtain to accurately perform this task. We propose multiple curtain placement strategies guided by maximizing information gain and verifying predicted object locations. Then, we combine these strategies using an online learning framework. We propose a novel self-supervised reward function that evaluates the accuracy of current velocity estimates using future light curtain placements. We use a multi-armed bandit framework to intelligently switch between placement policies in real time, outperforming fixed policies. We develop a full-stack navigation system that uses position and velocity estimates from light curtains for downstream tasks such as localization, mapping, path-planning, and obstacle avoidance. This work paves the way for controllable light curtains to accurately, efficiently, and purposefully perceive and navigate complex and dynamic environments.\n\nActive Safety Envelopes using Light Curtains with Probabilistic Guarantees\n\nSiddharth Ancha, Gaurav Pathak, Srinivasa Narasimhan,\n\nDavid Held\n\nRSS 2021\n\nwebpage | abstract | pdf | bibtex | code | talk | blog\n\nTo safely navigate unknown environments, robots must accurately perceive dynamic obstacles. Instead of directly measuring the scene depth with a LiDAR sensor, we explore the use of a much cheaper and higher resolution sensor: programmable light curtains. Light curtains are controllable depth sensors that sense only along a surface that a user selects. We use light curtains to estimate the safety envelope of a scene: a hypothetical surface that separates the robot from all obstacles. We show that generating light curtains that sense random locations (from a particular distribution) can quickly discover the safety envelope for scenes with unknown objects. Importantly, we produce theoretical safety guarantees on the probability of detecting an obstacle using random curtains. We combine random curtains with a machine learning based model that forecasts and tracks the motion of the safety envelope efficiently. Our method accurately estimates safety envelopes while providing probabilistic safety guarantees that can be used to certify the efficacy of a robot perception system to detect and avoid dynamic obstacles. We evaluate our approach in a simulated urban driving environment and a real-world environment with moving pedestrians using a light curtain device and show that we can estimate safety envelopes efficiently and effectively.\n\nCombining Deep Learning and Verification for Precise Object Instance Detection\n\nSiddharth Ancha*, Junyu Nan*, David Held\n\nCoRL 2019\n\nwebpage | abstract | pdf | bibtex | talk | code\n\nDeep learning object detectors often return false positives with very high confidence. Although they optimize generic detection performance, such as mean average precision (mAP), they are not designed for reliability. For a reliable detection system, if a high confidence detection is made, we would want high certainty that the object has indeed been detected. To achieve this, we have developed a set of verification tests which a proposed detection must pass to be accepted. We develop a theoretical framework which proves that, under certain assumptions, our verification tests will not accept any false positives. Based on an approximation to this framework, we present a practical detection system that can verify, with high precision, whether each detection of a machine-learning based object detector is correct. We show that these tests can improve the overall accuracy of a base detector and that accepted examples are highly likely to be correct. This allows the detector to operate in a high precision regime and can thus be used for robotic perception systems as a reliable instance detection method."
    }
}
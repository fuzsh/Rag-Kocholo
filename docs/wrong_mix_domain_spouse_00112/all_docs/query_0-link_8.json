{
    "id": "wrong_mix_domain_spouse_00112_0",
    "rank": 8,
    "data": {
        "url": "https://ieg.worldbankgroup.org/evaluation-international-development/chapter-3-guidance-notes-evaluation-approaches-and-methods",
        "read_more_link": "",
        "language": "en",
        "title": "Chapter 3 | Guidance Notes on Evaluation Approaches and Methods in Development",
        "top_image": "https://ieg.worldbankgroup.org/themes/custom/iegbootstrap5/favicon.ico",
        "meta_img": "https://ieg.worldbankgroup.org/themes/custom/iegbootstrap5/favicon.ico",
        "images": [
            "https://ieg.worldbankgroup.org/sites/default/files/Data/logo_1_0.png",
            "https://ieg.worldbankgroup.org/sites/default/files/Data/inline-images/header-image_0.png",
            "https://ieg.worldbankgroup.org/sites/default/files/Data/inline-images/logo-footer.png",
            "https://ieg.worldbankgroup.org/sites/default/files/Data/inline-images/what_works.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "This chapter presents guidance notes on approaches and methods for evaluators working in international development. First, we describe several prevalent methodological approaches, followed by guidance notes on specific methods and tools. The latter are further divided according to their primary…",
        "meta_lang": "en",
        "meta_favicon": "/themes/custom/iegbootstrap5/favicon.ico",
        "meta_site_name": "",
        "canonical_link": "https://ieg.worldbankgroup.org/evaluation-international-development/chapter-3-guidance-notes-evaluation-approaches-and-methods",
        "text": "This chapter presents guidance notes on approaches and methods for evaluators working in international development. First, we describe several prevalent methodological approaches, followed by guidance notes on specific methods and tools. The latter are further divided according to their primary function for either data collection or data analysis.\n\nEach guidance note briefly describes the approach and its methodological variations, main procedural steps, key advantages and disadvantages, and applicability. Finally, each guidance note includes references for relevant background readings (for example, journal articles, book chapters, method guides), illustrative case examples, and, when relevant, other useful resources (for example, links to useful online tools or software).\n\nMain Methodological Approaches\n\n1. Efficiency Analysis: Cost-Benefit and Cost-Effectiveness\n\nBrief Description of the Approach\n\nEfficiency analysis commonly refers to economic approaches that compare the relative costs and benefits (outcomes) of the program being evaluated. The main reason to do efficiency analysis is to establish whether the benefits of the program outweigh its associated costs. This type of information is particularly relevant when making decisions about future program planning and design and when considering alternative programs. Evaluation questions to be answered by efficiency analysis include the following:\n\nWhat is the (cumulative) program effect relative to program costs?\n\nTo what extent does the benefit-cost ratio of the program vary across subgroups of the population?\n\nHow does the cost-effectiveness of the program compare with that of other programs (or other program variants)?\n\nThe Main Variations of the Approach\n\nTwo main variations of efficiency analysis are cost-benefit and cost-effectiveness analysis. In cost-benefit analysis, also known as benefit-cost analysis, the program costs and effects are both defined in monetary terms, allowing for a direct comparison of costs and effects. The analysis can be conducted from a strictly financial or more general economic perspective.\n\nIn contrast, cost-effectiveness analysis compares the program costs defined in monetary terms with program effects defined in nonmonetary terms. For example, the number of children vaccinated may be a program effect in cost-effective analysis. Moreover, cost-effectiveness analyses often involve the comparison of cost-effectiveness ratios of similar programs or program variations.\n\nOther closely related variants include cost-utility analysis, risk-benefit analysis, and social return on investment analysis, among others.\n\nThe Main Procedural Steps of the Approach\n\nEfficiency analysis usually involves seven core steps:\n\nDefining the program costs and effects (effects are usually specified by program objectives);\n\nDeciding which costs and benefits should be included;\n\nEstimating the program costs;\n\nQuantifying the net program benefits (in monetary terms for cost-benefit analysis);\n\nAdjusting costs and benefits to net present value using a discount rate (see discount rate in appendix A, Glossary of Key Terms);\n\nCalculating the estimated cost-effectiveness ratio (or net [present] value for cost-benefit analysis); and\n\nConducting robustness checks and sensitivity analysis.\n\nThe cornerstone of any efficiency analysis is the careful and thorough identification and measurement of all cost elements conceivably related to the program being evaluated. Conceptually, costs are defined as the sum of all program resources: staffing, supplies, facilities, and so on. Although many of these costs can be measured in monetary terms and valued through program records, other costs, such as in-kind contributions or other indirect costs incurred by partnering agencies, can be more difficult to accurately identify and quantify. For cost-benefit analysis, quantifying and assigning monetary values to the benefits constitutes a second cornerstone.\n\nThe Advantages and Disadvantages of the Approach\n\nEfficiency analysis can be conducted before or after a program has been implemented (that is, prospectively or retrospectively). If designed and implemented well, findings from efficiency analyses may serve well to inform future program planning and designs, for example, by motivating scale-up of cost-effective programs and modifications of cost-ineffective programs. Establishing cost-effectiveness for accountability purposes may also justify incurred program costs and support continuation of funding, especially in the context of budget or resource constraints. But the results of efficiency analyses are only one of many different inputs on which these decisions are and should be made.\n\nThere are also significant challenges in the use of efficiency analyses. Quantifying program costs and benefits (and any negative effects) in monetary terms can be difficult, especially when defining and measuring outcomes such as resilience, empowerment, or safety. Even if program costs and effects can be conceptually and thoughtfully defined, collecting relevant data can also be difficult. Designing and implementing rigorous efficiency analyses demands high capacity of the team in terms of economic and financial analysis, statistics, and program knowledge. Finally, the quality of cost-benefit analyses can be difficult to assess when there is limited transparency on how costs and benefits are defined, identified, and measured. The development and inclusion of a table specifying the included program costs and the provision of clear specifications of the program effects is considered standard practice.\n\nThe Applicability of the Approach\n\nEfficiency analyses are most useful to those involved in the design and oversight of programs, and, importantly, to those whom the program would directly affect, positively or negatively. Efficiency analyses may prompt decision makers to consider alternative program options, including taking no action when costs outweigh benefits. Efficiency analyses can provide significant value to independent evaluators as inputs to their broader evaluation studies—if they are implemented competently and transparently.\n\nPractical applications of cost-effectiveness and cost-benefit analysis include the following:\n\nA retrospective social cost-benefit analysis was used for cholera vaccination in Sub-Saharan Africa. Based on economic and epidemiological data collected in Beira, Mozambique, the analysis compares the net economic benefits of three immunization strategies. (Source: Jeuland, M., M. Lucas, J. Clemens, and D. Whittington. 2009. “A Cost-Benefit Analysis of Cholera Vaccination Programs in Beira, Mozambique.” The World Bank Economic Review 23 (2): 235–67. https://openknowledge.worldbank.org/handle/10986/4502 .)\n\nCost-benefit analysis was used for the AGEXPORT, a rural value chains project in Guatemala, where local producer associations were supported financially to help their members produce high-quality coffee beans. Net benefits were broken down by small, medium-size, and large farms. (Source: USAID [US Agency for International Development]. 2013. Economic Analysis of Feed the Future Investments—Guatemala. Washington, DC: USAID. https://www.usaid.gov/documents/1865/economic-analysis-feed-future-investments-guatemala .)\n\nCost-benefit analysis was used for the Markets II program in Nigeria, where multiple interventions for poor rural farmers sought to improve their access to better inputs, adequate finance, better water management, appropriate technology, extension services, and improved nutritional uses of grown or purchased basic foods. The results at the farm level were aggregated and projected for a 10-year prognosis.(Source: USAID [US Agency for International Development]. 2015. Cost-Benefit Analysis of USAID/Nigeria’s Markets II Program. Washington, DC: USAID. https://www.usaid.gov/documents/1865/cost-benefit-analysis-usaidnigeria%E2%80%99s-markets-ii-program#overlay-context=what-we-do/economic-growth-and-trade/promoting-sound-economic-policies-growth/working-more.)\n\nCost-effectiveness analysis was used to compare multiple education interventions for increasing school attendance in Kenya. Each intervention was subjected to a randomized design and cost-benefit analysis.(Source: Kremer, M., and E. Miguel. 2004. “Worms: Identifying Impacts on Education and Health in the Presence of Treatment Externalities.” Econometrica 72: 159–217.)\n\nAn ex post cost-benefit analysis was used to assess the economic justification of 50 World Bank–financed dam projects.(Source: World Bank. 2005. Influential Evaluations: Detailed Case Studies. Washington, DC: World Bank. http://documents.worldbank.org/curated/en/928001468330038416/pdf/328800Influent1luation1case1studies.pdf .)\n\nA prospective cost-benefit analysis was applied on an earthquake vulnerability reduction project in Colombia.(Source: Ghesquiere, F., L. Jamin, and O. Mahul. 2006. “Earthquake Vulnerability Reduction Program in Colombia: A Probabilistic Cost-Benefit Analysis.” Policy Research Working Paper 3939, World Bank, Washington, DC. https://openknowledge.worldbank.org/handle/10986/8438 .)\n\nCost-benefit analysis was used to assess land fragmentation and its impact on the efficiency of resource use in rural Rwanda.(Source: Ali, D. A., K. Deininger, and L. Ronchi. 2015. “Costs and Benefits of Land Fragmentation: Evidence from Rwanda.” Policy Research Working Paper 7290, World Bank, Washington, DC. https://openknowledge.worldbank.org/handle/10986/22163 .)\n\nA retroactive cost-benefit analysis was used in a reassessment of the welfare impact of rural electrification programs.(Source: World Bank. 2008. The Welfare Impact of Rural Electrification: A Reassessment of the Costs and Benefits. Washington, DC: World Bank. https://openknowledge.worldbank.org/handle/10986/6519 .)\n\nA retrospective cost-benefit analysis of six road projects was conducted in Argentina, Botswana, India, Kenya, the Lao People’s Democratic Republic, and Paraguay. The findings were used for scenario analysis to project economic viability of future road projects.(Source: Tsunokawa, K. 2010. Road Projects Cost Benefit Analysis: Scenario Analysis of the Effect of Varying Inputs. Working Paper 81577, World Bank, Washington, DC. https://openknowledge.worldbank.org/handle/10986/27814< .)\n\nReadings and Resources\n\nBackground\n\nAsian Development Bank. 2013. Cost-Benefit Analysis for Development: A Practical Guide. Mandaluyong City, Philippines: Asian Development Bank. https://www.adb.org/documents/cost-benefit-analysis-development-practical-guide .\n\nBelli, P., J. R. Anderson, H. N. Barnum, J. A. Dixon, and J-. P. Tan. 2001. Economic Analysis of Investment Operations—Analytical Tools and Practical Applications. Washington, DC: World Bank. http://documents.worldbank.org/curated/en/792771468323717830/pdf/298210REPLACEMENT.pdf .\n\nBoardman, A., A. Vining, D. Greenberg, and D. Weimer. 2001. Cost-Benefit Analysis: Concepts and Practice. New Jersey, NJ: Prentice Hall.\n\nDhaliwal, Iqbal, Esther Duflo, Rachel Glennerster, and Caitlin Tulloch. 2011. “Comparative Cost-Effectiveness Analysis to Inform Policy in Developing Countries: A General Framework with Applications for Education.” Abdul Latif Jameel Poverty Action Lab (J-PAL), MIT. Cambridge, MA: Massachusetts Institute of Technology. https://economics.mit.edu/files/6959/ .\n\nFletcher, J. D. 2010. “Cost Analysis in Evaluation Studies.” In International Encyclopedia of Education, edited by Penelope Peterson, Eva Baker, and Barry McGaw, 585–91. Oxford: Elsevier.\n\nJamison, Dean T. 2009. “Cost Effectiveness Analysis: Concepts and Applications.” In Methods of Public Health, vol. 2 of Oxford Textbook of Public Health, 5th ed., edited by R. Detels, J. McEwen, R. Beaglehole, and H. Tanaka, 767–82. Oxford: Oxford University Press. http://depts.washington.edu/cfar/sites/default/files/uploads/core-program/user164/Jamison%20CEA%20Concepts%20and%20Applications.pdf.\n\nLevin, Henry M., Patrick J. McEwan, Clive R. Belfield, A. Brooks Bowden, and Robert D. Shand. 2018. Economic Evaluation in Education: Cost-Effectiveness and Benefit-Cost Analysis, 3rd ed. Thousand Oaks, CA: SAGE.\n\nLittle, I. M. D., and James A. Mirrlees. 1974. Project Appraisal and Planning for Developing Countries. London: Heinemann Educational Books.\n\nMcEwan, Patrick J. 2012. “Cost-Effectiveness Analysis of Education and Health Interventions in Developing Countries.” Journal of Development Effectiveness 4 (2): 189–213. http://academics.wellesley.edu/Economics/mcewan/PDF/cea.pdf .\n\nvan der Tak, Herman, and Lyn Squire. 1975. Economic Analysis of Projects. Washington, DC: World Bank.\n\nWarner, A. 2010. Cost-Benefit Analysis in World Bank Projects. Washington, DC: World Bank. https://openknowledge.worldbank.org/bitstream/handle/10986/2561/624700PUB0Cost00Box0361484B0PUBLIC0.pdf?sequence=1 .\n\nYates, Brian T. 2015. “Cost-Benefit and Cost-Effectiveness Analyses in Evaluation Research.” In International Encyclopedia of the Social & Behavioral Sciences, 2nd ed., edited by James D. Wright, 55–62. Amsterdam: Elsevier.\n\nAdvanced\n\nBelfield, Clive R., A. Brooks Bowden, and Viviana Rodriguez. 2018. “Evaluating Regulatory Impact Assessments in Education Policy.” American Journal of Evaluation 40 (3): 335–53.\n\nBowden, A. Brooks, Robert Shand, Clive R. Belfield, Anyi Wang, and Henry M. Levin. 2017. “Evaluating Educational Interventions That Induce Service Receipt: A Case Study Application of City Connects.” American Journal of Evaluation 38 (3): 405–19.\n\nCordes, Joseph J. 2017. “Using Cost-Benefit Analysis and Social Return on Investment to Evaluate the Impact of Social Enterprise: Promises, Implementation, and Limitations.” Evaluation and Program Planning 64: 98–104.\n\nDa’ar, Omar B., and Abdulaziz Alshaya. 2018. “Is It Cost-Beneficial to Society? Measuring the Economic Worth of Dental Residency Training.” Evaluation and Program Planning 68: 117–23.\n\nEuropean Commission. 2014. Guide to Cost-Benefit Analysis of Investment Projects: Economic Appraisal Tool for Cohesion Policy 2014–2020. Brussels: European Commission.\n\nMarchante, A. J., and B. Ortega. 2010. “Evaluating Efficiency in the Implementation of Structural Funds Operations.” Evaluation XVI (2): 193–209.\n\nOECD. 2018. Cost-Benefit Analysis and the Environment: Further Developments and Policy Use. Paris: OECD Publishing.\n\nRobinson, Lisa, ed. 2019. “Benefit-Cost Analysis in Low- and Middle-Income Countries: Methods and Case Studies.” Special issue, Journal of Benefit-Cost Analysis 10 (S1). https://www.cambridge.org/core/journals/journal-of-benefit-cost-analysis/issue/special-issue-benefitcost-analysis-in-low-and-middleincome-countries-methods-and-case-studies/CEA50B949FD2F37E60A8E1C0528A9112 .\n\nWorld Bank. 2018. Socioeconomic Analysis of the Potential Benefits of Modernizing Hydrometeorological Services in the Lao People’s Democratic Republic. Washington, DC: World Bank. http://documents.albankaldawli.org/curated/ar/842621563163324249/pdf/Socioeconomic-Analysis-of-the-Potential-Benefits-of-Modernizing-Hydrometeorological-Services-in-the-Lao-People-s-Democratic-Republic.pdf .\n\nOther Resources\n\nThe Cost-Benefit & Cost-Effectiveness Hub of the Inter-American Development Bank is a one-stop shop for information and tools used for economic analysis. https://www.iadb.org/en/topics-effectiveness-improving-lives/cost-benefit-and-cost-effectiveness-resources .\n\n2. Experimental Approach\n\nBrief Description of the Approach\n\nThe primary purpose of experimental designs, commonly referred to as randomized controlled trials, is to provide an accurate estimate of (net) program effects. The defining feature of an experimental design is that people are allocated at random to either a treatment or a control group. Whereas the treatment group receives the program services, the control group receives regular or no services. The underlying logic of the random allocation is that any (observable and unobservable) differences among the treatment or the control groups are evenly distributed between the two groups. Accordingly, any observed outcome differences between the two groups can reasonably be attributed to the program being studied. In this way, experimental designs can help determine whether (and the extent to which) a cause-effect relation exists between the program and the outcome.\n\nEvaluation questions that experimental designs may answer include the following:\n\nWhat is the net effect of the program?\n\nHow does the net effect of the program vary across subgroups of the population?\n\nHow much do program variations affect the net effect estimate?\n\nThe Main Variations of the Approach\n\nA survey of real-world applications of experimental designs reveals a number of design variations. One variant is multiarm designs, where participants are randomly allocated to one of several treatment groups or one of several control groups. This design variant is useful when comparing multiple program variations (that is, multiple treatments). Another common variant is the wait-list design (or pipeline design), where individuals are randomly allocated to immediate program admission or to a wait-list for later program admission, allowing both for accurate effect size estimates and for all the participants to receive the treatment by the end of the evaluation.\n\nExperimental designs may also differ according to the level of randomization. Random allocation can be at the individual level (individual people are randomly assigned to treatment or control) or cluster level (groups of people [for example, communities, districts, or schools] are randomly assigned to treatment or control). Cluster-level randomization is often applied when the program being studied is directed at groups of people (for example, entire villages or communities), as opposed to specific individuals.\n\nThe Main Procedural Steps of the Approach\n\nIn practice, experimental designs consist of the following six steps:\n\nIdentifying the target population for the program;\n\nCollecting baseline data on a representative sample of this population;\n\nRandomly allocating the people in the sample to either the treatment or the control group;\n\nImplementing the program;\n\nCollecting outcome data on both groups (covering at least two data points over time); and\n\nComparing outcome patterns between the treatment and the control group.\n\nData from experimental designs can be analyzed in many different ways. Randomization allows simple mean comparisons between the treatment group and the control group (or subgroups within these) to provide an estimate of the average program effect.\n\nAnother common technique is to compare gain scores, that is, to compute the average difference between the baseline and endline outcome measures for each group and then compare these averages for a mean difference score (this is also known as the difference-in-differences approach, which is described in more detail in guidance note 3, Quasi-Experimental Approaches).\n\nIf the treatment and control group (despite randomization) differ on baseline characteristics, statistical techniques such as multiple regression analysis can be applied to adjust for these differences when estimating the program effect. Finally, when combined with information on program costs, data from experimental designs can also support retrospective cost-effectiveness and cost-benefit analysis (see guidance note 1, Efficiency Analysis: Cost-Benefit and Cost-Effectiveness).\n\nThe Advantages and Disadvantages of the Approach\n\nAn experimental design is particularly relevant when answering evaluation questions related to program effectiveness, particularly the net effect of the program on a specific outcome or set of outcomes. If implemented well, the experimental design provides the most accurate estimate of the net program effect on selected outcomes. The reliance on random assignment enhances the internal validity of any causal claims produced by experimental designs (see internal validity in appendix A, Glossary of Key Terms). In this way, the design serves well to establish cause-effect relationships between a program and its outcomes.\n\nExperimental designs also come with several practical and ethical challenges. First, the design relies on a very stable program implementation and a homogeneous target group to provide accurate program effect estimates. However, these conditions are difficult to maintain in practice and may even reduce the generalizability (external validity) of the evaluation findings (see external validity in appendix A, Glossary of Key Terms). Second, estimates significantly improve with multiple data points. Pathways of change may be nonlinear and two data points (for example, before and after only) may be too limited for reliably estimating the net effect. A third common methodological concern is the possibility of contamination. Contamination can arise from the program itself as a result of spillover effects from individuals in the treatment and the control group influencing each other (own contamination). A fourth concern is that the focus of the analysis is restricted to one or more measurable intended effects. Consequently, it is less suitable for assessing unintended effects. Finally, experimental designs are accurate only if two conditions are met: (i) The evolution or development of treatment and control groups is homogeneous throughout the intervention implementation. This includes homogeneity of intervention across the treatment group. Otherwise, emerging systematic differences between the two groups may result in bias when the program effects are estimated. (ii) A certain level of statistical power is needed to reach statistical significance, under which the findings are not reliable.\n\nIn addition to these practical challenges, the ethical implications of withholding program activities from people in the control group may pose a barrier; the use of wait-list (pipeline) designs, however, may alleviate this concern.\n\nThe Applicability of the Approach\n\nWhen considering the use of experimental designs, evaluators must plan for the randomized assignment of treatment and control groups, and the collection of baseline data for comparison with data collected later. The evaluator therefore should be involved in the design and implementation stages of the evaluation. The evaluators also need to plan and budget for the time- and resource-consuming task of collecting data on both the treatment and the control group. Many types of evaluators will be involved in these early stages of an evaluation, including impact evaluation experts (typically researchers), internal agency evaluators, and external evaluators hired for this purpose. Yet, because of this early and direct engagement with the intervention, a typical IEO evaluator is very unlikely to be involved directly in an experimental design. Nevertheless, though impact evaluations are not typically managed by IEOs, credible randomized controlled trial studies can be very helpful as inputs to other types of retrospective studies on those same programs or as part of a sectorwide evaluation, as sources of evidence for a structured or systematic review. In fact, in lieu of direct involvement in impact evaluations, IEOs, such as the World Bank’s Independent Evaluation Group, do perform systematic reviews (or variations thereof), which draw on impact evaluations carried out by others to deepen the evidence base for sector and thematic evaluations (see guidance note 11, Structured Literature Reviews).\n\nDespite their limitations, applications of experimental designs have over the years covered a broad range of programs and sectors in development evaluation, including the following:\n\nAn impact evaluation of an adolescent development program for girls in Tanzania used a multiarm experimental design.(Source: Buehran, N., M. Goldstein, S. Gulesci, M. Sulaiman, and V. Yam. 2017. “Evaluation of an Adolescent Development Program for Girls in Tanzania.” Policy Research Working Paper 7961, World Bank, Washington, DC. http://documents.worldbank.org/curated/en/245071486474542369/pdf/WPS7961.pdf .)\n\nAn experiment was used in the impact evaluation of a mobile point of service deposit collection for business owners in Sri Lanka. Self-employed individuals were randomly allocated to a treatment program offering weekly door-to-door savings deposit collection services and assistance opening bank accounts.(Source: Callen, M., C. McIntosh, S. de Mel, and C. Woodruff. 2014. “What Are the Headwaters of Formal Savings? Experimental Evidence from Sri Lanka.” NBER Working Paper 20736, National Bureau of Economic Research, Cambridge, MA. https://www.povertyactionlab.org/evaluation/impact-formal-savings-intervention-sri-lanka .)\n\nAn experiment was used to better understand the influence of psychic and economic barriers on vaccination rates.(Source: Sato, R., and Y. Takasaki. 2018. “Psychic vs. Economic Barriers to Vaccine Take-Up: Evidence from a Field Experiment in Nigeria.” Policy Research Working Paper 8347, World Bank, Washington, DC. http://documents.worldbank.org/curated/en/876061519138798752/pdf/WPS8347.pdf .)\n\nRandomized experiments were used in the evaluation of the Balsakhi remedial education program in Mumbai, India. (Source: Banerjee, A., S. Cole, E. Duflo, and L. Lindon. 2007. “Remedying Education: Evidence from Two Randomized Experiments in India.” Quarterly Journal of Economics 122 (3): 1235–64.)\n\nThe pipeline design using cluster (community) randomized allocation was applied in an evaluation of a conditional cash transfer program (the PROGRESA/Oportunidades program). (Source: Bamberger, M., and A. Kirk. 2009. Making Smart Policy: Using Impact Evaluation for Policy-Making: Case Studies on Evaluations That Influenced Policy. Doing Impact Evaluation 14. Washington, DC: World Bank. http://documents.worldbank.org/curated/en/239681468324546563/Making-smart-policy-using-impact-evaluation-for-policy-making-case-studies-on-evaluations-that-influenced-policy .)\n\nRandom allocation of interest rates was used in an evaluation of a loan and consumer credit program in South Africa. (Source: Karlan, D., and J. Zinman. 2003. “Interest Rates and Consumer Credit in South Africa.” Study Summary. New Haven, CT: Innovations for Poverty Action. https://www.poverty-action.org/printpdf/6326 .)\n\nAn experiment was used in the evaluation of a rural microfinance program on agricultural and nonagricultural activities, income, and expenditures in Morocco. (Source: Bamberger, M., and A. Kirk. 2009. Making Smart Policy: Using Impact Evaluation for Policy-Making: Case Studies on Evaluations That Influenced Policy. Doing Impact Evaluation 14. Washington, DC: World Bank. http://documents.worldbank.org/curated/en/239681468324546563/Making-smart-policy-using-impact-evaluation-for-policy-making-case-studies-on-evaluations-that-influenced-policy .)\n\nAn experiment was used to evaluate the effectiveness of insecticide-treated bed nets for malaria prevention in Kenya. (Source: Bamberger, M., and A. Kirk. 2009. Making Smart Policy: Using Impact Evaluation for Policy-Making: Case Studies on Evaluations That Influenced Policy. Doing Impact Evaluation 14. Washington, DC: World Bank. http://documents.worldbank.org/curated/en/239681468324546563/Making-smart-policy-using-impact-evaluation-for-policy-making-case-studies-on-evaluations-that-influenced-policy .)\n\nAn experiment was used in the evaluation of three mother-literacy interventions in rural India to improve child learning through increased mother literacy and direct encouragement of learning at home. Villages were randomly allocated to mother literacy interventions. (Source: Banerji, R., J. Berry, and M. Shortland. 2014. The Impact of Mother Literacy and Participation Programmes on Child Learning: Evidence from a Randomised Evaluation in India. 3ie Impact Evaluation Report 16. New Delhi: International Initiative for Impact Evaluation. https://www.3ieimpact.org/evidence-hub/publications/impact-evaluations/impact-mother-literacy-and-participation-programmes .)\n\nAn experiment was used in an impact evaluation of a voucher program for out-of-school youth, to measure earnings, including wage earnings; self-employed profits; and labor market outcomes. Individual youth were randomly allocated to treatment in the form of a voucher for vocational training. (Source: Hamory, J., M. Kremer, I. Mbiti, and E. Miguel. 2016. Evaluating the Impact of Vocational Education Vouchers on Out-Of-School Youth in Kenya. 3ie Impact Evaluation Report 37. New Delhi: International Initiative for Impact Evaluation. https://www.3ieimpact.org/evidence-hub/publications/impact-evaluations/evaluating-impact-vocational-education-vouchers-out .)\n\nReadings and Resources\n\nBackground\n\nDuflo, Esther, Rachel Glennerster, and Michael Kremer 2007. “Using Randomization in Development Economics Research: A Toolkit.” Center for Economic Policy Research Discussion Paper 6059, Massachusetts Institute of Technology, Cambridge, MA. https://economics.mit.edu/files/806 .\n\nGertler, P. J., S. Martinez, P. Premand, L. B. Rawlings, and C. M. J. Vermeersch. 2016. Impact Evaluation in Practice, 2nd ed. Washington, DC: Inter-American Development Bank and World Bank. https://openknowledge.worldbank.org/handle/10986/25030 .\n\nGlennerster, R., and K. Takavarasha. 2013. Running Randomized Evaluations: A Practical Guide. Princeton, NJ: Princeton University Press.\n\nKhandker, S. R., G. B. Koolwal, and H. A. Samad. 2009. Handbook on Quantitative Methods of Program Evaluation. Washington, DC: World Bank. http://documents.worldbank.org/curated/en/650951468335456749/pdf/520990PUB0EPI1101Official0Use0Only1.pdf .\n\nShadish, W. R., T. D. Cook, and D. T. Campbell. 2002. Experimental and Quasi-Experimental Designs for Generalized Causal Inference. New York: Houghton Mifflin.\n\nAdvanced\n\nBell, S. H., and L. R. Peck. 2016. “On the ‘How’ of Social Experiments: Experimental Designs for Getting Inside the Black Box.” New Directions for Evaluation 152: 97–107.\n\nFaulkner, W. N. 2014. “A Critical Analysis of a Randomized Controlled Trial Evaluation in Mexico: Norm, Mistake or Exemplar?” Evaluation 20 (2): 230–43.\n\nGlewwe, Paul, and Petra Todd, eds. 2020. Impact Evaluation in Developing Countries: Theory, Methods, and Practice. Washington, DC: World Bank.\n\nHiguchi, Yuki, Edwin P. Mhede, and Tetsushi Sonobe. 2019. “Short- and Medium-Run Impacts of Management Training: An Experiment in Tanzania.” World Development 114: 220–36.\n\nLedford, Jennifer R. 2018. “No Randomization? No Problem: Experimental Control and Random Assignment in Single Case Research.” American Journal of Evaluation 39 (1): 71–90.\n\nMcCarthy, Aine Seitz. 2019. “Intimate Partner Violence and Family Planning Decisions: Experimental Evidence from Rural Tanzania.” World Development 114: 156–74.\n\nTipton, E., and B. J. Matlen. 2019. “Improved Generalizability through Improved Recruitment: Lessons Learned from a Large-Scale Randomized Trial.” American Journal of Evaluation 40 (3): 414–30.\n\nOther Resources\n\nThe Abdul Latif Jameel Poverty Action Lab is a global research center working to reduce poverty by ensuring that policy is informed by scientific evidence. Anchored by a network of 194 affiliated professors at universities around the world, it conducts randomized impact evaluations to answer critical questions in the fight against poverty. https://www.povertyactionlab.org/about-j-pal; https://www.poverty-action.org/about/randomized-control-trials .\n\nThe International Initiative for Impact Evaluation funds, produces, quality assures, and synthesizes evidence of policy interventions that work in low- and middle-income countries and advocates the generation and use of quality evidence in development decision-making. https://www.3ieimpact.org/; https://developmentevidence.3ieimpact.org/.\n\nThe World Bank’s Development Impact Evaluation group generates high-quality and operationally relevant data and research to transform development policy, help reduce extreme poverty, and secure shared prosperity. https://www.worldbank.org/en/research/dime; https://dimewiki.worldbank.org/wiki/Main_Page .\n\n3. Quasi-Experimental Approach\n\nBrief Description of the Approach\n\nLike experimental designs, quasi-experimental designs are meant to provide an accurate estimate of (net) program effects. The main difference involves random assignment. Although random assignment is fundamental for experimental design, quasi-experiments do not rely on random assignment of people to establish treatment or comparison groups. Instead, quasi-experiments rely on a broad range of statistical techniques to construct treatment and comparison groups that are comparable in terms of a select set of baseline characteristics. For quasi-experimental designs, the term comparison group is often used instead of control group, which is the term used in experimental design with randomization.\n\nAs for randomized designs, evaluation questions that quasi-experimental designs may answer include the following:\n\nWhat is the net effect of the program?\n\nHow does the net effect of the program vary across subgroups of the population?\n\nHow much do programmatic variations affect the net effect estimate?\n\nThe Main Variations of the Approach\n\nThere are several quasi-experimental designs, and four common types are described here.\n\nIn propensity score matching, people in the treatment group are matched with comparable people (sometimes referred to as “twins”) in the comparison group. The matching is based on the (observable) characteristics of the population believed to affect the probability of participating in the program, summarized in an overall score representing their propensity to be enrolled. The common support (or overlapping) interval represents the range of propensity scores for which both enrolled and unenrolled units are available. The outcomes observed in these two groups are then compared to estimate the program effect. Matching may be applied at the individual or group level; for example, students could be matched with other students, or schools could be matched with other schools.\n\nIn regression discontinuity designs, a program eligibility criterion (for example, income level or test score) is used to construct comparable groups (ideally the program eligibility criterion is not associated with other benefits—for example, other state benefits). The core idea of regression discontinuity design is that individuals immediately below the program cut-off score (those who were not accepted into the program) are similar to those immediately above the cut-off (those who were accepted). To illustrate, consider a program where rural farmers above a specific income level are eligible for a tractor lease program. Those farmers just below the cut-off (the comparison group), although not admitted to the program, are likely to be comparable to those farmers immediately above the cut-off (the treatment group). A regression-based comparison of the difference in average outcomes for these two groups can be used to estimate the program effect.\n\nThe instrumental variable method uses a variable that is correlated with program participation (but not with the program outcome of interest) to adjust for factors affecting the likelihood of program participation. The program effect is then estimated using a regression model containing the instrumental variable, among other relevant covariates.\n\nThe difference-in-differences method estimates the program effect by comparing the difference over time among nonparticipants with that among program participants (that is, the difference in the differences). This approach eliminates external determinants of the outcome that are time-invariant for the treatment and comparison group during the program period.\n\nThe Main Procedural Steps of the Approach\n\nThis guide provides only broad descriptions of these four quasi-experimental approaches. Additional reading on these topics is advised for readers considering them. The basic steps for propensity score matching and regression discontinuity are presented in this section. But the steps for difference-in-differences and instrumental variable approaches require explanation of statistical and analytical steps that are outside of the scope of this guide. Many resources are readily available to describe the methods and steps, such as those from the International Initiative for Impact Evaluation, listed in Other Resources.\n\nThe procedural steps of quasi-experimental designs vary according to the way in which the treatment and comparison groups are constructed.\n\nPropensity score matching generally involves the following five steps:\n\nMaking assumptions on the factors affecting participation in the program;\n\nModeling the relevant variables with a logistic regression model explaining participation or exclusion;\n\nEstimating the propensity to participate in the program (for participants and nonparticipants);\n\nMatching participants and nonparticipants sharing similar propensity scores; and\n\nComparing their evolution over the course of the program and thus estimating program effects.\n\nThe regression discontinuity design consists of four steps:\n\nIdentifying the margin around the cut-off score for program participation where individuals are comparable;\n\nFitting a regression line on these individuals’ cut-off scores and outcome scores;\n\nIdentifying any shift (discontinuity) in the regression line at the cut-off score; and\n\nInterpreting the size of the shift as the estimated program effect.\n\nThe Advantages and Disadvantages of the Approach\n\nQuasi-experimental designs are particularly relevant when the evaluation emphasis is on program effectiveness, and random assignment of people to treatment and control is not possible. In these situations, the quasi-experimental designs may provide the least biased program effect estimates, as compared with, for instance, nonexperimental designs that usually have no or less robust comparison arrangements (see the Maryland Scientific Methods Scale in Other Resources for a ranking of treatment-comparison designs). Moreover, some quasi-experimental designs (for example, propensity score matching) can also be used retrospectively, that is, after the program has been implemented. However, baseline data are usually preferred. Many quasi-experimental designs, however, are attractive to evaluators who may have access to the data from an intervention but have no opportunity for direct involvement with the intervention (that is, in collecting baseline or endline data directly).\n\nQuasi-experiments are not without shortcomings. One methodological weakness of quasi-experimental designs emerges from the lack of random assignment, potentially resulting in treatment and comparison groups that are different in ways that may affect the estimated program effects. Because the construction of comparable groups solely by statistical means accounts for observable characteristics (or time-invariant unobservable differences), the extent to which estimates of program effects are influenced by unobserved differences is a persistent concern (see selection bias in appendix A, Glossary of Key Terms). Again, much depends on the availability of data and the number of data points for both treatment and comparison groups over time. Finally, even when the design is solid and the comparison group result is unbiased, accurate, and comparable with the treatment group result, the data might not be sufficiently precise, because a certain level of statistical power is needed to reach statistical significance.\n\nThe Applicability of the Approach\n\nThe lack of random assignment is typical of most development programs. Quasi-experiments are in practice more applicable in development contexts than experimental designs. However, other factors, including the requirement for baseline data for most quasi-experimental designs, make the approach less applicable to IEO operations. A further limit to the practical application of quasi-experimental designs is the time- and resource-consuming task of collecting data on both the program and the comparison groups.\n\nApplications of quasi-experimental design include the following:\n\nPropensity score matching was used to identify villages that were similar in socioeconomic terms as part of an evaluation of a conflict resolution program (Kecamatan) in Indonesia. (Source: Voss, John. 2008. “Impact Evaluation of the Second Phase of the Kecamatan Development Program in Indonesia.” Working Paper 45590, World Bank, Washington, DC. http://documents.worldbank.org/curated/en/551121468048909312/Impact-evaluation-of-the-second-phase-of-the-Kecamatan-development-program-in-Indonesia .)\n\nPropensity score matching was used to match households (on background variables) as part of an impact evaluation on water and sanitary interventions in Nepal.(Source: Bose, R. 2009. “The Impact of Water Supply and Sanitation Interventions on Child Health: Evidence from DHS Surveys.” Paper presented at the Biannual Conference on Impact Evaluation, Colombo, Sri Lanka, April 22–23. https://editorialexpress.com/cgi-bin/conference/download.cgi?db_name=FEMES09&paper_id=204 .)\n\nPropensity score matching combined with a difference-in-differences method was used to estimate the effect of a conditional cash transfer program in Chile that sought to improve several socioeconomic outcomes for families living in poverty. (Source: Martorano, Bruno, and Marco Sanfilippo. 2012. “Innovative Features in Conditional Cash Transfers: An Impact Evaluation of Chile Solidario on Households and Children.” Innocenti Working Paper 2012–03, UNICEF Innocenti Research Centre, Florence. https://www.unicef-irc.org/publications/656-innovative-features-in-conditional-cash-transfers-an-impact-evaluation-of-chile-solidario.html .)\n\nA regression discontinuity design was used in the evaluation of an educational program under the PROGRESA poverty alleviation program in Mexico City, where children were admitted on the basis of a household income index score.(Source: Buddelmeyer, Hielke, and Emmanuel Skoufias. 2004. “An Evaluation of the Performance of Regression Discontinuity Design on PROGRESA.” Policy Research Working Paper WPS 3386, World Bank, Washington, DC. http://documents.worldbank.org/curated/en/124781468773348613/An-evaluation-of-the-performance-of-regression-discontinuity-design-on-PROGRESA .)\n\nAn instrumental variable approach was used in a World Bank evaluation of an energy-efficiency program in Ethiopia that distributed compact fluorescent lamp bulbs free of charge to poor households. (Source: Costolanski, P., R. Elahi, A. Limi, and R. Kitchlu. 2013. “Impact Evaluation of Free-of-Charge CFL Bulb Distribution in Ethiopia.” Policy Research Working Paper 6383, World Bank, Washington, DC. http://documents1.worldbank.org/curated/en/294421468032720712/pdf/wps6383.pdf .)\n\nAn evaluation of the impact of minimum wages on employment used matched difference-in-differences estimates of the employment impact in select industries in Indonesia. (Source: Alatas, V., and L. A. Cameron. 2003. “The Impact of Minimum Wages on Employment in a Low-Income Country: An Evaluation Using Difference-in-Differences Approach.” Policy Research Working Paper 2985, World Bank, Washington, DC.)\n\nThe instrumental variable approach was applied to evaluate the impact of infrastructure development on economic growth and income distribution in Latin American countries. (Source: Calderon, C., and L. Serven. 2004. “The Effects of Infrastructure Development on Growth and Income Distribution.” Policy Research Working Paper 3400, World Bank, Washington, DC. http://documents1.worldbank.org/curated/en/438751468753289185/pdf/WPS3400.pdf .)\n\nAn impact evaluation of a World Bank Credit Program on small and medium enterprises in Sri Lanka used propensity score matching for measuring program impact. (Source: Aivazian, V. A., and E. Santor. 2008. “Financial Constraints and Investment: Assessing the Impact of a World Bank Credit Program on Small and Medium Enterprises in Sri Lanka.” Canadian Journal of Economics 41 (2): 475–500.)\n\nA regression discontinuity design was used in the evaluation of Burkinabé Response to Improve Girls’ Chances to Succeed, a two-year program aimed at improving girls’ access to primary school. (Source: Levy, D., M. Sloan, L. Linden, and H. Kazianga. 2009. Impact Evaluation of Burkina Faso’s BRIGHT Program. Washington, DC: Mathematica Policy Research. https://eric.ed.gov/?id=ED507466 .)\n\nA quasi-experimental design with propensity score–matched comparison villages was used in an impact assessment of a value chain development of bay leaf in Nepal. (Source: Shah, G. M., A. K. Nepal, G. Rasul, and F. Ahmad. 2018. “Value Chain Development of Bay Leaf in Nepal: An Impact Assessment.” Journal of Development Effectiveness 10 (2): 179–96.)\n\nA propensity score matching method was used to adjust for baseline differences in a randomized controlled trial, comparing microfinance institution borrowers to those without any loans and those with loans from other sources. (Source: Inna, C., and I. Love. 2017. “Re-evaluating Microfinance—Evidence from a Propensity-Score Matching.” Policy Research Working Paper 8028, World Bank, Washington, DC. http://documents.worldbank.org/curated/en/707891492451043846/Re-evaluating-microfinance-evidence-from-propensity-score-matching .)\n\nReadings and Resources\n\nBackground\n\nGertler, P. J., S. Martinez, P. Premand, L. B. Rawlings, and C. M. J. Vermeersch. 2016. Impact Evaluation in Practice, 2nd ed. Washington, DC: World Bank. https://openknowledge.worldbank.org/handle/10986/25030 .\n\nKhandker, S. R., G. B. Koolwal, and H. A. Samad. 2009. Handbook on Quantitative Methods of Program Evaluation. Washington, DC: World Bank.\n\nShadish, W. R., T. D. Cook, and D. T. Campbell. 2002. Experimental and Quasi-Experimental Designs for Generalized Causal Inference. Boston, MA: Houghton Mifflin.\n\nWhite, H., and S. Sabarwal. 2014. “Quasi-Experimental Design and Methods.” Methodological Briefs: Impact Evaluation 8, UNICEF Office of Research, Florence. https://www.unicef-irc.org/publications/753-quasi-experimental-design-and-methods-methodological-briefs-impact-evaluation-no.html .\n\nAdvanced\n\nBuddelmeyer, Hielke, and Emmanuel Skoufias. 2004. “An Evaluation of the Performance of Regression Discontinuity Design on PROGRESA.” Policy Research Working Paper WPS 3386, World Bank, Washington, DC. http://documents.worldbank.org/curated/en/124781468773348613/An-evaluation-of-the-performance-of-regression-discontinuity-design-on-PROGRESA .\n\nCaliendo, M., and S. Kopeinig. 2008. “Some Practical Guidance for the Implementation of Propensity Score Matching.” Journal of Economic Surveys 22: 31–72. http://ftp.iza.org/dp1588.pdf .\n\nGao, Xingyuan, Jianping Shen, and Huilan Y. Krenn. 2017. “Using WWC Sanctioned Rigorous Methods to Develop Comparison Groups for Evaluation.” Evaluation and Program Planning 65: 148–55.\n\nGlewwe, Paul, and Petra Todd, eds. 2020. Impact Evaluation in Developing Countries: Theory, Methods, and Practice. Washington, DC: World Bank.\n\nHansen, H., N. Klejnstrup, and O. W. Andersen. 2013. “A Comparison of Model-Based and Design-Based Impact Evaluations in Developing Countries.” American Journal of Evaluation 34 (3): 320–38.\n\nMourelo, Elva López, and Verónica Escudero. 2017. “Effectiveness of Active Labor Market Tools in Conditional Cash Transfers Programs: Evidence for Argentina.” World Development 94: 422–47.\n\nWeitzman, B. C., D. Silver, and K-N. Dillman. 2002. “Integrating a Comparison Group Design into a Theory of Change Evaluation: The Case of the Urban Health Initiative.” American Journal of Evaluation 23 (4): 371–86.\n\nWing, Coady, and Ricardo A. Bello-Gomez. 2018. “Regression Discontinuity and Beyond: Options for Studying External Validity in an Internally Valid Design.” American Journal of Evaluation 39 (1): 91–108.\n\nOther Resources\n\nThe International Initiative for Impact Evaluation funds, produces, assures quality, synthesizes evidence of what policy interventions work in low- and middle-income countries, and advocates the generation and use of quality evidence in development decision-making. https://www.3ieimpact.org/; https://developmentevidence.3ieimpact.org/ .\n\nThe Maryland Scientific Methods Scale is a five-point scale ranking treatment-comparison designs according to their power to reduce selection bias. https://whatworksgrowth.org/resources/the-scientific-maryland-scale/ .\n\nThe World Bank’s Development Impact Evaluation group generates high-quality and operationally relevant data and research to transform development policy, help reduce extreme poverty, and secure shared prosperity. https://www.worldbank.org/en/research/dime; https://dimewiki.worldbank.org/wiki/Main_Page.\n\n4. Case Study Design\n\nBrief Description of the Approach\n\nThe case study approach is a focused, in-depth examination of one or more specific and clearly defined cases (individuals, programs, organizations, communities, or even countries). The purpose of case studies in evaluation is often to explore and better understand how a program was implemented and to identify causal processes and configurations generating program outcomes, including contextual factors conditioning these. Case studies are particularly adept at documenting program contextual conditions, substantiating how and in what way the program generated (or failed to generate) one or more intended outcomes, and even producing insights about whether and how the program might make a difference in other settings, times, and populations (see analytical generalization in appendix A, Glossary of Key Terms). Case study is an umbrella term comprising several different design subtypes, many of which are discussed in this guide: process tracing, qualitative comparative analysis, participatory approaches, and (some) complex systems approaches are all case-based approaches and usually applied to a handful of cases at most.\n\nMany types of evaluation questions can be answered by case studies, including the following:\n\nHow was the program implemented?\n\nHow and in what way did the program generate the observed effect?\n\nWill the program make a difference in other settings, times, and populations?\n\nA case study design in principle can be used for any type of evaluation question and is not restricted to causal questions.\n\nThe Main Variations of the Approach\n\nCase studies can be designed and implemented in many different ways. Some case study designs center on a single case; others include multiple cases. Case studies can be implemented at a single point in time or repeated over time (for example, before, during, and after program implementation). In terms of data collection, case studies may involve and be greatly improved by a broad range of qualitative and quantitative methods, including combinations of these.\n\nCase studies may be further distinguished in purpose as illustrative (to describe a typical or abnormal case), theory-generating (to explore and generate hypotheses), theory testing (to test and revise hypotheses), and cumulative (to compare and synthesize multiple cases).\n\nCase study analyses may include within-case analysis, cross-case analysis, or some combination of these (see guidance notes 5, Process Tracing, and 6, Qualitative Comparative Analysis, for examples). There are many ways that case study data can be examined. These include strategies for identifying similarities and differences across cases; multiple data display tables for partitioning and grouping data in various ways, sometimes based on features of the included cases or time-ordered displays; and coding techniques for further qualitative or even statistical analyses.\n\nThe Main Procedural Steps of the Approach\n\nA case study typically involves the following five steps:\n\nIdentifying and defining the type of case(s) to be examined (this is also referred to as casing);\n\nIdentifying the conditions or factors that will be studied in more depth in these case(s);\n\nDeveloping a case selection strategy;\n\nCollecting data on the selected case(s); and\n\nAnalyzing the data using within-case or cross-case analytical techniques, or some combination of these.\n\nA central step in case study design, and one that in practice is all too often treated less carefully than it deserves, is the selection of the case or the cases to be included (step iii above). The purpose of the case study (and the type of information to be produced by it) should inform the selection of relevant cases. To illustrate, if the aim of the case study is to gauge the breadth of and variation among local program implementation processes, the case selection should focus on the program implementation processes considered most different on a set of relevant characteristics (for example, urban versus rural, small- versus large-scale cases). Conversely, if the aim of the case study is to better understand high-performing programs, a better case selection strategy could be to focus on these programs (as defined by one or more program goals). Finally, random selection is often inappropriate for case study selection, partly because the number of cases tends to be too low for randomization to balance out systematic differences.\n\nThe Advantages and Disadvantages of the Approach\n\nThere are several important advantages to case studies. Emphasizing in-depth analysis, case studies can identify and examine causal processes underlying programs (also known as mechanisms) and the context within which these processes are embedded (see guidance note 5, Process Tracing). In this way, case studies may generate hypotheses about these underlying processes, examine these in more detail within a single case or across multiple cases, and even identify the specific contextual conditions on which these processes are contingent.\n\nAnother notable strength of case studies, particularly multiple–case study designs, is the opportunity to generalize findings beyond the programs studied (that is, these cases) to other programs that are similar on one or more salient characteristics (see analytical generalization in appendix A, Glossary of Key Terms).\n\nThere are also limitations. Commonly cited limitations include the absence of clear procedural guidelines for different variants of cases studies, the vulnerability of case studies to evaluator subjectivity (in part from lack of procedural formalization of case studies), the limited generalizability (of single–case study designs—see chapter 2 for a discussion of the trade-offs between breadth and depth), and the practical challenge of time and effort needed to adequately carry out case studies (especially for mixed data collection methods across multiple cases). Finally, the case study design is also vulnerable to the analyst “cherry-picking” specific cases to find support for preestablished ideas about the program or to present the most dramatic cases (the poorest family, the most successful entrepreneur) rather than to present a broader range of cases.\n\nThe Applicability of the Approach\n\nThe case study design can be applied in most settings and contexts. As such, case studies are highly applicable in development evaluation in general and IEO evaluations specifically. Case studies may also complement other approaches and designs that tend to focus less on contextual conditions and how these interact with the program (for example, experimental or quasi-experimental designs).\n\nCase studies are widely used in development evaluation; examples include the following:\n\nCase studies and other approaches were used in an evaluation of citizen engagement mainstreaming in World Bank projects. (Source: World Bank. 2018. Engaging Citizens for Better Development Results. Independent Evaluation Group. Washington, DC: World Bank. https://ieg.worldbankgroup.org/evaluations/engaging-citizens-better-development-results .)\n\nMultiple case studies, anchored in and framed by a program theory, were used to enhance the generalizability of findings from an evaluation of the Africa Routine Immunization program. This is a good example of theory-informed case selection and use of within- and across-case analyses. (Source: Mookherji, S., and A. LaFond. 2013. “Strategies to Maximize Generalization from Multiple Case Studies: Lessons from the Africa Routine Immunization System Essentials (ARISE) Project.” Evaluation 19 (3): 284–303.)\n\nSix individual cases studies of cities (Bucaramanga, Colombia; Coimbatore, India; Kigali, Rwanda; Gaziantep, Turkey; Changsha, China; and Tangier, Morocco) were compared with each other to identify institutions and strategies that successful cities have relied on to spur economic development. (Source: Kulenovic, Z. J., and A. Cech. 2015. “Six Case Studies of Economically Successful Cities: Competitive Cities for Jobs and Growth.” Companion Paper 3, Washington, DC, World Bank. https://openknowledge.worldbank.org/handle/10986/23573 .)\n\nA case study approach was used in a poverty and social impact evaluation of Tanzania’s crop boards reform.(Source: Beddies, S., M. Correia, S. Kolavalli, and R. Townsend. 2006. “Tanzania Crop Boards Reform.” In Poverty and Social Impact Analysis of Reforms: Lessons and Examples from Implementation, edited by A. Coudouel, A. Dani, and S. Paternostro, 491–520. Washington, DC: World Bank. http://regulationbodyofknowledge.org/wp-content/uploads/2014/09/Coudouel_Poverty_and_Social.pdf .)\n\nA case study approach was used to assess the impact of a government decision to close down subsidized wheat flour ration shops intended to provide wheat flour to low-income groups in Pakistan. (Source: World Bank 2005. Influential Evaluations: Detailed Case Studies. Washington, DC: World Bank. http://documents.worldbank.org/curated/en/928001468330038416/pdf/328800Influent1luation1case1studies.pdf .)\n\nCase studies were used in the evaluation of the Children of Uruzgan Program in Afghanistan. The in-depth examination of the program development generated insights to inform program planning, decision-making, and scale-up.(Source: Save the Children Australia. 2012. Access Restricted: A Review of Remote Monitoring Practices in Uruzgan Province. East Melbourne, Victoria: Save the Children Australia. https://resourcecentre.savethechildren.net/node/8291/pdf/access-restricted-save-the-children.pdf .)\n\nA case study design was used to conduct a preliminary evaluation of the potential costs and benefits of rehabilitation of the Nakivubo wetland, Kampala, Uganda. (Source: Turpie, J., L. Day, D. Gelo Kutela, G. Letley, C. Roed, and K. Forsythe. 2016. Promoting Green Urban Development in Africa: Enhancing the Relationship between Urbanization, Environmental Assets and Ecosystem Services. Washington, DC: World Bank. https://openknowledge.worldbank.org/handle/10986/26425 .)\n\nA multiple–case study design of 14 water and sanitation policy initiatives (across seven countries) was used to identify when, why, and how sanitation utilities can work together toward specific policy outcomes. (Source: Kis, A. L., and M. Salvetti. 2017. Case Study—Alföldvíz, Hungary. WSS GSG Utility Turnaround Series, World Bank, Washington, DC. https://openknowledge.worldbank.org/handle/10986/27982 .)\n\nReadings and Resources\n\nBackground\n\nBox-Steffensmeier, Janet M., Henry E. Brady, and David Collier, eds. 2008. “Qualitative Tools for Descriptive and Causal Inference.” In The Oxford Handbook of Political Methodology. Oxford: Oxford University Press.\n\nCollier, David. 1993. “The Comparative Method.” In Political Science: The State of Discipline II, edited by Ada W. Finifter, 105–19. Washington, DC: American Political Science Association. https://ssrn.com/abstract=1540884 .\n\nGeorge, Alexander L., and Andrew Bennett 2005. Case Studies and Theory Development in the Social Sciences. Cambridge, MA: MIT Press.\n\nGerring, John 2017. Case Study Research: Principles and Practices. Cambridge, UK: Cambridge University Press.\n\nRagin, Charles C., and Howard Saul Becker. 1992. What Is a Case? Exploring the Foundations of Social Inquiry. Cambridge, UK: Cambridge University Press\n\nStake, R. E. 2006. Multiple Case Study Analysis. New York: Guilford Press.\n\nYin, R. K. 2017. Case Study Research and Applications: Design and Methods, 6th ed. Thousand Oaks, CA: SAGE.\n\nAdvanced\n\nBrady, Henry E., and David Collier. 2010. Rethinking Social Inquiry: Diverse Tools, Shared Standards. Lanham, MD: Rowman & Littlefield.\n\nGerring, John. 2004. “What Is a Case Study and What Is It Good For?” The American Political Science Review 98 (2): 341–54.\n\nHadfield, Mark, and Michael Jopling. 2018. “Case Study as a Means of Evaluating the Impact of Early Years Leaders: Steps, Paths and Routes.” Evaluation and Program Planning 67: 167–76.\n\nKing, Gary, Robert O. Keohane, and Sidney Verba. 1994. Designing Social Inquiry: Scientific Inference in Qualitative Research. Princeton University Press.\n\nUSAID (US Agency for International Development). 2013. “Evaluative Case Studies.” Technical Note. November, USAID, Washington, DC. https://usaidlearninglab.org/sites/default/files/resource/files/case_study_tech_note_final_2013_1115.pdf .\n\nWoolcock, M. 2013. “Using Case Studies to Explore the External Validity of ‘Complex’ Development Interventions.” Evaluation 19 (3): 229–48.\n\n5. Process Tracing\n\nBrief Description of the Approach\n\nProcess tracing is a case-based approach for examining and describing the causal processes (also referred to as mechanisms) generating program outcomes. Its purpose is to identify and empirically test the causal mechanisms connecting specific program components and a set of desired outcomes within a single case. Examining these processes supports fine-grained explanations of how and why programs generate specific outcomes. What differentiates this approach from most other theory-based ones is its focus on the assessment of evidence strength—the importance or value of given data and empirical observations to support or weaken specific theories. In this sense, process tracing can be considered a data analysis technique, particularly in its explicitly Bayesian version (see The Main Variations of the Approach section, below). Process tracing is conventionally applied in within-case analysis of single–case study designs.\n\nEvaluation questions that process tracing may answer include the following:\n\nHow, under what circumstances, and why did the program generate the desired outcome(s)?\n\nWhich mechanism(s) generated the desired outcome(s)?\n\nAre there alternative explanations for the desired outcome(s)?\n\nThe Main Variations of the Approach\n\nTwo main variations of process tracing have been defined: a traditional, purely qualitative one and a qualitative-quantitative variant where confidence levels are formally defined and updated with the Bayes formula (known sometimes as contribution tracing, Bayesian process tracing, process tracing with Bayesian updating, and, more recently, diagnostic evaluation). A secondary distinction concerns whether the investigation is conducted primarily from an inductive (theory-building) or deductive (theory-testing) perspective.\n\nTheory-building process tracing aims to identify and describe causal processes through an empirical case. The core idea of the approach is simply to provide the best possible explanation of how the program outcomes came about within a specific local context.\n\nTheory-testing process tracing aims to test whether specified causal processes are supported by an empirical case. This approach may serve well to identify whether a specific (theoretically derived) mechanism is present in a case being studied within a specific local context.\n\nBayesian formalization can coexist with any process tracing variant and also any theory-based evaluation approach, hence the more general term diagnostic (theory-based) evaluation. Although conceptually distinct, in practice, theory testing and theory building are usually combined, and there is often a back-and-forth exchange between theory and data across multiple iterations. Conversely, the distinction between traditional process tracing and the Bayesian version has considerable practical implications because the latter requires explicit establishment of confidence levels for a series of events concerning the theory and the empirical data in relation to the theory. Recently, process tracing has also been applied in multiple–case study designs.\n\nThe Main Procedural Steps of the Approach\n\nThere are four major steps in process tracing or diagnostic evaluation:\n\nFormulating the hypothesized mechanisms for the outcome;\n\nSpecifying the observable data (in the form of patterns, timelines, traces, or accounts) that must be collected and analyzed to test for the presence of the mechanisms;\n\nCollecting relevant data; and\n\nAnalyzing the collected data, considering the relative weight of evidence for the presence or absence of each hypothesized mechanism. In the fourth step, the evidence for each hypothesized mechanism is assessed using four tests (the straw-in-the-wind test, the hoop test, the smoking gun test, and the doubly decisive test), each of which provides different types of evidence (strong or weak, strengthening or weakening) for the mechanisms (see more detailed descriptions of these tests in appendix A, Glossary of Key Terms). In the Bayesian variant, confidence levels are often formalized in the fourth step.\n\nAlthough process tracing is grounded in the qualitative tradition, the method is entirely compatible with and in many situations improved by the use of a broad range of both qualitative and quantitative data, including documents, media transcripts, interview or focus group transcripts, and findings from surveys. The type of data to be collected can sometimes appear unusual in comparison with other approaches, being akin to the type of evidence that may be produced in a court of law. The (formal or informal) Bayesian logic of the approach is indeed applied whenever the evidence must be rigorously evaluated to serve some probative function, and mysterious realities are to be understood or uncovered: in medical diagnosis, crime investigation, and courts of law.\n\nThe Advantages and Disadvantages of the Approach\n\nProcess tracing serves well to develop and test hypotheses about the underlying processes generating program outcomes. As such, the method is particularly relevant when the aim of the evaluation is to explain how and under what circumstances a program works (or fails to work). A key strength of process tracing is that (especially in its Bayesian variant) it relies on a rigorous testing process where the probative value of data for given theories is established in a structured, transparent, and replicable manner. It thus has an advantage over similar approaches, particularly when theories are broad, vague, and ambiguous: the painstaking assessment of empirical data that the approach requires forces the evaluator to increase the precision and specificity of claims. By explicitly focusing on the weight of evidence in favor of or against specific mechanisms, the approach enhances the credibility of the conclusions drawn.\n\nOne limitation of process tracing is the difficulty of deciding how deep, how wide, and how far back to trace the causal processes. This limitation holds practical implications because the depth and breadth of the hypotheses considered determines the amount of data to be collected and analyzed (for best practice recommendations on this challenge, see Bennett and Checkel [2015] in Readings and Resources). This challenge can be mitigated with Bayesian formalization and by establishing an explicit confidence level at which the analysis is considered complete. Another challenge relates to the single-case nature of the analysis, which limits the generalizability of the findings. Finally, a practical obstacle is that the approach’s thirst for “forensic proof” may be met with resistance from stakeholders; for example, documentation that would prove the existence of important parts of processes or mechanisms might be withheld for lack of trust or because sharing it is considered a breach of confidentiality.\n\nThe Applicability of the Approach\n\nAs a single–case study approach, the process tracing approach may be widely applicable in development evaluation in general and IEO evaluations specifically. Published applications of process tracing in development evaluation are few but growing:\n\nProcess tracing was used to understand how citizen engagement affected intended outcomes in selected World Bank projects. (Source: World Bank. 2018. Engaging Citizens for Better Development Results. Independent Evaluation Group. Washington, DC: World Bank. https://ieg.worldbankgroup.org/evaluations/engaging-citizens-better-development-results .)\n\nProcess tracing was used to examine the effects of local civil society–led gender-responsive budgeting on maternal health service delivery in Kabale district in rural Uganda.(Source: Bamanyaki, P. A., and N. Holvoet. 2016. “Integrating Theory-Based Evaluation and Process Tracing in the Evaluation of Civil Society Gender Budget Initiatives.” Evaluation 22 (1): 72–90.)\n\nProcess tracing was used to understand how the influence process unfolded in an evaluation of the policy impact of the Uganda Poverty Conservation and Learning Group. (Source: D’Errico, S., B. Befani, F. Booker, and A. Giuliani. 2017. Influencing Policy Change in Uganda: An Impact Evaluation of the Uganda Poverty and Conservation Learning Group’s Work. London: International Institute for Environment and Development. http://pubs.iied.org/G04157/ .)\n\nProcess tracing was used to evaluate the governance effectiveness of budget support interventions.(Source: Schmitt, J., and D. Beach. 2015. “The Contribution of Process Tracing to Theory-Based Evaluations of Complex Aid Instruments.” Evaluation 21 (4): 429–47.)\n\nProcess tracing was used to evaluate the policy impact of the Hunger and Nutrition Commitment Index.(Source: te Lintelo, D. J. H., T. Munslow, K. Pittore, and R. Lakshman. 2019. “Process Tracing the Policy Impact of ‘Indicators.’” European Journal of Development Research 32: 1338.)\n\nProcess tracing was used to assess a program’s contribution to reducing the number of minors working in the adult entertainment sector in Nepal. (Source: Progress, Inc. and The Freedom Fund. 2020. Evaluation of the Central Nepal Hotspot Project Using the Process Tracing Methodology—Summary Report. London and New York: The Freedom Fund. https://freedomfund.org/our-reports/evaluation-of-the-central-nepal-hotspot-project-using-the-process-tracing-methodology/ .)\n\nProcess tracing was used to understand the mechanisms (in addition to price reduction) through which the sugar-sweetened beverage tax worked in Barbados. (Source: Alvarado, M., T. Penney, and J. Adams. 2019. “OP113 Seeking Causal Explanations in Policy Evaluation: An Assessment of Applying Process Tracing to the Barbados Sugar-Sweetened Beverage Tax Evaluation.” Journal of Epidemiology and Community Health 73: A53–A54.)\n\nProcess tracing was used to test the so-called kaleidoscope model of policy change. A set of 16 operational hypotheses identified the conditions under which food security interventions emerged on the policy agenda and were implemented in Zambia.(Source: Resnick, Danielle, Steven Haggblade, Suresh Babu, Sheryl L. Hendriks, and David Mather. 2018. “The Kaleidoscope Model of Policy Change: Applications to Food Security Policy in Zambia.” World Development 109: 101–20.)\n\nReadings and Resources\n\nBackground\n\nBeach, D., and R. Pedersen. 2013. Process-Tracing Methods: Foundations and Guidelines. Ann Arbor, MI: University of Michigan Press.\n\nBefani, B., S. D’Errico, F. Booker, and A. Giuliani. 2016. “Clearing the Fog: New Tools for Improving the Credibility of Impact Claims.” IIED Briefing. London: International Institute for Environment and Development. http://pubs.iied.org/17359IIED/ .\n\nBennett, Andrew. 2010. “Process Tracing and Causal Inference.” In Rethinking Social Inquiry, edited by Henry Brady and David Collier, chapter 10. Lanham, MD: Rowman & Littlefield. http://philsci-archive.pitt.edu/8872/ .\n\nBennett, A., and J. T. Checkel. 2015. Process Tracing: From Metaphor to Analytical Tool. Cambridge, UK: Cambridge University Press.\n\nCheckel, Jeffrey T. 2006. “Tracing Causal Mechanisms.” International Studies Review 8 (2): 362–70.\n\nCollier, D. 2011. “Understanding Process Tracing.” PS: Political Science & Politics 44 (4): 823–30. https://polisci.berkeley.edu/sites/default/files/people/u3827/Understanding%20Process%20Tracing.pdf; https://www.cambridge.org/core/journals/ps-political-science-and-politics/article/understanding-process-tracing/183A057AD6A36783E678CB37440346D1/core-reader .\n\nKincaid, H., and D. Waldner. 2012. “Process Tracing and Causal Mechanisms.” In The Oxford Handbook of Philosophy of Social Science. Oxford: Oxford University Press. https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780195392753.001.0001/oxfordhb-9780195392753-e-4 .\n\nPalier, Bruno, and Christine Trampusch. 2016. “Process Tracing: The Understandings of Causal Mechanisms.” Special issue, New Political Economy 21 (5). https://www.tandfonline.com/toc/cnpe20/21/5 .\n\nRicks, J., and A. Liu. 2018. “Process-Tracing Research Designs: A Practical Guide.” PS: Political Science & Politics 51 (4): 842–46.\n\nVennesson, P. 2008. “Case Studies and Process Tracing: Theories and Practices.” In Approaches and Methodologies in the Social Sciences: A Pluralist Perspective, edited by D. Della Porta and M. Keating, 223–39. Cambridge: Cambridge University Press. https://minorthesis.files.wordpress.com/2012/12/vennesson-case-study-methods.pdf .\n\nAdvanced\n\nBefani, B. 2016. “Testing Contribution Claims with Bayesian Updating.” Evaluation Policy Practice Note 2.1 (Winter), CECAN, London. https://drive.google.com/file/d/0B-yPZxBK8WjPdEJNNUx5SG1VSFk/view .\n\nBefani, B. 2020. “Quality of Quality: A Diagnostic Approach to Qualitative Evaluation.” Evaluation.\n\nBefani, B. Forthcoming. “Diagnostic Evaluation and Bayesian Updating: Practical Solutions to Common Problems.” Evaluation.\n\nBefani, B., and J. Mayne. 2014. “Process Tracing and Contribution Analysis: A Combined Approach to Generative Causal Inference for Impact Evaluation.” IDS Bulletin XLV (6): 17–36.\n\nBefani, B., and G. Steadman-Bryce. 2017. “Process Tracing and Bayesian Updating for Impact Evaluation.” Evaluation 23 (1): 42–60.\n\nBennett, Andrew. 2008. “Process Tracing: A Bayesian Perspective.” In The Oxford Handbook of Political Methodology, edited by Janet M. Box-Steffensmeier, Henry E. Brady, and David Collier. Oxford: Oxford University Press.\n\nElman, Colin, and John M. Owen, eds. 2015. “Process Tracing: A Symposium.” Special issue, Security Studies 24 (2). https://www.tandfonline.com/toc/fsst20/24/2?nav=tocList .\n\nFairfield, Tasha, and Andrew Charman. 2015. Formal Bayesian Process Tracing: Guidelines, Opportunities, and Caveats. London: The London School of Economics and Political Science. http://eprints.lse.ac.uk/62368/ .\n\nFairfield, T., and A. Charman. 2017. “Explicit Bayesian Analysis for Process Tracing: Guidelines, Opportunities, and Caveats.” Political Analysis 25 (3): 363–80.\n\nTansey, O. 2007. “Process Tracing and Elite Interviewing: A Case for Non-probability Sampling.” PS: Political Science & Politics 40 (4): 765–72.\n\nOther Resources\n\nBefani, B. 2017. “Dr. Barbara Befani’s Bayes Formula Confidence Updater Spreadsheet.” Toolkits. CECAN, London. http://cecan.ac.uk/resources . This Excel-based template provides easy-to-use worksheets for updating confidence estimates when using Bayes updating.\n\n6. Qualitative Comparative Analysis\n\nBrief Description of the Approach\n\nQualitative comparative analysis (QCA) is a case-based analytical approach for identifying the causal conditions (for example, contexts or specific program components) that either individually or collectively generate a specific outcome. Its primary purpose is to identify and describe these causal conditions across a set of cases. This type of information is relevant to understanding and explaining how programs generate (or fail to generate) desired outcomes. What differentiates this approach from most other cross-case comparative methods is that it provides a specific set of algorithms to analyze data sets (usually in the form of a table). In this sense QCA can also be considered a data analysis technique. QCA is traditionally applied in cross-case analysis of multiple–case study designs.\n\nEvaluation questions that QCA may answer include the following:\n\nUnder what circumstances did the program generate or not generate the desired outcome?\n\nWhich program components or ingredients (individually or collectively) are necessary or sufficient for the program outcome?\n\nAre there alternative explanations for the desired outcome (or lack thereof)?\n\nThe Main Variations of the Approach\n\nIn broad terms, there are two main variations of QCA: crisp-set QCA and fuzzy-set QCA. The distinction between them concerns how the causal conditions and outcomes are coded in the cases being studied.\n\nIn the traditional crisp-set QCA, each causal condition (and the outcome of interest) is coded as either present (1) or absent (0) in the cases to be analyzed. To illustrate, the use of a specific curriculum may be either present or absent in a given after-school program.\n\nFuzzy-set QCA uses more nuanced coding. It allows for causal conditions to be present or absent by degree. For instance, a specific causal condition may be fully implemented (coded 1), almost fully implemented (0.9), barely implemented (0.1), or not at all implemented (0). In the example of the after-school program, the use of the curriculum across schools may likely vary by degree among the teachers, with some teachers using it often (full implementation) and others using it only occasionally (barely implemented).\n\nThe Main Procedural Steps of the Approach\n\nThe application of QCA consists of seven steps:\n\nIdentifying and defining the causal conditions and outcomes of interest;\n\nAssembling relevant data on each case included in the analysis (as informed by the causal conditions and outcomes of interest);\n\nCoding each case according to the presence or absence (dichotomously or by degree) of each causal condition and outcome;\n\nUsing QCA software to summarize all the different causal configurations present among the cases;\n\nUsing QCA software to simplify the identified configurations into the essential set of causal recipes eliciting a positive or negative outcome;\n\nExamining the consistency and empirical coverage of these recipes; and\n\nReexamining the individual cases represented by each of the identified causal recipes to better understand the nature of the latter.\n\nIn practice, the steps are often applied iteratively, with the evaluator testing different causal models, perhaps moving back and forth between examination of the causal recipes identified and refining the way the cases have been coded.\n\nNumerous software packages, some free of charge, are available for both crisp-set and fuzzy-set QCA (see Other Resources).\n\nThe Advantages and Disadvantages of the Approach\n\nBenefits of QCA include the ability to handle causal complexity (including conflicting cases), to identify different combinations of necessary and sufficient conditions associated with the same outcome, and to help in explaining how the outcome is generated across a small, medium, or large set of cases. From an internal validity perspective, a noteworthy strength of QCA is that the formalization of the logical comparisons (using Boolean algebra) provides for a systematic, transparent, and fully replicable analysis of qualitative data. Moreover, synthesizing medium or large data sets with QCA software allows for identification of general patterns in the data that would be impossible to capture manually. Finally, despite its generalization capabilities, QCA can be used with relatively small samples.\n\nOne possible limitation of QCA is that the method is difficult to use with a large number of causal conditions, especially when the set of available cases is small. Finding a good balance between consistency (the sufficiency of each pathway), coverage (the number of cases represented by the pathways), and parsimony or a manageable level of complexity of the findings usually requires that an experienced analyst work with the evaluation team on successive iterations and test increasingly simple models while maintaining high coverage and consistency. Theory often plays a key role in the initial selection, but confirmation bias is avoided because the results can reject those assumptions very strongly if the theory is not supported empirically. Therefore multiple iterations are needed until a set of pathways is found that reaches the optimal balance among consistency, coverage, and complexity.\n\nTwo additional, perhaps more practical, challenges relate to (i) the need for highly comparable data across all the cases to start the analysis; and (ii) the amount of work needed for full transparency on how the cases are coded (especially when fuzzy sets are used and when the coding has not been highly structured) and to systematically compare the information available for each condition across all cases. The coding must be systematically traceable to case-level information for the analysis to be fully replicable.\n\nThe Applicability of the Approach\n\nThe introduction of QCA in development evaluations is fairly recent. Grounded on multiple–case study design, the QCA approach is widely applicable in both development evaluation in general and IEO evaluations specifically (the only requirement being the availability of comparable information across all cases). Examples include the following:\n\nQCA was used to understand which factors contributed to a series of outcomes in carbon reduction interventions. (Source: World Bank. 2018. Carbon Markets for Greenhouse Gas Emission Reduction in a Warming World. Independent Evaluation Group. Washington, DC: World Bank. https://ieg.worldbankgroup.org/evaluations/carbon-finance.)\n\nQCA was used in the impact evaluation of the Global Environment Facility / United Nations Development Programme Biodiversity, Protected Areas, and Protected Area Systems program. (Source: Befani, B. 2016. Pathways to Change: Evaluating Development Interventions with Qualitative Comparative Analysis (QCA). Report 05/16, EBA, Stockholm. http://eba.se/wp-content/uploads/2016/07/QCA_BarbaraBefani-201605.pdf .)\n\nQCA was applied in an evaluation of the effectiveness of gender-sensitive budget support in education. (Source: Holvoet, N., and L. Inberg. 2013. “Multiple Pathways to Gender-Sensitive Budget Support in the Education Sector.” Working Paper 105, United Nations University World Institute for Development Economics Research, Helsinki.)\n\nQCA was applied to better understand how and why Omar’s Dream—a program that aims to end child labor—worked. (Source: Millard, A., A. Basu, K. Forss, B. Kandyomunda, C. McEvoy, and A. Woldeyohannes. 2015. Is the End of Child Labour in Sight? A Critical Review of a Vision and Journey. Geneva: International Cocoa Initiative. https://cocoainitiative.org/wp-content/uploads/2017/10/8_HIVOS.pdf .)\n\nQCA was applied to identify factors affecting success in rendering water services sustainable. (Source: Welle, K., J. Williams, J. Pearce, and B. Befani. 2015. Testing the Waters: A Qualitative Comparative Analysis of the Factors Affecting Success in Rendering Water Services Sustainable Based on ICT Reporting. Brighton, UK: Institute of Development Studies and WaterAid. http://itad.com/wp-content/uploads/2015/09/MAVC_WaterAid_FINAL-report.pdf .)\n\nQCA was applied in a macro evaluation of 50 UK Department for International Development social accountability projects to better understand what works, for whom, in what contexts, and why.(Source: Holland, J., F. Schatz, B. Befani, and C. Hughes. 2016. Macro Evaluation of DFID’s Policy Frame for Empowerment and Accountability. Brighton, UK, and Washington, DC: ITAD. https://itad.com/wp-content/uploads/2017/06/EA-Macro-Evaluation-Technical-report-Dec16-FINAL.pdf .)\n\nQCA was applied in an evaluation of a development cooperation program.(Source: Pattyn, V., A. Molenveld, and B. Befani. 2019. “Qualitative Comparative Analysis as an Evaluation Tool: Lessons from an Application in Development Cooperation.” American Journal of Evaluation 40 (1): 55–74.)\n\nQCA was used to learn lessons from community forest management, comparing and synthesizing ten such cases from Africa, Asia, and Latin America. (Source: Arts, Bas, and Jessica de Koning. 2017. “Community Forest Management: An Assessment and Explanation of Its Performance through QCA.” World Development 96: 315–25.)\n\nQCA was used to synthesize fragmented studies, accumulate knowledge, and develop theory in water resource management.(Source: Mollinga, P., and D. Gondhalekar. 2014. “Finding Structure in Diversity: A Stepwise Small-N/Medium-N Qualitative Comparative Analysis Approach for Water Resources Management Research.” Water Alternatives 7 (1): 178–98. https://core.ac.uk/download/pdf/42549267.pdf .)\n\nQCA has been recommended as a useful tool to understand the complex two-way relationship between migration and development.(Source: Czaika, Mathias, and Marie Godin. 2019. “Qualitative Comparative Analysis for Migration and Development Research.” MIGNEX Background Paper. Oslo: Peace Research Institute Oslo www.mignex.org/d022 .)\n\nQCA was used to understand the reasons behind development research uptake.(Source: Scholz, Vera, Amy Kirbyshire, and Nigel Simister. 2016. “Shedding Light on Causal Recipes for Development Research Uptake: Applying Qualitative Comparative Analysis to Understand Reasons for Research Uptake.” London: INTRAC and CKDN. https://www.intrac.org/resources/shedding-light-causal-recipes-development-research-uptake-applying-qualitative-comparative-analysis-understand-reasons-research-uptake/ .)\n\nQCA was used to understand why programs aimed at monitoring water quality succeed or fail.(Source: Peletz, Rachel, Joyce Kisiangani, Mateyo Bonham, Patrick Ronoh, Caroline Delaire, Emily Kumpel, Sara Marks, and Ranjiv Khush. 2018. “Why Do Water Quality Monitoring Programs Succeed or Fail? A Qualitative Comparative Analysis of Regulated Testing Systems in Sub-Saharan Africa.” International Journal of Hygiene and Environmental Health 221 (6): 907–20.)\n\nReadings and Resources\n\nBackground\n\nBaptist, C., and B. Befani. 2015. “Qualitative Comparative Analysis—A Rigorous Qualitative Method for Assessing Impact.” Coffey How To Paper. Better Evaluation, Melbourne, Victoria. https://www.betterevaluation.org/sites/default/files/Qualitative-Comparative-Analysis-June-2015%20(1) .\n\nBefani, B. 2016. Pathways to Change: Evaluating Development Interventions with Qualitative Comparative Analysis (QCA). Report 05/16, EBA, Stockholm. http://eba.se/wp-content/uploads/2016/07/QCA_BarbaraBefani-201605.pdf .\n\nByrne, D. 2016. “Qualitative Comparative Analysis: A Pragmatic Method for Evaluating Intervention.” Evaluation and Policy Practice Note 1 (Autumn), CECAN, London. https://www.cecan.ac.uk/sites/default/files/2018-01/DAVE%20B%20PPN%20v2.1.pdf .\n\nKahwati, L. C., and H. L. Kane. 2018. Qualitative Comparative Analysis in Mixed Methods Research and Evaluation. Thousand Oaks, CA: SAGE.\n\nRagin, Charles. 2014. The Comparative Method: Moving beyond Qualitative and Quantitative Strategies. Berkeley: University of California Press.\n\nRihoux, Benoît, and Bojana Lobe. 2009. “The Case for Qualitative Comparative Analysis (QCA): Adding Leverage for Thick Cross-Case Comparison.” In The SAGE Handbook of Case-Based Methods, edited by D. S. Byrne and Charles C. Ragin, 222–42. Thousand Oaks, CA: SAGE.\n\nRihoux, Benoît, and Charles Ragin, eds. 2008. Configurational Comparative Methods: Qualitative Comparative Analysis (QCA) and Related Techniques. Thousand Oaks, CA: SAGE.\n\nSchneider, C. Q., and C. Wagemann. 2012. Set-Theoretic Methods for the Social Sciences—A Guide to Qualitative Comparative Analysis. New York: Cambridge University Press.\n\nAdvanced\n\nBefani, B., S. Ledermann, and F. Sager. 2007. “Realistic Evaluation and QCA: Conceptual Parallels and an Empirical Application.” Evaluation 13 (2): 171–92.\n\nBlackman, T., J. Wistow, and D. Byrne. 2013. “Using Qualitative Comparative Analysis to Understand Complex Policy Problems.” Evaluation 19 (2): 126–40.\n\nDuşa, Adrian. 2019. QCA with R: A Comprehensive Resource. Cham, Switzerland: Springer.\n\nGrofman, B., and C. Q. Schneider. 2009. “An Introduction to Crisp Set QCA, with a Comparison to Binary Logistic Regression.” Political Research Quarterly 62 (4): 662–72.\n\nHudson, John, and Stefan Kühner, eds. 2013. “Innovative Methods for Policy Analysis: QCA and Fuzzy Sets.” Special issue, Policy and Society 32 (4).\n\nMarx, A., and A. Duşa. 2011. “Crisp-Set Qualitative Comparative Analysis (csQCA), Contradictions and Consistency Benchmarks for Model Specification.” Methodological Innovations Online 6 (2): 103–48.\n\nRagin, C. C., D. Shulman, A. Weinberg, and B. Gran. 2003. “Complexity, Generality, and Qualitative Comparative Analysis.” Field Methods 15 (4): 323–40.\n\nSager, F., and C. Andereggen. 2012. “Dealing with Complex Causality in Realist Synthesis: The Promise of Qualitative Comparative Analysis.” American Journal of Evaluation 33 (1): 60–78.\n\nThomann, E., and M. Maggetti. 2020. “Designing Research with Qualitative Comparative Analysis (QCA): Approaches, Challenges, and Tools.” Sociological Methods & Research 49 (2): 356–86.\n\nThomas, James, Alison O’Mara-Eves, and Ginny Brunton. 2014. “Using Qualitative Comparative Analysis (QCA) in Systematic Reviews of Complex Interventions: A Worked Example.” Systematic Reviews 3, article 67.\n\nVerweij, S., and L. M. Gerrits. 2013. “Understanding and Researching Complexity with Qualitative Comparative Analysis: Evaluating Transportation Infrastructure Projects.” Evaluation 19 (1): 40–55.\n\nOther Resources\n\nCOMPASSS (COMPArative Methods for Systematic cross-caSe analySis) is a worldwide network bringing together scholars and practitioners interested in the further development and application of configurational comparative and set-theoretical methods (crisp-set QCA, multivalue QCA, fuzzy-set QCA, and linked methods and techniques). www.compasss.org .\n\nRagin, C. C., K. A. Drass, and S. Davey. 2006. “Fuzzy-Set/Qualitative Comparative Analysis 2.0.” Department of Sociology, University of Arizona, Tucson. http://www.u.arizona.edu/~cragin/fsQCA/software.shtml . This free online software facilitates crisp- and fuzzy-set QCA, including development of truth tables and complex, intermediate, and parsimonious solutions. A user manual supports the application of the workbooks.\n\n7. Participatory Evaluation\n\nBrief Description of the Approach\n\nParticipatory approaches emphasize stakeholder involvement (program staff and beneficiaries, among others) in all or most of the design, implementation, and reporting stages of an evaluation. Participatory evaluation is often motivated by the idea that actively involving stakeholders (including those affected by the program) in the evaluation process gives them a voice in how the evaluation is designed and implemented, promotes a sense of ownership and empowerment, and enhances the potential relevance and use of the evaluation. Participatory approaches tend to be more relevant when the primary purpose is to provide information for program improvement or organizational development and not necessarily to make definitive statements about program outcomes.\n\nEvaluation questions that participatory approaches may answer include the following:\n\nWhat are the primary goals or outcomes of the program from the perspective of different stakeholders?\n\nWhat program services are needed? How are they best delivered?\n\nHow was the program implemented?\n\nThe Main Variations of the Approach\n\nParticipatory approaches can be applied in combination with any other evaluation approach or method. Its many variations reach far beyond the scope of this guidance note. A distinction has been made between pragmatic and transformative participatory approaches. Pragmatic approaches are motivated by the practical benefits of including stakeholders (including increased use of findings), and transformative approaches aim to change specific conditions for the stakeholders. One of the most widely used participatory approaches is utilization-focused evaluation Readings and Resources for more detailed information on this approach).\n\nThe Main Procedural Steps of the Approach\n\nGiven the diversity of approaches and methods, a common set of procedural steps cannot be identified for participatory approaches to evaluation. However, a central step in all participatory approaches is carefully considering and defining the stakeholders to be included (for example, those who benefit from the program, those who influence the implementation of the program, those who oppose the program), the scope and nature of their involvement, and their concrete role and responsibilities in the evaluation. The selection of which stakeholders to include (and, in effect, which not to include) is a core step in any participatory approach, and it holds significant methodological and ethical implications.\n\nThe Advantages and Disadvantages of the Approach\n\nSome potential benefits of participatory approaches include enhancing the cultural responsiveness and relevance of the evaluation, building capacity and empowering local stakeholders, and enhancing the authenticity and accuracy of the data and findings. To realize these benefits, though, careful consideration should be given to the stakeholders to be included (whose voices are included?); their intended role and purpose in the evaluation (are they identifying relevant questions, or are they consulted in the collection and analysis of data?); the extent of their expected participation in the evaluation (what is the breadth and depth of the engagement?); how they will be involved in the different stages of the evaluation (what is the format of and process for their involvement?); the ability and capacity of the stakeholders to actively engage and participate (is skill development or training called for?); and the value of participation for the stakeholders.\n\nCommon concerns for participatory evaluation include the potential burden and cost of participation among the stakeholders (especially when these outweigh the associated benefits) and the difficulty of engaging stakeholders without reinforcing existing power hierarchies (participation might be biased toward specific groups of stakeholders). In multilevel, multisite evaluations, stakeholder participation must be weighed carefully against the costs and the potential benefits of including stakeholders from across the (complex) program.\n\nThe Applicability of the Approach\n\nParticipatory approaches are highly applicable in development evaluation in general and for certain types of evaluation. IEOs generally would not have direct involvement with the intervention or its stakeholders. In practice, this means that several variations of participatory evaluation are not applicable. Nonetheless, creative applications of participatory approaches that safeguard independence and the efficient incorporation of stakeholder inputs into IEO evaluations are encouraged in most IEOs today. The implementation of participatory approaches requires in-depth knowledge of the context of the stakeholders (especially in relation to the program) and facilitation skills to manage stakeholder interactions (see also guidance note 13, Focus Group).\n\nThe practical applications of participatory approaches cover a broad range of programs and sectors:\n\nParticipatory assessment was used as part of an evaluation of the effectiveness of a water and sanitation program in Flores, Indonesia. Marginalized groups, women, and the poor were included through local gender-balanced teams of evaluators. (Source: Sijbesma, Christine, Kumala Sari, Nina Shatifan, Ruth Walujan, Ishani Mukherjee, and Richard Hopkins. 2002. Flores Revisited: Sustainability, Hygiene and Use of Community-Managed Water Supply and Sanitation and the Relationships with Project Approaches and Rules. Delft: IRC International Water and Sanitation Centre; Jakarta: WSP-EAP. https://www.ircwash.org/sites/default/files/Sijbesma-2002-Flores.pdf .)\n\nParticipatory assessment approaches have been used by the World Bank to assess beneficiaries’ experiences and perceived benefits of agricultural programs in countries such as Ghana, Guinea, Mali, and Uganda. (Source: Salmen, L. F. 1999. “The Voice of the Farmer in Agricultural Extension. A Review of Beneficiary Assessment of Agricultural Extension and an Inquiry into Their Potential as a Management Tool.” AKIS Discussion Paper, World Bank, Washington, DC. http://documents.worldbank.org/curated/en/776431468322742990/pdf/multi0page.pdf .)\n\nIn the Uganda Participatory Poverty Assessment Process, local people were consulted in 36 rural and urban sites in nine districts in Uganda. In this assessment, “voices” and perspectives of the poor were brought to the fore to influence district and national planning, implementation, and monitoring. (Source: Uganda Participatory Poverty Assessment Process. 2002. “Deepening the Understanding of Poverty—Second Participatory Poverty Assessment.” Kampala: Uganda Ministry of Finance, Planning and Economic Development. http://www.participatorymethods.org/sites/participatorymethods.org/files/deepning%20the%20understanding%20of%20poverty.pdf .)\n\nA participatory ethnographic evaluation and research approach was used in Cambodia and Myanmar to help inform program design by gaining an in-depth understanding of the sexual partners and clients of informal sex workers. (Source: World Bank. 2007. Tools for Institutional, Political, and Social Analysis of Policy Reform. A Sourcebook for Development Practitioners. Washington, DC: World Bank. https://siteresources.worldbank.org/EXTTOPPSISOU/Resources/1424002-1185304794278/TIPs_Sourcebook_English.pdf .)\n\nParticipatory wealth ranking was applied to almost 10,000 households to assess the number of poor households and their level of poverty in rural South Africa. Local perceptions of poverty were used to generate a wealth index of asset indicators. (Source: Hargreaves, J. R., L. A. Morison, J. S. S. Gear, M. B. Makhubele, J. Porter, J. Buzsa, C. Watts, J. C. Kim, and P. M. Pronk. 2007. “‘Hearing the Voices of the Poor’: Assigning Poverty Lines on the Basis of Local Perceptions of Poverty: A Quantitative Analysis of Qualitative Data from Participatory Wealth Ranking in Rural South Africa.” World Development 35 (2): 212–19.)\n\nA participatory approach was used in the development and application of a self-assessment of household livelihood viability index in Ethiopia. Qualitative case studies of livelihoods in select villages and group discussions with people from these villages were used in the development of the self-assessment tool. (Source: Chambers, R. 2007. “Who Counts? The Quiet Revolution of Participation and Numbers.” Working Paper 296, IDS, Brighton. https://opendocs.ids.ac.uk/opendocs/handle/20.500.12413/398 .)\n\nReadings and Resources\n\nBackground\n\nBlackburn, James, and Jeremy Holland, eds. 1998. “Foreword.” In Who Changes? Institutionalizing Participation in Development. London: Intermediate Technology Publications. https://opendocs.ids.ac.uk/opendocs/bitstream/handle/20.500.12413/685/rc77.pdf .\n\nCousins, J. B., and E. Whitmore. 1998. “Framing Participatory Evaluation.” New Directions for Evaluation 80: 5–23.\n\nEstrella, M., J. Blauert, J. Gonsalves, D. Campilan, J. Gaventa, I. Guijt, D. Johnson, and R. Ricafort, eds. 2000. Learning from Change: Issues and Experiences in Participatory Monitoring and Evaluation. London: Intermediate Technology Publications and Interna"
    }
}
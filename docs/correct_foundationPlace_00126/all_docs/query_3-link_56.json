{
    "id": "correct_foundationPlace_00126_3",
    "rank": 56,
    "data": {
        "url": "https://impetustech.medium.com/6-best-practices-for-building-a-sustainable-cloud-data-lake-f18ae01c771d",
        "read_more_link": "",
        "language": "en",
        "title": "6 Best Practices for Building a Sustainable Cloud Data Lake",
        "top_image": "https://miro.medium.com/v2/resize:fit:1200/1*VgTpYhddXIeR1DzovYbRwA.png",
        "meta_img": "https://miro.medium.com/v2/resize:fit:1200/1*VgTpYhddXIeR1DzovYbRwA.png",
        "images": [
            "https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png",
            "https://miro.medium.com/v2/resize:fill:88:88/2*FiOVIhMKp_JzVIOpof-UtQ.jpeg",
            "https://miro.medium.com/v2/resize:fill:144:144/2*FiOVIhMKp_JzVIOpof-UtQ.jpeg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Impetus Technologies",
            "impetustech.medium.com"
        ],
        "publish_date": "2020-04-24T12:17:06.295000+00:00",
        "summary": "",
        "meta_description": "Enterprises have huge amounts of unstructured data, which, if mined, can provide valuable insights. To tap the potential of these data, enterprises are switching from traditional warehousing to…",
        "meta_lang": "en",
        "meta_favicon": "https://miro.medium.com/v2/1*m-R_BkNf1Qjr1YbyOIJY2w.png",
        "meta_site_name": "Medium",
        "canonical_link": "https://impetustech.medium.com/6-best-practices-for-building-a-sustainable-cloud-data-lake-f18ae01c771d",
        "text": "By Sulagna Ganguly\n\nEnterprises have huge amounts of unstructured data, which, if mined, can provide valuable insights. To tap the potential of these data, enterprises are switching from traditional warehousing to adaptive data lakes. The shift comes with changing needs and practices for how that storage should best be constructed. From our experience, we have identified six core components of a reliable, secure, sustainable, and flexible cloud data lake.\n\n1. Start with an enterprise foundation\n\nA cloud data lake is designed to support multiple data types to empower your analytics and business intelligence units across the organization. Use cases differ across organizations and with time. To support the growth and potential for a data lake, companies must start with a robust foundation. Before collection and management, it is beneficial to review security needs and challenges, develop appropriate reusable templates, and put access and financial controls into place.\n\nWhile there are standard data warehousing best practices, enterprises need to have a holistic view to leverage the data lake. For example, budgeting and ROI need to consider all units that might benefit from the data, goal alignment should be cross-organization, and overall design should be built to add more users and data over time.\n\n2. Understand compliance ahead of implementation\n\nWhile much of the tech world focuses on failing fast in early efforts, a robust enterprise data lake needs a more stable and secure foundation to get to the fail-fast stage. So, understanding the compliance needs and requirements you face ahead of data collection and use can significantly speed up your ability to offer products and support using the data lake. Newer regulations, such as the California Consumer Privacy Act (CCPA), are changing how data is to be stored, used, and managed — including when consumers can tell you to remove it all.\n\nLooking at such regulations and determining how to comply with not only specific requirements but the spirit of the law can help you construct a Hadoop data lake that is malleable enough to remove data or make changes in access without broader data loss or harm.\n\nSustainability is mostly a question of flexibility. Architecture that is adaptive and created to maintain control through change — especially regulatory change — will help you build an enterprise data lake that is sustainable and fruitful.\n\n3. Integrate DevOps clearly\n\nDevOps is a core component of keeping your data lake healthy. Protect your investment by putting together clear guidelines for data collection, management, and access. Then, put practices into place to check and ensure your guidance is always followed.\n\nPreventative measures, such as establishing trustworthy sources or limiting the collection of low-value information that comes with higher regulatory burdens, will protect you across applications.\n\nDevOps is a cultural focus that typically helps improve the delivery pipeline of products and services, making it easier to compete. For your data lake, it can be more inward-facing and have the feedback loop focus on security and access, speeding up your time to adopt new applications of data while protecting your lake.\n\n4. Plan for expansion\n\nUltimately, a cloud data lake should grow together with your business, not limiting your capabilities or coming up against data it can’t access and integrate. To achieve scalability, your data lake must be engineered for expansion.\n\nNot only do you want standard integration capabilities but look for workable APIs built on industry standards and create a framework for ensuring compliance capabilities. The goal is to be able to adjust or make minor changes to your enterprise data lake to be compliant as soon as you’re ready to expand to a new country or industry.\n\n5. Keep an eye on cost\n\nThe enterprise focus for your foundation and early development must translate into a cost-conscious approach for the rest of usage for a Hadoop data lake. Go beyond tracking the hard and soft costs of your lake and look for ways that you can leverage a higher ROI in its development or application.\n\nFor instance, you can use existing systems like the Impetus Data Lake to generate better time-to-value by putting all capabilities — metadata management, governance, ETL, quality assurance checks, audits, and business intelligence generation — together. Sometimes, it’s as simple as ensuring you have plug-and-play support for new data feeds.\n\nWorking in a single platform and solution can help you control costs and generate a positive return more quickly, while also maintaining the security and tooling you need.\n\n6. Choosing the right tools\n\nTool selection is one of the most critical choices you’ll make. Many customers have an affinity for traditional tools, which may not work on the cloud. We recommend an in-depth review of capabilities and gaps to protect your efforts. This will also help you avoid the trap of becoming too excited about native cloud services, many of which have their gaps or might not be a strong fit for a transition.\n\nTake time to assess your requirements and bring in the right tools. Consider your available off-the-shelf options as well as what can be built custom.\n\nReview the criteria that they have for functionality as well as long-term business impacts. Avoid lock-ins when possible if there’s not a clear roadmap to support you. Ask partners about how data is shared and accessible or where you might have leverage with its use. Push for access and interoperability and be wary of any proprietary data formats."
    }
}
{
    "id": "dbpedia_7750_2",
    "rank": 40,
    "data": {
        "url": "https://community.jmp.com/t5/Discovery-Summit-Americas-2022/Different-goals-different-models-How-to-use-models-to-sharpen-up/ta-p/505854",
        "read_more_link": "",
        "language": "en",
        "title": "Different goals, different models: How to use models to sharpen up your questio...",
        "top_image": "https://community.jmp.com/html/assets/DiscoverySummit/Breakout-ds-logo-og.png",
        "meta_img": "https://community.jmp.com/html/assets/DiscoverySummit/Breakout-ds-logo-og.png",
        "images": [
            "https://community.jmp.com/t5/image/serverpage/image-id/40552i8DD198187049224A/image-dimensions/40x40/image-coordinates/210%2C25%2C1308%2C1123/constrain-image/false?v=v2"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "community.jmp.com",
            "user-id"
        ],
        "publish_date": "2022-06-10T20:36:13.023000+00:00",
        "summary": "",
        "meta_description": "Ron Kenett, Professor, KPA, Samuel Neaman Institute, Technion, Israel and University of Turin, Italy Christopher Gotwalt, Chief Data Scientists, JMP",
        "meta_lang": "en",
        "meta_favicon": "https://community.jmp.com/html/@DA3F291D67E4D58306F6C237D24021FF/assets/favicon.svg",
        "meta_site_name": "JMP User Community",
        "canonical_link": "https://community.jmp.com/t5/Discovery-Summit-Americas-2022/Different-goals-different-models-How-to-use-models-to-sharpen-up/ta-p/505854",
        "text": "Hello, I'm Ron Kennett.\n\nThis is a joint talk with Chris Gotwalt.\n\nWe're going to talk to you about models.\n\nModels are used extensively.\n\nWe hope to bring some additional perspective\n\non how to use models in general.\n\nWe call the talk\n\n\"Different goals, different models:\n\nHow to use models to sharpen your questions.\"\n\nMy part will be an intro, and I'll give an example.\n\nYou'll have access to the data I'm using.\n\nThen Chris will continue with a more complex example,\n\nintroduce the SHAP values available in JMP 17,\n\nand provide some conclusions.\n\nWe all know that all models are wrong,\n\nbut some are useful.\n\nSam Karlin said something different.\n\nHe said that the purpose of models is not to fit the data,\n\nbut to sharpen the question.\n\nThen this guy, Pablo Picasso, he said something in...\n\nHe died in 1973, so you can imagine when this was said.\n\nI think in the early '70 s.\n\n\"Computers are useless. They can only give you answers.\"\n\nHe is more in line with Karlin.\n\nMy take on this is that\n\nthis presents the key difference between a model and a computer program.\n\nI'm looking at the model from a statistician's perspective.\n\nDealing with Box's famous statement.\n\n\"Yes, some are useful. Which ones?\"\n\nPlease help us.\n\nWhat do you mean by some are useful?\n\nIt's not very useful to say that .\n\nGoing to Karlin, \"Sharpen the question.\"\n\nOkay, that's a good idea.\n\nHow do we do that?\n\nThe point is that Box seems focused on the data analysis phase\n\nin the life cycle view of statistics, which starts with problem elicitation,\n\nmoves to goal formulation, data collection, data analysis,\n\nfindings, operationalization of finding,\n\ncommunicational findings, and impact assessment.\n\nKarlin is more focused on the problem elicitation phase.\n\nThese two quotes of Box and Karlin\n\nrefer to different stages in this life cycle.\n\nThe data I'm going to use is an industrial data set,\n\n174 observations.\n\nWe have sensor data.\n\nWe have 63 sensors.\n\nThey are labeled 1, 2, 3, 4 to 63.\n\nWe have two response variables.\n\nThese are coming from testing some systems.\n\nThe status report is fail/pass.\n\n52.8% of the systems that were tested failed.\n\nWe have another report,\n\nwhich is a more detailed report on test results.\n\nWhen the systems fail,\n\nwe have some classification of the failure.\n\nTest result is more detailed.\n\nStatus is go/no go.\n\nThe goal is to determine the system status from the sensor data\n\nso that we can maybe avoid the costs and delays of testing,\n\nand we can have some early predictions on the faith of the system.\n\nOne approach we can take is to use a boosted tree.\n\nWe put the status as the response, the 63 sensors X, factors.\n\nThe boosted tree is trained sequentially, one tree at a time.\n\nT he other model we're going to use is random forest,\n\nand that's done with independent trees.\n\nThere is a sequential aspect in boosted trees\n\nthat is different from random forests.\n\nThe setup of boosted trees involves three parameters:\n\nthe number of trees, depth of trees, and the learning rate.\n\nThis is what JMP gives as a default.\n\nBoosted trees can be used to solve most objective functions.\n\nWe could use it for poisson regression,\n\nwhich is dealing with counts that is a bit harder to achieve\n\nwith random forests.\n\nWe're going to focus on these two types of models.\n\nWhen we apply the boosted tree,\n\nand we have a validation set up\n\nwith 43 systems drawn randomly as the validation set.\n\nA hundred and thirty-one systems is used for the training set.\n\nWe are getting a 9.3% misclassification rate.\n\nThree failed systems.\n\nWe know that they failed because we have it in the data,\n\nwere actually classified as pass.\n\nThe 20 that passed, 19 were classified as pass.\n\nThe false predicted pass is 13 %.\n\nWe can look at the variable column contributions.\n\nWe see that Sensor 56, 18, 11, and 61 are the top four\n\nin terms of contributing to this classification.\n\nWe see that in the training set, we had zero misclassification.\n\nWe might have some over fitting i n this BT application.\n\nIf we look at the lift curve,\n\n40 % of the systems, we can get over two lift\n\nwhich is the performance that this classifier gives us.\n\nIf we try the boos trap forest,\n\nanother option, again, we do the same thing.\n\nWe use the same validation set.\n\nThe defaults of JMP are giving you some parameters\n\nfor the number of trees\n\nand the number of features to be selected at each mode.\n\nThis is how the random forest works.\n\nYou should be aware that this is not very good\n\nif you have categorical variables and missing data,\n\nwhich is not our case here.\n\nNow, the misclassification rate is 6.9, lower than before.\n\nOn the training set, we had some misclassification.\n\nThe random forest applied to the test status,\n\nwhich means when we have the details on the failures is 23.4,\n\nso bad performance.\n\nAlso, on the training set, we have 5% misclassification.\n\nBut we have now a wider range of options\n\nand that is explaining some of the lower performance.\n\nIn the lift curve on the test results,\n\nwe actually, with quite good performance,\n\ncan pick up the top 10 % of the systems with a leverage of above 10.\n\nSo we have over a ten fold increase for 10 % of the systems\n\nrelative to the grand average.\n\nNow this is posing a question— remember the topic of the talk—\n\nwhat are we looking at?\n\nDo we want to identify top score good systems?\n\nThe random forest would do that with the test result.\n\nOr do we want to predict a high proportion of pass?\n\nThe bootstrap tree would offer that.\n\nA secondary question is to look at what is affecting this classification.\n\nWe can look at the column contributions on the boosted tree.\n\nThree of the four top variables show up also on the random forest.\n\nIf we use the status pass/fail,\n\nor the detailed results,\n\nthere is a lot of similarity on the importance of the sensors.\n\nThis is just giving you some background.\n\nChris is going to follow up with an evaluation of the sensitivity\n\nof this variable importance, the use of SHAP v alues\n\nand more interesting stuff.\n\nThis goes back to questioning what is your goal,\n\nand how is the model helping you figure out the goal\n\nand maybe sharpening the question that comes from the statement of the goal.\n\nChris, it's all yours.\n\nThanks, Ron.\n\nI'm going to pick up from where Ron left off\n\nand seek a model that will predict whether or not a unit is good or not,\n\nand if it isn't, what's the likely failure mode that has resulted?\n\nThis would be useful in that if a model is good at predicting good units,\n\nwe may not have to subject them to much further testing.\n\nIf the model gives a predicted failure mode,\n\nwe're able to get a head start on diagnosing and fixing the problem,\n\nand possibly, we may be able to get some hints\n\non how to improve the production process in the future.\n\nI'm going to go through the sequence\n\nof how I approached answering this question from the data.\n\nI want to say at the outset that this is simply the path that I took\n\nas I asked questions of the data and acted on various patterns that I saw.\n\nThere are literally many other ways that one could proceed with this.\n\nThere's often not really a truly correct answer,\n\njust a criterion for whether or not the model is good enough,\n\nand the amount that you're able to get done\n\nin the time that you have to get a result back.\n\nI have no doubt that there are better models out there\n\nthan what I came up with here.\n\nOur goal is to show an actual process of tackling a prediction problem,\n\nillustrating how one can move forward\n\nby iterating through cycles of modeling and visualization,\n\nfollowed by observing the results and using them to ask another question\n\nuntil we find something of an answer.\n\nI will be using JMP as a statistical Swiss army knife,\n\nusing many tools in JMP\n\nand following the intuition I have about modeling data\n\nthat has built up over many years.\n\nFirst, let's just take a look\n\nat the frequencies of the various test result categories.\n\nWe see that the largest and most frequent category is Good.\n\nWe'll probably have the most luck being able to predict that category.\n\nOn the other hand, the SOS category has only two events\n\nso it's going to be very difficult for us to be able to do much with that category.\n\nWe may have to set that one aside.\n\nWe'll see about that.\n\nVelocity II, IMP, and Brake\n\nare all fairly rare with five or six events each.\n\nThere may be some limitations in what we're able to do with them as well.\n\nI say this because we have 174 observations\n\nand we have 63 predictors.\n\nSo we have a lot of predictors for a very small number of observations,\n\nwhich is actually even smaller when you consider the frequencies\n\nof some of the categories that we're trying to predict.\n\nWe're going to have to work iteratively by doing visualizations in modeling,\n\nrecognizing patterns, asking questions,\n\nand then acting on those with another modeling step iteratively\n\nin order to find a model that's going to do a good job\n\nof predicting these response categories.\n\nI have the data sorted by test results,\n\nso that the good results are at the beginning,\n\nfollowed by each of the different failure modes d ata a fter that.\n\nI went ahead and colored each of the rows by test results so that we can see\n\nwhich observation belongs to a particular response category.\n\nSo then I went into the model- driven multivariate control chart\n\nand I brought in all of the sensors as process variables.\n\nSince I had the good test results at the beginning,\n\nI labeled those as historical observations.\n\nThis gives us a T² chart.\n\nIt's chosen 13 principal components as its basis.\n\nWhat we see here\n\nis that the chart is dominated by these two points right here\n\nand all of the other points are very small in value\n\nrelative to those two.\n\nThose two points happen to be the SOS points.\n\nThey are very serious outliers in the sensor readings.\n\nSince we also only have two observations of those,\n\nI'm going to go ahead and take those out of the data set\n\nand say, well, SOS is obviously so bad that the sensors\n\nshould be just flying off the charts.\n\nIf we encounter it, we're just going to go ahead\n\nand try to concern ourselves with the other values\n\nthat don't have this off- the- charts behavior.\n\nSwitching to a log scale, we see that the good test results\n\nare fairly well -behaved.\n\nThen there's definite signals\n\nin the data for the different failure modes.\n\nNow we can drill down a little bit deeper,\n\ntaking a look at the contribution plots for the historical data,\n\nthe good test result data, and the failure modes\n\nto see if any patterns emerge in the sensors that we can act upon.\n\nI'm going to remove those two SOS observations\n\nand select the good units.\n\nIf I right-click in the plot,\n\nI can bring up a contribution plot\n\nfor the good units, and then I can go over to the units\n\nwhere there was a failure, and I can do the same thing,\n\nand we'll be able to compare the contribution plots side by side.\n\nSo what we see here are the contribution plots\n\nfor the pass units and the fail units.\n\nThe contribution plots\n\nare the amount that each column is contributing to the T ²\n\nfor a particular row.\n\nEach of the bars there correspond to an individual sensor for that row.\n\nContribution plots are colored green when that column is within three sigma,\n\nusing an individuals and moving range chart,\n\nand it's red if it's out of control.\n\nHere we see most of the sensors are in control for the good units,\n\nand most of the sensors are out of control for the failed units.\n\nWhat I was hoping for here\n\nwould have been if there was only a subset of the columns\n\nor sensors that were out of control over on the failed units.\n\nOr if I was able to see patterns\n\nthat changed across the different failure modes,\n\nwhich would help me isolate what variables are important\n\nfor predicting the test result outcome.\n\nUnfortunately, pretty much all of the sensors\n\nare in control when things are good,\n\nand most of them are out of control when things are bad.\n\nSo we're going to have to use some more sophisticated modeling\n\nto be able to tackle this prediction problem.\n\nHaving not found anything in the column contributions plots,\n\nI'm going to back up and return to the two models that Ron found.\n\nHere are the column contributions for those two models,\n\nand we see that there's some agreement in terms of\n\nwhat are the most important sensors.\n\nBut boosted tree found a somewhat larger set of sensors as being important\n\nover the bootstrap forest.\n\nWhich of these two models should we trust more?\n\nIf we look at the overall model fit report,\n\nwe see that the boosted tree model has a very high training RS quare of 0.998\n\nand a somewhat smaller v alidation RS quare of 0.58.\n\nThis looks like an overfit situation.\n\nWhen we look at the random forest, it has a somewhat smaller training RS quare,\n\nperhaps a more realistic one, than the bootstrap forest,\n\nand it has a somewhat larger validation RS quare.\n\nThe generalization performance of the random forest\n\nis hopefully a little bit better.\n\nI'm inclined to trust the random forest model a little bit more.\n\nPart of this is going to be based upon just the folklore of these two models.\n\nBoosted trees are renowned for being fast, highly accurate models\n\nthat work well on very large datasets.\n\nWhereas the hearsay is that random forests are more accurate on smaller datasets.\n\nThey are fairly robust, messy, and noisy data.\n\nThere's a long history of using these kinds of models\n\nfor variable selection that goes back to a paper in 2010\n\nthat has been cited almost 1200 times.\n\nSo this is a popular approach for variable selection.\n\nI did a similar search for boosting,\n\nand I didn't quite see as much history around variable selection\n\nfor boosted trees as I did for random forests.\n\nFor this given data set r ight here,\n\nwe can do a sensitivity analysis to see how reliable\n\nthe column contributions are for these two different approaches,\n\nusing the simulation capabilities in JMP Pro.\n\nWhat we can do is create a random validation column\n\nthat is a formula column\n\nthat you can reinitialize and will partition the data\n\ninto random training and holdout sets of the same portions\n\nas the original validation column.\n\nWe can have it rerun these two analyses\n\nand keep track of the column contribution portions\n\nfor each of these repartitionings.\n\nWe can see how consistent the story is\n\nbetween the boosted tree models and the random forests.\n\nThis is pretty easy to do.\n\nWe just go to the Make Validation Column utility\n\nand when we make a new column, we ask it to make a formula column\n\nso that it could be reinitialized.\n\nThen we can return to the bootstrap forest platform,\n\nright- click on the column contribution portion,\n\nselect Simulate.\n\nIt'll bring up a dialog\n\nasking us which of the input columns we want to switch out.\n\nI'm going to choose the validation column,\n\nand I'm going to switch in in replacement of it,\n\nthis random validation formula column.\n\nWe're going to do this a hundred times.\n\nBootstrap forest is going to be rerun\n\nusing new random partitions of the data into training and validation.\n\nWe're going to look at the distribution of the portions\n\nacross all the simulation runs.\n\nThis will generate a dataset\n\nof column contribution portions for each sensor.\n\nWe can take this and go into the graph builder\n\nand take a look and see how consistent those column contributions are\n\nacross all these random partitions of the data.\n\nHere is a plot of the column contribution portions\n\nfrom each of the 100 random reshufflings of the validation column.\n\nThose points we see in gray here,\n\nSensor 18 seems to be consistently a big contributor, as does Sensor 61.\n\nWe also see with these red crosses,\n\nthose are the column contributions from the original analysis that Ron did.\n\nThe overall story that this tells is that the tendency\n\ni s that whenever the original column contribution was small,\n\nthose re simulated column contributions also tended to be small.\n\nWhen the column contributions were large in the original analysis,\n\nthey tended to stay large.\n\nWe're getting a relatively consistent story from the bootstrap forest\n\nin terms of what columns are important.\n\nNow we can do the same thing with the boosted tree,\n\nand the results aren't quite as consistent as they were with the bootstrap forest.\n\nSo here is a bunch of columns\n\nwhere the initial column contributions came out very small\n\nbut they had a more substantial contribution\n\nin some of the random reshuffles of the validation column.\n\nThat also happened quite a bit over with these Columns 52 through 55 over here.\n\nThen there were also some situations\n\nwhere the original column contribution was quite large,\n\nand most, if not all,\n\nof the other column contributions found in the simulations were smaller.\n\nThat happens here with Column 48,\n\nand to some extent also with Column 11 over here.\n\nThe overall conclusion being that I think this validation shuffling\n\nis indicating that we can trust the column contributions\n\nfrom the bootstrap forest to be more stable than those of the boosted tree.\n\nBased on this comparison, I think I trust the column contributions\n\nfrom the bootstrap forest more,\n\nand I'm going to use the columns that it recommended\n\nas the basis for some other models.\n\nWhat I'd like to do\n\nis find a model that is both simpler than the bootstrap forest model\n\nand performs better in terms of validation set performance\n\nfor predicting pass or fail.\n\nBefore proceeding with the next modeling stuff,\n\nI'm going to do something that I should have probably done at the very beginning,\n\nwhich is to take a look at the sensors in a scatterplot matrix\n\nto see how correlated the sensor readings are,\n\nand also look at histograms of them as well to see if they're outlier- prone\n\nor heavily skewed or otherwise highly non- gaussian.\n\nWhat we see here is there is pretty strong multicollinearity\n\namongst the input variables generally.\n\nWe're only looking at a subset of them here,\n\nbut this high multicollinearity persists across all of the sensor readings.\n\nThis suggests that for our model,\n\nwe should try things like the logistic lasso,\n\nthe logistic elastic net, or a logistic ridge regression\n\nas candidates for our model to predict pass or fail.\n\nBefore we do that, we should go ahead\n\nand try to transform our sensor readings here\n\nso that they're a little bit better- behaved and more gaussian- looking.\n\nThis is actually really easy in JMP\n\nif you have all of the columns up in the distribution platform,\n\nbecause all you have to do is hold down Alt , choose Fit Johnson,\n\nand this will fit Johnson distributions to all the input variables.\n\nThis is a family of distributions\n\nthat is based on a four parameter transformation to normality.\n\nAs a result, we have a nice feature in there\n\nthat we can also broadcast using Alt Click,\n\nwhere we can save a transformation from the original scale\n\nto a scale that makes the columns more normally distributed.\n\nIf we go back to the data table,\n\nwe'll see that for each sensor column, a transform column has been added.\n\nIf we bring these transformed columns up with a scatterplot matrix\n\nand some histograms,\n\nwe clearly see that the data are less skewed\n\nand more normally distributed than the original sensor columns were.\n\nNow the bootstrap forest model that Ron found\n\nonly really recommended a small number of columns\n\nfor use in the model.\n\nBecause of the high collinearity amongst the columns,\n\nthe subset that we got could easily be part\n\nof a larger group of columns that are correlated with one another.\n\nIt could be beneficial to find that larger group of columns\n\nand work with that at the next modeling stage.\n\nAn exploratory way to do this\n\nis to go through the cluster variables platform in JMP .\n\nWe're going to work with the normalized version of the sensors\n\nbecause this platform is PCA and factor analysis based,\n\nand will provide more reliable results if we're working with data\n\nthat are approximately normally distributed.\n\nOnce we're in the variable clustering platform,\n\nwe see that there is very clear,\n\nstrong associations amongst the input columns.\n\nIt has identified that there are seven clusters,\n\nand the largest cluster, the one that explains the most variation,\n\nhas 25 members.\n\nThe set of cluster members is listed here on the right.\n\nLet's compare this with the bootstrap forest.\n\nHere on the left, we have the column contributions\n\nfrom the bootstrap forest model that you should be familiar with by now.\n\nOn the right, we have the list of members\n\nof that largest cluster of variables.\n\nIf we look closely, we'll see that the top seven contributing terms\n\nall happen to belong to this cluster.\n\nI'm going to hypothesize that this set of 25 columns\n\nare all related to some underlying mechanism\n\nthat causes the units to pass or fail.\n\nWhat I want to do next is I want to fit models\n\nusing the generalized regression platform with the variables in Cluster 1 here.\n\nIt would be tedious if I had to go through\n\nand individually pick these columns out and put them into the launch dialog.\n\nFortunately, there's a much easier way\n\nwhere you can just select the rows in that table\n\nand the columns will be selected in the original data table\n\nso that when we go into the fit model launch dialog,\n\nwe can just click Add\n\nand those columns will be automatically added for us as model effects.\n\nOnce I got into the Generalized Regression platform,\n\nI went ahead and fit a lasso model and elastic net model\n\nand a ridge model to have them compared here to each other,\n\nand also to the logistic regression model that came up by default.\n\nWe're seeing that the lasso model is doing a little bit better than the rest\n\nin terms of its validation generalized RS quare.\n\nThe difference between these methods\n\nis that there's different amounts of variable selection\n\nand multicollinearity handling in each of them.\n\nLogistic regression has no multicollinearity handling\n\nand no variable selection.\n\nThe lasso is more of a variable selection algorithm,\n\nalthough it has a little bit of multicollinearity handling in it\n\nbecause it's a penalized method.\n\nRidge regression has no variable selection\n\nand is heavily oriented around multicollinearity handling.\n\nThe elastic net is a hybrid between the lasso and ridge regression.\n\nIn this case, what we really care about\n\nis just the model that's going to perform the best.\n\nWe allow the validation to guide us.\n\nWe're going to be working with the lasso model from here on.\n\nHere's the prediction profiler for the lasso model that was selected.\n\nWe see that the lasso algorithm has selected eight sensors\n\nas being predictive of pass or fail.\n\nIt has some great built-in tools\n\nfor understanding what the important variables are,\n\nboth in the model overall and, new to JMP Pro 17,\n\nwe have the ability to understand\n\nwhat variables are most important for an individual prediction.\n\nWe can use the variable importance tools to answer the question,\n\n\"What are the important variables in the model?\"\n\nWe have a variety of different options for this.\n\nWe have a variety of different options for how this could be done.\n\nBut because of the multicollinearity and because this is not a very large model,\n\nI'm going to go ahead\n\nand use the dependent resampled inputs technique,\n\nsince we have multicollinearity in the data,\n\nand this has given us a ranking of the most important terms.\n\nWe see that Column 18 is the most important,\n\nfollowed by Column 27 and then 52, all the way down.\n\nWe can compare this to the bootstrap forest model,\n\nand we see that there's agreement that Variable 18 is important,\n\nalong with 52, 61, and 53.\n\nBut one of the terms that we have pulled in\n\nbecause of the variable clustering step that we had done,\n\nSensor 27 turns out to be the second most important predictive\n\nin this lasso model.\n\nWe've hopefully gained something by casting a wider net through that step.\n\nWe've found a term that didn't turn up\n\nin either of the bootstrap forest or the boosted tree methods.\n\nWe also see that the lasso model has an RS quare of 0.9,\n\nwhereas the bootstrap forest model had an RS quare of 0.8.\n\nWe have a simpler model that has an easier form to understand\n\nand is easier to work with,\n\nand also has a higher predictive capacity than the bootstrap forest model.\n\nNow, the variable importance metrics in the profiler\n\nhave been there for quite some time.\n\nThe question that they answer is, \"Which predictors have the biggest impact\n\non the shape of the response surface over the data or over a region?\"\n\nIn JMP 17 Pro, we have a new technique called SHAP Values\n\nthat is an additive decomposition of an individual prediction.\n\nIt tells you by how much each individual variable contributes\n\nto a single prediction,\n\nrather than talking about variability explained over the whole space.\n\nThe resolution of the question that's answered by Shapley values\n\nis far more local than either the variable importance tools\n\nor the column contributions i n the bootstrap forest.\n\nWe can obtain the Shapley Values by going to the red triangle menu for the profiler,\n\nand we'll find the option for them over here, fourth from the bottom.\n\nWhen we choose the option, the profiler saves back SHAP columns\n\nfor all of the input variables to the model.\n\nThis is, of course, happening for every row in the table.\n\nWhat you can see is that the SHAP V alues are giving you the effect\n\nof each of the columns on the predictive model.\n\nThis is useful in a whole lot of different ways,\n\nand for that reason, it's gotten a lot of attention in intelligible AI,\n\nbecause it allows us to see\n\nwhat the contributions are of each column to a black box model.\n\nHere, I've plotted the SHAP V alues for the columns that are predictive\n\nin the last fit model that we just built.\n\nIf we toggle back and forth between the good units and the units that failed,\n\nwe see the same story that we've been seeing\n\nwith the variable importance metrics for this,\n\nthat Column 18 and Column 27 are important in predicting pass or fail.\n\nWe're seeing this at a higher level of resolution\n\nthan we do with the variable importance metrics,\n\nbecause each of these points corresponds to an individual row\n\nin the original dataset.\n\nBut in this case, I don't see the SHAP Values\n\nreally giving us any new information.\n\nI had hoped that by toggling through\n\nthe other failure modes, maybe I could find a pattern\n\nto help tease out different sensors\n\nthat are more important for particular failure modes.\n\nBut the only thing I was able to find was that Column 18\n\nhad a somewhat stronger impact on the Velocity Type 1 failure mode\n\nthan the other failure modes.\n\nAt this point, we've had some success\n\nusing those Cluster 1 columns in a binary pass/ fail model.\n\nBut when I broke out the SHAP Values\n\nfor that model, by the different failure modes\n\nI wasn't able to discern a pattern or much of a pattern.\n\nWhat I did next was I went ahead\n\nand fit the failure mode response column test results\n\nusing the Cluster 1 columns,\n\nbut I went ahead and excluded all the pass rows\n\nso that the modeling procedure would focus exclusively\n\non discerning which failure mode it is given that we have a failure.\n\nI tried the multinomial lasso, elastic net, and ridge,\n\nand I was particularly happy with the lasso model\n\nbecause it gave me a validation RS quare of about 0.94.\n\nHaving been pretty happy with that,\n\nI went ahead and saved the probability formulas\n\nfor each of the failure modes.\n\nNow the task is to come up with a simple rule\n\nthat post processes that prediction formula\n\nto make a decision about which failure mode.\n\nI call this the partition trick.\n\nThe partition trick is where I put in the probability formulas\n\nfor a categorical response, or even a multinomial response.\n\nI put those probability formulas in as Xs.\n\nI use my categorical response as my Y.\n\nThis is the same response that was used\n\nfor all of these except for pass, actually.\n\nI retain the same validation column that I've been working with the whole time.\n\nNow that I'm in the partition platform,\n\nI'm going to hit Split a couple of times, and I'm going to hope\n\nthat I end up with an easily understandable decision rule\n\nthat's easy to communicate.\n\nThat may or may not happen.\n\nSometimes it works, sometimes it doesn't.\n\nSo I split once, and we end up seeing that\n\nwhenever the probability of pass is higher than 0.935,\n\nwe almost certainly have a pass.\n\nNot many passes are left over on the other side.\n\nI take another split.\n\nWe find a decision rule on ITM\n\nthat is highly predictive of ITM as a failure mode.\n\nSplit again.\n\nWe find that whenever Motor is less than 0.945,\n\nwe're either predicting Motor or Brake.\n\nWe take another split.\n\nWe find that whenever Velocity Type 1, its probability is bigger than 0.08\n\nor likely in a Velocity Type 1 situation or in a Velocity T ype 2 situation.\n\nWhenever Velocity Type 1 is less than 0.79,\n\nwe're likely in a gripper failure mode or an IMP failure mode.\n\nWhat do we have here? We have a simple decision rule.\n\nWe're going to not be able to break these failure modes down much further\n\nbecause of the very small number of actual events that we have.\n\nBut we can turn this into a simple rule\n\nfor identifying units that are probably good,\n\nand if they're not, we have an idea of where to look to fix the problem.\n\nWe can save this decision rule out as a leaf label formula.\n\nWe see that on the validation set,\n\nwhen we predict it's good, it's good most of the time.\n\nWe did have one misclassification of a Velocity Type 2 failure\n\nthat was actually predicted to be good.\n\nPredict grippers or IMP, it's all over the place.\n\nThat leaf was not super useful.\n\nPredicting ITM is 100 %.\n\nWhenever we predict a motor or brake,\n\non the validation set, we have a motor or a brake failure.\n\nWhen we predict a Velocity Type 1 or 2,\n\nit did a pretty good job of picking that up\n\nwith that one exception of the single Velocity Type 2 unit\n\nthat was in the validation set,\n\nand that one happened to have been misclassified.\n\nWe have an easily operational rule here that could be used to sort products\n\nand give us a head start on where we need to look to fix things.\n\nI think this was a pretty challenging problem,\n\nbecause we didn't have a whole lot of data.\n\nBut we didn't have a lot of rows,\n\nbut we had a lot of different categories to predict\n\nand a whole lot of possible predictors to use.\n\nWe've gotten there by taking a series of steps,\n\nasking questions,\n\nsometimes taking a step back and asking a bigger question.\n\nOther times, narrowing in on particular sub- issues.\n\nSometimes our excursions were fruitful, and sometimes they weren't.\n\nOur purpose here is to illustrate\n\nhow you can step through a modeling process,\n\nthrough this sequence of asking questions\n\nusing modeling and visualization tools to guide your next step,\n\nand moving on until you're able to find\n\na useful, actionable, predictive model.\n\nThank you very much for your attention.\n\nWe look forward to talking to you in our Q&A session coming up next."
    }
}
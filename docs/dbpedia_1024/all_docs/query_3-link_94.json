{
    "id": "dbpedia_1024_3",
    "rank": 94,
    "data": {
        "url": "https://www.ncbi.nlm.nih.gov/books/NBK218577/",
        "read_more_link": "",
        "language": "en",
        "title": "Risk: A Guide to Controversy",
        "top_image": "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/bookshelf/thumbs/th-nap1189-lrg.png",
        "meta_img": "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/bookshelf/thumbs/th-nap1189-lrg.png",
        "images": [
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/favicons/favicon-57.png",
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/icon-dot-gov.svg",
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/icon-https.svg",
            "https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/logos/AgencyLogo.svg",
            "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/bookshelf/thumbs/th-nap1189-lrg.png",
            "https://www.ncbi.nlm.nih.gov/books/NBK218577/bin/p20008426g220001.gif",
            "https://www.ncbi.nlm.nih.gov/books/NBK218577/bin/p20008426g221001.gif",
            "https://www.ncbi.nlm.nih.gov/books/NBK218577/bin/p20008426g223001.gif",
            "https://www.ncbi.nlm.nih.gov/books/NBK218577/bin/p20008426g224001.gif",
            "https://www.ncbi.nlm.nih.gov/books/NBK218577/table/ttt00003/?report=thumb",
            "https://www.ncbi.nlm.nih.gov/books/NBK218577/bin/p20008426g231001.gif",
            "https://www.ncbi.nlm.nih.gov/books/NBK218577/bin/p20008426g232001.gif",
            "https://www.ncbi.nlm.nih.gov/books/NBK218577/bin/p20008426g233001.gif",
            "https://www.ncbi.nlm.nih.gov/books/NBK218577/bin/p20008426g235001.gif",
            "https://www.ncbi.nlm.nih.gov/books/NBK218577/bin/p20008426g251001.gif",
            "https://www.ncbi.nlm.nih.gov/books/NBK218577/bin/p20008426g252001.gif",
            "https://www.ncbi.nlm.nih.gov/books/NBK218577/bin/p20008426g264001.gif",
            "https://www.ncbi.nlm.nih.gov/books/NBK218577/bin/p20008426g288001.gif",
            "https://www.ncbi.nlm.nih.gov/books/NBK218577/bin/p20008426g289001.gif",
            "https://www.ncbi.nlm.nih.gov/books/NBK218577/bin/p20008426g294001.gif",
            "https://www.ncbi.nlm.nih.gov/stat?jsdisabled=true&ncbi_db=books&ncbi_pdid=book-part&ncbi_acc=NBK218577&ncbi_domain=nap1189&ncbi_report=record&ncbi_type=fulltext&ncbi_objectid=&ncbi_pcid=/NBK218577/&ncbi_pagename=Risk: A Guide to Controversy - Improving Risk Communication - NCBI Bookshelf&ncbi_bookparttype=appendix&ncbi_app=bookshelf"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "1989-08-31T00:00:00",
        "summary": "",
        "meta_description": "This appendix was written by Baruch Fischhoff to assist in the deliberations of the National Research Council's Committee on Risk Perception and Communication. It describes in some detail the complications involved in controversies over managing risks in which risk perception and risk communication play significant roles. It addresses these issues from the perspective of many years of research in psychology and other disciplines. The text of the committee's report addresses many of the same issues, and, not surprisingly, many of the same themes, although the focus of the report is more general. The committee did not debate all points made in the guide. Even though this appendix represents the views of only one member, the committee decided to include it because we believe the guide to be a valuable introduction to an extremely complicated literature.",
        "meta_lang": "en",
        "meta_favicon": "//www.ncbi.nlm.nih.gov/favicon.ico",
        "meta_site_name": "NCBI Bookshelf",
        "canonical_link": "https://www.ncbi.nlm.nih.gov/books/NBK218577/",
        "text": "II. THE SCIENCE\n\nBy definition, all risk controversies concern the risks associated with some hazard. However, as argued in the text of the report and in this diagnostic guide, few controversies are only about the size of those risks. Indeed, in many cases, the risks prove to be a side issue, upon which are hung disagreements about the size and distribution of benefits or about the allocation of political power in a society. In all cases, though, some understanding of the science of risk is needed, if only to establish that a rough understanding of the magnitude of the risk is all that one needs for effective participation in the risk debate. Following the text, the term “hazard” is used to describe any activity or technology that produces a risk. This usage should not obscure the fact that hazards often produce benefits as well as risks.\n\nUnderstanding the science associated with a hazard requires a series of essential steps. The first is identifying the scope of the problem under consideration, in the sense of identifying the set of factors that determine the magnitude of the risks and benefits produced by an activity or technology. The second step is identifying the set of widely accepted scientific “facts” that can be applied to the problem; even when laypeople cannot understand the science underlying these facts, they may at least be able to ensure that such accepted wisdom is not contradicted or ignored in the debate over a risk. The third step in understanding the science of risk is knowing how it depends on the educated intuitions of scientists, rather than on accepted hard facts; although these may be the judgments of trained experts, they still need to be recognized as matters of conjecture that are both more likely to be overturned than published (and replicated) results and more vulnerable to the vagaries of psychological processes.\n\nWHAT ARE THE BOUNDS OF THE PROBLEM?\n\nThe science learned in school offers relatively tidy problems. The typical exercise in, say, physics gives all the facts needed for its solution and nothing but those facts. The difficulty of such problems for students comes in assembling those facts in a way that provides the right answer. (In more advanced classes, one may have to bring some general facts to bear as well.)\n\nThe same assembly problem arises when analyzing the risks and benefits of a hazard. Scientists must discover how its pieces fit together. They must also figure out what the pieces are. For example, what factors can influence the reliability of a nuclear power plant? Or, whose interests must be considered when assessing the benefits of its operation? Or, which alternative ways of generating electricity are realistic possibilities?\n\nThe scientists responsible for any piece of a risk problem must face a set of such issues before beginning their work. Laypeople trying to follow a risk debate must understand how various groups of scientists have defined their pieces of the problem. And, as mentioned in the report, even the most accomplished of scientists are laypeople when it comes to any aspects of a risk debate outside the range of their trained expertise.\n\nThe difficulties of determining the scope of a risk debate emerge quite clearly when one considers the situation of a reporter assigned to cover a risk story. The difficult part of getting most environmental stories is that no one person has the entire story to give. Such stories typically involve diverse kinds of expertise so that a thorough journalist might have to interview specialists in toxicology, epidemiology, economics, groundwater movement, meteorology, and emergency evacuation, not to mention a variety of local, state, and federal officials concerned with public health, civil defense, education, and transportation.\n\nEven if a reporter consults with all the relevant experts, there is no assurance of complete coverage. For some aspects of some hazards, no one may be responsible.\n\nFor example, no evacuation plans may exist for residential areas that are packed “hopelessly” close to an industrial facility. No one may be capable of resolving the jurisdictional conflicts when a train with military cargo derails near a reservoir just outside a major population center. There may be no scientific expertise anywhere for measuring the long-term neurological risks of a new chemical.\n\nEven when there is a central address for questions, those occupying it may not be empowered to take firm action (e.g., banning or exonerating a chemical) or to provide clear-cut answers to personal questions (e.g., “What should I do?” or “What should I tell my children?”). Often those who have the relevant information refuse to divulge it because it might reveal proprietary secrets or turn public opinion against their cause.\n\nHaving to piece together a story from multiple sources, even recalcitrant ones, is hardly new to journalists. What is new about many environmental stories is that no one knows what all of the pieces are or realizes the limits of their own understanding.\n\nExperts tend to exaggerate the centrality of their roles. Toxicologists may assume that everyone needs to know what they found when feeding rats a potential carcinogen or when testing groundwater near a landfill, even though additional information is always needed to make use of those results (e.g., physiological differences among species, routes of human exposure, compensating benefits of the exposure).\n\nAnother source of confusion is the failure of experts to remind laypeople of the acknowledged limits of the experts' craft. For example, cost-benefit analysts seldom remind readers that the calculations consider only total costs and benefits and, hence, ignore questions of who pays the costs and who pays the benefits (Bentkover et al., 1985; Smith and Desvousges, 1986).\n\nFinally, environmental management is an evolving field that is only beginning to establish comprehensive training programs and methods, making it hard for anyone to know what the full picture is and how their work fits into it.\n\nAn enterprising journalist with a modicum of technical knowledge should be able to get specialists to tell their stories in fairly plain English and to cope with moderate evasiveness or manipulation. However, what is the journalist to do when the experts do not know what they do not know? One obvious solution is to talk to several experts with maximally diverse backgrounds. Yet, sometimes such a perfect mix is hard to find. Available experts can all have common limitations of perspective.\n\nAnother solution is to use a checklist of issues that need to be covered in any comprehensive environmental story. Scientists themselves use such lists to ensure that their own work is properly performed, documented, and reported. Such a protocol does not create knowledge for the expert any more than it would provide an education to the journalist. It does, however, help users exploit all they know—and acknowledge what they leave out.\n\nSome protocols that can be used in looking at risk analyses are the causal model, the fault tree, a materials and energy flow diagram, and a risk analysis checklist.\n\nThe Causal Model\n\nThe causal model of hazard creation is a way to organize the full set of factors leading to and from an environmental mishap, both when getting the story and when telling it. The example in is an automobile accident, traced from the need for transportation to the secondary consequence of the collision. Between each stage, there is some opportunity for an intervention to reduce the risk of an accident. By organizing information about the hazard in a chronological sequence, this scheme helps ensure that nothing is left out, such as the deep-seated causes of the mishap (to the left) and its long-range consequences (to the right).\n\nFIGURE II.1\n\nThe causal chain of hazard evolution. The top line indicates seven stages of hazard development, from the earliest (left) to the final stage (right). These stages are expressed generically in the top of each box and in terms of a sample motor vehicle (more...)\n\nApplied to an “irregular event” at a nuclear power station, for example, this protocol would work to remind a reporter of such (left-handed) causes as the need for energy and the need to protect the large capital investment in that industry and such (right-handed) consequences as the costs of retooling other plants designed like the affected plant or the need to burn more fossil fuels if the plant is taken off line (without compensating reductions in energy consumption).\n\nThe Fault Tree\n\nA variant on this procedure is the fault tree ( ), which lays out the sequence of events that must occur for a particular accident to happen (Green and Bourne, 1972; U.S. Nuclear Regulatory Commission, 1983). Actual fault trees, which can be vastly more involved than this example, are commonly used to organize the thinking and to coordinate the work of those designing complex technologies such as nuclear power facilities and chemical plants. At times, they are also used to estimate the overall riskiness of such facilities. However, the numbers produced are typically quite imprecise (U.S. Nuclear Regulatory Commission, 1978).\n\nFIGURE II.2\n\nFault tree indicating the possible ways that radioactivity could be released from deposited wastes after the closure of a repository. SOURCE: Slovic and Fischhoff, 1983.\n\nIn effect, fault trees break open the right-handed parts of a causal model for detailed treatment. They can help a reporter to order the pieces of an accident story collected from different sources, see where an evolving incident (e.g., Three Mile Island or a leaking waste dump) is heading, and find out what safety measures were or were not taken.\n\nMaterials and Energy Flow Diagrams\n\nThe next model ( ) is adapted from the engineering notion of a materials or energy flow diagram. If something is neither created nor destroyed in a process, then one should be able to account schematically for every bit of it. In environmental affairs, one wants to account for all toxic materials. It is important to know where each toxic agent comes from and where each goes.\n\nFIGURE II.3\n\nMaterials and energy flow diagram: Current options for the nuclear fuel cycle. SOURCE: Gotchy, 1983.\n\nKeeping track of a substance can help anticipate where problems will appear, recur, and disappear. It can reveal when a problem has actually been treated and when it has merely been shifted to another time, place, or jurisdiction. With a story like EDB (ethylene dibromide, a fungicide used on grain) (Sharlin, 1987), such a chart would have encouraged questions such as, does it decay with storage or does it become something even worse when cooked and digested? Applying this approach led Harriss and Hohenemser (1978) to conclude that pollution controls had not reduced the total amount of mercury released into the environment, but only the distribution of releases (replacing a few big polluters with many smaller ones). In creating such figures, it is important to distinguish between where a substance is supposed to go and where it actually goes.\n\nA comparable figure might be drawn to keep track of where the money goes, identifying the beneficiaries and losers resulting from different regulatory actions. With the EDB story, such a chart would have encouraged questions about who would eventually pay for the grain lost to pests if that chemical were not used. That is, would reducing the risk of EDB reduce producers' profits or increase consumers' prices? In the former case, failure to ban EDB looks much more callous than in the latter.\n\nA Risk Analysis Checklist\n\nThe fourth aid ( ) is a list of questions that can be asked in a risk analysis (or of a risk analyst) in order to clarify what problem has been addressed and how well it has been solved.\n\nFIGURE II.4\n\nRisk analysis checklist. SOURCE: Northwest Coalition for Alternatives to Pesticides, 1985.\n\nThis list was compiled for a citizens' group concerned with pesticides. Its members had mastered many substantive details of the discipline, such as toxicology and biochemistry, involved in pesticide management, when suddenly they were confronted with a new procedure—risk analysis. In principle, risk analysis does no more than organize information from substantive disciplines in a way that allows overall estimates of risk to be computed. It can facilitate citizen access by forcing all the facts out on the table.\n\nHowever, unless one can penetrate all its formalisms, risk analysis can mystify and obscure the facts rather than reveal them. Such a checklist can clarify what an analysis has done in terms approximating plain English.\n\nWHAT IS THE HARD SCIENCE RELATED TO THE PROBLEM?\n\nWith most “interesting” hazards, the data run out long before enough is known to estimate their risks and benefits as precisely as one would want. Much of risk management involves going beyond the available data either to guess at what the facts might be or to figure out how to live with uncertainty. Obviously, one wants to reduce this uncertainty by making the best of the hard data available.\n\nUnfortunately, there is no short-cut to providing observers with ways to read critically all of the kinds of science that could be invoked in the course of characterizing a risk. There are too many sciences to consider and too many nuances in each type of science to know about in assessing the validity of studies conducted in any one field. Even the social sciences, which seem relatively accessible (compared with the physical sciences) and the results of which can be rendered into common English, routinely foil the efforts of amateur scientists.\n\nThese failures can be seen most clearly in the attempts by nonsocial scientists to make factual statements about the behavior of laypeople, solely on the basis of their untrained anecdotal observations. Such speculations can mislead more than inform if they are made without realizing that they lack the discipline of science.\n\nThe complexities of science arise in the details of creating, analyzing, and interpreting specific sets of data. To give a feeling for these strengths and limits of scientific research, several examples drawn from social science research into risk perception and communication are presented here. Each science has its own nuances. Featuring this science also provides background for interpreting the social science results described below.\n\nLike speculations about chemical reactions, speculations about human behavior must be disciplined by fact. Such speculations make important statements about people and their capabilities, and failure to validate them may mean arrogating to oneself considerable political power. Such happens, for example, when one says that people are so poorly informed (and ineducable) they require paternalistic institutions to defend them, and, furthermore, they might be better off surrendering some political rights to technical experts. It also happens, at the other extreme, when one claims that people are so well informed (and offered such freedom of choice) one need not ask them anything at all about their desires; to know what they want, one need only observe their behavior in the marketplace. It also happens when we assume that people are consummate hedonists, rational to the extreme in their consumer behavior but totally uncomprehending of broader economic issues, so we can impose effective fiscal policies on them without being second-guessed.\n\nOne reason for the survival of such simplistic and contradictory positions is political convenience. Some people want the lay public to participate actively in hazard management decisions, and need to be able to describe the public as competent; others need an incompetent public to legitimate an expert elite. A second reason is theoretical convenience. It is hard to build models of people who are sometimes wise and sometimes foolish, sometimes risk seeking and sometimes risk averse. A third reason is that one can effortlessly speculate about human nature and even produce a bit of supporting anecdotal information. Indeed, good social theory may be so rare because poor social theory is so easy.\n\nJudgments of Risk\n\nAt first sight, assessing the public's risk perceptions would seem to be very straightforward. Just ask questions like, “What is the probability of a nuclear core meltdown?” or “How many people die annually from asbestos-related diseases?” or “How does wearing a seat belt affect your probability of living through the year?” Once the results are in, they can be compared with the best available technical estimates, with deviations interpreted as evidence of respondents' ignorance.\n\nUnfortunately, how one asks the question may in large part determine the content (and apparent wisdom) of the response. Lichtenstein and her colleagues (Lichtenstein et al., 1978) asked two groups of educated laypeople to estimate the frequency of death in the United States from each of 40 different causes. The groups differed only in the information that was given to them about one cause of death in order to help scale their responses. One group was told about 50,000 people die annually in motor vehicle accidents, and the other was told about 1,000 annual deaths result from electrocution. Both reports were accurate, but receiving a larger number increased the estimates of most frequencies for respondents in the motor vehicle accident group. This is a special case of a general psychological phenomenon called “anchoring,” whereby people's responses are pulled toward readily available numbers in cases in which they do not know exactly what to say (Poulton, 1968, 1977; Tversky and Kahneman, 1974). Such anchoring on the original number changed the smallest estimates by roughly a factor of 5.\n\nFischhoff and MacGregor (1983) asked people to judge the lethality of various potential causes of death using one of four formally equivalent formats (e.g., “For each afflicted person who dies, how many survive?” or “For each 100,000 people afflicted, how many will die?”). expresses their judgments in a common format and reveals even more dramatic effects of question phrasing on expressed risk perceptions. For example, when people estimated the lethality rate for influenza directly (column 1), their mean response was 393 deaths per 100,000 cases. When told that 80 million people catch influenza in a normal year and asked to estimate the number who die (column 2), their mean response was 4800, representing a death rate of only 6 per 100,000 cases. This slight change in the question changed the estimated rate by a factor of more than 60. Similar discrepancies occurred with other questions and other hazards. One consequence for risk communicators is that whether lay people intuitively overestimate or underestimate risks (or perceive them accurately) depends on what question they are asked.\n\nTABLE II.1\n\nLethality Judgments with Four Different Response Modes (geometric mean).\n\nIn a recent study at an Ivy League college (Linville et al., 1988), students were asked to give estimates of the probability that the AIDS virus could be transmitted from a man to a woman in a single case of unprotected sex. The median estimate was about 10 percent, considerably above current scientific estimates (Fineberg, 1988). However, when asked to give estimates for the probability of transmission in 100 cases of unprotected sex, the median answer was about 25 percent. This risk estimate is considerably more in line with scientific thinking—so that an investigator asking this question would have a considerably more optimistic assessment of the state of public understanding. Unfortunately, it is also completely inconsistent with the single-case estimates produced by the same individuals. If one believes in a single-case probability of 10 percent, then transmission should be a virtual certainty with 100 exposures. Such failure to see how small risks mount up over repeated exposures has been observed in such diverse settings as the risks from playing simple gambles (Bar-Hillel, 1973), driving (Slovic et al., 1978), and relying on various contraceptive devices (Shaklee et al., 1988).\n\nSuch effects are hardly new; indeed, some have been recognized for close to 100 years. Early psychologists discovered that different numerical judgments may be attached to the same physical stimulus (e.g., the loudness of a tone) as a function of whether the set of alternatives is homogeneous or diverse, and whether the respondent makes one or many judgments. Even when the same presentation is used, different judgments might be obtained with a numerical or a comparative (ordinal) response mode, with instructions stressing speed or accuracy, with a bounded or an unbounded response set, and with verbal or numerical response labels.\n\nThe range of these effects may suggest that the study of judgment is not just difficult, but actually impossible. Closer inspection, however, reveals considerable orderliness underlying this apparent chaos (Atkinson et al., 1988; Carterette and Friedman, 1974; Woodworth and Schlosberg, 1954).\n\nJudgments of Values\n\nOnce the facts of an issue have been estimated and communicated, it is usually held that laypeople should (in a democracy) be asked about their values. What do they want—after the experts have told them what they can (conceivably) have? Here, too, the straightforward strategy of “just ask them” runs into trouble.\n\nThe problem of poorly (or even misleadingly) worded questions in attitude surveys is well known, although not necessarily well resolved (Bradburn and Sudman, 1979; National Research Council, 1982; Payne, 1952; Zeisel, 1980). For example, a major trade publication (Ventner, 1979) presented the results of a survey of public attitudes toward the chemical industry containing the following question:\n\nSome people say that the prime responsibility for reducing exposure of workers to dangerous substances rests with the workers themselves, and that all substances in the workplace should be clearly labeled as to their levels of danger and workers then encouraged or forced to be careful with these substances. Do you agree or disagree?\n\nIt is hard to know what one is endorsing when one says “Yes,” “No,” or “I don't know” to such a complex and unclear question.\n\nAlthough annoying, ambiguous wording is, in principle, a relatively easy problem to deal with because there are accepted ways to “do it right.” Much more complicated are cases in which seemingly arbitrary aspects of how a question is posed affect the values. Parducci (1974) has found that judged satisfaction with one's state in life may depend on the range of possible states mentioned in the question put to people. In an attempt to establish a dollar value for aesthetic degradation of the environment, Brookshire et al. (1976) asked visitors to Lake Powell how much they would be willing to pay in increased users' fees in order not to have an ugly (coal-fired) power plant looming on the opposite shore. They asked “Would you pay $1, $2, $3?” and so on, until the respondent answered “No” and then they retreated in decrements of a quarter (e.g., “Would you pay $5.75, $5.50,…?”). Rather different numerical values might have been obtained had the bidding procedure begun at $100 and decreased by steps of $10 or with other plausible variants. Any respondents who were not sure what they wanted in dollars and cents might naturally and necessarily look to the range of options presented, the difference between first and second options, and so on, for cues as to what are reasonable and plausible responses (Cummings et al., 1986; Smith and Desvousges, 1986).\n\nAt first glance, it might seem as though questions of value are the last redoubt of unaided intuition. Who knows better than an individual what he or she prefers? When people are considering simple, familiar events with which they have direct experience, it may be reasonable to assume that they have well-articulated opinions. Regarding the novel, global consequences potentially associated with CO2-induced climatic change, nuclear meltdowns, or genetic engineering, that may not be the case. Our values may be incoherent, not thought through. In thinking about what are acceptable levels of risk, for example, we may be unfamiliar with the terms in which issues are formulated (e.g., social discount rates, minuscule probabilities, or megadeaths). We may have contradictory values (e.g., a strong aversion to catastrophic losses of life and a realization that we are no more moved by a plane crash with 500 fatalities than by one with 300). We may occupy different roles in life (parents, workers, children) that produce clear-cut but inconsistent values. We may vacillate between incompatible, but strongly held, positions (e.g., freedom of speech is inviolate, but should be denied to authoritarian movements). We may not even know how to begin thinking about some issues (e.g., the appropriate trade-off between the opportunity to dye one's hair and a vague, minute increase in the probability of cancer 20 years from now). Our views may undergo changes over time (say, as we near the hour of decision or of experiencing the consequence) and we may not know which view should form the basis of our decision.\n\nAn extreme, but not uncommon, situation is having no opinion and not realizing it. In that state, we may respond with the first thing that comes to mind once a question is asked and then commit ourselves to maintaining that first expression and to mustering support for it, while suppressing other views and uncertainties. As a result, we may be stuck with stereotypical or associative responses, generated without serious contemplation.\n\nOnce an issue has been evoked, it must be given a label. In a world with few hard evaluative standards, such symbolic interpretations may be very important. While the facts of abortion remain constant, individuals may vacillate in their attitude as they attach and detach the label of murder. shows two versions of the same gamble, differing only in whether one consequence is labeled a “sure loss” or an “insurance premium.” Most people dislike the former and like the latter. When these two versions are presented sequentially, people often reverse their preferences for the two options (Hershey and Shoemaker, 1980). shows a labeling effect that produced a reversal of preference with practicing physicians; most preferred treatment A over treatment B, and treatment D over treatment C, despite the formal equivalence of A and C and of B and D. Saving lives and losing lives afforded very different perspectives on the same problem.\n\nFIGURE II.5\n\nTwo formulations of a choice problem: insurance versus certain loss. SOURCE: Fischhoff et al., 1980.\n\nFIGURE II.6\n\nTwo formulations of a choice problem: lives saved versus lives lost. SOURCE: Tversky and Kahneman, 1981. Copyright © 1981 by the American Association for the Advancement of Science.\n\nPeople solve problems, including the determination of their own values, with what comes to mind. The more detailed, exacting, and creative their inferential process, the more likely they are to think of all they know about a question. The briefer that process becomes, the more they will be controlled by the relative accessibility of various considerations. Accessibility may be related to importance, but it is also related to the associations that are evoked, the order in which questions are posed, imaginability, concreteness, and other factors only loosely related to importance. As one example of how an elicitor may (perhaps inadvertently) control respondents' perspective, Turner (1980) observed a large difference in responses to a simple question such as “Are you happy?” on two simultaneous surveys of the same population ( ). The apparent source of the difference was that one (NORC) preceded the happiness question with a set of questions about married life. In the United States, married people are generally happier than unmarried people. Reminding them of that aspect of their life apparently changed the information that they brought to the happiness question.\n\nFIGURE II.7\n\nTrends in self-reported happiness derived from sample surveys of the noninstitutionalized population of the continental United States aged 18 and over. Error bars demark ±1 standard error around sample estimate. SOURCE: Turner, 1980.\n\nIt would be comforting to be able to say which way of phrasing these questions is most appropriate. However, there is no general answer. One needs to know why the question is being asked (Fischhoff and Furby, 1988). If one wants to predict the quality of casual encounters, then a superficial measure of happiness may suffice. However, an appraisal of national malaise or suicide potential may require a questioning procedure that evokes an appreciation of all components of respondents' lives. It has been known for some time that white interviewers evoke more moderate responses from blacks on race-related questions than do black interviewers. The usual response has been to match the races of interviewer and interviewee (Martin, 1980). This solution may be appropriate for predicting voting behavior or conversation in same-race bars, but not for predicting behavior of blacks in white-dominated workplaces.\n\nThe fact that one has a question is no guarantee that respondents have answers, or even that they have devoted any prior thought to the matter. When one must have an answer (say, because public input is statutorily required), there may be no substitute for an elicitation procedure that educates respondents about how they might look at the question. The possibilities for manipulation in such interviews are obvious. However, one cannot claim to be serving respondents' best interests (letting them speak their minds) by asking a question that only touches one facet of a complex and incompletely formulated set of views.\n\nRefining Common Sense\n\nSocial scientists often find themselves in a no-win situation. If they describe their work in technical jargon, no one wants to listen. If they use plain language, no one feels a need to listen. Listeners feel that they “knew it all along” and that the social scientist was just “affirming the obvious” or “validating common sense.” One possible antidote to this feeling is to point out the evidence showing that, in hindsight, people exaggerate how much they could have known in foresight, leading them to discount the informativeness of scientific reports (Slovic and Fischhoff, 1977). A second antidote is to note that common sense often makes contradictory predictions (e.g., two heads are better than one versus too many cooks spoil the broth; absence makes the heart grow fonder versus out of sight, out of mind). Research is needed to determine which version of common sense is correct or what their respective ranges of validity are. A third strategy, adopted immediately below, is to present empirical results that contradict conventional wisdom (Lazarsfeld, 1949).\n\nInforming People About Risks\n\nIt is often claimed that people do not want to know very much about the health risks they face, since such information makes them anxious. Moreover, they cannot use that information very productively, even if it is given. If true, these claims would make it legitimate for someone else (e.g., physicians, manufacturers, government) to decide what health (and therapeutic) risks are acceptable, and not to invest too much effort on information programs. A number of investigators, however, have replaced anecdotal evidence with systematic observation and have found that, by and large, people want to be told about potential risks (Alfidi, 1971; Weinstein, 1980a). In clinical settings, this desire has been observed with such risky practices as psychotropic medication (Schwarz, 1978), endoscopy (Roling et al., 1977), and oral contraceptives (Applied Management Sciences, 1978; Joubert and Lasagna, 1975). shows respondents' strong opinions about the appropriate use of a pamphlet designed to explain the risks faced by temporary workers in a nuclear power plant. Ninety percent of these individuals gave the most affirmative answer possible to the question, “If you had taken such a job without being shown this pamphlet, would you feel that you had been deprived of necessary information?” (Fischhoff, 1981).\n\nFIGURE II.8\n\nOpinions about the appropriate use of a pamphlet describing the risks associated with temporary work in a facility handling nuclear materials. Respondents were drawn from the readers of a student newspaper and from unemployed individuals at a state labor (more...)\n\nRisk-Taking Propensity\n\nWe all know that some people are risk takers and others are risk avoiders; some are cautious, whereas others are rash. Indeed, attitude toward risk might be one of the first attributes that comes to mind when one is asked to describe someone else's personality. In 1962, Slovic compared the scores of 82 individuals on nine different measures of risk taking. He found no consistency at all in people's propensity for taking risks in the settings created by the various tests (Slovic, 1962). Correlations ranged from—.35 to .34, with a mean of .006. That is, people who are daring in one context may be timid in another, a result that has been replicated in numerous other studies (Davidshofer, 1976).\n\nThe surprising nature of these results may tell us something about ourselves as well as about the people we observe. One of the most robust psychological discoveries of the past 20 years has been identification of the fundamental attribution error, the tendency to view ourselves as highly sensitive to the demands of varying situations, but to see others as driven to consistent behavior by dominating personality traits (Nisbett and Ross, 1980). This misperception may be attributable to the fact that we typically see most others in only one role, as workers or spouses or parents or tennis players or drivers or whatever, in which the situational pressures are quite consistent. Thus, we may observe accurately the evidence available to us, but fail to understand the universe from which these data are drawn.\n\nProtective Behavior\n\nFor years, the United States has been building flood control projects. Despite these great expenditures, flood losses today (in constant dollars) are greater than they were before this enterprise began. Apparently, the behavioral models of the dam and levee builders failed to account for the extent to which eliminating the recurrence of small-to-moderate floods reduced residents' (and particularly newcomers') sensitivity to flood dangers, which in turn led to overbuilding the flood plain. As a result, when the big floods come (about once every 100 years), exceeding the containment capacity of the protective structures, much more lies in their path (White, 1974).\n\nThe official response to this situation has been the National Flood Insurance Program (Kunreuther et al., 1978), designed according to economic models of human behavior, which assumes that flood plain residents are all-knowing, all-caring, and entirely “rational” (as defined by economics). Initially, premiums were greatly subsidized by the federal government to make the insurance highly attractive; these subsidies were to be withdrawn gradually once the insurance-buying habit was established. Unfortunately for the program, few people bought the insurance. The typical explanation for this failure was that residents expected the government to bail them out in the event of flood. However, a field survey found this speculation, too, to be in error. Flood plain residents reported that they expected no help, feeling that they were willingly bearing an acceptable risk. When residents thought about insurance at all, they seemed to rely on a melange of ad hoc principles like, “I can't worry about everything” and “The chances of getting a return (reimbursement) on my investment (premium) are too small,” rather than on the concepts and procedures of economics (Kunreuther et al., 1978; Slovic et al., 1977).\n\nADHERENCE TO ESSENTIAL RULES OF SCIENCE\n\nLooking hard at other sciences would reveal them to be similarly complicated, and similarly surprising. Sciences may not reveal their intricacies readily, but committed citizen activists have often proven themselves capable of mastering enough of the relevant science to be able to ask hard questions about risk issues that interest them ( , for example, was created as a step toward this end). Many, of course, do not, and none could learn the hard questions about all of the sciences impinging on complex risk issues. This is, however, an option for those who care enough.\n\nShort of such intense involvement, it is possible to ask some generic questions about almost any science. These are ways of asking “How good could it be?”, given the conditions of its production.\n\nPerhaps the most basic question that one can ask about any bit of science introduced into an environmental dispute, whether it be a single rodent bioassay or a full-blown risk analysis, is whether it actually represents a bit of science. In applied settings, one often finds evidence that fails to adhere to such essential rules of science as: (1) subjecting the study to critical peer review; (2) making all data available to other investigators; (3) evaluating the statistical reliability of results; (4) considering alternative explanations of the results; (5) relating new results to those already in the literature; and (6) pointing out critical assumptions that have not been empirically verified. Studies that fail to follow such procedures may be attempting to assume the rights, but not the responsibilities of science. Conversely, good science can come even from partisan sources (e.g., industry labs, environmental activists), if the rules are followed.\n\nThe definitiveness of science is bounded not only by the process by which it is conducted, but also by the object of its study. Some topics are simply easier than others, allowing for results clouded by relatively little uncertainty. Unfortunately for the rapid understanding and resolution of problems, risk management often demands understanding of inherently difficult topics.\n\nThis difficulty for risk managers can be seen as a by-product of one fortunate feature of the natural environment, namely, that the most fearsome events are quite infrequent. Major floods, disastrous plagues, and catastrophic tremors are all the exception rather than the rule. Social institutions attempt to constrain hazards of human origin so that the probability of their leading to disaster is low. However great their promised benefit, projects that might frequently kill large numbers of people are unlikely to be developed. The difficult cases are those in which the probability of a disaster is known to be low, but we do not know just how low. Unfortunately, quantitative assessment of very small probabilities is often very difficult (Fairley, 1977).\n\nAt times, one can identify a historical record that provides frequency estimates for an event related to the calamity in question. The U.S. Geological Survey has perhaps 75 years of reliable data on which to base assessments of the likelihood of large earthquakes (Burton et al., 1978). Iceland's copious observations of ice-pack movements over the last millennium provide a clue to the probability of an extremely cold year in the future (Ingram et al., 1978). The absence of a full-scale meltdown in 500 to 1000 reactor-years of nuclear power plant operation sets some bounds on the probability of future meltdowns (Weinberg, 1979). Of course, extrapolation from any of these historical records is a matter of judgment. The great depth and volume of artificial reservoirs may enhance the probability of earthquakes in some areas. Increased carbon dioxide concentrations in the atmosphere may change the earth's climate in ways that amplify or moderate yearly temperature fluctuations. Changes in design, staffing, and regulation may render the next 1000 reactor-years appreciably different from their predecessors. Indeed, any attempt to learn from experience and make a technology safer renders that experience less relevant for predicting future performance.\n\nEven when experts agree on the interpretation of records, a sample of 1000 reactor-years or calendar-years may be insufficient. If one believes the worst-case scenarios of some opponents of nuclear power, a 0.0001 chance of a meltdown (per reactor-year) might seem unconscionable. However, we will be into the next century before we will have enough on-line experience to know with great confidence whether the historical probability is really that low.\n\nHOW DOES JUDGMENT AFFECT THE RISK ESTIMATION PROCESS?\n\nTo the extent that historical records (or records of related systems) are unavailable, one must rely on conjecture. The more sophisticated conjectures are based on models such as the fault-tree and event-tree analyses of a loss-of-coolant accident upon which the Reactor Safety Study was based (U.S. Nuclear Regulatory Commission, 1975). As noted in , a fault tree consists of a logical structuring of what would have to happen for an accident (e.g., a meltdown) to occur. If sufficiently detailed, it will reach a level of specificity for which one has direct experience (e.g., the operation of individual valves). The overall probability of system failure is determined by combining the probabilities of the necessary component failures.\n\nThe trustworthiness of such an analysis hinges on the experts' ability to enumerate all major pathways to disaster and on the assumptions that underlie the modeling effort. Unfortunately, a modicum of systematic data and many anecdotal reports suggest that experts may be prone to certain kinds of errors and omissions. suggests some problems that might underlie the confident veneer of a formal model.\n\nTABLE II.2Some Problems in Structuring Risk Assessments\n\nView in own window\n\nFailure to consider the ways in which human errors can affect technological systems.\n\nExample: Owing to inadequate training and control room design, operators at Three Mile Island repeatedly misdiagnosed the problems of the reactor and took inappropriate actions (Sheridan, 1980; U.S. Government, 1979).Overconfidence in current scientific knowledge.\n\nExample: DDT came into widespread and uncontrolled use before scientists had even considered the possibility of the side effects that today make it look like a mixed, and irreversible, blessing (Dunlap, 1978).Failure to appreciate how technological systems function as a whole.\n\nExample: The DC-10 failed in several early flights because its designers had not realized that decompression of the cargo compartment would destroy vital control systems (Hohenemser, 1975).Slowness in detecting chronic, cumulative effects.\n\nExample: Although accidents to coal miners have long been recognized as one cost of operating fossil-fueled plants, the effects of acid rain on ecosystems were slow to be discovered (Rosencranz and Wetstone, 1980).Failure to anticipate human response to safety measures.\n\nExample: The partial protection afforded by dams and levees gives people a false sense of security and promotes development of the flood plain. Thus, although floods are rarer, damage per flood is so much greater that the average yearly loss in dollars is larger than before the dams were built (Burton et al., 1978).Failure to anticipate common-mode failures, which simultaneously afflict systems that are designed to be independent.\n\nExample: Because electrical cables controlling the multiple safety systems of the reactor at Browns Ferry, Alabama, were not spatially separated, all five emergency core-cooling systems were damaged by a single fire (Jennergren and Keeney, 1982; U.S. Government, 1975).\n\nSOURCE: Fischhoff, Lichtenstein, et al., 1981a.\n\nWhen the logical structure of a system cannot be described to allow computation of its failure probabilities (e.g., when there are large numbers of interacting systems), physical or computerized simulation models may be used. If one believes the inputs and the programmed interconnections, one should trust the results. What happens, however, when the results of a simulation are counterintuitive or politically awkward? There may be a strong temptation to try it again, adjusting the parameters or assumptions a bit, given that many of these are not known with certainty in the first place. Susceptibility to this temptation could lead to a systematic and subtle bias in modeling. At the extreme, models would be accepted only if they confirmed expectations.\n\nAcknowledging the Role of Judgment\n\nAlthough the substance of sciences differs greatly, sciences do have in common the fact that they are produced by the minds of mortals. Those minds may contain quite different facts, depending on the disciplines in which they were trained. However, it is reasonable to suppose that they operate according to similar principles when they are pressed to make speculations—taking them beyond the limits of hard data—in order to produce the sorts of assessments needed to guide risk managers.\n\nIndeed, the need for judgment is a defining characteristic of risk assessment (Federal Register 49(100):21594–21661). Some judgment is, of course, a part of all science. However, the policy questions that hinge on the results of risk assessments typically demand greater scope and precision than can be provided by the “hard” knowledge that any scientific discipline currently possesses. As a result, risk assessors must fill the gaps as best they can. The judgments incorporated in risk assessments are typically those of esteemed technical experts, but they are judgments nonetheless, taking one beyond the realm of established fact and into the realm of educated opinions that cannot immediately be validated.\n\nJudgment arises whenever materials scientists estimate the failure rates for valves subjected to novel conditions (Joksimovich, 1984; Ostberg et al., 1977), whenever accident analysts attempt to recreate operators' perceptions of their situation prior to fatal mishaps (Kadlec, 1984; Pew et al., 1982), when toxicologists choose and weight extrapolation models (Rodricks and Tardiff, 1984; Tockman and Lilienfeld, 1984), when epidemiologists assess the reasons for nonresponse in a survey (Joksimovich, 1984; National Research Council, 1982), when pharmacokineticists consider how consumers alter the chemical composition of foods (e.g., by cooking and storage practices) before they consume them (National Research Council, 1983a; O'Flaherty, 1984), when physiologists assess the selection bias in the individuals who volunteer for their experiments (Hackney and Linn, 1984; Rosenthal and Rosnow, 1969), when geologists consider how the construction of underground storage facilities might change the structure of the rock media and the flow of fluids through them (Sioshansi, 1983; Travis, 1984), and when psychologists wonder how the dynamics of a particular group of interacting experts affect the distribution of their responses (Brown, 1965; Davis, 1969; Hirokawa and Poole, 1986).\n\nThe process by which judgments are produced may be as varied as the topics they treat. Individual scientists may probe their own experience for clues to the missing facts. Reviewers may be sponsored to derive the best conclusions that the literature can provide. Panels of specialists may be convened to produce a collective best guess. Trained interviewers may use structured elicitation techniques to extract knowledge from others. The experts producing these judgments may be substantive experts in almost any area of science and engineering, risk assessment generalists who take it upon themselves to extrapolate from others' work, or laypeople who happen to know more than anyone else about particular facts (e.g., workers assessing how respirators are really used, civil defense officials predicting how evacuation plans will work).\n\nFew experts would deny that they do not know all the answers. However, detailed treatments of the judgments they make in the absence of firm evidence are seldom forthcoming (Federal Register 49(100):21594–21661). There appear to be several possible causes for this neglect. Knowing which is at work in a particular risk assessment establishes what effect, if any, the informal treatment of judgment has had.\n\nOne common reason for treating the role of judgment lightly is the feeling that everyone knows that it is there, hence there is no point in repeating the obvious. Although this feeling is often justified, acting on it can have two deleterious consequences. One is that all consumers of an assessment may not share the same feeling. Some of these consumers may not realize that judgment is involved, whereas others may suspect that the judgments are being hidden for some ulterior purpose. The second problem is that failure to take this step precludes taking the subsequent steps of characterizing, improving, and evaluating the judgments involved.\n\nA second, complementary reason for doing little about judgment is the belief that nothing much can be done, beyond a good-faith effort to think as hard as one can. Considering the cursory treatment of judgmental issues in most methodological primers for risk analysts, this perception is understandable. Considering the importance of doing something and the extensive research regarding what can be done, it is, however, not justifiable. Although the research is unfamiliar to most practicing analysts, the study and cultivation of judgment have proven tractable. The vulnerability of analyses to judgmental difficulties means that those who ignore judgment for this reason may miss a significant opportunity to perform at the state of the art.\n\nA third reason for ignoring judgment is being rewarded for doing so. At times, analysts discern some strategic advantage to exaggerating the definitiveness of their work. At times, analysts feel that they must make a begrudging concession to the demands of political processes that attend only to those who speak with (unjustifiable) authority. At times, the neglect of judgment is (almost) a condition of employment, as when employers, hearings officials, or contracting agencies require statements of fact, not opinion.\n\nDiagnosing the Role of Judgment\n\nThe first step in dealing with the judgmental aspects of risk assessments is identifying them. All risk assessment, and most contemporary science, can be construed as the construction of models. These include both procedures used to assess discrete hazards (e.g., accidents), such as probabilistic risk analysis, and procedures used to assess continuous hazards (e.g., toxicity), such as dose-response curves or structural-activity relationships. Although these models take many forms, all require a similar set of judgmental skills, which can be used as a framework for diagnosing where judgment enters into analyses (and, subsequently, how good it is and what can be done about it). These skills are:\n\n1.\n\nIdentifying the active elements of the hazardous system being studied. These may be the physical components of a nuclear power plant (e.g., the valves, controls, and piping) (U.S. Nuclear Regulatory Commission, 1983), the environmental factors affecting the dispersal of toxins from a waste disposal site (e.g., geologic structure, rainfall patterns, adjacent construction) (Pinder, 1984), or the potential predictors of cancer in an epidemiological study (Tockman and Lilienfeld, 1984).\n\n2.\n\nCharacterizing the interrelationships among these elements. Not everything is connected to everything else. Reducing the set of interconnections renders the model more tractable, its results more comprehensible, and its data demands more manageable. The probabilistic risk analyst must judge which malfunctions in System X need to be considered when studying the performance of System Y. The epidemiologist needs to judge which interaction terms to include in regression models.\n\n3.\n\nAssessing the value of model parameters. The amount of this kind of judgment varies greatly both across and within analyses. Some values have a sound statistical base (e.g., the number of chemical workers, as revealed by a decennial census), whereas others must be created from whole cloth (e.g., the sabotage rate at an as-yet-unconstructed plant 10 years in the future). Yet even the firmest statistics require some interpretation, for example, to correct for sampling and reporting biases or to adjust for subsequent changes in conditions.\n\n4.\n\nEvaluating the quality of the analysis. Every analysis requires some summary statement of how good it is, whether for communicating its results to policymakers or for deciding whether to work on it more. Such evaluation requires consideration of both the substance and the purpose of the analysis. In both basic and applied sciences, the answer to “is the assessment good enough?” presupposes an answer to “good enough for what?”\n\n5.\n\nAdopting appropriate judgmental techniques. Just as each stage in risk assessment requires different judgmental skills, it also requires different elicitation procedures. The reason for this is that each kind of information is organized in people's minds in a different way, and needs, therefore, to be extracted in a different way. For example, listing all possible mistakes that operators of a process-control industry might make is different than estimating how frequently each mistake will be made. The former requires heavy reliance on memory for instances of past errors, whereas the latter requires aggregation across diverse experiences and their extrapolation to future situations. Different experts (e.g., veteran operators, human factors theorists) may be more accustomed to thinking about the topic in one way rather than the other. Although transfer of information between these modes of thinking is possible, it may be far from trivial (Lachman et al., 1979; Tulving, 1972).\n\nAs noted earlier, studies with laypeople have found that seemingly subtle variations in how judgments are elicited can have large effects on the beliefs that are apparently revealed. These effects are most pronounced when people are least certain about how to respond, either because they do not know the answers or because they are unaccustomed to expressing themselves in the required terms. Thus, in extrapolating these results one must ask how expert the respondents are both in the topic requiring judgment and in using that response mode.\n\nAssessing the Quality of the Judgment\n\nIf analysts have addressed the preceding steps conscientiously and left an audit trail of their work, all that remains is to review the protocol of the analysis to determine how heavily its conclusions depend on judgment and how adequate those judgments are likely to be. That evaluation should consider both the elicitation methods used and the judgmental capabilities of the experts. Ideally, the methods would have been empirically tested to show that they are: (1) compatible with the experts' mental representation of the problem, and (2) able to help the experts use their minds more effectively by overcoming common judgmental difficulties. Ideally, the experts would not only be knowledgeable about the topic, but also capable of translating that knowledge into the required judgments. The surest guarantees of that capability are having been trained in judgment or having provided judgments in conditions conducive to skill acquisition (e.g., prompt feedback).\n\nHow Good Are Expert Judgments?\n\nAs one might expect, considerably more is known about the judgmental processes of laypeople than about the judgmental processes of experts performing tasks in their areas of expertise. It is simply much easier to gain access to laypeople and create tasks about everyday events. Nonetheless, there are some studies of experts per se. In addition, there is some basis in psychological theory for extrapolating from the behavior of laypeople to that of experts. What follows is a selection of the kinds of problems that any of us may encounter when going beyond the available data, and which must be considered when weighing the usefulness of analyses estimating risks and benefits.\n\nSensitivity to Sample Size\n\nTversky and Kahneman (1971) found that even statistically sophisticated individuals have poor intuitions about the size of sample needed to test research hypotheses adequately. In particular, they expect small samples to represent the populations from which they were drawn to a degree that can only be assumed with much larger samples. This tendency leads them to gamble their research hypotheses on underpowered small samples, to place undue confidence in early data trends, and to underestimate the role of sampling variability in causing results to deviate from expectations (preferring instead to offer causal explanations for discrepancies). For example, in a survey of standard hematology texts, Berkson et al. (1939–1940) found that the maximum allowable difference between two successive blood counts was so small that it would normally be exceeded by chance 66 to 85 percent of the time. They mused about why instructors often reported that their best students had the most trouble attaining the desired standard.\n\nSmall samples mean low statistical power, that is, a small chance of detecting phenomena that really exist. Cohen (1962) surveyed published articles in a respected psychological journal and found very low power. Even under the charitable assumption that all underlying effects were large, a quarter of the studies had less than three chances in four of showing statistically significant results. He goes on to speculate that the one way to get a low-power study published is to keep doing it again and again (perhaps making subtle variations designed to “get it right next time”) until a significant result occurs. Consequently, published studies may be unrepresentative of the set of conducted studies in a way that inflates the rate of spuriously significant results (beyond that implied by the officially reported “significance level”). Page (1981) has similarly shown the low power of representative toxicological studies. In designing such studies, one inevitably must make a trade-off between avoiding false alarms (e.g., erroneously calling a chemical a carcinogen) and misses (e.g., erroneously calling a chemical a noncarcinogen). Low power increases the miss rate and decreases the false alarm rate. Hence, wayward intuitions may lead to experimental designs that represent, perhaps inadvertently, a social policy that protects chemicals more than people.\n\nHindsight\n\nExperimental work has shown that in hindsight people consistently exaggerate what could have been anticipated in foresight. They tend not only to view what has happened as having been relatively inevitable, but also to view it as having appeared relatively inevitable before it happened. People believe that others should have been able to anticipate events much better than was actually the case. They even misremember their own predictions so as to exaggerate in hindsight what they knew in foresight (Fischhoff, 1980).\n\nThe revisionist history of strategic surprises (e.g., Lanir, 1982; Wohlstetter, 1962) argues that such misperceptions have vitiated the efforts of scholars and “scalpers” attempting to understand questions like, “Who goofed at Pearl Harbor?” These expert scrutinizers were not able to disregard the knowledge that they had only as a result of knowing how things turned out. Although it is flattering to believe that we personally would not have been surprised, failing to realize the difficulty of the task that faced the individuals about whom we are speculating may leave us very exposed to future surprises.\n\nMethodological treatises for professional historians contain numerous warnings about related tendencies. One such tendency is telescoping the rate of historical processes, exaggerating the speed with which “inevitable” changes are consummated (Fischer, 1970). Mass immunization against poliomyelitis seems like such a natural idea that careful research is needed to show that its adoption met substantial snags, taking almost a decade to complete (Lawless, 1977). A second variant of hindsight bias may be seen in Barraclough's (1972) critique of the historiography of the ideological roots of Nazism; looking back from the Third Reich, one can trace its roots to the writings of many authors from whose writings one could not have projected Nazism. A third form of hindsight bias, also called “presentism,” is to imagine that the participants in a historical situation were fully aware of its eventual importance [“Dear Diary, The Hundred Years' War started today” (Fischer, 1970)].\n\nMore directly relevant to the resolution of scientific disputes, Lakatos (1970) has argued that the “critical experiment,” unequivocally resolving the conflict between two theories or establishing the validity of one, is typically an artifact of inappropriate reconstruction. In fact, “the crucial experiment is seen as crucial only decades later. Theories don't just give up, a few anomalies are always allowed. Indeed, it is very difficult to defeat a research programme supported by talented and imaginative scientists” (Lakatos, 1970:157–158).\n\nFuture generations may be puzzled by the persistence of the antinuclear movement after the 1973 Arab oil embargo guaranteed the future of nuclear power, or the persistence of nuclear advocates after Three Mile Island sealed the industry's fate—depending on how things turn out. Perhaps the best way to protect ourselves from the surprises and reprobation of the future in managing hazards is to “accept the fact of uncertainty and learn to live with it. Since no magic will provide certainty, our plans must work without it” (Wohlstetter, 1962:401).\n\nJudging Probabilistic Processes\n\nAfter seeing four successive heads in flips of a fair coin, most people expect a tails. Once diagnosed, this tendency is readily interpreted as a judgmental error. Commonly labeled the “gambler's fallacy” (Lindman and Edwards, 1961), it is one reflection of a strong psychological tendency to impose order on the results of random processes, making them appear interpretable and predictable (Kahneman and Tversky, 1972). Such illusions need not disappear with higher stakes or greater attention to detail. Feller (1968) offers one example in risk monitoring: Londoners during the Blitz devoted considerable effort to interpreting the pattern of German bombing, developing elaborate theories of where the Germans were aiming (and when to take cover). However, a careful statistical analysis revealed that the frequency distribution of bomb-hits in different sections of London was almost a perfect approximation of the Poisson (random) distribution. Dreman (1979) argues that the technical analysis of stock prices by market experts represents little more than opportunistic explication of chance fluctuations. Although such predictions generate an aura of knowing, they fail to outperform market averages.\n\nGilovich et al. (1985) found that, appearances to the contrary, basketball players have no more shooting streaks than one might expect from a random process generated by their overall shooting percentage. This result runs strongly counter to the conventional wisdom that players periodically have a “hot hand,” attributable to specific causes like a half-time talk or dedication to an injured teammate. One of the few basketball experts to accept this result claimed that he could not act on it anyway. Fans would not forgive him if, in the closing minutes of a game, he had an inbound pass directed to a higher percentage shooter, rather than to a player with an apparent “hot hand” (even knowing that opposing players would cluster on that player, expecting the pass).\n\nAt times, even scientific enterprises seem to represent little more than sophisticated capitalization on chance. Chapman and Chapman (1969) found that clinical psychologists see patterns that they expect to find even in randomly generated data. O'Leary et al. (1974) observed that the theories of foreign affairs analysts are so complicated that any imaginable set of data can be interpreted as being consistent with them. Short of this extreme, it is generally true that, given a set of events (e.g., environmental calamities) and a sufficiently large set of possible explanatory variables (antecedent conditions), one can always devise a theory for retrospectively predicting the events to any desired level of proficiency. The price one pays for such overfitting is shrinkage, failure of the theory to work on a new sample of cases. The frequency and vehemence of warnings against such correlational overkill suggest that this bias is quite resistant to even extended professional training (Armstrong, 1975; Campbell, 1975; Crask and Parreault, 1977; Kunce et al., 1975).\n\nEven when one is alert to such problems, it may be difficult to assess the degree to which one has capitalized on chance. For example, as a toxicologist, you are “certain” that exposure to chemical X is bad for one's health, so you compare workers who do and do not work with it in a particular plant for bladder cancer, but obtain no effect. So you try intestinal cancer, emphysema, dizziness, and so on, until you finally get a significant difference in skin cancer. Is that difference meaningful? Of course, the way to test these explanations or theories is by replication on new samples. That step, unfortunately, is seldom taken and is often not possible for technical or ethical reasons (Tukey, 1977).\n\nA further unintuitive property of probabilistic events is regression to the mean, the tendency for extreme observations to be followed by less extreme ones. One depressing failure by experts to appreciate this fact is seen in Campbell and Erlebacher's (1970) article, “How regression artifacts in quasi-experimental evaluations can mistakenly make compensatory education look harmful” (because upon retest, the performance of the better students seems to have deteriorated). Similarly unfair tests may be created when one asks only if environmental management programs have, say, weakened strong industries or reduced productivity in the healthiest sectors of the economy.\n\nJudging the Quality of Evidence\n\nSince cognitive and evidential limits prevent scientists from providing all the answers, it is important to have an appraisal of how much they do know. It is not enough to claim that “these are the ranking experts in the field,” for there are some fields in which the most knowledgeable individuals understand a relatively small portion of all there is to be known.\n\nWeather forecasters offer some reason for encouragement (Murphy and Brown, 1983; Murphy and Winkler, 1984). There is at least some measurable precipitation on about 70 percent of the occasions for which they say there is a 70 percent chance of rain. The conditions under which forecasters work and train suggest the following prerequisites for good performance in probabilistic judgment:\n\ngreat amounts of practice;\n\nthe availability of statistical data offering historical precipitation base rates (indeed, forecasters might be fairly well calibrated if they ignored the murmurings of their intuitions and always responded with the base rate);\n\ncomputer-generated predictions for each situation;\n\na readily verifiable criterion event (measurable precipitation), offering clear feedback; and\n\nexplicit admission of the imprecision of the trade and the need for training.\n\nIn experimental work, it has been found that large amounts of clearly characterized, accurate, and personalized feedback can improve the probability assessments of laypeople (e.g., Lichtenstein and Fischhoff, 1980).\n\nTraining professionals to assess and express their uncertainty is, however, a rarity. Indeed, the role of judgment is often acknowledged only obliquely. For example, civil engineers do not routinely assess the probability of failure for completed dams, even though approximately one dam in 300 collapses when first filled (U.S. Committee on Government Operations, 1978). The “Rasmussen” Reactor Safety Study (U.S. Nuclear Regulatory Commission, 1975) was an important step toward formalizing the role of risk in technological systems, although a subsequent review was needed to clarify the extent to which these estimates were but the product of fallible, educated judgment (U.S. Nuclear Regulatory Commission, 1978).\n\nUltimately, the quality of experts' assessments is a matter of judgment. Since expertise is so narrowly distributed, assessors are typically called upon to judge the quality of their own judgments. Unfortunately, an extensive body of research suggests that people are overconfident when making such assessments (Lichtenstein et al., 1982). A major source of such overconfidence seems to be failure to appreciate the nature and tenuousness of the assumptions on which judgments are based. To illustrate with a trivial example, when asked “To which country are potatoes native? (a) Ireland (b) Peru?”, many people are very confident that answer (a) is true. The Irish potato and potato blight are familiar to most people; however, that is no guarantee of origin. Indeed, the fact that potatoes were not indigenous to Ireland may have increased their susceptibility to blight there.\n\nExperts may be as prone to overconfidence as laypeople (in cases in which they, too, are pressed to evaluate judgments made regarding topics about which their knowledge is limited). For example, when several internationally known geotechnical experts were asked to predict the height of fill at which an embankment would fail and to give confidence intervals for their estimates, without exception, the true values fell outside the confidence intervals (Hynes and Vanmarcke, 1976), a result akin to that observed with other tasks and respondent populations (Lichtenstein et al., 1982). One of the intellectual challenges facing engineering is to systematize the role of judgment, both to improve its quality and to inform those who must rely on it in their decision making.\n\nThis basic pattern of results has proved so robust that it is hard to acquire much insight into the psychological processes producing it (Lichtenstein et al., 1982). One of the few effective manipulations is to force subjects to explain why their chosen answers might be wrong (Koriat et al., 1980). That simple instruction seems to prompt recall of contrary reasons that would not normally come to mind given people's natural thought processes, which seem to focus on retrieving reasons that support chosen answers. A second seemingly effective manipulation, mentioned earlier, is to train people intensively with personalized feedback that shows them how well they are calibrated.\n\nFigures and show one sign of the limits that exist on the capacity of expertise and experience to improve judgment—in the absence of the conditions for learning enjoyed, say, by weather forecasters. Particle physicists' estimates of the value of several physical constants are bracketed by what might be called confidence intervals, showing the range of likely values within which the true value should fall, once it is known. Narrower intervals indicate greater confidence. These intervals have shrunk over time, as physicists' knowledge has increased. However, at most points, they seem to have been too narrow. Otherwise, the new best estimates would not have fallen so frequently outside the range of what previously seemed plausible. In an absolute sense, the level of knowledge represented here is extremely high and the successive best estimates lie extremely close to one another. However, the confidence intervals define what constitute surprises in terms of current physical theory. Unless the possibility of overconfident judgment is considered, values falling outside the intervals suggest a weakness in theory.\n\nFIGURE II.9\n\nCalibration of confidence in estimates of physical constants. SOURCE: Henrion and Fischhoff, 1986. Copyright © 1986 by the American Association of Physics Teachers.\n\nFIGURE II.10\n\nRecommended values for fundamental constants, 1952 through 1973. SOURCE: Henrion and Fischhoff, 1986. Copyright © 1986 by the American Association of Physics Teachers.\n\nSUMMARY\n\nThe science of risk provides a critical anchor for risk controversies. There is no substitute for that science. However, it is typically an imperfect guide. It can mislead if one violates any of a wide variety of intricate methodological requirements—including the need to use judgment judiciously (and to understand its limitations). The general nature of these assumptions was illustrated with examples drawn from the science of understanding human behavior. Sections IV through VI deal with the human anchors for risk controversies: the nature of their political tensions, the strategies that risk communicators can take in them, and psychological barriers to risk communication. The next section (III) deals with the interface between science and behavior, specifically ways in which science shapes and is shaped by the political process.\n\nIV. THE NATURE OF THE CONTROVERSY\n\nA public opinion survey (Harris, 1980) reported the following three results:\n\n1.\n\nAmong four “leadership groups” (top corporate executives, investors and lenders, congressional representatives, and federal regulators), 94 to 98 percent of all respondents agreed with the statement “even in areas in which the actual level of risk may have decreased in the past 20 years, our society is significantly more aware of risk.”\n\n2.\n\nBetween 87 and 91 percent of those four leadership groups felt that “the mood of the country regarding risk” will have a substantial or moderate impact “on investment decisions—that is, the allocation of capital in our society in the decade ahead.” (The remainder believed that it would have a minimal impact, no impact at all, or were not sure.)\n\n3.\n\nNo such consensus was found, however, when these groups were asked about the appropriateness of this concern about risk. A majority of the top corporate executives and a plurality of lenders believed that “American society is overly sensitive to risk,” whereas a large majority of congressional representatives and federal regulators believed that “we are becoming more aware of risk and taking realistic precautions.” A sample of the public endorsed the latter statement over the former by 78 to 15 percent.\n\nIn summary, there is great agreement that risk decisions will have a major role in shaping our society's future and that those decisions will, in turn, be shaped by public perceptions of risk. There is, however, much disagreement about the appropriateness of those perceptions. Some believe the public to be wise; others do not. These contrary beliefs imply rather different roles for public involvement in risk management. As a result, the way in which this disagreement is resolved will affect not only the fate of particular technologies, but also the fate of our society and its social organization.\n\nTo that end, various investigators have been studying how and how well people think about risks. Although the results of that research are not definitive as yet, they do clearly indicate that a careful diagnosis is needed whenever the public and the experts appear to disagree. It is seldom adequate to attribute all such discrepancies to public misperceptions of the science involved. From a factual perspective, that assumption is often wrong; from a societal perspective, it is generally corrosive by encouraging disrespect among the parties involved. When the available research data do not allow one to make a confident alternative diagnosis, a sounder assumption is that there is some method in the other party's apparent madness. This section offers some ways to find that method. Specifically, it offers six reasons why disagreements between the public and the experts need not be interpreted merely as clashes between actual and perceived risks.\n\nTHE DISTINCTION BETWEEN “ACTUAL” AND “PERCEIVED” RISKS IS MISCONCEIVED\n\nAlthough there are actual risks, nobody knows what they are. All that anyone does know about risks can be classified as perceptions. Those assertions that are typically called actual risks (or facts or objective information) inevitably contain some element of judgment on the part of the scientists who produce them. In this light, what is commonly called the conflict between actual and perceived risk is better thought of as the conflict between two sets of risk perceptions: those of ranking scientists performing within their field of expertise and those of anybody else. The element of judgment is most minimal when all the experts do is to assess the competence of a particular study conducted within an established paradigm. It grows with the degree to which experts must integrate results from diverse studies or extrapolate from a domain in which results are readily obtainable to another in which they are really needed (e.g., from animal studies to human effects). Judgment becomes all when there are no (credible) available data, yet a policy decision requires some assessment of a particular fact. Section II discusses at length the trustworthiness of such judgments.\n\nThe expert opinions that make up the scientific literature aspire to be objective in two senses, neither of which can ever be achieved absolutely and neither of which is the exclusive province of technical experts. One meaning of objectivity is reproducibility: one expert should be able to repeat another's study, review another's protocol, reanalyze another's data, or recap another's literature summary and reach the same conclusions about the size of an effect. Clearly, as the role of judgment increases in any of these operations, the results become increasingly subjective. Typically, reproducibility should decrease (and subjectivity increase) to the extent that a problem attracts scientists with diverse training or falls into a field that has yet to reach consensus on basic issues of methodology.\n\nThe second sense of objectivity means immune to the influence by value considerations. One's interpretations of data should not be biased by one's political views or pecuniary interests. Applied sciences naturally have developed great sensitivity to such problems and are able to invoke some penalties for detected violations. There is, however, little possibility of regulating the ways in which values influence other acts, such as one's choice of topics to study or ignore. Some of these choices might be socially sanctioned, in the sense that one's values are widely shared (e.g., deciding to study cancer because it is an important problem); other choices might be more personal (e.g., not studying an issue because one's employer does not wish to have troublesome data created on that topic). Although a commitment to separating issues of fact from issues of value is a fundamental aspect of intellectual hygiene, a complete separation is never possible (see Section III).\n\nAt times, this separation is not even desired—as when experts offer their views on how risks should be managed. Because they mix questions of fact and value, such views might be better thought of as the opinions of experts rather than as expert opinions, a term that should be reserved for expressions of substantive expertise. It would seem as though members of the public are the experts when it comes to striking the appropriate trade-offs between costs, risks, and benefits. That expertise is best tapped by surveys, hearings, and political campaigns.\n\nOf course, there is no all-purpose public any more than there are all-purpose experts. The ideal expert on a matter of fact has studied that particular issue and is capable of rendering a properly qualified opinion in a form useful to decision makers. Using the same criteria for selecting value experts might lead one to philosophers, politicians, psychologists, sociologists, clergy, intervenors, pundits, shareholders, or well-selected bystanders. Thus, one might ask, “in what sense,” whenever someone says “expert” or “public” (Schnaiburg, 1980; Thompson, 1980). This appendix uses “expert” in the restrictive sense and “public” or “laypeople” to refer to everyone else, including scientists in their private lives.\n\nLAYPEOPLE AND EXPERTS ARE SPEAKING DIFFERENT LANGUAGES\n\nExplicit risk analyses are a fairly new addition to the repertoire of intellectual enterprises. As a result, risk experts are only beginning to reach consensus on basic issues of terminology and methodology, such as how to define risk (see Section III). Their communications to the public reflect this instability. They are only beginning to express a sufficiently coherent perspective to help the public sort out the variety of meanings that “risk” could have. Under these circumstances some miscommunication may be inevitable. Studies (Slovic et al., 1979, 1980) have found that when expert risk assessors are asked to assess the risk of a technology on an undefined scale, they tend to respond with numbers that approximate the number of recorded or estimated fatalities in a typical year. When asked to estimate average year fatalities, laypeople produce fairly similar numbers. When asked to assess risk, however, laypeople produce quite different responses. These estimates seem to be an amalgam of their average-year fatality judgments, along with their appraisal of other features, such as a technology's catastrophic potential or how equitably its risks are distributed. These catastrophic potential judgments match those of the experts in some cases, but differ in others (e.g., nuclear power).\n\nOn semantic grounds, words can mean whatever a population group wants them to mean, as long as that usage is consistent and does not obscure important substantive differences. On policy grounds, the choice of a definition is a political question regarding what a society should be concerned about when dealing with risk. Whether we attach special importance to potential catastrophic losses of life or convert such losses to expected annual fatalities (i.e., multiply the potential loss by its annual probability of occurrence) and add them to the routine toll is a value question—as would be a decision to weight those routine losses equally rather than giving added weight to losses among the young (or among the nonbeneficiaries of a technology).\n\nFor other concepts that recur in risk discussions, the question of what they do or should mean is considerably murkier. It is often argued, for example, that different standards of stringency should apply to voluntarily and involuntarily incurred risks (e.g., Starr, 1969). Hence, for example, skiing could (or should) legitimately be a more hazardous enterprise than living below a major dam. Although there is general agreement among experts and laypeople about the voluntariness of food preservatives and skiing, other technologies are more problematic (Fischhoff et al., 1978b; Slovic et al., 1980). There is considerable disagreement within expert and lay groups in their ratings of the voluntariness of technologies such as prescription antibiotics, commercial aviation, handguns, and home appliances. These disagreements may reflect differences in the exposures considered; for example, use of commercial aviation may be voluntary for vacationers, but involuntary for certain business people (and scientists). Or, they may reflect disagreements about the nature of society or the meaning of the term. For example, each decision to ride in a car may be voluntarily undertaken and may, in principle, be foregone (i.e., by not traveling or by using an alternative mode of transportation); but in a modern industrial society, these alternatives may be somewhat fictitious. Indeed, in some social sets, skiing may be somewhat involuntary. Even if one makes a clearly volitional decision, some of the risks that one assumes may be indirectly and involuntarily imposed on one's family or the society that must pick up the pieces (e.g., pay for hospitalization due to skiing accidents).\n\nSuch definitional problems are not restricted to “social” terms such as “voluntary.” Even a technical term such as “exposure” may be consensually defined for some hazards (e.g., medical x rays), but not for others (e.g., handguns). In such cases, the disagreements within expert and lay groups may be as large as those between them. For orderly debate to be possible, one needs some generally accepted definition for each important term—or at least a good translating dictionary. For debate to be useful, one needs an explicit analysis of whether each concept, so defined, makes a sensible basis for policy. Once they have been repeated often enough, ideas such as the importance of voluntariness or catastrophic potential tend to assume a life of their own. It does not go without saying that society should set a double standard on the basis of voluntariness or catastrophic potential, however they are defined.\n\nLAYPEOPLE AND EXPERTS ARE SOLVING DIFFERENT PROBLEMS\n\nMany debates turn on whether the risk associated with a particular configuration of a technology is acceptable. Although these disagreements may be interpreted as reflecting conflicting social values or confused individual values, closer examination suggests that the acceptable-risk question itself may be poorly formulated (Otway and von Winterfeldt, 1982).\n\nTo be precise, one does not accept risks—one accepts options that entail some level of risk among their consequences. Whenever the decision-making process has considered benefits or other (nonrisk) costs, the most acceptable option need not be the one with the least risk. Indeed, one might choose (or accept) the option with the highest risk if it had enough compensating benefits. The attractiveness of an option depends on its full set of relevant positive and negative consequences (Fischhoff, Lichtenstein, et al., 1981).\n\nIn this light, the term “acceptable risk” is ill defined unless the options and consequences to be considered are specified. Once the options and consequences are specified, “acceptable risk” might be used to denote the risk associated with the most acceptable alternative. When using that designation, it is important to remember its context dependence. That is, people may disagree about the acceptability of risks not only because they disagree about what those consequences are (i.e., they have different risk estimates) or because they disagree about how to evaluate the consequences (i.e., they have different values), but also because they disagree about what consequences and options should be considered.\n\nSome familiar policy debates might be speculatively attributed, at least in part, to differing conceptions of what the set of possible options is. For example, saccharin (with its risks) may look unacceptable when compared with life without artificial sweeteners (one possible alternative option). Artificial sweeteners may, however, seem more palatable when the only alternative option considered is another sweetener that appears to be more costly and more risky. Or, nuclear power may seem acceptable when compared with alternative sources of generating electricity (with their risks and costs), but not so acceptable when aggressive conservation is added to the option set. Technical people from the nuclear industry seem to prefer the narrower problem definition, perhaps because they prefer to concentrate on the kinds of solutions most within their domain of expertise. Citizens involved in energy debates may feel themselves less narrowly bound; they may also be more comfortable with solutions, such as conservation, that require their kind of expertise (Bickerstaffe and Peace, 1980).\n\nPeople who agree about the facts and share common values may still disagree about the acceptability of a technology because they have different notions about which of those values are relevant to a particular decision. For example, all parties may think that equity is a good thing in general, without agreeing also that energy policy is the proper arena for resolving inequities. For example, some may feel that both those new inequities caused by a technology and those old ones endemic to a society are best handled separately (e.g., through the courts or with income policies).\n\nThus, when laypeople and experts disagree about the acceptability of a risk, one must always consider the possibility that they are addressing different problems, with different sets of alternatives or different sets of relevant consequences. Assuming that each group has a full understanding of the implications of its favored problem definition, the choice among definitions is a political question. Unless a forum is provided for debating problem definitions, these concerns may emerge in more indirect ways (Stallen, 1980).\n\nDEBATES OVER SUBSTANCE MAY DISGUISE BATTLES OVER FORM, AND VICE VERSA\n\nIn most political arenas, the conclusion of one battle often sets some of the initial conditions for its successor. Insofar as risk management decisions are shaping the economic and political future of a country, they are too important to be left to risk managers (Wynne, 1980). When people from outside the risk community enter risk battles, they may try to master the technical details or they may concentrate on monitoring and shaping the risk management process itself. The latter strategy may exploit their political expertise and keep them from being outclassed on technical issues. As a result, their concern about the magnitude of a risk may emerge in the form of carping about how it has been studied. They may be quick to criticize any risk assessment that does not have such features as eager peer review, ready acknowledgment of uncertainty, or easily accessible documentation. Even if they admit that these features are consonant with good research, scientists may resent being told by laypeople how to conduct their business even more than they resent being told by novices what various risks really are.\n\nLay activists' critiques of the risk assessment process may be no less irritating, but somewhat less readily ignored, when they focus on the way in which scientists' agendas are set. As veteran protagonists in hazard management struggles know, without scientific information it may be hard to arouse and sustain concern about an issue, to allay inappropriate fears, or to achieve enough certainty to justify action. However, information is, by and large, created only if someone has a (professional, political, or economic) use for it. Whether the cause is fads or finances, failure to study particular topics can thwart particular parties and may lead them to impugn the scientific process.\n\nAt the other extreme, debates about political processes may underlie disputes that are ostensibly about scientific facts. As mentioned earlier, the definition of an acceptable-risk problem circumscribes the set of relevant facts, consequences, and options. This agenda setting is often so powerful that a decision has effectively been made once the definition is set. Indeed, the official definition of a problem may preclude advancing one's point of view in a balanced fashion. Consider, for example, an individual who is opposed to increased energy consumption but is asked only about which energy source to adopt. The answers to these narrower questions provide a de facto answer to the broader question of growth. Such an individual may have little choice but to fight dirty, engaging in unconstructive criticism, poking holes in analyses supporting other positions, or ridiculing opponents who adhere to the more narrow definition. This apparently irrational behavior can be attributed to the rational pursuit of officially unreasonable objectives.\n\nAnother source of deliberately unreasonable behavior arises when participants in technology debates are in it for the fight. Many approaches to determining acceptable-risk levels (e.g., cost-benefit analyses) make the political-ideological assumption that our society is sufficiently cohesive and common-goaled that its problems can be resolved by reason and without struggle. Although such a “get on with business” orientation will be pleasing to many, it will not satisfy all. For those who do not believe that society is in a fine-tuning stage, a technique that fails to mobilize public consciousness and involvement has little to recommend it. Their strategy may involve a calculated attack on what they interpret as narrowly defined rationality (Campen, 1985).\n\nA variant on this theme occurs when participants will accept any process as long as it does not lead to a decision. Delay, per se, may be the goal of those who wish to preserve some status quo. These may be environmentalists who do not want a project to be begun or industrialists who do not want to be regulated. An effective way of thwarting practical decisions is to insist on the highest standards of scientific rigor.\n\nLAYPEOPLE AND EXPERTS DISAGREE ABOUT WHAT IS FEASIBLE\n\nLaypeople are often berated for misdirecting their efforts when they choose risk issues on which to focus their energies. However, a more careful diagnosis can often suggest several defensible strategies for setting priorities. For example, Zentner (1979) criticizes the public because its rate of concern about cancer (as measured by newspaper coverage) is increasing faster than the cancer rate. One reasonable explanation for this pattern is that people may believe that too little concern has been given to cancer in the past (e.g., our concern for acute hazards like traffic safety and infectious disease allowed cancer to creep up on us). A second is that people may realize that some forms of cancer are among the only major causes of death that experience increasing rates.\n\nSystematic observation and questioning are, of course, needed to tell whether these speculations are accurate (and whether the assumption of rationality holds in this particular case). False positives in divining people's underlying rationality can be as deleterious as false negatives. Erroneously assuming that laypeople understand an issue may deny them a needed education; erroneously assuming that they do not understand may deny them a needed hearing. Pending systematic studies, these error rates are likely to be determined largely by the rationalist or emotionalist cast of one's view of human nature.\n\nWithout solid evidence to the contrary, perhaps the most reasonable general assumption is that people's investment in problems depends on their feelings of personal efficacy. That is, they are unlikely to get involved unless they feel that they can make a difference, personally or collectively. In this light, their decision-making process depends on a concern that is known to influence other psychological processes: perceived feelings of control (Seligman, 1975). As a result, people will deliberately ignore major problems if they see no possibility of effective action. Here are some reasons why they might reject a charge of “misplaced priorities” when they neglect a hazard that poses a large risk:\n\nthe hazard is needed and has no substitutes;\n\nthe hazard is needed and has only riskier substitutes;\n\nno feasible scientific study can yield a sufficiently clear and incontrovertible signal to legitimate action;\n\nthe hazard is distributed naturally, and hence cannot be controlled;\n\nno one else is worried about the risk in question, and thus no one will heed messages of danger or be relieved by evidence of safety; and\n\nno one is empowered to or able to act on the basis of evidence about the risk.\n\nThus, the problems that actively concern people need not be those whose resolution they feel should rank highest on society's priorities. For example, one may acknowledge that the expected deaths from automobile accidents over the next century are far greater than those expected from nuclear power, and yet still be active only in fighting nuclear power out of the conviction, “Here, I can make a difference. This industry is on the ropes now. It's important to move in for the kill before it becomes as indispensable to American society as automobile transportation.”\n\nThus, differing priorities between experts and laypeople may not reflect disagreements about the size of risks, but differing opinions on what can be done about them. At times, the technical knowledge or can-do perspective of the experts may lead them to see a broader range of feasible actions. At other times, laypeople may feel that they can exercise the political clout needed to make some options happen, whereas the experts feel constrained to doing what they are paid for. In still other cases, both groups may be silent about very large problems because they see no options.\n\nLAYPEOPLE AND EXPERTS SEE THE FACTS DIFFERENTLY\n\nThere are, of course, situations in which disputes between laypeople and experts cannot be traced to disagreements about objectivity, terminology, problem definitions, process, or feasibility. Having eliminated those possibilities, one may assume the two groups really do see the facts of the matter differently. Here, it may be useful to distinguish between two types of situations: those in which laypeople have no source of information other than the experts, and those in which they do. The reasonableness of disagreements and the attendant policy implications look quite different in each case.\n\nHow might laypeople have no source of information other than the experts, and yet come to see the facts differently? One way is for the experts' messages not to get through intact, perhaps because: (1) The experts are unconcerned about disseminating their knowledge or hesitant to do so because of its tentative nature; (2) only a biased portion of the experts' information gets out, particularly when the selection has been influenced by those interested in creating a particular impression; (3) the message gets garbled in transmission, perhaps due to ill-informed or sensationalist journalists; or (4) the message gets garbled upon reception, either because it was poorly explicated or because recipients lacked the technical knowledge needed to understand the message (Friedman, 1981; Hanley, 1980; Nelkin, 1977). For example, Lord Rothschild (1978) has noted that the BBC does not like to trouble its listeners with the confidence intervals surrounding technical estimates.\n\nA second way of going astray is to misinterpret not the substance, but the process of the science. For example, unless an observer has reason to believe otherwise, it might seem sensible to assume that the amount of scientific attention paid to a risk is a good measure of its importance. Science can, however, be more complicated than that, with researchers going where the contracts, limelight, blue-ribbon panels, or juicy controversies are. In that light (and in hindsight), science may have done a disservice to public understanding by the excessive attention it paid to saccharin (“scientists wouldn't be so involved if this were not a major threat”).\n\nA second aspect of the scientific process that may cause confusion is its frequent disputatiousness. It may be all too easy for observers to feel that “if the experts can't agree, my guess may be as good as theirs” (Handler, 1980). Or, they may feel justified in picking the expert of their choice, perhaps on spurious grounds, such as assertiveness, eloquence, or political views. Indeed, it may seldom be the case that the distribution of lay opinions on an issue does not overlap some of the distribution of expert opinions. At the other extreme, laypeople may be"
    }
}
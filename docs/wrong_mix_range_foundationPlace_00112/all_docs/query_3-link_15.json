{
    "id": "wrong_mix_range_foundationPlace_00112_3",
    "rank": 15,
    "data": {
        "url": "https://informs-sim.org/wsc23papers/by_area.html",
        "read_more_link": "",
        "language": "en",
        "title": "WSC 2023 Proceedings",
        "top_image": "",
        "meta_img": "",
        "images": [],
        "movies": [
            "https://player.vimeo.com/video/894605270?h=aaf53c335d",
            "https://player.vimeo.com/video/912587418?h=4462675fc2",
            "https://player.vimeo.com/video/912590810?h=1dceac718e"
        ],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Plenary · Plenary\n\nOpening Plenary: Modeling for Energy Resilience: How DOE Uses Simulation to Model and Manage Everything from the Power Grid to the Strategic Petroleum Reserve\n\nChair: Bahar Biller (SAS Institute, Inc)\n\nModeling for Energy Resilience: How DOE Uses Simulation to Model and Manage Everything from the Power Grid to the Strategic Petroleum Reserve\n\nAnn Dunkin (Department of Energy)\n\nAbstract Abstract\n\nThe U.S. Department of Energy’s responsibilities run the gamut from managing the nuclear stockpile and the strategic petroleum reserve to running the power grid in 36 states to performing basic and applied research to protect national security, ensure stable power sector operations and accelerate the clean energy transition. Leveraging the power of DOE’s computing infrastructure, including the world’s fastest supercomputer, simulation models are used to accelerate advancements in nearly every field of research across DOE. Through a series of examples highlighting grid management, cybersecurity, cavern modeling and fundamental physical phenomena, this keynote will illuminate how DOE applies modeling and simulation to both research and operations.\n\npdf\n\nWelcome and Keynote from INFORMS on Vimeo.\n\nPlenary · Plenary\n\nTitans of Simulation: Resilience of Supply Chains and the Role of Simulation\n\nChair: John Shortle (George Mason University)\n\nResilience of Supply Chains and the Role of Simulation\n\nJohn Fowler (Arizona State University)\n\nAbstract Abstract\n\nSupply chain resilience refers to the capacity of a supply chain to proactively prepare for unforeseen events, effectively address disruptions, and bounce back from them while ensuring the sustained smooth operation of the supply chain at the preferred level of connectivity and management of its structure and functions. Recent disruptive events including the Covid-19 pandemic and the Russian invasion of Ukraine have caused an increased emphasis on supply chain resilience. In this presentation, we discuss strategies to prepare for, address, and bounce back from (potential) disruptions and the role that simulation can play in enhancing supply chain resilience.\n\npdf\n\nWSC 2023 Titan of Simulation - John Fowler from INFORMS on Vimeo.\n\nPlenary · Plenary\n\nTitans of Simulation: Ensuring Food Security under Climate Change: How Simulation Can Help in Making Agricultural Supply Chains More Resilient\n\nChair: John Shortle (George Mason University)\n\nEnsuring Food Security under Climate Change: How Simulation Can Help in Making Agricultural Supply Chains More Resilient\n\nEnver Yücesan (INSEAD)\n\nAbstract Abstract\n\nClimate change and the resulting increased frequency of unpredictable extreme weather events create new operational challenges for the commercial seed industry, which is a key pillar of a sustainable and secure global food supply. More specifically, extreme weather events translate into two main effects on agricultural production: Higher yield variability and lower expected yields. In recent years, extreme weather events already caused reductions in the yields of cereals, maize, and other staple crops. It is also projected that a warming of +2C (+4C) would increase the coefficient of variation of corn yield by 62% (192%) in six countries that collectively account for 73% of global production. In this presentation, we first examine how the increased likelihood of extreme weather events affects agricultural supply chains in terms of R&D, production planning, contracting, allocation, and storage decisions. We then discuss the key challenges associated with each stage and highlight how simulation can help address them under increased volatility.\n\npdf\n\nWSC 2023 Titan of Simulation - Enver Yucesan from INFORMS on Vimeo.\n\nAdvanced Tutorials\n\nTrack Coordinator - Advanced Tutorials: Henry Lam (Columbia University), Giulia Pedrielli (Arizona State University)\n\nTutorial · Advanced Tutorials\n\nScreening Simulated Systems for Optimization\n\nChair: Eunhye Song (Georgia Institute of Technology)\n\nJinbo Zhao (Texas A&M University), Javier Gatica (Pontificia Universidad Catolica de Chile), and David Eckman (Texas A&M University)\n\nAbstract Abstract\n\nScreening procedures for ranking and selection have received less attention than selection procedures, yet they serve as a cheap and powerful tool for decision making under uncertainty. Research on screening procedures has been less active in recent years, just as the advent of parallel computing has dramatically reshaped how selection procedures are designed and implemented. As a result, screening procedures used in modern practice continue to largely operate offline on fixed data. In this tutorial, we provide an overview of screening procedures with the goal of clarifying the current state of research and laying out opportunities for future development. We discuss several guarantees delivered by screening procedures and their role in different decision-making settings and investigate their impact on screening power and sampling efficiency in numerical experiments. We also study the implementation of screening procedures in parallel computing environments and how they can be combined with selection procedures.\n\npdf\n\nTutorial · Advanced Tutorials\n\nPractical Impact and Academia Are Not Antonyms\n\nChair: Russell R. Barton (Pennsylvania State University)\n\nShane Henderson (Cornell University)\n\nAbstract Abstract\n\nThis tutorial discusses principles and strategies for the interplay between applied work with organizations and an academic research agenda. I emphasize lessons I have learned through my own work and my own mistakes, with special focus on some high-stakes settings, including advising Cornell University’s response to the COVID-19 pandemic and work with the emergency services, among other applications.\n\npdf\n\nTutorial · Advanced Tutorials\n\nStatistical Limit Theorems in Distributionally Robust Optimization\n\nChair: Henry Lam (Columbia University)\n\nJose Blanchet (Stanford University) and Alexander Shapiro (Georgia Institute of Technology)\n\nAbstract Abstract\n\nThe goal of this paper is to develop a methodology for the systematic analysis of asymptotic statistical properties of data-driven DRO formulations based on their corresponding non-DRO counterparts. We illustrate our approach in various settings, including both phi-divergence and Wasserstein uncertainty sets. Different types of asymptotic behaviors are obtained depending on the rate at which the uncertainty radius decreases to zero as a function of the sample size and the geometry of the uncertainty sets.\n\npdf\n\nTutorial · Advanced Tutorials\n\nDigital Twins: Features, Models, and Services\n\nChair: Feng Ju (Arizona State University)\n\nAndrea Matta (Politecnico di Milano, Via La Masa 1) and Giovanni Lugaresi (KU Leuven)\n\nAbstract Abstract\n\nThis work provides an overview of digital twins, digital replicas of real entities conceived to support analysis, improvements, and optimal decisions. Specifically, it aims to better clarify what digital twins are by pointing out their main features, what they can do to support their related physical twins, and which models they use. An illustrative example together with a few selected application examples is used to better describe digital twins. A discussion on the actual challenges and research opportunities is also reported.\n\npdf\n\nTutorial · Advanced Tutorials\n\nBootstrapping and Batching for Output Analysis\n\nChair: Sara Shashaani (North Carolina State University)\n\nRaghu Pasupathy (Purdue University)\n\nAbstract Abstract\n\nWe review bootstrapping and batching as devices for statistical inference in simulation output analysis. Bootstrapping, discovered in the late 1970s and developed over the ensuing three decades, is widely held as being among the important scientific discoveries of the previous century due primarily to its facility for general statistical inference. By contrast, batching was introduced in the 1960s but was developed within the simulation community (in the 1980s) for the narrower contexts of variance parameter estimation and confidence interval construction. In recent years, however, there has been increasing realization that batching, much like bootstrapping, can be used also for general statistical inference, and that batching often compares favorably with bootstrapping in dependent data contexts. Bootstrapping and batching have tremendous applicability for uncertainty quantification in simulation, and are prime candidates for adoption in simulation software. We describe the general principles underlying bootstrapping and batching, outline guarantees, and discuss implementation.\n\npdf\n\nTutorial · Advanced Tutorials\n\nCoarse-Grained Simulations of DNA and RNA Systems with oxDNA and oxRNA Models: Tutorial\n\nChair: Wei Xie (Northeastern University)\n\nMatthew Sample, Michael Matthies, and Petr Sulc (Arizona State University)\n\nAbstract Abstract\n\nWe present a tutorial on setting-up the oxDNA coarse-grained model for simulations of DNA and RNA nanotechnology. The model is a popular tool used both by theorists and experimentalists to simulate nucleic acid systems both in biology and nanotechnology settings. The tutorial is aimed at new users asking \"Where should I start if I want to use oxDNA\". We assume no prior background in using the model. This tutorial shows basic examples that can get a novice user started with the model, and points the prospective user towards additional reading and online resources depending on which aspect of the model they are interested in pursuing.\n\npdf\n\nTutorial · Advanced Tutorials\n\nImportance Sampling Strategy for Heavy-tailed Systems with Catastrophe Principle\n\nChair: Henry Lam (Columbia University)\n\nImportance Sampling Strategy for Heavy-Tailed Systems with Catastrophe Principle\n\nXingyu Wang and Chang-Han Rhee (Northwestern University)\n\nAbstract Abstract\n\nLarge deviations theory has a long history of providing powerful machinery for designing efficient rare-event simulation techniques. However, traditional large deviations theory fails to provide useful bounds in heavy-tailed contexts, and designing efficient rare-event simulation algorithms for heavy-tailed systems has been considered challenging. Recent developments in the theory of heavy-tailed large deviations enable designing a strongly efficient importance sampling scheme that is universally applicable to a wide range of rare events. This tutorial aims to provide an accessible overview of the recent developments in the large deviations theory for heavy-tailed stochastic processes, which is followed by a detailed account of the design principle behind the strongly efficient importance sampling scheme for such processes. The implementations of the general principle are demonstrated through a few specific heavy-tailed rare events that arise in stochastic approximation, finance, and queueing theory contexts.\n\npdf\n\nAgent-based Simulation\n\nTrack Coordinator - Agent-Based Simulation: Andrew J. Collins (Old Dominion University), Chris Kuhlman (University of Virginia)\n\nTechnical Session · Agent-based Simulation\n\nMilitary and Homeland Security Agent-based Modeling\n\nChair: Berry Gerrits (University of Twente)\n\nSquashing Bugs and Improving Design: Using Data Farming to Support Verification and Validation of Military Agent-Based Simulations\n\nSusan K. Aros and Mary L. McDonald (Naval Postgraduate School)\n\nAbstract Abstract\n\nVerification and validation of complex agent-based human behavior simulation models is a challenging endeavor, particularly since a dearth of real-world data makes it impossible to use most traditional validation methods. Data farming techniques have stepped up to the challenge, proving to be a valuable tool for verification and validation of complex models. In this paper we demonstrate how data farming and analysis aids in the verification and validation of complex models by presenting specific examples pertaining to WRENCH, an agent-based simulation model that represents complex interactions between security forces and civilians during civil security stability operations. We first provide an overview of data farming and its relevance for verification and validation of military agent-based simulation models, then give an overview of WRENCH, and finally demonstrate with examples how we have used data farming to aid in the verification and validation of WRENCH.\n\npdf\n\nBeyond Accuracy: Cybersecurity Resilience Evaluation of Intrusion Detection System against DoS Attacks using Agent-based Simulation\n\nJeongkeun Shin, Geoffrey B. Dobson, L. Richard Carley, and Kathleen M. Carley (Carnegie Mellon University)\n\nAbstract Abstract\n\nMachine Learning has become increasingly popular in developing Intrusion Detection Systems (IDS) for cybersecurity. However, the focus has mainly been on achieving high detection accuracy rather than evaluating the impact on cybersecurity resiliency. In this paper, we use agent-based simulation to investigate the impact of different IDS algorithms on the cybersecurity resiliency of organizations under DoS attacks. Our simulation includes a server agent equipped with either Naive Bayes or SMO-based IDS, and a cybercriminal agent capable of launching different types of Denial of Service attacks. Our results suggest that the choice of IDS algorithm can significantly affect an organization’s cybersecurity resiliency against DoS attacks. Specifically, while SMO shows better overall accuracy on the KDD Cup 1999 dataset, Naive Bayes-based IDS proves more effective in practice due to its better-balanced detection rates across different types of DoS attacks. Our findings have important implications for improving organizations’ cybersecurity posture.\n\npdf\n\nUsing Evolutionary Model Discovery to Develop Robust Policies\n\nAlex Isherwood, Matthew Koehler, and David Slater (MITRE Corporation)\n\nAbstract Abstract\n\nAgent-based models can be a powerful tool for evaluating the impact of policy decisions on a population. However, analyses are traditionally beholden to one set of rules hypothesized at the conception of the model. Modelers make assumptions of agent behavior that are not necessarily governed by data and the actual behavior of the true population can vary. Evolutionary Model Discovery provides a solution to this problem by leveraging genetic algorithms and genetic programming to explore the plausible set of rules that can explain agent behavior. Here we describe an initial use of the EMD system to develop robust policies in a resource constrained environment. In this instance, we extend the NetLogo implementation of the Epstein Rebellion model model of civil violence as a sample problem. We use the EMD framework to generate plausible populations and then develop policy responses for the government that are robust across the plausible populations.\n\npdf\n\nTechnical Session · Agent-based Simulation\n\nHealthcare Agent-based Modeling\n\nChair: Xueying Liu (Virginia Polytechnic Institute and State University)\n\nAn Iterative Analysis Method Using Causal Discovery Algorithms to Enhance ABM as a Policy Tool\n\nShuang Chang, Takashi Kato, Yusuke Koyanagi, Kento Uemura, and Koji Maruhashi (Fujitsu Laboratories Ltd.)\n\nAbstract Abstract\n\nAgent-based modelling (ABM) is becoming a popular policy tool by modelling the reasoning processes and interactive behaviors of individuals against external environments. However, the presence of heterogeneous agents, non-linear interactions and complex emergent patterns raised by even simple behavior rules pose challenges in the model explanation process. In this work, we propose a novel iterative analysis method that leverages causal discovery algorithms to facilitate policy formulation and evaluation based on a causal understanding of the model. It strengthens the explanation power of ABM by elucidating causal relations among modelled components. We applied the method to an agent-based simulator that models passengers' routing behaviors in a virtual airport terminal. By discovering the causal relations among passengers' goals, actions, and an airport terminal environment under different COVID-19 regulations, we showed that the method can inform more effective indirect-control policies leading to positive passenger experiences, compared with a conventional ABM analysis method.\n\npdf\n\nA Review of Agent-based Modeling Applications in Substance Abuse Policy Research\n\nXiang Zhong (University of Florida), Xuanjing Li (Tsinghua University), and Samantha Mangoni (University of Florida)\n\nAbstract Abstract\n\nThis study provides a systematic review of existing studies that used agent-based modeling (ABM) to inform substance abuse policies and identifies future research directions. The detailed review included 20 articles, among which, tobacco, alcohol, cannabis, opioids, and heroin substance abuse were studied. These studies examined substance abuse interventions and the associations between substance use and social behavior, such as peer interaction and selection. Effective interventions included retailer density reduction policies, restriction of trading hours of licensed venues, ecstasy pill-testing and passive-alert detection dogs by police at public venues, and a mass-media drug prevention education policy. ABM can capture the dynamic interactions among and between agents and environments, making it appropriate to model complex substance abuse behaviors. Limitations in current studies include a lack of ABM validation efforts and generalizable data. Future studies should use generalizable and abundant information to inform their ABM, as well as have an explicit validation method.\n\npdf\n\nSupporting Emergency Department Risk Mitigation with a Modular and Reusable Agent-Based Simulation Infrastructure\n\nThomas Godfrey (King's College London); Rahul Batra, Sam Douthwaite, and Jonathan Edgeworth (Guy's and St Thomas' NHS Foundation Trust); Matthew Edwards (King's College Hospital NHS Foundation Trust); Simon Miles (Aerogility Ltd); and Steffen Zschaler (King's College London)\n\nAbstract Abstract\n\nFor emergency departments (EDs) to maintain sustainable care of patients, hospital management must continually explore potential interventions to clinical practice. Agent-based modelling (ABM) can be a valuable tool to support this planning in a controlled environment. Existing approaches to ABM development are best suited for one-off models. However, conditions in EDs can change frequently, making the use of one-off models infeasible. Decision-makers must be able to trust simulations appropriately for them to be effective in intervention exploration. Domain-specific modelling languages (DSMLs) can address these challenges by offering a reusable library of appropriately-abstract, domain-familiar, modelling concepts across case studies and automatic translation of these concepts into executable models. In this paper we present a DSML to support repeated modelling exercises in the ED domain and illustrate the use and reuse of this DSML across two concrete case studies in London-based NHS emergency departments.\n\npdf\n\nTechnical Session · Agent-based Simulation\n\nSustainable Transportation Agent-based Modeling\n\nChair: Xiang Zhong (University of Florida)\n\nSimulating Interaction Behaviors in Bi-directional Shared Corridor with Real Case Study\n\nYun-Pang Flötteröd, Jakob Erdmann, and Daniel Krajzewicz (German Aerospace Center (DLR)) and Johan Olstam (The Swedish National Road and Transport Research Institute)\n\nAbstract Abstract\n\nMicroscopic traffic simulation tools are able to evaluate possible impacts induced by automated shuttles under various conditions. However, automated shuttles operate more and more often in shared space areas and few microscopic traffic simulation tools are able to handle networks with shared space infrastructure. Interaction behaviors between road users and automated shuttles are addressed only seldom as well. In this paper, we propose the concept of bi-directional edges in the open source microscopic traffic simulation suite SUMO to simulate road users’ interactions in a bi-directional shared-space corridor. A case study, where automated shuttles and cyclists share the bike path, and the related data collection were conducted to examine the performance of the proposed concept and understand the usage of the shared corridor. The simulation results are promising. Further refinement of the proposed concept is planned for properly reflecting complex interaction behaviors among diverse road users, and their surrounding environment.\n\npdf\n\nRebalancing Integrated, Demand-responsive Passenger and Freight Transport – An Agent-based Simulation Approach\n\nJohannes Staritz, Julia Kütemeier, Helen Sand, Christoph von Viebahn, and Maylin Wartenberg (Hochschule Hannover)\n\nAbstract Abstract\n\nIntegrated, demand-responsive passenger and freight transport (IDRT) potentially provides flexibility and higher service frequency in areas of low demand due to economies of scale, while reducing negative traffic-related externalities such as pollutant emissions, noise emissions or accidents. However, to allow for efficient operations in terms of minimum travel distances, short customer waiting times, and high vehicle utilization rates, IDRT requires effective rebalancing strategies that balance supply and demand capacities by strategically positioning vehicle resources in the operational area. Therefore, we propose a rebalancing strategy for IDRT and measure its effectiveness through an agent-based simulation model. To evaluate our approach, we compare the rebalanced IDRT with a static scenario with backhauls to a central depot. Our results indicate that the proposed rebalancing approach can outperform a system without rebalancing by up to 15.1% in terms of total fleet kilometers and 30% in terms of passenger waiting time.\n\npdf\n\nA Simulation Model for Bio-Inspired Charging Strategies for Electric Vehicles in Industrial Areas\n\nBerry Gerrits and Martijn Mes (University of Twente) and Robert Andringa (Distribute)\n\nAbstract Abstract\n\nThis paper presents an open-source agent-based simulation model to study bio-inspired charging policies for local sustainable energy systems in an industrial setting where electric vehicles (EVs) perform transportation jobs. Within this context, we focus on a system that allows to control the charging-schemes of individual EVs. To this end, we develop an agent-based simulation model in NetLogo. We present and implement a bio-inspired approach based on the foraging behavior of honeybees and our approach results in simple, yet effective decision-making logic. Our approach provides the necessary parameters to control and balance sustainable energy systems in terms of EV productivity and the consumption of locally generated energy. Our simulation results look promising: the balance between EV productivity and the use of sustainable energy can be efficiently tweaked in a predictable manner using the parameters and thresholds of the model, yielding close-to-optimal performance.\n\npdf\n\nTechnical Session · Agent-based Simulation\n\nGames and Agent-based Modeling\n\nChair: Haibei Zhu (J.P. Morgan)\n\nModeling Reactive Game Agents Using the Cell-DEVS Modeling Formalism\n\nAlvi Jawad, Cristina Ruiz-Martín, and Gabriel Wainer (Carleton University)\n\nAbstract Abstract\n\nIntelligent game agents are a vital part of modern games as they add life, story, and immersion to the game environment. The requests in the gaming industry for more realism have made intelligent agents more important than ever before. Modeling and simulation of game agents and their surrounding environment provide an alternate setting to study dynamic agent behavior before integration into the game engine. The Cell-DEVS formalism, an extension of Cellular Automata, allows modeling such behaviors using the rigorously formalized Discrete Event Systems Specification (DEVS) formalism. In this paper, we explain how to model and test reactive game agents using the Cell-DEVS formalism and the CD++ toolkit. To analyze the dynamic behavior of such agents, we perform several experiments in varying system configurations. Our experimental results confirm the versatility of Cell-DEVS and the functionalities in the CD++ toolkit to model comfort-driven, exploratory, and desire-driven game agents.\n\npdf\n\nA Calibration Model for Bot-Like Behaviors in Agent-Based Anagram Game Simulation\n\nXueying Liu, Zhihao Hu, and Xinwei Deng (Virginia Tech) and Chris Kuhlman (University of Virginia)\n\nAbstract Abstract\n\nExperiments that are games played among a network of players are widely used to study human behavior. Furthermore, bots or intelligent systems can be used in these games to produce contexts that elicit particular types of human responses. Bot behaviors could be specified solely based on experimental data. In this work, we take a different perspective, called the Probability Calibration (PC) approach, to simulate networked group anagram games with certain players having bot-like behaviors. The proposed method starts with data-driven models and calibrates in principled ways the parameters that alter player behaviors. It can alter the performance of each type of agent (e.g., bot) in group anagram games. Further, statistical methods are used to test whether the PC models produce results that are statistically different from those of the original models. Case studies demonstrate the merits of the proposed method.\n\npdf\n\nFeature Importance for Uncertainty Quantification in Agent-based Modeling\n\nGayane Grigoryan and Andrew J. Collins (Old Dominion University)\n\nAbstract Abstract\n\nSimulation models are subject to uncertainty and sensitivity, meaning that even small variations of input can cause considerable fluctuations in the output results. Consequently, this can amplify the uncertainty associated with the simulation, thereby limiting the confidence one can have in its outcomes. To mitigate these effects, this paper suggests using a cooperative game theory-based feature importance method, which can identify uncertainty in a dataset, and provide additional insights that could be used in the development or analysis of a simulation model. A predator-prey scenario was considered, demonstrating its usefulness in identifying important parameters or features. By identifying the most influential parameters or features, this approach can help improve the accuracy, explainability, and reliability of simulation models as well as other models with highly variable input parameters.\n\npdf\n\nTechnical Session · Agent-based Simulation\n\nTransportation Agent-based Modeling\n\nChair: Kshama Dwarakanath (J.P. Morgan AI Research)\n\nA Simulation-Based Method for Analyzing Supply Chain Vulnerability Under Pandemic: A Special Focus on the Covid-19\n\nXinglu Xu and Bochi Liu (Dalian University of Technology) and Weihong Grace Guo (Rutgers, The State University of New Jersey)\n\nAbstract Abstract\n\nThis paper develops a simulation-based quantitative method to investigate the joint impact of multiple risks on the supply chain system during the pandemic. A hybrid simulation method that combines the susceptible-infected-recovered (SIR) model and the agent-based simulation method is proposed to simulate the risk propagation along the supply chain and the interactions between distribution centers and retailers. By analyzing the results of scenarios with different interventions under COVID-19, results show that the impact of interventions is diminishing along the supply chain. For intervention deployment, adding testing capacity is of great importance. For stakeholder management strategies, diversifying the upstream partners is helpful. Against the backdrop of a multi-wave global pandemic, this paper takes the COVID-19 pandemic as an example to provide a paradigm for modeling the risk propagation in supply chain systems. Also, the study demonstrates how to estimate possible time-varying risk scenarios in face of the data shortage challenge.\n\npdf\n\nSystem Simulation and Machine Learning-Based Maintenance Optimization for an Inland Waterway Transportation System\n\nMaryam Aghamohammadghasem, Jose Azucena, Farid Hashemian, Haitao Liao, Shengfan Zhang, and Heather Nachtmann (University of Arkansas)\n\nAbstract Abstract\n\nTo continue operations of the inland waterway transportation system (IWTS), the interconnected infrastructure, such as locks and dam systems, must remain in good operating condition. However, as the IWTS ages, unexpected disruptions increase, causing significant transportation delays and economic losses. To evaluate the impacts of IWTS disruptions, a Python-enhanced NetLogo simulation tool is developed, where the extreme natural events are considered and represented by a spatiotemporal model. Utilizing this tool, optimal maintenance strategies that maximize cargo throughput on the IWTS are determined via deep reinforcement learning. A case study of the lower Mississippi River system and the McClellan-Kerr Arkansas River Navigation System is conducted to illustrate the capability of the developed simulation and machine learning-based method for IWTS maintenance optimization.\n\npdf\n\nFour Years of Not-Using a Simulator: The Agent-Based Template\n\nDominik Brunmeir and Martin Bicher (TU Wien); Matthias Rößler, Christoph Urach, Claire Rippinger, and Matthias Wastian (dwh GmbH); and Niki Popper (TU Wien)\n\nAbstract Abstract\n\nWith steadily increasing performance of computers, agent-based modeling has evolved from an analysis method for qualitative phenomena to strategy for quantitative decision support. With this orientation, however, the modeler faces new challenges during implementation. In particular, an appropriate simulation tool must feature the combination of data and model flexibility, process reproducibility, performance and portability. While existing simulators often do not sufficiently cover these features, it is also not sustainable to generally implement models from scratch. In this work, we want to present the idea of simulation templates as a compromise between the two strategies. We show, on the example of our Agent-Based Template and two use cases, the importance of the described challenges and how the simulation template concept supports solving them. We aim to generally promote the idea of developing a customized template, which, as a layer between simulator and from-the-scratch implementation, combines the advantages of both approaches.\n\npdf\n\nTechnical Session · Agent-based Simulation\n\nAgent-based Modeling Design\n\nChair: Gayane Grigoryan (Old Dominion University)\n\nTransparency as Delayed Observability in Multi-Agent Systems\n\nKshama Dwarakanath and Svitlana Vyetrenko (J.P. Morgan AI Research), Toks Oyebode (J.P. Morgan Regulatory Affairs), and Tucker Balch (J.P. Morgan AI Research)\n\nAbstract Abstract\n\nIs transparency always beneficial in complex systems such as traffic networks and stock markets? How is transparency defined in multi-agent systems, and what is its optimal degree at which social welfare is highest? We take an agent-based view to define transparency (or its lacking) as delay in agent observability of environment states, and utilize simulations to analyze the impact of delay on social welfare. To model the adaptation of agent strategies with varying delays, we model agents as learners maximizing the same objectives under different delays in a simulated environment. Focusing on two agent types - constrained and unconstrained, we use multi-agent reinforcement learning to evaluate the impact of delay on agent outcomes and social welfare. Empirical demonstration of our framework in simulated financial markets shows opposing trends in outcomes of the constrained and unconstrained agents with delay, with an optimal partial transparency regime at which social welfare is maximal.\n\npdf\n\nOnce Burned, Twice Shy? The Effect of Stock Market Bubbles on Traders that Learn by Experience\n\nHaibei Zhu and Svitlana Vyetrenko (J.P. Morgan), Serafin Grundl (Federal Reserve Board), David Byrd (Bowdoin College), and Kshama Dwarakanath and Tucker Balch (J.P. Morgan)\n\nAbstract Abstract\n\nWe study how experience with asset price bubbles changes the trading strategies of reinforcement learning (RL) traders and ask whether the change in trading strategies helps to prevent future bubbles. We train the RL traders in a multi-agent market simulation platform, ABIDES, and compare the strategies of traders trained with and without bubble experience. We find that RL traders without bubble experience behave like short-term momentum traders, whereas traders with bubble experience behave like value traders. Therefore, RL traders without bubble experience amplify bubbles, whereas RL traders with bubble experience tend to suppress and sometimes prevent them. This finding suggests that learning from experience is a mechanism for a boom and bust cycle where the experience of a collapsing bubble makes future bubbles less likely for a period of time until the memory fades and bubbles become more likely to form again.\n\npdf\n\nMatchmaking in Crowd-shipping Platforms: The Effects of Mediator Control\n\nPreetam Kulkarni and Caroline C. Krejci (University of Texas at Arlington)\n\nAbstract Abstract\n\nA critical design decision for crowdsourcing platforms is the degree to which the platform mediator controls participant interactions. Platforms having a centralized model of mediation optimize for convenience, speed, and security in participant interactions, while platforms operating under decentralized control require greater user effort but offer them greater control and agency. The research described in this paper is a preliminary study using agent-based modeling to evaluate and compare the performance of crowd-shipping platforms with centralized/decentralized control over matchmaking of carriers and senders. Results indicate that centralized matchmaking protects the platform from premature failure when initial carrier/sender participation is low. Furthermore, when the platform’s assignment algorithm is designed to maximize platform revenue, subject to meeting carriers’ profit expectations, centralized matchmaking will tend to outperform decentralized matchmaking for both the mediator and the carriers.\n\npdf\n\nAnalysis Methodology\n\nTrack Coordinator - Analysis Methodology: Ben Feng (University of Waterloo), Sara Shashaani (North Carolina State University)\n\nTechnical Session · Analysis Methodology\n\nSimulation in Queueing Systems\n\nChair: Jun Luo (Shanghai Jiao Tong University)\n\nReal-Time Estimations for the Waiting-Time Distribution in Time-Varying Queues\n\nKurtis Konrad and Yunan Liu (North Carolina State University)\n\nAbstract Abstract\n\nCustomers’ waiting times are the most commonly used performance data to measure the quality of service in service systems such as call centers and healthcare. Unlike stationary queueing models where customers’ waiting times are statistically similar, the prediction of waiting times is far less straightforward in time-varying queues having nonstationary demand (i.e., arrival rate) and supply (i.e., number of servers). In this paper, we develop a novel methodology for more accurately computing the wait time distribution in a time-varying queueing system. We design extensive simulation experiments to evaluate our prediction methods. In addition, we discover that the waiting-time prediction is highly sensitive to the work-releasing policy of the staffing plan, i.e., the rule under which the number of servers changes in time.\n\npdf\n\nAchieving Stable Service-Level Targets in Time-Varying Queueing Systems: A Simulation-Based Offline Learning Staffing Algorithm\n\nKurtis Konrad and Yunan Liu (North Carolina State University)\n\nAbstract Abstract\n\nIn this paper, we develop a new staffing algorithm for achieving stable service-level targets in queues with time-varying arrivals. Specifically, we aim to stabilize the tail probability of delay, which is the probability that the waiting time exceeds a designated target τ > 0. We integrate reinforcement learning into the decision making in queueing models; our new method recursively evolve the staffing decision by alternating between two phases: (i) we generate simulated queueing data by operating the system under the present staffing function (exploration), and (ii) we utilize the newly generated data to devise improved staffing decision (exploitation). We demonstrate the effectiveness of our new method using various numerical examples.\n\npdf\n\nEstimating Spline-based Nonhomogeneous Poisson Intensities Using Constrained Quadratic Programming\n\nSiqi Chen, Jing Yang (Sunny) Xi, and Wai Kin (Victor) Chan (Tsinghua-Berkeley Shenzhen Institute, Shenzhen International Graduate School, Tsinghua University)\n\nAbstract Abstract\n\nThis paper estimates the intensity function of a nonhomogeneous Poisson process (NHPP) using a spline-based method with constrained quadratic programming (CQP). Based on the property of B-splines, we transform the estimation problem into an optimization problem and apply CQP to obtain the estimated intensity function with low computational expense. Numerical experiments are conducted to verify the performance of our method. In addition, the impacts of the number of intervals from event-count data and the number of knots in B-splines are also discussed to explore the properties of spline-based models.\n\npdf\n\nTechnical Session · Analysis Methodology\n\nAdvances in Rare-event Simulation\n\nChair: Linyun He (Georgia Institute of Technology)\n\nEfficiency of Estimating Functions of Means in Rare-Event Contexts\n\nMarvin Nakayama (New Jersey Institute of Technology) and Bruno Tuffin (INRIA, University of Rennes)\n\nAbstract Abstract\n\nWhen estimating a function of means, where some but not necessarily all of them correspond to rare events, we provide conditions under which having efficient estimators of each individual mean leads to an efficient estimator of the function of the means. We illustrate this setting through several examples, and numerical results complement the theory.\n\npdf\n\nConditional Importance Sampling for Convex Rare-Event Sets\n\nDohyun Ahn and Lewen Zheng (The Chinese University of Hong Kong)\n\nAbstract Abstract\n\nThis paper studies the efficient estimation of expectations defined on convex rare-event sets using importance sampling. Classical importance sampling methods often neglect the geometry of the target set, resulting in a significant number of samples falling outside the target set. This can lead to an increase in the relative error of the estimator as the target event becomes rarer. To address this issue, we develop a conditional importance sampling scheme that achieves bounded relative error by changing the sampling distribution to ensure that a majority of samples lie inside the target set. The proposed method is easy to implement and significantly outperforms the existing approaches in various numerical experiments.\n\npdf\n\nCurse of Dimensionality in Rare-Event Simulation\n\nBest Contributed Theoretical Paper - Finalist\n\nYuanlu Bai, Antonius B. Dieker, and Henry Lam (Columbia University)\n\nAbstract Abstract\n\nIn rare-event simulation, importance sampling (IS) is widely used to improve the efficiency of probability estimation. Asymptotic optimality is a common efficiency criterion, which requires that the relative error of the estimator only grows subexponentially in the rarity parameter. Most studies, however, consider low-dimensional problems and the effect of dimensionality is seldom analyzed. Motivated by recent AI-related applications, we take a first step towards high-dimensional rare-event simulation and demonstrate that for very simple examples, IS proposals that utilize exponential tilting, arguably the most common IS approach, can suffer from the \"curse of dimensionality\". That is, while the growth rate of the relative error is polynomial in the rarity parameter thus leading to asymptotic optimality, the degree of the polynomial depends on the problem dimensionality. Therefore, when the dimension is high, the relative error can be huge even in the rarity parameter regime where IS is conventionally believed to work well.\n\npdf\n\nTechnical Session · Analysis Methodology\n\nAdvances in Importance Sampling\n\nChair: Dohyun Ahn (The Chinese University of Hong Kong)\n\nEfficient Input Uncertainty Quantification for Regenerative Simulation\n\nBest Contributed Theoretical Paper - Finalist\n\nLinyun He (Georgia Institute of Technology), Mingbin Ben Feng (University of Waterloo), and Eunhye Song (Georgia Institute of Technology)\n\nAbstract Abstract\n\nThe initial bias in steady-state simulation can be characterized as the bias of a ratio estimator if the simulation model has a regenerative structure. This work tackles input uncertainty quantification for a regenerative simulation model when its input distributions are estimated from finite data. Our aim is to construct a bootstrap-based confidence interval (CI) for the true simulation output mean performance that provides a correct coverage with significantly less computational cost than the traditional methods. Exploiting the regenerative structure, we propose a $k$-nearest neighbor ($k$NN) ratio estimator for the steady-state performance measure at each set of bootstrapped input models and construct a bootstrap CI from the computed estimators. Asymptotically optimal choices for $k$ and bootstrap sample size are discussed. We further improve the CI by combining the $k$NN and likelihood ratio methods. We empirically compare the efficiency of the proposed estimators with the standard estimator using queueing examples.\n\npdf\n\nRobust Importance Sampling for Stochastic Simulations with Uncertain Parametric Input Model\n\nSeung Min Baik and Young Myoung Ko (Pohang University of Science and Technology (POSTECH)) and Eunshin Byon (University of Michigan)\n\nAbstract Abstract\n\nIn stochastic simulations, input model uncertainty may significantly impact output estimation accuracy. Although variance reduction techniques alleviate the computational burden, input model uncertainty remains unaddressed. Among several variance reduction techniques, we propose a robust version of the importance sampling method. We formulate a min-max optimization problem for finding a robust sampling density for simulation inputs considering a parametric uncertainty set that represents candidates of the true input distribution. We utilize the Bayesian optimization framework for solving the outer problem and the barrier method for tackling the inner problem. By incorporating input model uncertainty in the sampling stage, our method effectively allocates simulation effort to improve estimation robustness. Numerical experiments demonstrate the advantages of the proposed method over a benchmark model assuming a precisely known input model. Our approach produces more accurate output estimation (i.e., an estimator with lower variance), highlighting its robustness and potential applicability in a variety of situations.\n\npdf\n\nGeneralized Importance Sampling for Nested Simulation\n\nQingyuan Chen (Cornell University) and Mingbin Ben Feng (University of Waterloo)\n\nAbstract Abstract\n\nImportance sampling (IS) is a classical variance reduction technique. Under mild conditions, an IS estimator is unbiased, so one often seeks variance-minimizing optimal sampling distribution. IS has remarkable success in many applications such as engineering, operations research, and finance. In some applications such as enterprise risk management and input uncertainty quantification, complex simulation designs such as nested simulation arises naturally: The outer-level simulation generates a set of risk factors, i.e., the scenarios, which are used as inputs for inner-level simulations. Nested simulation leads to wasteful use of computations as inner simulation outputs in each scenario are isolated from other scenarios. In this study, we propose, analyze, and test a generalized importance sampling technique for nested simulation. Our generalized IS approach reuses one set of inner simulation outputs across different outer scenarios. Numerical experiments show that our proposal is orders of magnitudes more efficient than the standard procedure.\n\npdf\n\nTechnical Session · Analysis Methodology\n\nOutput Analysis\n\nChair: Sara Shashaani (North Carolina State University)\n\nBootstrap Confidence Intervals for Simulation Output Parameters\n\nRussell R. Barton (The Pennsylvania State University) and Luke A. Rhodes-Leader (Lancaster University)\n\nAbstract Abstract\n\nBootstrapping has been used to characterize the impact on discrete-event simulation output arising from input model uncertainty for thirty years. The distribution of simulation output statistics can be very non-normal, especially in simulation of heavily loaded queueing systems, and systems operating at a near optimal value of the output measure. This paper presents issues facing simulationists in using bootstrapping to provide confidence intervals for parameters related to the distribution of simulation output statistics, and identifies appropriate alternatives to the basic and percentile bootstrap methods. Both input uncertainty and ordinary output analysis settings are included.\n\npdf\n\nOptimal Batching under Computation Budget\n\nShengyi He and Henry Lam (Columbia University)\n\nAbstract Abstract\n\nBatching methods operate by dividing data into batches and conducting inference by aggregating estimates from batched data. These methods have been used extensively in simulation output analysis and, among other strengths, an advantage is the light computation cost when using a small number of batches. However, under computation budget constraints, it is open to our knowledge which batching approach among the range of alternatives is statistically optimal, which is important in guiding procedural configuration. We show that standard batching, but also certain carefully designed schemes using uneven-size batches or overlapping batches, are large-sample optimal in the sense of so-called uniformly most accurate unbiasedness from a dual view of hypothesis testing.\n\npdf\n\nConfidence Intervals for Randomized Quasi-Monte Carlo Estimators\n\nPierre L'Ecuyer (Université de Montréal), Marvin K. Nakayama (New Jersey Institute of Technology), Art B. Owen (Stanford University), and Bruno Tuffin (Inria)\n\nAbstract Abstract\n\nRandomized Quasi-Monte Carlo (RQMC) methods provide unbiased estimators whose variance often converges at a faster rate than standard Monte Carlo as a function of the sample size. However, computing valid confidence intervals is challenging because the observations from a single randomization are dependent and the central limit theorem does not ordinarily apply. A natural solution is to replicate the RQMC process independently a small number of times to estimate the variance and use a standard confidence interval based on a normal or Student t distribution. We investigate the standard Student t approach and two bootstrap methods for getting nonparametic confidence intervals for the mean using a modest number of replicates. Our main conclusion is that intervals based on the Student t distribution are more reliable than even the bootstrap t method on the integration problems arising from RQMC.\n\npdf\n\nTechnical Session · Analysis Methodology\n\nSteady-state Simulation\n\nChair: David Goldsman (Georgia Institute of Technology)\n\nA Fixed-Sample-Size Method for Estimating Steady-State Quantiles\n\nAthanasios Lolos, Christos Alexopoulos, and David Goldsman (Georgia Institute of Technology); Kemal Dinçer Dingeç (Gebze Technical University); Anup C. Mokashi (Memorial Sloan Kettering Cancer Center); and James R. Wilson (North Carolina State University)\n\nAbstract Abstract\n\nWe propose FQUEST, a fully automated fixed-sample-size procedure for computing confidence intervals (CIs) for steady-state quantiles. The user provides a (simulation-generated) dataset of arbitrary size and specifies the required quantile and nominal coverage probability of the anticipated CI. FQUEST incorporates the simulation analysis methods of batching, standardized time series (STS), and sectioning. Preliminary experimentation with the waiting-time process in a congested M/M/1 queueing system showed that FQUEST performed well by delivering CIs with estimated coverage probability close to the nominal level, even in unfavorable circumstances where the sample sizes were inadequate. In the latter cases and for very small samples for steady-state quantile estimation, the close conformance of the CI coverage probability typically came at the expense of loose CI precision.\n\npdf\n\nCOSIMLA with General Regeneration Set to Compute Markov Chain Stationary Expectations\n\nPeter W. Glynn (Stanford University) and Zeyu Zheng (University of California Berkeley)\n\nAbstract Abstract\n\nWe extend the COSIMLA approach (short for \"COmbined SIMulation and Linear Algebra'') recently developed in Zheng, Infanger, and Glynn (2022) to compute stationary expectations for Markov chains with large or infinite discrete state space. Our work follows the idea of combing the best of linear algebra and simulation---using linear algebra to compute the \"center'' of the state space and using simulation to compute the contributions from outside of the \"center''. Different from Zheng, Infanger, and Glynn (2022) that needed to fix a single regeneration state, our work develops a new method that allows the use of a flexible regeneration set with a finite number of states. We show that this new method allows more efficient computation for the COSIMLA approach.\n\npdf\n\nFast Approximation to Discrete-Event Simulation of Markovian Queueing Networks\n\nTan Wang (Fudan University), Yingda Song (Shanghai Jiaotong University), and Jeff Hong (Fudan University)\n\nAbstract Abstract\n\nSimulation of queueing networks is generally carried out by discrete-event simulation (DES), in which the simulation time is driven by the occurrence of the next event. However, for large-scale queueing networks, especially when the network is very busy, keeping track of all events is computationally inefficient. Moreover, as the traditional DES is inherently sequential, it is difficult to harness the capability of parallel computing. In this paper, we propose a parallel fast simulation approximation framework for large-scale Markovian queueing networks, where the simulation horizon is discretized into small time intervals and the system state is updated according to the events happening in each time interval. The computational complexity analysis demonstrates that our method is more efficient for large-scale networks compared with traditional DES. We also show its relative error converges to zero. The experimental results show that our framework can be much faster than the state-of-the-art DES tools.\n\npdf\n\nTechnical Session · Analysis Methodology\n\nInnovative Applications of Simulation Methodology\n\nChair: Hua Zheng (Northeastern University)\n\nStructure-function Dynamics Hybrid Modeling: RNA Degradation\n\nHua Zheng, Wei Xie, Paul C. Whitford, Ailun Wang, Chunsheng Fang, and Wandi Xu (Northeastern University)\n\nAbstract Abstract\n\nRNA structure and functional dynamics play fundamental roles in controlling biological systems. Molecular dynamics simulation, which can characterize interactions at an atomistic level, can advance the understanding on new drug discovery, manufacturing, and delivery mechanisms. However, it is computationally unattainable to support the development of a digital twin for enzymatic reaction network mechanism learning, and end-to-end bioprocess design and control. Thus, we create a hybrid (\"mechanistic + machine learning\") model characterizing the interdependence of RNA structure and functional dynamics from atomistic to macroscopic levels. To assess the proposed modeling strategy, we consider RNA degradation which is a critical process in cellular biology that affects gene expression. The empirical study on RNA lifetime prediction demonstrates the promising performance of the proposed multi-scale bioprocess hybrid modeling strategy.\n\npdf\n\nTracking and Detecting Systematic Errors in Digital Twins\n\nLuke A. Rhodes-Leader (Lancaster University) and Barry L. Nelson (Northwestern University)\n\nAbstract Abstract\n\nDigital Twins (DTs) have immense promise for exploiting the power of computer simulation to control large-scale real-world systems. The key idea is to evaluate or optimize decisions using the DT, and then implement them in the real-world system. Even with best practices, the DT and the real-world system may become misaligned over time. In this paper we provide a statistical method to detect such misalignment even though both the simulation and the real-world system are inherently stochastic. An empirical evaluation and a realistic illustration are provided.\n\npdf\n\nSensitivity Analysis for Stopping Criteria with Application to Organ Transplantations\n\nXingyu Ren, Michael Fu, and Steven Marcus (University of Maryland)\n\nAbstract Abstract\n\nWe consider a stopping problem and its application to the decision-making process regarding the optimal timing of organ transplantation for individual patients. At each decision period, the patient state is inspected and a decision is made whether to transplant. If the organ is transplanted, the process terminates; otherwise, the process continues until a transplant happens or the patient dies. Under suitable conditions, we show that there exists a control limit optimal policy. We propose a smoothed perturbation analysis (SPA) estimator for the gradient of the total expected discounted reward with respect to the control limit. Moreover, we show that the SPA estimator is asymptotically unbiased.\n\npdf\n\nTechnical Session · Analysis Methodology\n\nDesign of Experiments and Screening\n\nChair: Zeyu Zheng (University of California, Berkeley)\n\nThe Variability in Design Quality Measures for Multiple Types of Space-filling Designs Created by Leading Software Packages\n\nThomas W. Lucas (Naval Postgraduate School) and Jeffrey D. Parker (United States Marine Corps)\n\nAbstract Abstract\n\nSpace-filling designs (SFDs) underpin many large-scale simulation studies. The algorithms that construct SFDs are mostly stochastic and cannot guarantee that optimal solutions can be found within a practical amount of time. This paper uses massive experimentation to find the empirical distributions of a diverse set of design-quality measures in highly-used classes of SFDs constructed by leading software packages. The objective is to provide simulation practitioners with a better understanding of what they can expect from different SFD choices. The results show substantial variability in measures of correlation and space-fillingness in the design classes and dimensions investigated. Therefore, computer experimenters should generate and assess several candidate designs using different random-number-generator seeds to reduce the risk of using a poor design simply due to random chance. We also find that in the largest designs investigated, the uniform designs generally perform best for both our correlation and uniformity measures.\n\npdf\n\nTop-m Factor Screening for Stochastic Simulation: Multi-Armed Bandit And Sequential Bifurcation Combined\n\nWen Shi (Central South University), Hong Wan (North Carolina State University), and Xiang Xie (Central South University)\n\nAbstract Abstract\n\nWe propose a novel screening framework (abbreviated to TopmSB) to identify the top m key factors affecting the system performance. Our framework builds on the standard SB screening mechanism but incorporates an adaptive multi-armed bandit (MAB) procedure in each stage to prioritize the largest group. Compared to SB, TopmSB avoids specifying perplexing (un)importance threshold parameters, while providing desired computational efficiency and statistical precision guarantee. Numerical experiments demonstrate the efficiency and effectiveness of the proposed method.\n\npdf\n\nBest Arm Identification with Fairness Constraints on Subpopulations\n\nYuhang Wu, Zeyu Zheng, and Tingyu Zhu (University of California, Berkeley)\n\nAbstract Abstract\n\nWe formulate, analyze and solve the problem of best arm identification with fairness constraints on subpopulations (BAICS). Standard best arm identification problems aim at selecting an arm that has the largest expected reward where the expectation is taken over the entire population. The BAICS problem requires that a selected arm must be fair to all subpopulations (e.g., different ethnic groups or different types of customers) by satisfying constraints that the expected reward conditional on every subpopulation needs to be larger than some thresholds. The BAICS problem aims at correctly identify, with high confidence, the arm with the largest expected reward from all arms that satisfy subpopulation constraints. We analyze the complexity of the BAICS problem by proving a best achievable lower bound on the sample complexity with closed-form representation. We then design an algorithm and prove the sample complexity to match with the lower bound in terms of order.\n\npdf\n\nTechnical Session · Analysis Methodology\n\nAnalysis Uses in Optimization\n\nChair: Ilya Ryzhov (University of Maryland)\n\nEfficient Bandwidth Selection for Kernel Density Estimation\n\nHaidong Li (University of Chinese Academy of Sciences), Long Wang and Yijie Peng (Peking University), and Di Wang (Shanghai Jiao Tong University)\n\nAbstract Abstract\n\nWe consider bandwidth selection for kernel density estimation. The performance of kernel density estimator heavily relies on the quality of the bandwidth. In this paper, we propose an efficient plug-in kernel density estimator which first perturbs the bandwidth to estimate the optimal bandwidth, followed by applying a kernel density estimator with the estimated optimal bandwidth. The proposed method utilizes the zeroth-order information of kernel function and has a faster convergence rate than other plug-in methods in existing literature. Simulation results demonstrate superior finite sample performance and robustness of the proposed method.\n\npdf\n\nCGPT: A Conditional Gaussian Process Tree for Grey-Box Bayesian Optimization\n\nMengrui (Mina) Jiang, Tanmay Khandait, and Giulia Pedrielli (Arizona State University)\n\nAbstract Abstract\n\nIn black-box optimization problems, Bayesian optimization algorithms are often applied by generating inputs and measure values to discover hidden structure and determine where to sample sequentially. However, information about system properties can be available. In different learning tasks, we may know that the objective is the minimum of functions, or a network. In this paper we consider the case where the structure of the objective function can be encoded as a tree. We propose the new Conditional Gaussian Process tree (CGPT) model for \"tree functions'' to embed the function structure and improving the prediction power of the Gaussian process. We utilize the intermediate information at the tree nodes, to formulate a novel likelihood for the estimation of the CGPT parameters. We formulate the learning and investigate the performance of the proposed approach. Our study shows that CGPT always outperforms a single Gaussian process model.\n\npdf\n\nMean-Variance Portfolio Optimization with Nonlinear Derivative Securities\n\nShiyu Wang and Guowei Cai (Lingnan College, Sun Yat-sen University); Peiwen Yu (Soochow University); Guangwu Liu (City University of Hong Kong); and Jun Luo (Shanghai Jiao Tong University)\n\nAbstract Abstract\n\nIn this paper, we propose a simulation approach to mean-variance optimization for portfolios comprised of derivative securities. The key of the proposed method is on the development of an unbiased and consistent estimator of the covariance matrix of asset returns which do not admit closed-form formulas but require Monte Carlo estimation, leading to a sample-based optimization problem that is easy to solve. We characterize the asymptotic properties of the proposed covariance estimator, and the solution to and the objective value of the sample-based optimization problem. Performance of the proposed approach is demonstrated via numerical experiments.\n\npdf\n\nAviation Modeling and Analysis\n\nTrack Coordinator - Aviation Modeling and Analysis: Sameer Alam (Nanyang Technological University), Miguel Mujica Mota (Amsterdam University of Applied Sciences), Michael Schultz (Bundeswehr University Munich)\n\nTechnical Session · Aviation Modeling and Analysis\n\nAirport and Airspace Operations\n\nTactical Minimization of the Environmental Impact of Holding in the Terminal Airspace and an Associated Economic Model\n\nAditya Paranjape and Anwesha Basu (Tata Consultancy Services Ltd)\n\nAbstract Abstract\n\nMinimization of the carbon footprint of aviation is an active area of interest to the industry and policy makers alike. Optimization of the individual flight phases is an important step in that direction. This paper considers the holding phase, wherein aircraft hold in the terminal airspace of airports prior to approach and landing during times of busy operation or when the arrival capacity is reduced due to factors such as bad weather. We propose a tactical method to allocate landing slots while minimizing the environmental impact of holds. An environmentally-driven policy can be perceived as unfair, particularly by airlines whose environmentally friendly aircraft which might need to hold longer than they would under a fair first-come-first-served policy. To alleviate this challenge, we propose a number of economic reward schemes, including one based on a linear programming problem obtained by applying complementary slackness to the dual of the assignment problem.\n\npdf\n\nUse of Variable Sized Entities to Model Airport Passenger Flow with Pedestrian Dynamics\n\nErich Deines and Tanuj Babele (TransSolutions LLC) and Gary Gardner (InControl)\n\nAbstract Abstract\n\nThis paper describes the use of variable-sized entities within the framework of the InControl simulation software product Pedestrian Dynamics to rapidly model passenger flow and congestion for a series of check-in hall lobby designs for a US domestic airline terminal. Note that the airline and airport will remain anonymous for this presentation due to confidentiality.\n\npdf\n\nTechnical Session · Aviation Modeling and Analysis\n\nMachine Learning Applications in Aviation\n\nChair: John Shortle (George Mason University)\n\nAircraft Line Maintenance Scheduling using Simulation and Reinforcement Learning\n\nSimon Widmer, Syed Shaukat, and Cheng-Lung Wu (UNSW)\n\nAbstract Abstract\n\nThis paper presents a reinforcement learning (RL) algorithm prototype to solve the aircraft line maintenance scheduling problem. The Line Maintenance Scheduling Problem (LMSP) is concerned with scheduling a set of maintenance tasks during an aircraft's ground time. To address this problem, we introduce a novel LMSP method combining a hybrid simulation model and reinforcement learning to schedule maintenance tasks at multiple airports. Initially, this paper briefly reviews the existing literature on optimization-based and AI-enhanced aircraft maintenance scheduling. Secondly, the novel reinforcement learning LMSP method is introduced, evaluated using industry data, and compared with optimization-based LMSP solutions. Our experiments demonstrate that the LMSP method using reinforcement learning is capable of identifying near-optimal policies for scheduling line maintenance jobs when compared to the exact and heuristics-based methods. The proposed model provides an excellent foundation for future studies on AI-enhanced scheduling problems.\n\npdf\n\nNeural Networks for GNSS Matrix Attitude Determination in Aerospace Transportation\n\nRaul de Celis, Jose Gonzalez-Barroso, Pablo Solano-Lopez, and Luis Cadarso (Rey Juan Carlos University)\n\nAbstract Abstract\n\nAccurate navigation and control of Aerial Vehicles requires precise estimations of their position and attitude. Measuring an aircraft's rotation involves comparing two vectors in different reference frames, such as inertial and body axes. Typically, a GNSS sensor-based matrix with at least three sensors is utilized for this purpose, taking advantage of the carrier phase measurements. However, factors such as multipath, frequency lock loss, cycle slips, and severe clock drifts can impede accurate integer ambiguity resolution. To address these challenges, a new neural network-based technique has been developed to optimize the management of large amounts of data and increase carrier phase ambiguity resolution reliability. By using carrier phase difference and pseudorange information, various neural network configurations can be trained to solve the ambiguity and estimate the precise attitude of the GNSS sensor matrix. The provided solution can be used alone or hybridized with other attitude sensor such as gyroscope information.\n\npdf\n\nComplex and Resilient Systems\n\nTrack Coordinator - Complex and Resilient Systems: Saurabh Mittal (MITRE Corporation), Claudia Szabo (The University of Adelaide, University of Adelaide)\n\nTechnical Session · Complex and Resilient Systems\n\nCyber Resilience in Complex Systems\n\nChair: Claudia Szabo (University of Adelaide, The University of Adelaide)\n\nA Mathematical Theory to Quantify Cyber-Resilience in IT/OT Networks\n\nRanjan Pal (Massachusetts Institute of Technology), Rohan Sequeira (University of Southern California), and Michael Siegel (Massachusetts Institute of Technology)\n\nAbstract Abstract\n\nModern enterprise infrastructures (EIs) including those of industrial control systems (ICSs) are becoming increasingly crucial to businesses in a wide range of sectors spanning multiple end-user verticals (e.g., energy, chemical, manufacturing, biotechnology). These EIs improve the (real-time) decision support, productivity, and efficiency of business processes, but necessarily reliant upon the cyber-resilience of complex infrastructures for sustainable business continuity. We are interested in the long-standing open question in the cyber-resilience domain: how can managers formally quantify cyber-resilience for any complex networked EI (sub-)system in the event of a cyber-attack affecting its multiple (inter-dependent) components? We propose a simulation-backed framework derived from probabilistic graph theory to answer this question. We pioneer the derivation and analysis of a quantifiable, closed-form manager friendly expression exhibiting the degree of cyber-resilience (dependent upon individual EI component functionality quality and the varying extents of functional dependencies across networked components) within the (sub-)system post cyber-attack(s) affecting an EI.\n\npdf\n\nTrustworthy Artificial Intelligence Framework for Proactive Detection and Risk Explanation of Cyber Attacks in Smart Grid\n\nShirajum Munir and Sachin Shetty (Old Dominion University)\n\nAbstract Abstract\n\nThe rapid growth of distributed energy resources (DERs), such as renewable energy sources, generators, consumers, and prosumers in the smart grid infrastructure, poses significant cybersecurity and trust challenges to the grid controller. Consequently, it is crucial to identify adversarial tactics and measure the strength of the attacker’s DER. To enable a trustworthy smart grid controller, this work investigates a trustworthy artificial intelligence (AI) mechanism for proactive identification and explanation of the cyber risk caused by the control/status message of DERs. Thus, proposing and developing a trustworthy AI framework to facilitate the deployment of any AI algorithms for detecting potential cyber threats and analyzing root causes based on Shapley value interpretation while dynamically quantifying the risk of an attack based on Ward’s minimum variance formula. The experiment with a state-of-the-art dataset establishes the proposed framework as a trustworthy AI by fulfilling the capabilities of reliability, fairness, explainability, transparency, reproducibility, and accountability.\n\npdf\n\nA Mathematical Theory to Price Cyber-Cat Bonds Boosting IT/OT Security\n\nRanjan Pal (MIT Sloan School of Management) and Bodhibrata Nag (Indian Institute of Management Calcutta)\n\nAbstract Abstract\n\nThe density of enterprise cyber (re-)insurance markets to manage (aggregate) enterprise cyber-risk has been low enough to realize their potential to significantly improve cyber-security and consequently the cyber-reliability of (ICS) enterprise ecosystems. In this paper, we propose the use of catastrophic (CAT) bonds as a radical and alternative residual cyber-risk management methodology to alleviate the big supply demand gap in the current cyber (re-)insurance industry, by boosting capital injection in the latter industry. Two important follow up questions arise: (i) when is it feasible for cyber (re-)insurers to invest in CAT bonds? and (ii) how can we price cyber-CAT bonds conditioned on the feasibility condition(s)? We focus on answering the second question pivoted upon an existential answer to the first. We propose a novel practically motivated information asymmetry (IA) driven cyber-CAT bond pricing model, built upon theories of financial stochastic processes and Monte Carlo simulations, in realistic arbitraged incomplete markets.\n\npdf\n\nTechnical Session · Complex and Resilient Systems\n\nPanel: Resilience and Complexity in Socio-cyber-physical Systems\n\nChair: Claudia Szabo (University of Adelaide, The University of Adelaide)\n\nResilience and Complexity in Socio-Cyber-Physical Systems\n\nClaudia Szabo (University of Adelaide), Rodrigo Castro (CIFASIS-CONICET), Joachim Denil (University of Antwerp), and Susan M. Sanchez (Naval Postgraduate School)\n\nAbstract Abstract\n\nSocio-Cyber-Physical Systems are ubiquitous in today’s world. They are inherently complex systems built out of many large-scale systems that encompass different perspectives and numerous stakeholders. This leads to several challenges in managing their complexity and emergent behavior. In addition, these systems tend to include many adaptive and autonomous systems with different goals and different adaptations to environment changes or failures. The design, analysis, and testing of such systems is inherently challenging but is becoming critical due to their wide adoption. In this panel, we aim to discuss some of these challenges and potential solutions.\n\npdf\n\nTechnical Session · Complex and Resilient Systems\n\nPanel: Using Simulation to Improve Trust and Autonomy Adoption\n\nChair: Kelly Neville (MITRE Corporation)\n\nThe Use of Simulation to Improve Trust and Adoption of Autonomy and AI in High-Consequence Work Systems\n\nEmily Barrett, Lisa Billman, Theresa Fersch, Valerie Gawron, and Kelly Neville (MITRE Corporation); Emily Patterson (The Ohio State University); and Eric Vorm (Naval Air Warfare Center)\n\nAbstract Abstract\n\nWe assert that simulation should be an integral part of technology development and acquisition. Its use to iteratively evaluate new technology across the development timeline can help ensure technologies contribute to resilience in work operations. This, in turn, benefits trust and likelihood of adoption. Potential hindrances to simulation in technology development are the time and complexity simulation can introduce. Time may be needed to model entities and dynamics to be simulated, plan and conduct simulation-based tests and experiments, and translate the results into requirements, user stories, or other inputs to the technology’s design and implementation plan. Complexity is increased when simulation results suggest new or changed requirements, identify technology design and implementation improvements, or produce conflicting feedback from potential users. We will discuss these challenges, methods and tools that minimize their disruptive effects, varieties of simulation we have used to support technology development, and benefits of using simulation in development.\n\npdf\n\nTechnical Session · Complex and Resilient Systems\n\nResilient Enterprise and Services\n\nChair: Claudia Szabo (University of Adelaide, The University of Adelaide)\n\nSymbiotic Use of Digital Twin, Simulation and Design Thinking Approach for Resilient Enterprise\n\nSouvik Barat, Sylvan Lobo, Reshma Korabu, Himabindu Thogaru, and Ravi Mahamuni (Tata Consultancy Services Research)\n\nAbstract Abstract\n\nEnterprises are increasingly facing the need to be resilient in the face of uncertainty and dynamism. Simulatable digital twins have become critical aids for analyzing and adapting complex systems. Design thinking and service design methodologies, in contrast, are gaining momentum for ideation, subjective evaluation, and innovation. A systematic application of these methodologies to explore innovative ideas and a faithful virtual environment to test and fine-tune those ideas without impacting real systems could be transformational. This paper presents an approach that establishes a symbiotic relationship between these two approaches to introduce precision and innovativeness to make enterprises resilient. We describe the key characteristics of resilient enterprises, present our approach, and illustrate its effectiveness with a case study focusing on a transformation toward a new normal to address the Covid-19 pandemic induced disruptions in the IT industry.\n\npdf\n\nMarkov Process Simulations of Service Systems with Concurrent Hawkes Service Interactions\n\nAndrew Daw (University of Southern California) and Galit B. Yom-Tov (Technion - Israel Institute of Technology)\n\nAbstract Abstract\n\nIn multi-tasked services such as in messaging-based contact centers, parallel service interactions share a mutual dependence through the agent's concurrency. Here, we introduce Markov process simulation methods for bivariate Hawkes cluster service models that are not Markovian by default due to their concurrency dependence. To do so, we propose an alternate construction that maintains extra \"shadow\" variables for how the process would be under other concurrency levels. We prove that this construction yields an equivalent Markov process, and we show through numerical experiments that its corresponding simulation algorithm is significantly more efficient than the non-Markovian alternatives.\n\npdf\n\nStochastic Climate Simulation for Power Grid Net Demand Risk Assessment\n\nRob Cirincione (Sunairio)\n\nAbstract Abstract\n\nPower grid planners and power portfolio managers are increasingly concerned with anticipating “net demand” risks, which is defined as customer demand minus renewables for a particular time period. Net demand is a better predictor of grid stress than peak demand in a grid with significant renewables penetration. For Holy Cross Energy, Sunairio simulated 1,000 probabilistic outcomes of hourly weather across a geographic region that encompassed the locations of customers and renewable energy resources (wind, solar), for 15 years. The hourly weather simulations were transformed to hourly energy simulations of customer demand, wind generation, and solar generation via machine learning models, creating a broad, climate-change-aware, coincident data set from which to quantify concurrent risks to net demand. Net demand paths of particular interest for grid planning were curated via statistical processing.\n\npdf\n\nTechnical Session · Complex and Resilient Systems\n\nHandling Uncertainty in Complex and Resilient Systems\n\nChair: Souvik Barat (TCS)\n\nEffects of Timing of Agents' Reactions in Pharmaceutical Supply Chains under Disruption\n\nRozhin Doroudi, Ozlem Ergun, Jacqueline Griffin, and Stacy Marsella (Northeastern University)\n\nAbstract Abstract\n\nDisruptions in the supply chain network can have significant and far-reaching consequences, especially in pharmaceutical supply chains that affect health and financial outcomes and raise equity concerns. To inform strategies that can address this critical global problem, we study disruptions in pharmaceutical supply chains using multiagent simulations. These simulations include decision-theoretic agents with a theory of mind reasoning that allows them to reason about the other agents in the supply chain, including their trustworthiness. The simulations reveal how supplier-buyer interactions have non-local effects which can exacerbate and extend disruption impacts. In addition, a distributor’s focus on its own short-term profit can lower its long-term profit and damage equity in healthcenters. We also demonstrate how agents adapt to changes in the environment and changes in other agents’ behavior and how in the absence of explicit communication and coordination, the timing of these adaptations inhibits disruption mitigation efforts from transpiring.\n\npdf\n\nModel Predictive Control in Optimal Intervention of COVID-19 with Mixed Epistemic-Aleatoric Uncertainty\n\nJinming Wan, Saeideh Mirghorbani, N. Eva Wu, and Changqing Cheng (Binghamton University)\n\nAbstract Abstract\n\nNon-pharmaceutical interventions (NPI) have been proven vital in the fight against the COVID-19 pandemic before the massive rollout of vaccinations. Considering the inherent epistemic-aleatoric uncertainty of parameters, accurate simulation and modeling of the interplay between the NPI and contagion dynamics are critical to the optimal design of intervention policies. We propose a modified SIRD-MPC model that combines a modified stochastic Susceptible-Infected-Recovered-Deceased (SIRD) compartment model with mixed epistemic-aleatoric parameters and Model Predictive Control (MPC), to develop robust NPI control policies to contain the infection of the COVID-19 pandemic with minimum economic impact. The simulation result indicates that our proposed model can significantly decrease the infection rate compared to the practical results under the same initial conditions.\n\npdf\n\nTechnical Session · Complex and Resilient Systems\n\nReliability in Power Systems\n\nChair: Jinming Wan (Binghamton University)\n\nCascading Transformer Failure Probability Model Under Geomagnetic Disturbances\n\nPratishtha Shukla, James Nutaro, and Srikanth Yoginath (Oak Ridge National Laboratory)\n\nAbstract Abstract\n\nThis paper develops a probabilistic model to assess the cascading failure of transformers in an electric power grid experiencing geomagnetic disturbances caused by a solar storm. We propose a model in which the probability of failure is a function of the intensity of the solar storm, the physical properties of the transformer, the geographical location of the transformer, and the flow of electrical power. We demonstrate the proposed model using the IEEE 14-bus system and several notional solar storms. The model quickly computes the initial and cascading failure probabilities of the transformers in the system as a first step towards quantifying the risks posed by future solar storms.\n\npdf\n\nImpact of Salt-To-Steam Heat Exchanger Failure Rates on Lifetime Production of Concentrating Solar Power Tower Plants\n\nKaroline Hood (US Army, Colorado School of Mines) and Alex Zolan (National Renewable Energy Laboratory)\n\nAbstract Abstract\n\nHeat exchangers in the steam generation system (SGS) of concentrated solar power (CSP) plants are unique in their functionality. Consequently, equipment replacements have long lead times. A typical CSP plant using an organic Rankine cycle has one or two salt-to-steam trains (SSTs) within the SGS. When one heat exchanger in the SGS fails, the individual SGS fails. We use an existing framework that combines simulation and optimization models to assess the impacts of irrecoverable failures on long-term production. The methodology provides an optimized dispatch with the integration of unplanned simulated failures over a thirty-year period. Our work shows a system of two trains provides resiliency and reduces downtime of a plant by six to eight times compared to a single train. The gross revenue increases by 31% and 11% for single and two trains, respectively, when the expected lifetime increases from five to 10 years.\n\npdf\n\nData Science for Simulation\n\nTrack Coordinator - Data Science for Simulation: Abdolreza Abhari (Ryerson University), Hamdi Kavak (George Mason University)\n\nTechnical Session · Data Science for Simulation\n\nMachine Learning for Simulation\n\nChair: Hamdi Kavak (George Mason University)\n\nCausal Dynamic Bayesian Networks for Simulation Metamodeling\n\nBest Contributed Theoretical Paper - Finalist\n\nPracheta Boddavaram Amaranath (University of Massachusetts Amherst), Sam Witty (Basis Research Institute), and Peter J. Haas and David Jensen (University of Massachusetts Amherst)\n\nAbstract Abstract\n\nA traditional metamodel for a discrete-event simulation approximates a real-valued performance measure as a function of the input-parameter values. We introduce a novel class of metamodels based on modular dynamic Bayesian networks (MDBNs), a subclass of probabilistic graphical models which can be used to efficiently answer a rich class of probabilistic and causal queries (PCQs). Such queries represent the joint probability distribution of the system state at multiple time points, given observations of, and interventions on, other state variables and input parameters. This paper is a first demonstration of how the extensive theory and technology of causal graphical models can be used to enhance simulation metamodeling. We demonstrate this potential by showing how a single MDBN for an M/M/1 queue can be learned from simulation data and then be used to quickly and accurately answer a variety of PCQs, most of which are out-of-scope for existing metamodels.\n\npdf\n\nDeep-learning-assisted Cardiac Electrophysiology Simulation\n\nWeixuan Dong, Yifu Li, and Rui Zhu (The University of Oklahoma)\n\nAbstract Abstract\n\nSimulation built upon partial and ordinary differential equations has been a classic approach to modeling cardiac electrophysiological dynamics. However, mitigating the computational burden of differential equations is still a challenging problem. This paper provides a novel alternative utilizing data-driven recurrent neural networks for cardiac electrophysiological dynamic simulation. Specifically, we develop a long short-term memory (LSTM)-assisted simulation to capture the underlying dynamics of cardiac electrophysiology while preserving computational efficiency. Experimental results demonstrate the efficiency and effectiveness of the proposed method, which outperforms the differential equation-based simulation approach while significantly reducing the computational cost. The proposed method offers a promising alternative to traditional simulation and may contribute to the development of more efficient and accurate approaches for simulating cardiac electrophysiology.\n\npdf\n\nInferring Epidemic Dynamics Using Gaussian Process Emulation of Agent-Based Simulations\n\nAbdulrahman Ahmed, M. Amin Rahimian, and Mark Roberts (University of Pittsburgh)\n\nAbstract Abstract\n\nComputational models help decision makers understand epidemic dynamics to optimize public health interventions. Agent-based simulation of disease spread in synthetic populations allows us to compare and contrast different effects across identical populations or to investigate the effect of interventions keeping every other factor constant between \"digital twins.\" FRED (A Framework for Reconstructing Epidemiological Dynamics) is an agent-based modeling system with a geo-spatial perspective using a synthetic population that is constructed based on the U.S. Census data. In this paper, we show how Gaussian process regression can be used on FRED-synthesized data to infer the differing spatial dispersion of the epidemic dynamics for two disease conditions that start from the same initial conditions and spread among identical populations. Our results showcase the utility of agent-based simulation frameworks such as FRED for inferring differences between conditions where controlling for all confounding factors for such comparisons is next to impossible without synthetic data.\n\npdf\n\nTechnical Session · Data Science for Simulation\n\nData Analytics for Simulation\n\nChair: Abdolreza Abhari (Toronto Metropolitan University)\n\nAutonomic Orchestration of In-Situ and In-Transit Data Analytics for Simulation Studies\n\nXiaorui Du (Technical University of Munich); Adriano Pimpini (Sapienza, University of Rome); Andrea Piccione (Huawei Munich Research Center); Zhuoxiao Meng and Anibal Siguenza-Torres (Technical University of Munich); Stefano Bortoli (Huawei Munich Research Center); Alois Knoll (Technical University of Munich); and Alessandro Pellegrini (University of Rome Tor Vergata)\n\nAbstract Abstract\n\nModern parallel/distributed simulations can produce large amounts of data. The historical approach of performing analyses at the end of the simulation is unlikely to cope with modern, extremely large-scale analytics jobs. Indeed, the I/O subsystem can quickly become the global bottleneck. Similarly, processing on-the-fly the data produced by simulations can significantly impair the performance in terms of computational capacity and network load. We present a methodology and reference architecture for constructing an autonomic control system to determine at runtime the best placement for data processing (on simulation nodes or a set of external nodes). This allows for a good tradeoff between the load on the simulation's critical path and the data communication system. Our preliminary experimentation shows that autonomic orchestration is crucial to improve the global performance of a data analysis system, especially when the simulation node's rate of data production varies during simulation.\n\npdf\n\nScaling Cross-Relations with Larger Dataset\n\nVictor Diakov (Simfoni Ltd.)\n\nAbstract Abstract\n\nSimulation and optimization of procurements might employ clustering dataset elements to exclude possible duplicates and improve processing resiliency. This study presents a case of applying scaling methods to reduce computation time of clustering between a smaller and a larger dataset. In this example (of selecting close supplier names), computation time scales as square of N (the number of elements), and the presented approach in effect brings computing time to be linear in N. As a result, computation time in our case is reduced by over an order of magnitude.\n\npdf\n\nUncovering Competitor Pricing Patterns in the Danish Pharmaceutical Market via Subsequence Time Series Clustering: A Case Study\n\nRuhollah Jamali (University of Southern Denmark) and Sanja Lazarova-Molnar (Karlsruhe Institute of Technology)\n\nAbstract Abstract\n\nAdopting data-driven decision-making approaches can significantly enhance profitability and foster growth in economic situations through quantitative analysis of market dynamics. One intriguing market that warrants examination is the price competition observed within the Danish pharmaceutical sector, where numerous companies are vying for a larger market share through the offering of diverse pharmaceutical products. This paper aims to shed light on this market by employing subsequence time series clustering techniques to identify pricing patterns among the players involved in the Danish pharmaceutical industry. The data analysis pipeline performed in this study allows for the identification of price patterns for clustering and discovering different agent groups, as well as providing a foundation for expanding the current agent-based model of the European pharmaceutical parallel trade market by analyzing the pricing behavior and patterns of players, facilitating the utilization of historical data to model agent behavior and advancing research in this area.\n\npdf\n\nTechnical Session · Data Science for Simulation\n\nSimulation in Action\n\nChair: Hamdi Kavak (George Mason University)\n\nA Preliminary Study of Regularization Framework for Constructing Task-Specific Simulators\n\nDilara Aykanat (University of California, Berkeley); Nian Si (The University of Chicago); and Zeyu Zheng (University of California, Berkeley)\n\nAbstract Abstract\n\nOne approach to construct or calibrate simulators, when representative real data exist, is to ensure that the synthetic data generated by the simulated match the empirical distribution of the real data. However, such approach to construct simulators does not take into consideration where the constructed simulators will be used. For some applications, there are clear tasks (such as performance evaluation of different decisions) in users’ mind where the simulated data will serve as input to the tasks. In this work, we propose an approach to use the knowledge of these tasks to guide the construction of simulators, in addition to the distribution match of simulated data and real data by regularizing the objective function with a task related penalty. We conduct a preliminary numerical study of this approach to illustrate the effectiveness compared to not taking into consideration the specific tasks of the simulators.\n\npdf\n\nUsing Simulation to Assess the Reliability of Forecasts in High-tech Industry\n\nBhoomica Mysore Nataraja (Eindhoven University of Technology); Tanmay Aggarwal (Lambda Function Inc); and Nitish Singh, Koen Herps, and Ivo Adan (Eindhoven University of Technology)\n\nAbstract Abstract\n\nIn a high-tech production environment, capacity investment and production planning are often based on the demand information from manufacturers within a supply chain. A supplier solicits forecast information from a manufacturer, and the manufacturer provides demand forecasts that are updated on a rolling horizon basis. Problems arise with this setup if the manufacturer provides volatile forecast quantities due to the market's fluctuating demand or internal bias. As a result, suppliers' mistrust regarding forecast quantities grows, leading to adjusted production plans based on planners' anecdotal experience. The paper presents a decision model to determine the reliability of forecasts provided by manufacturers to facilitate better production planning. The study also suggests alternate forecasting techniques in case of low reliability. To evaluate the effectiveness of the proposed approach, a simulation study is conducted for different manufacturers and scenarios. Our experiments showed an average cost reduction of 14% across all instances.\n\npdf\n\nDigital Twin Based Learning Framework for Adaptive Fault Diagnosis in Microgrids with Autonomous Reconfiguration Capabilities\n\nTemitope Runsewe, Abdurrahman Yavuz, and Nurcin Celik (University of Miami)\n\nAbstract Abstract\n\nThe world is increasingly reliant on energy systems, making them a critical infrastructure for essential services. This also makes them vulnerable to attacks, which can result in significant disruptions and damage. Microgrid (MG) monitoring systems play a crucial role in ensuring the safety and reliability of energy systems. However, traditional fault diagnosis techniques are limited to already established faults due to the use of only historical data, making it challenging to keep up with the increasing demand for safety and reliability. This paper proposes a digital twin based machine learning (DTML) framework for fault diagnosis in MG monitoring systems, with a focus on assessing the resilience of MG end-to-end systems to potential disruptions from adversaries. The proposed framework utilizes digital twin based random forest (RF) and support vector machine (SVM) and logistic regression (LR) model and shows that the RF based model outperforms other models with an accuracy of 95%.\n\npdf\n\nEnvironment Sustainability and Resilience\n\nTechnical Session · Environment Sustainability and Resilience\n\nCritical Infrastructures\n\nChair: Raymond Smith (East Carolina University)\n\nA Network Theory to Quantify and Bound Cyber-risk in IT/OT Systems\n\nBest Contributed Applied Paper - Finalist\n\nRanjan Pal (MIT Sloan School of Management), Rohan Xavier Sequeira (University of Southern California), and Sander Zeijlemaker and Michael Siegel (MIT Sloan School of Management)\n\nAbstract Abstract\n\nIT/OT driven industrial control systems (ICSs) such as water/power/transportation networks are increasingly meeting the daily functional needs of civilian society around the globe. This, alongside making societal businesses more automated, efficient, productive, and profitable. However, often poorly configured IoT security settings increase the chances of occurrence of (nation-sponsored) stealthy spread-based APT malware attacks in ICSs that might go undetected over a considerable period of time. The ICS enterprise management is often keen to get apriori statistical estimates of cyber-loss impact post any cyber-attack event such that it can plan ahead on its cyber-resilience budget. In this paper, we propose the first mathematical theory, based upon stochastic processes and concentration inequalities, to (a) statistically quantify apriori the cyber-loss impact (distribution) on an ICS infrastructure network post an APT cyber-attack event, and subsequently (b) bound the tail of such a cyber-risk distribution, for arbitrary impact distributions.\n\npdf\n\nSafeguarding Infrastructure from Cyber Threats with NLP-based Information Retrieval\n\nChristin J. Salley, Neda Mohammadi, and John E. Taylor (Georgia Institute of Technology)\n\nAbstract Abstract\n\nNatural disasters disrupt systems, leading to critical infrastructure vulnerabilities prone to cyber-attacks. The MITRE ATT&CK Enterprise Matrix is a knowledge base for threat analyses in the cybersecurity community. Existing processes to derive possible attack methodologies from this Matrix are largely manual and time-consuming. It is essential to automate the information retrieval process to reduce human errors, improve efficiency, and free up resources for identifying unrevealed cyber-attacks. We propose a framework that incorporates Natural Language Processing (NLP) and Text Mining to automatically generate sets of attack paths from the technique descriptions in the Matrix. The framework generates similarity between techniques based on their descriptions and creates an output showing potential pathways an adversary can take to infiltrate a system. The outputs are compared against an annotated approach and attack report. The results of this study provide an approach to more quickly and effectively assess potential cyber-attacks towards protecting critical infrastructure.\n\npdf\n\nModeling of Circular Economy Strategies for CFRP-made Aircrafts\n\nArnd Schirrmann and Uwe Beier (Airbus)\n\nAbstract Abstract\n\nIn a circular economy, recycling of materials at the end of a product's life cycle is a key issue. This paper discusses the sustainability impacts of different recycling strategies for CFPR-made aircraft and how they weigh up against alternative measures such as waste reduction and lower material consumption in the manufacture of the product. The analysis includes environmental and cost impacts for different strategies and market scenarios. A quantitative system dynamic simulation of the life cycle of an aircraft program is used. The subject of the life cycle simulation model is the CFRP mass flow, CO2 emissions and associated costs. In addition, the effects of R&T investments in new technologies for recycling and waste prevention as well as the reduction of material consumption were investigated.\n\npdf\n\nTechnical Session · Environment Sustainability and Resilience\n\nFood and Supply Chains\n\nChair: Virginia Fani (University of Florence)\n\nSystem Dynamics Simulation of External Supply Chain Disruptions on a Simplified Semiconductor Supply Chain\n\nAnna Christina Hartwick, Abdelgafar Ismail, Beatriz Kalil Valladão Novais, Mohammed Zeeshan, and Hans Ehm (Infineon Technologies AG)\n\nAbstract Abstract\n\nDue to the vitality of semiconductor products for other industries, the production of semiconductors and impact of external disruptions on the semiconductor supply chain should be well understood. As semiconductor manufacturing is accompanied with intrinsic long manufacturing cycle times ranging from 50 to 100 days where operations run 24/7, 365 days per year, correct understanding of potential disturbances should be considered. Examples of these disturbances include pandemics, extreme weather events, geopolitical tensions and war. These hazards pose various risks for supply chains, for example, the bullwhip and ripple effect. To simulate the result of such risks, a simplified system dynamics model of a typical semiconductor manufacturing supply chain was constructed using the Anylogic Software. The model serves as a what-if scenario foundation to evaluate certain external circumstances dependent on current global situations to enhance supply chain resilience\n\npdf\n\nAn Agent-Based Model of Agricultural Land Use in Support of Local Food Systems\n\nPoojan Patel and Caroline Krejci (University of Texas at Arlington), Nicholas Schwab (University of Northern Iowa), and Michael Dorneich (Iowa State University)\n\nAbstract Abstract\n\nLocal food systems, in which consumers source food from nearby farmers, offer a sustainable alternative to the modern industrial food supply system. However, scaling up local food production to meet consumer demand will require farmers to allocate more land to this purpose. This paper describes an agent-based model that represents commodity-producing Iowa farmers and their decisions about converting some of their acreage to specialty crop production for local consumption. Farmer agents’ land-use decisions are informed by messages passed to them via their social connections with other farmers in their communities and messages from agricultural extension agents. Preliminary experimentation revealed that leveraging extension agents to increase the frequency and strength of messages to farmers in support of local food production has a modest positive impact on adoption. By itself, however, this intervention is unlikely to yield significant improvements to food system sustainability.\n\npdf\n\nTechnical Session · Environment Sustainability and Resilience\n\nSimulation for Sustainability\n\nChair: Jonathan M. Gilligan (Vanderbilt University)\n\nSustainability Assessment Through Simulation: The Case Of Fashion Renting\n\nVirginia Fani and Romeo Bandinelli (University of Florence)\n\nAbstract Abstract\n\nThe fashion industry is widely known as one of the most environmentally impacting. To address the overconsumption issue, the fashion renting business model allows renting clothes or accessories instead of buying them, extending the useful life of products. However, concerns about the sustainability of fashion renting supply chains are arisen, especially due to reverse logistics. In this context, a hybrid simulation model is developed to support fashion companies in the design and evaluation of renting supply chain configurations. Through Discrete Event Simulation (DES) logistics flows are represented, while Agent-Based Modeling (ABM) integrated with Geographic Information System (GIS) allow to represent supply chain’s nodes in the real environment. GIS concurs to estimate the sustainability of the supply chain importing effective data related to the covered distances. The proposed parametric model will enable performing scenario analyses to assess the best configuration in terms of environmental impact.\n\npdf\n\nSimulative Analysis of the Sustainability Driven Transformation of Casting Plants\n\nJohannes Dettelbacher, Wolfgang Schlüter, and Alexander Buchele (Ansbach University of Applied Sciences)\n\nAbstract Abstract\n\nThe current energy crisis and high fossil fuel costs are challenging energy intensive industries such as non-ferrous foundries. It is therefore important to promote the transition to renewable energy sources with the electrification of melting units. This pilot study is the first to simulate the transition of conventional foundries to sustainable technologies. For this purpose, a simulation model based on a selected example company is developed. It takes into account the energy consumption and the logistical effects of a converted operation. The simulation model is implemented as a hybrid simulation combining a discrete event simulation at the plant level"
    }
}
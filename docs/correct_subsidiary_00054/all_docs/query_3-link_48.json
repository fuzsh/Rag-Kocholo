{
    "id": "correct_subsidiary_00054_3",
    "rank": 48,
    "data": {
        "url": "https://dl.acm.org/doi/10.1145/2659003",
        "read_more_link": "",
        "language": "en",
        "title": "Nonstrict Hierarchical Reinforcement Learning for Interactive Systems and Robots",
        "top_image": "https://dl.acm.org/cms/asset/1ed3d888-4cad-4ef7-81a7-fca0931fdb65/2660857.cover.jpg",
        "meta_img": "https://dl.acm.org/cms/asset/1ed3d888-4cad-4ef7-81a7-fca0931fdb65/2660857.cover.jpg",
        "images": [
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/acm-dl-logo-white-1ecfb82271e5612e8ca12aa1b1737479.png",
            "https://dl.acm.org/doi/10.1145/specs/products/acm/releasedAssets/images/acm-logo-1-ad466e729c8e2a97780337b76715e5cf.png",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/footer-logo1-45ae33115db81394d8bd25be65853b77.png",
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/Default_image_lazy-0687af31f0f1c8d4b7a22b686995ab9b.svg",
            "https://dl.acm.org/action/showDoPubAsset?doi=10.1145/contrib-81100202818&format=rel-imgonly&assetId=before-cloud-tall.jpg",
            "https://dl.acm.org/action/showDoPubAsset?doi=10.1145/contrib-81100593751&format=rel-imgonly&assetId=2016.05-kgajos2-400.png",
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/loader-7e60691fbe777356dc81ff6d223a82a6.gif",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/acm-logo-dl-8437178134fce530bc785276fc316cbf.png",
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/acm-logo-3-10aed79f3a6c95ddb67053b599f029af.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            "Interactive robots",
            "flexible interaction",
            "function approximation",
            "hierarchical control",
            "human--robot interaction",
            "machine learning",
            "reinforcement learning",
            "spoken dialogue systems",
            "user simulation"
        ],
        "tags": null,
        "authors": [
            "United Kingdom View Profile",
            "Germany View Profile",
            "Heriberto Cuayáhuitl",
            "Ivana Kruijff-Korbayová",
            "Nina Dethlefs",
            "Kruijff-KorbayováIvana"
        ],
        "publish_date": null,
        "summary": "",
        "meta_description": "Conversational systems and robots that use reinforcement learning for policy optimization\nin large domains often face the problem of limited scalability. This problem has been\naddressed either by using function approximation techniques that estimate the ...",
        "meta_lang": "en",
        "meta_favicon": "/pb-assets/head-metadata/apple-touch-icon-1574252172393.png",
        "meta_site_name": "ACM Transactions on Interactive Intelligent Systems",
        "canonical_link": "https://dl.acm.org/doi/10.1145/2659003",
        "text": "Abstract\n\nConversational systems and robots that use reinforcement learning for policy optimization in large domains often face the problem of limited scalability. This problem has been addressed either by using function approximation techniques that estimate the approximate true value function of a policy or by using a hierarchical decomposition of a learning task into subtasks. We present a novel approach for dialogue policy optimization that combines the benefits of both hierarchical control and function approximation and that allows flexible transitions between dialogue subtasks to give human users more control over the dialogue. To this end, each reinforcement learning agent in the hierarchy is extended with a subtask transition function and a dynamic state space to allow flexible switching between subdialogues. In addition, the subtask policies are represented with linear function approximation in order to generalize the decision making to situations unseen in training. Our proposed approach is evaluated in an interactive conversational robot that learns to play quiz games. Experimental results, using simulation and real users, provide evidence that our proposed approach can lead to more flexible (natural) interactions than strict hierarchical control and that it is preferred by human users.\n\nReferences\n\n[1]\n\nA. Atrash, R. Kaplow, J. Villemure, R. West, H. Yamani, and J. Pineau. 2009. Development and validation of a robust speech interface for improved human-robot interaction. International Journal of Social Robotics 1, 4 (2009), 345--356.\n\n[2]\n\nA. Atrash and J. Pineau. 2009. A Bayesian reinforcement learning approach for customizing human-robot interfaces. In International Conference on Intelligent User Interfaces (IUI’09). 355--360.\n\n[3]\n\nJ. Baillie. 2005. URBI: Towards a universal robotic low-level programming language. In International Conference on Intelligent Robots and Systems (IROS’05). IEEE, 3219--3224.\n\n[4]\n\nA. Barto and S. Mahadevan. 2003. Recent advances in hierarchical reinforcement learning. Discrete Event Dynamic Systems: Theory and Applications 13, 1--2 (2003), 41--77.\n\n[5]\n\nA. Beck, L. Cañamero, and K. A. Bard. 2010. Towards an affect space for robots to display emotional body language. In International Symposium on Robot and Human Interactive Communication (Ro-Man’10). IEEE, 464--469.\n\n[6]\n\nT. Belpaeme, P. Baxter, R. Read, R. Wood, H. Cuayáhuitl, B. Kiefer, S. Racioppa, I. Kruijff Korbayová, G. Athanasopoulos, V. Enescu, R. Looije, M. Neerincx, Y. Demiris, R. Ros-Espinoza, A. Beck, L. Canãmero, A. Hiolle, M. Lewis, I. Baroni, M. Nalin, P. Cosi, G. Paci, F. Tesser, G. Sommavilla, and R. Humbert 2012. Multimodal child-robot interaction: Building social bonds. Journal of Human-Robot Interaction 1, 2 (2012), 32--455.\n\n[7]\n\nJ. Betteridge, A. Carlson, S. A. Hong, E. R. Hruschka Jr., E. L. M. Law, T. M. Mitchell, and S. H. Wang. 2009. Toward never ending language learning. In AAAI Spring Symposium: Learning by Reading and Learning to Read. 1--2.\n\n[8]\n\nD. Bohus and A. I. Rudnicky. 2009. The ravenclaw dialog management framework: Architecture and systems. Computer Speech & Language 23, 3 (2009), 332--361.\n\n[9]\n\nF. Cao and S. Ray. 2012. Bayesian hierarchical reinforcement learning. In Neural Information Processing Systems Foundation (NIPS’12). 73--81.\n\n[10]\n\nC. Chao and A. L. Thomaz. 2012. Timing in multimodal reciprocal interactions: Control and analysis using timed petri nets. Journal of Human-Robot Interaction 1, 1 (2012), 4--25.\n\n[11]\n\nP. A. Crook, A. Wang, X. Liu, and L. Lemon. 2012. A statistical spoken dialogue system using complex user goals and value directed compression. In Conference of the European Chapter of the Association for Computational Linguistics (EACL’12). 46--50.\n\n[12]\n\nH. Cuayáhuitl. 2009. Hierarchical Reinforcement Learning for Spoken Dialogue Systems. Ph.D. Dissertation. School of Informatics, University of Edinburgh.\n\n[13]\n\nH. Cuayáhuitl and N. Dethlefs. 2011. Optimizing situated dialogue management in unknown environments. In Annual Conference of the International Speech Communication Association (INTERSPEECH’11). 1009--1012.\n\n[14]\n\nH. Cuayáhuitl and N. Dethlefs. 2011. Spatially-aware dialogue control using hierarchical reinforcement learning. ACM Transactions on Speech and Language Processing 7, 3 (2011), 5:1--5:26.\n\n[15]\n\nH. Cuayáhuitl and N. Dethlefs. 2012. Hierarchical multiagent reinforcement learning for coordinating verbal and non-verbal actions in robots. In ECAI Workshop on Machine Learning for Interactive Systems (MLIS’12). 27--29.\n\n[16]\n\nH. Cuayáhuitl, N. Dethlefs, H. Hastie, and O. Lemon. 2013. Barge-in effects in Bayesian dialogue act recognition and simulation. In IEEE Automatic Speech Recognition and Understanding Workshop (ASRU’13).\n\n[17]\n\nH. Cuayáhuitl and I. Kruijff-Korbayová. 2011. Learning human-robot dialogue policies combining speech and visual beliefs. In International Workshop on Spoken Dialogue Systems (IWSDS’11). 133--140.\n\n[18]\n\nH. Cuayáhuitl, S. Renals, O. Lemon, and H. Shimodaira. 2007. Hierarchical dialogue optimization using semi-arkov decision processes. In Annual Conference of the International Speech Communication Association (INTERSPEECH’07). 2693--2696.\n\n[19]\n\nH. Cuayáhuitl, S. Renals, O. Lemon, and H. Shimodaira. 2010. Evaluation of a hierarchical reinforcement learning spoken dialogue system. Computer Speech and Language 24, 2 (2010), 395--429.\n\n[20]\n\nH. Cuayáhuitl, M. van Otterlo, N. Dethlefs, and L. Frommberger. 2013. Machine learning for interactive systems and robots: A brief introduction. In IJCAI Workshop on Machine Learning for Interactive Systems (MLIS’13).\n\n[21]\n\nL. Daubigney, M. Geist, S. Chandramohan, and O. Pietquin. 2012. A comprehensive reinforcement learning framework for dialogue management optimization. Journal of Selected Topics in Signal Processing 6, 8 (2012).\n\n[22]\n\nN. Dethlefs. 2013. Hierarchical Joint Learning for Natural Language Generation. Ph.D. Dissertation. University of Bremen.\n\n[23]\n\nN. Dethlefs and H. Cuayáhuitl. 2010. Hierarchical reinforcement learning for adaptive text generation. In International Conference on Natural Language Generation (INLG’10).\n\n[24]\n\nN. Dethlefs and H. Cuayáhuitl. 2011a. Combining hierarchical reinforcement learning and Bayesian networks for natural language generation in situated dialogue. In European Workshop on Natural Language Generation (ENLG’11). 110--120.\n\n[25]\n\nN. Dethlefs and H. Cuayáhuitl. 2011b. Hierarchical reinforcement learning and hidden Markov models for task-oriented natural language generation. In Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT’11). 654--659.\n\n[26]\n\nN. Dethlefs, H. Cuayáhuitl, and J. Viethen. 2011. Optimising natural language generation decision making for situated dialogue. In Annual Meeting on Discourse and Dialogue (SIGdial’11).\n\n[27]\n\nN. Dethlefs, H. Hastie, R. Rieser, and O. Lemon. 2012a. Optimising incremental dialogue decisions using information density for interactive systems. In Conference on Empirical Methods in Natural Language Processing (EMNLP’12).\n\n[28]\n\nN. Dethlefs, V. Rieser, H. Hastie, and O. Lemon. 2012b. Towards optimising modality allocation for multimodal output generation in incremental dialogue. In ECAI Workshop on Machine Learning for Interactive Systems (MLIS’12). 31--36.\n\n[29]\n\nT. Dietterich. 2000a. Hierarchical reinforcement learning with the MAXQ value function decomposition. Journal of Artificial Intelligence Research 13, 1 (2000), 227--303.\n\n[30]\n\nT. Dietterich. 2000b. An overview of MAXQ hierarchical reinforcement learning. In Symposium on Abstraction, Reformulation, and Approximation (SARA’00). 26--44.\n\n[31]\n\nT. G. Dietterich. 2000c. An overview of MAXQ hierarchical reinforcement learning. In Symposium on Abstraction, Reformulation, and Approximation (SARA’00).\n\n[32]\n\nL. Frommberger. 2012. Qualitative Spatial Abstraction in Reinforcement Learning. Springer-Verlag, New York.\n\n[33]\n\nM. Gašić and S. Young. 2011. Effective handling of dialogue state in the hidden information state POMDP-based dialogue manager. ACM Transactions on Speech and Language Processing 7, 3 (2011).\n\n[34]\n\nP. Heeman. 2007. Combining reinforcement learning with information-state update rules. In Human Language Technology Conference (HLT’07). 268--275.\n\n[35]\n\nJ. Henderson, O. Lemon, and K. Georgila. 2008. Hybrid reinforcement/supervised learning of dialogue policies from fixed data sets. Computational Linguistics 34, 4 (2008), 487--511.\n\n[36]\n\nS. Janarthanam, L. Lemon, X. Liu, P. Bartie, W. Mackaness, T. Dalmas, and J. Goetze. 2012. Integrating location, visibility, and question-answering in a spoken dialogue system for pedestrian city exploration. In Workshop on Semantics and Pragmatics of Dialogue (SEMDIAL’12).\n\n[37]\n\nF. Jurčíček, B. Thomson, and S. Young. 2011. Natural actor and belief critic: Reinforcement algorithm for learning parameters of dialogue systems modelled as POMDPs. ACM Transactions on Speech and Language Processing 7, 3 (2011).\n\n[38]\n\nS. Keizer, M. E. Foster, O. Lemon, A. Gaschler, and M. Giuliani. 2013. Training and evaluation of an MDP model for social multi-user human-robot interaction. In Annual Meeting on Discourse and Dialogue (SIGDIAL’13).\n\n[39]\n\nJ. Kober, J. A. Bagnell, and J. Peters. 2013. Reinforcement learning in robotics: A survey. International Journal of Robotics Research 32, 11 (2013).\n\n[40]\n\nI. Kruijff-Korbayová, H. Cuayáhuitl, B. Kiefer, M. Schröder, P. Cosi, G. Paci, G. Sommavilla, F. Tesser, H. Sahli, G. Athanasopoulos, W. Wang, V. Enescu, and W. Verhelst. 2012a. Spoken language processing in a conversational system for child-robot interaction. In INTERSPEECH Workshop on Child-Computer Interaction.\n\n[41]\n\nI. Kruijff-Korbayová, H. Cuayáhuitl, B. Kiefer, M. Schröder, P. Cosi, G. Paci, G. Sommavilla, F. Tesser, H. Sahli, G. Athanasopoulos, W. Wang, V. Enescu, and W. Verhelst. 2012b. A conversational system for multi-session child-robot interaction with several games. In German Conference on Artificial Intelligence (KI’12). System demonstration description.\n\n[42]\n\nO. Lemon. 2011. Learning what to say and how to say it: Joint optimization of spoken dialogue management and natural language generation. Computer Speech and Language 25, 2 (2011).\n\n[43]\n\nO. Lemon and O. Pietquin. 2007. Machine learning for spoken dialogue systems. In Annual Conference of the International Speech Communication Association (INTERSPEECH’07). 2685--2688.\n\n[44]\n\nE. Levin, R. Pieraccini, and W. Eckert. 2000. A stochastic model of human machine interaction for learning dialog strategies. IEEE Transactions on Speech and Audio Processing 8, 1 (2000), 11--23.\n\n[45]\n\nL. Li, D. J. Williams, and S. Balakrishnan. 2009. Reinforcement learning for dialog management using least-squares policy iteration and fast feature selection. In Annual Conference of the International Speech Communication Association (INTERSPEECH’09). 2475--2478.\n\n[46]\n\nD. Litman, M. Kearns, S. Singh, and M. Walker. 2000. Automatic optimization of dialogue management. In International Conference on Computational Linguistics (COLING’00). 502--508.\n\n[47]\n\nN. Mehta, S. Ray, P. Tadepalli, and T. G. Dietterich. 2008. Automatic discovery and transfer of MAXQ hierarchies. In International Conference on Machine Learning (ICML’08). 648--655.\n\n[48]\n\nN. Mitsunaga, C. Smith, T. Kanda, H. Ishiguro, and N. Hagita. 2005. Robot behavior adaptation for human-robot interaction based on policy gradient reinforcement learning. In International Conference on Intelligent Robots and Systems (IROS’05). 218--225.\n\n[49]\n\nM. Nalin, I. Baroni, I. Kruijff-Korbayová, L. Cañamero, M. Lewis, A. Beck, H. Cuayáhuitl, and A. Sanna. 2012. Children’s adaptation in multi-session interaction with a humanoid robot. In International Symposium on Robot and Human Interactive Communication (RO-MAN’12). 351--357.\n\n[50]\n\nO. Pietquin. 2011. Batch reinforcement learning for spoken dialogue systems with sparse value function approximation. In NIPS Workshop on Learning and Planning from Batch Time Series Data.\n\n[51]\n\nO. Pietquin, M. Geist, and S. Chandramohan. 2011. Sample-efficient batch reinforcement learning for dialogue management optimization. ACM Transactions on Speech and Language Processing 7, 3 (2011), 7.\n\n[52]\n\nJ. Pineau. 2004. Tractable Planning Under Uncertainty: Exploiting Structure. Ph.D. Dissertation. Carnegie Mellon University.\n\n[53]\n\nN. Roy, J. Pineau, and S. Thrun. 2000. Spoken dialogue management using probabilistic reasoning. In International Conference on Computational Linguistics (ACL’00). 93--100.\n\n[54]\n\nD. Schlangen and G. Skantze. 2009. A general, abstract model of incremental dialogue processing. In Conference of the European Chapter of the Association for Computational Linguistics (EACL’09).\n\n[55]\n\nS. Singh, D. Litman, M. Kearns, and M. Walker. 2002. Optimizing dialogue management with reinforcement learning: Experiments with the NJFun system. Journal of Artificial Intelligence Research 16 (2002), 105--133.\n\n[56]\n\nR. Stiefelhagen, H. K. Ekenel, C. Fügen, P. Gieselmann, H. Holzapfel, F. Kraft, K. Nickel, M. Voit, and A. Waibel. 2007. Enabling multimodal human-robot interaction for the Karlsruhe humanoid robot. IEEE Transactions on Robotics 23, 5 (2007), 840--851.\n\n[57]\n\nR. Sutton and A. Barto. 1998. Reinforcement Learning: An Introduction. MIT Press.\n\n[58]\n\nR. S. Sutton, D. Precup, and S. P. Singh. 1999. Between MDPs and Semi-MDPs: A framework for temporal abstraction in reinforcement learning. Artificial Intelligence 112, 1--2 (1999), 181--211.\n\n[59]\n\nC. Szepesvári. 2010. Algorithms for Reinforcement Learning. Morgan and Claypool.\n\n[60]\n\nA. L. Thomaz and C. Breazeal. 2006. Reinforcement learning with human teachers: Evidence of feedback and guidance with implications for learning performance. In AAAI Conference on Artificial Intelligence. 1000--1006.\n\n[61]\n\nB. Thomson. 2009. Statistical Methods for Spoken Dialogue Management. Ph.D. Dissertation. University of Cambridge.\n\n[62]\n\nM. Walker. 2000. An application of reinforcement learning to dialogue strategy selection in a spoken dialogue system for email. Journal of Artificial Intelligence Research 12 (2000), 387--416.\n\n[63]\n\nJ. Williams. 2007. Partially observable Markov decision processes for spoken dialog systems. Computer Speech and Language 21, 2 (2007), 393--422.\n\n[64]\n\nJ. Williams. 2008. The best of both worlds: Unifying conventional dialog systems and POMDPs. In Annual Conference of the International Speech Communication Association (INTERSPEECH). Brisbane, Australia.\n\n[65]\n\nS. Young. 2000. Probabilistic methods in spoken dialogue systems. Philosophical Transactions of the Royal Society (Series A) 358, 1769 (2000), 1389--1402.\n\n[66]\n\nY. Young, M. Gašić, S. Keizer, F. Mairesse, J. Schatzmann, B. Thomson, and K. Yu. 2010. The hidden information state model: A practical framework for POMDP-based spoken dialogue management. Computer Speech and Language 24, 2 (2010), 150--174.\n\n[67]\n\nV. Zue and J. Glass. 2000. Conversational interfaces: Advances and challenges. IEEE Transactions on Speech and Audio Processing 88, 8 (2000), 1166--1180.\n\nCited By\n\nView all\n\nReimann MKunneman FOertel CHindriks KA Survey on Dialogue Management in Human-robot InteractionACM Transactions on Human-Robot Interaction10.1145/364860513:2(1-22)\n\nUc-Cetina VNavarro-Guerrero NMartin-Gonzalez AWeber CWermter SSurvey on reinforcement learning for language processingArtificial Intelligence Review10.1007/s10462-022-10205-556:2(1543-1575)\n\nXiang XFoo SRecent Advances in Deep Reinforcement Learning Applications for Solving Partially Observable Markov Decision Processes (POMDP) Problems: Part 1—Fundamentals and Applications in Games, Robotics and Natural Language ProcessingMachine Learning and Knowledge Extraction10.3390/make30300293:3(554-581)\n\nShow More Cited By\n\nIndex Terms\n\nNonstrict Hierarchical Reinforcement Learning for Interactive Systems and Robots\n\nComputing methodologies\n\nMachine learning\n\nRecommendations\n\nSpatially-aware dialogue control using hierarchical reinforcement learning\n\nThis article addresses the problem of scalable optimization for spatially-aware dialogue systems. These kinds of systems must perceive, reason, and act about the spatial environment where they are embedded. We formulate the problem in terms of Semi-...\n\nSample-efficient batch reinforcement learning for dialogue management optimization\n\nSpoken Dialogue Systems (SDS) are systems which have the ability to interact with human beings using natural language as the medium of interaction. A dialogue policy plays a crucial role in determining the functioning of the dialogue management module. ...\n\nStatistical Spoken Dialogue Systems and the Challenges for Machine Learning\n\nWSDM '17: Proceedings of the Tenth ACM International Conference on Web Search and Data Mining\n\nThis talk will review the principal components of a spoken dialogue system and then discuss the opportunities for applying machine learning for building robust high performance open-domain systems. The talk will be illustrated by recent work at ...\n\nInformation & Contributors\n\nInformation\n\nPublished In\n\n115 pages\n\nISSN:2160-6455\n\nEISSN:2160-6463\n\nDOI:10.1145/2660857\n\nEditors:\n\nAnthony Jameson\n\nGerman Research Center for Artifi cial Intelligence (DFKI), Germany\n\n,\n\nKrzysztof Gajos\n\nHarvard University, U.S.A.\n\nIssue’s Table of Contents\n\nCopyright © 2014 ACM.\n\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from [email protected].\n\nPublisher\n\nAssociation for Computing Machinery\n\nNew York, NY, United States\n\nPublication History\n\nPublished: 14 October 2014\n\nAccepted: 01 July 2014\n\nRevised: 01 June 2014\n\nReceived: 01 March 2013\n\nPublished in TIIS Volume 4, Issue 3\n\nPermissions\n\nRequest permissions for this article.\n\nCheck for updates\n\nAuthor Tags\n\nInteractive robots\n\nflexible interaction\n\nfunction approximation\n\nhierarchical control\n\nhuman--robot interaction\n\nmachine learning\n\nreinforcement learning\n\nspoken dialogue systems\n\nuser simulation\n\nQualifiers\n\nResearch-article\n\nResearch\n\nRefereed\n\nFunding Sources\n\nSeventh Framework Programme\n\nContributors\n\nOther Metrics\n\nBibliometrics & Citations\n\nBibliometrics\n\nArticle Metrics\n\n5\n\nTotal Citations\n\nView Citations\n\n278\n\nTotal Downloads\n\nDownloads (Last 12 months)5\n\nDownloads (Last 6 weeks)0\n\nOther Metrics\n\nCitations\n\nCited By\n\nView all\n\nReimann MKunneman FOertel CHindriks KA Survey on Dialogue Management in Human-robot InteractionACM Transactions on Human-Robot Interaction10.1145/364860513:2(1-22)\n\nUc-Cetina VNavarro-Guerrero NMartin-Gonzalez AWeber CWermter SSurvey on reinforcement learning for language processingArtificial Intelligence Review10.1007/s10462-022-10205-556:2(1543-1575)\n\nXiang XFoo SRecent Advances in Deep Reinforcement Learning Applications for Solving Partially Observable Markov Decision Processes (POMDP) Problems: Part 1—Fundamentals and Applications in Games, Robotics and Natural Language ProcessingMachine Learning and Knowledge Extraction10.3390/make30300293:3(554-581)\n\nCuayáhuitl HSimpleDS: A Simple Deep Reinforcement Learning Dialogue SystemDialogues with Social Robots10.1007/978-981-10-2585-3_8(109-118)\n\nCuayáhuitl HFrommberger LDethlefs NRaux AMarge MZender HIntroduction to the Special Issue on Machine Learning for Multiple Modalities in Interactive Systems and RobotsACM Transactions on Interactive Intelligent Systems (TiiS)10.1145/26705394:3(1-6)\n\nView Options\n\nGet Access\n\nLogin options\n\nCheck if you have access through your login credentials or your institution to get full access on this article.\n\nSign in\n\nFull Access\n\nView options\n\nPDF\n\nView or Download as a PDF file.\n\nPDF\n\neReader\n\nView online with eReader.\n\neReader\n\nMedia\n\nFigures\n\nOther\n\nTables\n\nShare\n\nShare\n\nShare this Publication link\n\nCopied!\n\nCopying failed.\n\nShare on social media\n\nAffiliations\n\nHeriberto Cuayáhuitl\n\nHeriot-Watt University, United Kingdom\n\nIvana Kruijff-Korbayová\n\nGerman Research Centre for Artificial Intelligence, Germany\n\nNina Dethlefs\n\nHeriot-Watt University, United Kingdom\n\nRequest permissions Authors Info & Affiliations"
    }
}
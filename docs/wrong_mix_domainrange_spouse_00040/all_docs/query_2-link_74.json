{
    "id": "wrong_mix_domainrange_spouse_00040_2",
    "rank": 74,
    "data": {
        "url": "https://dl.acm.org/doi/10.1145/3503161.3548002",
        "read_more_link": "",
        "language": "en",
        "title": "D2Animator: Dual Distillation of StyleGAN For High-Resolution Face Animation",
        "top_image": "https://dl.acm.org/cms/asset/51415ab9-4d79-4273-ae24-79bf02a4e07c/3503161.3548002.key.jpg",
        "meta_img": "https://dl.acm.org/cms/asset/51415ab9-4d79-4273-ae24-79bf02a4e07c/3503161.3548002.key.jpg",
        "images": [
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/acm-dl-logo-white-1ecfb82271e5612e8ca12aa1b1737479.png",
            "https://dl.acm.org/doi/10.1145/specs/products/acm/releasedAssets/images/acm-logo-1-ad466e729c8e2a97780337b76715e5cf.png",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/footer-logo1-45ae33115db81394d8bd25be65853b77.png",
            "https://dl.acm.org/cms/asset/51415ab9-4d79-4273-ae24-79bf02a4e07c/3503161.3548002.key.jpg",
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/Default_image_lazy-0687af31f0f1c8d4b7a22b686995ab9b.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/action/showDoPubAsset?doi=10.1145/contrib-81100074462&format=rel-imgonly&assetId=adbc.jpg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/loader-7e60691fbe777356dc81ff6d223a82a6.gif",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/acm-logo-dl-8437178134fce530bc785276fc316cbf.png",
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/acm-logo-3-10aed79f3a6c95ddb67053b599f029af.png"
        ],
        "movies": [
            "https://iframe.videodelivery.net/eyJraWQiOiI3YjgzNTg3NDZlNWJmNDM0MjY5YzEwZTYwMDg0ZjViYiIsImFsZyI6IlJTMjU2In0.eyJzdWIiOiI4NDAyMzk5NWZmNjNkNWY2YmRhYTk3NzhkZWFlY2E5YyIsImV4cCI6MTcyMTc2NTA1Niwia2lkIjoiN2I4MzU4NzQ2ZTViZjQzNDI2OWMxMGU2MDA4NGY1YmIifQ.XFyKQBI88A_zqopMdPvAN5ZuMiyyTMRNfXe9ePjddxxdiv18tusziEA9e3_RrseO6NZfM446VXHY5v4dfOlSbiUQCQtHqyzi4xbmn7OzNJLo-ymRezORfd0ddCOueglbRc7lT8yFzloVJsZn35I8Dsv4yHpomSu0_IAbeiY85Gjt-WCxyXXSX5KVRxVba_7d8lBx51fOEOYuZwg7VckcHDl59MEjpn0PmrZtFMPRWehVg7pB6Nv31nowwkp4JTKRiBV7x3H722FokNSTRFfH53LO-WR1GlQnfxrKihzyyBNHESCnqpSQdT_HtBU57VdX5lxz3Hksmb1BtssaLnpYjw?poster=https%3A%2F%2Fvideodelivery.net%2FeyJraWQiOiI3YjgzNTg3NDZlNWJmNDM0MjY5YzEwZTYwMDg0ZjViYiIsImFsZyI6IlJTMjU2In0.eyJzdWIiOiI4NDAyMzk5NWZmNjNkNWY2YmRhYTk3NzhkZWFlY2E5YyIsImV4cCI6MTcyMTc2NTA1Niwia2lkIjoiN2I4MzU4NzQ2ZTViZjQzNDI2OWMxMGU2MDA4NGY1YmIifQ.XFyKQBI88A_zqopMdPvAN5ZuMiyyTMRNfXe9ePjddxxdiv18tusziEA9e3_RrseO6NZfM446VXHY5v4dfOlSbiUQCQtHqyzi4xbmn7OzNJLo-ymRezORfd0ddCOueglbRc7lT8yFzloVJsZn35I8Dsv4yHpomSu0_IAbeiY85Gjt-WCxyXXSX5KVRxVba_7d8lBx51fOEOYuZwg7VckcHDl59MEjpn0PmrZtFMPRWehVg7pB6Nv31nowwkp4JTKRiBV7x3H722FokNSTRFfH53LO-WR1GlQnfxrKihzyyBNHESCnqpSQdT_HtBU57VdX5lxz3Hksmb1BtssaLnpYjw%2Fthumbnails%2Fthumbnail.jpg%3Ftime%3D10.0s"
        ],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Tsinghua University",
            "China View Profile",
            "Australia View Profile",
            "Zhuo Chen",
            "Chaoyue Wang",
            "Haimei Zhao",
            "Bo Yuan",
            "Xiu Li"
        ],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "/pb-assets/head-metadata/apple-touch-icon-1574252172393.png",
        "meta_site_name": "ACM Conferences",
        "canonical_link": "https://dl.acm.org/doi/10.1145/3503161.3548002",
        "text": "Abstract\n\nThe style-based generator architectures (e.g. StyleGAN v1, v2) largely promote the controllability and explainability of Generative Adversarial Networks (GANs). Many researchers have applied the pretrained style-based generators to image manipulation and video editing by exploring the correlation between linear interpolation in the latent space and semantic transformation in the synthesized image manifold. However, most previous studies focused on manipulating separate discrete attributes, which is insufficient to animate a still image to generate videos with complex and diverse poses and expressions. In this work, we devise a dual distillation strategy (D2Animator) for generating animated high-resolution face videos conditioned on identities and poses from different images. Specifically, we first introduce a Clustering-based Distiller (CluDistiller) to distill diverse interpolation directions in the latent space, and synthesize identity-consistent faces with various poses and expressions, such as blinking, frowning, looking up/down, etc. Then we propose an Augmentation-based Distiller (AugDistiller) that learns to encode arbitrary face deformation into a combination of interpolation directions via training on augmentation samples synthesized by CluDistiller. Through assembling the two distillation methods, D2Animator can generate high-resolution face animation videos without training on video sequences. Extensive experiments on self-driving, cross-identity and sequence-driving tasks demonstrate the superiority of the proposed D2Animator over existing StyleGAN manipulation and face animation methods in both generation quality and animation fidelity.\n\nSupplementary Material\n\nMP4 File (MM2022_camera_ready_d_2_animator_dual_distillation.mp4)\n\n\"Presentation video\", \"High-Resolution Face Animation\"\n\nDownload\n\n21.74 MB\n\nReferences\n\n[1]\n\nRameen Abdal, Yipeng Qin, and Peter Wonka. 2019. Image2stylegan: how to embed images into the stylegan latent space? In Proceedings of the IEEE/CVF International Conference on Computer Vision, 4432--4441.\n\n[2]\n\nRameen Abdal, Yipeng Qin, and Peter Wonka. 2020. Image2stylegan: how to edit the embedded images? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 8296--8305.\n\n[3]\n\nRameen Abdal, Peihao Zhu, Niloy Mitra, and Peter Wonka. 2021. Labels4free: unsupervised segmentation using stylegan. arXiv preprint arXiv:2103.14968.\n\n[4]\n\nRameen Abdal, Peihao Zhu, Niloy J Mitra, and Peter Wonka. 2021. Styleflow: attribute-conditioned exploration of stylegan-generated images using conditional continuous normalizing flows. ACM Transactions on Graphics (TOG), 40, 3, 1--21.\n\n[5]\n\nHadar Averbuch-Elor, Daniel Cohen-Or, Johannes Kopf, and Michael F Cohen. 2017. Bringing portraits to life. ACM Transactions on Graphics (TOG), 36, 6, 196.\n\n[6]\n\nDmitri Bitouk, Neeraj Kumar, Samreen Dhillon, Peter Belhumeur, and Shree K Nayar. 2008. Face swapping: automatically replacing faces in photographs. In ACM Transactions on Graphics (TOG) number 3. Vol. 27. ACM, 39.\n\n[7]\n\nKonstantinos Bousmalis, Nathan Silberman, David Dohan, Dumitru Erhan, and Dilip Krishnan. 2017. Unsupervised pixel-level domain adaptation with generative adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), 3722--3731.\n\n[8]\n\nAdrian Bulat and Georgios Tzimiropoulos. 2017. How far are we from solving the 2d & 3d face alignment problem? (and a dataset of 230,000 3d facial landmarks). In International Conference on Computer Vision.\n\n[9]\n\nRenwang Chen, Xuanhong Chen, Bingbing Ni, and Yanhao Ge. 2020. Simswap: an efficient framework for high fidelity face swapping. In Proceedings of the 28th ACM International Conference on Multimedia, 2003--2011.\n\n[10]\n\nZhuo Chen, Chaoyue Wang, Bo Yuan, and Dacheng Tao. 2020. Puppeteergan: arbitrary portrait animation with semantic-aware appearance transformation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 13518--13527.\n\n[11]\n\nMengyu Chu, You Xie, Jonas Mayer, Laura Leal-Taixé, and Nils Thuerey. 2020. Learning temporal coherence via self-supervision for gan-based video generation. ACM Transactions on Graphics (TOG), 39, 4, 75--1.\n\n[12]\n\nEdo Collins, Raja Bala, Bob Price, and Sabine Susstrunk. 2020. Editing in style: uncovering the local semantics of gans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 5771--5780.\n\n[13]\n\nJiankang Deng, Jia Guo, Xue Niannan, and Stefanos Zafeiriou. 2019. Arcface: additive angular margin loss for deep face recognition. In CVPR.\n\n[14]\n\nMichail Christos Doukas, Stefanos Zafeiriou, and Viktoriia Sharmanska. 2021. Headgan: one-shot neural head synthesis and editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 14398--14407.\n\n[15]\n\nYuki Endo and Yoshihiro Kanamori. 2021. Few-shot semantic image synthesis using stylegan prior. arXiv preprint arXiv:2103.14877.\n\n[16]\n\nPablo Garrido, Levi Valgaerts, Ole Rehmsen, Thorsten Thormahlen, Patrick Perez, and Christian Theobalt. 2014. Automatic face reenactment. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 4217--4224.\n\n[17]\n\nIan J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. 2014. Generative adversarial nets. In NIPS.\n\n[18]\n\nHao Guan, Chaoyue Wang, and Dacheng Tao. 2021. Mri-based Alzheimer's disease prediction via distilling the knowledge in multi-modal data. NeuroImage, 244, 118586.\n\n[19]\n\nYuxuan Han, Jiaolong Yang, and Ying Fu. 2021. Disentangled face attribute editing via instance-aware latent space search. arXiv preprint arXiv:2105.12660.\n\n[20]\n\nErik Härkönen, Aaron Hertzmann, Jaakko Lehtinen, and Sylvain Paris. 2020. Ganspace: discovering interpretable gan controls. arXiv preprint arXiv:2004.02546.\n\n[21]\n\nFengxiang He and Dacheng Tao. 2020. Recent advances in deep learning theory. arXiv preprint arXiv:2012.10931.\n\n[22]\n\nFengxiang He, Bohan Wang, and Dacheng Tao. 2020. Piecewise linear activations substantially shape the loss surfaces of neural networks. In International Conference on Learning Representations.\n\n[23]\n\nKaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 770--778.\n\n[24]\n\nXun Huang and Serge Belongie. 2017. Arbitrary style transfer in real-time with adaptive instance normalization. In Proceedings of the IEEE International Conference on Computer Vision, 1501--1510.\n\n[25]\n\nAapo Hyvärinen and Erkki Oja. 2000. Independent component analysis: algorithms and applications. Neural networks : the official journal of the International Neural Network Society, 13 4--5, 411--30.\n\n[26]\n\nOmer Kafri, Or Patashnik, Yuval Alaluf, and Daniel Cohen-Or. 2021. Stylefusion: a generative model for disentangling spatial segments. arXiv preprint arXiv:2107.07437.\n\n[27]\n\nTero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. 2020. Training generative adversarial networks with limited data. arXiv preprint arXiv:2006.06676.\n\n[28]\n\nTero Karras, Samuli Laine, and Timo Aila. 2019. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 4401--4410.\n\n[29]\n\nTero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. 2020. Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 8110--8119.\n\n[30]\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. 2012. Imagenet classification with deep convolutional neural networks. Communications of the ACM, 60, 84--90.\n\n[31]\n\nSam Kwong, Jialu Huang, and Jing Liao. 2021. Unsupervised image-to-image translation via pre-trained stylegan2 network. IEEE Transactions on Multimedia.\n\n[32]\n\nCheng-Han Lee, Ziwei Liu, Lingyun Wu, and Ping Luo. 2020. Maskgan: towards diverse and interactive facial image manipulation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n\n[33]\n\nKathleen M Lewis, Srivatsan Varadharajan, and Ira Kemelmacher-Shlizerman. 2021. Vogue: try-on by stylegan interpolation optimization. arXiv preprint arXiv:2101.02285.\n\n[34]\n\nDaiqing Li, Junlin Yang, Karsten Kreis, Antonio Torralba, and Sanja Fidler. 2021. Semantic segmentation with generative models: semi-supervised learning and strong out-of-domain generalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 8300--8311.\n\n[35]\n\nLingzhi Li, Jianmin Bao, Hao Yang, Dong Chen, and FangWen. 2020. Advancing high fidelity identity swapping for forgery detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 5074--5083.\n\n[36]\n\nHuan Ling, Karsten Kreis, Daiqing Li, Seung Wook Kim, Antonio Torralba, and Sanja Fidler. 2021. Editgan: high-precision semantic image editing. In Thirty-Fifth Conference on Neural Information Processing Systems.\n\n[37]\n\nYu-Ding Lu, Hsin-Ying Lee, Hung-Yu Tseng, and Ming-Hsuan Yang. 2020. Unsupervised discovery of disentangled manifolds in gans. arXiv preprint arXiv:2011.11842.\n\n[38]\n\nTianxiang Ma, Dongze Li,WeiWang, and Jing Dong. 2021. Face anonymization by manipulating decoupled identity representation. arXiv preprint arXiv:2105.11137.\n\n[39]\n\nArsha Nagrani, Joon Son Chung, and Andrew Zisserman. 2017. Voxceleb: a large-scale speaker identification dataset. Telephony, 3, 33--039.\n\n[40]\n\nYuval Nirkin, Yosi Keller, and Tal Hassner. 2019. Fsgan: subject agnostic face swapping and reenactment. In Proceedings of the IEEE/CVF international conference on computer vision, 7184--7193.\n\n[41]\n\nTaesung Park, Ming-Yu Liu, Ting-ChunWang, and Jun-Yan Zhu. 2019. Semantic image synthesis with spatially-adaptive normalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2337--2346.\n\n[42]\n\nOr Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. 2021. Styleclip: text-driven manipulation of stylegan imagery. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2085--2094.\n\n[43]\n\nAntoine Plumerault, Hervé Le Borgne, and Céline Hudelot. 2020. Controlling generative models with continuous factors of variations. arXiv preprint arXiv:2001.10238.\n\n[44]\n\nElad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, and Daniel Cohen-Or. 2021. Encoding in style: a stylegan encoder for image-to-image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2287--2296.\n\n[45]\n\nTim Salimans, Ian J. Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. 2016. Improved techniques for training gans. In NIPS, 2226--2234. http://papers.nips.cc/paper/6125-improved-techniques-for-training-ga-ns.\n\n[46]\n\nYujun Shen, Jinjin Gu, Xiaoou Tang, and Bolei Zhou. 2020. Interpreting the latent space of gans for semantic face editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 9243--9252.\n\n[47]\n\nYujun Shen and Bolei Zhou. 2021. Closed-form factorization of latent semantics in gans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1532--1540.\n\n[48]\n\nAliaksandr Siarohin, Stéphane Lathuilière, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. 2019. Animating arbitrary objects via deep motion transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2377--2386.\n\n[49]\n\nAliaksandr Siarohin, Stéphane Lathuilière, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. 2019. First order motion model for image animation. Advances in Neural Information Processing Systems, 32, 7137--7147.\n\n[50]\n\nAliaksandr Siarohin, Oliver J Woodford, Jian Ren, Menglei Chai, and Sergey Tulyakov. 2021. Motion representations for articulated animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 13653--13662.\n\n[51]\n\nRobert R. Sokal and Peter H. A. Sneath. 1961. Principles of Numerical Taxonomy. W. H. Freeman.\n\n[52]\n\nAyush Tewari, Mohamed Elgharib, Gaurav Bharaj, Florian Bernard, Hans-Peter Seidel, Patrick Pérez, Michael Zollhofer, and Christian Theobalt. 2020. Stylerig: rigging stylegan for 3d control over portrait images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 6142--6151.\n\n[53]\n\nYu Tian, Jian Ren, Menglei Chai, Kyle Olszewski, Xi Peng, Dimitris N Metaxas, and Sergey Tulyakov. 2021. A good image generator is what you need for high-resolution video synthesis. arXiv preprint arXiv:2104.15069.\n\n[54]\n\nSergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. 2018. Mocogan: decomposing motion and content for video generation. In Proceedings of the IEEE conference on computer vision and pattern recognition, 1526--1535.\n\n[55]\n\nTengfei Wang, Yong Zhang, Yanbo Fan, Jue Wang, and Qifeng Chen. 2022. High-fidelity gan inversion for image attribute editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).\n\n[56]\n\nTing-Chun Wang, Ming-Yu Liu, Andrew Tao, Guilin Liu, Jan Kautz, and Bryan Catanzaro. 2019. Few-shot video-to-video synthesis. In Proceedings of the 33rd International Conference on Neural Information Processing Systems, 5013--5024.\n\n[57]\n\nTing-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. 2018. Video-to-video synthesis. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, 1152--1164.\n\n[58]\n\nTing-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. 2018. High-resolution image synthesis and semantic manipulation with conditional gans. In Proceedings of the IEEE conference on computer vision and pattern recognition, 8798--8807.\n\n[59]\n\nTing-Chun Wang, Arun Mallya, and Ming-Yu Liu. 2021. One-shot free-view neural talking-head synthesis for video conferencing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 10039--10049.\n\n[60]\n\nYaohuiWang, Di Yang, Francois Bremond, and Antitza Dantcheva. 2021. Latent image animator: learning to animate images via latent space navigation. In International Conference on Learning Representations.\n\n[61]\n\nOlivia Wiles, A Koepke, and Andrew Zisserman. 2018. X2face: a network for controlling face generation using images, audio, and pose codes. In Proceedings of the European conference on computer vision (ECCV), 670--686.\n\n[62]\n\nJianjin Xu and Changxi Zheng. 2021. Linear semantics in generative adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 9351--9360.\n\n[63]\n\nCeyuan Yang, Yujun Shen, and Bolei Zhou. 2021. Semantic hierarchy emerges in deep generative representations for scene synthesis. International Journal of Computer Vision, 129, 5, 1451--1466.\n\n[64]\n\nZuopeng Yang, Daqing Liu, Chaoyue Wang, Jie Yang, and Dacheng Tao. 2022. Modeling image composition for complex scene generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 7764--7773.\n\n[65]\n\nXu Yao, Alasdair Newson, Yann Gousseau, and Pierre Hellier. 2021. A latent transformer for disentangled face editing in images and videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 13789--13798.\n\n[66]\n\nFei Yin et al. 2022. Styleheat: one-shot high-resolution editable talking face generation via pretrained stylegan. arXiv preprint arXiv:2203.04036.\n\n[67]\n\nShan You, Tao Huang, Mingmin Yang, Fei Wang, Chen Qian, and Changshui Zhang. 2020. Greedynas: towards fast one-shot nas with greedy supernet. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1999--2008.\n\n[68]\n\nShan You, Chang Xu, Chao Xu, and Dacheng Tao. 2017. Learning from multiple teacher networks. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 1285--1294.\n\n[69]\n\nBaosheng Yu and Dacheng Tao. 2021. Heatmap regression via randomized rounding. IEEE Transactions on Pattern Analysis and Machine Intelligence.\n\n[70]\n\nEgor Zakharov, Aleksei Ivakhnenko, Aliaksandra Shysheya, and Victor Lempitsky. 2020. Fast bi-layer neural synthesis of one-shot realistic head avatars. In European Conference of Computer vision (ECCV). (Aug. 2020).\n\n[71]\n\nEgor Zakharov, Aliaksandra Shysheya, Egor Burkov, and Victor Lempitsky. 2019. Few-shot adversarial learning of realistic neural talking head models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 9459--9468.\n\n[72]\n\nYuxuan Zhang, Huan Ling, Jun Gao, Kangxue Yin, Jean-Francois Lafleche, Adela Barriuso, Antonio Torralba, and Sanja Fidler. 2021. Datasetgan: efficient labeled data factory with minimal human effort. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 10145--10155.\n\n[73]\n\nHaimei Zhao, Wei Bian, Bo Yuan, and Dacheng Tao. 2020. Collaborative learning of depth estimation, visual odometry and camera relocalization from monocular videos. In IJCAI, 488--494.\n\n[74]\n\nYuhao Zhu, Qi Li, Jian Wang, Cheng-Zhong Xu, and Zhenan Sun. 2021. One shot face swapping on megapixels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 4834--4844.\n\nCited By\n\nView all\n\nZhao HZhang JChen ZYuan BTao DOn Robust Cross-view Consistency in Self-supervised Monocular Depth EstimationMachine Intelligence Research10.1007/s11633-023-1474-021:3(495-513)\n\nIndex Terms\n\nD2Animator: Dual Distillation of StyleGAN For High-Resolution Face Animation\n\nComputing methodologies\n\nComputer graphics\n\nImage manipulation\n\nRecommendations\n\nAudio-driven talking face generation with diverse yet realistic facial animations\n\nAbstract\n\nAudio-driven talking face generation, which aims to synthesize talking faces with realistic facial animations (including accurate lip movements, vivid facial expression details and natural head poses) corresponding to the audio, has achieved ...\n\nHighlights\n\nGenerate diverse yet realistic talking faces from the same input audio.\n\nNetwork for modelling the uncertain relations between audio and visual signals.\n\nNovel technique that enables to generate temporally coherent talking faces.\n\nGANimation: Anatomically-Aware Facial Animation from a Single Image\n\nComputer Vision – ECCV 2018\n\nAbstract\n\nRecent advances in Generative Adversarial Networks (GANs) have shown impressive results for task of facial expression synthesis. The most successful architecture is StarGAN, that conditions GANs’ generation process with images of a specific domain,...\n\nA virtual teleconferencing system based on face detection and 3D animation in a low‐bandwidth environment\n\nIn this article, we proposed a novel teleconferencing system that combines a facial muscle model and the techniques of face detection and facial feature extraction to synthesize a sequence of life‐like face animation. The proposed system can animate ...\n\nInformation & Contributors\n\nInformation\n\nPublished In\n\n7537 pages\n\nISBN:9781450392037\n\nDOI:10.1145/3503161\n\nGeneral Chairs:\n\nJoão Magalhães\n\nNOVA University of Lisbon, Portugal\n\n,\n\nAlberto del Bimbo\n\nUniversity of Florence, Italy\n\n,\n\nShin'ichi Satoh\n\nNational Institute of Informatics, Japan\n\n,\n\nNicu Sebe\n\nUniversity of Trento, Italy\n\n,\n\nProgram Chairs:\n\nXavier Alameda-Pineda\n\nInria, Grenoble, France\n\n,\n\nQin Jin\n\nRenmin University of China, China\n\n,\n\nVincent Oria\n\nNew Jersey Institute of Technology, USA\n\n,\n\nLaura Toni\n\nUniversity College London, UK\n\nCopyright © 2022 Owner/Author.\n\nThis work is licensed under a Creative Commons Attribution International 4.0 License.\n\nPublisher\n\nAssociation for Computing Machinery\n\nNew York, NY, United States\n\nPublication History\n\nPublished: 10 October 2022\n\nPermissions\n\nRequest permissions for this article.\n\nCheck for updates\n\nAuthor Tags\n\nGANs\n\nface animation\n\nhigh-resolution image generation\n\nQualifiers\n\nResearch-article\n\nFunding Sources\n\nScience and Technology Innovation 2030 \"\"Brain Science and Brain-like Research\" Major Project\n\nConference\n\nMM '22\n\nAcceptance Rates\n\nOverall Acceptance Rate 995 of 4,171 submissions, 24%\n\nUpcoming Conference\n\nContributors\n\nOther Metrics\n\nBibliometrics & Citations\n\nBibliometrics\n\nArticle Metrics\n\n1\n\nTotal Citations\n\nView Citations\n\n499\n\nTotal Downloads\n\nDownloads (Last 12 months)220\n\nDownloads (Last 6 weeks)19\n\nOther Metrics\n\nCitations\n\nCited By\n\nView all\n\nZhao HZhang JChen ZYuan BTao DOn Robust Cross-view Consistency in Self-supervised Monocular Depth EstimationMachine Intelligence Research10.1007/s11633-023-1474-021:3(495-513)\n\nView Options\n\nView options\n\nPDF\n\nView or Download as a PDF file.\n\nPDF\n\neReader\n\nView online with eReader.\n\neReader\n\nGet Access\n\nLogin options\n\nCheck if you have access through your login credentials or your institution to get full access on this article.\n\nSign in\n\nFull Access\n\nMedia\n\nFigures\n\nOther\n\nTables\n\nShare\n\nShare\n\nShare this Publication link\n\nCopied!\n\nCopying failed.\n\nShare on social media\n\nAffiliations\n\nZhuo Chen\n\nTsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, China\n\nChaoyue Wang\n\nJD Explore Academy, Beijing, China\n\nHaimei Zhao\n\nThe University of Sydney, Sydney, NSW, Australia\n\nBo Yuan\n\nQianyuan Institute of Sciences, Hangzhou, China\n\nXiu Li\n\nTsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, China\n\nRequest permissions Authors Info & Affiliations"
    }
}
{
    "id": "correct_publicationDate_00092_3",
    "rank": 92,
    "data": {
        "url": "https://medium.com/%40cassidybeevemorris/determining-best-science-fiction-fantasy-novels-since-1970-e232ecbdc34d",
        "read_more_link": "",
        "language": "en",
        "title": "Determining the greatest Science Fiction & Fantasy novels since 1970",
        "top_image": "https://miro.medium.com/v2/resize:fit:1200/1*Y8m3jx_9wDC79gzqAXZV5w.jpeg",
        "meta_img": "https://miro.medium.com/v2/resize:fit:1200/1*Y8m3jx_9wDC79gzqAXZV5w.jpeg",
        "images": [
            "https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png",
            "https://miro.medium.com/v2/resize:fill:88:88/1*Fg2jtosckcOR5Zps_z-f7w.jpeg",
            "https://miro.medium.com/v2/resize:fill:144:144/1*Fg2jtosckcOR5Zps_z-f7w.jpeg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Cassidy Beeve-Morris",
            "medium.com"
        ],
        "publish_date": "2022-01-25T17:49:58.895000+00:00",
        "summary": "",
        "meta_description": "Using book awards (Hugo, Nebula, ...) to build a definitive list of the greatest science fiction and fantasy novels, series, & authors of the past 50 years.",
        "meta_lang": "en",
        "meta_favicon": "https://miro.medium.com/v2/1*m-R_BkNf1Qjr1YbyOIJY2w.png",
        "meta_site_name": "Medium",
        "canonical_link": "https://medium.com/@cassidybeevemorris/determining-best-science-fiction-fantasy-novels-since-1970-e232ecbdc34d",
        "text": "Leveraging SFF awards to calculate a definitive ‘best of’ list\n\nThe one habit I actually stuck with throughout quarantine is reading. In particular, reading all of the science fiction and fantasy novels I can get my hands on. For an hour or two a day, I escape COVID to a distant world.\n\nLike any good obsession, this one is full-on. Since the early days of the pandemic, I’ve read 70 or so science fiction and fantasy novels. I frequent a myriad of SFF communities online. I even started collecting rare and limited editions of my favorites: an expensive hobby that thoroughly confuses most of my family and friends, but does mean I spent an inordinate amount of time picking out first edition covers for all the graphics in this article (you’re welcome).\n\nHowever, as someone trying to fully engross themselves in a new hobby, I was surprised at how challenging it is to determine which novels are critical to my education.\n\nWhich works must I read first? Which authors have I missed out on?\n\nTo be fair, I do know the classics. I know Asimov, Tolkien, Dune, Heinlein, Bradbury, 1984, and A Canticle for Leibowitz. But, for many, our education stops there, which only gets you to the 1960s at best. What about the past 50 years? Are none of these works and authors worthy of canonizing?\n\nTo try to figure this out, I first turned to Goodreads, where unfortunately the answer is typically the latest Young Adult vampire novel. The question posed to my favorite hive mind on Reddit often elicits Ian Banks’ ‘Culture’ series; although the order to read them in is up for much debate. On Facebook, the 50K strong fantasy group will unequivocally tell you to read the 10-book epic that is ‘Malazan Book of the Fallen’. No matter the question, in fact, the answer appears to be “MALAZAN!!”\n\nWhile each of these communities and suggestions has its merit (reading is subjective after all), it got me wondering… could there be a more objective answer? Is there a dataset that could yield a more robust list?\n\nNow, one of my favorite past-times is using my amateur programming and data science skills to make the world a bit easier to understand. My only other article on Medium runs through a similar exercise to summarize how major cities in the United States will be effected by climate change. But after nearly 2 years of a horrific pandemic, it was time to switch gears to something a bit more... diverting.\n\nAnd the winner goes to…\n\nThankfully, each year groups of fans and people in the industry ask this very same question, perform their own mini-algorithms, and broadcast the results. Of course, I’m talking about awards; and in particular, the award for best novel. It turns out there’s a lot of them, each with their own eligibility rules, committees, number of nominees, and more. Plenty to sink our teeth into.\n\nBut before we get to methodology, no discussion of awards is without its own controversy and debate. (If you haven’t gone down the rabbit hole of puppygate and the Hugos, good on you). Still, awards do have a few pleasing properties that cannot be found in datasets elsewhere.\n\nFirst, they are awarded at the same time as the work was written. There’s no recency bias, or rose colored glasses to tint people’s judgement. In contrast, consider this list by Bookriot which has some great novels on it and even goes all the way back to Frankenstein (1818). Yet, by the end, nearly half of the “most influential sci-fi books of all time” were written in just the past 5 years. Or, take the Goodreads ratings for Fahrenheit 451, a bona fide classic by Ray Bradbury. While a 3.98 is respectable, all of these reviews came decades after the book was first published — and typically, let’s face it, from a classroom full of high school students that had to read it for an assignment. When the Goodreads reviews suggest that Vampire Academy (4.11 rating) is better than Fahrenheit 451, it’s time to look elsewhere for our source of truth.\n\nSecond, awards are granted by groups of people immersed in the industry. Oftentimes by editors, writers, and publishers themselves. People who read lots of sci-fi/fantasy (not students tasked with an assignment). Now, each tight-knit communities have their quirks, so we are going to run into issues of bias and prejudice (especially for older awards). While there is no algorithm that can erase this bias, we can do a bit of crowdsourcing — pulling in lots of awards, from lots of communities — to try and provide a slightly more robust view than that of each award alone.\n\nLastly, before we go any further, one final note on what awards are and aren’t. Book awards do not typically signal the “most popular”or easiest reads. Similar to the Oscars for best movie, the winner is rarely the latest blockbuster (as far as I know, The Fast and Furious franchise has yet to win). Many of my favorite vacation reads have not won an award. Instead, in theory, awards attempt to identify work that gives us a unique glimpse into the depth of humanity. Award winners, hopefully, help us truly see the world (and ourselves) more clearly than before. For my sci-fi/fantasy education, that will do.\n\nDesigning the algorithm\n\nAlrighty, so we have a dozen or so awards with varying tenures and goals across 50+ years. The challenge now is, how do we combine this treasure trove of knowledge into a single, easy-to-digest ranking?\n\nChoosing the awards\n\nI pulled as many major sci-fi/fantasy awards as I could: Hugo (est. 1953), Nebula (1966), British SF Association (1970), Locus (1971), British Fantasy (1972), Campbell (1973), World Fantasy (1975), Philip K. Dick (1983), Arthur C. Clarke (1987), and Dragon awards (2016). Further, I included the Goodreads Choice Awards (est. 2011) for best science fiction novel and best fantasy novel. The Goodreads Choice Awards fairly elegantly compromise between an editor curated list of nominees (15 nominees via editors) and leveraging the masses (additional 5 nominees, plus voting on the winner). A ton of thanks to The Science Fiction Awards Database and Locus Foundation for their work to catalogue a lot of this data.\n\nDefining our weights\n\nWe have to start here, but to be honest, this is (perhaps surprisingly) the least interesting part of the problem. Weighting these awards was less about subjective preference (although I will admit that the relatively new “Dragon awards” are discounted until they demonstrate a bit more tenure), and more about balancing genres and regions. Meaning, a fantasy novel published in the US should have an equal opportunity for points as a science fiction novel published in the UK. Further, it’s important that we don’t reward a committee for dialing up their number of nominations or winners. For example, have a gander at the nominees and winners for the Locus Awards in 1985. You’ll need to scroll a few times to see the full list, and that’s the point. Being nominated for a Locus Award this year meant that you were 1 of the top 57 books (!) published.\n\nOne must wonder what poor books were not nominated? Compare that to the Hugos from the same year:\n\nIt would be criminal to weight a Locus nomination as highly as a Hugo this year. Further, perhaps more subtly, the Locus award typically grants two winners — one for each genre. Again, we need to control for this.\n\nThe greatest novels of each year (1970–2021)\n\nA ton of data mining and coalescing, sprinkled with the simple algorithm described above, and we can already start to sort books within a given year (we still can’t compare books across years however). Let’s take a look: The following graphics show the top ranked book from every year in our awards data. Note that all years quoted in this article are the year of the award, which is almost always 1 year after the publishing date.\n\nIn our current algorithm, Gateway is the clear winner, having won all the same awards as Speaker for the Dead, plus pulling out a victory in the John W. Campbell awards (Speaker was nominated, but lost). Further, there were less awards to win (7 in 1978 vs 9 in 1987), so Gateway won a greater proportion.\n\nTo truly answer this question though, we need to confront two glaring asymmetries in our dataset:\n\nAsymmetry #1: An ever increasing number of awards\n\nThe number of awards has increased over time. In 1972, there were four major awards: Hugo, Nebula, British SF, and British Fantasy Awards. However, a book published in 2019 is eligible for more than a dozen awards in our dataset. The naive solution for handling this would be to suggest that it’s the proportion of awards you win that matters. Meaning, winning 2 of 4 awards (50%) is equivalent to 3 of 6 (still 50%). However, as the number of awards increases, this heuristic seems to break down. This is likely because the probability of winning multiple awards is exponential, not linear. The probability of winning n awards is p(w)^n, not n*p(w). Thus, we will use a logarithmic transform in our algorithm to account for this.\n\nIn our Gateway vs. Speaker example above we can see the difference a log makes. Assuming a linear (weighted sum) of awards we’d expect this to result in a 13% boost for Gateway. However, applying the proper transformation only nets us a 6% boost in favor of Frederick Pohl’s novel. Still edging it ahead of Speaker, but not by as much as we might intuit.\n\nAsymmetry #2: Varying levels of competition\n\nSimilarly, just like the number of awards has changed over time, so has the competition. While it’s difficult to fully understand how many “good” sci-fi books were being published in a given year, we can look at some examples to get a sense for what competition indicators we might already have in our data.\n\nConsider the Locus Awards in the mid-1970s. Joe Haldeman’s Forever War is nominated for best novel in 1975, and then again in 1976 when he wins the award. Or, look at the 1972 and 1973 Hugo Awards. The talented Robert Silverberg is nominated FOUR (!?) times in two years for the same award. He’s quite literally competing with himself.\n\nSo, while there’s no disputing Forever War and Silverberg are greats, we do want to try and control for how difficult it is to edge out the competition in a given period. Using a more scaled version of this exercise we can tease out a competition factor, which we use to further refine our algorithm. Our method suggests it has not been perfectly linear, but certainly has increased over-time (as we’d expect).\n\nWe can now account for competition when comparing Gateway to Speaker for the Dead. As we saw in the Haldeman and Silverberg examples above, the early 70’s was a relatively low point for competition. This is not to say some of the greatest novels of all time didn’t arise from the 1970’s (they did, and our data will show that), but instead is a commentary on the number of distinct authors writing in the genre at the time. The 80’s, however, is a different story — enter Gene Wolfe, Lois McMaster Bujold, Kim Stanley Robinson, Connie Willis, David Brin, and of course, Orson Scott Card. This additional competition results in a material boost for Speaker.\n\nWhere does that leave our two novels? Check out the final rankings below, but (spoiler), Speaker for the Dead leapfrogs Gateway after the above is accounted for.\n\nOne algorithm to rule them all\n\nAnd voila! Here we have it: A function that takes in a bunch of award data, cleans it up, weights it reasonably, controls for the number of awards over time, and properly accounts for the level of competition. The notation is pseudo-math of course, but the final algo looks something like the following:\n\nNow, onto what you have been waiting for…\n\nThe top ranked SFF novels of the past 50 years\n\nFor those that read all of the above and are still here, cheers. For the rest of you that just skipped straight to the results… as one of my favorite fantasy characters would say, “You have to be realistic about these things.” So without further ado, a ranking of the best sci-fi/fantasy novels since 1970:"
    }
}
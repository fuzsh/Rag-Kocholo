{
    "id": "correct_publicationDate_00014_3",
    "rank": 26,
    "data": {
        "url": "https://dl.acm.org/doi/abs/10.1145/3528233.3530701",
        "read_more_link": "",
        "language": "en",
        "title": "NeuralPassthrough: Learned Real-Time View Synthesis for VR",
        "top_image": "https://dl.acm.org/cms/asset/108eb972-c6b9-4361-9f07-2070796fb29a/3528233.cover.jpg",
        "meta_img": "https://dl.acm.org/cms/asset/108eb972-c6b9-4361-9f07-2070796fb29a/3528233.cover.jpg",
        "images": [
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/acm-dl-logo-white-1ecfb82271e5612e8ca12aa1b1737479.png",
            "https://dl.acm.org/doi/abs/10.1145/specs/products/acm/releasedAssets/images/acm-logo-1-ad466e729c8e2a97780337b76715e5cf.png",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/footer-logo1-45ae33115db81394d8bd25be65853b77.png",
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/Default_image_lazy-0687af31f0f1c8d4b7a22b686995ab9b.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/loader-7e60691fbe777356dc81ff6d223a82a6.gif",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/acm-logo-dl-8437178134fce530bc785276fc316cbf.png",
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/acm-logo-3-10aed79f3a6c95ddb67053b599f029af.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Switzerland View Profile",
            "Lei Xiao",
            "Salah Nouri",
            "Joel Hegland",
            "Alberto Garcia Garcia",
            "Douglas Lanman"
        ],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "/pb-assets/head-metadata/apple-touch-icon-1574252172393.png",
        "meta_site_name": "ACM Conferences",
        "canonical_link": "https://dl.acm.org/doi/10.1145/3528233.3530701",
        "text": "Abstract\n\nVirtual reality (VR) headsets provide an immersive, stereoscopic visual experience, but at the cost of blocking users from directly observing their physical environment. Passthrough techniques are intended to address this limitation by leveraging outward-facing cameras to reconstruct the images that would otherwise be seen by the user without the headset. This is inherently a real-time view synthesis challenge, since passthrough cameras cannot be physically co-located with the user’s eyes. Existing passthrough techniques suffer from distracting reconstruction artifacts, largely due to the lack of accurate depth information (especially for near-field and disoccluded objects), and also exhibit limited image quality (e.g., being low resolution and monochromatic). In this paper, we propose the first learned passthrough method and assess its performance using a custom VR headset that contains a stereo pair of RGB cameras. Through both simulations and experiments, we demonstrate that our learned passthrough method delivers superior image quality compared to state-of-the-art methods, while meeting strict VR requirements for real-time, perspective-correct stereoscopic view synthesis over a wide field of view for desktop-connected headsets.\n\nSupplementary Material\n\nSupplemental file (NeuralPassthrough_SupplementaryPDF.pdf)\n\nDownload\n\n12.14 MB\n\nReferences\n\n[1]\n\n2021. Azure Kinect DK. https://azure.microsoft.com/en-us/services/kinect-dk/\n\n[2]\n\n2021. Rift-S VR. https://www.oculus.com/rift-s/features/\n\n[3]\n\nKara-Ali Aliev, Artem Sevastopolsky, Maria Kolos, Dmitry Ulyanov, and Victor Lempitsky. 2020. Neural point-based graphics. In European Conference on Computer Vision. Springer, 696–712.\n\n[4]\n\nMichael Broxton, John Flynn, Ryan Overbeck, Daniel Erickson, Peter Hedman, Matthew Duvall, Jason Dourgarian, Jay Busch, Matt Whalen, and Paul Debevec. 2020. Immersive light field video with a layered mesh representation. ACM Transactions on Graphics (TOG) 39, 4 (2020), 86–1.\n\n[5]\n\nAlexandre Chapiro, Simon Heinzle, Tunç Ozan Aydın, Steven Poulakos, Matthias Zwicker, Aljosa Smolic, and Markus Gross. 2014. Optimizing stereo-to-multiview conversion for autostereoscopic displays. In Computer graphics forum, Vol. 33. Wiley Online Library, 63–72.\n\n[6]\n\nGaurav Chaurasia, Arthur Nieuwoudt, Alexandru-Eugen Ichim, Richard Szeliski, and Alexander Sorkine-Hornung. 2020. Passthrough+ Real-time Stereoscopic View Synthesis for Mobile Mixed Reality. Proceedings of the ACM on Computer Graphics and Interactive Techniques 3, 1(2020), 1–17.\n\n[7]\n\nShenchang Eric Chen and Lance Williams. 1993. View interpolation for image synthesis. In Proceedings of the 20th annual conference on Computer graphics and interactive techniques. 279–288.\n\n[8]\n\nPiotr Didyk, Pitchaya Sitthi-Amorn, William Freeman, Frédo Durand, and Wojciech Matusik. 2013. Joint view expansion and filtering for automultiscopic 3D displays. ACM Transactions on Graphics (TOG) 32, 6 (2013), 1–8.\n\n[9]\n\nJohn Flynn, Michael Broxton, Paul Debevec, Matthew DuVall, Graham Fyffe, Ryan Overbeck, Noah Snavely, and Richard Tucker. 2019. Deepview: View synthesis with learned gradient descent. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2367–2376.\n\n[10]\n\nChen Gao, Ayush Saraf, Johannes Kopf, and Jia-Bin Huang. 2021. Dynamic View Synthesis from Dynamic Monocular Video. arXiv preprint arXiv:2105.06468(2021).\n\n[11]\n\nStephan J Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, and Julien Valentin. 2021. Fastnerf: High-fidelity neural rendering at 200fps. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 14346–14355.\n\n[12]\n\nSteven J Gortler, Radek Grzeszczuk, Richard Szeliski, and Michael F Cohen. 1996. The lumigraph. In Proceedings of the 23rd annual conference on Computer graphics and interactive techniques. 43–54.\n\n[13]\n\nPengsheng Guo, Miguel Angel Bautista, Alex Colburn, Liang Yang, Daniel Ulbricht, Joshua M Susskind, and Qi Shan. 2022. Fast and Explicit Neural View Synthesis. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 3791–3800.\n\n[14]\n\nPeter Hedman, Suhib Alsisan, Richard Szeliski, and Johannes Kopf. 2017. Casual 3D photography. ACM Transactions on Graphics (TOG) 36, 6 (2017), 1–15.\n\n[15]\n\nPeter Hedman, Pratul P Srinivasan, Ben Mildenhall, Jonathan T Barron, and Paul Debevec. 2021. Baking neural radiance fields for real-time view synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 5875–5884.\n\n[16]\n\nAjay Jain, Matthew Tancik, and Pieter Abbeel. 2021. Putting nerf on a diet: Semantically consistent few-shot view synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 5885–5894.\n\n[17]\n\nNima Khademi Kalantari, Ting-Chun Wang, and Ravi Ramamoorthi. 2016. Learning-based view synthesis for light field cameras. ACM Transactions on Graphics (TOG) 35, 6 (2016), 1–10.\n\n[18]\n\nJohannes Kopf, Kevin Matzen, Suhib Alsisan, Ocean Quigley, Francis Ge, Yangming Chong, Josh Patterson, Jan-Michael Frahm, Shu Wu, Matthew Yu, 2020. One shot 3d photography. ACM Transactions on Graphics (TOG) 39, 4 (2020), 76–1.\n\n[19]\n\nBrooke Krajancich, Petr Kellnhofer, and Gordon Wetzstein. 2020. Optimizing Depth Perception in Virtual and Augmented Reality through Gaze-Contingent Stereo Rendering. ACM Trans. Graph. 39, 6, Article 269 (nov 2020), 10 pages.\n\n[20]\n\nMarc Levoy and Pat Hanrahan. 1996. Light field rendering. In Proceedings of the 23rd annual conference on Computer graphics and interactive techniques. 31–42.\n\n[21]\n\nLahav Lipson, Zachary Teed, and Jia Deng. 2021. Raft-stereo: Multilevel recurrent field transforms for stereo matching. arXiv preprint arXiv:2109.07547(2021).\n\n[22]\n\nGuilin Liu, Fitsum A Reda, Kevin J Shih, Ting-Chun Wang, Andrew Tao, and Bryan Catanzaro. 2018. Image inpainting for irregular holes using partial convolutions. In Proceedings of the European Conference on Computer Vision (ECCV). 85–100.\n\n[23]\n\nLingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. 2020. Neural sparse voxel fields. arXiv preprint arXiv:2007.11571(2020).\n\n[24]\n\nAndrew Maimone and Junren Wang. 2020. Holographic Optics for Thin and Lightweight Virtual Reality. ACM Trans. Graph. 39, 4, Article 67 (jul 2020), 14 pages.\n\n[25]\n\nRicardo Martin-Brualla, Rohit Pandey, Shuoran Yang, Pavel Pidlypenskyi, Jonathan Taylor, Julien Valentin, Sameh Khamis, Philip Davidson, Anastasia Tkach, Peter Lincoln, 2018. LookinGood: enhancing performance capture with real-time neural re-rendering. ACM Transactions on Graphics (TOG) 37, 6 (2018), 1–14.\n\n[26]\n\nBen Mildenhall, Pratul P Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and Abhishek Kar. 2019. Local light field fusion: Practical view synthesis with prescriptive sampling guidelines. ACM Transactions on Graphics (TOG) 38, 4 (2019), 1–14.\n\n[27]\n\nBen Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. 2020. Nerf: Representing scenes as neural radiance fields for view synthesis. In European conference on computer vision. Springer, 405–421.\n\n[28]\n\nSimon Niklaus and Feng Liu. 2020. Softmax splatting for video frame interpolation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 5437–5446.\n\n[29]\n\nOculus. 2016. asynchronous spacewarp. https://www.oculus.com/blog/introducing-asw-2-point-0-better-accuracy-lower-latency/\n\n[30]\n\nAlbert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. 2021. D-nerf: Neural radiance fields for dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 10318–10327.\n\n[31]\n\nChristian Reiser, Songyou Peng, Yiyi Liao, and Andreas Geiger. 2021. KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs. arXiv preprint arXiv:2103.13744(2021).\n\n[32]\n\nJonathan Shade, Steven Gortler, Li-wei He, and Richard Szeliski. 1998. Layered depth images. In Proceedings of the 25th annual conference on Computer graphics and interactive techniques. 231–242.\n\n[33]\n\nMeng-Li Shih, Shih-Yang Su, Johannes Kopf, and Jia-Bin Huang. 2020. 3d photography using context-aware layered depth inpainting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 8028–8038.\n\n[34]\n\nRajiv Soundararajan and Alan C Bovik. 2012. Video quality assessment by reduced reference spatio-temporal entropic differencing. IEEE Transactions on Circuits and Systems for Video Technology 23, 4(2012), 684–694.\n\n[35]\n\nPratul P Srinivasan, Richard Tucker, Jonathan T Barron, Ravi Ramamoorthi, Ren Ng, and Noah Snavely. 2019. Pushing the boundaries of view extrapolation with multiplane images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 175–184.\n\n[36]\n\nZhou Wang, Alan C Bovik, Hamid R Sheikh, Eero P Simoncelli, 2004. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing 13, 4 (2004), 600–612.\n\n[37]\n\nOlivia Wiles, Georgia Gkioxari, Richard Szeliski, and Justin Johnson. 2020. Synsin: End-to-end view synthesis from a single image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 7467–7477.\n\n[38]\n\nLei Xiao, Anton Kaplanyan, Alexander Fix, Matthew Chapman, and Douglas Lanman. 2018. DeepFocus: learned image synthesis for computational displays. ACM Transactions on Graphics (TOG) 37, 6 (2018), 1–13.\n\n[39]\n\nAlex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. 2021a. Plenoctrees for real-time rendering of neural radiance fields. arXiv preprint arXiv:2103.14024(2021).\n\n[40]\n\nAlex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. 2021b. pixelnerf: Neural radiance fields from one or few images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 4578–4587.\n\n[41]\n\nKai Zhang, Gernot Riegler, Noah Snavely, and Vladlen Koltun. 2020. Nerf++: Analyzing and improving neural radiance fields. arXiv preprint arXiv:2010.07492(2020).\n\n[42]\n\nTinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. 2018. Stereo magnification: Learning view synthesis using multiplane images. arXiv preprint arXiv:1805.09817(2018).\n\n[43]\n\nC Lawrence Zitnick, Sing Bing Kang, Matthew Uyttendaele, Simon Winder, and Richard Szeliski. 2004. High-quality video view interpolation using a layered representation. ACM transactions on graphics (TOG) 23, 3 (2004), 600–608.\n\nCited By\n\nView all\n\nBanquiero MValdeolivas GRamón DJuan MA color Passthrough mixed reality application for learning pianoVirtual Reality10.1007/s10055-024-00953-w28:2\n\nZhao CBeams RGeometric distortion on video see‐through head‐mounted displaysJournal of the Society for Information Display10.1002/jsid.128232:5(184-193)\n\nKuo GPenner EMoczydlowski SChing ALanman DMatsuda NPerspective-Correct VR Passthrough Without ReprojectionACM SIGGRAPH 2023 Conference Proceedings10.1145/3588432.3591534(1-9)\n\nShow More Cited By\n\nRecommendations\n\nPerspective-Correct VR Passthrough Without Reprojection\n\nSIGGRAPH '23: ACM SIGGRAPH 2023 Conference Proceedings\n\nVirtual reality (VR) passthrough uses external cameras on the front of a headset to allow the user to see their environment. However, passthrough cameras cannot physically be co-located with the user’s eyes, so the passthrough images have a different ...\n\nVR Grabbers: Ungrounded Haptic Retargeting for Precision Grabbing Tools\n\nUIST '18: Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology\n\nHaptic feedback in VR is important for realistic simulation in virtual reality. However, recreating the haptic experience for hand tools in VR traditionally requires hardware with precise actuators, adding complexity to the system. We propose Ungrounded ...\n\nEffect of VR technology matureness on VR sickness\n\nAbstract\n\nIn this paper relationship of perceived virtual reality (VR) sickness phenomenon with different generations of virtual reality head mounted displays (VR HMD) is presented. Action content type omnidirectional video clip was watched by means of four ...\n\nInformation & Contributors\n\nInformation\n\nPublished In\n\n553 pages\n\nISBN:9781450393379\n\nDOI:10.1145/3528233\n\nCopyright © 2022 Owner/Author.\n\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the Owner/Author.\n\nPublisher\n\nAssociation for Computing Machinery\n\nNew York, NY, United States\n\nPublication History\n\nPublished: 24 July 2022\n\nCheck for updates\n\nAuthor Tags\n\nPassthrough\n\nReal-Time View Synthesis\n\nVirtual Reality\n\nQualifiers\n\nResearch-article\n\nResearch\n\nRefereed limited\n\nConference\n\nSIGGRAPH '22\n\nAcceptance Rates\n\nOverall Acceptance Rate 1,822 of 8,601 submissions, 21%\n\nContributors\n\nOther Metrics\n\nBibliometrics & Citations\n\nBibliometrics\n\nArticle Metrics\n\n7\n\nTotal Citations\n\nView Citations\n\n1,890\n\nTotal Downloads\n\nDownloads (Last 12 months)867\n\nDownloads (Last 6 weeks)58\n\nOther Metrics\n\nCitations\n\nCited By\n\nView all\n\nBanquiero MValdeolivas GRamón DJuan MA color Passthrough mixed reality application for learning pianoVirtual Reality10.1007/s10055-024-00953-w28:2\n\nZhao CBeams RGeometric distortion on video see‐through head‐mounted displaysJournal of the Society for Information Display10.1002/jsid.128232:5(184-193)\n\nKuo GPenner EMoczydlowski SChing ALanman DMatsuda NPerspective-Correct VR Passthrough Without ReprojectionACM SIGGRAPH 2023 Conference Proceedings10.1145/3588432.3591534(1-9)\n\nIshihara AAga HIshihara YIchikawa HKaji HKawasaki KKobayashi DKobayashi TNishida KHamasaki TMori HMorikubo YIntegrating Both Parallax and Latency Compensation into Video See-through Head-mounted DisplayIEEE Transactions on Visualization and Computer Graphics10.1109/TVCG.2023.324746029:5(2826-2836)\n\nZeng HZhao RPerceptually-guided Dual-mode Virtual Reality System For Motion-adaptive DisplayIEEE Transactions on Visualization and Computer Graphics10.1109/TVCG.2023.324709729:5(2249-2257)\n\nKhan NXiao LLanman DTiled Multiplane Images for Practical 3D Photography2023 IEEE/CVF International Conference on Computer Vision (ICCV)10.1109/ICCV51070.2023.00959(10420-10430)\n\nKhan NPenner ELanman DXiao LTemporally Consistent Online Depth Estimation Using Point-Based Fusion2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)10.1109/CVPR52729.2023.00880(9119-9129)\n\nView Options\n\nView options\n\nPDF\n\nView or Download as a PDF file.\n\nPDF\n\neReader\n\nView online with eReader.\n\neReader\n\nHTML Format\n\nView this article in HTML Format.\n\nHTML Format\n\nGet Access\n\nLogin options\n\nCheck if you have access through your login credentials or your institution to get full access on this article.\n\nSign in\n\nFull Access\n\nMedia\n\nFigures\n\nOther\n\nTables\n\nShare\n\nShare\n\nShare this Publication link\n\nCopied!\n\nCopying failed.\n\nShare on social media\n\nAffiliations\n\nLei Xiao\n\nReality Labs Research, Meta, United States of America\n\nSalah Nouri\n\nReality Labs Research, Meta, United States of America\n\nJoel Hegland\n\nReality Labs Research, Meta, United States of America\n\nAlberto Garcia Garcia\n\nReality Labs, Meta, Switzerland\n\nDouglas Lanman\n\nReality Labs Research, Meta, United States of America\n\nRequest permissions Authors Info & Affiliations"
    }
}
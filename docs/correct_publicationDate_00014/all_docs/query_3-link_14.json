{
    "id": "correct_publicationDate_00014_3",
    "rank": 14,
    "data": {
        "url": "https://orbit.dtu.dk/en/publications/a-simulation-system-for-scene-synthesis-in-virtual-reality",
        "read_more_link": "",
        "language": "en",
        "title": "A Simulation System for Scene Synthesis in Virtual Reality",
        "top_image": "https://orbit.dtu.dk/skin/headerImage/",
        "meta_img": "https://orbit.dtu.dk/skin/headerImage/",
        "images": [
            "https://orbit.dtu.dk/skin/headerImage/",
            "https://orbit.dtu.dk/skin/footerIcon/"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Jingyu Liu",
            "Claire Mantel",
            "Florian Schweiger",
            "Søren Forchhammer"
        ],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "/skin/favIcon/",
        "meta_site_name": "Welcome to DTU Research Database",
        "canonical_link": "https://orbit.dtu.dk/en/publications/a-simulation-system-for-scene-synthesis-in-virtual-reality",
        "text": "TY - GEN\n\nT1 - A Simulation System for Scene Synthesis in Virtual Reality\n\nAU - Liu, Jingyu\n\nAU - Mantel, Claire\n\nAU - Schweiger, Florian\n\nAU - Forchhammer, Søren\n\nPY - 2021\n\nY1 - 2021\n\nN2 - Real-world scene synthesis can be realized through view synthesis or 3D reconstruction methods. While industrial and commercial demands emerge for real-world scene synthesis in virtual reality (VR) using head-mounted displays (HMDs), the methods in the literature generally do not target specific display devices. To meet the rising demands, we propose a simulation system to evaluate scene synthesis methods in VR. Our system aims at providing the full pipeline of scene capturing, processing, rendering, and evaluation. The capturing module provides various input dataset formulations. The processing and rendering module integrates three representative scene synthesis methods with a voluntary performance-aid option. Finally, the evaluation module supports traditional metrics as well as perception-based metrics. An experiment demonstrates the use of our system for identifying the best capturing strategy among the three degrees of foveation tested. As can be expected, the FovVideoVDP metric (based on a model of the human visual system) finds the highest degree of foveation giving best results. The three other quality metrics from the evaluation module (which use features to measure similarity) confirm that result. The synthetic scenes in the experiment can run in VR with an average latency of 5.9 ms for the two selected scenarios across the tested methods on Nvidia GTX 2080 ti.\n\nAB - Real-world scene synthesis can be realized through view synthesis or 3D reconstruction methods. While industrial and commercial demands emerge for real-world scene synthesis in virtual reality (VR) using head-mounted displays (HMDs), the methods in the literature generally do not target specific display devices. To meet the rising demands, we propose a simulation system to evaluate scene synthesis methods in VR. Our system aims at providing the full pipeline of scene capturing, processing, rendering, and evaluation. The capturing module provides various input dataset formulations. The processing and rendering module integrates three representative scene synthesis methods with a voluntary performance-aid option. Finally, the evaluation module supports traditional metrics as well as perception-based metrics. An experiment demonstrates the use of our system for identifying the best capturing strategy among the three degrees of foveation tested. As can be expected, the FovVideoVDP metric (based on a model of the human visual system) finds the highest degree of foveation giving best results. The three other quality metrics from the evaluation module (which use features to measure similarity) confirm that result. The synthetic scenes in the experiment can run in VR with an average latency of 5.9 ms for the two selected scenarios across the tested methods on Nvidia GTX 2080 ti.\n\nKW - View synthesis\n\nKW - 3D reconstruction\n\nKW - Virtual reality\n\nU2 - 10.1007/978-3-030-90739-6_5\n\nDO - 10.1007/978-3-030-90739-6_5\n\nM3 - Article in proceedings\n\nSN - 978-3-030-90738-9\n\nT3 - Lecture Notes in Computer Science\n\nSP - 67\n\nEP - 84\n\nBT - International Conference on Virtual Reality and Mixed Reality\n\nPB - Springer\n\nT2 - 18<sup>th</sup> EuroXR International Conference\n\nY2 - 24 November 2021 through 26 November 2021\n\nER -"
    }
}
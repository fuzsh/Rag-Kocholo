{
    "id": "correct_publicationDate_00014_3",
    "rank": 42,
    "data": {
        "url": "https://dl.acm.org/doi/10.1145/3411764.3445532",
        "read_more_link": "",
        "language": "en",
        "title": "Scene-Aware Behavior Synthesis for Virtual Pets in Mixed Reality",
        "top_image": "https://dl.acm.org/cms/asset/f51e7891-e100-4500-a40c-7428405a5c7d/3411764.cover.jpg",
        "meta_img": "https://dl.acm.org/cms/asset/f51e7891-e100-4500-a40c-7428405a5c7d/3411764.cover.jpg",
        "images": [
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/acm-dl-logo-white-1ecfb82271e5612e8ca12aa1b1737479.png",
            "https://dl.acm.org/doi/10.1145/specs/products/acm/releasedAssets/images/acm-logo-1-ad466e729c8e2a97780337b76715e5cf.png",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/footer-logo1-45ae33115db81394d8bd25be65853b77.png",
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/Default_image_lazy-0687af31f0f1c8d4b7a22b686995ab9b.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/action/showDoPubAsset?doi=10.1145/contrib-81331502006&format=rel-imgonly&assetId=8b6ba4bb-712f-4aed-abb7-8ff0a557333b.jpeg",
            "https://dl.acm.org/action/showDoPubAsset?doi=10.1145/contrib-99659489823&format=rel-imgonly&assetId=ella-square.jpg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/loader-7e60691fbe777356dc81ff6d223a82a6.gif",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/acm-logo-dl-8437178134fce530bc785276fc316cbf.png",
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/acm-logo-3-10aed79f3a6c95ddb67053b599f029af.png"
        ],
        "movies": [
            "https://iframe.videodelivery.net/eyJraWQiOiI3YjgzNTg3NDZlNWJmNDM0MjY5YzEwZTYwMDg0ZjViYiIsImFsZyI6IlJTMjU2In0.eyJzdWIiOiI2NjFmZGVlNWI5NWZjMWM2NTkyYTUyZDRiMzVkYjA3ZiIsImV4cCI6MTcyMTYxNzQ5OSwia2lkIjoiN2I4MzU4NzQ2ZTViZjQzNDI2OWMxMGU2MDA4NGY1YmIifQ.UQkAtvDm6pXilgWf9Te4xv5f7en6HrExfiXFl09hwRie5TMSESiCujGADHYqzhWNUxZTO6Q8YiyryUpDvI8pyJa7lQ_PjUB2fg_Y-o3meO1ygfyGYOszpnFCweiKyCTAZ-45BtTc5n-rtyP_FWHDWrqQcg69SCPqmDn-Ym3ikJ0vU2d3pG1yMOaz3iAaPIHOmh0YdqVKc9OzswBZSdYXXzXSCp-F3s6xz-vY7A5clH5EKkM6S3PWmzPrbjFOgLS_A90lPSVCbwJJcmRkZ9nKS-QbZoQ_I6LUKO-jMrFEneTe1vOKBhBv8bqoWsc7xNePSnz-uPO_UTkk8c-D2iHN9Q?poster=https%3A%2F%2Fvideodelivery.net%2FeyJraWQiOiI3YjgzNTg3NDZlNWJmNDM0MjY5YzEwZTYwMDg0ZjViYiIsImFsZyI6IlJTMjU2In0.eyJzdWIiOiI2NjFmZGVlNWI5NWZjMWM2NTkyYTUyZDRiMzVkYjA3ZiIsImV4cCI6MTcyMTYxNzQ5OSwia2lkIjoiN2I4MzU4NzQ2ZTViZjQzNDI2OWMxMGU2MDA4NGY1YmIifQ.UQkAtvDm6pXilgWf9Te4xv5f7en6HrExfiXFl09hwRie5TMSESiCujGADHYqzhWNUxZTO6Q8YiyryUpDvI8pyJa7lQ_PjUB2fg_Y-o3meO1ygfyGYOszpnFCweiKyCTAZ-45BtTc5n-rtyP_FWHDWrqQcg69SCPqmDn-Ym3ikJ0vU2d3pG1yMOaz3iAaPIHOmh0YdqVKc9OzswBZSdYXXzXSCp-F3s6xz-vY7A5clH5EKkM6S3PWmzPrbjFOgLS_A90lPSVCbwJJcmRkZ9nKS-QbZoQ_I6LUKO-jMrFEneTe1vOKBhBv8bqoWsc7xNePSnz-uPO_UTkk8c-D2iHN9Q%2Fthumbnails%2Fthumbnail.jpg%3Ftime%3D10.0s"
        ],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "China View Profile",
            "United States View Profile",
            "Yining Lang Alibaba Group",
            "Wei Liang",
            "Xinzhe Yu",
            "Rawan Alghofaili",
            "Yining Lang",
            "Lap-Fai Yu"
        ],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "/pb-assets/head-metadata/apple-touch-icon-1574252172393.png",
        "meta_site_name": "ACM Conferences",
        "canonical_link": "https://dl.acm.org/doi/10.1145/3411764.3445532",
        "text": "Abstract\n\nVirtual pets are an alternative to real pets, providing a substitute for people with allergies or preparing people for adopting a real pet. Recent advancements in mixed reality pave the way for virtual pets to provide a more natural and seamless experience for users. However, one key challenge is embedding environmental awareness into the virtual pet (e.g., identifying the food bowl’s location) so that they can behave naturally in the real world.\n\nWe propose a novel approach to synthesize virtual pet behaviors by considering scene semantics, enabling a virtual pet to behave naturally in mixed reality. Given a scene captured from the real world, our approach synthesizes a sequence of pet behaviors (e.g., resting after eating). Then, we assign each behavior in the sequence to a location in the real scene. We conducted user studies to evaluate our approach, which showed the efficacy of our approach in synthesizing natural virtual pet behaviors.\n\nSupplementary Material\n\nSupplementary Materials (3411764.3445532_supplementalmaterials.zip)\n\nDownload\n\n1.36 MB\n\nMP4 File (3411764.3445532_videopreview.mp4)\n\nPreview video\n\nDownload\n\n10.89 MB\n\nReferences\n\n[1]\n\nSun Joo Ahn, Kyle Johnsen, Tom Robertson, James Moore, Scott Brown, Amanda Marable, and Aryabrata Basu. 2015. Using virtual pets to promote physical activity in children: An application of the youth physical activity promotion model. Journal of health communication 20, 7 (2015), 807–815.\n\n[2]\n\nEric Lewin Altschuler. 1999. Pet-facilitated therapy for posttraumatic stress disorder. Annals of Clinical Psychiatry 11, 1 (1999), 29–30.\n\n[3]\n\nShir Amir, Anna Zamansky, and Dirk van der Linden. 2017. K9-Blyzer: Towards video-based automatic analysis of canine behavior. In Proceedings of the Fourth International Conference on Animal-Computer Interaction. 1–5.\n\n[4]\n\nLing Bao and Stephen S Intille. 2004. Activity recognition from user-annotated acceleration data. In International conference on pervasive computing. Springer, 1–17.\n\n[5]\n\nAndrea Beetz, Kerstin Uvnäs-Moberg, Henri Julius, and Kurt Kotrschal. 2012. Psychosocial and psychophysiological effects of human-animal interactions: the possible role of oxytocin. Frontiers in psychology 3 (2012), 234.\n\n[6]\n\nLinda P Case 2003. The cat: its behavior, nutrition & health.Iowa State Press.\n\n[7]\n\nZhi-Hong Chen, Calvin Liao, Tzu-Chao Chien, and Tak-Wai Chan. 2011. Animal companions: Fostering children’s effort-making by nurturing virtual pets. British Journal of Educational Technology 42, 1 (2011), 166–180.\n\n[8]\n\nThomas Chesney and Shaun Lawson. 2007. The illusion of love: Does a virtual pet provide the same companionship as a real one?Interaction Studies 8, 2 (2007), 337–342.\n\n[9]\n\nJunyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014. Empirical evaluation of gated recurrent neural networks on sequence modeling. (2014). arxiv:1412.3555\n\n[10]\n\nShi-Gang Cui, Hui Wang, and Li Yang. 2012. A simulation study of A-star algorithm for robot path planning. In 16th international conference on mechatronics technology. 506–510.\n\n[11]\n\nHayley Cutt, Billie Giles-Corti, Matthew Knuiman, Anna Timperio, and Fiona Bull. 2008. Understanding dog owners’ increased levels of physical activity: results from RESIDE. American journal of public health 98, 1 (2008), 66–69.\n\n[12]\n\nRuta Desai, Fraser Anderson, Justin Matejka, Stelian Coros, James McCann, George Fitzmaurice, and Tovi Grossman. 2019. Geppetto: Enabling Semantic Design of Expressive Robot Behaviors. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. 1–14.\n\n[13]\n\nRodrigo Dias and Carlos Martinho. 2011. Adapting content presentation and control to player personality in videogames. In Proceedings of the 8th International Conference on Advances in Computer Entertainment Technology. 1–8.\n\n[14]\n\nTawanna Dillahunt, Geof Becker, Jennifer Mankoff, and Robert Kraut. 2008. Motivating environmentally sustainable behavior changes with a virtual polar bear. In Pervasive 2008 Workshop Proceedings, Vol. 8. 58–62.\n\n[15]\n\nEric Dybsand. 2001. A Generic Fuzzy State. Game Programming Gems 2(2001), 337.\n\n[16]\n\nENIX. 2017. DragonQuest XI. https://dragonquest.square-enix-games.com/tact/en-us/.\n\n[17]\n\nDumitru Erhan, Christian Szegedy, Alexander Toshev, and Dragomir Anguelov. 2014. Scalable object detection using deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition. 2147–2154.\n\n[18]\n\nMark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. 2010. The pascal visual object classes (voc) challenge. International journal of computer vision 88, 2 (2010), 303–338.\n\n[19]\n\nIram Fatima, Muhammad Fahim, Young-Koo Lee, and Sungyoung Lee. 2013. A unified framework for activity recognition-based behavior analysis and action prediction in smart homes. Sensors 13, 2 (2013), 2682–2699.\n\n[20]\n\nMichael Georgeff, Barney Pell, Martha Pollack, Milind Tambe, and Michael Wooldridge. 1998. The belief-desire-intention model of agency. In International workshop on agent theories, architectures, and languages. Springer, 1–10.\n\n[21]\n\nAndrew Gilbey, June McNicholas, and Glyn M Collis. 2007. A longitudinal test of the belief that companion animal ownership can help reduce loneliness. Anthrozoös 20, 4 (2007), 345–353.\n\n[22]\n\nRoss Girshick. 2015. Fast r-cnn. In Proceedings of the IEEE international conference on computer vision. 1440–1448.\n\n[23]\n\nAlex Graves. 2013. Generating sequences with recurrent neural networks. (2013). arxiv:1308.0850\n\n[24]\n\nJuho Hamari, David J Shernoff, Elizabeth Rowe, Brianno Coller, Jodi Asbell-Clarke, and Teon Edwards. 2016. Challenging games help students learn: An empirical study on engagement, flow and immersion in game-based learning. Computers in human behavior 54 (2016), 170–179.\n\n[25]\n\nPeter E Hart, Nils J Nilsson, and Bertram Raphael. 1968. A formal basis for the heuristic determination of minimum cost paths. IEEE transactions on Systems Science and Cybernetics 4, 2(1968), 100–107.\n\n[26]\n\nKaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. 2017. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision. 2961–2969.\n\n[27]\n\nSepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural computation 9, 8 (1997), 1735–1780.\n\n[28]\n\nDaniel Holden, Jun Saito, and Taku Komura. 2016. A deep learning framework for character motion synthesis and editing. ACM Transactions on Graphics (TOG) 35, 4 (2016), 138.\n\n[29]\n\nYouichi Horry, Ken-Ichi Anjyo, and Kiyoshi Arai. 1997. Tour into the picture: using a spidery mesh interface to make animation from a single image. (1997).\n\n[30]\n\nKyle Johnsen, Sun Joo Ahn, James Moore, Scott Brown, Thomas P Robertson, Amanda Marable, and Aryabrata Basu. 2014. Mixed reality virtual pets to reduce childhood obesity. IEEE transactions on visualization and computer graphics 20, 4(2014), 523–530.\n\n[31]\n\nMalte F Jung. 2017. Affective grounding in human-robot interaction. In 2017 12th ACM/IEEE International Conference on Human-Robot Interaction (HRI. IEEE, 263–273.\n\n[32]\n\nAndrej Karpathy, Armand Joulin, and Li F Fei-Fei. 2014. Deep fragment embeddings for bidirectional image sentence mapping. In Advances in neural information processing systems. 1889–1897.\n\n[33]\n\nDavid W Kritt. 2000. Loving a virtual pet: Steps toward the technological erosion of emotion. The Journal of American Culture 23, 4 (2000), 81.\n\n[34]\n\nVining Lang, Wei Liang, and Lap-Fai Yu. 2019. Virtual agent positioning driven by scene semantics in mixed reality. In 2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR). IEEE, 767–775.\n\n[35]\n\nBettina Laugwitz, Theo Held, and Martin Schrepp. 2008. Construction and evaluation of a user experience questionnaire. In Symposium of the Austrian HCI and usability engineering group. Springer, 63–76.\n\n[36]\n\nAna Lilia Laureano-Cruces and Arturo Rodriguez-Garcia. 2012. Design and implementation of an educational virtual pet using the OCC theory. Journal of Ambient Intelligence and Humanized Computing 3, 1 (2012), 61–71.\n\n[37]\n\nS Lawson and Thomas Chesney. 2007. The impact of owner age on companionship with virtual pets. Eighth International Conference on Information Visualisation (2007).\n\n[38]\n\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. 2014. Microsoft coco: Common objects in context. In European conference on computer vision. Springer, 740–755.\n\n[39]\n\nLi Liu, Wanli Ouyang, Xiaogang Wang, Paul Fieguth, Jie Chen, Xinwang Liu, and Matti Pietikäinen. 2018. Deep learning for generic object detection: A survey. (2018). arxiv:1809.02165\n\n[40]\n\nChristopher D Manning, Mihai Surdeanu, John Bauer, Jenny Rose Finkel, Steven Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In Proceedings of 52nd annual meeting of the association for computational linguistics: system demonstrations. 55–60.\n\n[41]\n\nMicrosoft. 2015. HoloPet. https://www.windowscentral.com/microsoft-registers-holopet-trademark-augmented-reality-kittens-all.\n\n[42]\n\n[42] Neopets.2003. http://neopets.com.\n\n[43]\n\nNeuroHive. 2019. Pets ARound - Virtual Friend. https://appadvice.com/app/pets-around-virtual-friend/1441956434.\n\n[44]\n\nNahal Norouzi, Kangsoo Kim, Myungho Lee, Ryan Schubert, Austin Erickson, Jeremy Bailenson, Gerd Bruder, and Greg Welch. 2019. Walking your virtual dog: Analysis of awareness and proxemics with simulated support animals in augmented reality. In 2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR). IEEE, 157–168.\n\n[45]\n\nNuria Oliver, Ashutosh Garg, and Eric Horvitz. 2004. Layered representations for learning and inferring office activity from multiple sensory channels. Computer Vision and Image Understanding 96, 2 (2004), 163–180.\n\n[46]\n\nFco Javier Ordóñez, José Antonio Iglesias, Paula De Toledo, Agapito Ledezma, and Araceli Sanchis. 2013. Online activity recognition using evolving classifiers. Expert Systems with Applications 40, 4 (2013), 1248–1255.\n\n[47]\n\nHeather L O’Brien, Paul Cairns, and Mark Hall. 2018. A practical approach to measuring user engagement with the refined user engagement scale (UES) and new UES short form. International Journal of Human-Computer Studies 112 (2018), 28–39.\n\n[48]\n\nXue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel van de Panne. 2018. Deepmimic: Example-guided deep reinforcement learning of physics-based character skills. ACM Transactions on Graphics (TOG) 37, 4 (2018), 143.\n\n[49]\n\nPatricia Pons and Javier Jaen. 2016. Towards the Creation of Interspecies Digital Games: An Observational Study on Cats’ Interest in Interactive Technologies. In Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems. 1737–1743.\n\n[50]\n\nPatricia Pons, Javier Jaen, and Alejandro Catala. 2017. Assessing machine learning classifiers for the detection of animals’ behavior using depth-based tracking. Expert Systems with Applications 86 (2017), 235–246.\n\n[51]\n\nThomas S Ray. 2001. Aesthetically evolved virtual pets. Leonardo 34, 4 (2001), 313–316.\n\n[52]\n\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing systems. 91–99.\n\n[53]\n\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, 2015. Imagenet large scale visual recognition challenge. International journal of computer vision 115, 3 (2015), 211–252.\n\n[54]\n\nSana-malik. 2014. Cat Location Dataset. Github.\n\n[55]\n\nPierre Sermanet, David Eigen, Xiang Zhang, Michaël Mathieu, Rob Fergus, and Yann LeCun. 2013. Overfeat: Integrated recognition, localization and detection using convolutional networks. (2013). arxiv:1312.6229\n\n[56]\n\nSichao Song and Seiji Yamada. 2017. Expressing emotions through color, sound, and vibration with an appearance-constrained social robot. In 2017 12th ACM/IEEE International Conference on Human-Robot Interaction (HRI. IEEE, 2–11.\n\n[57]\n\nPlayrock Studios. 2018. Wildlife AR. https://play.google.com/store/apps/dev?id=8981397870093114957.\n\n[58]\n\nIlya Sutskever, James Martens, and Geoffrey E Hinton. 2011. Generating text with recurrent neural networks. In Proceedings of the 28th International Conference on Machine Learning (ICML-11). 1017–1024.\n\n[59]\n\nChristian Szegedy, Alexander Toshev, and Dumitru Erhan. 2013. Deep neural networks for object detection. In Advances in neural information processing systems. 2553–2561.\n\n[60]\n\nTim Van Kasteren, Athanasios Noulas, Gwenn Englebienne, and Ben Kröse. 2008. Accurate activity recognition in a home setting. In Proceedings of the 10th international conference on Ubiquitous computing. ACM, 1–9.\n\n[61]\n\nFerdinand Wagner, Ruedi Schmuki, Thomas Wagner, and Peter Wolstenholme. 2006. Modeling software with finite state machines: a practical approach. CRC Press.\n\n[62]\n\nHao Wang and Jing Liu. 2009. Mobile phone based health care technology. Recent Patents on Biomedical Engineering 2, 1 (2009), 15–21.\n\n[63]\n\nSHI Xingjian, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun Woo. 2015. Convolutional LSTM network: A machine learning approach for precipitation nowcasting. In Advances in neural information processing systems. 802–810.\n\n[64]\n\nChloe Shu-Hua Yeh. 2015. Exploring the effects of videogame play on creativity performance and emotional responses. Computers in Human Behavior 53 (2015), 396–407.\n\n[65]\n\nXin Zhang, Yee-Hong Yang, Zhiguang Han, Hui Wang, and Chao Gao. 2013. Object class detection: A survey. ACM Computing Surveys (CSUR) 46, 1 (2013), 10.\n\nCited By\n\nView all\n\nKim MAlghofaili RLi CYu LDragon's Path: Synthesizing User-Centered Flying Creature Animation Paths for Outdoor Augmented Reality ExperiencesACM SIGGRAPH 2024 Conference Papers10.1145/3641519.3657397(1-11)\n\nLiu JZhang RSawabe TFujimoto YKanbara MKato HIntegrating Spatial Design with Reality: An AR Game Scene Generation Approach2024 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)10.1109/VRW62533.2024.00154(715-716)\n\nShen YXu MLiang WContext-Aware Head-and-Eye Motion Generation with Diffusion Model2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)10.1109/VR58804.2024.00039(157-167)\n\nShow More Cited By\n\nIndex Terms\n\nScene-Aware Behavior Synthesis for Virtual Pets in Mixed Reality\n\nComputing methodologies\n\nArtificial intelligence\n\nComputer vision\n\nComputer graphics\n\nGraphics systems and interfaces\n\nVirtual reality\n\nHuman-centered computing\n\nHuman computer interaction (HCI)\n\nInteraction paradigms\n\nMixed / augmented reality\n\nVirtual reality\n\nIndex terms have been assigned to the content through auto-classification.\n\nRecommendations\n\nMixed Reality Virtual Pets to Reduce Childhood Obesity\n\nNovel approaches are needed to reduce the high rates of childhood obesity in the developed world. While multifactorial in cause, a major factor is an increasingly sedentary lifestyle of children. Our research shows that a mixed reality system that is of ...\n\nMixed reality in virtual world teleconferencing\n\nVR '10: Proceedings of the 2010 IEEE Virtual Reality Conference\n\nIn this paper we present a Mixed Reality (MR) teleconferencing application based on Second Life (SL) and the OpenSim virtual world. Augmented Reality (AR) techniques are used for displaying virtual avatars of remote meeting participants in real physical ...\n\nHeadset removal for virtual and mixed reality\n\nSIGGRAPH '17: ACM SIGGRAPH 2017 Talks\n\nVirtual Reality (VR) has advanced significantly in recent years and allows users to explore novel environments (both real and imaginary), play games, and engage with media in a way that is unprecedentedly immersive. However, compared to physical reality,...\n\nInformation & Contributors\n\nInformation\n\nPublished In\n\n10862 pages\n\nISBN:9781450380966\n\nDOI:10.1145/3411764\n\nGeneral Chairs:\n\nYoshifumi Kitamura\n\nTohoku University, Japan\n\n,\n\nAaron Quigley\n\nUniversity of New South Wales, Australia\n\n,\n\nProgram Chairs:\n\nKatherine Isbister\n\nUniversity of California Santa Cruz, USA\n\n,\n\nTakeo Igarashi\n\nThe University of Tokyo, Japan\n\n,\n\nPublications Chairs:\n\nPernille Bjørn\n\nUniversity of Copenhagen, Denmark\n\n,\n\nSteven Drucker\n\nMicrosoft Research, USA\n\nCopyright © 2021 ACM.\n\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from [email protected]\n\nPublisher\n\nAssociation for Computing Machinery\n\nNew York, NY, United States\n\nPublication History\n\nPublished: 07 May 2021\n\nPermissions\n\nRequest permissions for this article.\n\nCheck for updates\n\nAuthor Tags\n\nBehavior Synthesis\n\nScene Semantics\n\nVirtual Pets\n\nQualifiers\n\nResearch-article\n\nResearch\n\nRefereed limited\n\nFunding Sources\n\nNSF CAREER Award\n\nNational Natural Science Foundation of China\n\nConference\n\nCHI '21\n\nAcceptance Rates\n\nOverall Acceptance Rate 6,199 of 26,314 submissions, 24%\n\nContributors\n\nOther Metrics\n\nBibliometrics & Citations\n\nBibliometrics\n\nArticle Metrics\n\n12\n\nTotal Citations\n\nView Citations\n\n820\n\nTotal Downloads\n\nDownloads (Last 12 months)207\n\nDownloads (Last 6 weeks)22\n\nOther Metrics\n\nCitations\n\nCited By\n\nView all\n\nKim MAlghofaili RLi CYu LDragon's Path: Synthesizing User-Centered Flying Creature Animation Paths for Outdoor Augmented Reality ExperiencesACM SIGGRAPH 2024 Conference Papers10.1145/3641519.3657397(1-11)\n\nLiu JZhang RSawabe TFujimoto YKanbara MKato HIntegrating Spatial Design with Reality: An AR Game Scene Generation Approach2024 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)10.1109/VRW62533.2024.00154(715-716)\n\nShen YXu MLiang WContext-Aware Head-and-Eye Motion Generation with Diffusion Model2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)10.1109/VR58804.2024.00039(157-167)\n\nNing BPei MTask and Environment-Aware Virtual Scene Rearrangement for Enhanced Safety in Virtual RealityIEEE Transactions on Visualization and Computer Graphics10.1109/TVCG.2024.337211530:5(2517-2526)\n\nDongye XWeng DJiang HFeng LA Modular Haptic Agent System with Encountered-Type Active InteractionElectronics10.3390/electronics1209206912:9(2069)\n\nChen CNguyen CHoffswell JHealey JBui TWeibel NPaperToPlace: Transforming Instruction Documents into Spatialized and Context-Aware Mixed Reality ExperiencesProceedings of the 36th Annual ACM Symposium on User Interface Software and Technology10.1145/3586183.3606832(1-21)\n\nKim MLee KBalan RLee YBubbleu: Exploring Augmented Reality Game Design with Uncertain AI-based InteractionProceedings of the 2023 CHI Conference on Human Factors in Computing Systems10.1145/3544548.3581270(1-18)\n\nLi WLi CKim MHuang HYu LLocation-Aware Adaptation of Augmented Reality NarrativesProceedings of the 2023 CHI Conference on Human Factors in Computing Systems10.1145/3544548.3580978(1-15)\n\nSierra Rativa APostma Mvan Zaanen MMeasuring Emotional Facial Expressions in Students with FaceReader: What Happens if Your Teacher is Not a Human, Instead, It is a Virtual Robotic Animal?Robotics in Education10.1007/978-3-031-38454-7_30(367-379)\n\nLi CLi WHuang HYu LInteractive augmented reality storytelling guided by scene semanticsACM Transactions on Graphics10.1145/3528223.353006141:4(1-15)\n\nShow More Cited By\n\nView Options\n\nGet Access\n\nLogin options\n\nCheck if you have access through your login credentials or your institution to get full access on this article.\n\nSign in\n\nFull Access\n\nView options\n\nPDF\n\nView or Download as a PDF file.\n\nPDF\n\neReader\n\nView online with eReader.\n\neReader\n\nHTML Format\n\nView this article in HTML Format.\n\nHTML Format\n\nMedia\n\nFigures\n\nOther\n\nTables\n\nShare\n\nShare\n\nShare this Publication link\n\nCopied!\n\nCopying failed.\n\nShare on social media\n\nAffiliations\n\nWei Liang\n\nSchool of Computer Science Beijing Institute of Technology, China\n\nXinzhe Yu\n\nBeijing Institute of Technology, China\n\nRawan Alghofaili\n\nGeorge Mason University, United States\n\nYining Lang\n\nAlibaba Group, China\n\nLap-Fai Yu\n\nComputer Science George Mason University, United States\n\nRequest permissions Authors Info & Affiliations"
    }
}
{
    "id": "correct_publicationDate_00014_3",
    "rank": 18,
    "data": {
        "url": "https://arxiv.org/abs/2306.02739",
        "read_more_link": "",
        "language": "en",
        "title": "Driven Robot Program Synthesis from Human VR Demonstrations",
        "top_image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png",
        "meta_img": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png",
        "images": [
            "https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg",
            "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg",
            "https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg",
            "https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg",
            "https://arxiv.org/icons/licenses/by-nc-nd-4.0.png",
            "https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png",
            "https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Franklin Kenghagho"
        ],
        "publish_date": null,
        "summary": "",
        "meta_description": "Aging societies, labor shortages and increasing wage costs call for assistance robots capable of autonomously performing a wide array of real-world tasks. Such open-ended robotic manipulation requires not only powerful knowledge representations and reasoning (KR&R) algorithms, but also methods for humans to instruct robots what tasks to perform and how to perform them. In this paper, we present a system for automatically generating executable robot control programs from human task demonstrations in virtual reality (VR). We leverage common-sense knowledge and game engine-based physics to semantically interpret human VR demonstrations, as well as an expressive and general task representation and automatic path planning and code generation, embedded into a state-of-the-art cognitive architecture. We demonstrate our approach in the context of force-sensitive fetch-and-place for a robotic shopping assistant. The source code is available at https://github.com/ease-crc/vr-program-synthesis.",
        "meta_lang": "en",
        "meta_favicon": "/static/browse/0.3.4/images/icons/apple-touch-icon.png",
        "meta_site_name": "arXiv.org",
        "canonical_link": "https://arxiv.org/abs/2306.02739",
        "text": "arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them."
    }
}
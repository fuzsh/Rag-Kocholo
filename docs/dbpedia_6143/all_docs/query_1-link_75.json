{
    "id": "dbpedia_6143_1",
    "rank": 75,
    "data": {
        "url": "https://vdoc.pub/documents/data-intensive-storage-services-for-cloud-environments-m3bof5b1c300",
        "read_more_link": "",
        "language": "en",
        "title": "Data Intensive Storage Services For Cloud Environments [PDF] [m3bof5b1c300]",
        "top_image": "https://vdoc.pub/img/detail/m3bof5b1c300.jpg",
        "meta_img": "https://vdoc.pub/img/detail/m3bof5b1c300.jpg",
        "images": [
            "https://vdoc.pub/theme/static/images/header-logo3.png",
            "https://vdoc.pub/theme/static/images/logo-socudoc-square.png",
            "https://vdoc.pub/theme/static/images/logo-socudoc-square.png",
            "https://vdoc.pub/img/detail/m3bof5b1c300.jpg",
            "https://vdoc.pub/img/crop/300x300/m3bof5b1c300.jpg",
            "https://vdoc.pub/img/crop/300x300/45el0bvtmic0.jpg",
            "https://vdoc.pub/img/crop/300x300/1euppu25q0lg.jpg",
            "https://vdoc.pub/img/crop/300x300/2c0j2qsuvahg.jpg",
            "https://vdoc.pub/img/crop/300x300/1al1lb783gs0.jpg",
            "https://vdoc.pub/img/crop/300x300/5rgss1e7apv0.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "Data Intensive Storage Services For Cloud Environments [PDF] [m3bof5b1c300]. With the evolution of digitized data, our society has become dependent on services to extract valuable information and e...",
        "meta_lang": "en",
        "meta_favicon": "https://vdoc.pub/theme/static/images/favicon/favicon-32x32.png",
        "meta_site_name": "",
        "canonical_link": "https://vdoc.pub/documents/data-intensive-storage-services-for-cloud-environments-m3bof5b1c300",
        "text": "E-Book Overview\n\nWith the evolution of digitized data, our society has become dependent on services to extract valuable information and enhance decision making by individuals, businesses, and government in all aspects of life. Therefore, emerging cloud-based infrastructures for storage have been widely thought of as the next generation solution for the reliance on data increases.\n\nData Intensive Storage Services for Cloud Environments provides an overview of the current and potential approaches towards data storage services and its relationship to cloud environments. This reference source brings together research on storage technologies in cloud environments and various disciplines useful for both professionals and researchers.\n\nE-Book Content\n\nData Intensive Storage Services for Cloud Environments Dimosthenis Kyriazis National Technical University of Athens, Greece Athanasios Voulodimos National Technical University of Athens, Greece Spyridon V. Gogouvitis National Technical University of Athens, Greece Theodora Varvarigou National Technical University of Athens, Greece\n\nManaging Director: Editorial Director: Production Manager: Publishing Systems Analyst:: Assistant Acquisitions Editor: Typesetter: Cover Design:\n\nLindsay Johnston Joel Gamon Jennifer Yoder Adrienne Freeland Kayla Wolfe Christina Barkanic Jason Mull\n\nPublished in the United States of America by Business Science Reference (an imprint of IGI Global) 701 E. Chocolate Avenue Hershey PA 17033 Tel: 717-533-8845 Fax: 717-533-8661 E-mail: [email protected] Web site: http://www.igi-global.com Copyright © 2013 by IGI Global. All rights reserved. No part of this publication may be reproduced, stored or distributed in any form or by any means, electronic or mechanical, including photocopying, without written permission from the publisher. Product or company names used in this set are for identification purposes only. Inclusion of the names of the products or companies does not indicate a claim of ownership by IGI Global of the trademark or registered trademark.\n\nLibrary of Congress Cataloging-in-Publication Data\n\nData intensive storage services for cloud environments / Dimosthenis P. Kyriazis, Athanasios S. Voulodimos, Spyridon V. Gogouvitis, and Theodora A. Varvarigou, editors. pages cm Includes bibliographical references and index. Summary: “This book provides an overview of the current and potential approaches towards data storage services and its relationship to cloud environments, bringing together research on storage technologies in cloud environments and various disciplines useful for both professionals and researchers”--Provided by publisher. ISBN 978-1-4666-3934-8 (hardcover : alk. paper) -- ISBN 978-1-4666-3935-5 (ebook : alk. paper) -- ISBN 978-1-46663936-2 (print & perpetual access : alk. paper) 1. Cloud computing. 2. Data warehousing. 3. Web services. I. Kyriazis, Dimosthenis P., 1973QA76.585.D38 2013 004.67’82--dc23 2013009979\n\nBritish Cataloguing in Publication Data A Cataloguing in Publication record for this book is available from the British Library. All work contributed to this book is new, previously-unpublished material. The views expressed in this book are those of the authors, but not necessarily of the publisher.\n\nEditorial Advisory Board Jörn Altmann, Seoul National University, South Korea Lorenzo Blasi, HP Italy Innovation Center, Italy Antonios Chatziparaskevas, National Bank of Greece, Greece Tariq Ellahi, SAP Research, UK Paolo Romano, Instituto de Engenharia de Sistemas e Computadores Investigação e Desenvolvimento, Portugal Osama Sammodi, University of Duisburg – Essen, Germany Aidan Shribman, SAP Research, Israel Ignacio Soler, ATOS Research and Innovation, Spain Massimo Villari, University of Messina, Italy\n\nList of Reviewers Michael Braitmaier, University of Stuttgart, Germany Konstantinos Christogiannis, National Technical University of Athens, Greece Tommaso Cucinotta, Scuola Superiore Sant’Anna, Italy Francisco Javier Martínez Elicegui, Telefónica I+D, Spain Ana Maria Juan Ferrer, ATOS Spain, Spain Ciro Formisano, Engineering Ingegneria Informatica SpA, Italy Georgina Gallizo, University of Stuttgart, Germany Michael Hauck, FZI Forschungszentrum Informatik, Germany Wolfgang Huther, Digital Film Technology, Germany Michael C. Jaeger, Siemens AG, Germany Hillel Kolodner, IBM Haifa, Israel Pavlos Kranas, National Technical University of Athens, Greece Dominik Lamp, University of Stuttgart, Germany Mirko Lorenz, Deutsche Welle, Germany Alessandro Mazzetti, eXact Learning Solutions, Italy Alberto Messina, RAI Radiotelevisione Italiana, Italy Malcolm Muggeridge, Xyratex, UK Karsten Oberle, Alcatel Lucent Bell Labs, Germany Eduardo Oliveros, Telefónica Investigación y Desarrollo, Spain Fredrik Solsvik, Telenor ASA, Norway Manuel Stein, Alcatel Lucent Bell Labs, Germany Gil Vernik, IBM Haifa, Israel Wolfgang Ziegler, Fraunhofer-Institut für Algorithmen und Wissenschaftliches Rechnen SCAI, Germany\n\nTable of Contents\n\nForeword . ........................................................................................................................................... xiv Preface . ............................................................................................................................................... xvi Acknowledgment . .............................................................................................................................. xix Chapter 1 Commercial and Distributed Storage Systems ....................................................................................... 1 Spyridon V. Gogouvitis, National Technical University of Athens, Greece Athanasios Voulodimos, National Technical University of Athens, Greece Dimosthenis Kyriazis, National Technical University of Athens, Greece Chapter 2 Key Distributed Components for a Large-Scale Object Storage ............................................................ 9 Miriam Allalouf, Jerusalem College of Engineering, Israel Ghislain Chevalier, Orange Labs, France Danny Harnik, IBM Haifa Research Labs, Israel Sivan Tal, Infinidat Ltd., Israel Chapter 3 Content Centric Storage and Current Storage Systems ........................................................................ 27 Michael C. Jaeger, Siemens AG, Corporate Technology, Germany Uwe Hohenstein, Siemens AG, Corporate Technology, Germany Chapter 4 Business Models and Billing Challenges ............................................................................................. 47 Javier Martínez Elicegui, Telefónica I+D, Spain Lei Xu, Umeå University, Sweden Emilio García Escobar, Telefónica I+D, Spain Chapter 5 Towards Federation and Interoperability of Cloud Storage Systems ................................................... 60 Sebastian Dippl, Siemens AG Corporate Technology, Germany Michael C. Jaeger, Siemens AG Corporate Technology, Germany Achim Luhn, Siemens AG Corporate Technology, Germany Alexandra Shulman-Peleg, IBM Haifa Research Lab, Israel Gil Vernik, IBM Haifa Research Lab, Israel\n\nChapter 6 SLA Management in Storage Clouds ................................................................................................... 72 Nikoletta Mavrogeorgi, National Technical University of Athens, Greece Spyridon V. Gogouvitis, National Technical University of Athens, Greece Athanasios Voulodimos, National Technical University of Athens, Greece Vasilios Alexandrou, National Technical University of Athens, Greece Chapter 7 Cloud Access Control Mechanisms ...................................................................................................... 94 Ciro Formisano, Engineering Ingegneria Informatica SPA, Italy Lucia Bonelli, Engineering Ingegneria Informatica SPA, Italy Kanchanna Ramasamy Balraj, Engineering Ingegneria Informatica SPA, Italy Alexandra Shulman-Peleg, IBM Haifa Research Lab, Israel Chapter 8 Compliance in the Cloud .................................................................................................................... 109 Lucia Bonelli, Engineering Ingegneria Informatica, Italy Luisa Giudicianni, Engineering Ingegneria Informatica, Italy Angelo Immediata, Engineering Ingegneria Informatica, Italy Antonio Luzzi, Engineering Ingegneria Informatica, Italy Chapter 9 Media Convergence and Cloud Technologies: Smart Storage, Better Workflows.............................. 132 Mirko Lorenz, Deutsche Welle, Germany Linda Rath-Wiggins, Deutsche Welle, Germany Wilfried Runde, Deutsche Welle, Germany Alberto Messina, RAI, Italy Paola Sunna, RAI, Italy Giorgio Dimino, RAI, Italy Maurizio Montagnuolo, RAI, Italy Roberto Borgotallo, RAI, Italy Chapter 10 Telecommunication Industry: Storage and Mobility........................................................................... 145 Fredrik Solsvik, Telenor ASA, Norway Michel Dao, Orange Labs, France Chapter 11 Data Intensive Enterprise Applications . ............................................................................................. 158 Peter Izsak, SAP Research Israel, Israel Aidan Shribman, SAP Research Israel, Israel\n\nChapter 12 Cloud Computing for Earth Observation . .......................................................................................... 166 Roberto Cossu, European Space Agency (ESRIN), Italy Claudio Di Giulio, European Space Agency (ESRIN), Italy Fabrice Brito, Terradue, Italy Dana Petcu, Institute e-Austria, Austria & West University of Timisoara, Romania Chapter 13 Cloud-TM: An Elastic, Self-Tuning Transactional Store for the Cloud.............................................. 192 João Barreto, Technical University Lisbon, Portugal Pierangelo Di Sanzo, Sapienza Università di Roma, Italy Roberto Palmieri, Sapienza Università di Roma, Italy Paolo Romano, Technical University Lisbon, Portugal Chapter 14 Storage Security and Technical Challenges of Cloud Computing . .................................................... 225 Shantanu Pal, University of Calcutta, India Chapter 15 Flashing in the Cloud: Shedding Some Light on NAND Flash Memory Storage Systems................ 241 Jalil Boukhobza, University of Western Brittany, France Chapter 16 XtreemFS: A File System for the Cloud.............................................................................................. 267 Jan Stender, Zuse Institute Berlin, Germany Michael Berlin, Zuse Institute Berlin, Germany Alexander Reinefeld, Zuse Institute Berlin, Germany Compilation of References ............................................................................................................... 286 About the Contributors .................................................................................................................... 309 Index...................................................................................................................................................319\n\nDetailed Table of Contents\n\nForeword.............................................................................................................................................xiv Preface.................................................................................................................................................xvi Acknowledgment................................................................................................................................xix Chapter 1 Commercial and Distributed Storage Systems ....................................................................................... 1 Spyridon V. Gogouvitis, National Technical University of Athens, Greece Athanasios Voulodimos, National Technical University of Athens, Greece Dimosthenis Kyriazis, National Technical University of Athens, Greece Distributed storage systems are becoming the method of data storage for the new generation of applications, as it appears a promising solution to handle the immense volume of data produced in today’s rich and ubiquitous digital environment. In this chapter, the authors first present the requirements end users pose on Cloud Storage solutions. Then they compare some of the most prominent commercial distributed storage systems against these requirements. Lastly, the authors present the innovations the VISION Cloud project brings in the field of Storage Clouds. Chapter 2 Key Distributed Components for a Large-Scale Object Storage ............................................................ 9 Miriam Allalouf, Jerusalem College of Engineering, Israel Ghislain Chevalier, Orange Labs, France Danny Harnik, IBM Haifa Research Labs, Israel Sivan Tal, Infinidat Ltd., Israel This chapter discusses distributed mechanisms that serve as building blocks in the construction of the VISION Cloud object service. Two are fundamental building blocks in the creation of a large-scale clustered object storage. These are distributed file systems and distributed data management systems. In addition, the authors study two complimentary topics that aim to improve the qualities of the underlying infrastructure. These are resource allocation mechanisms and improvements to data mobility via data reduction.\n\nChapter 3 Content Centric Storage and Current Storage Systems ........................................................................ 27 Michael C. Jaeger, Siemens AG, Corporate Technology, Germany Uwe Hohenstein, Siemens AG, Corporate Technology, Germany Content-centric storage represents an approach for handling large amounts of data. It is one of the innovations pursued by the VISION Cloud project. The goal of the VISION Cloud project is the development of an industry grade storage system using cloud technology. The envisaged use of the VISION Cloud involves the storage and management of millions of data items, potentially several hundreds of terabytes in size. On the one hand, the technical foundations must be capable of efficiently storing such an amount of data. On the other hand, the VISION Cloud must provide adequate means of an API for allowing the efficient navigation, search, and access for the right data item in this storage. For the latter purpose, VISION Cloud provides a data access layer, which is called “Content Centric Interface.” Applications can use this data access layer for accessing the VISION Cloud storage from a content-centric point of view, abstracted from actual storage representation. The content centric interface is different from existing cloud storage interfaces and is similar, from an architectural point of view, to object relational mapping frameworks for traditional applications with relational database systems. Chapter 4 Business Models and Billing Challenges ............................................................................................. 47 Javier Martínez Elicegui, Telefónica I+D, Spain Lei Xu, Umeå University, Sweden Emilio García Escobar, Telefónica I+D, Spain The advent of the Cloud has leveraged a number of challenges, both for customers and service providers. Companies willing to embrace the new paradigm must face some entrance barriers, such as security, privacy and trust concerns, vendor locking risk, legal issues, etc. While service providers may work to minimize these barriers, they must be especially careful when defining what may constitute the most crucial aspect for the success of their offerings: the business model. Different incarnations of the cloud (IaaS, PaaS, and SaaS) add to the possibility of offering public or private solutions, or even federated models. On top of this is the billing strategy: the ubiquitous pay-per-use approach (either in its most common post-paid incarnation, or in a novel pre-paid version) is only the starting point for a wide range of innovative solutions, including bundling or QoS considerations, which European project VISION Cloud is tackling as part of its research efforts. This chapter aims to provide a comprehensive discussion on the most relevant business factors that the Cloud confronts. Chapter 5 Towards Federation and Interoperability of Cloud Storage Systems ................................................... 60 Sebastian Dippl, Siemens AG Corporate Technology, Germany Michael C. Jaeger, Siemens AG Corporate Technology, Germany Achim Luhn, Siemens AG Corporate Technology, Germany Alexandra Shulman-Peleg, IBM Haifa Research Lab, Israel Gil Vernik, IBM Haifa Research Lab, Israel While it is common to use storage in a cloud-based manner, the question of true interoperability is rarely fully addressed. This question becomes even more relevant since the steadily growing amount of data that needs to be stored will supersede the capacity of a single system in terms of resources, availability, and network throughput quite soon. The logical conclusion is that a network of systems needs to be created\n\nthat is able to cope with the requirements of big data applications and data deluge scenarios. This chapter shows how federation and interoperability will fit into a cloud storage scenario. The authors take a look at the challenges that federation imposes on autonomous, heterogeneous, and distributed cloud systems, and present approaches that help deal with the special requirements introduced by the VISION Cloud use cases from healthcare, media, telecommunications, and enterprise domains. Finally, the authors give an overview on how VISION Cloud addresses these requirements in its research scenarios and architecture. Chapter 6 SLA Management in Storage Clouds ................................................................................................... 72 Nikoletta Mavrogeorgi, National Technical University of Athens, Greece Spyridon V. Gogouvitis, National Technical University of Athens, Greece Athanasios Voulodimos, National Technical University of Athens, Greece Vasilios Alexandrou, National Technical University of Athens, Greece The need for online storage and backup of data constantly increases. Many domains, such as media, enterprises, healthcare, and telecommunications need to store large amounts of data and access them rapidly any time and from any geographic location. Storage Cloud environments satisfy these requirements and can therefore provide an adequate solution for these needs. Customers of Cloud environments do not need to own any hardware for storing their data or handle management tasks, such as backups, replication levels, etc. In order for customers to be willing to move their data to Cloud solutions, proper Service Level Agreements (SLAs) should be offered and guaranteed. SLA is a contract between the customer and the service provider, where the terms and conditions of the offered service are agreed upon. In this chapter, the authors present existing SLA schemas and SLA management mechanisms and compare various features that Cloud providers support with existing SLAs. Finally, they address the problem of managing SLAs in cloud computing environments exploiting the content term that concerns the stored objects, in order to provide more efficient capabilities to the customer. Chapter 7 Cloud Access Control Mechanisms ...................................................................................................... 94 Ciro Formisano, Engineering Ingegneria Informatica SPA, Italy Lucia Bonelli, Engineering Ingegneria Informatica SPA, Italy Kanchanna Ramasamy Balraj, Engineering Ingegneria Informatica SPA, Italy Alexandra Shulman-Peleg, IBM Haifa Research Lab, Israel Cloud storage systems provide highly scalable and continuously available storage services to millions of geographically distributed clients. In order for users to trust their data to these systems, they need to be confident that their data is secure. Thus, cloud services should implement an access control mechanism preventing unauthorized access and manipulation of their data. This chapter presents the existing access control mechanisms and describes their advantages and limitations in the Cloud set-up. The authors address the main access control aspects that include managing the identities and defining access policies. Furthermore, they describe more complex scenarios of identity federation and integration of separate identity silos which is required in various scenarios, like collaboration, merge on acquisition, or migration. For each topic, the authors present the existing solutions and describe the motivation for the architecture developed by the VISION Cloud project.\n\nChapter 8 Compliance in the Cloud .................................................................................................................... 109 Lucia Bonelli, Engineering Ingegneria Informatica, Italy Luisa Giudicianni, Engineering Ingegneria Informatica, Italy Angelo Immediata, Engineering Ingegneria Informatica, Italy Antonio Luzzi, Engineering Ingegneria Informatica, Italy Despite the huge economic, handling, and computational benefits of the cloud technology, the multitenant and geographically distributed nature of clouds hides a large crowd of security and regulatory issues to be addressed. The main reason for these problems is the unavoidable loss of physical control that costumers are forced to accept when opting for the cloud model. This aspect, united with the lack of knowledge (i.e. transparency) of the vendor’s infrastructure implementation, represents a nasty question when costumers are asked to respond to audit findings, produce support for forensic investigations, and, more generically, to ensure compliance with information security standards and regulations. Yet, support for security standards compliance is a need for cloud providers to overcome customers hesitancy and meet their expectations. In this context, tracking, auditing, and reporting practices, while transcending the compliance regimes, represent the primary vehicle of assurance for security managers and auditors on the achievement of security and regulatory compliance objectives. The aim of this chapter is to provide a roundup of crucial requirements resulting from common security certification standards and regulation. Then, the chapter reports an overview of approaches and methodologies for addressing compliance coming from the most relevant initiatives on cloud security and a survey of what storage cloud vendors declare to do in terms of compliance. Finally, the SIEM-based approach as a supporting technology for the achievement of security compliance objectives is described and, the architecture of the security compliance component of the VISION Cloud architecture is presented. Chapter 9 Media Convergence and Cloud Technologies: Smart Storage, Better Workflows.............................. 132 Mirko Lorenz, Deutsche Welle, Germany Linda Rath-Wiggins, Deutsche Welle, Germany Wilfried Runde, Deutsche Welle, Germany Alberto Messina, RAI, Italy Paola Sunna, RAI, Italy Giorgio Dimino, RAI, Italy Maurizio Montagnuolo, RAI, Italy Roberto Borgotallo, RAI, Italy Why do media organizations look out for cloud storage? In short, the media industry as a whole is facing various challenges. Due to digital convergence there is more material, less time, and multiple channels to fill, while budgets get smaller. TV, video on demand, and mobile content have become big drivers in pushing a search for innovative storage solutions. In addition to that, the opportunity to work with raw data, which can be used for deeper analysis, mapping, visualization, and personalized services is another aspect of why there is a need for novel storage solutions, preferably in the cloud. The media industry could lower production costs and increase speed to market of time critical reporting. This book chapter provides an overview of how far VISION Cloud can provide novel concepts for these demands.\n\nChapter 10 Telecommunication Industry: Storage and Mobility........................................................................... 145 Fredrik Solsvik, Telenor ASA, Norway Michel Dao, Orange Labs, France The operators Telefónica, Orange, and Telenor represent the telecommunication industry in the VISION Cloud project. Together, they provide a telco-oriented use case, which provides feedback and requirements to the work on the reference architecture being developed. The use cases are developed based on the challenges and opportunities that are identified that relate to storage and mobility technologies. The use cases validate the reference architecture of VISION Cloud based on prototype tests and experimentations that enable the use case to be evaluated in scenarios. Telecommunication industry challenges are being addressed by the advancements made in the VISION Cloud project iterations, which takes the inputs from the telco use case and other use cases into consideration. This chapter is a study in the telecommunication industry challenges and possibilities with respect to the cloud storage technology advancements made in VISION Cloud. Chapter 11 Data Intensive Enterprise Applications . ............................................................................................. 158 Peter Izsak, SAP Research Israel, Israel Aidan Shribman, SAP Research Israel, Israel Today almost all big enterprises act globally, which results in a growing need for a new kind of data analytics. Imagine a company where data from distribution and sales needs to be combined with increasing online sales on multiple platforms and marketing across new social media channels. Here, new real-time analytics using Cloud Computing concepts can open new perspectives. SAP has had a strong presence in the Business Intelligence (BI) market. The company pioneered concepts to collect, combine, and analyze company wide information. As a result, SAP customers enjoy BI capabilities that are strongly integrated with their SAP operational systems (e.g., ERP, CRM). In recent years, companies have leveraged Cloud Computing as a means for lowering the Total Cost of Ownership (TCO) of various types of business applications that are provided On-Demand. SAP already offers products such as SAP Business ByDesign, which is offered as a Software-as-a-Service (SaaS) On-Demand product. Feature-rich Cloud storage solution such as VISION Cloud enables SAP to integrate new innovations to its On-Demand software portfolio. This chapter describes how VISION Cloud enriches SAP’s Instant Business Intelligence analytical On-Demand service. Chapter 12 Cloud Computing for Earth Observation . .......................................................................................... 166 Roberto Cossu, European Space Agency (ESRIN), Italy Claudio Di Giulio, European Space Agency (ESRIN), Italy Fabrice Brito, Terradue, Italy Dana Petcu, Institute e-Austria, Austria & West University of Timisoara, Romania This chapter elaborates on the impact and benefits Cloud Computing may have on Earth Observation. Earth Observation satellites generate in fact Tera- to Peta-bytes of data, and Cloud Computing provides many capabilities that allow an efficient storage and exploitation of such data. Several scenarios related to Earth Observation activities are analyzed in order to identify the possible benefits from the adoption of Cloud Computing. As concrete proofs-of-concept, several activities related to Cloud Computing in the context of Earth Observation are exposed and discussed. Technical details are provided for a particular framework used by Earth Observation applications that has made the transition from using Grid services towards using Cloud services. A special attention is given to the avoidance of the vendor-lock-in problem.\n\nChapter 13 Cloud-TM: An Elastic, Self-Tuning Transactional Store for the Cloud.............................................. 192 João Barreto, Technical University Lisbon, Portugal Pierangelo Di Sanzo, Sapienza Università di Roma, Italy Roberto Palmieri, Sapienza Università di Roma, Italy Paolo Romano, Technical University Lisbon, Portugal By shifting data and computation away from local servers towards very large scale, world-wide spread data centers, Cloud Computing promises very compelling benefits for both cloud consumers and cloud service providers: freeing corporations from large IT capital investments via usage-based pricing schemes, drastically lowering barriers to entry and capital costs; leveraging the economies of scale for both services providers and users of the cloud; facilitating deployment of services; attaining unprecedented scalability levels. However, the promise of infinite scalability catalyzing much of the recent hype about Cloud Computing is still menaced by one major pitfall: the lack of programming paradigms and abstractions capable of bringing the power of parallel programming into the hands of ordinary programmers. This chapter describes Cloud-TM, a self-optimizing middleware platform aimed at simplifying the development and administration of applications deployed on large scale Cloud Computing infrastructures. Chapter 14 Storage Security and Technical Challenges of Cloud Computing . .................................................... 225 Shantanu Pal, University of Calcutta, India Cloud computing has leaped ahead as one of the biggest technological advances of the present time. In cloud, users can upload or retrieve their desired data from anywhere in the world at anytime, making this the most important and primary function in cloud computing technology. While this technology reduces the geographical barriers and improves the scalability in the way we compute, keeping data in a Cloud Data Center (CDC) faces numerous challenges from unauthorized users and hackers within the system. Creating proper Service Level Agreements (SLA) and providing high-end storage security is the biggest barrier being developed for better Quality of Service (QoS) and implementation of a safer cloud computing environment for the Cloud Service Users (CSU) as well as for the Cloud Service Providers (CSP). Therefore, cloud applications need to have increased QoS and effective security measures and policies set in place to provide better services and to decline unauthorized access. The purpose of this chapter is to examine the cloud computing technology behind innovative business approaches and establishing SLA in cloud computing applications. This chapter provides a clear understanding of different cloud computing security challenges, risks, attacks, and solutions that exist in the present heterogeneous cloud computing environment. Storage security, different cloud infrastructures, the many advantages, and limitations are also discussed. Chapter 15 Flashing in the Cloud: Shedding Some Light on NAND Flash Memory Storage Systems........................................................................................................... 241 Jalil Boukhobza, University of Western Brittany, France Data and storage systems are one of the most important issues to tackle when dealing with cloud computing. Performance, in terms of data transfer and energy cost, predictability, and scalability are the main challenges researchers are faced with, and new techniques for storing, managing, and accessing huge amounts of data are required to make cloud computing technology feasible. With the emergence of flash memories in mass storage systems and the advantages it can provide in terms of speed and power efficiency as compared to traditional disks, one must rethink the storage system architectures accordingly. Indeed, the integration of flash memories is considered a key to leverage the performance of data-centric computing. The purpose of this chapter is to introduce flash memory storage systems by focusing on their specific architectures and algorithms, and finally their integration into servers and data centers.\n\nChapter 16 XtreemFS: A File System for the Cloud.............................................................................................. 267 Jan Stender, Zuse Institute Berlin, Germany Michael Berlin, Zuse Institute Berlin, Germany Alexander Reinefeld, Zuse Institute Berlin, Germany Cloud computing poses new challenges to data storage. While cloud providers use shared distributed hardware, which is inherently unreliable and insecure, cloud users expect their data to be safely and securely stored, available at any time, and accessible in the same way as their locally stored data. In this chapter, the authors present XtreemFS, a file system for the cloud. XtreemFS reconciles the need of cloud providers for cheap scale-out storage solutions with that of cloud users for a reliable, secure, and easy data access. The main contributions of the chapter are: a description of the internal architecture of XtreemFS, which presents an approach to build large-scale distributed POSIX-compliant file systems on top of cheap, off-the-shelf hardware; a description of the XtreemFS security infrastructure, which guarantees an isolation of individual users despite shared and insecure storage and network resources; a comprehensive overview of replication mechanisms in XtreemFS, which guarantee consistency, availability, and durability of data in the face of component failures; an overview of the snapshot infrastructure of XtreemFS, which allows to capture and freeze momentary states of the file system in a scalable and fault-tolerant fashion. The authors also compare XtreemFS with existing solutions and argue for its practicability and potential in the cloud storage market. Compilation of References.................................................................................................................286 About the Contributors......................................................................................................................309 Index...................................................................................................................................................319\n\nxiv\n\nForeword\n\nBy now, the concept of storing data on the Cloud needs no introduction. We keep our email and our photographs on the Cloud, pass documents between our mobile devices on the Cloud, and even do our tax returns on the Cloud. Yet, Cloud Computing, and in particular, storage services for Cloud environments, is still an emerging field. The rise in popularity of storage Clouds holds the promise of gaining value from data in ways that are only now starting to be investigated. The ability of storage Clouds to swallow vast amounts of heterogeneous data is outpacing our ability to efficiently search and manage this data. Data Intensive Storage Services for Cloud Environments is a visionary book. Realizing that the value of a storage Cloud comes from the ability to easily locate and manipulate vast amounts data while providing a secure and scalable infrastructure, this book surveys the state of the art in a wide number of technologies required to create a cutting-edge storage Cloud. The choice of chapters in this book was guided largely by the experience gained through the creation of VISION Cloud, an innovative storage Cloud created under the auspices of the European Union’s FP7 funding. Gathering together leading researchers from academia and industry across Europe, VISION Cloud has created a prototype of a next generation storage Cloud, which deals with such concepts as efficiency in big data environments, federation of data to avoid vendor lock in, security, multi-tenancy, and scalability. VISION Cloud’s development was guided by actual use cases from industry, which in turn helped those industrial partners realize new business opportunities afforded by adoption of VISION Cloud technologies. This book will take the reader from a survey of storage Clouds commercially available today, to a survey of the existing technologies required to create the infrastructure for a storage Cloud, such as distributed file systems and distributed management systems. Design decisions made in VISION Cloud are explained and will be instructive to others. The technologies considered to allow efficient searching of data stores are analyzed in light of VISION Cloud’s goal of implementing a rich API for accessing content. This book examines those management aspects required to make a storage Cloud commercially viable, such as providing for accounting and billing of services used, and the creation and management of Service Level Agreements – contracts between Cloud customers and service providers that create obligations on Cloud infrastructure. Security measures are studied, both for creating Cloud access mechanisms and for tracking compliance in the Cloud. Presented use cases from different industrial sectors illustrate new and improved workflows that storage Clouds can enable.\n\nxv\n\nData Intensive Storage Services for Cloud Environments will be useful to a technical audience ranging from students wanting to learn about Cloud Computing to engineers and researchers interested in developing Cloud technologies. The technology chapters can be read separately and in any order, and the chapters describing the use cases help to present a unifying picture of convergence of all of these technologies into a coherent solution. It is expected that many of the topics presented here can be researched future for potential Master and PhD topics. Eliot Salant IBM Haifa Labs, Israel\n\nEliot Salant has held a number of managerial and technical leadership positions over the last twenty years at the IBM Haifa Labs. Additionally, he has served as Project Coordinator for several large European Union sponsored Cloud computing projects, including RESERVOIR and VISION Cloud. Eliot holds a B.Eng. degree in Mechanical Engineering from McGill University, an M.S. degree in Biomedical Engineering from The Technion – Israel Institute of Technology, and an M.S. degree in Computer Science from Union College.\n\nxvi\n\nPreface\n\nThe explosion of personal and organisational digital data is presently recognised as one of the most significant characteristics of the last few years. Market reports indicate that since 2007 generated data is growing faster than the ability of humanity to store it. In parallel with the explosion of digital data, our society has become critically dependent on services to extract valuable information from data and driven by decision making by individuals, businesses, and government, across all aspects of life. In the emerging era of the Future Internet, the explosion of raw data and the dependence on data services is expected to be further amplified due to two important trends, namely the strong proliferation of data-intensive services and the digital convergence of Telco, Media, and ICT. In this context, emerging Cloud-based infrastructures for storage have been widely accepted as the next-generation solution to address the data proliferation and the reliance on data. Such infrastructures promise essentially infinite scalability and capacity, with continuous and ubiquitous availability at substantially lower costs than traditional storage solutions. Nevertheless, today’s cloud environments need to overcome specific limitations with regard to data intensive services. Creating an innovative storage cloud calls for progressing beyond the state-of-the-art in several technologies. The book at hand surveys the existing technologies pertaining to infrastructure as well as management aspects of a storage cloud, thus aiming at becoming a reference point for the research community. The target audience for the book ranges from PhD or Masters’ students to researchers and teachers. From the domain or community point of view, researchers in data-intensive services and cloud storage can discover through this book the particularities, strengths, weaknesses, and future trends of different research areas, and potentially draft a “research agenda” for future projects according to these. The book initially (Chapter 1) presents a survey of storage clouds commercially available today, compared and contrasted against the most important requirements that end users pose on Cloud storage solutions. Chapter 2 discusses two fundamental building blocks in the creation of large-scale clustered object storage, namely distributed file systems and distributed data management systems. Furthermore, resource allocation mechanisms and data reduction are explored as a means of improving the underlying infrastructure. The innovative concept of content centric storage is analyzed in Chapter 3. The content centric interface is different from existing cloud storage interfaces, being similar from an architectural point of view to object relational mapping frameworks for traditional applications with relational database systems. Chapter 4 addresses a crucial aspect behind any commercial Cloud offering: the business model. Special emphasis is placed on billing strategy: the ubiquitous pay-per-use approach (either in its most common post-paid implementation, or in a novel pre-paid version) is only the starting point for\n\nxvii\n\na wide range of innovative solutions, including bundling or QoS considerations. Chapter 5 shows how federation and interoperability can fit into a cloud storage scenario, describing the challenges that federation imposes on autonomous, heterogeneous, and distributed Cloud systems and presenting approaches that help deal with the special requirements introduced by healthcare, media, telecommunications, and enterprise use cases. The significant topic of SLA management in storage cloud is addressed in Chapter 6. The authors present existing SLA schemas and SLA management mechanisms, and also propose new solutions based on VISION Cloud’s content centric concept presented in a previous chapter. Chapter 7 presents existing access control mechanisms and describes their advantages and limitations in the Cloud set-up. The authors address the main access control aspects that include managing the identities and defining access policies. Moreover, they describe more complex scenarios of identity federation and integration of separate identity silos which is required in various scenarios like collaboration, merge on acquisition, or migration. Chapter 8, on the other hand, investigates another security-related aspect, that of compliance in the Cloud. Tracking, auditing, and reporting practices, while transcending the compliance regimes, represent the primary vehicle of assurance for security manager and auditors on the achievement of security and regulatory compliance objectives. The chapter provides a roundup of crucial requirements resulting from common security certification standards and regulation. It also reports an overview of approaches and methodologies for addressing compliance and details the SIEM-based approach as a supporting technology for the achievement of security compliance objectives. The following three chapters deal with Cloud-related use cases from three different domains. Chapter 9 provides an overview in how far a novel storage Cloud such as VISION Cloud can provide innovative concepts in the media industry, thus helping lower production costs and increase speed to market of time critical reporting. Subsequently, a study of telecommunication industry challenges and possibilities with respect to Cloud storage technology advancements is given in Chapter 10. The authors of Chapter 11 explore the added value that a cutting-edge storage Cloud can offer to companies that leverage on Cloud Computing as a means for lowering the Total Cost of Ownership (TCO) of various types of business applications that are provided on-demand. Turning to a more scientific domain, Chapter 12 elaborates on the impact and benefits Cloud Computing may have on Earth Observation. Earth Observation satellites generate in fact Tera to Peta bytes of data and Cloud Computing provides many capabilities that allow an efficient storage and exploitation of such data. Several scenarios related to Earth Observation activities are analyzed in order to identify the possible benefits from the adoption of Cloud Computing. In the next chapter, the authors pinpoint a major pitfall in Cloud environments: the lack of programming paradigms and abstractions capable of bringing the power of parallel programming into the hands of ordinary programmers. In this context, Chapter 13 describes Cloud-TM, a self-optimizing middleware platform aimed at simplifying the development and administration of applications deployed on large-scale Cloud Computing infrastructures. Moving on to Chapter 14, the authors examine the Cloud Computing technology behind innovative business approaches and establishing SLA in Cloud Computing applications. This chapter also provides a clear understanding of different Cloud Computing security challenges, risks, attacks, and solutions that exist in the present heterogeneous Cloud Computing environment. Storage security, different Cloud infrastructures, the many advantages, and limitations are also discussed. The purpose of Chapter 15, on the other hand, is to introduce flash memory storage systems by focusing on their specific architectures and algorithms, and finally their integration into servers and data centers. Finally, in Chapter 16 the authors present XtreemFS, a file system for the Cloud. XtreemFS reconciles the need\n\nxviii\n\nof Cloud providers for cheap scale-out storage solutions with that of Cloud users for a reliable, secure, and easy data access. Special emphasis is put on the internal architecture, the security infrastructure, the replication mechanisms, and the snapshot infrastructure of XtreemFS, which allows capturing and freezing momentary states of the file system in a scalable and fault-tolerant fashion. Covering a vast range of topics pertaining to data-intensive storage services in Cloud environments, we believe that this publication provides a substantial point of reference for the research community in its endeavor to find innovative ways to efficiently search and manage the great amounts of heterogeneous data produced across all aspects of life today. Dimosthenis Kyriazis National Technical University of Athens, Greece Athanasios Voulodimos National Technical University of Athens, Greece Spyridon V. Gogouvitis National Technical University of Athens, Greece Theodora A. Varvarigou National Technical University of Athens, Greece\n\nxix\n\nAcknowledgment\n\nFirst and foremost, we would like to thank all the chapter authors who have contributed their research results to this book. Without their efforts this book would not exist. We also owe a debt of gratitude to the reviewers who have provided valuable comments on the book chapters. We would also like to acknowledge the members of the Editorial Advisory Board for their assistance and constructive comments. Last but not least, special thanks are given to Mr. Joel Gamon of IGI Global for his assistance in the book preparation. Dimosthenis Kyriazis National Technical University of Athens, Greece Athanasios Voulodimos National Technical University of Athens, Greece Spyridon V. Gogouvitis National Technical University of Athens, Greece Theodora A. Varvarigou National Technical University of Athens, Greece\n\n1\n\nChapter 1\n\nCommercial and Distributed Storage Systems Spyridon V. Gogouvitis National Technical University of Athens, Greece Athanasios Voulodimos National Technical University of Athens, Greece Dimosthenis Kyriazis National Technical University of Athens, Greece\n\nABSTRACT Distributed storage systems are becoming the method of data storage for the new generation of applications, as it appears a promising solution to handle the immense volume of data produced in today’s rich and ubiquitous digital environment. In this chapter, the authors first present the requirements end users pose on Cloud Storage solutions. Then they compare some of the most prominent commercial distributed storage systems against these requirements. Lastly, the authors present the innovations the VISION Cloud project brings in the field of Storage Clouds.\n\nINTRODUCTION Distributed storage systems are becoming the method of data storage for the new generation of applications - Web applications by companies like Google, Amazon and Yahoo!. There are several reasons that explain the increasing trend towards distributed processing. On one hand, programs should be scalable and should take advantage of multiple systems as well as multi-core CPU architectures. On the other hand, Web servers have to be globally distributed for low latency and\n\nfailover. Object systems differ from file systems in the data model and access semantics they provide. Distributed object stores support wide distribution of data and access to data with high availability, even in presence of frequent node failures. They support data storage cloud services in which data is read and written by a wide variety of client applications running anywhere and typically using HTTP interfaces to access the storage service and their data. Several main differences in the characteristics of distributed object stores compared to file systems are:\n\nDOI: 10.4018/978-1-4666-3934-8.ch001\n\nCopyright © 2013, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.\n\nCommercial and Distributed Storage Systems\n\n•\n\n•\n\n•\n\n•\n\n2\n\nFlat namespace: Each object is addressed individually and independently of other objects, eliminating the hierarchical directory structure of file systems. With a flat namespace, each operation affects a single object, which helps keep consistency in widely distributed systems. Object metadata support provides some data management capabilities that compensate for the absence of directories. Some object stores provide the notion of containers, or buckets, that act as collections of objects and divide the namespace into multiple ones, also serving as a means of managing data and isolation between different data sets and multiple clients. Fixed content: Objects are typically written as a whole, rather than updated at the byte range level. This also helps keeping data consistency in the presence of multiple geographically distributed writers, while keeping the object stored highly available for read and write. Relaxed consistency models: Storage clouds consisting of distributed object stores deploy an eventual consistency model, meaning that when no node and network failures occur, eventually all nodes will have the same view of the data. Modern distributed object stores are typically built for world-wide spread with nodes distributed over a WAN. Data redundancy is typically supported through replication rather than RAID. Replicating data across distant locations also improves the availability and latency in access to the data. Each object storage cloud service has its own HTTP based API, and each API evolves over time by adding more features support through the API. The flexible APIs are developer-friendly and allow for rapid enhancements of the service. On the other hand, lack of standardization in this area results in lack of interoperability and the\n\ndata lock-in problem. Providing storage as a cloud service introduces a new ICT delivery model for storage. Businesses and individuals can consume flexible, variable and unlimited amount of storage without acquiring hardware, managing and maintaining it, paying per use for storage consumed “on demand”. In this chapter, we first present the requirements end-users pose on cloud storage services. Then we provide a high-level overview of the most prominent commercial offerings in cloud storage and compare them against the requirements set out. Lastly, we briefly present how the innovations of VISION Cloud compare to the presented commercial solutions\n\nREQUIREMENTS Within the context of the VISION Cloud project a number of requirements for a storage service have been identified. These can be basically broken down to the following categories: •\n\n•\n\n•\n\n•\n\nBasic access to the storage and interface requirements. The service should support the Create, Read, Update and Delete (CRUD) operations on the data objects and metadata through a standard interface such as CDMI. Requirements having to do with the efficient tagging of data with metadata, as well as the efficient search and retrieval of objects based on these metadata. Computational requirements. The storage service should allow for computations to be defined and executed on the stored objects and their metadata. Security and Compliance requirements. The issue of security is of major importance in storage clouds. This is not limited to providing encryption mechanisms, but also\n\nCommercial and Distributed Storage Systems\n\n•\n\n•\n\n•\n\nthe means to control access to the stored data through elaborate Access Control Lists (ACLs), Digital Rights Management issues, supporting Single-Sign-On features and others. Federation requirements. The storage system should be able to be seamlessly interconnected with other storage systems. This avoids vendor lock-in problems as the user is able to easily migrate to other providers. Moreover, it is desired that a user will be able to have a unified view of the data he/ she owns in different providers. SLA Requirements. User requirements are typically defines in terms of Service Level Agreements which define specific Key Performance Indicators of the provided service as well as costs and penalties in case these are not met. Accounting and Billing requirements. Every commercial storage cloud needs to have proper mechanisms to accurately bill its customers. The more agile these mechanisms are the more use cases can be covered.\n\nAPPROACHES AND IMPLEMENTATIONS Amazon S3 Amazon Simple Storage Service (S3) (Amazon Simple Storage Service (Amazon S3), n.d.) is one of the most prominent commercial cloud storage services. Amazon S3 is reported to store more than a trillion objects as of June 2012 1. The organization of data in S3 follows a twolevel approach. At the top level are buckets that are identified by a user-assigned global key. Buckets can contain an unlimited amount of data objects, where each object can be up to 5 terabytes in size. Each object also has a set of system defined metadata as well as user defined metadata in the\n\nform of name-value pairs. Objects are stored in multiple devices in multiple locations to provide high durability levels. S3 provides read-after-write consistency for PUTS of new objects and eventual consistency for overwrite PUTS and DELETES. Users are able to interact with the service through a REST or a SOAP interface. Moreover, S3 provides the ability to download any publicly available data through the use of the BitTorrent protocol. S3 also provides various services to users such as versioning capabilities. A user is able to define that a bucket should have versioning enabled in which case all objects are preserved in cases of PUT, POST, COPY or DELETE operations on them. A user is able to retrieve any version by supplying the version number. Also the service provides a Multipart Upload API that can be used to upload objects of sizes from 5 MB up to 5TB. Multi-Object Delete is another feature of the service that can be used to delete multiple objects in one request, thus speeding up the process. The user is also able to define expiration rules per bucket. Every object defined in the rule will be deleted once the expiration period has elapsed. S3 also provides a reduced redundancy storage (RRS) option at reduced pricing but with a lower promised durability level of 99.99% over a given year. Amazon’s pricing model follows a tiered approach based on the overall storage purchased by each user. Moreover, there are additional charges for data transfer out of the S3 Cloud as well as per PUT, COPY, POST, LIST and GET Requests. S3 comes with a 99.9% monthly uptime guarantee giving 10% Service Credits back in case the Monthly Uptime Percentage is equal to or greater than 99% and less than 99.9% and 25% back in case the Monthly Uptime is less than 99% (Amazon S3 service level agreement, 2009). Amazon also promises 99.999999999% durability over a given year, something not included in the SLA. S3 provides four different access control mechanisms: Identity and Access Management (IAM) policies, Access Control Lists (ACLs),\n\n3\n\nCommercial and Distributed Storage Systems\n\nbucket policies, and query string authentication. Moreover, Amazon promises that objects stored in a Region never leave the Region unless the user explicitly transfers them out. Data can be encrypted through Server Side Encryption in which case Amazon handles the management of all security keys or through client libraries before uploading the data to S3. Amazon S3 also supports data access auditing through access log records for all requests made against it. It is also worth mentioning that Amazon has started to provide an archiving service named Glacier2. Glacier is “optimized for data that is infrequently accessed and for which retrieval times of several hours are suitable” according to Amazon. S3 plans to incorporate an option to seamlessly move data between S3 and Glacier using lifecycle policies.\n\nGoogle Cloud Storage Google also provides a storage service through Google Cloud Storage (Google Cloud Storage, n.d.). It follows the same organization as Amazon S3 through the use of buckets and objects. Objects in Google Cloud Storage are immutable, meaning that no append or truncate operations are allowed. The consistency model followed is strong read-after-write consistency for upload and delete operations on objects and eventual consistency for list operations on objects and buckets. Users can interact with the service through a REST interface. Google Cloud Storage, much like S3, follows a tiered pricing model, charging for the overall storage consumed, network used for data exiting the cloud as well as the number of requests handled. Google Cloud Storage comes with a 99.9% monthly uptime guarantee giving 10% Service Credits back in case the Monthly Uptime Percentage is equal to or greater than 99% and less than 99.9% and 25% back in case the Monthly Uptime is between 95.0% and 99.9% and 50% back if it less than 95.0%.\n\n4\n\nGoogle Cloud Storage uses OAuth 2.0 for authorization and authentication as well as ACLs for controlling access to buckets and objects.\n\nWindows Azure Storage Microsoft provides a storage service through the Windows Azure Platform (Windows Azure Storage Services API Reference, 2010). Data are stored in the form of Blobs (Binary Large Objects) and each blob needs to reside within a container. Blobs come in two types, namely block blobs, which are more common, are optimized for streaming, and have a maximum size of 200GB and page blobs, which are usually used as virtual hard drives, since they are optimized for random read/write operations and provide the ability to write to a range of bytes in a blob. Page blobs have a maximum size of 1TB. Blobs and containers can have custom metadata in the form of name-value pairs. Each data object is replicated three times in the same sub-region (locally redundant option) and the user is also able to choose to have the data replicated to another sub-region to provide disaster recovery functionalities (geographically redundant option). Each account in Windows Azure Storage can hold up to 100 TB of data. The Windows Azure also provides a Tables service, which provides basic CRUD operations for non-relational tabular data, a SQL Database service which provides a scalable relational database, a Windows Azure Drives service which provides a mechanism for applications to mount a single volume NTFS VHD as a Page blob and a Queue service for storing messages. The Windows Azure SDK offers a REST API and a managed API for working with the storage services. The storage services may be accessed from within a service running in Windows Azure or directly over the Internet from any application that can send and receive data over HTTP/HTTPS. Windows Azure Storage follows a tiered pricing scheme based on the storage consumed\n\nCommercial and Distributed Storage Systems\n\nand whether local or geographical redundancy was chosen. Transactions are billed flatly ($0.01 per 100000 transactions). Windows Azure Storage comes with a monthly uptime guarantee of 99.9% giving 10% Service Credits back in case the Monthly Uptime Percentage less than 99.9% and 25% back in case the Monthly Uptime is less than 99%. It is also worth noting that the SLA defines specific metrics for when a transaction is considered to have failed. For example, a PutBlob or GetBlob (includes blocks and pages) operation must complete within the product of 2 seconds multiplied by the number of MBs transferred in processing the request, a CopyBlob operation must complete processing within 90 seconds, a PutBlockList or a GetBlockList operation must complete processing within 60 seconds, etc.\n\nRackspace Cloud Files Cloud Files (Rackspace Cloud Files, n.d.) is the Rackspace’s cloud storage offering powered by open-source OpenStack3. Rackspace provides a REST API for accessing the storage service as well as language specific bindings. The data model of Cloud Files follows a two level scheme, with objects being the basic storage entities which are located inside containers. There is no hierarchy of containers, i.e. containers cannot be nested. Objects cannot be larger than 5GB, but there is the option to upload larger objects in 5GB segments than can be later downloaded as a single concatenated object. This is achieved through a manifest object that when retrieved returns the concatenated objects. Custom metadata can be defined for objects and containers, but should not exceed 90 individual key/value pairs and the total byte length should not exceed 4KB. Since Rackspace Cloud Files is based on Opestack Swift it follows an eventual consistency model. Redundancy is achieved by replicating every data object three times in different storage nodes. Cloud Files also allows for an expiration date to be set when creating an object, which leads to the object\n\nbeing deleted when the target time limit is met. It also provides a versioning option by which when new data is written to an object the non-current version is moved to a different container. One of the most prominent features of Cloud Files is the ability to integrate with Akamai’s Content Delivery Network (CDN). This is achieved by defining a container to be CDN-enabled. Each CDN-enabled container also has a streaming URL that can be used to stream content. Cloud Files has a fixed cost per Gigabyte, irrespective of the storage consumed and does not charge for the requests handled, while still charging for outgoing bandwidth. The price for outgoing bandwidth is the same whether Akamai’s CDN is used or not. Cloud Files provides a 99.9% availability SLA while reductions are depicted in Table 1. Rackspace gives the following definition for unavailability: Unavailability means: (1) The Rackspace Cloud network is down, or (2) the Cloud Files service returns a server error response to a valid user request during two or more consecutive 90 second intervals, or (3) the Content Delivery Network fails to deliver an average download time for a 1-byte reference document of 0.3 seconds or less, as measured by The Rackspace Cloud’s third party measuring service. Security in Cloud Files is enforced through an authentication token that is received during login and used in all subsequent requests to the service. Table 1. Cloud files SLA Total Cloud Files Available Time 100% - 99.9%\n\nCredit Amount 0%\n\n99.89%\n\n- 99.5%\n\n10%\n\n99.49%\n\n- 99.0%\n\n25%\n\n98.99%\n\n- 98.0%\n\n40%\n\n97.99%\n\n- 97.5%\n\n55%\n\n97.49%\n\n- 97.0%\n\n70%\n\n96.99%\n\n- 96.5%\n\n85%\n\nLess than 96.5%\n\n100%\n\n5\n\nCommercial and Distributed Storage Systems\n\nThere is no option to encrypt data on the server side, but each customer is responsible for encrypting the data prior to uploading it to the service.\n\nEMC Atmos EMC Atmos (Atmos Online Programmer’s Guide, 2010) is different to the solutions presented thus far in that it is not a commercial storage service, but a storage platform that can be used to build private or public clouds. EMC Atmos has two different deployment models. The standard EMC Atmos where specific hardware along with appropriate software is used and Atmos Virtual Edition, where the software stack can be installed on third-party storage systems. The platform provides REST and SOAP Webbased APIs, as well as CIFS/NFS traditional file access. It has two data models, an object interface and a namespace interface that is similar to a file system with folders. Objects in Atmos have metadata associated with them, which are both system as well as user defined. Moreover, each object has an ACL specifying which users and groups are allowed access to the data object. EMC Atmos has a rich account model, supporting a three-level hierarchy of tenants, subtenants and users. Each tenant can be assigned its own interface nodes and dedicated resources while subtenants as logically separated by each having its own namespace. The notion of policies also exists in EMC Atmos, through which various management operations can be set, such as number of replicas, compression, deduplication, retention and deletion periods. Replication policies can be set through the metadata of each object. Atmos provides the options of having a mixture of synchronous and asynchronous replicas, while also allowing for geo-constraints to be defined. EMC provides security through ACLs per data object, while there are no capabilities for server side encryption.\n\n6\n\nCOMPARISON Table 2 compares the five services under investigation against the high-level requirements set forth.\n\nVISION CLOUD APPROACH Comparing VISION Cloud to the presented commercial offerings leads to the following conclusions:\n\nRich Metadata While the notion of coupling data with metadata is not new, VISION Cloud allows for much richer metadata than what is currently offered by commercial solutions. Metadata in VISION Cloud can be used for active indexing and other content management support operations inside the storage infrastructure. This option is only available in EMC Atmos, in a more basic form.\n\nContent-Centric Access VISION Cloud also goes a step beyond in the use of metadata by providing the ability to set relations between objects. Therefore, content networks can be created dynamically and automatically by discovery and construction processes that are integrated into the infrastructure, driven by userprovided specifications for the specific domains of interest.\n\nComputational Storage VISION Cloud also promotes the idea of bringing computations close to the storage, through the notion of storlets. While most commercial offerings provide computational resources apart from storage resources, these are considered as separate services and there is no guarantee of their co-location. Storlets, on the other hand, provide\n\nCommercial and Distributed Storage Systems\n\nTable 2. Comparison of commercial offerings to VISION cloud requirements (grouped) Amazon S3\n\nGoogle Cloud Storage\n\nWindows Azure Storage\n\nRackspace Cloud Files\n\nEMC Atmos\n\nInterface\n\nREST, SOAP, BitTorrent\n\nREST\n\nREST, Managed API\n\nREST\n\nREST,SOAP, CIFS. NFS, Atmos filesystem\n\nQuery Capabilities on Metadata\n\nNO\n\nNO\n\nNO\n\nNO\n\nYES (only by metadata key)\n\nComputations\n\nThrough EC2 instances\n\nThrough Google Compute Engine\n\nThrough Azure Compute Service\n\nThrough Cloud Servers\n\nNO\n\nFederation\n\nNO\n\nNO\n\nNO\n\nNO\n\nYES (with Atmos public providers)\n\nSecurity\n\nIAM policies, ACLs, bucket policies and query string authentication\n\nOAuth 2.0, ACLs\n\nACLs, Shared Access Signatures\n\nAuthentication Key\n\nAuthentication Key, ACLs\n\nServer-side encryption\n\nYES\n\nNO\n\nNO\n\nNO\n\nNO\n\nSLA metrics\n\nAvailability metric\n\nAvailability metric\n\nAvailability, clearly defined max. processing times for operations\n\nAvailability\n\nN/A\n\na programming framework able to run computations close to the data. Moreover, storlets can be activated by events such as uploading of a new object, adding metadata to it etc.\n\nFederation By leveraging and extending the CDMI standard, as well as implementing the appropriate mechanisms, VISION Cloud is able to provide federation capabilities, allowing for companies to bridge private cloud installations to public cloud storages hosted at different providers.\n\nSecurity VISION Cloud supports the authentication both with the internal user services as well as with external identity providers through SAML SSO. Moreover, it combines ACL-based and ABAC models for granting authorization on the object or container levels, allowing for versatile authorization schemes. While VISION Cloud does\n\nnot currently provide a server-side encryption mechanism, this can be relatively easily achieved by using storlets.\n\nRich SLAs and Billing Mechanisms Current commercial offerings provide rudimentary SLAs that are focused on service provisioning, storage resources, capacity, and service availability. In VISION Cloud SLAs are more dynamic in nature and able to accommodate more of the customers’ needs. This coupled with advanced billing mechanisms that can accommodate not only post-paid but also pre-paid scenarios allows VISION Cloud to support new business models.\n\nCONCLUSION Providing storage as a service is an important aspect of the emerging Cloud ecosystem. Commercial offerings are nowadays well established, but still issues such as ease of management, data\n\n7\n\nCommercial and Distributed Storage Systems\n\nmobility and federation, coupling storage with computing power and guaranteeing QoS need to be researched to address the increasing volumes of data that are being produced and need to be processed and stored. In this chapter we investigated how commercial cloud offerings differ and what the VISION Cloud project proposes as a means to advance in this field.\n\nREFERENCES Amazon Simple Storage Service (Amazon S3). (n.d.). Retrieved from http://aws.amazon.com/s3/ Atmos Online Programmer’s Guide. (2010). Retrieved from https://community.emc.com/docs/ DOC-3481 Google Cloud Storage. (n.d.). Retrieved from http://www.google.com/enterprise/cloud/ storage/ Rackspace Cloud Files. (n.d.). Retrieved from http://www.rackspace.com/cloud/cloud_hosting_products/files/ Windows Azure Storage Services API Reference. (2010). Retrieved from http://msdn.microsoft. com/en-us/library/dd179355.aspx\n\n8\n\nKEY TERMS AND DEFINITIONS Cloud Computing: Cloud computing is a model of offering compute, network and storage, possibly virtualized, resources over a network. Distributed Computing: Distributed Computing refers to any computing system that individual nodes are distributed and cooperate through a network to solve a problem. Federation: The ability to use more than one Cloud provider and have their services interoperate. Service Level Agreement (SLA): A SLA is the negotiated contract between a customer and a provider of a service. The SLA defines the quality of the service, its cost, the responsibilities of the provider and the customer and other binding terms. Virtualization: The creation and provision of resources which are virtual, rather than actual, enabling them to be used as a service.\n\nENDNOTES 1\n\n2\n\n3\n\nhttp://aws.typepad.com/aws/2012/06/ amazon-s3-the-first-trillion-objects.html http://aws.amazon.com/glacier/ www.openstack.org/\n\n9\n\nChapter 2\n\nKey Distributed Components for a Large-Scale Object Storage Miriam Allalouf Jerusalem College of Engineering, Israel Ghislain Chevalier Orange Labs, France Danny Harnik IBM Haifa Research Labs, Israel Sivan Tal Infinidat Ltd., Israel\n\nABSTRACT This chapter discusses distributed mechanisms that serve as building blocks in the construction of the VISION Cloud object service. Two are fundamental building blocks in the creation of a large-scale clustered object storage. These are distributed file systems and distributed data management systems. In addition, the authors study two complimentary topics that aim to improve the qualities of the underlying infrastructure. These are resource allocation mechanisms and improvements to data mobility via data reduction.\n\nINTRODUCTION Building a large-scale object storage requires the ability to distribute storage across multiple disks, nodes, racks and eventually data centers. Our design identified two main tools that can form the foundation of such a large object storage. The first is a distributed file system that should form the storage backbone for the actual data of the objects. The second is a distributed data management mechanism that will serve as DOI: 10.4018/978-1-4666-3934-8.ch002\n\na central management tool for metadata (both system metadata and user metadata). The properties and performance of these components will have major influence on the ability of the storage cloud to serve requests both in terms of speed and in terms of scale. As such, they must fulfil the following crucial requirements (as referenced in the requirements section): Scalability (in terms of number of objects, total capacity, containers, etc, the ability to support performance at a high scale (scale out performance), Elasticity, Availability and eventual consistency. In addition, we survey in this chapter two additional topics regarding\n\nCopyright © 2013, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.\n\nKey Distributed Components for a Large-Scale Object Storage\n\nhighly distributed mechanisms. Unlike the above mentioned distributed file system and data base, that are fundamental building blocks for the object service, the next two topics study means and methodologies of gaining added value to the VISION Cloud over existing designs. The first topic is a resource allocation mechanism across a global cloud, with an emphasis on placement of object containers. The goal here is to make smarter allocations of resources such as disk, bandwidth and CPU both on a cloud wide basis and locally in a cluster. The ultimate goal is to improve performance and reduce overall cost in the cloud. The second is methodologies of reducing traffic and improving data mobility in the cloud with a positive side effect of reducing the required storage capacity. Since the amount of data being transferred across a WAN in the cloud is huge (this can be for replication, for user applications or for recovery), the time and resources spent on this transfer make bandwidth a scarce resource and moving data harder to handle. The aim of this work package is to reduce this burden and the underlying technique is by using data reduction methodologies, and most notably deduplication (finding repeating data across the cloud, and thus not transferring data that is already at the target location).\n\nto span across all nodes in the DFS cluster, effectively creating a unified Global Namespace for all files: • • •\n\nUnifying files on different computers into a single namespace. Files are distributed across multiple servers appearing to users as being stored at a unique place on the network. Users no longer need to know and specify the actual physical location of files in order to access them.\n\nDistributed file systems may include facilities for transparent replication and fault tolerance. When a limited number of nodes in a file system go offline, the system continues to work without any data loss or unavailability. Distributed File Systems (DFS) maintain control of file and data layout across the nodes and employ metadata and locking mechanisms that are fully distributed and cohesively maintained across the cluster, enabling the creation of a very large global pool of storage. A DFS can seamlessly scale to petabytes of storage. A fully distributed file system can handle metadata operations, file locking, and cache management tasks by distributing operations across all the nodes in the cluster.\n\nDISTRIBUTED FILE SYSTEMS\n\nApproaches, Implementations, and Comparisons\n\nOverview\n\nGPFS\n\nFile systems evolved over time. Starting with local file systems over time additional file systems appeared focusing on specialized requirements such as data sharing, remote file access, distributed file access, parallel files access, HPC, archiving, etc. A Distributed file System (DFS) is a network file system whose clients, servers, and storage devices are dispersed among the machines of a distributed system or intranet. Using a networking protocol between nodes, a DFS allows a single file system\n\nGPFS (General Parallel File System) (Schmuck & Haskin, 2002) is IBM’s clustered file system. The following description is taken from (IBM General Parallel File System, n.d.). IBM’s high-performance shared-disk clustered file system GPFS (General Parallel File System) powers many of the world’s largest scientific and commercial applications requiring high-speed access to large volumes of data such as:\n\n10\n\nKey Distributed Components for a Large-Scale Object Storage\n\n• • • • • • •\n\nDigital media Engineering design Business intelligence Financial analytics Seismic data processing Geographic information systems Scalable file serving\n\nGPFS provides online storage management, scalable access, and integrated information lifecycle management tools capable of managing petabytes of data and billions of files. Virtualising your file storage space and allowing multiple systems and applications to share common pools of storage provides you the flexibility to transparently administer the infrastructure without disrupting applications, improving cost and energy efficiency, while reducing management overhead. IBM’s Scale Out NAS (SONAS) product exports the clustered file system through industry standard protocols like CIFS, NFS, FTP and HTTP. Released in 2010, SONAS is a second generation file services architecture first used within IBM to store employees’ files since 2001. All of the file system nodes export all files of all file systems simultaneously (they are called interface nodes in SONAS terminology). This is a different approach from some other clustered NAS solutions which pin individual files to a single node or pair of nodes thus limiting the single file performance. Each file system can be multiple Petabytes in size. SONAS combines proprietary IBM technology (storage & server hardware and GPFS) with open source components like Linux, Samba (free software re-implementation of SMB/CIFS networking protocol) and CTDB (cluster implementation of the TDB database used by Samba and other projects to store temporary data).\n\nOneFS OneFS is Isilon System’s clustered file system support, as described in Isilon’s Web site (Isilon OneFS Operating System, n.d.): OneFS is Isilon’s sixth-generation operating system that provides\n\nthe intelligence behind all Isilon scale-out storage systems. It combines the three layers of traditional storage architectures-file system, volume manager and RAID-into one unified software layer, creating a single intelligent file system that spans all nodes within a cluster. Isilon’s OneFS enables: •\n\n• •\n\nIndependent or linear scalability of performance and capacity to over 85 Gigabytes per second of throughput and more than 15.5 petabytes of capacity in a single file system. A single point of management for large and rapidly growing repositories of data. Mission-critical reliability and high availability with state-of-the-art data protection.\n\nUnlike simple NAS namespace aggregation products, Isilon’s OneFS operating system is truly distributed and stripes data across all nodes in a cluster to create a single, shared pool of storage. OneFS offers mission-critical reliability and industry-leading drive rebuild times. OneFS also delivers cluster-aware Symmetric Multiprocessing (SMP) capabilities that enable the system to move tasks between processors for extremely efficient workload balancing. In conjunction with OneFS’ ability to stripe data across all nodes in a cluster, Isilon achieves the high aggregate bandwidth and transactional performance required to power next generation enterprise data centers. Each node in an Isilon clustered storage system is a peer, so any node can handle a request. Using InfiniBand for intracluster communication and synchronization, OneFS provides each node with knowledge of the entire file system layout and where each file and part thereof is located. OneFS controls the placement of files directly on individual disks and dramatically improves the performance of the disk sub-system by optimally distributing files across the cluster. By laying data on disks in a file-by-file manner, OneFS is able to control the redundancy level of the storage system at the volume, directory, and even file levels.\n\n11\n\nKey Distributed Components for a Large-Scale Object Storage\n\nGlusterFS GlusterFS (GlusterFS, n.d.) is a general purpose distributed file system for scalable storage. A summary of its main concept is presented below and taken from (GlusterFS wiki, n.d.): GlusterFS aggregates so called “storage bricks” over Infiniband or TCP/IP interconnect into one large parallel network file system. Its design supports a stack-able user space design without compromising performance and has been used for a variety of applications ranging from Cloud Computing, Biomedical Sciences to Archival Storage. GlusterFS has a client and server component. Servers are typically deployed as storage bricks, with each server running a glusterfsd daemon to export a local file system as a volume. The glusterfs client process, which connects to servers with a custom protocol over TCP/IP, InfiniBand or SDP, composes remote volumes into larger ones. Applications doing large amounts of file I/O can also use the libglusterfs client library to connect to the servers directly and run translators in-process, without going through the file system and incurring extra overheads. The main features supported by GlusterFS are the following, as is implemented by so called “translators”: • • • • • •\n\nFile-based mirroring and replication File-based striping File-based load balancing Volume failover Scheduling and disk caching Storage quotas\n\nThe GlusterFS server exports an existing file system as-is, leaving it up to client-side translators to structure the store. The clients themselves are stateless, do not communicate with each other, and are expected to have translator configurations consistent with each other. This can cause coherency problems, but allows GlusterFS to scale up to several petabytes on commodity hardware by\n\n12\n\navoiding bottlenecks that normally affect more tightly-coupled distributed file systems.\n\nLustre Lustre (Lustre, n.d.) is a massively parallel distributed file system, generally used for large scale cluster computing. The name Lustre is derived from Linux and cluster. Available under the GNU GPL, the project aims to provide a file system for clusters of tens of thousands of nodes with petabytes of storage capacity, without compromising speed, security, or availability. Lustre is designed, developed, and maintained by Oracle Corporation, by way of its 2010 acquisition of Sun Microsystems, with input from many other individuals and companies. Lustre file systems are used in computer clusters ranging from small workgroup clusters to large-scale, multi-site clusters. Fifteen of the top 30 supercomputers in the world use Lustre file systems, including the world’s fastest supercomputer (as of October 2010). The Lustre architecture is comprised of a single Metadata Target (MDT) per filesystem and multiple Object Storage Servers (OSSes) that store file data on one or more Object Storage Targets (OSTs). In addition, there are client components for accessing the data. Lustre exposes to the clients a standard POSIX interface with a unified namespace for all of the files and data in the filesystem, and allows concurrent reads and writes. In typical installations, the MDT and OST functions are located on separate nodes communicating over a network. Lustre supports several network types, including native Infiniband verbs, TCP/IP on Ethernet and a variety of other networks. The storage attached to the servers is typically accessed using the ext4 file system. In Lustre clients do not directly modify the objects on the OST filesystems, but, instead, delegate this task to OSSes. This is in contrast to some shared block-based filesystems that allow direct access to the underlying storage by all of the clients in the filesystem.\n\nKey Distributed Components for a Large-Scale Object Storage\n\nGoogleFS Google File System (GFS) (Ghemawat, Gobioff, & Leung, 2003) is a file system designed by Google to support its applications. As such, it is not designed for the general purpose use of a file system interface, but rather for the internal use of Google’s search engine data collecting mechanisms. The most frequent operation in this environment is an append operation of chunks of data onto large existing files. Only rarely are files modified, so operations that edit exiting content can have very bad response times without hurting the main cause. The basic architecture of a GFS cluster is composed of many Chunkservers along with one single Master node that serves for metadata queries. Actual chunks are communicated directly between Clients and Chunkservers, while the Master’s sole purpose is to manage and answer periodic queries regarding chunk distributions across the nodes. Other design points are the preference of providing high throughput rather than low latency and the use of cheap commodity hardware. The later calls for intensive replication of data to avoid data loss, and indeed each chunk is replicated across at least three Chunkservers. As opposed to many filesystems, GFS is not implemented in the kernel of an operating system, but is instead provided as a userspace library.\n\nHDFS The Hadoop Distributed File System (HDFS) (HDFS, Apache Hadoop Project, n.d.) is a distributed, scalable, and portable filesystem written in Java for the Hadoop framework. Much like GFS (which HDFS originally attempted to mimic) HDFS stores large files across multiple machines, and is designed to handle very large files. It has a primary Namenode for metadata handling and numerous data nodes. HDFS achieves reliability by replicating data across multiple hosts, where data is typically stored on three nodes: two on the same rack, and one on a different rack. Data\n\nnodes can talk to each other to rebalance data, to move copies around, and to keep the replication level of data. HDFS is not fully POSIX compliant because the requirements for a POSIX filesystem differ from the target goals for a Hadoop application, namely, increased performance for data throughput (on the other hand, high availability is not one of HDFS’s stated target goals). File access is supported through the native Java API or the Thrift API (for generating a client in other languages). Being built to be used by Hadoop jobs, HDFS cannot be directly mounted by an existing operating system. Getting data into and out of the HDFS file system, an action that often needs to be performed before and after executing a job, can be inconvenient. A Filesystem in Userspace (FUSE) virtual file system has been developed to address this problem, at least for Linux and some other Unix systems.\n\nPVFS The Parallel Virtual File System (PVFS) (Blumer & Ligon, 1994) is an open source parallel file system. A parallel file system is a type of distributed file system that distributes file data across multiple servers and provides for concurrent access by multiple tasks of a parallel application. PVFS was designed for use in large scale cluster computing. PVFS focuses on high performance access to large data sets. It consists of a server process and a client library, both of which are written entirely of user-level code. A Linux kernel module and a pvfs-client process allow the file system to be mounted and used with standard utilities. The client library provides for high performance access via the Message Passing Interface (MPI). PVFS is being jointly developed between the Parallel Architecture Research Laboratory at Clem-son University and the Mathematics and Computer Science Division at Argonne National Laboratory, and the Ohio Supercomputer Centre and has been funded by NASA Goddard Space Flight Centre, DOE Argonne National Laboratory, NSF PACI,\n\n13\n\nKey Distributed Components for a Large-Scale Object Storage\n\nand HECURA programs, and other government and private agencies. In a cluster using PVFS nodes are designated as one or more of: client, data server, and metadata server. Data servers hold file data, metadata servers hold metadata including stat-info, attributes, and datafile-handles as well as directory entries. Clients run applications that utilize the file system by sending requests to the servers over the network. The above information was collected from (Parallel Virtual File System wiki, n.d.).\n\nXtreemFS XtreemFS (XtreemFS, n.d.) is introduced as a distributed and replicated file system for the Cloud. In its framework, distributed Clients and servers can be distributed world-wide. XtreemFS supports installations across many data centers and is able to handle the failures that occur in wide-area installations. Clients can mount XtreemFS volumes from anywhere with an Internet connection. XtreemFS has been under development since early 2007 and a first public release was made in August 2008. XtreemFS is part of the XtreemOS project which is funded by the European Commission’s IST programme. Some of the main features are: •\n\n• • • •\n\n14\n\nReplication: All XtreemFS services can be replicated with hot stand-byes which automatically take over when the primary server fails. POSIX compatibility. Elasticity: An XtreemFS installation can add or remove storage serves and scale to thousands of storage and metadata servers. Security: Supports SSL and X.509 certificates to allow running XtreemFS over the Internet without a VPN. Extensibility: Has a plug-in architecture through which one can modify the behavior of XtreemFS, e.g., to better integrate with a security infrastructure.\n\n•\n\nStriping of files over several storage servers (similar to RAID-0) for increased I/O bandwidth and capacity.\n\nMicrosoft DFS Microsoft DFS (What is DFS?, n.d.) is a set of services allowing administrators to group shared folders located on different servers by transparently connecting them to one or more DFS namespaces. A DFS namespace is a virtual view of shared folders in an organization. Using the DFS tools, an administrator selects which shared folders to present in the namespace, designs the hierarchy in which those folders appear, and determines the names that the shared folders show in the namespace. When a user views the namespace, the folders appear to reside on a single, high-capacity hard disk. Users can navigate the namespace without needing to know the server names or shared folders hosting the data. Some of the benefits that DFS also provides include: • •\n\n•\n\n•\n\nSimplified data migration: Users do not need to readjust to the fact that data has changed its location. Increased availability of file server data: In the event of a server failure, DFS refers client computers to the next available server. Load sharing: DFS provides a degree of load sharing by mapping a given logical name to shared folders on multiple file servers. Security: The existing NTFS file system and shared folder permissions are used on each target in DFS.\n\nCONCLUSION The VISION Cloud project intends to provide scalable, metadata-rich object services with a new data model that is not compatible with file system POSIX API. The approach that will be\n\nKey Distributed Components for a Large-Scale Object Storage\n\ntaken in this project is to leverage state of the art distributed file systems rather than expand their capabilities. In particular: 1. Distributed file systems may be used within a data center, while other advanced data distribution mechanisms will be deployed across sites. The challenge then becomes how to synchronize between the data that is replicated on two or more different file systems, taking into account latencies and possible separation in the network topology. This is a major shift from an architecture that is based on a single file system that handles the entire pool of files in a centralized manner (using locking, centralized meta data server, etc). 2. Diverging from the POSIX semantics requires a definition of a new type of semantics. 3. A cloud storage infrastructure capable of providing services, with accounting and billing for users, required richer meta data and monitoring at the system level than currently supplied by file systems. This includes information on capacity used by entities (rather than directories), access statistics to files, bandwidth and more. The challenge is then to (1) extend the reporting and accounting within a given distributed file system (2) aggregate this information across many file systems. IBM’s GPFS SNC (shared nothing cluster) produce was selected for use in VISION Cloud. It fulfills the general requirements of VISION Cloud, namely Scalability, scale out performance, elasticity, and availability. The selection of this technology was heavily influenced by GPFS’s strong commercial track record, as well as IBM HRL’s ability to provide a high level of support if required, and license-free installations for the VISION Cloud test bed. Other potential solutions were either proprietary (e.g., OneFS, GFS, MS-DFS) or open source solutions that do not carry the maturity and insurance of stability as a product such as GPFS (XtreemFS, PVFS, HDFS, GlusterFS, Lustre).\n\nThe unique accessibility to this product provided by IBM HRL made it the leading candidate for the VISION Cloud.\n\nDISTRIBUTED DATA MANAGEMENT SYSTEMS Overview Modern distributed database systems have become a central tool for massive data storage (in the order of petabytes) that store large amount of metadata information. Distributed database systems use an architecture that distributes storage and processing across multiple servers to address performance, scalability requirements, caching, flexible graph handling and easy query manipulation. All these requirements cannot be met by traditional relational databases mainly because requests inside the same transaction have to be run by a single node of the database. With the rise of scale and Internet applications, more and more companies made the choice to skip relational databases in favor of simpler and more scalable “NoSQL” databases (DeWitt & Gray, 1990; Distributed Databases Survey, n.d.; NoSQL Databases, n.d.). These types of databases allow simple requests like single row transaction, but do not allow complex SQL request such as join operations. Notable production implementations include Google’s BigTable (Chang et al., 2006), Amazon’s Dynamo (DeCandia et al., 2007a), and Cassandra (The Apache Cassandra Project, n.d.; Lakshman & Malik, 2009).\n\nApproaches, Implementations, and Comparisons In the following, we try to give a short summary of the main characteristics of NoSQL databases and highlight some implementations most relevant to our requirements.\n\n15\n\nKey Distributed Components for a Large-Scale Object Storage\n\nNoSQL Databases Characteristics\n\ntabases emphasize performance and availability. According to the well-known CAP (Consistency, Availability, Partition tolerance) theorem (Gilbert & Lynch, 2002) one cannot have all three properties together and due to the emphasis on performance and availability in cloud applications the consistency is typically relaxed.\n\nThe excellent survey by Randy Guck (Distributed Databases Survey, n.d.; NoSQL Databases, n.d.) describe NoSQL databases as ones that follow most or all of the following characteristics: •\n\n•\n\n•\n\n•\n\n•\n\n16\n\nNo pre-defined Schema for “Tables”: Traditional databases typically require a predefined scheme that each entry (row) in the table should follow. In modern NoSQL databases such a requirement is not mandatory and entries can vary from row to row. Partitioning (Ganesan, Bawa, & GarciaMolina, 2004; Ford et al., 2010): Storage rows are partitioned into small storage entities (usually managed by a single server), and replicated. Shared nothing architecture: According to the above mentioned partition, each node handles the entries at its responsibility locally, rather than using some shared storage with a global view. Elasticity (Stonebraker et al., 2007): Storage and server capacity can be increased by adding more servers without downtime. Asynchronous replication and BASE instead of ACID: Reliability is handled by replication of elements across different nodes in the system. In order to avoid latency in write operations, this replication is done in asynchronous fashion (Belaramani et al., 2006; Belaramani, Dahlin, Nayate, & Zheng, 2008), allowing the write to complete after the first replica is in place, and assuring eventual consistency (Das, Agrawal, & El Abbadi, 2010; Vogels, 2009; Burrows, 2006) rather than atomicity. This is a choice to adopt the BASE (Basically Available, Soft state, Eventual consistency) principle over ACID (Atomicity, Consistency, Isolation, Durability) (Pritchett, 2008): NoSQL da-\n\nIn VISION Cloud, the distributed database is designated for an efficient management of metadata of objects in the cloud. Scale is perhaps the most dominant factor in this cloud, and thus we focus our attention to solutions that seems appropriate. The most relevant categories of distributed databases types for the VISION project are the key/value stores and “Big Table” databases. Other categories can be examined in (Distributed Databases Survey, n.d.; NoSQL Databases, n.d.). We highlight some of the properties of these categories and some existing solutions for them (note that some solutions fall into both categories).\n\nNoSQL Database Implementations The simplest implementations of DBs are key/ value stores by which each entry can be accessed solely by its key. More sophisticated NoSQL DBs have additional mechanisms for indexing data entries. This is typically carried out by handling multiple tables for the same data and enabling synchronization between them. The exact data model can vary from one example to the other and typically relies on the target application of the product. In the following section, we list some prominent implementations of such DB: •\n\nGoogle’s BigTable (Chang et al., 2006): This DB has been the initial model for many of the other implementations to come. Initially designed for the use of the search engine operations, it is now also a data storage solution known as datastore in Google’s Cloud Computing framework\n\nKey Distributed Components for a Large-Scale Object Storage\n\n•\n\n•\n\n•\n\n•\n\n•\n\n- the Google App Engine (GAE). It is a distributed schema-less DB that supports multiple updates within a single transaction and has an indexing system that serves the mechanism to returns entities in different desired orders (typically predefined). GAE datastores can be accessed with Python or Java. Cassandra (Apache) (The Apache Cassandra Project, n.d.): An open source project that originated from Facebook. This project is surveyed in greater depth in the following section as it was chosen to be a central clustered DB for the VISION Cloud. HBase (Apache HBase Reference Guide, n.d.): An Apache open source project that gives an analogue of Google’s Bigtable built upon Hadoop’s HDFS. Hypertable (Hypertable inc., n.d.): Another open source, “Web scale” database modeled after Google’s BigTable. It is available under the GNU GPLv2. Hypertable can be deployed on top of the Hadoop HDFS or CloudStore KFS file systems. It supports an SQL-like language for creating tables called HQL. Azure Tables (Windows Azure – Data Management, n.d.): Microsoft’s Azure cloud computing platform, first came into production use in 2010. The underlying Azure fabric provides distributed computing services such as communication, service management, replication, and failo"
    }
}
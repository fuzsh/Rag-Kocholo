{
    "id": "dbpedia_8586_3",
    "rank": 59,
    "data": {
        "url": "https://subject1356.rssing.com/chan-61438685/latest.php",
        "read_more_link": "",
        "language": "en",
        "title": "What The Mac?!",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://pixel.quantserve.com/pixel/p-KygWsHah2_7Qa.gif",
            "https://sofa.macadmins.io/images/custom_logo.png",
            "https://grahampugh.github.io//assets/images/macOS-not-up-to-date-smart-group.png",
            "https://grahampugh.github.io//assets/images/software-update-beta-scope.png",
            "https://grahampugh.github.io//assets/images/xprotect-not-up-to-date-smart-group.png",
            "https://sofa.macadmins.io/images/custom_logo.png",
            "https://grahampugh.github.io//assets/images/macos-has-unpatched-cves.png",
            "https://grahampugh.github.io//assets/images/macos-has-unpatched-0day-cves.png",
            "https://grahampugh.github.io//assets/images/software-update-beta-cves.png",
            "https://sofa.macadmins.io/images/custom_logo.png",
            "https://media.jamf.com/images/news/2024/jamf-how-to-blog-image.webp?q=80&w=1600",
            "https://assets.rappler.com/BFC459FEAD4148FFB4D3C69721F3CB7A/img/EB2A893552A14C398204CAE6B09F555D/roxas-launch-aquino-endorsement-20150731-008.jpg",
            "https://3.bp.blogspot.com/-FZ-dCCO9uAY/VNOdG8a3t-I/AAAAAAAADiA/B1ZuoVimhUc/s1600/mass-of-solute-mass-of-solvent.jpg",
            "https://1.bp.blogspot.com/-3DmnYeyqGg4/Va-yFsbEqhI/AAAAAAAAK8g/9djm1Frva7E/s640/the%2Bpool%2B1001%2Bhotel%2Bjakarta.jpg",
            "https://mycommunitysource.com/wp-content/uploads/2012/12/robert-stern-125x125.png",
            "https://i.imgur.com/V8Qf3KI.png",
            "https://www.inettutor.com/wp-content/uploads/2019/02/Online-Grading-System-with-Grade-Viewing-Conceptual-Framework.png",
            "https://ic.pics.livejournal.com/dark_flamenko/9208692/1673506/1673506_original.jpg",
            "https://busyteacher.org/uploads/posts/2015-10/thumbs/1445892197_ing-vs-to-infinitive.png",
            "https://a2.mzstatic.com/us/r30/Purple1/v4/08/25/56/0825565e-8100-b566-ee09-aa660e56f559/screen1136x1136.jpeg",
            "https://photos-a.propertyimages.ie/media/5/9/5/3035595/c8986a32-f6d4-4bf9-aeb1-4cfc4103f0b5_m.jpg",
            "https://augustacrime.com/wp-content/uploads/2024/07/hailey-hecker-18-of-warrenville-narcotics-possession-drug-offense-giving-false-info-identity-fraud-to-obtain-employment-or-avoid-detection-driving-under-suspension-150x150.jpg",
            "https://images.qvc.com/is/image/pic/co/Danjob.jpg",
            "https://1.bp.blogspot.com/-YOPGWJWdd3c/WToRtjhqKNI/AAAAAAABFDI/6P7-_HaxmtQlo643FAb5TLZCJx7dqX_dwCLcB/s1600/18893232_10155410415748501_7328552017510958888_n.jpg",
            "https://www.ksstradio.com/wp-content/uploads/2019/07/Trondamion-Andrzhel-Cleveland.jpg",
            "https://4.bp.blogspot.com/-qsPB2CTvMwo/WFaNEb3cNRI/AAAAAAABbbc/v2BMl79iVwgZk1MOMlZQTcKwEFt1hrM2gCLcB/s1600/%2524_57%2B%25282a%2529.JPG",
            "https://i.imgur.com/QfNCYCP.png",
            "https://assets.suredone.com/1517/media-pics/cp049425-rear-license-plate-holder-vw-golf-mk3-north-american-tub-tray-1hm-853-481-d.jpg",
            "https://4.bp.blogspot.com/-BaHEXZarDas/WJdhe2T6aSI/AAAAAAAANlE/vmZsTuSVv7QlMiEfaDPwU7Lkx4MJhoyQACLcB/s640/meenakshi%2Bjoshi.jpg",
            "https://www.mindef.gov.bn//Mindef%20topmenu%20pictures/Leadership-His%20Majesty/DEC%20%20photos/Pengarah%20DDWS%20Hjh%20Marliyana.jpg",
            "https://www.homesnacks.com/images/tn/brownsville-tn-0.jpg",
            "https://www.thesun.co.uk/wp-content/uploads/2024/08/martin-odegaard-arsenal-premier-league-925852987.jpg?strip=all&w=703",
            "https://www.thesun.co.uk/wp-content/uploads/2024/08/2024-martin-odegaard-arsenal-celebrates-926032710.jpg?strip=all&w=960",
            "https://www.rappler.com/tachyon/2024/08/dindo-track-august-18-2024-11pm.png?fit=1024%2C1024",
            "https://www.thesun.co.uk/wp-content/uploads/2024/08/warning-taken-without-permission-i-925787029.jpg?strip=all&w=904",
            "https://www.thesun.co.uk/wp-content/uploads/2024/08/guys-i-m-proud-finally-891685655.jpg?strip=all&w=960",
            "https://i.etsystatic.com/19970584/r/il/93109a/6032426221/il_570xN.6032426221_3jas.jpg",
            "https://i.etsystatic.com/5225632/r/il/eb60d0/4874868937/il_570xN.4874868937_5j1w.jpg",
            "https://www.thesun.co.uk/wp-content/uploads/2024/08/449ba300-734f-4533-a6ea-b651e23d0da6.jpg?strip=all&w=960",
            "https://imgix.bustle.com/uploads/image/2022/11/15/6f0be778-1d72-43ad-b16e-4458c541dd3b-screen-shot-2022-11-15-at-13351-pm.png?w=500&fit=max",
            "https://www.thesun.co.uk/wp-content/uploads/2024/08/2024-celebrated-best-soap-operas-916849093.jpg?strip=all&w=639"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "//www.rssing.com/favicon.ico",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "For many years while working at ETH Zürich, I developed a set of shell scripts for performing API actions on multiple Jamf instances. These remained for internal use only due to the specific nature of the ETH setup. However, as I am between jobs at the moment while waiting for a German work permit (received today! Woohoo!), I have taken some time to work on these scripts so that they could be used by other admins faced with dealing with multiple Jamf Pro instances, whilst still remaining functional for the specific use case at ETH. These scripts are now available for the Mac Admins community at the GitHub repo multitenant-jamf-tools. I hope they may be of use to somebody!\n\njocads.sh\n\nJOCADS - jocads.sh - is the “Jamf Object Copier and Deleter” script. It’s role is to copy existing items from one Jamf instance to any number of others, or to delete matching items across multiple instances. It can be used interactively in a command line window, or automated using script arguments.\n\nIt also handles dependencies, so, for example, if a policy depends on any categories, smart computer groups, static computer groups, scripts, extension attributes or packages, these objects are all copied first in order, so that the policy will successfully copy.\n\nSmart groups are often indirectly dependent on other smart groups, static groups, and extension attributes, so smart group criteria are checked to a depth of three dependencies, and copied.\n\nMultiple objects of a single type can be copied or deleted at once, chosen from a list. For example, you could filter on all policies with “Firefox” in the title, select any number of them, and copy or delete those objects to any number of other instances.\n\nJOCADS can be used to copy the following API objects from one instance to another:\n\nAdvanced Computer Search\n\nComputer Configuration Profile\n\nMobile Device Configuration Profile\n\nExtension Attribute\n\nComputer Group (smart or static)\n\nDock Item\n\nPolicy\n\nApp Store App (Mac or iOS)\n\nPackage Object\n\nSoftware Restriction\n\nScript\n\nCategory\n\nJamf Pro User (local or LDAP)\n\nJamf Pro Group (local or LDAP)\n\nNote that a package itself is not copied to another instance if each instance has its own distribution point - as this script was developed for use with multiple instances all sharing the same fileshare distribution point, no functionality has been added so far for copying packages themselves. Reach out if you might have a use for that.\n\nJOCADS can also be used to delete most of these API objects. In certain circumstances it can also delete the actual package in addition to the package objects.\n\nConfiguration Profiles are safely copied by handling UUIDs within the profiles, maintaining the existing UUID within a single instance so that the profile is properly redeployed. Note, however, that signed profiles cannot be copied using the API without stripping the signature, so certain types of profile are not suitable for distribution via API using any tool.\n\nA requirement at ETH Zürich was that certain computer groups should not be overwritten if they already exist on a destination Jamf instance, but that this should not provent the script from proceeding to copy objects that have such groups as dependencies. This is achieved by providing a text file containing a list of strings which are pattern-matched during the copy process, and any group matching any of the patterns is only copied if it doesn’t already exist on the destination. However, there is a way to force-copy these items for remediation of any broken objects.\n\nSetting up\n\nInstructions for installing and setting up the environment for using JOCADS and the other scripts are available in the README document.\n\nIn short, lists of instances are provided as text files. It’s possible to have multiple instance lists, for example production and test instance lists. The first item in the list is defined as the default “source” instance, but you can override this to copy from any one instance to multiple others. You can also copy from an instance in one list to instances in another list.\n\nCredentials are set up using the script set-credentials.sh. This asks for credentials and stores them in the login keychain for future use. In case multiple instances use the same credentials, this process can be made faster by supplying the credentials for a list of instances at once.\n\nAt present, an API user account must be used for each instance. At some point I hope to add support for API Clients.\n\nOther scripts\n\nThere are a bunch of other scripts that are mostly used during the initial set up of new Jamf Pro instances. Tthese work with input variables and/or a supplied XML template rather than copying from a source instance. Scripts currently available can perform the following actions on one or multiple instances:\n\nCreate or update an LDAP user\n\nDisable Engage\n\nDefine Inventory Collection settings\n\nSet up a Fileshare Distribution Point\n\nSet up or update an LDAP server\n\nSet up or update the SMTP server\n\nUpdate the Activation Code\n\nUpdate an LDAP group\n\nThe script that sets up a Fileshare Distribution Point also adds the SMB share credentials into the keychain for use by the JOCADS script for deleting packages.\n\nFurther scripts are available to collect information from one or multiple Jamf Pro instances. These currently include:\n\nNumber of computers not checked in (requires a smart group to have been copied to each instance)\n\nNumber of installations of an app\n\nNumber of managed and unmanaged computers and devices\n\nConclusion\n\nThese scripts have been lightly tested since their conversion to these open source versions, as I do not currently have access to a large set of Jamf instances. Hopefully they will work in your environment, but please reach out via the GitHub repo or DM me in MacAdmins Slack with any bugs, or for any clarifications on how to set it all up! I’d also be interested to hear from anyone who uses them successfully, and if you have any ideas for potential future improvements.\n\nYour organisation may want to ensure that XProtect is up to date on Mac. So long as Software Update settings are set to “Install Security Updates and System Files”, all should be well… assuming that Software Update is functioning today…\n\nBut how can you verify that the XProtect version on the system is the latest version available? For that, you need to check against Apple’s software catalogs or some external source.\n\nSilent Knight\n\nHoward Oakley’s excellent Silent Knight app and silnite command line tool checks the system version against his own GitHub repo, where he maintains a history of updates for XProtect and other tools.\n\nsilnite outputs a JSON file when run, so you’ll need to use a JSON interpreter to read the data. When testing this out, I’ve used the excellent Little JSON Tool (ljt) code snippet from Joel Bruner.\n\n# Latest XProtect VersionxProtectLatestVersion=$(ljt /XProtectE < /path/to/silnite_output_file.json)# System XProtect VersionxProtectInstalledVersion=$(ljt /XProtectV < /path/to/silnite_output_file.json)\n\nThat’s great, but to use this, you have to deploy silnite to all your Mac fleet, which requires maintenance, and create a method of running it periodically.\n\nAlternatively, you can interrogate Apple’s software catalog directly using curl, and compare this with the installed version.\n\nCURRENT_CATALOG=\"https://swscan.apple.com/content/catalogs/others/index-14-13-12-10.16-10.15-10.14-10.13-10.12-10.11-10.10-10.9-mountainlion-lion-snowleopard-leopard.merged-1.sucatalog\"# create cache directory /bin/mkdir -p\"/var/sucatalog\"# download catalog curl -s\"$CURRENT_CATALOG\">\"/var/sucatalog/current-catalog.sucatalog\"# get the latest XProtect config data URLxProtectURL=$(grep-m 1 -o'https.*XProtectPlistConfigData.*pkm'< \"/var/sucatalog/current-catalog.sucatalog\")# download the config data file and extract the version string xProtectLatestVersion=$(curl -s\"$xProtectURL\" | grep-o'CFBundleShortVersionString[^ ]*' | cut-d'\"'-f 2)# obtain the installed XProtect versionxProtectInstalledVersion=$(defaults read /Library/Apple/System/Library/CoreServices/XProtect.bundle/Contents/Info.plist CFBundleShortVersionString)# compare the two to find if the latest version is installedif[[\"$xProtectLatestVersion\"==\"$xProtectInstalledVersion\"]];then echo\"Latest version installed\"else echo\"Latest version NOT installed\"fi\n\nBest practice for Jamf Pro\n\nNote that with either of these methods, polling the internet is required. If you were intending to use an Extension Attribute, it’s generally better to avoid polling the internet directly from an EA, as there could be delays or timeouts. So, it’s best to create a LaunchDaemon to run a script which does the internet polling at intervals, and let the EA read the output from files downloaded by that script.\n\nIf you want to go that route, you’ll need a policy to deploy the script and LaunchDaemon, and the EA can then read the downloaded files to do the comparison… so, only marginally less work than deploying silnite, but arguably better as it polls Apple’s software update catalogs directly rather than an independent list that may or may not be current.\n\nI’ve prepared versions of both these scripts, go check them out if you’re interested: check-xprotect-version\n\nConclusion\n\nIn all probability, it’s OK to rely on softwareupdate to keep XProtect and other security components up to date. But if you really want to verify it, choose one of the methods above.\n\nSimilar methods could be utilised for the other security components such as GateKeeper and MRT (though MRT appears to be obsolete since macOS 12) - silnite reports all these values, and they are all present in the .sucatalog. Here’s some hints:\n\n# MRTMRT_URL=$(grep-m 1 -o'https.*MRTConfigData.*pkm'< \"/var/sucatalog/current-catalog.sucatalog\")# GatekeeperGatekeeperURL=$(grep-m 1 -o'https.*GatekeeperConfigData.pkm'< \"/var/sucatalog/current-catalog.sucatalog\")\n\nIf you maintain a Jamf Pro server that has a Cloud Distribution Point located on an Amazon Web Services S3 Bucket, you typically either use the Jamf Pro admin user interface to upload packages, or the Jamf Admin app. This post is concerned with how to upload a package using a script, using Jamf’s Classic API and the AWS command line interface tools (aws-cli).\n\nIntroduction to the workflow for uploading a package via the API\n\nTo upload a package to a Jamf Pro Cloud Distribution Point hosted on AWS S3, we require the following steps:\n\nInstall the aws-cli tools and configure them to point to your S3 bucket\n\nObtain a Bearer Token for the Jamf Pro Classic API\n\nCheck for an existing package item in Jamf Pro\n\nIf replacing an existing package, sync the package to the S3 bucket\n\nUpload package metadata to Jamf Pro\n\nNOTE: Jamf requires bundle-style installer packages (which are actually folders) to be zipped prior to upload. Most packages are flat packages which are a single archive file and do not need zipping up. Only a very few vendors still provide bundle-style packages, such as Adobe’s Creative Cloud apps. The Jamf Pro admin console will perform the zip automatically when fed a bundle-style package. JamfPackageUploader for AutoPkg also zips up packages as required. The shell scripts I present here do not, so you will need to zip up the package prior to uploading it.\n\nThe easiest way to interact with an S3 bucket from a shell script is to install the aws-cli tool as an installer package, available from https://aws.amazon.com/cli/.\n\nTo configure the tools to point to your S3 bucket, type aws configure to begin an interactive session. This only has to be done once. Provide your AWS Access Key ID, Secret Access Key, and optionlly provide a default region and an output format.\n\n% aws configure AWS Access Key ID [****************Z344]: AWS Secret Access Key [****************/Gy7]: Default region name [us-east-1]: Default output format [json]:\n\nStep 2 - obtaining a Bearer Token\n\nThis step is common to all Jamf Pro API queries so should be familiar to anyone who has written a script to interact with the Jamf Pro API. The endpoint is /api/v1/auth/token. No special privileges are required to access this endpoint.\n\nThe following example outputs the response to a file. Instead you could output to stdout and wrap the curl command into a variable.\n\nFirst let’s define some variables we’ll need to use:\n\npkg_path=\"/path/to/my.pkg\"pkg=$(basename\"$pkg_path\")pkg_dir=$(dirname\"$pkg_path\")url=\"https://my.jamf.pro.server\"token_file=\"/tmp/token.json\"curl_output_file=\"/tmp/output.txt\"s3_bucket=\"myjamfpros3bucketname\"\n\nNow let’s get the token. Note that here we are using the traditional method of obtaining a Bearer Token using Basic Authentication. If you prefer to use an API Client Secret to obtain the token, please adjust this step accordingly. All subsequent steps will be the same.\n\n# encode the credentials into base64credentials=$(printf\"%s\"\"$user:$pass\" | iconv -t ISO-8859-1 | base64-i -)# post the request to the token-issuing endpoint curl --request POST \\--header\"authorization: Basic $credentials\"\\--url\"$url/api/v1/auth/token\"\\--header'Accept: application/json'\\--output\"$token_file\"# extract the token from the JSON responsetoken=$(plutil -extract token raw \"$token_file\")\n\nStep 3 - check if there is an existing package in Jamf Pro\n\nFirst of all we want to see if there’s an existing package object in Jamf. We use the Classic API for this. Don’t forget that if the package name has any funky characters in it - or spaces - these will need to be escaped for any of the URLs that contain the name.\n\ncurl --request GET \\--header\"authorization: Bearer $token\"\\--header'Accept: application/json'\\\"$url/JSSResource/packages/name/$pkg\"\\--output\"$curl_output_file\"\n\nIf we get a response, we can extract the package ID for later.\n\npkg_id=$(plutil -extract package.id raw -expect integer \"$curl_output_file\")\n\nStep 4 - sync the package to the S3 bucket\n\nThanks to the sync feature of the aws-cli tools, we don’t need to check whether the package already exists on the S3 bucket, we can just send a sync command and if there is no existing package of this name, or the local package is different from the one on the server, then it will be replaced, otherwise it will be left alone.\n\naws s3 sync\"$pkg_dir/\"\"s3://$s3_bucket/\"--exclude\"*\"--include\"$pkg\"\n\nNote: I have found that in a very limited set of circumstances, using this method (aws s3 sync) to upload a single file will fail. This appears to happen due to specific other content in the folder that the package is placed. If you find that it fails, move the package into its own folder or an alternative, less “noisy”, location. Or feel free to use aws s3 cp \"$pkg\"\"s3://$s3_bucket/\" instead. I just prefer the sync command as it builds in a check as to whether the package needs to be replaced or not.\n\nStep 5 - upload the package metadata to Jamf Pro\n\nNow we need to tell Jamf Pro about the package, so we upload the package metadata to the Classic API packages endpoint. There are a bunch of variables you can add to the package metadata such as category, info, etc. Here we give the bare minimum: name and filename.\n\nNote that we’re using the pkg_id key from earlier to determine whether we are replacing an existing package’s metadata or posting a new one.\n\npkg_data=\"<package> <name>$pkg</name> <filename>$pkg</filename> </package>\"echo\"Posting the package metadata to the server\"if[[$pkg_id-gt 0 ]];then req=\"PUT\"else req=\"POST\"fi curl --request\"$req\"\\--header\"authorization: Bearer $token\"\\--header'Content-Type: application/xml'\\--data\"$pkg_data\"\\\"$url/JSSResource/packages/id/$pkg_id\"\\\n\nAnd that’s it! There could be a short period where the package appears as “Availability Pending” in the GUI, but in my tests, the package can still be installed.\n\nComplete script\n\nI have prepared a complete script for uploading a package in shell.\n\nShell:Click here to see the shell script.\n\nWhat about JamfPackageUploader?\n\nIf you are familiar with using the AutoPkg processor JamfPackageUploader, a new aws_cdp_mode has now been added, which uses the same method as described above.\n\nNote that aws_cdp_mode requires the use of the aws-cli tools described above. To ensure that you can use aws_cdp_mode, please follow the installation and configuration instructions described above. You will also need to provide the S3 bucket name using the S3_BUCKET_NAME key.\n\nConclusion\n\nGood luck with your testing and let me know if you find any problems. I have not attempted to find a solution for the other supported type of third party cloud distribution points, namely Rackspace and Akamai, but if those services offer a command line interaction tool, it should be easy to substitute the aws s3 commands for something appropriate for that service. The rest of the workflow would be the same.\n\nWhat is SOFA?\n\nSOFA, short for “Simple Organized Feed for Apple Software Updates”, is a new open source resource for Mac Admins, developed primarily by Henry Stamerjohann of Zentral. It consists of a machine-readable feed and user-friendly web interface, providing always-up-to-date information on XProtect data, OS updates, and the CVEs (critical vulnerabilities) that have been reported as patched by Apple in each release. CVEs that are reported as actively exploited are listed distinctly.\n\nSOFA is hosted by the Mac Admins Open Source organisation, but it can be hosted in an organisation’s own infrastructure, allowing a high level of control over the information.\n\nThere are many ways this feed will be useful to Mac Admins, so you’re going to read many blog posts about it in the future. This post describes how to integrate the macOS and XProtect data exposed in SOFA with Jamf Pro, using extension attribute scripts.\n\nmacOS Version Check\n\nMany existing tools use data from the client to verify if macOS is up to date on a Mac. Typically, obtaining the latest available version either involves interrogating /Library/Preferences/com.apple.SoftwareUpdate.plist on the local system for the results of the system’s last software update check, or running softwareupdate --list to perform a new check. The current system version is then compared with the latest available version reported by either of those methods. I described these methods in the post Do you need to use the softwareupdate command to discover available updates?. However, if the software update checks on the system are not functional, we won’t get an accurate comparison.\n\nThe SOFA feed information is polled directly from Apple’s servers, so provides a quick way of performing the comparison without relying on software update checks on the system to be working - so long as the system can reach the SOFA web page.\n\nThe SOFA feed includes information of which macOS models are compatible with each major version. We can use this information when determining whether a system is running the latest compatible OS.\n\nThe Extension Attribute script provided in the SOFA GitHub is named macOSVersionCheck-EA.sh (click here). This checks the local system version against the latest compatible version in the SOFA JSON feed, and sends either Pass or Fail to the Jamf Pro server. This can then be used to scope non-compliant computers into a Smart Group, which can be used to push MDM/DDM software update commands or any other compliance-related action.\n\nIn this example, we have named the Extension Attribute macOS Version Check. The Smart Group is named macOS not up to date. The single criterion is that macOS Version Check is Fail.\n\nHere, we use this smart group in Software Update to initiate MDM update commands.\n\nXProtect Version Check\n\nI recently published a post Is XProtect up to date?, which described how to check Apple’s feed directly to see if the local system is up to date. This functions well, though it does require a certain amount of grep and cut hackery to get the correct version.\n\nSOFA provides this information in a more readable JSON format, so we have an opportunity to use the SOFA feed to test whether XProtect and XProtect Remediator are up to date.\n\nThe Extension Attribute script provided in the SOFA GitHub is named XProtectVersionCheck-EA.sh (click here). This checks the local system version against the latest compatible version in the SOFA JSON feed, and sends either Pass or Fail to the Jamf Pro server. This can then be used to scope non-compliant computers into a Smart Group, which, as with the macOS check, can be used to push MDM/DDM software update commands or any other compliance-related action.\n\nIn this example, we have named the Extension Attribute XProtect Version Check. The Smart Group is named XProtect not up to date. The single criterion is that XProtect Version Check is Fail.\n\nConclusion\n\nSOFA is sure to be extremely useful to Mac Admins, and its feed will no doubt be used in many projects in the future. The extension attributes described here are just two examples of how to use the feed to trigger processes in Jamf Pro and any other management tool. I’m looking forward to seeing what else comes out of Henry’s innovation.\n\nJamf announced in the 11.4.0 Release Notes that the undocumented and unsupported dbfileupload endpoint will be discontinued in Jamf Pro 11.6.0, along with support for the Jamf Admin application:\n\nJamf Admin—Support for Jamf Admin will be discontinued with the release of Jamf Pro 11.6.0 (estimated release date: June 2024). The Jamf Admin app has been removed from the Jamf Pro Apps DMG as of Jamf Pro 11.4.0. Older versions of Jamf Admin will continue to work with Jamf Pro until all functionality and endpoints associated with Jamf Admin are removed in Jamf Pro 11.6.0. In addition, Jamf Admin content has been removed from documentation.\n\nAlthough it was never officially supported, the dbfileupload API endpoint was used in many scripts and tools coming from the community to upload packages to any cloud-based Distribution server, whether it was Jamf’s Cloud Distribution Service (JCDS), or a cloud distribution point hosted on Amazon S3, Rackspace or Akamai.\n\nAren’t there existing alternatives?\n\nAlternative API endpoints for uploading to JCDS was introduced with Jamf Pro 10.49.0, although it required the JCDS to be upgraded to JCDS 2.0, something that has been ongoing for Jamf Premium Cloud customers - see my blog post “Introducing JCDS 2.0”. It is more complex to use than the dbfileupload endpoint as it requires the installation of a tool to communicate directly with an Amazon AWS S3 bucket, such as the aws-cli binary or the python boto3 module.\n\nSimilarly, it’s always been possible to upload directly to a third-party cloud DP hosted on AWS, Rackspace or Akamai using those vendors’ tools, and then update the package metadata in Jamf Pro using the Classic API, although not many community tools and scripts have supported this. For example, I recently described a way of uploading to AWS in my blog post “Scripting the upload of packages to Jamf Pro with a Cloud Distribution Point located on an Amazon Web Services S3 Bucket”.\n\nIntroduction to the official “packages” API endpoint\n\nIntroduced with Jamf Pro 11.5.0, the new packages API endpoint can be used to upload packages to any cloud DP, agnostic to whether the service is JCDS, AWS, Akamai or Rackspace.\n\nThe new endpoint is part of the Jamf Pro API, and is a replacement for the JSSResource/packages endpoint of the Classic API. The Classic API endpoint remains functional, as is the v1/jcds endpoint introduced in 10.49.0, so scripts and tools using these endpoints do not need to be updated right away. However, since the new endpoint is simpler, has no dependencies, and works across all cloud DPs, anyone who has not yet migrated their scripts from the deprecated dbfileupload endpoint will want to use this.\n\nUploading a package with the v1/packages endpoint\n\nTo upload a package to a Cloud Distribution Point , we require the following steps:\n\nObtain a bearer or OAuth token for the Jamf Pro API\n\nCheck for an existing package item in Jamf Pro\n\nIf replacing an existing package, upload package metadata to Jamf Pro\n\nUpload the new package\n\nNOTE: Jamf requires bundle-style installer packages (which are actually folders) to be zipped prior to upload. Most packages are flat packages which are a single archive file and do not need zipping up. Only a very few vendors still provide bundle-style packages, such as Adobe’s Creative Cloud apps. The Jamf Pro admin console will perform the zip automatically when fed a bundle-style package. JamfPackageUploader for AutoPkg also zips up packages as required. The shell scripts I present here do not, so you will need to zip up the package prior to uploading it.\n\nStep 1 - obtaining a Bearer Token\n\nThis step is common to all Jamf Pro API queries so should be familiar to anyone who has written a script to interact with the Jamf Pro API. The endpoint is /api/v1/auth/token. No special privileges are required to access this endpoint.\n\nThe following example outputs the response to a file. Instead you could output to stdout and wrap the curl command into a variable.\n\nFirst let’s define some variables we’ll need to use. Note that I like to output the token to a file so that it can be used by multiple runs of the same (or other) scripts:\n\npkg_path=\"/path/to/my.pkg\"pkg=$(basename\"$pkg_path\")pkg_dir=$(dirname\"$pkg_path\")url=\"https://my.jamf.pro.server\"token_file=\"/tmp/token.json\"curl_output_file=\"/tmp/output.txt\"\n\nNow let’s get the token. If using the traditional method of obtaining a Bearer Token using Basic Authentication, use the method below, supplying the $user and $password values directly (curl handles encoding of these values into base64, so there is no need to do that in a separate step).\n\n# post the request to the token-issuing endpoint curl --request POST \\--url\"$url/api/v1/auth/token\"\\--user\"$user:$password\"\\--header'Accept: application/json'\\--output\"$token_file\"# extract the token from the JSON responsetoken=$(plutil -extract token raw \"$token_file\")\n\nIf you prefer to use an API Client Secret to obtain the token, use the following request. You will need to populate the $client_id and $client_secret values as obtained from the Jamf Pro GUI:\n\n# post the request to the token-issuing endpoint curl --request POST \\--url\"$url/api/oauth/token\"\\--data-urlencode\"client_id=$client_id\"\\--data-urlencode\"grant_type=client_credentials\"\\--data-urlencode\"client_secret=$client_secret\"\\--header'Accept: application/json'\\--output\"$token_file\"# extract the token from the JSON responsetoken=$(plutil -extract access_token raw \"$token_file\")\n\nStep 2 - check if there is an existing package in Jamf Pro\n\nFirst of all we want to see if there’s an existing package object in Jamf. We can use the new endpoint for this. Note that we have to encode the package name for injection into the request URL.\n\npkg_name_encoded=\"$(echo\"$pkg\" | sed-e's| |%20|g' | sed-e's|&amp;|%26|g')\" curl --request GET \\--header\"authorization: Bearer $token\"\\--header'Accept: application/json'\\\"$url/api/v1/packages/?filter=packageName%3D%3D%22$pkg_name_encoded%22\"\\--location\\--output\"$curl_output_file\"\n\nIf we get a response, we can determine whether there is a matching package by getting the totalCount value:\n\npkg_count=$(plutil -extract totalCount raw -expect integer \"$curl_output_file\")\n\nAnd if the $pkg_count value is greater than 0, we can obtain the ID of the package like this:\n\npkg_id=$(plutil -extract results.0.id raw -expect string \"$curl_output_file\")\n\nIf $pkg_count is 0, however, then we are dealing with a new package.\n\nStep 3 - upload the package metadata to Jamf Pro\n\nWith the new packages endpoint, we need to tell Jamf Pro about the package before we upload it, so we upload the package metadata first. There are a bunch of variables you can add to the package metadata such as category, info, etc. Here we give the bare minimum required for the response to be accepted - if any of the following keys are omitted, the request will fail.\n\nNote that we’re using the pkg_id key from earlier to determine whether we are replacing an existing package’s metadata or posting a new one. If there was no existing package, then we post to the packages endpoint without an ID.\n\n# put the required variables into a single string using a heredocread-d''-r data_json <<JSON { \"packageName\": \"$pkg\", \"fileName\": \"$pkg\", \"categoryId\": \"-1\", \"priority\": 3, \"fillUserTemplate\": false, \"uninstall\": false, \"rebootRequired\": false, \"osInstall\": false, \"suppressUpdates\": false, \"suppressFromDock\": false, \"suppressEula\": false, \"suppressRegistration\": false } JSON echo\"Posting the package metadata to the server\"if[[$pkg_id-gt 0 ]];then req=\"PUT\"jss_url=\"$url/api/v1/packages/$pkg_id\"else req=\"POST\"jss_url=\"$url/api/v1/packages\"fi curl --request\"$req\"\\--header\"authorization: Bearer $token\"\\--header'Content-Type: application/json'\\--header'Accept: application/json'\\--data\"$data_json\"\\\"$jss_url\"\\--location\\--output\"$output_file_record\"\n\nWe get the ID of the package object as follows:\n\npkg_id=$(plutil -extractid raw -expect string \"$output_file_record\")\n\nStep 4 - upload the package\n\nAssuming that we have determined that we need to upload the package, this is done as the final step, since we always need to supply the ID of the package metadata object. This is always done as a POST operation. Any existing package will be replaced.\n\ncurl --request\"POST\"\\--location\\--header\"authorization: Bearer $token\"\\--header'Content-Type: multipart/form-data'\\--header'Accept: application/json'\\--form\"file=@$pkg_path\"\\\"$url/api/v1/packages/$pkg_id/upload\"\n\nAnd that’s it! There could be a short period where the package appears as “Availability Pending” in the GUI, but in my tests, the package can still be installed.\n\nI won’t cover it here, but if you need to add a manifest to the package, you can do that with a POST request to the v1/packages/$pkg_id/manifest endpoint, with the file posted as multipart/form-data as above.\n\nComplete script\n\nI have prepared a complete shell script for uploading a package.\n\nClick here to see the shell script.\n\nWhat about JamfPackageUploader?\n\nFor those of you familiar with using the AutoPkg processor JamfPackageUploader, a new pkg_api_mode argument has now been added, which uses the same method as described above. This is set as the default mode for Jamf Pro servers running 11.5.0 or above, or can be explicitly set. Anyone running 11.5.0 or above and using the existing jcds2_mode or aws_cli_mode can remove the key from their preferences to enabled pkg_api_mode.\n\nDeleting Packages\n\nDeleting packages is now straightforward, as deleting the metadata will remove the package itself. You first need to determine the ID of the package, as shown in Step 2.\n\ncurl --request DELETE \\--location\\--header\"authorization: Bearer $token\"\\--header'Accept: application/json'\\\"$url/api/v1/packages/$pkg_id\"\n\nIt is now also possible to delete multiple packages in one operation. To do this, supply the package IDs in a JSON file. You should first check that each ID is a valid package:\n\nread-d''-r data_json <<JSON { \"ids\": [ \"1\", \"2\", \"4\", \"8\" ] } JSON curl --request POST \\--location\\--header\"authorization: Bearer $token\"\\--header'Accept: application/json'\\--data\"$data_json\"\\\"$url/api/v1/packages/delete-multiple\"\n\nExporting a list of packages information\n\nYou can get a list of all the packages in CSV format using the new export endpoint. The following example shows the bare minimum fields of ID and name.\n\nread-d''-r data_json <<JSON { \"page\": 0, \"pageSize\": 100, \"sort\": [ \"id:asc\" ], \"filter\": \"\", \"fields\": [ { \"fieldName\": \"id\", \"fieldLabelOverride\": \"Package ID\" }, { \"fieldName\": \"packageName\", \"fieldLabelOverride\": \"Package Name\" } ] } JSON curl --request POST \\--location\\--header\"authorization: Bearer $token\"\\--header'accept: text/csv'\\--header'Content-Type: application/json'\\--data\"$data_json\"\\\"$url/api/v1/packages/export\"\\--output\"/path/to/file.csv\"\n\nDownloading packages from the Cloud DP\n\nThe new packages endpoint does not expose the URL of the package for download, but for JCDS 2, the v1/jcds endpoint can be used for this. See Rich Trouton’s blog post “Updated scripts for downloading packages from a JCDS2 distribution point” for details on how to do this.\n\nConclusion\n\nGood luck with your testing and let me know if you find any problems. I have not attempted to find a solution for the other supported type of third party cloud distribution points, namely Rackspace and Akamai, but if those services offer a command line interaction tool, it should be easy to substitute the aws s3 commands for something appropriate for that service. The rest of the workflow would be the same.\n\nNote: My colleague William Smith created a similar blog post How to convert Classic API scripts to use bearer token authentication in 2022. This post serves as a timely reminder of what is changing with the Classic API, as well as providing updated information about the possible ways to authenticate against both Jamf Pro APIs using Bearer Tokens or API Clients.\n\nContents\n\nContents\n\nIntroduction\n\nEnd of support of Basic Authentication - what does it mean?\n\nHow to adapt an existing shell script to use a Bearer Token\n\nAdditional tips and tricks\n\nUse the same Bearer Token for multiple scripts\n\nCheck the expiration of an existing token\n\nCreate a new token using an existing token\n\nClient Credentials-based Authentication\n\nObtaining a Bearer Token\n\nConclusion\n\nFurther Reading\n\nIntroduction\n\nFrom its introduction, the original API for Jamf Pro, now known as the Classic API, used Basic Authentication for all endpoints. Basic authentication means supplying the username and password of a user account directly with every request. From a security perspective, sending credentials repeatedly over the internet could be considered a credible risk.\n\nThe Jamf Pro API was first introduced in 2016 with Casper Suite 9.93, although it remained in beta until the release of Jamf Pro 10.26.0. This separate API supports Basic Authentication, but only for one single endpoint: /api/v1/auth/token. This endpoint is used to obtain a “Bearer Token”, which is a unique, time-limited access token that is used to make requests to all other endpoints, and can also be revoked after use. This reduces the exposure of the original credentials, since they need only be used once per task.\n\nFrom Jamf Pro version 10.35.0, released April 2022, Bearer Token authentication was introduced for the Classic API. Since then, it has been possible to disable Basic Authentication for the Classic API (apart from the /api/v1/auth/token endpoint used to obtain the bearer token), by navigating to Settings > Jamf Pro User Accounts & Groups > Password Policy and deselecting the Allow Basic authentication in addition to Bearer Token authentication checkbox.\n\nNew Jamf Pro instances created with version 10.42.0 or newer had Basic Authentication disabled by default, though it could be enabled using the checkbox above.\n\nEnd of support of Basic Authentication - what does it mean?\n\nBasic Authentication in the Classic API is no longer supported and will be turned off for all 11.5.0 instances to provide enhanced security. This does not remove the ability to obtain a Bearer Token using basic authentication, which is then used to authenticate requests to the Classic API.\n\nIf you have composed your own scripts, or are using other people’s scripts, that communicate with the Jamf Pro API, you will already be familiar with how to go about obtaining, using and revoking a Bearer Token. If, however, you have previously only needed to communicate using the Classic API, you may need to make some changes to these scripts to continue to use them on Jamf Pro 11.5.0 and above.\n\nNote that although it is no longer supported as of version 11.5.0, it is currently possible to re-enable Basic Authentication for the Classic API.\n\nHow to adapt an existing shell script to use a Bearer Token\n\nIn this post I will concentrate on the use of shell scripts (bash, zsh), as this is the most common method of communicating with the API. For help with other languages, the Jamf Pro Classic API Reference has examples of how to use the APIs with many different programming languages.\n\nLet’s say we want to get a list of computers on a Jamf Pro server. In the past we would have used a single curl command such as the example below.\n\nNote: you may see short or long parameter names in curl statements. I’ve used the long names here, so here’s a short glossary of equivalents that are relevant to this post:\n\n-X is the same as --request (POST, GET, PUT or DELETE)\n\n-H is the same as --header\n\n-u is the same as --user\n\n-d is the same as --data (but for URL-encoding, --data-urlencode should be specified)\n\n-o is the same as --output (i.e. write the output to a file)\n\n--url is optional; a URL can be supplied with no parameter name\n\nIn all examples, we will assume an admin account with the username jamfsw and the password jamf1234.\n\n# request list of computers using Basic Authentication /usr/bin/curl --request GET \\--url\"https://yourserver.jamfcloud.com/JSSResource/computers\"\\--header'accept: application/json'\\--user\"jamfsw:jamf1234\"\n\nFrom Jamf Pro 11.5.0, we need to change this to first create a Bearer Token, and then use it to get the computer list. Note that we need to extract the token from the JSON output we receive. To do this we use the plutil tool which has gained the ability to extract JSON data since macOS 12 Monterey (for more details on the use of plutil, see Richard Purves’ blog post Plutil JSON parsing for Fun and Profit).\n\n# obtain a Bearer Token using Basic Authentication, write the output into a variablerequest=$( /usr/bin/curl --request POST \\--url\"https://yourserver.jamfcloud.com/api/v1/auth/token\"\\--header'accept: application/json'\\--user\"jamfsw:jamf1234\")# extract the token from the JSONtoken=$(/usr/bin/plutil -extract token raw -o - - <<<\"$request\")# request list of computers using Bearer Token /usr/bin/curl --request GET \\--url\"https://yourserver.jamfcloud.com/JSSResource/computers\"\\--header'accept: application/json'\\--header\"authorization: Bearer $token\"\n\nIf we have multiple curl requests to use in a single script, we can continue to use the token for each request. If/when we want to revoke the token at the end of the script, so that it cannot be used again, this is done using the token itself:\n\n# revoke the Bearer Token /usr/bin/curl --request POST \\--url\"https://yourserver.jamfcloud.com/api/v1/auth/invalidate-token\"\\--header'accept: application/json'\\--header\"authorization: Bearer $token\"\n\nAdditional tips and tricks\n\nUse the same Bearer Token for multiple scripts\n\nIf you have a workflow that runs several scripts each of which are communicating with the Jamf Pro or Classic APIs, you may wish to reuse the Bearer Token to minimise the number of Basic Authentication requests made, and to speed up the transactions. In this case, I would recommend outputting it to a file.\n\nHere’s an example of exporting the token to a file at /tmp/bearer-token.txt.\n\n# obtain a Bearer Token using Basic Authentication, write the output to a file /usr/bin/curl --request POST \\--url\"https://yourserver.jamfcloud.com/api/v1/auth/token\"\\--header'accept: application/json'\\--user\"jamfsw:jamf1234\"\\--output\"/tmp/bearer-token.txt\"\n\nNow we can get the token from any script using plutil as before.\n\n# extract the token from the JSONtoken=$(/usr/bin/plutil -extract token raw \"/tmp/bearer-token.txt\")\n\nNote that since the token is now written to disk, it’s more important to revoke the token if we don’t need it any more.\n\nCheck the expiration of an existing token\n\nIf we’re storing the token in a file for use over multiple runs of scripts over an extended period of time, we may wish to check that it is still valid. We can check the token’s expiry limit in the JSON file that contains the token. If we want to compare the expiry time to the current time, it’s best to convert the timestamp to a Universal Time Epoch to rule out timezone issues.\n\n# check the expiry time of the tokenexpires=$(/usr/bin/plutil -extract expires raw \"/tmp/bearer-token.txt\" | /usr/bin/awk -F.'{print $1}')# convert expiry time to a UTC epochexpiration_epoch=$(/bin/date -j-f\"%Y-%m-%dT%T\"\"$expires\" +\"%s\")# compare expiry epoch to current timecurrent_time_epoch=$(/bin/date -j-f\"%Y-%m-%dT%T\"\"$(/bin/date -u +\"%Y-%m-%dT%T\")\" +\"%s\")if[[$expiration_epoch-gt$current_time_epoch]];then echo\"Token valid until the following epoch time: $expiration_epoch\"else echo\"No valid token available\"fi\n\nCreate a new token using an existing token\n\nBy default, tokens expire after 20 minutes, but we can use a currently valid token to generate a new token using the /api/v1/auth/keep-alive endpoint, giving us another 20 minutes.\n\n# obtain a Bearer Token using an existing token, write the output to a file /usr/bin/curl --request POST \\--url\"https://yourserver.jamfcloud.com/api/v1/auth/keep-alive\"\\--header'accept: application/json'\\--header\"authorization: Bearer $token\"\\--output\"/tmp/bearer-token.txt\"\n\nClient Credentials-based Authentication\n\nA new, more secure way of obtaining a Bearer Token was introduced with the release of Jamf Pro 10.49.0. This allows us to obtain a Bearer Token using an API Client, which would normally be configured from within Jamf Pro. Unlike the credentials required to obtain a Bearer Token using Basic Authentication, API Clients have no access to the Jamf Pro user interface.\n\nPrivileges are assigned to an API Client using one or more API Roles, which are also configured in the Jamf Pro user interface, with the privileges of each API Role acting cumulatively on the API Client. In most circumstances, it will be simplest to create one API Role per API Client.\n\nFor detailed instructions on how to set up and use API Roles and Clients, see Jamf Pro Documentation: API Roles and Clients. I just want to point out the basic differences between the use of API Clients and a normal account username/password combination.\n\nObtaining a Bearer Token\n\nWhen setting up an API Client, we obtain a Client ID and we generate a Client Secret. These are required to obtain the token. The endpoint used for obtaining the token is /api/oauth/token. As earlier, in this example we are outputting the JSON to a file for further use, but we can import it into a variable or environment variable exactly as described above.\n\n/usr/bin/curl --request POST \\--url\"https://yourserver.jamfcloud.com/api/oauth/token\"\\--header'Content-Type: application/x-www-form-urlencoded'\\--data-urlencode'client_id=6cabf059-21c9-44d6-bbde-02898f7430dd'\\--data-urlencode'grant_type=client_credentials'\\--data-urlencode'client_secret=dzmsPks-FwXpks80jhQGZZrAV3H2_ER0NAk91RE-xOBZvfghd98EM1hF9msfkanl'\\--output\"/tmp/access-token.txt\"\n\nThe access token file generated by this method is different to that generated when requesting a token using Basic Authentication, in that the key names are different. The token key is named access_token, but is used in exactly the same way as the token obtained using Basic Authentication. And rather than storing the expiry as a timestamp, we get a value in seconds for how long it will be until it expires - this is the expires_in key.\n\nLet’s look at how we extract the token, and how to calculate if the token is still valid.\n\n# extract the token from the JSONtoken=$(/usr/bin/plutil -extract access_token raw \"/tmp/access-token.txt\")# compare expiry epoch to current timecurrent_time_epoch=$(/bin/date +%s)expires_in=$(/usr/bin/plutil -extract expires_in raw \"/tmp/access-token.txt\")expiration_epoch=$(($current_epoch+$token_expires_in-1))if[[$expiration_epoch-gt$current_time_epoch]];then echo\"Token valid until the following epoch time: $expiration_epoch\"else echo\"No valid token available\"fi\n\nTwo last things to note, token expiration is achieved exactly as before using the /api/v1/auth/invalidate-token endpoint. But it is not possible to keep a bearer token alive that was generated using /api/oauth/token, so we always need to use the Client ID and Client Secret to create a new token.\n\nConclusion\n\nI hope this post provides clarity on what is being removed from Jamf Pro and what remains. To reiterate, we can continue to use normal account credentials to interact with the API, but if your scripts are using those credentials to directly request a Classic API endpoint, then before Jamf Pro is upgraded to 11.5.0, you should adapt the scripts to use those credentials to generate a Bearer Token, and then use that Bearer Token to make those subsequent requests.\n\nFor security reasons, moving forward you may want to consider setting up API Roles and Clients instead of using actual account credentials. This is especially pertinent when setting up accounts for use by third party integrations, external teams, and so on. However, this does not need to be done before the release of Jamf Pro 11.5.0.\n\nFurther Reading\n\nJamf Pro API Overview - includes code examples for obtaining a Bearer Token using Basic Authentication, and provides details on the HTTP Response Codes we may encounter.\n\nClassic API Reference - Details on how to use the Classic API and a list of all endpoints.\n\nClassic API Authentication Changes - Details on the depracation of the use of Basic Authentication for Classic API endpoints.\n\nJamf Pro API Reference - Details on how to use the Jamf Pro API and a list of all endpoints.\n\nHow to convert Classic API scripts to use bearer token authentication - Jamf Tech Thoughts post by William Smith.\n\nAPI Roles and Clients - Technical documentation about API Roles and Clients, detailing how to set them up in the Jamf Pro user interface.\n\nClient Credentials - Further details about API Roles and Clients, including a recipe for obtaining a Bearer Token using API Clients.\n\nAPI code recipes - Easy to read shell code recipes for many of the workflows described in this post.\n\nBack in April I posted about SOFA, short for “Simple Organized Feed for Apple Software Updates”, which consists of a machine-readable feed and user-friendly web interface, providing always-up-to-date information on XProtect data, OS updates, and associated patched CVEs.\n\nSOFA is hosted by the Mac Admins Open Source organisation, but it can be hosted in an organisation’s own infrastructure, allowing a high level of control over the information if your organisation requires it.\n\nWhat’s new?\n\nThe SOFA project has been a huge success, so much so that the amount of web traffic it has generated has exceeded all expectations. We have been investigating ways to reduce the amount of traffic, while maintaining the same great service. This is especially important since the release of Nudge 2.0 last week, which now utilises the SOFA feed, giving new opportunities to simplify the administration of the app.\n\nSo, we have made some changes in the Extension Attribute scripts we provided in the SOFA project’s GitHub repo, and if you are using those scripts, we are asking you to use the new versions of these scripts in your Jamf Pro instances.\n\nFurthermore, if you have crafted your own scripts or tools that take advantage of the SOFA feed, please take note of the following changes that you can and should make, to help reduce unnecessary costs for the Mac Admins Open Source organisation:\n\nNew Feed URL\n\nThe macOS feed URL has changed to https://sofafeed.macadmins.io/v1/macos_data_feed.json. This allows the feed to use specific content encoding rules and ensure that everyone is using them. Please update any scripts that are using the old sofa.macadmins.io address of the json feed.\n\nNote that the URL of the iOS feed has also changed, this is now https://sofafeed.macadmins.io/v1/ios_data_feed.json.\n\nCompression\n\ncurl offers a --compressed option which allows the file to be transferred as a gzipped version, saving extra bandwidth. This option is required for the new feed URL - the json feed file may not download properly without it.\n\ncurl offers the option to interrogate ETags, which are a fingerprint of the feed, without having to download the whole file. We have updated the Extension Attribute scripts, and the OSQuery plugin hads also been updated to take advantage of ETags, so that the feed file is only fully downloaded when changes are detected.\n\nIf you wrote your own scripts, you can write the ETag to a file using curl. You can therefore check first if there is a cached ETag signature file already on disk, and if it is present, compare the remote file with the locally cached one using the --etag-compare option. If there is no ETag file on disk, or the ETag doesn’t match, then you can go ahead and download the feed to disk and write the ETag to a file, by using the --etag-save option. Here’s how that works in bash code:\n\n# URL to the online JSON dataonline_json_url=\"https://sofafeed.macadmins.io/v1/macos_data_feed.json\"# local storejson_cache_dir=\"/private/tmp/sofa\"json_cache=\"$json_cache_dir/macos_data_feed.json\"etag_cache=\"$json_cache_dir/macos_data_feed_etag.txt\"# ensure local store folder exists /bin/mkdir -p\"$json_cache_dir\"# check local vs online using etag (only available on macOS 12+) - only download if changedif[[-f\"$etag_cache\"&&-f\"$json_cache\"]];then echo\"Cached SOFA json file and ETag found, comparing with latest online version - only downloading if different\" /usr/bin/curl --compressed--location--silent--etag-compare\"$etag_cache\"--etag-save\"$etag_cache\"--output\"$json_cache\"\"$online_json_url\"else echo\"No ETag cached, proceeding to download SOFA json file\" /usr/bin/curl --compressed--location--silent--etag-save\"$etag_cache\"--output\"$json_cache\"\"$online_json_url\"fi\n\nThe above code was edited on 23 July thanks to advice from Gabriel Sroka\n\nConclusion\n\nSOFA is still very new, so changes are inevitable as features are added, use cases grow, and problems are ironed out. We’re glad you’re part of the early adopters, and ask that you take a little time to ensure that you are using the latest feed and best practices. The old feed will be removed in a month or two to ensure that the old versions of these scripts are not used forever.\n\nWe also continue to recommend hosting a mirror of the feed for your own organisation if that’s feasible."
    }
}
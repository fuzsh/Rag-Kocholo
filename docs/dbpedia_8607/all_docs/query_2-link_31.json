{
    "id": "dbpedia_8607_2",
    "rank": 31,
    "data": {
        "url": "https://slejournal.springeropen.com/articles/10.1186/s40561-018-0080-z",
        "read_more_link": "",
        "language": "en",
        "title": "Engagement detection in online learning: a review",
        "top_image": "https://static-content.springer.com/image/art%3A10.1186%2Fs40561-018-0080-z/MediaObjects/40561_2018_80_Fig1_HTML.png",
        "meta_img": "https://static-content.springer.com/image/art%3A10.1186%2Fs40561-018-0080-z/MediaObjects/40561_2018_80_Fig1_HTML.png",
        "images": [
            "https://pubads.g.doubleclick.net/gampad/ad?iu=/270604982/springer_open/slejournal/articles&sz=728x90,970x90&pos=LB1&doi=10.1186/s40561-018-0080-z&type=article&kwrd=Engagement detection,Affect detection,Facial expression recognition,Action units,Emotion detection&pmc=O00000,I24032&",
            "https://slejournal.springeropen.com/static/images/springeropen/logo-springer-open-d04c3ea16c.svg",
            "https://media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs40561-018-0080-z/MediaObjects/40561_2018_80_Fig1_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs40561-018-0080-z/MediaObjects/40561_2018_80_Fig2_HTML.png",
            "https://pubads.g.doubleclick.net/gampad/ad?iu=/270604982/springer_open/slejournal/articles&sz=300x250&pos=MPU1&doi=10.1186/s40561-018-0080-z&type=article&kwrd=Engagement detection,Affect detection,Facial expression recognition,Action units,Emotion detection&pmc=O00000,I24032&",
            "https://slejournal.springeropen.com/track/article/10.1186/s40561-018-0080-z",
            "https://slejournal.springeropen.com/static/images/logo-springernature-acb40b85fb.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "M. Ali Akber"
        ],
        "publish_date": "2019-01-03T00:00:00",
        "summary": "",
        "meta_description": "Online learners participate in various educational activities including reading, writing, watching video tutorials, online exams, and online meetings. During the participation in these educational activities, they show various engagement levels, such as boredom, frustration, delight, neutral, confusion, and learning gain. To provide personalized pedagogical support through interventions to online learners, it is important for online educators to detect their online learners’ engagement status precisely and efficiently. This paper presents a review of the state of the art in engagement detection in the context of online learning. We classify the existing methods into three main categories—automatic, semi-automatic and manual—considering the methods’ dependencies on learners’ participation. Methods in each category are then divided into subcategories based on the data types (e.g., audio, video, texts for learner log data etc.) they process for the engagement detection. In particular, the computer vision based methods in the automatic category that use facial expressions are examined in more details because they are found to be promising in the online learning environment. These methods are nonintrusive in nature, and the hardware and the software that these methods use to capture and analyze video data are cost-effective and easily achievable. Different techniques in the field of computer vision and machine learning are applied in these methods for the engagement detection. We then identify their challenges of engagement detection and explore available datasets and performance metrics for engagement detection, and provide recommendations for the future to advance the technology of engagement detection for online education.",
        "meta_lang": "en",
        "meta_favicon": "/static/img/favicons/darwin/apple-touch-icon.png",
        "meta_site_name": "SpringerOpen",
        "canonical_link": "https://slejournal.springeropen.com/articles/10.1186/s40561-018-0080-z",
        "text": "A key issue in online learning is to improve learners’ engagement with their educational activities. Since the 1980s, learner engagement has been a key topic in the education literature (Whitehill et al. 2014). This interest may be driven by the concerns about high drop-out rates in online courses (Rothkrantz 2016). It is widely acknowledged that engagement and affect are linked to increased productivity and learning gain. Some research shows that engagement is malleable, and proper pedagogical interventions, learning designs and feedback can enhance learner engagement (Monkaresi et al. 2017). To provide personalized pedagogical support through intervention to online learners, detecting learners’ engagement has become important to online education. Fostering learners’ engagement can benefit not only in online learning, but also in other learning settings such as traditional classrooms, educational games, and intelligent tutoring systems (Karumbaiah et al. 2017).\n\nSeveral facets of learners’ engagements have been discussed in the literature (Bosch 2016; Fredrick et al. 2004; Anderson et al. 2004). Bosch (2016) organizes engagement as three different forms: affective, behavioral, and cognitive. Fredrick et al. (2004) define engagement as behavioral, cognitive, and emotional, whereas Anderson et al. (2004) define as academic, behavioral, cognitive, and psychological in their research studies. Affective engagement refers to the emotional attitude, for example, being interested in a topic and enjoying learning about it (Bosch 2016)), whereas academic engagement refers to academic identification (e.g., getting along with teachers) and participation (e.g., time on tasks, not skipping classes) towards learning (Al-Hendawi 2012). Behavioral engagement draws on the idea of participation including participation in the classroom and extra-curricular activities, stay focused, submit assigned tasks, and follow the instructor’s dictation (Christenson et al. 2012). Cognitive engagement refers to the thoughtfulness and willingness to exert the effort necessary to comprehend complex ideas and master difficult skills (e.g., focused attention, memory, and creative thinking (Anderson et al. 2004)). Emotional engagement encompasses positive and negative reactions to teachers, classmates, and academics (Fredrick et al. 2004). Psychological engagement refers to the sense of belonging and relationships with teachers and peers (Christenson and Anderson 2002).\n\nDifferent types of engagements in the context of learning are useful to know for personalized intervention design to improve learners’ experience. However, studies that focus on learner engagement need a way of measuring it (Harris 2008). This can be done with one of the two types of data identified by engagement theorists: internal to the individual (cognitive and affective) and external observable factors (perceptible facial features, postures, speech, and actions) (Bosch 2016). Some research studies also emphasized that measuring engagement requires bringing together observational data with the data internal to the individual (e.g., self-reports) (Whitehill et al. 2014).\n\nThis paper presents a review of the state of the art of engagement detection methods in the context of online learning, and then it identifies the challenges of detecting engagement in online learning. We classify the existing methods into three main categories—automatic, semi-automatic and manual—considering the methods’ dependencies on learners’ participation. And, then the methods in each category are divided into subcategories based on the types of data used, e.g., audio, video, learner log data etc. In particular, the computer vision based methods in the automatic category that use facial expressions are examined because they are promising in an online learning environment, nonintrusive in nature, and cost-effective when considering the hardware and the software needed for capturing and analyzing video data. Finally, we explore available datasets and performance metrics for engagement measurement, and provide recommendations for the future to advance the technology of engagement measurement for online education.\n\nThe remainder of the paper is organized as follows. In Section II, a taxonomy of engagement detection methods is proposed and related trends are discussed. Among the different methods, the computer vision based methods in the automatic category are found to be beneficial and further detailed in Section III. Benchmarking datasets, performance metrics, and evaluation strategies along with some results are discussed in Section IV. Section V concludes the paper with some critical discussions and future recommendations.\n\nTaxonomy of engagement detection methods\n\nSeveral research studies on learners’ engagement detection can be found in the literature. To review them, we propose a taxonomy which is shown in Fig. 1. First, we divide the existing methods for learners’ engagement detection into three main categories — automatic, semi-automatic, and manual — based on the strategy and the type of users’ involvement in the engagement detection process. The manual methods are further divided into self-reporting and observational check-list categories. The methods related to engagement tracing are categorized as semi-automatic in the taxonomy. The methods in the automatic category are divided into computer vision based methods, sensor data analysis, and log-file analysis depending on the information that these methods process for engagement detection. The computer vision based methods are further divided into three sub-categories — facial expression, gestures and postures, and eye movement — based on the modalities they use for the engagement detection. Although some research studies use the above modalities separately, some others find it promising to combine two or more of them to achieve a higher accuracy.\n\nThe manual category refers to the methods where learners’ direct involvement is needed in the engagement detection process. In the manual category, self-reporting is a popular technique where a set of questionnaire is posted in which learners report their own level of attention, distraction, excitement, or boredom O'Brien and Toms 2010. All the survey questionnaires do not necessarily indicate the level of engagement of the learners directly, rather they imply engagement as a descriptive latent variable using factor analysis (Matthews et al. 2002; Wixon et al. 2016). Self-reporting is of great interest to many researchers because it is easy to administer and it provides some useful information regarding learner engagement. For example, it is useful to know that 25 and 60% of the learners report being bored and disengaged, respectively (Shernoff et al. 2000). However, the validity of the self-reporting results depends on a number of factors that are outside of the control of the researchers, such as learners’ honesty, their willingness to report their emotion, and the accuracy of learners’ perception about their emotions (D’Mello et al. 2014).\n\nObservational checklist is another popular method in the manual category for detecting learner engagement that relies on questionnaires completed by external observers instead of the learners. These questionnaires often consider teachers’ personal opinion regarding the learners’ engagement levels. They may also contain checklists for objective measures that are supposed to indicate engagement. Some example questions are “do the learners sit quietly?”, “do they do their homework?”, “are they on time?”, “do they ask questions?” (Parsons and Taylor 2011). In some cases, external observers may rate learner engagement based on live or pre-recorded videos of educational activities (Kapoor and Picard 2005; 2012). Observers may also consider samples of the learner’s work such as essays, projects, and class notes (Parsons and Taylor 2011). Observational checklists also have some limitations. Observational metrics may not always be related to engagement. For example, sitting quietly, good behavior, and no tardy cards appear to measure compliance and willingness to adhere to rules and regulations rather than engagement (Whitehill et al. 2014). Another major limitation for both the self-reporting and observational checklist is that they require a great deal of time and effort from both the learners and the observers to detect learners’ engagement.\n\nMethods in the semi-automatic category, learners’ indirect involvements are needed in the engagement detection process. Engagement tracing is a popular method in the semi-automatic category that utilizes the timing and accuracy of learner responses to practice problems and test questions (Beck 2005). In order to evaluate the time and accuracy pattern, probabilistic inference is used and it remains consistent with an engaged or disengaged learner (Beck 2005; Johns and Woolf 2006). For example, very short response times on easy questions indicates that the learners are not engaged and are simply giving random answers without any effort. Although this method has been widely used in intelligent tutoring systems (ITS), not many applications of this method can be found in other educational settings, such as in online learning (Whitehill et al. 2014).\n\nMethods in automatic category extract features from various traits captured by image sensors (e.g., eyes movement, facial expressions, and gestures and postures), physiological and neurological sensors (e.g., heart rate, EEG, blood pressure, or galvanic skin response) or by tracing learners’ activities in their learning environments (e.g., total time spent on study, number of forum posts, average time to solve a problem, number of submissions correct etc.). These methods extract features automatically and do not interrupt learners in the engagement detection process. The methods in the automatic category are further divided into three groups: log-file analysis, sensor data analysis, and computer vision based methods.\n\nIn the log-file analysis, learners’ actions preserved in log files are analyzed for the engagement detection. Especially, in an online learning environment, the learners’ actions are stored in log files and this can provide valuable information for the engagement detection. Different data mining and machine learning approaches are used in the log-file analysis. Cocea and Weibelzahl (2009, 2011) analyzed log-files in a web-based learning environment called HTML-Tutor. This research study analyzed 30 attributes of the online learners’ from the log file including a number of pages accessed, average time spent on pages, number of tests attended, number of correctly answered tests, and number of incorrectly answered tests. Sundar and Kumar (2016) proposed an improvement over the above methods by combining the attributes of the log file with the user profile. In another research study, Aluja-Baneta et al. (2017) applied psychometric theory to 14 behavioral indicators to measure the learners’ engagement in a virtual learning environment.\n\nIn the sensor data analyses, physiological and neurological sensor readings are used to measure engagement. In the neuroscience literature, engagement is typically equated with the level of arousal or alertness (Whitehill et al. 2014). Various physiological measures, such as EEG, blood pressure, heart rate, or galvanic skin response, are used to measure engagement and alertness (Chaouachi et al. 2010; Fairclough and Venables 2006; Goldberg et al. 2011). However, these measures require specialized sensors and are not convenient to use in real-life education settings.\n\nThe computer vision based methods offer a number of ways to measure learners’ engagement by investigating the cues from the gestures and postures, eye movement, and facial expressions (D’Mello et al. 2009; D’Mello and Graesser 2010; Kapoor and Picard 2005; McDaniel et al. 2007). The main advantage of computer vision based methods is the unobtrusiveness of the assessment process and easy to use, similar to the classroom situation where a teacher observes whether a learner is motivated without interrupting his/her activities. Affective computing techniques and low cost of cameras and wide-spread of its availability in cell phones, tablets, computers, and even automobiles, are allowing to detect learners’ engagement using computer vision (Monkaresi et al. 2017; Kamath et al. 2016). While vision-based methods for engagement detection have been pursued previously by the ITS community, much work remains to be done before making these automatic systems practical in a wide variety of education settings, such as in online learning.\n\nComputer vision based methods for detecting engagement in online learning\n\nComputer vision based methods are used to estimate learners’ perceived engagement, i.e., engagement as judged by an external observer. Since teachers rely on perceived engagement to adapt their teaching behavior in a conventional classroom setting, the automation of perceived engagement detection is likely to be useful for online learning. The online courses can take advantage of this technological advancement for personalized intervention design, and reduce learners’ frustration and dropout rates.\n\nGeneric framework\n\nTo facilitate the review, we present a generic framework (see Fig. 2) for learner’s perceived engagement detection using the computer vision based methods. The framework is consisted with five different modules that include detection, feature extraction, tracking, classification, and decision.\n\nIn a computer vision based engagement detection system, video streams are captured using a webcam or a surveillance camera, where the camera provides a particular view of learners participating in a learning activity. The system seeks to detect the region of interests (ROIs) (e.g., face, gestures, postures or eye) of the learners in the live video stream. Typically, engagement detection in such system is performed with a track-and-classify approach. The system first performs segmentation to isolate the ROIs using a detection module in each frame. For each ROI, features are then extracted in a feature extraction module and selected into patterns to initiate tracking and classification. A classification module is used to match input patterns against patterns extracted from training dataset and generates classification scores. A tracking module is designed for tracking the movement or changes in the ROIs in consecutive frames and generates tracking trajectories. Finally, a decision module combines classification scores over trajectories to output a list of engagement levels of the learners in the input video stream.\n\nWe found that the most commonly used modalities in computer vision based methods are facial expressions, gestures and postures, and eye movement. Thus, in the following subsections, we further review the engagement detection methods focusing on these three modalities.\n\nFacial expressions\n\nIt has been hypothesized that a good deal of information used by humans to make engagement judgment is based on human faces, and facial expressions are directly linked to the perceived engagement (Whitehill et al. 2008, 2014; Ekman et al. 2002; Littlewort et al. 2011). Using cameras provide a continuous and non-intrusive way of capturing face images as a learner uses a mobile device or a personal computer for his or her learning activities. The captured facial information is used to understand certain facets of the learner’s current state of mind. Many different methods have been proposed to automate this detection process by analyzing the face images (Booth et al. 2017; Bosch et al. 2014). Based on how the information from a face appearance is used, these methods are divided into two groups: part-based and appearance-based. Both the part-based methods and the appearance-based methods use geometric and holistic features in their engagement detection process (Dewan et al. 2018).\n\nPart-based methods\n\nPart-based methods refer to the techniques that analyze different parts of a face (e.g., eyes, mouth, nose, forehead, chin and so on) for the engagement detection. A comprehensive way to analyze the parts of a face is the Facial Action Coding System (FACS). Ekman and Friesen (1978) is the pioneer in developing the FACS system for the analysis of facial expressions. The FACS uses facial muscle movements also known as action units (AUs) in order to design the theoretical measure of specific discrete emotions (Ekman and Friesen 1978; Ekman et al. 2002). FACS has been extensively used by psychologists and neuroscientists on various aspects of facial expression analysis. The AUs can occur either singly or in combination. Ekman et al. (2002) acknowledged that although the number of AUs is relatively small, more than 7000 AU combinations are observed in our everyday life, and certain AUs or certain combinations of AUs are more frequent than the others. For example, happiness is sometimes viewed as a combination of AU12 and AU6. Using FACS, every possible facial expression can be empirically described as a mixture of AUs.\n\nMeasuring AUs is a descriptive analysis of behavior, whereas measuring facial expressions, such as anger or happiness, is an inferential process (Ekman and Friesen 1978). Any observational system (e.g., engagement detection) requires inferences about that which is being measured (Whitehill et al. 2014; Grafsgaard et al. 2013a). Although the FACS has been widely used in facial expression recognition in the last several years, it has just got attention for the engagement detection in learning context (Grafsgaard et al. 2013b). In the literature, the mapping of AUs to those each of the expressions are relatively well defined (Martinez et al. 2017). However, the mapping of AUs to learning-centered affective states are still at its early stage. Some mapping has been done in recent years and these are listed in Table 1. McDaniel et al. (2007) mapped the AUs into 5 different engagement levels — boredom, confusion, delight, flow, frustration, and surprise — when the learners interact with a web-based educational tool called AutoTutor. The learners’ facial expressions are coded using FACS by the human experts. The set of AUs that accompany the above affective states are listed in Table 1. The authors acknowledged that the above estimated affective states were the most prominent uses in online learning. They also acknowledged that some of these affective states were correlated with learning gains, e.g., boredom was negatively correlated with learning, whereas confusion and flow were positively correlated.\n\nInstead of discriminating the engagement in different levels, Booth et al. (2017) measured learners’ engagement in a scalable and accessible manner. In this study, videos were collected from a screen-mounted camera of learners studying online lectures. AUs (Al-Hendawi 2012; Aluja-Baneta et al. 2017; Aslan et al. 2014; Bartlett et al. 2006; Bosch 2016; Chen et al. 2013; Christenson et al. 2012; Cocea and Weibelzahl 2009; Dewan et al. 2018; D’Mello et al. 2014; D’Mello et al. 2009; Fredrick et al. 2004; Grafsgaard et al. 2013b; Grafsgaard et al. 2013c) were used with facial landmarks, eye gaze, emotion probabilities, average optical flow magnitude and direction, and head pose and size. A decision on engagement detection was done using the K-nearest neighbor (KNN) classifier. Both subject-independent and individual-specific models were analyzed, where the individual-specific models outperformed the others. Whitehill et al. (2008) estimated the difficulty level of an online lecture by analyzing 12 AUs which were automatically recognized by support vector machines (SVM) and Gabor energy filters (Bartlett et al. 2006). The output of the 12 AUs detectors (see Table 1) were analyzed to make inference on the difficulty levels the learners feel about an online lecture. By using this measurement, the speed of the instruction was adjusted automatically to avoid any frustration, confusion, and boredom that could potentially happen to the online learners. In another study, Whitehill et al. (2014) investigated the face and facial landmark (i.e., eyes, nose, and mouth) with four binary classifiers, one for each engagement category: not engaged, nominally engaged, normally engaged, and very engaged. The classifiers were formed by combining GentleBoost with Box Filter features (Boost (BF)), SVM with Gabor features (SVM (Gabor)), Multinomial logistic regression (MLR) with expression outputs from the Computer Expression Recognition Toolbox CERT (MLR (CERT)) (Littlewort et al. 2011).\n\nCERT has also been used in several other research studies for engagement detection in the context of learning. CERT gives intensity values for facial AUs from a wide range of FACS, thus empowering fine-tuned analyses for inferring affective states of learners’ using facial expression analysis. Grafsgaard et al. (2013b) analyzed facial movements consisting of brow raising, brow lowering, eyelid tightening, and mouth dimpling that occurred during computer-mediated tutoring using the CERT. In this study, upper face movements were found to be predictive of engagement, frustration, and learning. Mouth dimpling was found to be a positive predictor of learning and self-reported performance. The authors also acknowledged that both the intensity and frequency of facial expressions could be used to predict tutoring outcomes. In another study, Grafsgaard et al. (2013a) acknowledged that AU2 was negatively correlated with learning gain, whereas AU4 was positively correlated with frustration. AU14 was positively correlated with both frustration and learning gain.\n\nBosch et al. (2014) used CERT to track FACS facial features, and the features were used to build classification models to detect five engagment levels—confusion, frustration, boredom, neutral, and engaged. In this study, the engagement levels—confusion and frustration—were detected with a higher chance than the levels—boredom, neutral, and engaged. AU45 was found to be a predictive feature to identify confusion and frustration, where AU1 or a combination of AU1 and AU4 was found as a separator for the confusion from the frustration. The authors also conducted experiments in different real-world settings in a school environment with some unique challenges and achieved similar success (Bosch 2016; Bosch et al. 2015, 2016). For example, Bosch et al. (2016) used FACET (commercial version of CERT) to estimate the presence of 19 AUs along with head poses, head positions, and interaction patterns of learners’ with learning environments. Features extracted from these patterns were used with C4.5 trees and Bayesian classifiers. The most common affective states observed by the authors were engagement, followed by frustration, boredom, delight, and confusion. Vail et al. (2016a) examined the responses received from CERT with the skin conductance responses, postures, and gestures. Authors argued that among the modalities they used, facial expressions and skin conductance responses were found to be highly predictive of learning gain. CERT was used to analyze different AUs of learners, and the AU4, AU5, AU15, and AU23 were found to be highly predictive to learners’ learning. In another study, Vail et al. (2016b) acknowledged that the intense expression of AU12 or AU5 represented higher engagement. AU12 was likely to be related to higher engagement and AU5 to signifying paying attention to and concentrating on the task.\n\n2D and 3D information from different sensors were combinely used with AUs for engagement detection. Saneiro et al. (2014) analyzed 2D points of a face, 3D head poses, and animation and shape units, where the animation and shape units include jaw lowered, lip stretcher, brow lowered, lip corner depressor, and outer brow raiser from a Kinect camera. This method applied machine learning technique to infer five engagement levels—excited, relaxed, resolution, interested and concentrated—when dealing with cognitive tasks. Psaltis et al. (2017) combined AUs to body motions from Kinect sensors and the gameplay events to detect affective states—engaged and not-engaged—with the intensity values in ranges [−2, 0] and [0, 2], respectively. Artificial Neural Network (ANN) was used for engagement classification. Sathik and Jonathan (2013) examined different non-verbal communications and AUs to interpret the comprehension level of learners in a virtual classroom.\n\nAppearance-based methods\n\nIn appearance-based methods, features extracted from whole-face regions are used to generate patterns for engagement classification. Among different feature extraction techniques, Local Binary Patterns (LBP) and Histogram of Oriented Gradients (HOG) are found to be popular for engagement detection. Monkaresi et al. (2017) used LBP in three orthogonal planes (LBP-TOP) of face appearance with Kinect face tracker and heart-rate for learners’ engagement detection in educational activities. Both decision-level and feature-level fusion were used with Updateable Naive Bayes, Bayes Net, K-means clustering, Rotation Forest, and Dagging classifiers. Two level engagement detection (i.e., engaged and not-engaged) was done, where the feature-level fusion was found to be more successful than the decision-level fusion when small size dataset was available to train the classifiers. The accuracy of the facial expression-based channels (LBP-TOP and face tracker) was found to be higher than the heart-rate channel. Kamath et al. (2016) presented an instance-weighted multiple kernel learning SVM model that considers vote distributions from crowdsourcing platforms for learner’s engagement detection during e-learning sessions.\n\nDeep learning approaches have been used in engagement detection. Kaur et al. (2018) used LBP-TOP and Deep Multi-Instance Learning (DMIL) for engagement detection. Since the labeling of the engagements at frequent intervals in user videos is expensive and noisy, in this research study, the prediction and localization of learner engagement were formulated as a Multi-Instance Learning (MIL) problem and derived baseline scores based on DMIL. The dataset was annotated using crowdsourcing, where the labelers were instructed to label the videos on the basis of their engagement intensity (from facial expressions) ranging from disengaged, barely engaged, engaged, and highly engaged. Gupta et al. (2018) investigated different models of CNN with face appearance features for detecting four engagement levels—engagement, boredom, confusion, and frustration. Each of the engagement levels were further ranked from low to high scales.\n\nFeatures extracted from face appearance are combined with different visual cues to enhance learners’ engagement detection. Happy et al. (2013) automatically identified learners’ cognitive state using non-intrusive visual cues—facial expression, ocular parameters, gestures, and postures—captured by webcam. The LBP features extracted from the face appearance were used to determine the basic emotions such as happiness, surprise, anger, fear, and sadness. SVM was used to identify the eye state as open or closed to observe the interest of learner. Body movements from vision cue was used to understand attention and interest. The postures of hand and head gestures were used to detect boredom and frustration. By combining information from the above cues, the system inferred the learner’s state of alertness to generate appropriate feedback. Hwang and Yang (2008) proposed a fuzzy-based engagement detection method using face appearance. The moves of facial features, distances between facial features and facial edges, and records of mouse and keyboard operations in computers were used to evaluate learners’ engagement. Drowsiness, turning head to talk, and leaving seats were identified as a low engagement attitude in the context of online learning in this research study.\n\nFace appearance information along with 2D and 3D information collected from different sensors are used for engagement detection. Frank et al. (2016) proposed a framework for engagement detection that includes facial appearance, voice, body postures and motion using 2D and 3D sensors. A SVM classifier is used to classify in one of the six individual engagement levels — disengagement, relaxed engagement, involved engagement, intention to act, action, and involved action. This method is applied to detect engagement levels in a group meeting. Khelfallah et al. (2015) proposed a web-based intelligent tutoring system called Remote Laboratory that allowed learners from anywhere to use the Internet and perform computational experiments in real laboratory equipment, where the learners’ levels of engagement were examined in terms of frustration and serenity by using 70 small classifiers.\n\nGestures and postures\n\nGesture and postures are two important forms of non-verbal communication through our body language. These are important components of embodied affect with ties to cognitive-affective states that may help or hinder learning. Grafsgaard et al. (2013c) analyzed gestures and postures in a computer-mediated tutorial dialogue, where relationships between learner postures, gestures, dialogue, and tutor were investigated. Hand-to-face and hand-over-face gestures were found to be promising for informing the runtime behavior of tutoring. Gestures and postures combined key mechanisms of holistic methods of nonverbal behavioral communication and included affects while learning. In this study, learner data was collected from database logs, webcam video, skin conductance, and Kinect depth video to infer the knowledge about learners’ engagement. Hand gestures were also used by Tofighi et al. (2016) to identify disengagement, attention, intention, and actions (DAIA). In DAIA, several binary classifiers were designed to detect various hand movements, e.g., raise hands above the waist, different levels of hand speeds, and so on. These classifiers detect user intention for performing an action. A Finite State Transducer (FST) of engagement detection was finally used to flow among different emotional states by analyzing the decisions by the classifiers.\n\nSome research studies focused on revealing learner-tutor interactions by analyzing gestural-activities in a spatial and temporal domain (Sathayanarayana et al. 2014). In these systems, gestures are captured by an overhead camera and then manually labeled as deictic, beat, iconic, and writing for the training of a classifier. Sathayanarayana et al. (2014) employed visual deictic gestures to understand learner-tutor interactions. A graph-based visual saliency (GBVS) (Harel et al. 2006) was used to detect potential deictic gestural regions in the input image. Then an HOG and SVM based classifier was used to determine actual deictic gesture points from candidate regions. Learner-tutor interactions were finally inferred from incorporating gestural-activity information in a spatial and temporal domain. Among different visual cues from gestures and posters, the deictic gestures found to be as a key component to reveal learner-tutor interactions in this research study.\n\nEye movement\n\nUsers’ gazes and regions of interests from eye trackers have been used to understand the moods of learners while engaging in any educational activity in online learning. Aslan et al. (2014) used an eye tracker to detect the users’ gazes and combined this information with statistical facial features and depth information. Nine pilot sessions on five machine learning algorithms—decision trees, random forest, naive Bayes, logistic regression, and multilayer perceptron—were tested for engagement detection. The authors acknowledged that the use of touch-free 2D and 3D cameras to collect the above information enabled the system to get more accurate facial landmarks and achieved better results for engagement detection. Krithika and Lakshmi (2016) employed the moving patterns of eyes with head motions to infer information on concentration levels in an e-learning environments.\n\nRaina et al. (2016) presented an eye-tracking-based model to reduce content skipping, thus enhancing engagement in online learning. Two learning modules — one with a large amount of content on a single screen (linear) and the other with the same content broken into smaller chunks — were tested. Learners required to go through the content and answer a set of questions. Authors examined learners’ content skipping behavior between linear and segmented modules based on reading scores (Buscher et al. 2008) and reading depths by eye tracking.\n\nAlthough the methods based on eye-tracking are effective, the main challenge of these methods is proper eye-calibration. To receive accurate data precision, these methods require several calibration rounds for each participant. Participants wearing eye-glasses or having eye disorders have difficulty in calibration and often need to be excluded from the studies (Raina et al. 2016). Another major challenge is restricting participants’ movement to stay within an eye-tracker range which is not feasible in a real-life educational environment.\n\nDataset and evaluation techniques\n\nInformation about open and online datasets and knowing about metrics and evaluation techniques might be of great interest for researchers in any research field. A review on available datasets, metrics for evaluation and evaluation techniques suitable for users’ engagement detection in the context of learning, and some research results are discussed in this section.\n\nDataset\n\nThe need for large, labeled, publicly available datasets for training, evaluating, and benchmarking has been widely acknowledged, and a number of efforts to address this need have been made in the last few years. In user engagement detection, while many research studies use their in-house datasets, very few of them are made publicly available online. A summary of publicly available and annotated datasets are listed in Table 2.\n\nGupta et al. (2018) created the DAiSEE dataset with an intent to capture learners’ engagement in online courses. This dataset includes 112 individuals, where 80 male and 32 female. The videos in the dataset were collected in unconstrained environments, such as at dorm rooms, crowded lab spaces and libraries, with three different illumination settings—light, dark, and neutral. The videos were captured with a webcam mounted on a computer focusing on learners’ watching some video tutorial. The annotation of the video frames were done in four different levels—engaged, bored, confused, and frustrated, by relying on the “wisdom-of-the crowd”. The annotations were further rated from 0 to 3 based on the intensity. The advantage of the above annotation is that it can be changed to any other n-levels of engagement as required. The dataset HBCU (Whitehill et al. 2014) were taken from 34 individuals from two different pools, where 9 male and 35 female. In both pools, individuals participated in Cognitive Skills Training study arranged by the Historically Black College/University (HBCU) and the University in California (UC). Annotation of the dataset was done manually by human experts.\n\nKaur et al. (2018) introduced “in-the-wild” dataset, where videos were captured from 78 individuals — 25 females and 53 males. The dataset was collected in an unconstrained environment, such as at a computer laboratory, hostel rooms, and an open ground, via Skype. The annotation of the data was done to one of the four possible engagement levels—disengaged, barely engaged, normally engaged, and highly engaged based on crowdsourcing. Sathayanarayana et al. (2014) introduced the dataset SDMATH, where the videos were captured for one-to-one mathematics tutoring sessions. This dataset offers a set of richly labeled data with both video and audio modalities.\n\nAlthough each of the datasets mentioned above has their own characteristics and advantages, they do have some limitations. In these datasets, the videos were recorded with limited participants from a particular race. For example, all the participants in HBCU datasets are African-American, whereas in DAiSEE and “in-the-wild” are Asian. Also, the male-female ratio in the datasets are high. The above issues may cause generalization problem during training and testing with the classifiers. Another limitation of these datasets is the ambiguity in labeling the frames with appropriate engagement levels. The frames in the DAiSEE and “in-the-wild” are labeled based on crowdsourcing, where as in the HBCU by human experts. In both cases, ambiguity in labeling frequently occur due to not having a clear guideline for mapping facial indicators to different affective states or engagement levels of the online learners. To alleviate this issue, frames with ambiguous labeling are often removed during the experiments, which eventually reduces sizes and removes the diversity of information in the datasets. Visual cues along with users’ activity in the learning environment, self-evaluation and transfer learning could further be investigated to solve the above problems.\n\nEvaluation techniques and metrics\n\nDifferent metrics and techniques are used to evaluate the performance of engagement detection systems. A widely used technique for evaluating the performance of engagement detection is the investigation of correlation between human and automatic perceptions of engagements. Whitehill et al. (2014) compared the automatic perceptions of engagement and the learner pre- and post-test performance to evaluate the performance of an engagement detection system. Grafsgaard et al. (2013a, 2013b) evaluated the performance by comparing the results of automated engagement detection with manual annotations. Cohen’s Kappa, R2, multinomial logistic regression, Pearson’s correlations and Krippendorff’s alpha are often used for correlation-measurement (Kaur et al. 2018; D’Mello et al. 2009; Vail et al. 2016b).\n\nFor the automatic perceptions of engagement, different classifiers are used, where the accuracy of classification is measured in a Receiver Operational Characteristic (ROC) space. In the ROC space, the area under the curve (AUC) is estimated that provides a global measure of the system performance (Monkaresi et al. 2017; Bosch et al. 2015; Bosch et al. 2016). In practice, an empirical ROC curve is obtained by connecting the observed tpr and fpr for a classifier at each threshold. The AUC assesses ranking in terms of class separation – the fraction of positive–negative pairs that are ranked correctly. For instance, with an AUC = 1, all positives are ranked higher than negatives indicating a perfect discrimination between classes. A random classifier has an AUC = 0.5, and both classes are ranked at random. The partial AUC, pAUC (5%), is measured by taking the AUC at 0 < fpr ≤ 5% from the ROC curve.\n\nClass priors for positive samples and negative samples may vary over time in a real scenarios. Traditional ROC analysis cannot distinguish between two classifiers for specific class miss-classification costs. ROC curves and the AUC allow for a performance evaluation that is independent of costs and priors by integrating performance over a range of decision thresholds. However, it is important to observe performance as the proportion of the correctly predicted positive samples out of the total number of input samples predicted to belong to an affective state. Otherwise, when processing highly imbalanced data, and the minority positive samples are of interest, a system may outperform others by predicting a very large number of samples as minority, resulting in an increased tpr at the expense of an increased fpr (Sathayanarayana et al. 2014; Cocea and Weibelzahl 2009, 2011). Given the imbalance between a target and non-target captures, performance is assessed in the Precision–Recall (PR) space (Bosch et al. 2015), where the area under the PR curve (AUPR) provides another global scalar measure.\n\nThe 2-alternative forced choice (2AFC) is another measure which expresses the probability of correctly discriminating a positive example from a negative example in a 2-alternative forced choice classification task (Whitehill et al. 2008; Fei and Pavlidis 2010; Mason and Weigel 2009). The 2AFC is an unbiased estimate of the area under the ROC curve, which is commonly used in the facial expression recognition literature. A 2AFC value of 1 indicates perfect discrimination, whereas 0:5 indicates that the classifier is “random by the chance”."
    }
}
{
    "id": "yago_19690_2",
    "rank": 93,
    "data": {
        "url": "https://gwern.net/backstop",
        "read_more_link": "",
        "language": "en",
        "title": "Evolution as Backstop for Reinforcement Learning",
        "top_image": "https://gwern.net/static/img/logo/logo-whitebg-large-border.png",
        "meta_img": "https://gwern.net/static/img/logo/logo-whitebg-large-border.png",
        "images": [],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            "ai/nn",
            "economics/automation",
            "insight-porn",
            "philosophy/epistemology",
            "psychology/energy",
            "psychology/willpower",
            "reinforcement-learning/multi-agent",
            "reinforcement-learning/safe",
            "statistics/bayes",
            "statistics/decision",
            "technology"
        ],
        "tags": null,
        "authors": [
            "Gwern Branwen"
        ],
        "publish_date": null,
        "summary": "",
        "meta_description": "Markets/evolution as backstops/ground truths for reinforcement learning/optimization: on some connections between Coase’s theory of the firm/linear optimization/DRL/evolution/multicellular life/pain/Internet communities as multi-level optimization problems.",
        "meta_lang": "en",
        "meta_favicon": "/static/img/logo/logo-favicon-small.png",
        "meta_site_name": "",
        "canonical_link": "https://gwern.net/backstop",
        "text": "Artificial Persons\n\nThe striking thing about corporations improving is that they don’t; corporations don’t evolve (see the Price equation & multi-level selection, which can be applied to many things). The business world would look completely different if they did! Despite large persistent differences in efficiency between corporations, the best management practices or corporations don’t simply ‘clone’ themselves and regularly take over arbitrary industries with their superior skills (and after reaching fixation, eventually succumbing to mutant offspring who have become even more efficient than them, and so on).\n\nWe can copy the best software algorithms, like AlphaZero, indefinitely and they will perform as well as the original, and we can tweak them in various ways to make them steadily better (and this is in fact how many algorithms are developed, by constant iteration); species can reproduce themselves, steadily evolving to ever better exploit their niches, not to mention the power of selective breeding programs; individual humans can refine teaching methods and transmit competence (calculus used to be reserved for the most skilled mathematicians, and now is taught to ordinary high school students, and chess grandmasters have become steadily younger with better & more intensive teaching methods like chess engines); we could even clone exceptional individuals to get more similarly talented individuals, if we really wanted to. But we don’t see this happen with corporations. Instead, despite desperate struggles to maintain “corporate culture”, companies typically coast along, getting more and more sluggish, failing to spin off smaller companies as lean & mean as they used to be, until conditions change or random shocks or degradation finally do them in, such as perhaps some completely-unrelated company (sometimes founded by a complete outsider like a college student) eating their lunch.\n\nWhy do we not see exceptional corporations clone themselves and take over all market segments? Why don’t corporations evolve such that all corporations or businesses are now the hyper-efficient descendants of a single ur-corporation 50 years ago, all other corporations having gone extinct in bankruptcy or been acquired? Why is it so hard for corporations to keep their “culture” intact and retain their youthful lean efficiency, or if avoiding ‘aging’ is impossible, why copy themselves or otherwise reproduce to create new corporations like themselves? Instead, successful large corporations coast on inertia or market failures like regulatory capture/monopoly, while successful small ones worry endlessly about how to preserve their ‘culture’ or how to ‘stay hungry’ or find a replacement for the founder as they grow, and there is constant turnover. The large corporations function just well enough that maintaining their existence is an achievement .\n\nEvolution & the Price equation requires 3 things: entities which can replicate themselves; variation of entities; and selection on entities. Corporations have variation, they have selection—but they don’t have replication.\n\nCorporations certainly undergo selection for kinds of fitness, and do vary a lot. The problem seems to be that corporations cannot replicate themselves. They can set up new corporations, yes, but that’s not necessarily replicating themselves—they cannot clone themselves the way a bacteria can. When a bacteria clones itself, it has… a clone, which is difficult to distinguish in any way from the ‘original’. In sexual organisms, children still resemble their parents to a great extent. But when a large corporation spins off a division or starts a new one, the result may be nothing like the parent and completely lack any secret sauce. A new acquisition will retain its original character and efficiencies (if any). A corporation satisfies the Peter Principle by eventually growing to its level of incompetence, which is always much smaller than ‘the entire economy’. Corporations are made of people, not interchangeable easily-copied widgets or strands of DNA. There is no ‘corporate DNA’ which can be copied to create a new one just like the old. The corporation may not even be able to ‘replicate’ itself over time, leading to scleroticism and aging—but this then leads to underperformance and eventually selection against it, one way or another. So, an average corporation appears little more efficient, particularly if we exclude any gains from new technologies, than an average corporation 50 years ago, and the challenges and failures of the rare multinational corporation 500 years ago like the Medici bank look strikingly similar to challenges and failures of banks today.\n\nWe can see a similar problem with other large-scale human organizations: ‘cultures’. An idea seen sometimes is that cultures undergo selection & evolution, and as such, are made up of adaptive beliefs/practices/institutions, which no individual understands (such as farming practices optimally tailored to local conditions); even apparently highly irrational & wasteful traditional practices may actually be an adaptive evolved response, which is optimal in some sense we as yet do not appreciate (sometimes linked to “Chesterton’s fence” as an argument for status quo-ism).\n\nThis is not a ridiculous position, since occasionally certain traditional practices have been vindicated by scientific investigation, but the lenses of multilevel selection as defined by the Price equation shows there are serious quantitative issues with this: cultures or groups are rarely driven extinct, with most large-scale ones persisting for millennia; such ‘natural selection’ on the group-level is only tenuously linked to the many thousands of distinct practices & beliefs that make up these cultures; and these cultures mutate rapidly as fads and visions and stories and neighboring cultures and new technologies all change over time (compare the consistency of folk magic/medicine over even small geographic regions, or in the same place over several centuries). For most things, ‘traditional culture’ is simply flatout wrong and harmful and all forms are mutually contradictory, not verified by science, and contains no useful information, and—contrary to “Chesterton’s fence”—the older and harder it is to find a rational basis for a practice, the less likely it is to be helpful:\n\nChesterton’s meta-fence: “in our current system (democratic market economies with large governments) the common practice of taking down Chesterton fences is a process which seems well established and has a decent track record, and should not be unduly interfered with (unless you fully understand it)”.\n\nThe existence of many erroneous practices, and the successful diffusion of erroneous ones, is acknowledged by proponents of cultural evolution like Heinrich (eg. Heinrich provides several examples which are comparable to genetic drift spreading harmful mutations, and Primo Levi coined “the onion in the varnish” for such things), so the question here is one of emphasis or quantity: is the glass 1% full or 99% empty? It’s worth recalling the conditions for human expertise (Armstrong, Principles of Forecasting; Tetlock , Expert Political Judgment: How Good Is It? How Can We Know?; ed Ericsson, The Cambridge Handbook of Expertise and Expert Performance; Kahneman & Klein): repeated practice with quick feedback on objective outcomes in unchanging environments; these conditions are satisfied for relatively few human activities, which are more often rare, with long-delayed feedback, left to quite subjective appraisals mixed in with enormous amounts of randomness & consequences of many other choices before/after, and subject to potentially rapid change (and the more so the more people are able to learn). In such environments, people are more likely to fail to build expertise, be fooled by randomness, and construct elaborate yet erroneous theoretical edifices of superstition (like Tetlock’s hedgehogs). Evolution is no fairy dust which can overcome these serious inferential problems, which are why reinforcement learning is so hard.\n\nFor something like farming, with regular feedback, results which are enormously important to both individual and group survival, and relatively straightforward mechanistic cause-and-effect relationships, it is not surprising that practices tend to be somewhat optimized (although still far from optimal, as enormously increased yields in the Industrial Revolution demonstrate, in part by avoiding the errors of traditional agriculture & using simple breeding techniques) ; but none of that applies to ‘traditional medicine’, dealing as it does with complex self-selection, regression to the mean, and placebo effects, where aside from the simplest cases like setting broken bones (again, straightforward, with cause-and-effect relationship), hardly any of it works and one is lucky if a traditional remedy is merely ineffective rather than outright poisonous, and in the hardest cases like snake bites, it would be better to wait for death at home than waste time going to the local witch doctor.\n\nSo—just like corporations—‘selection’ of cultures happens rarely with each ‘generation’ spanning centuries or millennia, typically has little to do with how reality-based their beliefs tend to be (for a selection coefficient approaching zero), and if one culture did in fact consume another one thanks to more useful beliefs about some herb, it is likely to backslide under the bombardment of memetic mutation (so any selection is spent just purging mutations, creating a mutation-selection balance); under such conditions, there will be little long-term ‘evolution’ towards higher optima, and the information content of culture will be minimal and closely constrained to only the most universal, high-fitness-impact, and memetically-robust aspects.\n\nRL\n\nBlack Box vs White Box Optimization\n\nLet’s put it another way.\n\nImagine trying to run a business in which the only feedback given is whether you go bankrupt or not. In running that business, you make millions or billions of decisions, to adopt a particular model, rent a particular store, advertise this or that, hire one person out of scores of applicants, assign them this or that task to make many decisions of their own (which may in turn require decisions to be made by still others), and so on, extended over many years. At the end, you turn a healthy profit, or go bankrupt.\n\nSo you get 1 bit of feedback, which must be split over billions of decisions. When a company goes bankrupt, what killed it? Hiring the wrong accountant? The CEO not investing enough in R&D? Random geopolitical events? New government regulations? Putting its HQ in the wrong city? Just a generalized inefficiency? How would you know which decisions were good and which were bad? How do you solve the “credit assignment problem”?\n\nIdeally, you would have some way of tracing back every change in the financial health of a company back to the original decision & the algorithm which made that decision and compare that to the counterfactual, but of course this is impossible since there is no way to know who said or did what or even who discussed what with whom when or to know every possible counterfactual world and compare with an ideal company.\n\nThere would seem to be no general approach other than the truly brute force one of evolution: over many companies, have some act one way and some act another way, and on average, good decisions will cluster in the survivors and not-so-good decisions will cluster in the deceased. ‘Learning’ here works (under certain conditions—like sufficiently reliable replication—which in practice may not obtain) but is horrifically expensive & slow. By the same logic, there may be no better way to pay executives that to tie it to the stock performance: what a CEO does cannot be reduced to a few uncheatable numbers about how many widgets they carved a day or to any simple set of rules—CEOs exist to oversee everything else and decide things like strategy, and to set the culture from the top. A bad CEO can destroy a highly-successful company, and a good one boost it further, while following all rules. This will lead to absurdities like a CEO reaping rewards from things that “obviously” have nothing to do with them; but attempting to improve pay-for-performance methods leads to Nobel Prize-winning complications. No excuses, no justifications, no explanations of why it wasn’t one’s fault—just results. (“First prize is a Cadillac. Anyone wanna see second prize? Second prize is a set of steak knives. Third prize is you’re fired. Get the picture? You laughing now?”) Likewise, for teams, where agent effort can’t be easily observed and useful actions are unknown, it may be hard to do better than partnership-style profit sharing among team members.\n\nIn RL, this would correspond to black box/gradient-free methods, particularly evolutionary methods. For example, Salimans et al uses an evolutionary method in which thousands of slightly-randomized neural networks play an Atari game simultaneously, and at the end of the games, a new average neural network is defined based on the performance of them all; no attempt is made to figure out which specific changes are good or bad or even to get a reliable estimate—they simply run and the scores are what they are. If we imagine a schematic like ‘models → model parameters → environments → decisions → outcomes’, evolution collapses it to just ‘models → outcomes’; feed a bunch of possible models in, get back outcomes, pick the models with best outcomes. Brutally inefficient, like evolution, but brutally effective eventually.\n\nA more sample-efficient method would be something like REINFORCE, which Andrej Karpathy explains with an ALE Pong agent; what does REINFORCE do to crack the black box open a little bit? It’s still horrific and amazing that it works:\n\nSo here is how the training will work in detail. We will initialize the policy network with some W1, W2 and play 100 games of Pong (we call these policy “rollouts”). Lets assume that each game is made up of 200 frames so in total we’ve made 20,000 decisions for going UP or DOWN and for each one of these we know the parameter gradient, which tells us how we should change the parameters if we wanted to encourage that decision in that state in the future. All that remains now is to label every decision we’ve made as good or bad. For example suppose we won 12 games and lost 88. We’ll take all 200 × 12 = 2400 decisions we made in the winning games and do a positive update (filling in a +1.0 in the gradient for the sampled action, doing backprop, and parameter update encouraging the actions we picked in all those states). And we’ll take the other 200 × 88 = 17600 decisions we made in the losing games and do a negative update (discouraging whatever we did). And… that’s it. The network will now become slightly more likely to repeat actions that worked, and slightly less likely to repeat actions that didn’t work. Now we play another 100 games with our new, slightly improved policy and rinse and repeat.\n\nPolicy Gradients: Run a policy for a while. See what actions led to high rewards. Increase their probability.\n\nIf you think through this process you’ll start to find a few funny properties. For example what if we made a good action in frame 50 (bouncing the ball back correctly), but then missed the ball in frame 150? If every single action is now labeled as bad (because we lost), wouldn’t that discourage the correct bounce on frame 50? You’re right—it would. However, when you consider the process over thousands/millions of games, then doing the first bounce correctly makes you slightly more likely to win down the road, so on average you’ll see more positive than negative updates for the correct bounce and your policy will end up doing the right thing.\n\n…I did not tune the hyperparameters too much and ran the experiment on my (slow) Macbook, but after training for 3 nights I ended up with a policy that is slightly better than the AI player. The total number of episodes was approximately 8,000 so the algorithm played roughly 200,000 Pong games (quite a lot isn’t it!) and made a total of ~800 updates.\n\nThe difference here from evolution is that the credit assignment is able to use backpropagation to reach into the NN and directly adjust their contribution to the decision which was ‘good’ or ‘bad’; the difficulty of tracing out the consequences of each decision and labeling it ‘good’ is simply bypassed with the brute force approach of decreeing “all actions taken in an ultimately-successful game are good”, and “all actions are bad if the game is ultimately bad”. Here we optimize something more like ‘model parameters → decisions → outcomes’; we feed parameters in to get out decisions which then are assumed to cause the outcome, and reverse it to pick the parameters with the best outcomes.\n\nThis is still crazy, but it works, and better than simple-minded evolution: Salimans et al compares their evolution method to more standard methods which are fancier versions of the REINFORCE policy gradient approach, and this brutally limited use of backpropagation for credit assignment still cuts the sample size by 3–10x, and more on more difficult problems.\n\nCan we do better? Of course. It is absurd to claim that all actions in a game determine the outcome, since the environment itself is stochastic and many decisions are either irrelevant or were the opposite in true quality of whatever the outcome was. To do better, we can connect the decisions to the environment by modeling the environment itself as a white box which can be cracked open & analyzed, using a model-based RL approach like the well-known PILCO.\n\nIn PILCO, a model of the environment is learned by a powerful model (the non-neural-network Gaussian process, in this case), and the model is used to do planning: start with a series of possible actions, run them through the model to predict what would happen, and directly optimize the actions to maximize the reward. The influence of the parameters of the model causing the chosen actions, which then partially cause the environment, which then partially cause the reward, can all be traced from the final reward back to the original parameters. (It’s white boxes all the way down.) Here the full ‘models → model parameters → environments → decisions → outcomes’ pipeline is expressed and the credit assignment is performed correctly & as a whole.\n\nThe result is state-of-the-art sample efficiency: in a simple problem like Cartpole, PILCO can solve it within as little as 10 episodes, while standard deep reinforcement learning approaches like policy gradients can struggle to solve it within 10,000 episodes.\n\nThe problem, of course, with model-based RL such as PILCO is that what they gain in correctness & sample-efficiency, they give back in computational requirements: I can’t compare PILCO’s sample-efficiency with Salimans et al ’s ALE sample-efficiency or even Karpathy’s Pong sample-efficiency because PILCO simply can’t be run on problems all that much more complex than Cartpole.\n\nSo we have a painful dilemma: sample-efficiency can be many orders of magnitude greater than possible with evolution, if only one could do more precise fine-grained credit assignment—instead of judging billions of decisions based solely on a single distant noisy binary outcome, the algorithm generating each decision can be traced through all of its ramifications through all subsequent decisions & outcomes to a final reward—but these better methods are not directly applicable. What to do?\n\nTwo-Level Meta-Learning\n\n…the decisive phenomenon [in science is] that scientists criticize their theories and so kill them. Scientists try to eliminate their false theories, they try to let them die in their stead. The believer—whether animal or man—perishes with his false beliefs.\n\nKarl Popper (1968)\n\nCosma Shalizi, elsewhere, enjoys noting formal identities between natural selection and Bayesian statistics (especially particle filtering) and markets, where the population frequency of an allele corresponds to a parameter’s prior probability or starting wealth of a trader, and fitness differentials/profits correspond to updates based on new evidence, typically in the form of a multiplication. (See also Evstigneev et al /Lensberg & Schenk-Hoppé, Campbell, Czégel et al ; on a historical note, Galton invented something like ABC while trying to model evolution.) While a parameter may start with erroneously low prior, at some point the updates will make the posterior converge on it. (The relationship between populations of individual with noisy fixed beliefs, and Thompson sampling, is also interesting: Krafft. Can we see the apparently-inefficient stream of startups trying ‘failed’ ideas—and occasionally winding up winning big—as a kind of collective Thompson sampling & more efficient than it seems?) And stochastic gradient descent can be seen as secretly an approximation or variational form of Bayesian updates by estimating its gradients (because everything that works works because it’s Bayesian?) and of course evolutionary methods can be seen as calculating finite difference approximations to gradients…\n\nAnalogies between different optimization/inference models. (For more, see “Information Geometry”, John C. Baez, on unifying evolution, Bayesian inference, & gradient descent through information geometry.)\n\nModel\n\nParameter\n\nPrior\n\nUpdate\n\nEvolution\n\nAllele\n\nPopulation Frequency\n\nFitness Differential\n\nMarket\n\nTrader\n\nStarting Wealth\n\nProfit\n\nParticle Filtering\n\nParticles\n\nPopulation Frequency\n\nAccept/Reject Sample\n\nSGD\n\nParameter\n\nRandom Initialization\n\nGradient Step\n\nThis pattern surfaces in our other examples too. This two-level learning is analogous to meta-learning: the outer or meta-algorithm learns how to generate an inner or object-level algorithm which can learn most effectively, better than the meta-algorithm. Inner algorithms themselves can learn better algorithms, and so on, gaining power, compute-efficiency, or sample-efficiency, with every level of specialization. (“It’s optimizers all the way up, young man!”) It’s also analogous to cells in a human body: overall reproductive fitness is a slow signal that occurs only a few times in a lifetime at most, but over many generations, it builds up fast-reacting developmental and homeostatic processes which can build an efficient and capable body and respond to environmental fluctuations within minutes rather than millennia, and the brain is still superior with split-second situations. It’s also analogous to corporations in a market: the corporation can use whatever internal algorithms it pleases, such as linear optimization or neural networks, and evaluate them internally using internal metrics like “number of daily users”; but eventually, this must result in profits…\n\nThe central problem a corporation solves is how to motivate, organize, punish & reward its sub-units and constituent humans in the absence of direct end-to-end losses without the use of slow external market mechanisms. This is done by tapping into social mechanisms like peer esteem (soldiers don’t fight for their country, they fight for their buddies), selecting workers who are intrinsically motivated to work usefully rather than parasitically, constant attempts to instill a “company culture” with sloganeering or handbooks or company songs, use of multiple proxy measures for rewards to reduce Goodhart-style reward hacking, ad hoc mechanisms like stock options to try to internalize within workers the market losses, replacing workers with outsourcing or automation, acquiring smaller companies which have not yet decayed internally or as a selection mechanism (“acquihires”), employing intellectual property or regulation… All of these techniques together can align the parts into something useful to eventually sell…\n\nPain As Grounding\n\nSo with all that for background, what is the purpose of pain?\n\nThe purpose of pain, I would say, is as a ground truth or outer loss. (This is a motivational theory of pain with a more sophisticated RL/psychiatric grounding.)\n\nThe pain reward/loss cannot be removed entirely for the reasons demonstrated by the diabetics/lepers/congenital insensitives: the unnoticed injuries and the poor planning are ultimately fatal. Without any pain qualia to make pain feel painful, we will do harmful things like run on a broken leg or jump off a roof to impress our friends , or just move in a not-quite-right fashion and a few years later wind up paraplegics. (An intrinsic curiosity drive alone would interact badly with a total absence of painful pain: after all, what is more novel or harder to predict than the strange and unique states which can be reached by self-injury or recklessness?)\n\nIf pain couldn’t be removed, could pain be turned into a reward, then? Could we be the equivalent of Morsella’s mind that doesn’t experience pain, as it infers plans and then executes them, experiencing only more or less rewards? It only experience positive rewards (pleasure) as it runs across burning-hot sands, as this is the optimal action for it to be taking according to whatever grand plan it has thought of.\n\nPerhaps we could… but what stops Morsella’s mind from enjoying rewards by literally running in circles on those sands until it dies or is crippled? Morsella’s mind may make a plan and define a reward function which avoids the need for any pain or negative rewards, but what happens if there is any flaw in the computed plan or the reward estimates? Or if the plan is based on mistaken premises? What if the sands are hotter than expected, or if the distance is much further than expected, or if the final goal (perhaps an oasis of water) is not there? Such a mind raises serious questions about learning and dealing with errors: what does such a mind experience when a plan fails? Does it experience nothing? Does it experience a kind of “meta-pain”?\n\nConsider what Brand (The Gift of Pain again, pg191–197) describes as the ultimate cause of the failure of years of research into creating ‘pain prosthetics’, computerized gloves & socks that would measure heat & pressure in real-time in order to warn those without pain like lepers or diabetics: the patients would just ignore the warnings, because stopping to prevent future problems was inconvenient while continuing paid off now. And when electrical shockers were added to the system to stop them from doing a dangerous thing, Brand observed patients simply disabling it to do the dangerous thing & re-enabling it afterwards! Less exotically, consider professional athletes: promising careers end every day due to injuries, and one of the most important playing skills of a top athlete in the NBA or NFL is being able to not play.\n\nWhat pain provides is a constant, ongoing feedback which anchors all the estimates of future rewards based on planning or bootstrapping. It anchors our intelligence in a concrete estimation of bodily integrity: the intactness of skin, the health of skin cells, the lack of damage to muscles, joints sliding and moving as they ought to, and so on. If we are planning well and acting efficiently in the world, we will, in the long run, on average, experience higher levels of bodily integrity and physical health; if we are learning and choosing and planning poorly, then… we won’t. The badness will gradually catch up with us and we may find ourselves blind scarred paraplegics missing fingers and soon to die. A pain that was not painful would not serve this purpose, as it would merely be another kind of “tickling” sensation. (Some might find it interesting or enjoyable or it could accidentally become sexually-linked.) The perceptions in question are simply more ordinary tactile, kinesthetic, thermoreceptor, or other standard categories of perception; without painful pain, a fire burning your hand simply feels warm (before the thermal-perceptive nerves are destroyed and nothing further is felt), and a knife cutting flesh might feel like a rippling stretching rubbing movement.\n\nWe might say that a painful pain is a pain which forcibly inserts itself into the planning/optimization process, as a cost or lack of reward to be optimized. A pain which was not motivating is not what we mean by ‘pain’ at all. The motivation itself is the qualia of pain, much like an itch is an ordinary sensation coupled with a motivational urge to scratch. Any mental quality or emotion or sensation which is not accompanied by a demandingness, an involuntary taking-into-consideration, is not pain. The rest of our mind can force its way through pain, if it is sufficiently convinced that there is enough reason to incur the costs of pain because the long-term reward is so great, and we do this all the time: we can convince ourselves to go to the gym, or withstand the vaccination needle, or, in the utmost extremity, saw off a trapped hand to save our life. And if we are mistaken, and the predicted rewards do not arrive, eventually the noisy constant feedback of pain will override the decisions leading to pain, and whatever incorrect beliefs or models led to the incorrect decisions will be adjusted to do better in the future.\n\nBut the pain cannot and must not be overridden: human organisms can’t be trusted to simply ‘turn off’ pain and indulge an idle curiosity about cutting off hands. Note that we can kill ourselves by starvation or thirst, but we cannot kill ourselves by refusing to sleep, or have a heart beat, or breathe—unless one suffers from the (extremely lethal) central hypoventilation syndrome, that is. We are insufficiently intelligent, our priors insufficiently strong, our reasoning and planning too poor, and we must do too much learning within each life to do without pain.\n\nA similar argument might apply to the puzzle of ‘willpower’, ‘procrastination’. Why do we have such problems, particularly in a modern context, doing aught we know we should and doing naught we oughtn’t?\n\nOn the grave of the ‘blood glucose’ level theory, Kurzban et al (see later Shenhav et al ) erects an opportunity cost theory of willpower. Since objective physical measurements like blood glucose levels fail to mechanically explain poorer brain functionality or why strenuous activities like sports are ‘restful’ & reduce ‘burnout’, similar to the failure of objective physical measurements like lactate levels to explain why people are able to physically exercise only a certain amount (despite being able to exercise far more if properly motivated or if tricked), the reason for willpower running out must be subjective.\n\nTo explain the sugar-related observations, Kurzban et al suggest that the aversiveness of long focus and cognitive effort is a simple heuristic which creates a baseline cost to focusing for ‘too long’ on any one task, to the potential neglect of other opportunities, with the sugar interventions (such as merely tasting sugar water) which appear to boost willpower actually serving as proximate reward signals (signals, because the actual energetic content is nil, and cognitive effort doesn’t meaningfully burns calories in the first place), which justify to the underlying heuristic that further effort on the same task is worthwhile and the opportunity cost is minimal.\n\nThe lack of willpower is a heuristic which doesn’t require the brain to explicitly track & prioritize & schedule all possible tasks, by forcing it to regularly halt tasks—“like a timer that says, ‘Okay you’re done now.’” If one could override fatigue at will, the consequences can be bad. Users of dopaminergic drugs like amphetamines often note issues with channeling the reduced fatigue into useful tasks rather than alphabetizing one’s bookcase. In more extreme cases, if one could ignore fatigue entirely, then analogous to lack of pain, the consequences could be severe or fatal: ultra-endurance cyclist Jure Robič would cycle for thousands of kilometers, ignoring such problems as elaborate hallucinations, and was eventually killed while cycling. The ‘timer’ is implemented, among other things, as a gradual buildup of adenosine, which creates sleep homeostatic drive pressure and possibly physical fatigue during exercise (Noakes, Martin et al ), leading to a gradually increasing subjectively perceived ‘cost’ of continuing with a task/staying awake/continuing athletic activities, which resets when one stops/sleeps/rests. (Glucose might work by gradually dropping over perceived time without rewards.) Since the human mind is too limited in its planning and monitoring ability, it cannot be allowed to ‘turn off’ opportunity cost warnings and engage in hyperfocus on potentially useless things at the neglect of all other things; procrastination here represents a psychic version of pain.\n\nFrom this perspective, it is not surprising that so many stimulants are adenosinergic or dopaminergic , or that small children might especially struggle with mental fatigue (there is a world full of novel opportunities tempting them away), or that many anti-procrastination strategies (like Getting Things Done or the Procrastination Equation) boil down to optimizing for more rewards or more frequent rewards (eg. breaking tasks down into many smaller tasks, which can be completed individually & receive smaller but more frequent rewards, or thinking more clearly about whether something is worth doing): all of these would affect the reward perception itself, and reduce the baseline opportunity cost ‘pain’. This perspective may also shed light on depression , or on occupational burnout and why restorative hobbies are ideally maximally different from jobs and more miscellaneous observations like the lower rate of ‘hobbies’ outside the West: burnout may be a long-term homeostatic reaction to spending ‘too much’ time too frequently on a difficult not-immediately rewarding task despite earlier attempts to pursue other opportunities (perhaps tasks which would never be rewarding), which were always overridden, ultimately resulting in a total collapse ; and hobbies ought to be as different in location and physical activity and social structure (eg. a solitary programmer indoors should pursue a social physical activity outdoors as soulcraft) to ensure that it feels completely different for the mind than the regular occupation; and in places with less job specialization or fewer work-hours, the regular flow of a variety of tasks and opportunities means that no such special activity as a ‘hobby’ is necessary. A further analogy might be to communication: email and chat message overload, compared to ‘worse’ communication technologies, may reflect a lack of ‘pain’ to the sender—resulting in ‘damage’ to the overloaded recipient.\n\nPerhaps if we were superintelligent AIs who could trivially plan flawless humanoid locomotion at Hz taking into account all possible damages, or if we were emulated brains sculpted by endless evolutionary procedures to execute perfectly adaptive plans by pure instinct, or if we were simple amoeba in a Petri dish who had no real choices to make, there would be no need for a pain which was painful. And likewise, were we endlessly planning and replanning to the end of days, we should never experience akrasia, we should merely do what is necessary (perhaps not even experiencing any qualia of effort or deliberation, merely seeing events endlessly unfold as they always had to). But we are not. The pain keeps us honest. In the end, pain is our only teacher.\n\nKnuth\n\nyosefk observes that Chuck Moore’s systems designed using Forth & custom hardware/OS/userland are breathtakingly more efficient than ‘standard’ approaches would yield, and this is because Moore takes a global perspective and optimizes “end-to-end”: changing the language if that makes the OS simpler, changing the hardware if that’d make the text editor smaller, redefining the problem if necessary, and making large globally-coherent sets of changes that more myopic or constrained programmers could not or would not do. They are less ‘designed’ than evolved (but evolved by intelligence), and share similar traits: a general contempt for rigid abstraction or principles like “modularity”, a reliance on ‘coincidence’ for correctness, and incredible performance (in both terms of efficiency and in solving whatever the problem is). This approach makes for amazing custom ‘one-off’ systems; but these are inherently difficult to understand by lesser mortals, the general ‘just git gud’ approach unreplicable, and the system may not be modifiable by lesser mortals (even if the original designer could easily modify it or just rewrite it overnight to be brilliantly optimized for the new set of requirements). Similar issues bedevil Lisp systems & programmers: they can, and so they do.\n\nThis reminds me of Donald Knuth. Knuth is one of the most brilliant computer scientists ever, who does things like program an ALGOL compiler by himself, mostly in his head, for a summer job. He writes projects like the TeX system by sitting down and spending days thinking hard about it, creating a single program littered with poor software-engineering like global variables & GOTOs but which is nevertheless lightning-fast & almost bug-free. TeX is not his only programming language either, he has created others like METAFONT (for fonts) or MIX/MMIX (an assembly language & computer architecture to provide simple & timeless implementations of his TAOCP programs).\n\nPerhaps the most striking thing about these various languages is that everyone who uses them loves the quality of the output & what you can do with them, but hate the confusing, complicated, inconsistent, buggy experience of using them (to the extent that there is a whole cottage industry of people attempting to rewrite or replace TeX/LaTeX, typically copying the core ideas of how TeX does typesetting—the ‘box and glue’ paradigm of how to lay out stuff onto a page, the Knuth-Plass line-breaking, the custom font families etc—doing all that reimplementation work just so they don’t have to ever deal with the misery of TeX-the-language). Knuth himself, however, appears to have no more difficulty programming in his languages than he did writing 1960s assembler or designing fonts with 60 parameters. As he puts it, he ignores most programming techniques and just writes the right code, “Otherwise, lots of time is wasted on activities that I simply never need to perform or even think about.” If you need to make a complex use of TeX like typesetting an entire Bible, then Knuth says you should simply read it, and fork it however necessary. That should be easy—after all, that’s what he would do!\n\nHow do you write code as well as Don Knuth? The answer seems to be, well, ‘be as naturally gifted as Don Knuth’. A major omission in his œuvre is that he has never done major work on operating systems (which cannot be kept in one’s head), and his major effort in improving software engineering, “literate programming”, which treats programs as 1 long story to be told, fell stillborn from the presses. (Knuth, who has the privilege of an academic in being able to ship a program without any worries about maintenance or making a living, and just declaring something done, like TeX, perhaps does not appreciate how little real-world source code is like telling stories and instead readers decode them.)\n\nSo this is a bit of a problem for any attempts at making programming languages more powerful or more ergonomic, as well as for various kinds of ‘tools for thought’ like Douglas Engelbart’s intelligence amplification program. It is terribly easy for such powerful systems to be terribly hard to learn, and the ability to handle extreme levels of complexity can cripple one’s ability to remove complexity. (The ability to define all sorts of keyboard shortcuts & abbreviations invoking arbitrary code dynamically in your Lisp machine text editor is great until you ask ‘how am I going to remember all these well enough to save any time on net?’) Just as it tends to be easier to write your own incorrect code than to understand old correct code, it is easier for large groups of people to write repetitive concrete code case by case than to understand the abstract code that solves it all. Tasking people like Knuth or Engelbart to develop such things is a bit like asking a top sports player to be a coach: it’s not the same thing at all, and the very factors which made them so freakishly good at the sport may damage their ability to critique or improve—they may not be able to do much more than demonstrate how to do it well, and say, ‘now you do that too and git gud’.\n\nFrom an AI perspective, this is interesting because it suggests that AIs might be powerful even while coding with human-legible code. If the problem with the ‘Moore/Knuth approach’ is that you can’t clone him indefinitely to rewrite the system every time it’s necessary, then what happens with AIs which you can ‘just clone’ and apply exclusively to the task? Quantity has a quality all its own. (For a fictional example, see Vernor Vinge’s SF novel A Deepness in the Sky, where talented individuals can be put into a ‘Focused’ state, where they become permanently monomaniacally obsessed with a single technical task like rewriting computer systems to be optimal, and always achieve Knuth-level results; giving their enslavers de facto superpowers compared to rivals who must employ ordinary people. For a more real-world example, consider how Google fights bitrot & infrastructure decay: not by incredibly sophisticated programming languages—quite the opposite, considering regressions like Go—but employing tens of thousands of top programmers to literally rewrite all its code every few years on net, developing an entire parallel universe of software tools, and storing it all in a single giant repository to assist the endless global rewrites of the big ball of mud.)\n\nInternet Community Design\n\nIt’s been just a month [since Stable Diffusion was released]. What about in a year? I probably won’t be able to find my work out there because [the internet] will be flooded with AI art. That’s concerning.\n\nGreg Rutkowski, September 2022\n\nInternet community architecture can be seen as a bi-level optimization design too. There are fast and slow methods of interaction, and ideas or knowledge (or just ‘memes’) are created, varied, and selected on. These interactions happen inside discrete communities which are themselves created, varied, and grow. So, they are instances of multilevel selection.\n\nThe “warrens and plazas” interpretation of communities is a 2-level design. The classic example used to be Usenet and FAQs: the fast daily (or even minute by minute) discussion would happen, and knowledge would be distilled down into FAQ entries to save time, foster a higher level of discussion, and spread the knowledge outside of participants. A more contemporary example is Reddit: the fast flux of link submissions and comments can be distilled into “stickied” (permanent) links, and a simple ‘wiki’ system of editable pages (enabling FAQs and more). Some subreddits for niche interests (often gaming or medical-related) have built up considerable knowledge bases and greatly advanced their particular niche. Discord has done well in marrying the relatively slow IRC-style chat channels with the even faster-paced voice communication of gaming, while simultaneously supporting long-term use-cases through stickied (‘pinned’) comments which can contain complicated formatting like blockquotes & be edited, search of full channel histories, bots, allowing many channels/servers with complicated security permissions, etc. (It has done less well in enabling any of this to be archived or exported.)\n\nCounter-examples also exist. Many social networks are actively hostile to any kind of 2-level structure, emphasizing one level at the expense of another. The value of each time-scale can be seen in how social networks can thrive while targeting only a single time-scale. Facebook is moderately hostile to long or in-depth posts; they can exist, but no real support is given to them, capabilities like formatting are minimal to nonexistent, and the UI & all affordances are 100% oriented to ‘most recent first’. Slack chat is the evil twin of Discord: its free plan destroys history almost immediately, and should one pay through the nose for full Slack, one quickly discovers that it’s email but worse. Twitter goes further, providing essentially no support for any kind of longform at all, never mind editable wiki/FAQ pages; but just because a technology does not enable a use case doesn’t mean users don’t need it, it just means they’ll have to do it painfully and worse than if it did, and so Twitter users struggle with threads to provide some sort of slow structure to the usual nauseatingly evanescent stream of fluff tweets. Instagram & TikTok are even worse, and Snapchat makes no bones of trying to destroy even the possibility of a slow stream. YouTube enables slow content quite well, and is excellent at education; it seems to struggle with fast, though—comments were a notorious cesspit for many years, and chatting or interactive streaming is something it’s been trying to catch up with compared to pioneers like Twitch.\n\nWeird hybrid examples exist. Consider 4chan, famously the meme-maker to the Internet. A chan consists of flat ‘threads’ of comments one after another (any tree being implicit), ordered by most-recently-updated thread, with the last thread being deleted after a certain timeout; threads are nested within a general topic or ‘board’. 4chan threads typically move fast and disappear within days. At first glance, this might seem to preclude any kind of progress. But 4chan is nevertheless famous for incubating memes and projects over the course of many threads (all long since deleted by the final success). How? Part of it is that successful threads may export to other boards; then other boards export to slow sites like specialist wikis or other social media networks, like Twitter, Facebook, and Reddit. So there is a 3-level selection: a comment within a thread, interesting threads within the board, and interesting board contents within the Internet. Threads select brutally for memetic efficiency (the ticking clock makes chan threads almost literally a viral evolution lagoon), and while this selection is extremely error-prone and inaccurate, there are so many comments & memes, and fans of a meme will maintain personal archives and keep posting variants (being anonymous, they are free to keep posting without care or consequence), that the memetic virus can keep mutating and evolving until perhaps it starts percolating outwards. (And if it doesn’t, oh well, plenty more where that came from!)\n\nA broad perspective on this is to think of graphs of communities, where each node is a community of a certain size operating at a certain speed with differing norms about quality/topic/esthetics/anonymity etc. If we think of each community as generating & filtering memes, then there are tradeoffs between size, accuracy of filtering, and throughput. If you have 1 very large community, it will have extremely accurate selection on popularity (fitness) of a given meme, because it is averaging the assessments of almost everyone (despite steeply diminishing returns to spending more people to do selection); however, it will struggle to keep up with potential throughput and its queues will overflow creating a bottleneck impeding bursty collaboration of exciting new ideas, and where will new memes come from if slightly-inferior variants are harshly punished immediately compared to the current fit baseline? Over-exploitation will become boring, driving away users—at best, stasis; at worst, decadence, exhaustion, & collapse. If you have lots of tiny communities, they will undergo extreme levels of “genetic drift” due to randomness in popularity, and the fitness of their best meme will typically be quite poor; but on the other hand, it is likely that at least one of those communities has random-walked its way to something neat (if only you could figure out which one…) Depending on how these communities are connected, these neat new variants may be confined to a ghetto and eventually die out due to randomness (either themselves or perhaps the communities) if they can’t grow fast enough to reach fixation; but if you make them all hyper-connected, you may just wind up constructing the 1 large bottlenecked community again!\n\nThe architecture of speed and size is probably responsible for a lot of the ‘feel’ of social networking. Twitter, for example, is lauded for giving access to the latest by the greatest anywhere, but produces a certain exhaustion and apathy and chronic low-grade anxiety and lowest-common denominator humor, and this is the flipside: because it emphasizes only the latest, there is no progression or collaborative creation (people can advertise on Twitter to collaborate elsewhere, or ask specific questions, or learn about something on Twitter, but there is nothing like a “Twitter FAQ thread” or long-term collaboration on Twitter), and because it can be from anywhere, the norms are unpredictable and “context collapse” means an entire outside community could decide to coordinate to make you the target of the latest ‘5-minute hate’.\n\nPavlogiannis et al (“Construction of arbitrarily strong amplifiers of natural selection using evolutionary graph theory”) considers this sort of scenario and finds that good graph structures & distributions tend to look like a hierarchical “star” or “hub-and-spoke” (or perhaps the ubiquitous “bow-tie”). There are many small ‘local’ nodes at the periphery, which focus on ‘generating’ innovations, and these feed in a generally one-way direction into progressively larger nodes focused on ‘selecting’, which eventually reach a few ‘global’ nodes which are connected to all the peripheries again. (Like in biological evolution, the number of nodes or ‘populations’ can matter a lot, as does the history of the populations, which may have an ‘effective’ population count much smaller the visible one.)\n\nLarge masses of raw material, be they writing, or images, or videos, or sick skating techniques, are collaboratively written, proceeding from rough draft through editing to the final perfected version. As a kid I wondered vaguely how famous intellectuals could have “30 boxes of notebooks” or “20 volumes of collected letters” or “10 volumes of unpublished papers”—wasn’t writing and thinking hard, how did they have time for all that incessant note-taking and letter-writing, while still living and researching and actually writing the published work they were famous for? The answer, it turned out, is simply that writing is a sequential collaborative process: those letters and unpublished papers were part of a pipeline; it was not “letters vs books” but “letters into books”. A heuristic like the rule of three (“if you find yourself explaining something for the third time, write it up”) is about deciding what to promote up a level: the repetition implies that it’s important, and conveniently, three rough drafts are already available.\n\nThis will suddenly all sound eerily familiar: it is our old friend reinforcement learning and its explore-exploit tradeoff, with outer evolutionary losses and inner learned losses, all over again! Too small a batch size and you don’t learn anything; too large, and it takes an eternity to improve; too little exploration & too much greedy exploitation, one learns unnecessarily slowly & may get trapped in a local optimum, but too much exploration ensures one never learns anything before bouncing off to the next random point; breaking up into multiple agents and populations can cover more ground than a single uniform population but only if they are balanced properly and transfer improvements; hierarchical structure can enable deep exploration and modularity, where a monolithic structure flails around locally but can be done poorly; an evolutionary loss is extremely inefficient compared to a learned inner loss explicitly optimizing for proxy goals, yet, without the evolutionary loss, the proxy goals may be wrong; and so on. But with our graph to explore and amplify, and individual humans as myopically Bayesian agents, we discover our overall community looks like a giant Thompson sampling engine!\n\nSo this offers a general theory of Internet community design: one wants an architecture which is hierarchical, supporting a smooth flow of content from a wide variety of small peripheral nodes operating on fast time-scales with their own unique norms fostering social contagion of ambition with incentives for directed exploration of new niches or uncovered territory , upwards through a hierarchy of larger slower nodes with gradually more intense filtering & more conventional norms (including escalating reliance on reputation), to a final global central ‘arena’ where the best can duke it out for transmission back to all peripheral nodes. The design should not privilege a particular time-scale, and should enable linking and copying through the various time-scales; nodes should be constrained in size, and broken up if necessary to keep them at an efficient size.\n\nOne could imagine a Reddit which integrated chat, links, & wiki pages to create a smoother transition between nodes and directly support promotion:\n\na subreddit has a chat channel which defaults to anonymous and is not logged or archived, with the minimum possible moderation;\n\nblocks of fast-time chat (seconds to minutes), however, can be highlighted and right-clicked to automatically turn into a comment on a link or their own text post, ‘promoting’ them up one level to slower-time, where they can be discussed over hours to days;\n\nsuch comments, perhaps on some topic of sudden new interest, may be further selected, and transcluded into a new wiki page devoted to that topic (crude, but a starting point as a comment index), which can then be hand-edited later to add in additional commentary, links, citations, etc;\n\nthese pages become a long-term resource for that subreddit, and perhaps turn out to be of broader interest, being crossposted to other, bigger, subreddits,\n\nand, amazingly enough, eventually reach /r/all where it is pushed to all users as part of their default feed—adding a new thing to the global ‘common knowledge’, which (return to #1) some other subreddit chat might idly discuss for a while and then make an unexpected breakthrough, kicking off a new cycle."
    }
}
{
    "id": "yago_19690_3",
    "rank": 19,
    "data": {
        "url": "https://craftofcoding.wordpress.com/tag/donald-knuth/",
        "read_more_link": "",
        "language": "en",
        "title": "Donald Knuth – The Craft of Coding",
        "top_image": "https://s0.wp.com/i/blank.jpg",
        "meta_img": "https://s0.wp.com/i/blank.jpg",
        "images": [
            "https://craftofcoding.wordpress.com/wp-content/uploads/2021/06/plankalkul_tpk-1.jpg?w=732",
            "https://s2.wp.com/i/logo/wpcom-gray-white.png",
            "https://s2.wp.com/i/logo/wpcom-gray-white.png",
            "https://pixel.wp.com/b.gif?v=noscript"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2021-06-23T14:37:28+00:00",
        "summary": "",
        "meta_description": "Posts about Donald Knuth written by spqr",
        "meta_lang": "en",
        "meta_favicon": "https://s1.wp.com/i/favicon.ico",
        "meta_site_name": "The Craft of Coding",
        "canonical_link": "https://craftofcoding.wordpress.com/tag/donald-knuth/",
        "text": "In 1976 Donald E. Knuth and Luis Trabb Pardo produced a technical report titled “The Early Development of Programming Languages” [1]. In it they surveyed the evolution of high level programming languages from 1945-1958, describing each languages principal features, and comparing them using a particular algorithm which they called the “TPK algorithm”. Below is a reproduction of the program implemented in Plankalkül, with relevant components of the program highlighted. It provides one of the best (and simple) illustrations of a Plankalkül program [1].\n\nNow for a brief explanation. LIne 1 introduces the data types A2 which is basically an ordered pair comprised of an integer and a floating-point component. The lines in ② and ③ comprise the function f(t), while the lines in ④-⑦ comprise the main program TPK. In reality the program is only 7 lines in length, with each operation spanning several lines. Operations are performed on “Resultatwerte“, or output variables, “Variablen“, or input variables, and “Zwischenwerte” or intermediary variables.\n\nProcedure P1\n\nLines 2-4 indicate that P1 is a procedure that takes V0 (of type A∆1, i.e. floating-point) as input and produces R0 of the same type. Lines 5-7 perform the functions calculation.\n\nMain Program P2\n\nAt the start of the main program P2, lines 8-10 map V0 (of type 11×A∆1) into a result R0 (of type A2). This basically maps a vector of floating-points to a vector of ordered pairs. The W2(11) on line 11 specifies a for loop that iterates from n-1 down to 0. The notation on line 11, namely R10(x) implies the result R0 of applying P1 to x. Lines 15-18 basically mean:\n\nif Z0 > 400 then R0[10-i] = (i,+∞)\n\nLines 19-22 are similarly defined:\n\nif Z0 <= 400 then R0[10-i] = (i,+∞)\n\nThere was no else statement, hence the use of the bar over “Z0>400“, indicating negation.\n\nKnuth and Pardo provide a base algorithm for TPK implemented in Algol-60. I have reproduced the algorithm in Pascal for comparison to the Plankalkül program.\n\nprogram tpk; uses Math; var i : integer; y : real; a : array[0..10] of real; function f(t: real): real; begin f := sqrt(abs(t)) + 5 * power(t,3); end; begin for i:= 0 to 10 do read(a[i]); for i := 10 downto 0 do begin y := f(a[i]); if (y > 400) then write(i:4,' TOO LARGE') else write(i:4,' ',y:0:2); writeln end; end.\n\nKnuth, D.E., Pardo, L.T., “The Early Development of Programming Languages“, STAN-CS-76-562, Computer Science Department, Stanford University (1976).\n\nMost people treat electricity as an limitless resource. We use it to power up our mobile devices with very little thought to whether it is green or not. It tends to be cheap, so we don’t think twice about it. Programmers tend to treat CPU cycles, and memory in a similar way. It wasn’t always this way.\n\nFirst, an observation by Gordon Bell:\n\nThe cheapest, fastest, and most reliable components of a computer system are those that aren’t there.\n\nWhich in an ideal world would make programs omnipotent. But computers exist, and so do programs. Yet for the most part, programmers are warned off playing around with a programs efficiency. Consider Donald Knuth’s 1974 paper “Structured Programming with Goto statements” [1] in which he wrote:\n\nThere is no doubt that the grail of efficiency leads to abuse. Programmers waste enormous amounts of time thinking about, or worrying about, the speed of noncritical parts of their programs, and these attempts at efficiency actually have a strong negative impact when debugging and maintenance are considered. We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil.\n\nYet we should not pass up our opportunities in that critical 3%. A good programmer will not be lulled into complacency by such reasoning, he will be wise to look carefully at the critical code; but only after that code has been identified. It is often a mistake to make a priori judgments about what parts of a program are really critical, since the universal experience of programmers who have been using measurement tools has been that their intuitive guesses fail. (…)\n\nYet have programmers forgotten about efficiency altogether? Many people tend to forget that even in a techno-obsessed world where memory in one form or another is cheap, not all software is efficient. Ever downloaded a webpage, or app that just consumes copious amounts of resources? On mobile devices this can be devastating, draining the battery. Even in digital cameras, some functions are battery bleeders, and best used sparingly. Some algorithms are just impossible to make more efficient, some programmers write horrible code. The basics of program efficiency rely on understanding the algorithm and the environment it is to be used.\n\n[1] Donald Knuth, Structured Programming with go to Statements, JACM Computing Surveys, Vol 6, No. 4, Dec. 1974, p.268\n\nIn Donald Knuth’s 1974 paper Structured Programming with go to Statements¹, he makes the following statement:\n\n“There is no doubt that the grail of efficiency leads to abuse. Programmers waste enormous amounts of time thinking about, or worrying about, the speed of noncritical parts of their programs, and these attempts at efficiency actually have a strong negative impact when debugging and maintenance are considered. We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%. A good programmer will not be lulled into complacency by such reasoning, he will be wise to look carefully at the critical code; but only after that code has been identified.”\n\nThis was the early 1970s, a time where computers still had very little memory, so optimization was key in using all the available resources. Is code optimization still evil? Likely not as much as it once was, in the age of mobile and embedded systems which require efficient code, it way be as important as ever. For example, inefficient code could lead to premature battery drain on a mobile device. In code from inexperienced programmers, it often manifests itself as simple things such as redundancy due to the recomputation of common expressions, or loops containing loop independent expressions. For example, take the simple quadratic equation, which would be transformed into the following statements in C:\n\nr1 = (-b + sqrt(b * b - 4.0 * a * c)) / (2.0 * a); r2 = (-b - sqrt(b * b - 4.0 * a * c)) / (2.0 * a);\n\nThe problem with this code is that each expression contains elements that are repeated: the discriminant (√b²-4ac), and the redundant computation of the subexpression 2a. This code would be more efficient if written as:\n\ndenom = 2.0 * a; sdisc = sqrt(b * b - 4.0 * a * c); r1 = (-b + sdisc) / denom; r2 = (-b - sdisc) / denom;\n\nAs another example, consider the following piece of code:\n\nx = 3.4; for (i=0; i<100; i=i+1) y = y + a[i] * (x*x + 3.0*x + 2.0);\n\nHere the expression x²-3x+2 is independent of the loop variable i, but would still be independently calculated 100 times. This would be more efficient if replaced by:\n\nx = 3.4; z = (x*x + 3.0*x + 2.0); for (i=0; i<100; i=i+1) y = y + a[i] * z;\n\nIt is sometimes the small things that make all the difference, maybe not in languages such as C, but certainly in languages such as Python which are more susceptible to slow code.\n\n¹ACM Computing Surveys, 6(4), p.268, (1974)"
    }
}
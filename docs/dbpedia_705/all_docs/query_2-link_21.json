{
    "id": "dbpedia_705_2",
    "rank": 21,
    "data": {
        "url": "https://stackoverflow.com/questions/70402/why-is-quicksort-better-than-mergesort",
        "read_more_link": "",
        "language": "en",
        "title": "Why is quicksort better than mergesort?",
        "top_image": "https://cdn.sstatic.net/Sites/stackoverflow/Img/apple-touch-icon@2.png?v=73d79a89bded",
        "meta_img": "https://cdn.sstatic.net/Sites/stackoverflow/Img/apple-touch-icon@2.png?v=73d79a89bded",
        "images": [
            "https://www.gravatar.com/avatar/e4bac7ebd5f5a06c716f508af656e24b?s=64&d=identicon&r=PG",
            "https://i.sstatic.net/Jty8q.jpg?s=64",
            "https://www.gravatar.com/avatar/fdd630f72eef3790bfb4ef38d08c7f85?s=64&d=identicon&r=PG",
            "https://www.gravatar.com/avatar/?s=64&d=identicon&r=PG",
            "https://i.sstatic.net/ZNmEF.jpg?s=64",
            "https://www.gravatar.com/avatar/fddbb59c40bc4305b4954c95cc3dbc1f?s=64&d=identicon&r=PG",
            "https://www.gravatar.com/avatar/9ef256cd299f05b32798386a086c208f?s=64&d=identicon&r=PG",
            "https://www.gravatar.com/avatar/5e4103707f37d83ebd26d960c70948c2?s=64&d=identicon&r=PG",
            "https://www.gravatar.com/avatar/88dfd3b24ef593f3011381d427f95da5?s=64&d=identicon&r=PG&f=y&so-version=2",
            "https://www.gravatar.com/avatar/c81dee2ef36d2c0d115aebfc660b2f1f?s=64&d=identicon&r=PG",
            "https://www.gravatar.com/avatar/e7c7be744116c83a61b6cbc61f5ced18?s=64&d=identicon&r=PG",
            "https://graph.facebook.com/10153199803316745/picture?type=large",
            "https://www.gravatar.com/avatar/f902fdeef6ebee3ab55ef09f512b87f4?s=64&d=identicon&r=PG",
            "https://i.sstatic.net/kqgas.jpg?s=64",
            "https://www.gravatar.com/avatar/cf140e210335908720b26b01b7bff6c1?s=64&d=identicon&r=PG",
            "https://www.gravatar.com/avatar/a2d6e0ceb2a252b3ac0c011b9c6147d5?s=64&d=identicon&r=PG",
            "https://www.gravatar.com/avatar/ffa537cdf1542fcc73fca19521e1774b?s=64&d=identicon&r=PG",
            "https://www.gravatar.com/avatar/1091300cf9283f0f468ad024cea2504f?s=64&d=identicon&r=PG&f=y&so-version=2",
            "https://www.gravatar.com/avatar/1dda1e1c73f91d78278c1491c0598499?s=64&d=identicon&r=PG&f=y&so-version=2",
            "https://i.sstatic.net/2ZLGy.png?s=64",
            "https://www.gravatar.com/avatar/90664a3c93685f403171d58d13a3531e?s=64&d=identicon&r=PG",
            "https://www.gravatar.com/avatar/dd3f0d1dfd3f41118cd332dbdfa93314?s=64&d=identicon&r=PG",
            "https://www.gravatar.com/avatar/aaaacb8daa2d58de1afe2b0c73f616ef?s=64&d=identicon&r=PG",
            "https://lh4.googleusercontent.com/-wVd00r-Qktg/AAAAAAAAAAI/AAAAAAAAAL4/nt5UY5EguKk/photo.jpg?sz=64",
            "https://i.sstatic.net/UzF4Q.jpg?s=64",
            "https://www.gravatar.com/avatar/771622e9aa0ea7ce46fc41d8a9d92b8e?s=64&d=identicon&r=PG",
            "https://www.gravatar.com/avatar/dc6fd4818eee7b52fb030de806dbc2da?s=64&d=identicon&r=PG",
            "https://www.gravatar.com/avatar/88f777523e3731cf4e28a0f5b7dfe44e?s=64&d=identicon&r=PG",
            "https://www.gravatar.com/avatar/e84233388ae5b1d001f637ce4301a4f2?s=64&d=identicon&r=PG&f=y&so-version=2",
            "https://i.sstatic.net/CiS0e.jpg?s=64",
            "https://www.gravatar.com/avatar/d99068c78d44a26613180666edaaed7d?s=64&d=identicon&r=PG",
            "https://www.gravatar.com/avatar/4f8baec8a56c0e01f53175c89a294b90?s=64&d=identicon&r=PG",
            "https://www.gravatar.com/avatar/3c789553228a241f6615ca695bb90b34?s=64&d=identicon&r=PG&f=y&so-version=2",
            "https://stackoverflow.com/posts/70402/ivc/f0bf?prg=6d571c23-954a-4b3c-82a5-9ea9a0797c5b"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2008-09-16T08:37:52",
        "summary": "",
        "meta_description": "I was asked this question during an interview. They're both O(nlogn) and yet most people use Quicksort instead of Mergesort. Why is that?",
        "meta_lang": "en",
        "meta_favicon": "https://cdn.sstatic.net/Sites/stackoverflow/Img/favicon.ico?v=ec617d715196",
        "meta_site_name": "Stack Overflow",
        "canonical_link": "https://stackoverflow.com/questions/70402/why-is-quicksort-better-than-mergesort",
        "text": "Quicksort has O(n2) worst-case runtime and O(nlogn) average case runtime. However, it’s superior to merge sort in many scenarios because many factors influence an algorithm’s runtime, and, when taking them all together, quicksort wins out.\n\nIn particular, the often-quoted runtime of sorting algorithms refers to the number of comparisons or the number of swaps necessary to perform to sort the data. This is indeed a good measure of performance, especially since it’s independent of the underlying hardware design. However, other things – such as locality of reference (i.e. do we read lots of elements which are probably in cache?) – also play an important role on current hardware. Quicksort in particular requires little additional space and exhibits good cache locality, and this makes it faster than merge sort in many cases.\n\nIn addition, it’s very easy to avoid quicksort’s worst-case run time of O(n2) almost entirely by using an appropriate choice of the pivot – such as picking it at random (this is an excellent strategy).\n\nIn practice, many modern implementations of quicksort (in particular libstdc++’s std::sort) are actually introsort, whose theoretical worst-case is O(nlogn), same as merge sort. It achieves this by limiting the recursion depth, and switching to a different algorithm (heapsort) once it exceeds logn.\n\nAs many people have noted, the average case performance for quicksort is faster than mergesort. But this is only true if you are assuming constant time to access any piece of memory on demand.\n\nIn RAM this assumption is generally not too bad (it is not always true because of caches, but it is not too bad). However if your data structure is big enough to live on disk, then quicksort gets killed by the fact that your average disk does something like 200 random seeks per second. But that same disk has no trouble reading or writing megabytes per second of data sequentially. Which is exactly what mergesort does.\n\nTherefore if data has to be sorted on disk, you really, really want to use some variation on mergesort. (Generally you quicksort sublists, then start merging them together above some size threshold.)\n\nFurthermore if you have to do anything with datasets of that size, think hard about how to avoid seeks to disk. For instance this is why it is standard advice that you drop indexes before doing large data loads in databases, and then rebuild the index later. Maintaining the index during the load means constantly seeking to disk. By contrast if you drop the indexes, then the database can rebuild the index by first sorting the information to be dealt with (using a mergesort of course!) and then loading it into a BTREE datastructure for the index. (BTREEs are naturally kept in order, so you can load one from a sorted dataset with few seeks to disk.)\n\nThere have been a number of occasions where understanding how to avoid disk seeks has let me make data processing jobs take hours rather than days or weeks.\n\nI would like to add to the existing great answers some math about how QuickSort performs when diverging from best case and how likely that is, which I hope will help people understand a little better why the O(n^2) case is not of real concern in the more sophisticated implementations of QuickSort.\n\nOutside of random access issues, there are two main factors that can impact the performance of QuickSort and they are both related to how the pivot compares to the data being sorted.\n\n1) A small number of keys in the data. A dataset of all the same value will sort in n^2 time on a vanilla 2-partition QuickSort because all of the values except the pivot location are placed on one side each time. Modern implementations address this by methods such as using a 3-partition sort. These methods execute on a dataset of all the same value in O(n) time. So using such an implementation means that an input with a small number of keys actually improves performance time and is no longer a concern.\n\n2) Extremely bad pivot selection can cause worst case performance. In an ideal case, the pivot will always be such that 50% the data is smaller and 50% the data is larger, so that the input will be broken in half during each iteration. This gives us n comparisons and swaps times log-2(n) recursions for O(n*logn) time.\n\nHow much does non-ideal pivot selection affect execution time?\n\nLet's consider a case where the pivot is consistently chosen such that 75% of the data is on one side of the pivot. It's still O(n*logn) but now the base of the log has changed to 1/0.75 or 1.33. The relationship in performance when changing base is always a constant represented by log(2)/log(newBase). In this case, that constant is 2.4. So this quality of pivot choice takes 2.4 times longer than the ideal.\n\nHow fast does this get worse?\n\nNot very fast until the pivot choice gets (consistently) very bad:\n\n50% on one side: (ideal case)\n\n75% on one side: 2.4 times as long\n\n90% on one side: 6.6 times as long\n\n95% on one side: 13.5 times as long\n\n99% on one side: 69 times as long\n\nAs we approach 100% on one side the log portion of the execution approaches n and the whole execution asymptotically approaches O(n^2).\n\nIn a naive implementation of QuickSort, cases such as a sorted array (for 1st element pivot) or a reverse-sorted array (for last element pivot) will reliably produce a worst-case O(n^2) execution time. Additionally, implementations with a predictable pivot selection can be subjected to DoS attack by data that is designed to produce worst case execution. Modern implementations avoid this by a variety of methods, such as randomizing the data before sort, choosing the median of 3 randomly chosen indexes, etc. With this randomization in the mix, we have 2 cases:\n\nSmall data set. Worst case is reasonably possible but O(n^2) is not catastrophic because n is small enough that n^2 is also small.\n\nLarge data set. Worst case is possible in theory but not in practice.\n\nHow likely are we to see terrible performance?\n\nThe chances are vanishingly small. Let's consider a sort of 5,000 values:\n\nOur hypothetical implementation will choose a pivot using a median of 3 randomly chosen indexes. We will consider pivots that are in the 25%-75% range to be \"good\" and pivots that are in the 0%-25% or 75%-100% range to be \"bad\". If you look at the probability distribution using the median of 3 random indexes, each recursion has an 11/16 chance of ending up with a good pivot. Let us make 2 conservative (and false) assumptions to simplify the math:\n\nGood pivots are always exactly at a 25%/75% split and operate at 2.4*ideal case. We never get an ideal split or any split better than 25/75.\n\nBad pivots are always worst case and essentially contribute nothing to the solution.\n\nOur QuickSort implementation will stop at n=10 and switch to an insertion sort, so we require 22 25%/75% pivot partitions to break the 5,000 value input down that far. (10*1.333333^22 > 5000) Or, we require 4990 worst case pivots. Keep in mind that if we accumulate 22 good pivots at any point then the sort will complete, so worst case or anything near it requires extremely bad luck. If it took us 88 recursions to actually achieve the 22 good pivots required to sort down to n=10, that would be 4*2.4*ideal case or about 10 times the execution time of the ideal case. How likely is it that we would not achieve the required 22 good pivots after 88 recursions?\n\nBinomial probability distributions can answer that, and the answer is about 10^-18. (n is 88, k is 21, p is 0.6875) Your user is about a thousand times more likely to be struck by lightning in the 1 second it takes to click [SORT] than they are to see that 5,000 item sort run any worse than 10*ideal case. This chance gets smaller as the dataset gets larger. Here are some array sizes and their corresponding chances to run longer than 10*ideal:\n\nArray of 640 items: 10^-13 (requires 15 good pivot points out of 60 tries)\n\nArray of 5,000 items: 10^-18 (requires 22 good pivots out of 88 tries)\n\nArray of 40,000 items:10^-23 (requires 29 good pivots out of 116)\n\nRemember that this is with 2 conservative assumptions that are worse than reality. So actual performance is better yet, and the balance of the remaining probability is closer to ideal than not.\n\nFinally, as others have mentioned, even these absurdly unlikely cases can be eliminated by switching to a heap sort if the recursion stack goes too deep. So the TLDR is that, for good implementations of QuickSort, the worst case does not really exist because it has been engineered out and execution completes in O(n*logn) time.\n\nThis is a pretty old question, but since I've dealt with both recently here are my 2c:\n\nMerge sort needs on average ~ N log N comparisons. For already (almost) sorted sorted arrays this gets down to 1/2 N log N, since while merging we (almost) always select \"left\" part 1/2 N of times and then just copy right 1/2 N elements. Additionally I can speculate that already sorted input makes processor's branch predictor shine but guessing almost all branches correctly, thus preventing pipeline stalls.\n\nQuick sort on average requires ~ 1.38 N log N comparisons. It does not benefit greatly from already sorted array in terms of comparisons (however it does in terms of swaps and probably in terms of branch predictions inside CPU).\n\nMy benchmarks on fairly modern processor shows the following:\n\nWhen comparison function is a callback function (like in qsort() libc implementation) quicksort is slower than mergesort by 15% on random input and 30% for already sorted array for 64 bit integers.\n\nOn the other hand if comparison is not a callback, my experience is that quicksort outperforms mergesort by up to 25%.\n\nHowever if your (large) array has a very few unique values, merge sort starts gaining over quicksort in any case.\n\nSo maybe the bottom line is: if comparison is expensive (e.g. callback function, comparing strings, comparing many parts of a structure mostly getting to a second-third-forth \"if\" to make difference) - the chances are that you will be better with merge sort. For simpler tasks quicksort will be faster.\n\nThat said all previously said is true: - Quicksort can be N^2, but Sedgewick claims that a good randomized implementation has more chances of a computer performing sort to be struck by a lightning than to go N^2 - Mergesort requires extra space\n\nIn merge-sort, the general algorithm is:\n\nSort the left sub-array\n\nSort the right sub-array\n\nMerge the 2 sorted sub-arrays\n\nAt the top level, merging the 2 sorted sub-arrays involves dealing with N elements.\n\nOne level below that, each iteration of step 3 involves dealing with N/2 elements, but you have to repeat this process twice. So you're still dealing with 2 * N/2 == N elements.\n\nOne level below that, you're merging 4 * N/4 == N elements, and so on. Every depth in the recursive stack involves merging the same number of elements, across all calls for that depth.\n\nConsider the quick-sort algorithm instead:\n\nPick a pivot point\n\nPlace the pivot point at the correct place in the array, with all smaller elements to the left, and larger elements to the right\n\nSort the left-subarray\n\nSort the right-subarray\n\nAt the top level, you're dealing with an array of size N. You then pick one pivot point, put it in its correct position, and can then ignore it completely for the rest of the algorithm.\n\nOne level below that, you're dealing with 2 sub-arrays that have a combined size of N-1 (ie, subtract the earlier pivot point). You pick a pivot point for each sub-array, which comes up to 2 additional pivot points.\n\nOne level below that, you're dealing with 4 sub-arrays with combined size N-3, for the same reasons as above.\n\nThen N-7... Then N-15... Then N-32...\n\nThe depth of your recursive stack remains approximately the same (logN). With merge-sort, you're always dealing with a N-element merge, across each level of the recursive stack. With quick-sort though, the number of elements that you're dealing with diminishes as you go down the stack. For example, if you look at the depth midway through the recursive stack, the number of elements you're dealing with is N - 2^((logN)/2)) == N - sqrt(N).\n\nDisclaimer: On merge-sort, because you divide the array into 2 exactly equal chunks each time, the recursive depth is exactly logN. On quick-sort, because your pivot point is unlikely to be exactly in the middle of the array, the depth of your recursive stack may be slightly greater than logN. I haven't done the math to see how big a role this factor and the factor described above, actually play in the algorithm's complexity.\n\nOne of the reason is more philosophical. Quicksort is Top->Down philosophy. With n elements to sort, there are n! possibilities. With 2 partitions of m & n-m which are mutually exclusive, the number of possibilities go down in several orders of magnitude. m! * (n-m)! is smaller by several orders than n! alone. imagine 5! vs 3! *2!. 5! has 10 times more possibilities than 2 partitions of 2 & 3 each . and extrapolate to 1 million factorial vs 900K!*100K! vs. So instead of worrying about establishing any order within a range or a partition,just establish order at a broader level in partitions and reduce the possibilities within a partition. Any order established earlier within a range will be disturbed later if the partitions themselves are not mutually exclusive.\n\nAny bottom up order approach like merge sort or heap sort is like a workers or employee's approach where one starts comparing at a microscopic level early. But this order is bound to be lost as soon as an element in between them is found later on. These approaches are very stable & extremely predictable but do a certain amount of extra work.\n\nQuick Sort is like Managerial approach where one is not initially concerned about any order , only about meeting a broad criterion with No regard for order. Then the partitions are narrowed until you get a sorted set. The real challenge in Quicksort is in finding a partition or criterion in the dark when you know nothing about the elements to sort. That is why we either need to spend some effort to find a median value or pick 1 at random or some arbitrary \"Managerial\" approach . To find a perfect median can take significant amount of effort and leads to a stupid bottom up approach again. So Quicksort says just a pick a random pivot and hope that it will be somewhere in the middle or do some work to find median of 3 , 5 or something more to find a better median but do not plan to be perfect & don't waste any time in initially ordering. That seems to do well if you are lucky or sometimes degrades to n^2 when you don't get a median but just take a chance. Any way data is random. right. So I agree more with the top ->down logical approach of quicksort & it turns out that the chance it takes about pivot selection & comparisons that it saves earlier seems to work better more times than any meticulous & thorough stable bottom ->up approach like merge sort. But"
    }
}
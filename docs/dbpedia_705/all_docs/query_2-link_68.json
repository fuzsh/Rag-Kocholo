{
    "id": "dbpedia_705_2",
    "rank": 68,
    "data": {
        "url": "https://arxiv.org/html/2402.18659v1",
        "read_more_link": "",
        "language": "en",
        "title": "Large Language Models and Games: A Survey and Roadmap",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://arxiv.org/html/extracted/5438418/graphics/aipeople_2.png",
            "https://arxiv.org/html/extracted/5438418/graphics/1001nights_steam_screenshot.jpg",
            "https://arxiv.org/html/extracted/5438418/graphics/infinite-craft.jpg",
            "https://arxiv.org/html/extracted/5438418/graphics/gpt3_sokoban.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            "Large Language Models",
            "Digital Games",
            "Video Games",
            "Survey",
            "Generative Text",
            "Gameplaying",
            "Procedural Content Generation",
            "Generative AI."
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "License: arXiv.org perpetual non-exclusive license\n\narXiv:2402.18659v1 [cs.CL] 28 Feb 2024\n\nLarge Language Models and Games:\n\nA Survey and Roadmap\n\nRoberto Gallotta11{}^{1}start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT, Graham Todd22{}^{2}start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT, Marvin Zammit11{}^{1}start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT, Sam Earle22{}^{2}start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT, Antonios Liapis11{}^{1}start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT, Julian Togelius22{}^{2}start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT and Georgios N. Yannakakis11{}^{1}start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT\n\n1: Institute of Digital Games, University of Malta, Msida, Malta\n\n2: Tandon School of Engineering, New York University, New York, USA\n\nroberto.gallotta@um.edu.mt, gdrtodd@nyu.edu, marvin.zammit@um.edu.mt, se2161@nyu.edu, antonios.liapis@um.edu.mt, julian@togelius.com, georgios.yannakakis@um.edu.mt\n\nAbstract\n\nRecent years have seen an explosive increase in research on large language models (LLMs), and accompanying public engagement on the topic. While starting as a niche area within natural language processing, LLMs have shown remarkable potential across a broad range of applications and domains, including games. This paper surveys the current state of the art across the various applications of LLMs in and for games, and identifies the different roles LLMs can take within a game. Importantly, we discuss underexplored areas and promising directions for future uses of LLMs in games and we reconcile the potential and limitations of LLMs within the games domain. As the first comprehensive survey and roadmap at the intersection of LLMs and games, we are hopeful that this paper will serve as the basis for groundbreaking research and innovation in this exciting new field.\n\nIndex Terms:\n\nLarge Language Models, Digital Games, Video Games, Survey, Generative Text, Gameplaying, Procedural Content Generation, Generative AI.\n\n1 Introduction\n\nFive years ago, autoregressive language modeling was a somewhat niche topic within natural language processing. Training models to simply predict text based on existing text was considered something of primarily theoretical interest, though it might have applications as writing support systems. This changed drastically in 2019 when the GPT-2 model was released [1]. GPT-2 demonstrated convincingly that transformer models trained on large text corpora could not only generate surprisingly high-quality and coherent text, but also that the text generation could be controlled by carefully prompting the model. Subsequent developments, including larger models, instruction fine-tuning, reinforcement learning from human feedback [2], and the combination of these features in ChatGPT in late 2022, turbocharged interest in large language models (LLMs). It suddenly appeared that LLMs could do almost anything‚Äîas long as both problem and solution could be formulated as text.\n\nLLMs are currently a very active research field, where researchers are both focused on improving the capabilities of LLMs while reducing their compute and memory footprint, and on understanding and learning to harness the capabilities of existing LLMs. Informed opinions on the ultimate capabilities of LLM technology vary widely, from those who see such models as ‚Äúsparks of AGI‚Äù [3] to those who see them as largely doing approximate retrieval from a lossy compression of the internet [4].\n\nGames, including board games and video games, serve both as a source of important benchmarks for AI research and as an important application area for AI techniques [5]. Almost any game utilizes some kind of AI technology, and we are currently in an exploratory phase where the developers and researchers try to figure out how to best make use of recent advances in AI [6]. One might also make the argument that video game design and video game technology might underlie much of the development of future human-computer interfaces and, for lack of a better word, the metaverse.\n\nIn this paper, we set out to chart the impact LLMs have had on games and games research, and the impact they are likely to have in the near- to mid-term future. We survey existing work from both academia and (mostly independent) game creators that use LLMs with and for games. This paper does not set out to capture modern advances in LLM technology or algorithms for training LLMs. Not only do such resources exist [7], but the breakneck speed of technical advances in this field will likely make our writeup obsolete in a year or so. Instead, we focus on work that leverages LLMs in games and propose a range of roles that the LLM can take in the broader ecosystem of games (both within the game and beyond). We lay out promising future directions for efforts to use LLMs in games, and discuss limitations (both technical and ethical) that should be addressed for a brighter future of LLM research in games.\n\n2 A Note on Terminology\n\nThis paper concerns the intersection between games, including board games, video games, and other types of games people play, and large language models. But what exactly is an LLM?\n\nBroadly speaking, an LLM is a model that is trained on text so as to be able to reproduce text in response to other text. But this definition is overly broad, as it would include Shannon‚Äôs original nùëõnitalic_n-gram models from 1946 [8], rudimentary recurrent neural nets from the early 1990s [9], and the Tegic T9 text prediction system that would help you write text messages with your Nokia 3210. For readers who are unfamiliar with such technologies, they were ubiquitous in 90‚Äôs mobile phones.\n\nWhat distinguishes LLMs from other text generative models is that they are large. But which model size is considered large enough? LLMs became a well-recognized term with the introduction of OpenAI‚Äôs GPT-2 network, released in 2019 [1]. GPT stands for Generative Pretrained Transformer, where transformer is a type of neural network introduced in 2017 [10]. This model became very influential because of what was perceived as a quantum leap in output quality compared to previous models. The various versions of GPT-2 have between 117 millions and 1.5 billions of parameters. Because of the association between the term LLM and the GPT-class of models, we will use the size of GPT-2 as a soft cutoff on the type of models we consider as LLMs; we are concerned with models of few hundred million parameters or more. While language models could in principle be based on various architectures, including LSTM networks, the current LLM landscape is dominated by variants of the transformer architecture; in this survey we rely primarily on this type of model architecture.\n\nIt is important to note that LLMs are by no means limited to the GPT family of models. There is by now a large variety of LLMs of varying size and capabilities, including open-source models such as Mistral [11] and the Llama [12] family, which can be fine-tuned and analyzed in various ways and run locally, and even be embedded in games‚Äô runtimes.\n\nOne could also argue that the definition is somewhat narrow, as many modern LLMs are multimodal models, meaning that they can take as input and/or produce as output modalities other than text. In particular, many modern LLMs can process and produce images. This is often achieved through combining the core transformer network with a convolutional network for input and a diffusion model for output. Examples include GPT-4V [13] and the open-source Llava [14]. In this paper, we consider large multimodal models (LMMs) [13] as long as they retain their ability to both consume and produce text.\n\nThis survey will not concern itself with AI and machine learning techniques that are not LLMs as defined above. In particular, we will not be covering the large literature on game playing and content generation using machine learning methods [15] that is not using textual input and output. We will, however, occasionally mention some of that work when relevant, in particular to help provide historical context.\n\n3 Roles of LLMs in Games\n\nPast attempts at a typology for AI in games focused on three roles the AI can take in a game: to play a game, to design a game, or to model the (human) players [5]. LLMs are typically presented as conversational agents, which often lead the public to give them anthropomorphic qualities‚Äîsuch as reasoning and creativity. Therefore, we follow these trends when considering the roles an LLM can be called to play within the game or within the game development process. An LLM can operate within the game as a player (replacing a human player while imitating their goals), as a non-player character such as an enemy or interlocutor, as an assistant providing hints or handling menial tasks for a human player, as a Game Master controlling the flow of the game, or hidden within the games‚Äô ruleset (controlling a minor or major mechanic of the game). There are however other roles an LLM can play outside of the game‚Äôs runtime, such as a designer for the game (replacing a human designer) or as an assistant to a human designer. Finally, the LLM can interface with a player or an audience in different ways, acting as a commentator of an ongoing play session (during runtime) or a reteller of past game events in some narrative form (outside runtime). Some of these roles (autonomous player, autonomous designer) are prominent in the broader AI and games research [5] and LLM research has targeted them extensively, while some of the other roles have been toyed with in exploratory research. The following sections present the roles themselves, surveying research undertaken for each role, while we identify gaps and opportunities for future research in Section 4.\n\n3.1 Player\n\nHow can an LLM play a game? Fundamentally, language model players require some transformation from their typical output space (i.e. sequences of tokens) into the input space of the game. In addition, aspects of the game and its current state must be provided to the LLM in some form in order for it to play at any kind of reasonable level. Depending on the game itself, these mappings might be intuitive or complex. We identify three general classes of games to which LLM players are well suited: (a) games where states and actions can be compactly represented as sequences of abstract tokens, (b) games where the main input and output modalities are natural language, and (c) games for which external programs can control player actions via an API.\n\nThe first class of games mostly includes turn-based board games (e.g. Chess), since the discrete set of board positions and moves is more easily transformed into a compact representation (e.g. Portable Game Notation) than, for instance, a first-person shooter. By tokenizing sequences of moves taken from a game database, the problem of action selection can then be mapped to the standard autoregressive learning objective on which LLMs are trained ‚Äì predicting the next move given the context of those that preceded it. Chess [16, 17, 18], Go [19], and Othello [20] have all been used as testbeds for LLM players in this way. However, board games are not the only kind of game that can be represented as token sequences: the generalist GATO [21] agent can play a variety of Atari games at human or near-human levels by processing visual inputs as sequences of pixel values in raster order. Pixel values are interleaved with separator tokens and previous actions, allowing the model to accurately predict the appropriate game action in a dataset of human play traces. It is possible that continued improvement in Transformer models that capture both spatial and visual dynamics [22, 23] could allow for a similar approach to scale to even more complex games. However, such approaches require large datasets of gameplay video that may be comparatively more difficult to collect. In addition, we note that the reliance on human gameplay traces as the basis for learning may make it more difficult for an LLM player to reach super-human performance without leaps in terms of reasoning and generalization, which we revisit in the limitations of LLMs (see Section 5).\n\nThe second class of games most obviously includes text adventure games such as Zork (Infocom, 1977), where game states are presented as natural language descriptions and the game is already equipped with a parser to handle natural language responses. This means that LLMs can be queried for game actions in a way that still leverages their large-scale pretraining on natural language text. The earliest application of LLMs to these kinds of text games is CALM [24], a GPT-2 system finetuned on a dataset of human gameplay transcripts collected from a variety of text-based adventure games. The model is trained to predict the natural language string provided by human players given the context of previous states and actions and information about the avatar (e.g. their inventory). To actually play a game, the trained language model generates multiple candidate actions and deep reinforcement learning (RL) is used to optimize a policy that selects actions from among the candidates. At the time of its publication, this RL component was necessary because the LLM alone was not capable of generalizing well to unseen games or situations [24]. However, a more recent investigation of ChatGPT as a Zork player has indicated that LLM performance is increasing [25]. In a preliminary experiment, Yao et al. [24] show that the performance of ChatGPT can approach that of existing algorithms for text game playing, as long as a human interlocutor remains in the loop to assist the model (e.g. by reminding it of actions it has already tried). However, there is obviously much room for improvement in directly applying LLMs to text games in this way. Additionally, the ability for LLMs to play entirely novel, niche, or unseen text games (especially important given the likelihood that such systems encounter walkthroughs or playtraces of popular text games during their training) remains largely unexplored.\n\nBeyond text adventure games, the most well-known application of LLM players in board games is CICERO [26] for playing the deal-making and subterfuge game Diplomacy (Avalon Hill Games, 1976). Diplomacy is well-positioned for natural language instruction, as players can freely converse in private messages in order to arrange plans, negotiate, or deceive. CICERO builds from a pre-trained LLM and is fine-tuned on a large corpus of Diplomacy transcripts. Throughout the game, samples from the model are sent to other players and the various dialogue transcripts are collected to condition the potential action. CICERO is further trained to condition its outputs on specific game intents (inferred from the transcripts and added as additional context during training). In order to select an action, CICERO uses as ‚Äústrategic reasoning module‚Äù that predicts the actions of other players, using a value and policy function learned from self-play. Diplomacy is an interesting game in part because the action space is split between natural language utterances and a more standard set of moves on a discrete game board, and CICERO demonstrates how an LLM can be integrated as part of a larger system for high-level play.\n\nFinally, we consider games for which a robust API exists. This is less of a kind of game in the sense of its style or mechanics, and more a fact about its popularity or its ease of implementation. An API is an important attribute because it allows LLMs to act as players not by directly generating actions, but by producing programs that act as policies. Improvements in the code-generation abilities of LLMs have allowed them to write small programs that can produce actions given game states without further intervention from the model. For instance, the VOYAGER system [27] leverages the code-generation abilities of GPT-4 to play Minecraft (Mojang Studios, 2011) by interacting with the popular Mineflayer API. Using a sophisticated chain of prompts, VOYAGER generates blocks of code that leverage calls to the API in order to execute high-level ‚Äúskills‚Äù (e.g. ‚ÄúAttack nearest zombie‚Äù) that are automatically converted into low-level game inputs (e.g. mouse movements and key presses). GPT-4 is also used as a high-level goal generator and planner, which in turn informs the code generation. This approach has proved very successful, with VOYAGER being the first automated system to complete a variety of in-game Minecraft challenges. The results are impressive and indicate that generating action-producing programs may be a more efficient way to leverage latent LLM knowledge than direct action sampling. However, VOYAGER does benefit substantially from the availability of a robust API and vast amounts of internet discussion for its target game of Minecraft. As with the analysis of ChatGPT on Zork, the ability of this approach to generalize to less popular or entirely unseen games remains to be seen.\n\n3.2 Non-Player Characters\n\nNon-player characters (NPCs) are agents which exist in virtual game worlds but whose actions are not directly controlled by the players. NPCs exist to enrich the player‚Äôs experience and deepen immersion by adding to the world‚Äôs ambiance and making it more believable [29]. NPCs may serve as pets, allies, enemies , merchants, quest givers, or bystanders. Therefore, they have different agency even from AI-controlled players, and their goal is never to win. This makes designing AI for NPCs interesting [5], while LLMs can provide unique advantages in this task. Their ability to ‚Äúunderstand‚Äù game-world settings allows LLMs to adapt their responses to match these settings. It has been shown that LLMs are able to role-play through different scenarios [30], thereby highlighting their potential to provide a more flexible and apt tool to emulate human behavior. We identify two ways in which LLMs can control NPCs: (a) through their dialogue, and (b) through their behavior. Behavior relates to in-game action selection, discussed in Section 3.1; however, we note that the heuristics and goals of such behavior is different than an AI player trying to win the game.\n\nLLMs are naturally suited for natural language conversation, and as NPC dialogue systems they can generate dynamic and contextually appropriate responses based on player input. This makes interactions with NPCs more engaging and realistic, reduces repetitive discourse and provides a more explorative experience within the game [31]. LLMs can engage the players in the gameworld‚Äôs narrative as foreground NPCs, background NPCs, or narrator NPCs. We discuss narrator LLMs as commentators in Section 3.4 whereas we cover the other two NPC types here. Foreground NPCs form part of the overarching narrative of the game, or one of its sub-narratives. They may be enemies, allies, information-givers, quest-givers, or item-providers. Their dialogue is heavily constrained by the scope of the narrative, their role within it, and the player actions. Foreground NPCs‚Äô text generation process via LLMs must consider the overall context of the game and the interaction with the player and keep track of events transpiring in the playthrough. This raises concerns regarding the memory capacity of LLMs, as well as the impact of possible hallucinations, i.e. plausible but false statements [32]; we revisit these limitations in Section 5.\n\nBackground NPCs populate the majority of the virtual world but are not part of any specific task the player is carrying out. Their purpose is to make the environment more believable and act independently of the players [31]. Since such NPCs‚Äô presence is purely decorative and their dialogue is essentially small talk, their dialogue generation is less constrained, perhaps bound only by the identity of the speaker and their background. That said, their believability hinges on their ability to maintain the illusion that they have their own agency in the world and can interact with it [33].\n\nStudies have shown that multiple agents are able to follow game rules and engage in game playing [34, 35], with different models consistently exhibiting their own aptitudes and weaknesses when applied to specific roles. This ability to interact within constraints is useful to instill believable behaviors in foreground and background NPCs, grounding their actions and dialogue within the rules of the game environment. Other work has focused more on the conversational and story-writing abilities of LLMs, such as the creation of dialogue between multiple characters, each having their unique personality, whilst following a consistent plot. One such example is the use of LLMs to generate a South Park (Comedy Central, 1997) episode [36] with multiple characters within a well-known setting. There are limitations to this approach, primarily that LLMs perform something like a theatrical improvisation, rather than acting as an actor studying a part [30]. Through this unconstrained process, the LLM is prone to hallucinations which do not fit the desired scenario. This volatility can be mitigated by providing the LLM not only with the conversation history but also with the current state of the environment, such as the items within it and their affordances, as well as the other characters and their corresponding actions. This method has been successfully employed to create conversational NPCs in text-based games [37], but may also be extended to other scenarios or to cover the use of LLMs as active or interactive narrators.\n\n3.3 Player Assistant\n\nA somewhat less explored role for LLMs in games is that of a player assistant: an interactive agent intended to enrich or guide the player experience in some way. This could be in the form of a sequence of tutorial-style tips, a character that does not causally interact with the game world at all, or an agent that is able to interact with the game world at a similar level as the player. Existing games make use of player assistants in different ways. For example, in The Sims (Electronic Arts, 2000) a disembodied assistant provides tips specific to the game context via dialogue boxes. Civilization VI (Firaxis Games, 2016) uses different assistants with a visual depiction to provide similar advice suggesting the best build option according to their idiosyncratic heuristic; they may thus alleviate some decision-making from the player. In management games, AI may automate menial tasks such as assigning jobs to a planet‚Äôs population in Stellaris (Paradox Interactive, 2016); this assistance reduces cognitive load from the player, but the player can always micro-manage this task if they wish.\n\nLLMs will likely be appealing as player assistants given their potential expressive and conversational capacities. Coupled with either LLM-powered or heuristic-based methods for finding the best policy or action given the current game context (see Section 3.1), an LLM-powered player assistant can construct the suggested action and the explanation as a natural language utterance delivered by a disembodied or embodied agent, expressed with a corresponding sentiment, and manifested through its body stance, gestures and facial expression. Similarly, LLMs may assist the player by undertaking some minor tasks in the game via a tailored smaller role as ‚Äúplayer‚Äù within that smaller task description (see Section 3.1). The potential of LLM-powered player assistants is not explored in current research, to the best of our knowledge. We highlight the potential of this future application in Section 4.\n\n3.4 Commentator/Reteller\n\nWe view LLMs to be ideally suited as commentators or retellers. Here, we identify these roles as an agent that produces and narrates a sequence of events, for the benefit of either human players or spectators. Such an agent may consider only in-game events and in-game context, acting as an in-game entity such as a sports commentator in FIFA (EA Sports, 1993), or also consider out-of-game events and context such as the player (their actions, strategies, motivations, etc.). The reteller [38] exclusively narrates past events‚Äîoften grouped into a concise ‚Äúchunk‚Äù such as a game session (i.e. based on out-of-game context) or a quest (i.e. based only on in-game context). The commentator may be narrating current, ongoing events which have not been concluded, similar to a streamer concurrently discussing their current actions (including out-of-game context) or a sportscaster in an in-progress sports game such as FIFA.\n\nThe vision of automated ‚Äúlet‚Äôs play‚Äù-style commentary generation is not new. It was proposed in [39] and implemented via classical machine learning methods, to limited success. In [40], an LSTM with text, vision and game-state input was trained to generate characters for a commentary script in a racing game. Results of this approach featured repetitive and context-irrelevant generated text. LSTMs were also used by [41] to generate text, at a character level, for Getting Over It With Bennett Foddy (Bennett Foddy, 2017), a challenging side-scrolling climbing game.\n\nLLMs for commentary are also explored by Renella and Eger [42], who argue that LLMs could assist games streamers (e.g. on Twitch) while the streamer multitasks gameplay with audience interaction. The authors develop a pipeline for automatically commenting upon League of Legends (Riot Games, 2009) games. They take a multi-phased approach, training a model on hand-annotated data to recognize key events, then prompting ChatGPT to generate zero-shot commentary on these events in the style of a particular (known) fictional character, and finally sending the generated text through the FakeYou API to be voiced in the timbre of this same character. For example, once the event detection model has identified an enemy double kill in a particular frame, ChatGPT responds in the style of Rick Sanchez, from Rick and Morty (Cartoon Network, 2013): ‚ÄúWhat the heck?! That enemy team just got a double kill! I can‚Äôt believe it! They must be pretty good! I better watch out for them!‚Äù An additional loop buffers detected events‚Äîfor example, delaying commentary on a double kill in case it should escalate into a triple kill, or prioritizing among a quick barrage of events‚Äîand prompts ChatGPT to generate random fillers, such as thanking (fictional) new subscribers.\n\nDespite the existence of the aforementioned studies, research on LLMs as game commentators remains rather limited. The appeal is obvious: simulation-based games of emergent narrative already generate rich narrative histories, and are remixed by human players to produce secondary content that is often popular in its own right. In principle, LLMs could be used to generate more succinct retellings or highlight reels of these game events. Prompting current LLMs for stories, without any further specification of style or substance, tends to produce output that feels generic. Past events recorded in simulation games could ultimately provide specificity and narrative coherence to these outputs. Exploring more concepts beyond automating streamer commentary, such as assisting streamers via LLM commentary of the audiences‚Äô reactions rather than the in-game actions, remains unexplored. We revisit this along other future applications in Section 4.\n\n3.5 Game Master\n\nA Game Master (GM) in table-top role-playing games (TTRPGs) is the person who creates the plot of a game, its characters, and narrative. GMs wear many hats during the course of the game session [43], but also before and after the session by preparing and adapting adventures and stories and checking in with the group [44]. Digital games have mostly prescripted stories or level progressions, and their players have a restricted range of affordances, compared to TTRPG players whose actions are only limited by their imagination. Similarly, the story told around the table can take any direction. Since human GMs mostly communicate about the gameworld, story, game state and action resolutions via natural language (although props such as maps, miniatures, hand-outs are also common), the potential of LLMs as a GM is often mentioned both in research circles and TTRPG discussion boards [45]. LLMs as GMs also open the potential for solo play, while a TTRPG requires at least one player and the human GM.\n\nOne of the first notable text-based adventures managed by a fine-tuned version of GPT-2 is AI Dungeon [46]. AI Dungeon is an online interactive chat-based storytelling application where the player takes actions through semantic input alone. The LLM continues the story based on the player‚Äôs input, in the fashion of a human GM. The game has evolved since its creation to make use of more recent LLM models, which the player can choose from before starting a play session. Different game world settings are also offered, and players are also able to share the stories they create. Similar games have emerged online ever since , and a freely available code repository, Kobold AI Client , allows a local or remote installation of a client for such LLM-run games. Some of these games also use Stable Diffusion text-to-image models [47] to generate visuals accompanying different parts of the narrative.\n\nIn lieu of replacing a human GM, LLMs have also been employed as GM assistants. CALYPSO [48] is a set of tools running on a Discord server which the GM can query either to generate random encounters, brainstorm ideas, or alternatively chat with a fictional character in a Dungeons & Dragons (TSR, 1974) TTRPG setting. CALYPSO highlights that hallucinations of GPT-3 can have both positive effects when it generates plausible details not included in descriptions published in the original game manual (e.g. the shapes of creatures‚Äô eyes) and also negative effects when the created details are outright incorrect (e.g. describing the wings of a canonically wingless creature). In addition, the model‚Äôs preconditioning to avoid racial bias was found to occasionally prevent it from generating racial details of fantasy creatures in the game. Other work used smaller GPT models to improvise in-game conversations [49] by monitoring and transcribing verbal exchanges between the GM and the players, and attempting to generate appropriate responses. This example was integrated into ‚ÄúShoelace‚Äù [50], which is itself a GM assisting tool helping with content lookup by creating a node-based plan of the game narrative and encounters. The versatility of LLMs given their ability to rapidly process text input paves the way for their integration into the multitude of existing tools and aids for human GMs.\n\n3.6 Game Mechanic\n\nGames can also be built around a specific mechanic that relies on LLMs, similar to the AI-based game design patterns identified in [52]. An obvious mechanic revolves around the social interactions facilitated by LLM-powered conversational NPCs. In this vein, [53] has employed LLMs to populate a virtual village with 25 characters, enabling them to communicate and engage in social behavior within a sandbox environment. Players were able to interact with these agents using a text-based interface. The environment state and actions of each agent were stored in a language-based format and summarized in order to retain knowledge for each agent when prompting for its actions. This led to emerging believable social interactions, such as the agents spontaneously inviting other agents to a party which one of them was organizing. Similarly, GoodAI are developing the AI people video game which operates as a sandbox simulation where LLM-powered NPCs ‚Äúinteract with each other and their environment, forming relationships and displaying emotions‚Äù [28]. The player can interact with the agents via natural language chat, triggering reactions and potentially disrupting the relationship between NPCs (see Fig. 1).\n\nNatural language interactions form a natural pool of mechanics to build games around, such as gamifying users‚Äô attempts at jailbreaking LLMs [54]. The game 1001 nights, depicted in Figure 2, exemplifies this by having an LLM co-create a story from human prompts, where the player‚Äôs objective is to try and steer the story to include specific keywords in order for the main character, Scheherazade, to turn these into tangible items in aid of her escape [51]. Similarly, Gandalf challenges the player to trick an LLM into revealing a password. The game increases the difficulty of the task as levels progress by adjusting the prompt specifications, such as forcing the LLM to re-examine its generated response to ensure it does not include the password.\n\nAnother strength of LLMs is language synthesis, which is leveraged by Infinite Craft . Infinite Craft is an ‚Äúalchemy‚Äù game, in which the player combines elements to produce new ones (see Fig. 3). In Infinite Craft, the player begins with a set of core elements (water, fire, wind and earth). But while the former have a set of interactions defined manually by the designer, Infinite Craft prompts LLaMA 2 [12] to imagine the product of the combination of these elements [55]. Judging from gameplay, it appears that for each distinct combination, LLaMA is prompted to produce the result only once, with the product stored in a database for future reference. Thus seemingly anything in the language model‚Äôs vocabulary might ‚Äúemerge‚Äù from the combination of these elements, including all 50 states , ‚ÄúDream‚Äù , and the fictional ‚ÄúSuper Stonedosaurus Tacosaurus Rex‚Äù . On occasion, the model can choose to return one of the combined elements, or refuse to combine (in particular very lengthy or complex) elements.\n\n3.7 Automated Designer\n\nA key role of AI in games [5] is the algorithmic generation of game content such as levels and visuals, or even entire games. Unlike a Game Master who creates a game via natural language‚Äîmeant to exist in the ‚Äútheater of the mind‚Äù of the players‚Äîthe aim of procedural content generation (PCG) is to create content intended for use in a digital game and thus it is required to satisfy certain constraints such as playability and aesthetic quality.\n\nAny PCG method that is trained on available content corpora fits under the Procedural Content Generation via Machine Learning (PCGML) paradigm [15]. Strictly speaking the original PCGML framework of 2018 did not consider LLMs; instead it relied on machine learning methods such as autoencoders and LSTMs. However, important challenges of PCGML remain when considering LLMs for PCG: notably, the reliance on high-quality, machine-readable datasets from human-authored levels. While some datasets exist for arcade game levels [56], for most games the content remains both unavailable and protected by intellectual property (IP) laws. We revisit this issue in Section 6.\n\nPrior work in PCG has demonstrated that tile-based game levels can be reliably generated with sequence-based prediction models (e.g. LSTMs) from a modest set of examples, by treating such levels as linear sequences of tile types in raster order [57]. More recently, modern LLMs have been leveraged in a similar way to generally greater success. In [58], a GPT-2 model was fine-tuned on a large dataset of Sokoban (Thinking Rabbit, 1982) levels and, at test time, samples from the model to produce novel puzzles (see Figure 4). Interestingly, their results indicate that while the GPT-2 model struggles when the fine-tuning dataset is restricted in size, GPT-3 (and, presumably, larger models released since then) are better able to accommodate limited training sets.\n\nA similar approach, MarioGPT, trains a GPT-2 model on a relatively small dataset of Super Mario Bros (Nintendo, 1985) levels [60]. MarioGPT overcomes the issue of data sparsity by using the initial dataset as the starting point for an evolutionary algorithm. Existing levels are selected and then sections of the level are mutated by sampling from the GPT-model and then correcting the border between the re-generated section and the rest of the level with a similarly-trained BERT (i.e. bi-directional) model [61]. This approach produces a large and diverse set of playable levels, despite starting from less than 20 levels.\n\nBoth aforementioned GPT-based level generation approaches also show the promise of incorporating natural language instructions to produce conditional level generators, either by prefixing game levels in the training dataset with desired level characteristics [58] or by embedding user instructions and allowing the model to attend to the embedding during generation [60]. It seems likely that more sophisticated techniques such as reinforcement learning with human feedback [2] could produce level generators that are even more capable of accommodating user guidance.\n\n3.8 Design Assistant\n\nAn AI for design assistance can provide several benefits to the creative process. Depending on the type of tool, type of AI, and type of creative process, the AI can minimize development time and cost, reduce human effort, support collaboration among members of a design team or elicit a user‚Äôs creativity [62]. So far, in games, most of the AI-powered design assistant tools focus on autocompleting a human‚Äôs in-progress design [63] or providing many possible suggestions for the designer to consider [64, 65, 66, 67]. Based on the level of control the AI co-creator [68] has on the process, we identify three levels of assistance:\n\n‚Ä¢\n\nConceptual assistance where the AI provides high-level guidance which is not game-ready, requiring that the designer adapts and curates the AI output in a way that fits their own vision and the constraints of the game.\n\n‚Ä¢\n\nProcedural assistance where the AI is part of the creative process and through interaction with the user can produce increasingly more final versions of the intended artifact. Here, AI is expected to understand the context of the game for which the content is intended, in order to provide meaningful assistance. However, the AI does not need to produce a final, playable artifact but could instead simply provide the next creative step for discussion with the designer [69]. Moreover, the designer is ultimately responsible for curating and adapting the generated content, as well as deciding when the co-creative process is completed [70].\n\n‚Ä¢\n\nProduction assistance where the AI must create the final artifact based on user requirements. This is the closest level to PCG (see Section 3.7), but is different in that the designer remains in control and can refine their specification or reject a created artifact (versus an autonomous generator which directly sends content to the player). As expected, however, the AI operates in a much more constrained space in this scenario as it must account for all other game mechanics (the design of which are presumed finalized) and designer goals which are somehow encoded or presumed via learned designer models [71].\n\nOne can argue that existing interfaces with LLMs, Foundation Models and Large Multimodal Models act as design assistants. The designer provides their specifications and receives one (in LLMs) or multiple (in AI image generators) suggestions that they can refine further. Many creatives report using such interfaces for brainstorming and concept development [72], including game developers [73]. However, the applicability of LLMs as design assistants is somewhat limited, reverting only to conceptual assistance. Similarly, their potential for refining an existing idea (i.e. offering procedural assistance) is underexplored, as we discuss in Section 4.\n\nConceptual assistance is thus the easiest for LLMs, and is the first case explored in games. In [74], design assistance is envisioned in a tool that combines the game description provided by the user with existing knowledge of similar games to suggest possible game features to a designer. The features suggested are fairly generic, few-word guidelines (e.g.: ‚Äúlearn new combat‚Äù) which would need extensive design effort and creativity to transform into an implementable and coherent game design.\n\nSince production assistance is also close to traditional PCG pipelines, it is also understandably explored for games. In [75], GPT-3 generates levels from a prompt that describes the level‚Äôs features (e.g. width and height) while a human curates and edits the results to ensure playability. This curated set of levels is then used for further fine-tuning, potentially automating the generative process. Instead of generating the final artifact directly, in [76] the LLM is employed as a parser that extrapolates from user requests, expressed in natural language, the high level parameters (level difficulty and size, type of sustainability problem, and simulation goal metrics) required by the PCG system to generate the final game. A collection of candidate game levels are generated from a single user request. Each candidate is then evaluated by an RL agent, and the best game is presented to the user via the Unity game engine.\n\nAs noted above, so far research has focused on either LLMs for conceptual assistance (putting significant onus on a human designer) or as production assistance (leveraging a human designer as curator). The conversational nature of LLMs, however, seems particularly well-suited for procedural assistance when designing content; we revisit this missed opportunity in Section 4.\n\n4 A Roadmap for Future Applications of LLMs in Games\n\nThe previous section attempted to group current research in LLMs and games into a typology focused on the roles an LLM is asked to play. As part of this exercise, we identified a number of roles that have been heavily researched. Unsurprisingly, the role of player and automated designer have received attention: this matches the general trends within AI and Games research more broadly [5]. Following general trends in Game AI for playing or generating content, LLM-based approaches are likely to flourish via community events, benchmarks and competitions, with first steps already being taken in this direction [77]. Based on the roles listed in Section 3, we identify below some gaps found in the literature, and lay out possible research directions that leverage the power of LLMs in new ways.\n\nWhile academic interest in design assistance within games has blossomed in the last decade, we find that the potential of LLMs has so far been underutilized. LLM design assistants either ask too much of a human designer in terms of creative interpretation and actual development [74] or too little, demoting them to content curator [75]. Past research in mixed-initiative systems [68] assumes a more co-creative initiative from both human and machine, and the power of LLMs as conversational agents matches the original vision of a creative dialogue between initiatives [70]. Therefore, a promising unexplored direction lies in a more procedural assistance (see Section 3.8) where the LLM not only produces output but also reasons about it to the human designer. LLMs seem especially well-suited for this task, as the context is retained and the designer can iteratively refine past products that the LLM has generated. However, concerns of LLMs‚Äô limited memory may arise (see Section 5) in long-term design process. On the other hand, iterative refining is not as straightforward for other state-of-the-art technology such as LMMs, despite some promising results via e.g. InstructPix2Pix [78]. It is expected that such applications will raise new challenges in terms of hallucinations, explainability [79], capturing or modeling designer intent [71], and more. We discuss such challenges further in Section 6.\n\nWhile we identified player assistance as an important role that LLMs can play, we have not yet found any work that targets any aspect of this. The conversational ability of LLMs make them ideally suited for tutorial writing or hint-giving, especially in short snippets as provided e.g. by a conversational agent. However, it is important to note that LLMs often hallucinate or overfit to the corpus they have been trained on, and may be challenged, for instance, to summarize or lookup specific rules given a game manual. Similar limitations were identified when using an LLM as assistant to a human Game Master [49], where the LLM could not find elements in the pre-written adventure to highlight when asked a question about the scene. Other technologies (as simple as a database search query) could be used instead, with the LLM undertaking only the task of converting the found information into a natural language utterance. Beyond mere hint-giving, however, an LLM could also act as a more hands-on player assistant, taking over more trivial tasks (such as managing minutiae of one city in a strategy game). This is also powerful for Game Master assistance, as the LLM can keep track of locations visited and NPCs met, or looking up rules. In both cases, addressing the issue of hallucinations and consistency will need to be addressed, which we review in Section 5.\n\nAnother role seemingly well-suited for LLMs that has received limited attention is that of commentator or reteller. Work so far has focused on automating the commentary of streamers or eSport casters [42]. While this direction is still largely uncharted, there are more directions that could leverage LLMs for streamer assistance rather than automation (and replacement). Rather than narrate events occurring within the game (or video stream), LLMs can summarize the audience interactions and engagement levels‚Äîthus acting as a commentator not of the game but of the audience watching it. This could allow a human streamer to better keep track of topics discussed in the chat, and engage as needed without having to read every comment. While this has been identified as a research direction for AI already [80], it has not yet been implemented. Under the role of streamer assistance, issues of explainability of the LLM‚Äôs commentary would become pertinent (e.g. to address one audience member by name); we revisit this in Section 5.\n\nFinally, it is worth noting that one pillar of AI in games research is missing altogether from LLM research and applications surveyed: that of modeling players [5]. LLMs appear to offer promising, yet admittedly unexplored, methods for affective game computing [81]. At first glance, this is not particularly surprising: while player modeling often relies on supervised machine learning algorithms, it is not as clear how this can be achieved via text input or output. In principle, an LLM could predict affective state transitions such as ‚Äúthe game is more engaging now‚Äù and thereby adapt the game environment to elicit a supposedly more engaging experience for the player. Learning such transitions builds on the experience-driven procedural content generation paradigm [82] but with an LLM acting as the player experience model. We therefore envision the fine tuning of LLMs so that they learn to represent and infer player experience transitions based on in-game observations and demonstrations of experience. As highlighted in Section 5, however, current LLMs struggle to capture user intent during conversation‚Äîlet alone more ill-defined concepts such as players‚Äô emotion or engagement [83]. Current datasets on affect in games are formatted as continuous or categorical variables, often fluctuating over time [84], which would be challenging to format as text without processing. While perhaps using language as input or output for the player model requires some innovative pre-processing or more advanced LLM technologies, the underlying transformer architecture and attention-based algorithms show promise already. We expect more research on player modeling powered by transformers, such as leveraging behavior transformers [85] to imitate human playtraces grouped by playstyle [86].\n\nTo wrap up, we believe that every role an LLM could be called to play in (or around) a game identified in Section 3 could benefit from additional attention. This technology remains nascent, and changes are forthcoming which may address several limitations we identified both in the above paragraphs and, more extensively, in Section 5. The natural language capabilities (especially for text generation) make LLMs ideal conversational assistants (for a player, a designer, a game master, or a streamer). The ability of LLMs to consume and reason from text corpora also opens new possibilities for automated design moving beyond tile-based level generation (which needs carefully crafted corpora) and more towards open-ended content such as game narratives [87, 88, 89, 90] or even game design documents. The potential of LLMs in that regard is already voiced by many evangelists in the field, but research on actual implementation of such ideas and on addressing the IP concerns they may raise (see Section 6) are still forthcoming.\n\n5 Limitations of LLMs in games\n\nLarge language models have exciting potential for video games, but they also come with inherent limitations. Mainly, LLMs suffer from hallucinations [32, 91], meaning that they will output plausible but false statements simply because they are a probable sequence of words. Hallucinations are inevitable given how the world is described to the machine [92]; LLMs lack grounding, so the text they generate is detached from constraints of reality. Yet LLMs always ‚Äúact‚Äù confidently in their responses, even when wholly mistaken. Beyond hallucinations, LLMs suffer from factual errors [93, 94, 95], outputting responses that are wrong even though the LLM has access to information that proves otherwise. In the context of video games, these limitations affect certain applications of LLMs more than others, for example NPCs may hallucinate quests that do not exist in the game, or a player assistant may provide suggestions to the user based on wrong assumptions.\n\nAnother limitation of using LLMs in video games is that LLMs sometimes struggle to capture user intent. This is especially evident with expressions of sarcasm [96]. The ability to capture user intent is important for applications of LLMs that converse directly with the player. An LLM that correctly understands user intent then also correctly understands the context of the conversation, which is not always the case for current LLMs. Many LLMs make errors in correctly understanding user requests [97], and clarifying to the LLM multiple times results in a frustrating experience for the user. This limitation is most relevant to cases where the LLM is in direct conversation with the user, e.g. as design assistant, player assistant or game master. Depending on how much the LLM output controls the user experience (e.g. as game master or offering production assistance to a human designer), the inability to capture user intent can be a source of frustration.\n\nOn a larger scale, LLMs suffer from losing context and struggling with continuity. This is because the ‚Äúmemory‚Äù of an LLM is constrained by its context size, which limits the extent of its inputs and outputs, as well as its response time due to the attention mechanism [10]. The longer the conversation, the less likely it is that the LLM will recall early events [98]. In video games, it is possible to separately summarize the game events (see Section 3.4) and process them as part of the input to the LLM. As a game progresses past a few game sessions, however, this summary may still be too long or details of increasing significance will be omitted, leading to a degraded performance. This is especially relevant for roles requiring a long-term engagement, such as LLM-powered retellers or game masters. In Infinite Craft (see Section 3.6), this is handled by an external database that stores and lookups past combination rules‚Äîensuring consistency in future uses of the same mechanic. However, LLMs could theoretically tackle this issue directly. A Retrieval-Augmented Generation (RAG) system [99] could address this limitation, drawing from a database containing vector representations or other latent representations of pertinent text or data. When the text generator processes a sequence, the RAG system would retrieve similar entries from this external data source. This would hypothetically provide a streamlined archive of game events and actions for the LLMs to consult in order to generate a consistent narrative progression.\n\nAnother challenge is that currently LLMs are trained to be highly compliant to the users‚Äô requests. For an LLM-as-assistant, this is not a cause for concern, but in the role of a game master this can create issues. Human game masters frequently curb the more exotic player requests, which could drastically diverge from the game narrative, or which would result in an unrecoverable disruption of a required sequence of game events. An LLM game master would try to accommodate for even the most bizarre requests, with little consideration for the consequential impacts to any predetermined game events.\n\nFinally, the implementation and deployment of LLMs to video game applications is still very limited. A video game is a domain where responsiveness is vital for players, so it follows that LLMs should also be able to provide their responses quickly. Unfortunately, while research on more efficient and faster architectures is being carried out [100], the real-time application of LLMs is still not plausible. This is especially evident in other domains such as design applications, where ‚Äúreal time‚Äù responses are generated in around 30 seconds to over a minute [101].\n\n6 Ethical Issues with LLMs in games\n\nWith the improvement of AI methods applied to video games over the recent years, many questions regarding their ethics and real-world impact have been raised [102]. Using LLMs raises ethical issues regarding sustainability, copyright, explainability, and biases. Naturally, each of these issues has serious implications in the field of video games.\n\nThe reliance of LLMs on training data and training time raises concerns regarding their carbon footprint. Beyond training costs, inference over the model‚Äôs lifespan has a greater environmental impact due to constant querying [103, 104]. Factors like renewable and local energy, better model architectures, and more meaningful (and thus less wasteful) training data can mitigate this. In the context of LLMs for digital games, sustainability remains crucial, considering the carbon footprint of frequent queries during gameplay (e.g. for Game Master or NPC responses, or for LLM-powered players). This is especially pertinent if the LLM is intended to run locally, on consumer-level hardware which is usually powered by non-renewable sources.\n\nWhen it comes to copyright, issues apply to the input data, the output data, and the model itself. LLMs trained on data under copyright is an unfortunate common practice [105], deservedly raising public outrage [106, 107]. The models themselves have different copyright licenses applied, which can also lead to artifacts they generate to fall under the public domain [11, 108]. For the game industry, matters of IP and copyright are extremely important. This is as much a concern regarding having the company‚Äôs copyrighted content somehow used as training by competitors, as it is about using LLMs that can produce material that the company cannot copyright. It is important to note here that, at least when it comes to the latter concern, the role the LLM takes is very pertinent. If an LLM or LMM produces content automatically (see Section 3.7), past legal consensus in the US indicates that the material can not be copyrighted [109]. If an LLM or LMM acts as an ‚Äúassistive tool‚Äù [110] to a designer (especially for conceptual assistance, see Section 3.8) then the extensive and impactful human effort needed to transform these concepts into game design and game art likely makes the final product copyrightable [110]. The limited rulings in copyright courts regarding this, however, and the ‚Äúlikely‚Äù caveat we include in our own text, understandably would make game companies hesitant to tread in untested waters for major game IPs beyond e.g. small-scale indie productions [45]. For researchers, however, the ethical issues of copyright breach and exploitation by large corporations, and the public outcry for the above, leave a bad taste and make research in LLMs less palatable [111].\n\nIn applications, understanding how a final result or product is reached is a extremely crucial, particularly when a product is iteratively refined as with design assistants (see Section 3.8). This is a problem of explainability [79], whereas LLMs are inherently opaque in their generation process. In [112], the authors highlight different methods to improve the explainability of language models, such as concept-based explanations or saliency maps. Particularly for LLMs, the self-explanation applied via the chain-of-thought (CoT) [113] reasoning has received attention by the research community [114, 115]. While this method adds a layer of explained reasoning to the generated output, there are multiple examples in the literature that demonstrate how this reasoning may just be an illusion of reasoning capabilities. Such examples include disregarding the provided reasoning in the final output [116], or reaching the correct solution via incorrect steps in math problems [117]. In video-games, explainability is paramount across roles, ensuring gameplay coherence and user engagement.\n\nFinally, biases emerge as LLMs are trained on a large corpus, usually scraped from the (Western-focused part of the) internet. This allows models to capture a current reality snapshot, which is advantageous for a conversational or questions answering model, though it requires curating this data from different kinds of biases. Some biases, such as social stereotypes, could be targeted and alleviated; others, such as exclusionary norms, pose greater challenges. In video games, we identify two main concerns when interacting with an LLM: toxic behavior, and stereotypes or incorrect notions. Toxic behavior is a harmful property that a language model may learn from its training corpus, which often contains text from community-based fora or social platforms. Tools that combat toxic language in video games are constantly evolving, with some even blocking chat messages before they are even delivered to the user [118, 119]. Therefore, similar applications could theoretically be developed to target toxic outputs from language models. Unlike human players, however, when an LLM plays the role of an NPC, it should align with the game themes and avoid any kind of toxic language or racial slurs. This requires developers to ensure proper behavior of the model through data cleaning, if the model is trained from scratch, or supplying tailored data if finetuning it to their needs. Addressing prejudices such as stereotypes and incorrect notions is complex, as they are not necessarily related to single words or expressions, but instead present themselves as a collection of ideals that can be wrong at best, and harmful at worst. An NPC LLM may exhibit real world stereotypes that can impact negatively the player experience, although we argue that the impact of prejudices from a player commentator or game master is much stronger and disturbing due to their perceived authority.\n\n7 Conclusions\n\nAs discussed in this paper, LLMs can take up many different roles that can improve the experience of players in video games, or enhance the ability of game designers to bring their ideas to life. However, we also highlighted many different challenges specific to the applications of LLMs and intrinsic to the nature of LLMs and the ecosystem that surrounds them. Despite technical, ethical, and legal challenges posed by LLMs, it is not realistic to ignore the impact that this research will likely have on both Game AI research and the game industry. We expect to see many new technical innovations from LLM researchers and corporations. Anticipating this, we propose promising directions where LLMs could be applied to games in the future.\n\nAcknowledgments\n\nThis work has been supported by the European Union‚Äôs Horizon 2020 research and innovation programme from the AI4media project (Grant Agreement No. 951911).\n\nReferences\n\n[1] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al., ‚ÄúLanguage models are unsupervised multitask learners,‚Äù OpenAI blog, vol. 1, no. 8, 2019.\n\n[2] T. Kaufmann, P. Weng, V. Bengs, and E. H√ºllermeier, ‚ÄúA survey of reinforcement learning from human feedback,‚Äù arXiv preprint arXiv:2312.14925, 2023.\n\n[3] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg, H. Nori, H. Palangi, M. T. Ribeiro, and Y. Zhang, ‚ÄúSparks of artificial general intelligence: Early experiments with GPT-4,‚Äù arXiv preprint arXiv:2303.12712, 2023.\n\n[4] R. T. McCoy, S. Yao, D. Friedman, M. Hardy, and T. L. Griffiths, ‚ÄúEmbers of autoregression: Understanding large language models through the problem they are trained to solve,‚Äù arXiv preprint arXiv:2309.13638, 2023.\n\n[5] G. N. Yannakakis and J. Togelius, Artificial Intelligence and Games. Springer, 2018.\n\n[6] J. Gwertzman and J. Soslow, ‚ÄúThe generative AI revolution in games,‚Äù https://a16z.com/the-generative-ai-revolution-in-games/, 2022, accessed 25 Feb 2024.\n\n[7] S. Minaee, T. Mikolov, N. Nikzad, M. Chenaghlu, R. Socher, X. Amatriain, and J. Gao, ‚ÄúLarge language models: A survey,‚Äù arXiv preprint arXiv:2402.06196, 2024.\n\n[8] C. E. Shannon, ‚ÄúA mathematical theory of communication,‚Äù The Bell system technical journal, vol. 27, no. 3, 1948.\n\n[9] J. L. Elman, ‚ÄúFinding structure in time,‚Äù Cognitive science, vol. 14, no. 2, 1990.\n\n[10] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù in Proceedings of the Conference on Neural Information Processing Systems, 2017.\n\n[11] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed, ‚ÄúMistral 7B,‚Äù arXiv preprint arXiv:2310.06825, 2023.\n\n[12] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi√®re, N. Goyal, E. Hambro, F. Azhar et al., ‚ÄúLlama: Open and efficient foundation language models,‚Äù arXiv preprint arXiv:2302.13971, 2023.\n\n[13] Z. Yang, L. Li, K. Lin, J. Wang, C.-C. Lin, Z. Liu, and L. Wang, ‚ÄúThe dawn of LMMs: Preliminary explorations with GPT-4V(ision),‚Äù arXiv preprint arXiv:2309.17421, 2023.\n\n[14] H. Liu, C. Li, Q. Wu, and Y. J. Lee, ‚ÄúVisual instruction tuning,‚Äù in Proceedings of the Neural Information Processing Systems Conference, 2023.\n\n[15] A. Summerville, S. Snodgrass, M. Guzdial, C. Holmg√•rd, A. K. Hoover, A. Isaksen, A. Nealen, and J. Togelius, ‚ÄúProcedural content generation via machine learning (PCGML),‚Äù IEEE Transactions on Games, vol. 10, no. 3, 2018.\n\n[16] D. Noever, M. Ciolino, and J. Kalin, ‚ÄúThe chess transformer: Mastering play using generative language models,‚Äù arXiv preprint arXiv:2008.04057, 2020.\n\n[17] A. St√∂ckl, ‚ÄúWatching a language model learning chess,‚Äù in Proceedings of the Recent Advances in Natural Language Processing International Conference, 2021.\n\n[18] S. Toshniwal, S. Wiseman, K. Livescu, and K. Gimpel, ‚ÄúChess as a testbed for language model state tracking,‚Äù in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 36, no. 10, 2022.\n\n[19] M. Ciolino, J. Kalin, and D. Noever, ‚ÄúThe Go transformer: Natural language modeling for game play,‚Äù in Proceedings of the Artificial Intelligence for Industries Internation Conference, 2020.\n\n[20] K. Li, A. K. Hopkins, D. Bau, F. Vi√©gas, H. Pfister, and M. Wattenberg, ‚ÄúEmergent world representations: Exploring a sequence model trained on a synthetic task,‚Äù in Proceedings of the International Conference on Learning Representations, 2023.\n\n[21] S. Reed, K. Zolna, E. Parisotto, S. G. Colmenarejo, A. Novikov, G. Barth-Maron, M. Gimenez, Y. Sulsky, J. Kay, J. T. Springenberg et al., ‚ÄúA generalist agent,‚Äù arXiv preprint arXiv:2205.06175, 2022.\n\n[22] M. Xu, W. Dai, C. Liu, X. Gao, W. Lin, G.-J. Qi, and H. Xiong, ‚ÄúSpatial-temporal transformer networks for traffic flow forecasting,‚Äù arXiv preprint arXiv:2001.02908, 2020.\n\n[23] H. Chang, H. Zhang, L. Jiang, C. Liu, and W. T. Freeman, ‚ÄúMaskGIT: Masked generative image transformer,‚Äù in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022.\n\n[24] S. Yao, R. Rao, M. Hausknecht, and K. Narasimhan, ‚ÄúKeep CALM and explore: Language models for action generation in text-based games,‚Äù in Proceedings of the Empirical Methods in Natural Language Processing Conference, 2020.\n\n[25] C. F. Tsai, X. Zhou, S. S. Liu, J. Li, M. Yu, and H. Mei, ‚ÄúCan large language models play text games well? current state-of-the-art and open questions,‚Äù arXiv preprint arXiv:2304.02868, 2023.\n\n[26] Meta Fundamental AI Research Diplomacy Team (FAIR), A. Bakhtin, N. Brown, E. Dinan, G. Farina, C. Flaherty, D. Fried, A. Goff, J. Gray, H. Hu et al., ‚ÄúHuman-level play in the game of diplomacy by combining language models with strategic reasoning,‚Äù Science, vol. 378, no. 6624, 2022.\n\n[27] G. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao, Y. Zhu, L. Fan, and A. Anandkumar, ‚ÄúVoyager: An open-ended embodied agent with large language models,‚Äù arXiv preprint arXiv:2305.16291, 2023.\n\n[28] GoodAI, ‚ÄúIntroducing our work on general-purpose llm agents,‚Äù https://www.goodai.com/introducing-general-purpose-llm-agents/, 2023, accessed 24 Feb 2024.\n\n[29] M. √á. Uludaƒülƒ± and K. Oƒüuz, ‚ÄúNon-player character decision-making in computer games,‚Äù Artificial Intelligence Review, vol. 56, no. 12, 2023.\n\n[30] M. Shanahan, K. McDonell, and L. Reynolds, ‚ÄúRole play with large language models,‚Äù Nature, vol. 623, p. 493‚Äì498, 2023.\n\n[31] H. Warpefelt and H. Verhagen, ‚ÄúA model of non-player character believability,‚Äù Journal of Gaming & Virtual Worlds, vol. 9, no. 1, 2017.\n\n[32] J. Duan, H. Cheng, S. Wang, A. Zavalny, C. Wang, R. Xu, B. Kailkhura, and K. Xu, ‚ÄúShifting attention to relevance: Towards the uncertainty estimation of large language models,‚Äù arXiv preprint arXiv:2307.01379, 2023.\n\n[33] A. Mehta, Y. Kunjadiya, A. Kulkarni, and M. Nagar, ‚ÄúExploring the viability of conversational AI for Non-Playable Characters: A comprehensive survey,‚Äù in Proceedings of the 4th International Conference on Recent Trends in Computer Science and Technology, 2022.\n\n[34] E. Akata, L. Schulz, J. Coda-Forno, S. J. Oh, M. Bethge, and E. Schulz, ‚ÄúPlaying repeated games with large language models,‚Äù arXiv preprint arXiv:2305.16867, 2023.\n\n[35] Y. Xu, S. Wang, P. Li, F. Luo, X. Wang, W. Liu, and Y. Liu, ‚ÄúExploring large language models for communication games: An empirical study on Werewolf,‚Äù arXiv preprint arXiv:2309.04658, 2023.\n\n[36] P. Maas, F. Carey, C. Wheeler, E. Saatchi, P. Billington, and J. Yaffa Shamash, ‚ÄúTo infinity and beyond: SHOW-1 and showrunner agents in multi-agent simulations,‚Äù https://fablestudio.github.io/showrunner-agents/, 2023, accessed 27 Feb 2024.\n\n[37] J. Urbanek, A. Fan, S. Karamcheti, S. Jain, S. Humeau, E. Dinan, T. Rockt√§schel, D. Kiela, A. Szlam, and J. Weston, ‚ÄúLearning to speak and act in a fantasy text adventure game,‚Äù in Proceedings of the Empirical Methods in Natural Language Processing Conference and the Natural Language Processing International Joint Conference, 2019.\n\n[38] M. P. Eladhari, ‚ÄúRe-tellings: The fourth layer of narrative as an instrument for critique,‚Äù in Proceedings of the Interactive Digital Storytelling International Conference, 2018.\n\n[39] M. Guzdial, S. Shah, and M. Riedl, ‚ÄúTowards automated let‚Äôs play commentary,‚Äù in Proceedings of the AIIDE workshop on Experimental AI in Games, 2018.\n\n[40] T. Ishigaki, G. Topiƒá, Y. Hamazono, H. Noji, I. Kobayashi, Y. Miyao, and H. Takamura, ‚ÄúGenerating racing game commentary from vision, language, and structured data,‚Äù in Proceedings of the Natural Language Generation International Conference, 2021.\n\n[41] C. Li, S. Gandhi, and B. Harrison, ‚ÄúEnd-to-end let‚Äôs play commentary generation using multi-modal video representations,‚Äù in Proceedings of the Foundations of Digital Games International Conference, 2019.\n\n[42] N. Renella and M. Eger, ‚ÄúTowards automated video game commentary using generative AI,‚Äù in Proceedings of the AIIDE workshop on Experimental AI in Games, 2023.\n\n[43] A. Tychsen, M. Hitchens, T. Brolund, and M. Kavakli, ‚ÄúThe game master,‚Äù in Proceedings of the Australasian conference on Interactive entertainment, 2005.\n\n[44] A. Liapis and A. Denisova, ‚ÄúThe challenge of evaluating player experience in tabletop role-playing games,‚Äù in Proceedings of the Foundations of Digital Games International Conference, 2023.\n\n[45] A. Liapis, ‚ÄúThe discourse on computational creativity in indie tabletop role-playing games,‚Äù in preparation.\n\n[46] M. Hua and R. Raley, ‚ÄúPlaying with unicorns: AI dungeon and citizen NLP,‚Äù Digital Humanities Quarterly, vol. 14, no. 4, 2020.\n\n[47] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, ‚ÄúHigh-resolution image synthesis with latent diffusion models,‚Äù in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022.\n\n[48] A. Zhu, L. Martin, A. Head, and C. Callison-Burch, ‚ÄúCALYPSO: LLMs as Dungeon Masters‚Äô assistants,‚Äù in Proceedings of the Nineteenth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, 2023.\n\n[49] J. Kelly, M. Mateas, and N. Wardrip-Fruin, ‚ÄúTowards computational support with language models for TTRPG game masters,‚Äù in Proceedings of the FDG workshop on Human-AI Interaction through Play, 2023.\n\n[50] D. Acharya, J. Kelly, W. Tate, M. Joslyn, M. Mateas, and N. Wardrip-Fruin, ‚ÄúShoelace: A storytelling assistant for GUMSHOE One-2-One,‚Äù in Proceedings of the Foundations of Digital Games International Conference, 2023.\n\n[51] Y. Sun, Z. Li, K. Fang, C. H. Lee, and A. Asadipour, ‚ÄúLanguage as reality: A co-creative storytelling game experience in 1001 nights using generative AI,‚Äù in Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, 2023.\n\n[52] M. Treanor, A. Zook, M. P. Eladhari, J. Togelius, G. Smith, M. Cook, T. Thompson, B. Magerko, J. Levine, and A. Smith, ‚ÄúAI-based game design patterns,‚Äù in Proceedings of the Foundations of Digital Games International Conference, 2015.\n\n[53] J. S. Park, J. O‚ÄôBrien, C. J. Cai, M. R. Morris, P. Liang, and M. S. Bernstein, ‚ÄúGenerative agents: Interactive simulacra of human behavior,‚Äù in Proceedings of the Annual ACM Symposium on User Interface Software and Technology, 2023.\n\n[54] Y. Liu, G. Deng, Z. Xu, Y. Li, Y. Zheng, Y. Zhang, L. Zhao, T. Zhang, and Y. Liu, ‚ÄúJailbreaking ChatGPT via prompt engineering: An empirical study,‚Äù arXiv preprint arXiv:2305.13860, 2023.\n\n[55] T. Litchfield, ‚ÄúThis browser-based ‚Äòendless crafting game‚Äô starts you off with fire and water, but it quickly escalates to God, the Big Bang, and ‚ÄòYin-Yoda‚Äô,‚Äù https://www.pcgamer.com/this-browser-based-endless-crafting-game-starts-you-off-with-fire-and-water-but-it-quickly-escalates-to-god-the-big-bang-and-yin-yoda/, 2024, accessed 28 Feburary 2024.\n\n[56] A. J. Summerville, S. Snodgrass, M. Mateas, and S. Ontanon, ‚ÄúThe VGLC: The video game level corpus,‚Äù in Proceedings of the FDG workshop on Procedural Content Generation, 2016.\n\n[57] A. Summerville and M. Mateas, ‚ÄúSuper Mario as a string: Platformer level generation via LSTMs,‚Äù in Proceedings of the Joint Conference of DIGRA and FDG, 2016.\n\n[58] G. Todd, S. Earle, M. U. Nasir, M. C. Green, and J. Togelius, ‚ÄúLevel generation through large language models,‚Äù in Proceedings of the Foundations of Digital Games International Conference, 2023.\n\n[59] C. Bamford, ‚ÄúGriddly: A platform for AI research in games,‚Äù Software Impacts, vol. 8, 2021.\n\n[60] S. Sudhakaran, M. Gonz√°lez-Duque, M. Freiberger, C. Glanois, E. Najarro, and S. Risi, ‚ÄúMarioGPT: Open-ended text2level generation through large language models,‚Äù Advances in Neural Information Processing Systems, vol. 36, 2024.\n\n[61] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‚ÄúBERT: Pre-training of deep bidirectional transformers for language understanding,‚Äù in Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics, 2019.\n\n[62] A. Liapis, ‚ÄúSearching for sentient design tools for game development,‚Äù Ph.D. dissertation, Center for Computer Games, IT University of Copenhagen, Copenhagen, Denmark, 2014.\n\n[63] G. Smith, J. Whitehead, and M. Mateas, ‚ÄúTanagra: Reactive planning and constraint solving for mixed-initiative level design,‚Äù IEEE Transactions on Computational Intelligence and AI in Games, vol. 3, no. 3, 2011.\n\n[64] A. Liapis, G. N. Yannakakis, and J. Togelius, ‚ÄúSentient sketchbook: Computer-aided game level authoring,‚Äù in Proceedings of the Foundations of Digital Games Conference, 2013.\n\n[65] P. Migkotzidis and A. Liapis, ‚ÄúSuSketch: Surrogate models of gameplay as a design assistant,‚Äù IEEE Transactions on Games, vol. 14, no. 2, 2021.\n\n[66] M. Charity, I. Dave, A. Khalifa, and J. Togelius, ‚ÄúBaba is Y‚Äôall 2.0: Design and investigation of a collaborative mixed-initiative system,‚Äù IEEE Transactions on Games, 2022, early Access.\n\n[67] R. Gallotta, K. Arulkumaran, and L. B. Soros, ‚ÄúPreference-learning emitters for mixed-initiative Quality-Diversity algorithms,‚Äù IEEE Transactions on Games, 2023, early Access.\n\n[68] G. N. Yannakakis, A. Liapis, and C. Alexopoulos, ‚ÄúMixed-initiative co-creativity,‚Äù in Proceedings of the Foundations of Digital Games Conference, 2014.\n\n[69] A. Liapis, G. N. Yannakakis, and J. Togelius, ‚ÄúSentient world: Human-based procedural cartography,‚Äù in Proceedings of Evolutionary and Biologically Inspired Music, Sound, Art and Design Conference, 2013, vol. 7834, LNCS.\n\n[70] D. Novick and S. Sutton, ‚ÄúWhat is mixed-initiative interaction?‚Äù in Proceedings of the AAAI Spring Symposium on Computational Models for Mixed Initiative Interaction, 1997.\n\n[71] A. Liapis, G. N. Yannakakis, and J. Togelius, ‚ÄúDesigner modeling for personalized game content creation tools,‚Äù in Proceedings of the AIIDE Workshop on Artificial Intelligence & Game Aesthetics, 2013.\n\n[72] V. Vimpari, A. Kultima, P. H√§m√§l√§inen, and C. Guckelsberger, ‚Äú‚ÄúAn adapt-or-die type of situation‚Äù: Perception, adoption, and use of text-to-image-generation AI by game industry professionals,‚Äù in Proceedings of the ACM on Human-Computer Interaction, vol. 7, 2023.\n\n[73] J. Boucher, G. Smith, and Y. Telliel, ‚ÄúExamining early professionals‚Äô use of generative AI in the game development process,‚Äù in Proceedings of the AIIDE Workshop on Experimental Artificial Intelligence in Games, 2023.\n\n[74] M. Charity, Y. Bhartia, D. Zhang, A. Khalifa, and J. Togelius, ‚ÄúA Preliminary Study on a Conceptual Game Feature Generation and Recommendation System,‚Äù in Proceedings of the IEEE Conference on Games, 2023.\n\n[75] M. U. Nasir and J. Togelius, ‚ÄúPractical PCG through large language models,‚Äù in Proceedings of the IEEE Conference on Games, 2023.\n\n[76] V. Kumaran, D. Carpenter, J. Rowe, B. Mott, and J. Lester, ‚ÄúEnd-to-end procedural level generation in educational games with natural language instruction,‚Äù in Proceedings of the IEEE Conference on Games, 2023.\n\n[77] P. Taveekitworachai, F. Abdullah, M. F. Dewantoro, R. Thawonmas, J. Togelius, and J. Renz, ‚ÄúChatGPT4PCG competition: Character-like level generation for science birds,‚Äù arXiv preprint arXiv:2303.15662, 2023.\n\n[78] T. Brooks, A. Holynski, and A. A. Efros, ‚ÄúInstructPix2Pix: Learning to follow image editing instructions,‚Äù in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023.\n\n[79] J. Zhu, A. Liapis, S. Risi, R. Bidarra, and G. M. Youngblood, ‚ÄúExplainable AI for designers: A human-centered perspective on mixed-initiative co-creation,‚Äù in Proceedings of the IEEE Conference on Computational Intelligence and Games, 2018.\n\n[80] A. Liapis, M. Awiszus, A. J. Champandard, M. Cook, A. Denisova, A. Dockhorn, T. Thompson, and J. Zhu, ‚ÄúArtificial intelligence for audiences,‚Äù in Human-Game AI Interaction (Dagstuhl Seminar 22251), D. Ashlock, S. Maghsudi, D. P. Liebana, P. Spronck, and M. Eberhardinger, Eds., 2023.\n\n[81] G. N. Yannakakis and D. Melhart, ‚ÄúAffective game computing: A survey,‚Äù Proceedings of the IEEE, 2023.\n\n[82] G. N. Yannakakis and J. Togelius, ‚ÄúExperience-driven procedural content generation,‚Äù IEEE Transactions on Affective Computing, vol. 2, no. 3, pp. 147‚Äì161, 2011.\n\n[83] K. Pinitas, D. Renaudie, M. Thomsen, M. Barthet, K. Makantasis, A. Liapis, and G. N. Yannakakis, ‚ÄúPredicting player engagement in Tom Clancy‚Äôs The Division 2: A multimodal approach via pixels and gamepad actions,‚Äù in Proceedings of the ACM International Conference on Multimodal Interaction, 2023.\n\n[84] D. Melhart, A. Liapis, and G. N. Yannakakis, ‚ÄúThe Arousal video Game AnnotatIoN (AGAIN) dataset,‚Äù IEEE Transactions on Affective Computing, vol. 13, no. 4, 2022.\n\n[85] N. M. M. Shafiullah, Z. J. Cui, A. Altanzaya, and L. Pinto, ‚ÄúBehavior transformers: Cloning kùëòkitalic_k modes with one stone,‚Äù in Proceedings of the Neural Information Processing Systems Conference, 2022.\n\n[86] J. Pfau, A. Liapis, G. N. Yannakakis, and R. Malaka, ‚ÄúDungeons & Replicants II: Automated game balancing across multiple difficulty dimensions via deep player behavior modeling,‚Äù IEEE Transactions on Games, 2022.\n\n[87] S. Johnson-Bey, M. Mateas, and N. Wardrip-Fruin, ‚ÄúToward using ChatGPT to generate theme-relevant simulated storyworlds,‚Äù in Proceedings of the AIIDE Workshop on Experimental Artificial Intelligence in Games, 2023.\n\n[88] M. Kreminski, D. Acharya, N. Junius, E. Oliver, K. Compton, M. Dickinson, C. Focht, S. Mason, S. Mazeika, and N. Wardrip-Fruin, ‚ÄúCozy mystery construction kit: Prototyping toward an AI-assisted collaborative storytelling mystery game,‚Äù in Proceedings of the Foundations of Digital Games International Conference, 2019.\n\n[89] C. Fern√°ndez-Vara and A. Thomson, ‚ÄúProcedural generation of narrative puzzles in adventure games: The puzzle-dice system,‚Äù in Proceedings of the Workshop on Procedural Content Generation in Games, 2012.\n\n[90] J. Dormans, ‚ÄúAdventures in level design: generating missions and spaces for action adventure games,‚Äù in Proceedings of the Workshop on Procedural Content Generation in Games, 2010.\n\n[91] P. Manakul, A. Liusie, and M. J. F. Gales, ‚ÄúSelfCheckGPT: Zero-resource black-box hallucination detection for generative large language models,‚Äù in Proceedings of the Empirical Methods in Natural Language Processing Conference, 2023.\n\n[92] A. Strasser, ‚ÄúOn pitfalls (and advantages) of sophisticated large language models,‚Äù arXiv preprint arXiv:2303.17511, 2023.\n\n[93] N. Bian, H. Lin, P. Liu, Y. Lu, C. Zhang, B. He, X. Han, and L. Sun, ‚ÄúInfluence of external information on large language models mirrors social cognitive patterns,‚Äù arXiv preprint arXiv:2305.04812, 2023.\n\n[94] M. Karpinska and M. Iyyer, ‚ÄúLarge language models effectively leverage document-level context for literary translation, but critical errors persist,‚Äù in Proceedings of the Machine Translation Conference, 2023.\n\n[95] Z. Gekhman, J. Herzig, R. Aharoni, C. Elkind, and I. Szpektor, ‚ÄúTrueTeacher: Learning factual consistency evaluation with large language models,‚Äù in Proceedings of the Empirical Methods in Natural Language Processing Conference, 2023.\n\n[96] J. Zhou, ‚ÄúAn evaluation of state-of-the-art large language models for sarcasm detection,‚Äù arXiv preprint arXiv:2312.03706, 2023.\n\n[97] Y. Liu, T. Han, S. Ma, J. Zhang, Y. Yang, J. Tian, H. He, A. Li, M. He, Z. Liu, Z. Wu, L. Zhao, D. Zhu, X. Li, N. Qiang, D. Shen, T. Liu, and B. Ge, ‚ÄúSummary of chatgpt-related research and perspective towards the future of large language models,‚Äù Meta-Radiology, vol. 1, no. 2, 2023.\n\n[98] D. Li, A. S. Rawat, M. Zaheer, X. Wang, M. Lukasik, A. Veit, F. Yu, and S. Kumar, ‚ÄúLarge language models with controllable working memory,‚Äù in Findings of the Association for Computational Linguistics, 2023.\n\n[99] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. K√ºttler, M. Lewis, W.-t. Yih, T. Rockt√§schel, S. Riedel, and D. Kiela, ‚ÄúRetrieval-augmented generation for knowledge-intensive nlp tasks,‚Äù in Proceedings of the Conference on Neural Information Processing Systems, 2020.\n\n[100] X. Miao, G. Oliaro, Z. Zhang, X. Cheng, H. Jin, T. Chen, and Z. Jia, ‚ÄúTowards efficient generative large language model serving: A survey from algorithms to systems,‚Äù arXiv preprint arXiv:2312.15234, 2023.\n\n[101] F. de la Torre, C. M. Fang, H. Huang, A. Banburski-Fahey, J. A. Fernandez, and J. Lanier, ‚ÄúLLMR: Real-time prompting of interactive worlds using large language models,‚Äù arXiv preprint arXiv:2309.12276, 2023.\n\n[102] D. Melhart, J. Togelius, B. Mikkelsen, C. Holmg√•rd, and G. N. Yannakakis, ‚ÄúThe ethics of AI in games,‚Äù IEEE Transactions on Affective Computing, 2023.\n\n[103] A. A. Chien, L. Lin, H. Nguyen, V. Rao, T. Sharma, and R. Wijayawardana, ‚ÄúReducing the carbon impact of generative AI inference (today and in 2035),‚Äù in Proceedings of the SCS Workshop on Sustainable Computer Systems, 2023.\n\n[104] S. A. Khowaja, P. Khuwaja, and K. Dev, ‚ÄúChatGPT needs SPADE (sustainability, PrivAcy, digital divide, and ethics) evaluation: A review,‚Äù arXiv preprint arXiv:2305.03123, 2023.\n\n[105] Free Law Project, ‚ÄúAuthors Guild v. OpenAI Inc.‚Äù https://www.courtlistener.com/docket/67810584/authors-guild-v-openai-inc/, accessed 27 Feb 2024.\n\n[106] J. A. Rothchild, ‚ÄúCopyright implications of the use of code repositories to train a machine learning model,‚Äù Free Software Foundation, 2022.\n\n[107] HackerNews, ‚ÄúGithub copilot,‚Äù https://news.ycombinator.com/item?id=27676266, 2021, accessed 27 Feb 2024.\n\n[108] P. Zhang, G. Zeng, T. Wang, and W. Lu, ‚ÄúTinyllama: An open-source small language model,‚Äù 2024.\n\n[109] U.S. Copyright Office Review Board, ‚ÄúDecision affirming refusal of registration of A Recent Entrance to Paradise,‚Äù 2022.\n\n[110] ‚Äî‚Äî, ‚ÄúRegistration decision on Zarya of the Dawn,‚Äù 2023.\n\n[111] C. E. Lamb and D. G. Brown, ‚ÄúShould we have seen the coming storm? Transformers, society, and CC,‚Äù in Proceedings of the International Conference on Computational Creativity, 2023.\n\n[112] Y. Liu, Y. Yao, J.-F. Ton, X. Zhang, R. Guo, H. Cheng, Y. Klochkov, M. F. Taufiq, and H. Li, ‚ÄúTrustworthy LLMs: a survey and guideline for evaluating large language models‚Äô alignment,‚Äù in Proceedings of the NeurIPS Workshop on Socially Responsible Language Modelling Research, 2023.\n\n[113] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa, ‚ÄúLarge language models are zero-shot reasoners,‚Äù in Proceedings of the Advances in Neural Information Processing Systems Conference, 2022.\n\n[114] S. Chen, B. Li, and D. Niu, ‚ÄúBoosting of thoughts: Trial-and-error problem solving with large language models,‚Äù arXiv preprint arXiv:2402.11140, 2024.\n\n[115] D. Mondal, S. Modi, S. Panda, R. Singh, and G. S. Rao, ‚ÄúKAM-CoT: Knowledge augmented multimodal Chain-of-Thoughts reasoning,‚Äù arXiv preprint arXiv:2401.12863, 2024.\n\n[116] M. Turpin, J. Michael, E. Perez, and S. R. Bowman, ‚ÄúLanguage models don‚Äôt always say what they think: Unfaithful explanations in chain-of-thought prompting,‚Äù in Proceedings of the Neural Information Processing Systems Conference, 2023.\n\n[117] S. Frieder, L. Pinchetti, A. Chevalier, R.-R. Griffiths, T. Salvatori, T. Lukasiewicz, P. C. Petersen, and J. Berner, ‚ÄúMathematical Capabilities of ChatGPT,‚Äù in Proceedings of the Neural Information Processing Systems Conference, 2023.\n\n[118] Y. Jia, W. Wu, F. Cao, and S. C. Han, ‚ÄúIn-game toxic language detection: Shared task and attention residuals,‚Äù in Proceedings of the AAAI Conference on Artificial Intelligence, 2022.\n\n[119] J. Thomas, N. Chorakhalikar, A. Dantrey, R. R. Nalla, and P. Mehta, ‚ÄúAutomatic classification and reporting of inappropriate language in online applications,‚Äù US Patent 20 210 370 188, 2021."
    }
}
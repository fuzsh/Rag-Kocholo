{
    "id": "dbpedia_6595_3",
    "rank": 28,
    "data": {
        "url": "https://www.bmj.com/content/372/bmj.n160",
        "read_more_link": "",
        "language": "en",
        "title": "PRISMA 2020 explanation and elaboration: updated guidance and exemplars for reporting systematic reviews",
        "top_image": "https://www.bmj.com/sites/default/files/highwire/bmj/386/8439.cover-source.jpg",
        "meta_img": "https://www.bmj.com/sites/default/files/highwire/bmj/386/8439.cover-source.jpg",
        "images": [
            "https://www.bmj.com/sites/all/modules/contrib/panels_ajax_tab/images/loading.gif",
            "https://www.bmj.com/sites/default/themes/bmj/the_bmj/img/vector_iD_icon.svg",
            "https://www.bmj.com/content/bmj/372/bmj.n160/F1.medium.jpg",
            "https://www.bmj.com/content/bmj/372/bmj.n160/F2.medium.jpg",
            "https://www.bmj.com/content/bmj/372/bmj.n160/F3.medium.jpg",
            "https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_horizontal.svg",
            "https://googleads.g.doubleclick.net/pagead/viewthroughconversion/973817434/?value=0&guid=ON&script=0"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Patrick M Bossuyt",
            "Isabelle Boutron",
            "Tammy C Hoffmann",
            "Cynthia D Mulrow",
            "Larissa Shamseer",
            "Jennifer M Tetzlaff",
            "Elie A Akl",
            "Sue E Brennan",
            "Roger Chou",
            "Julie Glanville"
        ],
        "publish_date": "2021-03-29T00:00:00",
        "summary": "",
        "meta_description": "The methods and results of systematic reviews should be reported in sufficient detail to allow users to assess the trustworthiness and applicability of the review findings. The Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA) statement was developed to facilitate transparent and complete reporting of systematic reviews and has been updated (to PRISMA 2020) to reflect recent advances in systematic review methodology and terminology. Here, we present the explanation and elaboration paper for PRISMA 2020, where we explain why reporting of each item is recommended, present bullet points that detail the reporting recommendations, and present examples from published reviews. We hope that changes to the content and structure of PRISMA 2020 will facilitate uptake of the guideline and lead to more transparent, complete, and accurate reporting of systematic reviews.\n\nSystematic reviews are essential for healthcare providers, policy makers, and other decision makers, who would otherwise be confronted by an overwhelming volume of research on which to base their decisions. To allow decision makers to assess the trustworthiness and applicability of review findings, reports of systematic reviews should be transparent and complete. Furthermore, such reporting should allow others to replicate or update reviews. The Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA) statement published in 2009 (hereafter referred to as PRISMA 2009)123456789101112 was designed to help authors prepare transparent accounts of their reviews, and its recommendations have been widely endorsed and adopted.13 We have updated the PRISMA 2009 statement (to PRISMA 2020) to ensure currency and relevance and to reflect advances in systematic review methodology and terminology.\n\n### Summary points",
        "meta_lang": "en",
        "meta_favicon": "”/sites/default/themes/bmj/the_bmj/img/icon.png”/",
        "meta_site_name": "The BMJ",
        "canonical_link": "https://www.bmj.com/content/372/bmj.n160",
        "text": "Matthew J Page, senior research fellow1,\n\nDavid Moher, director and professor2,\n\nPatrick M Bossuyt, professor3,\n\nIsabelle Boutron, professor4,\n\nTammy C Hoffmann, professor5,\n\nCynthia D Mulrow, professor6,\n\nLarissa Shamseer, doctoral student7,\n\nJennifer M Tetzlaff, research product specialist8,\n\nElie A Akl, professor9,\n\nSue E Brennan, senior research fellow1,\n\nRoger Chou, professor10,\n\nJulie Glanville, associate director11,\n\nJeremy M Grimshaw, professor12,\n\nAsbjørn Hróbjartsson, professor13,\n\nManoj M Lalu, associate scientist and assistant professor14,\n\nTianjing Li, associate professor15,\n\nElizabeth W Loder, professor16,\n\nEvan Mayo-Wilson, associate professor17,\n\nSteve McDonald, senior research fellow1,\n\nLuke A McGuinness, research associate18,\n\nLesley A Stewart, professor and director19,\n\nJames Thomas, professor20,\n\nAndrea C Tricco, scientist and associate professor21,\n\nVivian A Welch, associate professor22,\n\nPenny Whiting, associate professor18,\n\nJoanne E McKenzie, associate professor1\n\n1School of Public Health and Preventive Medicine, Monash University, Melbourne, Australia\n\n2Centre for Journalology, Clinical Epidemiology Program, Ottawa Hospital Research Institute, Ottawa, Canada; School of Epidemiology and Public Health, Faculty of Medicine, University of Ottawa, Ottawa, Canada\n\n3Department of Clinical Epidemiology, Biostatistics and Bioinformatics, Amsterdam University Medical Centres, University of Amsterdam, Amsterdam, Netherlands\n\n4Université de Paris, Centre of Epidemiology and Statistics (CRESS), Inserm, F 75004 Paris, France\n\n5Institute for Evidence-Based Healthcare, Faculty of Health Sciences and Medicine, Bond University, Gold Coast, Australia\n\n6University of Texas Health Science Center at San Antonio, San Antonio, Texas, United States; Annals of Internal Medicine\n\n7Knowledge Translation Program, Li Ka Shing Knowledge Institute, Toronto, Canada; School of Epidemiology and Public Health, Faculty of Medicine, University of Ottawa, Ottawa, Canada\n\n8Evidence Partners, Ottawa, Canada\n\n9Clinical Research Institute, American University of Beirut, Beirut, Lebanon; Department of Health Research Methods, Evidence, and Impact, McMaster University, Hamilton, Ontario, Canada\n\n10Department of Medical Informatics and Clinical Epidemiology, Oregon Health & Science University, Portland, Oregon, United States\n\n11York Health Economics Consortium (YHEC Ltd), University of York, York, UK\n\n12Clinical Epidemiology Program, Ottawa Hospital Research Institute, Ottawa, Canada; School of Epidemiology and Public Health, University of Ottawa, Ottawa, Canada; Department of Medicine, University of Ottawa, Ottawa, Canada\n\n13Centre for Evidence-Based Medicine Odense, Odense University Hospital, Odense, Denmark; Department of Clinical Research, University of Southern Denmark, Odense, Denmark; Open Patient data Explorative Network, Odense University Hospital, Odense, Denmark\n\n14Department of Anesthesiology and Pain Medicine, The Ottawa Hospital, Ottawa, Canada; Clinical Epidemiology Program, Blueprint Translational Research Group, Ottawa Hospital Research Institute, Ottawa, Canada; Regenerative Medicine Program, Ottawa Hospital Research Institute, Ottawa, Canada\n\n15Department of Ophthalmology, School of Medicine, University of Colorado Denver, Denver, Colorado, United States; Department of Epidemiology, Johns Hopkins Bloomberg School of Public Health, Baltimore, Maryland, United States\n\n16Division of Headache, Department of Neurology, Brigham and Women's Hospital, Harvard Medical School, Boston, Massachusetts, United States; Head of Research, The BMJ, London, UK\n\n17Department of Epidemiology and Biostatistics, Indiana University School of Public Health-Bloomington, Bloomington, Indiana, United States\n\n18Population Health Sciences, Bristol Medical School, University of Bristol, Bristol, UK\n\n19Centre for Reviews and Dissemination, University of York, York, UK\n\n20EPPI-Centre, UCL Social Research Institute, University College London, London, UK\n\n21Li Ka Shing Knowledge Institute of St. Michael's Hospital, Unity Health Toronto, Toronto, Canada; Epidemiology Division of the Dalla Lana School of Public Health and the Institute of Health Management, Policy, and Evaluation, University of Toronto, Toronto, Canada; Queen's Collaboration for Health Care Quality Joanna Briggs Institute Centre of Excellence, Queen's University, Kingston, Canada\n\n22Methods Centre, Bruyère Research Institute, Ottawa, Ontario, Canada; School of Epidemiology and Public Health, Faculty of Medicine, University of Ottawa, Ottawa, Canada\n\nCorrespondence to: M Page matthew.page{at}monash.edu\n\nAccepted 4 January 2021\n\nThe methods and results of systematic reviews should be reported in sufficient detail to allow users to assess the trustworthiness and applicability of the review findings. The Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA) statement was developed to facilitate transparent and complete reporting of systematic reviews and has been updated (to PRISMA 2020) to reflect recent advances in systematic review methodology and terminology. Here, we present the explanation and elaboration paper for PRISMA 2020, where we explain why reporting of each item is recommended, present bullet points that detail the reporting recommendations, and present examples from published reviews. We hope that changes to the content and structure of PRISMA 2020 will facilitate uptake of the guideline and lead to more transparent, complete, and accurate reporting of systematic reviews.\n\nSystematic reviews are essential for healthcare providers, policy makers, and other decision makers, who would otherwise be confronted by an overwhelming volume of research on which to base their decisions. To allow decision makers to assess the trustworthiness and applicability of review findings, reports of systematic reviews should be transparent and complete. Furthermore, such reporting should allow others to replicate or update reviews. The Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA) statement published in 2009 (hereafter referred to as PRISMA 2009)123456789101112 was designed to help authors prepare transparent accounts of their reviews, and its recommendations have been widely endorsed and adopted.13 We have updated the PRISMA 2009 statement (to PRISMA 2020) to ensure currency and relevance and to reflect advances in systematic review methodology and terminology.\n\nSummary points\n\nScope of this guideline\n\nThe PRISMA 2020 statement has been designed primarily for systematic reviews of studies that evaluate the effects of health interventions, irrespective of the design of the included studies. However, the checklist items are applicable to reports of systematic reviews evaluating other non-health-related interventions (for example, social or educational interventions), and many items are applicable to systematic reviews with objectives other than evaluating interventions (such as evaluating aetiology, prevalence, or prognosis). PRISMA 2020 is intended for use in systematic reviews that include synthesis (such as pairwise meta-analysis or other statistical synthesis methods) or do not include synthesis (for example, because only one eligible study is identified). The PRISMA 2020 items are relevant for mixed-methods systematic reviews (which include quantitative and qualitative studies), but reporting guidelines addressing the presentation and synthesis of qualitative data should also be consulted.1415 PRISMA 2020 can be used for original systematic reviews, updated systematic reviews, or continually updated (“living”) systematic reviews. However, for updated and living systematic reviews, there may be some additional considerations that need to be addressed. Extensions to the PRISMA 2009 statement have been developed to guide reporting of network meta-analyses,16 meta-analyses of individual participant data,17 systematic reviews of harms,18 systematic reviews of diagnostic test accuracy studies,19 and scoping reviews20; for these types of reviews we recommend authors report their review in accordance with the recommendations in PRISMA 2020 along with the guidance specific to the extension. Separate guidance for items that should be described in protocols of systematic reviews is available (PRISMA-P 2015 statement).2122\n\nPRISMA 2020 explanation and elaboration\n\nPRISMA 2020 is published as a suite of three papers: a statement paper (consisting of the 27-item checklist, an expanded checklist that details reporting recommendations for each item, the PRISMA 2020 abstract checklist, and the revised flow diagram23); a development paper (which outlines the steps taken to update the PRISMA 2009 statement and provides rationale for modifications to the original items24); and this paper, the updated explanation and elaboration for PRISMA 2020. In this paper, for each item, we explain why reporting of the item is recommended and present bullet points that detail the reporting recommendations. This structure is new to PRISMA 2020 and has been adopted to facilitate implementation of the guidance.2526 Authors familiar with PRISMA 2020 may opt to use the standalone statement paper23; however, for those who are new to or unfamiliar with PRISMA 2020, we encourage use of this explanation and elaboration document. Box 1 includes a glossary of terms used throughout the PRISMA 2020 explanation and elaboration paper.\n\nBox 1\n\nGlossary of terms\n\nSystematic review—A review that uses explicit, systematic methods to collate and synthesize findings of studies that address a clearly formulated question27\n\nStatistical synthesis—The combination of quantitative results of two or more studies. This encompasses meta-analysis of effect estimates (described below) and other methods, such as combining P values, calculating the range and distribution of observed effects, and vote counting based on the direction of effect (see McKenzie and Brennan28 for a description of each method)\n\nMeta-analysis of effect estimates—A statistical technique used to synthesize results when study effect estimates and their variances are available, yielding a quantitative summary of results28\n\nOutcome—An event or measurement collected for participants in a study (such as quality of life, mortality)\n\nResult—The combination of a point estimate (such as a mean difference, risk ratio or proportion) and a measure of its precision (such as a confidence/credible interval) for a particular outcome\n\nReport—A document (paper or electronic) supplying information about a particular study. It could be a journal article, preprint, conference abstract, study register entry, clinical study report, dissertation, unpublished manuscript, government report, or any other document providing relevant information\n\nRecord—The title or abstract (or both) of a report indexed in a database or website (such as a title or abstract for an article indexed in Medline). Records that refer to the same report (such as the same journal article) are “duplicates”; however, records that refer to reports that are merely similar (such as a similar abstract submitted to two different conferences) should be considered unique.\n\nStudy—An investigation, such as a clinical trial, that includes a defined group of participants and one or more interventions and outcomes. A “study” might have multiple reports. For example, reports could include the protocol, statistical analysis plan, baseline characteristics, results for the primary outcome, results for harms, results for secondary outcomes, and results for additional mediator and moderator analyses\n\nRETURN TO TEXT\n\nWe use standardised language in the explanation and elaboration to indicate whether the reporting recommendations for each item (which we refer to as “elements” throughout) are essential or additional. Essential elements should be reported in the main report or as supplementary material for all systematic reviews (except for those preceded by “If…,” which should only be reported where applicable). These have been selected as essential because we consider their reporting important for users to assess the trustworthiness and applicability of a review’s findings, or their reporting would aid in reproducing the findings. Additional elements are those which are not essential but provide supplementary information that may enhance the completeness and usability of systematic review reports. The essential and additional elements are framed in terms of reporting the “presence” of a method or result (such as reporting if individuals were contacted to identify studies) rather than reporting on their absence. In some instances, however, reporting the absence of a method may be helpful (for example, “We did not contact individuals to identify studies”). We leave these decisions to the judgment of authors. Finally, although PRISMA 2020 provides a template for where information might be located, the suggested location should not be seen as prescriptive; the guiding principle is to ensure the information is reported.\n\nJournals and publishers might impose word and section limits, and limits on the number of tables and figures allowed in the main report. In such cases, if the relevant information for some items already appears in a publicly accessible review protocol, referring to the protocol may suffice. Alternatively, placing detailed descriptions of the methods used or additional results (such as for less critical outcomes) in supplementary files is recommended. Ideally, supplementary files should be deposited to a general-purpose or institutional open-access repository that provides free and permanent access to the material (such as Open Science Framework, Dryad, figshare). A reference or link to the additional information should be included in the main report.\n\nWe sought examples of good reporting for each checklist item from published systematic reviews and present one for each item below; more examples are available in table S1 in the data supplement on bmj.com. We have edited the examples by removing all citations within them (to avoid potential confusion with the citation for each example), and we spelled out abbreviations to aid comprehension. We encourage readers to submit evidence that informs any of the recommendations in PRISMA 2020 and any examples that could be added to our bank of examples of good reporting (via the PRISMA statement website http://www.prisma-statement.org/).\n\nTitle\n\nItem 1. Identify the report as a systematic review\n\nExplanation: Inclusion of “systematic review” in the title facilitates identification by potential users (patients, healthcare providers, policy makers, etc) and appropriate indexing in databases. Terms such as “review,” “literature review,” “evidence synthesis,” or “knowledge synthesis” are not recommended because they do not distinguish systematic and non-systematic approaches. We also discourage using the terms “systematic review” and “meta-analysis” interchangeably because a systematic review refers to the entire set of processes used to identify, select, and synthesise evidence, whereas meta-analysis refers only to the statistical synthesis. Furthermore, a meta-analysis can be done outside the context of a systematic review (for example, when researchers meta-analyse results from a limited set of studies that they have conducted).\n\nEssential elements\n\nIdentify the report as a systematic review in the title.\n\nReport an informative title that provides key information about the main objective or question that the review addresses (for reviews of interventions, this usually includes the population and the intervention(s) that the review addresses).\n\nAdditional elements\n\nConsider providing additional information in the title, such as the method of analysis used (for example, “a systematic review with meta-analysis”), the designs of included studies (for example, “a systematic review of randomised trials”), or an indication that the review is an update of an existing review or a continually updated (“living”) systematic review.\n\nExample of item 1 of PRISMA 2020 checklist\n\n“Comparison of the therapeutic effects of rivaroxaban versus warfarin in antiphospholipid syndrome: a systematic review”167\n\nAbstract\n\nItem 2. See the PRISMA 2020 for Abstracts checklist (box 2)\n\nBox 2\n\nItems in the PRISMA 2020 for Abstracts checklist\n\nThe PRISMA 2020 for Abstracts checklist retains the same items as those included in the PRISMA for Abstracts statement published in 201329 but has been revised to make the wording consistent with the PRISMA 2020 statement and includes a new item recommending authors specify the methods used to present and synthesize results (item #6). The checklist includes the following 12 items:\n\nIdentify the report as a systematic review\n\nProvide an explicit statement of the main objective(s) or question(s) the review addresses\n\nSpecify the inclusion and exclusion criteria for the review\n\nSpecify the information sources (such as databases, registers) used to identify studies and the date when each was last searched\n\nSpecify the methods used to assess risk of bias in the included studies\n\nSpecify the methods used to present and synthesise results\n\nGive the total number of included studies and participants and summarise relevant characteristics of studies\n\nPresent results for main outcomes, preferably indicating the number of included studies and participants for each. If meta-analysis was done, report the summary estimate and confidence/credible interval. If comparing groups, indicate the direction of the effect (that is, which group is favoured)\n\nProvide a brief summary of the limitations of the evidence included in the review (such as study risk of bias, inconsistency, and imprecision)\n\nProvide a general interpretation of the results and important implications\n\nSpecify the primary source of funding for the review\n\nProvide the register name and registration number\n\nRETURN TO TEXT\n\nExplanation: An abstract providing key information about the main objective(s) or question(s) that the review addresses, methods, results, and implications of the findings should help readers decide whether to access the full report.29 For some readers, the abstract may be all that they have access to. Therefore, it is critical that results are presented for all main outcomes for the main review objective(s) or question(s) regardless of the statistical significance, magnitude, or direction of effect. Terms presented in the abstract will be used to index the systematic review in bibliographic databases. Therefore, reporting keywords that accurately describe the review question (such as population, interventions, outcomes) is recommended.\n\nEssential elements\n\nReport an abstract addressing each item in the PRISMA 2020 for Abstracts checklist (see box 2).\n\nExample of item 2 of PRISMA 2020 checklist\n\n“Title: Psychological interventions for common mental disorders in women experiencing intimate partner violence in low-income and middle-income countries: a systematic review and meta-analysis.\n\nBackground: Evidence on the effectiveness of psychological interventions for women with common mental disorders (CMDs) who also experience intimate partner violence is scarce. We aimed to test our hypothesis that exposure to intimate partner violence would reduce intervention effectiveness for CMDs in low-income and middle-income countries (LMICs).\n\nMethods: For this systematic review and meta-analysis, we searched MEDLINE, Embase, PsycINFO, Web of Knowledge, Scopus, CINAHL, LILACS, ScieELO, Cochrane, PubMed databases, trials registries, 3ie, Google Scholar, and forward and backward citations for studies published between database inception and Aug 16, 2019. All randomised controlled trials (RCTs) of psychological interventions for CMDs in LMICs which measured intimate partner violence were included, without language or date restrictions. We approached study authors to obtain unpublished aggregate subgroup data for women who did and did not report intimate partner violence. We did separate random-effects meta-analyses for anxiety, depression, post-traumatic stress disorder (PTSD), and psychological distress outcomes. Evidence from randomised controlled trials was synthesised as differences between standardised mean differences (SMDs) for change in symptoms, comparing women who did and who did not report intimate partner violence via random-effects meta-analyses. The quality of the evidence was assessed with the Cochrane risk of bias tool. This study is registered on PROSPERO, number CRD42017078611.\n\nFindings: Of 8122 records identified, 21 were eligible and data were available for 15 RCTs, all of which had a low to moderate risk of overall bias. Anxiety (five interventions, 728 participants) showed a greater response to intervention among women reporting intimate partner violence than among those who did not (difference in standardised mean differences [dSMD] 0.31, 95% CI 0.04 to 0.57, I2=49.4%). No differences in response to intervention were seen in women reporting intimate partner violence for PTSD (eight interventions, n=1436; dSMD 0.14, 95% CI −0.06 to 0.33, I2=42.6%), depression (12 interventions, n=2940; 0.10, −0.04 to 0.25, I2=49.3%), and psychological distress (four interventions, n=1591; 0.07, −0.05 to 0.18, I2=0.0%, p=0.681).\n\nInterpretation: Psychological interventions treat anxiety effectively in women with current or recent intimate partner violence exposure in LMICs when delivered by appropriately trained and supervised health-care staff, even when not tailored for this population or targeting intimate partner violence directly. Future research should investigate whether adapting evidence-based psychological interventions for CMDs to address intimate partner violence enhances their acceptability, feasibility, and effectiveness in LMICs.\n\nFunding: UK National Institute for Health Research ASSET and King's IoPPN Clinician Investigator Scholarship.”168\n\nRationale\n\nItem 3. Describe the rationale for the review in the context of existing knowledge\n\nExplanation: Describing the rationale should help readers understand why the review was conducted and what the review might add to existing knowledge.\n\nEssential elements\n\nDescribe the current state of knowledge and its uncertainties.\n\nArticulate why it is important to do the review.\n\nIf other systematic reviews addressing the same (or a largely similar) question are available, explain why the current review was considered necessary (for example, previous reviews are out of date or have discordant results; new review methods are available to address the review question; existing reviews are methodologically flawed; or the current review was commissioned to inform a guideline or policy for a particular organisation). If the review is an update or replication of a particular systematic review, indicate this and cite the previous review.\n\nIf the review examines the effects of interventions, also briefly describe how the intervention(s) examined might work.\n\nAdditional elements\n\nIf there is complexity in the intervention or context of its delivery, or both (such as multi-component interventions, interventions targeting the population and individual level, equity considerations30), consider presenting a logic model (sometimes referred to as a conceptual framework or theory of change) to visually display the hypothesised relationship between intervention components and outcomes.3132\n\nExample of item 3 of PRISMA 2020 checklist\n\n“To contain widespread infection and to reduce morbidity and mortality among health-care workers and others in contact with potentially infected people, jurisdictions have issued conflicting advice about physical or social distancing. Use of face masks with or without eye protection to achieve additional protection is debated in the mainstream media and by public health authorities, in particular the use of face masks for the general population; moreover, optimum use of face masks in health-care settings, which have been used for decades for infection prevention, is facing challenges amid personal protective equipment (PPE) shortages. Any recommendations about social or physical distancing, and the use of face masks, should be based on the best available evidence. Evidence has been reviewed for other respiratory viral infections, mainly seasonal influenza, but no comprehensive review is available of information on SARS-CoV-2 or related betacoronaviruses that have caused epidemics, such as severe acute respiratory syndrome (SARS) or Middle East respiratory syndrome (MERS). We, therefore, systematically reviewed the effect of physical distance, face masks, and eye protection on transmission of SARS-CoV-2, SARS-CoV, and MERS-CoV.”169\n\nObjectives\n\nItem 4. Provide an explicit statement of the objective(s) or question(s) the review addresses\n\nExplanation: An explicit and concise statement of the review objective(s) or question(s) will help readers understand the scope of the review and assess whether the methods used in the review (such as eligibility criteria, search methods, data items, and the comparisons used in the synthesis) adequately address the objective(s). Such statements may be written in the form of objectives (“the objectives of the review were to examine the effects of…”) or as questions (“what are the effects of…?”).31\n\nEssential elements\n\nProvide an explicit statement of all objective(s) or question(s) the review addresses, expressed in terms of a relevant question formulation framework (see Booth et al33 and Munn et al34 for various frameworks).\n\nIf the purpose is to evaluate the effects of interventions, use the Population, Intervention, Comparator, Outcome (PICO) framework or one of its variants to state the comparisons that will be made.\n\nExample of item 4 of PRISMA 2020 checklist\n\n“Objectives: To evaluate the benefits and harms of down‐titration (dose reduction, discontinuation, or disease activity‐guided dose tapering) of anti‐tumour necrosis factor-blocking agents (adalimumab, certolizumab pegol, etanercept, golimumab, infliximab) on disease activity, functioning, costs, safety, and radiographic damage compared with usual care in people with rheumatoid arthritis and low disease activity.”170\n\nEligibility criteria\n\nItem 5. Specify the inclusion and exclusion criteria for the review and how studies were grouped for the syntheses\n\nExplanation: Specifying the criteria used to decide what evidence was eligible or ineligible in sufficient detail should enable readers to understand the scope of the review and verify inclusion decisions.35 The PICO framework is commonly used to structure the reporting of eligibility criteria for reviews of interventions.36 In addition to specifying the review PICO, the intervention, outcome, and population groups that were used in the syntheses need to be identified and defined.37 For example, in a review examining the effects of psychological interventions for smoking cessation in pregnancy, the authors specified intervention groups (counselling, health education, feedback, incentive-based interventions, social support, and exercise) and the defining components of each group.38\n\nEssential elements\n\nSpecify all study characteristics used to decide whether a study was eligible for inclusion in the review, that is, components described in the PICO framework or one of its variants,3334 and other characteristics, such as eligible study design(s) and setting(s) and minimum duration of follow-up.\n\nSpecify eligibility criteria with regard to report characteristics, such as year of dissemination, language, and report status (for example, whether reports such as unpublished manuscripts and conference abstracts were eligible for inclusion).\n\nClearly indicate if studies were ineligible because the outcomes of interest were not measured, or ineligible because the results for the outcome of interest were not reported. Reporting that studies were excluded because they had “no relevant outcome data” is ambiguous and should be avoided.39\n\nSpecify any groups used in the synthesis (such as intervention, outcome, and population groups) and link these to the comparisons specified in the objectives (item #4).\n\nAdditional elements\n\nConsider providing rationales for any notable restrictions to study eligibility. For example, authors might explain that the review was restricted to studies published from 2000 onward because that was the year the device was first available.\n\nExample of item 5 of PRISMA 2020 checklist\n\n“Population: We included randomized controlled trials of adult (age ≥18 years) patients undergoing non-cardiac surgery, excluding organ transplantation surgery (as findings in patients who need immunosuppression may not be generalisable to others).\n\n“Intervention: We considered all perioperative care interventions identified by the search if they were protocolised (therapies were systematically provided to patients according to pre-defined algorithm or plan) and were started and completed during the perioperative pathway (that is, during preoperative preparation for surgery, intraoperative care, or inpatient postoperative recovery). Examples of interventions that we did or did not deem perioperative in nature included long term preoperative drug treatment (not included, as not started and completed during the perioperative pathway) and perioperative physiotherapy interventions (included, as both started and completed during the perioperative pathway). We excluded studies in which the intervention was directly related to surgical technique.\n\nOutcomes: To be included, a trial had to use a defined clinical outcome relating to postoperative pulmonary complications, such as “pneumonia” diagnosed according to the Centers for Disease Control and Prevention’s definition. Randomized controlled trials reporting solely physiological (for example, lung volumes and flow measurements) or biochemical (for example, lung inflammatory markers) outcomes are valuable but neither patient centric nor necessarily clinically relevant, and we therefore excluded them. We applied no language restrictions. Our primary outcome measure was the incidence of postoperative pulmonary complications, with postoperative pulmonary complications being defined as the composite of any of respiratory infection, respiratory failure, pleural effusion, atelectasis, or pneumothorax…Where a composite postoperative pulmonary complication was not reported, we contacted corresponding authors via email to request additional information, including primary data.”171\n\nInformation sources\n\nItem 6. Specify all databases, registers, websites, organisations, reference lists, and other sources searched or consulted to identify studies. Specify the date when each source was last searched or consulted\n\nExplanation: Authors should provide a detailed description of the information sources, such as bibliographic databases, registers and reference lists that were searched or consulted, including the dates when each source was last searched, to allow readers to assess the completeness and currency of the systematic review, and facilitate updating.40 Authors should fully report the “what, when, and how” of the sources searched; the “what” and “when” are covered in item #6, and the “how” is covered in item #7. Further guidance and examples about searching can be found in PRISMA-Search, an extension to the PRISMA statement for reporting literature searches in systematic reviews.41\n\nEssential elements\n\nSpecify the date when each source (such as database, register, website, organisation) was last searched or consulted.\n\nIf bibliographic databases were searched, specify for each database its name (such as MEDLINE, CINAHL), the interface or platform through which the database was searched (such as Ovid, EBSCOhost), and the dates of coverage (where this information is provided).\n\nIf study registers (such as ClinicalTrials.gov), regulatory databases (such as Drugs@FDA), and other online repositories (such as SIDER Side Effect Resource) were searched, specify the name of each source and any date restrictions that were applied.\n\nIf websites, search engines, or other online sources were browsed or searched, specify the name and URL (uniform resource locator) of each source.\n\nIf organisations or manufacturers were contacted to identify studies, specify the name of each source.\n\nIf individuals were contacted to identify studies, specify the types of individuals contacted (such as authors of studies included in the review or researchers with expertise in the area).\n\nIf reference lists were examined, specify the types of references examined (such as references cited in study reports included in the systematic review, or references cited in systematic review reports on the same or a similar topic).\n\nIf cited or citing reference searches (also called backwards and forward citation searching) were conducted, specify the bibliographic details of the reports to which citation searching was applied, the citation index or platform used (such as Web of Science), and the date the citation searching was done.\n\nIf journals or conference proceedings were consulted, specify the names of each source, the dates covered and how they were searched (such as handsearching or browsing online).\n\nExample of item 6 of PRISMA 2020 checklist\n\n“On 21 December 2017, MAJ searched 16 health, social care, education, and legal databases, the names and date coverage of which are given in the Table 1…We also carried out a ‘snowball’ search to identify additional studies by searching the reference lists of publications eligible for full-text review and using Google Scholar to identify and screen studies citing them…On 26 April 2018, we conducted a search of Google Scholar and additional supplementary searches for publications on websites of 10 relevant organisations (including government departments, charities, think-tanks, and research institutes). Full details of these supplementary searches can be found in the Additional file. Finally, we updated the database search on 7 May 2019, and the snowball and additional searches on 10 May 2019 as detailed in the Additional file. We used the same search method, except that we narrowed the searches to 2017 onwards.”172\n\nView this table:\n\nSearch strategy\n\nItem 7. Present the full search strategies for all databases, registers, and websites, including any filters and limits used\n\nExplanation: Reporting the full details of all search strategies (such as the full, line by line search strategy as run in each database) should enhance the transparency of the systematic review, improve replicability, and enable a review to be more easily updated.4042 Presenting only one search strategy from among several hinders readers’ ability to assess how comprehensive the searchers were and does not provide them with the opportunity to detect any errors. Furthermore, making only one search strategy available limits replication or updating of the searches in the other databases, as the search strategies would need to be reconstructed through adaptation of the one(s) made available. As well as reporting the search strategies, a description of the search strategy development process can help readers judge how far the strategy is likely to have identified all studies relevant to the review’s inclusion criteria. The description of the search strategy development process might include details of the approaches used to identify keywords, synonyms, or subject indexing terms used in the search strategies, or any processes used to validate or peer review the search strategies. Empirical evidence suggests that peer review of search strategies is associated with improvements to search strategies, leading to retrieval of additional relevant records.43 Further guidance and examples of reporting search strategies can be found in PRISMA-Search.41\n\nEssential elements\n\nProvide the full line by line search strategy as run in each database with a sophisticated interface (such as Ovid), or the sequence of terms that were used to search simpler interfaces, such as search engines or websites.\n\nDescribe any limits applied to the search strategy (such as date or language) and justify these by linking back to the review’s eligibility criteria.\n\nIf published approaches such as search filters designed to retrieve specific types of records (for example, filter for randomised trials)44 or search strategies from other systematic reviews, were used, cite them. If published approaches were adapted—for example, if existing search filters were amended—note the changes made.\n\nIf natural language processing or text frequency analysis tools were used to identify or refine keywords, synonyms, or subject indexing terms to use in the search strategy,4546 specify the tool(s) used.\n\nIf a tool was used to automatically translate search strings for one database to another,47 specify the tool used.\n\nIf the search strategy was validated—for example, by evaluating whether it could identify a set of clearly eligible studies—report the validation process used and specify which studies were included in the validation set.40\n\nIf the search strategy was peer reviewed, report the peer review process used and specify any tool used, such as the Peer Review of Electronic Search Strategies (PRESS) checklist.48\n\nIf the search strategy structure adopted was not based on a PICO-style approach, describe the final conceptual structure and any explorations that were undertaken to achieve it (for example, use of a multi-faceted approach that uses a series of searches, with different combinations of concepts, to capture a complex research question, or use of a variety of different search approaches to compensate for when a specific concept is difficult to define).40\n\nExample of item 7 of PRISMA 2020 checklist\n\nNote: the following is an abridged version of an example presented in full in supplementary table S1 on bmj.com.\n\n“MEDLINE(R) In-Process & Other Non-Indexed Citations and Ovid MEDLINE were searched via OvidSP. The database coverage was 1946 to present and the databases were searched on 29 August 2013.\n\nUrinary Bladder, Overactive/\n\n((overactiv$ or over-activ$ or hyperactiv$ or hyper-activ$ or unstable or instability or incontinen$) adj3 bladder$).ti,ab.\n\n(OAB or OABS or IOAB or IOABS).ti,ab.\n\n(urge syndrome$ or urge frequenc$).ti,ab.\n\n((overactiv$ or over-activ$ or hyperactiv$ or hyper-activ$ or unstable or instability) adj3 detrusor$).ti,ab.\n\nUrination Disorders/\n\nexp Urinary Incontinence/\n\nUrinary Bladder Diseases/\n\n(urge$ adj3 incontinen$).ti,ab.\n\n(urin$ adj3 (incontinen$ or leak$ or urgen$ or frequen$)).ti,ab.\n\n(urin$ adj3 (disorder$ or dysfunct$)).ti,ab.\n\n(detrusor$ adj3 (hyperreflexia$ or hyper-reflexia$ or hypertoni$ or hyper-toni$)).ti,ab.\n\n(void$ adj3 (disorder$ or dysfunct$)).ti,ab.\n\n(micturition$ adj3 (disorder$ or dysfunct$)).ti,ab.\n\nexp Enuresis/\n\nNocturia/\n\n(nocturia or nycturia or enuresis).ti,ab.\n\nor/1-17\n\n(mirabegron or betmiga$ or myrbetriq$ or betanis$ or YM-178 or YM178 or 223673-61-8 or “223673618” or MVR3JL3B2V).ti,ab,rn.\n\nexp Electric Stimulation Therapy/\n\nElectric Stimulation/\n\n((sacral or S3) adj3 (stimulat$ or modulat$)).ti,ab.\n\n(neuromodulat$ or neuro-modulat$ or neural modulat$ or electromodulat$ or electro-modulat$ or neurostimulat$ or neuro-stimulat$ or neural stimulat$ or electrostimulat$ or electro-stimulat$).ti,ab.\n\n(InterStim or SNS).ti,ab.\n\n((electric$ or nerve$1) adj3 (stimulat$ or modulat$)).ti,ab.\n\n(electric$ therap$ or electrotherap$ or electro-therap$).ti,ab.\n\nTENS.ti,ab.\n\nexp Electrodes/\n\nelectrode$1.ti,ab.\n\n((implant$ or insert$) adj3 pulse generator$).ti,ab.\n\n((implant$ or insert$) adj3 (neuroprosthe$ or neuro-prosthe$ or neural prosthe$)).ti,ab.\n\nPTNS.ti,ab.\n\n(SANS or Stoller Afferent or urosurg$).ti,ab.\n\n(evaluat$ adj3 peripheral nerve$).ti,ab.\n\nexp Botulinum Toxins/\n\n(botulinum$ or botox$ or onabotulinumtoxin$ or 1309378-01-5 or “1309378015”).ti,ab,rn.\n\nor/19-36\n\n18 and 37\n\nrandomized controlled trial.pt.\n\ncontrolled clinical trial.pt.\n\nrandom$.ti,ab.\n\nplacebo.ti,ab.\n\ndrug therapy.fs.\n\ntrial.ti,ab.\n\ngroups.ab.\n\nor/39-45\n\n38 and 46\n\nanimals/ not humans/\n\n47 not 48\n\nlimit 49 to english language\n\nSearch strategy development process: Five known relevant studies were used to identify records within databases. Candidate search terms were identified by looking at words in the titles, abstracts and subject indexing of those records. A draft search strategy was developed using those terms and additional search terms were identified from the results of that strategy. Search terms were also identified and checked using the PubMed PubReMiner word frequency analysis tool. The MEDLINE strategy makes use of the Cochrane RCT filter reported in the Cochrane Handbook v5.2. As per the eligibility criteria the strategy was limited to English language studies. The search strategy was validated by testing whether it could identify the five known relevant studies and also three further studies included in two systematic reviews identified as part of the strategy development process. All eight studies were identified by the search strategies in MEDLINE and Embase. The strategy was developed by an information specialist and the final strategies were peer reviewed by an experienced information specialist within our team. Peer review involved proofreading the syntax and spelling and overall structure, but did not make use of the PRESS checklist.”173\n\nSelection process\n\nItem 8. Specify the methods used to decide whether a study met the inclusion criteria of the review, including how many reviewers screened each record and each report retrieved, whether they worked independently, and, if applicable, details of automation tools used in the process\n\nExplanation: Study selection is typically a multi-stage process in which potentially eligible studies are first identified from screening titles and abstracts, then assessed through full text review and, where necessary, contact with study investigators. Increasingly, a mix of screening approaches might be applied (such as automation to eliminate records before screening or prioritise records during screening). In addition to automation, authors increasingly have access to screening decisions that are made by people independent of the author team (such as crowdsourcing) (see box 3). Authors should describe in detail the process for deciding how records retrieved by the search were considered for inclusion in the review, to enable readers to assess the potential for errors in selection.49505152\n\nBox 3\n\nStudy selection methods\n\nSeveral approaches to selecting studies exist. Here we comment on the advantages and disadvantages of each.\n\nAssessment of each record by one reviewer—Single screening is an efficient use of time and resources, but there is a higher risk of missing relevant studies495051\n\nAssessment of records by more than one reviewer—Double screening can vary from duplicate checking of all records (by two or more reviewers independently) to a second reviewer checking a sample only (for example, a random sample of screened records, or all excluded records). This approach may be more reliable than single screening but at the expense of increased reviewer time, given the time needed to resolve discrepancies495051\n\nPriority screening to focus early screening effort on most relevant records—Instead of screening records in year, title, author or random order, machine learning is used to identify relevant studies earlier in the screening process than would otherwise be the case. Priority screening is an iterative process in which the machine continually reassesses unscreened records for relevance. This approach can increase review efficiency by enabling the review team to start on subsequent steps of the review while less relevant records are still being screened. Both single and multiple reviewer assessments can be combined with priority screening5253\n\nPriority screening with the automatic elimination of less relevant records—Once the most relevant records have been identified using priority screening, teams may choose to stop screening based on the assumption that the remaining records are unlikely to be relevant. However, there is a risk of erroneously excluding relevant studies because of uncertainty about when it is safe to stop screening; the balance between efficiency gains and risk tolerance will be review-specific5253\n\nMachine learning classifiers—Machine learning classifiers are statistical models that use training data to rank records according to their relevance. They can be calibrated to achieve a given level of recall, thus enabling reviewers to implement screening rules, such as eliminating records or replacing double with single screening. Because the performance of classifiers is highly dependent on the data used to build them, classifiers should only be used to classify records for which they are designed5354\n\nPrevious “known” assessments—Screening decisions for records that have already been manually checked can be reused to exclude the same records from being reassessed, provided the eligibility criteria are the same. For example, groups that maintain registers of controlled trials to facilitate systematic reviews can avoid continually rescreening the same records by matching and then including/excluding those records from further consideration.\n\nCrowdsourcing—Crowdsourcing involves recruiting (usually via the internet) a large group of individuals to contribute to a task or project, such as screening records. If crowdsourcing is integrated with other study selection approaches, the specific platforms used should have well established and documented agreement algorithms, and data on crowd accuracy and reliability5556\n\nRETURN TO TEXT\n\nEssential elements for systematic reviews regardless of the selection processes used\n\nReport how many reviewers screened each record (title/abstract) and each report retrieved, whether multiple reviewers worked independently (that is, were unaware of each other’s decisions) at each stage of screening or not (for example, records screened by one reviewer and exclusions verified by another), and any processes used to resolve disagreements between screeners (for example, referral to a third reviewer or by consensus).\n\nReport any processes used to obtain or confirm relevant information from study investigators.\n\nIf abstracts or articles required translation into another language to determine their eligibility, report how these were translated (for example, by asking a native speaker or by using software programs).\n\nEssential elements for systematic reviews using automation tools in the selection process\n\nReport how automation tools were integrated within the overall study selection process; for example, whether records were excluded based solely on a machine assessment or whether machine assessments were used to double-check human decisions.\n\nIf an externally derived machine learning classifier was applied (such as Cochrane RCT Classifier), either to eliminate records or to replace a single screener, include a reference or URL to the version used. If the classifier was used to eliminate records before screening, report the number eliminated in the PRISMA flow diagram as “Records marked as ineligible by automation tools.”\n\nIf an internally derived machine learning classifier was used to assist with the screening process, identify the software/classifier and version, describe how it was used (such as to remove records or replace a single screener) and trained (if relevant), and what internal or external validation was done to understand the risk of missed studies or incorrect classifications. For example, authors might state that the classifier was trained on the set of records generated for the review in question (as may be the case when updating reviews) and specify which thresholds were applied to remove records.\n\nIf machine learning algorithms were used to prioritise screening (whereby unscreened records are continually re-ordered based on screening decisions), state the software used and provide details of any screening rules applied (for example, screening stopped altogether leaving some records to be excluded based on automated assessment alone, or screening switched from double to single screening once a pre-specified number or proportion of consecutive records was eliminated).\n\nEssential elements for systematic reviews using crowdsourcing or previous “known” assessments in the selection process\n\nIf crowdsourcing was used to screen records, provide details of the platform used and specify how it was integrated within the overall study selection process.\n\nIf datasets of already-screened records were used to eliminate records retrieved by the search from further consideration, briefly describe the derivation of these datasets. For example, if prior work has already determined that a given record does not meet the eligibility criteria, it can be removed without manual checking. This is the case for Cochrane’s Screen4Me service, in which an increasingly large dataset of records that are known not to represent randomised trials can be used to eliminate any matching records from further consideration.\n\nExample of item 8 of PRISMA 2020 checklist\n\n“Three researchers (AP, HB-R, FG) independently reviewed titles and abstracts of the first 100 records and discussed inconsistencies until consensus was obtained. Then, in pairs, the researchers independently screened titles and abstracts of all articles retrieved. In case of disagreement, consensus on which articles to screen full-text was reached by discussion. If necessary, the third researcher was consulted to make the final decision. Next, two researchers (AP, HB-R) independently screened full-text articles for inclusion. Again, in case of disagreement, consensus was reached on inclusion or exclusion by discussion and if necessary, the third researcher (FG) was consulted.”174\n\nFor examples of systematic reviews using automation tools, crowdsourcing, or previous “known” assessments in the selection process, see supplementary table S1 on bmj.com\n\nData collection process\n\nItem 9. Specify the methods used to collect data from reports, including how many reviewers collected data from each report, whether they worked independently, any processes for obtaining or confirming data from study investigators, and, if applicable, details of automation tools used in the process\n\nExplanation: Authors should report the methods used to collect data from reports of included studies, to enable readers to assess the potential for errors in the data presented.575859\n\nEssential elements\n\nReport how many reviewers collected data from each report, whether multiple reviewers worked independently or not (for example, data collected by one reviewer and checked by another),60 and any processes used to resolve disagreements between data collectors.\n\nReport any processes used to obtain or confirm relevant data from study investigators (such as how they were contacted, what data were sought, and success in obtaining the necessary information).\n\nIf any automation tools were used to collect data, report how the tool was used (such as machine learning models to extract sentences from articles relevant to the PICO characteristics),6162 how the tool was trained, and what internal or external validation was done to understand the risk of incorrect extractions.\n\nIf articles required translation into another language to enable data collection, report how these articles were translated (for example, by asking a native speaker or by using software programs).63\n\nIf any software was used to extract data from figures,64 specify the software used.\n\nIf any decision rules were used to select data from multiple reports corresponding to a study, and any steps were taken to resolve inconsistencies across reports, report the rules and steps used.65\n\nExample of item 9 of PRISMA 2020 checklist\n\n“We designed a data extraction form based on that used by Lumley 2009, which two review authors (RC and TC) used to extract data from eligible studies. Extracted data were compared, with any discrepancies being resolved through discussion. RC entered data into Review Manager 5 software (Review Manager 2014), double checking this for accuracy. When information regarding any of the above was unclear, we contacted authors of the reports to provide further details.”175\n\nData items\n\nItem 10a. List and define all outcomes for which data were sought. Specify whether all results that were compatible with each outcome domain in each study were sought (for example, for all measures, time points, analyses), and, if not, the methods used to decide which results to collect\n\nExplanation: Defining outcomes in systematic reviews generally involves specifying outcome domains (such as pain, quality of life, adverse events such as nausea) and the time frame of measurement (such as less than six months).37 Included studies may report multiple results that are eligible for inclusion within the review outcome definition.6667 For example, a study may report results for two measures of pain (such as the McGill Pain Questionnaire and the Brief Pain Inventory), at two time points (such as four weeks and eight weeks), all of which are compatible with a review outcome defined as “pain <6 months.” Multiple results compatible with an outcome domain in a study might also arise when study investigators report results based on multiple analysis populations (such as all participants randomised, all participants receiving a specific amount of treatment), methods for handling missing data (such as multiple imputation, last-observation-carried-forward), or methods for handling confounding (such as adjustment for different covariates).676869\n\nReviewers might seek all results that were compatible with each outcome definition from each study or use a process to select a subset of the results.6569 Examples of processes to select results include selecting the outcome definition that (a) was most common across studies, (b) the review authors considered “best” according to a prespecified hierarchy (for example, which prioritises measures included in a core outcome measurement set), or (c) the study investigators considered most important (such as the study’s primary outcome). It is important to specify the methods that were used to select the results when multiple results were available so that users are able to judge the appropriateness of those methods and whether there is potential for bias in the selection of results.\n\nReviewers may make changes to the inclusion or definition of the outcome domains or to the importance given to them in the review (for example, an outcome listed as “important” in the protocol is considered “critical” in the review). Providing a rationale for the change allows readers to assess the legitimacy of the change and whether it has potential to introduce bias in the review process.70\n\nEssential elements\n\nList and define the outcome domains and time frame of measurement for which data were sought.\n\nSpecify whether all results that were compatible with each outcome domain in each study were sought, and, if not, what process was used to select results within eligible domains.\n\nIf any changes were made to the inclusion or definition of the outcome domains or to the importance given to them in the review, specify the changes, along with a rationale.\n\nIf any changes were made to the processes used to select results within eligible outcome domains, specify the changes, along with a rationale.\n\nAdditional elements\n\nConsider specifying which outcome domains were considered the most important for interpreting the review’s conclusions (such as “critical” versus “important” outcomes) and provide rationale for the labelling (such as “a recent core outcome set identified the outcomes labelled ‘critical’ as being the most important to patients”).\n\nExample of item 10a of PRISMA 2020 checklist\n\nNote: the following is an abridged version of an example presented in full in supplementary table S1 on bmj.com.\n\n“Eligible outcomes were broadly categorised as follows:\n\nCognitive function\n\nGlobal cognitive function\n\nDomain-specific cognitive function (especially domains that reflect specific alcohol-related neuropathologies, such as psychomotor speed and working memory)\n\nClinical diagnoses of cognitive impairment\n\nMild cognitive impairment (also referred to as mild neurocognitive disorders)\n\nAny measure of cognitive function was eligible for inclusion. The tests or diagnostic criteria used in each study should have had evidence of validity and reliability for the assessment of mild cognitive impairment, but studies were not excluded on this basis…Results could be reported as an overall test score that provides a composite measure across multiple areas of cognitive ability (i.e. global cognitive function), sub-scales that provide a measure of domain-specific cognitive function or cognitive abilities (such as processing speed, memory), or both…Studies with a minimum follow-up of 6 months were eligible, a time frame chosen to ensure that studies were designed to examine more persistent effects of alcohol consumption…No restrictions were placed on the number of points at which the outcome was measured, but the length of follow-up and number of measurement points (including a baseline measure of cognition) was considered when interpreting study findings and in deciding which outcomes were similar enough to combine for synthesis.\n\nWe anticipated that individual studies would report data for multiple cognitive outcomes. Specifically, a single study may report results:\n\nFor multiple constructs related to cognitive function, for example, global cognitive function and cognitive ability on specific domains (e.g. memory, attention, problem-solving, language);\n\nUsing multiple methods or tools to measure the same or similar outcome, for example reporting measures of global cognitive function using both the Mini-Mental State Examination and the Montreal Cognitive Assessment;\n\nAt multiple time points, for example, at 1, 5, and 10 years.\n\nWhere multiple cognition outcomes were reported, we selected one outcome for inclusion in analyses and for reporting the main outcomes (e.g. for GRADEing), choosing the result that provided the most complete information for analysis. Where multiple results remained, we listed all available outcomes (without results) and asked our content expert to independently rank these based on relevance to the review question, and the validity and reliability of the measures used. Measures of global cognitive function were prioritised, followed by measures of memory, then executive function. In the circumstance where results from multiple multivariable models were presented, we extracted associations from the most fully adjusted model, except in the case where an analysis adjusted for a possible intermediary along the causal pathway (i.e. post-baseline measures of prognostic factors (e.g. smoking, drug use, hypertension)).”176\n\nItem 10b. List and define all other variables for which data were sought (such as participant and intervention characteristics, funding sources). Describe any assumptions made about any missing or unclear information\n\nExplanation: Authors should report the data and information collected from the studies so that readers can understand the type of the information sought and to inform data collection in other similar reviews. Variables of interest might include characteristics of the study (such as countries, settings, number of centres, funding sources, registration status), characteristics of the study design (such as randomised or non-randomised), characteristics of participants (such as age, sex, socioeconomic status), number of participants enrolled and included in analyses, the results (such as summary statistics, estimates of effect and measures of precision, factors adjusted for in analyses), and competing interests of study authors. For reviews of interventions, authors may also collect data on characteristics of the interventions (such as what interventions and comparators were delivered, how they were delivered, by whom, where, and for how long).\n\nEssential elements\n\nList and define all other variables for which data were sought. It may be sufficient to report a brief summary of information collected if the data collection and dictionary forms are made available (for example, as additional files or deposited in a publicly available repository).\n\nDescribe any assumptions made about any missing or unclear information from the studies. For example, in a study that includes “children and adolescents,” for which the investigators did not specify the age range, authors might assume that the oldest participants would be 18 years, based on what was observed in similar studies included in the review, and should report that assumption.\n\nIf a tool was used to inform which data items to collect (such as the Tool for Addressing Conflicts of Interest in Trials (TACIT)7172 or a tool for recording intervention details737475), cite the tool used.\n\nExample of item 10b of PRISMA 2020 checklist\n\n“We collected data on:\n\nthe report: author, year, and source of publication;\n\nthe study: sample characteristics, social demography, and definition and criteria used for depression;\n\nthe participants: stroke sequence (first ever vs recurrent), social situation, time elapsed since stroke onset, history of psychiatric illness, current neurological status, current treatment for depression, and history of coronary artery disease;\n\nthe research design and features: sampling mechanism, treatment assignment mechanism, adherence, non‐response, and length of follow up;\n\nthe intervention: type, duration, dose, timing, and mode of delivery.”177\n\nStudy risk of bias assessment\n\nItem 11. Specify the methods used to assess risk of bias in the included studies, including details of the tool(s) used, how many reviewers assessed each study and whether they worked independently, and, if applicable, details of automation tools used in the process\n\nExplanation: Users of reviews need to know the risk of bias in the included studies to appropriately interpret the evidence. Numerous tools have been developed to assess study limitations for various designs.76 However, many tools have been criticised because of their content (which may extend beyond assessing study limitations that have the potential to bias findings) and the way in which the items are combined (such as scales where items are combined to yield a numerical score) (see box 4).72 Reporting details of the selected tool enables readers to assess whether the tool focuses solely on items that have the potential to bias findings. Reporting details of how studies were assessed (such as by one or two authors) allows readers to assess the potential for errors in the assessments.58 Reporting how risk of bias assessments were incorporated into the analysis is addressed in Items #13e and #13f.\n\nBox 4\n\nAssessment of risk of bias in studies and bias due to missing results\n\nTerminology\n\nThe terms “quality assessment” and “critical appraisal” are often used to describe the process of evaluating the methodological conduct or reporting of studies.76 In PRISMA 2020, we distinguish “quality” from “risk of bias” and have focused the relevant items and elaborations on the latter. Risk of bias refers to the potential for study findings to systematically deviate from the truth due to methodological flaws in the design, conduct or analysis.72 Quality is not well defined, but has been shown to encompass constructs beyond those that may bias the findings, including, for example, imprecision, reporting completeness, ethics, and applicability.777879 In systematic reviews, focus should be given to the design, conduct, and analysis features that may lead to important bias in the findings.\n\nDifferent types of risk of bias\n\nIn PRISMA 2020, two aspects of risk of bias are considered. The first aspect is risk of bias in the results of the individual studies included in a systematic review. Empirical evidence and theoretical considerations suggest that several features of study design are associated with larger intervention effect estimates in studies; these features include inadequate generation and concealment of a random sequence to assign participants to groups, substantial loss to follow-up of participants, and unblinded outcome assessment.80\n\nThe second aspect is risk of bias in the result of a synthesis (such as meta-analysis) due to missing studies or results within studies. Missing studies/results may introduce bias when the decision to publish a study/result is influenced by the observed P value or magnitude or direction of the effect.81 For example, studies with statistically non-significant results may not have been submitted for publication (publication bias), or particular results that were statistically non-significant may have been omitted from study reports (selective non-reporting bias).8283\n\nTools for assessing risk of bias\n\nMany tools have been developed to assess the risk of bias in studies767879 or bias due to missing results.84 Existing tools typically take the form of composite scales and domain-based tools.7885 Composite scales include multiple items which each have a numeric score attached, from which an overall summary score might be calculated. Domain-based tools require users to judge risk of bias within specific domains, and to record the information on which each judgment was based.728687 Specifying the components/domains in the tool used in the review can help readers determine whether the tool focuses on risk of bias only or addresses other “quality” constructs. Presenting assessments for each component/domain in the tool is preferable to reporting a single “quality score” because it enables users to understand the specific components/domains that are at risk of bias in each study.\n\nIncorporating assessments of risk of bias in studies into the analysis\n\nThe risk of bias in included studies should be considered in the presentation and interpretation of results of individual studies and syntheses. Different analytic strategies may be used to examine whether the risks of bias of the studies may influence the study results: (i) restricting the primary analysis to studies judged to be at low risk of bias (sensitivity analysis); (ii) stratifying studies according to risk of bias using subgroup analysis or meta-regression; or (iii) adjusting the result from each study in an attempt to remove the bias. Further details about each approach are available elsewhere.72\n\nRETURN TO TEXT\n\nEssential elements\n\nSpecify the tool(s) (and version) used to assess risk of bias in the included studies.\n\nSpecify the methodological domains/components/items of the risk of bias tool(s) used.\n\nReport whether an overall risk of bias judgment that summarised across domains/components/items was made, and if so, what rules were used to reach an overall judgment.\n\nIf any adaptations to an existing tool to assess risk of bias in studies were made (such as omitting or modifying items), specify the adaptations.\n\nIf a new risk of bias tool was developed for use in the review, describe the content of the tool and make it publicly accessible.\n\nReport how many reviewers assessed risk of bias in each study, whether multiple reviewers worked independently (such as assessments performed by one reviewer and checked by another), and any processes used to resolve disagreements between assessors.\n\nReport any processes used to obtain or confirm relevant information from study investigators.\n\nIf an automation tool was used to assess risk of bias in studies, report how the automation tool was used (such as machine learning models to extract sentences from articles relevant to risk of bias88), how the tool was trained, and details on the tool’s performance and internal validation.\n\nExample of item 11 of PRISMA 2020 checklist\n\n“We assessed risk of bias in the included studies using the revised Cochrane ‘Risk of bias’ tool for randomised trials (RoB 2.0) (Higgins 2016a), employing the additional guidance for cluster-randomised and cross-over trials (Eldridge 2016; Higgins 2016b). RoB 2.0 addresses five specific domains: (1) bias arising from the randomisation process; (2) bias due to deviations from intended interventions; (3) bias due to missing outcome data; (4) bias in measurement of the outcome; and (5) bias in selection of the reported result. Two review authors independently applied the tool to each included study, and recorded supporting information and justifications for judgements of risk of bias for each domain (low; high; some concerns). Any discrepancies in judgements of risk of bias or justifications for judgements were resolved by discussion to reach consensus between the two review authors, with a third review author acting as an arbiter if necessary. Following guidance given for RoB 2.0 (Section 1.3.4) (Higgins 2016a), we derived an overall summary 'Risk of bias' judgement (low; some concerns; high) for each specific outcome, whereby the overall RoB for each study was determined by the highest RoB level in any of the domains that were assessed.”178\n\nEffect measures\n\nItem 12. Specify for each outcome the effect measure(s) (such as risk ratio, mean difference) used in the synthesis or presentation of results\n\nExplanation: To interpret a synthesised or study result, users need to know what effect measure was used. Effect measures refer to statistical constructs that compare outcome data between two groups. For instance, a risk ratio is an example of an effect measure that might be used for dichotomous outcomes.89 The chosen effect measure has implications for interpretation of the findings and might affect the meta-analysis results (such as heterogeneity90). Authors might use one effect measure to synthesise results and then re-express the synthesised results using another effect measure. For example, for meta-analyses of standardised mean differences, authors might re-express the combined results in units of a well known measurement scale, and for meta-analyses of risk ratios or odds ratios, authors might re-express results in absolute terms (such as risk difference).91 Furthermore, authors need to interpret effect estimates in relation to whether the effect is of importance to decision makers. For a particular outcome and effect measure, this requires specification of thresholds (or ranges) used to interpret the size of effect (such as minimally important difference; ranges for no/trivial, small, moderate, and large effects).91\n\nEssential elements\n\nSpecify for each outcome or type of outcome (such as binary, continuous) the effect measure(s) (such as risk ratio, mean difference) used in the synthesis or presentation of results.\n\nState any thresholds or ranges used to interpret the size of effect (such as minimally important difference; ranges for no/trivial, small, moderate, and large effects) and the rationale for these thresholds.\n\nIf synthesised results were re-expressed to a different effect measure, report the methods used to re-express results (such as meta-analysing risk ratios and computing an absolute risk reduction based on an assumed comparator risk).\n\nAdditional elements\n\nConsider providing justification for the choice of effect measure. For example, a standardised mean difference may have been chosen because multiple instruments or scales were used across studies to measure the same outcome domain (such as different instruments to assess depression).\n\nExample of item 12 of PRISMA 2020 checklist\n\n“We planned to analyse dichotomous outcomes by calculating the risk ratio (RR) of a successful outcome (i.e. improvement in relevant variables) for each trial…Because the included resilience‐training studies used different measurement scales to assess resilience and related constructs, we used standardised mean difference (SMD) effect sizes (Cohen's d) and their 95% confidence intervals (CIs) for continuous data in pair‐wise meta‐analyses.”179\n\nSynthesis methods\n\nItem 13a. Describe the processes used to decide which studies were eligible for each synthesis (such as tabulating the study intervention characteristics and comparing against the planned groups for each synthesis (item #5))\n\nExplanation: Before undertaking any statistical synthesis (item #13d), decisions must be made about which studies are eligible for each planned synthesis (item #5). These decisions will likely involve subjective judgments that could alter the result of a synthesis, yet the processes used and information to support the decisions are often absent from reviews. Reporting the processes (whether formal or informal) and any supporting information is recommended for transparency of the decisions made in grouping studies for synthesis. Structured approaches may involve the tabulation and coding of the main characteristics of the populations, interventions, and outcomes.92 For example, in a review examining the effects of psychological interventions for smoking cessation in pregnancy, the main intervention component of each study was coded as one of the following based on pre-specified criteria: counselling, health education, feedback, incentive-based interventions, social support, and exercise.38 This coding provided the basis for determining which studies were eligible for each planned synthesis (such as incentive-based interventions versus usual care). Similar coding processes can be applied to populations and outcomes.\n\nEssential elements\n\nDescribe the processes used to decide which studies were eligible for each synthesis.\n\nExample of item 13a of PRISMA 2020 checklist\n\n“Given the complexity of the interventions being investigated, we attempted to categorize the included interventions along four dimensions: (1) was housing provided to the participants as part of the intervention; (2) to what degree was the tenants’ residence in the provided housing dependent on, for example, sobriety, treatment attendance, etc.; (3) if housing was provided, was it segregated from the larger community, or scattered around the city; and (4) if case management services were provided as part of the intervention, to what degree of intensity. We created categories of interventions based on the above dimensions:\n\nCase management only\n\nAbstinence-contingent housing\n\nNon-abstinence-contingent housing\n\nHousing vouchers\n\nResidential treatment with case management\n\nSome of the interventions had multiple components (e.g. abstinence-contingent housing with case management). These interventions were categorized according to the main component (the component that the primary authors emphasized). They were also placed in separate analyses. We then organized the studies according to which comparison intervention was used (any of the above interventions, or usual services).”180\n\nItem 13b. Describe any methods required to prepare the data for presentation or synthesis, such as handling of missing summary statistics or data conversions\n\nExplanation: Authors may need to prepare the data collected from studies so that it is suitable for presentation or to be included in a synthesis. This could involve algebraic manipulation to convert reported statistics to required statistics (such as converting standard errors to standard deviations),89 transforming effect estimates (such as converting standardised mean differences to odds ratios93), or imputing missing summary data (such as missing standard deviations for continuous outcomes, intra-cluster correlations in cluster randomised trials).949596 Reporting the methods required to prepare the data will allow readers to judge the appropriateness of the methods used and the assumptions made and aid in attempts to replicate the synthesis.\n\nEssential elements\n\nReport any methods required to prepare the data collected from studies for presentation or synthesis, such as handling of missing summary statistics or data conversions.\n\nExample of item 13b of PRISMA 2020 checklist\n\n“We used cluster-adjusted estimates from cluster randomised controlled trials (c-RCTs) where available. If the studies had not adjusted for clustering, we attempted to adjust their standard errors using the methods described in the Cochrane Handbook for Systematic Reviews of Interventions (Higgins 2019), using an estimate of the intra-cluster correlation coefficient (ICC) derived from the trial. If the trial did not report the cluster-adjusted estimated or the ICC, we imputed an ICC from a similar study included in the review, adjusting if the nature or size of the clusters was different (e.g. households compared to classrooms). We assessed any imputed ICCs using sensitivity analysis.”181\n\nItem 13c. Describe any methods used to tabulate or visually display results of individual studies and syntheses\n\nExplanation: Presentation of study results using tabulation and visual display is important for transparency (particularly so for reviews or outcomes within reviews where a meta-analysis has not been undertaken) and facilitates the identification of patterns in the data. Tables may be used to present results from individual studies or from a synthesis (such as Summary of Findings table9798; see item #22). The purpose of tabulating data varies but commonly includes the complete and transparent reporting of the results or comparing the results across study characteristics.28 Different purposes will likely lead to different table structures. Reporting the chosen structure(s), along with details of the data presented (such as effect estimates), can aid users in understanding the basis and rationale for the structure (such as, “Table have been structured by outcome domain, within which studies are ordered from low to high risk of bias to increase the prominence of the most trustworthy evidence.”).\n\nThe principal graphical method for meta-analysis is the forest plot, which displays the effect estimates and confidence intervals of each study and often the summary estimate.99100 Similar to tabulation, ordering the studies in the forest plot based on study characteristics (such as by size of the effect estimate, year of publication, study weight, or overall risk of bias) rather than alphabetically (as is often done) can reveal patterns in the data.101 Other graphs that aim to display information about the magnitude or direction of effects might be considered when a forest plot cannot be used due to incompletely reported effect estimates (such as no measure of precision reported).28102 Careful choice and design of graphs is required so that they effectively and accurately represent the data.99\n\nEssential elements\n\nReport chosen tabular structure(s) used to display results of individual studies and syntheses, along with details of the data presented.\n\nReport chosen graphical methods used to visually display results of individual studies and syntheses.\n\nAdditional elements\n\nIf studies are ordered or grouped within tables or graphs based on study characteristics (such as by size of the study effect, year of publication), consider reporting the basis for the chosen ordering/grouping.\n\nIf non-standard graphs were used, consider reporting the rationale for selecting the chosen graph.\n\nExample of item 13c of PRISMA 2020 checklist\n\n“Meta-analyses could not be undertaken due to the heterogeneity of interventions, settings, study designs and outcome measures. Albatross plots were created to provide a graphical overview of the data for interventions with more than five data points for an outcome. Albatross plots are a scatter plot of p-values against the total number of individuals in each study. Small p-values from negative associations appear at the left of the plot, small p-values from positive associations at the right, and studies with null results towards the middle. The plot allows p-values to be interpreted in the context of the study sample size; effect contours show a standardised effect size (expressed as relative risk—RR) for a given p-value and study size, providing an indication of the overall magnitude of any association. We estimated an overall magnitude of association from these contours, but this should be interpreted cautiously.”182\n\nItem 13d. Describe any methods used to synthesise results and provide a rationale for the choice(s). If meta-analysis was performed, describe the model(s), method(s) to identify the presence and extent of statistical heterogeneity, and software package(s) used\n\nExplanation: Various statistical methods are available to synthesise results, the most common of which is meta-analysis of effect estimates (see box 5). Meta-analysis is used to synthesise effect estimates across studies, yielding a summary estimate. Different meta-analysis models are available, with the random-effects and fixed-effect models being in widespread use. Model choice can importantly affect the summary estimate and its confidence interval; hence the rationale for the selected model should be provided (see box 5). For random-effects models, many methods are available, and their performance has been shown to differ depending on the characteristics of the meta-analysis (such as the number and size of the included studies113114).\n\nBox 5\n\nMeta-analysis and its extensions\n\nMeta-analysis is a statistical technique used to synthesise results when study effect estimates and their variances are available, yielding a quantitative summary of results.103 The method facilitates interpretation that would otherwise be difficult to achieve if, for example, a narrative summary of each result was presented, particularly as the number of studies increases. Furthermore, meta-analysis increases the chance of detecting a clinically important effect as statistically significant, if it exists, and increases the precision of the estimated effect.104\n\nMeta-analysis models and methods\n\nThe summary estimate is a weighted average of the study effect estimates, where the study weights are determined primarily by the meta-analysis model. The two most common meta-analysis models are the “fixed-effect” and “random-effects” models.103 The assumption underlying the fixed-effect model is that there is one true (common) intervention effect and that the observed differences in results across studies reflect random variation only. This model is sometimes referred to as the “common-effects” or “equal-effects” model.103 A fixed-effect model can also be interpreted under a different assumption, that the true intervention effects are different and unrelated. This model is referred to as the “fixed-effects” model.105 The random-effects model assumes that there is not one true intervention effect but, rather, a distribution of true intervention effects and that the observed differences in results across studies reflect real differences in the effects of an intervention.104 The random-effects and fixed-effects models are similar in that they assume the true intervention effects are different, but they differ in that the random-effects model assumes the effects are related through a distribution, whereas the fixed-effects model does not make this assumption.\n\nMany considerations may influence an author’s choice of meta-analysis model. For example, their choice may be based on the clinical and methodological diversity of the included studies and the expectation that the underlying intervention effects will differ (potentially leading to selection of a random-effects model) or concern about small-study effects (the tendency for smaller studies to show different effects to larger ones,106 potentially leading to fitting of both a random-effects and fixed-effect model). Sometimes authors select a model based on the heterogeneity statistics observed (for example, switch from a fixed-effect to a random-effects model if the I2 statistic was >50%).107 However, this practice is strongly discouraged.\n\nThere are different methods available to assign weights in fixed-effect or random-effects meta-analyses (such as Mantel-Haenszel, inverse-variance).103 For random-effects meta-analyses, there are also different ways to estimate the between-study variance (such as DerSimonian and Laird, restricted maximum likelihood (REML)) and calculate the confidence interval for the summary effect (such as Wald-type confidence interval, Hartung-Knapp-Sidik-Jonkman108). Readers are referred to Deeks et al103 for further information on how to select a particular meta-analysis model and method.\n\nSubgroup analyses, meta-regression, and sensitivity analyses\n\nExtensions to meta-analysis, including subgroup analysis and meta-regression, are available to explore causes of variation of results across studies (that is, statistical heterogeneity).103 Subgroup analyses involve splitting studies or participant data into subgroups and comparing the effects of the subgroups. Meta-regression is an extension of subgroup analysis that allows for the effect of continuous and categorical variables to be investigated.109 Authors might use either type of analysis to explore, for example, whether the intervention effect estimate varied with different participant characteristics (such as mild versus severe disease) or intervention characteristics (such as high versus low dose of a drug).\n\nSensitivity analyses are undertaken to examine the robustness of findings to decisions made during the review process. This involves repeating an analysis but using different decisions from those originally made and informally comparing the findings.103 For example, sensitivity analyses might have been done to examine the impact on the meta-analysis of including results from conference abstracts that have never been published in full, including studies where most (but not all) participants were in a particular age range, including studies at high risk of bias, or using a fixed-effect versus random-effects meta-analysis model.\n\nSensitivity analyses differ from subgroup analyses. Sensitivity analyses consist of making informal comparisons between different ways of estimating the same effect, whereas subgroup analyses consist of formally undertaking a statistical comparison across the subgroups.103\n\nExtensions to meta-analysis that model or account for dependency\n\nIn most meta-analyses, effect estimates from independent studies are combined. Standard meta-analysis methods are appropriate for this situation, since an underlying assumption is that the effect estimates are independent. However, standard meta-analysis methods are not appropriate when the effect estimates are correlated. Correlated effect estimates arise when multiple effect estimates from a single study are calculated using some or all of the same participants and are included in the same meta-analysis. For example, where multiple effect estimates from a multi-arm trial are included in the same meta-analysis, or effect estimates for multiple outcomes from the same study are included. For this situation, a range of methods are available that appropriately model or account for the dependency of the effect estimates. These methods include multivariate meta-analysis,110 multilevel models,111 or robust variance estimation.112 See Lopez-Lopez for further discussion.69\n\nRETURN TO TEXT\n\nWhen study data are not amenable to meta-analysis of effect estimates, alternative statistical synthesis methods (such as calculating the median effect across studies, combining P values) or structured summaries might be used.28115 Additional guidance for reporting alternative statistical synthesis methods is available (see Synthesis Without Meta-analysis (SWiM) reporting guideline116).\n\nRegardless of the chosen synthesis method(s), authors should provide sufficient detail such that readers are able to assess the appropriateness of the selected methods and could reproduce the reported results (with access to the data).\n\nEssential elements\n\nIf statistical synthesis methods were used, reference the software, packages, and version numbers used to implement synthesis methods (such as metan in Stata 16,117 metafor (version 2.1-0) in R118).\n\nIf it was not possible to conduct a meta-analysis, describe and justify the synthesis methods (such as combining P values was used because no or minimal information beyond P values and direction of effect was reported in the studies) or summary approach used.\n\nIf meta-analysis was done, specify:\n\nthe meta-analysis model (fixed-effect, fixed-effects, or random-effects) and provide rationale for the selected model.\n\nthe method used (such as Mantel-Haenszel, inverse-variance).103\n\nany methods used to identify or quantify statistical heterogeneity (such as visual inspection of results, a formal statistical test for heterogeneity,103 heterogeneity variance (τ2), inconsistency (such as I2119), and prediction intervals120).\n\nIf a random-effects meta-analysis model was used, specify:\n\nthe between-study (heterogeneity) variance estimator used (such as DerSimonian and Laird, restricted maximum likelihood (REML)).\n\nthe method used to calculate the confidence interval for the summary effect (such as Wald-type confidence interval, Hartung-Knapp-Sidik-Jonkman108).\n\nIf a Bayesian approach to meta-analysis was used, describe the prior distributions about quantities of interest (such as intervention effect being analysed, amount of heterogeneity in results across studies).103\n\nIf multiple effect estimates from a study were included in a meta-analysis (as may arise, for example, when a study reports multiple outcomes eligible for inclusion in a particular meta-analysis), describe the method(s) used to model or account for the statistical dependency (such as multivariate meta-analysis, multilevel models, or robust variance estimation).3769\n\nIf a planned synthesis was not considered possible or appropriate, report this and the reason for that decision.\n\nAdditional elements\n\nIf a random-effects meta-analysis model was used, consider specifying other details about the methods used, such as the method for calculating confidence limits for the heterogeneity variance.\n\nExamples of item 13d of PRISMA 2020 checklist\n\nExample 1: meta-analysis\n\n“As the effects of functional appliance treatment were deemed to be highly variable according to patient age, sex, individual maturation of the maxillofacial structures, and appliance characteristics, a random-effects model was chosen to calculate the average distribution of treatment effects that can be expected. A restricted maximum likelihood random-effects variance estimator was used instead of the older DerSimonian-Laird one, following recent guidance. Random-effects 95% prediction intervals were to be calculated for meta-analyses with at least three studies to aid in their interpretation by quantifying expected treatment effects in a future clinical setting. The extent and impact of between-study heterogeneity were assessed by inspecting the forest plots and by calculating the tau-squared and the I-squared statistics, respectively. The 95% CIs (uncertainty intervals) around tau-squared and the I-squared were calculated to judge our confidence about these metrics. We arbitrarily adopted the I-squared thresholds of >75% to be considered as signs of considerable heterogeneity, but we also judged the evidence for this heterogeneity (through the uncertainty intervals) and the localization on the forest plot…All analyses were run in Stata SE 14.0 (StataCorp, College Station, TX) by one author.”183\n\nExample 2: calculating the median effect across studies\n\n“We based our primary analyses upon consideration of dichotomous process adherence measures (for example, the proportion of patients managed according to evidence-based recommendations). In order to provide a quantitative assessment of the effects associated with reminders without resorting to numerous assumptions or conveying a misleading degree of confidence in the results, we used the median improvement in dichotomous process adherence measures across studies…With each study represented by a single median outcome, we calculated the median effect size and interquartile range across all included studies for that comparison.”184\n\nItem 13e. Describe any methods used to explore possible causes of heterogeneity among study results (such as subgroup analysis, meta-regression)\n\nExplanation: If authors used methods to explore possible causes of variation of results across studies (that is, statistical heterogeneity) such as subgroup analysis or meta-regression (see box 5), they should provide sufficient details so that readers are able to assess the appropriateness of the selected methods and could reproduce the reported results (with access to the data). Such methods might be used to explore whether, for example, participant or intervention characteristics or risk of bias of the included studies explain variation in results.\n\nEssential elements\n\nIf methods were used to explore possible causes of statistical heterogeneity, specify the method used (such as subgroup analysis, meta-regression).\n\nIf subgroup analysis or meta-regression was performed, specify for each:\n\nwhich factors were explored, levels of those factors, and which direction of effect modification was expected and why (where possible).\n\nwhether analyses were conducted using study-level variables (where each study is included in one subgroup only), within-study contrasts (where data on subsets of participants within a study are available, allowing the study to be included in more than one subgroup), or some combination of the above.121\n\nhow subgroup effects were compared (such as statistical test for interaction for subgroup analyses103).\n\nIf other methods were used to explore heterogeneity because data were not amenable to meta-analysis of effect estimates, describe the methods used (such as structuring tables to examine variation in results across studies based on subpopulation, key intervention components, or contextual factors) along with the factors and levels.28116\n\nIf any analyses used to explore heterogeneity were not pre-specified, identify them as such.\n\nExample of item 13e of PRISMA 2020 checklist\n\n“Given a sufficient number of trials, we used unadjusted and adjusted mixed-effects meta-regression analyses to assess whether variation among studies in smoking cessation effect size was moderated by tailoring of the intervention for disadvantaged groups. The resulting regression coefficient indicates how the outcome variable (log risk ratio (RR) for smoking cessation) changes when interventions take a socioeconomic-position-tailored versus non-socioeconomic-tailored approach. A statistically significant (p<0.05) coefficient indicates that there is a linear association between the effect estimate for smoking cessation and the explanatory variable. More moderators (study-level variables) can be included in the model, which might account for part of the heterogeneity in the true effects. We pre-planned an adjusted model to include important study covariates related to the intensity and delivery of the intervention (number of sessions delivered (above median vs below median), whether interventions involved a trained smoking cessation specialist (yes vs no), and use of pharmacotherapy in the intervention group (yes vs no). These covariates were included a priori as potential confounders given that programmes tailored to socioeconomic position might include more intervention sessions or components or be delivered by different professionals with varying experience. The regression coefficient estimates how the intervention effect in the socioeconomic-position-tailored subgroup differs from the reference group of non-socioeconomic-position-tailored interventions.”185\n\nItem 13f. Describe any sensitivity analyses conducted to assess robustness of the synthesised results\n\nExplanation: If authors performed sensitivity analyses to assess robustness of the synthesised results to decisions made during the review process (see box 5), they should provide sufficient details so that readers are able to assess the appropriateness of the analyses and could reproduce the reported results (with access to the data). Ideally, sensitivity analyses should be pre-specified in the protocol, but unexpected issues may emerge during the review process that necessitate their use.\n\nEssential elements\n\nIf sensitivity analyses were performed, provide details of each analysis (such as removal of studies at high risk of bias, use of an alternative meta-analysis model).\n\nIf any sensitivity analyses were not pre-specified, identify them as such.\n\nExample of item 13f of PRISMA 2020 checklist\n\n“We conducted sensitivity meta-analyses restricted to trials with recent publication (2000 or later); overall low risk of bias (low risk of bias in all seven criteria); and enrolment of generally healthy women (rather than those with a specific clinical diagnosis). To incorporate trials with zero events in both intervention and control arms (which are automatically dropped from analyses of pooled relative risks), we also did sensitivity analyses for dichotomous outcomes in which we added a continuity correction of 0.5 to zero cells.”186\n\nReporting bias assessment\n\nItem 14. Describe any methods used to assess risk of bias due to missing results in a synthesis (arising from reporting biases)\n\nExplanation: The validity of a synthesis may be threatened when the available results differ systematically from the missing results. This is known as “bias due to missing results” and arises from “reporting biases” such as selective non-publication and selective non-reporting of results (see box 4).81 Direct methods for assessing the risk of bias due to missing results include comparing outcomes and analyses pre-specified in study registers, protocols, and statistical analysis plans with results that were available in study reports. Statistical and graphical methods exist to assess whether the observed data suggest potential for missing results (such as contour enhanced funnel plots, Egger’s test) and how robust the synthesis is to different assumptions about the nature of potentially missing results (such as selection models).84122123124 Tools (such as checklists, scales, or domain-based tools) that prompt users to consider some or all of these approaches are available.8184 Therefore, reporting methods (tools, graphical, statistical, or other) used to assess risk of bias due to missing results is recommended so that readers are able to assess how appropriate the methods were. The process by which assessments were conducted should also be reported to enable readers to assess the potential for errors and facilitate replicability.\n\nEssential elements\n\nSpecify the methods (tool, graphical, statistical, or other) used to assess the risk of bias due to missing results in a synthesis (arising from reporting biases).\n\nIf risk of bias due to missing results was assessed using an existing tool, specify the methodological components/domains/items of the tool, and the process used to reach a judgment of overall risk of bias.\n\nIf any adaptations to an existing tool to assess risk of bias due to missing results were made (such as omitting or modifying items), specify the adaptations.\n\nIf a new tool to assess risk of bias due to missing results was developed for use in the review, describe the content of the tool and make it publicly accessible.\n\nReport how many reviewers assessed risk of bias due to missing results in a synthesis, whether multiple reviewers worked independently, and any processes used to resolve disagreements between assessors.\n\nReport any processes used to obtain or confirm relevant information from study investigators.\n\nIf an automation tool was used to assess risk of bia"
    }
}
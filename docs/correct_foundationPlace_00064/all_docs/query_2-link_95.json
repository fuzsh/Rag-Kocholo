{
    "id": "correct_foundationPlace_00064_2",
    "rank": 95,
    "data": {
        "url": "https://spark.apache.org/developer-tools.html",
        "read_more_link": "",
        "language": "en",
        "title": "Useful Developer Tools",
        "top_image": "https://spark.apache.org/favicon.ico",
        "meta_img": "https://spark.apache.org/favicon.ico",
        "images": [
            "https://spark.apache.org/images/spark-logo-rev.svg",
            "https://spark.apache.org/images/running-benchamrks-using-github-actions.png",
            "https://spark.apache.org/images/intellij_remote_debug_configuration.png",
            "https://spark.apache.org/images/intellij_start_remote_debug.png",
            "https://www.apache.org/events/current-event-234x60.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "/favicon.ico",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Test coverage\n\nApache Spark community uses various resources to maintain the community test coverage.\n\nGitHub Actions\n\nGitHub Actions provides the following on Ubuntu 22.04.\n\nApache Spark 4\n\nScala 2.13 SBT build with Java 17\n\nScala 2.13 Maven build with Java 17/21\n\nJava/Scala/Python/R unit tests with Java 17/Scala 2.13/SBT\n\nTPC-DS benchmark with scale factor 1\n\nJDBC Docker integration tests\n\nKubernetes integration tests\n\nDaily Java/Scala/Python/R unit tests with Java 21 and Scala 2.13/SBT\n\nApache Spark 3\n\nScala 2.12 SBT build with Java 8\n\nScala 2.12 Maven build with Java 11/17\n\nJava/Scala/Python/R unit tests with Java 8/Scala 2.12/SBT\n\nKubernetes integration tests\n\nDaily Java/Scala/Python/R unit tests with Java 8 and Scala 2.13/SBT\n\nDaily JDBC Docker integration tests with Java 8 and Scala 2.13/SBT\n\nDaily TPC-DS benchmark with scale factor 1 with Java 8 and Scala 2.12/SBT\n\nAppVeyor\n\nAppVeyor provides the following on Windows.\n\nR unit tests with Java 17/Scala 2.13/SBT\n\nScaleway\n\nScaleway provides the following on MacOS and Apple Silicon.\n\nJava/Scala/Python/R unit tests with Java 17/Scala 2.12/SBT\n\nK8s integration tests (TBD)\n\nUseful developer tools\n\nReducing build times\n\nSBT: Avoiding re-creating the assembly JAR\n\nSpark’s default build strategy is to assemble a jar including all of its dependencies. This can be cumbersome when doing iterative development. When developing locally, it is possible to create an assembly jar including all of Spark’s dependencies and then re-package only Spark itself when making changes.\n\nBuilding submodules individually\n\nFor instance, you can build the Spark Core module using:\n\nRunning Individual Tests\n\nWhen developing locally, it’s often convenient to run a single test or a few tests, rather than running the entire test suite.\n\nTesting with SBT\n\nThe fastest way to run individual tests is to use the sbt console. It’s fastest to keep a sbt console open, and use it to re-run tests as necessary. For example, to run all of the tests in a particular project, e.g., core:\n\nYou can run a single test suite using the testOnly command. For example, to run the DAGSchedulerSuite:\n\nThe testOnly command accepts wildcards; e.g., you can also run the DAGSchedulerSuite with:\n\nOr you could run all of the tests in the scheduler package:\n\nIf you’d like to run just a single test in the DAGSchedulerSuite, e.g., a test that includes “SPARK-12345” in the name, you run the following command in the sbt console:\n\nIf you’d prefer, you can run all of these commands on the command line (but this will be slower than running tests using an open console). To do this, you need to surround testOnly and the following arguments in quotes:\n\nFor more about how to run individual tests with sbt, see the sbt documentation.\n\nTesting with Maven\n\nWith Maven, you can use the -DwildcardSuites flag to run individual Scala tests:\n\nYou need -Dtest=none to avoid running the Java tests. For more information about the ScalaTest Maven Plugin, refer to the ScalaTest documentation.\n\nTo run individual Java tests, you can use the -Dtest flag:\n\nTesting PySpark\n\nTo run individual PySpark tests, you can use run-tests script under python directory. Test cases are located at tests package under each PySpark packages. Note that, if you add some changes into Scala or Python side in Apache Spark, you need to manually build Apache Spark again before running PySpark tests in order to apply the changes. Running PySpark testing script does not automatically build it.\n\nAlso, note that there is an ongoing issue to use PySpark on macOS High Serria+. OBJC_DISABLE_INITIALIZE_FORK_SAFETY should be set to YES in order to run some of tests. See PySpark issue and Python issue for more details.\n\nTo run test cases in a specific module:\n\nTo run test cases in a specific class:\n\nTo run single test case in a specific class:\n\nYou can also run doctests in a specific module:\n\nLastly, there is another script called run-tests-with-coverage in the same location, which generates coverage report for PySpark tests. It accepts same arguments with run-tests.\n\nYou can check the coverage report visually by HTMLs under /.../spark/python/test_coverage/htmlcov.\n\nPlease check other available options via python/run-tests[-with-coverage] --help.\n\nTesting K8S\n\nAlthough GitHub Actions provide both K8s unit test and integration test coverage, you can run it locally. For example, Volcano batch scheduler integration test should be done manually. Please refer the integration test documentation for the detail.\n\nhttps://github.com/apache/spark/blob/master/resource-managers/kubernetes/integration-tests/README.md\n\nRunning the Docker integration tests\n\nDocker integration tests are covered by GitHub Actions. However, you can run it locally to speedup deveplopment and testing. Please refer the Docker integration test documentation for the detail.\n\nTesting with GitHub Actions workflow\n\nApache Spark leverages GitHub Actions that enables continuous integration and a wide range of automation. Apache Spark repository provides several GitHub Actions workflows for developers to run before creating a pull request.\n\nRunning benchmarks in your forked repository\n\nApache Spark repository provides an easy way to run benchmarks in GitHub Actions. When you update the benchmark results in a pull request, it is recommended to use GitHub Actions to run and generate the benchmark results in order to run them on the environment as same as possible.\n\nClick the “Actions” tab in your forked repository.\n\nSelect the “Run benchmarks” workflow in the “All workflows” list.\n\nClick the “Run workflow” button and enter the fields appropriately as below:\n\nBenchmark class: the benchmark class which you wish to run. It allows a glob pattern. For example, org.apache.spark.sql.*.\n\nJDK version: Java version you want to run the benchmark with. For example, 11.\n\nFailfast: indicates if you want to stop the benchmark and workflow when it fails. When true, it fails right away. When false, it runs all whether it fails or not.\n\nNumber of job splits: it splits the benchmark jobs into the specified number, and runs them in parallel. It is particularly useful to work around the time limits of workflow and jobs in GitHub Actions.\n\nOnce a “Run benchmarks” workflow is finished, click the workflow and download benchmarks results at “Artifacts”.\n\nGo to your root directory of Apache Spark repository, and unzip/untar the downloaded files which will update the benchmark results with appropriately locating the files to update.\n\nScalaTest issues\n\nIf the following error occurs when running ScalaTest\n\nIt is due to an incorrect Scala library in the classpath. To fix it:\n\nRight click on project\n\nSelect Build Path | Configure Build Path\n\nAdd Library | Scala Library\n\nRemove scala-library-2.10.4.jar - lib_managed\\jars\n\nIn the event of “Could not find resource path for Web UI: org/apache/spark/ui/static”, it’s due to a classpath issue (some classes were probably not compiled). To fix this, it sufficient to run a test from the command line:\n\nBinary compatibility\n\nTo ensure binary compatibility, Spark uses MiMa.\n\nEnsuring binary compatibility\n\nWhen working on an issue, it’s always a good idea to check that your changes do not introduce binary incompatibilities before opening a pull request.\n\nYou can do so by running the following command:\n\nA binary incompatibility reported by MiMa might look like the following:\n\nSolving a binary incompatibility\n\nIf you believe that your binary incompatibilies are justified or that MiMa reported false positives (e.g. the reported binary incompatibilities are about a non-user facing API), you can filter them out by adding an exclusion in project/MimaExcludes.scala containing what was suggested by the MiMa report and a comment containing the JIRA number of the issue you’re working on as well as its title.\n\nFor the problem described above, we might add the following:\n\nOtherwise, you will have to resolve those incompatibilies before opening or updating your pull request. Usually, the problems reported by MiMa are self-explanatory and revolve around missing members (methods or fields) that you will have to add back in order to maintain binary compatibility.\n\nChecking out pull requests\n\nGit provides a mechanism for fetching remote pull requests into your own local repository. This is useful when reviewing code or testing patches locally. If you haven’t yet cloned the Spark Git repository, use the following command:\n\nTo enable this feature you’ll need to configure the git remote repository to fetch pull request data. Do this by modifying the .git/config file inside of your Spark directory. The remote may not be named “origin” if you’ve named it something else:\n\nOnce you’ve done this you can fetch remote pull requests\n\nGenerating dependency graphs\n\nOrganizing imports\n\nYou can use a IntelliJ Imports Organizer from Aaron Davidson to help you organize the imports in your code. It can be configured to match the import ordering from the style guide.\n\nFormatting code\n\nTo format Scala code, run the following command prior to submitting a PR:\n\nBy default, this script will format files that differ from git master. For more information, see scalafmt documentation, but use the existing script not a locally installed version of scalafmt.\n\nIDE setup\n\nIntelliJ\n\nWhile many of the Spark developers use SBT or Maven on the command line, the most common IDE we use is IntelliJ IDEA. You can get the community edition for free (Apache committers can get free IntelliJ Ultimate Edition licenses) and install the JetBrains Scala plugin from Preferences > Plugins.\n\nTo create a Spark project for IntelliJ:\n\nDownload IntelliJ and install the Scala plug-in for IntelliJ.\n\nGo to File -> Import Project, locate the spark source directory, and select “Maven Project”.\n\nIn the Import wizard, it’s fine to leave settings at their default. However it is usually useful to enable “Import Maven projects automatically”, since changes to the project structure will automatically update the IntelliJ project.\n\nAs documented in Building Spark, some build configurations require specific profiles to be enabled. The same profiles that are enabled with -P[profile name] above may be enabled on the Profiles screen in the Import wizard. For example, if developing for Hadoop 2.7 with YARN support, enable profiles yarn and hadoop-2.7. These selections can be changed later by accessing the “Maven Projects” tool window from the View menu, and expanding the Profiles section.\n\nOther tips:\n\n“Rebuild Project” can fail the first time the project is compiled, because generate source files are not automatically generated. Try clicking the “Generate Sources and Update Folders For All Projects” button in the “Maven Projects” tool window to manually generate these sources.\n\nThe version of Maven bundled with IntelliJ may not be new enough for Spark. If that happens, the action “Generate Sources and Update Folders For All Projects” could fail silently. Please remember to reset the Maven home directory (Preference -> Build, Execution, Deployment -> Maven -> Maven home directory) of your project to point to a newer installation of Maven. You may also build Spark with the script build/mvn first. If the script cannot locate a new enough Maven installation, it will download and install a recent version of Maven to folder build/apache-maven-<version>/.\n\nSome of the modules have pluggable source directories based on Maven profiles (i.e. to support both Scala 2.11 and 2.10 or to allow cross building against different versions of Hive). In some cases IntelliJ’s does not correctly detect use of the maven-build-plugin to add source directories. In these cases, you may need to add source locations explicitly to compile the entire project. If so, open the “Project Settings” and select “Modules”. Based on your selected Maven profiles, you may need to add source folders to the following modules:\n\nspark-hive: add v0.13.1/src/main/scala\n\nspark-streaming-flume-sink: add target\\scala-2.11\\src_managed\\main\\compiled_avro\n\nspark-catalyst: add target\\scala-2.11\\src_managed\\main\n\nCompilation may fail with an error like “scalac: bad option: -P:/home/jakub/.m2/repository/org/scalamacros/paradise_2.10.4/2.0.1/paradise_2.10.4-2.0.1.jar”. If so, go to Preferences > Build, Execution, Deployment > Scala Compiler and clear the “Additional compiler options” field. It will work then although the option will come back when the project reimports. If you try to build any of the projects using quasiquotes (eg., sql) then you will need to make that jar a compiler plugin (just below “Additional compiler options”). Otherwise you will see errors like:\n\n/Users/irashid/github/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala Error:(147, 9) value q is not a member of StringContext Note: implicit class Evaluate2 is not applicable here because it comes after the application point and it lacks an explicit result type q\"\"\" ^\n\nDebug Spark remotely\n\nThis part will show you how to debug Spark remotely with IntelliJ.\n\nSet up remote debug configuration\n\nFollow Run > Edit Configurations > + > Remote to open a default Remote Configuration template:\n\nNormally, the default values should be good enough to use. Make sure that you choose Listen to remote JVM as Debugger mode and select the right JDK version to generate proper Command line arguments for remote JVM.\n\nOnce you finish configuration and save it. You can follow Run > Run > Your_Remote_Debug_Name > Debug to start remote debug process and wait for SBT console to connect:\n\nTrigger the remote debugging\n\nIn general, there are 2 steps:\n\nSet JVM options using the Command line arguments for remote JVM generated in the last step.\n\nStart the Spark execution (SBT test, pyspark test, spark-shell, etc.)\n\nThe following is an example of how to trigger the remote debugging using SBT unit tests.\n\nEnter in SBT console\n\nSwitch to project where the target test locates, e.g.:\n\nCopy pasting the Command line arguments for remote JVM\n\nSet breakpoints with IntelliJ and run the test with SBT, e.g.:\n\nIt should be successfully connected to IntelliJ when you see “Connected to the target VM, address: ‘localhost:5005’, transport: ‘socket’” in IntelliJ console. And then, you can start debug in IntelliJ as usual.\n\nTo exit remote debug mode (so that you don’t have to keep starting the remote debugger), type “session clear” in SBT console while you’re in a project.\n\nEclipse\n\nEclipse can be used to develop and test Spark. The following configuration is known to work:\n\nEclipse Juno\n\nScala IDE 4.0\n\nScala Test\n\nThe easiest way is to download the Scala IDE bundle from the Scala IDE download page. It comes pre-installed with ScalaTest. Alternatively, use the Scala IDE update site or Eclipse Marketplace.\n\nSBT can create Eclipse .project and .classpath files. To create these files for each Spark sub project, use this command:\n\nTo import a specific project, e.g. spark-core, select File | Import | Existing Projects into Workspace. Do not select “Copy projects into workspace”.\n\nIf you want to develop on Scala 2.10 you need to configure a Scala installation for the exact Scala version thatâs used to compile Spark. Since Scala IDE bundles the latest versions (2.10.5 and 2.11.8 at this point), you need to add one in Eclipse Preferences -> Scala -> Installations by pointing to the lib/ directory of your Scala 2.10.5 distribution. Once this is done, select all Spark projects and right-click, choose Scala -> Set Scala Installation and point to the 2.10.5 installation. This should clear all errors about invalid cross-compiled libraries. A clean build should succeed now.\n\nScalaTest can execute unit tests by right clicking a source file and selecting Run As | Scala Test.\n\nIf Java memory errors occur, it might be necessary to increase the settings in eclipse.ini in the Eclipse install directory. Increase the following setting as needed:\n\nNightly builds\n\nSpark publishes SNAPSHOT releases of its Maven artifacts for both master and maintenance branches on a nightly basis. To link to a SNAPSHOT you need to add the ASF snapshot repository to your build. Note that SNAPSHOT artifacts are ephemeral and may change or be removed. To use these you must add the ASF snapshot repository at https://repository.apache.org/snapshots/.\n\nProfiling Spark applications using YourKit\n\nHere are instructions on profiling Spark applications using YourKit Java Profiler.\n\nOn Spark EC2 images\n\nAfter logging into the master node, download the YourKit Java Profiler for Linux from the YourKit downloads page. This file is pretty big (~100 MB) and YourKit downloads site is somewhat slow, so you may consider mirroring this file or including it on a custom AMI.\n\nUnzip this file somewhere (in /root in our case): unzip YourKit-JavaProfiler-2017.02-b66.zip\n\nCopy the expanded YourKit files to each node using copy-dir: ~/spark-ec2/copy-dir /root/YourKit-JavaProfiler-2017.02\n\nConfigure the Spark JVMs to use the YourKit profiling agent by editing ~/spark/conf/spark-env.sh and adding the lines\n\nSPARK_DAEMON_JAVA_OPTS+=\" -agentpath:/root/YourKit-JavaProfiler-2017.02/bin/linux-x86-64/libyjpagent.so=sampling\" export SPARK_DAEMON_JAVA_OPTS SPARK_EXECUTOR_OPTS+=\" -agentpath:/root/YourKit-JavaProfiler-2017.02/bin/linux-x86-64/libyjpagent.so=sampling\" export SPARK_EXECUTOR_OPTS\n\nCopy the updated configuration to each node: ~/spark-ec2/copy-dir ~/spark/conf/spark-env.sh\n\nRestart your Spark cluster: ~/spark/bin/stop-all.sh and ~/spark/bin/start-all.sh\n\nBy default, the YourKit profiler agents use ports 10001-10010. To connect the YourKit desktop application to the remote profiler agents, you’ll have to open these ports in the cluster’s EC2 security groups. To do this, sign into the AWS Management Console. Go to the EC2 section and select Security Groups from the Network & Security section on the left side of the page. Find the security groups corresponding to your cluster; if you launched a cluster named test_cluster, then you will want to modify the settings for the test_cluster-slaves and test_cluster-master security groups. For each group, select it from the list, click the Inbound tab, and create a new Custom TCP Rule opening the port range 10001-10010. Finally, click Apply Rule Changes. Make sure to do this for both security groups. Note: by default, spark-ec2 re-uses security groups: if you stop this cluster and launch another cluster with the same name, your security group settings will be re-used.\n\nLaunch the YourKit profiler on your desktop.\n\nSelect “Connect to remote application…” from the welcome screen and enter the the address of your Spark master or worker machine, e.g. ec2--.compute-1.amazonaws.com\n\nYourKit should now be connected to the remote profiling agent. It may take a few moments for profiling information to appear.\n\nPlease see the full YourKit documentation for the full list of profiler agent startup options.\n\nIn Spark unit tests\n\nWhen running Spark tests through SBT, add javaOptions in Test += \"-agentpath:/path/to/yjp\" to SparkBuild.scala to launch the tests with the YourKit profiler agent enabled.\n\nThe platform-specific paths to the profiler agents are listed in the YourKit documentation.\n\nGenerative tooling usage\n\nIn general, the ASF allows contributions co-authored using generative AI tools. However, there are several considerations when you submit a patch containing generated content.\n\nForemost, you are required to disclose usage of such tool. Furthermore, you are responsible for ensuring that the terms and conditions of the tool in question are compatible with usage in an Open Source project and inclusion of the generated content doesn’t pose a risk of copyright violation."
    }
}
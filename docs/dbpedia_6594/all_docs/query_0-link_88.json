{
    "id": "dbpedia_6594_0",
    "rank": 88,
    "data": {
        "url": "https://medium.com/%40emilymenonbender/on-nyt-magazine-on-ai-resist-the-urge-to-be-impressed-3d92fd9a0edd",
        "read_more_link": "",
        "language": "en",
        "title": "On NYT Magazine on AI: Resist the Urge to be Impressed",
        "top_image": "https://miro.medium.com/v2/resize:fit:1200/1*IsGk37WSaoeQBJdrLh4AKQ.jpeg",
        "meta_img": "https://miro.medium.com/v2/resize:fit:1200/1*IsGk37WSaoeQBJdrLh4AKQ.jpeg",
        "images": [
            "https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png",
            "https://miro.medium.com/v2/resize:fill:88:88/1*dCPCAOVYmxTwgjz-0ooINA.jpeg",
            "https://miro.medium.com/v2/resize:fill:144:144/1*dCPCAOVYmxTwgjz-0ooINA.jpeg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Emily M. Bender",
            "medium.com"
        ],
        "publish_date": "2022-04-18T00:15:09.917000+00:00",
        "summary": "",
        "meta_description": "[Now available as an â€œaudiopaperâ€ on my soundcloud. (Please excuse occasional noise from airplanes overhead + my inconsistency about whether to render quote marks out loud.)] On April 15, 2022â€¦",
        "meta_lang": "en",
        "meta_favicon": "https://miro.medium.com/v2/5d8de952517e8160e40ef9841c781cdc14a5db313057fa3c3de41c6f5b494b19",
        "meta_site_name": "Medium",
        "canonical_link": "https://medium.com/@emilymenonbender/on-nyt-magazine-on-ai-resist-the-urge-to-be-impressed-3d92fd9a0edd",
        "text": "[Now available as an â€œaudiopaperâ€ on my soundcloud. (Please excuse occasional noise from airplanes overhead + my inconsistency about whether to render quote marks out loud.)]\n\nContext\n\nOn April 15, 2022, Steven Johnson published a piece in the New York Times Magazine entitled â€œA.I. Is Mastering Language. Should We Trust What It Says?â€ I knew this piece was coming, because I had been interviewed for it, over email, a couple of weeks ago. I read it with some trepidation, because I had the sense that Johnsonâ€™s question and goals going into the article did not maintain sufficient skepticism of the claims of AI boosters. At the same time, I was also fairly confident my words werenâ€™t going to be taken out of context because Iâ€™d been contacted by a fact checker who was verifying the quotes they intended to use.\n\nOn reading the article, my expectations were met on both counts. Ordinarily, when I encounter AI hype in media coverage of research/products that claim to be â€œAIâ€, I get inspired to write tweet threads aiming to educate folks on how to spot and thus resist such hype. (Hereâ€™s a recent example.) Johnsonâ€™s article is ~10k words long, though, and so Iâ€™ve decided to try to do the same in blog form, rather than as a tweet thread.\n\ntl;dr\n\nYes, there is an urgent need to address the harm being done by so-called â€œAIâ€ and to set up effective regulation and governance so that those who are impacted by this technology have power over how it is deployed.\n\nBut no, the harms arenâ€™t going to come from autonomous â€œAIâ€ that just hasnâ€™t been taught appropriate values.\n\nAnd no, the solution isnâ€™t to try to build â€œAIâ€ (or â€œAGIâ€) faster or â€œoutsideâ€ the megacorps. (Scare quotes on â€œoutsideâ€ there, because OpenAI isnâ€™t really as independent as they claim â€” given both the source of their initial funding and their deal with Microsoft.)\n\nWhatâ€™s needed is not something out of science fiction â€” itâ€™s regulation, empowerment of ordinary people and empowerment of workers.\n\nPuff pieces that fawn over what Silicon Valley techbros have done, with amassed capital and computing power, are not helping us get any closer to solutions to problems created by the deployment of so-called â€œAIâ€. On the contrary, they make it harder by refocusing attention on strawman problems.\n\nIf youâ€™d like to learn more about what is going on and what shape meaningful solutions could take, I recommend authors such as Safiya Noble, Meredith Broussard, Ruha Benjamin, Shoshana Zuboff, Abeba Birhane, Joy Buolamwini and her colleagues at the Algorithmic Justice League, and journalists such as Khari Johnson, Edward Ongweso Jr, and Karen Hao (see especially this piece on OpenAI).\n\nOn asking the right questions\n\nI am not a journalist, but it seems to me that a key lesson from research must hold in journalism too: In research, the questions we ask shape what we can find and how the conversation of scholarship is advanced. Similarly, in journalism, I would believe that the question a journalist asks in a piece of writing shapes how the public can be educated.\n\nThe headline (not necessarily due to Johnson, but I think a not incongruent framing of the article, in this case) asks: â€œA.I. Is Mastering Language. Should We Trust What It Says?â€ In taking up this question, the article is not keeping any distance from the point of view of the primary organization it is covering (OpenAI) but rather adopting Open AIâ€™s stance towards their own technology wholesale. This headline asserts that â€œAIâ€ has â€œmastered languageâ€ (spoiler alert: it hasnâ€™t) and in the process presupposes the existence of something that can be referred to as â€œAIâ€. Those who disagree with this assertion (ğŸ‘‹) are framed in the article as â€œskepticsâ€ â€” more on this below. The second part of the headline â€œShould We Trust What It Says?â€ frames â€œsayingâ€ as done by â€œAIâ€ as analogous to â€œsayingâ€ as done by â€œpeopleâ€ â€¦ and nothing in the article really highlights any difference between the two either.\n\nWhen thinking about trust, trustworthiness, and the pattern recognition technology that gets billed as â€œAIâ€, I think there are a lot of valuable questions to be asked, including:\n\nWhy are people so quick to be impressed by the output of large language models (LLMs)? (Nota bene: This is not a new observation. It goes back at least to the way people reacted to Eliza, see Weizenbaum 1976)\n\nIn what ways are corporations leveraging that credulousness, on the part of users, investors, regulators?\n\nWhere is this technology being deployed, and what are the potential consequences? Who is bearing the brunt? (We â€” Timnit Gebru, Angelina McMillan-Major, Meg Mitchell, our further co-authors and I â€” talked about various kinds of potential harm in the Stochastic Parrots ğŸ¦œ paper, but I would be very interested in journalistic work on actual deployments.)\n\nWhat would effective regulation look like in this space? Who is working on that regulation?\n\nWith respect to OpenAI specifically, I think it would be useful to ask:\n\nHow is OpenAI shaping the conversation around so-called â€œAIâ€, as developed by them or others?\n\nHow does OpenAIâ€™s rhetoric around â€œartificial general intelligenceâ€ shape public and regulator understanding of claims of other companies, such as those who purport to â€œpredictâ€ recidivism risk, â€œrecognizeâ€ emotion, or â€œdiagnoseâ€ mental health conditions?\n\nWhat are the relationships between OpenAIâ€™s staff/board members/founders and other organizations?\n\nWhat are the financial incentives at play, and whose interests do they represent (noting OpenAIâ€™s â€œexclusive computing partnershipâ€ with Microsoft)?\n\nBuying into the hype\n\nHere are a selection of examples of how Johnsonâ€™s writing betrays his commitment to the notion that â€œAIâ€ (or maybe even â€œAGIâ€?) is a thing that exists or is on the verge of existing, very much in line with how the folks at OpenAI see things. Some are more subtle, others are more blatant. This section is quite long, because in 10k words, Johnson provided many examples of this. If you get bored, just jump ahead to the next headingâ€¦\n\nAll quotes here and below are from the NYT Magazine article linked above; all highlighting (via boldface) is added by me.\n\nGPT-3 has been trained to write Hollywood scripts and compose nonfiction in the style of Gay Taleseâ€™s New Journalism classic â€˜â€˜Frank Sinatra Has a Cold.â€™â€™\n\nTalking about â€œtrainingâ€ machine learning systems is the standard terminology. I think it is always worth pausing and considering on what grounds we are talking about â€œtrainingâ€, â€œlearningâ€ and â€œintelligenceâ€ in these systems; what metaphors are at play; and to what extent we are asked, as readers, to nod along to the metaphor without any clear guidance as to which aspects apply and which are merely suggestive.\n\nIn the case of (large) language models like GPT-3, the â€œtrainingâ€ involves taking a mathematical model with random mathematical parameters (â€œweightsâ€) and iteratively adjusting those weights in response to differences between model output and some point of comparison showing expected output. For GPT-3, the primary â€œtrainingâ€ is just next word prediction over enormous amounts of text. There is then also a notion of â€œfine-tuningâ€ where that pre-trained model is shown a small set of â€œpromptsâ€ and expected responses to those prompts. I actually canâ€™t tell if â€œtrainingâ€ in the quote above refers to the general pre-training or to people using that pre-trained model, via fine-tuning, in those different use cases.\n\nOthers have fed the software prompts that generate patently offensive or delusional responses, showcasing the limitations of the model and its potential for harm if adopted widely in its current state.\n\nHere, what I want to point up is how the phrase â€œits current stateâ€ suggests that there is a developmental path that systems like GPT-3 are treading, and the problems that people have noted with GPT-3 (in various use cases) are addressable, somewhere along that path, and not actually fundamental mismatches between technology and purpose.\n\nSo far, the experiments with large language models have been mostly that: experiments probing the model for signs of true intelligence, exploring its creative uses, exposing its biases.\n\nIndeed, people have been probing GPT-3 and its ilk in various ways. Iâ€™m not sure how many would say they are looking for â€œsigns of true intelligenceâ€ but OpenAIâ€™s Ilya Sutskever did infamously muse on Twitter in February:\n\nI would expect serious journalism looking into this work to ask why anyone would expect GPT-3 and its ilk to â€œhave intelligenceâ€ and furthermore to demand a definition of intelligence. Just noting off hand that people are looking into it again supports OpenAIâ€™s (and othersâ€™) AI hype.\n\nthere was a sense that the long â€˜â€˜A.I. winter,â€™â€™ the decades in which the field failed to live up to its early hype, was finally beginning to thaw.\n\nThis is an odd use of the term â€œAI winterâ€, which is usually used to refer to the lack of funding for AI research that results from over-promising (over-hyping) and then (necessarily) failing to deliver on the hype. This sentence instead suggests that AI is now living up to the hype, which in turn serves to support (and amplify) the hype.\n\nBut GPT-3â€™s intelligence, if intelligence is the right word for it, comes from the bottom up: through the elemental act of next-word prediction.\n\nI guess the most charitable reading of this one is that next-word prediction is all that GPT-3 does in its primary training phase, and as such, next-word prediction is elemental for GPT-3. But â€œelemental actâ€ is a pretty grandiose term for this, and seems to elevate this banal task to something of deep importance.\n\nOn the side of emergent intelligence, a few points are worth making. First, large language models have been making steady improvements, year after year, on standardized reading comprehension tests.\n\nThe second statement here is true, in the sense that scores have been going up. But it is also misleading: just because the tests were designed to test for reading comprehension by people, and even if we assume that they do a good job of measuring that, doesnâ€™t mean that comparable scores by machines on the same tests entail that machines are doing something comparable. This comes down to the concept of â€œconstruct validityâ€: does the test actually measure some construct which is coherent in itself and effectively measured by the test? To establish construct validity for reading comprehension tests, we need a definition of what reading comprehension is as well as evidence that answering the test questions more accurately corresponds to more reading comprehension. There is no reason to believe, a priori, that a test designed to achieve those ends for humans would be equally effective for machines, because there is no reason to believe that machines use the same processes for answering the questions as humans do. (For more on construct validity in the evaluation of so-called â€œAIâ€ systems, see Raji et al 2021 or Ben Dicksonâ€™s popular press coverage of it, as well as Jacobs and Wallach 2021.)\n\nFurthemore, contextualizing this (ultimately insufficiently supported) claim about reading â€œcomprehensionâ€ as a â€œpoint worth makingâ€ about â€œemergent intelligenceâ€ only serves to further emphasize the (misleading) reading of the sentence that it really is something about intelligence, i.e. really something about machines â€œcomprehendingâ€ text.\n\nâ€œYouâ€™ll see that it puts the arms and the legs in the right place,â€ Murati points out. â€˜â€˜And thereâ€™s a tutu, and itâ€™s walking the dog just like it was a human, even though itâ€™s a baby radish. It shows you that GPT-3 really has quite a good conception of all the things that you were asking it to combine.â€™â€™\n\nQuoting such statements from OpenAI personnel (Murati is a senior VP) uncritically means that NYT Magazine is just platforming AI hype. What does â€œquite a good conceptionâ€ mean? The most natural interpretation is something like â€œthinks abstractlyâ€, so this statement boils down to claiming that â€œit puts the arms and the legs in the right placeâ€ establishes that GPT-3 â€œthinks abstractlyâ€. But, again, this claim is made without evidence, and Johnson doesnâ€™t demand any. Even if Murati has some other technical definition of â€œgood conceptionâ€ in mind, how are NYT Magazine readers expected to understand it except as analogous to what humans do?\n\nIn a way, you can think of GPT-3 as a purely linguistic version of the Cartesian brain in a vat or in a â€˜â€˜Matrixâ€™â€™-style cocoon:\n\nIâ€™m wondering what a â€œpurely linguisticâ€ version of these would even be. I suspect that Johnson (like many others) has mistaken the ability of GPT-3 and its ilk to manipulate linguistic form with actually acquiring a linguistic system. Languages are symbolic systems, and symbols are pairings of form and meaning (or, per de Saussure, signifier and signified). But GPT-3 in its training was only provided with the form part of this equation and so never had any hope of learning the meaning part.\n\nPerhaps Johnson is imagining an existence where he becomes limited to using his own existing linguistic capability to interact with the world, being cut off from all of his senses. But this is not analogous to what GPT-3 is doing â€” or what anything else, no matter what its internal architecture might be, could do with a â€œtrainingâ€ regime analogous to GPT-3â€™s. Alexander Koller and I lay out this argument in detail in Bender and Koller 2020 (for popular media coverage, see this piece by Will Douglas Heaven).\n\nSo, again, suggesting that the GPT-3 is like any kind of brain is pure hype.\n\nGPT-3 and its peers have made one astonishing thing clear: The machines have acquired language.\n\nThis statement is only true if we interpret â€œacquired languageâ€ to mean â€œhave been programmed to produce strings of text that humans who speak that language find coherentâ€. Given all of the surrounding hype, the reader could be forgiven for not being able to find that interpretation, and instead taking it to mean something else â€” necessarily false.\n\nAlso, and this bears repeating, whenever a computer scientist claims to be able to do something with â€œlanguageâ€ or â€œnatural languageâ€, itâ€™s worthwhile to ask, â€œWhich language?â€. In the case of GPT-3, the answer is primarily English, with a smattering of other languages, as happened to have appeared in the training data.\n\nPerhaps the game of predict-the-next-word is what children unconsciously play when they are acquiring language themselves: listening to what initially seems to be a random stream of phonemes from the adults around them, gradually detecting patterns in that stream and testing those hypotheses by anticipating words as they are spoken. Perhaps that game is the initial scaffolding beneath all the complex forms of thinking that language makes possible.\n\nChild language acquisition is a well-established field of study, with rich methodology and results. Johnsonâ€™s speculation is completely unfounded (and doesnâ€™t claim any foundations, being presented as mere musings), and simply serves to bolster the claim that GPT-3 is doing something like what children do, without presenting any evidence to that end. For a very brief overview of what the child language acquisition literature has to say that is relevant to this debate, see section 6 of Bender and Koller 2020.\n\nAll that glitters is not gold\n\nOne further kind of AI hype that Johnsonâ€™s piece peddles, and one that is particular to â€œgenerativeâ€ technology like GPT-3, is a sleight of hand that asks readers to believe that something that takes the form of a human artifact is equivalent to that artifact. Here are a few quick examples:\n\nGPT-3 has been trained to write Hollywood scripts and compose nonfiction in the style of Gay Taleseâ€™s New Journalism classic â€˜â€˜Frank Sinatra Has a Cold.â€™â€™\n\nGPT-3 canâ€™t â€œcompose nonfictionâ€. Nonfiction by definition is factual writing about the world. But GPT-3 has no access to facts, only to strings in its training data. To the extent that it outputs strings of words that humans interpret and can verify as factual, that factuality is and can only ever be purely accidental.\n\nFor instance, even without the kind of targeted training that OpenAI employed to create Codex, GPT-3 can already generate sophisticated legal documents, like licensing agreements or leases.\n\nSomeone who needs to create a licensing agreement or a lease doesnâ€™t just need a document that looks and feels like a licensing agreement or a lease. They need something that speaks to their particular situation and is legally binding in their particular jurisdiction. A set of strings that take the form of legalese is not a â€œsophisticated legal documentâ€.\n\nâ€¦ GPT-3â€™s recent track record suggests that other, more elite professions may be ripe for disruption. A few months after GPT-3 went online, the OpenAI team discovered that the neural net had developed surprisingly effective skills at writing computer software, even though the training data had not deliberately included examples of code.\n\nâ€œWriting softwareâ€ entails much, much more than generating code, even code that is syntactically correct and compiles. It includes, at least, determining the specifications of the system to be produced and creating tests to ensure that the system behaves as desired.\n\nIf you gave 100 high school students the same prompt, I doubt you would get more than a handful of papers that exceeded GPT-3â€™s attempt. And of course, GPT-3 wrote its version of the essay in half a second.\n\nThis leaves me wondering if Johnson has forgotten (or never understood) why high school students are asked to write essays. It is not, to be sure, to keep the worldâ€™s supply of essays topped up! Rather, it is about what the students learn in the process of doing the writing.\n\nOpenAI hagiography\n\nJohnsonâ€™s NYT Magazine piece frequently strays into straight up hagiography of OpenAI and the people who lead it, as in this characterization of Ilya Sutskever:\n\nâ€˜â€˜Here is the underlying idea of GPT-3,â€™â€™ Sutskever said intently, leaning forward in his chair. He has an intriguing way of answering questions: a few false starts â€” â€˜â€˜I can give you a description that almost matches the one you asked forâ€™â€™ â€” interrupted by long, contemplative pauses, as though he were mapping out the entire response in advance.\n\nJohnson also uncritically presents the OpenAI crew as setting out to save the world:\n\nAnd once again, all the evidence suggested that this power was going to be controlled by a few Silicon Valley megacorporations.\n\nThe agenda for the dinner on Sand Hill Road that July night was nothing if not ambitious: figuring out the best way to steer A.I. research toward the most positive outcome possible,\n\nThe hubris in thinking that as a group they would be positioned to â€œfigure that outâ€ is perhaps only surpassed by the naÃ¯vetÃ© in believing that the way to achieve that lies primarily in â€¦ building so-called â€œAGIâ€ systems. But Johnson reports this ambition uncritically, even admiringly, and doesnâ€™t seem to notice (until much, much further down) that there isnâ€™t a whole lot of daylight between the leaders of â€œa few Silicon Valley megacorporationsâ€ and the people at that dinner.\n\nToday, roughly a fifth of the organization is focused full time on what it calls â€˜â€˜safetyâ€™â€™ and â€˜â€˜alignmentâ€™â€™ (that is, aligning the technology with humanityâ€™s interests)\n\nAgain, there is absolutely no evidence presented that the folks at OpenAI are positioned to understand â€œhumanityâ€™s interestsâ€ (and not just their own/those of people like them) and the idea is presented uncritically until much, much further down the ~10k word piece where Johnson writes:\n\nBut beyond the charter itself, and the deliberate speed bumps and prohibitions established by its safety team, OpenAI has not detailed in any concrete way who exactly will get to define what it means for A.I. to â€˜â€˜benefit humanity as a whole.â€™â€™ Right now, those decisions are going to be made by the executives and the board of OpenAI â€” a group of people who, however admirable their intentions may be, are not even a representative sample of San Francisco, much less humanity. Up close, the focus on safety and experimenting â€˜â€˜when the stakes are very lowâ€™â€™ is laudable. But from a distance, itâ€™s hard not to see the organization as the same small cadre of Silicon Valley superheroes pulling the levers of tech revolution without wider consent, just as they have for the last few waves of innovation.\n\nLLMs arenâ€™t inevitable and the â€œwider webâ€ isnâ€™t representative\n\nThereâ€™s another category of missteps in this piece that I think can also be attributed to insufficient distance from the subject (here: OpenAI) and skepticism of their claims. With all of the resources being poured into LLMs (and similarly into very large models trained on image datasets), itâ€™s worth remembering that nothing is predestined about this. We can imagine other futures, but to do so, we have to maintain independence from the narrative being pushed by those who believe that â€œAGIâ€ is desirable and that LLMs are the path to it.\n\nIn that light, consider this passage from the NYT Magazine piece:\n\nL.L.M.s have even more troubling propensities as well: They can deploy openly racist language; they can spew conspiratorial misinformation; when asked for basic health or safety information they can offer up life-threatening advice. All those failures stem from one inescapable fact: To get a large enough data set to make an L.L.M. work, you need to scrape the wider web. And the wider web is, sadly, a representative picture of our collective mental state as a species right now, which continues to be plagued by bias, misinformation and other toxins.\n\nFirst, in fact, the third of those (offering up life-threatening advice) doesnâ€™t (only) stem from the fact that their training data is uncurated. Rather, itâ€™s connected to the fact that by design LLMs are just making stuff up. More specifically, theyâ€™re making up text strings in the language theyâ€™re trained on, which humans who speak that language can interpret. So when a person comes in with a health and safety related question, if what comes back is wrong, chances are high it will also be dangerous. Most importantly: these can be understood as reasons not to use LLMs (perhaps at all, but at least in very many specific applications). However, the piece as a whole seems to follow OpenAI (and others) in seeing these propensities as temporary shortcomings of LLMs â€œin their current stateâ€ (see above) which will be addressed in time. (And this even as the tech is being commercialized, i.e. deployed with known risks of harm.)\n\nSecond, the wider web is not a representative picture of humanity. In section 4 of Stochastic Parrots ğŸ¦œ we go through in detail how various forces influence who has access to the web, who is comfortable continuing to contribute, who is represented in the parts of the web selected for LLM training data, and how the rudimentary filtering applied to that training data further creates further distortion. (The academic paper itself is long and admittedly dense, but the 20-minute video we prepared as the presentation of the paper at FAccT 2021 might be more approachable.) Relatedly, Safiya Noble, in her tour de force work Algorithms of Oppression, shows how the advertising-driven economy of web search shapes results that people see even as Google (quite misleadingly) presents these results as â€œjust whatâ€™s out there naturallyâ€.\n\nNot just an academic debate\n\nWhether or not â€œAIâ€ actually works isnâ€™t just an academic debate. It could have been, if the people working on â€œAIâ€ were simply doing esoteric projects in research labs without trying to monetize them. But that is not the world we live in: it seems like every day there is a new news story about someone selling an â€œAIâ€ system to do something inappropriate, like fill in grades for students who couldnâ€™t take tests, or diagnose mental health disorders, interview job candidates, or apprehend migrants.\n\nSo it matters that the public at large have a clear understanding of what â€œAIâ€ can actually do, which purported applications are so much snake oil, and what questions to ask to discover the possible harms from these systems (whether they are working properly or not). But Johnson misses the opportunity to educate the public in these ways, on two counts.\n\nFirst, he talks up a possible application of LLMs in information retrieval (i.e. to replace search engines):\n\nIf the existing trajectory continues, software like GPT-3 could revolutionize how we search for information in the next few years. [â€¦] if the GPT-3 true believers are correct, in the near future youâ€™ll just ask an L.L.M. the question and get the answer fed back to you, cogently and accurately.\n\nThis is not just an esoteric debate. If you can use next-word-prediction to train a machine to express complex thoughts or summarize dense material, then we could be on the cusp of a genuine technological revolution where systems like GPT-3 replace search engines or Wikipedia as our default resource for discovering information.\n\nBut this is in fact a terrible idea, as Will Douglas Heaven put it in his recent MIT Tech Review article covering work by Chirag Shah and me (at CHIIR 2022) and by Martin Potthast et al (in SIGIR Forum, 2020).\n\nBut more broadly, Johnson narrows the scope of possible casualties of AI hype to AI research itself:\n\nBut if the large language models are ultimately just â€˜â€˜stochastic parrots,â€™â€™ then A.G.I. retreats once again to the distant horizon â€” and we risk as a society directing too many resources, both monetary and intellectual, in pursuit of a false oracle.\n\nYes, it is a problem that we have over-concentrated research resources on â€œAIâ€, when there are so many other worthy (and in many cases urgent) problems in the world. (And Iâ€™d say thatâ€™s true even if â€œAGIâ€ were a well-defined and feasible research goal.) But those arenâ€™t the only harms: every time someone applies pattern recognition at scale over data to produce systems that are supposedly â€œunbiasedâ€ and â€œobjectiveâ€ in a way that makes decisions (or produces content) affecting real humans, thereâ€™s harm. And all of that is left out of view by Johnsonâ€™s framing.\n\nOn being placed into the â€œskepticsâ€ box\n\nTo be fair, Johnson does spare a little room for dissenting voices (including quotes from me, Meredith Whittaker, and Gary Marcus). But he frames us as â€œskepticsâ€, sort of token nods to the other side in some â€œboth sidesâ€ reporting. But itâ€™s worse than that, in two ways: First, the skeptics framing seems to shift the burden of proof away from those who claim to be doing something outlandish (building â€œAGIâ€) and towards those who call out the unfounded claims.\n\nSome skeptics argue that the software is capable only of blind mimicry â€” that itâ€™s imitating the syntactic patterns of human language but is incapable of generating its own ideas or making complex decisions, a fundamental limitation that will keep the L.L.M. approach from ever maturing into anything resembling human intelligence.\n\nHaving presented that skepticism, he asks:\n\nHow can we determine whether GPT-3 is actually generating its own ideas or merely paraphrasing the syntax of language it has scanned from the servers of Wikipedia, or Oberlin College, or The New York Review of Books?\n\nBut he doesnâ€™t ask on what grounds we should believe that it is generating its own ideas. Instead, itâ€™s â€œno one really knowsâ€:\n\nSome people argue that higher-level understanding is emerging, thanks to the deep layers of the neural net. Others think the program by definition canâ€™t get to true understanding simply by playing â€˜â€˜guess the missing wordâ€™â€™ all day. But no one really knows.\n\nBut more grating to me is that being relegated to the â€œskepticsâ€ box cedes the framing of the debate to the AI boosters. I am not just saying â€œno thatâ€™s not how to build â€˜AIâ€™â€, nor is Meredith Whittaker (though Gary Marcus does seem to be mostly concentrated on that argument). For me (and I believe also Whittaker as well as the authors I cited in the tl;dr at the top of this post), the relevant question is not â€œhow do we build â€˜AIâ€™?â€ but rather things like â€œHow do we shift power so that we see fewer (ideally no) cases of algorithmic oppression?â€, â€œHow do we imagine and deploy design processes that locate technology as tools, shaped for and in the service of people working towards pro-social ends?â€, and â€œHow do we ensure the possibility of refusal, making it possible to shut down harmful applications and ensure recourse for those being harmed?â€\n\nSo, while I agree that there are very large ethical issues at stake here, I fundamentally disagree with the framing of this article. Perhaps nowhere is that more acute than in the proposed solution to the ethical issues:\n\nThe very premise that we are now having a serious debate over the best way to instill moral and civic values in our software should make it clear that we have crossed an important threshold.\n\nand\n\nWeâ€™ve never had to teach values to our machines before.\n\nJohnson quotes me in this part as saying:\n\nâ€œSo long as so-called A.I. systems are being built and deployed by the big tech companies without democratically governed regulation, they are going to primarily reflect the values of Silicon Valley,â€™â€™ Emily Bender argues, â€˜â€˜and any attempt to â€˜teachâ€™ them otherwise can be nothing more than ethics washing.â€™â€™\n\nThat quote is accurate, but there was a bit more to it. What I actually sent him was:\n\nTalking about â€œteaching machines valuesâ€ is a fundamental misframing of the situation and a piece of AI hype. Software systems are artifacts, not sentient entities, and as such â€œteachingâ€ is a misplaced and misleading metaphor (as is â€œmachine learningâ€ or â€œartificial intelligenceâ€). Rather, as with any other artifact or system, the builders of these systems are designing values into them. To say that they could be â€œtaughtâ€ other values is just another way to hide the values of the system builders (already designed in) behind a veneer of faux objectivity. So long as so-called â€œAIâ€ systems are being built and deployed by the big tech cos without democratically governed regulation they are going to primarily reflect the values of Silicon Valley and any attempt â€œteachâ€ them otherwise can be nothing more than ethics washing.\n\nBut I donâ€™t think that Johnson really took my point, because the article firmly holds to the framing of â€œteachingâ€ machines values:\n\nShould we build an A.G.I. that loves the Proud Boys, the spam artists, the Russian troll farms, the QAnon fabulists? Itâ€™s easier to build an artificial brain that interprets all of humanityâ€™s words as accurate ones, composed in good faith, expressed with honorable intentions. Itâ€™s harder to build one that knows when to ignore us.\n\nI hope anyone who has made it this far in this blog post can see the issues in that statement, how they imagine the problems in terms of assuming that it is both possible and desirable to build fully autonomous agents.\n\nAnd to take just one further example, Johnson writes, again imagining autonomous agents (â€œcitizensâ€, forsooth):\n\nAnd if large language models are in our future, then the most urgent questions become: How do we train them to be good citizens? How do we make them â€˜â€˜benefit humanity as a wholeâ€™â€™ when humanity itself canâ€™t agree on basic facts, much less core ethics and civic values?\n\nWhat I kept waiting (in vain) for this article to do was to break out from the OpenAI frame. Why build these things in the first place? If we are building them, why not think in terms of democratic governance that creates the frame in which they can be built and deployed, and requirements of transparency and documentation, rather than in terms of tinkering with the algorithms and their training data?\n\nConclusion\n\nThere is a talk Iâ€™ve given a couple of times now (first at the University of Edinburgh in August 2021) titled â€œMeaning making with artificial interlocutors and risks of language technologyâ€. I end that talk by reminding the audience to not be too impressed, and to remember:\n\nJust because that text seems coherent doesnâ€™t mean the model behind it has understood anything or is trustworthy\n\nJust because that answer was correct doesnâ€™t mean the next one will be\n\nWhen a computer seems to â€œspeak our languageâ€, weâ€™re actually the ones doing all of the work\n\nMaintaining this skeptical stance is non-trivial. Johnson writes:\n\nThe first few times I fed GPT-3 prompts of this ilk, I felt a genuine shiver run down my spine. It seemed almost impossible that a machine could generate text so lucid and responsive based entirely on the elemental training of next-word-prediction.\n\nand\n\nItâ€™s important to stress that this is not a question about the softwareâ€™s becoming self-aware or sentient. L.L.M.s are not conscious â€” thereâ€™s no internal â€˜â€˜theater of the mindâ€™â€™ where the software experiences thinking in the way sentient organisms like humans do. But when you read the algorithm creating original sentences on the role of metafiction, itâ€™s hard not to feel that the machine is thinking in some meaningful way.\n\nâ€œHard not to feelâ€ is apt. When we encounter something that seems to be speaking our language, without even thinking about it, we use the skills associated with using that language to communicate with other people. Those skills centrally involve intersubjectivity and joint attention and so we imagine a mind behind the language even when it is not there.\n\nBut reminding ourselves that all of that work is on our side, the human side, is of critical importance because it allows us a clearer view of the present, in which we can more accurately track the harm that people are doing with technology, and a broader view of the future, where we can work towards meaningful, democratic governance and appropriate regulation.\n\nAcknowledgments\n\nIâ€™d like to thank Timnit Gebru and Meg Mitchell for encouragement to sit down and write this piece and both of them as well as Leon Derczynski and Meredith Whittaker for thoughtful yet quick turn-around comments."
    }
}
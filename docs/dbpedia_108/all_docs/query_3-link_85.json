{
    "id": "dbpedia_108_3",
    "rank": 85,
    "data": {
        "url": "https://stackoverflow.com/questions/1857668/c-visual-studio-character-encoding-issues",
        "read_more_link": "",
        "language": "en",
        "title": "C++ Visual Studio character encoding issues",
        "top_image": "https://cdn.sstatic.net/Sites/stackoverflow/Img/apple-touch-icon@2.png?v=73d79a89bded",
        "meta_img": "https://cdn.sstatic.net/Sites/stackoverflow/Img/apple-touch-icon@2.png?v=73d79a89bded",
        "images": [
            "https://i.sstatic.net/lRiDX.jpg?s=64",
            "https://www.gravatar.com/avatar/16adaecf2fe2758f238dd726fef0cdbc?s=64&d=identicon&r=PG",
            "https://www.gravatar.com/avatar/04ce27f0ebe4713b51911cd8475c3b42?s=64&d=identicon&r=PG",
            "https://www.gravatar.com/avatar/52c6b7ff2a4e600ab52373b4e6b78cd7?s=64&d=identicon&r=PG",
            "https://i.sstatic.net/qalYg.jpg?s=64",
            "https://i.sstatic.net/XTgip.jpg?s=64",
            "https://i.sstatic.net/IJ0rj.jpg?s=64",
            "https://www.gravatar.com/avatar/60e4b218c9eb2b3127f3384639227294?s=64&d=identicon&r=PG&f=y&so-version=2",
            "https://i.sstatic.net/71jPC.png",
            "https://www.gravatar.com/avatar/2c16c7a34114664cf0cd1c5cab0b02e4?s=64&d=identicon&r=PG",
            "https://www.gravatar.com/avatar/cadf7019107d37900ab87b52a145a6ab?s=64&d=identicon&r=PG&f=y&so-version=2",
            "https://i.sstatic.net/iIDxh.png",
            "https://i.sstatic.net/PpJ2E.png",
            "https://www.gravatar.com/avatar/49c63e46b600e9d86ad90a5af90ccda6?s=64&d=identicon&r=PG&f=y&so-version=2",
            "https://stackoverflow.com/posts/1857668/ivc/642f?prg=224071f2-09cb-458b-b6c9-c0cddf7ecd7d"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2009-12-07T03:32:27",
        "summary": "",
        "meta_description": "Not being able to wrap my head around this one is a real source of shame...\n\nI'm working with a French version of Visual Studio (2008), in a French Windows (XP). French accents put in strings sent ...",
        "meta_lang": "en",
        "meta_favicon": "https://cdn.sstatic.net/Sites/stackoverflow/Img/favicon.ico?v=ec617d715196",
        "meta_site_name": "Stack Overflow",
        "canonical_link": "https://stackoverflow.com/questions/1857668/c-visual-studio-character-encoding-issues",
        "text": "Before I go any further, I should mention that what you are doing is not c/c++ compliant. The specification states in 2.2 what character sets are valid in source code. It ain't much in there, and all the characters used are in ascii. So... Everything below is about a specific implementation (as it happens, VC2008 on a US locale machine).\n\nTo start with, you have 4 chars on your cout line, and 4 glyphs on the output. So the issue is not one of UTF8 encoding, as it would combine multiple source chars to less glyphs.\n\nFrom you source string to the display on the console, all those things play a part:\n\nWhat encoding your source file is in (i.e. how your C++ file will be seen by the compiler)\n\nWhat your compiler does with a string literal, and what source encoding it understands\n\nhow your << interprets the encoded string you're passing in\n\nwhat encoding the console expects\n\nhow the console translates that output to a font glyph.\n\nNow...\n\n1 and 2 are fairly easy ones. It looks like the compiler guesses what format the source file is in, and decodes it to its internal representation. It generates the string literal corresponding data chunk in the current codepage no matter what the source encoding was. I have failed to find explicit details/control on this.\n\n3 is even easier. Except for control codes, << just passes the data down for char *.\n\n4 is controlled by SetConsoleOutputCP. It should default to your default system codepage. You can also figure out which one you have with GetConsoleOutputCP (the input is controlled differently, through SetConsoleCP)\n\n5 is a funny one. I banged my head to figure out why I could not get the é to show up properly, using CP1252 (western european, windows). It turns out that my system font does not have the glyph for that character, and helpfully uses the glyph of my standard codepage (capital Theta, the same I would get if I did not call SetConsoleOutputCP). To fix it, I had to change the font I use on consoles to Lucida Console (a true type font).\n\nSome interesting things I learned looking at this:\n\nthe encoding of the source does not matter, as long as the compiler can figure it out (notably, changing it to UTF8 did not change the generated code. My \"é\" string was still encoded with CP1252 as 233 0 )\n\nVC is picking a codepage for the string literals that I do not seem to control.\n\ncontrolling what the console shows is more painful than what I was expecting\n\nSo... what does this mean to you ? Here are bits of advice:\n\ndon't use non-ascii in string literals. Use resources, where you control the encoding.\n\nmake sure you know what encoding is expected by your console, and that your font has the glyphs to represent the chars you send.\n\nif you want to figure out what encoding is being used in your case, I'd advise printing the actual value of the character as an integer. char * a = \"é\"; std::cout << (unsigned int) (unsigned char) a[0] does show 233 for me, which happens to be the encoding in CP1252.\n\nBTW, if what you got was \"ÓÚÛ¨\" rather than what you pasted, then it looks like your 4 bytes are interpreted somewhere as CP850.\n\nBecause I was requested to, I’ll do some necromancy. The other answers were from 2009, but this article still came up on a search I did in 2018. The situation today is very different. Also, the accepted answer was incomplete even back in 2009.\n\nThe Source Character Set\n\nEvery compiler (including Microsoft’s Visual Studio 2008 and later, gcc, clang and icc) will read UTF-8 source files that start with BOM without a problem, and clang will not read anything but UTF-8, so UTF-8 with a BOM is the lowest common denominator for C and C++ source files.\n\nThe language standard doesn’t say what source character sets the compiler needs to support. Some real-world source files are even saved in a character set incompatible with ASCII. Microsoft Visual C++ in 2008 supported UTF-8 source files with a byte order mark, as well as both forms of UTF-16. Without a byte order mark, it would assume the file was encoded in the current 8-bit code page, which was always a superset of ASCII.\n\nThe Execution Character Sets\n\nIn 2012, the compiler added a /utf-8 switch to CL.EXE. Today, it also supports the /source-charset and /execution-charset switches, as well as /validate-charset to detect if your file is not actually UTF-8. This page on MSDN has a link to the documentation on Unicode support for every version of Visual C++.\n\nCurrent versions of the C++ standard say the compiler must have both an execution character set, which determines the numeric value of character constants like 'a', and a execution wide-character set that determines the value of wide-character constants like L'é'.\n\nTo language-lawyer for a bit, there are very few requirements in the standard for how these must be encoded, and yet Visual C and C++ manage to break them. It must contain about 100 characters that cannot have negative values, and the encodings of the digits '0' through '9' must be consecutive. Neither capital nor lowercase letters have to be, because they weren’t on some old mainframes. (That is, '0'+9 must be the same as '9', but there is still a compiler in real-world use today whose default behavior is that 'a'+9 is not 'j' but '«', and this is legal.) The wide-character execution set must include the basic execution set and have enough bits to hold all the characters of any supported locale. Every mainstream compiler supports at least one Unicode locale and understands valid Unicode characters specified with \\Uxxxxxxxx, but a compiler that didn’t could claim to be complying with the standard.\n\nThe way Visual C and C++ violate the language standard is by making their wchar_t UTF-16, which can only represent some characters as surrogate pairs, when the standard says wchar_t must be a fixed-width encoding. This is because Microsoft defined wchar_t as 16 bits wide back in the 1990s, before the Unicode committee figured out that 16 bits were not going to be enough for the entire world, and Microsoft was not going to break the Windows API. It does support the standard char32_t type as well.\n\nUTF-8 String Literals\n\nThe third issue this question raises is how to get the compiler to encode a string literal as UTF-8 in memory. You’ve been able to write something like this since C++11:\n\nconstexpr unsigned char hola_utf8[] = u8\"¡Hola, mundo!\";\n\nThis will encode the string as its null-terminated UTF-8 byte representation regardless of whether the source character set is UTF-8, UTF-16, Latin-1, CP1252, or even IBM EBCDIC 1047 (which is a silly theoretical example but still, for backward-compatibility, the default on IBM’s Z-series mainframe compiler). That is, it’s equivalent to initializing the array with { 0xC2, 0xA1, 'H', /* ... , */ '!', 0 }.\n\nIf it would be too inconvenient to type a character in, or if you want to distinguish between superficially-identical characters such as space and non-breaking space or precomposed and combining characters, you also have universal character escapes:\n\nconstexpr unsigned char hola_utf8[] = u8\"\\u00a1Hola, mundo!\";\n\nYou can use these regardless of the source character set and regardless of whether you’re storing the literal as UTF-8, UTF-16 or UCS-4. They were originally added in C99, but Microsoft supported them in Visual Studio 2015.\n\nEdit: As reported by Matthew, u8\" strings are buggy in some versions of MSVC, including 19.14. It turns out, so are literal non-ASCII characters, even if you specify /utf-8 or /source-charset:utf-8 /execution-charset:utf-8. The sample code above works properly in 19.22.27905.\n\nThere is another way to do this that worked in Visual C or C++ 2008, however: octal and hexadecimal escape codes. You would have encoded UTF-8 literals in that version of the compiler with:\n\nconst unsigned char hola_utf8[] = \"\\xC2\\xA1Hello, world!\";"
    }
}
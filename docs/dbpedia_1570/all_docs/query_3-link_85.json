{
    "id": "dbpedia_1570_3",
    "rank": 85,
    "data": {
        "url": "https://arxiv.org/html/2407.06320v1",
        "read_more_link": "",
        "language": "en",
        "title": "An Open-source Hardware/Software Architecture and Supporting Simulation Environment to Perform Human FPV Flight Demonstrations for Unmanned Aerial Vehicle Autonomy",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://arxiv.org/html/x1.png",
            "https://arxiv.org/html/extracted/5718366/pictures/drone2.png",
            "https://arxiv.org/html/extracted/5718366/pictures/GUISS.png",
            "https://arxiv.org/html/extracted/5718366/pictures/simulation_flowchart.png",
            "https://arxiv.org/html/x2.png",
            "https://arxiv.org/html/extracted/5718366/pictures/Task1.jpg",
            "https://arxiv.org/html/extracted/5718366/pictures/Task2.jpg",
            "https://arxiv.org/html/extracted/5718366/pictures/Task3.jpg",
            "https://arxiv.org/html/extracted/5718366/pictures/Task4.jpg",
            "https://arxiv.org/html/x3.png",
            "https://arxiv.org/html/x4.png",
            "https://arxiv.org/html/x5.png",
            "https://arxiv.org/html/x6.png",
            "https://arxiv.org/html/x7.png",
            "https://arxiv.org/html/x8.png",
            "https://arxiv.org/html/x9.png",
            "https://arxiv.org/html/x10.png",
            "https://arxiv.org/html/x11.png",
            "https://arxiv.org/html/x12.png",
            "https://arxiv.org/html/x13.png",
            "https://arxiv.org/html/x14.png",
            "https://arxiv.org/html/x15.png",
            "https://arxiv.org/html/x16.png",
            "https://arxiv.org/html/x17.png",
            "https://arxiv.org/html/x18.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Haosong Xiao1, Prajit Krisshnakumar1, Jagadeswara P K V Pothuri1,\n\nPuru Soni2, Eric Butcher2 and Souma Chowdhury1,2 † Corresponding Author: soumacho@buffalo.edu1 Mechanical & Aerospace Eng. Dep., University at Buffalo, Buffalo, NY 2 Computer Science and Eng. Dep., University at Buffalo, Buffalo, NY *This work was supported by Stephen Still Institute for Sustainable Transportation and Logistics (SSISTL) at University at Buffalo, the National Science Foundation (NSF) award CMMI 2048020, and the AFOSR DURIP award 13369473. Any opinions, findings, conclusions, or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of the SSISTL, NSF, and AFOSR.*Copyright ©2024 AIAA. Personal use of this material is permitted. Permission from AIAA must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works\n\nAbstract\n\nSmall multi-rotor unmanned aerial vehicles (UAVs), mainly quadcopters, are nowadays ubiquitous in research on aerial autonomy, including serving as scaled-down models for much larger aircraft such as vertical-take-off-and-lift vehicles for urban air mobility. Among the various research use cases, first-person-view (FPV) RC flight experiments allow for collecting data on how human pilots fly such aircraft, which could then be used to compare, contrast, validate, or train (via imitative learning) autonomous flight agents. While this could be uniquely beneficial, especially for studying UAV operation in contextually complex and safety-critical environments such as in human-UAV shared spaces, the lack of inexpensive and open-source hardware/software platforms that offer this capability along with low-level (complete access) to the underlying control software and data remains limited. To address this gap and significantly reduce barriers to human-guided autonomy research with UAVs, this paper presents an open-source software architecture implemented with an inexpensive in-house built quadcopter platform based on the F450 Quadcopter Frame. This setup uses two cameras to provide a dual-view FPV and an open-source flight controller, Pixhawk. The underlying software architecture, developed using the Python-based Kivy library, allows logging telemetry, GPS, control inputs, and camera frame data in a synchronized manner on the ground station computer. Since costs (time) and weather constraints typically limit numbers of physical outdoor flight experiments, this paper also presents a unique AirSim/Unreal Engine based simulation environment and graphical user interface (GUI) – aka digital twin – that provides a Hardware In The Loop (HITL) setup via the Pixhawk flight controller. We demonstrate the usability and reliability of the overall framework through a set of diverse physical FPV flight experiments and corresponding flight tests in the digital twin.\n\nI INTRODUCTION\n\nUnmanned Aerial Vehicle (UAV) refers to a variety of flying equipment capable of performing tasks without an onboard human pilot. Initially introduced in the early 19th century for military use, quadcopters have evolved significantly with the advancement of sensors and microprocessors [1]. Modern UAVs range from small, electrically powered devices suitable for indoor navigation to larger aircraft capable of carrying passengers on long-range missions. Among UAVs, quadcopters are particularly popular due to their versatility and stability. The transportation sector greatly benefits from quadcopters, especially in Urban Air Mobility (UAM), last-mile delivery, and traffic monitoring, where their abilities in accurate GPS navigation, obstacle avoidance, and payload handling are indispensable. In the realm of entertainment, quadcopters elevate aerial filming with multi-UAV cooperation[2], introducing both redundancy and challenges like increased collision risks and the need for advanced obstacle avoidance[3][4]. Their role in localization, particularly in search and rescue missions[5][6], is pivotal, addressing crises in diverse terrains and demanding exceptional maneuverability[7]. In construction, quadcopters are revolutionizing practices by enabling high-altitude visual inspections[8], pipeline monitoring[9], and site surveillance[10]. While the adoption of quadcopters for these complex and dangerous operational tasks significantly reduces the likelihood of human injury, it concurrently elevates the demand for highly skilled First Person View (FPV) quadcopter pilots who can manage these complex tasks. Market research projects a substantial growth in this demand, anticipating a 51.1 percent increase from 2022 to 2027[11]. In FPV flying, pilots control the quadcopters remotely from a first-person view using cameras mounted on them to navigate. In Urban Air Mobility (UAM) and last-mile delivery scenarios, FPV pilots expertly navigate through dense urban environments, where their ability to maneuver with precision is crucial for ensuring the safety of both people and the quadcopter themselves. The explosive advancement in artificial intelligence (AI) research further underscores this trend, shifting quadcopter operations towards increased autonomy and thereby diminishing the need for direct human pilot intervention [12]. This shift is particularly consequential for developing and operating electric Vertical Take-Off and Landing (VTOL) vehicles. AI’s capability to process vast amounts of data in real-time potentially enhances the safety and reliability of these vehicles. For VTOLs, which are complex systems requiring precise control, AI algorithms can significantly improve navigation accuracy, energy efficiency, and overall operational effectiveness. This evolution marks a pivotal transition from traditional piloted systems to more autonomous, AI-driven solutions, reshaping the future of urban air mobility and beyond. Current research largely focuses on the design and control of VTOL systems [13, 14, 15], operational efficiency, and safety measures, with few emerging studies examining the strategic use of reinforcement learning for air traffic control and scheduling within vertiports to optimize the deployment of eVTOLs in urban air mobility scenarios [16, 17, 18]. To effectively integrate autonomous capability into any aerial vehicle, including VTOL, its AI must be trained on extensive data derived from human pilots. This is critical for replicating human-like decision-making and providing a benchmark for autonomous flight systems. This ensures AI’s adaptability and public trust [19, 20]. Recent fields of research to align the performance of autonomous quadcopters with that of expert FPV quadcopter pilots refer to deep reinforcement learning [21]and imitation learning [22]. The importance of deep reinforcement learning in quadcopter autonomy is crucial, particularly for autonomous decision-making in complex environments. This is evident in research by Wang et al.[23], Çetin et al.[24], and Anwar and Raychowdhury[25], which demonstrates the effectiveness of reinforcement learning in improving quadcopters’ decision-making, especially in challenging settings like obstacle-rich areas or resource-limited environments. In the field of imitation learning, quadcopter learning from expert pilots significantly enhances maneuverability and efficiency. Most of these methods used existing open-source flight data or expensive hardware to collect new data, limiting access to those with substantial resources. This barrier restricts smaller research teams and individual innovators who may not have the financial capability to invest in high-cost equipment. Further, current open-source datasets like EuRoC [26] and Zurich Urban MAV [27] provide robust image-based data with flight state information, but they are limited by their third-person view collection methodology. Moreover, frameworks used for autonomous FPV racing rely on indoor motion capture systems, and outdoor versions require external Leica trackers [28] for ground truth location, significantly posing challenges in terms of cost and spatial limitations. Hence, the primary objective of this paper is to introduce an open-source framework equipped with FPV capabilities that is both cost-effective and readily accessible. This framework’s software stack aims to enable precise capturing of human pilot data, facilitating the development of advanced AI systems. The platform is based entirely on the open-source flight controller - Pixhawk. Pixhawk supports multiple flying modes, accommodating FPV Quadcopter pilots of varying experience levels with its dual-view FPV user interface. In conjunction with this interface, Mavlink [29] communication will be established to fetch real-time quadcopter state and control signal data during FPV flights. Additionally, this data will be synchronously saved with dual-frame images and corresponding timestamps after each flight. The advantage of using Pixhawk as a flight controller is its flexibility in switching between quadcopter frames, allowing an extendable change from quadcopter FPV to fixed-wing FPV quadcopter at an acceptable cost compared to current market camera quadcopter models. The secondary objective is to develop a digital twin platform (virtual environment) with Hardware-in-the-loop (HITL) capabilities and a GUI, providing an alternative solution for generating realistic, high-fidelity data, especially in scenarios where physical experimentation is expensive or challenging to perform very often [30]. This digital twin approach allows for safe, repeatable, and controlled testing without the constraints associated with physical experiments.\n\nThe remainder of this paper is structured as follows: In Sec. II, we explain the components used in building the hardware, the software architecture and HITL simulation framework. Section III introduces the experiments performed to validate our framework, and in Sec. IV, we compare the results of experiments from both physical and digital twin approaches. Finally, in Sec. V, we provide our concluding remarks.\n\nII Hardware and Simulation Framework\n\nThe in-house built FPV quadcopter framework, as shown in figure 1, requires a finely-tuned FPV platform and an informative Graphical User Interface (GUI) to support pilots by providing real-time FPV camera views and flying state data such as ground speed, climbing speed, and relative altitude. We also implemented Airsim HITL with GUI in Unreal Engine to support FPV flight in virtual environment. The below subsections will deep into the development of hardware and software platforms. For simplicity, in later sections, the in-house built FPV quadcopter will be referred to as the physical quadcopter, and the Airsim HITL platform with compiled GUI will be referred to as the digital twin\n\nII-A Hardware Platform\n\nThis section outlines physical quadcopter construction. The final product is shown in figure 2, and it includes a frame, brushless motors, propeller blades, battery, electronic speed controller (ESC), RC receiver, transmitter, flight controller, companion computer, and two cameras. Table I lists the hardware components used in the platform as well as the weights of each component, while table II lists the performance parameters.\n\nII-A1 Frame\n\nThe frame of the physical quadcopter utilizes the F450 model from YoungRC, selected for its high-strength arms, which provide a balance of durability and weight management. The frame includes an integrated PCB board, streamlining the soldering process for ESCs and power modules and thereby improving the reliability of connections. Furthermore, the frame’s large mounting tab on the bottom plate affords ample space for mounting a bottom-facing camera, enhancing support for FPV flight and precision in landing tasks.\n\nII-A2 Motors and Propellers\n\nConsidering the chosen frame for the physical quadcopter, ESCs, motors, and propellers were carefully selected to ensure sufficient thrust, smooth flight performance, and efficient power consumption without overloading the motors. A set of 920 Kv brushless motors paired with 9.4-inch propeller blades was chosen, taking into account the frame’s size and the quadcopter’s agility requirements.\n\nII-A3 Battery and ESCs\n\nBased on the voltage regulation specified for the selected motors, the recommended operating voltage range is 7V to 12V. Consequently, a 3-cell LiPo battery, with each cell providing 3.7V, has been selected as the power source to support the quadcopter’s flight, yielding a total nominal voltage of 11.1V. To provide sufficient redundancy and prevent motor overheating, 30A Electronic Speed Controllers (ESCs) have been chosen for implementation in the FPV quadcopter.\n\nII-A4 Flight controller and companion computer\n\nThe flight controller is a critical component within this system, particularly given the aim of this study to develop a methodology for bridging the gap between the physical quadcopter framework and the digital twin. The Unreal Engine has achieved high-fidelity quadcopter simulation using AirSim’s HITL capability. Therefore, a flight controller that supports both HITL configurations and actual flight operations is essential. The Pixhawk 2.4.8 has been selected as the flight controller for the FPV quadcopter due to its affordability and capability to provide smooth flight control. Given the limitations of telemetry in the transmission rate of flight data, a Raspberry Pi has been integrated as a companion computer with the Pixhawk via a serial connection. This setup facilitates further development of the FPV GUI and enhances data collection capabilities\n\nII-A5 FPV Cameras, RC Transmitter and Receiver\n\nTo enable a dual-view FPV flying experience, two FPV cameras with 600 TV lines (TVL) were selected as image sources. A crucial factor in integrating FPV cameras with an RC transmitter and receiver, especially considering the range expectation of 200 meters minimum, is the support range. Wolfwhoop 600 TVL FPV cameras and the FrSky i6X RC transmitter and receiver have been chosen for this FPV physical platform to ensure reliable control quality and simultaneous image feeds during flight within this range. Please note that to successfully receive images from the FPV camera transmitter, a compatible radio receiver is also needed to connect to a local laptop.\n\nII-B Graphical User Interface\n\nTo facilitate smooth FPV (First-Person View) flight, an informative GUI supporting dual-view FPV flying has been developed using the Kivy library [31], a robust open-source Python framework designed for cross-platform GUI development. Reflecting on the construction of the physical quadcopter, the dual cameras and the flight controller are identified as the primary sources of information. These are concurrently displayed on the GUI during flight, providing critical real-time data. The dual-view GUI framework is adeptly designed to manage functionalities such as real-time flying state acquisition via local network communication and processing of image frames from both front and bottom cameras. The interface’s primary division is a box layout in horizon distribution: On the left, a larger image frame is for the front camera view display, and on the right is a vertical stack of two box widgets containing three real-time flying states and the bottom camera view. To address the discrepancy in refresh rates between the sensors and FPV cameras, the GUI framework leverages Kivy’s non-blocking clock events updates. This allows each data source to update with their unique fresh rate in the same thread. It also employs multiprocessing to prevent the main thread of the GUI app from being blocked in the sensor data reading process. Additionally, considering the limitations in data refresh rates through telemetry communication, the GUI framework integrated socket communication to receive data sent from Raspberry Pi, which receives Pixhawk data through the serial connection. In the air, a Raspberry Pi connects to the Pixhawk flight controller through GPIO pins, runs a Python script to read the real-time flight data via Pymavlink[32], and then sends it to the local GUI as the server using the Python script. Meanwhile, the GUI on the local machine runs as the client, receiving the data sent from the server on the Raspberry Pi. This data transmission process is handled separately through multiprocessing, allowing it to be retrieved recursively through a callback function without blocking the main thread. The codes for software stacks are available in our github.\n\nII-B1 GUI Main Structure\n\nThe core structure of the GUI framework comprises two independent processes: one for data fetching from the quadcopter via local network communication with the onboard computer, Raspberry Pi, and the other for the main application process. This process constructs the overall GUI layout and integrates all functionalities into a single interface. All functionalities are programmed using object-oriented programming (OOP), ensuring a structured and efficient approach to managing the complex interactions within the GUI. The main application process is carried out through three classes, with specific tasks:\n\n•\n\n‘Datatable’ Class: This class fetches, displays, and logs data in the GUI. It starts an independent process for socket communication, pushing real-time Pixhawk data sent from the Raspberry Pi to the main thread. Once the communication process is established, the callback function within the class is recursively called via Kivy’s Clock object to update and log the data transmitted from the other process per iteration, along with the corresponding image frames. Additionally, speed and relative altitude information, retrieved and computed from the updated data, are displayed on the main GUI frame’s widget in a grid layout.\n\n•\n\n‘Camview’ Class: The ‘Camview’ class is responsible for video streaming. It sets up the procedures from image capture to stream on the widget with desired FPS, through OpenCV [33] and schedules frame updates via Kivy’s Clock object. The class captures, resizes, and processes image frames into textures for display on the widget, retrieves the latest captured frame for synchronous logging, and releases camera resources when the app shuts down. This ensures proper management and display of real-time video feeds without lagging.\n\n•\n\n‘MainGUI’ Class: As the main application class, it arranges the overall GUI layout and assigns the widgets for functionalities. It initiates functionalities by calling the ‘Camview’ class twice to set up the front and bottom streaming, along with the ‘Datatable’ class for real-time data fetching. The main class adds widgets for each of the initiated functionality instances, placing these widgets in the corresponding horizontal box layout. Besides, the main GUI class initializes the folder path directory, using the timestamp as the name of the path, and passes through the ‘Datatable’ class for further data logging manipulation. This setup demonstrates the composition nature of OOP, where the ‘MainGUI’ class manages and coordinates the functionality of other classes.\n\nII-B2 Flying State Data Fetching and Data Collection\n\nRecalling the chosen flight controller is the Pixhawk 2.4.8, which communicates using the Mavlink [29] protocol. Two methods were considered for building the data transmission bridge: telemetry communication and a serial connection. The serial connection with a companion computer was selected due to its superior data transfer rate, ensures a robust and direct link between the Pixhawk and the companion computer, resulting in more efficient data handling. In this underlying GUI software setup, the data being transferred and saved includes raw GPS data, speed data, RC control PWM data, servo PWM data, and attitude data. One noticeable detail regarding altitude shown in the table widget is that the altitude displayed is the result of the real-time altitude minus the first altitude recorded when the quadcopter is armed, in units of meters. Compared to directly negating the relative altitude read through Pymavlink [32], this method helps ignore error readings caused by the initial powering position.\n\nThe synchronous data collection is facilitated through a collaborative effort between the functionality classes introduced in section II-B1. The ‘MainGUI’ class initializes the data file and image folder directories and passes them to the ‘Datatable’ class. Once real-time data from the Pixhawk starts being read by the Raspberry Pi, it is sent through socket communication from the Pi end as the server. The function in the GUI responsible for socket communication, running as the client, is passed to the multiprocessing object. In ‘Datatable’ class, once the multiprocessing starts after being initialized, the bridge for data transmission between the Raspberry Pi and the GUI is officially built and ready to be queued by the callback function at each clock schedule interval. Subsequently, the callback function retrieves the current timestamp and logs the received data, along with image frames obtained by invoking the ‘getframe’ method of the ‘Camview’ class, into predefined directories for each callback iteration until the flight is complete.\n\nII-C AirSim Simulation Environment\n\nII-C1 Introduction to Simulation Environment\n\nMicrosoft AirSim [34] is an open-source robotics simulation platform. AirSim helps us solve the need for large data sets for training and allows debugging in a simulator. AirSim leverages current game engine rendering, physics, and perception computation to create accurate, real-world simulations. Together, this realism, based on efficiently generated ground-truth data, enables the study and execution of complex, time-consuming, and risky missions in the real world. The default quadcopter model in AirSim is modified to match our real hardware; the morphology and the cameras are modified to match the physical quadcopter. Airsim allows HITL simulation with PX4, an open-source, popular flight controller. HITL simulation offers a blend of simulation safety and real-world unpredictability. While software simulations are insightful, HITL simulation captures the nuances of actual hardware. At the same time, the HITL simulation starts, a python file will run to help retrieve the digital twin’s flying states along with image frames from two views, front and bottom, in corresponding timestamps simultaneously through Mavlink [29] and Airsim API [34]. A virtual version of the 24,000-square-foot Structure of Outdoor Autonomy Research (SOAR) facility at the University at Buffalo has been implemented through Unreal Engine 4.27. The environment is built precisely to match the real-world dimensions. In this virtual environment, pilots can free roam around using a graphic user interface (GUI) which includes two camera feeds, front view and bottom view, altitude information, and a top view global map, or there are few pre-defined tasks that the researchers can use while collecting data. Unreal Engine allows easy drag-and-drop features for creating a new task, which helps generate various tasks quickly to generate data.\n\nThe same hardware controller, Pixhawk, will be used for the digital twin’s controller in the virtual environment, while it’s not controlling a real quadcopter kit set-up, it is interfaced with a virtual quadcopter with virtual sensors simulated from Airsim. For user ends, the pilot will control the digital twin in a simulation environment through embedded GUI shown in Fig. 5; third-person view flying is also available to switch. The simulation GUI consists of more feedback panels for the predefined tasks than the physical quadcopter platform. 3 widgets are added to the right side of the screen to show the users a top-view map to know their relative location, a procedure that shows their current task, and a downview camera similar to the physical quadcopter platform. Figure 4 shows the flowchart of simulation environment.\n\nII-C2 Flying State Data Collection\n\nThe Pixhawk processes virtual sensor data encompassing various quadcopter states, including raw GPS data, orientation, cruise speed, and climb rate. The pilot can observe this data as it manifests in the digital twin’s flying position and attitude. The pilot interacts by maneuvering the RC joystick, which is integrated with the RC receiver connected to the Pixhawk. The data collection for the digital twin requires collaboration between Mavlink communication and the Airsim API. This collaboration captures flying state data, including simulated raw GPS, attitude, speed, and joystick signal input, which is then retrieved via Mavlink communication and saved in a CSV file. Simultaneously, corresponding front and bottom images captured via the AirSim API are saved as PNG files in separate folders within the same directory.\n\nIII Physical Experiment and Demonstrations\n\nThis section outlines physical experiments to validate the physical quadcopter platform at the Structure of Outdoor Autonomy Research (SOAR) at the University at Buffalo. Throughout these experiments, one single pilot utilized the physical quadcopter to complete four flying tasks, as indicated in subsection A, Figure 6–7. The flying environment involved two 3-meter obstacles aligned in line between the starting and landing spots. This distribution was then replicated in a simulation environment. The same pilot also employed the virtual GUI in this simulation environment, using the FPV view, to complete identical tasks. This process aimed to collect flying state data through Airsim HITL, thus validating the dataset collection through the digital twin platform. Results will be collected from both platforms using the same ’Position’ flight mode [35] for a fair comparison. The complexity of the flying tasks was ascent so that the pilot’s trajectory for higher complexity tasks is expected to show more flying position adjustment.\n\nIII-A Experiment tasks\n\nIII-A1 Flying Task 1: Take off, hover and land\n\nIn Task 1, the pilot was required to perform a series of maneuvers, including taking off vertically and ascending to a height of 4 meters. Upon reaching this altitude, the task entailed hovering above the helipad. This required the pilot to adjust the pitch and roll of the quadcopter based on the bottom image feed. The quadcopter was expected to maintain this hover for 10 seconds before landing back at the original departure spot.\n\nIII-A2 Flying Task 2: Flying from point A to point B\n\nSimilar to the beginning of Task 1, for Task 2, the pilot was asked to perform a series of maneuvers that included taking off vertically and ascending to an altitude above 3 meters. Once reaching an altitude of 3 meters, the pilot then maneuvered the quadcopter above the obstacles straight to the landing spot. As soon as the landing marker appeared in the bottom view, the pilot was required to halt and execute a slow descent onto the landing spot.\n\nIII-A3 Flying Task 3: Obstacle avoidance\n\nFlying Task 3 required the pilot to navigate from the departure spot to the landing spot, similar to Task 2, but with the added challenge of obstacle avoidance. This task began with the pilot taking off and ascending to an altitude of 3 meters. However, unlike the previous task, upon reaching the midpoint of their route, pilots were tasked with an additional maneuver: circling the first obstacle. After completing the circular navigation around the first obstacle, the pilot maneuvered above the second obstacle and continued towards the landing spot, where he executed a slow descent to land.\n\nIII-A4 Flying task 4: Flying Figure 8\n\nFlying task 4 is the most difficult task considering the number of maneuvering requirements. The pilot started by taking off and ascending to an altitude of 3 meters, cruised to the first obstacle, and circled the two obstacles in the shape of Figure 8. Once the circulation maneuver was finished, the pilot then maneuvered back to the departing spot and slowly descended to land.\n\nIII-B Geo-coordinate conversion\n\nThe process of path visualization was carried out through the conversion of geographic coordinates. Given the raw GPS data collected from the dataset via the physical quadcopter GUI platform, this data was initially converted to Earth-Centered Earth-Fixed (ECEF) coordinates. This conversion generates the global position of each recorded position in the flying trajectory relative to the Earth’s center of mass. Using the initial starting point from the trajectory points, post-ECEF transformation, as the reference, all ECEF coordinates are then transformed into the East-North-Up (ENU) coordinate system. This transformation is essential for visualizing the trajectory path of the flights. To better visualize the flown trajectory comparison between the physical quadcopter and the digital twin, a reference coordinate aligned with the starting point from each environment was selected. The line drawn between the starting point and the reference point was then ensured to be straight and aligned with the pure east axis in the ENU frame.\n\nIV Results and Discussions\n\nThis section outlines the visualized results of the pilot’s flying trajectories from the physical experiments using the physical quadcopter platform and the digital twin. By implementing the alignment method introduced in III-B, all GPS data points recorded along flight trajectories are mapped to align with due east, with the starting point as the initial position. For simplicity, this ENU frame is referred to as the XYZ frame. X is used to record longitudinal motion, Y is used to record lateral motion, and Z is used to record flight height variation during the flight. The visualized results, shown in Figure 8–15 then are demonstrated in both 3D view and bird-view, along with plots showing detailed position information corresponding to timestamps.\n\nTo analyze the pilot’s maneuver behavior during the flights, the maneuvering portion of the data was extracted according to each of the flying tasks for statistical analysis. For task 1, which includes takeoff, hover, and landing, the maneuvering data was chosen between the point where the altitude first reached the target altitude of 4 meters and 10 seconds afterward. For the rest of the flying tasks associated with longitudinal motion, the maneuvering data was chosen between the longitudinal position of 2 meters after the task started and 2 meters before the task ended. The detailed statistic results are shown in table III–VI. Please note that the platform used in outdoor experiments is the physical quadcopter, while the one used in the virtual environment is the digital twin.\n\nIV-A Result visualization - Task 1\n\nIV-B Result visualization - Task 2\n\nIV-C Result visualization - Task 3\n\nIV-D Result visualization - Task 4\n\nIV-E Discussions\n\nThe statistical results demonstrated in Tables III–VI indicate the maneuver behavior differences between the same pilot using the physical platform and the digital twin in all four flying tasks. Task 1 results shown in Table III indicate that the average distance between the hovering positions and origin using the digital twin is 0.1056 meters; while using the physical platform, it is 0.3691 meters during the hovering period. The hovering position difference reflects how far the pilot drifted using both platforms and flying using the digital twin demonstrated slightly better hovering control. Unlike Task 1, Tasks 2 to 4 require the pilot to manage both longitudinal and lateral control with increasing complexity. Tables IV–VI demonstrate the standard deviation of the trajectory Y and Z positions when maneuvering, reflecting the variation in lateral position and height during the flight, along with the time taken to complete the task trails. As the flying task complexity increases, the time required for the pilot using the physical quadcopter to finish the trail shows an increasing trend from 110.3442 seconds to 232.8868 seconds. Additionally, the lateral deviation increases from 0.6728 meters to 2.7589 meters. These differences indicate that the pilot made larger turns in Task 4 than in Task 2 and Task 3 while maneuvering with the physical platform. Conversely, the pilot’s Task 4 maneuver performance using the digital twin in the Airsim simulation demonstrated less time consumption with a smaller lateral deviation than his result in Task 3. This suggests that when flying in AirSim simulation, as lateral motion requirements increase, the digital twin controlled through Pixhawk is easier to manage than the real physical quadcopter.\n\nEven though both the physical quadcopter platform and the digital twin embedded in the Airsim simulation share the same controller firmware, Pixhawk PX4, and both platforms are flown by the same pilot in identical flying tasks, discrepancies are still observed in the analysis of the results. Considering that Pixhawk 2.4.8 is no longer maintained for firmware updates, sensor messages for odometry information cannot be extracted through Pymavlink to visualize the trajectory directly. Therefore, the trajectory visualization is based solely on GPS measurements taken during the flights. GPS noise in outdoor experiments could potentially explain why task 1 (takeoff, hover, and land) shows slightly poorer motion control performance using the physical quadcopter compared to the digital twin in the Airsim simulation.\n\nAdditionally, the difference in software architecture between the GUI for the physical quadcopter framework and the virtual GUI for the digital twin in Airsim simulation may have contributed to the performance discrepancy in tasks 3 and 4. The GUI for the physical quadcopter embeds the local network communication in the real-time data extraction process. Even though reading Pixhawk data through a serial connection can make the data transmission frequency much faster than telemetry, extra encoding and decoding processes, along with the local network’s transmission and receiving speed, can add delay. Through testing, considering the factor of local network connection, an extra delay between 0.015 and 0.3 second was observed at every data refresh iteration, from the moment that real-time Pixhawk data was read by the socket server until it was received and decoded by the local GUI client and ready to be queued for update. However, in the Airsim simulation, the virtual GUI interface is free from the extra process of socket communication. This means that the flying information in the digital twin’s FPV interface is in absolute real-time and free from the delay caused by data encoding/decoding or the limitations of local network transmission speed. Consequently, the pilot needs more time to make decisions in more complex flying tasks using the physical quadcopter than when using the digital twin. This can be reflected in the time length of the X curve being flat in Figure 15(b).\n\nIV-F Data Processing for Reinforcement Learning\n\nConsidering the physical quadcopter and the digital twin detailed in this paper, there exists ample scope to integrate these tools with RL Libraries for the development of autonomous UAVs. Flight data from the physical quadcopter provides valuable insights into real-world flight dynamics and environmental interactions, while the digital twin enables risk-free data collection and testing across diverse scenarios. The collected data includes telemetry, GPS, control inputs, and dual-view camera frames captured during flights using the physical quadcopter and digital twin. These datasets capture the complex maneuvers of expert pilots. Before training an autonomous UAV using this data, pre-processing is crucial. The input stream must be synchronized to ensure that each training instance accurately represents the UAV’s state and corresponding pilot actions at a specific moment in time. This involves aligning timestamps across sensor readings, control inputs, and camera images. Furthermore, normalizing state variables ensures consistent input ranges for the learning algorithms.\n\nIV-F1 Custom Gym Environment\n\nTo effectively utilize RL algorithms, an environment should be developed following the standard framework established by Gymnasium [36]. Developing this allows easy integration with any popular open-source RL libraries such as stable-baselines3 [37] or RLlib [38]. This Gym environment leverages defined state and action spaces that reflect real-world operational parameters with the reward formulation for each step. An example custom environment is developed and is also made available in our GitHub repository. The state and action space for the environment is as follows,\n\n•\n\nState Space Includes spatial coordinates (latitude, longitude, altitude), velocities, and orientations derived from GPS and IMU sensors. Visual inputs from FPV camera and bottom camera. Additionally, navigational data, such as relative distances to target locations, are also integrated.\n\n•\n\nAction Space Actions include control inputs such as throttle, pitch, yaw, and roll adjustments, which directly affect the quadcopter’s moments.\n\nNote that the above state and action space is an example to show the API level connections to gather the collected experiment data and this should be modified based on the requirements.\n\nV CONCLUSIONS\n\nIn this paper, we introduced a low-cost hardware/software architecture developed based on the F450 quadcopter frame to support human FPV flight demonstrations with comprehensive low-level data logging capabilities. The underlying software architecture, developed using the Python-based Kivy library, ensures that real-time data read from the Pixhawk flight controller and image frames from dual-view FPV cameras can be concurrently displayed with synchronized data logging by leveraging socket communication and multiprocessing through Kivy’s unique non-blocking clock event updates. In addition, we employed a virtual FPV quadcopter within a scaled testing ground, as a digital twin for flights conducted in the SOAR outdoor space at the university. This is developed using AirSim/Unreal Engine-based simulations along with a HITL setup via Pixhawk PX4 firmware, which is identical to one implemented on the hardware architecture. With the two FPV quadcopter platforms, we conducted four flying experiments with varying levels of complexity. Analyses of the flights experiment data showed that the pilot demonstrated better hovering and lateral control with frequent turning requirements and longitudinal motion when using the digital twin based on the AirSim HITL simulation. In contrast, in the physical flight experiments, better altitude maintenance was demonstrated as the flying task complexity increased.\n\nReferences\n\n[1] K. Nonami, “Research and development of drone and roadmap to evolution,” Journal of Robotics and Mechatronics, vol. 30, no. 3, pp. 322–336, 2018.\n\n[2] K. C. Goh, R. B. Ng, Y.-K. Wong, N. J. Ho, and M. C. Chua, “Aerial filming with synchronized drones using reinforcement learning,” Multimedia Tools and Applications, vol. 80, pp. 18 125–18 150, 2021.\n\n[3] C. K. Pedersen and P. Manoonpong, “Neural control and synaptic plasticity for adaptive obstacle avoidance of autonomous drones,” in From Animals to Animats 15: 15th International Conference on Simulation of Adaptive Behavior, SAB 2018, Frankfurt/Main, Germany, August 14-17, 2018, Proceedings 15. Springer, 2018, pp. 177–188.\n\n[4] A. Devos, E. Ebeid, and P. Manoonpong, “Development of autonomous drones for adaptive obstacle avoidance in real world environments,” in 2018 21st Euromicro conference on digital system design (DSD). IEEE, 2018, pp. 707–710.\n\n[5] J. N. McRae, C. J. Gay, B. M. Nielsen, and A. P. Hunt, “Using an unmanned aircraft system (drone) to conduct a complex high altitude search and rescue operation: a case study,” Wilderness & environmental medicine, vol. 30, no. 3, pp. 287–290, 2019.\n\n[6] A. Behjat, H. Manjunatha, P. K. Kumar, A. Jani, L. Collins, P. Ghassemi, J. Distefano, D. Doermann, K. Dantu, E. Esfahani et al., “Learning robot swarm tactics over complex adversarial environments,” in 2021 international symposium on multi-robot and multi-agent systems (MRS). IEEE, 2021, pp. 83–91.\n\n[7] B. Mishra, D. Garg, P. Narang, and V. Mishra, “Drone-surveillance for search and rescue in natural disaster,” Computer Communications, vol. 156, pp. 1–10, 2020.\n\n[8] X. Guo, G. He, M. Mousaei, J. Geng, G. Shi, and S. Scherer, “Aerial interaction with tactile sensing,” arXiv preprint arXiv:2310.00142, 2023.\n\n[9] S. Marathe, “Leveraging drone based imaging technology for pipeline and rou monitoring survey,” in SPE Asia Pacific Health, Safety, Security, Environment and Social Responsibility Symposium? SPE, 2019, p. D021S006R001.\n\n[10] J. Seo, L. Duque, and J. Wacker, “Drone-enabled bridge inspection methodology and application,” Automation in construction, vol. 94, pp. 112–126, 2018.\n\n[11] Vaughn College, “Demand for drone pilots soars: Uav jobs that pay over $100k,” https://www.vaughn.edu/blog/demand-for-drone-pilots-soars-uav-jobs-that-pay-over-100k/, Aug 2022, accessed: 9 December 2023.\n\n[12] D. Floreano and R. J. Wood, “Science, technology and the future of small autonomous drones,” nature, vol. 521, no. 7553, pp. 460–466, 2015.\n\n[13] U. Ozdemir, Y. O. Aktas, A. Vuruskan, Y. Dereli, A. F. Tarhan, K. Demirbag, A. Erdem, G. D. Kalaycioglu, I. Ozkol, and G. Inalhan, “Design of a commercial hybrid vtol uav system,” Journal of Intelligent & Robotic Systems, vol. 74, pp. 371–393, 2014.\n\n[14] F. Li, W.-P. Song, B.-F. Song, and J. Jiao, “Dynamic simulation and conceptual layout study on a quad-plane in vtol mode in wind disturbance environment,” International Journal of Aerospace Engineering, vol. 2022, no. 1, p. 5867825, 2022.\n\n[15] R. Czyba, M. Lemanowicz, Z. Gorol, and T. Kudala, “Construction prototyping, flight dynamics modeling, and aerodynamic analysis of hybrid vtol unmanned aircraft,” Journal of Advanced Transportation, vol. 2018, no. 1, p. 7040531, 2018.\n\n[16] Q. Shao, M. Shao, and Y. Lu, “Terminal area control rules and evtol adaptive scheduling model for multi-vertiport system in urban air mobility,” Transportation Research Part C: Emerging Technologies, vol. 132, p. 103385, 2021.\n\n[17] P. KrisshnaKumar, J. Witter, S. Paul, H. Cho, K. Dantu, and S. Chowdhury, “Fast decision support for air traffic management at urban air mobility vertiports using graph learning,” in 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2023, pp. 1580–1585.\n\n[18] S. Paul, J. Witter, and S. Chowdhury, “Graph learning-based fleet scheduling for urban air mobility under operational constraints, varying demand & uncertainties,” in Proceedings of the 39th ACM/SIGAPP Symposium on Applied Computing, 2024, pp. 638–645.\n\n[19] B. Knowles and J. T. Richards, “The sanction of authority: Promoting public trust in ai,” in Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 2021, pp. 262–271.\n\n[20] N. Aoki, “The importance of the assurance that “humans are still in the decision loop” for public trust in artificial intelligence: Evidence from an online experiment,” Computers in Human Behavior, vol. 114, p. 106572, 2021.\n\n[21] Y. Li, “Deep reinforcement learning: An overview,” arXiv preprint arXiv:1701.07274, 2017.\n\n[22] A. Hussein, M. M. Gaber, E. Elyan, and C. Jayne, “Imitation learning: A survey of learning methods,” ACM Computing Surveys (CSUR), vol. 50, no. 2, pp. 1–35, 2017.\n\n[23] C. Wang, J. Wang, J. Wang, and X. Zhang, “Deep-reinforcement-learning-based autonomous uav navigation with sparse rewards,” IEEE Internet of Things Journal, vol. 7, no. 7, pp. 6180–6190, 2020.\n\n[24] E. Çetin, C. Barrado, G. Muñoz, M. Macias, and E. Pastor, “Drone navigation and avoidance of obstacles through deep reinforcement learning,” in 2019 IEEE/AIAA 38th Digital Avionics Systems Conference (DASC), 2019, pp. 1–7.\n\n[25] A. Anwar and A. Raychowdhury, “Autonomous navigation via deep reinforcement learning for resource constraint edge nodes using transfer learning,” IEEE Access, vol. 8, pp. 26 549–26 560, 2020.\n\n[26] M. Burri, J. Nikolic, P. Gohl, T. Schneider, J. Rehder, S. Omari, M. W. Achtelik, and R. Siegwart, “The euroc micro aerial vehicle datasets,” The International Journal of Robotics Research, 2016. [Online]. Available: http://ijr.sagepub.com/content/early/2016/01/21/0278364915620033.abstract\n\n[27] A. L. Majdik, C. Till, and D. Scaramuzza, “The zurich urban micro aerial vehicle dataset,” The International Journal of Robotics Research, vol. 36, no. 3, pp. 269–273, 2017.\n\n[28] Leica Geosystems, “Leica nova ms60 multistation: The scanning robotic total station,” https://leica-geosystems.com/products/total-stations/multistation/leica-nova-ms60, 2023, accessed: 9 December 2023.\n\n[29] “MAVLink Developer Guide,” https://mavlink.io/en/, (n.d.), accessed: 9 December, 2023.\n\n[30] P. K. Kumar, “Gui enhancements and ai interfaces for swarm simulation and flight learning of unmanned aerial vehicles,” Ph.D. dissertation, State University of New York at Buffalo, 2021.\n\n[31] “Kivy: The open source python app development framework,” https://kivy.org/, 2010, accessed: 9 December, 2023.\n\n[32] “Using pymavlink libraries (mavgen),” https://mavlink.io/en/mavgen_python/, (n.d.), accessed: 9 December, 2023.\n\n[33] G. Bradski, “The OpenCV Library,” Dr. Dobb’s Journal of Software Tools, 2000.\n\n[34] S. Shah, D. Dey, C. Lovett, and A. Kapoor, “Airsim: High-fidelity visual and physical simulation for autonomous vehicles,” in Field and Service Robotics: Results of the 11th International Conference. Springer, 2018, pp. 621–635.\n\n[35] A. PX4, “Position mode (multicopter),” https://docs.px4.io/main/en/flight_modes_mc/position.html#:~:text=Position%20Mode%20(Multicopter)%20%E2%80%8B,controls%20speed%20of%20ascent%2Ddescent., (n.d.), accessed: 9 December, 2023.\n\n[36] M. Towers, J. K. Terry, A. Kwiatkowski, J. U. Balis, G. d. Cola, T. Deleu, M. Goulão, A. Kallinteris, A. KG, M. Krimmel, R. Perez-Vicente, A. Pierré, S. Schulhoff, J. J. Tai, A. T. J. Shen, and O. G. Younis, “Gymnasium,” Mar. 2023. [Online]. Available: https://zenodo.org/record/8127025\n\n[37] A. Raffin, A. Hill, A. Gleave, A. Kanervisto, M. Ernestus, and N. Dormann, “Stable-baselines3: Reliable reinforcement learning implementations,” Journal of Machine Learning Research, vol. 22, no. 268, pp. 1–8, 2021. [Online]. Available: http://jmlr.org/papers/v22/20-1364.html\n\n[38] E. Liang, R. Liaw, R. Nishihara, P. Moritz, R. Fox, K. Goldberg, J. Gonzalez, M. Jordan, and I. Stoica, “Rllib: Abstractions for distributed reinforcement learning,” in International conference on machine learning. PMLR, 2018, pp. 3053–3062."
    }
}
{
    "id": "dbpedia_2236_0",
    "rank": 5,
    "data": {
        "url": "https://academic.oup.com/bioinformatics/article/34/21/3702/5001390",
        "read_more_link": "",
        "language": "en",
        "title": "PyCoTools: a Python toolbox for COPASI",
        "top_image": "https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/bioinformatics/34/21/10.1093_bioinformatics_bty409/2/m_bioinformatics_34_21_3702_f2.jpeg?Expires=1784416935&Signature=JTojUDGPTYCFmC~-b09JYEELuTcsqZxs7nIn3xr3IaYa~6Gq4-kmcTQPk0lvuof~x2Z8wfbkBLjA0UpbWu3my5V9MNres~2Hde9dRnl0qEAIJvjOAjs53xh0fmMy7VpVgRL-ldr1P4vrT9H9vQBlLSNQ1Sgx-uzax61uUFKubi47x4olHRbvM2uUL3cZj7ML1OtAuMm49evibrvDkE5wJTTf4GteZWYepMdeAvznFRbabn8~cUeqrOKGwprO31ZPpW0tj1at9O7dAcAVW8l85E~muaGk8qGlYd20SViELIEF5eowsZKymfLugmAwdvsV5NHNOw7EuXDSEqBBk4LR1g__&Key-Pair-Id=APKAIE5G5CRDK6RD3PGA",
        "meta_img": "https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/bioinformatics/34/21/10.1093_bioinformatics_bty409/2/m_bioinformatics_34_21_3702_f2.jpeg?Expires=1784416935&Signature=JTojUDGPTYCFmC~-b09JYEELuTcsqZxs7nIn3xr3IaYa~6Gq4-kmcTQPk0lvuof~x2Z8wfbkBLjA0UpbWu3my5V9MNres~2Hde9dRnl0qEAIJvjOAjs53xh0fmMy7VpVgRL-ldr1P4vrT9H9vQBlLSNQ1Sgx-uzax61uUFKubi47x4olHRbvM2uUL3cZj7ML1OtAuMm49evibrvDkE5wJTTf4GteZWYepMdeAvznFRbabn8~cUeqrOKGwprO31ZPpW0tj1at9O7dAcAVW8l85E~muaGk8qGlYd20SViELIEF5eowsZKymfLugmAwdvsV5NHNOw7EuXDSEqBBk4LR1g__&Key-Pair-Id=APKAIE5G5CRDK6RD3PGA",
        "images": [
            "https://oup.silverchair-cdn.com/UI/app/svg/umbrella/oxford-academic-logo.svg",
            "https://oup.silverchair-cdn.com/UI/app/svg/i.svg",
            "https://oup.silverchair-cdn.com/UI/app/svg/account.svg",
            "https://oup.silverchair-cdn.com/UI/app/svg/i.svg",
            "https://oup.silverchair-cdn.com/UI/app/svg/account.svg",
            "https://oup.silverchair-cdn.com/data/SiteBuilderAssets/Live/Images/bioinformatics/bioinformatics_title-205396838.svg",
            "https://oup.silverchair-cdn.com/data/SiteBuilderAssets/Live/Images/bioinformatics/bioinformatics_h11820233911.svg",
            "https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/bioinformatics/Issue/34/21/2/m_cover.jpeg?Expires=1726450553&Signature=Nt65XfhgnRSDWKjUKXtXicyLQxKjaNoz7ethYB0toWhe1uMz2Mq4wDPGdkPlFrGX-MQOpre~SPnCRLnwduxUCiqny50bl7T1RWR~ZDgVppBBmQ3DuKv5tZUkYDkL7B-hl92k3XVRPlhTNUt4bfULnb4pOO-S6FRMiOQs0ZCXu4TMeUpjqUmxPkd862fV-eZvvPTPWwhiQhTFdojkS3ftmLSv7A-odS70HxlnlZ9VBf4bKpdv4kcb~-JN4zgp683W-wqDdI0Fom65GRSC4gokD9nRINNOZVc6aEKDoM~J2zeNQw5kThAQR68eJK32LNjUdsgDHE3druUA6RWwTG~QBw__&Key-Pair-Id=APKAIE5G5CRDK6RD3PGA",
            "https://oup.silverchair-cdn.com/UI/app/svg/pdf.svg",
            "https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/bioinformatics/34/21/10.1093_bioinformatics_bty409/2/m_bioinformatics_34_21_3702_f2.jpeg?Expires=1724368934&Signature=izNRcMh1SbpsZc6EXiMHsDQWWJVyV0vG2vFHY3YTB01k8RlsjNobdcLVSRuR6C8buIiSxNn4Qb4i3s6qtQOIRxRhfC50ZiJ3MKvfJF-pEpAaMtEirLdXkJDz4e1MqaIQztTnt~5g81rLwUCXfSnMsHszOLtTKpfVjHw5USkrGKBdWdqZ6nSgM~Uibmey9yLGlp~gcD2102LT53lAxw2pyMdEO5YXRZ-C~ctSS~W8zqqdL~eHQBFyEUSMj9w-l1b3jU49u2b~C5c-aPfh4Gvr6p7ysTrNzsr3DGdjzj1cYiRBNvjEEyd4n2eSeBqqb1z2-LoogI7swHa-4ly18aznaA__&Key-Pair-Id=APKAIE5G5CRDK6RD3PGA",
            "https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/bioinformatics/34/21/10.1093_bioinformatics_bty409/2/m_bioinformatics_34_21_3702_f3.jpeg?Expires=1724368934&Signature=KgWFBhMPztIPyeCqBbJOKKCFtp5Bvxo1bqaF0COnTMZxCQovgMhgmBy6lXv7UEmC5YNmZw808RkscEFN7vvCl4SqaSr7glYDzic3jv5FdR81XTFop-HN-DBEEVAXp6dBaqb-NlV3yAEURqMZX-k6FVI6CYcNuMuLa2bf8zTUlvxfM-bYTx7Kobyem9eN-SyYiKU3Ykv2I7ilvVPAxyoanY4XA~gLS9jY~QltDoQ-fMh-HBqDQKejU42SkS2IPMZNZbkDRRQuaiu7k5mn~2BKXDAW0QqWahgSLxULYAXAB3h244wpAQGPwF3UGmFQV4bWLGia9eH1do35kl1~8FQKoQ__&Key-Pair-Id=APKAIE5G5CRDK6RD3PGA",
            "https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/bioinformatics/34/21/10.1093_bioinformatics_bty409/2/m_bioinformatics_34_21_3702_f4.jpeg?Expires=1724368934&Signature=eB0tS9nlWNAfbRE3LeSErkoo9XgMt~uSQCZiiQjDsTGbhgfbDBo6WDjhGJIaRwo7CZy2avUvPDE3~nTA9vkJe54t38UDnHMVk7fJnrk60DkMmzwZ6XvQWVdqwPc8qbMJAmEsymwArWseCnNJKMbCYb5OKv-8Y9twwj4JfBOFKt4CLe3vtdWPuRT3rH7S-647ZFFT6PSKgXQ23FKSd54B0KiLmQ9WdIFtoptcN9uKAmFM~N0X0R1nGOBMca7xiNXJIHEoqBmTib1Yau0IC291jWHyGjIEx3-k18-HZG4zD3F5UeOq1rgZuUVRsnEAZUU8eG031a2qSNXTiGEcmp2NEg__&Key-Pair-Id=APKAIE5G5CRDK6RD3PGA",
            "https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/bioinformatics/34/21/10.1093_bioinformatics_bty409/2/m_bioinformatics_34_21_3702_f5.jpeg?Expires=1724368934&Signature=ZkGq5KKai6VzNsllwxhM~SzImubWfFCItuVh3JZYto8g~GyhUQxxe8n6H0lXGrSFeGCNBrkYPKr8D034lDgsmzti9ZW5RZ7mKvZul9dchB~w0ST3v2P9ApA9fOZi-Melhv9T5OpqpwUDZEq8aIUvS7phUSGsiJMrbQnUn8kFkQbk62MOOfX2HGlMaOQd447NKaREBnjsY4-Xmgcgqxgm8vff3I66Fd4nzhFS3TsxNqPX~ac9bB9Y-Nr~36nG5vEFc25TmP1hFC8KKWndbb~ulq28YclXKisfrH1lgZjsfzXAfBnFtqYQK9Pmt50FZSKflkVDiiKfTqJZ2FXiPizGEg__&Key-Pair-Id=APKAIE5G5CRDK6RD3PGA",
            "https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/bioinformatics/34/21/10.1093_bioinformatics_bty409/2/m_bioinformatics_34_21_3702_f6.jpeg?Expires=1724368934&Signature=dbsPGRv45VVu8Iq9dVaPa3yaJG5-Wi2OZN0Ar6gmnhoDqV~Cn~tnwWkT~UW1SYQK~vnCq-mQjV80h4DB4UEfSwJEoHr9xqBLRRQQIByiHj4ysLgAimNIMYglqvgDtc5zMcBfFmGObzEZYeiK5Zgc-PuL3Wbk3-r0DNVufp~Ywo~yc~m~LSovOImy3vwdcOJJOaL2Vv7GSS5pLgyKBy~01huZRHpLpzk2Zmv8haAQ3WY5fWw5NgXwifHSo97WMa8uAnXo2ywk~jk9QtnQAL8WIQzbQAXe84jcTlRzTwRZv9RgUqBDUVWzRyIfd61wiSrVnY7Eggp9NV-eVTZaekPuxw__&Key-Pair-Id=APKAIE5G5CRDK6RD3PGA",
            "https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/bioinformatics/34/21/10.1093_bioinformatics_bty409/2/m_bioinformatics_34_21_3702_f7.jpeg?Expires=1724368934&Signature=nSjlWpGbxHBfesmFKOi6Icg4OUCmFoU8MfHNI7lZqpzFAU9GbITbMM9xTMCxndg~~9DQDnIWMC9njezP6tNilsSOfijVmmEd47rb~zk5bb8Zr1O4pH2tfdcEWE9fQ-E6UaXbvPGhzIH42gc9SbBKhe1hViiWxHACOc--iEhbe3TpVZoOeoXFirTLdCW5MAZ6dNtpMrm2yAc98Zw3lPtZEIz9cAhlbsFm946b4aWHVaUVNlToSSvbMm2eMhBxVxfFC089hgUYE5dZ1yasXxsZC6fCk3BgbOA~R~Yxxd~E19c~YvEYbGnjnSMqtlycdE-a4CEfaLbYWhIidlXodylI5w__&Key-Pair-Id=APKAIE5G5CRDK6RD3PGA",
            "https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/bioinformatics/34/21/10.1093_bioinformatics_bty409/2/m_bioinformatics_34_21_3702_f8.jpeg?Expires=1724368934&Signature=kpQC5GbSHHpYl-jDwbhSRhWGY7OW6WWT7eWVeEn29R3EqxU41ECZ01Z5yxds-47R2xZQwa~FtoSAR0vmSZVbnynDx4QQkhqB5Geh24Mvp0Nw2F9D5AJIFKsO1X-0IJsfJLGubK6l2zo6JPnF55cEU3FtU9I3~v6vm4Tpj1MDlzjTiyvAqQCqSFS5uMJq1tleChNKJE~tivIVrzrAtWQVyqWp4rPpM~uaRWGE9A~P4wgZIGjRTwnQNUg-SZcVQbFzlVmSr93BjdWGfZmFqF3EG28LxzbEyhpypiZOnOE7ROGz34D2w34mI-UveLoSw3BiTRsasIOBgJfk85gaqjoe3w__&Key-Pair-Id=APKAIE5G5CRDK6RD3PGA",
            "https://oup.silverchair-cdn.com/UI/app/svg/i.svg",
            "https://oup.silverchair-cdn.com/data/Multimedia/oup-logo.svg",
            "https://oup.silverchair-cdn.com/data/Multimedia/journals-careers-network.svg",
            "https://oup.silverchair-cdn.com/data/SiteBuilderAssets/Live/Images/bioinformatics/bioinformatics_f1-205396838.svg",
            "https://oup.silverchair-cdn.com/UI/app/svg/umbrella/oup-logo.svg",
            "https://ouptag.scholarlyiq.com/ntpagetag.gif?js=0"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Welsh, Ciaran M",
            "Fullard, Nicola",
            "Proctor, Carole J",
            "Martinez-Guimera, Alvaro",
            "Isfort, Robert J",
            "Bascom, Charles C",
            "Tasseff, Ryan",
            "Przyborski, Stefan A",
            "Shanley, Daryl P",
            "Ciaran M"
        ],
        "publish_date": "2018-05-22T00:00:00",
        "summary": "",
        "meta_description": "AbstractMotivation. COPASI is an open source software package for constructing, simulating and analyzing dynamic models of biochemical networks. COPASI is",
        "meta_lang": "en",
        "meta_favicon": "//oup.silverchair-cdn.com/UI/app/img/v-638552060786258310/apple-touch-icon.png",
        "meta_site_name": "OUP Academic",
        "canonical_link": "https://academic.oup.com/bioinformatics/article/34/21/3702/5001390",
        "text": "Abstract\n\nMotivation\n\nCOPASI is an open source software package for constructing, simulating and analyzing dynamic models of biochemical networks. COPASI is primarily intended to be used with a graphical user interface but often it is desirable to be able to access COPASI features programmatically, with a high level interface.\n\nResults\n\nPyCoTools is a Python package aimed at providing a high level interface to COPASI tasks with an emphasis on model calibration. PyCoTools enables the construction of COPASI models and the execution of a subset of COPASI tasks including time courses, parameter scans and parameter estimations. Additional ‘composite’ tasks which use COPASI tasks as building blocks are available for increasing parameter estimation throughput, performing identifiability analysis and performing model selection. PyCoTools supports exploratory data analysis on parameter estimation data to assist with troubleshooting model calibrations. We demonstrate PyCoTools by posing a model selection problem designed to show case PyCoTools within a realistic scenario. The aim of the model selection problem is to test the feasibility of three alternative hypotheses in explaining experimental data derived from neonatal dermal fibroblasts in response to TGF-β over time. PyCoTools is used to critically analyze the parameter estimations and propose strategies for model improvement.\n\nAvailability and implementation\n\nPyCoTools can be downloaded from the Python Package Index (PyPI) using the command ’pip install pycotools’ or directly from GitHub (https://github.com/CiaranWelsh/pycotools). Documentation at http://pycotools.readthedocs.io.\n\nSupplementary information\n\nSupplementary data are available at Bioinformatics online.\n\n1 Introduction\n\nIn biology, systems modelling is used to reproduce the dynamics of a biochemical network of molecular interactions with a mathematical model. It has proved particularly useful in the study of cell signalling systems such as NF-κB (Adamson et al., 2016; Ashall et al., 2009; Nelson et al., 2004), mTOR (Dalle Pezze et al., 2012, 2016), p53 (Purvis et al., 2012; Sun et al., 2011) and TGF-β (Schmierer et al., 2008; Vilar et al., 2006; Wang et al., 2014; Zi and Klipp, 2007; Zi et al., 2014). In these studies, the essential biological relationships are represented by a series of ordinary differential equations (ODE) to generate a model. Hypotheses can then be tested by performing in-silico experiments. Before ODE models can be used to make meaningful predictions they must first be calibrated to experimental data.\n\nModel calibration is a notoriously difficult problem typically due to the size and complexity of the systems involved and a lack of appropriate experimental data. ODE models are prevalent in systems biology because they are well-suited for predicting system dynamics and because many computational tools have been developed explicitly for the construction, simulation and analysis of biological networks. Among these tools are Data2Dynamics (Raue et al., 2015), Systems Biology Workbench (Sauro et al., 2003), AMIGO (Balsa-Canto and Banga, 2011), SBpipe (Dalle Pezze and Le Novère, 2017), libRoadRunner (Sauro et al., 2013; Somogyi et al., 2015), Antimony (Smith et al., 2009), Tellurium (Choi et al., 2016), Ecell (Takahashi et al., 2003), PyDsTool (http://www2.gsu.edu/∼matrhc/PyDSTool.htm), PySCeS (Olivier, 2005), ABC-SysBio (Liepe et al., 2010), Condor Copasi (Kent et al., 2012) and COPASI (Hoops et al., 2006).\n\nCOPASI is a widely used tool in modelling biological systems because it supports a variety of modelling applications including deterministic, stochastic and hybrid model solvers, parameter estimation, optimization, parameter scans, steady state analysis, local sensitivity analysis and metabolic control analysis. COPASI has a graphical user interface (GUI) which makes the tool accessible to non-expert programmers and mathematicians, but also has a command line interface for batch processing and an application programming interface (API) for several programming languages. These APIs have been used for integrating the COPASI framework with custom software, for example in JigCell Run Manager (Palmisano et al., 2015), CellDesigner (Matsuoka et al., 2014), ManyCell (Dada and Mendes, 2012) and ModelMage (Flöttmann et al., 2008).\n\nThe Python programming language is useful for scientific computing because of its concise syntax and the availability of open source toolboxes such as pandas (https://pandas.pydata.org/), numpy (http://www.numpy.org/), scipy (http://www.scipy.org/), sklearn (Pedregosa et al., 2011) and matplotlib (Hunter, 2007), which together provide a series of well-documented, easy-to-use, high-level tools for interacting with and manipulating numerical data. Development of further tools in Python is enabled by the Python Package Index (PyPI) where code can be made freely available to other developers. As a result, Python has an extensive publicly available code base for scientific computing that competes well with other commercial and non-commercial environments such as Matlab and R.\n\nHere we present PyCoTools, an open-source Python package which provides a high level interface to COPASI tasks with an emphasis on model calibration. COPASI tasks are integrated with the Python environment to provide additional features which are non-native to COPASI. Features include: the construction of COPASI models with Antimony (Smith et al., 2009); the automation of repeat parameter estimation configurations, chaser parameter estimations and parameter estimations for multiple models (e.g. model selection); automation of the profile likelihood method of identifiability analysis (Raue et al., 2013; Schaber, 2012) with visualization facilities which are flexible enough to support model reduction (Maiwald et al., 2016); visualization of time courses from ensembles of parameter sets and multiple ways of visualizing parameter estimation data. We demonstrate PyCoTools by defining a model selection problem to introduce a known negative feedback into a previously published model of TGF-β signalling (Zi and Klipp, 2007) using new data.\n\n2 Materials and methods\n\n2.1 Experimental\n\n2.1.1 Cell lines and treatment\n\nNeonatal human dermal fibroblasts (HDFn, Life Technologies, C-004-5C) were cultured as per manufacturer guidelines in M106 (Life Technologies M-106-500) supplemented with LSGS (Life Technologies S-003-10). HDFn were seeded at a density of 10 000 cells/cm2 into 12 well plates (Greiner 665180) in 4 ml complete M106 and cultured for 3 days. Media was aspirated, cells washed twice with DPBS and replaced with 4 ml M106 without LSGS and cells were serum starved for 24 h. HDFn were treated with 5 ng ml–1 TGF-β1 (Life Technologies, PHG9211) in M106 media without LSGS for 0, 1, 2, 4, 8, 12 h. To harvest, media was aspirated, cells were washed twice in DPBS and then lysed in 350 µl RLT buffer (Qiagen 79216).\n\n2.1.2 High-throughput qPCR\n\nLysates were snap frozen in liquid nitrogen and stored -80°C prior to quantification. Cell lystes were thawed at 4°C and then RNA was isolated using the Biomek FxP and the RNAdvance Tissue Isolation kit (Beckman Coulter, p/n A32646). The resulting RNA was quantified using the Nandrop 8000 (Nanodrop, ND-8000). cDNA was generated using 500 ng of TotalRNA and Applied Biosystems High Capacity cDNA with Reverse Transcription kit (Applied Biosystems p/n 4368814). cDNA, assays and dilutions of Applied Biosystems Taqman Fast Advanced MasterMix (Applied Biosystems, p/n 4444965) were plated onto a Wafergen MyDesign SmartChip (TakaraBio, p/n 640036) using the Wafergen Nanodispenser. The chip was then loaded into the SmartChip cycler and qPCR performed using the following conditions: hold Stage 50°C for 2 min, 95°C for 10 min, PCR Stage 95°C for 15 s and 60°C for 1 min. After 40 cycles the reaction was stopped and the data was exported for analysis.\n\nPrior to use for fitting, cycle threshold CT values were normalized using the 2−ΔΔCT method of quantitative PCR normalization to the geometric mean of four reference genes (B2M, PPIA, GAPDH, ACTB) per sample (Livak and Schmittgen, 2001).\n\n2.2 Computational\n\n2.2.1 PyCoTools availability and installation\n\nPyCoTools was developed partially on Windows 7 and partially on Ubuntu 16.04.2 with the Anaconda distribution of Python 2.7 and COPASI version’s 4.19.158 and 4.21.166. PyCoTools can be installed with ‘pip’, Python’s native package manager using the command ‘pip install pycotools’. PyCoTools can also be downloaded directly from source at https://github.com/CiaranWelsh/pycotools. More detailed instructions on installation and PyCoTools usage can be found in the PyCoTools documentation (http://pycotools.readthedocs.io).\n\n2.2.2 Definition of the model selection problem\n\n2.2.3 An idealized model selection problem\n\nIn addition to the main model selection demonstration, another idealized model selection demonstration has been provided in the supplementary content. The purpose of this alternative demonstration is to provide an example with short execution times that parallels the main model selection problem and provides code that users can run themselves. Specifically, in this alternative model selection problem we create three models (a negative feedback motif, a positive feedback motif and a feed-forward motif) using the Antimony interface. Analogous to the main problem defined above, we then perform model selection using synthetic experimental data from the negative feedback topology, visualize the results and run an identifiability analysis.\n\n3 Results\n\n3.1 Overview of PyCoTools facilities and architecture\n\nPyCoTools provides COPASI users with a means of efficiently configuring and running COPASI tasks from a Python environment. The PyCoTools package is comprised of three main modules: ‘model’, ‘tasks’ and ‘viz’.\n\nThe ‘Model’ object under the ‘model’ module plays a central role in PyCoTools by using Python’s ‘lxml’ library to extract model information from the COPASI XML and store it in Python classes. Manipulating XML was chosen because of its widespread use in systems biology and because well documented tools exist for its manipulation. The information extracted is subsequently available as ‘Model’ attributes. The ‘Model’ enables users to add, remove and change model components and acts as a central entity that can be modified and configured by other PyCoTools classes. As an alternative means of building models, the ‘model’ module provides an interface to and from the SBML model definition language, Antimony (Smith et al., 2009). PyCoTools wraps functions from Tellurium (Choi et al., 2016) and command line COPASI to convert between Antimony, SBML and COPASI models, thereby facilitating the transition between environments.\n\nThe ‘tasks’ module uses the ‘Model’ class extensively to configure COPASI tasks. Supported tasks include deterministic, stochastic or hybrid time courses, arbitrary dimensional parameter scans or repeat tasks, and parameter estimations. Additionally, tasks are provided which are not available in COPASI within a single function. Specifically, PyCoTools automates the configuration of ‘repeat parameter estimations’ and increases the rate by which parameter estimations can be run. This is achieved by automatically configuring COPASI’s repeat parameter estimation feature and running model replicates simultaneously. A queueing system is introduced to prevent overuse of limited computational resources. PyCoTools supports the configuration and running of ‘chaser estimations’ where parameter estimates from a global algorithm are inserted into the model and driven to a minimum with a local algorithm. Other tasks supported by PyCoTools include model selection and the calculation of profile likelihoods for assessing a identifiability status of a model (Raue et al., 2009; Schaber, 2012).\n\nThe ‘viz’ module [the concept of which takes inspiration from the Ecell software by Takahashi et al. (2003)] contains all PyCoTools visualization facilities. The aim of the ‘viz’ module is to produce publication quality figures of time courses, parameter estimations, profile likelihoods and model selection. The ‘viz’ module also provides a host of exploratory data analysis tools for analyzing repeat parameter estimation data. These tools and their usage are described next.\n\n3.1.1 Tools for analysis of repeat parameter estimation data\n\nRepeat parameter estimation data can be visualized in multiple ways and this information can be used to diagnose problems and direct modelling efforts. The tools provided in PyCoTools collectively allow one to gauge uncertainty in model predictions or parameter estimates, assess the performance of algorithms used for optimization, visualize distributions of parameters and visualize putative relationships between parameters.\n\nUsually the first item of interest after a parameter estimation is to visualize simulated predictions against empirical data. PyCoTools extends the basic ‘simulated versus experimental time course plot’ to calculate and display confidence intervals for each profile. This is achieved by inserting parameter sets into the model in turn, simulating a time course and aggregating the results by bootstrapping an estimator (e.g. the mean) of the users choice. By visualizing predictions from several parameter sets, uncertainty is propagated from parameter estimates to model predictions. The ‘ensemble time course’ thus emphasizes model strengths and weaknesses, highlighting regions of confidence and those which require attention.\n\nWhile ensemble time courses are used to inform our confidence on model predictions, profile likelihoods are used to inform our confidence on parameter values. Briefly, a profile likelihood is a parameter scan of parameter estimations, starting from a best parameter set. Each parameter is fixed in turn and its value is systematically varied over the course of the scan. The remaining parameters are re-optimized at each point of the scan and the objective function value traces a path through parameter space. The shape of this profile is then compared to a confidence threshold based on the likelihood ratio statistic (Raue et al., 2009).\n\nA profile likelihood typically has one of three interpretations. If the profile does not exceed the threshold in one or both directions and is not flat, the parameter is practically non-identifiable. In this case, the trajectory of the other model components over the profile may be used to direct model reduction strategies (Maiwald et al., 2016). If a profile is completely flat the parameter is structurally non-identifiable, which means the parameter is algebraically related to another. To resolve structural non-identifiabilities, one can fix one of the parameters in a relationship to an arbitrary value. Of note, one must be cautious about using profile likelihoods to render a parameter structurally non-identifiable because the profile likelihood method only samples the parameter space. It is possible that the profile appears flat but only on the scale of the sampled profile. Therefore, structurally non-identifiable parameters should be further investigated to determine any relationships which might exist. Finally, if the profile exceeds this threshold in both directions the parameter is identifiable and the parameter values at which the profile exceeds the threshold are the upper and lower confidence boundaries for the parameter (Raue et al., 2009). Ideally, for precise model predictions, every estimated parameter in a defined parameter estimation problem should be identifiable. In reality, limited data and overly complex model structures often lead to identifiability issues.\n\nMaiwald et al. (2016) extended the usefulness of profile likelihood from assessing identifiability to model reduction. A practical non-identifiability exists because the optimization does not have enough data to inform model parameters, or put another way, the model is too complex for the data. Viewing the paths traced by other parameters in a profile likelihood analysis (e.g. putting the trajectory of another parameter on the y-axis rather than the objective function value) provides information about the relationship between the parameter of interest on the x-axis and the parameter on the y-axis. Identifying this relationship enables steps to be taken to resolve the problem by fixing parameters or replacing non-identifiable species or parameters with algebraic equations. Profile likelihoods are therefore useful in a data-driven approach to iteratively refine an optimization problem, fixing parameters where possible and modifying the topology as necessary until the model fits the experimental data.\n\nProfile likelihood calculations are a computationally intense task and to be useful, it is required that the starting parameter set is optimal, or at least very close to optimal, with respect to the data. It is therefore prudent to assess this condition before conducting a profile likelihood analysis. The performance of an optimization problem can be evaluated by plotting the sorted objective function value [i.e. residual sum of squares (RSS) or likelihood] for each parameter estimation iteration against its rank of best fit (herein referred to as a ‘likelihood-ranks’ plot). In these plots the best case scenario is either a flat line for when there is only a single global minimum or more commonly, a monotonically increasing step-like function where each step marks a different minimum (Raue et al., 2013). Horizontal lines in the likelihood-ranks plot indicate that many iterations of the same optimization problem have located the same minimum, which increases our confidence that the problem is well-posed. In contrast a smooth curve indicates that estimations have not converged to a minimum.\n\nIf the likelihood-ranks plot shows a smooth curve, it is a good idea to either rerun the parameter estimation using a different algorithm or different algorithm settings. Alternatively, while others (Raue et al., 2013) employ a multi-start Latin-hypercube strategy with a local optimizer to ensure strategic and uniform sampling of the parameter space, given the choice of algorithms in COPASI it is easy to first run a global and then switch to a local algorithm. This strategy, here referred to as a ‘chaser estimation’, can be performed on all or a subset parameter sets to drive them closer to their respective minima.\n\nIn addition to profile likelihoods and time course ensembles, viewing distributions of parameter estimation data and correlations between parameters can provide information about an optimization problem. Box plots provide immediate information about the range of parameter estimates and how they compare to other parameters. Often a box plot can provide clues to a parameter’s identifiability status. Histograms on the other hand provide a more detailed view of parameter distributions and can identify behaviour (e.g. bimodal parameters) that would not be identified with box plots. Moreover, a combination of Pearson’s correlation heat maps and scatter graphs can be used to locate linear or log-linear relationships between parameters.\n\nAn important aspect of visualizing parameter estimation data is that not all parameter sets fit the model equally well. Parameter sets with higher objective function values can distort the distribution of better performing parameter sets or the shape of a relationship. For this reason PyCoTools implements flexible means of subsetting parameter estimation data before plotting.\n\n3.2 A demonstration: extending the Zi and Klipp (2007) model\n\nTo demonstrate PyCoTools, we define a model selection problem to extend a published model of canonical TGF-β signalling (Zi and Klipp, 2007) (Fig. 1). As an alternative demonstration, we also provide an another model selection problem in the supplementary content, as described in the methods.\n\nTGF-β binds to the autophosphorylated homodimeric type 2 TGF-β receptors which phosphorylate and heterodimerize with homodimers of type 1 TGF-β receptors (De Crescenzo et al., 2001). This event leads to internalization of the ligand–receptor complex into one of two types of membrane bound intracellular compartment: early endosomes or caveolae. Evidence in Di Guglielmo et al. (2003) suggests that ligand–receptor complexes in the early endosome, rather than the caveolae, are responsible for conveying the TGF-β signal, via phosphorylation, to the Smad second messenger system. Phosphorylated Smad2/3 binds to Smad4, translocates to the nucleus and induces transcription of TGF-β responsive genes (Schmierer et al., 2008). Smad7 is a well characterized negative regulator of the Smad system and is transiently produced in response to TGF-β (Hayashi et al., 1997; Nakao et al., 1997). Multiple mechanisms of negative regulation by Smad7 have been reported, including the recruitment of E3 ubiquitin ligases to either Smad2/3 in competition with Smad4 (Yan et al., 2016) or to activated TGF-β receptors in caveolae (Di Guglielmo et al., 2003; Kavsak et al., 2000). Many biological entities have been proposed as regulators of this process, including PPM1A (Lin et al., 2006), NEDD4L (Gao et al., 2009), SNoN (Stroschein et al., 1999) and Ski. Ski acts as co-repressor at Smad regulated genes by recruiting histone deacetylases which leads to epigenetic constriction of Smad-responsive genes (Akiyoshi et al., 1999).\n\nThe Zi and Klipp (2007) model (Fig. 1a) combines work by Vilar et al. (2006) describing TGF-β receptor internalization and recycling dynamics with a Smad nuclear-cytoplasmic translocation module. In this model, an explicit representation of the Smad7 negative feedback was not included, but was instead incorporated into the rate law for the reaction describing the degradation of the activated ligand–receptor complexes from within caveolar compartments (‘LRC_Cave’ in Fig. 1a). The purpose of the model selection problem presented here is to investigate the feasibility of three alternative mechanisms of negative regulation (Fig. 1) in explaining the experimental data (Fig. 2).\n\nAfter calibration, the ‘viz.ModelSelection’ class was used to calculate and visualize the Akaike information criteria (AIC) corrected for small sample sizes (AICc) (Fig. 3a) and the Bayesian information criteria (BIC) (Supplementary Fig. S1). With these statistics, a lower value indicates a better agreement with the data and thus a better model. In the current problem, a closer inspection of the best model selection values (Fig. 3b) indicates that from a purely statistical perspective, the topologies of Models 1 and 2 are indistinguishable in terms of the experimental data (Fig. 2) while Model 3 is worse.\n\nThe simulated profiles for each model (Fig. 4) supports the model selection results. While the Smad7 mRNA and Ski mRNA profiles are slightly greater in Model 1 and Model 3 respectively, all profiles are virtually indistinguishable between all the models. It is likely that the difference in the Ski mRNA profile in Model 3 accounts for the difference observed in the best model selection criteria (Fig. 3b). Regardless of this slight difference, the same qualitative interpretation holds for each model: the speed and magnitude of both Smad7 and Ski mRNA induction profiles are overestimated while the protein level data fits each model to a high degree of confidence.\n\nWhen looking at model predictions it is important to consider whether the parameter sets used to produce them are actually the best parameter sets. This is important because it is quite common for parameter estimation algorithms to find sub-optimal parameters. Here, while improvements can still be made, the algorithm and settings were reasonably well-chosen because the likelihood-ranks plot produced a step-like shape for each model (Fig. 5), heuristically mapping out where the local and global minima are.\n\nProfile likelihoods are only meaningful when calculated from a minimum with respect to the data. For this reason the best three parameter sets from the stochastic genetic algorithm in Model 2 were ‘chased’ with a Hooke & Jeeves algorithm (tolerance = 1e−10 and iteration limit = 1000) using the ‘PyCoTools.tasks.ChaserParameterEstimations’ class. Profile likelihoods were then computed around these three parameter sets, again using the Hooke & Jeeves algorithm (tolerance = 1e−6 and iteration limit = 50). Sampling was conducted on a log10 scale over 6 orders of magnitude, 1e3 times above and below the best estimated parameter values. For brevity, profile likelihoods for Models 1 and 3 are not discussed. The identifiability analysis shows that seven of the ten parameters are identifiable and the remaining three are practically non-identifiable (Fig. 6 and Supplementary Fig. S2).\n\nTo investigate the source of these non-identifiabilities, two strategies were employed: Pearson’s correlation analysis and the ‘profile likelihood model reduction’ approach as described in Maiwald et al. (2016). The Pearson’s correlation approach identified several parameter pairs as putative linear correlations (Supplementary Fig. S3). Of these, only the most correlated pair, the km and I50 parameters of Smad7 transcription, was verified to be log-linearly related in both scatter graphs (Fig. 7a) and profile likelihood traces (Fig. 7b). To resolve this issue, one could replace one of the free parameters in the relationship with the algebraic equation resulting from the fit of a linear model to the profile likelihood trace (Fig. 7b). The other putative relationships suggested by the Pearson’s correlation analysis (Supplementary Fig. S3) were also investigated but the relationships were more difficult to interpret. As an example, Supplementary Figure S4 shows the relationship between ‘(SkiDeg).k1’ and ‘(SkimRNADeg).k1’ parameters. While the scatter graph shows a reasonable linear correlation (Supplementary Fig. S4a), it is defined on a very small interval and the profile likelihood is clearly non-linear, albeit linear on a sub-domain of the parameter space (Supplementary Fig. S4b).\n\nLastly, distributions of parameter estimates were visualized using box plots (Supplementary Fig. S5) and histograms (Supplementary Fig. S6). Despite being presented last, these are computationally inexpensive to generate and are good to view prior to more involved analyses such as profile likelihoods. To demonstrate the effect of sub-optimal parameter sets, a comparison is made between box plots generated for Model 2 using all parameter estimation data (Supplementary Fig. S5a) to those using only the top 10% ranking parameter sets (Supplementary Fig. S5b). Supplementary Figure S5 demonstrates that suboptimal parameter sets can distort the insight that can be gained from visually exploring parameter estimation data. Without truncating the parameter estimation data, the observation that the distributions of parameters from the best parameter sets reflect the identifiability status of the model, would be missed.\n\n4 Discussion\n\nPyCoTools is an open source Python package designed to assist COPASI users in the task of modelling biological systems. PyCoTools offers an alternative high level interface to COPASI tasks including time courses, parameter scans and parameter estimations. While COPASI implements the heavy computation, PyCoTools automates task configuration and execution, thereby promoting efficiency, organization and reproducibility.\n\nPyCoTools bridges COPASI with the Python environment allowing users to take advantage of Python’s numerical computation, visualization, file management and code development facilities. One tool in particular, the Jupyter notebook, allows annotation of code blocks with rich text elements and is a powerful environment from which to develop and share annotated workflows. The combination of Jupyter notebooks, COPASI and PyCoTools therefore enables the production of reproducible and shareable models that are annotated with justifications.\n\nPyCoTools supports model editing using both an object-oriented approach and with Antimony, a model specification language for building SBML models (Smith et al., 2009). The Antimony and COPASI user interface are complementary and can be used together to enhance the modelling process. For example, models in Antimony format can be used as a ‘hard copy’ while a parallel COPASI model can be used for exploratory changes that are ‘committed’ to the hard copy when satisfactory.\n\nPyCoTools supports the configuration of ‘composite’ tasks which are those comprised of a combination of other tasks. These tasks can be configured using the COPASI user interface but generally take time and are vulnerable to human error. For example, users can automatically configure repeat parameter estimations, chaser parameter estimations and model selection problems, thereby circumventing the requirement for manual configuration.\n\nAnother composite task supported by PyCoTools is the profile likelihood method of identifiability analysis (Raue et al., 2009). Models with non-identifiable parameters are common in systems biology and it is useful to have a means of assessing which parameters are reliably defined by an estimation problem. PyCoTools automates the procedure outlined by Schaber (2012) for conducting profile likelihoods in COPASI, thereby enabling COPASI users to perform an identifiability analysis more efficiently and in a way less amenable to errors than manual configuration. PyCoTools also enables users to calculate profile likelihoods from multiple parameter sets thereby enabling users to address one of the shortcomings of the profile likelihood approach: that it is a local method of identifiability analysis.\n\nOne alternative to COPASI and PyCoTools is Data2Dynamics (Raue et al., 2015). While Data2Dynamics provides an excellent range of model analysis tools, the transfer of files between COPASI and Data2Dynamics is imperfect, often necessitating that a COPASI user redefine their model within the Data2Dynamics environment. PyCoTools allows COPASI users to stay within the COPASI environment, thereby making profile likelihood analysis more accessible to COPASI users.\n\nIn this work we have demonstrated PyCoTools by posing a model selection problem to discriminate between three model topologies (Fig. 1) with respect to some experimental data in response to TGF-β (Fig. 2). Rather than using synthetic data, our aim was to demonstrate in a ‘real world’ scenario how PyCoTools can be used together with COPASI to calibrate a set of models and discriminate between them.\n\nAs this is primarily a software demonstration and not a biological investigation, the model selection problem proposed was designed to be as simple as possible whilst still being non-trivial. Mechanistically the three models (Fig. 1) are alternative hypotheses which attempt to address the dynamics of the Smad7 (Fig. 2) negative feedback. Model alternatives were based on a published dynamic model of TGF-β signalling (Zi and Klipp, 2007) that was adapted to incorporate Smad7. Since the decay of Smad7 is transient and fast (Fig. 2a), the simplest mechanism involving only Smad7 with first order mass action degradation kinetics would not be able to account for the observed decline in Smad7. Therefore Smad7 degradation was assumed to be an active process. Since Ski is a known Smad co-repressor (Akiyoshi et al., 1999) and Smad7 is a Smad responsive gene (Hayashi et al., 1997), Ski was proposed to be transcribed in response to TGF-β (Fig. 2b) and inhibit Smad7 transcription. The model alternatives are slightly different representations of this hypothesis (Fig. 1).\n\nIn this model selection problem it is clear that the model topologies chosen are too similar to be discriminated with the experimental data and therefore the models are virtually indistinguishable (Fig. 4). Generally, with model selection, the strongest statement that can be made about a model is a rejection, since accepting the hypothesis does not necessarily guarantee that it is correct. By comparing the performance of multiple models in model calibration it is possible reject one or more topologies in favour of another. Here, however, because the models are so similar, it was not possible to provide support for any model being worse than any other, despite the minor differences in model selection criteria for Model 3 (Fig. 3a). In a more comprehensive investigation many more topologies would be similarly compared to iteratively reject topologies until the model is capable of making useful, validatable predictions.\n\nRegardless of the biological interpretation, we have demonstrated the process of using PyCoTools and COPASI to discriminate between model alternatives and to critically assess the parameter estimation process. Model calibration is an essential part of a systems modelling investigations, but it is often limited by a vast, underdetermined parameter space and therefore, procedures that provide a measure of uncertainty are valuable. In PyCoTools, we have implemented a number of features aimed towards gauging confidence and uncertainty in the optimization process so that COPASI users can diagnose problems and make better informed decisions based on their parameter estimation output. These tools include: the likelihood-ranks plot (Fig. 5) which enables evaluation of an optimization algorithm and settings on a specific problem (Raue et al., 2013); ensemble time courses (Fig. 4) which calculate confidence intervals from predictions made from multiple best parameter sets and propagates uncertainty from parameter estimates to model predictions; profile likelihoods for assessing identifiability (Fig. 6, Supplementary Fig. S2) and for model reduction (Fig. 7b) (Maiwald et al., 2016); Pearson’s correlation heat maps (Supplementary Fig. S3) and scatter graphs (Fig. 7a) for identifying relationships, and box plots (Supplementary Fig. S5) and histograms (Supplementary Fig. S6) for visualizing distributions of parameter estimates. Together these tools provide detailed information about an optimization problem that can be used to guide the modelling process.\n\n5 Conclusion\n\nPyCoTools is an open-source and extensible Python package designed to facilitate the use of COPASI, particularly for model calibration. PyCoTools supports a range of tools which are either wrappers around COPASI tasks, an ordered workflow of task configurations, or plotting facilities for exploratory data analysis on parameter estimation data. Use of PyCoTools can enhance the effectiveness with which one can calibrate models to experimental data and discriminate between alternate hypotheses.\n\nFunding\n\nThis work was funded by Procter & Gamble. The contribution from AGM and CJP was supported by the Medical Research Council (https://www.mrc.ac.uk/) and Arthritis Research UK (http://www.arthritisresearchuk.org/) as part of the MRC-Arthritis Research UK Centre for Integrated research into Musculoskeletal Ageing (CIMA) (MR/K006312/1). The work builds on a BBSRC LINK grant awarded to SAB (BB/K019260/1).\n\nConflict of Interest: none declared.\n\nReferences\n\n© The Author(s) 2018. Published by Oxford University Press."
    }
}
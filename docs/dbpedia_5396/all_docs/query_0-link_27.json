{
    "id": "dbpedia_5396_0",
    "rank": 27,
    "data": {
        "url": "https://www.sysbee.net/blog/monitoring-origins-where-it-all-began/",
        "read_more_link": "",
        "language": "en",
        "title": "Monitoring origins: Where it all began (part one)",
        "top_image": "https://cdn-web.sysbee.net/wp-content/uploads/2020/06/monitoring-1.png?x55080",
        "meta_img": "https://cdn-web.sysbee.net/wp-content/uploads/2020/06/monitoring-1.png?x55080",
        "images": [
            "https://px.ads.linkedin.com/collect/?pid=6399289&fmt=gif",
            "https://www.facebook.com/tr?id=553625016460550&ev=PageView&noscript=1",
            "https://cdn-web.sysbee.net/wp-content/uploads/2019/09/sysbee-logo-amber-1.png?x52680",
            "https://cdn-web.sysbee.net/wp-content/uploads/2021/06/sysbee-logo-amber-1.png?x52680",
            "https://cdn-web.sysbee.net/wp-content/uploads/2020/06/monitoring-1.png?x52680",
            "https://cdn-web.sysbee.net/wp-content/uploads/porto_placeholders/100x64.jpg?x52680",
            "https://cdn-web.sysbee.net/wp-content/uploads/porto_placeholders/100x59.jpg?x52680",
            "https://cdn-web.sysbee.net/wp-content/uploads/porto_placeholders/100x49.jpg?x52680",
            "https://cdn-web.sysbee.net/wp-content/uploads/porto_placeholders/100x49.jpg?x52680",
            "https://cdn-web.sysbee.net/wp-content/themes/porto/images/lazy.png?x52680",
            "https://cdn-web.sysbee.net/wp-content/uploads/2020/11/shutterstock_resized-1-450x231.jpg?x52680",
            "https://cdn-web.sysbee.net/wp-content/uploads/2020/07/InfluxData-webinars-OG-1-1-450x231.png?x52680",
            "https://cdn-web.sysbee.net/wp-content/uploads/2022/02/Ansible-preparation-1-450x231.jpg?x52680",
            "https://cdn-web.sysbee.net/wp-content/uploads/2021/11/shutterstock_1460862404-ai-1-2-450x231.png?x52680",
            "https://cdn-web.sysbee.net/wp-content/uploads/2021/10/New-Project-3-min-450x231.png?x52680",
            "https://cdn-web.sysbee.net/wp-content/uploads/2023/07/monolith-to-microservice-450x231.jpg?x52680",
            "https://cdn-web.sysbee.net/wp-content/uploads/2020/04/New-Project-51-450x231.jpg?x52680",
            "https://cdn-web.sysbee.net/wp-content/uploads/2020/01/Screenshot-2019-09-27-at-11.27.17-1-450x231.png?x52680",
            "https://cdn-web.sysbee.net/wp-content/uploads/2020/04/webshop-stability-1536x1024-1-450x231.jpg?x52680",
            "https://cdn-web.sysbee.net/wp-content/uploads/2023/03/AWS-data-transfer-2023-450x231.jpg?x52680",
            "https://cdn-web.sysbee.net/wp-content/uploads/2019/09/sysbee-logo-amber-1.png?x52680"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Branko Toić"
        ],
        "publish_date": "2020-06-03T13:27:10+00:00",
        "summary": "",
        "meta_description": "Today’s monitoring system have many features to help you get all of the data you need. However, tools were very limited.",
        "meta_lang": "en",
        "meta_favicon": "https://cdn-web.sysbee.net/wp-content/uploads/2021/06/sysbee_logo_amber-1-svg-2.png?x52680",
        "meta_site_name": "Sysbee",
        "canonical_link": "https://www.sysbee.net/blog/monitoring-origins-where-it-all-began/",
        "text": "Why do we need monitoring\n\nMonitoring is a vital component of any infrastructure setup. As we discussed previously, monitoring plays an integral role when you wish to ensure that your system works smoothly and efficiently.\n\nThere are two main reasons why we use monitoring in our business:\n\nTo resolve issues before they arise\n\nBy monitoring the status of our services, we can see problems as soon as they occur. With a sound monitoring system, our engineers can react faster and fix problems instantly, before our customers even notice them.\n\nTo see patterns and anticipate potential issues\n\nBy analyzing data collected by our monitoring system, we can spot trends and anticipate potential issues. With such an approach, we can provide our customers with proactive support, instead of acting reactively.\n\nA good monitoring system needs to answer the following questions:\n\nDoes it work? (basic ping or uptime test)\n\nHow does it work? (collecting performance metrics)\n\nWhat’s the trend? (what was happening / predictions)\n\nAdditionally, monitoring shouldn’t affect your system’s normal operations, and should also be able to send out alarms if something is amiss.\n\nNow that we’ve established why we needed monitoring, to begin with, and what a good monitoring system is, we can go down the memory lane and discuss all of the different tools our engineers tried out before coming up with our current monitoring setup.\n\nMonitoring beginnings – 1999 to 2006\n\nWe consider RRDtool to be the first cornerstone in server-side monitoring. Although there are tools which officially started with their development much earlier (e.g. Nagios in 1996), the timeline we set up is by project popularity.\n\nThe RRD tool has evolved as a by-product of the MRTG or Multi Router Traffic Grapher. MRTG extracted data via SNMP primarily from network equipment, but it could also be configured to graph server metrics exposed via MIBs such as CPU usage, memory usage, etc. In practice, most people used it to monitor network equipment and server load.\n\nAfter that comes Cacti, a project aimed at creating an interface which will be more user friendly compared to MRTG. Since Cacti inherited MRTG, its primary focus was collecting information through SNMP.\n\n2006 – GIGRIB and Nagios\n\nThe year is 2006, and the first public monitoring system called GIGRIB (today’s Pingdom) becomes available to the public eye.\n\nThe idea behind GIGRIB was fantastic, as it distributed ping and HTTP status monitoring of target hosts. There was a free version available, which allowed monitoring up to 10 hosts provided that the p2p distributed monitoring tool is downloaded and launched (sort of like a SETI@home just for uptime monitoring).\n\nWe are still in 2006, and Nagios is starting to gain popularity.\n\nNagios brought a real hype in the monitoring world, something like Kubernetes does today, and of course, we had to dig a bit deeper into this monitoring system.\n\nAfter a couple of tests performed we’ve confirmed that:\n\nService uptime checks – work\n\nMetrics – sort of collected\n\nTrends – followed\n\nAlarms – available\n\nAnd with that, we were saved – Nagios was the way to go and we had a platform ready for our new production monitoring system.\n\nWhat we’ve learned over the years by using Nagios:\n\nIt has the best performance compared to similar products.\n\nIt has an extensive community, accompanied by a lot of extensions and service check plugins.\n\nIt’s quite complex – the initial setup took a good two months, and we had to reconfigure it from scratch at least three to four times, every couple of months.\n\nThe updates (at the time) were not an easy task\n\nScalability, and High Availability, although it exists in theory, is terribly complicated and quite easy to mess up. We do not recommend beginners to tackle this task.\n\nNonetheless, we do recommend giving Nagios or some of its derivatives a shot, even today.\n\nMissing Metrics\n\nNagios is an excellent tool for detecting the current status; however, Nagios lacks one key component – metrics. Without metrics, we have no insight into machine performance while the machine is working as expected.\n\nThe typical workflow with Nagios is:\n\nDon’t touch anything while it’s working ok\n\nWhen it stops working, Nagios will let you know\n\nYou log in and check what’s happening\n\nBut what happens when the notification arrives too late, and the only option is to reboot the system? What was happening on the system until that point? That’s why we need metrics.\n\n2007 – Munin\n\nWith that in mind, we started using Munin. The Munin projects first public release was back in 2003, with very active development up to 2005. The project shows signs of some development efforts even today, as quite a lot of people are still using it.\n\nWe started using Munin sometime during 2007 as on-site monitoring. Munin is a tool which is clearly server-oriented; it contains a large number of plugins for various server-side services and is relatively easy to set up.\n\nEven though Munin was a great tool at the time, it comes with its own set of problems we encountered during our day-to-day use:\n\nThe manual deployment of Munin and desired plugins is laborious.\n\nMunin used to be pretty resource hungry when generating graphs\n\nIf the host is down, we have no idea why it went down until we recovered it.\n\nCertain plugins have been known to trigger bugs and additional resource usage.\n\nA particularly annoying “feature” was that by default it had no central place to view graphs from all hosts\n\nEven when you overcome the previous issue, Disk IO becomes a massive problem after 200+ hosts are loaded onto the central Munin\n\nTo tackle these issues, we were still looking for a better solution, and in 2013 we found Ganglia.\n\n2013 – Ganglia\n\nGanglia is a scalable, distributed monitoring tool for high-performance computing systems, clusters and networks. The software is used to view either live or recorded statistics covering metrics such as CPU load averages or network utilization for many nodes.\n\nGanglia was used by Wikimedia at the time and was initially designed as a monitoring system for HPC clusters in universities. Ganglia comes with a broad set of default metrics (350 to 650 metrics per host), it’s easy to scale, and by default, everything is centralized. Additionally, Ganglia has an easy to use web interface.\n\nHow Ganglia works\n\nBy default, all gmond processes (agents) talk to each other in multicast. So each host in the cluster is aware of the other and has information about its performance.\n\nIt can also be configured in Unicast mode, which proved to be a rather good fit for our environment. In this setup, one or more gmond processes (aggregators) are aware of all hosts, and others know only about their own metrics.\n\nAn aggregator collects information for its cluster in our case the datacenter or logical entity of a client cluster.\n\nGmeta is another process on a centralized host that stores RRDs and collects metrics from primary or failover gmond aggregators.\n\nIf necessary, the client can have his own gmeta collector and web interface, and forward all the information to our central interface. The web interface is a simple LAMP stack application.\n\nGanglia’s web interface\n\nGanglia’s web interface shows us the entire GRID, which consists of one or more clusters.\n\nThe interface supports search and has the ability to create custom views (dashboards). We can easily generate graph aggregates, compare hosts, see trends (e.g. when a server runs out of disk) and add annotations to graphs.\n\nThe cluster heatmap is a nice touch but is of little use if the cluster is not load balanced.\n\nThe timeshift option shown in the graph below was a total killer feature for us, as it provided an instant view to compare server behavior against the same interval in the past.\n\nChallenges with Metric systems so far\n\nWe used Munin which writes to RRDs, and even with smaller sets of metrics we already experienced disk IO issues. Using Ganglia was just like adding fuel to the fire.\n\nThe problem lies in a large number of tiny writes. Each data point must read a 1k header, write 1k into it and write an additional 1k as a metric point. Because of the 4k block level, we end up with 12k writes, or 9k wasted IOs.\n\nFortunately, the Ganglia integrates very well with RRDcached, which aggregates a good deal of data and writes to disk so that they become manageable.\n\nLike Munin, Ganglia also suffers from various bugs in plugins which can block the entire gmond, resulting in missing metrics. That brings us to the challenge of meta monitoring, or how to monitor problems with missing metrics.\n\nWe had to standardize many things that weren’t related to monitoring, which turned out to be a great thing as it made us work out some things that would otherwise never have come to our attention.\n\nAnd finally, the fact remains that Ganglia’s dashboard is old-ish, and we needed something more advanced and modern.\n\nWhy not Graphite?\n\nGood question. At one point, we had the perfect opportunity to try it out.\n\nGraphite comes by default with Grafana support\n\nDrop-in replacement, gmeta can already push into its carbon storage.\n\nIt’s supposed to be scalable as is used by big companies.\n\nHowever, by testing it, the performance proved to be disastrous\n\nIn a situation where we had a new server with a better disk subsystem to replace our current Ganglia host, we did a little experiment.\n\nOn the original Gmeta collector, we started pushing one copy of all collected metrics to Graphite on a new server. The same set of metrics, same time interval. Graphite spasms in the throes of disk IO. Carbon written in Python just can’t keep up with RRD.\n\nOne option would be to buy more monitoring servers, but that’s not the direction in which any company wants to go.\n\nAll is not lost, Graphite web interface still supports reading metrics from RRD, and can be easily connected to Ganglia’s RRD store to act as a proxy for Grafana.\n\nThe three G’s setup\n\nUsing the best from all the available tools at the time we consolidated our monitoring infrastructure around Ganglia, Graphite and Grafana.\n\nSince Ganglia is already collecting information very efficiently, it made no sense to perform double service checks through Nagios, so we wrote an internal integration set of scripts called gnag.\n\nThe idea behind this was to use Nagios as a service alert dashboard only. And offload all the metrics and availability checking to Ganglia. In this scenario, Nagios can use gnag to gather large sets of metrics in batches from gmond processes quickly and efficiently.\n\nAt the same time, Nagios and gnag were able to check for stale metrics, this way detecting any metric that isn’t collecting data in predefined intervals.\n\nGanglia RRD storage then serves as a backend for the Graphite’s Django web app, acting as metrics datastore for Grafana web interface."
    }
}
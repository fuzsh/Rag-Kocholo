{
    "id": "dbpedia_5396_3",
    "rank": 37,
    "data": {
        "url": "https://arxiv.org/html/2406.02565v1",
        "read_more_link": "",
        "language": "en",
        "title": "Sequence-to-sequence models in peer-to-peer learning: A practical application",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://arxiv.org/html/extracted/5573249/imgs/02_process.png",
            "https://arxiv.org/html/extracted/5573249/imgs/03_userlibri_example.png",
            "https://arxiv.org/html/extracted/5573249/imgs/03_padding_short.png",
            "https://arxiv.org/html/extracted/5573249/imgs/03_padding_long.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            "peer-to-peer",
            "sequence-to-sequence",
            "Deep Speech 2",
            "UserLibri",
            "Automatic speech recognition"
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "I Introduction\n\nSequence-to-sequence (Seq2Seq) models have emerged as a powerful framework in various natural language processing (NLP) and sequence generation tasks, ranging from machine translation [Diyah2021], text summarization [Tian2021] to even automatic speech recognition (ASR) [Li2021]. However, their application in peer-to-peer deep learning scenarios [Blot2016], even within the realm of Federated Learning (FL) [BrendanMcMahan2017], remains relatively under explored. Peer-to-peer and FL environments are comprised of agents, each holding local private data and preforming local computations, with the primary mode of interaction being the sharing of models with outside entities or other agents. Agentâ€™s local data is considered as private and must never leave the agent, therefore only the machine learning models are exchanged with other participants in the environment. In the context of Federated Learning (FL), agents disseminate their trained models solely with a central server, whereas in peer-to-peer settings, agents engage in direct model exchange among themselves, thereby rendering peer-to-peer environments notably more complex scenarios.\n\nAn agent, which may be any edge device such as phone or laptop, owns local data which is most likely generated by the agent or the user using the device. When considering the applications of peer-to-peer learning, the availability of local data must also be considered. In this context, local textual exchanges or user-generated written content emerge as potential dataset that can be utilized for training neural network models tailored to tasks like next-word prediction [Sajina2023]. In the context of Seq2Seq model applications, an Automatic Speech Recognition (ASR) task presents itself as a viable option. Users can contribute to the generation of data by orally presenting specific paragraphs or sentences, therefore creating a dataset of audio clips and corresponding textual representations which can be utilized to train a local Seq2Seq model. The purpose of this study is to investigate if such application of the Seq2Seq model is viable peer-to-peer environment. Reproducible code is available on a publicly available repository: https://github.com/rosaj/p2p_bn.\n\nIII Related work\n\nResearch in the domain of Seq2Seq models for Automatic Speech Recognition (ASR) has been extensively explored and documented within the context of training a single centralized model [Fan2020, Li2021]. However, the applicability of these models within decentralized systems remains an area requiring further investigation. In the realm of centralized distributed learning, such as Federated Learning, numerous studies have emerged involving sequence-to-sequence models across various natural language processing (NLP) tasks.\n\nFor instance, Lin et al. [Lin2022] developed FedNLP, a benchmarking framework tailored for evaluating Federated Learning methods on common NLP tasks. his framework utilizes Transformer-based language models for tasks like text classification, sequence tagging, question answering, and sequence-to-sequence generation. Similarly, Lu et al. [Lu2021] proposed the Federated Natural Language Generation (FedNLG) framework, which facilitates the learning of personalized representations from distributed datasets across devices. FedNLG enables the implementation of personalized dialogue systems by pre-training standard neural conversational models over large dialogue corpora and fine-tuning model parameters and persona embeddings in a federated manner.\n\nFurthermore, recent research by Nguyen et al. [Nguyen2023] specifically addresses the ASR task within the context of Federated Learning. Their study employs the Wav2vec 2.0 model [Baevski2020], which features a Transformer-influenced architecture, demonstrating the feasibility and effectiveness of leveraging advanced neural network architectures for speech recognition tasks within decentralized learning frameworks.\n\nRelated work does not address the applicability of Seq2Seq models based on LSTM cells for the ASR task, especially within the peer-to-peer environments. Several potential problems may arise from the architecture comprised of LSTMs, as the vanishing or exploding gradients problems associated with RNN cells. This problems may especially be highlighted due to the nature of local datasets comprised on agents which may contain small amount of data and differ from agent to agent [Ma2022]. This work will analyze the applicability of Seq2Seq model based on architecture utilizing LSTM cells for agents collaborating in peer-to-peer environments.\n\nThe existing literature lacks a comprehensive exploration of the applicability of Seq2Seq models based on LSTM cells for the ASR task, particularly within peer-to-peer environments. Such architectures pose several potential challenges, including issues related to vanishing or exploding gradients commonly associated with recurrent neural network (RNN) cells. These challenges may be exacerbated by the nature of local datasets held by individual agents in peer-to-peer settings, which may vary in size and composition [Ma2022]. By investigating the performance of such models under two different communication and aggregation techniques, this research seeks to elucidate the potential benefits and limitations of employing LSTM-based Seq2Seq models in decentralized learning settings. Through empirical evaluations and experimental validations, this work aims to provide insights into the feasibility and effectiveness of utilizing LSTM-based Seq2Seq models for ASR tasks in distributed peer-to-peer environments.\n\nIV ASR in peer-to-peer learning\n\nDataset. UserLibri [UserLibri] dataset will be used in the experiments. This UserLibri dataset is a re-formatted version of the LibriSpeech data [Librispeech] that is derived from English audiobooks and contains 1000 hours of speech sampled at 16 kHz. An example of audio-text pair from the UserLibri dataset is shown in Figure 2. In UserLibri, the data is reorganized into individual user datasets consisting of paired audio-transcript examples and domain-matching text-only data for each user. Data segregated in this manner allows for a simulation of realistic scenario with user-specific audio clips. The UserLibri dataset contains 55 unique users (as part of the test-clean split), with average of 47.1 audio clips per user. The duration of audio samples ranges from 1 to 35 seconds, with an average length of 7.5 seconds, resulting in approximately 5 hours of audio recording in total. Within these audio clips, the number of spoken words spans from 1 to 96, with an average of 20 words per clip. Such small amount of local data may cause problems during local training which may lead to poor model performance. Threfore, an additional LJ Speech dataset [ljspeech17] will be used in additional experiments to confirm any findings from the experiments regarding the UserLibri dataset.\n\nThe LJ Speech dataset consists of 13,100 short audio clips with accompanying transcriptions for clip. The audio recordings sourced from the LJ Speech dataset exhibit varying durations, spanning from 1 to 10 seconds, with an average duration of 6.5 seconds per clip, resulting in around 24 hours of audio recordings in total. Within these recordings, the number of spoken words ranges from 1 to 39, with an average of 17 words per clip. Both datasets were split in a 70%-30% for training and validation purposes.\n\nTo ensure uniformity in input dimensions for training, spectrograms and character tokens underwent a padding process, aligning them to a consistent length. Specifically, each spectrogram was extended or truncated to a fixed length of 2048 time steps, while character tokens were padded or trimmed to 256 characters. Spectrograms and token sequences were padded with zeros where necessary. An example of short and long audio-text sequence in Figure 3.\n\nModel. Model architecture consists of two blocks of 2D Convolutional layers that are followed by a Batch Normalization layer and ReLu activation function. A single RNN GRU layer with 512 units is used to constrain the number of model parameters. GRU layer is followed by a Fully connected layer, Dropout layer, and the final Fully connected output layer. The resulting model is comprised of 7.7M model parameters which is of acceptable size for edge devices, and satisfactory for this use case since the goal is to evaluate the applicability of Seq2Seq models in peer-to-peer environments, rather than achiveing state-of-the-art results. To reduce memory consumption and improve the training process, batch size is set to 8. Adam [kingma2017adam] optimizer with a fixed learning rate of 0.0001 is used in all experiments. Model predicts the probabilities of each individual character of English alphabet and uses the greedy decoder to select the most probable character at each time step.\n\nMethodology. The learning process of agents was emulated in memory using two distinct peer-to-peer learning methodologies: Pull-gossip [Jin2016] and P2P-BN [Sajina2023]. This simulation engenders a cyclical process wherein each local training step is succeeded by a communication step, with these phases iteratively recurring. A directed sparse network topology with three peer connections was utilized for both methods. In the Pull-gossip approach, upon pulling models from its peers, an agent forms a new model by aggregating all received models using a weighted summation mechanism: xi=âˆ‘{wiâ¢j>0}wiâ¢jâ¢xjsubscriptğ‘¥ğ‘–subscriptsubscriptğ‘¤ğ‘–ğ‘—0subscriptğ‘¤ğ‘–ğ‘—subscriptğ‘¥ğ‘—x_{i}=\\sum_{\\{w_{ij}>0\\}}w_{ij}x_{j}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = âˆ‘ start_POSTSUBSCRIPT { italic_w start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT > 0 } end_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT, where xisubscriptğ‘¥ğ‘–x_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT represents the new model of agent iğ‘–iitalic_i, xjsubscriptğ‘¥ğ‘—x_{j}italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT denotes the received model from peer jğ‘—jitalic_j, and wiâ¢jsubscriptğ‘¤ğ‘–ğ‘—w_{ij}italic_w start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT signifies the weight associated with the connection between agents iğ‘–iitalic_i and jğ‘—jitalic_j. Conversely, in the P2P-BN method, upon receiving a model from a peer, an agent constructs a new model by averaging the received model with its local model: xi=xi+xj2subscriptğ‘¥ğ‘–subscriptğ‘¥ğ‘–subscriptğ‘¥ğ‘—2x_{i}=\\frac{x_{i}+x_{j}}{2}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = divide start_ARG italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_ARG start_ARG 2 end_ARG.\n\nAgentâ€™s local test accuracy is measured after performing local SGD updates (i.e. an epoch). As comparison, results of a centrally trained model on pooled data are also analyzed. Experiments regarding UserLibri dataset utilized all 55 user dataset in a 55 agent environment, while the data from LJ Speech was uniformly divided across 55 agents in the experiments utilizing the LJ Speech dataset. To assess the performance of agentsâ€™ models, the average model CTC loss and Word Error Rate (WER) metrics will be examined. The computation of average local model performance across all agents using their respective local datasets serves as a pivotal evaluation metric within peer-to-peer environments [Mills2022, Sajina2023, SajinaMT2023].\n\nVI Conclusion\n\nIn conclusion, this discussion has shed light on the utilization of Seq2Seq models in both centralized and peer-to-peer learning environments for Automatic Speech Recognition (ASR) tasks. While centralized training on pooled data from all users demonstrated more efficient convergence and lower Word Error Rates (WER), peer-to-peer learning, where each agent independently trains its own model using local data, showcased promising potential despite requiring more training iterations to achieve comparable results. These findings underscore the applicability of Seq2Seq models in peer-to-peer settings. However, a critical observation gleaned from the experiments is the pivotal role played by the availability and richness of local training data across decentralized agents. The correlation between larger local datasets and lower Word Error Rates (WER) on the LJ Speech dataset underscores the importance of data quantity in achieving optimal performance with Seq2Seq models. However, itâ€™s crucial not to assume this constraint for decentralized agents, as they may not have access to substantial amounts of local data. Therefore, future research endeavors should prioritize the exploration and development of methods that enable robust learning performance for Seq2Seq models, even in scenarios where agents have access to only small quantities of data Additionally, future research endeavors should focus on addressing challenges such as slow convergence rates in peer-to-peer learning and devising techniques to expedite the learning process, thereby facilitating more efficient and scalable deployment of Seq2Seq models in decentralized settings.\n\n\\printbibliography"
    }
}
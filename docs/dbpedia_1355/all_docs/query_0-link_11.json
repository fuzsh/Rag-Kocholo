{
    "id": "dbpedia_1355_0",
    "rank": 11,
    "data": {
        "url": "https://computingconf.com/2015/technical-papers.php",
        "read_more_link": "",
        "language": "en",
        "title": "2015 IEEE International Advance Computing Conference",
        "top_image": "",
        "meta_img": "",
        "images": [],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            "5th International Advanced Computing Conference"
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "img/apple-touch-icon.png",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "The growth of E commerce has led to the abundance growth of opinions on the web, thereby necessitating the task of Opinion Summarization, which in turn has great commercial significance. Feature extraction in Opinion Summarization is very crucial as selection of relevant features reduce the feature space which successfully reduces the complexity of the classification task. The paper suggests extensive pre-processing technique & an algorithm for extracting features from Reviews/Blogs. The proposed technique of Feature Extraction is unsupervised, automated and also domain independent. The improved effectiveness of the proposed approach is demonstrated on a real life dataset that is crawled from many reviewing websites such as CNET, Amazon etc.\n\nA rapid growth and development of life styles as well communicating media in recent years have changed considerably and called as information age. In this information age lot of changes were taken place in all domine like speed, data capacity, and distance of communication due to the development. Also hacking, tracing communicating bodies are also in place. The news channels and cameras are playing the major role in communication. The advancement in the TV news channel enabled by all the recent information occurs in the world available instantaneously as for as possible. This is due to moving line which is associated with a TV news channel but few treats the moving line text as disturbance where the above that watching above video so we plan to detect and extract the moving text from the news video using hybrid technology in association of edge and connected component detection.\n\nNow-a-day's monitoring the objects (human beings, animals, buildings, vehicles etc.,) in a video is a major issue in the areas such as airports, banks, military installations etc., Classification of objects in a video involves the process of searching, retrieving and indexing. This process is implemented by extracting the features such as color, texture and shape. This technique is difficult but it has its limitations at various situations. Techniques such as edge detection using various filters, edge detection operators, CBIR (Content Based Image Retrieval) and Bag-of visual words are used to classify videos into fixed broad classes which would assist searching and indexing using semantic keywords. The proposed approach extracts three types of features viz. Color features using RGB and HSV histograms, Structure features using HoG, DHoG, Harris, Prewitt, LoG operators and Texture features using LBP, Fourier and Wavelet transforms. Additionally BoV is used for improving the classification performance and accuracy. SVM, Bagging, Boosting, J48 classifiers is used for classification.\n\nNow-a-days large amount of data is generated from various stake holders such as data from sensors and satellites regarding environment and climate, social networking sites about messages, tweets, photos, videos and data from telecommunications etc. This big data, if processed in real-time, helps decision makers to make timely decisions when an event occurred. When source data sets are large (velocity, variety, veracity) traditional ETL (Extract, Transform, Load) is time consuming process. This paves path to extend traditional data management techniques for extracting business value from big data. This paper extends the hadoop framework for performing entity resolution in two phases. In phase 1 MapReduce generate rules for matching two real world objects with similarities. The more the similarity, the objects are similar. Similarity is calculated using domain dependent and independent Natural language processing measures. In Phase 2 these rules are used for matching stream data. Our proposed approach uses 13 semantic measures for resolving entities in stream data. Stream data such as tweets, messages, e-catalogues are used for testing the proposed system.\n\nCiphertext-policy attribute based encryption (CP-ABE) is becoming very significant in distributed computing environment. Distributed computing environment is very popular way of storing and distributing information, because it is easy to store some information at one place and distribute the information from there. In CP-ABE scheme, every information is encrypted under an access structures, this access structures defines, who can access the encrypted information. But in CP-ABE scheme, we also store access structures along with encrypted information and sometimes this access structures can reveal many things about the plaintext information and the decryptor, therefore any adversary can learn many things. Therefore, it is appropriate to have CP-ABE scheme in which the access structures are not public means access structures are hidden from the users. We have proposed one scheme, in which users do not store access structures with encrypted information (ciphertext). We have used composite-order bilinear groups in our scheme.\n\nVehicular Ad-Hoc Networks is an ad hoc network connecting mobile vehicles which possess dynamic topology thus low link stability in terms of connectivity. The critical concern in ad hoc routing is routing which is handled via various routing solutions. This paper focuses on improving the I-AODV routing by considering V2V and V2I communication. The I-AODV (Infrastructure based AODV) is routing protocol that facilitate communication among vehicles through RSUs and is broadcasting in nature. This paper discusses prediction based multicasting which aids in reducing delay and improves other performance metrics. Broadcasting technique does not utilizes network resources which makes the network inefficient thus reduces throughput of the network, applying multicasting solves the purpose of proper utilization of resources as well as prediction technique helps in improving localization overhead. This technique improves the network performance metrics such as end-end delay, throughput, fuel emission, packet overhead and Packet Delivery Ratio.\n\nThe advancement of wireless communication leads researchers to conceive and develop the idea of vehicular networks, also known as vehicular ad hoc networks (VANETs). In Sybil attack, the WSN is destabilized by a malicious node which create an innumerable fraudulent identities in favor of disrupting networks protocols. In this paper, a novel technique has been proposed to detect and isolate Sybil attack on vehicles resulting in proficiency of network. It will work in two-phases. In first phase RSU registers the nodes by identifying their credentials offered by them. If they are successfully verified, second phase starts & it allots identification to vehicles thus, RSU gathers information from neighboring nodes & define threshold speed limit to them & verify the threshold value is exceed the defined limit of speed. A multiple identity generated by Sybil attack is very harmful for the network & can be misused to flood the wrong information over network. Simulation results show that proposed detection technique increases the possibilities of detection and reduces the percentage of Sybil attack.\n\nHeuristic redundancy optimization with ST (Source-Terminal) reliability measure for complex network; an extensively studied problem has been modified to heuristic redundancy optimization with SAT (source to all terminal) reliability measure.\n\nComputer based automated system is one of the important diagnostic tools in medical field. Diabetic Retinopathy is an eye disorder in which red lesions due to blood leakages can be spotted on retinal surface. This disease is commonly observed in long term diabetic patients. Ignorance to this disease can result into permanent blindness. Early stage signs of diabetic retinopathy are called as Red lesions viz. microaneurysms and hemorrhages. This paper presents a unique methodology for automatic detection of red lesions in fundus images. Proposed methodology employs modified approach to matched filtering for extraction of retinal vasculature and detection of candidate lesions. Features of all candidate lesions are extracted and are used to train Support Vector Machine classifier. In turn support Vector Machine classifies input image object into lesion or non-lesion category. The method is tested on 89 fundus images from DIARETDB1 database. The proposed algorithm gives performance as sensitivity 96.42%, specificity 100% and accuracy 96.62%.\n\nIn sensing it's the ad-hoc sensor and data routing which is an important research direction. Security work is prioritized in this area and focusing primarily at medium access control or the routing levels on denial of communication. Attacks focusing on routing protocol layer are known as resource depletion attacks in this paper. This attack impacts by persistently disabling the network and causing the node's battery power drain drastically. There are protocols established which tends to protect from DOS attacks, however it isn't possible perfectly. Vampire attack is one such DOS attack. These Vampire attacks depends on various characteristics of well-known many classes of routing protocols as these are not specific to any particular protocol. These Vampire attacks can be easily executed using even a single malicious intruder, who sends simply protocol complaint message, these attacks are thus destructing and very hard to detect. In the nastiest condition, an individual attacker has the ability to enlarge the energy usage of the network by a factor of O(N), where N is the quantity of nodes in the network. A new proof-of-concept protocol is a method discussed to mitigate these kinds of attacks. This protocol limits the damage caused at the time of packet forwarding done by Vampires. To diminish the Vampire attacks using PLGP-a which identifies malicious attack, certain approaches have also been discussed\n\nAn effective content-based image retrieval system is essential to locate required medical images in huge databases. This paper proposes an effective approach to improve the effectiveness of retrieval system. The proposed scheme involves first, by detecting the boundary of the image, based on intensity gradient vector image model followed by exploring the content of the interior boundary with the help of multiple features using Gabor feature, Local line binary pattern and moment based features. The Euclidean distance are used for similarity measure and then these distances are sorted out and ranked. As a result, the Recall rate enormously improved and Error rate has been decreased when compared to the existing retrieval systems.\n\nThe main purpose of optimization of relay coordination in an extensively large electrical network is to enhance the selectivity and at the same time reducing the fault clearing time to improve reliability of the system. The relays provided are set to function properly in normal as well as abnormal condition. In this paper; the main focus is to find out minimum Time Dial Setting (TDS) for the relays connected in-whatever configuration using Flower Pollination Algorithm. The said algorithm is compared with Linear Programming Technique. The algorithm has been implemented in MATLAB and tested on radial feeder fed from one end as well as on parallel Feeder system. The innovative feature of the paper is application of nature inspired algorithm to one of the major problem of optimization in the field of power system protection.\n\nNetwork-on-Chip (NoC) is a new approach for designing the communication subsystem among IP cores in a System-on-Chip (SoC). NoC applies networking theory and related methods to on-chip communication and brings out notable improvements over conventional bus and crossbar interconnections. NoC offers a great improvement over the issues like scalability, productivity, power efficiency and signal integrity challenges of complex SoC design. In a NoC, the communication among different nodes is achieved by routing packets through a pre-designed network according to different routing algorithms. Therefore, architecture and related routing algorithm plays an important role to the improvement of overall performance of a NoC. The technique one which is used presently in node is priority based technique packet routing which leads the packet stacking which intern leads to performance degradation. In this paper, proposes a modified random Arbiter combined with deterministic XY routing algorithm to be used on router of NoC. In this method router contains random arbiter along with priority encoder which results into fast way to transfer packet via a specific path between the nodes of the network without stacking. This in turn optimizes the packet storage area and avoids collision because node arbiter will service the packets randomly without any priority. In addition to that this method will ensure a packet always reaches the destination through the possible shortest path without deadlock and livelock.\n\nIn the Twenty First Century wireless communication being used like essential communication mean for data transfer, video streaming & multimedia applications. To use buoyant application fit for such growing demand for data transfer in smart phones latest technology is being searched. To satisfied such demand; two hop Orthogonal Frequency Division Multiple Access (OFDMA) relay network are being powerfully used in combination with Multicasting which forms a encouraging communication model for many applications along with data transfer. Innovation shows that, Association of relay network and cooperation between them is proper way of optimization. For active use of relay network for retransmission, cross layer optimization technique is processed. Joint incorporation of OFDMA scheduling and network coding mechanism is carried out in intellectual method. The aim of this paper is to know how wireless node automatically selects the channel in presence of network coding also how OFDMA scheduling assigns the network source to it. We then formulated the optimization problem for different channel gain subcarrier and stated the helpful solution to assign sub channels to relay node dynamically with code aware and allocation scheme. This problem is NP-hard we applied a heuristic algorithm to implement it. Comparative rational algorithm uses the combination of scheduling and coding the relay logically their by aggregate the throughput, trustworthiness of the network and decreasing the overhead of the coordinated nodes.\n\nA sincere and ingenious effort has been made towards addressing the localization problem in the modern world by marrying the primary and well established inter-vehicular communication assisted localization (IVCAL) using GPS method to newly devised previous path detection (PPD) technology. The outcome is a new robust methodology combining the two systems standalone benefits of absolute and relative location methods respectively. The proposed system includes GPS, Kalman Filter, IVCAL and PPD as basic building blocks. The system architecture of the proposed method has been presented in a stepwise manner. The pitfalls of earlier localization methods using GPS and then the suggested improvements using PPD had been demonstrated with the help of developed model framework within MATLAB© simulation environment. The results are presented for an assumed localization problem similar to the one reported out in literature, which has been built up so as to challenge and test effectively the basic capabilities of proposed system.\n\nCognitive Radio networks (CRNs) implement dynamic spectrum sensing and management techniques to address the underutilization of limited available spectrum in wireless networks. Both dynamic and opportunistic spectrum sensing techniques are frequently used by the cognitive radios (CRs) to identify the spectral holes in licensed spectrum of the primary users (PUs) and allocate these bands to the unlicensed secondary users (SUs). In this paper we have presented a state-based probabilistic sensing model for primary and secondary user nodes to access the communication channel. Simulations are performed to measure the uniform and cumulative distribution functions the degree of the primary nodes, and channel accessing probabilities for the secondary nodes.\n\nAs technology improves, it is possible to integrate more number of transistors on a single die, which means it is possible to design any complex system on single chip. The networking in such system is very difficult as compared to less complex systems one which uses switching and bus technique. In order to overcome this, a special networking technique is used which is Network on Chip. In this paper, a technique is proposed to develop the platform level design to receive the error-free packets which improves the performance of Network on Chip. By receiving the error free packets, there is no need for the network to check the packet once again for correctness thereby reducing the time taken for the packet to reach from source to destination resulting in an increase in efficiency. This helps in improving the performance since the error packet will be eliminated at the platform level before sending it to the cluster level. The packets are stored into RAM and are classified with the help of packet classifier. The packet classifier will send the packets to respective cluster and node agents at lower hierarchical level. This will improve the performance by avoiding the erroneous packets in network.\n\nMagnetic field effects the charge transport in quantum dot. The grain(quantum dot) I-V characteristics are subject to change due to magnetic field, extent of variation in the Quantum Dot I-V characteristics is function of magnetic field strength, size and type of the Quantum Dot. We present the study of the I-V characteristics of the grain in non-equilibrium and steady state conditions in a magnetic field. We use non-equilibrium green function based technique for the computations.\n\nGreen computing is the environmentally responsible use of computers and related resources. Such practices include the implementation of energy-efficient computing solutions. Multi-core and General Purpose Graphics Processing Units (GPGPUs) computing has become trend of high performance processors and energy efficient computing. Video processing technique like moving object detection which is a computationally intensive task can be made to exploit the multi-core architecture to extract information more efficiently. Implementing moving object detection on GPU using CUDA or other platforms provides greater speedup and scalability in terms of input size. Different types of input videos or videos with different types of effects are tested. The output obtained from the system is compared with the ground-truth to verify the correctness of the system.\n\nFemtocell technology has gradually evolved in the field of wireless technology. Femtocell base stations (FBS) are installed mainly indoor to improve the signal coverage for the user held equipment. Interference management is of prime concern in FBS installation. The work is carried on understanding the Dynamic Assignment of Transmit Power for interference avoidance in co-tier femtocell networks. The drawback of dropping Femtocell User Equipment (FUE) has been identified. Solution to the problem is achieved by introducing handoff mechanism. Several handoff strategies have been categorized under survey. The work proposes handoff strategy, appended with existing Dynamic Assignment of Transmit Power system. The formulation to Dynamic Assignment of Transmit Power-Handoff (DATP-H) improves the throughput of existing system.\n\nVehicle crime has long been a menace to the world with a gamut of felonies which go as far as terrorism. Fortunately, with the advent of Global Positioning System (GPS) technology the tracking of a vehicle has been made remarkably simpler. However, the task of tracking of multiple vehicles and scaling such a system to track an enormous number of vehicles is a rigorous task. The need for similar inexpensive system is quite demanding. In this paper, a cloud based multi-vehicle tracking and locking system is presented that lets the owner of the vehicle to track any vehicle in real time. In the event of a malicious activity such as burglary, the owner can lock the system. The paper also professes a cost-effective system that lets the owner learn the instances of accidents or possible drink driving cases.\n\nInformation assimilation and dissemination is the major task and challenge in the new era of technology. There are many means of information storing. Underwater Videos and images reveal much information when they are analyzed. The proposed method uses Mixture of Gaussian as a basic model to segment the moving object under dynamic condition. The proposed method remove the motion of the underwater algae or plants which exists as background by checking the status of each foreground pixels in each frame and decides whether to be present as output or not in the post processing stage. Finally the output is compared with a validated ground truth.\n\nAirplane cockpit security is a crucial function of flight management. A key aspect of cockpit security is the identification of the persons occupying the pilot and co-pilot seats. Existing biometric techniques have certain drawbacks, and this paper presents a novel method to identify the person sitting in a seat, based on pressure patterns. In this paper, a standard machine learning tool is used to compute specific patterns in the pressure signature; these patterns are compared with existing patterns in a database to identify the correct person.\n\nThis paper deals with Bi-modal Image segmentation with active shape model. This method has been applied to varieties of segmentation problems, where considerable information about the shape is available. The discussion about the implementation on a set of both synthetic and natural bimodal images is presented.\n\nWireless Mobile Adhoc network is an infrastructure less network which consists of equally distributed self configuring mobile nodes. Secured access to these mobile nodes is a major issue, since these devices are most widely used in our day to day life due to their diverse capabilities like online transactions processing. Designing a reliable authentication technique for users of these mobile nodes with minimum delay incurred for the authentication process is the most vital and challenging task, so that only legitimate users can access their personal data and also communicate with the other mobile devices in the network. In this paper we present an approach for authentication of the Mobile users with minimum time delay incurred for authentication process, which is well explained with a scenario of setting up a call session during an emergency, unlike traditional techniques and hence reducing the average delay caused due to setting up a call session after authenticating the user. Performance valuation indicate that this approach achieves a reliable security for nodes with a minimum time overhead.\n\nThis paper describes an experimental set up for generating database of model aircraft images. The database thus obtained is useful in validating image processing algorithms used in aircraft image recognition. The physical models are prepared from the three dimensional (3D) Computer Aided Design (CAD) models of aircraft by using Rapid Prototyping (RP) Fused Deposition Modeling (FDM) machine. Acrylonitrile Butadiene Styrene (ABS) plastic material is used for the physical models. Five types of models are printed for the sample database, viz., Mirage 2000, P51 Mustang, F-16 Fighting Falcon, G1-Fokker and MIG25-F. The experimental set up for generating database of model aircraft images consists of a 89C2051 microcontroller based 3D-movement controlling mechanism for capturing images of the aircraft models. Three stepper motors are used in the system to simulate yaw, pitch and roll movements of aircraft model. These motors can be rotated clockwise or anticlockwise independently to any desired angular position. A program has been developed in the assembly language of AT89C2051 microcontroller using Keil software to activate the stepper motors. A digital camera is mounted at predefined location to capture the 3D maneuvering of the aircraft models. Finally, validation of both phase correlation and colour based image detection algorithms are made using this experimental set up.\n\nAn effective mechanism to improve the performance of a computing device is to include multiple processing units on a single integrated die. To exploit the performance gain of this mechanism, developing parallel programs is necessary. However, some existing programs are developed for serial execution and manual redesign of all such programs is tedious. Hence, automatic parallelization of existing serial programs is advantageous. One of the methods to execute programs in parallel is to make use of parallel computing platforms. With myriad number of parallel computing platforms, abstracting them from the developer is propitious. In this paper we propose an algorithm which is capable of detecting potential `for' loops in C code that can be parallelized using OpenMP platform. The algorithm proposed ensures the correctness of the program. It performs the required parallelization without post execution analysis, which avoids both execution of code and monitoring of resources accessed by the program.\n\nCloud data center have to accommodate many users with isolated and independent networks in the distributed environment to support multi-tenant network and integrity. User applications are stored separately in virtual network. To support huge network traffic data center networks require software to change physically connected devices into virtual networks. Software defined networking is a new paradigm allows networks easily programmable that helps in controlling and managing virtual network devices. Flow decisions are made based upon real-time analysis of network consumption statistics with software defined networking. Managing these virtual networks is the real challenge for the network administrators. In this paper, we propose a novel network management approach between the controller and virtual switches and provide QoS of virtual LAN in the distributed cloud environment. This approach provides a protocol to deploy in Cloud Data center network environments using OpenFlow architecture in switches. Our approach delivers two different features, dynamic network configuration and Virtual management protocol between controller and open vswitch. This technique is very useful for cloud network administrators to provide better network services in cloud data center to multi-tenant users quickly.\n\nWireless sensor network (WSN) is considered as a most trusted technology because of its wide range of applications in various fields like healthcare, industry, military, agriculture etc. WSN consists of seveal sensor nodes with each having the capacity to sense the data, process the sensed data and communicate the processed data. Usually sensor nodes are densely, randomly deployed at the region of interest. This kind of deployment leads to the generation of enormous ammount of redundant sensor data. Routing of such redundant data consumes more energy and saturates the network resources. Hence Data fussion technique is used to reduce the redundant transmissions in the network by fussing the redundant data packets so that network lifetime is enhanced. There are different data fussion techniques which perform data fussion in a single level or in two levels. In this paper we are proposing a multilevel hierarchical data aggregation technique which handles the redundant transmissions in an efficient manner.\n\nRapid increase in the demand for scientific, business and web applications has led to large scale computation. Cloud computing has emerged as a scalable, reliable, affordable, flexible source for such type of applications. The need to mange such applications require proper load balancing and scheduling techniques. These techniques are different from the algorithms used for distributed computing. This is mainly due to the high scalability and high availability in the cloud environment. The proposed algorithm based on load balancing is presented in this paper. The principle of time scheduling and priority is utilized. The approach implements division of time into multiple slices and allocating each process to particular time interval based on priority. The processor serves the user request within the allotted time slot. At the end of the time slice, the next queued user request is ready for execution. The user exits from the queue upon completion of user request, otherwise user waits for its next slot. The increase in waiting time increases the time slot the user requests gets in the virtual machine. This reduces the overhead of context switching.\n\nThe emerging technological demands of users call for expanding service model which avoids problem of purchasing and maintaining IT infrastructure and supports for computation-intensive services. This has directed to the development of a new computing model termed Cloud Computing. In cloud computing, the computing resources are distributed in various data centers worldwide and these resources are offered to the customers on demand on a pay as usage basis. Currently, due to the increased usage of cloud, there is a tremendous increase in workload. The uneven distribution of load among the servers results in server overloading and may lead to the server crash. This affects the performance. Cloud computing service providers can attract the customers and maximize their profit by providing Quality of Service (QoS). Providing both QoS and load balancing among the servers are the most challenging research issues. Hence, in this paper, a framework is designed to offer both QoS and balancing the load among the servers in cloud. This paper proposes a two stage scheduling algorithm. The servers with different processing power are grouped into different clusters. In the first stage, Service Level Agreement (SLA) based scheduling algorithm determines the priority of the tasks and assigns the tasks to the respective cluster. In the second stage, the Idle-Server Monitoring algorithm balances the load among the servers within each cluster. The proposed algorithm has used the response time as a QoS parameter and is implemented using CloudSim simulator. Experimental results shows that our algorithm provides better response time, waiting time, effective resource utilization and balancing load among the servers as compared to other existing algorithms.\n\nNatural Language Processing (NLP) involves many phases of which the significant one is Word-sense disambiguation (WSD). WSD includes the techniques of identifying a suitable meaning of words and sentences in a particular context by applying various computational procedures. WSD is an Artificial Intelligence problem that needs resolution for ambiguity of words. WSD is essential for many NLP applications like Machine Translation, Information Retrieval, Information Extraction and for many others. The WSD techniques are mainly categorized into knowledge-based approaches, Machine Learning based approaches and hybrid approaches. The assessment of WSD systems is discussed in this study and it includes comparisons of different WSD approaches in the context of Indian languages.\n\nVehicular Adhoc NETworks (VANET), a special category of Mobile Adhoc Networks (MANET) are networks formed by vehicles which help the vehicles communicate with one another. The challenging mode of communication in VANETs is the video mode which can be used to give faster and clear information to the end users in vehicles. The transmission of video streams is prevalently seen in the applications of VANETs like the infotainment applications and safety messages dissemination. As video streams are normally immense and expected to maintain strict deadlines, the parameters delay and jitter play a vital role in maintaining the quality of streaming. One of the popular techniques used in reducing the above said parameters is Network Coding. The mobility of the vehicles influence the network characteristics to a large extend. This paper analyses the delay and jitter parameters in VANETs by simulating the network coded video streams being transmitted between the vehicles which are not in the range of each other where the vehicles show varying mobility scenarios.\n\nDegraded character recognition is one of the most challenging topic in the field of Kannada character recognition. The degraded characters which are broken and deformed will have missing features and will be difficult for any recognition method. Rebuilding the degraded character is very important for better recognition. This paper proposes a novel method to rebuild the broken characters. These characters are thinned and the endpoints of the lines are obtained. The line segments are effectively rebuilt so as to preserve the degraded character. Experimental results on this method are presented to establish its efficiency.\n\nEnergy conservation in the field of Wireless Sensor Networks (WSN) is of a great concern to the modern day researchers which has lead to numerous research studies. However, irrespective of the various energy efficient routing protocol for WSN, the energy of a sensor node gets exhausted soon which in turn might lead to disastrous problems. To address this problem of energy depletion, a technique to intensify the sensor node energy is proposed. A centralized energy accumulator node will transmit the energy to the nearest nodes through intermediate hops using Energy Reconciling Medium Access Protocol (ERMAC) via radio frequency. Considering the unjustness, the protocol can efficiently adapt itself to power up the lifetime of sensor networks. The results through simulation show the stability of the wireless sensor network with ERMAC and the energy harvesting rate of a sensor node.\n\nThe Cognitive Radio is the most emerging research area in communication and to bringing this technology into implementation the first and foremost challenge for researchers is the estimation of the spectral holes of a wide band spectrum. So the spectral estimation techniques are most going to play a vital role in Cognitive Radio. In this paper we are proposing a novel non parametric spectral estimation technique for better spectral efficiency of the signal with minimized power errors in the estimation. ERLS Technique is introduced for better result of power signal spectral estimation. The ERLS Technique is the combination of wavelet algorithm and artificial neural network (ANN). The wavelet algorithm is used to extract the frequency components of the power signal. Then, using the neural network, the power error signals is determined. So, the complexity and computational time of spectral estimation are reduced.\n\nTransmission Control Protocol is commonly used communication protocol. Congestion Control is one of the hardest problems in robust networked systems. Throughput is a measured performance metric in all communication systems. Many TCP variants play an important role in controlling the congestion. TCP New-Reno works comparatively better when congestion is more in the network, however the data rate will be always constant. HSTCP works optimal with scaled data rate. A mode of switching from HSTCP to New-Reno and also from New-Reno to HSTCP depending upon the number of users in the network called as Switching TCP (STCP). Switching TCP variant avoids the congestion and also increases the data rate in the ad-hoc networked systems.\n\nSince the wireless sensor network is upgrading, the need for high quality and energy efficient way of using sensors are becoming more important. This paper works with the inventive wireless sensor network based on the selection of better Cluster Head among the other sensor nodes. The communication happens in the network through the Cluster Head, so the battery power of Cluster Head is required to be high. This can be achieved by dynamic elimination of far away sensors. At the same time addition of new sensors for servicing is also possible.\n\nIn this paper, we present a face part detection such as eyes, nose and mouth using shared memory programming concept of OpenMP. The task parallelism is exhibited in multiple threads of a core where each thread is assigned with different task that are independent. The existing algorithm haar classifiers has been used for face part detection where initially the face is detected, localized and then the parts are detected, localized. This proposed idea of face part detection resulted in depletion in time compared to sequential execution and increase in speedup.\n\nThe technical adoption of optical network is gaining a fast pace to accomplish the internet connectivity in a larger geographic region. The system has proved to be one with effective utilization as photons are used to carry a massive bundle of data packets, something which cannot be done in a conventional networking system which exists today. Significant amount of research attempt have been done to enhance the performance of optical network, however that there are still further scope for improvement. Hence, we propose a model that targets to enhance the quality of the signal in optical network during for peak traffic condition. The formulation of the model is solely based on considering the investigation of channel power and the signal noise as the prime attributes of signal quality in optical network. Incorporated with potential features of ROADM (Reconfigurable Optical Add-Drop Multiplexer), the proposed system has been simulated in Matlab under multi-channel condition and power constraint to see the proposed model has outperformed the existing research benchmark.\n\nThis paper presents the 3 rd Generation Partnership Project-Long Term Evolution (3GPP-LTE) [10] HandOver (HO) administration to minimize target Femto Access Points (FAPs) list amid macro-to-femto and femto-to-femto handover. HandOver is the procedure that transfer an ongoing call from one cell to another as the users move through the coverage area of cellular system. The femtocell covers a little scope zone (15 meters to 30 meters), and subsequently, HandOver will rely on car (mobile femtocell users) or User Equipment (UE) speed to handover between macrocell-to-femtocell and femtocell-to-femtocell. During handover procedure, femto will decide cell selection using the signal strength and capacity of the target femtocell. Femto-to-Macro HO has two main failure scenario's i.e. Too Late HO (TLH) and Too Early HO (TEH). TLH results in the Failure in Radio Link (FRL); TEH affects the Ping-Pong or FRL. In this paper, we present the handover management scheme for cognitive femtocell network to reduce target FAPs to avoid overloaded femtocells and also reduce the number of handovers.\n\nAutomation testing is efficient and less time consuming as compared to manual testing. Automation testing does not require human intervention to execute the test steps. Test automation tools remarkably reduce human efforts. Automation testing reduces errors. It eases the procedure and is faster than manual testing. Manual testing results in delay of the release. It affects the financial growth of the company. The limitations of manual testing are overcome using automation testing that can cover wider range of testing possibilities. Automation Procedure (AP) is a process executed before starting automation testing to improve the speed and efficiency of automation testing. In this paper, a unique method of automation procedure is defined which is mainly used for upgrading the remote network devices by software images, called as software builds. AP is developed using Python. The AP described provides an efficient method for Automation testing. As AP requires minimal input and less processing time, it reduces the time consumed and human efforts involved over manual testing.\n\nThe Hierarchical Chinese postman problem is a special type of Chinese postman problem. The aim is to find a shortest tour that traverses each edge of a given graph at least once. The constraint is that the arcs are partitioned into classes and a precedence relation orders the classes according to priority. Different forms of the HCPP are applied in real life applications such as snow plowing, winter gritting and street sweeping. The problem is solvable in polynomial time if the ordering relation is linear and each class is connected. Dror et al. (1987) presented an algorithm which provides time complexity of O (kn 5 ). CPP which is lower bound for HCPP. We give alternate approach by using Kruskal's method to reduce number of edges in graph which is having time complexity of O (k 2 n 2 ), where k is number of layers in graph and n is number of nodes in graph. It is found that the suggested kruskal-based HCPP-solution gives average 21.64% improvement compare to simple HCPP and we get average 13.35% improvement over CPP when number of hierarchy is less than 3 and numbers of edges in graph are less than 10.\n\nDynamic Partial Reconfiguration (DPR) is an efficient technique in terms of power, resources and performance in order to achieve varying user requirements. In this paper we have identified various modes of operation of electronic stethoscope and also studied several existing implementations. We propose a Dynamic Reconfigurable Design for the multi-mode electronic stethoscope with five operational modes. Implementations were performed on Virtex-5 XC5VLX110T FF1136 device. Comparative results were presented between the proposed method and the normal FPGA implementation method. Also the reconfiguration timings were analyzed and the results were compared with the ideal reconfiguration time of Virtex-5. The proposed Dynamic Reconfigurable design is able to achieve a reconfiguration time of 28.86 ms with the reconfiguration speed of 3.46 MB/sec.\n\nThe area of underwater acoustic sensor networks (UWASN) is developing expeditiously as it shows a major aspect in various military and civilian practices, such as avoidance of disaster, diplomatic vigilance, seaward analysis, environmental monitoring, oceanographic data collection and mine reconnaissance. There are numerous challenges in UWASN posed by underwater acoustic propagation channel, which includes gradual propagation of acoustic waves, bounded bandwidth, immense and irregular propagation delay, ambient noise and transmission loss. This paper provides the analysis on the working of an acoustic channel in conditions of sound speed and transmission loss using MATLab simulation. The parameters analyzed are absorption coefficient, propagation delay and sound speed at various depth and transmission loss.\n\nSQL Injection (SQLI) is a quotidian phenomenon in the field of network security. It is a potent and effective way of intruding into secured databases thereby jeopardizing the confidentiality, integrity and availability of information in them. SQL Injection works by inserting malicious queries into legal queries thereby rendering it increasingly arduous for most detection systems to be able to discern its occurrence. Hence, the need of the hour is to build a coherent and a smart SQL Injection detection system to make web applications safer and thus, more reliable. Unlike a great majority of current detection tools and systems that are deployed at a region between the web server and the database server, the proposed system is deployed between client and the web server, thereby shielding the web server from the inimical impacts of the attack. This approach is nascent and efficient in terms of detection, ranking and notification of the attack designed using pattern matching algorithm based on the concept of hashing.\n\nSecurity is the main concern in today's wireless network environment. However, cipher algorithms consume a lot of resources to provide the required confidentiality. Ad-Hoc wireless networks are one area where the devices are extremely resource constrained. Therefore computationally simple yet cryptographically strong cipher algorithms are required for such kind of networks. In this paper a light weight Quasigroup based stream cipher is proposed and implemented on a Virtex-6 FPGA. It is also subjected to the NIST-STS test suite. Its performance is evaluated in MANETs using Glomosim simulator.\n\nThe vision of Internet of Things (IoT) is to enable devices to collaborate with each other on the Internet. Multiple devices collaborating with each other have opened up various opportunities in multitude of areas. It has presented unique set of challenges in scaling the Internet, techniques for identification of the devices, power efficient algorithms and communication protocols. Always connected devices have access to private sensitive information and any breach in them is a huge security risk. The IoT environment is composed of the hardware, software and middleware components making it a complex system to manage and secure. The objective of this paper is to present the challenges in IoT related to security, its challenges and recent developments through a comprehensive review of the literature.\n\nCrop yield prediction is important as it can support decision makers in agriculture sector. It also assists in identifying the relevance of attributes which significantly affect the crop yield. Wheat is one of the widely grown crops around the world. Its accurate prediction can solve various problems related to wheat farming. This work analyses how yield of a particular crop is determined by few attributes. In this paper, the models of Fuzzy logic (FL), Adaptive Neuro Fuzzy Inference System (ANFIS) and Multiple Linear Regression (MLR) are used for predicting the yield of wheat by considering biomass, extractable soil water (esw), Radiation and rain as input parameters. The outcome of the prediction models will assist agriculture agencies in providing farmers with valuable information as to which factors contribute to high wheat yield. We compare all these models based on RMSE values. Results show that the ANFIS model performs better than MLR and FL models with a lower RMSE value.\n\nThis paper illustrates the application of a Hybrid Quantum Inspired Evolutionary Algorithm (HQIEA) in evolving a variety of Quantum equivalents of classical circuits. Taking the matrix corresponding to an oracle as input, this HIQEA designs classical circuits using quantum gates. A library consisting of single, two and three qubit Quantum gates and the desired circuit matrix were given as input and algorithm was able to successfully design half adder, full adder and binary-gray conversion circuits apart from circuits for two, three and four qubit Boolean functions, using Quantum gates. The circuits obtained compare favorably with earlier attempts in terms of number of gates, ancillary inputs and garbage outputs required for constructing these circuits and the time taken to evolve them.\n\nMicroarrays store gene expression data of each cell; thereby microarray contains thousands of features. Each row represents gene samples and each column represents conditions. In any classification task, selection of irrelevant or redundant features greatly reduces the performance of classifier. Therefore, selection of optimal number of significant features is a major challenge for any classification problem. Filter and wrapper approaches are mostly used for feature subset selection. Filters are computationally faster but wrapper approach is more efficient in terms of classification accuracy. This paper proposes a hybrid approach combining the filters and wrappers is proposed which takes the features from both the filters and the wrappers. In the initial step, a feature subset is selected using filters and the feature subset size is further reduced using the wrapper approach. The proposed method is tested on Colon Tumor, B-Cell Lymphoma (DLBCL) and Leukemia datasets. Simulation results show that the hybrid method achieved higher accuracy with a smaller feature subset when compared to the existing methods.\n\nThis research work carried on GI/Geo/1/K queue with N threshold policy. The server resumes it service when the number of consumers in the queue reaches a pre-specified value N. This article describes modeling and detail analysis on the above mentioned model. The arbitrary and prearrival epochs probabilities are being obtained for the steady state distribution. The distribution of the waiting time and various performance measures are studied. At the end, the computational results are presented. This model can be applied on various networking/industrial automation system.\n\nIn today's HPC world, numerous applications executed with high speed. The multi core architecture has provided a wide scope of exploration towards any kind of high-end applications. The paper gives an analogy on ability to handle the high-end input triggers by managing the core utilities at the lower end of the computer. The optimized way of memory block utilization, debugging proliferation and data management at configured input level discussed in the paper. The Data Proliferation framework model named as COMCO (Component for Compiler Optimization) model and it elaborates on how to handle configured inputs at the OS level. The paper gives an exploration of the configured inputs safely at the multi thread level using the COMCO (Component for Compiler Optimization) strategy.\n\nIn cognitive radio networks (CRNs), the unlicensed nodes opportunistically access the unutilized spectrum that is owned by licensed nodes. The article presents a fuzzy logic based spectrum handoff and assignment approach that enhances the channel utilization and avoids frequent channel switching. The approach considers interference as well as bit error and signal strength in order to find quality channel. Based on generated fuzzy patterns of channel quality, the neural network is trained in order to estimate the channel gain. It is used to select the efficient spectrum in heterogeneous environment.\n\nSoftware testing is a process which is used to examine the correctness and quality of a computer software. Software testing is important to point out the defects and errors that are made during software development and to ensure that the developed software works fine in the real environment with different operating systems, devices, browsers and concurrent users. Making reliable software applications is becoming increasingly important and thus software testing is indispensable. One of the main challenges in software testing is that there are many quality concerns that cut across different modules in the software and thus testing requires modifying the source code of various modules. In this paper, we propose the use of Aspect Oriented Programming (AOP) for the purpose of creeping inside the program's modules without modifying their source code and test components where we suspect bugs. Aspects in AOP can capture one or more execution points in the program using pointcuts and further advices can be written to insert relevant code at such execution points for the purpose of testing. We examine the suitability of using aspects for writing testing codes and perform various types of software testing.\n\nRelay technologies have been studied and considered in the standardization process of the next generation of MIMO system, such as LTE 8, Advance LTE 10, IEEE 802.16e, IEEE 802.16m. Presently two wireless technologies, WiMAX and LTE both based on IEEE standard, are two rival technologies, nevertheless, are technically very similar but deployment is differ. This paper introduces and compares features of two advance technologies in physical layer, and also gives performance analysis of different modulation schemes (BPSK, QPSK, and 16-QAM) in WiMAX & LTE technologies.\n\nWeb Services are emerging technologies enabling machine-machine communication and services reuse over Web. They have an innovative mechanism to render services over diversified environments. Semantic web services are reusable, self-contained software components, to be used independently to fulfil needs or combined with other web services for carrying out complex aggregation through web services composition. There are many factors, due to which methods used for web services composition vary. Service categorization facilitates service retrieval using manual browsing service repositories or through the mechanism of automatic discovery. Classification taxonomies are huge, comprising 1000s categories, at multiple hierarchical levels. Multi-Layer Perceptron Neural Network (MLPNN) is used for classification problems. In this work, Multi-Layer Perceptron optimized with Tabu search (MLP-TS) for learning is proposed. Experimental results demonstrate that the proposed MLP-TS outperforms Multi-Layer Perceptron-Levenberg-Marquardt (MLP-LM) and Multi-Layer Perceptron Back Propagation (MLP-BPP) for web service classification.\n\nThis paper presents HDL implementation of Kalman Filter. Kalman Filter is a mathematical tool, which uses sequence of noisy measurement taken over time to predict unknown state vector parameter. In this paper implementation in HDL has been done using new method that is chebyshev inversion method. The approach of new method is for reducing hardware and complexity. Kalman Filter has very complex matrix calculation and matrix inversion. To perform matrix inversion in HDL(Hardware Description Language) using less hardware is difficult. Chebyshev method is less complex so hardware can reduced for calculation of Kalman Filter equations Here simulation is done in FPGA(Field Programmable Gate Array) Vertex-6.\n\nThe existing System on Chip (SoC) design will soon become a critical bottle neck in chip performance with its inability to scale its communication network effectively with decreasing feature sizes and increasing number of transistors. The Network on Chip (NoC) has been recognized as the next evolutionary step to tackle these issues by using an intelligent and common communication network between all the different components within chip. In this paper we propose a new routing algorithm that uses a combination of a fully adaptive and partial adaptive routing algorithm called Adaptive Look Ahead algorithm. The algorithm decides next two hops within one node to allow quick packet transfer in next node, hence the algorithm only periodically calculates the packets route along the minimal path. Experimental results show that our proposed algorithm has lower latency and higher throughput than existing benchmarks.\n\nIn advanced intelligent transport systems, detection of the vehicles has become very popular in the traffic area and also to identify the density of the vehicles in that particular area. As per the survey background subtraction is identified as one of the best approaches in identifying the vehicles for static camera. An improvised background subtraction model is adopted, wherein it works for real time tracking and also solves the problems of shadow detection. In background subtraction each pixel is updated with update equations. A component labeling technique is introduced after background subtraction to label the different objects so as to bifurcate between the two objects and each region is labelled with the different label values. Detections of the moving vehicles are identified and the density of vehicles travelling in the sight of the camera is determined.\n\nBit Parallel String Matching algorithms are the most efficient and latest class of String Matching Algorithms. These algorithms use the intrinsic property of computer known as Bit Parallelism for performing bit operations in parallel with-in single computer word. BNDM, TNDM, BNDMq are the popular single pattern bit parallel string matching algorithms whereas Shift-OR and Shift-OR with Q-grams algorithms are popular in multiple pattern string matching. All these algorithms are restricted by the limitation of pattern size. These all algorithms are not working on patterns whose sizes are longer than computer word. In this paper, we proposed the generic method named WSR (Word Size Removal) for bit parallel string matching. With the inclusion of WSR these bit parallel string matching algorithms can work on longer than computer word size patterns. This paper presented WSR method and all modified algorithms.\n\nThis paper proposes a new index and method to find strings approximately in spatial databases. Specifically, the task of candidate generation is as follows. Given a location name with wrong spelling, the system finds location in OSM dataset which are most similar to that location name which are misspelled. An approximate solution is proposed using log linear model which is defined as a conditional probability distribution of a corrected word and a rule set for the correction conditioned on wrong location name. An Aho-corasic tree which is used for storing and applying correction rules referred to as rule index and an Aho-Corasic algorithm which is efficient and gives guarantee to find top k candidates. Experiment on large real OSM dataset demonstrates the accuracy of proposed method upon existing methods.\n\nMobile location-based services and GPS enabled devices has gained increasing popularity by using spatial data outsourcing over the past few years. There is increasing trends in the industry to store data on cloud to gain the benefit of its flexible infrastructure and affordable storage cost, which support location-based applications. This article talks about outsourced spatial databases (OSDB) model and a competent method EX-VN Auth, which provide accurate and complete results. EX-VN Auth used to verify the result set as well as allows a customer to offer the approach called neighborhood information derived from the underlying spatial dataset based on Voronoi diagram. Different methods of finding nearest locations are adopted like Voronoi diagram Spatial dataset underlying and basic spatial query type, as like k Nearest neighbor and range queries, also very superior query such as total closest neighbor, reverse k nearest neighbor, and spatial horizons. EX-VN Auth had been tested as real world data sets by means of mobile gadgets (Android OS smart phone) as client. The results of Merkle hash trees had been compared, VNAuth with EX-VN Auth and experiments produce significantly minor substantiation items and more data processing capability, with lesser-search criteria.\n\nSoftware that gathers information regarding the computer's use secretly and conveys that information to a third party is Spyware. This paper proposes a click based Graphical CAPTCHA to overcome the spyware attacks. In case of traditional Text-Based CAPTCHA's user normally enters disorder strings to form a CAPTCHA, the same is stored in the key loggers where spywares can decode it easily. To overcome this, Click-Based Graphical CAPTCHA uses a unique way of verification where user clicks on a sequence of images to form a CAPTCHA, and that sequence is stored in pixels with a random predefined order. This paper also analyzes the proposed scheme in terms of usability, security and performance.\n\nIn BioWorld, a medical intelligent tutoring system, novice physicians are tasked with diagnosing virtual patient cases. Although we are often interested in considering whether learners diagnosed the case correctly or not, we cannot discount the actions that learners take to arrive at a final diagnosis. Thus, the consideration of the sequence of actions becomes important. In this preliminary study, we propose a line of research to investigate learner actions involved in diagnosing virtual patient cases using Hidden Markov Models.\n\nData clustering has been playing important roles in many areas like pattern recognition, image segmentation, social networks and database anonymisation. Since most of the data available in real life situation are imprecise by nature, many imprecision based data clustering algorithms are found in literature using individual imprecise models as well as their hybrids. It was observed by Krishnapuram and Keller that the possibilistic approach to the basic clustering algorithms is more efficient as the drawbacks of the basic algorithms are removed. This approach was used to develop the possibilistic versions of fuzzy, rough and rough fuzzy C-Means algorithms to develop their corresponding possibilistic versions. In this paper, we extend these algorithms further by proposing a possibilistic rough intuitionistic fuzzy C-Means algorithm (PRIFCM) and compare its efficiency with other possibilistic algorithms and the RIFCM. Experimental analysis is carried out by taking both numeric as well as the image data. Also, DB and the D indices are used for the comparison which establishes the superiority of PRIFCM.\n\nResidue Number System (RNS) is widely used in various applications such as design of cryptoprocessor, digital filters etc. For better performance of these RNS systems conversion module should be fast enough. This paper presents a new reverse converter architecture for a novel five moduli set Residue Number System (RNS) {2 n - 1, 2 n , 2 n + 1, 2 n + 1 - 1, 2 n - 1 - 1} for even values of n. It exploits the special properties of the numbers of the form 2 n ± 1, and extends the dynamic range of the present triple moduli {2 n - 1, 2 n , 2 n + 1} based systems. The proposed moduli set has a dynamic range that can represent upto 5n - 1 bit numbers while keeping the moduli small enough and the converter efficient considering computation required. In our new Five moduli set Reverse Converter design we use both the Chinese Remainder Theorem (CRT) and Mixed Radix Conversion (MRC) techniques.\n\nCirculating Tumor Cells (CTCs) in Peripheral Blood (PB) testing is measured as significant investigative and promising microarray technology for breast cancer examination. Few numbers of the work have been proposed in earlier for the role of CTCs detection in breast cancer; but still the development of novel method for identification of CTC becomes difficult because of hundreds and thousands of indicative genes is presented. The main intention of the work is to the identification of CTC in PB during Breast Cancer (BC) regarding to Metastatic (MS), Non-Metastatic (NMS) and hybrid MS and NMS. The proposed method is not only the identification of CTC in BC, in addition it solves gene selection by proposing hybrid fuzzy online sequential Particle Swarm Genetic (PSG) kernel extreme learning machine finally named as (FOP-KELM) classification. The proposed FOP-KELM method calculates the mean values for each gene features and it is compared objective function of KELM to select and remove unimportant gene features. In order to reduce the fuzzy membership assumption value in ELM, it is optimized using PSG algorithm. The impact of selected features from FOP-KELM has been investigated using clustering method. To perform classification task for selected gene features, a novel Hierarchical Artificial Bee clustering algorithm (HABCA) is proposed. It capably distinguishes the CTC through the separation of tumor samples into a hierarchical tree structure in a top-down manner, where the distance between two gene tumor samples is determined by using ABC. Clustering results are classified into MS, NMS, MS and NMS.\n\nIn this era of technology with an increasing usage of Internet, data security has become a major issue. Various cryptographic hash function such as MD4, MD5, SHA-1, SHA-2 has been defined to provide data security. In this paper we proposed a new algorithm, TL-SMD (Two Layered-Secure Message Digest) for building a secure hash function, which can provide two level processing security. For the construction of this algorithm, various techniques have been used that includes block cipher technique, modified combination of Merkle-Damgard construction and fast wide pipe construction. For computing the hash value from the input block, combination of cipher block chaining (CBC) mode and electronic codebook (ECB) mode with some modification is used.\n\nDigital transactions have permeated almost every sphere of activity in today's world. This increase in digital transactions has introduced additional and stringent requirements with regard to security and privacy. People reveal lots of personal information by compromising their private credentials during digital interactions. It is imperative that in addition to security of on-line transactions, user credentials must also be safeguarded. This has necessitated the requirement for a rigorous and foolproof credential system with provision for anonymous and revocable identity management system. Biometric system based identity management systems offer advantage over conventional knowledge and possession based systems. Considerable research has been undertaken in the past to identify newer and reliable biometrics for more efficient and secure identity management. Fusion of multiple biometrics to achieve better results is also an area of active research. However, making biometric credential systems revocable and anonymous without sacrificing efficiency and efficacy of detection, still remains a challenge. This survey paper makes an attempt to give an insight into the approaches that have been made in the direction of multimodal biometric fusion and into the various options that have been explored to make biometric authentication systems revocable and anonymous.\n\nA database is a vast collection of data which helps us to collect, retrieve, organize and manage the data in an efficient and effective manner. Databases are critical assets. They store client details, financial information, personal files, company secrets and other data necessary for business. Today people are depending more on the corporate data for decision making, management of customer service and supply chain management etc. Any loss, corrupted data or unavailability of data may seriously affect its performance. The database security should provide protected access to the contents of a database and should preserve the integrity, availability, consistency, and quality of the data This paper describes the architecture based on placing the Elliptical curve cryptography module inside database management software (DBMS), just above the database cache. Using this method only selected part of the database can be encrypted instead of the whole database. This architecture allows us to achieve very strong data security using ECC and increase performance using cache.\n\nThe paper proposes a novel method to analyze the correctness of frequency correction loop in a 2G system, so that the mobile station accurately synchronizes to the base station. The setup contains an automatic frequency control (AFC) module to track and control the frequency/timing offset of the crystal. The main functionality of the AFC module is to estimate the optimum frequency correction value and minimize the frequency error of the system. It also contains a long term learning (LTL) module whose functionality is to derive an estimate for the crystal behavior due to temperature and aging and maintain a learning database containing the frequency correction values for the crystal in the operating temperature range. A separate module is used to correct advance/forward time drift on slave SIM, when oscillator is locked to the Master SIM. Various COST 207 GSM/EDGE channel models for mobile radio communication are implemented to test the frequency correction loop.\n\nCurrently face recognition has reached a certain degree of maturity when operating under constrained environments. When it comes to real time situations, the system degrades sharply in handling variations like illumination, occlusions, skin tone, cosmetics, image misalignment, age, pose, etc., inherent in the face images acquired. Hence understanding and eliminating the effects of each of these factors is crucial to any face recognition system. This paper deals with studying the effect of variances in the Eye Blink Strengths (EBS) on a face image undergoing face recognition, thereby testing the efficiency of face recognition algorithm. The study makes exclusive usage of Brain Computer Interface (BCI) technology to detect eye blinks and to measure their corresponding EBS values using Electroencephalograph (EEG) device. The face recognition algorithm under test was the amalgamation of Principal Component Analysis (PCA), Local Binary Pattern (LBP) based feature extraction and Support Vector Machine (SVM) based classification. EBS is assessed using an inexpensive, portable, non-invasive EEG device. The efficiency of the face recognition algorithm to withstand the eye blinks with varying degree of EBS values for the given face images was determined. It was found that the proposed methodology of test case generation can be effectively be used to evaluate various other face recognition algorithms against varying eye blinks.\n\nThe proliferation of data mining techniques is common across various corporate functions in an organization to discover deeper insights for making better decisions. One such opportunity emerges in the procurement function to streamline the process of procuring indirect materials. This paper proposes a two-step approach 1) adaption of association rule mining to derive the associated materials and 2) identification of right set of supplier(s) for the associated materials based on supplier selection methodology - Data Envelopment Analysis (DEA). The two step approach is used in the purchase requisition process, as a recommendation engine to assist the requester (user who request for materials) with a list of associated materials that can be requested together and also recommend the right supplier(s) for the associated materials. This significantly reduces the number of purchase requests (PR), and thus reduces the man hours in the procure-to-pay cycle and optimizes the supplier base. This is implemented on a sample dataset and a case study is provided for illustration.\n\nA highly accelerated growth of e-market has lead to a well flourished online auctions scenario. Along with the attraction of numerous users world-wide, the online auctions have also allured in multiple frauds, periodically changing in nature and strategy to accustom to the proposed fraud detection and prevention approaches. As per the Internet Crime Complaint Center report 2013, auction fraud is enlisted as the topmost fraud accounting for drastic monetary losses. Amongst the online auctions frauds, shill bidding seems to be the most prominent fraud. In this paper, we present a variable bid fee methodology as a prevention technique for shill bidders. A bidder is charged for each of his bid based on the amount he bids. The winner of an auction wins back the charges he paid as bid fee; he gains an additional benefit to recover the bid fee he paid for the auctions he earlier lost in. This maintains the competitive spirit of an auction. On the contrary, the inherent nature of a shill bidder of frequently bidding in an auction and never winning one, will cause him perpetual monetary losses. We proposed this methodology based on the idea that the risk of losing money will reduce the tendency to exhibit shill behavior.\n\nWireless sensor network (WSN) is the vast area of research in the field of networking. A number of challenges are faced by such type of battery operated networks. In WSN, the sensors are attached to the hardware motes devices. These sensors are responsible to sense the certain events happening in the near surrounding. This sensed data is communicated with the radio/antenna attached to the sensor nodes via wireless medium. There are basically four layers discussed in WSN, namely, physical layer, MAC layer, network layer and application layer. In our paper, we discussed some recent congestion control techniques of 2013, 2014 and 2015 also.\n\nThreat of security issues in Information Science has now become an important subject of discussion amongst the concerned users. E-Commerce is one of the parts of Information Science framework and its uses are gradually becoming popular. However now-a-days, ironically, these users are gradually found to be bit reluctant on pain of threats of security and privacy issues. Needless to say, E-Commerce business has opened a new era in banking industry too. But unfortunately the banking business through E-Commerce is covered with risks for these issues. Thus if these threats of privacy and security are not eliminated, users will not have trust and users will not visit or shop at a site and the sites will also not be able to function properly. These two issues i.e. security and privacy are required to be looked into through social, organizational, technical and economic perspectives. In this paper attempts are being taken to discuss with overview of security and privacy issues in E-Commerce transactions. We shall also discuss in particular different steps required to be taken before online shopping and also shall discuss the purpose of security and privacy in E-Commerce and after discussion we shall provide a guideline to be adopted to mitigate risks and vulnerabilities while an user is involved in E-Commerce transaction.\n\nDiabetes mellitus (DM) is reaching possibly epidemic proportions in India. The degree of disease and destruction due to diabetes and its potential complications are enormous, and originated a significant health care burden on both households and society. The concerning factor is that diabetes is now being proven to be linked with a number of complications and to be occurring at a comparatively younger age in the country. In India, the migration of people from rural to urban areas and corresponding modification in lifestyle are all moving the degree of diabetes. Deficiency of knowledge about diabetes causes untimely death among the population at large. Therefore, acquiring a proficiency that should spread awareness about diabetes may affect the people in India. In this work, a mobile/android application based solution to overcome the deficiency of awareness about diabetes has been shown. The application uses novel machine learning techniques to predict diabetes levels for the users. At the same time, the system also provides knowledge about diabetes and some suggestions on the disease. A comparative analysis of four machine learning (ML) algorithms were performed. The Decision Tree (DT) classifier outperforms amongst the 4 ML algorithms. Hence, DT classifier is used to design the machinery for the mobile application for diabetes prediction using real world dataset collected from a reputed hospital in the Chhattisgarh state of India.\n\nIn the era of big data, the applications generating tremendous amount of data are becoming the main focus of attention as the wide increment of data generation and storage that has taken place in the last few years. This scenario is challenging for data mining techniques which are not arrogated to the new space and time requirements. In many of the real world applications, classification of imbalanced data-sets is the point of attraction. Most of the classification methods focused on two-class imbalanced problem. So, it is necessary to solve multi-class imbalanced problem, which exist in real-world domains. In the proposed work, we introduced a methodology for classification of multi-class imbalanced data. This methodology consists of two steps: In first step we used Binarization techniques (OVA and OVO) for decomposing original dataset into subsets of binary classes. In second step, the SMOTE algorithm is applied against each subset of imbalanced binary class in order to get balanced data. Finally, to achieve classification goal Random Forest (RF) classifier is used. Specifically, oversampling technique is adapted to big data using MapReduce so that this technique is able to handle as large data-set as needed. An experimental study is carried out to evaluate the performance of proposed method. For experimental analysis, we have used different datasets from UCI repository and the proposed system is implemented on Apache Hadoop and Apache Spark platform. The results obtained shows that proposed method outperforms over other methods.\n\nTechnical support service providers receive thousands of customer queries daily. Traditionally, such organizations discard the data due to lack of storage capacity. However, value of storing such data is needed for the better results of analysis and to improve the closure rate of the daily customer queries. Data mining is the process of finding important and meaningful information, patterns through the large amount of data. Clustering is used as one of the best concept for data analysis, using machine learning approach with mathematical and statistical methods. Cluster analysis is widely applicable for practical applications in emerging trends in data mining. Analysis of clustering algorithms such as K-Means, Dirichlet, Fuzzy K-Means Canopy algorithms is done by means of the practical approach, in this research work. Performance of algorithm is observed based on the execution or computational time and results are compared with each of these algorithms. This paper proposes the streaming K-Means algorithm which resolves the queries as it arrives and analyses the data. Cosine distance measure plays an important role in clustering dataset. Sum of Square error is measured to check the quality of the cluster.\n\nTranslation of English documents into Hindi language is becoming an integral part for facilitating communication. The major population of India where 366 million people uses Hindi as primary language, for them providing information in Hindi is an important task the translation of English documents may be manually or automatically. When translation done manually the chances are rare for errors, but when we use any translation machines or engine the outputs received from these machines for Hindi have lots of grammatical mistakes or errors. One of the major issues observed is of tense. Thus, we propose solution to build a rule based tense synthesizer that would recognise the subject, verb and auxiliary verb, analyse the tense, then modify the verb and auxiliary verb according to the subject and put the sentence in the correct tense. This system could be integrated with Machine Translation engines to boost up the quality of Hindi translation.\n\nOne of the major causes of vision loss is Diabetic Retinopathy (DR). Presence of Hard Exudates (HE) in retinal images is one of the prominent and most reliable symptoms of Diabetic Retinopathy. Thus, it is essential to clinically examine for HEs to perform an early diagnosis and monitoring of DR. In this paper, a classification-based approach using Functional Link Artificial Neural Network (FLANN) classifier to extract HEs in a retinal fundus image is illustrated. Luminosity Contrast Normalization pre-processing step was employed. Classification performances were compared between Multi-Layered Perceptron (MLP), Radial Basis Function (RBF) and FLANN classifiers. Better classification performance was observed for FLANN classifier. GUI package with Region of Interest (ROI) selection tool was developed.\n\nNetwork of things is expanding day by day, with that security, flexibility and ease of use became concern of the user. We do have a different technique to full fill user's demands. Some of them are: Single Sign On (SSO), Cryptography techniques like RSA-VES, Serpent etc. In this paper an effort is made to provide all mentioned facilities to the user. Single Sign On (SSO) authorizes user only once and allow user to access multiple services and make the system very easy to use and also provides flexibility to use multiple programs or applications. The combination of cryptographic algorithms: Serpent (symmetric encryption) and RSA-VES (asymmetric encryption) which are known as one of the secured cryptographic algorithms are used with “session time” which makes communication very secure and reliable.\n\nThis article compares the Bit Error Rate (BER) performance of soft and hard decision decoding algorithms of LDPC codes on AWGN channel at different code rates and Signal to Noise Ratio (SNR) levels. Even though, hard decision decoding algorithm is computationally simple, its BER performance is not appreciable. Devising soft decision decoding algorithms which are simple and good in BER performance requires comparison of probabilistic and log domain methods. Towards this, a code word is generated through modulo(2) addition between message bits and generator matrix. After Binary Phase Shift Keying (BPSK) modulation the AWGN noise is introduced to the modulated code word. BER performance is computed by comparing the message decoded by soft and hard decision algorithms with the transmitted message. The experiment is conducted in MATLAB. Soft decision decoding algorithm in log domain provides better BER performance than hard decision decoding algorithm regardless of the SNR level.\n\nThis paper proposes a new dynamic packet scheduling scheme which guarantees the delay and jitter properties of differentiated services (DiffServ) network for both real time and non-real time traffics. The proposed dynamic packet scheduling algorithm uses a new weighted computation scheme known as dynamic benefit weighted scheduling (DB-WS) which is loosely based on weighted round robin (WRR) or fair queuing policy. The novelty of this scheduler is that it predicts the weight required by expedited forwarding (EF) service for current time slot (t) based on two factors: (i) previous weight allocated to it at time slot (t-1), and (ii) average increase in weights of EF traffic in consecutive time slots. This prediction provides smoother and balanced bandwidth allocation to EF, assured forwarding (AF) and best effort (BE) packets by not allocating all the resources to EF and also ensuring minimal packet losses for EF service. Adopting such a dynamic resource allocation effectively achieves reduction in packet loss, end to end delay and delay jitter. The algorithm is tested with different data rates and found to outperform other existing methods in terms of packet loss and end to end delay.\n\nNearly 70% of the earth is surrounded by water; hence it is appropriate to use underwater sensor networks to enable oceanographic research. In case of UWSN radio waves are not suitable for communication as propagation capability of radio waves is very poor in under water. Hence UWSN uses acoustic signals for communication. The transmission speed of acoustic wave is less than the transmission speed of radio wave due to various physical parameters of underwater acoustic channel. Hence the throughput of the channel is affected by this large delay. The long and uncertain delay makes many classical protocols unsatisfactory because they depend on multiple handshakes and appropriate calculations of the roundtrip time (RTT) between two nodes. We are designing and implementing a novel approach to eliminate inconsistent delays by reducing the control message exchanges in the network.\n\nThis paper deals with the study of the control mechanism and the practical application of electrical appliances using an android phone in a Zigbee network. The system measures the voltage and current parameters of electric devices and thus helps to view the power consumed. The proposed system is a flexible system which provides an efficient and effective control mechanism from a remote location. The system also focuses on voice based control and thus helps to save the electricity expense of the consumers. The other alternatives to Zigbee are also discussed in the paper.\n\nAuthentication is one of the important security aspects to secure the critical or sensitive information in a system. The authentication system must allow only the authorized users to access the critical information. So it must be strong enough to identify only the valid users and at the same time it should be user friendly. There are many authentication systems designed and used, but most commonly used authentication system is login-password. But this suffers with the attack called shoulder surfing, and brute force method of password guessing. The work carried out to explore the strengths of different graphical based password system to avoid the attack of shoulder surfing and enhance the security in terms of authentication. Also we have proposed a new graphical based authentication system.\n\nTime series analysis is one of the major prediction techniques for forecasting of time dependent variables. These days the time series analysis is applicable to a variety of applications. In this work the time series analysis technique using ARIMA model is applied on per capita disposable income for future forecasting. Per capita disposable income is the average available money per person after income taxes have been accounted for. It is an indicator of the overall state of an economy. Forecasting of per capita disposable income is necessary as it may help government assess country's economic condition in comparison with the economy of other countries of the world. Forecasting per capita disposable income may also help assess inflation and financial critical situation. The results obtained from this work can be used by the planning commission of a country to formulate future policies and plans.\n\nIn cloud computing scenario, efficiency of application execution not only depends on quality and quantity of the resources in the data center, but also on underlying resource allocation approaches. An efficient resource allocation technique is necessary for building an efficient system. The objective of this work is to propose an agent based Best-Fit resource allocation scheme which increases utilization of the resources, lowers the service cost and reduces the execution time. The work employs two types of agents: user's cloudlet agent and provider's resource agent. Cloudlet agent is located at the client system, which collects job requirement and offers various QoS options for its execution. Resource agent at the server, uses Best-Fit approach to allocate resources for jobs received from Cloudlet agent. The proposed work is simulated and the results are compared with other agent based resource allocation approaches using First-Come-First-Serve and Round-Robin. It is observed that Best-Fit approach performs better in terms of VMs allocation, job execution time, cost and resource utilization.\n\nCloud computing systems host most of today's commercial business applications yielding it high revenue which makes it a target of cyber attacks. This emphasizes the need for a digital forensic mechanism for the cloud environment. Conventional digital forensics cannot be directly presented as a cloud forensic solution due to the multi tenancy and virtualization of resources prevalent in cloud. While we do cloud forensics, the data to be inspected are cloud component logs, virtual machine disk images, volatile memory dumps, console logs and network captures. In this paper, we have come up with a remote evidence collection and pre-processing framework using Struts and Hadoop distributed file system. Collection of VM disk images, logs etc., are initiated through a pull model when triggered by the investigator, whereas cloud node periodically pushes network captures to HDFS. Pre-processing steps such as clustering and correlation of logs and VM disk images are carried out through Mahout and Weka to implement cross drive analysis.\n\nIn this paper a novel and enhanced version of Modified Decision Based Unsymmetric Trimmed Median Filter (MDBUTMF) have been proposed for removing high density impulse noise from images. The MDBUTMF algorithm fails at high density noise. Hence a new algorithm is presented to remove high density noise. Histogram estimation technique is used to decide whether the processing pixel is noisy or noise free. Parameters like Mean Square Error (MSE) and Peak Signal to Noise Ratio (PSNR) are used to compare the filter with its previous versions. Simulation results show that the filter works better with high density noise and out performs MDBUTMF.\n\nIn a Wireless Sensor Networks (WSNs) the sensor nodes are placed in an environment depending on the applications where secure communication is in high demand. To ensure the privacy and safety of data transactions in the network, a unique identification for the nodes and secure key for transportation have become major concerns. In order to establish a secure communication channel in the network, care and address the recourse constraints related to the devices and the scalability of the network when designing a secure key management. An approach for secure communication channel establishment is made in order to suite the functional and architectural features of WSNs. Here a hybrid key management scheme for symmetric key cryptography is attempted to establish a secure communication. An ECC and DH based key management and a certificate generation scheme, where the key is generated to decrypt the certificates to establish link for communication in the network. The hybrid scheme is tested based on amount of energy consumed and security analysis by simulation.\n\nIn the world of image processing domain, removal of high density impulse noise is always a prominent area for research. In this paper, an efficient Modified Decision based unsymmetric trimmed median filter algorithms for the removal of impulse noise has been proposed with color images rather than gray scale images by separation red- green- blue plane of color image. The performance of the system is analyzed in terms of Mean square error (MSE), Peak signal to noise ratio (PSNR) image enhancement factor (IEF) and time required for executing the algorithms for different noise densities. Simulation results shows that proposed algorithm outperforms the existing algorithms even at high noise densities for color images. Many experiments are conducted to validate efficiency of the proposed algorithm.\n\nMobile application developers and users can feel a direct advantage within mobile based cloud computing for overcoming the inherent constraints present in mobile devices - be it battery life, memory space or processing power.\n\nThe field of Network Centric Warfare seeks to use information technology in order to gain competitive advantage over the enemies during war. An essential part of network centric warfare deals with the firing of highly destructive weapons. It includes the determination of exact direction of firing the weapon so as to successfully destroy the target for a given unguided shell. This can be done using different trajectory models. The major challenge is that this decision be quick in the time critical scenario of war and this urges the use of previous experience using data mining.\n\nSession Initiation Protocol is an IP based signaling protocol used for establishing, modifying and terminating sessions. During the signaling process, both the peers may initiate similar or controversial signaling messages resulting in Race / Glare conditions. SIP protocol handles glare conditions by transmitting the signaling messages after a random interval of time, within a specified time range. This process reduces the chances for occurring of glare conditions, but results in increased signaling messages. This paper proposes a method for avoiding / reducing glare conditions for SIP BYE requests by enabling state-full SIP Proxy servers with extra intelligence to identify and reduce signaling messages transmitted end to end during Glare conditions. This work analyzes the amount of bandwidth reduced and number of signaling messages reduced with proposed method in scenarios where the glare condition is identified between two proxy servers and between a proxy server and a UserAgent and the glare condition that occurs at a proxy server.\n\nVoltage pulse or current pulse can be used as the main monitoring parameter of the sparks that occur in micro electrical discharge machining (μEDM) process. The spark gap should be monitored for many purposes including study of material removal characteristics, which can be followed by tool wear monitoring and compensation. The system which fulfils the requirements of monitoring spark gap by using gap waveforms is called pulse discriminating (PD) system. The purpose of this research is to compare the capability of effective pulse discrimination using voltage and current pulses by online application of an algorithm written in LABVIEW separately. The comparison is based on the evaluation of the following responses - effective counting of total number of pulses and percentage of different kind of pulses that exist in μEDM.\n\nA patent is an intellectual property document that protects new inventions. It covers how things work, what they do, how they do it, what they are made of and how they are made. The owner of the granted patent application has the ability to take a legal action to stop others from making, using, importing or selling the invention without permission. While applying for a patent, the inventor has issues in identifying similar patents. Citations of related patents, which are referred to as the prior art, should be included while applying for a patent. We propose a system to develop a Patent Search Engine to identify related patents. We also propose a system to predict Business Trends by analyzing the patents. In our proposed system, we carry out a query independent clustering of patent documents to generate topic clusters using LDA. From these clusters, we retrieve query specific patents based on relevance thereby maximizing the query likelihood. Ranking is based on relevancy and recency which can be performed using BM25F algorithm. We analyze the Topic-Company trends and forecast the future of the technology which is based on the Time Series Algorithm - ARIMA. We evaluate the proposed methods on USPTO patent database. The experimental results show that the proposed techniques perform well as compared to the corresponding baseline methods.\n\nThe diffusion of ICT - Information and Communication Technology has witnessed a remarkable growth in the Past decade across the globe. This growth is fuelled and propelled by technological advances, economic investment, social and cultural changes that have facilitated the integration of ICT into everyday life. As information technology in the healthcare industry evolves, the scope of information sharing is expanding beyond four walls of individual institutions. Achieving this level of integration will require the software models overcome a host of technical obstacles and they are accessible, affordable and widely supported primarily from the government. Indian healthcare sector is undergoing a transformation with improved services becoming available to a larger population. Indian healthcare is substantially poised under the influence of government and diplomatic beliefs, interests and the state of affairs of the economic conditions rather than the documentation and conformation of the facts and figures. There have been many attempts to improve quality and investment in healthcare, but most have been based on management fads and have been unsustainable. In this paper we have evaluated the technology acceptance and financial investment of healthcare professionals from field survey.\n\nIn recent generation higher performance and high computational capability are possible by small feature size and high density of transistors in integrated circuit. In CMOS circuit as scaling down of both supply voltage (Vdd) and threshold voltage (Vt) result in increased sub-threshold leakage current hence more power dissipation. Small feature size and decrease in both Vdd and Vt has hostile delay reduction. LECTOR and INDEP are the techniques for CMOS circuit designing which mitigate leakage current without affecting the dynamic power dissipation. This paper presents the comparative study for area, delay and power dissipation of CMOS inverter for LECTOR and INDEP techniques. Simulation of INDEP Inverter and LECTOR Inverter circuit with and without body biasing has been done. The sizing effect of extra and conventional transistor is also addressed in this paper. Simulation of the circuit is done using Tanner EDA tool at 70nm with supply voltage of 1V is considered.\n\nThe channels for expressing opinions seem to increase daily and hence, they are important sources of business insight. For product planning, marketing and customer service, it is necessary to capture and analyse these opinions. The social blogs are massive corpus for text and opinion mining. So, in this competitive and greedy world we find the need of analysis of the opinions about the products a vital task, to evaluate and improve the product quality, and as a result these opinions are relevant to companies. These opinions are easily accessed through the social blogs. This paper discusses various methods in performing sentiment analysis. We can see that presently we are provided facility to post the opinions in different languages and this paper tried to consider the processing of texts of different languages that are posted in blogs. Along with that the relevance of the quality of dataset used for the analysis are also the subject of discussion in this paper.\n\nNow a days, cloud computing is most popular network in world. Cloud computing provides resource sharing and online data storage for the end users. In existed cloud computing systems there are many security issues. So, security becomes essential part for the data which is stored on cloud. To solve this problem we have proposed this paper. This paper presents client side AES encryption and decryption technique using secret key. AES encryption and decryption is high secured and fastest technique. Client side encryption is an effective approach to provide security to transmitting data and stored data. This paper proposed user authentication to secure data of encryption algorithm with in cloud computing. Cloud computing allows users to use browser without application installation and access their data at any computer using browser. This infrastructure guaranteed to secure the information in cloud server.\n\nIn the emerging advances of communication technologies, Wireless Sensor Networks (WSN)s, consisting of numerous sensor nodes, are extensively used in the various application areas such as, vehicle tracking, agriculture, military, forest surveillance, healthcare, environment and earthquake observation etc., The sensor nodes are provided with the smaller computing power, little memory, lesser battery power and slighter range of communication strength. These sensor nodes are to be deployed in a particular location to monitor the environmental system on the basis of applications of WSN. The complexity of deployment of a wireless sensor networks is depending on optimization and application requirements. The deployment of WSN is categorized as static, dynamic and energy aware node placement. This work is an extension of our paper “Analogy of Static and Dynamic Node Deployment Algorithms in WSN”. The present work provides the comparison between the different deployment algorithms of static, dynamic and energy aware protocols. The comparison of algorithms and protocols is achieved on the basis of different parameters like energy consumption; coverage of nodes, average distance between the nodes etc., The result indicates that, which deployment algorithm has the better performance. The main goal of this paper is to afford the knowledge for the best of static, dynamic and energy aware deployme"
    }
}
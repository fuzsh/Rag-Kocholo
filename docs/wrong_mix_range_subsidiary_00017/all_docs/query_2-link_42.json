{
    "id": "wrong_mix_range_subsidiary_00017_2",
    "rank": 42,
    "data": {
        "url": "https://purl.pt/302/1/dlib/january17/01contents.html",
        "read_more_link": "",
        "language": "en",
        "title": "Lib Magazine",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://purl.pt/302/1/img2/space.gif",
            "https://purl.pt/302/1/img2/search2.gif",
            "https://purl.pt/302/1/img2/space.gif",
            "https://purl.pt/302/1/img2/D-Lib-blocks.gif",
            "https://purl.pt/302/1/img2/transparent.gif",
            "https://purl.pt/302/1/img2/magazine.gif",
            "https://purl.pt/302/1/img2/transparent.gif",
            "https://purl.pt/302/1/img2/transparent.gif",
            "https://purl.pt/302/1/img2/transparent.gif",
            "https://purl.pt/302/1/img2/transparent.gif",
            "https://purl.pt/302/1/img2/transparent.gif",
            "https://purl.pt/302/1/img2/transparent.gif",
            "https://purl.pt/302/1/dlib/january17/images/FC-3.png",
            "https://purl.pt/302/1/dlib/january17/images/Fc-2.png",
            "https://purl.pt/302/1/dlib/january17/images/FC-1.png",
            "https://purl.pt/302/1/img2/feed-icon-14x14.png",
            "https://purl.pt/302/1/img2/transparent.gif",
            "https://purl.pt/302/1/img2/transparent.gif"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            "D-Lib Magazine",
            "Digital Library Research"
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "D-Lib Magazine is an electronic publication with a focus on digital library research and development, including new technologies, applications, and contextual social and economic issues.",
        "meta_lang": "",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "T A B L E O F C O N T E N T S\n\nJ A N U A R Y / F E B R U A R Y 2 0 1 7\n\nVolume 23, Number 1/2\n\nISSN: 1082-9873\n\nhttps://doi.org/10.1045/january2017-contents\n\nE D I T O R I A L S\n\nSpecial Issue\n\nEditorial by Laurence Lannom, Corporation for National Research Initiatives\n\nRepScience2016\n\nGuest Editorial by Amir Aryani, Australian National Data Service; Oscar Corcho, Departamento de Inteligencia Artificial, Universidad Politécnica de Madrid; Paolo Manghi, Istituto di Scienza e Tecnologie dell'Informazione, Consiglio Nazionale delle Ricerche; Jochen Schirrwagen, Bielefeld University Library\n\nA R T I C L E S\n\nFrom Data to Machine Readable Information Aggregated in Research Objects\n\nArticle by Markus Stocker, PANGAEA, MARUM Center for Marine Environmental Sciences, University of Bremen\n\nAbstract: Data interpretation is an important process in scientific investigations. It results in information and gives data meaning. As a case in point, earth and environmental scientists interpret data  increasingly often collected in large-scale environmental research infrastructures  to gain information about the studied environment. Information is typically represented to suit human consumption  in natural language text, figures and tables designed for expert processing. Building on a case study in aerosol science, we discuss how information resulting in data interpretation can be represented as machine readable information objects, here of type Interpretation. As the main contribution, we present the aggregation of interpretations in Research Objects. Together with data, metadata, workflows, software, and articles, interpretations are contextual resources of scientific investigations. The explicit aggregation of interpretations in Research Objects is arguably a further step toward a more complete representation of the context of scientific investigations.\n\nThe Scholix Framework for Interoperability in Data-Literature Information Exchange\n\nArticle by Adrian Burton and Amir Aryani, Australian National Data Service; Hylke Koers, Elsevier; Paolo Manghi and Sandro La Bruzzo, Istituto di Scienza e Tecnologie dell'Informazione, Consiglio Nazionale delle Ricerche; Markus Stocker, Michael Diepenbroek and Uwe Schindler, PANGAEA, MARUM Center for Marine Environmental Sciences, University of Bremen; Martin Fenner, DataCite\n\nAbstract: The Scholix Framework (SCHOlarly LInk eXchange) is a high level interoperability framework for exchanging information about the links between scholarly literature and data, as well as between datasets. Over the past decade, publishers, data centers, and indexing services have agreed on and implemented numerous bilateral agreements to establish bidirectional links between research data and the scholarly literature. However, because of the considerable differences inherent to these many agreements, there is very limited interoperability between the various solutions. This situation is fueling systemic inefficiencies and limiting the value of these, separated, sets of links. Scholix, a framework proposed by the RDA/WDS Publishing Data Services working group, envisions a universal interlinking service and proposes the technical guidelines of a multi-hub interoperability framework. Hubs are natural collection and aggregation points for data-literature information from their respective communities. Relevant hubs for the communities of data centers, repositories, and journals include DataCite, OpenAIRE, and Crossref, respectively. The framework respects existing community-specific practices while enabling interoperability among the hubs through a common conceptual model, an information model and open exchange protocols. The proposed framework will make research data, and the related literature, easier to find and easier to interpret and reuse, and will provide additional incentives for researchers to share their data.\n\nSupporting Data Reproducibility at NCI Using the Provenance Capture System\n\nArticle by Jingbo Wang, Ben Evans and Lesley Wyborn, National Computational Infrastructure, Australia; Nick Car, Geoscience Australia; Edward King, CSIRO, Australia\n\nAbstract: Scientific research is published in journals so that the research community is able to share knowledge and results, verify hypotheses, contribute evidence-based opinions and promote discussion. However, it is hard to fully understand, let alone reproduce, the results if the complex data manipulation that was undertaken to obtain the results are not clearly explained and/or the final data used is not available. Furthermore, the scale of research data assets has now exponentially increased to the point that even when available, it can be difficult to store and use these data assets. In this paper, we describe the solution we have implemented at the National Computational Infrastructure (NCI) whereby researchers can capture workflows, using a standards-based provenance representation. This provenance information, combined with access to the original dataset and other related information systems, allow datasets to be regenerated as needed which simultaneously addresses both result reproducibility and storage issues.\n\nGraph Connections Made By RD-Switchboard Using NCI's Metadata\n\nArticle by Jingbo Wang, Ben Evans and Lesley Wyborn, National Computational Infrastructure, Australia; Amir Aryani and Melanie Barlow, Australian National Data Service\n\nAbstract: This paper demonstrates the connectivity graphs made by Research Data Switchboard (RD-Switchboard) using NCI's metadata database. Making research data connected, discoverable and reusable are some of the key enablers of the new data revolution in research. We show how the Research Data Switchboard identified the missing critical information in our database, and what improvements have been made by this system. The connections made by the RD-Switchboard demonstrated the various use of the datasets, and the network of researchers and cross-referenced publications.\n\nOpening the Publication Process with Executable Research Compendia\n\nArticle by Daniel Nüst, Markus Konkol, Edzer Pebesma and Christian Kray, Institute for Geoinformatics; Marc Schutzeichel, Holger Przibytzin and Jörg Lorenz, University and State Library, Münster\n\nAbstract: A strong movement towards openness has seized science. Open data and methods, open source software, Open Access, open reviews, and open research platforms provide the legal and technical solutions to new forms of research and publishing. However, publishing reproducible research is still not common practice. Reasons include a lack of incentives and a missing standardized infrastructure for providing research material such as data sets and source code together with a scientific paper. Therefore we first study fundamentals and existing approaches. On that basis, our key contributions are the identification of core requirements of authors, readers, publishers, curators, as well as preservationists and the subsequent description of an executable research compendium (ERC). It is the main component of a publication process providing a new way to publish and access computational research. ERCs provide a new standardisable packaging mechanism which combines data, software, text, and a user interface description. We discuss the potential of ERCs and their challenges in the context of user requirements and the established publication processes. We conclude that ERCs provide a novel potential to find, explore, reuse, and archive computer-based research.\n\nConquaire: Towards an Architecture Supporting Continuous Quality Control to Ensure Reproducibility of Research\n\nArticle by Vidya Ayer, Cord Wiljes, Philipp Cimiano, CITEC, Bielefeld University; Christian Pietsch, Johanna Vompras, Jochen Schirrwagen and Najko Jahn, Bielefeld University Library\n\nAbstract: Analytical reproducibility in scientific research has become a keenly discussed topic within scientific research organizations and acknowledged as an important and fundamental goal to strive for. Recently published scientific studies have found that irreproducibility is widely prevalent within the research community, even after releasing data openly. At Bielefeld University, nine research project groups from varied disciplines have embarked on a \"reproducibility\" journey by collaborating on the Conquaire project as case study partners. This paper introduces the Conquaire project. In particular, we describe the goals and objectives of the project as well as the underlying system architecture which relies on a DCVS system for storing data, and on continuous integration principles to foster data quality. We describe a first prototype implementation of the system and discuss a running example which illustrates the functionality and behaviour of the system.\n\nTowards Reproducibility of Microscopy Experiments\n\nArticle by Sheeba Samuel, Frank Taubert and Daniel Walther, Institute for Computer Science, Friedrich Schiller University Jena; Birgitta König-Ries and H. Martin Bücker, Institute for Computer Science, Friedrich Schiller University Jena, Michael Stifel Center Jena for Data-driven and Simulation Science\n\nAbstract: The rapid evolution of various technologies in different scientific disciplines has led to the generation of large volumes of high dimensional data. Studies have shown that most of the published work is not reproducible due to the non-availability of the datasets, code, algorithm, workflow, software, and technologies used for the underlying experiments. The lack of sufficient documentation and the deficit of data sharing among particular research communities have made it extremely difficult to reproduce scientific experiments. In this article, we propose a methodology enhancing the reproducibility of scientific experiments in the domain of microscopy techniques. Though our approach addresses the specific requirements of an interdisciplinary team of scientists from experimental biology to store, manage, and reproduce the workflow of their research experiments, it can also be extended to the requirements of other scientific communities. We present a proof of concept of a central storage system that is based on OMERO (Allan et al., Nature Methods 9, 245-253, 2012). We discuss the criteria and attributes needed for reproducibility of microscopy experiments.\n\nHyWare: a HYbrid Workflow lAnguage for Research E-infrastructures\n\nArticle by Leonardo Candela and Paolo Manghi, Istituto di Scienza e Tecnologie dell'Informazione, Consiglio Nazionale delle Ricerche; Fosca Giannotti, Valerio Grossi and Roberto Trasarti, KDD Lab, ISTI CNR, Pisa, Italy\n\nAbstract: Research e-infrastructures are \"systems of systems\", patchworks of tools, services and data sources, evolving over time to address the needs of the scientific process. Accordingly, in such environments, researchers implement their scientific processes by means of workflows made of a variety of actions, including for example usage of web services, download and execution of shared software libraries or tools, or local and manual manipulation of data. Although scientists may benefit from sharing their scientific process, the heterogeneity underpinning e-infrastructures hinders their ability to represent, share and eventually reproduce such workflows. This work presents HyWare, a language for representing scientific process in highly-heterogeneous e-infrastructures in terms of so-called hybrid workflows. HyWare lays in between \"business process modeling languages\", which offer a formal and high-level description of a reasoning, protocol, or procedure, and \"workflow execution languages\", which enable the fully automated execution of a sequence of computational steps via dedicated engines.\n\nEnabling Reproducibility for Small and Large Scale Research Data Sets\n\nArticle by Stefan Pröll, SBA Research, Austria and Andreas Rauber, Vienna University of Technology, Austria\n\nAbstract: A large portion of scientific results is based on analysing and processing research data. In order for an eScience experiment to be reproducible, we need to able to identify precisely the data set which was used in a study. Considering evolving data sources this can be a challenge, as studies often use subsets which have been extracted from a potentially large parent data set. Exporting and storing subsets in multiple versions does not scale with large amounts of data sets. For tackling this challenge, the RDA Working Group on Data Citation has developed a framework and provides a set of recommendations, which allow identifying precise subsets of evolving data sources based on versioned data and timestamped queries. In this work, we describe how this method can be applied in small scale research data scenarios and how it can be implemented in large scale data facilities having access to sophisticated data infrastructure. We describe how the RDA approach improves the reproducibility of eScience experiments and we provide an overview of existing pilots and use cases in small and large scale settings.\n\nN E W S & E V E N T S\n\nIn Brief: Short Items of Current Awareness\n\nIn the News: Recent Press Releases and Announcements\n\nClips & Pointers: Documents, Deadlines, Calls for Participation"
    }
}
{
    "id": "dbpedia_1312_2",
    "rank": 25,
    "data": {
        "url": "https://aspe.hhs.gov/reports/performance-improvement-1998",
        "read_more_link": "",
        "language": "en",
        "title": "Performance Improvement 1998",
        "top_image": "https://aspe.hhs.gov/themes/custom/aspe_uswds/assets/img/opengraph/aspe-tw-cover.png",
        "meta_img": "https://aspe.hhs.gov/themes/custom/aspe_uswds/assets/img/opengraph/aspe-tw-cover.png",
        "images": [
            "https://aspe.hhs.gov/themes/custom/aspe_uswds/assets/img/us_flag_small.png",
            "https://aspe.hhs.gov/themes/custom/aspe_uswds/assets/img/icon-dot-gov.svg",
            "https://aspe.hhs.gov/themes/custom/aspe_uswds/assets/img/icon-https.svg",
            "https://aspe.hhs.gov/themes/custom/aspe_uswds/assets/img/hhs-icon.png",
            "https://aspe.hhs.gov/themes/custom/aspe_uswds/assets/img/close.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "1998-01-31T19:00:00-05:00",
        "summary": "",
        "meta_description": "ForewordPerformance Improvement 1998: Evaluation Activities of the U.S. Department of Health and Human Servicesis the third annual report summarizing evaluation efforts by the Department's agencies and offices during the past fiscal year. This report is intended to help those interested in evaluations of health or social services to locate information on study results, recommendations, or applications of program improvement or policy development.",
        "meta_lang": "en",
        "meta_favicon": "/themes/custom/aspe_uswds/favicon.ico",
        "meta_site_name": "ASPE",
        "canonical_link": "https://aspe.hhs.gov/reports/performance-improvement-1998",
        "text": "Foreword\n\nPerformance Improvement 1998: Evaluation Activities of the U.S. Department of Health and Human Servicesis the third annual report summarizing evaluation efforts by the Department's agencies and offices during the past fiscal year. This report is intended to help those interested in evaluations of health or social services to locate information on study results, recommendations, or applications of program improvement or policy development. Generally, the audience for this report includes decisionmakers who need information on program results; program managers who want to know how other service programs are operating and how their own performance can be improved; and the community of researchers, advocates, and practitioners who use the program information and evaluation tools.\n\nThe mission of the Department of Health and Human Services (HHS) is to enhance the health and well-being of Americans by providing for effective health and human services and by fostering strong, sustained advances in the sciences underlying medicine, public health, and social services. To accomplish this mission, the Department manages an array of programs in basic and applied science, public health, and child and adolescent development; programs that foster economic self-sufficiency and support working families; and programs that finance or regulate health, mental health, and social services.\n\nThrough departmentwide strategic planning and performance management, HHS programs are continuously improving and redirecting to meet new needs of customers, beneficiaries, and other constituencies. The 1990's trend toward results-oriented management of HHS programs makes evaluation an important resource for producing knowledge to measure program performance, develop and improve programs, and analyze and develop policy.\n\nFor example, last year the Substance Abuse and Mental Health Services Administration in HHS completed one of the largest and most rigorous studies of drug treatment programs (Chapter II, page 12). Among the key findings were that clients of HHS-supported drug treatment significantly reduced their alcohol and other drug use; that treatment has had lasting benefits for clients; and that clients reported increases in employment and income, as well as decreases in criminal behavior, risk for human immunodeficiency virus/acquired immuno-deficiency syndrome (HIV/AIDS), and homelessness. This evaluation gives us new knowledge about what works in substance abuse treatment and the cost, which can be applied to improving current programs and designing new programs to advance future treatment.\n\nEvaluations also are useful for implementing or operating programs. One such evaluation completed last year by the Centers for Disease Control and Prevention on syphilis control programs is helping local health departments in the Southern States to expand syphilis prevention efforts, increase access to services, and improve program operations and methods of contact tracing and notification (Chapter II, page 19). The evaluation findings offered new information on population groups at risk, barriers to reaching those at greatest risk, how best to reach those at risk through community institutions, and innovations in sexually-transmitted disease control and prevention programs. This study illustrates how HHS officials can effectively use evaluation resources to produce information useful for working with our partners in State and local communities to improve public health systems.\n\nAssessing the environmental or external factors that affect HHS program or policy development is another major application of HHS evaluation activity. This past year, the Office of the Assistant Secretary for Planning and Evaluation completed a policy study on informal and formal kinship care of children and its importance for the child welfare system (Chapter II, page 26). Children living with relatives but away from parents, either informally or through formal foster care placements, is a growing phenomenon. This study provides new knowledge that will help shape future policy and program development in foster care services.\n\nPerformance Improvement 1998 contains information on HHS agency and office evaluation projects completed and in progress during fiscal year (FY) 1997. The report is organized into three Chapters. Chapter I describes the organization of HHS evaluations--activities, resources, and planning and management. Chapter II highlights the results from nine FY 1997 evaluations, selected by an outside review panel for their potential use by the larger health and human services community. Chapter III presents the evaluation activities of the 11 HHS agencies and the Office of the Secretary, including information on their evaluation programs, evaluations completed in FY 1997, and evaluations in progress. A complete inventory of the 155 HHS evaluation projects completed in FY 1997 is provided in Appendix A, and the 335 HHS agency projects currently in progress are listed in Appendix B.\n\nWe hope that you will find this report useful and informative.\n\nDonna E. Shalala\n\nSecretary\n\nU.S. Department of Health and Human Services\n\nMargaret A. Hamburg, M.D.\n\nAssistant Secretary for Planning and Evaluation\n\nAcknowledgments\n\nPerformance Improvement 1998: Evaluation Activities of the U.S. Department of Health and Human Services describes the continuous efforts of the various HHS agencies to examine service and research programs for the efficiency of their operations and their effectiveness in achieving objectives. The planning, development, and coordination of those evaluations is largely the responsibility of the following HHS planning and evaluation offices:\n\nAdministration for Children and Families\n\nOffice of Planning, Research and Evaluation\n\nHoward Rolston, Director\n\nAdministration on Aging\n\nOffice of Program Operations and Development\n\nEdwin Walker, Director\n\nAgency for Health Care Policy and Research\n\nOffice of the Administrator\n\nLisa Simpson, Deputy Administrator\n\nAgency for Toxic Substances and Disease Registry\n\nOffice of Policy and External Affairs\n\nGeorgi A. Jones, Director\n\nCenters for Disease Control and Prevention\n\nOffice of Program Planning and Evaluation\n\nKathy Cahill, Director\n\nFood and Drug Administration\n\nOffice of Planning and Evaluation\n\nPaul L. Coppinger, Associate Commissioner\n\nHealth Care Financing Administration\n\nOffice of Strategic Planning\n\nBarbara S. Cooper, Director\n\nHealth Resources and Services Administration\n\nOffice of Planning, Evaluation and Legislation\n\nPaul W. Nannis, Director\n\nIndian Health Service\n\nOffice of Public Health\n\nLeo Nolan, Director\n\nNational Institutes of Health\n\nOffice of Science Policy and Technology Transfer\n\nLana Skirboll, Associate Director\n\nOffice of the Assistant Secretary for Planning and Evaluation\n\nOffice of Program Systems\n\nSusanne A. Stoiber, Deputy Assistant Secretary for Program Systems\n\nOffice of Public Health and Science\n\nCarol Roddy, Senior Program Advisor\n\nSubstance Abuse and Mental Health Services Administration\n\nOffice of Policy and Program Coordination\n\nMary Knipmeyer, Director\n\nPreparation of this report and the noted appendices was coordinated by staff in the Office of the Assistant Secretary for Planning and Evaluation, under the supervision of Susanne A. Stoiber, Deputy Assistant Secretary for Program Systems. Responsible staff include Mike Herrell, Paul Johnson, Colleen Monaghan, Carolyn Solomon, and Joan Lee Turek. Development and production of the report was supported, under contract, by the services of Camille Collett, Sacha Sanger, and Margaret Long of Sanad Support Technologies, Inc.; and C. Brannon Underwood, Reid Jackson, Wayne Hall, and Lydia Paddock of the South Carolina State University Policy Analysis Consortium.\n\nThe following persons from HHS agencies contributed to writing, collecting information on evaluation activities, and reviewing the report:\n\nAdministration for Children and Families\n\nJames V. Dolson Helen Howerton\n\nAdministration on Aging\n\nSaadia Greenberg\n\nAgency for Health Care Policy and Research\n\nWendy Perry\n\nHarvey Schwartz\n\nAgency for Toxic Substances and Disease Registry\n\nWoodrow Garrett\n\nDonna Jones\n\nCenters for Disease Control and Prevention\n\nNancy Cheal\n\nFood and Drug Administration\n\nKathleen McEvoy Mary Gamunev\n\nHealth Care Financing Administration\n\nWilliam Saunders\n\nSydney Galloway\n\nHealth Resources and Services Administration\n\nAnabel Crane\n\nIndian Health Service\n\nFrank Marion\n\nSarah Crazythunder\n\nNational Institutes of Health\n\nJohn Uzzell\n\nJoan Bailey\n\nOffice of Public Health and Science\n\nCarol Roddy\n\nSubstance Abuse and Mental Health Services Administration\n\nAnna Marsh\n\nTOP OF DOCUMENT\n\nEvaluation Review Panel\n\nThe following individuals served on the evaluation review panel that made recommendations of the reports highlighted in Chapter II of Performance Improvement 1998: Evaluation Activities of the U.S. Department of Health and Human Services.\n\nHeather Becker, Ph.D.\n\nProfessor, School of Nursing\n\nUniversity of Texas\n\nFACE=\"Times New Roman\">Austin, Texas\n\nFrank Caro, Ph.D.\n\nUniversity of Massachusetts at Boston\n\nGerontology Institute and Center\n\nFACE=\"Times New Roman\">Boston, Massachusetts\n\nTresmaine R. Grimes, Ph.D.\n\nSouth Carolina State University\n\nOrangeburg, South Carolina\n\nHolly Korda, Ph.D.\n\nEvaluation Consultant\n\nBoston, Massachusetts\n\nJohn Kralewski, Ph.D.\n\nDirector, Institute for Health Services Research\n\nSchool of Public Health\n\nUniversity of Minnesota\n\nMinneapolis, Minnesota\n\nKenneth McLeroy, Ph.D.\n\nHealth Promotion Sciences Department\n\nUniversity of Oklahoma\n\nOklahoma City, Oklahoma\n\nDonna Mertens, Ph.D.\n\nProfessor\n\nGallaudet University\n\nWashington, D.C.\n\nIra Raskin, Ph.D.\n\nHealth Policy Consultant\n\nBethesda, Maryland\n\nFelix A. Okojie, Ph.D.\n\nJackson State University\n\nJackson, Mississippi\n\nTOP OF DOCUMENT\n\nExecutive Summary\n\nPerformance Improvement 1998 is the third annual report of the U.S. Department of Health and Human Services (HHS) on its evaluation activities. As a report to Congress, it summarizes the findings of HHS evaluations completed in fiscal year (FY) 1997. In that year, HHS agencies produced 155 evaluation reports and supported more than 335 evaluation projects in progress.\n\nIn general, the report is intended for three audiences: decisionmakers, who need information on program results across the broad spectrum of health and human services; program managers, who need information on how they can make improvements in program operations and outcomes; and the health and human services community, which can benefit by applying the knowledge and lessons learned from HHS evaluations.\n\nIn the Foreword, Secretary Donna Shalala and Dr. Margaret A. Hamburg, Assistant Secretary for Planning and Evaluation, stress that evaluation is an important tool for producing the knowledge necessary to develop and improve the performance of HHS programs and services to meet the needs of the 21st century. HHS program managers are continuously engaged in efforts to determine whether programs and services reach the intended populations or communities, perform efficiently, and achieve desired results.\n\nCHAPTER I\n\nChapter I provides an overview of evaluation at HHS. The overview contains information about the types of evaluation activities; evaluation resources; and evaluation management, including planning and coordination, project management, quality assurance, and dissemination. The last section of the Chapter discusses four ways in which HHS makes effective use of evaluation results for strategic planning and program and policy development. These are performance measurement and data systems to monitor progress in achieving departmental goals and strategies as well as program outcomes; evaluations to assess the effectiveness of HHS strategies and programs used in achieving goals and objectives; environmental assessments to study changes in the larger society and their impact on departmental programs and strategies; and evaluation resources to improve program management and support evaluation efforts.\n\nCHAPTER II\n\nChapter II highlights nine HHS evaluations completed during FY 1997 and selected by the Evaluation Review Panel as potentially useful to the larger health and human services research and practice community. For the selection criteria, see Appendix C. Summaries of each report are presented according to three general purposes of evaluation: performance measurement or assessment, program management and development, and policy analysis and development.\n\nPerformance Measurement or Assessment\n\nLEAP: Final Report on Ohio's Welfare Initiative to Improve School Attendance Among Teenage Parents\n\nThe goal of Ohio's Learning, Earning, and Parenting (LEAP) Program is to use financial incentives to increase the proportion of pregnant teenagers and teen parents on welfare who graduate from high school or receive a General Equivalency Diploma (GED), find jobs, and ultimately achieve self-sufficiency. The immediate goals were to induce dropouts to return to high school or to enroll in General Educational Development programs, and to promote better attendance among those already enrolled in school. These short-term goals were accomplished--statistically significant increases in school enrollment and attendance were observed. Longer-term goals were to increase the rate of high school graduation, GED attainment, employment, and to reduce welfare payments. These long-term goals were only partly successful. The evaluation of LEAP is timely in light of new changes to national welfare laws which increases emphasis on school attendance and on more employment-oriented activities for teens and other welfare recipients.\n\nNational Treatment Improvement Evaluation Study: Final Report\n\nThe congressionally mandated 5-year National Treatment Improvement Evaluation Study (NTIES) examined the impact of three federally-sponsored drug and alcohol treatment demonstration programs. The study found that the demonstration programs significantly reduced drug and alcohol use, and that treatment had lasting benefits. Clients reported increases in employment and income, improvements in mental and physical health, decreases in criminal activities and homelessness, and modification of sexually risky behavior.\n\nProgram Management and Development\n\nSelf-Sufficiency Project Implementation Manual: Lessons Learned from Eight Years of Office of Community Services Demonstration Partnership Programs\n\nThis manual summarizes lessons learned during eight years of the Demonstration Partnership Program (DPP), which supports innovative approaches toward increasing the self-sufficiency of the poor, tests and evaluates the approaches, and encourages their replication through dissemination of project results and findings. These projects were divided into five areas: (1) case management, (2) micro-enterprise development, (3) minority males, (4) homelessness, and (5) youth at risk. The manual was designed as a step-by-step guide for designing and implementing Self-Sufficiency Projects by community action agencies, community-based organizations, and local community program planners.\n\nThe Effects of Informatics Tools and Decision Aids to Help Patient Decision-making about Medical Screening and Treatment\n\nThis study identified, evaluated, and synthesized research on our knowledge base on the effectiveness of tools intended to help inform patients about their medical choices, their treatment alternatives, and the risks and benefits of those alternatives. These included interactive computer disks, videotapes, audiotapes, brochures, and computer-generated fact sheets to help patients make decisions about medical screening and treatment. The research found that although some studies do point to the promise these tools hold for helping patients and consumers, few controlled and comparative studies have been done.\n\nSyphilis in the South: A Case Study Assessment in Eight Southern Communities\n\nThis project provided a comparative case study of local-level syphilis prevention efforts in eight communities in Alabama, Mississippi, South Carolina, and Tennessee. The project focused on groups perceived to be at high risk of becoming infected with syphilis, the extent to which public health activities target such groups, and identification of factors that affect the reach of services to these groups. African Americans were found to be at greatest risk of syphilis infection. The report highlights the cultural, programmatic, and political barriers that restrict the prevention and control of syphilis and other sexually transmitted diseases for those at greatest risk. Cultural barriers include local norms that restrict public discussion of human sexuality, distrust of the public health system among African Americans, and a low priority of health relative to other issues of poverty in the community.\n\nStakeholders Revisit Healthy People 2000 to Maximize the Impact for 2010\n\nThis report presents an assessment by Healthy People 2000 stakeholders on how to restructure the goals and objectives of Healthy People 2000 for the next decade. Healthy People 2000 is a set of national objectives for improving the health of Americans through effective prevention strategies. These objectives form a model framework for results-oriented performance measurements that are relevant to both population-based and individual-based health care initiatives. The objectives also serve as benchmarks against which performance can be measured over time. The report explores new approaches for the development of Healthy People 2010, taking into consideration the many major changes in health care since 1990, and past experience with Healthy People 2000 and the health objectives for 1990.\n\nPolicy Analysis and Development\n\nImpact of the Medicare Fee Schedule on Teaching Physicians\n\nThis study assessed the impact of a new Medicare Fee Schedule (MFS) on the revenues of teaching physicians. The new schedule was based on the Resource-Based Relative Value Scale and altered the relative prices paid to physicians. Relative payment levels were greatly increased for visits (less intensive relative value units, RVUs), and reduced for most types of diagnostic tests and surgical procedures (more intensive RVUs). Relative payment levels were raised in rural areas and lowered in large urban areas. It was hypothesized that teaching physicians would be adversely affected by the MFS because they perform more high-technology procedures and provide comparatively fewer primary care services. Teaching physicians also are located in large urban areas and experience customary charges that may be higher than the area-wide historical payments calculated for fee schedule transition. The study analyzed all Medicare Part B claims associated with discharges during 1991-1993 from a sample of teaching and non-teaching hospitals. Part B revenues and selected price variables were assessed for different categories of service. No evidence was found to suggest that teaching physicians had been adversely affected by the MFS.\n\nInformal and Formal Kinship Care\n\nThis study used existing national data sources to describe the characteristics of children in kinship living arrangements, or living with relatives in the absence of a parent, and to define recent trends in the pattern of kinship caregiving. Comparisons were made between formal kinship arrangements in which foster care was provided under State auspices, and informal kinship arrangements in which foster care was provided without State intervention. Over 2 million, or 3 percent, of all children in the United States are in kinship care (i.e., a child living in a relative's household without a parent present). Kinship caregiving and the children living in single mother families both appear to be adaptations to family disruption and the decline of the traditional two-parent family.\n\nMarket Barriers to the Development of Pharmacotherapies for the Treatment of Cocaine Abuse and Addiction\n\nThis study analyzed market barriers to the development of drug therapies for substance abuse and addiction, and for cocaine use and addiction in particular. Using market analysis, quantitative modeling, case studies, and industry interviews, the study concluded that the development of a new cocaine pharmacotherapy was not economically viable for the pharmaceutical industry under current conditions. The industry faces three critical market barriers: a small and uncertain market for cocaine addiction and abuse drug therapy, a substance abuse treatment system that limits access to this market, and limited and uncertain payment possibilities.\n\nCHAPTER III\n\nChapter III provides an overview of HHS agency evaluation activities, including information on the evaluation program, summaries of evaluations completed during FY 1997, and evaluations in progress.\n\nAdministration for Children and Families\n\nThe Administration for Children and Families (ACF) administers a broad range of programs to support children, families, and other targeted populations. ACF evaluation objectives include providing information on program design and operations, testing services delivery approaches, conducting policy analyses, and disseminating and applying evaluation results. In FY 1997, evaluations were completed on welfare-to-work strategies; responsible fatherhood programs; a paternity acknowledgment in child support program; the Head Start Program; protective, preventative, and reunification services for children and their families; special needs adoptions; child maltreatment; and the Child Abuse and Neglect Prevention Programs. Evaluations in progress during FY 1997 include State welfare reform programs, employment training programs for welfare recipients, child impact studies related to welfare reform, community services demonstrations, child support programs, various aspects of the Head Start Program, child care and welfare reform, homeless youth programs, and quality of foster care.\n\nAdministration on Aging\n\nThe Administration on Aging (AoA) supports studies that provide information on the implementation of the Older Americans Act of 1992. During FY 1997, AoA initiated a FACE=\"Times New Roman\">n evaluation of the Supportive Services and Senior Centers provisions of the Act (Title III-B), which provides home- and community-based services for all elderly persons, particularly those in great economic or social need. Most home- and community-based services fall under three broad categories: access services, in-home services, and other community-based services. Many State and Area Agencies on Aging also are conducting evaluations of their Older Americans Act service programs.\n\nAgency for Health Care Policy and Research\n\nThe goals of the evaluation program at the Agency for Health Care Policy and Research (AHCPR) are to assess the Agency's effectiveness in meeting major and long-term priorities, to obtain information to respond to critical agency and departmental concerns, and to conduct internal evaluations to improve the efficacy of key program areas. Evaluation projects completed in FY 1997 included assessments of private sector outcomes and effectiveness research, informatics as a tool for patient diagnosis and treatment decisions, quality of care measurement, data on managed care organizations for health services research, and customer satisfaction with AHCPR's research dissemination efforts. Ongoing evaluations during FY 1997 include projects to examine AHCPR's grant application and review processes and survey data collections procedures.\n\nAgency for Toxic Substances and Disease Registry\n\nThe Agency for Toxic Substances and Disease Registry (ATSDR) was created as a Federal agency by the Comprehensive Environmental Response, Compensation, and Liability Act (CERCLA), more commonly known as Superfund legislation. The evaluation program is coordinated with the Agency's strategic planning process and implementation of the Government Performance and Results Act (GPRA). During FY 1997, ATSDR developed an agencywide inventory of evaluation activities, conducted an assessment of its site-specific evaluation needs, and developed a guidance document for State cooperative agreement evaluation activities. ATSDR also initiated an evaluation of its Hazardous Substances Emergency Events Surveillance (HSEES) program to measure the sensitivity and reliability of the system.\n\nCenters for Disease Control and Prevention\n\nThe Centers for Disease Control and Prevention (CDC) places high priority on evaluations seeking to answer policy, program, and strategic planning questions related to the Agency's mission. In FY 1997, CDC's completed projects include major program evaluations, such as the National Laboratory Training Network, the Business Responds to AIDS Program, and suicide interventions for Native American communities. Several data policy/surveillance studies were completed, such as one on the medical certification process for death certificates. CDC also finished a major project on performance measurement to support GPRA implementation. CDC's evaluations in progress include program assessments in such areas as diabetes control, field epidemiology training, teen pregnancy prevention, and sexually-transmitted disease partner notification. Evaluation of data policy and surveillance systems are important at CDC. There are four projects currently underway to develop performance measurement systems, several of which are in the area of human immunodeficiency virus (HIV) prevention programs. CDC also invests it evaluation resources in more specialized assessments, such as evaluating the activities of tuberculosis outreach workers and making recommendations on bicycle helmet use.\n\nFood and Drug Administration\n\nThe evaluation program at the Food and Drug Administration (FDA) continues to be aligned with the agency's policy, program and strategic planning initiatives as well as with the implementation of GPRA. FACE=\"Times New Roman\">FDA managers also are identifying further opportunities to involve their customers in the design and testing of alternative ways of doing business. In FY 1997, FDA continued to identify performance measures jointly with industry customers as part of the drug and biologic review processes necessary for implementation of the Prescription Drug User Fee Act. In addition, FDA completed evaluation projects on mammography quality standards, diagnostic medical devices, and food safety issues. FDA's evaluation projects in progress include continuing studies on the Prescription Drug User Fee Act, mammography quality standards, and the assistance program for small businesses. New studies were initiated on food safety standards and the President's tobacco initiative.\n\nHealth Care Financing Administration\n\nThe Office of Strategic Planning, the research arm of the Health Care Financing Administration (HCFA), performs, coordinates, and supports research and demonstration projects (through intramural studies, contracts, grants, and waivers) to develop and implement new health care financing policies and to provide information on the impact of HCFA's programs. The scope of HCFA's research and demonstration activities embraces all areas of health care: costs, access, quality, service delivery models, and financing approaches. In FY 1997, HCFA completed 20 evaluations on the Medicare Fee Schedule, Medicaid quality of care, effectiveness of the Operation Restore Trust demonstrations, inpatient hospital services, alternative payment of Medicare cataract surgery, ambulatory patient groups, Medicare case management demonstrations, nursing home quality indicators, and health plan report cards. HCFA has over 22 projects in progress, with 13 of them scheduled for completion in FY 1998. Some of the demonstrations involve the Medicaid managed care programs or waivers, Medicaid uninsured demonstrations (particularly the expansion of insurance to children), rural health clinics, Medicare Alzheimer's disease demonstration, use of telemedicine, and long-term care for persons with developmental disabilities.\n\nHealth Resources and Services Administration\n\nThe purposes of the Health Resources and Services Administration (HRSA) evaluation program are to enhance strategic and performance planning, to strengthen budget and legislative development, and to improve program management. Evaluation priorities include developing and supporting performance measurements, assessing program implementation, and conducting crosscutting policy analysis and research. During FY 1997, HRSA completed evaluations related to its mission to eliminate barriers to care, which included assessments of rural applications of telemedicine and living-related kidney transplant operations. Related to HRSA's mission to assure quality of care, HRSA completed a study on human immunodeficiency virus/acquired immunodeficiency syndrome (HIV/AIDS) treatment services and tuberculosis treatment programs in major cities. Another major study assessed the availability of primary care services under Medicaid managed care. Ongoing evaluations in FY 1997 include effectiveness of community health centers, development and refinement of tools for monitoring cultural competence in managed care; the Healthy Start Program to reduce infant mortality; Ryan White CARE Act programs; and development of a comprehensive performance monitoring system for the Bureau of Health Professions.\n\nIndian Health Service\n\nThe evaluation program of the Indian Health Service (IHS) is managed by the Office of Public Health's Staff Office of Planning, Evaluation, and Research. This Office provides national leadership and consultation for IHS and Area Offices on strategic and tactical planning, program evaluation and assessment, public health and medical services, research agendas, and special public health initiatives for the agency. One IHS evaluation completed in FY 1997 focused on the impact of the Alaska Tribal Health Compact on programs and services in Anchorage, Alaska, and another project looked at the prior trauma care of intoxicated patients as a predictor of subsequently fatal injury. Major projects in progress include an examination of the IHS Resource Requirement Methodology, a process for determining needs to deliver adequate care; an assessment of aftercare services provided by IHS regional substance abuse treatment centers; and a study of the IHS capacity for epidemiologic surveillance.\n\nNational Institutes of Health\n\nThe National Institutes of Health (NIH) Evaluation Program provides information to assist the NIH Director and the various Institute, Center, and Division Directors in determining whether NIH goals and objectives are being achieved and to help guide policy development and program direction. In FY 1997, NIH evaluated the intramural research program at the National Institute of Mental Health; the Bridges to the Future Program, a minority student initiative; and the NIH Consensus Conference on physician prescribing patterns for treatment of peptic ulcer disease. Other major evaluations looked at the status of biomedical research facilities; the National Research Service Award Research Training Program; article citation and patent references at the National Heart, Lung and Blood Institute; the rare disease clinical research database; laboratory animal use, facilities, and resources; safe and effective genetic testing; the Physician Data Query database at the National Cancer Institute; research instrumentation needs; toxicology and environmental health information resources; the international cooperative biodiversity groups; and the Federal Interagency Forum on Aging-Related Statistics. Projects in progress include the Surgeon General's Report on Oral Health, an evaluation of the NIH Human Resources Management System, completion of the Study of National Needs for Biomedical Research Personnel, and the biennial Survey of Scientific and Engineering Research Facilities at Colleges and Universities.\n\nOffice of the Assistant Secretary for Planning and Evaluation\n\nThe Assistant Secretary for Planning and Evaluation (ASPE) functions as principal advisor to the Secretary on policy development and conducts evaluations and policy research studies. It is also responsible for HHS-wide coordination of legislative, planning, and evaluation activities. During FY 1997, ASPE's major evaluation projects looked at pharmacotherapies for treating cocaine abuse and addiction; public health laboratories; performance measures for public health, substance abuse, and mental health; insurer response to health care market changes; privacy and health research; State-assisted living policies; HIV/AIDS home- and community-based services; consumer-directed personal assistance services; welfare-to-work program approaches; welfare reform and teenage parents; informal and formal kinship care; child welfare; and domestic violence. Major evaluations in progress include understanding private sector changes; improving data and analytic capability; disability, aging, and long-term care trends; promoting active lifestyles and maximum independence; managed care and people with disabilities; assisted living; moving welfare recipients to work; measuring implementation and outcomes of welfare reform, child support, child and youth development, and teen pregnancy programs.\n\nOffice of Public Health and Science\n\nThe Office of Public Health and Science (OPHS) conducts evaluations to support the Surgeon General and the Assistant Secretary for Health in his dual role as the Nation's top doctor and senior advisor to the Secretary on public health and science matters. OPHS evaluations completed in FY 1997 focused on developing a framework of national health goals and disease prevention objectives in anticipation of the development of the Healthy People 2010 plan; public health expenditures of nine State and local public health, mental health, substance abuse and environmental agencies; and the Office of Minority Health-sponsored program for Historically Black Colleges and Universities. Evaluations and policy studies in progress include dietary supplement labels, school health programs, public health infrastructure, a new model of coordinating and integrating HIV-prevention and primary care services, the Minority Health Resource Center, the Bilingual/Bicultural Service Demonstration Grant Program, and linguistically and culturally appropriate community health promotion programs in local health departments.\n\nSubstance Abuse and Mental Health Services Administration\n\nThe Substance Abuse and Mental Health Services Administration (SAMHSA) is committed to evaluating its programs and grant projects to assess the effectiveness of prevention, treatment, and rehabilitation approaches and system of care. In FY 1997, SAMHSA completed the National Treatment Improvement Evaluation Study, a comprehensive assessment of the results of 157 three-year substance abuse treatment demonstrations, and an assessment of the Job Corps program, which provides enriched substance abuse treatment for adolescents. Also completed were studies on substance abuse prevention services for pregnant and postpartum women and their infants; the National Training System for planning and delivering substance abuse prevention services; the Faculty Development Program to enhance research, teaching, and clinical practices on issues of alcohol, tobacco, and other drugs; and a program for mental health care provider education in HIV/AIDS programs. Evaluations in progress include a services integration experiment for the chronically mentally ill homeless persons; the impact of managed care on access, cost, and quality of substance abuse treatment; an outcome evaluation of the Community Partnership Program; evaluation of substance abuse treatment improvement protocols; the long-term effects of drug abuse treatment; and an evaluation of opioid treatment program accreditation.\n\nAPPENDICES\n\nA complete inventory of HHS evaluations completed in FY 1997 is provided in Appendix A, and HHS Agency projects currently in progress are listed in Appendix B.\n\nThe criteria used by the Evaluation Review Panel to select the reports highlighted in Chapter II are located in Appendix C.\n\nChapter I. Evaluation in the Department of Health and HumanServices\n\nThe mission of the U.S. Department of Health and Human Services (HHS) is to enhance the health and well-being of Americans by providing for effective health and human services and by fostering strong, sustained advances in the sciences underlying medicine, public health, and social services. The following HHS agencies and offices accomplish this goal through their program activities and performance evaluations.\n\nAdministration for Children and Families (ACF)\n\nAdministration on Aging (AoA)\n\nAgency for Health Care Policy and Research (AHCPR)\n\nAgency for Toxic Substances and Disease Registry (ATSDR)\n\nCenters for Disease Control and Prevention (CDC)\n\nFood and Drug Administration (FDA)\n\nHealth Care Financing Administration (HCFA)\n\nHealth Resources and Services Administration (HRSA)\n\nIndian Health Service (IHS)\n\nNational Institutes of Health (NIH)\n\nOffice of the Secretary\n\nSubstance Abuse and Mental Health Services Administration (SAMHSA)\n\nThe Assistant Secretary for Planning and Evaluation (ASPE), located in the Office of the Secretary, coordinates evaluation activities throughout HHS.\n\nEvaluation plays an integral role in carrying out the HHS mission. Assessing various aspects of agency program performance allows staff to identify means of improving that performance. The HHS evaluation function has three goals: (1) to provide information on HHS programs that helps government officials and members of the Congress make decisions related to programs, policies, budgets, and strategic planning; (2) to help HHS managers improve program operations and performance; and (3) to disseminate evaluation results and methodological tools useful to the larger health and human services community of State and local officials, researchers, advocates, and practitioners for improving the performance of their programs. This last goal is very important to HHS. Its agencies have an important obligation to foster the development of new knowledge about the effectiveness of health and human services programs and interventions and evaluation tools for use by the larger health and human services community. Although the findings and recommendations of HHS evaluations are usually first used by the Administration and the Congress, they can also be applied by others in the research and practice communities to improve program performance at the State and community levels. The purpose of this report is to disseminate information about recent HHS evaluations and to make sure that the potential for wider application is realized.\n\nThis Chapter describes the organization and operation of evaluation at HHS. It first provides an overview of the kinds of evaluation activities supported by HHS agencies and then describes the resources and funding mechanisms used to support them. The Chapter details HHS evaluation management, including planning procedures, project management, quality assurance, and dissemination of results. The Chapter concludes with a discussion of effective uses of HHS evaluations, with illustrations from projects completed in the past year.\n\nHHS Evaluation Activities\n\nHHS defines evaluation as the assessment of program performance (efficiency, effectiveness, and responsiveness) through the analysis of data or information collected systematically and ethically, and the effective use of resulting information in program or policy decisionmaking and program management. This definition encompasses a range of evaluation activities, such as those listed below.\n\nOutcome evaluations measure the immediate or intermediate effects of a program with respect to the stated goals or objectives\n\nImpact evaluations look at the broader and long-term results, intended or unintended, of a program on populations or institutions involved.\n\nImplementation or process evaluations assess the nature of program inputs and outputs and their relationship to the stated goals and objectives.\n\nPolicy assessments examine health policies with respect to their development, implementation, or their impact on public health or program activity.\n\nCost-benefit or cost-effectiveness analyses develop methodology and its application to assess the relationship of program results to program costs (direct and indirect), often in comparison with alternative programs.\n\nSurvey data analyses evaluate the results of HHS programs or policies by analyzing data obtained from surveys.\n\nPerformance measurement and data systems identify and test the validity and reliability of process, output, and outcome indicators to measure the performance of programs and develop data systems supporting implementation of the Government Performance and Results Act (GPRA) of 1993.\n\nSimulations and models use computer simulations and modeling techniques to analyze the impact of policy changes on service delivery systems and beneficiaries.\n\nManagement studies examine the effectiveness or efficiency of the administration or operation of HHS programs and offices.\n\nEvaluation syntheses integrate the results from multiple independent evaluation studies within a defined program or policy area in a fashion that improves the accessibility and application of those results.\n\nEvaluation feasibility studies assess the clarity and importance of program goals and objectives, the consensus of program stakeholders on the potential utility of evaluation information, and the availability of relevant performance data before committing to a full-scale program evaluation.\n\nEvaluation design projects procure assistance developing of an evaluation design, measurement tools, or analytic models in preparation for fully implementing an evaluation.\n\nInstrument development projects develop evaluation instruments (design, measurement, or analytic) for a specific HHS program or for general use by the health and human services community.\n\nEvaluation technical assistance helps HHS officials and grantees with any aspect of evaluation planning, project design implementation analysis, or use of results.\n\nEvaluation dissemination identifies target audiences and mechanisms to inform program constituencies and evaluation stakeholders about evaluation results.\n\nEvaluation training/conferences maintain the professional skills and expertise of evaluation staff through training opportunities, and promote the dissemination of HHS evaluations through conferences.\n\nTOP OF DOCUMENT\n\nEvaluation Resources\n\nEvaluation activities of the various HHS agencies are largely supported through two funding mechanisms: direct use of program funds and use of special legislative set-aside authorities for evaluation. The first is a common mechanism by which programs managers have discretionary authority to use appropriated program funds to support contracts that will design, implement, and analyze evaluation data. In some cases, a program's legislative authority calls for a specially mandated evaluation, and program funds are used directly to support the evaluation.\n\nThe second mechanism for evaluation funding is the legislative set-aside authority which permits the Secretary of HHS to use a proportion of overall program funds for evaluation purposes. The largest of such set-aside authorities is one established for evaluations conducted by several agencies of the U.S. Public Health Service (AHCPR, CDC, HRSA, NIH, and SAMHSA), ASPE, and the Office of Public Health and Science in the Office of the Secretary. The mechanism is called the 1-percent evaluation set-aside legislative authority, which is provided for in Section 241 of the Public Health Service (PHS) Act. This authority was established in 1970, when the Congress amended the Act to permit the HHS Secretary to use up to 1 percent of appropriated funds to evaluate authorized programs. Section 241 limits the base from which 1 percent of appropriated funds can be reserved for evaluations of programs authorized by the PHS Act. Excluded are funds appropriated for FDA, IHS(1), and certain other programs that are managed by PHS agencies but not authorized by the Act (e.g., HRSA's Maternal and Child Health Block Grant and CDC's National Institute for Occupational Safety and Health). In FY 1997, HHS invested more than $35 million in 1-percent evaluation funds to carry out evaluation activities related to relevant public health programs. The FY 1997 Labor, HHS, and Education Appropriations Act provided that an additional $96 million in 1-percent evaluation funds be used to support data collection and analysis activities in AHCPR and CDC's National Center for Health Statistics.\n\nEvaluation Management\n\nThe management of HHS evaluations, which are carried out on a regular basis by the HHS agencies and offices and coordinated by ASPE, involves four basic functions:\n\n1. Evaluation planning and coordination\n\n2. Project management\n\n3. Quality assurance\n\n4. Dissemination of evaluation reports.\n\nA description of each function in general terms follows. Additional information on the individual HHS agencies, ASPE, and OPHS evaluation functions is found in Chapter III.\n\nEvaluation Planning and Coordination\n\nThe HHS agencies, ASPE, the Office of the Inspector General (OIG), and OPHS develop evaluation plans annually in concert with HHS's program planning, legislative development, and budgeting cycles. Plan development is coordinated by ASPE. Each agency or office plan generally states the evaluation priorities, or projects under consideration for implementation. Typically, HHS evaluation priorities include congressionally mandated program evaluations, evaluations of Secretarial program or policy initiatives, and assessments of new programs, programs that are candidates for reauthorization, or programs for which key budget decisions are anticipated.\n\nMore specifically, HHS evaluation planning activities involve preparing two reports to the Congress. First, those agencies and offices that use the PHS 1-percent evaluation set-aside authority--AHCPR, CDC, HRSA, NIH, ASPE, OPHS, SAMHSA--submit a formal plan to ASPE, which coordinates and develops the individual plans into the HHS report to the Congress on the use of the PHS 1-percent authority. This report must be submitted to the Congress before HHS can implement the plan.\n\nSecond, the Congress requests that HHS coordinate all of its research, demonstration, and evaluation programs to ensure that the results of these projects address HHS program goals and objectives. ASPE and the Assistant Secretary for Management and Budget work together with HHS agencies to provide the Congress with a special annual research, demonstration, and evaluation budget plan that coincides with the preparation of the President's fiscal year budget. The plan outlines each HHS agency's research, demonstration, and evaluation priorities as related to its strategic goals and objectives.\n\nProject Management\n\nThe design and management of evaluation projects at HHS is principally decentralized--the HHS agencies, OIG, and ASPE all are responsible for executing annual evaluation plans, developing evaluation contracts, and disseminating and applying evaluation results. Even within agencies-- while there is some oversight responsibility and execution capability in the Office of the Director or Administrator--the various subunits (centers, institutes, and bureaus) conduct much of the day-to-day evaluation activity.\n\nThe OIG performs independent evaluations through its Office of Evaluations and Inspections (OEI). The OEI's mission is to improve HHS programs by conducting inspections that provide timely, useful, and reliable information and advice to decisionmakers. This information (findings of deficiencies or vulnerabilities and recommendations for corrective action) is usually disseminated through inspection reports issued by the Inspector General. A summary of individual inspection reports and other OIG reports can be viewed on the Internet at http://www.sbaonline.sba.gov/ignet. OEI also provides technical assistance to HHS agencies in conducting their evaluations.\n\nQuality Assurance\n\nMost evaluation projects are developed at the program level. The initial quality review is generally conducted by a committee of agency- or office-level policy and planning staff members. Before a project is approved, however, it is also reviewed for technical quality by a second committee with expertise in evaluation methodology. Technical review committees follow a set of criteria for quality evaluation practice established by each agency. Some HHS agencies also have external evaluation review committees composed of evaluation researchers and policy experts from universities and research centers. More details on the quality assurance procedures for the various HHS agencies, ASPE, and OPHS are presented in Chapter III.\n\nDissemination of Evaluation Reports\n\nMaintaining report libraries and distributing information on evaluation results is an important component of HHS evaluation management. Project information and reports are continuously submitted to the HHS Policy Information Center (PIC)--the departmental evaluation data base and library maintained by ASPE. As an information data base and library resource, the PIC contains nearly 7,000 completed and in-progress evaluation and policy research studies conducted by the Department, as well as key studies completed outside of HHS by the U.S. General Accounting Office (GAO) and private foundations.\n\nTypically, the results of HHS evaluations are disseminated through targeted distribution of final reports, articles in refereed journals, and presentations at professional meetings and conferences. Although individual HHS agencies have primary responsibility for disseminating results, there is a departmentwide effort under way to expand dissemination to the larger research and practice communities through centralized computer communications and publications. Abstracts of all studies maintained in the PIC evaluation data base are now accessible on the Internet at http://www.os.HHS.gov. Information is available on completed projects, including the name and telephone number of an HHS official responsible for the project.\n\nIn addition, HHS widely distributes copies of the annual evaluation report series, beginning with Performance Improvement 1995 and continuing to this report, through the HHS website. For convenience, these reports can be accessed on the Internet at: http://aspe.os.HHS.gov/PIC/gate2pic.htm.\n\nEnsuring Effective Use of Evaluation Results\n\nHHS is committed to ensuring that evaluations yield a high return on the investment of available program funds. Today, the Department's evaluation resources are used in several ways related to strategic planning, program, and policy development. Performance measurement and data systems are the primary mechanisms used to monitor progress in achieving departmental goals and objectives as well as specific program outcomes. Effectiveness of programs and strategies are in-depth evaluation studies to understand how HHS strategies and programs are linked to performance goals and objectives. Environmental assessments are prospective evaluation studies that assess how changes in the larger society affect the Department's programs and strategies. Program management and support evaluations are used to improve the management of health and social service programs and the quality of the Department's evaluation efforts. Each of these four uses, with examples of current HHS evaluations, is described below.\n\nPerformance Measurement and Data Systems\n\nImplementation of the HHS strategic plan and the performance plans of the PHS agencies, pursuant to the Government Performance and Results Act (GPRA) of 1993, requires the Department to focus a portion of its evaluation activities on the development of performance objectives and measures and information systems necessary to produce the data needed to assess progress toward achieving its goals. During FY 1997, evaluation priorities of the individual HHS agencies included a number of projects serving this purpose. For example, ACF completed a project for Head Start Programs to develop outcome performance measures (e.g., how these children benefit from the program), in addition to process measures (e.g., how many teachers have the appropriate credentials?) (6693). FDA completed its fifth and final report on performance data for the Prescription Drug User Fee Act (PDUFA), documenting the improvements obtained in the last few years in speeding up the process for drug reviews and approvals (6079.2). ASPE produced a report synthesizing the state-of-the-art performance measurement for public health and developed recommendations for measures that could be used to monitor Federal-State performance partnership grants being considered for the future (6177). An example of a major survey providing nationally representative data is one completed last year by HRSA on the users of community health centers (CHC) and the services they were provided (5737). It produced, for the first time, nationally representative data on CHC users that is comparable to similar data on health care use from the National Health Interview Survey. The data are now being analyzed to address questions about users' race and ethnicity, health risk behaviors, most prevalent diagnoses, and services used.\n\nEffectiveness of Programs and Strategies\n\nImplementation of the Department's strategic and performance plans also requires evaluative information on how well the programs and strategies are working. Most of the HHS agency evaluations completed in FY 1997 provide this information. For example, HCFA submitted a report to the Congress on the quality of care in the Medicaid program (6302). It assessed the variations in the rate of performance of selected treatments and procedures on Medicaid beneficiaries for small areas within and between the States. Medical records in a sample of hospitals were also examined for three conditions: pediatric asthma, hysterectomy, and complicated delivery. Overall, the report concluded that care was considered adequate and comparable to that received by privately insured patients. Another example of a focused program effectiveness study is one conducted by HRSA on rural applications of telemedicine (5749). The project included a mail survey of all non-Federal hospitals located outside metropolitan areas in the summer of 1995, plus a short survey for hospitals doing only teleradiology and a longer survey for hospitals applying telemedicine to purposes beyond radiology. The study demonstrated that although rural telemedicine is in the earliest stages of development, it is expanding rapidly. HRSA is using the results to design a common telemedicine evaluation instrument for HRSA grantees and for consideration by other Federal sponsors, as well as in shaping the Rural Telemedicine Grant Program. In terms of evaluating research programs, NIH conducted an evaluation of the NIH Consensus Conference as an effective mechanism for translating research into practice (6284).\n\nEnvironmental Assessment\n\nThe Department's strategic plan acknowledges that achievement of its goals and objectives is contingent on many external factors that are beyond the Department's control. For example, the managed care revolution is having a significant impact on access to services and health outcomes for those populations traditionally served by the public health system. Understanding the impact of these environmental forces on public health programs and the customers they serve is essential for assessing and adjusting the Department's goals and strategies in the future. A number of evaluations in FY 1997 addressed these environmental issues.\n\nFor example, AHCPR conducted a study to help clarify its future priorities in outcomes and effectiveness research and to provide insights regarding future opportunities for public-private partnerships in this area (6385). The study found that many private sector organizations conduct effectiveness research, often to monitor health care performance against accreditation standards. AHCPR will also use the findings to help guide its support to developing and validating effectiveness and outcome research tools used by the private sector. In another example, ASPE last year analyzed market barriers to the development of pharmacotherapies for substance abuse and addiction, particularly for abuse of and addiction to cocaine (6694). This report, featured in Chapter II, found several critical market barriers that must be taken into account in future HHS program and policy development. Since the substance abuse treatment market relies heavily upon State and Federal reimbursement, most substance abuse treatment services are subsumed under the mental health benefits of entitlement programs. Drug companies are reluctant to rely upon this kind of reimbursement in an age of shrinking budgets for mental health services.\n\nProgram Management and Support\n\nEffective management of programs that achieve departmental goals and objectives is essential to the success of those programs. In FY 1997, the evaluations completed and in progress include a number of priorities devoted to assessing and improving the management of health and social service programs. For example, HCFA last year looked at the implementation of Ambulatory Patient Groups (APGs)--the Medicaid outpatient prospective payment system that groups patients for payment purposes rather than paying on a cost basis (6320). In this case study of Iowa's implementation of the APG system and an analysis of the reimbursement methodology, success was reported in reducing outpatient costs, where that was the immediate goal, and the system encouraged higher-cost facilities to reduce costs and reward lower-cost facilities. In another example of studies to improve program operations, OPHS conducted a Healthy People 2000 stakeholder study to help plan and prepare for the Healthy People 2010 goals and objectives (6491), also featured in Chapter II. This evaluation reviewed the successes and failures of the Healthy People 2000 benchmarks, with the hope of making this national framework of performance indicators on health status more results-oriented. The focus group participants recommended new communication avenues to make Healthy People 2010 available to more professionals and community leaders.\n\nChapter II. Highlights of Selected Evaluations Completed During Fiscal Year 1997\n\nIn this Chapter, the U.S. Department of Health and Human Services (HHS) highlights evaluations of general interest to the public health, health care services, and human services community and illustrates the diversity of HHS evaluations completed in fiscal year (FY) 1997. Included are summaries of 9 evaluation projects selected by the Evaluation Review Panel--on the basis of criteria identified in Appendix C--and applied to 28 reports nominated by HHS agencies. These criteria are as follows:\n\nIs the report important? Does it address a significant issue or problem for which evaluation would help confirm or change program direction, or measure program impact? Are the findings likely to be useful and generalizable?\n\nIs the report methodologically sound? Are its concepts, designs, data collection, and analyses conducted and reported in a competent manner?\n\nIs the report faithful to the data? Do the conclusions and recommendations logically follow from the data and analyses, and are they relevant to the questions asked?\n\nThe nine studies are organized under three headings: performance measurement or assessment, program management and development, and policy analysis and development. These headings represent the three most common uses of HHS evaluation resources. Performance measurement or assessment is a high priority for HHS agencies as the development, implementation, and refinement of programs are more results oriented in the 1990's and are required under the Government Performance and Results Act (GPRA) of 1993. Program management and development reflects the kind of evaluation projects that program managers initiate to obtain information or data that will help them manage a program efficiently and ensure successful results. Policy analysis and development includes the evaluation projects conducted by HHS agencies to examine the impact of alternative policies, either in the past or in the future, on the future direction of HHS programs or services.\n\nEach summary includes a brief abstract; a description of the study, including its purpose, background, methods, findings, and use of results; the names of any publications that resulted; and the name and phone number of the HHS official to contact for additional information.\n\nPerformance Measurement and Assessment\n\nLEAP: Final Report on Ohio's Welfare Initiative to Improve School Attendance Among Teenage Parents--Ohio's Learning,Earning, and Parenting Program\n\nHighlights\n\nThe goal of Ohio's Learning, Earning, and Parenting (LEAP) Program was to use financial incentives to increase the proportion of pregnant teenagers and teen parents on welfare who graduate from high school or receive a General Educational Development (GED) certificate, find jobs, and ultimately achieve self-sufficiency. The teens are recipients of cash assistance, formerly under the Aid to Families with Dependent Children (AFDC) program and more recently under the Temporary Assistance for Needy Families program. This report is the final product in a multi-year evaluation of LEAP. Immediate goals were to induce dropouts to return to high school or to enroll in GED programs, and to promote better attendance among those already enrolled when they were called into LEAP. These short-term goals were accomplished--statistically significant increases in school enrollment and attendance were observed. Longer-term goals were to increase the rate of high school graduation, GED attainment, and teen employment, and to reduce welfare payments. These long-term goals were only partly successful. The LEAP program did not increase high school completion rates except for teens who were initially enrolled in school in the city of Cleveland. LEAP increased employment among those initially enrolled in school but not among those not initially enrolled. There were initial impacts on earnings for the initially enrolled group, but earnings were matched by the control group after two years. Welfare payments fell by more than the cost of the LEAP Program. The evaluation of LEAP is timely in light of new changes to national welfare law, which increases emphasis on school attendance by those who have not completed high school (or its equivalent) and on more employment-oriented activities for teens and other welfare recipients.\n\nPurpose\n\nThis study concluded a multi-year evaluation of LEAP's effect on school enrollment, attendance, employment, earnings, and reduction of dependency on welfare among pregnant teenagers and teen parents who receive AFDC. This report is based on a 4-year followup that traced the cumulative impact of LEAP on these school- and work-related outcomes in Ohio.\n\nBackground\n\nFamilies started by women who first gave birth as teenagers account for approximately 50 percent of all long-term AFDC recipients. The enactment of the Personal Responsibility and Work Opportunity Reconciliation Act (PRWORA) of 1996 replaced the entitlement to AFDC with block grants to States. Under the law, unmarried custodial minor parents who have not graduated from high school or received a GED certificate may not receive federally funded welfare assistance unless they attend high school or a program that prepares them to earn an alternative education or training credential. Prior to its enactment, however, many States had already implemented some form of school requirement for teen parents on welfare. The LEAP Program in Ohio, initiated in 1989, was one of the first to mandate school attendance for the entire teen parent welfare caseload and to include a rigorous program evaluation. It was hoped that LEAP's direct effects on enrollment and attendance would translate into improvement in other outcomes, such as greater progress in school, more receipt of high school diplomas and GEDs, increased employment, and reduced welfare dependence.\n\nMethods\n\nThe evaluation of LEAP uses a random assignment research design and multiple data sources. For this report, 12 Ohio counties were sampled to provide data on 4,151 teens, of whom 3,479 were randomly assigned as program group members, and 672 were assigned as control group members. Teens in the program group who met LEAP's requirements had their welfare checks increased by $62 for school enrollment and an additional $62 for each month of acceptable school attendance. Teens who did not comply had $62 deducted from their welfare grant every month until they complied with program rules. During 1992, a teen living on her own with one child was eligible for a monthly AFDC grant of $274--the bonus or sanction of $62 was a significant amount.\n\nA survey was administered to 1,188 teens 1 year after entry to assess school enrollment status and experience with LEAP. Selected county case files were reviewed to study LEAP's implementation and the bonus/sanction process. A survey of 913 teens in 7 counties was made 3 years after random assignment to measure education, employment, and welfare outcomes. Data on employment earnings and welfare dependency were drawn from unemployment and AFDC records, respectively.\n\nFindings\n\nA statistically significant increase in teen parent enrollment and attendance in school or in a GED program was achieved. Statistically significant increases were also observed in the completion of the 9th, 10th, and 11th grades. However, the longer-term goal of high school completion or its equivalent was not achieved for the full sample. The program increased GED certificate receipt, but not high school graduation rates, among the subgroup of teens who were enrolled in school when they entered LEAP. No statistically significant differences were found in GED certificate receipt or high school graduation among teens who were not enrolled in school when they entered LEAP. High school graduation rates increased only in Cleveland, which may be attributable to an earlier and enhanced LEAP pilot program in the city Cleveland.\n\nBoth the program and control groups of teens experienced substantial growth in employment rates and earnings during the 4-year followup period, with quarterly employment rates increasing from about 15 percent to about 40 percent. The LEAP program increased employment for initially enrolled teens, but not for their control group counterparts during the followup years. Earnings of the initially-enrolled students also increased during the first 2 years of followup, but were ultimately matched by earnings in the control group of teens. Rates of AFDC receipt remained high throughout the followup period (more than 60 percent of all teens were on welfare), but declined from 100 percent at the time of random assignment to 60.9 percent in the last quarter of the fourth year. Program group members were somewhat less likely to receive AFDC than were control group members in at least 1 month in most quarters. LEAP reduced both the number of teens receiving AFDC and the amount of AFDC received, with more of the reduction found among those who were initially enrolled in school. Although LEAP was deemed cost-effective, the size and duration of the effects on employment and AFDC receipts may have been suppressed by LEAP's disappointing effect on school completion.\n\nRecommendations for improved policy and program design focused on ensuring case management and well-designed management information systems, improving teens' understanding of the rules and expectations, intervening early to prevent teens from dropping out of school, addressing teens' reservations about school attendance, rewarding academic progress and school completion, and integrating more work opportunities into the program.\n\nUse of Results\n\nThis study provides a useful State model for operating new national welfare rules. It suggests that such programs may need to be complemented by incentives to reward academic progress and school completion in addition to school enrollment and attendance. The education message, however, may need to be combined with more employment-oriented activities in order to meet the more work-oriented expectations and time-limited benefit rules that were mandated in 1996. Programs may be more effective if they take into account the specific needs of distinct groups of teen parents. What works for a 17-year-old who is still in school and can complete within 1 year may not work for a 19-year-old who has been out of school for 3 years and has two children. Providing more tailored services and incentives that are meaningful to specific groups may be necessary to improve upon the outcomes observed under LEAP and to attain improvements in the longer term outcomes for these young families.\n\nAGENCY SPONSOR: Administration for Children and Families, Office of Planning, Research, and Evaluation\n\nFEDERAL CONTACT: Nancy Campbell\n\nPHONE NUMBER: (202) 401-5760\n\nPIC ID: 6668\n\nPERFORMER ORGANIZATION: Manpower Demonstration Research Corporation, New York, NY\n\nNTIES: National Treatment Improvement Evaluation Study\n\nHighlights\n\nThe congressionally mandated 5-year study National Treatment Improvement Evaluation Study (NTIES) examined the impact of three federally-sponsored drug and alcohol treatment demonstration programs. The study found that the demonstration programs significantly reduced drug and alcohol use, and that treatment had lasting benefits. Clients reported increases in employment and income, improvements in mental and physical health, decreases in criminal activities and homelessness, and modification of sexually-risky behavior.\n\nPurpose\n\nIn addition to profiling the services, programs, and clients involved in three demonstration programs funded by the Center for Substance Abuse Treatment (CSAT), the study was designed to address the effects of comprehensive treatment on client access to services and on patient outcomes. Lessons learned about the implementation and cost of delivering comprehensive drug treatment services were to be documented.\n\nBackground\n\nA comprehensive treatment model assumes that positive and well-sustained outcomes occur when substance abuse treatment systems draw from a full array of medical and rehabilitative services. These services can include primary medical care, psychiatric services, health education, HIV/AIDS counseling, psychological counseling, legal assistance, social and welfare services, spiritual counseling, job training, educational counseling, social activities, and continuing care services. To demonstrate the value of such a comprehensive substance abuse treatment model, CSAT initiated three major demonstration projects to enhance treatment services by building on an existing infrastructure for substance abuse treatment. The three demonstration programs were Cooperative Agreement for Drug Abuse Treatment in Large Cities, Model Comprehensive Treatment Programs for Critical Populations (with special components for adolescent/juvenile justice and public housing), and Model Drug Abuse Treatment Programs for Criminal Justice Populations (for nonincarcerated offenders and those in correctional settings). Recipients of demonstration funding were required to initiate some form of local self-study and to participate in a multisite evaluation (the NTIES) to capture and disseminate lessons learned from these programs.\n\nMethods\n\nNTIES employed a two-level study design to assess the effectiveness of the demonstration programs and a comprehensive treatment approach. The first level was a multiphase survey that addressed treatment unit administration, including treatment orientation, size, budget, and staffing distribution for each of the demonstration programs. The second level involved collecting data on clinical outcomes from clients enrolled in drug treatment at a subsample of the funded sites. This level included three waves of longitudinal data collection. Clients were interviewed at admission to treatment, when they left treatment, and at a followup 1 year after treatment. A total of 6,593 clients at 78 service delivery units were enrolled in the clinical outcomes study. More than 82 percent of the recruited sample completed the followup interview. The outcomes analyses focused on the 4,411 clients for whom followup data were available, along with either an interview at treatment exit or a patient records abstract form.\n\nNTIES, like many other major research surveys, relied primarily on self-reported data. To validate self-reports regarding illicit drug use and arrests, the study also collected data on urine sample drug testing and arrest records. A pre- and post-panel design was used. This method compares behaviors or other individual characteristics in the same research subjects, measured in similar ways before and after intervention. The findings are expressed as percentage of the occurrence of a behavior or circumstances in the NTIES population during clearly defined intervals.\n\nFindings\n\nThe study generally found significant reductions in substance abuse during and immediately following treatment. Clients served by the demonstration programs were able to reduce their drug use by about 50 percent for as long as 1 year after leaving treatment. The NTIES data indicate that substance abuse treatment can play a major role in crime reduction. Respondents reported significant decreases in multiple indicators of criminal involvement. The study found that self-reported outcome data on arrests were similar to what might be seen if official records were used, especially for clients interviewed in the community rather than in jails or prisons.\n\nNTIES also explored whether specific patient or treatment unit characteristics could explain variations in pre- and post-treatment outcomes. It found that drug and alcohol use, criminal activity, and employment outcomes were measurably better among individuals who completed their treatment, received more intensive treatment, and were treated longer. The results showed that the length of stay varied across gender, race and ethnicity, and age groups within each type of treatment.\n\nNearly all clients were involved in the development of their own treatment plans. About 38 percent of the clients said that they received HIV tests, 57 percent received TB tests, and 72 percent received AIDS prevention classes or counseling. Although almost half of the clients needed legal or housing services, only 10 percent were receiving these services. The multivariate analysis showed a significant relationship between the duration of services provided and client satisfaction with the treatment program.\n\nTreatment was determined to be less costly than incarceration. Treatment costs ranged from $1,800 to almost $6,800 per client, while the American Correctional Association estimates the annual cost of incarceration at $18,330.\n\nUse of Results\n\nThe study documents that comprehensive and focused substance abuse treatment can make a difference in patient outcomes. The evidence is compelling that the nature and duration of treatment has a positive impact on clinical and employment outcomes and on rates of crime and incarceration. The lower cost of substance abuse treatment (compared to that of incarceration) could have significant implications for the allocation of resources among competing public program priorities.\n\nAGENCY SPONSOR: Substance Abuse and Mental Health Services Administration, Center for Substance Abuse Treatment\n\nFEDERAL CONTACT: Ron Smith\n\nPHONE NUMBER: (301) 443-7730\n\nPIC ID: 5346.1\n\nPERFORMER ORGANIZATION: National Opinion Research Center (NORC), Chicago, IL; and Research Triangle Park, Chapel Hill, NC\n\nProgram Management and Development\n\nSelf-Sufficiency Project Implementation Manual: Lessons Learned from Eight Years of Office of Community Services Demonstration Partnership Programs\n\nHighlights\n\nThis manual summarizes lessons learned during eight years of the Demonstration Partnership Program (DPP) projects. DPP has been developing innovative approaches toward increasing the self-sufficiency of the poor, testing and evaluating these approaches, and encouraging their replication through dissemination of project results and findings. The DPP projects were designed to strengthen the ability of grantees to integrate, coordinate, and redirect activities through community partnerships that promote maximum self-sufficiency among low-income individuals and families who rely on or who are at risk of relying on public assistance. These projects were divided into five program areas and project types: (1) case management, (2) micro-enterprise development, (3) minority males, (4) homelessness, and (5) youth at risk . Generic models for establishing effective community-based programs are presented, and materials for program evaluation are elaborated. For each of the five areas, general and specific lessons learned are presented. A project implementation manual was designed as a step-by-step guide to the successful design and implementation of Self-Sufficiency Projects by Community Action Agencies (CAAs), community-based organizations, and local community program planners.\n\nPurpose\n\nThe DPP was designed to permit CAAs to implement and assess innovative approaches toward increasing the self-sufficiency of the poor, including individuals and families who rely on, or are at risk of relying on, public assistance. The purpose of the Self-Sufficiency Project Implementation Manual was to disseminate evaluation findings and information about best practices in the DPP in order to guide and facilitate the future development of each of the five types of self-sufficiency projects supported by the program.\n\nBackground\n\nCAAs have been the principal field organizations in the war on poverty for three decades. Since 1981, they have been the primary recipients of Community Services Block Grant funds, dealing with the problems of poverty and attempting to bring poor people up to decent standards of living in economically healthy communities. With the advent of funding for DPP in 1986, CAAs were encouraged by the Congress to add a new dimension to their activities by forming partnerships with public and private entities in their communities and seeking innovative approaches to community revitalization and the problems of poverty. Avoidance of dependency, development of new ways to improve the capabilities of the poor, and overcoming the barriers to workforce entry were central values of the DPP projects. The CAAs have typically served for many years as successful advocates for the poor, forging special bonds with poor people in their communities. As a result of these bonds, CAAs serve as important generators of innovative ideas. The linkage of CAAs with the DPP is intended to build on these special bonds and innovative ideas, and to join them with community partners to develop and experiment with new ways of increasing the self-sufficiency of the poor.\n\nMethods\n\nDevelopment of the implementation manual involved three steps. First, \"best practices\" were identified by Project Directors and Project Evaluators at a 1994 DPP Reporting Out Conference. These individuals, who were involved in projects funded in 1991 and concluded in 1993, were asked to reflect on their experiences. The evaluators then conducted a more in-depth assessment of what had been learned by the Office of Community Services (OCS) from these projects. In the second step, five team leaders reviewed DPP monographs for each of the five program areas and project types (case management, micro-enterprise development, minority males, homelessness, and youth at risk). The team leaders used quantitative and qualitative information to identify lessons learned and to articulate logic models based on project results and evaluation findings for each program area. In the third step, intensive inputs were obtained from small panels made up of Project Directors and Project Evaluators. These individuals were then convened in focus groups to review and discuss concepts. Team leaders then modified the lessons learned and the logic model based on the discussion. A draft implementation manual was reviewed by experts, project staff, and OCS/DPP personnel.\n\nThe results of the study are presented as \"lessons learned\" in each of the five program and project areas, as well as general lessons that cut across different projects. The Implementation Manual is intended as a brief, step-by-step guide to successful design and implementation of DPP projects. The manual is one of a series of publications developed by OCS to provide technical assistance to CAAs and other grantees. References are provided to other documents in the series for more detailed discussion of evaluation methods, as well as specific project descriptions and evaluation monographs from successful programs.\n\nFindings\n\nThe findings indicate that each project is unique because of different environmental factors that affect implementation. Some lessons, however, are generalizable. The findings demonstrate that it is prudent for project directors and managers to learn from prior DPP projects, from literature on promoting self-sufficiency, and from the CAAs' project experiences.\n\nIn each of the five project types, the complexities and difficulties of project management are elaborated. This includes basic recommendations, such as the importance of focusing on clients, partner relationships, and staff support. Client focus is essential to ensure that project elements are appropriate for their needs, status, and stage of development toward self-sufficiency. Relationships with partners require particular attention to establish and maintain operational communications and coordinated actions within a noncompetitive environment of cooperation. Staff members are also crucial to success--the selection and professional development of staff should be a well-planned aspect of project management, and communication among staff at all levels cannot be neglected.\n\nFindings also show that each stage of a project requires special attention to ensure success. At startup, emphasis needs to be placed on the physical environment for the safety and convenience of clients and staff. The political environment and issues of competition will require attention, as does the exploration of relationships with other agencies and programs serving the same client group. Although many project managers are tempted to think of evaluation in a post hoc manner, successful evaluation is based, in large part, on careful planning and preparation during the startup phase of a project. Key outcomes need to be framed from the outset in measurable terms.\n\nDuring the initial operations stage of the project, the design assumptions should be tested and may require adjustment of the model. Client and program operator expectations should be reviewed and tested early in implementation. Organizational management may require special attention as the program structure and staff management issues are being worked out. Findings also point to the importance of beginning to collect evaluation data at this stage.\n\nDuring the ongoing operations stage, program operators are reminded to be flexible, as past experiences do not necessarily dictate present needs. Staff nurturing becomes increasingly important as the excitement of startup wears off, staff burnout becomes a problem, and additional time is needed to recruit, train, and support volunteers. Evaluation begins to play a greater role as data become available for monitoring and feedback.\n\nAt all stages, readers of the Implementation Manual are cautioned not to overlook the complexity of self-sufficiency programs. Logic models are presented for each of the five project types, providing a graphic illustration of the linkages among project assumptions, implementation strategies, and project outcomes, and demonstrating the importance of considering these linkages when managing projects.\n\nUse of Results\n\nExperience-based and empirically supported lessons learned from demonstration projects can be valuable tools for helping to ensure the success of future programs seeking to help low-income families move toward self-sufficiency. The DPP projects offer well grounded insights about the challenges of effective partnerships, about the importance of understanding the logic of the causal relationships between program interventions and expected outcomes and program goals, and about the many complex interactions among program components and program staff. The implementation manual explores the administrative requirements and challenges of successive stages of a project and the critical nature of staff and volunteer training and support, and the profiles of successful self-sufficiency projects attest to the powerful role of evaluation in establishing proper monitoring and feedback loops and quantifiable outcome measures. The manual has already proved to be a valuable tool for staff training in a variety of social agencies, and is being used as a text in at least one university graduate school. It can provide a critical reality check for legislators and policymakers as they strive to design laws and programs that will use scarce resources more effectively to foster self-sufficiency and build sustaining capacity within low-income families and communities.\n\nPUBLICATION: The full report is available from the U.S. Superintendent of Documents, Stock Number 017-090-000-84-4\n\nAGENCY SPONSOR: Administration for Children and Families, Office of Community Services\n\nFEDERAL CONTACT: Richard Saul\n\nPHONE NUMBER: (202) 401-9341\n\nPIC ID: 4336.4\n\nPERFORMER ORGANIZATION: BHM International, Inc., Silver Spring, MD\n\nEffects of Information Tools and Decision Aids to Help Patient Decision-making About Medical Screening and Treatment\n\nHighlights\n\nThis study identified, evaluated, and synthesized research on the effectiveness of tools intended to inform patients about their medical choices, their treatment alternatives, and the risks and benefits of those alternatives. These tools include interactive computer disks, videotapes, audiotapes, brochures, and computer-generated fact sheets to help patients make decisions about medical screening and treatment. The research found that although some studies do point to the promise these tools hold for helping patients and consumers make informed decisions, few controlled and comparative studies have been done. Specifically, only a few treatment choices have been examined, and most studies have looked more at the effects of the tools on patient knowledge/satisfaction and less at the effects of the tools on patient-clinician communication and health behavior/outcomes. Future studies need to be better grounded in theory and draw upon the empirical evidence of other relevant disciplines. Four areas have been identified as priorities for future research. These include an assessment of the effects of informatics tools on a wider range of outcomes, identification of factors that influence patient use of these tools, assessment of the effects of tools on patient/clinician communication, and analysis of the cost effectiveness of different types of patient informatics tools.\n\nPurpose\n\nThe purpose of this study was to identify, evaluate, and synthesize research on the effects of informatics tools and decision aids on patient decisions about medical screening and treatment. The study assessed the findings, the quality of the research, recommendations for research design, and priorities for future investigation.\n\nBackground\n\nHealth information and medical care organizations are investing in the development and dissemination of health informatics tools to help patients and consumers make decisions about screening and treatment. These tools provide treatment- and disease-specific health information to patients, especially when they are facing choices about the ways to treat and manage their health conditions. Information about the risks and benefits of making alternative choices is often included in these tools. An example might be the choice between \"watchful waiting\" and surgery as responses to low back pain or for benign prostatic hyperplasia. Although patient health informatics tools have the potential to empower patients to make more informed choices, there is limited empirical evidence of their value. Many studies have been small and exploratory, and their research designs have not always employed rigorous controls, adequate sample size, or standardized measurements. Further, it is difficult to ascertain what the tools' effects have been on patient outcomes or health care costs. This study attempted to analyze the literature to see if there were any answers to these questions.\n\nMethods\n\nThe study reviews both computerized and noncomputerized informatics tools and decision aids, including such media as interactive computer disks, videotapes, audiotapes, brochures, and computer-generated fact sheets. Experts and researchers in the United States, Canada, and the United Kingdom were consulted to make sure that the study captured as much of the relevant existing research as possible, and that the analysis of it was systematic and comprehensive. Both the published and unpublished literature about the effects of patient informatics tools and decision aids were reviewed, with particular attention paid to findings and the research quality.\n\nFindings\n\nOnly 30 controlled studies were identified for evaluation and synthesis in this area; the study presents detailed comparative information on these studies. Some of the more substantive findings suggest that the use of informatics tools can increase patient knowledge of treatment alternatives; further patients report that they like the tools. Some studies report effects on treatment choice, but the number of underlying health conditions and range of tools studied are quite limited. Little is known about the operational and background factors that may influence patient use of informatics tools, such as the requirement for a second visit to use them.\n\nAlthough patients may tend to ask their physicians more questions as a result of using the tools, their effects on broader patient/clinician communication patterns or on time burdens imposed on clinical staff have not been studied. Some studies have shown that information tools can increase patient adherence to prescribed medical regimen and influence patient preference for nonsurgical interventions. The body of literature would improve if future studies assess a wider range of treatment choices, describe the context of implementation, and document the effects of information tools on patient attitudes, treatment selection, health behaviors and outcomes, and costs.\n\nMuch work remains to be done to better understand patient health informatics and to ensure that future tools will be developed and supported on the basis of sound health services research. There are many issues to be addressed. Only a few treatment choices have been examined, and few studies have investigated the effects of tools on patient/clinician communications and health behavior/outcomes. There are no comparative studies of the cost and effectiveness of different types of tools, particularly in the case of computerized versus noncomputerized tools. The studies are of varying methodological quality and most have been exploratory. The cost and clinical significance of informatics tools need to be clearly established by more rigorous research designs, including use of randomized control groups, adequate sample sizes, a wider range of measurable effects, and standard outcome measures.\n\nFuture research priorities include 1) identifying factors that promote the use of information tools, 2) assessing the effects of tools on the nature and content of patient/clinician communications and related clinician workload; 3) assessing the effects of tools on health outcomes and health behavior, including quality of life; and 4) comparing and examining the cost effectiveness of different types of patient informatics tools for specified objectives.\n\nUse of Results\n\nThis study points to the need to conduct systematic and rigorous research on the use of informatics tools at a time when investments in these tools are being made without sufficient knowledge about their effect on patient decisionmaking, health outcomes, and related costs. Suggestions are made for upgrading the quality of future studies to methodological standards found in health services and clinical research.\n\nBy organizing information about the state of our knowledge base, the report is intended to help decisionmakers better understand what is known about consumer/patient health informatics tools. Researchers from all fields can use the report to develop future research projects, and especially to identify specific variables that need further investigation. Practical information on how to reach the developers of some of the existing tools is included as well.\n\nAGENCY SPONSOR: Agency for Health Care Policy and Research, Center for Organization and Delivery Studies\n\nFEDERAL CONTACT: Denise Dougherty\n\nPHONE NUMBER: (301) 594-1321\n\nPIC ID: 6376\n\nPERFORMER ORGANIZATION: Research Triangle Institute, Research Triangle Park, NC\n\nSyphilis in the South: A Case Study Assessment in Eight Southern Communities\n\nHighlights\n\nThis project provided a comparative case study of local-level syphilis prevention efforts in eight communities in Alabama, Mississippi, South Carolina, and Tennessee. The project focused on groups perceived to be at high risk of becoming infected with syphilis, the extent to which public health activities target such groups, and identification of factors that affect the reach of services to these groups. African Americans were found to be at greatest risk of syphilis infection. The report highlights the cultural, programmatic, and political barriers that restrict the prevention and control of syphilis and other sexually transmitted diseases (STDs) for those at greatest risk. Cultural barriers include restrictive local norms about public discussion of human sexuality, distrust of the public health system among African Americans, and a low priority of health relative to other issues of poverty in the community. The region also suffers from a shortage of providers and facilities, with inconvenient hours of operation, lack of transportation, and few trained minority staff. Women's access to services is especially limited by the need for child care and transportation, and is complicated by comorbidities (domestic violence and substance abuse). Several innovations in STD control and prevention are described. Recommendations include technical assistance to improve community prevention efforts, gender-sensitive programs that deal with the special problems of women, training/recruitment activities, and improvements in electronic data exchange.\n\nPurpose\n\nThe purpose of this study was to compare syphilis prevention efforts in southern communities during an epidemic in the Southern States between 1990 and 1992. Case studies of factors affecting the natural history, epidemiology, and management of syphilis in the South were conducted. Specific objectives included development of an improved understanding of service delivery to persons at high risk of infection with syphilis, discovery of innovative syphilis prevention and control measures planned or implemented in Southern States, and production of recommendations for improving community-level prevention strategies.\n\nBackground\n\nThe syphilis epidemic in the South during 1990-1992 involved the reemergence of an easily diagnosed and readily treatable STD. Syphilis is a systemic disease with an initial acute stage followed by a long period of latency. Transmission occurs through lesions, normally during sexual contact. These lesions increase the likelihood that HIV transmission will occur as well. Syphilis is treated with penicillin, and it is controlled through activities that involve identification, testing, and treatment of exposed sex partners. Historically, African Americans in the South have suffered a disproportionate burden of early syphilis and congenital syphilis. During 1985-1990, rates of syphilis rose 165 percent in this population, while significantly decreasing for all other races and ethnic groups in this region. African Americans represent 90 percent of all reported cases. There is little empirical evidence to explain why syphilis rates are highest in the South and why the disease primarily affects African Americans in that region.\n\nMethods\n\nCase studies were conducted in eight communities in four States (Alabama, Mississippi, South Carolina, and Tennessee). Sites eligible for the study were communities in ten Southern States identified on the basis of high syphilis morbidity during 1990. Criteria for selecting States and communities included consistently high syphilis rates since 1990, or rates that show a decrease suggestive of successful control activities, and known demographic indicators of high syphilis risk (significant number of African-American residents and proportion of households with incomes below the poverty line). Selected metropolitan areas each were paired with a rural counterpart to allow urban/rural comparison of social contexts and public health activities. Background information about each community was gathered. Week-long site visits were held, with interviews of between 40 and 60 public health providers, other providers, and community representatives at each site. Open-ended interviews focused on who is perceived to be at greatest risk for syphilis transmission/\n\ninfection, what institutions are best able to reach these individuals, what barriers stand in the way of reaching at-risk individuals, and innovative ideas or activities that STD prevention programs in other locations might find useful.\n\nMultiple interviewers were used, providing an ongoing check for validity and reliability of data collection. A descriptive case study was prepared for each study site using interview data and information from a background document review. The case studies were sent to leaders in the State and local STD programs to supply any missing information, correct misunderstandings, or add comments.\n\nFindings\n\nThis study emphasized that a set of dynamic interactions between distinct social scenarios, institutional situations, and persons in the communities placed certain groups and individuals at greater risk for syphilis infection. Of particular importance in these communities is the overarching issue of poverty. African Americans are the demographic group in the South at greatest risk of syphilis infection. The exchange of sex for drugs, especially when related to crack cocaine use, was considered an important risk behavior. Other risk groups include homeless persons, incarcerated individuals, adolescents, and male homosexuals.\n\nLocal health departments are the only community organizations that focus directly on syphilis and STD control and prevention. Other organizations that offer STD diagnosis/treatment provide limited partner notification and contact tracing. Public health agencies tended to assign priority to disease control. Schools were a consistent source of STD prevention messages but the content of these messages was limited by local restrictions on sexu"
    }
}
{
    "id": "dbpedia_4201_1",
    "rank": 34,
    "data": {
        "url": "https://icml.cc/virtual/2024/events/workshop",
        "read_more_link": "",
        "language": "en",
        "title": "ICML 2024 Workshops",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://icml.cc/static/core/img/icml-navbar-logo.svg",
            "https://icml.cc/static/core/img/ICML-logo.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "This workshop aims to highlight translational ML research in biology and chemistry ML for real-world applications in life-and materials science. The goal is to bridge theoretical advanceswith practical applications and connect academic and industry researchers.\n\nBiology and chemistry play a central role in understanding life, and are a fundamental pillar ofhuman well-being through their roles as medicines, materials, or agro-chemicals.\n\nWith increasingchallenges associated with climate change, growth of the global population, diseases associatedwith aging, and the global supply of food and energy, it is becoming increasingly urgent toaccelerate the pace at which technical discoveries can be made, and translated into practical solutions to these societal issues.\n\nHowever, compared to other modalities such as images orlanguage, the study of biology and chemistry with machine learning is not as industriallyestablished. Multiple factors contribute to this delay. Different research questions require manylevels and scales of representation, from electronic structure to graph and point cloudrepresentations of (bio) molecules, to protein and nucleic acid sequences, crystals, omics data, celland tissue-level representations.\n\nWe envision abalanced scientific industrial and academic attendance, and propose committees and a lineup thatreflect a mix of top industry scientists, academic leaders and double-affiliated scientists, as well asemerging scientists and new voices in …\n\nMathematical reasoning is one of the most advanced forms of human intelligence. Humans develop formal languages for rigorously describing mathematical problems and deriving mathematical knowledge. The machine learning community has endeavored to develop neural models with mathematical reasoning capabilities as humans. On the other hand, a shared vision in the community is that the models collaborate with humans for mathematical discoveries. The goal of this workshop is to bring together researchers working on various domains to discuss the progress and the future of applying AI technologies to mathematics. As mathematics is fundamental for almost all modern sciences (including computer science), a vast range of related topics are also within our scope. To this end, this workshop focuses on several crucial yet underexplored problems. Specifically, we are expecting attendants from various backgrounds, institutions, and disciplines to discuss areas related to the following: * Autoformalization and the reversed auto-informalization: How can we develop methods that improve the precision of the autoformalization process from natural language proof to formal proof, and as a dual process describing a formal proof in natural language?* Automated theorem proving: How do build consistent theorem proving? How do we relieve or solve the intermediate step errors …\n\nGradients and derivatives are integral to machine learning, as they enable gradient-based optimization. In many real applications, however, models rest on algorithmic components that implement discrete decisions, or rely on discrete intermediate representations and structures. These discrete steps are intrinsically non-differentiable and accordingly break the flow of gradients. To use gradient-based approaches to learn the parameters of such models requires turning these non-differentiable components differentiable. This can be done with careful considerations, notably, using smoothing or relaxations to propose differentiable proxies for these components. With the advent of modular deep learning frameworks, these ideas have become more popular than ever in many fields of machine learning, generating in a short time-span a multitude of \"differentiable everything\", impacting topics as varied as rendering, sorting and ranking, convex optimizers, shortest-paths, dynamic programming, physics simulations, NN architecture search, top-k, graph algorithms, weakly- and self-supervised learning, and many more.\n\nAligning AI agents with human intentions and values is one of the main barriers to the safe and ethical application of AI systems in the real world. Current approaches mostly rely on highly questionable assumptions about the meaning of observed human feedback or interactions. These include assumptions about rationality in decision-making and belief forming, homogeneity of the population, and other restrictive feedback assumptions. However, the role of such modeling assumptions has mostly been neglected in the literature on AI alignment. In this workshop, we want to bring together perspectives from various disciplines besides ML, including computational social choice, behavioral psychology, and economics, to share experiences and perspectives on models of human feedback and their importance for human-AI alignment and collaboration.\n\nIn recent years, general-purpose AI has experienced a meteoric rise in capabilities and applications. This rise has continued to bring forth new safety challenges, requiring mitigation to ensure AI systems meet trustworthiness standards. In this workshop, we take a proactive approach to safety and focus on five emerging trends in AI and explore the challenges associated with deploying these technologies safely:1. Agentic AI: As AI agents become more autonomous, concerns about unintended consequences, ethical issues, and adversary exploitation emerge. How do we ensure these agents respect privacy, and adhere to safety protocols?2. Multimodal: With the evolution of AI systems to process and generate diverse modalities like audio, video, and images, concerns around content appropriateness, privacy, bias, and misinformation arise. How do we craft robust guidelines and security measures to tackle these challenges?3. Personalized Interactions: As conversational agents evolve for social and personal interaction, risks like data privacy breaches and echo chambers grow. How do we balance tailored experiences with user safety?4. Sensitive Applications: With AI’s integration into high-risk domains like legal, medical, and mental health, the stakes rise with risks such as overreliance on automation and potential catastrophic errors. How do we ensure that AI systems in these critical areas …\n\nMulti-modal Foundation Model meets Embodied AI (MFM-EAI)In recent years, Multi-modal Foundation Models (MFM) such as CLIP, ImageBind, DALL·E 3, GPT-4V, and Gemini have emerged as one of the most captivating and rapidly advancing areas in AI, drawing significant attention and progressing swiftly. The open-source community for MFM has also seen vigorous growth, with the emergence of models and algorithms like LLaVA, LAMM, Stable Diffusion, and OpenFlamingo. These MFMs are now actively exploring ultimate application scenarios beyond traditional computer vision tasks.Recent studies have unveiled the immense potential these models hold in empowering embodied AI agents, marking the intersection of these fields with a multitude of open questions and unexplored territories. This workshop, MFM-EAI, is dedicated to exploring these critical challenges:- How can we train and evaluate MFM in open-ended environments?- What constitutes an effective system architecture for MFM-based Embodied AI Agents?- And importantly, how can MFM augment the perceptual and decision-making capabilities of these agents, balancing their high-level decision-making prowess with the nuanced requirements of low-level control in embodied systems?Topics include but are not limited to:- Training and evaluation of MFM in open-ended scenarios- Data collection for training Embodied AI Agents and corresponding MFM- Framework design for MFM-powered embodied agents- Decision-making …\n\nThis workshop aims to bring together various researchers to chart the course for the next generation of sequence models. The focus will be on better understanding the limitations of existing models like transformer architectures, recurrent neural networks, and state space models (e.g., S4, Mamba), as well as describing existing open problems. We will touch on topics such as memory, long-range context and in-context learning, optimization stability of these architectures, and their ability to represent different classes of problems. We will also cover interpretability and pragmatic aspects of getting these models to be efficient and perform well: how they should be scaled up, and the trade-offs and limitations imposed by current hardware. We will place additional emphasis on the discussion regarding how we should evaluate and benchmark sequential models at scale, for example, in the context of language or other domains like vision, audio, or biological signals.\n\nClimate change is a major concern for human civilization, yet significant uncertainty remains in future warming, change in precipitation patterns, and frequency of climate extremes. Proper adaptation and mitigation demands accurate climate projections capable of simulating the atmosphere, ocean, land, and their interactions. Numerical models exhaustively tuned by domain scientists have been the gold standard for modeling both weather and climate because of their interpretability and ability to simulate “what-if” scenarios not present in the historical record. Although AI forecasts have started to make operational progress in weather prediction, climate projections are a harder problem. For example, High Impact-Low Likelihood events are undersampled in ERA5 reanalysis data, and substantial decadal variability in modes of climate variability (like the El-Niño Southern Oscillation) limit the ability of AI forecasts to reliably extrapolate into the future. This workshop seeks to accelerate progress on using machine learning to improve climate projections, emphasizing areas that domain scientists have deemed amenable to machine learning approaches. Examples include hybrid physics-ML climate models, where machine learning is used to emulate subgrid processes too expensive to resolve explicitly, and dynamical downscaling, where high-resolution climate variables are inferred from coarse-resolution models in a physically consistent manner. In service of this, …\n\nModeling learning dynamics has long been a goal of the empirical science and theory communities in deep learning. These communities have grown rapidly in recent years, as our newly expanded understanding of the latent structures and capabilities of large models permits researchers to study these phenomena through the lens of the training process. Recent progress in understanding fully trained models can therefore enable understanding of their development and lead to insights that improve optimizer and architecture design, provide model interpretations, inform evaluation, and generally enhance the science of neural networks and their priors. We aim to foster discussion, discovery, and dissemination of state-of-the-art research in high-dimensional learning dynamics relevant to ML.\n\nWe invite participation in the 2nd Workshop on High-dimensional Learning Dynamics (HiLD), to be held as a part of the ICML 2024 conference. This year’s theme focuses on understanding how reasoning capabilities and internal structures develop over the course of neural network training; we encourage submissions related to our theme as well as other topics around the theoretical and empirical understanding of learning in high dimensional spaces. We will accept high quality submissions as poster presentations during the workshop, especially work-in-progress and state-of-art ideas.\n\nWe welcome any topics in …\n\nThe workshop focuses on theory, methodology, and application of structured probabilistic inference and generative modeling, both of which are important topics in machine learning.Specifically, probabilistic inference addresses the problem of amortization,sampling, and integration of complex quantities from graphical models, while generative modeling captures the underlying probability distributions of a dataset. Apart from applications in computer vision, natural language processing, and speech recognition, probabilistic inference and generative modeling approaches have also been widely used in natural science domains, including physics, chemistry, molecular biology, and medicine. Beyond applications in these domains, the span of tasks of the methods have been expanding beyond probabilistic inference and generative model such as optimal control, decision making, sampling, optimization, etc.Despite the promising results, probabilistic methods face challenges when applied to highly structured data, which are ubiquitous in real-world settings, limiting the applications of such methods. This workshop aims to bring experts from diverse backgrounds and related domains together to discuss the applications and challenges of probabilistic methods. The workshop will emphasize challenges in encoding domain knowledge when learning representations, performing inference and generations. By bringing together experts from academia and industry, the workshop will provide a platform for researchers to share their latest results and ideas, …\n\nFoundation models have become a cornerstone in the advancement of artificial intelligence, widely used across both academic and practical applications. Across domains, many challenging tasks require synthesizing information over thousands to millions of individual pieces of data, which may take many forms, including images, text, audio, genomes, etc. As a result, much recent work has focused on developing long-context models capable of processing, understanding, and generating responses based on extensive inputs. Enabling foundation models to process long contexts introduces several key challenges: (1) Computation efficiency: transformers, the predominate architecture for foundation models, incur a quadratic computational complexity with respect to the input length. (2) Lack of data: The development of long-context foundation models requires access to a large amount of long-sequence data, which is difficult to satisfy due to the limited availability of such collections. (3) Evaluation complexity: Evaluating the performance of long-context foundation models is inherently complex, as it is costly to collect, construct, or verify such evaluation data by humans.Our workshop aims to convene researchers to address these challenges, fostering discussions, developments, and evaluation of long-context foundation models across various AI disciplines.\n\nAI is integrated into scientific discovery ever more profusely to augment and accelerate research, helping scientists to generate hypotheses, design experiments, collect and interpret large datasets, and gain new insights that might not have been possible using traditional scientific methods alone. The main goal of this series of workshop is to discover synergy across a variety of scientific fields, encourage interdisciplinary discussions, and enhance the flow of knowledge between AI and Science communities. Throughout history, bridging seemly different fields has brought overarching benefits, with notable examples: entropy in thermodynamics and information theory, neuroscience and AI, and algorithms inspired by discoveries in science (e.g. genetic algorithm, simulated annealing and diffusion-based generative models). In the current AI era, successes of AI methods in different fields of science have alluded to the general effectiveness of collecting large simulated data, finding suitable architectures, enforcing invariances/equivariances, and utilizing foundation models. Our mission is to bring more scientists to attend ICML to share different perspectives on the use of AI, and to illuminate exciting research directions for AI researchers. In the following, we concentrate our discussion in this workshop on Scaling in AI for Science.Scaling models has addressed challenges once deemed insurmountable, including predicting 3D protein …\n\nThis is a workshop proposal, targeting the intersection of Agentic AI and Market/Incentives Design.Workshop Summary: Recent developments in foundation models have paved the way for the wide adoption of AI agents that interact with humans and each other. The cooperation and safety of those models are a necessity, especially as they gain autonomy and participate in high stakes markets as autonomous systems, making those markets \"agentic.\" However, those agentic markets face significant challenges as most existing methods at improving their performance and robustness presume critical use of policy and regulation, which are insufficient and too slow for an economy driven by a mixture of human and algorithmic participants, especially in zero-shot scenarios.As we advance towards an AI-centric future, the emergence of markets, mechanisms, and mediation platforms dedicated to preference elicitation and resource allocation for those highly agentic systems is inevitable. We expect many existing multi-agent security and cooperation approaches to break in high-stakes situations where hyper-adversarial incentives are present. This is compounded by the emergence of complexity from AI interactions, exemplified by intricate interdependencies within agentic systems.Given this complexity, how can we fully understand and assess the associated risks? How can we improve the performance and robustness of these markets? …\n\nThe past few years have seen the rapid development of Generative AI, with powerful foundation models demonstrating the ability to generate new, creative content in multiple modalities. Following breakthroughs in text and image generation, it is clear the next frontier lies in video. One challenging but compelling aspect unique to video generation is the various forms in which one could control such generation: from specifying the content of a video with text, to viewing a scene with different camera angles, or even directing the actions of characters within the video. We have also seen the use cases of these models diversify, with works that extend generation to 3D scenes, use such models to learn policies for robotics tasks or create an interactive environment for gameplay. Given the great variety of algorithmic approaches, the rapid progress, and the tremendous potential for applications, we believe now is the perfect time to engage the broader machine learning community in this exciting new research area. We thus propose the first workshop on Controllable Video Generation (CVG), focused on algorithms that can control videos with multiple modalities and frequencies, and the swathe of potential applications. We anticipate CVG would be uniquely relevant to ICML as …\n\nBy recognizing that nearly all data is rooted in our physical world, and thus inherently grounded in geometry and physics, it becomes evident that learning systems should preserve this grounding throughout the process of representation learning in order to be meaningful. For example, preserving group transformation laws and symmetries through equivariant layers is crucial in domains such as computational physics, chemistry, robotics, and medical imaging. It leads to effective and generalizable architectures and improved data efficiency. Similarly, in generative models applied to non-Euclidean data spaces, maintaining the manifold structure is essential to obtain meaningful samples. Therefore, this workshop focuses on the principle of grounding in geometry, which we define as follows: A representation, method, or theory is grounded in geometry if it can be amenable to geometric reasoning, that is, it abides by the mathematics of geometry.\n\nLarge Language Models (LLMs) have undoubtedly taken center stage in the AI revolution, showing impressive performance in a wide variety of tasks, including machine translation, standardized tests, and conversational chatbots. It is even more impressive to uncover that these models exhibit unpredictable capabilities in solving unseen tasks. This demonstration of emergent abilities, often credited to the scale of the parameters and data size in the case of LLMs, is being considered as the footprint of intelligence.The goal of this workshop is to assess and understand the position of current LLMs’ abilities in the landscape of intelligent systems, with a strong focus on cognitive abilities. By bringing in experts from different scientific disciplines, such as AI/ML, neuroscience, cognitive science, and psychology, we aim to discuss topics that include but not limited to:• Where do LLMs stand in terms of performance on cognitive tasks, such as reasoning, navigation, planning, and theory of mind?What are the fundamental limits of language models with respect to cognitive abilities?• How do LLMs fine-tuned on specific tasks end-to-end compare to augmented LLMs coupled withexternal modules?• What are the similarities and differences between mechanistic interpretability approaches in AI and inneuroscience? What do they tell us about similarities and …\n\nRecent advancements in generative foundation models (FMs) such as large language models (LLMs) and diffusion models have propelled the capability of deep neural models to seemingly magical heights. Yet, the soaring growth in the model size and capability has also led to pressing concerns surrounding such modern AI systems. The scaling of the models significantly increases their energy consumption and deployment cost. Overreliance on AI may perpetuate existing inequalities and lead to widening discrimination against certain groups of people. The gap between the understanding of the internal workings of FMs and their empirical success has also reached an unprecedented level, hindering accountability and transparency.For decades, theoretical tools from statistics, information theory, and optimization have played a pivotal role in extracting information from unstructured data. Currently, the rapid pace of FM development has outstripped theoretical investigation, creating a potential gap between theoretical researchers and the challenges surrounding FMs. This workshop proposes a platform for bringing together researchers and practitioners from the foundation model and theory community (including statistics, information theory, optimization, and learning theory), to discuss advances and challenges in addressing these concerns, with a focus on responsible AI, efficiency, and principled foundations.\n\nExcitement about the capabilities of generative-AI systems has touched nearly every corner of ML research and public life. Amid such exhilarating potential, there is also intensifying unease around the development and deployment of generative-AI systems. By now, it is well-known that generative models ingest vast quantities of intellectual property (IP) [8–10], which they can regurgitate verbatim [1–3, 11, 12]. Such memorization has been the continued focus of copyright-focused lawsuits [4], but memorization and copyright just scratch the surface of potential legal issues at play. In the report from our ICML workshop last year, we produced a taxonomy of emerging issues that touch on intent, privacy, misinformation and disinformation, and IP (more broadly) [5]. Indeed, based on the events of the past year alone — executive orders [13], lawsuits [4], new and amended laws [7], and labor strikes [6] — it has only become clearer that there are significant “technical, doctrinal, and policy challenges presented by law for Generative AI, and by Generative AI for law” [5]. Within this challenging and fast-moving landscape, GenLaw has played an important clarifying and cross-educational role. The first GenLaw workshop at ICML 2023 hosted over 400 attendees in person, and our workshop recording has been …\n\nThe past few years has seen a surge of interest in reinforcement learning, with breakthrough successes of applying RL in games, robotics, chemistry, logistics, nuclear fusion and more. These headlines, however, blur the picture of what remains a brittle technology,with many successes relying on heavily engineered solutions. Indeed, several recent works have demonstrated that RL algorithms are brittle to seemingly mundane design choices. Thus, it is often a significant challenge to effectively apply RL in practice, especially on novel problems, limiting its potential impact and narrowing its accessibility. In this workshop, we want to bring together different communities working on solving these problems. A variety of distinct sub-communities spanning RL, Meta-Learning and AutoML havebeen working on making RL work “out-of-the-box” in arbitrary settings - this is the AutoRL setting. Recently, with the emergence of LLMs and their in-context learning abilities, they have significantly impacted all these communities. There are LLM agents tacklingtraditional RL tasks as well as few-shot RL agents increasing efficiency and generalization that arealso trying to automate RL. LLMs have also been influencing AutoML directly with papers such as OptFormer. However, there is currently little crossover between these communities. As such, we want to create the space to …\n\nThere is a growing gap between machine learning (ML) research on biology-inspired problems and the actual broad-based use of ML in the lab or the clinic. This gap is especially pressing in the context of foundation models and other large ML models. Accessibility and efficiency concerns limit the adoption of these models by biologists and clinicians. Large ML models may require extensive GPU clusters to train, while most biological labs only have access to much more modest computational resources. The usability of these models for non-expert users is also a concern, as is the need to iteratively adapt these models based on lab discoveries. This workshop seeks to bring ML and biomedical researchers together to identify interdisciplinary approaches to design and apply large, complex ML models for biomedical discovery. We invite researchers from academia and industry to submit original papers to bridge the accessibility and efficiency gap between ML research and wet lab use. All accepted papers will be invited to present posters at the workshop, and a few will be invited to give individual spotlight presentations.\n\nWith the widespread adoption of machine learning in social technologies, there are increasingly complex interactions between humans, algorithmic decision-makers, and society at large. For instance, algorithmic decisions influence the information and opportunities that are available to individuals, the news they read, the job listings they are matched to, the credit lines they receive, and the social circle they form. On a macroscopic level, such decisions can therefore affect societal outcomes such as social mobility, mental health, polarization etc. At the same time, humans also influence algorithmic decision-makers, for instance, by expressing their preferences through observed behaviors which might be inconsistent or strategic. To understand long-term individual and societal outcomes resulting from these interactions, and to develop algorithms that mitigate undesired outcomes, it has therefore become increasingly important to model these complex interactions as a whole. The goal of this workshop is to bring together researchers from both academia and industry who work on modeling interactions between AI systems, humans, and society. We aim to cover a wide range of topics including both theory and practice. In particular, we encourage submissions on the following topics:- Feedback loops between human and algorithmic decisions, and their long-term impacts- Strategic behavior and its impact …"
    }
}
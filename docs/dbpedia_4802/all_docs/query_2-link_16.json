{
    "id": "dbpedia_4802_2",
    "rank": 16,
    "data": {
        "url": "https://www.commscope.com/insights/the-enterprise-source/migration-to-400g800g-the-fact-file-part-1/",
        "read_more_link": "",
        "language": "en",
        "title": "400G/800G for Hyperscale and Multi Tenant Data Centers",
        "top_image": "https://www.commscope.com/globalassets/digizuite/939013-400g-800g-ff-twin-640.jpg",
        "meta_img": "https://www.commscope.com/globalassets/digizuite/939013-400g-800g-ff-twin-640.jpg",
        "images": [
            "https://www.commscope.com/globalassets/digizuite/909589-commscope-logo-nmn-header.svg",
            "https://www.commscope.com/globalassets/digizuite/914074-heliax-menu-logo.png/Preview",
            "https://www.commscope.com/globalassets/digizuite/914074-heliax-menu-logo.png/Preview",
            "https://www.commscope.com/globalassets/digizuite/914076-systimax-menu-logo.png/Preview",
            "https://www.commscope.com/globalassets/digizuite/914074-heliax-menu-logo.png/Preview",
            "https://www.commscope.com/globalassets/digizuite/914076-systimax-menu-logo.png/Preview",
            "https://www.commscope.com/globalassets/digizuite/914074-heliax-menu-logo.png/Preview",
            "https://www.commscope.com/globalassets/digizuite/914076-systimax-menu-logo.png/Preview",
            "https://www.commscope.com/globalassets/digizuite/914074-heliax-menu-logo.png/Preview",
            "https://www.commscope.com/globalassets/digizuite/914076-systimax-menu-logo.png/Preview",
            "https://www.commscope.com/globalassets/digizuite/914076-systimax-menu-logo.png/Preview",
            "https://www.commscope.com/globalassets/digizuite/914079-era-menu-logo.png/Preview",
            "https://www.commscope.com/globalassets/digizuite/914078-onecell-menu-logo.png/Preview",
            "https://www.commscope.com/globalassets/digizuite/914076-systimax-menu-logo.png/Preview",
            "https://www.commscope.com/globalassets/digizuite/914074-heliax-menu-logo.png/Preview",
            "https://www.commscope.com/globalassets/digizuite/914074-heliax-menu-logo.png/Preview",
            "https://www.commscope.com/globalassets/digizuite/914076-systimax-menu-logo.png/Preview",
            "https://www.commscope.com/globalassets/digizuite/914076-systimax-menu-logo.png/Preview",
            "https://www.commscope.com/globalassets/digizuite/914079-era-menu-logo.png/Preview",
            "https://www.commscope.com/globalassets/digizuite/914078-onecell-menu-logo.png/Preview",
            "https://www.commscope.com/globalassets/digizuite/914074-heliax-menu-logo.png/Preview",
            "https://www.commscope.com/globalassets/digizuite/914074-heliax-menu-logo.png/Preview",
            "https://www.commscope.com/globalassets/digizuite/924160-commscope-world-hero500.png",
            "https://www.commscope.com/globalassets/digizuite/992972-992942-bbe-300x250-mc-116770-en-2.jpg",
            "https://www.commscope.com/globalassets/digizuite/995907-arinteractive2023.jpg",
            "https://www.commscope.com/globalassets/digizuite/995908-arprintversion2023.jpg",
            "https://www.commscope.com/globalassets/digizuite/995909-arproxy2023.jpg",
            "https://www.commscope.com/globalassets/digizuite/934187-400g-800g-ff-ethernet-roadmap.jpg/Preview",
            "https://www.commscope.com/globalassets/digizuite/934309-400g-800g-ff-more-servers-icon.jpg",
            "https://www.commscope.com/globalassets/digizuite/934306-400g-800g-ff-higher-bandwidth-icon.jpg",
            "https://www.commscope.com/globalassets/digizuite/934308-400g-800g-ff-faster-connections-icon.jpg",
            "https://www.commscope.com/globalassets/digizuite/934305-400g-800g-ff-higher-uplink-speeds-icon.jpg",
            "https://www.commscope.com/globalassets/digizuite/934307-400g-800g-ff-rapid-expansion-icon.jpg",
            "https://www.commscope.com/globalassets/digizuite/934310-400g-800g-ff-ethernet-port-shipments.jpg/Preview",
            "https://www.commscope.com/globalassets/digizuite/934311-400g-800g-ff-ethernet-speeds.jpg/Preview",
            "https://www.commscope.com/globalassets/digizuite/934315-400g-800g-ff-four-pillars-diagram.jpg/Preview",
            "https://www.commscope.com/globalassets/digizuite/934316-400g-800g-ff-higher-radix-switches-diagram.jpg/Preview",
            "https://www.commscope.com/globalassets/digizuite/934317-400g-800g-ff-tor-to-mor-eor-diagram.jpg/Preview",
            "https://www.commscope.com/globalassets/digizuite/934463-400g-800g-ff-osfp-vs-qsfp-dd-transceiver.png",
            "https://www.commscope.com/globalassets/digizuite/934462-400g-800g-ff-higher-speed-modulation-schemes-diagram.png/Preview",
            "https://www.commscope.com/globalassets/digizuite/934465-400g-800g-ff-data-center.jpg/Preview",
            "https://www.commscope.com/globalassets/digizuite/934466-400g-800g-ff-distributing-capacity-options.jpg/Preview",
            "https://www.commscope.com/globalassets/digizuite/934468-400g-800g-ff-mpo-12-16-fiber-configurations.jpg/Preview",
            "https://www.commscope.com/globalassets/digizuite/934470-400g-800g-ff-400g-capacity-qsfp-dd.jpg/Preview",
            "https://www.commscope.com/globalassets/digizuite/934469-400g-800g-ff-800g-capacity-qsfp-dd.jpg/Preview",
            "https://www.commscope.com/globalassets/digizuite/934472-400g-800g-ff-size-relationship-btw-duplex-connectors-breakout-applications.jpg/Preview",
            "https://play.vidyard.com/CVVuDFvWwToXDhZpYtey15.jpg?play_button=1",
            "https://www.commscope.com/globalassets/digizuite/930294-enterprise-data-centers-main-hero-400b.jpg",
            "https://www.commscope.com/globalassets/digizuite/930300-hyperscale-cloud-data-center-main-hero-400b.jpg",
            "https://www.commscope.com/globalassets/digizuite/930305-mtdc-main-hero-400b.jpg",
            "https://www.commscope.com/globalassets/digizuite/930310-service-provider-data-centers-hero-400b.jpg",
            "https://www.commscope.com/globalassets/digizuite/934473-mmf-large-factfileimage.jpg",
            "https://www.commscope.com/globalassets/digizuite/934474-hsm-lib-ipad.jpg",
            "https://www.commscope.com/globalassets/digizuite/934475-osfpmsa-pic.jpg",
            "https://www.commscope.com/globalassets/digizuite/934476-qsfp-dd-msa-rep-image.jpg",
            "https://www.commscope.com/globalassets/digizuite/934476-qsfp-dd-msa-rep-image.jpg",
            "https://www.commscope.com/globalassets/digizuite/940041-propel-webpage-hero500.jpg",
            "https://www.commscope.com/globalassets/digizuite/924074-rapid-fiber-panel-co-310111-en-herox400.jpg",
            "https://www.commscope.com/globalassets/digizuite/934489-400g-800g-ff-proven-performance-icon.jpg/Preview",
            "https://www.commscope.com/globalassets/digizuite/934492-400g-800g-ff-agility-and-adaptability-icon.jpg/Preview",
            "https://www.commscope.com/globalassets/digizuite/934491-400g-800g-ff-future-ready-icon.jpg/Preview",
            "https://www.commscope.com/globalassets/digizuite/934488-400g-800g-ff-guaranteed-reliability-icon.jpg/Preview",
            "https://www.commscope.com/globalassets/digizuite/934490-400g-800g-ff-global-availability-icon.jpg/Preview",
            "https://www.commscope.com/globalassets/digizuite/934505-hsm-data-center-products-ff-card-hero400.jpg",
            "https://www.commscope.com/globalassets/digizuite/934504-fiber-structured-cabling-products-ff-card-hero400.jpg",
            "https://www.commscope.com/globalassets/digizuite/934507-copper-trunk-assemblies-products-ff-card-hero400.jpg",
            "https://www.commscope.com/globalassets/digizuite/934506-fiber-raceways-products-ff-card-hero400.jpg",
            "https://www.commscope.com/globalassets/digizuite/934508-aim-products-ff-card-hero400.jpg",
            "https://www.commscope.com/globalassets/digizuite/924757-logo-commscope.png",
            "https://www.commscope.com/globalassets/digizuite/932699-qr-code-china-weibo.jpg/Preview",
            "https://www.commscope.com/globalassets/digizuite/932709-qr-code-china-wechat.jpg/Preview"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "Hyperscale and multi tenant data centers need to plan now for 400G800G migration. Get the insight you need to prepareoptics fiber cabling design and more.",
        "meta_lang": "en",
        "meta_favicon": "/favicon.ico",
        "meta_site_name": "CommScope",
        "canonical_link": "https://www.commscope.com/insights/the-enterprise-source/migration-to-400g800g-the-fact-file-part-1/",
        "text": "Global data usage\n\nOf course, at the heart of the changes are the global trends that are reshaping consumer expectations and demand for more and faster communications, such as:\n\nExplosive growth in social media traffic\n\nRollout of 5G services, enabled by massive small cell densification\n\nAccelerating deployments of IoT and IIoT (Industrial IoT)\n\nA shift from traditional office-based work to remote options\n\nGrowth of hyperscale providers\n\nGlobally, true hyperscale data centers may number less than a dozen or so, but their impact on the overall data center landscaping is significant. According to recent research, the world spent a combined 1.25 billion years online in 2020 alone.1 About 53% of that traffic passes through a hyperscale facility. 2\n\nHyperscale partnering with Multi-Tenant Data Center (MTDC / co-Location) facilities\n\nAs demand for more lower latency performance increases, hyperscale and cloud-scale providers work to extend their presence closer to the end user/end device. Many are partnering with MTDC or co-location data centers to locating their services at the so-called network “edge”3. When the edge is physically close, lower latency and network costs expand the value of new low-latency services. As a result, growth in the hyperscale arena is forcing MTDCs and co-location facilities to adapt their infrastructures and architectures to support the increased scale and traffic demands that are more typical of hyperscale data centers. At the same time, these largest of data centers must continue to be flexible to customer requests for cross-connections to cloud provider on-ramps.\n\nSpine-leaf and fabric mesh networks\n\nThe need to support low-latency, high-availability, very high bandwidth applications is hardly limited to hyperscale and co-location data centers. All data center facilities must now rethink their ability to handle the rising demands of end users and stakeholders. In response, data center managers are rapidly moving toward more fiber-dense mesh fabric networks. The any-to-any connectivity, higher fiber-count backbone cables and new connectivity options enable network operators to support ever-higher lane speeds as they prepare to make the transition to 400 Gigabits per second4 (G).\n\nEnabling artificial intelligence (AI) and machine learning (ML)\n\nIn addition, the larger data center providers, driven in part by IoT and smart city applications, are turning to AI and ML to help create and refine the data models that power near real-time compute capabilities at the edge. Besides having the potential to enable a new world of applications (think commercially viable self-driving cars), these technologies require massive data sets, often referred to as data lakes, and massive compute power within the data centers and large enough pipes to push the refined models to the edge when needed.5\n\nMore\n\nhigh-performance virtualized servers\n\nHigher\n\nbandwidth and lower latency\n\nFaster\n\nswitch-to-server connections\n\nHigher\n\nuplink/backbone speeds\n\nRapid\n\nexpansion capabilities\n\nWithin the cloud itself, the hardware is changing. Multiple disparate networks typical in a legacy data center have evolved to a more virtualized environment that uses pooled hardware resources and software-driven management. This virtualization is driving the need to route application access and activity in the fastest possible way, forcing many network managers to ask, “How do I design my infrastructure to support these cloud-first applications?”\n\nThe answer begins with enabling higher per-lane speeds. The progression from 25 to 50 to 100G and above is key to getting to 400G and beyond, and it has begun to replace the traditional 1/10G migration path. But there’s more to it than increasing lane speeds, a lot more. We have to dig a bit deeper.\n\nIn the data network, capacity is a matter of checks and balances among servers, switches and connectivity. Each pushes the other to be faster and less expensive, to efficiently track the demand produced by increased data sets, AI and ML. For years, switch technology was the primary bottleneck. With the introduction of Broadcom’s StrataXGS® Tomahawk® 3, data center managers can now boost switching and routing speeds to 12.8 Terabits/sec (Tb/s) and reduce their cost per port by 75 percent. Broadcom’s Tomahawk 4 switch chip, with a bandwidth of 25 Tb/s., provides the data center industry with more switching capability to stay ahead of those increasing AI and ML workloads. Today, this chip supports 64x 400G ports; but at 25.6Tb/s capacity, semiconductor technology is taking us down a path where in the future we could see 32x 800G ports on a single chip. 32, coincidentally, being the maximum number of QSFP-DD, or OSFP (800G transceivers) that can be presented at a 1U switch faceplate.\n\nSo now, the limiting factor is the CPU processing capability. Right? Wrong. Earlier this year, NVIDIA introduced its new Ampere chip for servers. It turns out, the processors used in gaming are perfect for handling the training and inference-based processing needed for AI and ML. According to NVIDIA, one Ampere-based machine can do the work of 120 Intel-powered servers.\n\nFigure 3: Ethernet speeds\n\nWith switches and servers on schedule to support 400G and 800G by the time they’re needed, the pressure shifts to the physical layer to keep the network balanced. IEEE 802.3bs, approved in 2017, paved the way for 200G and 400G Ethernet. However, the IEEE has just now completed its bandwidth assessment regarding 800G and beyond. The IEEE has started a study group to identify the objectives for applications beyond 400G and, given the time required to develop and adopt new standards, we may already be falling behind. The industry is now working together introducing 800G and beginning to work toward 1.6T and beyond while improving the power and cost per bit.\n\nSwitching speeds are increasing as serializer/deserializer (SERDES) that provide the electrical I/O for the switching ASIC move from 10G, 25G, 50G. SERDES are expected to hit 100G once IEEE802.3ck becomes a ratified standard. Switch application-specific integrated circuits (ASICs) are also increasing the I/O port density (a.k.a. radix). Higher radix ASICs support more network device connections, offering the potential to eliminate a layer top-of-rack (ToR) switches. This, in turn, reduces the overall number of switches needed for a cloud network. (A data center with 100,000 servers can be supported with two levels of switching with a RADIX of 512.) The higher radix ASICs translate into lower CAPEX (less switches), lower OPEX (less energy required to power and cool fewer switches) and improved network performance through lower latencies.\n\nFigure 4: Effects of higher Radix switches on switch bandwidth\n\nClosely related to the increase in radix and switching speed is the move from a top-of-rack (ToR) topology to either middle-of-row (MoR) or end-of-row (EoR) configuration, and the benefit that structured cabling approach holds when facilitating the many connections between the in-row servers and the MoR/EoR switches. The ability to manage the large number of server attachments with greater efficiency is required to make use of new high-radix switches. This, in turn, requires new optic modules and structured cabling, such as those defined in the IEEE802.3cm standard. The IEEE802.3cm standard supports the benefits of pluggable transceivers for use with high-speed server network applications in large data centers defining eight host attachments to one QSFP-DD transceiver.\n\nFigure 5: Architectures shifting from ToR to MoR/EoR\n\nJust as the adoption of the QSFP28 form factor drove the adoption of 100G by offering high density and lower power consumption, the jump to 400G and 800G is being enabled by new transceiver form factors. The current SFP, SFP+ or QSFP+ optics are sufficient to enable 200G link speeds. However, making the jump to 400G will require doubling the density of the transceivers. No problem.\n\nQSFP-Double Density (QSFP-DD7) and octal (2 times a quad) small form factor pluggable (OSFP8) Multi Source Agreements (MSAs) enable networks to double the number of electrical I/O connections to the ASIC. This not only allows summing more I/Os to reach higher aggregate speeds, it also allows the total number of ASIC I/O connections to reach the network.\n\nThe 1U switch form factor with 32 QSFP-DD ports matches 256 (32x8) ASIC I/Os. In this way, we can build high-speed links between switches (8*100 or 800G) but also have the ability to maintain the maximum number of connections when attaching servers.\n\nModulation schemes\n\nNetwork engineers have long utilized non-return to zero (NRZ) modulation for 1G, 10G and 25G, using host-side forward error correction (FEC) to enable longer distance transmissions. To get from 40G to 100G, the industry simply turned to parallelization of the 10G/25G NRZ modulations, also utilizing host side FEC for the longer distances. When it comes to achieving speeds of 200G/400G and faster, new solutions are needed.\n\nFigure 7: Higher-speed modulation schemes are used to enable 50G and 100G technologies\n\nAs a result, optical networking engineers have resorted to four-level pulse amplitude modulation (PAM4) to bring ultra-high bandwidth network architectures to fruition; PAM4 is the current solution for 400GPAM4. This is based in large measure on IEEE802.3, which has completed new Ethernet standards for rates up to 400G (802.3bs/cd/cu) for both multi-mode (MM) and single-mode (SM) applications. A variety of breakout options are available to accommodate diverse network topologies in large scale data centers.\n\nMore complex modulation schemes imply the need for an infrastructure that can provide better return loss and attenuation.\n\nPredictions – OSFP vs QSFP-DD\n\nWith regards to OSFP versus QSFP-DD, it's too early to tell which way the industry will go right now; both form factors are supported by leading data center Ethernet switch vendors and both have large customer support. Perhaps the enterprise will prefer QSFP-DD as an enhancement to current QSFP-based optics. OSFP seems to be pushing the horizon with the introduction of OSFP-XD, extending the number of lanes to 16 with an eye toward 200G lane rates in the future.\n\nFor speeds up to 100G, QSFP has become a go-to solution because of its size, power and cost advantage compared to duplex transceivers. QSFP-DD builds on this success and provides backwards compatibility which allows the use of QSFP transceivers in a switch with the new DD interface.\n\nLooking to the future, many believe that the 100G QSFP-DD footprint will be popular for years to come. OSFP technology may be favored for DCI optical links or those specifically requiring higher power and more optical I/Os. OSFP proponents envision 1.6T and perhaps 3.2T transceivers in the future.\n\nCo-packaged optics (CPOs) provide an alternate path to 1.6T and 3.2T. But CPOs will need a new ecosystem that can move the optics closer to the switch ASICs to achieve the increased speeds while reducing power consumption. This track is being developed in the Optical Internetworking Forum (OIF). The OIF is now discussing the technologies that might be best suited to the “next rate,” with many arguing for a doubling to 200G. Other options include more lanes – perhaps 32, as some believe that more lanes and higher lane rates will eventually be needed to keep pace with network demand at an affordable network cost.\n\nThe only sure prediction is that the cabling infrastructure must have the built-in flexibility to support your future network topologies and link requirements. While astronomers have long held that “every photon counts” as network designers look to reduce the energy per bit to a few pJ/Bit9, conservation at every level is important. High-performance cabling will help reduce network overhead.\n\nMPO connectors\n\nUntil recently, the primary method of connecting switches and servers within the data center involved cabling organized around 12- or 24-fibers, typically using MPO connectors. The introduction of octal technology (eight switch lanes per switch port) enables data centers to match the increased number of ASIC I/Os (currently 256 per switch ASIC) with optical ports. This yields the maximum number of I/Os available to connect servers or other devices.\n\nThe optical I/Os use connectors that are appropriate for the number of optical lanes used. A 400G transceiver may have a single duplex LC connector with a 400G optical I/O, it could also have 4 X 100G optical I/Os requiring 8 fibers. The MPO12 or perhaps 4 SN duplex connectors will fit within the transceiver case and provide the 8 fibers this application needs. Sixteen fibers are required to match 8 electrical and optical I/Os, preserving the radix of the switch ASIC. The optic ports can be either single mode or multimode depending on the distance the link is designed to support.\n\nFor example, multimode technology continues to provide the most cost-effective high-speed optical data rates for short reach links in the data center. IEEE standards support 400G in a single link (802.3 400G SR4.2) technology, which uses four fibers to transmit and four fibers to receive, with each fiber carrying two wavelengths. This standard extends the use of bi-directional wavelength division multiplexing (BiDi WDM) techniques and was originally intended to support switch-to-switch links. This standard uses the MPO12 connector and was the first to optimize using OM5 MMF.\n\nMaintaining the switch radix is important where many devices, such as server racks, need to be connected to the network. 400G SR8, addressed in the IEEE 802.3cm standard (2020), supports eight server connections using eight fibers to transmit and eight fibers to receive. This application has gained support amongst cloud operators. MPO-16 architectures are being deployed to optimize this solution.\n\nSingle-mode standards support longer reach applications (switch-to-switch, for example). IEEE 400G-DR4 supports 500 meters reach with 8 fibers. This application can be supported by MPO-12 or MPO-16. The value of the 16-fiber approach is added flexibility; data center managers can divide a 400G circuit into manageable 50/100G links. For example, a 16-fiber connection at the switch can be broken out to support up to eight servers connecting at 50/100G while matching the electrical lane rate. MPO 16-fiber connectors are keyed differently to prevent connection with the 12-fiber MPO connectors.\n\nThe electrical lane rate then determines the output capabilities of the optical interface. Table 1 shows examples of the 400G (50G X 8) module standards/possibilities.\n\nTable 1: 400G Capacity QSFP-DD with 50G Electrical Lanes\n\nWhen lane rates are doubled to 100G, the following optical interfaces become possible. At the time of writing, 100G lane rate standards (802.3 ck) have not been completed; however, early products are being released and many of these possibilities are in fact shipping. Table 2, presented at ECOC 2020 by J. Maki (Juniper), shows the early industry interest in the 800G modules.\n\nTable 2: 800G Capacity QSFP-DD with 100G Electrical Lanes\n\nConnector speed limits?\n\nConnectors typically do not dictate speed, economics do. Optical technologies were initially developed and deployed by service providers who had the financial means and bandwidth demands to support their development, as well as the long-haul links that are most economically bridged using the fewest number of fibers. Today, most service providers prefer simplex or duplex connector technology paired with optical transport protocols that use single-fiber connector technologies like LC or SC.\n\nHowever, these long-haul solutions can be too expensive, especially when there are hundreds or thousands of links and shorter link distances to traverse; both conditions are typical of a data center. Therefore, data centers often deploy parallel optics. Since parallel transceivers provide a lower cost per Gigabit, MPO-based connectivity is a good option over shorter distances. Thus, connector choices today are not driven so much by speed, but by the number of data lanes they can support, the space they take up, and the price impact on transceivers and switch technologies.\n\nIn the final analysis, the range of optical transceivers and optical connectors is expanding, driven by a wide variety of network designs. Hyperscale data centers may choose to implement a very custom optical design; given the scale of these market movers, standards bodies and OEMs often respond by developing new standards and market opportunities. As a result, investment and scale lead the industry in new directions and cabling designs evolve to support these new requirements."
    }
}
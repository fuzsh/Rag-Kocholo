{
    "id": "dbpedia_1854_1",
    "rank": 70,
    "data": {
        "url": "https://arxiv.org/html/2406.13378v1",
        "read_more_link": "",
        "language": "en",
        "title": "Any360D: Towards 360 Depth Anything with Unlabeled 360 Data and Möbius Spatial Augmentation",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://arxiv.org/html/x1.png",
            "https://arxiv.org/html/x2.png",
            "https://arxiv.org/html/x3.png",
            "https://arxiv.org/html/x4.png",
            "https://arxiv.org/html/x5.png",
            "https://arxiv.org/html/x6.png",
            "https://arxiv.org/html/x7.png",
            "https://arxiv.org/html/x8.png",
            "https://arxiv.org/html/x9.png",
            "https://arxiv.org/html/x10.png",
            "https://arxiv.org/html/x11.png",
            "https://arxiv.org/html/x12.png",
            "https://arxiv.org/html/extracted/5677956/Figures/supp_1.jpg",
            "https://arxiv.org/html/extracted/5677956/Figures/supp_2.jpg",
            "https://arxiv.org/html/extracted/5677956/Figures/supp_3.jpg",
            "https://arxiv.org/html/extracted/5677956/Figures/supp_4.jpg",
            "https://arxiv.org/html/extracted/5677956/Figures/supp_5.jpg",
            "https://arxiv.org/html/extracted/5677956/Figures/supp_6.jpg",
            "https://arxiv.org/html/extracted/5677956/Figures/supp_7.jpg",
            "https://arxiv.org/html/extracted/5677956/Figures/supp_8.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Zidong Cao1 Jinjing Zhu1∗ Weiming Zhang1∗ Lin Wang1,2†\n\n1AI Thrust, HKUST(GZ) 2Dept. of CSE, HKUST\n\n{zcao740,jzhu706}@connect.hkust-gz.edu.cn, zweiming996@gmail.com, linwang@ust.hk\n\nhttps://vlislab22.github.io/Any360D/\n\nAbstract\n\nRecently, Depth Anything Model (DAM) [1] – a type of depth foundation model – reveals impressive zero-shot capacity for diverse perspective images. Despite its success, it remains an open question regarding DAM’s performance on 360 images that enjoy large field-of-view (180∘×360∘superscript180superscript360180^{\\circ}\\times 360^{\\circ}180 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT × 360 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT) but suffer from spherical distortions. To this end, we establish, to our knowledge, the first benchmark that aims to 1) evaluate the performance of DAM on 360 images and 2) develop a powerful 360 DAM for the benefit of the community. For this, we conduct a large suite of experiments that consider the key properties of 360 images, e.g., different 360 representations, various spatial transformations, and diverse indoor and outdoor scenes. This way, our benchmark unveils some key findings, e.g., DAM is less effective for diverse 360 scenes and sensitive to spatial transformations. To address these challenges, we first collect a large-scale unlabeled dataset including diverse indoor and outdoor scenes. We then propose a semi-supervised learning (SSL) framework to learn a 360 DAM, dubbed Any360D. Under the umbrella of SSL, Any360D first learns a teacher model by fine-tuning DAM via metric depth supervision. Then, we train the student model by uncovering the potential of large-scale unlabeled data with pseudo labels from the teacher model. Möbius transformation-based spatial augmentation (MTSA) is proposed to impose consistency regularization between the unlabeled data and spatially transformed ones. This subtly improves the student model’s robustness to various spatial transformations even under severe distortions. Extensive experiments demonstrate that Any360D outperforms DAM and many prior data-specific models, e.g., PanoFormer [2] across diverse scenes, showing impressive zero-shot capacity for being a 360 depth foundation model.\n\n1 Introduction\n\n360 cameras have gained significant interest for their ability to capture surrounding environments in a single shot [3, 4]. Monocular 360 depth estimation is a crucial task for 3D scene perception with various applications, such as virtual reality (VR) [5] and autonomous driving [6]. However, estimating reliable 360 depth is challenging due to its ill-posed problem [7, 8] and lack of large-scale labeled dataset – because of the expensive depth annotations and the stitching process that often requires intensive labor costs. As a result, most existing 360 datasets, e.g., [9, 10, 11], are scene-specific, typically limited to indoor scenes such as rooms. This renders existing 360 depth estimation methods, e.g., [12, 13, 14], often produce blurry results and struggle for real outdoor scenes [15].\n\nRecently, vision foundation models [16, 17, 1] have been developed for various vision tasks. For the monocular depth estimation, several foundation models [18, 19, 20, 1] have been proposed. Among them, Depth Anything Model (DAM) [1] is a state-of-the-art (SOTA) depth foundation model that works robustly across diverse perspective images.\n\nDespite the success of DAM on perspective images, it is unclear about its performance on 360 images, which are superior to large field-of-view (FoV) (180∘×360∘superscript180superscript360180^{\\circ}\\times 360^{\\circ}180 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT × 360 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT) but limited by spherical distortions, as depicted in Fig. 1. This motivates us to explore whether directly applying DAM to 360 images can be effective under various conditions. To this end, we establish, for the first time, a unified and comprehensive benchmark (See Fig. 2) that evaluates DAM’s performance across several key properties of 360 images and models: 1) Different representations of 360 images: The choice of representations is vital for the model to learn effective features. The representations include equirectangular projection (ERP), cube map, tangent patches, etc. Each representation offers distinct advantages and disadvantages concerning the FoVs and distortion levels. 2) Various spatial transformations: They occur when 360 images are not captured vertically, which makes the appearance of 360 images vary greatly [21]. For instance, in the VR environment, users tend to transform 360 images by changing their viewing directions and zooming in on objects of interest [22]. After the transformations, extra distortions are introduced to 360 images, making it nontrivial to estimate 360 depth. 3) Diverse scenes: While 360 depth datasets are mainly collected inside buildings [9], it is crucial to assess DAM’s generalization capacity to wider scenes, e.g., outdoor scenes (See Fig. 1). Additionally, we examine other factors that might impact DAM’s performance, including the backbone model sizes of DAM and optimization space (supervision with disparity or metric depth) [18]. Our benchmark unveils several key findings: 1) The ERP representation exhibits the best zero-shot capacity when no post-processing is considered; 2) The robustness of DAM to spatial transformations is not expected (See Tab. 2); 3) The performance of DAM is less effective in some scenes, especially for the objects at the equator, as illustrated in Fig. 1; 4) After pilot experiments, we find that supervising with disparity is less effective for fine-tuning DAM to 360 images, despite the size of the backbone model.\n\nTo address these challenges, we first collect a large-scale unlabeled dataset encompassing a broad range of indoor and outdoor scenes (See Sec. 4.1). We then propose a semi-supervised learning (SSL) framework to develop a 360 DAM, named Any360D. This framework leverages a combination of large-scale unlabeled data and labeled data. Under the umbrella of SSL, Any360D initially trains a teacher model by fine-tuning DAM with the Low-Rank Adaptation (LoRA) [23] (See Sec. 4.2). The optimization space is chosen as metric depth, as we discover that metric depth supervision is effective in recovering structural details at the equator. To harness the potential of large-scale unlabeled data, we propose an Möbius transformation-based spatial augmentation (MTSA) to impose consistency regularization between the unlabeled data and spatially transformed ones (See Sec. 4.3). Such augmentation improves the robustness of our student model to various spatial transformations even under significant distortions (See Tab. 4). Extensive experiments confirm the effectiveness of our Any360D under various spatial transformations and diverse scenes.\n\nIn summary, our contributions are three-fold: (I) We establish the first comprehensive benchmark for evaluating the performance of DAM for 360 images across several key properties. (II) Drawing insights from the benchmark, we introduce a semi-supervised learning framework, dubbed Any360D, which leverages large-scale unlabeled 360 images and Möbius transformation-based spatial augmentation (MTSA) to improve the generalization capacity and robustness of our model, respectively. (III) Experimental results show the impressive zero-shot capacity of Any360D for being a 360 depth foundation model across various spatial transformations and diverse scenes.\n\n2 Related Work\n\nMonocular 360 Depth Estimation. With the advance of deep learning and 360 depth datasets [9, 10, 24], monocular 360 depth estimation methods have obtained good performance in specific datasets [9, 10, 24, 11]. Previous methods mainly focus on mitigating the negative effects of distortion. For example, they have carefully designed distortion-aware convolution kernels [25, 2], considered spherical prior [26], or transformed the ERP image into distortion-less representations, e.g., cube map [27] and tangent patches [13, 12], and narrow FoV slices [28, 29, 30]. However, as most 360 depth datasets are captured in indoor scenes with limited amounts, these methods are difficult to generalize to unseen scenes, especially outdoor scenes [15].\n\nZero-shot Monocular 2D Depth Estimation. To enable the depth estimation model to have zero-shot ability, MiDaS [18, 19] proposes to train on multiple perspective depth datasets. To mitigate the gap between different datasets, it introduces an affine-invariant loss to avoid the influence of depth scale and instead focuses on the consistency of depth distribution between prediction and ground truth. Following this direction, ZoeDepth [20] combines disparity and metric depth estimation together. ZoeDepth first trains a disparity depth estimation model on several datasets, and then fine-tunes it on specific datasets to generalize to metric depth estimation. Recently, Depth Anything [1] additionally leverages large-scale unlabeled perspective images to enhance the model’s representation capability with semi-supervised learning. This approach demonstrates excellent generalization ability across diverse scenes, ranging from indoor to outdoor scenes.\n\nSemi-supervised Learning (SSL). It [31, 32] aims to leverage a large number of unlabeled data to improve learning performance with a limited number of labeled samples. Consequently, SSL has been applied to various tasks over the past decade, including image classification [33, 34], object detection [35, 36], semantic segmentation [37, 38], and depth estimation [39, 40]. Inspired by the success of SSL in these tasks, this work aims to leverage a large-scale unlabeled 360 images dataset for developing a 360 DAM. We employ the Möbius transformation as spatial augmentation and utilize consistency regularization between the unlabeled and spatially transformed ones to enhance the training of the student model. The results demonstrate that unlabeled 360 images can significantly enhance the student model’s generalization capability on diverse scenes and robustness on various transformations.\n\n3 Benchmarking DAM\n\nIn this section, we aim to evaluate the performance of DAM on 360 images that consider key properties of 360 images, i.e., 360 image representations, spatial transformations, diverse scenes including both indoor and outdoor ones, optimization space, and backbone model sizes, as shown in Fig. 2. We first introduce the evaluation dataset and evaluation metrics in Sec 3.1. Then, we conduct a large suite of experiments to examine the mentioned properties in Sec. 3.2. Finally, we reveal the crucial findings about the generalization capacity and robustness of DAM.\n\n3.1 Evaluation Protocol\n\nDatasets. We utilize the testing set of the Matterport3D dataset [9] for quantitative comparison, whose scenes are inside buildings. As the DAM outputs disparity depth, we reverse the values of metric depth labels in the Matterport3D dataset. For qualitative evaluation, in addition to the Matterport3D dataset, we also incorporate samples from our collected dataset with diverse indoor and outdoor scenarios. (Details of our dataset can be found in Sec. 4.1).\n\nMetrics. We evaluate the relative depth estimation with standard metrics including Absolute Relative Error (Abs Rel) and Root Mean Squared Error (RMSE). Only the observed pixels in the ground truth depth are considered in these calculations.\n\nPool of 360 Image Representations. The selection of 360 data representations is crucial for the model to learn robust and effective representation. We collect the most commonly used representations: ERP, cube map, tangent patches, and horizontal and vertical slices. These representations vary in FoVs and distortion levels.\n\nPool of Spatial Transformations. We collect two types of spatial transformations that make significant changes for 360 images, i.e., vertical rotation and zoom. Specifically, we set three vertical rotation angles: (2∘superscript22^{\\circ}2 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT, 5∘superscript55^{\\circ}5 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT, 10∘superscript1010^{\\circ}10 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT). We also set three zoom levels: (0.8, 1.2, 1.5). The vertical rotation and zoom are achieved with Möbius transformation, which is the only conformal bijective transformation on the sphere [21].\n\nPool of Backbone Model Sizes. The backbone models are selected from ViT-S, ViT-B, and ViT-L, following [1].\n\n3.2 Evaluation Results\n\nImpact of 360 Image Representations. In Tab. 1, we comprehensively measure the performance of DAM with various 360 image representations as inputs. It can be seen that with different backbone models, ERP representation always obtains the best performance. It demonstrates that the strong zero-shot capacity of DAM can handle the distortions in ERP images in the indoor scenes from the Matterport3D dataset. In addition, we find that distortion-less 360 image representations, e.g., tangent patches can recover more local details, such as the bed in Fig. 3. However, the discrepancies among different patches degrade the overall performance, which requires post-processing. Finally, utilizing slices with narrow FoVs also results in discrepancies that influence the performance.\n\nImpact of Spatial Transformations. Tab. 2 shows DAM’s performance under different vertical rotation angles and zoom levels. Specifically, with the rotation angle increasing, the performance drops slightly. However, there is a drastic performance degradation as the zoom level increases.\n\nPerformance on Indoor and Outdoor Scenes. In addition to indoor scenes from the Matterport3D dataset, we further evaluate the generalization capacity of DAM in diverse indoor and outdoor scenes. These scenes are challenging. For example in Fig. 1(a), the ceiling and floor cover large portions of 360 images, causing objects located at the equator to have small appearance sizes. Unfortunately, DAM shows poor results in these indoor and outdoor scenes. Specifically, several objects at the equator, e.g., corridors and cars, are missing in the results.\n\nImpact of Optimization Space. Previous depth foundation models [1, 20] employ the affine-invariant loss to enable multi-dataset joint training. In this case, the optimization space is based on disparities, which have the largest value for the closest object. In our pilot studies, we find that although fine-tuning DAM with disparity supervision would obtain finer results in the specific dataset [9], its generalization capacity for unseen scenes is still limited. Moreover, the unsatisfactory regions are mostly at the equator. We conjecture that the disparity supervision would focus more on close objects. However, distant regions are often located at the equator in 360 images. To make the optimization focus on the equator region, we attempt to optimize the model with metric supervision and impressively find that the performance of DAM improves significantly for the equator region, as depicted in Fig. 4.\n\nImpact of Different Backbone Models. As shown in Tabs. 1 and 2, with the model size increasing, the zero-shot capability of DAM increases slightly. However, in Fig. 1, DAM with ViT-L as the backbone are still less effective at the equator, causing structural details missing or blurry.\n\nOur findings can be summarized as follows: 1) The ERP representation has the best zero-shot performance when no post-processing is added. 2) The robustness of DAM for spatial transformation needs to be improved, especially for the zoom operation. 3) The optimization space needs to be transformed from disparity supervision to metric depth supervision to make the structural details of 360 images clear, especially for the equator region. 4) DAM performs well in some scenes, e.g., rooms, but has poor results for scenes when the appearance sizes of objects at the equator become small, even with the ViT-L backbone.\n\n4 Any360D Model with New Designs\n\nBased on the findings from our benchmark, we initially collect a large-scale unlabeled dataset, aimed at enriching the diversity and generalization capability of our model (Refer to Sec.4.1). Following this, we exploit a supervised training framework to fine-tune DAM using LoRA [23] with a labeled 360 indoor dataset, resulting in our teacher model 𝒯𝒯\\mathcal{T}caligraphic_T (See Sec. 4.2). Subsequently, the teacher model 𝒯𝒯\\mathcal{T}caligraphic_T is utilized to generate pseudo depth labels for the existing unlabeled datasets as well as for our collected dataset. Formally, the labeled and unlabeled sets are denoted as 𝒟l=superscript𝒟𝑙absent\\mathcal{D}^{l}=caligraphic_D start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT = {(xi,di)}i=1Msuperscriptsubscriptsubscript𝑥𝑖subscript𝑑𝑖𝑖1𝑀\\left\\{\\left(x_{i},d_{i}\\right)\\right\\}_{i=1}^{M}{ ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT and 𝒟u={ui}i=1Nsuperscript𝒟𝑢superscriptsubscriptsubscript𝑢𝑖𝑖1𝑁\\mathcal{D}^{u}=\\left\\{u_{i}\\right\\}_{i=1}^{N}caligraphic_D start_POSTSUPERSCRIPT italic_u end_POSTSUPERSCRIPT = { italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT, respectively. M𝑀Mitalic_M and N𝑁Nitalic_N are the number of samples in the labeled and unlabeled datasets, respectively. Lastly, the student model 𝒮𝒮\\mathcal{S}caligraphic_S is trained on a combination of the labeled data and the pseudo-labeled set, employing color and Möbius transformation-based spatial augmentation (MTSA) specific to 360 images (See Sec. 4.3). The overview of Any360D is shown in Fig. 6.\n\n4.1 Dataset Collection\n\nWe collect and integrate existing 360 datasets, both labeled and unlabeled. Specifically, for indoor scenes, we utilize the Matterport3D dataset [9] with depth ground truth, and the training set of the Zillow Indoor Dataset (ZInD)[41] without depth ground truth. Additionally, we collect an unlabeled dataset encompassing both indoor and outdoor scenes. This dataset enhances our benchmark by enriching the validation for DAM under diverse scenes and is utilized to train our model. The collected dataset features a comprehensive array of scenes at the campus level (See Fig. 5). To ensure privacy, we anonymize the footage by blurring identifiable faces, employing methods similar to those in [42].\n\n4.2 Stage 1: Fine-tuning DAM with Labeled Data\n\nBased on the quantitative and qualitative results regarding 360 image representations and spatial transformations, etc. In Sec. 3.2, we observe that DAM performs suboptimally on 360 images due to inherent distortions compared to perspective images. This necessitates fine-tuning DAM for accurate depth prediction in 360 images. However, the large model size and limited labeled 360 images make direct training impractical. Consequently, we employ the LoRA to fine-tune the DAM encoder. Additionally, results indicate a significant performance drop at the equator regions, particularly in outdoor scenes, due to disparity depth supervision (See Fig. 4). Existing depth models provide disparity depth estimation by factoring out scale, resulting in outputs that lack metric meaning, thereby limiting applicability in distant regions and objects with small sizes. To address this, we employ two types of metric depth heads to fine-tune DAM with metric depth supervision instead of disparity depth supervision. The first type of head employs a bin structure [20], while the second type of head consists of two convolutional layers. Detailed methodology is provided in the following sections.\n\nFor each weight in the encoder, we utilize a low-rank approximation ω=A⁢B𝜔𝐴𝐵\\omega=ABitalic_ω = italic_A italic_B, where only A𝐴Aitalic_A and B𝐵Bitalic_B are updated via backpropagation during adaptation. By inputting labeled data xisubscript𝑥𝑖x_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT into DAM with this low-rank approximation, we obtain the disparity depth. Subsequently, this disparity depth is fed into a metric depth head to generate the metric depth prediction di^^subscript𝑑𝑖\\hat{d_{i}}over^ start_ARG italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG. To train the teacher model 𝒯𝒯\\mathcal{T}caligraphic_T, we employ the Scale-Invariant Logarithmic (SILog) [43] loss to quantify the discrepancy between the predictions di^^subscript𝑑𝑖\\hat{d_{i}}over^ start_ARG italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG and the ground truth disubscript𝑑𝑖d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. This SILog loss is formulated as ℒsT=S⁢I⁢L⁢o⁢g⁢(d^i,di)superscriptsubscriptℒ𝑠𝑇𝑆𝐼𝐿𝑜𝑔subscript^𝑑𝑖subscript𝑑𝑖\\mathcal{L}_{s}^{T}=SILog\\left(\\hat{d}_{i},d_{i}\\right)caligraphic_L start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT = italic_S italic_I italic_L italic_o italic_g ( over^ start_ARG italic_d end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ).\n\n4.3 Stage 2: Semi-supervised Learning with Unlabeled Data\n\nDespite the limited availability of labeled data, leveraging a comprehensive collection of readily available and diverse unlabeled images significantly improves the model’s generalization capability and robustness. To harness the potential of large-scale unlabeled data, we introduce strong color augmentation (CA) and Möbius transformation-based spatial augmentation (MTSA) to impose consistency regularization between unlabeled data and spatially transformed ones, as shown in Fig. 6.\n\nColor Augmentation (CA). Given an image uisubscript𝑢𝑖u_{i}italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, we exploit strong color augmentations 𝒞𝒞\\mathcal{C}caligraphic_C, such as color jittering, resulting in augmented data 𝒞⁢(ui)𝒞subscript𝑢𝑖\\mathcal{C}(u_{i})caligraphic_C ( italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ).\n\nMöbius Transformation-based Spatial Augmentation (MTSA). We briefly introduce the process of MTSA ℳ(.)\\mathcal{M}(.)caligraphic_M ( . ). As illustrated in Fig. 7, a 360 image uisubscript𝑢𝑖u_{i}italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT undergoes an initial projection from the plane to the sphere via spherical projection (SP). Subsequently, this spherical representation is projected onto the complex plane using stereographic projection (STP). In our conduction, the specific point on the complex plane is determined by the intersection of the equator point and a designated spherical point. The Möbius transformation is applied on the complex plane. Following this, we apply the inverse stereographic projection (STP-1) and inverse spherical projection (SP-1) to obtain the transformed 360 image ℳ⁢(ui)ℳsubscript𝑢𝑖\\mathcal{M}(u_{i})caligraphic_M ( italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ).\n\nSemi-supervision. The following outlines the process of training the student model 𝒮𝒮\\mathcal{S}caligraphic_S with semi-supervision on labeled and unlabeled datasets. As depicted in Fig. 6, initially, the teacher model 𝒯𝒯\\mathcal{T}caligraphic_T generates the pseudo depth d^iusuperscriptsubscript^𝑑𝑖𝑢\\hat{d}_{i}^{u}over^ start_ARG italic_d end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_u end_POSTSUPERSCRIPT for the unlabeled data uisubscript𝑢𝑖u_{i}italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, while the student model 𝒮𝒮\\mathcal{S}caligraphic_S provides the depth prediction d¯iusuperscriptsubscript¯𝑑𝑖𝑢\\bar{d}_{i}^{u}over¯ start_ARG italic_d end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_u end_POSTSUPERSCRIPT. Subsequently, the augmented unlabeled data 𝒞⁢(ui)𝒞subscript𝑢𝑖\\mathcal{C}(u_{i})caligraphic_C ( italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) and ℳ⁢(ui)ℳsubscript𝑢𝑖\\mathcal{M}(u_{i})caligraphic_M ( italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) are fed into the student model 𝒮𝒮\\mathcal{S}caligraphic_S to produce predictions d~iusuperscriptsubscript~𝑑𝑖𝑢\\widetilde{d}_{i}^{u}over~ start_ARG italic_d end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_u end_POSTSUPERSCRIPT and d¨iusuperscriptsubscript¨𝑑𝑖𝑢\\ddot{d}_{i}^{u}over¨ start_ARG italic_d end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_u end_POSTSUPERSCRIPT, respectively. Consistency regularization is enforced by ensuring structural information consistency between the unlabeled data and spatially transformed ones, thereby enhancing the student model’s robustness to distortions. The consistency regularization is defined as ℒc=ℒc⁢1+ℒc⁢2subscriptℒ𝑐subscriptℒ𝑐1subscriptℒ𝑐2\\mathcal{L}_{c}=\\mathcal{L}_{c1}+\\mathcal{L}_{c2}caligraphic_L start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT = caligraphic_L start_POSTSUBSCRIPT italic_c 1 end_POSTSUBSCRIPT + caligraphic_L start_POSTSUBSCRIPT italic_c 2 end_POSTSUBSCRIPT, where ℒc⁢1=S⁢I⁢L⁢o⁢g⁢(d~iu,d¯iu)subscriptℒ𝑐1𝑆𝐼𝐿𝑜𝑔superscriptsubscript~𝑑𝑖𝑢superscriptsubscript¯𝑑𝑖𝑢\\mathcal{L}_{c1}=SILog(\\widetilde{d}_{i}^{u},\\bar{d}_{i}^{u})caligraphic_L start_POSTSUBSCRIPT italic_c 1 end_POSTSUBSCRIPT = italic_S italic_I italic_L italic_o italic_g ( over~ start_ARG italic_d end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_u end_POSTSUPERSCRIPT , over¯ start_ARG italic_d end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_u end_POSTSUPERSCRIPT ) and ℒc⁢2=S⁢I⁢L⁢o⁢g⁢(d¨iu,ℳ⁢(d¯iu))subscriptℒ𝑐2𝑆𝐼𝐿𝑜𝑔superscriptsubscript¨𝑑𝑖𝑢ℳsuperscriptsubscript¯𝑑𝑖𝑢\\mathcal{L}_{c2}=SILog(\\ddot{d}_{i}^{u},\\mathcal{M}(\\bar{d}_{i}^{u}))caligraphic_L start_POSTSUBSCRIPT italic_c 2 end_POSTSUBSCRIPT = italic_S italic_I italic_L italic_o italic_g ( over¨ start_ARG italic_d end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_u end_POSTSUPERSCRIPT , caligraphic_M ( over¯ start_ARG italic_d end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_u end_POSTSUPERSCRIPT ) ). Note that d¨iusuperscriptsubscript¨𝑑𝑖𝑢\\ddot{d}_{i}^{u}over¨ start_ARG italic_d end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_u end_POSTSUPERSCRIPT is obtained from the data with Möbius transformation ℳℳ\\mathcal{M}caligraphic_M. To keep the consistency with unlabeled data, the prediction d¯iusuperscriptsubscript¯𝑑𝑖𝑢\\bar{d}_{i}^{u}over¯ start_ARG italic_d end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_u end_POSTSUPERSCRIPT of unlabeled data is also transformed with Möbius transformation ℳℳ\\mathcal{M}caligraphic_M. Moreover, to fully harness the potential of the unlabeled dataset, we employ pseudo labels dusuperscript𝑑𝑢d^{u}italic_d start_POSTSUPERSCRIPT italic_u end_POSTSUPERSCRIPT of unlabeled data for self-training, the objective of self-training is formulated as ℒp=S⁢I⁢L⁢o⁢g⁢(di¯u,d^iu).subscriptℒ𝑝𝑆𝐼𝐿𝑜𝑔superscript¯subscript𝑑𝑖𝑢superscriptsubscript^𝑑𝑖𝑢\\mathcal{L}_{p}=SILog\\left(\\bar{d_{i}}^{u},\\hat{d}_{i}^{u}\\right).caligraphic_L start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT = italic_S italic_I italic_L italic_o italic_g ( over¯ start_ARG italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG start_POSTSUPERSCRIPT italic_u end_POSTSUPERSCRIPT , over^ start_ARG italic_d end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_u end_POSTSUPERSCRIPT ) .\n\n4.4 Total Objective\n\nFinally, based on the designed loss terms: consistency regularization loss and self-training loss, the objective for training the student model 𝒮𝒮\\mathcal{S}caligraphic_S is formulated as ℒ=ℒss+λp∗ℒp+λc∗ℒc,ℒsuperscriptsubscriptℒ𝑠𝑠subscript𝜆𝑝subscriptℒ𝑝subscript𝜆𝑐subscriptℒ𝑐\\mathcal{L}=\\mathcal{L}_{s}^{s}+\\lambda_{p}*\\mathcal{L}_{p}+\\lambda_{c}*% \\mathcal{L}_{c},caligraphic_L = caligraphic_L start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT + italic_λ start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ∗ caligraphic_L start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT + italic_λ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ∗ caligraphic_L start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT , where λcsubscript𝜆𝑐\\lambda_{c}italic_λ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT and λpsubscript𝜆𝑝\\lambda_{p}italic_λ start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT are the trade-off parameters to balance the three terms, both set to 1. And ℒsSsuperscriptsubscriptℒ𝑠𝑆\\mathcal{L}_{s}^{S}caligraphic_L start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT is introduced to supervise the training of student model 𝒮𝒮\\mathcal{S}caligraphic_S with ground truth disubscript𝑑𝑖d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and prediction d˘isubscript˘𝑑𝑖\\breve{d}_{i}over˘ start_ARG italic_d end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT of labeled data. Note that we evaluate our student model during inference to demonstrate the effectiveness of Any360D.\n\n5 Experiment\n\nThe datasets utilized in this work include the Matterport3D dataset [9], our self-collected dataset, and unlabeled ZInD [41] dataset with indoor scenes. The spatial resolution of 360 images and corresponding depth maps is 504×10085041008504\\times 1008504 × 1008. All experiments are conducted on a single A800 GPU with a batch size of 4 and a learning rate of 1e-4, using the Adam optimizer [45]. By default, the teacher model employs the ViT-Las the backbone, while the student model employs the ViT-Small as the backbone for efficient learning. The teacher model is trained for 20 epochs, while the student model is trained for 40 epochs when the quantities of labeled and unlabeled data are equal, and for 20 epochs otherwise. Color augmentation is consistent with the previous work [14]. For the Möbius transformation, the vertical rotation angle is randomly sampled from [−10∘superscript10-10^{\\circ}- 10 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT, 10∘superscript1010^{\\circ}10 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT], and the zoom level is randomly sampled from [1, 1.2].\n\n5.1 Qualitative and Quantitative Evaluation\n\nTab. 3 presents a quantitative comparison of SOTA monocular 360 depth estimation methods on the Matterport3D dataset. To ensure fairness, we re-train these methods following their official protocols. Our student model, incorporating a CNN metric depth head, surpasses previous methods across all metrics. Notably, it demonstrates robustness under various spatial transformations. For instance, at a zoom level of 1.5, our student model achieves a 44.2% improvement in the Abs Rel metric compared to EGFormer [26]. This significant enhancement is attributed not only to the strong teacher model with fine-tuning but also to our semi-supervised learning approach utilizing MTSA. Moreover, the student model with ViT-S as the backbone sometimes even outperforms the teacher model with ViT-L as the backbone at certain zoom levels. As shown in Fig.8, our method accurately predicts structural details such as sofas and boards, which are absent in the results of other methods. Unlike data-specific approaches, our method exhibits impressive zero-shot capacity, as demonstrated in Fig. 9 across diverse scenes.\n\n5.2 Ablation Studies\n\nFine-tuning and Metric Depth Supervision. Tab. 5 shows the impact of fine-tuning on different backbone models and metric heads. The first and second rows reveal that fine-tuning is crucial since DAM is trained on perspective images. Comparing the second and third rows, we observe that using CNN layers as the metric depth head yields superior performance compared to the ZoeDepth head [20].\n\nThis can be attributed to the complexity of the ZoeDepth metric head, which introduces artifacts in texture-less regions when the backbone model is small, disrupting the continuity of depth prediction and leading to sub-optimal results. With a large backbone model, e.g., ViT-L, utilizing the metric depth head of ZoeDepth performs better.\n\nLoss Functions. In Tab. 4, we discuss the impacts of loss functions in semi-supervision process. We take DAM with the metric depth head of ZoeDepth as an example. By default, the loss function ℒsSsuperscriptsubscriptℒ𝑠𝑆\\mathcal{L}_{s}^{S}caligraphic_L start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT is applied. Initially, incorporating unlabeled data with pseudo labels yields a performance improvement. Subsequently, adding color augmentation facilitate the student model to learn robust representations, resulting in enhanced model performance. However, there is no significant improvement in robustness for vertical rotation and zoom. Finally, by introducing Möbius augmentation into the semi-supervision process, the model’s robustness to various spatial transformations is significantly enhanced, demonstrating the necessity of incorporating Möbius transformation as a data augmentation technique.\n\nUtilization of Unlabeled Data. In Tab. 6, we examine the impact of varying amounts of unlabeled data on overall performance, utilizing the metric depth head with CNN layers by default. We begin by adding unlabeled data from the ZInD dataset in the same quantity as the Matterport3D dataset. Given the similar indoor scenes in both datasets, the unlabeled data helps the model learn robust and effective representations. Furthermore, scaling up the unlabeled data to 54,034 samples (the entire ZInD training set) results\n\nin no significant performance improvement. This suggests that increasing the amount of training data with similar scenes does not significantly enhance model performance. However, by incorporating our collected data with diverse indoor and outdoor scenes, the model’s performance improves significantly. This demonstrates that large-scale, unlabeled 360 data with diverse scenes can substantially empower monocular 360 depth estimation.\n\n6 Conclusion and Limitations\n\nConclusion. In this paper, we provided a comprehensive benchmark for evaluating the performance of DAM on 360 images by exploring several key properties, e.g., representations and spatial transformations. The benchmark reveals some key findings, including the limited effectiveness of disparity supervision in refining structural details in 360 images. Leveraging these crucial findings, we fine-tune DAM and uncover the potential of large-scale unlabeled data with Möbius augmentation under the umbrella of semi-supervised learning. The experiments underscore our model’s impressive zero-shot capacity, establishing it as a foundational model for 360 depth estimation task.\n\nBroader Impacts. Our established benchmark and proposed semi-supervised learning framework offer valuable insights for researchers aiming to fine-tune foundation models for 360 images. Any360D can provide effective structural priors to support various tasks, such as 360 semantic segmentation.\n\nLimitations and Future Work. Due to the scarcity of 360 depth labels in diverse scenes, particularly outdoor scenes, our teacher model is trained on a limited scenes, potentially generating noisy pseudo labels for unseen scenes. To enhance the zero-shot capability of our model, future work will focus on collecting 360 images paired with depth labels across a broader range of environments.\n\n7 Appendix\n\nAppendix A Dataset\n\nDuring the data collection phase, we used the RICOH THETA Z1 360 camera to capture videos in various scenes at the campus level. The RICOH THETA Z1 360 camera features dual 1-inch back-illuminated CMOS sensors, capable of capturing 4K (3840 ×\\times× 1920) video at 30 frames per second. During the video collection phase, we used a selfie stick to minimize the presence of the camera operator in the footage, thereby ensuring the quality of the collected dataset. We collected dozens of indoor and outdoor videos. Specifically, we collected 20 video clips of indoor scenes and 10 video clips of outdoor scenes. To ensure high-quality footage, we meticulously planned the shooting routes in advance to improve our shooting efficiency. Each video was kept under 5 minutes in duration. During the export phase, we utilized the camera’s built-in correction options to ensure the footage was in equirectangular format. To protect the privacy of the camera operators and passersby, we anonymized the videos by identifying and applying blurring filters to faces in each frame, similar to [42]. Subsequently, we selected anonymized 360 images from each frame based on the following criteria:\n\n•\n\nThe frame of the scene needs to be concise, diverse, and representative.\n\n•\n\nThere are no adverse effects from the environment, lighting, or humidity.\n\n•\n\nPrivacy protection measures should be successfully implemented.\n\nAfter completing the data selection, our Diverse360 dataset contains a total of 12,063 images, comprising 7,887 indoor 360 images and 4,176 outdoor 360 images. Our Diverse360 dataset showcases a wide variety of scenes at the campus level (See Fig. 10. The Diverse360 dataset enriches the validation for DAM in diverse scenes. In addition, this dataset is utilized to train our model, which is demonstrated to improve the overall performance of our model.\n\nAppendix B Metrics\n\nMetrics. We evaluate the performance of models with two standard metrics: Absolute Relative Error (Abs Rel), and Root Mean Squared Error (RMSE). We only evaluate the performance on the valid regions with the ground truth depth D∗superscript𝐷D^{*}italic_D start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT. We denote the number of valid pixels as K𝐾Kitalic_K. With the prediction D𝐷Ditalic_D, the two metrics can be formulated as follows:\n\n•\n\nAbsolute Relative Error (Abs Rel):\n\n1K⁢∑i=1K‖D⁢(i)−D∗⁢(i)‖D∗⁢(i).1𝐾superscriptsubscript𝑖1𝐾norm𝐷𝑖superscript𝐷𝑖superscript𝐷𝑖\\frac{1}{K}\\sum_{i=1}^{K}\\frac{||D(i)-D^{*}(i)||}{D^{*}(i)}.divide start_ARG 1 end_ARG start_ARG italic_K end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT divide start_ARG | | italic_D ( italic_i ) - italic_D start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_i ) | | end_ARG start_ARG italic_D start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_i ) end_ARG . (1)\n\n•\n\nRoot Mean Square Error (RMSE):\n\n1K⁢∑i=1K‖D⁢(i)−D∗⁢(i)‖2.1𝐾superscriptsubscript𝑖1𝐾superscriptnorm𝐷𝑖superscript𝐷𝑖2\\sqrt{\\frac{1}{K}\\sum_{i=1}^{K}||D(i)-D^{*}(i)||^{2}}.square-root start_ARG divide start_ARG 1 end_ARG start_ARG italic_K end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT | | italic_D ( italic_i ) - italic_D start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_i ) | | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG . (2)\n\nMore details of benchmarking DAM. Depth Anything model (DAM) is not trained on 360 images. To evaluate the zero-shot performance of DAM on 360 images, in Tabs. 1, and 2 on the main paper, we manually align the prediction of DAM with the depth ground truth with scale and shift, similar to [18, 19]. In addition, since our Any360D is first fine-tuned on a 360 depth dataset [9], we do not utilize any alignment operation in the ablation studies (Tabs. 3, 4, and 6 on the main paper) and comparison with SOTA monocular 360 depth estimation methods (Tab. 5 on the main paper).\n\nAppendix C The Proposed Method\n\nC.1 Möbius Transformation-based Spatial Augmentation (MTSA)\n\nWe provide more details of conducting the MTSA. Firstly, we provide the formulas of the geometric projections. Then, we describe the parameters of the MTSA in rotation and zoom. The formulas are based on [21]. Differently, we take the equator center as the pole to zoom in on the objects at the equator.\n\nThe Möbius transformation is conducted in the complex plane. To achieve it, a 360 image with ERP format is first projected from the plane to the sphere via spherical projection (SP). The plane coordinate is proportional to the angle coordinate (θ,ϕ𝜃italic-ϕ\\theta,\\phiitalic_θ , italic_ϕ) (where θ𝜃\\thetaitalic_θ represents the longitude and ϕitalic-ϕ\\phiitalic_ϕ represents the latitude), while the spherical coordinate can be defined as (x,y,z𝑥𝑦𝑧x,y,zitalic_x , italic_y , italic_z). In this case, SP can be defined as follows:\n\nSP:(xyz)=(cos⁡(ϕ)⁢cos⁡(θ)cos⁡(ϕ)⁢sin⁡(θ)sin⁡(ϕ)).:SPmatrix𝑥𝑦𝑧matrixitalic-ϕ𝜃italic-ϕ𝜃italic-ϕ\\text{SP}:\\begin{pmatrix}x\\\\ y\\\\ z\\\\ \\end{pmatrix}=\\begin{pmatrix}\\cos(\\phi)\\cos(\\theta)\\\\ \\cos(\\phi)\\sin(\\theta)\\\\ \\sin(\\phi)\\\\ \\end{pmatrix}.SP : ( start_ARG start_ROW start_CELL italic_x end_CELL end_ROW start_ROW start_CELL italic_y end_CELL end_ROW start_ROW start_CELL italic_z end_CELL end_ROW end_ARG ) = ( start_ARG start_ROW start_CELL roman_cos ( italic_ϕ ) roman_cos ( italic_θ ) end_CELL end_ROW start_ROW start_CELL roman_cos ( italic_ϕ ) roman_sin ( italic_θ ) end_CELL end_ROW start_ROW start_CELL roman_sin ( italic_ϕ ) end_CELL end_ROW end_ARG ) . (3)\n\nThen, we project from the sphere to the complex plane with stereographic projection (STP). By defining the coordinate of the complex plane as Z=(x′,y′)𝑍superscript𝑥′superscript𝑦′Z=(x^{\\prime},y^{\\prime})italic_Z = ( italic_x start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT , italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) and selecting the equator center as the pole, the STP can be formulated as follows:\n\nSTP:x′=y1−x,y′=z1−x.:STPformulae-sequencesuperscript𝑥′𝑦1𝑥superscript𝑦′𝑧1𝑥\\text{STP}:x^{\\prime}={\\frac{y}{1-x}}\\ ,\\ y^{\\prime}={\\frac{z}{1-x}}.STP : italic_x start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT = divide start_ARG italic_y end_ARG start_ARG 1 - italic_x end_ARG , italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT = divide start_ARG italic_z end_ARG start_ARG 1 - italic_x end_ARG . (4)\n\nIn the complex plane, the Möbius transformation is conducted with the following formulation:\n\nf⁢(Z)=a⁢Z+bc⁢Z+d,𝑓𝑍𝑎𝑍𝑏𝑐𝑍𝑑f(Z)={\\frac{aZ+b}{cZ+d}},italic_f ( italic_Z ) = divide start_ARG italic_a italic_Z + italic_b end_ARG start_ARG italic_c italic_Z + italic_d end_ARG , (5)\n\nwhere a𝑎aitalic_a, b𝑏bitalic_b, c𝑐citalic_c, and d𝑑ditalic_d are complex numbers satisfying a⁢d−b⁢c≠0𝑎𝑑𝑏𝑐0ad-bc\\neq 0italic_a italic_d - italic_b italic_c ≠ 0. For the vertical rotation with angle β𝛽\\betaitalic_β and zoom level s𝑠sitalic_s, the parameters of Möbius transformations can be represented as follows:\n\n(abcd)=(cos⁡(β)+j⁢sin⁡(β)001).matrix𝑎𝑏𝑐𝑑matrix𝛽𝑗𝛽001\\begin{pmatrix}a&b\\\\ c&d\\\\ \\end{pmatrix}=\\begin{pmatrix}\\cos(\\beta)+j\\sin(\\beta)&0\\\\ 0&1\\\\ \\end{pmatrix}.( start_ARG start_ROW start_CELL italic_a end_CELL start_CELL italic_b end_CELL end_ROW start_ROW start_CELL italic_c end_CELL start_CELL italic_d end_CELL end_ROW end_ARG ) = ( start_ARG start_ROW start_CELL roman_cos ( italic_β ) + italic_j roman_sin ( italic_β ) end_CELL start_CELL 0 end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL 1 end_CELL end_ROW end_ARG ) . (6)\n\nFor the zoom operation with level s𝑠sitalic_s, the parameters of Möbius transformations can be represented as follows:\n\n(abcd)=(s001).matrix𝑎𝑏𝑐𝑑matrix𝑠001\\begin{pmatrix}a&b\\\\ c&d\\\\ \\end{pmatrix}=\\begin{pmatrix}s&0\\\\ 0&1\\\\ \\end{pmatrix}.( start_ARG start_ROW start_CELL italic_a end_CELL start_CELL italic_b end_CELL end_ROW start_ROW start_CELL italic_c end_CELL start_CELL italic_d end_CELL end_ROW end_ARG ) = ( start_ARG start_ROW start_CELL italic_s end_CELL start_CELL 0 end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL 1 end_CELL end_ROW end_ARG ) . (7)\n\nWe show some examples of transformed 360 images in Fig. 12. The Möbius transformation obeys the matrix chain multiplication rule. After the Möbius transformation in the complex plane, we conduct inverse projections to project from the complex plane to the sphere and the plane, respectively. The inverse projections can be formulated as follows:\n\nSTP−1:(xyz)=(−1+x′⁣2+y′⁣21+x′⁣2+y′⁣22⁢x′1+x′⁣2+y′⁣22⁢y′1+x′⁣2+y′⁣2);SP−1:(θϕ)=(arctan⁡(y/x)arcsin⁡(z)).:superscriptSTP1matrix𝑥𝑦𝑧matrix1superscript𝑥′2superscript𝑦′21superscript𝑥′2superscript𝑦′22superscript𝑥′1superscript𝑥′2superscript𝑦′22superscript𝑦′1superscript𝑥′2superscript𝑦′2superscriptSP1:matrix𝜃italic-ϕmatrix𝑦𝑥𝑧\\begin{split}\\text{STP}^{-1}:\\begin{pmatrix}x\\\\ y\\\\ z\\\\ \\end{pmatrix}&=\\begin{pmatrix}\\frac{-1+x^{\\prime 2}+y^{\\prime 2}}{1+x^{\\prime 2% }+y^{\\prime 2}}\\\\ \\frac{2x^{\\prime}}{1+x^{\\prime 2}+y^{\\prime 2}}\\\\ \\frac{2y^{\\prime}}{1+x^{\\prime 2}+y^{\\prime 2}}\\\\ \\end{pmatrix}\\ ;\\\\ \\ \\text{SP}^{-1}:\\begin{pmatrix}\\theta\\\\ \\phi\\\\ \\end{pmatrix}&=\\begin{pmatrix}\\arctan(y/x)\\\\ \\arcsin(z)\\\\ \\end{pmatrix}\\ .\\end{split}start_ROW start_CELL STP start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT : ( start_ARG start_ROW start_CELL italic_x end_CELL end_ROW start_ROW start_CELL italic_y end_CELL end_ROW start_ROW start_CELL italic_z end_CELL end_ROW end_ARG ) end_CELL start_CELL = ( start_ARG start_ROW start_CELL divide start_ARG - 1 + italic_x start_POSTSUPERSCRIPT ′ 2 end_POSTSUPERSCRIPT + italic_y start_POSTSUPERSCRIPT ′ 2 end_POSTSUPERSCRIPT end_ARG start_ARG 1 + italic_x start_POSTSUPERSCRIPT ′ 2 end_POSTSUPERSCRIPT + italic_y start_POSTSUPERSCRIPT ′ 2 end_POSTSUPERSCRIPT end_ARG end_CELL end_ROW start_ROW start_CELL divide start_ARG 2 italic_x start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_ARG start_ARG 1 + italic_x start_POSTSUPERSCRIPT ′ 2 end_POSTSUPERSCRIPT + italic_y start_POSTSUPERSCRIPT ′ 2 end_POSTSUPERSCRIPT end_ARG end_CELL end_ROW start_ROW start_CELL divide start_ARG 2 italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_ARG start_ARG 1 + italic_x start_POSTSUPERSCRIPT ′ 2 end_POSTSUPERSCRIPT + italic_y start_POSTSUPERSCRIPT ′ 2 end_POSTSUPERSCRIPT end_ARG end_CELL end_ROW end_ARG ) ; end_CELL end_ROW start_ROW start_CELL SP start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT : ( start_ARG start_ROW start_CELL italic_θ end_CELL end_ROW start_ROW start_CELL italic_ϕ end_CELL end_ROW end_ARG ) end_CELL start_CELL = ( start_ARG start_ROW start_CELL roman_arctan ( italic_y / italic_x ) end_CELL end_ROW start_ROW start_CELL roman_arcsin ( italic_z ) end_CELL end_ROW end_ARG ) . end_CELL end_ROW (8)\n\nC.2 Metric Depth Head\n\nWe exploit two metric depth heads with our proposed model. (1) The first is a bin structure proposed by ZoeDepth [20]. In our pilot experiment, we find that with this metric depth head, the decoder of the DAM should be frozen for stable training. Although this can preserve structural details, the bin prediction might destroy the continuity of depth prediction in some regions, especially when the backbone model size is small, as shown in Fig. 11(a). (2) Therefore, we try the other metric depth head, which is composed of two convolutional layers. The input and output channels of the first convolutional layer are 1 and 64, respectively. The input and output channels of the second convolutional layer are 64 and 1, respectively. With this head, the decoder of the DAM can be trained simultaneously. The convolutional metric depth head can generate a smooth prediction. However, some details might be neglected. For balance, our teacher model takes the metric depth head with bin structure to provide more structural details in the pseudo labels, while the student model employs the metric depth head with convolutional layers to predict smooth depths in both indoor and outdoor scenes.\n\nMore explanation about the metric depth output. The purpose of supervision in the metric depth space is to recover more structural details at the equator region than supervision in the disparity space. The improvement of metric depth supervision for recovering structural details is also demonstrated in [46, 47]. Differently, as we only train on one 360 depth dataset with labels, we do not conduct additional normalization operations. The metric depth output in our Any360D ranges from 0m to 10m (The maximum depth in the Matterport3D dataset). For scenes not in the Matterport3D dataset, the depth prediction can not reflect the absolute distance between the object and the camera. In addition, utilizing metric depth as supervision on 360 images can neglect the effect of camera intrinsics with various focal lengths. This is because most 360 cameras have fixed focal lengths, and can not achieve optical zoom. The digital zoom is conducted with the Möbius transformation.\n\nC.3 Loss Function\n\nSILog Loss. We only supervise the valid regions with the depth ground truth D∗superscript𝐷D^{*}italic_D start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT. Given the predicted depth D𝐷Ditalic_D, the SILog Loss employs the logarithm difference: Δ⁢Di=log⁡Di−log⁡Di∗Δsubscript𝐷𝑖subscript𝐷𝑖superscriptsubscript𝐷𝑖\\begin{aligned} \\Delta D_{i}=\\log D_{i}-\\log D_{i}^{*}\\end{aligned}start_ROW start_CELL roman_Δ italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = roman_log italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - roman_log italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT end_CELL end_ROW. Then, by defining the number of valid pixels as K𝐾Kitalic_K, the SILog Loss can be formulated as follows:\n\nℒ=α⁢1K⁢∑iΔ⁢Di2−λK2⁢(∑iΔ⁢Di)2,ℒ𝛼1𝐾subscript𝑖Δsuperscriptsubscript𝐷𝑖2𝜆superscript𝐾2superscriptsubscript𝑖Δsubscript𝐷𝑖2\\mathcal{L}=\\alpha\\sqrt{\\frac{1}{K}\\sum_{i}\\Delta D_{i}^{2}-\\frac{\\lambda}{K^{% 2}}(\\sum_{i}\\Delta D_{i})^{2}},caligraphic_L = italic_α square-root start_ARG divide start_ARG 1 end_ARG start_ARG italic_K end_ARG ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT roman_Δ italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT - divide start_ARG italic_λ end_ARG start_ARG italic_K start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ( ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT roman_Δ italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG , (9)\n\nwhere α𝛼\\alphaitalic_α and λ𝜆\\lambdaitalic_λ are two hyper-parameters. We set α𝛼\\alphaitalic_α to 10 and λ𝜆\\lambdaitalic_λ to 0.85, following [7].\n\nAppendix D Experiment\n\nD.1 More Ablation Studies\n\nFor our proposed MTSA, the default setting is that: the vertical rotation angle is uniformly sampled in (−10∘,10∘)superscript10superscript10(-10^{\\circ},10^{\\circ})( - 10 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT , 10 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT ), denoted as 𝒰⁢(−10∘,10∘)𝒰superscript10superscript10\\mathcal{U}(-10^{\\circ},10^{\\circ})caligraphic_U ( - 10 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT , 10 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT ). Moreover, the zoom level is uniformly sampled in (1,1.2)11.2(1,1.2)( 1 , 1.2 ), denoted as 𝒰⁢(1,1.2)𝒰11.2\\mathcal{U}(1,1.2)caligraphic_U ( 1 , 1.2 ). To further discuss the effect of the MTSA by introducing vertical rotation and zoom into spatial augmentation, we conduct ablation studies for the range of sampling distribution.\n\nVertical rotation angle distribution. As shown in Tab. 8, we explore the effects of different angle distributions. It can be seen that different angle distributions have a small influence on the depth prediction on raw 360 images. Moreover, MTSA with a larger angle distribution benefits the depth prediction on 360 images with larger rotation angles.\n\nZoom level distribution. As shown in Tab. 9, we explore the effects of different zoom level distributions. We observe that MTSA with larger zoom level distribution can improve the depth estimation performance on the raw 360 images. For instance, MTSA with zoom level distribution 𝒰⁢(1,2.5)𝒰12.5\\mathcal{U}(1,2.5)caligraphic_U ( 1 , 2.5 ) can obtain 2.2% gain in Abs Rel metric compared with distribution 𝒰⁢(1,1.2)𝒰11.2\\mathcal{U}(1,1.2)caligraphic_U ( 1 , 1.2 ). Moreover, MTSA with zoom level distribution 𝒰⁢(1,2.0)𝒰12.0\\mathcal{U}(1,2.0)caligraphic_U ( 1 , 2.0 ) obtains the best performance for depth prediction with zoom level 1.5. It demonstrates that a larger zoom level distribution can improve the robustness of the model in an appropriate margin. Furthermore, with the zoom level distribution increasing to 𝒰⁢(1,2.5)𝒰12.5\\mathcal{U}(1,2.5)caligraphic_U ( 1 , 2.5 ), the model performance has a slight drop for depth prediction under zoom level 1.5. It reveals that the choice of distribution influences the model performance in different conditions.\n\nMore visualization results. We provide more visualization results of the Depth Anything model (DAM) [1] and our Any360D with the ViT-S as the backbone in various scenes, ranging from indoor scenes to outdoor scenes. Figs. 13, 14, 15, 16, 17, 18 are indoor scenes, while Figs. 19, 20 are outdoor scenes.\n\nDemo description. In the demo video, in addition to the scenes in our Diverse360 dataset, we also provide the visualization results in open-world web scenes. The videos are captured in the following websites: https://www.miraikan.jst.go.jp/en/research/AccessibilityLab/dataset360/. We have obtained their approval to utilize their 360 videos.\n\nReferences\n\n[1] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In CVPR, 2024.\n\n[2] Zhijie Shen, Chunyu Lin, Kang Liao, Lang Nie, Zishuo Zheng, and Yao Zhao. Panoformer: Panorama transformer for indoor 360∘ depth estimation. In ECCV, 2022.\n\n[3] Hao Ai, Zidong Cao, Jinjing Zhu, Haotian Bai, Yucheng Chen, and Lin Wang. Deep learning for omnidirectional vision: A survey and new perspectives. arXiv preprint arXiv:2205.10468, 2022.\n\n[4] Weiming Zhang, Yexin Liu, Xu Zheng, and Lin Wang. Goodsam: Bridging domain and capacity gaps via segment anything model for distortion-aware panoramic semantic segmentation. arXiv preprint arXiv:2403.16370, 2024.\n\n[5] Qi Feng, Hubert PH Shum, and Shigeo Morishima. 360 depth estimation in the wild-the depth360 dataset and the segfuse network. In 2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR), pages 664–673. IEEE, 2022.\n\n[6] Markus Schön, Michael Buchholz, and Klaus Dietmayer. Mgnet: Monocular geometric scene understanding for autonomous driving. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15804–15815, 2021.\n\n[7] Weihao Yuan, Xiaodong Gu, Zuozhuo Dai, Siyu Zhu, and Ping Tan. Neural window fully-connected crfs for monocular depth estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 3916–3925, 2022.\n\n[8] Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka. Adabins: Depth estimation using adaptive bins. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4009–4018, 2021.\n\n[9] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-d data in indoor environments. arXiv preprint arXiv:1709.06158, 2017.\n\n[10] Iro Armeni, Sasha Sax, Amir R Zamir, and Silvio Savarese. Joint 2d-3d-semantic data for indoor scene understanding. arXiv preprint arXiv:1702.01105, 2017.\n\n[11] Georgios Albanis, Nikolaos Zioulis, Petros Drakoulis, Vasileios Gkitsas, Vladimiros Sterzentsenko, Federico Alvarez, Dimitrios Zarpalas, and Petros Daras. Pano3d: A holistic benchmark and a solid baseline for 360deg depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3727–3737, 2021.\n\n[12] Yuyan Li, Yuliang Guo, Zhixin Yan, Xinyu Huang, Ye Duan, and Liu Ren. Omnifusion: 360 monocular depth estimation via geometry-aware fusion. CoRR, abs/2203.00838, 2022.\n\n[13] Hao Ai, Zidong Cao, Yan-Pei Cao, Ying Shan, and Lin Wang. Hrdfuse: Monocular 360deg depth estimation by collaboratively learning holistic-with-regional depth distributions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13273–13282, 2023.\n\n[14] Hualie Jiang, Zhe Sheng, Siyu Zhu, Zilong Dong, and Rui Huang. Unifuse: Unidirectional fusion for 360∘ panorama depth estimation. IEEE Robotics and Automation Letters, 6:1519–1526, 2021.\n\n[15] Manuel Rey-Area, Mingze Yuan, and Christian Richardt. 360monodepth: High-resolution 360deg monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3762–3772, 2022.\n\n[16] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4015–4026, 2023.\n\n[17] Lei Ke, Mingqiao Ye, Martin Danelljan, Yu-Wing Tai, Chi-Keung Tang, Fisher Yu, et al. Segment anything in high quality. Advances in Neural Information Processing Systems, 36, 2024.\n\n[18] René Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE transactions on pattern analysis and machine intelligence, 44(3):1623–1637, 2020.\n\n[19] Reiner Birkl, Diana Wofk, and Matthias Müller. Midas v3. 1–a model zoo for robust monocular relative depth estimation. arXiv preprint arXiv:2307.14460, 2023.\n\n[20] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias Müller. Zoedepth: Zero-shot transfer by combining relative and metric depth. arXiv preprint arXiv:2302.12288, 2023.\n\n[21] Zidong Cao, Hao Ai, Yan-Pei Cao, Ying Shan, Xiaohu Qie, and Lin Wang. Omnizoomer: Learning to move and zoom in on sphere at high-resolution. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12897–12907, 2023.\n\n[22] Zidong Cao, Zhan Wang, Yexin Liu, Yan-Pei Cao, Ying Shan, Wei Zeng, and Lin Wang. Learning high-quality navigation and zooming on omnidirectional images in virtual reality. arXiv preprint arXiv:2405.00351, 2024.\n\n[23] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.\n\n[24] Nikolaos Zioulis, Antonis Karakottas, Dimitrios Zarpalas, and Petros Daras. Omnidepth: Dense depth estimation for indoors spherical panoramas. In Proceedings of the European Conference on Computer Vision (ECCV), pages 448–465, 2018.\n\n[25] Benjamin Coors, Alexandru Paul Condurache, and Andreas Geiger. Spherenet: Learning spherical representations for detection and classification in omnidirectional images. In Proceedings of the European conference on computer vision (ECCV), pages 518–533, 2018.\n\n[26] Ilwi Yun, Chanyong Shin, Hyunku Lee, Hyuk-Jae Lee, and Chae Eun Rhee. Egformer: Equirectangular geometry-biased transformer for 360 depth estimation. arXiv preprint arXiv:2304.07803, 2023.\n\n[27] Fu-En Wang, Yu-Hsuan Yeh, Min Sun, Wei-Chen Chiu, and Yi-Hsuan Tsai. Bifuse: Monocular 360 depth estimation via bi-projection fusion. In CVPR, pages 459–468. Computer Vision Foundation / IEEE, 2020.\n\n[28] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Hohonet: 360 indoor holistic understanding with latent horizontal features. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2573–2582, 2020.\n\n[29] Giovanni Pintore, Marco Agus, Eva Almansa, Jens Schneider, and Enrico Gobbetti. Slicenet: deep dense depth estimation from a single indoor panorama using a slice-based representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11536–11545, 2021.\n\n[30] Haozheng Yu, Lu He, Bing Jian, Weiwei Feng, and Shan Liu. Panelnet: Understanding 360 indoor environment via panel representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 878–887, 2023.\n\n[31] Jesper E Van Engelen and Holger H Hoos. A survey on semi-supervised learning. Machine learning, 109(2):373–440, 2020.\n\n[32] Xiangli Yang, Zixing Song, Irwin King, and Zenglin Xu. A survey on deep semi-supervised learning. IEEE Transactions on Knowledge and Data Engineering, 35(9):8934–8954, 2022.\n\n[33] Yidong Wang, Hao Chen, Yue Fan, Wang Sun, Ran Tao, Wenxin Hou, Renjie Wang, Linyi Yang, Zhi Zhou, Lan-Zhe Guo, et al. Usb: A unified semi-supervised learning benchmark for classification. Advances in Neural Information Processing Systems, 35:3938–3961, 2022.\n\n[34] Jinjing Zhu, Haotian Bai, and Lin Wang. Patch-mix transformer for unsupervised domain adaptation: A game perspective. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3561–3571, 2023.\n\n[35] Kihyuk Sohn, Zizhao Zhang, Chun-Liang Li, Han Zhang, Chen-Yu Lee, and Tomas Pfister. A simple semi-supervised learning framework for object detection. arXiv preprint arXiv:2005.04757, 2020.\n\n[36] Mengde Xu, Zheng Zhang, Han Hu, Jianfeng Wang, Lijuan Wang, Fangyun Wei, Xiang Bai, and Zicheng Liu. End-to-end semi-supervised object detection with soft teacher. In Proceedings of the IEEE/CVF international conference on computer vision, pages 3060–3069, 2021.\n\n[37] Aneesh Rangnekar, Christopher Kanan, and Matthew Hoffman. Semantic segmentation with active semi-supervised learning. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 5966–5977, 2023.\n\n[38] Yassine Ouali, Céline Hudelot, and Myriam Tami. Semi-supervised semantic segmentation with cross-consistency training. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12674–12684, 2020.\n\n[39] Jongbeom Baek, Gyeongnyeon Kim, and Seungryong Kim. Semi-supervised learning with mutual distillation for monocular depth estimation. In 2022 International Conference on Robotics and Automation (ICRA), pages 4562–4569. IEEE, 2022.\n\n[40] Yevhen Kuznietsov, Jorg Stuckler, and Bastian Leibe. Semi-supervised deep learning for monocular depth map prediction. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6647–6655, 2017.\n\n[41] Steve Cruz, Will Hutchcroft, Yuguang Li, Naji Khosravan, Ivaylo Boyadzhiev, and Sing Bing Kang. Zillow indoor dataset: Annotated floor plans with 360deg panoramas and 3d room layouts. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2133–2143, 2021.\n\n[42] Hao Chen, Yuqi Hou, Chenyuan Qu, Irene Testini, Xiaohan Hong, and Jianbo Jiao. 360+ x: A panoptic multi-modal scene understanding dataset. arXiv preprint arXiv:2404.00989, 2024.\n\n[43] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from a single image using a multi-scale deep network. Advances in neural information processing systems, 27, 2014.\n\n[44] Chuanqing Zhuang, Zhengda Lu, Yiqun Wang, Jun Xiao, and Ying Wang. Acdnet: Adaptively combined dilated convolution for monocular panorama depth estimation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 3653–3661, 2022.\n\n[45] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n\n[46] Wei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus, Long Mai, Simon Chen, and Chunhua Shen. Learning to recover 3d scene shape from a single image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 204–213, 2021.\n\n[47] Chi Zhang, Wei Yin, Billzb Wang, Gang Yu, Bin Fu, and Chunhua Shen. Hierarchical normalization for robust monocular depth estimation. Advances in Neural Information Processing Systems, 35:14128–14139, 2022."
    }
}
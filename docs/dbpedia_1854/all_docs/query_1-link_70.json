{
    "id": "dbpedia_1854_1",
    "rank": 70,
    "data": {
        "url": "https://arxiv.org/html/2406.13378v1",
        "read_more_link": "",
        "language": "en",
        "title": "Any360D: Towards 360 Depth Anything with Unlabeled 360 Data and MÃ¶bius Spatial Augmentation",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://arxiv.org/html/x1.png",
            "https://arxiv.org/html/x2.png",
            "https://arxiv.org/html/x3.png",
            "https://arxiv.org/html/x4.png",
            "https://arxiv.org/html/x5.png",
            "https://arxiv.org/html/x6.png",
            "https://arxiv.org/html/x7.png",
            "https://arxiv.org/html/x8.png",
            "https://arxiv.org/html/x9.png",
            "https://arxiv.org/html/x10.png",
            "https://arxiv.org/html/x11.png",
            "https://arxiv.org/html/x12.png",
            "https://arxiv.org/html/extracted/5677956/Figures/supp_1.jpg",
            "https://arxiv.org/html/extracted/5677956/Figures/supp_2.jpg",
            "https://arxiv.org/html/extracted/5677956/Figures/supp_3.jpg",
            "https://arxiv.org/html/extracted/5677956/Figures/supp_4.jpg",
            "https://arxiv.org/html/extracted/5677956/Figures/supp_5.jpg",
            "https://arxiv.org/html/extracted/5677956/Figures/supp_6.jpg",
            "https://arxiv.org/html/extracted/5677956/Figures/supp_7.jpg",
            "https://arxiv.org/html/extracted/5677956/Figures/supp_8.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Zidong Cao1 Jinjing Zhu1âˆ— Weiming Zhang1âˆ— Lin Wang1,2â€ \n\n1AI Thrust, HKUST(GZ) 2Dept. of CSE, HKUST\n\n{zcao740,jzhu706}@connect.hkust-gz.edu.cn, zweiming996@gmail.com, linwang@ust.hk\n\nhttps://vlislab22.github.io/Any360D/\n\nAbstract\n\nRecently, Depth Anything Model (DAM) [1] â€“ a type of depth foundation model â€“ reveals impressive zero-shot capacity for diverse perspective images. Despite its success, it remains an open question regarding DAMâ€™s performance on 360 images that enjoy large field-of-view (180âˆ˜Ã—360âˆ˜superscript180superscript360180^{\\circ}\\times 360^{\\circ}180 start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT Ã— 360 start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT) but suffer from spherical distortions. To this end, we establish, to our knowledge, the first benchmark that aims to 1) evaluate the performance of DAM on 360 images and 2) develop a powerful 360 DAM for the benefit of the community. For this, we conduct a large suite of experiments that consider the key properties of 360 images, e.g., different 360 representations, various spatial transformations, and diverse indoor and outdoor scenes. This way, our benchmark unveils some key findings, e.g., DAM is less effective for diverse 360 scenes and sensitive to spatial transformations. To address these challenges, we first collect a large-scale unlabeled dataset including diverse indoor and outdoor scenes. We then propose a semi-supervised learning (SSL) framework to learn a 360 DAM, dubbed Any360D. Under the umbrella of SSL, Any360D first learns a teacher model by fine-tuning DAM via metric depth supervision. Then, we train the student model by uncovering the potential of large-scale unlabeled data with pseudo labels from the teacher model. MÃ¶bius transformation-based spatial augmentation (MTSA) is proposed to impose consistency regularization between the unlabeled data and spatially transformed ones. This subtly improves the student modelâ€™s robustness to various spatial transformations even under severe distortions. Extensive experiments demonstrate that Any360D outperforms DAM and many prior data-specific models, e.g., PanoFormer [2] across diverse scenes, showing impressive zero-shot capacity for being a 360 depth foundation model.\n\n1 Introduction\n\n360 cameras have gained significant interest for their ability to capture surrounding environments in a single shot [3, 4]. Monocular 360 depth estimation is a crucial task for 3D scene perception with various applications, such as virtual reality (VR) [5] and autonomous driving [6]. However, estimating reliable 360 depth is challenging due to its ill-posed problem [7, 8] and lack of large-scale labeled dataset â€“ because of the expensive depth annotations and the stitching process that often requires intensive labor costs. As a result, most existing 360 datasets, e.g., [9, 10, 11], are scene-specific, typically limited to indoor scenes such as rooms. This renders existing 360 depth estimation methods, e.g., [12, 13, 14], often produce blurry results and struggle for real outdoor scenes [15].\n\nRecently, vision foundation models [16, 17, 1] have been developed for various vision tasks. For the monocular depth estimation, several foundation models [18, 19, 20, 1] have been proposed. Among them, Depth Anything Model (DAM) [1] is a state-of-the-art (SOTA) depth foundation model that works robustly across diverse perspective images.\n\nDespite the success of DAM on perspective images, it is unclear about its performance on 360 images, which are superior to large field-of-view (FoV) (180âˆ˜Ã—360âˆ˜superscript180superscript360180^{\\circ}\\times 360^{\\circ}180 start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT Ã— 360 start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT) but limited by spherical distortions, as depicted in Fig. 1. This motivates us to explore whether directly applying DAM to 360 images can be effective under various conditions. To this end, we establish, for the first time, a unified and comprehensive benchmark (See Fig. 2) that evaluates DAMâ€™s performance across several key properties of 360 images and models: 1) Different representations of 360 images: The choice of representations is vital for the model to learn effective features. The representations include equirectangular projection (ERP), cube map, tangent patches, etc. Each representation offers distinct advantages and disadvantages concerning the FoVs and distortion levels. 2) Various spatial transformations: They occur when 360 images are not captured vertically, which makes the appearance of 360 images vary greatly [21]. For instance, in the VR environment, users tend to transform 360 images by changing their viewing directions and zooming in on objects of interest [22]. After the transformations, extra distortions are introduced to 360 images, making it nontrivial to estimate 360 depth. 3) Diverse scenes: While 360 depth datasets are mainly collected inside buildings [9], it is crucial to assess DAMâ€™s generalization capacity to wider scenes, e.g., outdoor scenes (See Fig. 1). Additionally, we examine other factors that might impact DAMâ€™s performance, including the backbone model sizes of DAM and optimization space (supervision with disparity or metric depth) [18]. Our benchmark unveils several key findings: 1) The ERP representation exhibits the best zero-shot capacity when no post-processing is considered; 2) The robustness of DAM to spatial transformations is not expected (See Tab. 2); 3) The performance of DAM is less effective in some scenes, especially for the objects at the equator, as illustrated in Fig. 1; 4) After pilot experiments, we find that supervising with disparity is less effective for fine-tuning DAM to 360 images, despite the size of the backbone model.\n\nTo address these challenges, we first collect a large-scale unlabeled dataset encompassing a broad range of indoor and outdoor scenes (See Sec. 4.1). We then propose a semi-supervised learning (SSL) framework to develop a 360 DAM, named Any360D. This framework leverages a combination of large-scale unlabeled data and labeled data. Under the umbrella of SSL, Any360D initially trains a teacher model by fine-tuning DAM with the Low-Rank Adaptation (LoRA) [23] (See Sec. 4.2). The optimization space is chosen as metric depth, as we discover that metric depth supervision is effective in recovering structural details at the equator. To harness the potential of large-scale unlabeled data, we propose an MÃ¶bius transformation-based spatial augmentation (MTSA) to impose consistency regularization between the unlabeled data and spatially transformed ones (See Sec. 4.3). Such augmentation improves the robustness of our student model to various spatial transformations even under significant distortions (See Tab. 4). Extensive experiments confirm the effectiveness of our Any360D under various spatial transformations and diverse scenes.\n\nIn summary, our contributions are three-fold: (I) We establish the first comprehensive benchmark for evaluating the performance of DAM for 360 images across several key properties. (II) Drawing insights from the benchmark, we introduce a semi-supervised learning framework, dubbed Any360D, which leverages large-scale unlabeled 360 images and MÃ¶bius transformation-based spatial augmentation (MTSA) to improve the generalization capacity and robustness of our model, respectively. (III) Experimental results show the impressive zero-shot capacity of Any360D for being a 360 depth foundation model across various spatial transformations and diverse scenes.\n\n2 Related Work\n\nMonocular 360 Depth Estimation. With the advance of deep learning and 360 depth datasets [9, 10, 24], monocular 360 depth estimation methods have obtained good performance in specific datasets [9, 10, 24, 11]. Previous methods mainly focus on mitigating the negative effects of distortion. For example, they have carefully designed distortion-aware convolution kernels [25, 2], considered spherical prior [26], or transformed the ERP image into distortion-less representations, e.g., cube map [27] and tangent patches [13, 12], and narrow FoV slices [28, 29, 30]. However, as most 360 depth datasets are captured in indoor scenes with limited amounts, these methods are difficult to generalize to unseen scenes, especially outdoor scenes [15].\n\nZero-shot Monocular 2D Depth Estimation. To enable the depth estimation model to have zero-shot ability, MiDaS [18, 19] proposes to train on multiple perspective depth datasets. To mitigate the gap between different datasets, it introduces an affine-invariant loss to avoid the influence of depth scale and instead focuses on the consistency of depth distribution between prediction and ground truth. Following this direction, ZoeDepth [20] combines disparity and metric depth estimation together. ZoeDepth first trains a disparity depth estimation model on several datasets, and then fine-tunes it on specific datasets to generalize to metric depth estimation. Recently, Depth Anything [1] additionally leverages large-scale unlabeled perspective images to enhance the modelâ€™s representation capability with semi-supervised learning. This approach demonstrates excellent generalization ability across diverse scenes, ranging from indoor to outdoor scenes.\n\nSemi-supervised Learning (SSL). It [31, 32] aims to leverage a large number of unlabeled data to improve learning performance with a limited number of labeled samples. Consequently, SSL has been applied to various tasks over the past decade, including image classification [33, 34], object detection [35, 36], semantic segmentation [37, 38], and depth estimation [39, 40]. Inspired by the success of SSL in these tasks, this work aims to leverage a large-scale unlabeled 360 images dataset for developing a 360 DAM. We employ the MÃ¶bius transformation as spatial augmentation and utilize consistency regularization between the unlabeled and spatially transformed ones to enhance the training of the student model. The results demonstrate that unlabeled 360 images can significantly enhance the student modelâ€™s generalization capability on diverse scenes and robustness on various transformations.\n\n3 Benchmarking DAM\n\nIn this section, we aim to evaluate the performance of DAM on 360 images that consider key properties of 360 images, i.e., 360 image representations, spatial transformations, diverse scenes including both indoor and outdoor ones, optimization space, and backbone model sizes, as shown in Fig. 2. We first introduce the evaluation dataset and evaluation metrics in Sec 3.1. Then, we conduct a large suite of experiments to examine the mentioned properties in Sec. 3.2. Finally, we reveal the crucial findings about the generalization capacity and robustness of DAM.\n\n3.1 Evaluation Protocol\n\nDatasets. We utilize the testing set of the Matterport3D dataset [9] for quantitative comparison, whose scenes are inside buildings. As the DAM outputs disparity depth, we reverse the values of metric depth labels in the Matterport3D dataset. For qualitative evaluation, in addition to the Matterport3D dataset, we also incorporate samples from our collected dataset with diverse indoor and outdoor scenarios. (Details of our dataset can be found in Sec. 4.1).\n\nMetrics. We evaluate the relative depth estimation with standard metrics including Absolute Relative Error (Abs Rel) and Root Mean Squared Error (RMSE). Only the observed pixels in the ground truth depth are considered in these calculations.\n\nPool of 360 Image Representations. The selection of 360 data representations is crucial for the model to learn robust and effective representation. We collect the most commonly used representations: ERP, cube map, tangent patches, and horizontal and vertical slices. These representations vary in FoVs and distortion levels.\n\nPool of Spatial Transformations. We collect two types of spatial transformations that make significant changes for 360 images, i.e., vertical rotation and zoom. Specifically, we set three vertical rotation angles: (2âˆ˜superscript22^{\\circ}2 start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT, 5âˆ˜superscript55^{\\circ}5 start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT, 10âˆ˜superscript1010^{\\circ}10 start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT). We also set three zoom levels: (0.8, 1.2, 1.5). The vertical rotation and zoom are achieved with MÃ¶bius transformation, which is the only conformal bijective transformation on the sphere [21].\n\nPool of Backbone Model Sizes. The backbone models are selected from ViT-S, ViT-B, and ViT-L, following [1].\n\n3.2 Evaluation Results\n\nImpact of 360 Image Representations. In Tab. 1, we comprehensively measure the performance of DAM with various 360 image representations as inputs. It can be seen that with different backbone models, ERP representation always obtains the best performance. It demonstrates that the strong zero-shot capacity of DAM can handle the distortions in ERP images in the indoor scenes from the Matterport3D dataset. In addition, we find that distortion-less 360 image representations, e.g., tangent patches can recover more local details, such as the bed in Fig. 3. However, the discrepancies among different patches degrade the overall performance, which requires post-processing. Finally, utilizing slices with narrow FoVs also results in discrepancies that influence the performance.\n\nImpact of Spatial Transformations. Tab. 2 shows DAMâ€™s performance under different vertical rotation angles and zoom levels. Specifically, with the rotation angle increasing, the performance drops slightly. However, there is a drastic performance degradation as the zoom level increases.\n\nPerformance on Indoor and Outdoor Scenes. In addition to indoor scenes from the Matterport3D dataset, we further evaluate the generalization capacity of DAM in diverse indoor and outdoor scenes. These scenes are challenging. For example in Fig. 1(a), the ceiling and floor cover large portions of 360 images, causing objects located at the equator to have small appearance sizes. Unfortunately, DAM shows poor results in these indoor and outdoor scenes. Specifically, several objects at the equator, e.g., corridors and cars, are missing in the results.\n\nImpact of Optimization Space. Previous depth foundation models [1, 20] employ the affine-invariant loss to enable multi-dataset joint training. In this case, the optimization space is based on disparities, which have the largest value for the closest object. In our pilot studies, we find that although fine-tuning DAM with disparity supervision would obtain finer results in the specific dataset [9], its generalization capacity for unseen scenes is still limited. Moreover, the unsatisfactory regions are mostly at the equator. We conjecture that the disparity supervision would focus more on close objects. However, distant regions are often located at the equator in 360 images. To make the optimization focus on the equator region, we attempt to optimize the model with metric supervision and impressively find that the performance of DAM improves significantly for the equator region, as depicted in Fig. 4.\n\nImpact of Different Backbone Models. As shown in Tabs. 1 and 2, with the model size increasing, the zero-shot capability of DAM increases slightly. However, in Fig. 1, DAM with ViT-L as the backbone are still less effective at the equator, causing structural details missing or blurry.\n\nOur findings can be summarized as follows: 1) The ERP representation has the best zero-shot performance when no post-processing is added. 2) The robustness of DAM for spatial transformation needs to be improved, especially for the zoom operation. 3) The optimization space needs to be transformed from disparity supervision to metric depth supervision to make the structural details of 360 images clear, especially for the equator region. 4) DAM performs well in some scenes, e.g., rooms, but has poor results for scenes when the appearance sizes of objects at the equator become small, even with the ViT-L backbone.\n\n4 Any360D Model with New Designs\n\nBased on the findings from our benchmark, we initially collect a large-scale unlabeled dataset, aimed at enriching the diversity and generalization capability of our model (Refer to Sec.4.1). Following this, we exploit a supervised training framework to fine-tune DAM using LoRA [23] with a labeled 360 indoor dataset, resulting in our teacher model ğ’¯ğ’¯\\mathcal{T}caligraphic_T (See Sec. 4.2). Subsequently, the teacher model ğ’¯ğ’¯\\mathcal{T}caligraphic_T is utilized to generate pseudo depth labels for the existing unlabeled datasets as well as for our collected dataset. Formally, the labeled and unlabeled sets are denoted as ğ’Ÿl=superscriptğ’Ÿğ‘™absent\\mathcal{D}^{l}=caligraphic_D start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT = {(xi,di)}i=1Msuperscriptsubscriptsubscriptğ‘¥ğ‘–subscriptğ‘‘ğ‘–ğ‘–1ğ‘€\\left\\{\\left(x_{i},d_{i}\\right)\\right\\}_{i=1}^{M}{ ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT and ğ’Ÿu={ui}i=1Nsuperscriptğ’Ÿğ‘¢superscriptsubscriptsubscriptğ‘¢ğ‘–ğ‘–1ğ‘\\mathcal{D}^{u}=\\left\\{u_{i}\\right\\}_{i=1}^{N}caligraphic_D start_POSTSUPERSCRIPT italic_u end_POSTSUPERSCRIPT = { italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT, respectively. Mğ‘€Mitalic_M and Nğ‘Nitalic_N are the number of samples in the labeled and unlabeled datasets, respectively. Lastly, the student model ğ’®ğ’®\\mathcal{S}caligraphic_S is trained on a combination of the labeled data and the pseudo-labeled set, employing color and MÃ¶bius transformation-based spatial augmentation (MTSA) specific to 360 images (See Sec. 4.3). The overview of Any360D is shown in Fig. 6.\n\n4.1 Dataset Collection\n\nWe collect and integrate existing 360 datasets, both labeled and unlabeled. Specifically, for indoor scenes, we utilize the Matterport3D dataset [9] with depth ground truth, and the training set of the Zillow Indoor Dataset (ZInD)[41] without depth ground truth. Additionally, we collect an unlabeled dataset encompassing both indoor and outdoor scenes. This dataset enhances our benchmark by enriching the validation for DAM under diverse scenes and is utilized to train our model. The collected dataset features a comprehensive array of scenes at the campus level (See Fig. 5). To ensure privacy, we anonymize the footage by blurring identifiable faces, employing methods similar to those in [42].\n\n4.2 Stage 1: Fine-tuning DAM with Labeled Data\n\nBased on the quantitative and qualitative results regarding 360 image representations and spatial transformations, etc. In Sec. 3.2, we observe that DAM performs suboptimally on 360 images due to inherent distortions compared to perspective images. This necessitates fine-tuning DAM for accurate depth prediction in 360 images. However, the large model size and limited labeled 360 images make direct training impractical. Consequently, we employ the LoRA to fine-tune the DAM encoder. Additionally, results indicate a significant performance drop at the equator regions, particularly in outdoor scenes, due to disparity depth supervision (See Fig. 4). Existing depth models provide disparity depth estimation by factoring out scale, resulting in outputs that lack metric meaning, thereby limiting applicability in distant regions and objects with small sizes. To address this, we employ two types of metric depth heads to fine-tune DAM with metric depth supervision instead of disparity depth supervision. The first type of head employs a bin structure [20], while the second type of head consists of two convolutional layers. Detailed methodology is provided in the following sections.\n\nFor each weight in the encoder, we utilize a low-rank approximation Ï‰=Aâ¢Bğœ”ğ´ğµ\\omega=ABitalic_Ï‰ = italic_A italic_B, where only Ağ´Aitalic_A and BğµBitalic_B are updated via backpropagation during adaptation. By inputting labeled data xisubscriptğ‘¥ğ‘–x_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT into DAM with this low-rank approximation, we obtain the disparity depth. Subsequently, this disparity depth is fed into a metric depth head to generate the metric depth prediction di^^subscriptğ‘‘ğ‘–\\hat{d_{i}}over^ start_ARG italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG. To train the teacher model ğ’¯ğ’¯\\mathcal{T}caligraphic_T, we employ the Scale-Invariant Logarithmic (SILog) [43] loss to quantify the discrepancy between the predictions di^^subscriptğ‘‘ğ‘–\\hat{d_{i}}over^ start_ARG italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG and the ground truth disubscriptğ‘‘ğ‘–d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. This SILog loss is formulated as â„’sT=Sâ¢Iâ¢Lâ¢oâ¢gâ¢(d^i,di)superscriptsubscriptâ„’ğ‘ ğ‘‡ğ‘†ğ¼ğ¿ğ‘œğ‘”subscript^ğ‘‘ğ‘–subscriptğ‘‘ğ‘–\\mathcal{L}_{s}^{T}=SILog\\left(\\hat{d}_{i},d_{i}\\right)caligraphic_L start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT = italic_S italic_I italic_L italic_o italic_g ( over^ start_ARG italic_d end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ).\n\n4.3 Stage 2: Semi-supervised Learning with Unlabeled Data\n\nDespite the limited availability of labeled data, leveraging a comprehensive collection of readily available and diverse unlabeled images significantly improves the modelâ€™s generalization capability and robustness. To harness the potential of large-scale unlabeled data, we introduce strong color augmentation (CA) and MÃ¶bius transformation-based spatial augmentation (MTSA) to impose consistency regularization between unlabeled data and spatially transformed ones, as shown in Fig. 6.\n\nColor Augmentation (CA). Given an image uisubscriptğ‘¢ğ‘–u_{i}italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, we exploit strong color augmentations ğ’ğ’\\mathcal{C}caligraphic_C, such as color jittering, resulting in augmented data ğ’â¢(ui)ğ’subscriptğ‘¢ğ‘–\\mathcal{C}(u_{i})caligraphic_C ( italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ).\n\nMÃ¶bius Transformation-based Spatial Augmentation (MTSA). We briefly introduce the process of MTSA â„³(.)\\mathcal{M}(.)caligraphic_M ( . ). As illustrated in Fig. 7, a 360 image uisubscriptğ‘¢ğ‘–u_{i}italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT undergoes an initial projection from the plane to the sphere via spherical projection (SP). Subsequently, this spherical representation is projected onto the complex plane using stereographic projection (STP). In our conduction, the specific point on the complex plane is determined by the intersection of the equator point and a designated spherical point. The MÃ¶bius transformation is applied on the complex plane. Following this, we apply the inverse stereographic projection (STP-1) and inverse spherical projection (SP-1) to obtain the transformed 360 image â„³â¢(ui)â„³subscriptğ‘¢ğ‘–\\mathcal{M}(u_{i})caligraphic_M ( italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ).\n\nSemi-supervision. The following outlines the process of training the student model ğ’®ğ’®\\mathcal{S}caligraphic_S with semi-supervision on labeled and unlabeled datasets. As depicted in Fig. 6, initially, the teacher model ğ’¯ğ’¯\\mathcal{T}caligraphic_T generates the pseudo depth d^iusuperscriptsubscript^ğ‘‘ğ‘–ğ‘¢\\hat{d}_{i}^{u}over^ start_ARG italic_d end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_u end_POSTSUPERSCRIPT for the unlabeled data uisubscriptğ‘¢ğ‘–u_{i}italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, while the student model ğ’®ğ’®\\mathcal{S}caligraphic_S provides the depth prediction dÂ¯iusuperscriptsubscriptÂ¯ğ‘‘ğ‘–ğ‘¢\\bar{d}_{i}^{u}overÂ¯ start_ARG italic_d end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_u end_POSTSUPERSCRIPT. Subsequently, the augmented unlabeled data ğ’â¢(ui)ğ’subscriptğ‘¢ğ‘–\\mathcal{C}(u_{i})caligraphic_C ( italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) and â„³â¢(ui)â„³subscriptğ‘¢ğ‘–\\mathcal{M}(u_{i})caligraphic_M ( italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) are fed into the student model ğ’®ğ’®\\mathcal{S}caligraphic_S to produce predictions d~iusuperscriptsubscript~ğ‘‘ğ‘–ğ‘¢\\widetilde{d}_{i}^{u}over~ start_ARG italic_d end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_u end_POSTSUPERSCRIPT and dÂ¨iusuperscriptsubscriptÂ¨ğ‘‘ğ‘–ğ‘¢\\ddot{d}_{i}^{u}overÂ¨ start_ARG italic_d end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_u end_POSTSUPERSCRIPT, respectively. Consistency regularization is enforced by ensuring structural information consistency between the unlabeled data and spatially transformed ones, thereby enhancing the student modelâ€™s robustness to distortions. The consistency regularization is defined as â„’c=â„’câ¢1+â„’câ¢2subscriptâ„’ğ‘subscriptâ„’ğ‘1subscriptâ„’ğ‘2\\mathcal{L}_{c}=\\mathcal{L}_{c1}+\\mathcal{L}_{c2}caligraphic_L start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT = caligraphic_L start_POSTSUBSCRIPT italic_c 1 end_POSTSUBSCRIPT + caligraphic_L start_POSTSUBSCRIPT italic_c 2 end_POSTSUBSCRIPT, where â„’câ¢1=Sâ¢Iâ¢Lâ¢oâ¢gâ¢(d~iu,dÂ¯iu)subscriptâ„’ğ‘1ğ‘†ğ¼ğ¿ğ‘œğ‘”superscriptsubscript~ğ‘‘ğ‘–ğ‘¢superscriptsubscriptÂ¯ğ‘‘ğ‘–ğ‘¢\\mathcal{L}_{c1}=SILog(\\widetilde{d}_{i}^{u},\\bar{d}_{i}^{u})caligraphic_L start_POSTSUBSCRIPT italic_c 1 end_POSTSUBSCRIPT = italic_S italic_I italic_L italic_o italic_g ( over~ start_ARG italic_d end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_u end_POSTSUPERSCRIPT , overÂ¯ start_ARG italic_d end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_u end_POSTSUPERSCRIPT ) and â„’câ¢2=Sâ¢Iâ¢Lâ¢oâ¢gâ¢(dÂ¨iu,â„³â¢(dÂ¯iu))subscriptâ„’ğ‘2ğ‘†ğ¼ğ¿ğ‘œğ‘”superscriptsubscriptÂ¨ğ‘‘ğ‘–ğ‘¢â„³superscriptsubscriptÂ¯ğ‘‘ğ‘–ğ‘¢\\mathcal{L}_{c2}=SILog(\\ddot{d}_{i}^{u},\\mathcal{M}(\\bar{d}_{i}^{u}))caligraphic_L start_POSTSUBSCRIPT italic_c 2 end_POSTSUBSCRIPT = italic_S italic_I italic_L italic_o italic_g ( overÂ¨ start_ARG italic_d end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_u end_POSTSUPERSCRIPT , caligraphic_M ( overÂ¯ start_ARG italic_d end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_u end_POSTSUPERSCRIPT ) ). Note that dÂ¨iusuperscriptsubscriptÂ¨ğ‘‘ğ‘–ğ‘¢\\ddot{d}_{i}^{u}overÂ¨ start_ARG italic_d end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_u end_POSTSUPERSCRIPT is obtained from the data with MÃ¶bius transformation â„³â„³\\mathcal{M}caligraphic_M. To keep the consistency with unlabeled data, the prediction dÂ¯iusuperscriptsubscriptÂ¯ğ‘‘ğ‘–ğ‘¢\\bar{d}_{i}^{u}overÂ¯ start_ARG italic_d end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_u end_POSTSUPERSCRIPT of unlabeled data is also transformed with MÃ¶bius transformation â„³â„³\\mathcal{M}caligraphic_M. Moreover, to fully harness the potential of the unlabeled dataset, we employ pseudo labels dusuperscriptğ‘‘ğ‘¢d^{u}italic_d start_POSTSUPERSCRIPT italic_u end_POSTSUPERSCRIPT of unlabeled data for self-training, the objective of self-training is formulated as â„’p=Sâ¢Iâ¢Lâ¢oâ¢gâ¢(diÂ¯u,d^iu).subscriptâ„’ğ‘ğ‘†ğ¼ğ¿ğ‘œğ‘”superscriptÂ¯subscriptğ‘‘ğ‘–ğ‘¢superscriptsubscript^ğ‘‘ğ‘–ğ‘¢\\mathcal{L}_{p}=SILog\\left(\\bar{d_{i}}^{u},\\hat{d}_{i}^{u}\\right).caligraphic_L start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT = italic_S italic_I italic_L italic_o italic_g ( overÂ¯ start_ARG italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG start_POSTSUPERSCRIPT italic_u end_POSTSUPERSCRIPT , over^ start_ARG italic_d end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_u end_POSTSUPERSCRIPT ) .\n\n4.4 Total Objective\n\nFinally, based on the designed loss terms: consistency regularization loss and self-training loss, the objective for training the student model ğ’®ğ’®\\mathcal{S}caligraphic_S is formulated as â„’=â„’ss+Î»pâˆ—â„’p+Î»câˆ—â„’c,â„’superscriptsubscriptâ„’ğ‘ ğ‘ subscriptğœ†ğ‘subscriptâ„’ğ‘subscriptğœ†ğ‘subscriptâ„’ğ‘\\mathcal{L}=\\mathcal{L}_{s}^{s}+\\lambda_{p}*\\mathcal{L}_{p}+\\lambda_{c}*% \\mathcal{L}_{c},caligraphic_L = caligraphic_L start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT + italic_Î» start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT âˆ— caligraphic_L start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT + italic_Î» start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT âˆ— caligraphic_L start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT , where Î»csubscriptğœ†ğ‘\\lambda_{c}italic_Î» start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT and Î»psubscriptğœ†ğ‘\\lambda_{p}italic_Î» start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT are the trade-off parameters to balance the three terms, both set to 1. And â„’sSsuperscriptsubscriptâ„’ğ‘ ğ‘†\\mathcal{L}_{s}^{S}caligraphic_L start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT is introduced to supervise the training of student model ğ’®ğ’®\\mathcal{S}caligraphic_S with ground truth disubscriptğ‘‘ğ‘–d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and prediction dË˜isubscriptË˜ğ‘‘ğ‘–\\breve{d}_{i}overË˜ start_ARG italic_d end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT of labeled data. Note that we evaluate our student model during inference to demonstrate the effectiveness of Any360D.\n\n5 Experiment\n\nThe datasets utilized in this work include the Matterport3D dataset [9], our self-collected dataset, and unlabeled ZInD [41] dataset with indoor scenes. The spatial resolution of 360 images and corresponding depth maps is 504Ã—10085041008504\\times 1008504 Ã— 1008. All experiments are conducted on a single A800 GPU with a batch size of 4 and a learning rate of 1e-4, using the Adam optimizer [45]. By default, the teacher model employs the ViT-Las the backbone, while the student model employs the ViT-Small as the backbone for efficient learning. The teacher model is trained for 20 epochs, while the student model is trained for 40 epochs when the quantities of labeled and unlabeled data are equal, and for 20 epochs otherwise. Color augmentation is consistent with the previous work [14]. For the MÃ¶bius transformation, the vertical rotation angle is randomly sampled from [âˆ’10âˆ˜superscript10-10^{\\circ}- 10 start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT, 10âˆ˜superscript1010^{\\circ}10 start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT], and the zoom level is randomly sampled from [1, 1.2].\n\n5.1 Qualitative and Quantitative Evaluation\n\nTab. 3 presents a quantitative comparison of SOTA monocular 360 depth estimation methods on the Matterport3D dataset. To ensure fairness, we re-train these methods following their official protocols. Our student model, incorporating a CNN metric depth head, surpasses previous methods across all metrics. Notably, it demonstrates robustness under various spatial transformations. For instance, at a zoom level of 1.5, our student model achieves a 44.2% improvement in the Abs Rel metric compared to EGFormer [26]. This significant enhancement is attributed not only to the strong teacher model with fine-tuning but also to our semi-supervised learning approach utilizing MTSA. Moreover, the student model with ViT-S as the backbone sometimes even outperforms the teacher model with ViT-L as the backbone at certain zoom levels. As shown in Fig.8, our method accurately predicts structural details such as sofas and boards, which are absent in the results of other methods. Unlike data-specific approaches, our method exhibits impressive zero-shot capacity, as demonstrated in Fig. 9 across diverse scenes.\n\n5.2 Ablation Studies\n\nFine-tuning and Metric Depth Supervision. Tab. 5 shows the impact of fine-tuning on different backbone models and metric heads. The first and second rows reveal that fine-tuning is crucial since DAM is trained on perspective images. Comparing the second and third rows, we observe that using CNN layers as the metric depth head yields superior performance compared to the ZoeDepth head [20].\n\nThis can be attributed to the complexity of the ZoeDepth metric head, which introduces artifacts in texture-less regions when the backbone model is small, disrupting the continuity of depth prediction and leading to sub-optimal results. With a large backbone model, e.g., ViT-L, utilizing the metric depth head of ZoeDepth performs better.\n\nLoss Functions. In Tab. 4, we discuss the impacts of loss functions in semi-supervision process. We take DAM with the metric depth head of ZoeDepth as an example. By default, the loss function â„’sSsuperscriptsubscriptâ„’ğ‘ ğ‘†\\mathcal{L}_{s}^{S}caligraphic_L start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT is applied. Initially, incorporating unlabeled data with pseudo labels yields a performance improvement. Subsequently, adding color augmentation facilitate the student model to learn robust representations, resulting in enhanced model performance. However, there is no significant improvement in robustness for vertical rotation and zoom. Finally, by introducing MÃ¶bius augmentation into the semi-supervision process, the modelâ€™s robustness to various spatial transformations is significantly enhanced, demonstrating the necessity of incorporating MÃ¶bius transformation as a data augmentation technique.\n\nUtilization of Unlabeled Data. In Tab. 6, we examine the impact of varying amounts of unlabeled data on overall performance, utilizing the metric depth head with CNN layers by default. We begin by adding unlabeled data from the ZInD dataset in the same quantity as the Matterport3D dataset. Given the similar indoor scenes in both datasets, the unlabeled data helps the model learn robust and effective representations. Furthermore, scaling up the unlabeled data to 54,034 samples (the entire ZInD training set) results\n\nin no significant performance improvement. This suggests that increasing the amount of training data with similar scenes does not significantly enhance model performance. However, by incorporating our collected data with diverse indoor and outdoor scenes, the modelâ€™s performance improves significantly. This demonstrates that large-scale, unlabeled 360 data with diverse scenes can substantially empower monocular 360 depth estimation.\n\n6 Conclusion and Limitations\n\nConclusion. In this paper, we provided a comprehensive benchmark for evaluating the performance of DAM on 360 images by exploring several key properties, e.g., representations and spatial transformations. The benchmark reveals some key findings, including the limited effectiveness of disparity supervision in refining structural details in 360 images. Leveraging these crucial findings, we fine-tune DAM and uncover the potential of large-scale unlabeled data with MÃ¶bius augmentation under the umbrella of semi-supervised learning. The experiments underscore our modelâ€™s impressive zero-shot capacity, establishing it as a foundational model for 360 depth estimation task.\n\nBroader Impacts. Our established benchmark and proposed semi-supervised learning framework offer valuable insights for researchers aiming to fine-tune foundation models for 360 images. Any360D can provide effective structural priors to support various tasks, such as 360 semantic segmentation.\n\nLimitations and Future Work. Due to the scarcity of 360 depth labels in diverse scenes, particularly outdoor scenes, our teacher model is trained on a limited scenes, potentially generating noisy pseudo labels for unseen scenes. To enhance the zero-shot capability of our model, future work will focus on collecting 360 images paired with depth labels across a broader range of environments.\n\n7 Appendix\n\nAppendix A Dataset\n\nDuring the data collection phase, we used the RICOH THETA Z1 360 camera to capture videos in various scenes at the campus level. The RICOH THETA Z1 360 camera features dual 1-inch back-illuminated CMOS sensors, capable of capturing 4K (3840 Ã—\\timesÃ— 1920) video at 30 frames per second. During the video collection phase, we used a selfie stick to minimize the presence of the camera operator in the footage, thereby ensuring the quality of the collected dataset. We collected dozens of indoor and outdoor videos. Specifically, we collected 20 video clips of indoor scenes and 10 video clips of outdoor scenes. To ensure high-quality footage, we meticulously planned the shooting routes in advance to improve our shooting efficiency. Each video was kept under 5 minutes in duration. During the export phase, we utilized the cameraâ€™s built-in correction options to ensure the footage was in equirectangular format. To protect the privacy of the camera operators and passersby, we anonymized the videos by identifying and applying blurring filters to faces in each frame, similar to [42]. Subsequently, we selected anonymized 360 images from each frame based on the following criteria:\n\nâ€¢\n\nThe frame of the scene needs to be concise, diverse, and representative.\n\nâ€¢\n\nThere are no adverse effects from the environment, lighting, or humidity.\n\nâ€¢\n\nPrivacy protection measures should be successfully implemented.\n\nAfter completing the data selection, our Diverse360 dataset contains a total of 12,063 images, comprising 7,887 indoor 360 images and 4,176 outdoor 360 images. Our Diverse360 dataset showcases a wide variety of scenes at the campus level (See Fig. 10. The Diverse360 dataset enriches the validation for DAM in diverse scenes. In addition, this dataset is utilized to train our model, which is demonstrated to improve the overall performance of our model.\n\nAppendix B Metrics\n\nMetrics. We evaluate the performance of models with two standard metrics: Absolute Relative Error (Abs Rel), and Root Mean Squared Error (RMSE). We only evaluate the performance on the valid regions with the ground truth depth Dâˆ—superscriptğ·D^{*}italic_D start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT. We denote the number of valid pixels as Kğ¾Kitalic_K. With the prediction Dğ·Ditalic_D, the two metrics can be formulated as follows:\n\nâ€¢\n\nAbsolute Relative Error (Abs Rel):\n\n1Kâ¢âˆ‘i=1Kâ€–Dâ¢(i)âˆ’Dâˆ—â¢(i)â€–Dâˆ—â¢(i).1ğ¾superscriptsubscriptğ‘–1ğ¾normğ·ğ‘–superscriptğ·ğ‘–superscriptğ·ğ‘–\\frac{1}{K}\\sum_{i=1}^{K}\\frac{||D(i)-D^{*}(i)||}{D^{*}(i)}.divide start_ARG 1 end_ARG start_ARG italic_K end_ARG âˆ‘ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT divide start_ARG | | italic_D ( italic_i ) - italic_D start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( italic_i ) | | end_ARG start_ARG italic_D start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( italic_i ) end_ARG . (1)\n\nâ€¢\n\nRoot Mean Square Error (RMSE):\n\n1Kâ¢âˆ‘i=1Kâ€–Dâ¢(i)âˆ’Dâˆ—â¢(i)â€–2.1ğ¾superscriptsubscriptğ‘–1ğ¾superscriptnormğ·ğ‘–superscriptğ·ğ‘–2\\sqrt{\\frac{1}{K}\\sum_{i=1}^{K}||D(i)-D^{*}(i)||^{2}}.square-root start_ARG divide start_ARG 1 end_ARG start_ARG italic_K end_ARG âˆ‘ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT | | italic_D ( italic_i ) - italic_D start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( italic_i ) | | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG . (2)\n\nMore details of benchmarking DAM. Depth Anything model (DAM) is not trained on 360 images. To evaluate the zero-shot performance of DAM on 360 images, in Tabs. 1, and 2 on the main paper, we manually align the prediction of DAM with the depth ground truth with scale and shift, similar to [18, 19]. In addition, since our Any360D is first fine-tuned on a 360 depth dataset [9], we do not utilize any alignment operation in the ablation studies (Tabs. 3, 4, and 6 on the main paper) and comparison with SOTA monocular 360 depth estimation methods (Tab. 5 on the main paper).\n\nAppendix C The Proposed Method\n\nC.1 MÃ¶bius Transformation-based Spatial Augmentation (MTSA)\n\nWe provide more details of conducting the MTSA. Firstly, we provide the formulas of the geometric projections. Then, we describe the parameters of the MTSA in rotation and zoom. The formulas are based on [21]. Differently, we take the equator center as the pole to zoom in on the objects at the equator.\n\nThe MÃ¶bius transformation is conducted in the complex plane. To achieve it, a 360 image with ERP format is first projected from the plane to the sphere via spherical projection (SP). The plane coordinate is proportional to the angle coordinate (Î¸,Ï•ğœƒitalic-Ï•\\theta,\\phiitalic_Î¸ , italic_Ï•) (where Î¸ğœƒ\\thetaitalic_Î¸ represents the longitude and Ï•italic-Ï•\\phiitalic_Ï• represents the latitude), while the spherical coordinate can be defined as (x,y,zğ‘¥ğ‘¦ğ‘§x,y,zitalic_x , italic_y , italic_z). In this case, SP can be defined as follows:\n\nSP:(xyz)=(cosâ¡(Ï•)â¢cosâ¡(Î¸)cosâ¡(Ï•)â¢sinâ¡(Î¸)sinâ¡(Ï•)).:SPmatrixğ‘¥ğ‘¦ğ‘§matrixitalic-Ï•ğœƒitalic-Ï•ğœƒitalic-Ï•\\text{SP}:\\begin{pmatrix}x\\\\ y\\\\ z\\\\ \\end{pmatrix}=\\begin{pmatrix}\\cos(\\phi)\\cos(\\theta)\\\\ \\cos(\\phi)\\sin(\\theta)\\\\ \\sin(\\phi)\\\\ \\end{pmatrix}.SP : ( start_ARG start_ROW start_CELL italic_x end_CELL end_ROW start_ROW start_CELL italic_y end_CELL end_ROW start_ROW start_CELL italic_z end_CELL end_ROW end_ARG ) = ( start_ARG start_ROW start_CELL roman_cos ( italic_Ï• ) roman_cos ( italic_Î¸ ) end_CELL end_ROW start_ROW start_CELL roman_cos ( italic_Ï• ) roman_sin ( italic_Î¸ ) end_CELL end_ROW start_ROW start_CELL roman_sin ( italic_Ï• ) end_CELL end_ROW end_ARG ) . (3)\n\nThen, we project from the sphere to the complex plane with stereographic projection (STP). By defining the coordinate of the complex plane as Z=(xâ€²,yâ€²)ğ‘superscriptğ‘¥â€²superscriptğ‘¦â€²Z=(x^{\\prime},y^{\\prime})italic_Z = ( italic_x start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT , italic_y start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ) and selecting the equator center as the pole, the STP can be formulated as follows:\n\nSTP:xâ€²=y1âˆ’x,yâ€²=z1âˆ’x.:STPformulae-sequencesuperscriptğ‘¥â€²ğ‘¦1ğ‘¥superscriptğ‘¦â€²ğ‘§1ğ‘¥\\text{STP}:x^{\\prime}={\\frac{y}{1-x}}\\ ,\\ y^{\\prime}={\\frac{z}{1-x}}.STP : italic_x start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT = divide start_ARG italic_y end_ARG start_ARG 1 - italic_x end_ARG , italic_y start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT = divide start_ARG italic_z end_ARG start_ARG 1 - italic_x end_ARG . (4)\n\nIn the complex plane, the MÃ¶bius transformation is conducted with the following formulation:\n\nfâ¢(Z)=aâ¢Z+bcâ¢Z+d,ğ‘“ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘‘f(Z)={\\frac{aZ+b}{cZ+d}},italic_f ( italic_Z ) = divide start_ARG italic_a italic_Z + italic_b end_ARG start_ARG italic_c italic_Z + italic_d end_ARG , (5)\n\nwhere ağ‘aitalic_a, bğ‘bitalic_b, cğ‘citalic_c, and dğ‘‘ditalic_d are complex numbers satisfying aâ¢dâˆ’bâ¢câ‰ 0ğ‘ğ‘‘ğ‘ğ‘0ad-bc\\neq 0italic_a italic_d - italic_b italic_c â‰  0. For the vertical rotation with angle Î²ğ›½\\betaitalic_Î² and zoom level sğ‘ sitalic_s, the parameters of MÃ¶bius transformations can be represented as follows:\n\n(abcd)=(cosâ¡(Î²)+jâ¢sinâ¡(Î²)001).matrixğ‘ğ‘ğ‘ğ‘‘matrixğ›½ğ‘—ğ›½001\\begin{pmatrix}a&b\\\\ c&d\\\\ \\end{pmatrix}=\\begin{pmatrix}\\cos(\\beta)+j\\sin(\\beta)&0\\\\ 0&1\\\\ \\end{pmatrix}.( start_ARG start_ROW start_CELL italic_a end_CELL start_CELL italic_b end_CELL end_ROW start_ROW start_CELL italic_c end_CELL start_CELL italic_d end_CELL end_ROW end_ARG ) = ( start_ARG start_ROW start_CELL roman_cos ( italic_Î² ) + italic_j roman_sin ( italic_Î² ) end_CELL start_CELL 0 end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL 1 end_CELL end_ROW end_ARG ) . (6)\n\nFor the zoom operation with level sğ‘ sitalic_s, the parameters of MÃ¶bius transformations can be represented as follows:\n\n(abcd)=(s001).matrixğ‘ğ‘ğ‘ğ‘‘matrixğ‘ 001\\begin{pmatrix}a&b\\\\ c&d\\\\ \\end{pmatrix}=\\begin{pmatrix}s&0\\\\ 0&1\\\\ \\end{pmatrix}.( start_ARG start_ROW start_CELL italic_a end_CELL start_CELL italic_b end_CELL end_ROW start_ROW start_CELL italic_c end_CELL start_CELL italic_d end_CELL end_ROW end_ARG ) = ( start_ARG start_ROW start_CELL italic_s end_CELL start_CELL 0 end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL 1 end_CELL end_ROW end_ARG ) . (7)\n\nWe show some examples of transformed 360 images in Fig. 12. The MÃ¶bius transformation obeys the matrix chain multiplication rule. After the MÃ¶bius transformation in the complex plane, we conduct inverse projections to project from the complex plane to the sphere and the plane, respectively. The inverse projections can be formulated as follows:\n\nSTPâˆ’1:(xyz)=(âˆ’1+xâ€²â£2+yâ€²â£21+xâ€²â£2+yâ€²â£22â¢xâ€²1+xâ€²â£2+yâ€²â£22â¢yâ€²1+xâ€²â£2+yâ€²â£2);SPâˆ’1:(Î¸Ï•)=(arctanâ¡(y/x)arcsinâ¡(z)).:superscriptSTP1matrixğ‘¥ğ‘¦ğ‘§matrix1superscriptğ‘¥â€²2superscriptğ‘¦â€²21superscriptğ‘¥â€²2superscriptğ‘¦â€²22superscriptğ‘¥â€²1superscriptğ‘¥â€²2superscriptğ‘¦â€²22superscriptğ‘¦â€²1superscriptğ‘¥â€²2superscriptğ‘¦â€²2superscriptSP1:matrixğœƒitalic-Ï•matrixğ‘¦ğ‘¥ğ‘§\\begin{split}\\text{STP}^{-1}:\\begin{pmatrix}x\\\\ y\\\\ z\\\\ \\end{pmatrix}&=\\begin{pmatrix}\\frac{-1+x^{\\prime 2}+y^{\\prime 2}}{1+x^{\\prime 2% }+y^{\\prime 2}}\\\\ \\frac{2x^{\\prime}}{1+x^{\\prime 2}+y^{\\prime 2}}\\\\ \\frac{2y^{\\prime}}{1+x^{\\prime 2}+y^{\\prime 2}}\\\\ \\end{pmatrix}\\ ;\\\\ \\ \\text{SP}^{-1}:\\begin{pmatrix}\\theta\\\\ \\phi\\\\ \\end{pmatrix}&=\\begin{pmatrix}\\arctan(y/x)\\\\ \\arcsin(z)\\\\ \\end{pmatrix}\\ .\\end{split}start_ROW start_CELL STP start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT : ( start_ARG start_ROW start_CELL italic_x end_CELL end_ROW start_ROW start_CELL italic_y end_CELL end_ROW start_ROW start_CELL italic_z end_CELL end_ROW end_ARG ) end_CELL start_CELL = ( start_ARG start_ROW start_CELL divide start_ARG - 1 + italic_x start_POSTSUPERSCRIPT â€² 2 end_POSTSUPERSCRIPT + italic_y start_POSTSUPERSCRIPT â€² 2 end_POSTSUPERSCRIPT end_ARG start_ARG 1 + italic_x start_POSTSUPERSCRIPT â€² 2 end_POSTSUPERSCRIPT + italic_y start_POSTSUPERSCRIPT â€² 2 end_POSTSUPERSCRIPT end_ARG end_CELL end_ROW start_ROW start_CELL divide start_ARG 2 italic_x start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT end_ARG start_ARG 1 + italic_x start_POSTSUPERSCRIPT â€² 2 end_POSTSUPERSCRIPT + italic_y start_POSTSUPERSCRIPT â€² 2 end_POSTSUPERSCRIPT end_ARG end_CELL end_ROW start_ROW start_CELL divide start_ARG 2 italic_y start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT end_ARG start_ARG 1 + italic_x start_POSTSUPERSCRIPT â€² 2 end_POSTSUPERSCRIPT + italic_y start_POSTSUPERSCRIPT â€² 2 end_POSTSUPERSCRIPT end_ARG end_CELL end_ROW end_ARG ) ; end_CELL end_ROW start_ROW start_CELL SP start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT : ( start_ARG start_ROW start_CELL italic_Î¸ end_CELL end_ROW start_ROW start_CELL italic_Ï• end_CELL end_ROW end_ARG ) end_CELL start_CELL = ( start_ARG start_ROW start_CELL roman_arctan ( italic_y / italic_x ) end_CELL end_ROW start_ROW start_CELL roman_arcsin ( italic_z ) end_CELL end_ROW end_ARG ) . end_CELL end_ROW (8)\n\nC.2 Metric Depth Head\n\nWe exploit two metric depth heads with our proposed model. (1) The first is a bin structure proposed by ZoeDepth [20]. In our pilot experiment, we find that with this metric depth head, the decoder of the DAM should be frozen for stable training. Although this can preserve structural details, the bin prediction might destroy the continuity of depth prediction in some regions, especially when the backbone model size is small, as shown in Fig. 11(a). (2) Therefore, we try the other metric depth head, which is composed of two convolutional layers. The input and output channels of the first convolutional layer are 1 and 64, respectively. The input and output channels of the second convolutional layer are 64 and 1, respectively. With this head, the decoder of the DAM can be trained simultaneously. The convolutional metric depth head can generate a smooth prediction. However, some details might be neglected. For balance, our teacher model takes the metric depth head with bin structure to provide more structural details in the pseudo labels, while the student model employs the metric depth head with convolutional layers to predict smooth depths in both indoor and outdoor scenes.\n\nMore explanation about the metric depth output. The purpose of supervision in the metric depth space is to recover more structural details at the equator region than supervision in the disparity space. The improvement of metric depth supervision for recovering structural details is also demonstrated in [46, 47]. Differently, as we only train on one 360 depth dataset with labels, we do not conduct additional normalization operations. The metric depth output in our Any360D ranges from 0m to 10m (The maximum depth in the Matterport3D dataset). For scenes not in the Matterport3D dataset, the depth prediction can not reflect the absolute distance between the object and the camera. In addition, utilizing metric depth as supervision on 360 images can neglect the effect of camera intrinsics with various focal lengths. This is because most 360 cameras have fixed focal lengths, and can not achieve optical zoom. The digital zoom is conducted with the MÃ¶bius transformation.\n\nC.3 Loss Function\n\nSILog Loss. We only supervise the valid regions with the depth ground truth Dâˆ—superscriptğ·D^{*}italic_D start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT. Given the predicted depth Dğ·Ditalic_D, the SILog Loss employs the logarithm difference: Î”â¢Di=logâ¡Diâˆ’logâ¡Diâˆ—Î”subscriptğ·ğ‘–subscriptğ·ğ‘–superscriptsubscriptğ·ğ‘–\\begin{aligned} \\Delta D_{i}=\\log D_{i}-\\log D_{i}^{*}\\end{aligned}start_ROW start_CELL roman_Î” italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = roman_log italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - roman_log italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT end_CELL end_ROW. Then, by defining the number of valid pixels as Kğ¾Kitalic_K, the SILog Loss can be formulated as follows:\n\nâ„’=Î±â¢1Kâ¢âˆ‘iÎ”â¢Di2âˆ’Î»K2â¢(âˆ‘iÎ”â¢Di)2,â„’ğ›¼1ğ¾subscriptğ‘–Î”superscriptsubscriptğ·ğ‘–2ğœ†superscriptğ¾2superscriptsubscriptğ‘–Î”subscriptğ·ğ‘–2\\mathcal{L}=\\alpha\\sqrt{\\frac{1}{K}\\sum_{i}\\Delta D_{i}^{2}-\\frac{\\lambda}{K^{% 2}}(\\sum_{i}\\Delta D_{i})^{2}},caligraphic_L = italic_Î± square-root start_ARG divide start_ARG 1 end_ARG start_ARG italic_K end_ARG âˆ‘ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT roman_Î” italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT - divide start_ARG italic_Î» end_ARG start_ARG italic_K start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ( âˆ‘ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT roman_Î” italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG , (9)\n\nwhere Î±ğ›¼\\alphaitalic_Î± and Î»ğœ†\\lambdaitalic_Î» are two hyper-parameters. We set Î±ğ›¼\\alphaitalic_Î± to 10 and Î»ğœ†\\lambdaitalic_Î» to 0.85, following [7].\n\nAppendix D Experiment\n\nD.1 More Ablation Studies\n\nFor our proposed MTSA, the default setting is that: the vertical rotation angle is uniformly sampled in (âˆ’10âˆ˜,10âˆ˜)superscript10superscript10(-10^{\\circ},10^{\\circ})( - 10 start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT , 10 start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT ), denoted as ğ’°â¢(âˆ’10âˆ˜,10âˆ˜)ğ’°superscript10superscript10\\mathcal{U}(-10^{\\circ},10^{\\circ})caligraphic_U ( - 10 start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT , 10 start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT ). Moreover, the zoom level is uniformly sampled in (1,1.2)11.2(1,1.2)( 1 , 1.2 ), denoted as ğ’°â¢(1,1.2)ğ’°11.2\\mathcal{U}(1,1.2)caligraphic_U ( 1 , 1.2 ). To further discuss the effect of the MTSA by introducing vertical rotation and zoom into spatial augmentation, we conduct ablation studies for the range of sampling distribution.\n\nVertical rotation angle distribution. As shown in Tab. 8, we explore the effects of different angle distributions. It can be seen that different angle distributions have a small influence on the depth prediction on raw 360 images. Moreover, MTSA with a larger angle distribution benefits the depth prediction on 360 images with larger rotation angles.\n\nZoom level distribution. As shown in Tab. 9, we explore the effects of different zoom level distributions. We observe that MTSA with larger zoom level distribution can improve the depth estimation performance on the raw 360 images. For instance, MTSA with zoom level distribution ğ’°â¢(1,2.5)ğ’°12.5\\mathcal{U}(1,2.5)caligraphic_U ( 1 , 2.5 ) can obtain 2.2% gain in Abs Rel metric compared with distribution ğ’°â¢(1,1.2)ğ’°11.2\\mathcal{U}(1,1.2)caligraphic_U ( 1 , 1.2 ). Moreover, MTSA with zoom level distribution ğ’°â¢(1,2.0)ğ’°12.0\\mathcal{U}(1,2.0)caligraphic_U ( 1 , 2.0 ) obtains the best performance for depth prediction with zoom level 1.5. It demonstrates that a larger zoom level distribution can improve the robustness of the model in an appropriate margin. Furthermore, with the zoom level distribution increasing to ğ’°â¢(1,2.5)ğ’°12.5\\mathcal{U}(1,2.5)caligraphic_U ( 1 , 2.5 ), the model performance has a slight drop for depth prediction under zoom level 1.5. It reveals that the choice of distribution influences the model performance in different conditions.\n\nMore visualization results. We provide more visualization results of the Depth Anything model (DAM) [1] and our Any360D with the ViT-S as the backbone in various scenes, ranging from indoor scenes to outdoor scenes. Figs. 13, 14, 15, 16, 17, 18 are indoor scenes, while Figs. 19, 20 are outdoor scenes.\n\nDemo description. In the demo video, in addition to the scenes in our Diverse360 dataset, we also provide the visualization results in open-world web scenes. The videos are captured in the following websites: https://www.miraikan.jst.go.jp/en/research/AccessibilityLab/dataset360/. We have obtained their approval to utilize their 360 videos.\n\nReferences\n\n[1] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In CVPR, 2024.\n\n[2] Zhijie Shen, Chunyu Lin, Kang Liao, Lang Nie, Zishuo Zheng, and Yao Zhao. Panoformer: Panorama transformer for indoor 360âˆ˜ depth estimation. In ECCV, 2022.\n\n[3] Hao Ai, Zidong Cao, Jinjing Zhu, Haotian Bai, Yucheng Chen, and Lin Wang. Deep learning for omnidirectional vision: A survey and new perspectives. arXiv preprint arXiv:2205.10468, 2022.\n\n[4] Weiming Zhang, Yexin Liu, Xu Zheng, and Lin Wang. Goodsam: Bridging domain and capacity gaps via segment anything model for distortion-aware panoramic semantic segmentation. arXiv preprint arXiv:2403.16370, 2024.\n\n[5] Qi Feng, Hubert PH Shum, and Shigeo Morishima. 360 depth estimation in the wild-the depth360 dataset and the segfuse network. In 2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR), pages 664â€“673. IEEE, 2022.\n\n[6] Markus SchÃ¶n, Michael Buchholz, and Klaus Dietmayer. Mgnet: Monocular geometric scene understanding for autonomous driving. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15804â€“15815, 2021.\n\n[7] Weihao Yuan, Xiaodong Gu, Zuozhuo Dai, Siyu Zhu, and Ping Tan. Neural window fully-connected crfs for monocular depth estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 3916â€“3925, 2022.\n\n[8] Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka. Adabins: Depth estimation using adaptive bins. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4009â€“4018, 2021.\n\n[9] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-d data in indoor environments. arXiv preprint arXiv:1709.06158, 2017.\n\n[10] Iro Armeni, Sasha Sax, Amir R Zamir, and Silvio Savarese. Joint 2d-3d-semantic data for indoor scene understanding. arXiv preprint arXiv:1702.01105, 2017.\n\n[11] Georgios Albanis, Nikolaos Zioulis, Petros Drakoulis, Vasileios Gkitsas, Vladimiros Sterzentsenko, Federico Alvarez, Dimitrios Zarpalas, and Petros Daras. Pano3d: A holistic benchmark and a solid baseline for 360deg depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3727â€“3737, 2021.\n\n[12] Yuyan Li, Yuliang Guo, Zhixin Yan, Xinyu Huang, Ye Duan, and Liu Ren. Omnifusion: 360 monocular depth estimation via geometry-aware fusion. CoRR, abs/2203.00838, 2022.\n\n[13] Hao Ai, Zidong Cao, Yan-Pei Cao, Ying Shan, and Lin Wang. Hrdfuse: Monocular 360deg depth estimation by collaboratively learning holistic-with-regional depth distributions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13273â€“13282, 2023.\n\n[14] Hualie Jiang, Zhe Sheng, Siyu Zhu, Zilong Dong, and Rui Huang. Unifuse: Unidirectional fusion for 360âˆ˜ panorama depth estimation. IEEE Robotics and Automation Letters, 6:1519â€“1526, 2021.\n\n[15] Manuel Rey-Area, Mingze Yuan, and Christian Richardt. 360monodepth: High-resolution 360deg monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3762â€“3772, 2022.\n\n[16] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4015â€“4026, 2023.\n\n[17] Lei Ke, Mingqiao Ye, Martin Danelljan, Yu-Wing Tai, Chi-Keung Tang, Fisher Yu, et al. Segment anything in high quality. Advances in Neural Information Processing Systems, 36, 2024.\n\n[18] RenÃ© Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE transactions on pattern analysis and machine intelligence, 44(3):1623â€“1637, 2020.\n\n[19] Reiner Birkl, Diana Wofk, and Matthias MÃ¼ller. Midas v3. 1â€“a model zoo for robust monocular relative depth estimation. arXiv preprint arXiv:2307.14460, 2023.\n\n[20] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias MÃ¼ller. Zoedepth: Zero-shot transfer by combining relative and metric depth. arXiv preprint arXiv:2302.12288, 2023.\n\n[21] Zidong Cao, Hao Ai, Yan-Pei Cao, Ying Shan, Xiaohu Qie, and Lin Wang. Omnizoomer: Learning to move and zoom in on sphere at high-resolution. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12897â€“12907, 2023.\n\n[22] Zidong Cao, Zhan Wang, Yexin Liu, Yan-Pei Cao, Ying Shan, Wei Zeng, and Lin Wang. Learning high-quality navigation and zooming on omnidirectional images in virtual reality. arXiv preprint arXiv:2405.00351, 2024.\n\n[23] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.\n\n[24] Nikolaos Zioulis, Antonis Karakottas, Dimitrios Zarpalas, and Petros Daras. Omnidepth: Dense depth estimation for indoors spherical panoramas. In Proceedings of the European Conference on Computer Vision (ECCV), pages 448â€“465, 2018.\n\n[25] Benjamin Coors, Alexandru Paul Condurache, and Andreas Geiger. Spherenet: Learning spherical representations for detection and classification in omnidirectional images. In Proceedings of the European conference on computer vision (ECCV), pages 518â€“533, 2018.\n\n[26] Ilwi Yun, Chanyong Shin, Hyunku Lee, Hyuk-Jae Lee, and Chae Eun Rhee. Egformer: Equirectangular geometry-biased transformer for 360 depth estimation. arXiv preprint arXiv:2304.07803, 2023.\n\n[27] Fu-En Wang, Yu-Hsuan Yeh, Min Sun, Wei-Chen Chiu, and Yi-Hsuan Tsai. Bifuse: Monocular 360 depth estimation via bi-projection fusion. In CVPR, pages 459â€“468. Computer Vision Foundation / IEEE, 2020.\n\n[28] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Hohonet: 360 indoor holistic understanding with latent horizontal features. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2573â€“2582, 2020.\n\n[29] Giovanni Pintore, Marco Agus, Eva Almansa, Jens Schneider, and Enrico Gobbetti. Slicenet: deep dense depth estimation from a single indoor panorama using a slice-based representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11536â€“11545, 2021.\n\n[30] Haozheng Yu, Lu He, Bing Jian, Weiwei Feng, and Shan Liu. Panelnet: Understanding 360 indoor environment via panel representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 878â€“887, 2023.\n\n[31] Jesper E Van Engelen and Holger H Hoos. A survey on semi-supervised learning. Machine learning, 109(2):373â€“440, 2020.\n\n[32] Xiangli Yang, Zixing Song, Irwin King, and Zenglin Xu. A survey on deep semi-supervised learning. IEEE Transactions on Knowledge and Data Engineering, 35(9):8934â€“8954, 2022.\n\n[33] Yidong Wang, Hao Chen, Yue Fan, Wang Sun, Ran Tao, Wenxin Hou, Renjie Wang, Linyi Yang, Zhi Zhou, Lan-Zhe Guo, et al. Usb: A unified semi-supervised learning benchmark for classification. Advances in Neural Information Processing Systems, 35:3938â€“3961, 2022.\n\n[34] Jinjing Zhu, Haotian Bai, and Lin Wang. Patch-mix transformer for unsupervised domain adaptation: A game perspective. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3561â€“3571, 2023.\n\n[35] Kihyuk Sohn, Zizhao Zhang, Chun-Liang Li, Han Zhang, Chen-Yu Lee, and Tomas Pfister. A simple semi-supervised learning framework for object detection. arXiv preprint arXiv:2005.04757, 2020.\n\n[36] Mengde Xu, Zheng Zhang, Han Hu, Jianfeng Wang, Lijuan Wang, Fangyun Wei, Xiang Bai, and Zicheng Liu. End-to-end semi-supervised object detection with soft teacher. In Proceedings of the IEEE/CVF international conference on computer vision, pages 3060â€“3069, 2021.\n\n[37] Aneesh Rangnekar, Christopher Kanan, and Matthew Hoffman. Semantic segmentation with active semi-supervised learning. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 5966â€“5977, 2023.\n\n[38] Yassine Ouali, CÃ©line Hudelot, and Myriam Tami. Semi-supervised semantic segmentation with cross-consistency training. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12674â€“12684, 2020.\n\n[39] Jongbeom Baek, Gyeongnyeon Kim, and Seungryong Kim. Semi-supervised learning with mutual distillation for monocular depth estimation. In 2022 International Conference on Robotics and Automation (ICRA), pages 4562â€“4569. IEEE, 2022.\n\n[40] Yevhen Kuznietsov, Jorg Stuckler, and Bastian Leibe. Semi-supervised deep learning for monocular depth map prediction. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6647â€“6655, 2017.\n\n[41] Steve Cruz, Will Hutchcroft, Yuguang Li, Naji Khosravan, Ivaylo Boyadzhiev, and Sing Bing Kang. Zillow indoor dataset: Annotated floor plans with 360deg panoramas and 3d room layouts. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2133â€“2143, 2021.\n\n[42] Hao Chen, Yuqi Hou, Chenyuan Qu, Irene Testini, Xiaohan Hong, and Jianbo Jiao. 360+ x: A panoptic multi-modal scene understanding dataset. arXiv preprint arXiv:2404.00989, 2024.\n\n[43] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from a single image using a multi-scale deep network. Advances in neural information processing systems, 27, 2014.\n\n[44] Chuanqing Zhuang, Zhengda Lu, Yiqun Wang, Jun Xiao, and Ying Wang. Acdnet: Adaptively combined dilated convolution for monocular panorama depth estimation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 3653â€“3661, 2022.\n\n[45] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n\n[46] Wei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus, Long Mai, Simon Chen, and Chunhua Shen. Learning to recover 3d scene shape from a single image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 204â€“213, 2021.\n\n[47] Chi Zhang, Wei Yin, Billzb Wang, Gang Yu, Bin Fu, and Chunhua Shen. Hierarchical normalization for robust monocular depth estimation. Advances in Neural Information Processing Systems, 35:14128â€“14139, 2022."
    }
}
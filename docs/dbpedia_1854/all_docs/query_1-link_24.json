{
    "id": "dbpedia_1854_1",
    "rank": 24,
    "data": {
        "url": "https://arxiv.org/html/2407.17438v1",
        "read_more_link": "",
        "language": "en",
        "title": "HumanVid: Demystifying Training Data for Camera-controllable Human Image Animation",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://arxiv.org/html/x1.png",
            "https://arxiv.org/html/x2.png",
            "https://arxiv.org/html/x3.png",
            "https://arxiv.org/html/x4.png",
            "https://arxiv.org/html/x5.png",
            "https://arxiv.org/html/x6.png",
            "https://arxiv.org/html/x7.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Zhenzhi Wang1 \\AndYixuan Li1 \\AndYanhong Zeng2 \\AndYouqing Fang2 \\AndYuwei Guo1 \\AndWenran Liu2 \\AndJing Tan1 \\AndKai Chen2 \\AndTianfan Xue1,2 \\AndBo Dai2 \\AndDahua Lin1,2\n\n1CUHK 2Shanghai Artificial Intelligence Laboratory\n\nhttps://humanvid.github.io/\n\nAbstract\n\nHuman image animation involves generating videos from a character photo, allowing user control and unlocking potential for video and movie production. While recent approaches yield impressive results using high-quality training data, the inaccessibility of these datasets hampers fair and transparent benchmarking. Moreover, these approaches prioritize 2D human motion and overlook the significance of camera motions in videos, leading to limited control and unstable video generation.To demystify the training data, we present HumanVid, the first large-scale high-quality dataset tailored for human image animation, which combines crafted real-world and synthetic data. For the real-world data, we compile a vast collection of copyright-free real-world videos from the internet. Through a carefully designed rule-based filtering strategy, we ensure the inclusion of high-quality videos, resulting in a collection of 20K human-centric videos in 1080P resolution. Human and camera motion annotation is accomplished using a 2D pose estimator and a SLAM-based method. For the synthetic data, we gather 2,300 copyright-free 3D avatar assets to augment existing available 3D assets. Notably, we introduce a rule-based camera trajectory generation method, enabling the synthetic pipeline to incorporate diverse and precise camera motion annotation, which can rarely be found in real-world data. To verify the effectiveness of HumanVid, we establish a baseline model named CamAnimate, short for Camera-controllable Human Animation, that considers both human and camera motions as conditions. Through extensive experimentation, we demonstrate that such simple baseline training on our HumanVid achieves state-of-the-art performance in controlling both human pose and camera motions, setting a new benchmark. Code and data will be publicly available at https://github.com/zhenzhiwang/HumanVid/.\n\n1 Introduction\n\nHigh-quality and highly controllable human image animation has significantly progressed as an emerging popular task [17, 29, 31, 71]. Imagine the possibilities of recreating iconic movie performances using just a single photo of the characters, capturing them from any desired angle. This technique has the potential to significantly impact video and movie production. In this study, we focus on animating characters from a single image, considering both human and camera motions as crucial factors for generating realistic human videos.\n\nDespite some promising progress [29, 86], human image animation presents two main challenges: the absence of a high-quality public dataset and the neglect of camera motions in human videos. Specifically, state-of-the-art approaches rely on private datasets for training similar models, underscoring the importance of datasets in this field. However, these datasets remain inaccessible, while accessible alternatives like TikTok [30] and UBC-Fashion [78] possess limitations in scale and quality, hindering fair and transparency evaluation and community development. Furthermore, despite leveraging enigmatic private datasets, these methods struggle to animate characters from new viewpoints with camera movement. They primarily rely on 2D pose extraction from static camera videos [72], neglecting the crucial aspect of camera motion. This design choice compromises video controllability and impedes high-quality character animation for complex motions.\n\nTo address the lack of a high-quality public dataset with accurate camera motion annotations, we introduce a synthetic dataset along with a high-quality real-world video dataset for human image animation. Our scalable pipeline allows us to create a large-scale dataset with precise annotations for both human and camera motions. Through extensive experiments, we validate the significance of this dataset combination in achieving high-quality and controllable human image animation.\n\nTo begin with, we compile a vast collection of real-world videos from diverse scenes on copyright-free internet platforms. This uncurated dataset often contains noise, such as frequent lens switching, user-generated special visual effects, occlusions and object motions. To ensure high-quality videos, we employ a carefully designed rule-based filtering strategy to ensure that pixel motions in our collected videos are exclusively resulted from human and camera motions. Additionally, we utilize SLAM-based methods [66, 60] for accurate camera trajectory extraction and a precise pose estimator [72] to extract human pose sequences. This results in a remarkable collection of over 20K human-centric videos in 1080P resolution. Experimental results demonstrate that training models exclusively on this curated video dataset achieve state-of-the-art performances.\n\nSynthesizing diverse and high-fidelity human videos for animation is non-trivial. Existing 3D synthetic datasets for humans primarily focus on reconstruction or perception, resulting in limited appearance diversity, over-smoothed textures, and restricted camera motions [10, 73]. To overcome these limitations, we first augment our dataset with approximately 2,300 copyright-free 3D avatar assets. These assets undergo rigorous rigging using motions from motion-captured datasets [39] and open-source software [4], enabling a wide range of character shapes, appearances, and human motions. Notably, to compensate for the limited camera motions observed in real-world videos, we introduce an innovative rule-based camera trajectory system, enriching the diversity of camera movements in training data. Specifically, multiple camera locations are randomly sampled for the keyframes throughout the space, and each camera is purposefully directed toward the human face. These sampled key camera motions are then connected and smoothed, replicating common camera movements in videos and films. This design enables us to generate lifelike human videos with accurate annotations of human and camera motions. The illustration of our synthetic data is in Fig. 1. Our experiments conclusively demonstrate that utilizing our synthetic dataset significantly enhances animation, particularly regarding motion control.\n\nTo validate the collected dataset, we incorporate camera control [23] into a widely-used model that only considers pose condition [29]. Our main contributions are as follows,\n\n•\n\nTo the best of our knowledge, we are the first to introduce a synthetic dataset combined with the first high-quality real-world video dataset for human image animation.\n\n•\n\nWe design a scalable pipeline that facilitates lifelike human video generation and provides accurate and diverse annotations of human and camera motions. This results in improved visual quality and enhanced control over human image animation.\n\n•\n\nThrough extensive experiments, we validate the effectiveness of each dataset component, establish a new state-of-the-art, and create a comprehensive, fair, and transparent evaluation benchmark for the field.\n\n2 Related Work\n\nHuman Image Animation. The task of human image animation aims to generate coherent human videos from a single image. To enhance controllability, the mainstream works in this field often employ explicit human skeleton representation, e.g., OpenPose [14, 56, 69] and DensePose [20], as additional guidance. Early solutions are majorly developed upon GANs for image animation and pose transfers [16, 48, 54, 53, 55, 77, 83]. More recently, diffusion models (DMs) [25, 59, 41, 65] have been drawing attention from human image animation considering their remarkable success and high-quality results in image [50, 42, 47, 52, 7, 45] and video [11, 85, 57, 27, 26, 51, 75, 64, 22] synthesis. For instance, MagicDance [17] proposes a two-stage training strategy to disentangle the learning of appearance and human motion. Animate Anyone [29] utilizes a reference network to extract the appearance representation from the source image and adopts a motion module similar to AnimateDiff [22] to enhance temporal consistency. It also incorporates a lightweight pose guider to encode pose information to the pre-trained models. Similarly, MagicAnimate [71] adopts DensePose [20] as the motion representation and uses a ControlNet [80] to encode pose information. Champ [86] further introduces the SMPL [38] model sequence and the rendered depth and normal for better alignment. Though with remarkable visual quality, these works mostly adopt a static camera setting and do not consider camera viewpoint movement.\n\nCamera-aware Video Generation. As a significant component in video and movie production, camera viewpoint movement determines the content dynamics and the overall feeling of the audience. While many works focus on guiding video generative models with structural signals [18, 75, 63, 82, 32, 21], less attention has been paid to controlling the pose/viewpoint of camera in generating videos [70, 76]. To control camera motion with reference videos, MotionDirector [84] proposes a dual-path LoRA [28] adapter to decouple the motion and appearance learning and can roughly control camera movements to produce a surrounding shot. For more precise control, MotionCtrl [68] directly injects the camera extrinsic matrix to the temporal attention layer in pre-trained text-to-video models and can precisely specify the camera viewpoint by providing camera poses at inference. CameraCtrl [23] further enhances the controllability by representing the camera pose with Plücker ray embeddings [58, 36]. CamViG [40] explores the camera control in token-based video generator [35] by introducing camera embedding as a new modality.\n\nHuman Video Datasets. Diverse and large-scale human-centric video datasets are essential for enabling human image animation tasks. For real-world datasets collected from the Internet, TikTok [30] provides 340 human-centric video clips from social media with diverse appearances and performances, while UBC-Fashion [78] contains 500 fashion video clips with blank backgrounds. To scale up the dataset at a lower cost, several synthetic datasets have been proposed. SURREAL [62] generates over 6M realistic images of people rendered from 3D sequences of captured human poses, AGORA [43] renders from real human scans in diverse poses and natural clothing, and HSPACE [8] combines diverse individuals with different motions and scenes, obtaining the animation by fitting a human body model. GTA-Human [13] constructs a dataset with the GTA-V game engine, featuring a diverse set of subjects, actions, and scenarios. SynBody [74] includes 1.7M images with 3D human body annotations, covering diverse body models, actions, viewpoints, and scene styles. BEDLAM [10] renders people in realistic scenes and utilizes physics simulation to obtain realistically rendered clothing. While previous synthetic human datasets were designed for pose estimation or multi-view reconstruction, our work is the first to explore the use of synthetic data for video generation.\n\n3 Dataset\n\nGiven that diffusion models typically require large amounts of data, we are pioneering the use of synthetic data in human video generation. While previous datasets [10, 73] only contain single-view image data or clips with basic camera movements (i.e. zoom-in, orbit), we show that accurate annotations, extensive scale and rich camera trajectories from synthetic data could be vital for generation. Our synthetic videos are rendered by Unreal Engine 5 (UE5) [3] or Blender [12]. To enhance the diversity of human appearance, we also curate human-centric internet videos from copyright-free platforms and leverage pose estimation methods [72] for automatic annotation. Both synthetic and internet data are fully scalable without any human supervision.\n\n3.1 Synthetic Data Construction\n\nThe synthetic video data are rendered with one or more characters moving in various 3D scenes using diverse camera trajectories. Consequently, constructing the synthetic data involves three key steps: character creation, motion retargeting, and 3D scene and camera placement.\n\n3.1.1 Character Creation\n\nWe create two types of characters for animating images in the diverse domains: (1) Human-like characters from SMPL-X [44] meshes and clothing. (2) Anime characters from user-uploaded assets. Diverse body shapes and skin textures, 3D clothing and textures are considered for highly varied human representation.\n\nBody shapes and skin tone. For human-like characters, we sample body shapes from a diverse set of 271 body shapes with different BMI collected from the ARGOA [43] and CAESAR [49] datasets following Bedlam [10]. To reduce the gender and ethnicity bias, we use 50 female and 50 male commercial skin albedo textures collected from Meshcapade [19] with a resolution of 4096 ×\\times× 4096, spanning over seven ethnic groups.\n\n3D Clothing and textures. To generate realistic human videos, it’s crucial to have 3D clothing motions that are physically plausible and consistent with human body movements. For instance, the LSMPL-X representation from Synbody [73] adds a clothing layer to SMPL-X [44], but lacks realistic physics simulation for clothing motion, leading to unnatural movements in loose-fitting clothes like dresses. We collect 111 unique outfits, including T-shirts, sweaters, coats, jeans and skirts from Bedlam [10] dataset, and use commercial simulation software [1] to obtain realistic clothing deformations. On top of realistic meshes of human mesh and clothing from physics simulation, 1691 unique clothing textures are used for diverse clothing appearances. We shows examples of clothed SMPL-X characters in Fig. 4.\n\nAnime characters. To enhance the diversity of characters in our synthetic data, we focused on VRoidHub [6], a platform designed for sharing and showcasing 3D character models. Creators on VRoidHub have uploaded a vast array of intricate 3D character models, featuring diverse appearances, clothing styles, and hairstyles. From this rich repository, we manually selected 2,387 characters for further rendering.\n\n3.1.2 Motion Retargeting\n\nGiven the character assets, we transfer diverse motions to these characters by re-targeting motion data from various sources, including motion capture datasets [39] and open-source software Rokoko [4].\n\nSPML-X characters. For human-like SMPL-X [44] characters, we sample human motions from large-scale motion capture datasets [39]. To enhance motion diversity, we sample based on motion annotations from [46], following the approach of Bedlam [10].\n\nAnime characters. Conversely, anime character assets can have diverse skeleton lengths. We utilize the automatic re-targeting software Rokoko to transfer existing motions to the anime character assets. The clothing and hair are treated as part of the body, so their motion is also determined by source motions. The illustration of transferring motion to anime assets is shown in Fig. 3.\n\n3.1.3 3D Scenes and Camera Placements\n\n3D Scenes. The realistic and diverse 3D scene backgrounds for synthetic video are constructed from about 100 panoramic HDRI images [2] or high-quality 3D scenes to cover both indoor and outdoor environment. We manually select panorama images with flat ground for characters to move on, while avoiding excessive scene components that might lead to unnatural human-scene interactions. We also exclude images with uniform visual patterns across different views, such as grasslands, deserts, or farms. The selected panorama backgrounds feature high-quality, complex texture details that highlight varying background textures from different camera angles. We provide more details in the supplementary materials.\n\nCamera Trajectory Design. Unlike [10, 73, 43], our dataset highlights rich and diverse camera trajectories in human-centric videos. Each camera trajectory consists of a sequence of 6666-DoF translations and rotations. We carefully design a rule-based camera motion generation pipeline to obtain diverse trajectories. This pipeline randomly sample camera locations adaptive to human positions and orientations in the keyframes, and use spline interpolation to get smooth camera locations and rotations in the whole sequence. Specifically, in each keyframe scene space, we randomly sample camera locations within a semi-cylinder of radius ∈[3⁢m,5⁢m]absent3𝑚5𝑚\\in[3m,5m]∈ [ 3 italic_m , 5 italic_m ] and height ∈[0.6⁢m,1.2⁢m]absent0.6𝑚1.2𝑚\\in[0.6m,1.2m]∈ [ 0.6 italic_m , 1.2 italic_m ] in front of the human. Then, we set the camera orientation’s yaw and pitch to point at the person. To create a more natural camera trajectory that smoothly follows the person, we adjust the camera’s position by adding the human’s position offset from the keyframe to the camera position in each frame. Finally, we also sample the roll of camera rotation ∈[−30∘,30∘]absentsuperscript30superscript30\\in[-30^{\\circ},30^{\\circ}]∈ [ - 30 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT , 30 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT ] in keyframes. Our design of camera keyframe sampling enables all types of camera trajectories, significantly enhancing the camera trajectory diversity and cinematic effect of human videos compared to existing video datasets.\n\nRendering and Annotations. We render the image sequences of SMPL-X characters using UE5 game engine and the built-in movie render function (Movie Render Queue) for high-quality images. The anime characters are rigged and rendered with blender. With our synthetic data source, a variety of ground-truth annotations, including camera trajectories, human skeletons, segmentation masks, depth maps and normal maps, could be obtained without manual efforts. Although our dataset is curated for human video generation, these ground-truth annotations could also be useful for other downstream applications.\n\n3.2 Internet Data Curation\n\nTo enhance the appearance diversity on top of synthetic videos, we collect real human-centric videos from copyright-free internet platforms [5] with the same distribution of synthetic data: the pixel motions in such videos is only resulted from human skeleton motion or camera motion, not object movements or background dynamics.\n\nWe utilize the Pexels API [5] to scrape data based on around 100 keywords and employed a pose detector [72] to analyze the data. The pose detector focused on measuring the upper body keypoints’ confidence, the ratio of the largest human bounding box over the frame r𝑟ritalic_r, the average number of humans present in each frame n𝑛nitalic_n, and the average motion (position offsets) of the keypoints Δ⁢𝐩¯Δ¯𝐩\\Delta\\bar{\\mathbf{p}}roman_Δ over¯ start_ARG bold_p end_ARG. With these statistics, we apply a specific filtering criteria: a) the human should occupy the main part of the image (r>0.07𝑟0.07r>0.07italic_r > 0.07); b) there should be few people (n≤4𝑛4n\\leq 4italic_n ≤ 4); c) there should be a noticeable motion in keypoints to remove static videos (Δ⁢𝐩¯>0.01Δ¯𝐩0.01\\Delta\\bar{\\mathbf{p}}>0.01roman_Δ over¯ start_ARG bold_p end_ARG > 0.01); d) No exits, entrances or occlusions of individuals in videos (n∈ℤ𝑛ℤn\\in\\mathbb{Z}italic_n ∈ blackboard_Z). As a result, we collect around 20k high-quality, real human-centric video clips with various appearances.\n\nCamera Trajectory Estimation. Reconstructing global camera trajectories from in-the-wild videos is a challenging problem. Following the curation of human-centric videos from previous steps, we adopt TRAM [66] to utilize a SLAM method [60] for recovering camera extrinsic parameters from in-the-wild videos with explicit human movement. To ensure camera parameters are robust to dynamic humans, we employ a masking technique [34] that removes dynamic regions from both input images and dense bundle adjustment steps. By compelling SLAM to rely solely on the background for camera estimation from the outset, we mitigate the risk of catastrophic failure. To convert camera estimation to metric scale, we leverage semantic cues from the background by utilizing noisy depth predictions [9]. Consequently, we recover accurate, metric-scale camera motion that serves as an optimal camera condition for training diffusion models. For videos with texture-less backgrounds, where the SLAM system reports large reconstruction errors, we empirically set such samples to static cameras. Additionally, we filter out videos with very large rotations or translations, such as cycling, or those with sudden shot changes, as these videos fall outside the scope of human image animation.\n\nStatistics. As shown in Tab. 1, our Real Internet dataset, with over 20k clips and 10M frames at 1080P resolution, significantly surpasses existing datasets like TikTok [30], UBC-Fashion [78] and IDEA-400 [37] in both size and resolution. Additionally, our synthetic dataset from SMPL-X and Anime characters, is 5×5\\times5 × larger in scale compared to Bedlam [10], providing accurate camera and human pose groundtruth, and more diverse camera trajectories.\n\n3.3 CamAnimate\n\nTo validate our dataset’s capability for animating humans with moving cameras, we propose a simple baseline for the camera-controllable human image animation task, named CamAnimate. By leveraging CameraCtrl [23]’s advanced camera pose control and Animate Anyone [29]’s character animation framework, CamAnimate ensures consistent and high-quality human video generation with simultaneous human and camera movements. As shown in Fig. 5, it utilizes plücker embeddings to accurately parameterize camera trajectories and incorporates an additional camera pose encoder to encode camera information for the Denoising UNet via zero-convolution [79], while ReferenceNet and a pose guider maintain appearance consistency and pose controllability. By training our method on general human videos with both camera and human movement, these two types of motion can be decoupled in the network and learned by separate modules in an end-to-end manner. In addition to the moving camera setting, CamAnimate can seamlessly handle traditional human image animation given a static camera parameter.\n\nImplementation Details. We use the checkpoint of Stable Diffusion 1.5 [50] to initialize the Denoising UNet and ReferenceNet, and use the weights of ControlNet [80] on OpenPose [15] to initialize the Pose Guider. The camera encoder weights from CameraCtrl [23] are used to initialize our camera encoder. We mix train horizontal and vertical videos with a resolution of (long side, short side) =(896,512)absent896512=(896,512)= ( 896 , 512 ), i.e., for horizontal videos (w,h)=(896,512)𝑤ℎ896512(w,h)=(896,512)( italic_w , italic_h ) = ( 896 , 512 ) and for vertical videos (w,h)=(512,896)𝑤ℎ512896(w,h)=(512,896)( italic_w , italic_h ) = ( 512 , 896 ). Each batch only samples either all horizontal or all vertical videos, and the choice between horizontal and vertical videos is made randomly between batches. The setting of such resolution is according to the GPU memory in our experiments, i.e., maximum GPU memory usage with such pairs of batch size and resolution in both stages. We empirically find that such resolution could achieve a balance of visual quality and computational cost. In the first stage, we train all network parameters using a batch size of 8. In the second stage, we freeze the denoising UNet, reference UNet, and pose guider, and only train the camera encoder and motion module. The motion module in the second stage is initialized with the weights from AnimateDiff [22] V3. The frame rate is set to 24, and the batch size is 1. We use 8 NVIDIA A100 GPUs for all training stages and 1 NVIDIA A100 GPU for testing. The first and second stages are trained for 30,000 and 10,000 iterations, respectively, with a learning rate of 1e-5 and AdamW optimizer [33]. Our camera embedding representation is the Plücker embedding from CameraCtrl, which is computed from the camera’s intrinsic and extrinsic parameters. For real internet data, the intrinsic parameters are set using a heuristic value, while the extrinsic parameters are obtained using SLAM [60] methods. For synthetic data, the intrinsic and extrinsic parameters are directly exported.\n\n4 Experiments\n\n4.1 Evaluation Benchmark\n\nDue to the lack of a unified benchmark for previous methods, the testing protocols for each method have been varied significantly. However, when the form of the samples differs, the disparity between the reference image and the target image can vary greatly, leading to highly inconsistent results for the same method when inferring different reference and target images. Therefore, as the first large-scale dataset, we provide a unified testing protocol for human video generation. Specifically, we use the middle (12th) frame as the reference image, predict frames in the range [1,72) with a stride of 3, resulting in a sequence of 24 frames. Finally, we evaluate each video under this setting using PSNR [67], SSIM [67], LPIPS [81], FID [24], and FVD [61] metrics.\n\nWe use the last 40 videos from the TikTok dataset [30] out of a total of 340 videos as the test set for evaluation on static camera. Additionally, we provide 40 videos each in both portrait and landscape orientations as a test set for evaluation on moving camera human video generation, sampled from our collected Internet videos. It is worth noting that SSIM, PSNR, and LPIPS are only reliable under static camera conditions without any human turning. For human videos with camera movements, these reconstruction metrics may not be trustworthy, as video generation inherently involves ambiguity, where we prefer generation metrics like FID and FVD.\n\n4.2 Comparison with the State-of-the-Art\n\nIn this section, we compare our baseline model with previous state-of-the-art methods, namely Animate Anyone [29], Magic-animate [71] and Champ [86]. As animate anyone is not open-sourced, we use a third-party implementation . We use the official implementations for other two methods. As shown in Tab. 2, although our method is trained on videos with moving cameras, it is still able generate high-quality static camera videos on TikTok dataset and achieves best performance in all metrics due to our precise camera control ability. For human videos with camera movement on our test set, previous methods commonly do not consider the camera condition, so they struggle to produce natural videos with camera movements. Such result could be observed from both reconstruction metrics like SSIM, PSNR, and LPIPS and generation metrics like FID and FVD.\n\nUser-study. We also conduct a user-study to compare our method with previous methods, as shown in Tab 4. We collect 2 videos from Tiktok test set and 8 videos from our test set, and compare our results with other three methods’ results as a ranking question with 4 options. A total of 20 participants take part in our user study and we assign 3, 2, 1 points for the first, second and third method respectively. The final average score is normalized by total points, so the upper bound of this metric is 0.5. We conclude the average points and top-1 preference of each method in Tab 4, which shows a dominate advantage (0.44 points and 0.73 top-1 preference) over previous methods due to their artifacts in appearance, human pose and camera movements.\n\nQualitative comparisons. In Fig. 6 , we show the qualitative comparison with previous SOTA methods, where the artifacts are highlighted by red boxes. We show that previous SOTA methods may have different artifacts when applied to our challenging evaluation test sets. For example, animate anyone suffers from inaccurate and low-quality appearances of humans. Champ suffers from missing 3D skeletons due to the difficulty of estimating accurate 3D skeletons, especially in a crowded scene. Magic-animate is not able to correctly generate human face expressions due to the ambiguity of face representation of Densepose. Our method is able to accurately generate facial expressions, body pose and background motions from the camera movement.\n\nQualitative results on in-the-wild cases. In Fig. 7, we further show our methods ability in generating in-the-wild videos with explicit camera movements. We show that we can achieve high-quality and visually pleasant videos that is close to the actual movies filmed by professionals. We hope that our dataset and the baseline method could serve as a solid base for generating movie-level human videos. Please visit our project page to view more video results.\n\n5 Conclusion\n\nIn this paper, our study addresses the significant challenges in the field of human image animation by introducing a novel combination of a high-quality real-world video dataset and a meticulously crafted synthetic dataset. Our proposed dataset not only enhances the visual quality and controllability of human animations, but also introduces a new benchmark for camera control in human videos. Without bells and whistles, our proposed simple baseline demonstrate superior performance when it is trained on our combined dataset, particularly in scenarios involving complex human and camera motions. We believe that our contributions will pave the way for more transparent and comprehensive evaluations in this field, fostering further advancements and innovations in video and movie production.\n\nBroader Impacts. Our dataset and baseline method are highly effective at creating realistic human videos. Nonetheless, it’s important to recognize that improvements in generative model technologies could lead to the creation of realistic deepfakes, which may be misused to spread misinformation.\n\nAcknowledgment. This project is funded in part by Shanghai AI Laboratory (P23KS00020, 2022ZD0160201), CUHK Interdisciplinary AI Research Institute, and the Centre for Perceptual and Interactive Intelligence (CPII) Ltd under the Innovation and Technology Commission (ITC)’s InnoHK.\n\nReferences\n\nclo [2022] Clo. https://www.clo3d.com, 2022.\n\npol [2022] Polyhaven. https://polyhaven.com, 2022.\n\nunr [2022] Unreal Engine 5. https://www.unrealengine.com, 2022.\n\nrok [2023] Rokoko. https://github.com/Rokoko/rokoko-studio-live-blender, 2023.\n\nPex [2024] Pexels. https://www.pexels.com/, 2024.\n\nVRo [2024] Vroidhub. https://hub.vroid.com, 2024.\n\nBalaji et al. [2022] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion models with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022.\n\nBazavan et al. [2021] Eduard Gabriel Bazavan, Andrei Zanfir, Mihai Zanfir, William T. Freeman, Rahul Sukthankar, and Cristian Sminchisescu. Hspace: Synthetic parametric humans animated in complex environments. arXiv: Comp. Res. Repository, 2021.\n\nBhat et al. [2023] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias Müller. Zoedepth: Zero-shot transfer by combining relative and metric depth. arXiv preprint arXiv:2302.12288, 2023.\n\nBlack et al. [2023] Michael J Black, Priyanka Patel, Joachim Tesch, and Jinlong Yang. Bedlam: A synthetic dataset of bodies exhibiting detailed lifelike animated motion. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages 8726–8737, 2023.\n\nBlattmann et al. [2023] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22563–22575, 2023.\n\nBlender Online Community [2022] Blender Online Community. Blender - a 3D modelling and rendering package. Blender Foundation, Blender Institute, Amsterdam, 2022. URL http://www.blender.org.\n\nCai et al. [2021] Zhongang Cai, Mingyuan Zhang, Jiawei Ren, Chen Wei, Daxuan Ren, Zhengyu Lin, Haiyu Zhao, Lei Yang, Chen Change Loy, and Ziwei Liu. Playing for 3d human recovery. arXiv preprint arXiv:2110.07588, 2021.\n\nCao et al. [2017] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh. Realtime multi-person 2d pose estimation using part affinity fields. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7291–7299, 2017.\n\nCao et al. [2019] Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, and Yaser Sheikh. Openpose: realtime multi-person 2d pose estimation using part affinity fields. IEEE Trans. Pattern Anal. Mach. Intell., 43(1):172–186, 2019.\n\nChan et al. [2019] Caroline Chan, Shiry Ginosar, Tinghui Zhou, and Alexei A Efros. Everybody dance now. In Proceedings of the IEEE/CVF international conference on computer vision, pages 5933–5942, 2019.\n\nChang et al. [2023] Di Chang, Yichun Shi, Quankai Gao, Jessica Fu, Hongyi Xu, Guoxian Song, Qing Yan, Xiao Yang, and Mohammad Soleymani. Magicdance: Realistic human dance video generation with motions & facial expressions transfer. arXiv preprint arXiv:2311.12052, 2023.\n\nEsser et al. [2023] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. arXiv preprint arXiv:2302.03011, 2023.\n\nGmbH [2022] Meshcapade GmbH. Meshcapade. https://meshcapade.com, 2022.\n\nGüler et al. [2018] Rıza Alp Güler, Natalia Neverova, and Iasonas Kokkinos. Densepose: Dense human pose estimation in the wild. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages 7297–7306, 2018.\n\nGuo et al. [2023a] Yuwei Guo, Ceyuan Yang, Anyi Rao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Sparsectrl: Adding sparse controls to text-to-video diffusion models. arXiv preprint arXiv:2311.16933, 2023a.\n\nGuo et al. [2023b] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023b.\n\nHe et al. [2024] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv preprint arXiv:2404.02101, 2024.\n\nHeusel et al. [2017] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017.\n\nHo et al. [2020] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840–6851, 2020.\n\nHo et al. [2022a] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022a.\n\nHo et al. [2022b] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. arXiv preprint arXiv:2204.03458, 2022b.\n\nHu et al. [2021] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.\n\nHu et al. [2023] Li Hu, Xin Gao, Peng Zhang, Ke Sun, Bang Zhang, and Liefeng Bo. Animate anyone: Consistent and controllable image-to-video synthesis for character animation. arXiv preprint arXiv:2311.17117, 2023.\n\nJafarian and Park [2021] Yasamin Jafarian and Hyun Soo Park. Learning high fidelity depths of dressed humans by watching social media dance videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12753–12762, 2021.\n\nKarras et al. [2023] Johanna Karras, Aleksander Holynski, Ting-Chun Wang, and Ira Kemelmacher-Shlizerman. Dreampose: Fashion image-to-video synthesis via stable diffusion. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 22623–22633. IEEE, 2023.\n\nKhachatryan et al. [2023] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion models are zero-shot video generators. IEEE International Conference on Computer Vision (ICCV), 2023.\n\nKingma and Ba [2014] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv: Comp. Res. Repository, 2014.\n\nKirillov et al. [2023] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and Ross Girshick. Segment anything. arXiv:2304.02643, 2023.\n\nKondratyuk et al. [2023] Dan Kondratyuk, Lijun Yu, Xiuye Gu, José Lezama, Jonathan Huang, Rachel Hornung, Hartwig Adam, Hassan Akbari, Yair Alon, Vighnesh Birodkar, et al. Videopoet: A large language model for zero-shot video generation. arXiv preprint arXiv:2312.14125, 2023.\n\nLin et al. [2023] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 300–309, 2023.\n\nLin et al. [2024] Jing Lin, Ailing Zeng, Shunlin Lu, Yuanhao Cai, Ruimao Zhang, Haoqian Wang, and Lei Zhang. Motion-x: A large-scale 3d expressive whole-body human motion dataset. Advances in Neural Information Processing Systems, 36, 2024.\n\nLoper et al. [2023] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J Black. Smpl: A skinned multi-person linear model. In Seminal Graphics Papers: Pushing the Boundaries, Volume 2, pages 851–866. 2023.\n\nMahmood et al. [2019] Naureen Mahmood, Nima Ghorbani, Nikolaus F Troje, Gerard Pons-Moll, and Michael J Black. Amass: Archive of motion capture as surface shapes. In Proc. IEEE Int. Conf. Comp. Vis., pages 5442–5451, 2019.\n\nMarmon et al. [2024] Andrew Marmon, Grant Schindler, José Lezama, Dan Kondratyuk, Bryan Seybold, and Irfan Essa. Camvig: Camera aware image-to-video generation with multimodal transformers. arXiv preprint arXiv:2405.13195, 2024.\n\nNi et al. [2023] Haomiao Ni, Changhao Shi, Kai Li, Sharon X Huang, and Martin Renqiang Min. Conditional image-to-video generation with latent flow diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18444–18455, 2023.\n\nNichol et al. [2021] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.\n\nPatel et al. [2021] Priyanka Patel, Chun-Hao P. Huang, Joachim Tesch, David T. Hoffmann, Shashank Tripathi, and Michael J. Black. AGORA: Avatars in geography optimized for regression analysis. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., June 2021.\n\nPavlakos et al. [2019] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and Michael J Black. Expressive body capture: 3d hands, face, and body from a single image. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages 10975–10985, 2019.\n\nPodell et al. [2023] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023.\n\nPunnakkal et al. [2021] Abhinanda R Punnakkal, Arjun Chandrasekaran, Nikos Athanasiou, Alejandra Quiros-Ramirez, and Michael J Black. Babel: Bodies, action and behavior with english labels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 722–731, 2021.\n\nRamesh et al. [2022] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n\nRen et al. [2020] Yurui Ren, Ge Li, Shan Liu, and Thomas H Li. Deep spatial transformation for pose-guided person image generation and animation. IEEE Transactions on Image Processing, 29:8622–8635, 2020.\n\nRobinette et al. [2002] Kathleen M Robinette, Sherri Blackwell, Hein Daanen, Mark Boehmer, Scott Fleming, Tina Brill, David Hoeferlin, and Dennis Burnsides. Civilian american and european surface anthropometry resource (caesar), final report, volume i: Summary. Sytronics Inc Dayton Oh, page 3, 2002.\n\nRombach et al. [2022] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684–10695, 2022.\n\nRuan et al. [2023] Ludan Ruan, Yiyang Ma, Huan Yang, Huiguo He, Bei Liu, Jianlong Fu, Nicholas Jing Yuan, Qin Jin, and Baining Guo. Mm-diffusion: Learning multi-modal diffusion models for joint audio and video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10219–10228, 2023.\n\nSaharia et al. [2022] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:36479–36494, 2022.\n\nSiarohin et al. [2019a] Aliaksandr Siarohin, Stéphane Lathuilière, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. Animating arbitrary objects via deep motion transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2377–2386, 2019a.\n\nSiarohin et al. [2019b] Aliaksandr Siarohin, Stéphane Lathuilière, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First order motion model for image animation. Advances in neural information processing systems, 32, 2019b.\n\nSiarohin et al. [2021] Aliaksandr Siarohin, Oliver J Woodford, Jian Ren, Menglei Chai, and Sergey Tulyakov. Motion representations for articulated animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13653–13662, 2021.\n\nSimon et al. [2017] Tomas Simon, Hanbyul Joo, Iain Matthews, and Yaser Sheikh. Hand keypoint detection in single images using multiview bootstrapping. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 1145–1153, 2017.\n\nSinger et al. [2022] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022.\n\nSitzmann et al. [2021] Vincent Sitzmann, Semon Rezchikov, Bill Freeman, Josh Tenenbaum, and Fredo Durand. Light field networks: Neural scene representations with single-evaluation rendering. Advances in Neural Information Processing Systems, 34:19313–19325, 2021.\n\nSong et al. [2020] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020.\n\nTeed and Deng [2021] Zachary Teed and Jia Deng. DROID-SLAM: Deep Visual SLAM for Monocular, Stereo, and RGB-D Cameras. Advances in neural information processing systems, 2021.\n\nUnterthiner et al. [2018] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: A new metric & challenges. arXiv preprint arXiv:1812.01717, 2018.\n\nVarol et al. [2017] Gul Varol, Javier Romero, Xavier Martin, Naureen Mahmood, Michael J Black, Ivan Laptev, and Cordelia Schmid. Learning from synthetic humans. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 109–117, 2017.\n\nWang et al. [2023a] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion controllability. arXiv preprint arXiv:2306.02018, 2023a.\n\nWang et al. [2023b] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, Yuwei Guo, Tianxing Wu, Chenyang Si, Yuming Jiang, Cunjian Chen, Chen Change Loy, Bo Dai, Dahua Lin, Yu Qiao, and Ziwei Liu. Lavie: High-quality video generation with cascaded latent diffusion models, 2023b.\n\nWang et al. [2023c] Yaohui Wang, Xin Ma, Xinyuan Chen, Cunjian Chen, Antitza Dantcheva, Bo Dai, and Yu Qiao. Leo: Generative latent image animator for human video synthesis. 2023c.\n\nWang et al. [2024] Yufu Wang, Ziyun Wang, Lingjie Liu, and Kostas Daniilidis. Tram: Global trajectory and motion of 3d humans from in-the-wild videos. arXiv preprint arXiv:2403.17346, 2024.\n\nWang et al. [2004] Zhou Wang, Alan C. Bovik, Hamid R. Sheikh, and Eero P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Trans. Image Process., 13(4):600–612, 2004. doi: 10.1109/TIP.2003.819861. URL https://doi.org/10.1109/TIP.2003.819861.\n\nWang et al. [2023d] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: A unified and flexible motion controller for video generation. arXiv preprint arXiv:2312.03641, 2023d.\n\nWei et al. [2016] Shih-En Wei, Varun Ramakrishna, Takeo Kanade, and Yaser Sheikh. Convolutional pose machines. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 4724–4732, 2016.\n\nXu et al. [2024] Dejia Xu, Weili Nie, Chao Liu, Sifei Liu, Jan Kautz, Zhangyang Wang, and Arash Vahdat. Camco: Camera-controllable 3d-consistent image-to-video generation, 2024.\n\nXu et al. [2023] Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng Shou. Magicanimate: Temporally consistent human image animation using diffusion model. arXiv preprint arXiv:2311.16498, 2023.\n\nYang et al. [2023a] Zhendong Yang, Ailing Zeng, Chun Yuan, and Yu Li. Effective whole-body pose estimation with two-stages distillation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4210–4220, 2023a.\n\nYang et al. [2023b] Zhitao Yang, Zhongang Cai, Haiyi Mei, Shuai Liu, Zhaoxi Chen, Weiye Xiao, Yukun Wei, Zhongfei Qing, Chen Wei, Bo Dai, et al. Synbody: Synthetic dataset with layered human models for 3d human perception and modeling. arXiv preprint arXiv:2303.17368, 2023b.\n\nYang et al. [2023c] Zhitao Yang, Zhongang Cai, Haiyi Mei, Shuai Liu, Zhaoxi Chen, Weiye Xiao, Yukun Wei, Zhongfei Qing, Chen Wei, Bo Dai, et al. Synbody: Synthetic dataset with layered human models for 3d human perception and modeling. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 20282–20292, 2023c.\n\nYin et al. [2023] Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang Li, Gong Ming, and Nan Duan. Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory. arXiv preprint arXiv:2308.08089, 2023.\n\nYou et al. [2024] Meng You, Zhiyu Zhu, Hui Liu, and Junhui Hou. Nvs-solver: Video diffusion model as zero-shot novel view synthesizer. arXiv preprint arXiv:2405.15364, 2024.\n\nYu et al. [2023] Wing-Yin Yu, Lai-Man Po, Ray CC Cheung, Yuzhi Zhao, Yu Xue, and Kun Li. Bidirectionally deformable motion modulation for video-based human pose transfer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7502–7512, 2023.\n\nZablotskaia et al. [2019] Polina Zablotskaia, Aliaksandr Siarohin, Bo Zhao, and Leonid Sigal. Dwnet: Dense warp-based network for pose-guided human video generation. arXiv preprint arXiv:1910.09139, 2019.\n\nZhang and Agrawala [2023] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. arXiv preprint arXiv:2302.05543, 2023.\n\nZhang et al. [2023a] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3836–3847, 2023a.\n\nZhang et al. [2018] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In CVPR, pages 586–595. IEEE, 2018.\n\nZhang et al. [2023b] Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng Zhang, Wangmeng Zuo, and Qi Tian. Controlvideo: Training-free controllable text-to-video generation. arXiv preprint arXiv:2305.13077, 2023b.\n\nZhao and Zhang [2022] Jian Zhao and Hui Zhang. Thin-plate spline motion model for image animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3657–3666, 2022.\n\nZhao et al. [2023] Rui Zhao, Yuchao Gu, Jay Zhangjie Wu, David Junhao Zhang, Jiawei Liu, Weijia Wu, Jussi Keppo, and Mike Zheng Shou. Motiondirector: Motion customization of text-to-video diffusion models. arXiv preprint arXiv:2310.08465, 2023.\n\nZhou et al. [2022] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video generation with latent diffusion models. arXiv preprint arXiv:2211.11018, 2022.\n\nZhu et al. [2024] Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai, Yinghui Xu, Xun Cao, Yao Yao, Hao Zhu, and Siyu Zhu. Champ: Controllable and consistent human image animation with 3d parametric guidance. arXiv preprint arXiv:2403.14781, 2024."
    }
}
{
    "id": "correct_foundationPlace_00057_2",
    "rank": 69,
    "data": {
        "url": "https://theagileadmin.com/tag/amazon/",
        "read_more_link": "",
        "language": "en",
        "title": "the agile admin",
        "top_image": "https://s0.wp.com/i/blank.jpg",
        "meta_img": "https://s0.wp.com/i/blank.jpg",
        "images": [
            "https://theagileadmin.com/wp-content/uploads/2020/09/cropped-austin1-1024x435-2.jpg",
            "https://theagileadmin.com/wp-content/uploads/2014/11/ec2container.png?w=500&h=327",
            "https://theagileadmin.com/wp-content/uploads/2014/11/amazonlambda.png?w=500&h=322",
            "https://theagileadmin.com/wp-content/uploads/2014/08/cis_q413_graphic.jpg?w=500&h=280",
            "https://theagileadmin.com/wp-content/uploads/2014/03/awsaccountcreation.png?w=300&h=142",
            "https://theagileadmin.com/wp-content/uploads/2014/03/awssignupmfa.png?w=300&h=155",
            "https://theagileadmin.com/wp-content/uploads/2014/03/awsiamdash.png?w=300&h=141",
            "https://theagileadmin.com/wp-content/uploads/2014/03/awsiamgroupstep1.png?w=500&h=142",
            "https://theagileadmin.com/wp-content/uploads/2014/03/awsiamgroupstep2.png?w=500&h=252",
            "https://theagileadmin.com/wp-content/uploads/2014/03/awsiamgroupstep3.png?w=500&h=401",
            "https://theagileadmin.com/wp-content/uploads/2014/03/awsiamgroupstep4.png?w=500&h=384",
            "https://theagileadmin.com/wp-content/uploads/2014/03/awsiamgroupstep5.png?w=500&h=385",
            "https://theagileadmin.com/wp-content/uploads/2014/03/awscloudtrail.png?w=500&h=342",
            "https://theagileadmin.com/wp-content/uploads/2014/03/awslogexpire.png?w=500&h=239",
            "https://theagileadmin.com/wp-content/uploads/2014/03/sumostep1.png?w=500&h=211",
            "https://theagileadmin.com/wp-content/uploads/2014/03/sumostep2.png?w=500",
            "https://theagileadmin.com/wp-content/uploads/2014/03/awssecgroup1.png?w=500&h=155",
            "https://theagileadmin.com/wp-content/uploads/2014/03/awsfingerprintcheck.png?w=500&h=436",
            "https://farm6.staticflickr.com/5523/10956024265_941e160edf.jpg",
            "https://farm6.staticflickr.com/5501/10956202914_c4626a4eaf.jpg",
            "https://farm6.staticflickr.com/5505/10956190834_db4fc75d87.jpg",
            "https://i0.wp.com/farm4.staticflickr.com/3824/10956839326_0c5b666144.jpg",
            "https://i0.wp.com/farm6.staticflickr.com/5550/10956621803_dd23335254.jpg",
            "https://i0.wp.com/farm8.staticflickr.com/7372/10956389325_93baf9d6e7.jpg",
            "https://i0.wp.com/farm3.staticflickr.com/2822/10956615743_5029480fda.jpg",
            "https://i0.wp.com/farm3.staticflickr.com/2816/10956478276_0942e02b56.jpg",
            "https://i0.wp.com/farm8.staticflickr.com/7302/10956128976_4b36177d7f.jpg",
            "https://i0.wp.com/farm8.staticflickr.com/7377/10956375525_93f29b0768.jpg",
            "https://i0.wp.com/farm4.staticflickr.com/3775/10956410135_c6cc4e2549.jpg",
            "https://i0.wp.com/farm6.staticflickr.com/5516/10956501826_293822b7a6.jpg",
            "https://i0.wp.com/farm4.staticflickr.com/3753/10956494866_1bfa0ecb2e.jpg",
            "https://s2.wp.com/i/logo/wpcom-gray-white.png",
            "https://s2.wp.com/i/logo/wpcom-gray-white.png",
            "https://pixel.wp.com/b.gif?v=noscript"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Ernest Mueller"
        ],
        "publish_date": null,
        "summary": "",
        "meta_description": "Posts about amazon written by Ernest Mueller and karthequian",
        "meta_lang": "en",
        "meta_favicon": "https://s1.wp.com/i/favicon.ico",
        "meta_site_name": "the agile admin",
        "canonical_link": "https://theagileadmin.com/tag/amazon/",
        "text": "Boy, it’s been quite a week for the cloud-schadenfreude crowd. If you listen to the various news outlets, apparently Rackspace has given up on cloud and Amazon is in free-fall. Here’s some representative hack jobs pieces:\n\nAmazon’s Scorpion Problem\n\nAWS In Fight Of Its Life\n\nRackspace Bows Out Of IaaS Market\n\nMore accurate are these:\n\nAmazon Web Services Revenue Growth Is Way Down\n\nRackspace’s Pivot Is A Sign Of The Times For IaaS Providers\n\nLet’s look at what’s actually going on.\n\nFirst, Rackspace. I was on the Spiceworks forum yesterday and the news is definitely being interpreted as “Rackspace is getting out of cloud, don’t consider them any more.” Now, it is their own fault for bungling the messaging here, but if you actually go look at what they are doing, at its heart they are making this change:\n\nRackspace Cloud will be sold only with a support contract now.\n\nYes, that’s it. That’s the change. Now it’s “managed cloud!” Which is fine, a heck a lot of software I buy has mandatory maintenance contracts nowadays, but this doesn’t mean “Rackspace is leaving the cloud business!” They just want to add in their “Fanatical Support ™” to the value proposition and not compete purely on a bare-metal (bare-API?) SaaS “how much does a 2-CPU 4 GB server cost” basis.\n\nRackspace has to get back out in front of this messaging hard – it’s definitely made its way to the practitioner trenches as “they’re pulling out.” I mean, I have to say Rackspace’s strategy is pretty opaque to most folks, but this message misstep has graduated from “muddled and unclear” to “actively harmful.”\n\nNow, Amazon. The real story is:\n\nAmazon Web Services only grew 38.39% last quarter.\n\nFor a large company that’s a pretty good growth rate, right, is yours higher? The press likes to turn IaaS into a 3 provider horse race. But so far – it’s not. Check out this recent (March 2014) Synergy Research graph.\n\nThe fact of the matter is that Amazon is beating the holy hell out of everyone else in IaaS. It’s more neck and neck in PaaS, but sadly the entire PaaS market is still low (due to Joe Average IT Shop basically interpreting PaaS pitches as someone standing up and screaming “I’m a sorcerer!!!”).\n\nIBM, HP, etc. don’t have credible offerings yet. I know they’re investing, I know they have roped some random companies that love them into doing it, but they are just not there yet. IBM is not a commodity company, they’re a “you have a billion dollar contract with us we’re going to build out whatever we feel like with that.”\n\nGoogle, same thing. It’s cool, it’s well priced, it’s dev friendly – but at the big price cut announcement, we had a big get-together at Capital Factory here in town. I looked around at the crowd of 40 clouderati types and said “OK, so who is comfortable running production apps on Google cloud yet?” Result: zero. Google’s throwing money at it but as with most of Google’s new offerings, it’s hard to trust it’s not just going to dry up tomorrow and get cancelled because they are running after private spaceships or whatever now, and nothing makes them money like their ad business so “it’s revenue generating” won’t save it. And Google is so bad at enterprise support…\n\nMicrosoft Azure was really good. Better than it had a right to be! I was very impressed with Azure in years 1 and 2. Execution was good (we used it for a SaaS service at National Instruments) and the vision was definitely “where the puck is going to be.” But post-Ozzie, it hasn’t exactly been shaking the sheets. At CloudAustin there was more Azure interest two years ago than there is now. They were going strong on dev friendliness and all, but trying to get into IaaS has been a distraction and they just aren’t keeping pace with Amazon’s rate of new features. Docker support, SSDs, new instances, vCenter integration, Dropbox competitor, desktop-as-a-service Citrix competitor…\n\nLet me address the four big “why AWS is crashing and burning (despite being in an obvious position of market dominance)” points from the “Scorpion” article.\n\nAWS is not the low price provider.\n\nEh. Not sure why this is relevant and also not sure it’s true for what you are getting… It’s like saying “there are books cheaper than that book you just bought.” Well sure there are, but do they have the information I want in them? See below for why not always having the lowest cent per minute under Google and Microsoft doesn’t really concern me.\n\nAWS is not the best product at anything – most of their features are mediocre knock offs of other products.\n\nThis misses the point – their features are SIMPLER knockoffs of other products. That’s why it’s an accelerator. Dropbox and Salesforce and all the successful cloud entities have said “you know, some enterprise user left to their own devices is going to generate a list of 1000 requirements they don’t really need. Forget that. Let’s make the actual core functionality they need and leave off the rest so it’ll actually get used.” This is why they dominate the IaaS business. Many of their products are named to match. “SIMPLE email service.” “SIMPLE queue service.” “SIMPLE notification service.” This drives a new wave of architectural thought – instead of complicated services packed with stuff, what if instead I integrate simple, well-designed microservices? After doing a lot of cloud architecture work, those attributes are positives, not negatives.\n\nAWS is unbelievably lousy at support.\n\nI’m not sure I’d want to be in a race with Amazon, Microsoft, and Google to see who supports customers worse. I’m not sure I’ve ever been part of an enterprise happy with its Google support, and all experiences I’ve had with Microsoft support have been some Brazil-esque “you can’t actually ask them questions, only some VP is a designated contact on the corporate contract…”. Amazon is positioning themselves more like a hardware vendor, you don’t bother getting much support from them besides parts replacement, you get support from the managed hosting provider or whatnot that’s a MSP on top of them if you need it.\n\nOnce you are at $200k / month of spend, it’s cheaper and much more effective to build your own infrastructure\n\nThis is frequently untrue and based on people not understanding the full costs of getting stuck in the infrastructure business. What’s your cost of delay? Average enterprise “wait for servers” time is about 6 weeks; assuming you’re not just using them for nothing, your ROI is delayed by that amount. And what about all the operation of those complex systems? You can’t just stick in the salary of the developers and sysadmins you’d need – stick in your revenue per employee instead, because that headcount could be doing something useful for your company instead of plumbing. Not to mention the cascading percentage of each layer of management’s time spent worrying ab out the plumbing and the plumbers instead of conducting the core business of the company. Cost of delay from lost agility and opportunity cost are never taken into account but definitely should be.\n\nI know a lot of the old guard want cloud to dry up and go away, it bothers their lovely datacenters. And some of the very new guard resent it because Amazon continues to be so successful – they keep up a rate of innovation that new players can’t disrupt. But this whole week of “the cloud is falling” news is complete BS, and won’t amount to much.\n\nSo you’ve decided to start playing around with Amazon Web Services and are worried about doing so securely. Here’s the basics to do when you set up to ensure you’re on sound footing. In fact, I’m going to use the free tier of all these items for this walkthrough, so feel free and do it yourself if you’ve never taken the plunge into AWS!\n\nAccount Setup\n\nSigning up for Amazon Web Services is as simple as going to aws.amazon.com and clicking the “Sign Up” button.\n\nIt will want a password – choose a strong one, obviously – and some credit card info for if you exceed the free tier. It’ll want a phone number for a robot to call you – they show you a PIN, the robot calls you, you give the robot the PIN, and you’re good to go.\n\nMultifactor Authentication Setup For Your Account\n\nNext, set up multifactor authentication (MFA) for your Amazon account. You should see an option like this to go directly there immediately post signup, or you can pick the “IAM” section out of the main Amazon console.\n\nWhen you go to the IAM console you’ll see two options under Security Status to turn on – Root Account MFA and Password Policy. I won’t talk much about the password policy except to say “go turn it on and check all the boxes to ensure strong passwords.” To turn on MFA, you need some kind of MFA device. The Amazon docs to walk you through the process are here. Unless you have a Gemalto hardware token already your best best is to just download Google Authenticator (GA) onto your iPhone/Android from the relevant app store (other choices here).\n\nOnce you’ve installed Google Authenticator, click on “Manage MFA Device” and choose virtual; it’ll show you a QR code that GA can scan to do the setup. Then you enter two tokens in a row from GA and it’s hooked up to the account. Now, to log into the console you need both your password and a MFA token. (You can also use GA for Dropbox, Evernote, Gmail, WordPress, etc. and is a good safeguard against the inevitable password losses these companies sometimes have.) Of course once you do this you need to be careful – if you lose the phone or device you can’t get in!\n\nNow make sure and save all your credentials in a password vault. I prefer Keepass Password Safe along with MiniKeePass on the iPhone. Besides the password, you should go to your Security Credentials (off a popdown form your name in the top right) and store your AWS Account ID, Canonical User ID (used for S3), and any access keys or X.509 certificates you make – but it’s better not to make these for the main account, just for IAM accounts. Proceed onward to hear more about that!\n\nIdentity and Access Management Setup\n\nAll right – now your main login is secure. But you want to take another step past this, which is setting up an Identity and Access Management (IAM) account. IAM used to be a lot less robustly supported in AWS but they’ve gotten it to be pretty pervasive across all their services now. Here’s the grid of what services support IAM and how. You can think of this as the cloud analogy to UNIX’s “secure your root account, but still you shouldn’t log in as it but as a more restricted user.”\n\nFirst, you have to set up a group. On the IAM dashboard click on the big ol’ “Create A New Group Of Users” button in the yellow box at the top.\n\nThen make a group, call it “Admins” for the sake of argument.\n\nChoose the “Administrator Access” policy template. If you know what you’re doing, you can change this up extensively.\n\nAdd a username for yourself (and whatever other people or entities you want to have full admin access, ideally not a long list) to the group.\n\nFor each user it’ll give an Access Key ID and Secret Access Key – these are the private credentials for that user, they should take them and put them in whatever password vault they use.\n\nIf you want to use that account to log into the console – and for this first one, you probably do – then once this is complete you go into Users, select the user, and under Sign-In Credentials it will say Password: No. Click Manage Password and set a password for that user; they can then log into the custom IAM login URL shown on the front page of the IAM dashboard (it’ll put it in the file when you Download Credentials, too).\n\nMFA Setup for IAM\n\nJust as with the main user, you can (and should) also set up MFA on IAM users. It’s the same process as with the main account so I won’t belabor it.\n\nAfter you set up your IAM user and its MFA, you shouldn’t log in using your main AWS account credentials to do work – only if you need the enhanced access to mess with account credentials. Log out and then back in with your IAM account to proceed. If you want to take it a step farther and make an even less privileged account without admin rights, which you can use for everyday tasks like logging in and looking at state or just starting/stopping instances but not manipulating more sensitive functions, you can do that too.\n\nMore IAM\n\nOf course if you are looking to manage multiple people, or separate apps that have access to your account (many SaaS solutions that integrate with your Amazon account will ask for IAM credentials with specific access), you can set up more groups with less access and have those entities use those. In general I’d suggest using a group+user and not just a user (different SaaS monitoring services recommend different approaches, but I think a plain user is less flexible). You can also get fancy with roles (used for app access from your instances) and identity providers. Remember the principle of least privilege – give things only as much access as they need, so that if those credentials are compromised there’s a limit to what they can do. There’s an AWS IAM best practices guide with more tips.\n\nTurn On Accounting\n\nSecurity folks know that nothing’s complete without the ability to audit it. Well, you can turn on logging of AWS security events using CloudTrail, the AWS logging service. This will basically dump IAM (and other Amazon API events) to a JSON file in S3. This is a whole can of whupass unto itself, but the short form is to follow this guide to set up your trail, making sure to say Yes to “Include global services?”.\n\nYou can also go into S3 and set your bucket (properties.. lifecycle) to expire (archive, delete, etc.) the logs after a certain time.\n\nThen you can do something like set up SumoLogic to watch it and review/alert on your logs. If you want to try that, the short HOWTO is:\n\nSign up for Sumo free trial (need a non-gmail email account)\n\nAdd a sumo IAM group with permissions to get to your CloudTrail S3 bucket (there’s suggested JSON with the exact settings in the Sumo help) and a user in that group\n\nAdd a hosted collector\n\nAdd a S3 source to that collector, point it at your bucket, give it the sumo user’s AWS creds\n\nYour data’s going to come in in big clumps of JSON though, which you can parse with some pain. Hint, your searches chould look like:\n\n* | json “userIdentity.userName”, “eventTime”, “eventName” as username, time, action | sort by time\n\nThey also have an app specific to CloudTrail; you have to contact Sumo support to get it turned on though.\n\nNetwork Setup\n\nAll of this, of course, is about access at the Amazon account and API level. For your actual instances, you’ll want to set up secure network access and then manage the SSH keys you use to log into them.\n\nVPCs used to be a limited option, and mostly people just used security groups. Nowadays, VPCs are standard and an expected part of your setup. They’re like a private virtual network. When you create your account, it’ll actually create one default one for you automatically. You can see it by going to the VPC Console. This default VPC, though, is set up for convenience and back compatibility – instances you launch into it will get a public IP address, which may not be what you want, and the default security group allows all outbound traffic and all traffic from within the security group.\n\nYou should consider starting one instance this way and then using it as a bastion host to gateway into your other instances, which shouldn’t have public IPs unless you really want them to be publicly facing. It’s hard to prescribe other specifics here because it really depends on what you plan to do. At a bare minimum you need to add an inbound SSH rule from your location to the security group so you can log into your first instance when you start it below. (They have a neat new “My IP” choice that’ll detect where you’re coming from. Of course that won’t work when you drive to the Starbucks…) Consider removing the rule allowing all traffic from within a security group – even within a group it’s more secure to allow specific protocols instead of “everything from everywhere.”\n\nIdeally, you’d set up a VPN to the VPC’s Internet Gateway – but this requires expensive hardware or setting up your own server and is way out of scope here.\n\nSystem Setup\n\nThen, of course, you finally get to starting instances! Each instance will start with a default root ssh key. Things you want to do here are:\n\nYou will want to use personal SSH keys to log in. Generate a public-private pair (using putty-keygen or whatever’s best for your client). This doc tells you how to upload the public key to AWS. This will start any instance with that public key as root (or ec2-user or other non-root username depending on the distro you’re using), so this is a pretty sensitive root credential. You can add more users and distribute more keys to the instances later either via your favorite CM tool, by using AWS OpsWorks which is based on Chef, or however else.\n\nStart an instance in the EC2 section of the console into your default VPC/sec group/etc. using your uploaded public key. I’m not going to do a detailed HOWTO on this because it’s pretty well-trod ground. If you don’t have an opinion, start with Amazon Linux.\n\nLog in using your private key. Check the SSH fingerprint the first time you log in; it’ll be in the console output of the instance which you can see through the console (Actions… Get System Log) or an ec2-get-console-output command line.\n\nPatch the instance. The AMI you’re using may be more or less super old and a “sudo yum update” or similar is a really good idea.\n\nTurn off passworded login if it’s not already, and the ability to directly log in as root if it’s not already.\n\nIf you’d like this to be your bastion host, then add other security groups for other instances to go to – don’t allow inbound SSH to them from anywhere, just from this security group.\n\nAutomate\n\nThe final step to doing all this securely is to not be making manual changes. Via the CLI or API you can automate a lot of this, but even better is using CloudFormation, maybe in conjunction with OpsWorks or another CM tool, to define in a readable config how you want your system to behave (VPCs, security groups, etc.) and instantiate off that. Nothing’s more auditable than a system that’s built automatically from a spec! You can cheat a little and set up your VPCs and all the way you want and use their CloudFormer tool to generate a CloudFormation template from your running system. Then you can edit that and tear down/restart from scratch.\n\nThe more you automate, the tighter you can make the security controls without inconveniencing yourself. A trivial example is you could have a script that uses the CLI to change the security group to allow SSH from wherever you are right now, and then close it afterwards – so there’s no SSH access from a location unless you allow it! In the same vein, allowing “all access” within a security group or from one group to another is usually done out of laziness and flexibility for manual changes – if you automate such that if you add a new set of servers, they also configure their connectivity needs specifically, you’re more secure. For defense in depth you could automatically configure the onboard firewalls on the boxes to mimic the security groups, just read the security group settings and transform into similar iptables (or whatever) settings. Voila, a HIPS. Pump those logs into Sumo too.\n\nYou could add tripwire or OSSEC for change detection, but also if you run your servers from trusted images and recreate them frequently, you can very much reduce the risk of compromise.\n\nThat’s my quick HOWTO on how to get servers running in a mode that’s likely way more secure than the average enterprise server unless you work for a bank or something, inside a couple hours. MFA, key based auth, all the network separation you could want, separation of privileges…\n\nAfter John Allspaw and Steve Souders caper about in fake muscles, we get started with the keynotes.\n\nBuilding for a Billion Users (Facebook)\n\nJay Parikh, Facebook, spoke about building for a billion users. Several folks yesterday warned with phrases like “if you have a billion users and hundreds of engineers then this advice may be on point…”\n\nToday in 30 minutes, Facebook did…\n\n10 TB log data into hadoop\n\nScan 105 TB in Hive\n\n6M photos\n\n160m newsfeed stories\n\n5bn realtime messages\n\n10bn profile pics\n\n108bn mysql queries\n\n3.8 tn cache ops\n\nPrinciple 1: Focus on Impact\n\nDay one code deploys. They open sourced fabricator for code review. 30 day coder boot camp – Ops does coder boot camp, then weeks of ops boot camp. Mentorship program.\n\nPrinciple 2: Move Fast\n\nCommits scale with people, but you have to be safe! Perflab does a performance test before every commit with log-replay. Also does checks for slow drift over time.\n\nGatekeeper is the feature flag tool, A/B testing – 500M checks/sec. We have one of these at Bazaarvoice and it’s super helpful.\n\nClaspin is a high density heat map viewer for large services\n\nfast deployment technique – used shared memory segment to connect cache, so they can swap out binaries on top of it, took weeks of back end deploys down to days/hours\n\nBuilt lots of ops tools – people | tools | process\n\nRandom Bits:\n\nBe bold\n\nThey use BGP in the data center\n\nDid we mention how cool we are?\n\nCapacity engineering isn’t just buying stuff, it’s APM to reduce usage\n\nMassive fail from gatekeeper bug. Had to pull dns to shut the site down\n\nFix more, whine less\n\nInvestigating Anomalies, Amazon\n\nJohn Rauser, Amazon data scientist, on investigating anomalies. He gave a well received talk on statistics for operations last year.\n\nHe used a very long “data in the time of cholera” example. Watch the video for the long version.\n\nYou can’t just look at the summary, look at distributions, then look into the logs themselves.\n\nLook at the extremes and you’ll find things that are broken.\n\nCheck your long tail, monitor percentiles long in the tail.\n\nBuilding Resilient User Experiences\n\nMike Brittain, Etsy on Building Resilient User Experiences – here’s the video.\n\nDon’t mess up a page just because one of the 14 lame back end services you compose into it is down. Distinguish critical back end service failures and just suppress other stuff when composing a product page. Consider blocking vs non-blocking ajax. Load critical stuff synchronously and noncritical async/not at all if it times out.\n\nGoogle apps has the nice problem/retrying in an interval messages (use exponential back off in your ui!)\n\nPlanning for 100% availability is not enough; plan for failure. Your UI should adapt to failure. That’s usually a joint dev+ops+product call.\n\nDo operability reviews, postmortems. Watch your page views for your error template! (use an error template)\n\nSpeeding up the web using prediction of user activity\n\nArvind Jain/Dominic Hamon from Google – see the video.\n\nWhy things would be so much faster if you just preload the next pages a user might go to. Sure. As long as you don’t mind accidentally triggering all kinds of shit. It’s like cross browser request forgery as a feature!\n\n<link rel=’prerender’> to provide guidance.\n\nAnnoyed by this talk, and since I’ve read How Complex Systems Fail already, I spent rest of the morning plenary time wandering the vendor floor\n\nIt’s All About Telemetry\n\nFrom the famous and irascible Theo Schlossnagle (@postwait). Here’s the slides.\n\nMonitor what matters! Most new big data problems are created by our own solutions in the first place, and are thus solvable despite their ROI. e.g. logs.\n\nWhat’s the cost/benefit of your data?\n\nDon’t erode granularity (as with RRD). It controls your storage but your ability to, say, do YOY black friday compares sucks.\n\nAs you zoom in you see obscured major differences and patterns.\n\nThere’s a cost/benefit curve for monitoring that goes positive but eventually goes negative.Value=benefit-cost. So you don’t go to the end of the curve, you want the max difference!\n\nTechnique 1: Text\n\nJust store changes- be careful not to have too many changes and store them\n\nTechnique 2: Numeric\n\nstore rollups, over 1 minute min/max/avg/sttdev/covar/50/95/99%\n\nstore first order derivative and then derivative of that (jerkiness)\n\ndb replication – lag AND RATE OF LAG CHANGE\n\n“It’s a lot easier to see change when you’re actually graphing change”\n\nProject numbers out. Graph storage space doing down!\n\nWith simple numeric data you can do prediction (Holt-Winters) even hacked into RRD\n\nTechnique 3: Histograms\n\nabout 2k for a 1 minute histogram (5b for a single bucket)\n\nEvent correlation – change mgmt vs performance, is still human eye driven\n\nMonitor everything.\n\nthe business – financials, marketing, support! problems, custsat, res time\n\noperations! durr\n\nsystem/db/yeah\n\nmiddleware! messaging, apis, etc.\n\nThey use reconnoiter, statsd, d3, flot for graphing.\n\nThis was one of the best sessions of the day IMO.\n\nRUM For Breakfast\n\nCarlos from Facebook on routing users to the closest datacenter with Doppler. Normal DNS geo-routing depends on resolvers being near the user, etc.\n\nThey inject js into some browsers and log packet latency. Map resolver IP to user IP, datacenter, latency. Then you can cluster users to nearest data centers. They use for planning and analysis – what countries have poor latency, where do we need peering agreements\n\nAkamai said doing it was impractical, so they did it.\n\nAmazon has “latency based routing” but no one knows how it works.\n\nGoogle as part of their standard SOP has proposed a DNS extension that will never be adopted by enough people to work.\n\nLooking at log-normal perf data – look at the whole spread but that can be huge, so filter then analyze.\n\nMargin of error – 1.96*sd/sqrt(num), need less than 5% error.\n\nHow does performance influence human behavior?\n\nVery fast sessions had high bounce rates (probably errors)\n\nStrong bounce rate to load time correlation, and esp front end speed\n\nToxicology has the idea of a “median lethal dose”- our LD50 is where we pass 50% bounce rate\n\nThese are: Back end 1.7s, dom load 1.8s, dom interactive 2.75s, front end 3.5s, dom complete 4.75s, load event 5.5s\n\nRollback, the Impossible Dream\n\nby James Turnbull! Here’s the slides.\n\nRollback is BS. You are insane if you rely on it. It’s theoretically possible, if you apply sufficient capital, and all apps are idempotent, resources…\n\nSolve for availability, not rollback. Do small iterative changes instead.\n\nAccept that failure happens, and prevent it from happening again.\n\nNobody gives a crap whose fault it was.\n\nAssumption is the mother of all fuckups.\n\nThis can’t be upgraded like that because… challenge it.\n\nStability Patterns\n\nby Michael Nygard (@mtnygard) – Slides are here. For more see his pragmatic programmers book Release It!\n\nFailures come in patterns. Here’s some major ones!\n\nIntegrations are the #1 risk to stability, from both “in spec” and “out of spec” errors. Integrations are a necessary evil. To debug you have to peel back the layers of abstraction; you have to diagnose a couple levels lower than the level an error manifests. Larger systems fail faster than small ones.\n\nChain Reactions – #2!\n\nFailure moves horizontally across tiers, search engines and app servers get overloaded, esp. with connection pools. Look for resource leaks.\n\nCascading Failure\n\nFailure moves vertically cross tiers. Common in SOA and enterprise services. Contain the damage – decouple higher tiers with timeouts, circuit breakers.\n\nBlocked Threads\n\nAll threads blocked = “crash”. Use java.util/concurrent or system.threading (or ruby/php, don’t do it). Hung request handlers = less capacity and frustrated users. Scrutinize resource pools. Decompile that lame ass third party code to see how it works, if you’re using it you’re responsible for it.\n\nAttacks of Self-Denial\n\nMaking your own DoS attack, often via mass unexpected promotions. Open lines of communication with marketers\n\nUnbalanced Capacities\n\nYour environment scaling ratios are different dev to qa to prod. Simulate back end failures or overloading during testing!\n\nUnbounded result sets\n\nDev/test have smaller data volumes and unreal relationships (what’s the max in prod?). SOA with chatty providers – client can’t trust not being hurt. Don’t trust data producers, put limits in your APIs.\n\nStability patterns!\n\nCircuit Breaker\n\nRemote call wrapped with a retry loop (always 3!)\n\nImmediate retries are very likely to fail again – TCP fixes packet drops for you man\n\nMakes the user wait longer for their error, and floods the servers (cascading failure)\n\nCount failures (leaky bucket) and stop calling the back end for a cool-off period\n\nCan critical work be queued for later, or rejected, or what?\n\nState of circuit breakers is a good dashboard!\n\nBulkheads\n\nPartition the system and allow partial failure without losing service.\n\nClasses of customer, etc.\n\nIf foo and bar are coupled with baz, then hitting baz can bork both. Make baz pools or whatever.\n\nLess efficient resource use but important with shared-service models\n\nTest Harness\n\nReal-world failures are hard to create in QA\n\nIntegration tests don’t find those out-of-spec errors unless you force them\n\nService acting like a back end but doing crazy shit – slow, endless, send a binary\n\nSupplement testing methods\n\nThe Colo or the Cloud?\n\nKen King/James Sheridan, Yammer\n\nThey started in a colo.\n\nDual network, racks have 4 2U quad and 10 1Us\n\nEC2 provides you: network, cabling, simple load balancing, geo distribution, hardware repair, network and rack diagrams (?)\n\nThere are fixed and variable costs, assume 3 year depreciation\n\nAWS gives you 20% off for 2M/year?\n\nThree year commit, reserve instances\n\none rack one year – $384k cool, $378k ec2, $199k reserved\n\n20 racks one year – $105k colo, $158 ec2 even with reserve and 20% discount\n\n20 racks 3 years – $50k colo, $105k ec2\n\nBut – speed (agility) of ramping up…\n\nTo me this is like cars. You build your own car, you know it. But for arbitrary small numbers of cars, you don’t have time for that crap. If you run a taxi fleet or something, then you start moving back towards specialized needs and spec/build your own.\n\nColo benefits – ownership and knowledge. You know it and control it.\n\nLoad balancing (loadbalancer.com or zeus/riverbed only cloud options)\n\nIDS etc. (appliances)\n\nChoose connectivity\n\nknow your IO, get more RAM, more cores\n\nThrow money at vertical scalability\n\nPerception of control = security\n\nCloud benefits – instant. Scaling.\n\nForces good architecture\n\nImmediate replacement, no sparing etc.\n\nUnlimited storage, snapshots (1 TB volume limit)\n\nNo long term commitment\n\nProvisioning APIs, autoscaling, EU storage, geodist\n\nHybrid.\n\ncrocodoc and encoder yammer partners are good to be close to\n\nneed to burst work\n\ndev servers, windows dev, demo servers banished to AWS\n\nmoving cross connects to ec2 with vpc\n\nWhew! Man I hope someone’s reading this because it’s a lot of work. Next, day three and bonus LSPE meeting!\n\nAs I write this, our ops team is working furiously to bring up systems outside Amazon’s US East region to recover from the widespread outage they are having this morning. Naturally the Twitterverse, and in a day the blogosphere, and in a week the trade rag…axy? will be talking about how the cloud is unreliable and you can’t count on it for high availability.\n\nAnd of course this is nonsense. These outages make the news because they hit everyone at once, but all the outages people are continually having at their own data centers are just as impactful, if less hit-generating in terms of news stories.\n\nSure, we’re not happy that some of our systems are down right now. But…\n\nThe outage is only affecting one of our beta products, our production SaaS service is still running like a champ, it just can’t scale up right now.\n\nOur cloud product uptime is still way higher than our self hosted uptime. We have network outages, Web system outages, etc. all the time even though we have three data centers and millions of dollars in network gear and redundant Internet links and redundant servers.\n\nPeople always assume they’d have zero downtime if they were hosting it. Or maybe that they could “make it get fixed” when it does go down. But that’s a false sense of security based on an archaic misconception that having things on premise gives you any more control over them.We run loads of on premise Web applications and a batch of cloud ones, and once we put some Keynote/Gomez/Alertsite against them we determined our cloud systems have much higher uptime.\n\nNow, there are things that Amazon could do to make all this better on customers. In the Amazon SLAs, they say of course you can have super high uptime – if you are running redundantly across AZs and, in this case, regions. But Amazon makes it really unattractive and difficult to do this.\n\nWhat Amazon Can Do Better\n\nWe can work around this issue by bringing up instances in other regions. Sadly, we didn’t already have our AMIs transferred into those regions, and you can only bring up instances off AMIs that are already in those regions. And transferring regions is a pain in the ass. There is absolutely zero reason Amazon doesn’t provide an API call to copy an AMI from region 1 to region 2. Bad on them. I emailed my Amazon account rep and just got back the top Google hits for “Amazon AMI region migrate”. Thanks, I did that already.\n\nWe weren’t already running across multiple regions and AZs because of cost. Some of that is the cost of redundancy in and of itself, but more importantly is the hateful way Amazon does reserve pricing, which very much pushes you towards putting everything in one AZ.\n\nAlso, redundancy only really works if you have everything, including data, in that AZ. If you are running redundant app servers across 4 AZs, but have your database in one of them – 0r have a database master in one and slaves in the others – you still get hosed by a particular region downtime.\n\nAmazon needs to have tools that inherently let you distribute your stuff across their systems and needs to make their pricing/reserve strategy friendlier to doing things in what they say is “the right way.”\n\nWhat We Could Do Better\n\nWe weren’t completely prepared for this. Once that region was already borked, it was impossible to migrate AMIs out of it, and there are so many ticky little region specific things all through the Amazon config – security groups, ELBs, etc – doing that on the fly is not possible unless you have specifically done it before, and we hadn’t.\n\nWe have an automation solution (PIE) that will regen our entire cloud for us in a short amount of time, but it doesn’t handle the base images, some of which we modify and re-burn from the Amazon ones. We don’t have that process automated and the documentation was out of date since Fedora likes to move their crap around all the time.\n\nIn the end, Amazon started coming back just as we got new images done in us-west-1. We’ll certainly work on automating that process, and hope that Amazon will also step up to making it easier for their customers to do so."
    }
}
{
    "id": "dbpedia_118_1",
    "rank": 11,
    "data": {
        "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5037948/",
        "read_more_link": "",
        "language": "en",
        "title": "Basic statistical tools in research and data analysis",
        "top_image": "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/pmc-card-share.jpg?_=0",
        "meta_img": "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/pmc-card-share.jpg?_=0",
        "images": [
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/favicons/favicon-57.png",
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/icon-dot-gov.svg",
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/icon-https.svg",
            "https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/logos/AgencyLogo.svg",
            "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/logo-ijanaesth.gif",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5037948/bin/IJA-60-662-g001.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5037948/bin/IJA-60-662-g002.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5037948/bin/IJA-60-662-g003.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5037948/bin/IJA-60-662-g004.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5037948/bin/IJA-60-662-g005.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5037948/bin/IJA-60-662-g006.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5037948/bin/IJA-60-662-g007.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5037948/bin/IJA-60-662-g008.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5037948/bin/IJA-60-662-g009.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5037948/bin/IJA-60-662-g010.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5037948/bin/IJA-60-662-g011.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5037948/bin/IJA-60-662-g012.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5037948/bin/IJA-60-662-g013.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5037948/bin/IJA-60-662-g014.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5037948/bin/IJA-60-662-g015.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5037948/bin/IJA-60-662-g016.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5037948/bin/IJA-60-662-g017.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5037948/bin/IJA-60-662-g018.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Zulfiqar Ali",
            "S Bala Bhaskar"
        ],
        "publish_date": "2016-09-09T00:00:00",
        "summary": "",
        "meta_description": "Statistical methods involved in carrying out a study include planning, designing, collecting data, analysing, drawing meaningful interpretation and reporting of the research findings. The statistical analysis gives meaning to the meaningless numbers, ...",
        "meta_lang": "en",
        "meta_favicon": "https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon.ico",
        "meta_site_name": "PubMed Central (PMC)",
        "canonical_link": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5037948/",
        "text": "Measures of central tendency\n\nThe measures of central tendency are mean, median and mode.[6] Mean (or the arithmetic average) is the sum of all the scores divided by the number of scores. Mean may be influenced profoundly by the extreme variables. For example, the average stay of organophosphorus poisoning patients in ICU may be influenced by a single patient who stays in ICU for around 5 months because of septicaemia. The extreme values are called outliers. The formula for the mean is\n\nMean,\n\nwhere x = each observation and n = number of observations. Median[6] is defined as the middle of a distribution in a ranked data (with half of the variables in the sample above and half below the median value) while mode is the most frequently occurring variable in a distribution. Range defines the spread, or variability, of a sample.[7] It is described by the minimum and maximum values of the variables. If we rank the data and after ranking, group the observations into percentiles, we can get better information of the pattern of spread of the variables. In percentiles, we rank the observations into 100 equal parts. We can then describe 25%, 50%, 75% or any other percentile amount. The median is the 50th percentile. The interquartile range will be the observations in the middle 50% of the observations about the median (25th -75th percentile). Variance[7] is a measure of how spread out is the distribution. It gives an indication of how close an individual observation clusters about the mean value. The variance of a population is defined by the following formula:\n\nwhere σ2 is the population variance, X is the population mean, Xi is the ith element from the population and N is the number of elements in the population. The variance of a sample is defined by slightly different formula:\n\nwhere s2 is the sample variance, x is the sample mean, xi is the ith element from the sample and n is the number of elements in the sample. The formula for the variance of a population has the value ‘n’ as the denominator. The expression ‘n−1’ is known as the degrees of freedom and is one less than the number of parameters. Each observation is free to vary, except the last one which must be a defined value. The variance is measured in squared units. To make the interpretation of the data simple and to retain the basic unit of observation, the square root of variance is used. The square root of the variance is the standard deviation (SD).[8] The SD of a population is defined by the following formula:\n\nwhere σ is the population SD, X is the population mean, Xi is the ith element from the population and N is the number of elements in the population. The SD of a sample is defined by slightly different formula:\n\nwhere s is the sample SD, x is the sample mean, xi is the ith element from the sample and n is the number of elements in the sample. An example for calculation of variation and SD is illustrated in .\n\nParametric tests\n\nThe parametric tests assume that the data are on a quantitative (numerical) scale, with a normal distribution of the underlying population. The samples have the same variance (homogeneity of variances). The samples are randomly drawn from the population, and the observations within a group are independent of each other. The commonly used parametric tests are the Student's t-test, analysis of variance (ANOVA) and repeated measures ANOVA.\n\nStudent's t-test\n\nStudent's t-test is used to test the null hypothesis that there is no difference between the means of the two groups. It is used in three circumstances:\n\nTo test if a sample mean (as an estimate of a population mean) differs significantly from a given population mean (this is a one-sample t-test)\n\nThe formula for one sample t-test is\n\nwhere X = sample mean, u = population mean and SE = standard error of mean\n\nTo test if the population means estimated by two independent samples differ significantly (the unpaired t-test). The formula for unpaired t-test is:\n\nwhere X1 − X2 is the difference between the means of the two groups and SE denotes the standard error of the difference.\n\nTo test if the population means estimated by two dependent samples differ significantly (the paired t-test). A usual setting for paired t-test is when measurements are made on the same subjects before and after a treatment.\n\nThe formula for paired t-test is:\n\nwhere d is the mean difference and SE denotes the standard error of this difference.\n\nThe group variances can be compared using the F-test. The F-test is the ratio of variances (var l/var 2). If F differs significantly from 1.0, then it is concluded that the group variances differ significantly.\n\nAnalysis of variance\n\nThe Student's t-test cannot be used for comparison of three or more groups. The purpose of ANOVA is to test if there is any significant difference between the means of two or more groups.\n\nIn ANOVA, we study two variances – (a) between-group variability and (b) within-group variability. The within-group variability (error variance) is the variation that cannot be accounted for in the study design. It is based on random differences present in our samples.\n\nHowever, the between-group (or effect variance) is the result of our treatment. These two estimates of variances are compared using the F-test.\n\nA simplified formula for the F statistic is:\n\nwhere MSb is the mean squares between the groups and MSw is the mean squares within groups.\n\nRepeated measures analysis of variance\n\nAs with ANOVA, repeated measures ANOVA analyses the equality of means of three or more groups. However, a repeated measure ANOVA is used when all variables of a sample are measured under different conditions or at different points in time.\n\nAs the variables are measured from a sample at different points of time, the measurement of the dependent variable is repeated. Using a standard ANOVA in this case is not appropriate because it fails to model the correlation between the repeated measures: The data violate the ANOVA assumption of independence. Hence, in the measurement of repeated dependent variables, repeated measures ANOVA should be used.\n\nTests to analyse the categorical data\n\nChi-square test, Fischer's exact test and McNemar's test are used to analyse the categorical or nominal variables. The Chi-square test compares the frequencies and tests whether the observed data differ significantly from that of the expected data if there were no differences between groups (i.e., the null hypothesis). It is calculated by the sum of the squared difference between observed (O) and the expected (E) data (or the deviation, d) divided by the expected data by the following formula:\n\nA Yates correction factor is used when the sample size is small. Fischer's exact test is used to determine if there are non-random associations between two categorical variables. It does not assume random sampling, and instead of referring a calculated statistic to a sampling distribution, it calculates an exact probability. McNemar's test is used for paired nominal data. It is applied to 2 × 2 table with paired-dependent samples. It is used to determine whether the row and column frequencies are equal (that is, whether there is ‘marginal homogeneity’). The null hypothesis is that the paired proportions are equal. The Mantel-Haenszel Chi-square test is a multivariate test as it analyses multiple grouping variables. It stratifies according to the nominated confounding variables and identifies any that affects the primary outcome variable. If the outcome variable is dichotomous, then logistic regression is used."
    }
}
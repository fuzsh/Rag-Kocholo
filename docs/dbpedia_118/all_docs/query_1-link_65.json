{
    "id": "dbpedia_118_1",
    "rank": 65,
    "data": {
        "url": "https://www.emeraldgrouppublishing.com/how-to/research/data-analysis/choose-right-statistical-technique",
        "read_more_link": "",
        "language": "en",
        "title": "Choose the right statistical technique",
        "top_image": "https://www.emeraldgrouppublishing.com/sites/default/files/image/Emerald-social.jpg",
        "meta_img": "https://www.emeraldgrouppublishing.com/sites/default/files/image/Emerald-social.jpg",
        "images": [
            "https://www.emeraldgrouppublishing.com/sites/default/files/2021-06/bubble.svg",
            "https://www.emeraldgrouppublishing.com/themes/custom/emerald_publishing/logo.svg",
            "https://www.emeraldgrouppublishing.com/sites/default/files/2020-01/FW%20-%20Wooden%20directional%20signpost%20poiting%20towards%20viewer%20with%20rollling%20countryside%20in%20background_0.jpg",
            "https://www.emeraldgrouppublishing.com/themes/custom/emerald_publishing/assets/svg/emerald-publishing-logo_white.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "Explore the basics of statistical techniques, including testing validity, advanced techniques and graphical presentation.",
        "meta_lang": "en",
        "meta_favicon": "/sites/default/files/emerald-favicon.png",
        "meta_site_name": "Emerald Publishing",
        "canonical_link": "https://www.emeraldgrouppublishing.com/how-to/research/data-analysis/choose-right-statistical-technique",
        "text": "Fundamentals\n\nStart to think about the techniques you will use for your analysis before you collect any data.\n\nWhat do you want to know?\n\nThe analysis must relate to the research questions, and this may dictate the techniques you should use.\n\nWhat type of data do you have?\n\nThe type of data you have is also fundamental – the techniques and tools appropriate to interval and ratio variables are not suitable for categorical or ordinal measures. (See How to collect data for notes on types of data)\n\nWhat assumptions can – and can’t – you make?\n\nMany techniques rely on the sampling distribution of the test statistic being a Normal distribution (see below). This is always the case when the underlying distribution of the data is Normal, but in practice, the data may not be Normally distributed. For example, there could be a long tail of responses to one side or the other (skewed data). Non-parametric techniques are available to use in such situations, but these are inevitably less powerful and less flexible. However, if the sample size is sufficiently large, the Central Limit Theorem allows use of the standard analyses and tools.\n\nTechniques for a non-Normal distribution\n\nParametric or non-parametric statistics?\n\nParametric methods and statistics rely on a set of assumptions about the underlying distribution to give valid results. In general, they require the variables to have a Normal distribution.\n\nNon-parametric techniques must be used for categorical and ordinal data, but for interval & ratio data they are generally less powerful and less flexible, and should only be used where the standard, parametric, test is not appropriate – e.g. when the sample size is small (below 30 observations).\n\nCentral limit theorem\n\nAs the sample size increases, the shape of the sampling distribution of the test statistic tends to become Normal, even if the distribution of the variable which is being tested is not Normal.\n\nIn practice, this can be applied to test statistics calculated from more than 30 observations.\n\nHow much can you expect to get out of your data?\n\nThe smaller the sample size, the less you can get out of your data. Standard error is inversely related to sample size, so the larger your sample, the smaller the standard error, and the greater chance you will have of identifying statistically significant results in your analysis.\n\nBasic techniques\n\nIn general, any technique which can be used on categorical data may also be used on ordinal data. Any technique which can be used on ordinal data may also be used on ratio or interval data. The reverse is not the case.\n\nDescribing your data\n\nThe first stage in any analysis should be to describe your data, and the hence the population from which it is drawn. The statistics appropriate for this activity fall into three broad groups, and depend on the type of data you have.\n\nWhat do you want to do? With what type of data? Appropriate techniques Look at the distribution Categorical / Ordinal Plot the percentage\n\nin each category\n\n(column or bar chart) Ratio / Interval Histogram\n\nCumulative frequency\n\ndiagram Describe the\n\ncentral tendency Categorical n/a Ordinal Median\n\nMode Ratio / Interval Mean\n\nMedian Describe the spread Categorical n/a Ordinal Range\n\nInter-quartile range Ratio / Interval Range\n\nInter-quartile range\n\nVariance\n\nStandard variation\n\nSee Graphical presentation for descriptions of the main graphical techniques.\n\nMean – the arithmetic average, calculated by summing all the values and dividing by the number of values in the sum.\n\nMedian – the mid point of the distribution, where half the values are higher and half lower.\n\nMode – the most frequently occurring value.\n\nRange – the difference between the highest and lowest value.\n\nInter-quartile range – the difference between the upper quartile (the value where 25 per cent of the observations are higher and 75 per cent lower) and the lower quartile (the value where 75 per cent of the observations are higher and 25 per cent lower). This is particularly useful where there are a small number of extreme observations much higher, or lower, than the majority.\n\nVariance – a measure of spread, calculated as the mean of the squared differences of the observations from their mean.\n\nStandard deviation – the square root of the variance.\n\nDifferences between groups and variables\n\nChi-squared test – used to compare the distributions of two or more sets of categorical or ordinal data.\n\nt-tests – used to compare the means of two sets of data.\n\nWilcoxon U test – non-parametric equivalent of the t-test. Based on the rank order of the data, it may also be used to compare medians.\n\nANOVA – analysis of variance, to compare the means of more than two groups of data.\n\nWhat do you want to do? With what type of data? Appropriate techniques Compare two groups Categorical Chi-squared test Ordinal Chi-squared test\n\nWicoxon U test Ratio / Interval t-test for\n\nindependent samples Compare more than two groups Categorical / Ordinal Chi-squared test Ratio / Interval ANOVA Compare two variables\n\nover the same subjects Categorical / Ordinal Chi-squared test Ratio / Interval t-test for\n\ndependent samples\n\nRelationships between variables\n\nThe correlation coefficient measures the degree of linear association between two variables, with a value in the range +1 to -1. Positive values indicate that the two variables increase and decrease together; negative values that one increases as the other decreases. A correlation coefficient of zero indicates no linear relationship between the two variables. The Spearman rank correlation is the non-parametric equivalent of the Pearson correlation.\n\nWhat type of data? Appropriate techniques Categorical Chi-squared test Ordinal Chi-squared test\n\nSpearman rank\n\ncorrelation (Tau) Ratio / Interval Pearson\n\ncorrelation (Rho)\n\nNote that correlation analyses will only detect linear relationships between two variables. The figure below illustrates two small data sets where there are clearly relationships between the two variables. However, the correlation for the second data set, where the relationship is not linear, is 0.0. A simple correlation analysis of these data would suggest no relationship between the measures, when that is clearly not the case. This illustrates the importance of undertaking a series of basic descriptive analyses before embarking on analyses of the differences and relationships between variables.\n\nTesting validity\n\nSignificance levels\n\nThe statistical significance of a test is a measure of probability - the probability that you would have obtained that particular result of the test on that sample if the null hypothesis (that there is no effect due to the parameters being tested) you are testing was true. The example below tests whether scores in an exam change after candidates have received training. The hypothesis suggests that they should, so the null hyopothesis is that they won't.\n\nIn general, any level of probability above 5 per cent (p>0.05) is not considered to be statistically significant, and for large surveys 1 per cent (p>0.01) is often taken as a more appropriate level.\n\nNote that statistical significance does not mean that the results you have obtained actually have value in the context of your research. If you have a large enough sample, a very small difference between groups can be identified as statistically significant, but such a small difference may be irrelevant in practice. On the other hand, an apparently large difference may not be statistically significant in a small sample, due to the variation within the groups being compared.\n\nDegrees of freedom\n\nSome test statistics (e.g. chi-squared) require the number of degrees of freedom to be known, in order to test for statistical significance against the correct probability table. In brief, the degrees of freedom is the number of values which can be assigned arbitrarily within the sample.\n\nFor example:\n\nIn a sample of size n divided into k classes, there are k-1 degrees of freedom (the first k-1 groups could be of any size up to n, while the last is fixed by the total of the first k-1 and the value of n. In numerical terms, if a sample of 500 individuals is taken from the UK, and it is observed that 300 are from England, 100 from Scotland and 50 from Wales, then there must be 50 from Northern Ireland. Given the numbers from the first three groups, there is no flexibility in the size of the final group. Dividing the sample into four groups gives three degrees of freedom.\n\nIn a two-way contingency table with p rows and q columns, there are (p-1)*(q-1) degrees of freedom (given the values of the first rows and columns, the last row and column are constrained by the totals in the table)\n\nOne-tail or two-tail tests\n\nIf, as is generally the case, what matters is simply that the statistics for the populations are different, then it is appropriate to use the critical values for a two-tailed test.\n\nIf, however, you are only interested to find out if the statistic for population A has a larger value than that for population B, then a one-tailed test would be appropriate. The critical value for a one-tailed test is generally lower than for a two-tailed test, and should only be used if your research hypothesis is that population A has a greater value than population B, and it does not matter how different they are if population A has a value that is less than that for population B.\n\nFor example\n\nScenario 1\n\nNull hypothesis – there is no difference in mean exam scores before and after training (i.e. training has no effect on the exam score)\n\nAlternative – there is a difference in the mean scores before and after training (i.e. training has an unspecified effect)\n\nUse a two-tail test\n\nScenario 2\n\nNull hypothesis – Training does not increase the mean score\n\nAlternative – Mean score increases after training\n\nUse a one-tail test, if there is an observed increase in mean score.\n\n(If there is an observed fall in scores, there is no need to test, as you cannot reject the null hypothesis.)\n\nScenario 3\n\nNull hypothesis – Training does not cause mean scores to fall\n\nAlternative – Mean score falls after training\n\nUse a one-tail test, if there is an observed fall in mean score.\n\n(If there is an observed increase in scores, there is no need to test, as you cannot reject the null hypothesis.)\n\nt-Test: Paired Two Sample for Means Before After Mean\n\n360.4\n\n361.1\n\nVariance\n\n46,547\n\n46,830\n\nObservations\n\n62\n\n62\n\nDegrees of freedom (df)\n\n61\n\nt Stat\n\n1.79\n\nP(T<=t) one-tail\n\n0.04\n\nt Critical one-tail\n\n1.67\n\nP(T<=t) two-tail\n\n0.058\n\nt Critical two-tail\n\n2.00\n\nIf the above test results were obtained, then under scenario 1, using a two-tail test, you might conclude that there was no statistically significant difference between the scores (p=0.08), and, as a consequence, that training had no effect. Similarly, under scenario 3, you would conclude that there is no evidence to suggest that training causes mean scores to fall, as they have in fact risen. However, under scenario 2, using a one-tail test, you would conclude that there was an increase in mean scores, statistically significant at the 5 per cent level (p=0.04).\n\nA final warning!\n\nStatistical packages will do what you tell them, on the whole. They do not know whether the data you have provided is of good quality, or (with a very few exceptions) whether it is of an appropriate type for the analysis you have undertaken.\n\nRubbish in = Rubbish out!\n\nAdvanced techniques\n\nThese tools and techniques have specialist applications, and will generally be designed into the research methodology at an early stage, before any data are collected. If you are considering using any of these, you may wish to consult a specialist text or an experienced statistician before you start.\n\nIn each case, we give some examples of Emerald articles which use the technique.\n\nFactor analysis\n\nTo reduce the number of variables for subsequent analysis by creating combinations of the original variables measured which account for as much of the original variance as possible, but allow for easier interpretation of the results. Commonly used to create a small set of dimension ratings from a large number of opinion statements individually rated on Likert scales. You must have more observations (subjects) than you have variables to be analysed.\n\nFor example\n\nA Likert scale variable: \"I like to eat chocolate ice cream for breakfast\"\n\nStrongly agree\n\n1\n\n2\n\n3\n\n4\n\n5\n\nStrongly disagree\n\nA factor analysis of Page and Wong's servant leadership instrument\n\nRob Dennis and Bruce E. Winston\n\nLeadership & Organization Development Journal , vol. 24 no. 8\n\nUnderstanding factors for benchmarking adoption: New evidence from Malaysia\n\nYean Pin Lee, Suhaiza Zailani and Keng Lin Soh\n\nBenchmarking: An International Journal , vol. 13 no. 5\n\nCluster analysis\n\nTo classify subjects into groups with similar characteristics, according to the values of the variables measured. You must have more observations than you have variables included in the analysis.\n\nOrganic product avoidance: Reasons for rejection and potential buyers' identification in a countrywide survey\n\nC. Fotopoulos and A. Krystallis\n\nBritish Food Journal, vol. 104 no. 3/4/5\n\nDetection of financial distress via multivariate statistical analysis\n\nS. Gamesalingam and Kuldeep Kumar\n\nManagerial Finance, vol. 27 no. 4\n\nDiscriminant analysis\n\nTo identify those variables which best discriminate between known groups of subjects. The results may be used to allocate new subjects to the known groups based on their values of the discriminating variables\n\nDetection of financial distress via multivariate statistical analysis\n\nS. Gamesalingam and Kuldeep Kumar\n\nManagerial Finance, vol. 27 no. 4\n\nUnderstanding factors for benchmarking adoption: New evidence from Malaysia\n\nYean Pin Lee, Suhaiza Zailani and Keng Lin Soh\n\nBenchmarking: An International Journal , vol. 13 no. 5\n\nMethodology\n\nDiscriminant analysis was used to determine whether statistically significant differences exist between the average score profile on a set of variables for two a priori defined groups and so enabled them to be classified. Besides, it could help to determine which of the independent variables account the most for the differences in the average score profiles of the two groups. In this study, discriminant analysis was the main instrument to classify the benchmarking adopter and non-adopter. It was also utilised to determine which of the independent variables would contribute to benchmarking adoption.\n\nRegression\n\nTo model how one, dependant, variable behaves depending on the values of a set of other, independent, variables. The dependant variable must be interval or ratio in type; the independent variables may be of any type, but special methods must be used when including categorical or ordinal independent variables in the analysis.\n\nDevelopments in milk marketing in England and Wales during the 1990s\n\nJeremy Franks\n\nBritish Food Journal, vol. 103 no. 9\n\nTraining under fire: The relationship between obstacles facing training and SMEs' development in Palestine\n\nMohammed Al Madhoun\n\nJournal of European Industrial Training, vol. 30 no. 2\n\nTime series analysis\n\nTo investigate the patterns and trends in a variable measured regularly over a period of time. May also be used to identify and adjust for seasonal variation, for example in financial statistics.\n\nAn analysis of the trends and cyclical behaviours of house prices in the Asian markets\n\nMing-Chi Chen, Yuichiro Kawaguchi and Kanak Patel\n\nJournal of Property Investment & Finance, vol. 22 no. 1\n\nGraphical presentation\n\nPresenting data in graphical form can increase the accessibility of your results to a non-technical audience, and highlight effects and results which would otherwise require lengthy explanation, or complex tables. It is therefore important that appropriate graphical techniques are used. This section gives examples of some of the most commonly used graphical presentations, and indicates when they may be used. All, except the histogram, have been produced using Microsoft Excel®.\n\nColumn or bar charts\n\nThere are four main variations, and whether you display the data in horizontal bars or vertical columns is largely a matter of personal preference.\n\nHistogram\n\nTo illustrate a frequency distribution in categorical or ordinal data, or grouped ratio/interval data. Usually displayed as a column graph.\n\nClustered column/bar\n\nTo compare categorical, ordinal or grouped ratio/interval data across categories. The data used in fig 4 are the same as those in Figs 5 and 6.\n\nStacked column/bar\n\nTo illustrate the actual contribution to the total for categorical, ordinal or grouped ratio/interval data by categories. The data used in Fig 5 are the same as those in Figs 4 and 6.\n\nPercentage stacked column/bar\n\nTo compare the percentage contribution to the total for categorical, ordinal or grouped ratio/interval data across categories. The data used in fig 6 are the same as those in Figs 4 and 5.\n\nLine graphs\n\nTo show trends in ordinal or ratio/interval data. Points on a graph should only be joined with a line if the data on the x-axis are at least ordinal. One particular application is to plot a frequency distribution for interval/ratio data (fig 8).\n\nPie charts\n\nTo show the percentage contribution to the whole of categorical, ordinal or grouped ratio/interval data.\n\nScatter graphs\n\nTo illustrate the relationship between two variables, of any type (although most useful where both variables are ratio/interval in type). Also useful in the identification of any unusual observations in the data.\n\nBox and whisker plot\n\nA specialist graph illustrating the central tendency and spread of a large data set, including any outliers.\n\nResources\n\nConnecting Mathematics\n\nBrief explanations of mathematical terms and ideas\n\nStatistics Glossary\n\ncompiled by Valerie J. Easton and John H. McColl of Glasgow University\n\nStatsoft electronic textbook\n\n100 Statistical Tests by Gopal K. Kanji\n\n(Sage, 1993, ISBN 141292376X)"
    }
}
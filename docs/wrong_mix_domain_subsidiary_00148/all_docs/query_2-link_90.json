{
    "id": "wrong_mix_domain_subsidiary_00148_2",
    "rank": 90,
    "data": {
        "url": "https://comp.databases.sybase.narkive.com/19ajZ5Zk/sybase-faq-1-19-index",
        "read_more_link": "",
        "language": "en",
        "title": "Sybase FAQ: 1/19",
        "top_image": "https://narkive.net/favicon.ico",
        "meta_img": "https://narkive.net/favicon.ico",
        "images": [],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "",
        "meta_favicon": "https://narkive.net/favicon.ico",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "David Owen\n\nArchive-name: databases/sybase-faq/part3\n\nURL: http://www.isug.com/Sybase_FAQ\n\nVersion: 1.7\n\nMaintainer: David Owen\n\nLast-modified: 2003/03/02\n\nPosting-Frequency: posted every 3rd month\n\nA how-to-find-the-FAQ article is posted on the intervening months.\n\nSybase Frequently Asked Questions\n\nSybase FAQ Home PageAdaptive Server Enterprise FAQAdaptive Server Anywhere FAQ\n\nRepserver FAQSearch the FAQ\n\n[bar]\n\nSybase Replication Server\n\n1. Introduction to Replication Server\n\n2. Replication Server Administration\n\n3. Troubleshooting Replication Server\n\n4. Additional Information/Links\n\nIntroduction to Replication Server\n\n1.1 Introduction\n\n1.2 Replication Server Components\n\n1.3 What is the Difference Between SQL Remote and Replication Server?\n\nThanks go to Manish I Shah for major help with this introduction.\n\nnext prev ASE FAQ\n\n-------------------------------------------------------------------------------\n\n1.1 Introduction\n\n-------------------------------------------------------------------------------\n\nWhat is Replication Server\n\nReplication Server moves transactions (insert, updates and deletes) at the\n\ntable level from a source dataserver to one or more destination dataservers.\n\nThe dataserver could be ASE or other major DBMS flavour (including DB2,\n\nInformix, Oracle). The source and destinations need not be of the same type.\n\nWhat can it do ?\n\n* Move data from one source to another.\n\n* Move only a subset of data from source to destination. So, you can\n\nsubscribe to a subset of data, or a subset of the columns, in the source\n\ntable, e.g. select * from clients where state = NY\n\n* Manipulation/transformation of data when moving from source to destination.\n\nE.g. it can map data from a data-type in DB2 to an equivalent in Sybase.*\n\n* Provide a warm-standby system. Can be incorporated with Open Switch to\n\nprovide a fairly seamless fail-over environment.\n\n* Merge data from several source databases into one destination database\n\n(could be for a warehouse type environment for example).\n\n* Move data through a complicated network down to branch offices, say, only\n\nsending the relevant data to each branch.\n\n(* This is one of Sybase replication's real strengths, the ability to define\n\nfunction string classes which allow the conversion of statements from one SQL\n\ndialect to match the dialect of the destination machine. Ed)\n\nHow soon does the data move\n\nThe data moves asynchronously. The time it takes to reach the destination\n\ndepends on the size of your transaction, level of activity in that particular\n\ndatabase (a database as in Sybase systems), the length of the chain (one or\n\nmore replication servers that the transaction has to pass through to reach the\n\ndestination), the thickness of pipe (network), how busy your replication server\n\nis etc. Usually, on a LAN, for small transactions, this is about a second.\n\nBack to top\n\n-------------------------------------------------------------------------------\n\n1.2 Replication Server Components\n\n-------------------------------------------------------------------------------\n\nBasic\n\nPrimary Dataserver\n\nThe source of data where client applications enter/delete and modify data. As\n\nmentioned before, this need not be ASE, it can be Microsoft SQL Server, Oracle,\n\nDB2, Informix. (I know that I should get a complete list.)\n\nReplication Agent/Log Transfer Manager\n\nLog Transfer Manager (LTM) is a separate program/process which reads\n\ntransaction log from the source server and transfers them to the replication\n\nserver for further processing. With ASE 11.5, this has become part of ASE and\n\nis now called the Replication Agent. However, you still need to use an LTM for\n\nnon-ASE sources. I imagine there is a version of LTM for each kind of source\n\n(DB2, Informix, Oracle etc). When replication is active, you see one\n\nconnection per each replicated database in the source dataserver (sp_who).\n\nReplication Server (s)\n\nThe replication server is an Open Server/Open Client application. The server\n\npart receives transactions being sent by either the source ASE or the source\n\nLTM. The client part sends these transactions to the target server which could\n\nbe another replication server or the final dataserver. As far as I know, the\n\nserver does not include the client component of any of the other DBMSes out of\n\nthe box.\n\nReplicate (target) Dataserver\n\nServer in which the final replication server (in the queue) will repeat the\n\ntransaction done on the primary. You will see a connection, one for each target\n\ndatabase, in the target dataserver when the replication server is actively\n\ntransferring data (when idle, the replication server disconnects or fades out\n\nin replication terminology).\n\nBack to top\n\n-------------------------------------------------------------------------------\n\n1.3 What is the Difference Between Replication Server and SQL Remote?\n\n-------------------------------------------------------------------------------\n\nBoth SQL Remote and Replication Server perform replication. SQL Remote was\n\noriginally part of the Adaptive Server Anywhere tool kit and is intended for\n\nintermittent replication. (The classic example is that of a salesman\n\nconnecting on a daily basis to upload sales and download new prices and\n\ninventory.) Replication Server is intended for near real-time replication\n\nscenarios.\n\nBack to top\n\n-------------------------------------------------------------------------------\n\nnext prev ASE FAQ\n\nReplication Server Administration\n\n2.1 How can I improve throughput?\n\n2.2 Where should I install replication server?\n\n2.3 Using large raw partitions with Replication Server on Unix.\n\n2.4 How to replicate col = col + 1\n\n2.5 What is the difference between an LTMs an a RepAgent?\n\n2.6 Which Should I choose, RepAgent or LTM?\n\nnext prev ASE FAQ\n\n-------------------------------------------------------------------------------\n\n2.1 How can I improve throughput?\n\n-------------------------------------------------------------------------------\n\nCheck the Obvious\n\nFirst, ensure that you are only replicating those parts of the system that need\n\nto be replicated. Some of this is obvious. Don't replicate any table that\n\ndoes not need to be replicated. Check that you are only replicating the\n\ncolumns you need. Replication is very sophisticated and will allow you to\n\nreplicate both a subset of the columns as well as a subset of the rows.\n\nReplicate Minimum Columns\n\nOnce the replication is set up and synchronised, it is only necessary to\n\nreplicate those parts of the primary system that actually change. You are only\n\nreplicating those rows and columns that need to be replicated, but you only\n\nneed to replicate the actual changes. Check that each replication definition\n\nis defined using the clause:\n\ncreate replication definition rep_def_name\n\nwith primary...\n\n...\n\nreplicate minimal columns\n\nSecond Replication Server\n\nThis might be appropriate in a simple environment on systems with spare cycles\n\nand limited space on the network. When Sybase replicates from a primary to a\n\nreplicate using only one replication server the data is transferred across the\n\nnetwork uncompressed. However, the communication between two replication\n\nservers is compressed. By installing a second replication server it is\n\npossible to dramatically reduce the bandwidth needed to replicate your data.\n\nDedicated Network Card\n\nObviously, if replication is sharing the same network resources that all of the\n\nclients are using, there is the possibility for a bottleneck if the network\n\nbandwidth is close to saturation. If a second replication server is not going\n\nto cut it since you already have one or there are no spare cycles, then a\n\nsecond network card may be the answer.\n\nFirst, you will need to configure ASE to listen on two network connections.\n\nThis is relatively straightforward. There is no change to the client\n\nconfiguration. They all continue to talk to Sybase using the same connection.\n\nWhen defining the replication server, ensure that the interfaces/sql.ini entry\n\nthat it uses only has the second connection in it. This may involve some\n\njiggery pokery with environment variables, but should be possible, even on NT!\n\nYou need to be a little careful with network configuration. Sybase will\n\ncommunicate with the two servers on the correct address, but if the underlying\n\noperating system believes that both clients and repserver can be serviced by\n\nthe same card, then it will use the first card that it comes to. So, if you\n\nhad the situation that all of the clients, ASE and the replication server were\n\non 192.168.1.0, and the host running ASE had two cards onto this same segment,\n\nthen it would choose to route all packets through the first card. OK, so this\n\nis a very simplistic error to correct, but similar things can happen with more\n\nconvoluted and, superficially, better thought out configurations.\n\n+---------+ +-----------+ +-----------+\n\n| |--> NE(1) --> All Clients... | | | |\n\n| Primary | | repserver | | replicate |\n\n| |--> NE(2) --------------------->| |-->| |\n\n| | | | | |\n\n+---------+ +-----------+ +-----------+\n\nSo, configure NE(1) to be on 192.168.1.0, say, and NE(2) to be on 192.168.2.0\n\nand all should be well. OK, so my character art is not perfect, but I think\n\nthat you get the gist!\n\nNo Network Card\n\nIf RepServer resides on the same physical machine as either the primary or the\n\nreplicate, it is possible to use the localhost or loopback network device. The\n\nloopback device is a network interface that connects back to itself without\n\ngoing through the network interface card. It is almost always uses the IP\n\naddress 127.0.0.1. So, by applying the technique described above, but instead\n\nof using a dedicated network card, you use the loopback device. Obviously, the\n\ntwo servers have to be on the same physical machine or it won't work!\n\nBack to top\n\n-------------------------------------------------------------------------------\n\n2.2 Where should I install replication server?\n\n-------------------------------------------------------------------------------\n\nA seemingly trivial question, but one that can cause novices a bit of worry.\n\nThere are three answers: on the primary machine, on the replicate machine or on\n\na completely separate machine. There is no right answer, and if you are doing\n\nan initial install it probably pays to consider the future, consider the\n\nproposed configuration and have a look at the load on the available machines.\n\nIt is probably fair to say that replication is not power hungry but neither is\n\nit free. If the primary is only just about coping with its current load, then\n\nit might be as well looking into hosting it on another machine. The argument\n\napplies to the replicate. If you think that network bandwidth may be an issue,\n\nand you may have to add a second replication server, you may be better off\n\nstarting with repserver running on the primary. It is marginally easier to add\n\na repserver to an existing configuration if the first repserver is on the\n\nprimary.\n\nRemember that a production replication server on Unix will require raw devices\n\nfor the stable devices and that these can be more than 2GB in size. If you are\n\nrestricted in the number of raw partitions you have available on a particular\n\nmachine, then this may have a bearing. See Q2.3.\n\nInstalling replication server on its own machine will, of course, introduce all\n\nsorts of problems of its own, as well as answering some. The load on the\n\nprimary or the replicate is reduced considerably, but you are definitely going\n\nto add some load to the network. Remember that ASE->Rep and Rep->ASE is\n\nuncompressed. It is only Rep->Rep that is compressed.\n\nBack to top\n\n-------------------------------------------------------------------------------\n\n2.3 Using large raw partitions with Replication Server on Unix.\n\n-------------------------------------------------------------------------------\n\nIt is a good practice with production installations of Replication Server on\n\nUnix that you use raw partitions for the stable devices. This is for just the\n\nsame reason that production ASE's use raw partitions. Raw devices can be a\n\nmaximum of 2GB with replication server up to release 11.5. (I have not checked\n\n12.)\n\nIn order to utilise a raw partition that is greater than 2GB in size you can do\n\nthe following (remember all of the cautionary warnings about trying this sort\n\nof stuff out in development first!):\n\nadd partition firstpartition on '/dev/rdsk/c0t0d0s0' with size 2024\n\ngo\n\nadd partition secondpartition on '/dev/rdsk/c0t0d0s0' with size 2024\n\nstarting at 2048\n\ngo\n\nNotice that the initial partition is sized at 2024MB and not 2048. I have not\n\nfound this in the documentation, but replication certainly seems to have a\n\nproblem allocating a full 2GB. Interestingly, do the same operation through\n\nRep Server Manager and Sybase central caused no problems at all.\n\nBack to top\n\n-------------------------------------------------------------------------------\n\n2.4 How to replicate col = col + 1\n\n-------------------------------------------------------------------------------\n\nFirstly. While the rule that you never update a primary key may be a\n\nphilosophical choice in a non-replicated system, it is an architectural\n\nrequirement of a replicated system.\n\nIf you use simple data replication, and your primary table is:\n\nid\n\n---\n\n1\n\n2\n\n3\n\nand you issue a:\n\nupdate table set id=id+1\n\nRep server will do this in the replicate:\n\nbegin tran\n\nupdate table set id=2 where id=1\n\nupdate table set id=3 where id=2\n\nupdate table set id=4 where id=3\n\ncommit tran\n\nHands up all who can see a bit of a problem with this! Remember, repserver\n\ndoesn't replicate statements, it replicates the results of statements.\n\nOne way to perform this update is to build a stored procedure on both sides\n\nthat executes the necessary update and replicate the stored procedure call.\n\nBack to top\n\n-------------------------------------------------------------------------------\n\n2.5 What is the difference between an LTM and a RepAgent?\n\n-------------------------------------------------------------------------------\n\nAs described in Section 1.2, Log Transfer Managers (LTMs) and RepAgents are the\n\nprocesses that transfer data between ASE and the Replication Server.\n\nLTMs were delivered with the first releases of Replication Server. Each LTM is\n\na separate process at the operating system level that runs along side ASE and\n\nReplication Server. As with ASE and Replication Server, a RUN_<ltm_server> and\n\nconfiguration file is required for each LTM. One LTM is required for each\n\ndatabase being replicated.\n\nAlong with ASE 11.5 a new concept was introduced, that of RepAgent. I am not\n\nsure if you needed to use RepServer 11.5 as well, or whether the RepAgents\n\ncould talk to earlier versions of Replication Server. Each RepAgent is, in\n\neffect, a slot-in replacement for an LTM. However, instead of running as\n\nseparate operating system process, it runs as a thread within ASE. Pretty much\n\nall of the requirements for replication using an LTM apply to the RepAgents.\n\nOne per database being replicated, etc. but now you do not need to have\n\nseparate configuration files.\n\nBack to top\n\n-------------------------------------------------------------------------------\n\n2.6 Which should I use, RepAgent or LTM?\n\n-------------------------------------------------------------------------------\n\nThe differences between RepAgents and LTMs are discussed in Section 2.5.\n\nWhich then to choose. There are pros and cons to both, however, I think that\n\nit should be stated up front that RepAgents are the latest offering and I\n\nbelieve that Sybase would expect you you to use that. Certainly the\n\ndocumentation for LTMs is a little buried implying that they do not consider it\n\nto be as current as LTMs.\n\nLTM Cons:\n\n* Older technology. Not sure if it is being actively supported.\n\n* Not integrated within ASE, so there is a (small) performance penalty.\n\n* Separate processes, so need additional monitoring in production\n\nenvironments.\n\nLTM Pros:\n\n* Possible to restart LTM without having to restart ASE.\n\nRepAgent Cons\n\n* If it crashes it is possible that you will have to restart ASE in order to\n\nrestart RepAgent.\n\nRepAgent Pros\n\n* Latest, and presumably greatest, offering.\n\n* Tightly integrated with ASE so good performance.\n\n* Less to manage, no extra entries in the interfaces file.\n\nBack to top\n\n-------------------------------------------------------------------------------\n\nnext prev ASE FAQ\n\nReplication Server Trouble Shooting\n\n3.1 Why am I running out of locks on the replicate side?\n\n3.2 Someone was playing with replication and now the transaction log on\n\nOLTP is filling.\n\nnext prev ASE FAQ\n\n-------------------------------------------------------------------------------\n\n3.1 Why am I running out of locks on the replicate side?\n\n-------------------------------------------------------------------------------\n\nSybase replication works by taking each transaction that occurs in the primary\n\ndataserver and applying to the replicate. Since replication works on the\n\ntransaction log, a single, atomic, update on the primary side that updates a\n\nmillion rows will be translated into a million single row updates. This may\n\nseem very strange but is a simple consequence of how it works. On the primary,\n\nthis million row update will attempt to escalate the locks that it has taken\n\nout to an exclusive table lock. However, on the replicate side each row is\n\nupdated individually, much as if they were being updated within a cursor loop.\n\nNow, Sybase only tries to escalate locks from a single atomic statement (see\n\nASE Qx.y), so it will never try to escalate the lock. However, since the\n\nupdates are taking place within a single transaction, Sybase will need to take\n\nout enough page locks to lock the million rows.\n\nSo, how much should you increase the locks parameter on the replicate side? A\n\ngood rule of thumb might be double it or add 40,000 whichever is the larger.\n\nThis has certainly worked for us.\n\nBack to top\n\n-------------------------------------------------------------------------------\n\n3.2 Someone was playing with replication and now the transaction log on OLTP\n\nis filling.\n\n-------------------------------------------------------------------------------\n\nOnce replication has been configured, ASE adds another marker to the\n\ntransaction log. The first marker is the conventional one that marks which\n\ntransactions have had their data written to disk. The second is there to\n\nensure that the transactions have also been replicated. Clearly, if someone\n\ninstalled replication and did not clean up properly after themselves, this\n\nmarker will still be there and consequently the transaction log will be filling\n\nup. If you are certain that replication is not being used on your system, you\n\ncan disable the secondary truncation marker with the following commands:\n\n1> use <database>\n\n2> go\n\n1> dbcc settrunc(ltm, ignore)\n\n2> go\n\nThe above code is the normal mechanism for disabling the trucation point. I\n\nhave never had a problem with it. However, an alternative mechanism for\n\ndisabling the truncation point is given below. I do not know if it will work\n\nin situations that the previous example won't, or if it works for databases\n\nthat are damaged or what. If someone knows when you use it and why, please let\n\nme know (mailto:***@midsomer.org).\n\n1> sp_role \"grant\", sybase_ts_role, sa\n\n2> go\n\n1> set role sybase_ts_role on\n\n2> go\n\n1> dbcc dbrepair(dbname, ltmignore)\n\n2> go\n\n1> sp_role \"revoke\", sybase_ts_role, sa\n\n2> go\n\nThis scenario is also very common if you load a copy of your replicated\n\nproduction database into development.\n\nBack to top\n\n-------------------------------------------------------------------------------\n\nnext prev ASE FAQ\n\nAdditional Information/Links\n\n4.1 Links\n\n4.2 Newsgroups\n\nnext prev ASE FAQ\n\n-------------------------------------------------------------------------------\n\n4.1 Links\n\n-------------------------------------------------------------------------------\n\nThierry Antinolfi has a replication FAQ at his site http://pro.wanadoo.fr/\n\ndbadevil that covers a lot of good stuff.\n\nRob Verschoor has a 'Replication Server Tips & Tricks' section on his site, as\n\nwell as an indispensible quick reference guide!\n\nBack to top\n\n-------------------------------------------------------------------------------\n\n4.2 Newsgroups\n\n-------------------------------------------------------------------------------\n\nThere are a number of newsgroups that can deal with questions. Sybase have\n\nseveral in their own forums area.\n\nFor Replication Server:\n\nsybase.public.rep-server\n\nsybase.public.rep-agent\n\nfor SQL Remote and the issues of replicating with ASA:\n\nsybase.public.sqlanywhere.replication\n\nand of course, there is always the ubiquitous\n\ncomp.databases.sybase.\n\nBack to top\n\n-------------------------------------------------------------------------------\n\nnext prev ASE FAQ\n\nDavid Owen\n\nArchive-name: databases/sybase-faq/part4\n\nURL: http://www.isug.com/Sybase_FAQ\n\nVersion: 1.7\n\nMaintainer: David Owen\n\nLast-modified: 2003/03/02\n\nPosting-Frequency: posted every 3rd month\n\nA how-to-find-the-FAQ article is posted on the intervening months.\n\nSybase Frequently Asked Questions\n\nSybase FAQ Home PageAdaptive Server Enterprise FAQAdaptive Server Anywhere FAQ\n\nRepserver FAQSearch the FAQ\n\n[bar]\n\nAdaptive Server Enterprise\n\n0. What's in a name?\n\n1. ASE Administration\n\n1.1 Basic Administration\n\n1.2 User Database Administration\n\n1.3 Advanced Administration\n\n1.4 General Troubleshooting\n\n1.5 Performance and Tuning\n\n1.6 Server Monitoring\n\n2. Platform Specific Issues\n\n2.1 Solaris\n\n2.2 NT\n\n2.3 Linux\n\n3. DBCC's\n\n4. isql\n\n5. bcp\n\n6. SQL Development\n\n6.1 SQL Fundamentals\n\n6.2 SQL Advanced\n\n6.3 Useful SQL Tricks\n\n7. Open Client\n\n9. Freeware\n\n10. Sybase Technical News\n\n11. Additional Information\n\n12. Miscellany\n\n-------------------------------------------------------------------------------\n\nWhat's in a name?\n\nThroughout this FAQ you will find references to SQL Server and, starting with\n\nthis release, ASE or Adaptive Server Enterprise to give it its full name. You\n\nmight also be a little further confused, since Microsoft also seem to have a\n\nproduct called SQL Server.\n\nWell, back at about release 4.2 of Sybase SQL Server, the products were exactly\n\nthe same. Microsoft were to do the port to NT. Well, it is pretty well\n\ndocumented, but there was a falling out. Both companies kept the same name for\n\ntheir data servers and confusion began to reign. In an attempt to try and sort\n\nthis out, Sybase renamed their product Adaptive Server Enterprise (ASE)\n\nstarting with version 11.5.\n\nI found this quote in a Sybase manual the other day:\n\nSince changing the name of Sybase SQL Server to Adaptive Server Enterprise,\n\nSybase uses the names Adaptive Server and Adaptive Server Enterprise to refer\n\ncollectively to all supported versions of the Sybase SQL Server and Adaptive\n\nServer Enterprise. Version-specific references to Adaptive Server or SQL Server\n\ninclude version numbers.\n\nI will endeavour to try and do the same within the FAQ, but the job is far from\n\ncomplete!\n\nBack to Top\n\nBasic ASE Administration\n\n1.1.1 What is SQL Server and ASE anyway?\n\n1.1.2 How do I start/stop ASE when the CPU reboots?\n\n1.1.3 How do I move tempdb off of the master device?\n\n1.1.4 How do I correct timeslice -201?\n\n1.1.5 The how's and why's on becoming Certified.\n\n1.1.6 RAID and Sybase\n\n1.1.7 How to swap a db device with another\n\n1.1.8 Server naming and renaming\n\n1.1.9 How do I interpret the tli strings in the interface file?\n\n1.1.10 How can I tell the datetime my Server started?\n\n1.1.11 Raw partitions or regular files?\n\n1.1.12 Is Sybase Y2K (Y2000) compliant?\n\n1.1.13 How can I run the ASE upgrade manually?\n\n1.1.14 We have lost the sa password, what can we do?\n\n1.1.15 How do I set a password to be null?\n\n1.1.16 Does Sybase support Row Level Locking?\n\n1.1.17 What platforms does ASE run on?\n\n1.1.18 How do I backup databases > 64G on ASE prior to 12.x?\n\nUser Database Administration # ASE FAQ\n\n-------------------------------------------------------------------------------\n\n1.1.1: What is SQL Server and ASE?\n\n-------------------------------------------------------------------------------\n\nOverview\n\nBefore Sybase System 10 (as they call it) we had Sybase 4.x. Sybase System 10\n\nhas some significant improvements over Sybase 4.x product line. Namely:\n\n* the ability to allocate more memory to the dataserver without degrading its\n\nperformance.\n\n* the ability to have more than one database engine to take advantage of\n\nmulti-processor cpu machines.\n\n* a minimally intrusive process to perform database and transaction dumps.\n\nBackground and More Terminology\n\nA ASE (SQL Server) is simply a Unix process. It is also known as the database\n\nengine. It has multiple threads to handle asynchronous I/O and other tasks. The\n\nnumber of threads spawned is the number of engines (more on this in a second)\n\ntimes five. This is the current implementation of Sybase System 10, 10.0.1 and\n\n10.0.2 on IRIX 5.3.\n\nEach ASE allocates the following resources from a host machine:\n\n* memory and\n\n* raw partition space.\n\nEach ASE can have up to 255 databases. In most implementations the number of\n\ndatabases is limited to what seems reasonable based on the load on the ASE.\n\nThat is, it would be impractical to house all of a large company's databases\n\nunder one ASE because the ASE (a Unix process) will become overloaded.\n\nThat's where the DBAs experience comes in with interrogation of the user\n\ncommunity to determine how much activity is going to result on a given database\n\nor databases and from that we determine whether to create a new ASE or to house\n\nthe new database under an existing ASE. We do make mistakes (and businesses\n\ngrow) and have to move databases from one ASE to another. At times ASEs need to\n\nmove from one CPU server to another.\n\nWith Sybase System 10, each ASE can be configured to have more than one engine\n\n(each engine is again a Unix process). There's one primary engine that is the\n\nmaster engine and the rest of the engines are subordinates. They are assigned\n\ntasks by the master.\n\nInterprocess communication among all these engines is accomplished with shared\n\nmemory.\n\nSome times when a DBA issues a Unix kill command to extinguish a maverick\n\nASE, the subordinate engines are forgotten. This leaves the shared memory\n\nallocated and eventually we may get in to situations where swapping occurs\n\nbecause this memory is locked. To find engines that belong to no master\n\nASE, simple look for engines owned by /etc/init (process id 1). These\n\nengines can be killed -- this is just FYI and is a DBA duty.\n\nBefore presenting an example of a ASE, some other topics should be covered.\n\nConnections\n\nAn ASE has connections to it. A connection can be viewed as a user login but\n\nit's not necessarily so. That is, a client (a user) can spark up multiple\n\ninstances of their application and each client establishes its own connection\n\nto the ASE. Some clients may require two or more per invocation. So typically\n\nDBA's are only concerned with the number of connections because the number of\n\nusers typically does not provide sufficient information for us to do our job.\n\nConnections take up ASE resources, namely memory, leaving less memory for\n\nthe ASEs' available cache.\n\nASE Buffer Cache\n\nIn Sybase 4.0.1 there was a limit to the amount of memory that could be\n\nallocated to a ASE. It was around 80MB, with 40MB being the typical max. This\n\nwas due to internal implementations of Sybase's data structures.\n\nWith Sybase System 10 there really was no limit. For instance, we had an ASE\n\ncranked up to 300MB under 10. With System 11 and 12 this has been further\n\nextended. ASE's with 4G bytes of memory are not uncommon. I have not heard of\n\nan 11.9.3 or a 12 server with more that 4G bytes, but I am sure that they are\n\nnot far away.\n\nThe memory in an ASE is primarily used to cache data pages from disk. Consider\n\nthat the ASE is a light weight Operating System: handling user (connections),\n\nallocating memory to users, keeping track of which data pages need to be\n\nflushed to disk and the sort. Very sophisticated and complex. Obviously if a\n\ndata page is found in memory it's much faster to retrieve than going out to\n\ndisk.\n\nEach connection takes away a little bit from the available memory that is used\n\nto cache disk pages. Upon startup, the ASE pre-allocates the memory that is\n\nneeded for each connection so it's not prudent to configure 500 connections\n\nwhen only 300 are needed. We'd waste 200 connections and the memory associated\n\nwith that. On the other hand, it is also imprudent to under configure the\n\nnumber of connections; users have a way of soaking up a resource (like an ASE)\n\nand if users have all the connections a DBA cannot get into the server to\n\nallocate more connections.\n\nOne of the neat things about an ASE is that it reaches (just like a Unix\n\nprocess) a working set. That is, upon startup it'll do a lot of physical I/O's\n\nto seed its cache, to get lookup information for typical transactions and the\n\nlike. So initially, the first users have heavy hits because their requests have\n\nto be performed as a physical I/O. Subsequent transactions have less physical I\n\n/O and more logical I/O's. Logical I/O is an I/O that is satisfied in the ASEs'\n\nbuffer cache. Obviously, this is the preferred condition.\n\nDSS vs OLTP\n\nWe throw around terms like everyone is supposed to know this high tech lingo.\n\nThe problem is that they are two different animals that require a ASE to be\n\ntuned accordingly for each.\n\nWell, here's the low down.\n\nDSS\n\nDecision Support System\n\nOLTP\n\nOnline Transaction Processing\n\nWhat do these mean? OLTP applications are those that have very short orders of\n\nwork for each connection: fetch this row and with the results of it update one\n\nor two other rows. Basically, small number of rows affected per transaction in\n\nrapid sucession, with no significant wait times between operations in a\n\ntransaction.\n\nDSS is the lumbering elephant in the database world (unless you do some\n\ntricks... out of this scope). DSS requires a user to comb through gobs of data\n\nto aggregate some values. So the transactions typically involve thousands of\n\nrows. Big difference than OLTP.\n\nWe never want to have DSS and OLTP on the same ASE because the nature of OLTP\n\nis to grab things quickly but the nature of DSS is to stick around for a long\n\ntime reading tons of information and summarizing the results.\n\nWhat a DSS application does is flush out the ASE's data page cache because of\n\nthe tremendous amount of I/O's. This is obviously very bad for OTLP\n\napplications because the small transactions are now hurt by this trauma. When\n\nit was only OLTP a great percentage of I/O was logical (satisfied in the\n\ncache); now transactions must perform physical I/O.\n\nThat's why it's good not to mix DSS and OLTP if at all possible.\n\nIf mixing them cannot be avoided, then you need to think carefully about how\n\nyou configure your server. Use named data caches to ensure that the very\n\ndifferent natures of OLTP and DSS do not conflict with each other. If you\n\ntables that are shared, consider using dirty reads for the DSS applications if\n\nat all possible, since this will help not to block the OLTP side.\n\nAsynchronous I/O\n\nWhy async I/O? The idea is that in a typical online transaction processing\n\n(OLTP) application, you have many connections (over 200 connections) and short\n\ntransactions: get this row, update that row. These transactions are typically\n\nspread across different tables of the databases. The ASE can then perform each\n\none of these asynchronously without having to wait for others to finish. Hence\n\nthe importance of having async I/O fixed on our platform.\n\nEngines\n\nSybase System 10 can have more than one engine (as stated above). Sybase has\n\ntrace flags to pin the engines to a given CPU processor but we typically don't\n\ndo this. It appears that the master engine goes to processor 0 and subsequent\n\nsubordinates to the next processor.\n\nCurrently, Sybase does not scale linearly. That is, five engines do not make\n\nSybase perform five times as fast however we do max out with four engines.\n\nAfter that performance starts to degrade. This is supposed to be fixed with\n\nSybase System 11.\n\nPutting Everything Together\n\nAs previously mentioned, an ASE is a collection of databases with connections\n\n(that are the users) to apply and retrieve information to and from these\n\ncontainers of information (databases).\n\nThe ASE is built and its master device is typically built over a medium sized\n\n(50MB) raw partition. The tempdb is built over a cooked (regular - as opposed\n\nto a raw device) file system to realize any performance gains by buffered\n\nwrites. The databases themselves are built over the raw logical devices to\n\nensure their integrity. (Note: in System 12 you can use the dsync flag to\n\nensure that writes to file system devices are secure.\n\nPhysical and Logical Devices\n\nSybase likes to live in its own little world. This shields the DBA from the\n\noutside world known as Unix, VMS or NT. However, it needs to have a conduit to\n\nthe outside world and this is accomplished via devices.\n\nAll physical devices are mapped to logical devices. That is, given a physical\n\ndevice (such as /lv1/dumps/tempdb_01.efs or /dev/rdsk/dks1ds0) it is mapped by\n\nthe DBA to a logical device. Depending on the type of the device, it is\n\nallocated, by the DBA, to the appropriate place (vague enough?).\n\nOkay, let's try and clear this up...\n\nDump Device\n\nThe DBA may decide to create a device for dumping the database nightly. The DBA\n\nneeds to create a dump device.\n\nWe'll call that logically in the database datadump_for_my_db but we'll map it\n\nto the physical world as /lv1/dumps/in_your_eye.dat So the DBA will write a\n\nscript that connects to the ASE and issues a command like this:\n\ndump database my_stinking_db to datadump_for_my_db\n\ngo\n\nand the backupserver (out of this scope) takes the contents of my_stinking_db\n\nand writes it out to the disk file /lv1/dumps/in_your_eye.dat\n\nThat's a dump device. The thing is that it's not preallocated. This special\n\ndevice is simply a window to the operating system.\n\nData and Log Devices\n\nAh, now we are getting into the world of pre-allocation. Databases are built\n\nover raw partitions. The reason for this is because Sybase needs to be\n\nguaranteed that all its writes complete successfully. Otherwise, if it posted\n\nto a file system buffer (as in a cooked file system) and the machine crashed,\n\nas far as Sybase is concerned the write was committed. It was not, however, and\n\nintegrity of the database was lost. That is why Sybase needs raw partitions.\n\nBut back to the matter at hand...\n\nWhen building a new ASE, the DBA determines how much space they'll need for all\n\nthe databases that will be housed in this ASE.\n\nEach production database is composed of data and log.\n\nThe data is where the actual information resides. The log is where the changes\n\nare kept. That is, every row that is updated/deleted/inserted gets placed into\n\nthe log portion then applied to the data portion of the database.\n\nThat's why DBA strives to place the raw devices for logs on separate disks\n\nbecause everything has to single thread through the log.\n\nA transaction is a collection of SQL statements (insert/delete/update) that are\n\ngrouped together to form a single unit of work. Typically they map very closely\n\nto the business.\n\nI'll quote the Sybase ASE Administration Guide on the role of the log:\n\nThe transaction log is a write-ahead log. When a user issues a statement\n\nthat would modify the database, ASE automatically writes the changes to the\n\nlog. After all changes for a statement have been recorded in the log, they\n\nare written to an in-cache copy of the data page. The data page remains in\n\ncache until the memory is needed for another database page. At that time,\n\nit is written to disk. If any statement in a transaction fails to complete,\n\nASE reverses all changes made by the transaction. ASE writes an \"end\n\ntransaction\" record to the log at the end of each transaction, recording\n\nthe status (success or failure) of the transaction\n\nAs such, the log will grow as user connections affect changes to the database.\n\nThe need arises to then clear out the log of all transactions that have been\n\nflushed to disk. This is performed by issuing the following command:\n\ndump transaction my_stinking_db to logdump_for_my_db\n\ngo\n\nThe ASE will write to the dumpdevice all transactions that have been committed\n\nto disk and will delete the entries from its copy, thus freeing up space in the\n\nlog. Dumping of the transaction logs is accomplished via cron (the Unix\n\nscheduler, NT users would have to resort to at or some third party tool) . We\n\nschedule the heavily hit databases every 20 minutes during peak times.\n\nA single user can fill up the log by having begin transaction with no\n\ncorresponding commit/rollback transaction. This is because all their\n\nchanges are being applied to the log as an open-ended transaction, which is\n\nnever closed. This open-ended transaction cannot be flushed from the log,\n\nand therefore grows until it occupies all of the free space on the log\n\ndevice.\n\nAnd the way we dump it is with a dump device. :-)\n\nAn Example\n\nIf the DBA has four databases to plop on this ASE and they need a total of\n\n800MB of data and 100MB of log (because that's what really matters to us), then\n\nthey'd probably do something like this:\n\n1. allocate sufficient raw devices to cover the data portion of all the\n\ndatabases\n\n2. allocate sufficient raw devices to cover the log portion of all the\n\ndatabases\n\n3. start allocating the databases to the devices.\n\nFor example, assuming the following database requirements:\n\nDatabase\n\nRequirements\n\n+-----------------+\n\n| | | |\n\n|----+------+-----|\n\n| DB | Data | Log |\n\n|----+------+-----|\n\n|----+------+-----|\n\n| a | 300 | 30 |\n\n|----+------+-----|\n\n| b | 400 | 40 |\n\n|----+------+-----|\n\n| c | 100 | 10 |\n\n+-----------------+\n\nand the following devices:\n\nDevices\n\n+---------------------------------+\n\n| Logical | Physical | Size |\n\n|---------------+----------+------|\n\n| | /dev/ | |\n\n| dks3d1s2_data | rdsk/ | 500 |\n\n| | dks3d1s2 | |\n\n|---------------+----------+------|\n\n| | /dev/ | |\n\n| dks4d1s2_data | rdsk/ | 500 |\n\n| | dks4d1s2 | |\n\n|---------------+----------+------|\n\n| | /dev/ | |\n\n| dks5d1s0_log | rdsk/ | 200 |\n\n| | dks5d1s0 | |\n\n+---------------------------------+\n\nthen the DBA may elect to create the databases as follows:\n\ncreate database a on dks3d1s2_data = 300 log on dks5d1s0_log = 30\n\ncreate database b on dks4d1s2_data = 400 log on dks5d1s0_log = 40\n\ncreate database c on dks3d1s2_data = 50, dks4d1s2_data = 50 log on dks5d1s0_log = 10\n\nSome of the devices will have extra space available because out database\n\nallocations didn't use up all the space. That's fine because it can be used for\n\nfuture growth. While the Sybase ASE is running, no other Sybase ASE can\n\nre-allocate these physical devices.\n\nTempDB\n\nTempDB is simply a scratch pad database. It gets recreated when a SQL Server is\n\nrebooted. The information held in this database is temporary data. A query may\n\nbuild a temporary table to assist it; the Sybase optimizer may decide to create\n\na temporary table to assist itself.\n\nSince this is an area of constant activity we create this database over a\n\ncooked file system which has historically proven to have better performance\n\nthan raw - due to the buffered writes provided by the Operating System.\n\nPort Numbers\n\nWhen creating a new ASE, we allocate a port to it (currently, DBA reserves\n\nports 1500 through 1899 for its use). We then map a host name to the different\n\nports: hera, fddi-hera and so forth. We can actually have more than one port\n\nnumber for an ASE but we typically don't do this.\n\nBack to top\n\n-------------------------------------------------------------------------------\n\n1.1.2: How to start/stop ASE when CPU reboots\n\n-------------------------------------------------------------------------------\n\nBelow is an example of the various files (on Irix) that are needed to start/\n\nstop an ASE. The information can easily be extended to any UNIX platform.\n\nThe idea is to allow as much flexibility to the two classes of administrators\n\nwho manage the machine:\n\n* The System Administrator\n\n* The Database Administrator\n\nAny errors introduced by the DBA will not interfere with the System\n\nAdministrator's job.\n\nWith that in mind we have the system startup/shutdown file /etc/init.d/sybase\n\ninvoking a script defined by the DBA: /usr/sybase/sys.config/\n\n{start,stop}.sybase\n\n/etc/init.d/sybase\n\nOn some operating systems this file must be linked to a corresponding entry in\n\n/etc/rc.0 and /etc/rc.2 -- see rc0(1M) and rc2(1M)\n\n#!/bin/sh\n\n# last modified: 10/17/95, sr.\n\n#\n\n# Make symbolic links so this file will be called during system stop/start.\n\n# ln -s /etc/init.d/sybase /etc/rc0.d/K19sybase\n\n# ln -s /etc/init.d/sybase /etc/rc2.d/S99sybase\n\n# chkconfig -f sybase on\n\n# Sybase System-wide configuration files\n\nCONFIG=/usr/sybase/sys.config\n\nif $IS_ON verbose ; then # For a verbose startup and shutdown\n\nECHO=echo\n\nVERBOSE=-v\n\nelse # For a quiet startup and shutdown\n\nECHO=:\n\nVERBOSE=\n\nfi\n\ncase \"$1\" in\n\n'start')\n\nif $IS_ON sybase; then\n\nif [ -x $CONFIG/start.sybase ]; then\n\n$ECHO \"starting Sybase servers\"\n\n/bin/su - sybase -c \"$CONFIG/start.sybase $VERBOSE &\"\n\nelse\n\n<error condition>\n\nfi\n\nfi\n\n;;\n\n'stop')\n\nif $IS_ON sybase; then\n\nif [ -x $CONFIG/stop.sybase ]; then\n\n$ECHO \"stopping Sybase servers\"\n\n/bin/su - sybase -c \"$CONFIG/stop.sybase $VERBOSE &\"\n\nelse\n\n<error condition>\n\nfi\n\nfi\n\n;;\n\n*)\n\necho \"usage: $0 {start|stop}\"\n\n;;\n\nesac\n\n/usr/sybase/sys.config/{start,stop}.sybase\n\nstart.sybase\n\n#!/bin/sh -a\n\n#\n\n# Script to start sybase\n\n#\n\n# NOTE: different versions of sybase exist under /usr/sybase/{version}\n\n#\n\n# Determine if we need to spew our output\n\nif [ \"$1\" != \"spew\" ] ; then\n\nOUTPUT=\">/dev/null 2>&1\"\n\nelse\n\nOUTPUT=\"\"\n\nfi\n\n# 10.0.2 servers\n\nHOME=/usr/sybase/10.0.2\n\ncd $HOME\n\n# Start the backup server\n\neval install/startserver -f install/RUN_BU_KEPLER_1002_52_01 $OUTPUT\n\n# Start the dataservers\n\n# Wait two seconds between starts to minimize trauma to CPU server\n\neval install/startserver -f install/RUN_FAC_WWOPR $OUTPUT\n\nsleep 2\n\neval install/startserver -f install/RUN_MAG_LOAD $OUTPUT\n\nexit 0\n\nstop.sybase\n\n#!/bin/sh\n\n#\n\n# Script to stop sybase\n\n#\n\n# Determine if we need to spew our output\n\nif [ -z \"$1\" ] ; then\n\nOUTPUT=\">/dev/null 2>&1\"\n\nelse\n\nOUTPUT=\"-v\"\n\nfi\n\neval killall -15 $OUTPUT dataserver backupserver sybmultbuf\n\nsleep 2\n\n# if they didn't die, kill 'em now...\n\neval killall -9 $OUTPUT dataserver backupserver sybmultbuf\n\nexit 0\n\nIf your platform doesn't support killall, it can easily be simulated as\n\nfollows:\n\n#!/bin/sh\n\n#\n\n# Simple killall simulation...\n\n# $1 = signal\n\n# $2 = process_name\n\n#\n\n#\n\n# no error checking but assume first parameter is signal...\n\n# what ya want for free? :-)\n\n#\n\nkill -$1 `ps -ef | fgrep $2 | fgrep -v fgrep | awk '{ print $1 }'`\n\nBack to top\n\n-------------------------------------------------------------------------------\n\n1.1.3: How do I move tempdb off of the Master Device?\n\n-------------------------------------------------------------------------------\n\nThere used to be a section in the FAQ describing how to drop all of tempdb's\n\ndevices physically from the master device. This can make recovery of the\n\nserver impossible in case of a serious error and so it strongly recommended\n\nthat you do not do this but simply drop the segments as outlined below.\n\nSybase TS Preferred Method of Moving tempdb off the Master Device.\n\nThis is the Sybase TS method of removing most activity from the master device:\n\n1. Alter tempdb on another device:\n\n1> alter database tempdb on ...\n\n2> go\n\n2. Use the tempdb:\n\n1> use tempdb\n\n2> go\n\n3. Drop the segments:\n\n1> sp_dropsegment \"default\", tempdb, master\n\n2> go\n\n1> sp_dropsegment \"logsegment\", tempdb, master\n\n2> go\n\n1> sp_dropsegment \"system\", tempdb, master\n\n2> go\n\nNote that there is still some activity on the master device. On a three\n\nconnection test that I ran:\n\nwhile ( 1 = 1 )\n\nbegin\n\ncreate table #x (col_a int)\n\ndrop table #x\n\nend\n\nthere was one write per second. Not bad.\n\nAn Alternative\n\n(I recently did some bench marks comparing this method, the previous method\n\nand a combination of both. According to sp_sysmon there was no difference\n\nin activity at all. I leave it here just in case it proves useful to\n\nsomeone.)\n\nThe idea of this handy script is to simply fill the first 2MB of tempdb thus\n\neffectively blocking anyone else from using it. The slight gotcha with this\n\nscript, since we're using model, is that all subsequent database creates will\n\nalso have tempdb_filler installed. This is easily remedied by dropping the\n\ntable after creating a new database.\n\nThis script works because tempdb is rebuilt every time the ASE is rebooted.\n\nVery nice trick!\n\n/* this isql script creates a table in the model database. */\n\n/* Since tempdb is created from the model database when the */\n\n/* server is started, this effectively moves the active */\n\n/* portion of tempdb off of the master device. */\n\nuse model\n\ngo\n\n/* note: 2k row size */\n\ncreate table tempdb_filler(\n\na char(255) not null,\n\nb char(255) not null,\n\nc char(255) not null,\n\nd char(255) not null,\n\ne char(255) not null\n\n)\n\ngo\n\n/* insert 1024 rows */\n\ndeclare @i int\n\nselect @i = 1\n\nwhile (@i <= 1024)\n\nbegin\n\ninsert into tempdb_filler values('a','b','c','d','e')\n\nif (@i % 100 = 0) /* dump the transaction every 100 rows */\n\ndump tran model with truncate_only\n\nselect @i=@i+1\n\nend\n\ngo\n\nBack to top\n\n-------------------------------------------------------------------------------\n\n1.1.4: How do I correct timeslice -201\n\n-------------------------------------------------------------------------------\n\n(Note, this procedure is only really necessary with pre-11.x systems. In\n\nsystem 11 systems, these parameters are tunable using sp_configure.)\n\nWhy Increase It?\n\nBasically, it will allow a task to be scheduled onto the CPU for a longer time.\n\nEach task on the system is scheduled onto the CPU for a fixed period of time,\n\ncalled the timeslice, during which it does some work, which is resumed when its\n\nnext turn comes around.\n\nThe process has up until the value of ctimemax (a config block variable) to\n\nfinish its task. As the task is working away, the scheduler counts down\n\nctimemax units. When it gets to the value of ctimemax - 1, if it gets stuck and\n\nfor some reason cannot be taken off the CPU, then a timeslice error gets\n\ngenerated and the process gets infected.\n\nOn the other hand, ASE will allow a server process to run as long as it needs\n\nto. It will not swap the process out for another process to run. The process\n\nwill decide when it is \"done\" with the server CPU. If, however, a process goes\n\non and on and never relinquishes the server CPU, then Server will timeslice the\n\nprocess.\n\nPotential Fix\n\n1. Shutdown the ASE\n\n2. %buildmaster -dyour_device -yctimemax=2000\n\n3. Restart your ASE. If the problem persists contact Sybase Technical Support\n\nnotifying them what you have done already.\n\nBack to top\n\n-------------------------------------------------------------------------------\n\n1.1.5: Certified Sybase Professional\n\n-------------------------------------------------------------------------------\n\nThere have been changes in the process of becoming a Sybase Certified\n\nProfessional. There's a very informative link at http://www.sybase.com/\n\neducation/profcert, Professional Certification.\n\nRob Verschoor has put together some good stuff on his pages ( http://\n\nwww.euronet.nl/~syp_rob/certtips.html) that have pretty much all that you need\n\nto know. He also has a quiz which is intended to test each and everyone's\n\nknowledge of ASE and RepServer.\n\nSybase have released some sample questions (look for them at http://\n\nwww.sybase.com/education/). The GUI requires MS Windows (at the time of\n\nwriting), but they are definitely a sample of what you will be asked. There are\n\nalso a couple of CDs available with yet more questions on them.\n\nThe Certification Kickback\n\nThere have been a couple of articles recently covering the kickback that seems\n\nto be happening as far as certification is concerned. Serveral HR people have\n\nsaid that if a person's CV (resume) is sent in covered in certifications then\n\nit goes straight into the bit bucket. I do not know if this is true or not, but\n\none thing that you might wish to consider is the preparation of two CVs, one\n\nwith certifications, one without. If the job request specifies certification is\n\nnecessary, then send in the appropriate CV. If it does not specifiy\n\ncertification, send in the clean version. If you go into the interview for a\n\njob that did not specify certifications up front and the interviewer starts\n\ngoing about you not being certificated, you simply produce your card as proof.\n\n-------------------------------------------------------------------------------\n\n1.1.6: RAID and Sybase\n\n-------------------------------------------------------------------------------\n\nHere's a short summary of what you need to know about Sybase and RAID.\n\nThe newsgroup comp.arch.storage has a detailed FAQ on RAID, but here are a few\n\ndefinitions:\n\nRAID\n\nRAID means several things at once. It provides increased performance through\n\ndisk striping, and/or resistance to hardware failure through either mirroring\n\n(fast) or parity (slower but cheaper).\n\nRAID 0\n\nRAID 0 is just striping. It allows you to read and write quickly, but provides\n\nno protection against failure.\n\nRAID 1\n\nRAID 1 is just mirroring. It protects you against failure, and generally reads\n\nand writes as fast as a normal disk. It uses twice as many disks as normal (and\n\nsends twice as much data across your SCSI bus, but most machines have plenty of\n\nextra capacity on their SCSI busses.)\n\nSybase mirroring always reads from the primary copy, so it does not\n\nincrease read performance.\n\nRAID 0+1\n\nRAID 0+1 (also called RAID 10) is striping and mirroring together. This gives\n\nyou the highest read and write performance of any of the raid options, but uses\n\ntwice as many disks as normal.\n\nRAID 4/RAID 5\n\nRAID 4 and 5 have disk striping and use 1 extra disk to provide parity. Various\n\nvendors have various optimizations, but this RAID level is generally much\n\nslower at writes than any other kind of RAID.\n\nRAID 7\n\nI am not sure if this is a genuine RAID standard, further checking on your part\n\nis required.\n\nDetails\n\nMost hardware RAID controllers also provide a battery-backed RAM cache for\n\nwriting. This is very useful, because it allows the disk to claim that the\n\nwrite succeeded before it has done anything. If there is a power failure, the\n\ninformation will (hopefully) be written to disk when the power is restored. The\n\ncache is very important because database log writes cause the process doing the\n\nwrites to stop until the write is successful. Systems with write caching thus\n\ncomplete transactions much more quickly than systems without.\n\nWhat RAID levels should my data, log, etc be on? Well, the log disk is\n\nfrequently written, so it should not be on RAID 4 or 5. If your data is\n\ninfrequently written, you could use RAID 4 or 5 for it, because you don't mind\n\nthat writes are slow. If your data is frequently written, you should use RAID\n\n0+1 for it. Striping your data is a very effective way of avoiding any one disk\n\nbecoming a hot-spot. Traditionally Sybase databases were divided among devices\n\nby a human attempting to determine where the hot-spots are. Striping does this\n\nin a straight-forward fashion, and also continues to work if your data access\n\npatterns change.\n\nYour tempdb is data but it is frequently written, so it should not be on RAID 4\n\nor 5.\n\nIf your RAID controller does not allow you to create several different kinds of\n\nRAID volumes on it, then your only hope is to create a huge RAID 0+1 set. If\n\nyour RAID controller does not support RAID 0+1, you shouldn't be using it for\n\ndatabase work.\n\nBack to top\n\n-------------------------------------------------------------------------------\n\n1.1.7: How to swap a db device with another\n\n-------------------------------------------------------------------------------\n\nHere are four approaches. Before attempting any of the following: Backup,\n\nBackup, Backup.\n\nDump and Restore\n\n1. Backup the databases on the device, drop the databases, drop the devices.\n\n2. Rebuild the new devices.\n\n3. Rebuild the databases (Make sure you recreate the fragments correctly - See\n\nEd Barlow's scripts (http://www.tiac.net/users/sqltech/) for an sp that\n\nhelps you do this if you've lost your notes. Failure to do this will\n\npossibly lead to data on log segments and log on data segments).\n\n4. Reload the database dumps!\n\nTwiddle the Data Dictionary - for brave experts only.\n\n1. Shut down the server.\n\n2. Do a physical dump (using dd(1), or such utility) of the device to be\n\nmoved.\n\n3. Load the dump to the new device\n\n4. Edit the data dictionary (sysdevices.physname) to point to the new device.\n\nThe Mirror Trick\n\n1. Create a mirror of the old device, on the new device.\n\n2. Unmirror the primary device, thereby making the _backup_ the primary\n\ndevice.\n\n3. Repeat this for all devices until the old disk is free.\n\ndd (Unix only)\n\n(This option is no use if you need to move a device now, rather if you\n\nanticipate moving a device at some point in the future.)\n\nYou may want to use this approach for creating any database.\n\nCreate (or use) a directory for symbolic links to the devices you wish to use.\n\nThen create your database, but instead of going to /dev/device, go to /\n\ndirectory/symlink - When it comes time to move your devices, you shut down the\n\nserver, simply dd(1) the data from the old device to the new device, recreate\n\nthe symbolic links to the new device and restart the ASE. Simple as that.\n\nBackups are a requisite in all cases, just in case.\n\nBack to top\n\n-------------------------------------------------------------------------------\n\n1.1.8: Server naming and renaming\n\n-------------------------------------------------------------------------------\n\nThere are three totally separate places where ASE names reside, causing much\n\nconfusion.\n\nASE Host Machine interfaces File\n\nA master entry in here for server TEST will provide the network information\n\nthat the server is expected to listen on. The -S parameter to the dataserver\n\nexecutable tells the server which entry to look for, so in the RUN_TEST file,\n\n-STEST will tell the dataserver to look for the entry under TEST in the\n\ninterfaces file and listen on any network parameters specified by 'master'\n\nentries.\n\nTEST\n\nmaster tcp ether hpsrv1 1200\n\nquery tcp ether hpsrv1 1200\n\nNote that preceding the master/query entries there's a tab.\n\nThis is as far as the name TEST is used. Without further configuration the\n\nserver does not know its name is TEST, nor do any client applications.\n\nTypically there will also be query entries under TEST in the local interfaces\n\nfile, and client programs running on the same machine as the server will pick\n\nthis connection information up. However, there is nothing to stop the query\n\nentry being duplicated under another name entirely in the same interfaces file.\n\nARTHUR\n\nquery tcp ether hpsrv1 1200\n\nisql -STEST or isql -SARTHUR will connect to the same server. The name is\n\nsimply a search parameter into the interfaces file.\n\nClient Machine interfaces File\n\nAgain, as the server name specified to the client is simply a search parameter\n\nfor Open Client into the interfaces file, SQL.INI or WIN.INI the name is\n\nlargely irrelevant. It is often set to something that means something to the\n\nusers, especially where they might have a choice of servers to connect to. Also\n\nmultiple query entries can be set to point to the same server, possibly using\n\ndifferent network protocols. eg. if TEST has the following master entries on\n\nthe host machine:\n\nTEST\n\nmaster tli spx /dev/nspx/ \\xC12082580000000000012110\n\nmaster tcp ether hpsrv1 1200\n\nThen the client can have a meaningful name:\n\nACCOUNTS_TEST_SERVER\n\nquery tcp ether hpsrv1 1200\n\nor alternative protocols:\n\nTEST_IP\n\nquery tcp ether hpsrv1 1200\n\nTEST_SPX\n\nquery tli spx /dev/nspx/ \\xC12082580000000000012110\n\nsysservers\n\nThis system table holds information about remote ASEs that you might want to\n\nconnect to, and also provides a method of naming the local server.\n\nEntries are added using the sp_addserver system procedure - add a remote server\n\nwith this format:\n\nsp_addserver server_name, null, network_name\n\nserver_name is any name you wish to refer to a remote server by, but\n\nnetwork_name must be the name of the remote server as referenced in the\n\ninterfaces file local to your local server. It normally makes sense to make the\n\nserver_name the same as the network_name, but you can easily do:\n\nsp_addserver LIVE, null, ACCTS_LIVE\n\nWhen you execute for example, exec LIVE.master..sp_helpdb the local ASE will\n\ntranslate LIVE to ACCTS_LIVE and try and talk to ACCTS_LIVE via the ACCTS_LIVE\n\nentry in the local interfaces file.\n\nFinally, a variation on the sp_addserver command:\n\nsp_addserver LOCALSRVNAME, local\n\nnames the local server (after a restart). This is the name the server reports\n\nin the errorlog at startup, the value returned by @@SERVERNAME, and the value\n\nplaced in Open Client server messages. It can be completely different from the\n\nnames in RUN_SRVNAME or in local or remote interfaces - it has no bearing on\n\nconnectivity matters.\n\nBack to top\n\n-------------------------------------------------------------------------------\n\n1.1.9: How do I interpret the tli strings in the interface file?\n\n-------------------------------------------------------------------------------\n\nThe tli string contained with Solaris interface files is a hex string\n\ncontaining port and IP address. If you have an entry\n\nSYBSRVR\n\nmaster tli tcp /dev/tcp \\x000204018196c4510000000000000000\n\nThen it can be interpreted as follows:\n\nx0002 no user interpretation (header info?)\n\n0401 port number (1025 decimal)\n\n81 first part of IP address (129 decimal)\n\n96 second part of IP address (150 decimal)\n\nc4 third part of IP address (196 decimal)\n\n51 fourth part of IP address (81 decimal)\n\nSo, the above tli address is equivalent to\n\nSYBSRVR\n\nmaster tcp ether sybhost 1025\n\nwhere sybhost's IP address is 129.150.196.81.\n\nThe following piece of Sybperl (courtesy of Michael Peppler) takes a tli entry\n\nand returns the IP address and port number for each server in a Solaris'\n\ninterfaces file.\n\n#!/usr/local/bin/perl -w\n\nuse strict;\n\nmy $server;\n\nmy @dat;\n\nmy ($port, $ip);\n\nwhile(<>) {\n\nnext if /^\\s*$/;\n\nnext if /^\\s*\\#/;\n\nchomp;\n\nif(/^\\w/) {\n\n$server = $_;\n\n$server =~ s/\\s*$//;\n\nnext;\n\n}\n\n@dat = split(' ', $_);\n\n($port, $ip) = parseAddress($dat[4]);\n\nprint \"$server - $dat[0] on port $port, host $ip\\n\";\n\n}\n\nsub parseAddress {\n\nmy $addr = shift;\n\nmy $port;\n\nmy $ip;\n\nmy (@arr) = (hex(substr($addr, 10, 2)),\n\nhex(substr($addr, 12, 2)),\n\nhex(substr($addr, 14, 2)),\n\nhex(substr($addr, 16, 2)));\n\n$port = hex(substr($addr, 6, 4));\n\n$ip = join('.', @arr);\n\n($port, $ip);\n\n}\n\nBack to top\n\n-------------------------------------------------------------------------------\n\n1.1.10: How can I tell the datetime my Server started?\n\n-------------------------------------------------------------------------------\n\nMethod #1\n\nThe normal way would be to look at the errorlog, but this is not always\n\nconvenient or even possible. From a SQL session you find out the server startup\n\ntime to within a few seconds using:\n\nselect \"Server Start Time\" = crdate\n\nfrom master..sysdatabases\n\nwhere name = \"tempdb\"\n\nMethod #2\n\nAnother useful query is:\n\nselect * from sysengines\n\nwhich gives the address and port number at which the server is listening.\n\nBack to top\n\n-------------------------------------------------------------------------------\n\n1.1.11: Raw partitions or regular files?\n\n-------------------------------------------------------------------------------\n\nHmmm... as always, this answer depends on the vendor's implementation on a\n\ncooked file system for the ASE...\n\nPerformance Hit (synchronous vs asynchronous)\n\nIf on this platform, the ASE performs file system I/O synchronously then the\n\nASE is blocked on the read/write and throughput is decreased tremendously.\n\nThe way the ASE typically works is that it will issue an I/O (read/write) and\n\nsave the I/O control block and continue to do other work (on behalf of other\n\nconnections). It'll periodically poll the workq's (network, I/O) and resume\n\nconnections when their work has completed (I/O completed, network data\n\nxmit'd...).\n\nPerformance Hit (bcopy issue)\n\nAssuming that the file system I/O is asynchronous (this can be done on SGI), a\n\nperformance hit may be realized when bcopy'ing the data from kernel space to\n\nuser space.\n\nCooked I/O typically (again, SGI has something called directed I/O which allows\n\nI/O to go directly to user space) has to go from disk, to kernel buffers and\n\nfrom kernel buffers to user space; on a read. The extra layer with the kernel\n\nbuffers is inherently slow. The data is moved from kernel buffers to/from user\n\nspace using bcopy(). On small operations this typically isn't that much of an\n\nissue but in a RDBMS scenario the bcopy() layer is a significant performance\n\nhit because it's done so often...\n\nPerformance Gain!\n\nIt's true, using file systems, at times you can get performance gains assuming\n\nthat the ASE on your platform does the I/O asynchronously (although there's a\n\ncaveat on this too... I'll cover that later on).\n\nIf your machine has sufficient memory and extra CPU capacity, you can realize\n\nsome gains by having writes return immediately because they're posted to\n\nmemory. Reads will gain from the anticipatory fetch algorithm employed by most\n\nO/S's.\n\nYou'll need extra memory to house the kernel buffered data and you'll need\n\nextra CPU capacity to allow bdflush() to write the dirty data out to disk...\n\neventually... but with everything there's a cost: extra memory and free CPU\n\ncycles.\n\nOne argument is that instead of giving the O/S the extra memory (by leaving it\n\nfree) to give it to the ASE and let it do its caching... but that's a different\n\nthread...\n\nData Integrity and Cooked File System\n\nIf the Sybase ASE is not certified to be used over a cooked file system,\n\nbecause of the nature of the kernel buffering (see the section above) you may\n\nface database corruption by using cooked file system anyway. The ASE thinks\n\nthat it has posted its changes out to disk but in reality it has gone only to\n\nmemory. If the machine halts without bdflush() having a chance to flush memory\n\nout to disk, your database may become corrupted.\n\nSome O/S's allow cooked files to have a write through mode and it really\n\ndepends if the ASE has been certified on cooked file systems. If it has, it\n\nmeans that when the ASE opens a device which is on a file system, it fcntl()'s\n\nthe device to write-through.\n\nWhen to use cooked file system?\n\nI typically build my tempdb on cooked file system and I don't worry about data\n\nintegrity because tempdb is rebuilt every time your ASE/SQL Server is rebooted.\n\nBack to top\n\n-------------------------------------------------------------------------------\n\n1.1.12: Is Sybase Y2K (Y2000) compliant?\n\n-------------------------------------------------------------------------------\n\nSybase is year 2000 compliant at specific revisions of each product. Full\n\ndetails are available at http://www.sybase.com, specifically (as these links\n\nwill undoubtedly change):\n\nhttp://www.sybase.com/success/inc/corpinfo/year2000_int.html\n\nhttp://www.sybase.com/Company/corpinfo/year2000_matrix.html\n\nNote: Since we have made it to 2000 more or less intact, I see no reason to\n\ninclude this question. I plan to remove with the next release of the FAQ. If\n\nyou feel strongly about leaving it in then let me know.\n\nBack to top\n\n-------------------------------------------------------------------------------\n\n1.1.13 How Can I Run the ASE Upgrade Manually?\n\n-------------------------------------------------------------------------------\n\nHow to Run the ASE Upgrade Manually\n\nThis document describes the steps required to perform a manual upgrade for ASE\n\nfrom release 4.x or 10.0x to release 11.02. In most cases, however, you should\n\nuse sybinit to perform the upgrade.\n\nBE SURE TO HAVE GOOD BACKUPS BEFORE STARTING THIS PROCEDURE.\n\n1. Use release 11.0x sybinit to run the pre-eligibility test and Check\n\nReserved words. Make any necessary changes that are mentioned in the\n\nsybinit log. The sybinit log is located in $SYBASE/init/logs/logxxxx.yyy.\n\n2. Use isql to connect to the 4.x or 10.0x ASE and do the following tasks:\n\na. Turn on option to allow updates to system tables:\n\n1> sp_configure \"allow updates\", 1\n\n2> go\n\nb. Checkpoint all databases:\n\n1> use \"dbname\"\n\n2> go\n\n1> checkpoint\n\n2> go\n\nc. Shutdown the 4.x or 10.0x ASE.\n\n1> shutdown\n\n2> go\n\n3. Copy the interfaces file to the release 11.0x directory.\n\n4. Set the environment variable SYBASE to the release 11.0x directory.\n\n5. Copy the runserver file to the release 11.0x $SYBASE/install directory.\n\n6. Edit the $SYBASE/install/RUN_SYBASE (runserver file) to change the path\n\nfrom the 4.x or 10.x dataserver directory to the new release 11.0x\n\ndirectory.\n\n7. Start ASE using the new runserver file.\n\n% startserver -f$SYBASE/install/RUN_SYBASE\n\n8. Run the upgrade program:\n\nUNIX: $SYBASE/upgrade/upgrade -S\"servername\" -P\"sapassword\" > $SYBASE/init/\n\nlogs/mylog.log 2>&1 VMS: SYBASE_SYSTEM[SYBASE.UPGRADE]upgrade /password=\n\n\"sa_password\" /servername=\"servername\"\n\n9. Shut down SQL server after a successful upgrade.\n\n% isql -Usa -Pxxx\n\n-SSYBASE\n\n1> shutdown\n\n2> go\n\n10. Start ASE using the release 11.0x runserver file.\n\n% startserver -f$SYBASE/install/RUN_SYBASE\n\n11. Create the sybsystemprocs device and database if upgrading from 4.9.x. You\n\nshould create a 21mb sybsystemprocs device and database.\n\na. Use the disk init command to create the sybsytemprocs device and\n\ndatabase manually, for example:\n\ndisk init name = \"sybprocsdev\", physname=\"/dev/sybase/rel1102/\n\nsybsystemprocs.dat\", vdevno=4, size=10752 go To check to see which vdevno\n\nis available: type 1> select distinct low/16777216 from sysdevices 2> order\n\nby low 3> go A sample create database command: create database\n\nsybsystemprocs on sybprocsdev=21 go Please refer to the \"Sybase ASE\n\nReference Manual\", for more information on these commands.\n\n12. Run the installmaster and installmodel scripts:\n\nUNIX: %isql -Usa -Psapassword -i$SYBASE/scripts/installmaster\n\nUNIX: %isql -Usa -Psapassword -i$SYBASE/scripts/installmodel\n\nVMS: $isql /user=\"sa\" /password=\"sapass\"\n\n/input=\"[sybase_system.scripts]installm aster\"\n\nVMS: $isql /user=\"sa\" /password=\"sapass\"\n\n/input=\"[sybase_system.scripts]installm odel\"\n\n13. If you upgraded from ASE 4.9.2, you will need to run sp_remap to remap the\n\ncompiled objects. Sp_remap remaps stored procedures, triggers, rules,\n\ndefaults, or views to be compatible with the current release of ASE. Please\n\nrefer to the Reference Manual Volume II for more information on the\n\nsp_remap command.\n\nThe syntax for sp_remap:\n\nsp_remap object_name\n\nIf you are upgrading to ASE 11.0.x and the upgrade process failed when using\n\nsybinit, you can invoke sybinit and choose remap query tress from the upgrade\n\nmenu screen. This is a new option that is added, after a failed upgrade.\n\nBack to top\n\n-------------------------------------------------------------------------------\n\n1.1.14 We have lost the sa password, what can we do?\n\n-------------------------------------------------------------------------------\n\nRemember Douglas Adams famous quote \"Don't panic\" is the first thing!\n\nI know that most people use the 'sa' account all of the time, which is fine if\n\nthere is only ever one dba administering the system. If you have more than one\n\nperson accessing the server using the 'sa' account, consider using sa_role\n\nenabled accounts and disabling the 'sa' account. Funnily enough, this is\n\nobviously what Sybase think because it is one of the questions in the\n\ncertification exams.\n\nIf you see that someone is logged using the 'sa' account or is using an account\n\nwith 'sa_role' enabled, then you can do the following:\n\nsp_configure \"allow updates to system tables\",1\n\ngo\n\nupdate syslogins set password=null where name = 'sa'\n\ngo\n\nsp_password null,newPassword\n\ngo\n\nYou must rememeber to reset the password before exiting isql or sqsh. I thought\n\nthat setting it to null would be enough, and exited isql thinking that I would\n\nbe able to get in with a null password. Take it from me that the risk is not\n\nworth it. It failed for me and I had to kill the dataserver and get a new\n\npassword. I just tried the above method and it works fine.\n\nIf you have a user with sso_role enabled, login with that account and change\n\nthe 'sa' password that way. It is often a good idea to have a separate site\n\nsecurity officer, just to get you out of this sticky situation. Certainly stops\n\nyou looking an idiot in managements eyes for having to reboot production\n\nbecause you have locked yourself out!\n\nOK, so we have got to the point where there are no accounts with sufficient\n\npriviledges to allow you to change the 'sa' account password. (You are sure\n\nabout that, since the next part can cause data loss, so have another quick\n\nlook.) We now need to some more drastic stuff.\n\nIf the server is actually running, then you need to stop it. We know that the\n\nonly accounts that can stop the server in a nice manner are not available, so\n\nit has to be some sort of kill. You can try:\n\nkill -SIGTERM\n\nor\n\nkill -15\n\n(they are identical) which is designed to be caught by ASE, which then performs\n\nthe equivalent of shutdown with nowait. If ASE does not die, and you should\n\ngive it a little while to catch and act on the signal, then you might have to\n\ntry other measures, which is probably kill -9. Note that if you have tables\n\nwith identity columns, most of these will jump alarmingly, unless you are using\n\nASE 12.5 and the identity interval is set to 1.\n\nOnce down, edit the RUN_SERVER file ( RUN_SERVER.bat on NT) and add \"-psa\" (it\n\nis important not to leave a space between the\"-p\" and the \"sa\", and that it is\n\nall lower-case) to the end of the dataserver or sqlsrvr.exe line. You will end\n\nup with a file that looks a bit like:\n\n#!/bin/sh\n\n#\n\n# Adaptive Server name: N_UTSIRE\n\n# Master device path: /data/sybase/databases/N_UTSIRE/master.dat\n\n# Error log path: /opt/sybase-11.9.2/install/N_UTSIRE.log\n\n# Directory for shared memory files: /opt/sybase-11.9.2\n\n#\n\n# Regenerate sa password -psa\n\n#\n\n/opt/sybase-11.9.2/bin/dataserver \\\n\n-sN_UTSIRE \\\n\n-d/data/sybase/databases/N_UTSIRE/master.dat \\\n\n-e/opt/sybase-11.9.2/install/N_UTSIRE.log \\\n\n-M/opt/sybase-11.9.2 -psa \\\n\n(I add the line mentioning the regenerate, so that if I need to do this in a\n\nmoment of extreme pressure it is there in front of my nose.\n\nNow, start the server again and you should see the following on the screen:\n\n00:00000:00001:2001/05/26 18:29:21.39 server 'bin_iso_1' (ID = 50)\n\n00:00000:00001:2001/05/26 18:29:21.39 server on top of default character set:\n\n00:00000:00001:2001/05/26 18:29:21.39 server 'iso_1' (ID = 1).\n\n00:00000:00001:2001/05/26 18:29:21.39 server Loaded default Unilib conversion handle.\n\nNew SSO password for sa:tmfyrkdwpibung\n\nNote that it is not written to the log file, so keep your eyes peeled.\n\nOn NT you will have to start the server from the command line and not use\n\nSybase Central or the control panel.\n\nObviously, you will want to change the password to something much more\n\nmemorable as soon as possible.\n\nRemember to remove the \"-psa\" from the \"RUN\" file before you start the server\n\nagain or else the password will be changed again for you.\n\nBack to top\n\n-------------------------------------------------------------------------------\n\n1.1.15 How do I set a password to be null?\n\n-------------------------------------------------------------------------------\n\nSince ASE 11 (I cannot remember if it was with the very first release of 11,\n\nbut certainly not before) the password column in syslogins has been encrypted.\n\nSetting this column to NULL does not equate to that login having a NULL\n\npassword. A NULL password still requires the correct binary string to be in\n\nplace.\n\nIn release 12 and above, set the minimum password length to be 0 using\n\nsp_configure and give that account a null password, and all should be fine.\n\nBefore 12, it is not possible to set the minimum password length, so the direct\n\napproach is not possible. So, update the relevant record in syslogins setting\n\nthe password column to be the same as that of an account with a NULL password\n\nalready.\n\nHow does one get the correct binary value? When a new ASE is built, the 'sa'\n\naccount has a NULL password to start with. Setting an account to have the same\n\nbinary value as such an 'sa' account should work. Remember that the binary\n\nstring is going to be specific to the operating system and the exact release of\n\nASE etc. Obviously, if you have set the password of your 'sa' accounts to be\n\nsomething other than NULL (sensible move), then you are going to have to build\n\nyourself a dummy server just to get the correct string. If this is important to\n\nyou, then you may wish to store the value somewhere safe once you have\n\ngenerated it.\n\nYet another method would be to simply insert the correct hex string into the\n\npassword column. Rob Verschoor has a very nice stored proc on his site called\n\nsp_blank_password to allow you to do just this. Go to http://www.sypron.nl/\n\nblankpwd.html .\n\nBack to top\n\n-------------------------------------------------------------------------------\n\n1.1.16: Does Sybase support Row Level Locking?\n\n-------------------------------------------------------------------------------\n\nWith Adaptive Server Enterprise 11.9 Sybase introduced row level locking into\n\nits product. In fact it went further than that, it introduced 3 different\n\nlocking levels:\n\n* All Pages Locking\n\nThis is the scheme that is implemented in all servers prior to 11.9. Here\n\nlocks are taken out at the page level, which may included many rows. The\n\nname refers to the fact that all of the pages in any data manipulation\n\nstatement are locked, both data and index.\n\n* Data Page Locking\n\nThe other two locking schemes are bundled together under the title Data\n\nPage Locking, refering to the fact that only data pages are ever locked in\n\nthe conventional sense. Data Page Locking is divided into two categories\n\n+ Data Only Locking\n\nThis locking scheme still locks a page at a time, including all of the\n\nrows contained within that page, but uses a new mechanism, called\n\nlatches, to lock index pages for the shortest amount of time. One of\n\nthe consequences of this scheme is that it does not update index\n\npages. In order to support this Sybase has introduced a new concept,\n\nforwarded rows. These are rows that have had to move because they have\n\ngrown beyond space allowed for them on the page they were created. 2002\n\nbytes per page.\n\n+ Row Level Locking\n\nJust as it sounds, the lock manager only locks the row involved in the\n\noperation.\n\nBack to top\n\n-------------------------------------------------------------------------------\n\n1.1.17: What platforms does ASE run on?\n\n-------------------------------------------------------------------------------\n\nSybase has an excellent lookup page that tells you all of the releases that\n\nSybase has certifies as running on a particular platform. Got to http://\n\nohno.sybase.com/cgi-bin/ws.exe/cert/ase_cert.hts .\n\nBack to top\n\n-------------------------------------------------------------------------------\n\n1.1.18: How do I backup databases > 64G on ASE prior to 12.x?\n\n-------------------------------------------------------------------------------\n\nAs you are all well aware, prior to version of ASE 12, dumping large databases\n\nwas a real pain. Tape was the only option for anything greater than 64 gig.\n\nThis was because only 32 dump devices, or stripes, were supported, and since\n\nfile based stripes were restricted to no more than 2 gig, the total amount of\n\ndata that could be dumped was <= 32 * 2 = 64G.\n\nWith the introduction of ASE 12, the number of stripes was increased\n\nBack to top\n\n-------------------------------------------------------------------------------\n\nUser Database Administration # ASE FAQ\n\nDavid Owen\n\nArchive-name: databases/sybase-faq/part5\n\nURL: http://www.isug.com/Sybase_FAQ\n\nVersion: 1.7\n\nMaintainer: David Owen\n\nLast-modified: 2003/03/02\n\nPosting-Frequency: posted every 3rd month\n\nA how-to-find-the-FAQ article is posted on the intervening months.\n\nUser Database Administration\n\n1.2.1 Changing varchar(m) to varchar(n)\n\n1.2.2 Frequently asked questions on Table partitioning\n\n1.2.3 How do I manually drop a table?\n\n1.2.4 Why not create all my columns varchar(255)?\n\n1.2.5 What's a good example of a transaction?\n\n1.2.6 What's a natural key?\n\n1.2.7 Making a Stored Procedure invisible\n\n1.2.8 Saving space when inserting rows monotonically\n\n1.2.9 How to compute database fragmentation\n\n1.2.10 Tasks a DBA should do...\n\n1.2.11 How to implement database security\n\n1.2.12 How to shrink a database\n\n1.2.13 How do I turn on auditing of all SQL text sent to the server\n\n1.2.14 sp_helpdb/sp_helpsegment is returning negative numbers\n\nAdvanced Administration Basic Administration ASE FAQ\n\n-------------------------------------------------------------------------------\n\n1.2.1: Changing varchar(m) to varchar(n)\n\n-------------------------------------------------------------------------------\n\nBefore you start:\n\nselect max(datalength(column_name))\n\nfrom affected_table\n\nIn other words, please be sure you're going into this with your head on\n\nstraight.\n\nHow To Change System Catalogs\n\nThis information is Critical To The Defense Of The Free World, and you would be\n\nWell Advised To Do It Exactly As Specified:\n\nuse master\n\ngo\n\nsp_configure \"allow updates\", 1\n\ngo\n\nreconfigure with override /* System 10 and below */\n\ngo\n\nuse victim_database\n\ngo\n\nselect name, colid\n\nfrom syscolumns\n\nwhere id = object_id(\"affected_table\")\n\ngo\n\nbegin tran\n\ngo\n\nupdate syscolumns\n\nset length = new_value\n\nwhere id = object_id(\"affected_table\")\n\nand colid = value_from_above\n\ngo\n\nupdate sysindexes\n\nset maxlen = maxlen + increase/decrease?\n\nwhere id=object_id(\"affected_table\")\n\nand indid = 0\n\ngo\n\n/* check results... cool? Continue... else rollback tran */\n\ncommit tran\n\ngo\n\nuse master\n\ngo\n\nsp_configure \"allow updates\", 0\n\ngo\n\nreconfigure /* System 10 and below */\n\ngo\n\nReturn to top\n\n-------------------------------------------------------------------------------\n\n1.2.2: FAQ on partitioning\n\n-------------------------------------------------------------------------------\n\nIndex of Sections\n\n* What Is Table Partitioning?\n\n+ Page Contention for Inserts\n\n+ I/O Contention\n\n+ Caveats Regarding I/O Contention\n\n* Can I Partition Any Table?\n\n+ How Do I Choose Which Tables To Partition?\n\n* Does Table Partitioning Require User-Defined Segments?\n\n* Can I Run Any Transact-SQL Command on a Partitioned Table?\n\n* How Does Partition Assignment Relate to Transactions?\n\n* Can Two Tasks Be Assigned to the Same Partition?\n\n* Must I Use Multiple Devices to Take Advantage of Partitions?\n\n* How Do I Create A Partitioned Table That Spans Multiple Devices?\n\n* How Do I Take Advantage of Table Partitioning with bcp in?\n\n* Getting More Information on Table Partitioning\n\nWhat Is Table Partitioning?\n\nTable partitioning is a procedure that creates multiple page chains for a\n\nsingle table.\n\nThe primary purpose of table partitioning is to improve the performance of\n\nconcurrent inserts to a table by reducing contention for the last page of a\n\npage chain.\n\nPartitioning can also potentially improve performance by making it possible to\n\ndistribute a table's I/O over multiple database devices.\n\nPage Contention for Inserts\n\nBy default, ASE stores a table's data in one double-linked set of pages called\n\na page chain. If the table does not have a clustered index, ASE makes all\n\ninserts to the table in the last page of the page chain.\n\nWhen a transaction inserts a row into a table, ASE holds an exclusive page lock\n\non the last page while it inserts the row. If the current last page becomes\n\nfull, ASE allocates and links a new last page.\n\nAs multiple transactions attempt to insert data into the table at the same\n\ntime, performance problems can occur. Only one transaction at a time can obtain\n\nan exclusive lock on the last page, so other concurrent insert transactions\n\nblock each other.\n\nPartitioning a table creates multiple page chains (partitions) for the table\n\nand, therefore, multiple last pages for insert operations. A partitioned table\n\nhas as many page chains and last pages as it has partitions.\n\nI/O Contention\n\nPartitioning a table can improve I/O contention when ASE writes information in\n\nthe cache to disk. If a table's segment spans several physical disks, ASE\n\ndistributes the table's partitions across fragments on those disks when you\n\ncreate the partitions.\n\nA fragment is a piece of disk on which a particular database is assigned space.\n\nMultiple fragments can sit on one disk or be spread across multiple disks.\n\nWhen ASE flushes pages to disk and your fragments are spread across different\n\ndisks, I/Os assigned to different physical disks can occur in parallel.\n\nTo improve I/O performance for partitioned tables, you must ensure that the\n\nsegment containing the partitioned table is composed of fragments spread across\n\nmultiple physical devices.\n\nCaveats Regarding I/O Contention\n\nBe aware that when you use partitioning to balance I/O you run the risk of\n\ndisrupting load balancing even as you are trying to achieve it. The following\n\nscenarios can keep you from gaining the load balancing benefits you want:\n\n* You are partitioning an existing table. The existing data could be sitting\n\non any fragment. Because partitions are randomly assigned, you run the risk\n\nof filling up a fragment. The partition will then steal space from other\n\nfragments, thereby disrupting load balancing.\n\n* Your fragments differ in size.\n\n* The segment maps are configured such that other objects are using the\n\nfragments to which the partitions are assigned.\n\n* A very large bcp job inserts many rows within a single transaction. Because\n\na partition is assigned for the lifetime of a transaction, a huge amount of\n\ndata could go to one particular partition, thus filling up the fragment to\n\nwhich that partition is assigned.\n\nCan I Partition Any Table?\n\nNo. You cannot partition the following kinds of tables:\n\n1. Tables with clustered indexes (as of release 11.5 it is possible to have a\n\nclustered index on a partitioned table)\n\n2. ASE system tables\n\n3. Work tables\n\n4. Temporary tables\n\n5. Tables that are already partitioned. However, you can unpartition and then\n\nre-partition tables to change the number of partitions.\n\nHow Do I Choose Which Tables To Partition?\n\nYou should partition heap tables that have large amounts of concurrent insert\n\nactivity. (A heap table is a table with no clustered index.) Here are some\n\nexamples:\n\n1. An \"append-only\" table to which every transaction must write\n\n2. Tables that provide a history or audit list of activities\n\n3. A new table into which you load data with bcp in. Once the data is loaded\n\nin, you can unpartition the table. This enables you to create a clustered\n\nindex on the table, or issue other commands not permitted on a partition\n\ntable.\n\nDoes Table Partitioning Require User-Defined Segments?\n\nNo. By design, each table is intrinsically assigned to one segment, called the\n\ndefault segment. When a table is partitioned, any partitions on that table are\n\ndistributed among the devices assigned to the default segment.\n\nIn the example under \"How Do I Create A Partitioned Table That Spans Multiple\n\nDevices?\", the table sits on a user-defined segment that spans three devices.\n\nCan I Run Any Transact-SQL Command on a Partitioned Table?\n\nNo. Once you have partitioned a table, you cannot use any of the following\n\nTransact-SQL commands on the table until you unpartition it:\n\n1. drop table\n\n2. sp_placeobject\n\n3. truncate table\n\n4. alter table table_name partition n\n\nOn releases of ASE prior to 11.5 it was not possible to create a clustered\n\nindex on a partitioned table either.\n\nHow Does Partition Assignment Relate to Transactions?\n\nA user is assigned to a partition for the duration of a transaction. Assignment\n\nof partitions resumes with the first insert in a new transaction. The user\n\nholds the lock, and therefore partition, until the transaction ends.\n\nFor this reason, if you are inserting a great deal of data, you should batch it\n\ninto separate jobs, each within its own transaction. See \"How Do I Take\n\nAdvantage of Table Partitioning with bcp in?\", for details.\n\nCan Two Tasks Be Assigned to the Same Partition?\n\nYes. ASE randomly assigns partitions. This means there is always a chance that\n\ntwo users will vie for the same partition when attempting to insert and one\n\nwould lock the other out.\n\nThe more partitions a table has, the lower the probability of users trying to\n\nwrite to the same partition at the same time.\n\nMust I Use Multiple Devices to Take Advantage of Partitions?\n\nIt depends on which type of performance improvement you want.\n\nTable partitioning improves performance in two ways: primarily, by decreasing\n\npage contention for inserts and, secondarily, by decreasing i/o contention.\n\n\"What Is Table Partitioning?\" explains each in detail.\n\nIf you want to decrease page contention you do not need multiple devices. If\n\nyou want to decrease i/o contention, you must use multiple devices.\n\nHow Do I Create A Partitioned Table That Spans Multiple Devices?\n\nCreating a partitioned table that spans multiple devices is a multi-step\n\nprocedure. In this example, we assume the following:\n\n* We want to create a new segment rather than using the default segment.\n\n* We want to spread the partitioned table across three devices, data_dev1,\n\ndata_dev2, and data_dev3.\n\nHere are the steps:\n\n1. Define a segment:\n\nsp_addsegment newsegment, my_database,data_dev1\n\n2. Extend the segment across all three devices:\n\nsp_extendsegment newsegment, my_database, data_dev2\n\nsp_extendsegment newsegment, my_database, data_dev3\n\n3. Create the table on the segment:\n\ncreate table my_table\n\n(names, varchar(80) not null)\n\non newsegment\n\n4. Partition the table:\n\nalter table my_table partition 30\n\nHow Do I Take Advantage of Table Partitioning with bcp in?\n\nYou can take advantage of table partitioning with bcp in by following these\n\nguidelines:\n\n1. Break up the data file into multiple files and simultaneously run each of\n\nthese files as a separate bcp job against one table.\n\nRunning simultaneous jobs increases throughput.\n\n2. Choose a number of partitions greater than the number of bcp jobs.\n\nHaving more partitions than processes (jobs) decreases the probability of\n\npage lock contention.\n\n3. Use the batch option of bcp in. For example, after every 100 rows, force a\n\ncommit. Here is the syntax of this command:\n\nbcp table_name in filename -b100\n\nEach time a transaction commits, ASE randomly assigns a new partition for\n\nthe next insert. This, in turn, reduces the probability of page lock\n\ncontention.\n\nGetting More Information on Table Partitioning\n\nFor more information on table partitioning, see the chapter on controlling\n\nphysical data placement in the ASE Performance and Tuning Guide.\n\nReturn to top\n\n-------------------------------------------------------------------------------\n\n1.2.3: How to manually drop a table\n\n-------------------------------------------------------------------------------\n\nOccasionally you may find that after issuing a drop table command that the ASE\n\ncrashed and consequently the table didn't drop entirely. Sure you can't see it\n\nbut that sucker is still floating around somewhere.\n\nHere's a list of instructions to follow when trying to drop a corrupt table:\n\n1. sp_configure allow, 1\n\ngo\n\nreconfigure with override\n\ngo\n\n2. Write db_id down.\n\nuse db_name\n\ngo\n\nselect db_id()\n\ngo\n\n3. Write down the id of the bad_table:\n\nselect id\n\nfrom sysobjects\n\nwhere name = bad_table_name\n\ngo\n\n4. You will need these index IDs to run dbcc extentzap. Also, remember that if\n\nthe table has a clustered index you will need to run extentzap on index\n\n\"0\", even though there is no sysindexes entry for that indid.\n\nselect indid\n\nfrom sysindexes\n\nwhere id = table_id\n\ngo\n\n5. This is not required but a good idea:\n\nbegin transaction\n\ngo\n\n6. Type in this short script, this gets rid of all system catalog information\n\nfor the object, including any object and procedure dependencies that may be\n\npresent.\n\nSome of the entries are unnecessary but better safe than sorry.\n\ndeclare @obj int\n\nselect @obj = id from sysobjects where name =\n\ndelete syscolumns where id = @obj\n\ndelete sysindexes where id = @obj\n\ndelete sysobjects where id = @obj\n\ndelete sysprocedures where id in\n\n(select id from sysdepends where depid = @obj)\n\ndelete sysdepends where depid = @obj\n\ndelete syskeys where id = @obj\n\ndelete syskeys where depid = @obj\n\ndelete sysprotects where id = @obj\n\ndelete sysconstraints where tableid = @obj\n\ndelete sysreferences where tableid = @obj\n\ndelete sysdepends where id = @obj\n\ngo\n\n7. Just do it!\n\ncommit transaction\n\ngo\n\n8. Gather information to run dbcc extentzap:\n\nuse master\n\ngo\n\nsp_dboption db_name, read, true\n\ngo\n\nuse db_name\n\ngo\n\ncheckpoint\n\ngo\n\n9. Run dbcc extentzap once for each index (including index 0, the data level)\n\nthat you got from above:\n\nuse master\n\ngo\n\ndbcc traceon (3604)\n\ngo\n\ndbcc extentzap (db_id, obj_id, indx_id, 0)\n\ngo\n\ndbcc extentzap (db_id, obj_id, indx_id, 1)\n\ngo\n\nNotice that extentzap runs twice for each index. This is because the\n\nlast parameter (the sort bit) might be 0 or 1 for each index, and you\n\nwant to be absolutely sure you clean them all out.\n\n10. Clean up after yourself.\n\nsp_dboption db_name, read, false\n\ngo\n\nuse db_name\n\ngo\n\ncheckpoint\n\ngo\n\nsp_configure allow, 0\n\ngo\n\nreconfigure with override\n\ngo\n\nReturn to top\n\n-------------------------------------------------------------------------------\n\n1.2.4: Why not max out all my columns?\n\n-------------------------------------------------------------------------------\n\nPeople occasionally ask the following valid question:\n\nSuppose I have varying lengths of character strings none of which should\n\nexceed 50 characters.\n\nIs there any advantage of last_name varchar(50) over this last_name varchar\n\n(255)?\n\nThat is, for simplicity, can I just define all my varying strings to be\n\nvarchar(255) without even thinking about how long they may actually be? Is\n\nthere any storage or performance penalty for this.\n\nThere is no performance penalty by doing this but as another netter pointed\n\nout:\n\nIf you want to define indexes on these fields, then you should specify the\n\nsmallest size because the sum of the maximal lengths of the fields in the\n\nindex can't be greater than 256 bytes.\n\nand someone else wrote in saying:\n\nYour data structures should match the business requirements. This way the\n\ndata structure themselves becomes a data dictionary for others to model\n\ntheir applications (report generation and the like).\n\nReturn to top\n\n-------------------------------------------------------------------------------\n\n1.2.5: What's a good example of a transaction?\n\n-------------------------------------------------------------------------------\n\nThis answer is geared for Online Transaction Processing (OTLP)\n\napplications.\n\nTo gain maximum throughput all your transactions should be in stored procedures\n\n- see Q1.5.8. The transactions within each stored procedure should be short and\n\nsimple. All validation should be done outside of the transaction and only the\n\nmodification to the database should be done within the transaction. Also, don't\n\nforget to name the transaction for sp_whodo - see Q9.2.\n\nThe following is an example of a good transaction:\n\n/* perform validation */\n\nselect ...\n\nif ... /* error */\n\n/* give error message */\n\nelse /* proceed */\n\nbegin\n\nbegin transaction acct_addition\n\nupdate ...\n\ninsert ...\n\ncommit transaction acct_addition\n\nend\n\nThe following is an example of a bad transaction:\n\nbegin transaction poor_us\n\nupdate X ...\n\nselect ...\n\nif ... /* error */\n\n/* give error message */\n\nelse /* proceed */\n\nbegin\n\nupdate ...\n\ninsert ...\n\nend\n\ncommit transaction poor_us\n\nThis is bad because:\n\n* the first update on table X is held throughout the transaction. The idea\n\nwith OLTP is to get in and out fast.\n\n* If an error message is presented to the end user and we await their\n\nresponse, we'll maintain the lock on table X until the user presses return.\n\nIf the user is out in the can we can wait for hours.\n\nReturn to top\n\n-------------------------------------------------------------------------------\n\n1.2.6: What's a natural key?\n\n-------------------------------------------------------------------------------\n\nLet me think back to my database class... okay, I can't think that far so I'll\n\nparaphrase... essentially, a natural key is a key for a given table that\n\nuniquely identifies the row. It's natural in the sense that it follows the\n\nbusiness or real world need.\n\nFor example, assume that social security numbers are unique (I believe it is\n\nstrived to be unique but it's not always the case), then if you had the\n\nfollowing employee table:\n\nemployee:\n\nssn char(09)\n\nf_name char(20)\n\nl_name char(20)\n\ntitle char(03)\n\nThen a natural key would be ssn. If the combination of _name and l_name were\n\nunique at this company, then another natural key would be f_name, l_name. As a\n\nmatter of fact, you can have many natural keys in a given table but in practice\n\nwhat one does is build a surrogate (or artificial) key.\n\nThe surrogate key is guaranteed to be unique because (wait, get back, here it\n\ngoes again) it's typically a monotonically increasing value. Okay, my\n\nmathematician wife would be proud of me... really all it means is that the key\n\nis increasing linearly: i+1\n\nThe reason one uses a surrogate key is because your joins will be faster.\n\nIf we extended our employee table to have a surrogate key:\n\nemployee:\n\nid identity\n\nssn char(09)\n\nf_name char(20)\n\nl_name char(20)\n\ntitle char(03)\n\nThen instead of doing the following:\n\nwhere a.f_name = b.f_name\n\nand a.l_name = a.l_name\n\nwe'd do this:\n\nwhere a.id = b.id\n\nWe can build indexes on these keys and since Sybase's atomic storage unit is\n\n2K, we can stash more values per 2K page with smaller indexes thus giving us\n\nbetter performance (imagine the key being 40 bytes versus being say 4 bytes...\n\nhow many 40 byte values can you stash in a 2K page versus a 4 byte value? --\n\nand how much wood could a wood chuck chuck, if a wood chuck could chuck wood?)\n\nDoes it have anything to do with natural joins?\n\nUm, not really... from \"A Guide to Sybase..\", McGovern and Date, p. 112:\n\nThe equi-join by definition must produce a result containing two identical\n\ncolumns. If one of those two columns is eliminated, what is left is called\n\nthe natural join.\n\nReturn to top\n\n-------------------------------------------------------------------------------\n\n1.2.7: Making a Stored Procedure invisible\n\n-------------------------------------------------------------------------------\n\nSystem 11.5 and above\n\nIt is now possible to encrypt your stored procedure code that is stored in the\n\nsyscomments table. This is preferred than the old method of deleting the data\n\nas deleting will impact future upgrades. You can encrypt the text with the\n\nsp_hidetext system procedure.\n\nPre-System 11.5\n\nPerhaps you are trying to prevent the buyer of your software from defncopy'ing\n\nall your stored procedures. It is perfectly safe to delete the syscomments\n\nentries of any stored procedures you'd like to protect:\n\nsp_configure \"allow updates\", 1\n\ngo\n\nreconfigure with override /* System 10 and below */\n\ngo\n\nuse affected_database\n\ngo\n\ndelete syscomments where id = object_id(\"procedure_name\")\n\ngo\n\nuse master\n\ngo\n\nsp_configure \"allow updates\", 0\n\ngo\n\nI believe in future releases of Sybase we'll be able to see the SQL that is\n\nbeing executed. I don't know if that would be simply the stored procedure name\n\nor the SQL itself.\n\nReturn to top\n\n-------------------------------------------------------------------------------\n\n1.2.8: Saving space when inserting rows monotonically\n\n-------------------------------------------------------------------------------\n\nIf the columns that comprise the clustered index are monotonically increasing\n\n(that is, new row key values are greater than those previously inserted) the\n\nfollowing System 11 dbcc tune will not split the page when it's half way full.\n\nRather it'll let the page fill and then allocate another page:\n\ndbcc tune(ascinserts, 1, \"my_table\")\n\nBy the way, SyBooks is wrong when it states that the above needs to be reset\n\nwhen ASE is rebooted. This is a permanent setting.\n\nTo undo it:\n\ndbcc tune(ascinserts, 0, \"my_table\")\n\nReturn to top\n\n-------------------------------------------------------------------------------\n\n1.2.9: How to compute database fragmentation\n\n-------------------------------------------------------------------------------\n\nCommand\n\ndbcc traceon(3604)\n\ngo\n\ndbcc tab(production, my_table, 0)\n\ngo\n\nInterpretation\n\nA delta of one means the next page is on the same track, two is a short seek,\n\nthree is a long seek. You can play with these constants but they aren't that\n\nimportant.\n\nA table I thought was unfragmented had L1 = 1.2 L2 = 1.8\n\nA table I thought was fragmented had L1 = 2.4 L2 = 6.6\n\nHow to Fix\n\nYou fix a fragmented table with clustered index by dropping and creating the\n\nindex. This measurement isn't the correct one for tables without clustered\n\nindexes. If your table doesn't have a clustered index, create a dummy one and\n\ndrop it.\n\nReturn to top\n\n-------------------------------------------------------------------------------\n\n1.2.10: Tasks a DBA should do...\n\n-------------------------------------------------------------------------------\n\nA good presentation of a DBA's duties has been made available by Jeff Garbus (\n\n***@soaringeagleltd.com) of Soaring Eagle Consulting Ltd (http://\n\nwww.soaringeagleltd.com) and numerous books can be found here. These are\n\nPowerpoint slides converted to web pages and so may be difficult to view with a\n\ntext browser!\n\nAn alternative view is catalogued below. (OK, so this list is crying out for a\n\nbit of a revamp since checkstorage came along Ed!)\n\nDBA Tasks\n\n+-------------------------------------------------------------------------+\n\n| Task | Reason | Period |\n\n|------------------------+---------------+--------------------------------|\n\n| | I consider | If your ASE permits, daily |\n\n| | these the | before your database dumps. If |\n\n| dbcc checkdb, | minimal | this is not possible due to |\n\n| checkcatalog, | dbcc's to | the size of your databases, |\n\n| checkalloc | ensure the | then try the different options |\n\n| | integrity of | so that the end of, say, a |\n\n| | your database | week, you've run them all. |\n\n|------------------------+---------------+--------------------------------|\n\n| Disaster recovery | Always be | |\n\n| scripts - scripts to | prepared for | |\n\n| rebuild your ASE in | the worst. | |\n\n| case of hardware | Make sure to | |\n\n| failure | test them. | |\n\n|------------------------+---------------+--------------------------------|\n\n| scripts to logically | | |\n\n| dump your master | You can | |\n\n| database, that is bcp | selectively | |\n\n| the critical system | rebuild your | |\n\n| tables: sysdatabases, | database in | Daily |\n\n| sysdevices, syslogins, | case of | |\n\n| sysservers, sysusers, | hardware | |\n\n| syssegments, | failure | |\n\n| sysremotelogins | | |\n\n|------------------------+---------------+--------------------------------|\n\n| | A system | |\n\n| | upgrade is | After any change as well as |\n\n| %ls -la <disk_devices> | known to | daily |\n\n| | change the | |\n\n| | permissions. | |\n\n|------------------------+---------------+--------------------------------|\n\n| dump the user | CYA* | Daily |\n\n| databases | | |\n\n|------------------------+------------"
    }
}
{
    "id": "wrong_mix_domain_subsidiary_00148_0",
    "rank": 47,
    "data": {
        "url": "https://comp.databases.sybase.narkive.com/LJoLVvnK/sybase-faq-7-19-ase-admin-4-of-7",
        "read_more_link": "",
        "language": "en",
        "title": "ASE Admin (4 of 7)",
        "top_image": "https://narkive.net/favicon.ico",
        "meta_img": "https://narkive.net/favicon.ico",
        "images": [],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "",
        "meta_favicon": "https://narkive.net/favicon.ico",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "David Owen\n\nArchive-name: databases/sybase-faq/part6\n\nURL: http://www.isug.com/Sybase_FAQ\n\nVersion: 1.7\n\nMaintainer: David Owen\n\nLast-modified: 2003/03/02\n\nPosting-Frequency: posted every 3rd month\n\nA how-to-find-the-FAQ article is posted on the intervening months.\n\nAdvanced ASE Administration\n\n1.3.1 How do I clear a log suspend'd connection?\n\n1.3.2 What's the best value for cschedspins?\n\n1.3.3 What traceflags are available?\n\n1.3.4 How do I use traceflags 5101 and 5102?\n\n1.3.5 What is cmaxpktsz good for?\n\n1.3.6 What do all the parameters of a buildmaster -d<device> -yall mean?\n\n1.3.7 What is CIS and how do I use it?\n\n1.3.8 If the master device is full how do I make the master database\n\nbigger?\n\n1.3.9 How do I run multiple versions of Sybase on the same server?\n\n1.3.10 How do I capture a process's SQL?\n\nGeneral Troubleshooting User Database Administration ASE FAQ\n\n-------------------------------------------------------------------------------\n\n1.3.1 How to clear a log suspend\n\n-------------------------------------------------------------------------------\n\nA connection that is in a log suspend state is there because the transaction\n\nthat it was performing couldn't be logged. The reason it couldn't be logged is\n\nbecause the database transaction log is full. Typically, the connection that\n\ncaused the log to fill is the one suspended. We'll get to that later.\n\nIn order to clear the problem you must dump the transaction log. This can be\n\ndone as follows:\n\ndump tran db_name to data_device\n\ngo\n\nAt this point, any completed transactions will be flushed out to disk. If you\n\ndon't care about the recoverability of the database, you can issue the\n\nfollowing command:\n\ndump tran db_name with truncate_only\n\nIf that doesn't work, you can use the with no_log option instead of the with\n\ntruncate_only.\n\nAfter successfully clearing the log the suspended connection(s) will resume.\n\nUnfortunately, as mentioned above, there is the situation where the connection\n\nthat is suspended is the culprit that filled the log. Remember that dumping the\n\nlog only clears out completed transaction. If the connection filled the log\n\nwith one large transaction, then dumping the log isn't going to clear the\n\nsuspension.\n\nSystem 10\n\nWhat you need to do is issue an ASE kill command on the connection and then\n\nun-suspend it:\n\nselect lct_admin(\"unsuspend\", db_id(\"db_name\"))\n\nSystem 11\n\nSee Sybase Technical News Volume 6, Number 2\n\nRetaining Pre-System 10 Behaviour\n\nBy setting a database's abort xact on log full option, pre-System 10 behaviour\n\ncan be retained. That is, if a connection cannot log its transaction to the log\n\nfile, it is aborted by ASE rather than suspended.\n\nReturn to top\n\n-------------------------------------------------------------------------------\n\n1.3.2 What's the best value for cschedspins?\n\n-------------------------------------------------------------------------------\n\nIt is crucial to understand that cschedspins is a tunable parameter\n\n(recommended values being between 1-2000) and the optimum value is completely\n\ndependent on the customer's environment. cschedspins is used by the scheduler\n\nonly when it finds that there are no runnable tasks. If there are no runnable\n\ntasks, the scheduler has two options:\n\n1. Let the engine go to sleep (which is done by an OS call) for a specified\n\ninterval or until an event happens. This option assumes that tasks won't\n\nbecome runnable because of tasks executing on other engines. This would\n\nhappen when the tasks are waiting for I/O more than any other resource such\n\nas locks. Which means that we could free up the CPU resource (by going to\n\nsleep) and let the system use it to expedite completion of system tasks\n\nincluding I/O.\n\n2. Go and look for a ready task again. This option assumes that a task would\n\nbecome runnable in the near term and so incurring the extra cost of an OS\n\ncontext switch through the OS sleep/wakeup mechanism is unacceptable. This\n\nscenario assumes that tasks are waiting on resources such as locks, which\n\ncould free up because of tasks executing on other engines, more than they\n\nwait for I/O.\n\ncschedspins controls how many times we would choose option 2 before choosing\n\noption 1. Setting cschedspins low favours option 1 and setting it high favours\n\noption 2. Since an I/O intensive task mix fits in with option 1, setting\n\ncschedspins low may be more beneficial. Similarly since a CPU intensive job mix\n\nfavours option 2, setting cschedspins high may be beneficial.\n\nThe consensus is that a single CPU server should have cschedspins set to 1.\n\nHowever, I strongly recommend that users carefully test values for cschedspins\n\nand monitor the results closely. I have seen more than one site that has shot\n\nthemselves in the foot so to speak due to changing this parameter in production\n\nwithout a good understanding of their environment.\n\nReturn to top\n\n-------------------------------------------------------------------------------\n\n1.3.3 Trace Flag Definitions\n\n-------------------------------------------------------------------------------\n\nTo activate trace flags, add them to the RUN_* script. The following example is\n\nusing the 1611 and 260 trace flags. Note that there is no space between the\n\n'-T' and the traceflag, despite what is written in some documentation.\n\nUse of these traceflags is not recommended by Sybase. Please use at your\n\nown risk.\n\n% cd ~sybase/install\n\n% cat RUN_BLAND\n\n#!/bin/sh\n\n#\n\n# SQL Server Information:\n\n# name: BLAND\n\n# master device: /usr/sybase/dbf/BLAND/master.dat\n\n# master device size: 25600\n\n# errorlog: /usr/sybase/install/errorlog_BLAND\n\n# interfaces: /usr/sybase\n\n#\n\n/usr/sybase/dataserver -d/usr/sybase/dbf/BLAND/master.dat \\\n\n-sBLAND -e/usr/sybase/install/errorlog_BLAND -i/usr/sybase \\\n\n-T1611 -T260\n\n-------------------------------------------------------------------------------\n\nTrace Flags\n\n+-----------------------------------------------------------------------------+\n\n| | |\n\n|------+----------------------------------------------------------------------|\n\n| Flag | Description |\n\n|------+----------------------------------------------------------------------|\n\n| 108 | (Documented) To allow dynamic and host variables in create view |\n\n| | statements in ASE 12.5 and above. |\n\n|------+----------------------------------------------------------------------|\n\n| 200 | Displays messages about the before image of the query-tree. |\n\n|------+----------------------------------------------------------------------|\n\n| 201 | Displays messages about the after image of the query-tree. |\n\n|------+----------------------------------------------------------------------|\n\n| 241 | Compress all query-trees whenever the SQL dataserver is started. |\n\n|------+----------------------------------------------------------------------|\n\n| | Reduce TDS (Tabular Data Stream) overhead in stored procedures. Turn |\n\n| | off done-in-behaviour packets. Do not use this if your application |\n\n| | is a ct-lib based application; it'll break. |\n\n| 260 | |\n\n| | Why set this on? Glad you asked, typically with a db-lib application |\n\n| | a packet is sent back to the client for each batch executed within a |\n\n| | stored procedure. This can be taxing in a WAN/LAN environment. |\n\n|------+----------------------------------------------------------------------|\n\n| | Changes the hierarchy and casting of datatypes to pre-11.5.1 |\n\n| | behaviour. There was an issue is some very rare cases where a wrong |\n\n| | result could occur, but that's been cleared up in 11.9.2 and above. |\n\n| | |\n\n| 291 | The trace can be used at boot time or at the session level. Keep in |\n\n| | mind that it does not disqualify a table scan from occurring. What |\n\n| | it will do is result in fewer datatype mismatch situations and thus |\n\n| | the optimizer will be able to estimate the costs of SARGs and joins |\n\n| | on columns involved in a mismatch. |\n\n|------+----------------------------------------------------------------------|\n\n| 299 | This trace flag instructs the dataserver to not recompile a child |\n\n| | stored procedure that inherits a temp table from a parent procedure. |\n\n|------+----------------------------------------------------------------------|\n\n| 302 | Print information about the optimizer's index selection. |\n\n|------+----------------------------------------------------------------------|\n\n| 303 | Display OR strategy |\n\n|------+----------------------------------------------------------------------|\n\n| | Revert special or optimizer strategy to that strategy used in |\n\n| 304 | pre-System 11 (this traceflag resolved several bug issues in System |\n\n| | 11, most of these bugs are fixed in ASE 11.0.3.2) |\n\n|------+----------------------------------------------------------------------|\n\n| 310 | Print information about the optimizer's join selection. |\n\n|------+----------------------------------------------------------------------|\n\n| 311 | Display the expected IO to satisfy a query. Like statistics IO |\n\n| | without actually executing. |\n\n|------+----------------------------------------------------------------------|\n\n| 317 | Provide extra optimization information. |\n\n|------+----------------------------------------------------------------------|\n\n| 319 | Reformatting strategies. |\n\n|------+----------------------------------------------------------------------|\n\n| 320 | Turn off the join order heuristic. |\n\n|------+----------------------------------------------------------------------|\n\n| 324 | Turn off the like optimization for ad-hoc queries using |\n\n| | @local_variables. |\n\n|------+----------------------------------------------------------------------|\n\n| | (Only valid in ASE versions prior to 11.9.2.) Instructs the server |\n\n| | to use arithmetic averaging when calculating density instead of a |\n\n| 326 | geometric weighted average when updating statistics. Useful for |\n\n| | building better stats when an index has skew on the leading column. |\n\n| | Use only for updating the stats of a table/index with known skewed |\n\n| | data. |\n\n|------+----------------------------------------------------------------------|\n\n| | |\n\n|------+----------------------------------------------------------------------|\n\n| 602 | Prints out diagnostic information for deadlock prevention. |\n\n|------+----------------------------------------------------------------------|\n\n| 603 | Prints out diagnostic information when avoiding deadlock. |\n\n|------+----------------------------------------------------------------------|\n\n| 699 | Turn off transaction logging for the entire SQL dataserver. |\n\n|------+----------------------------------------------------------------------|\n\n| 1204 | Send deadlock detection to the errorlog. |\n\n| * | |\n\n|------+----------------------------------------------------------------------|\n\n| 1205 | Stack trace on deadlock. |\n\n|------+----------------------------------------------------------------------|\n\n| 1206 | Disable lock promotion. |\n\n|------+----------------------------------------------------------------------|\n\n| 1603 | Use standard disk I/O (i.e. turn off asynchronous I/O). |\n\n| * | |\n\n|------+----------------------------------------------------------------------|\n\n| 1605 | Start secondary engines by hand |\n\n|------+----------------------------------------------------------------------|\n\n| | Create a debug engine start file. This allows you to start up a |\n\n| | debug engine which can access the server's shared memory for running |\n\n| | diagnostics. I'm not sure how useful this is in a production |\n\n| 1606 | environment as the debugger often brings down the server. I'm not |\n\n| | sure if Sybase have ported the debug stuff to 10/11. Like most of |\n\n| | their debug tools it started off quite strongly but was never |\n\n| | developed. |\n\n|------+----------------------------------------------------------------------|\n\n| | Startup only engine 0; use dbcc engine(\"online\") to incrementally |\n\n| 1608 | bring up additional engines until the maximum number of configured |\n\n| | engines. |\n\n|------+----------------------------------------------------------------------|\n\n| 1610 | Boot the SQL dataserver with TCP_NODELAY enabled. |\n\n| * | |\n\n|------+----------------------------------------------------------------------|\n\n| 1611 | If possible, pin shared memory -- check errorlog for success/ |\n\n| * | failure. |\n\n|------+----------------------------------------------------------------------|\n\n| 1613 | Set affinity of the SQL dataserver engine's onto particular CPUs -- |\n\n| | usually pins engine 0 to processor 0, engine 1 to processor 1... |\n\n|------+----------------------------------------------------------------------|\n\n| 1615 | SGI only: turn on recoverability to filesystem devices. |\n\n|------+----------------------------------------------------------------------|\n\n| | Linux only: Revert to using cached filesystem I/O. By default, ASE |\n\n| 1625 | on Linux (11.9.2 and above) opens filesystem devices using O_SYNC, |\n\n| | unlike other Unix based releases, which means it is safe to use |\n\n| | filesystems devices for production systems. |\n\n|------+----------------------------------------------------------------------|\n\n| 2512 | Prevent dbcc from checking syslogs. Useful when you are constantly |\n\n| | getting spurious allocation errors. |\n\n|------+----------------------------------------------------------------------|\n\n| 3300 | Display each log record that is being processed during recovery. You |\n\n| | may wish to redirect stdout because it can be a lot of information. |\n\n|------+----------------------------------------------------------------------|\n\n| 3500 | Disable checkpointing. |\n\n|------+----------------------------------------------------------------------|\n\n| 3502 | Track checkpointing of databases in errorlog. |\n\n|------+----------------------------------------------------------------------|\n\n| 3601 | Stack trace when error raised. |\n\n|------+----------------------------------------------------------------------|\n\n| 3604 | Send dbcc output to screen. |\n\n|------+----------------------------------------------------------------------|\n\n| 3605 | Send dbcc output to errorlog. |\n\n|------+----------------------------------------------------------------------|\n\n| 3607 | Do not recover any database, clear behaviour start up checkpoint |\n\n| | process. |\n\n|------+----------------------------------------------------------------------|\n\n| 3608 | Recover master only. Do not clear tempdb or start up checkpoint |\n\n| | process. |\n\n|------+----------------------------------------------------------------------|\n\n| 3609 | Recover all databases. Do not clear tempdb or start up checkpoint |\n\n| | process. |\n\n|------+----------------------------------------------------------------------|\n\n| 3610 | Pre-System 10 behaviour: divide by zero to result in NULL instead of |\n\n| | error - also see Q6.2.5. |\n\n|------+----------------------------------------------------------------------|\n\n| 3620 | Do not kill infected processes. |\n\n|------+----------------------------------------------------------------------|\n\n| 4001 | Very verbose logging of each login attempt to the errorlog. Includes |\n\n| | tons of information. |\n\n|------+----------------------------------------------------------------------|\n\n| 4012 | Don't spawn chkptproc. |\n\n|------+----------------------------------------------------------------------|\n\n| 4013 | Place a record in the errorlog for each login to the dataserver. |\n\n|------+----------------------------------------------------------------------|\n\n| 4020 | Boot without recover. |\n\n|------+----------------------------------------------------------------------|\n\n| | Forces all I/O requests to go through engine 0. This removes the |\n\n| 5101 | contention between processors but could create a bottleneck if |\n\n| | engine 0 becomes busy with non-I/O tasks. For more information... |\n\n| | 5101/5102. |\n\n|------+----------------------------------------------------------------------|\n\n| 5102 | Prevents engine 0 from running any non-affinitied tasks. For more |\n\n| | information...5101/5102. |\n\n|------+----------------------------------------------------------------------|\n\n| 7103 | Disable table lock promotion for text columns. |\n\n|------+----------------------------------------------------------------------|\n\n| 8203 | Display statement and transaction locks on a deadlock error. |\n\n|------+----------------------------------------------------------------------|\n\n| * | Starting with System 11 these are sp_configure'able |\n\n+-----------------------------------------------------------------------------+\n\nReturn to top\n\n-------------------------------------------------------------------------------\n\n1.3.4 Trace Flags -- 5101 and 5102\n\n-------------------------------------------------------------------------------\n\n5101\n\nNormally, each engine issues and checks for its own Disk I/O on behalf of the\n\ntasks it runs. In completely symmetric operating systems, this behavior\n\nprovides maximum I/O throughput for ASE. Some operating systems are not\n\ncompletely symmetric in their Disk I/O routines. For these environments, the\n\nserver can be booted with the 5101 trace flag. While tasks still request disk I\n\n/O from any engine, the actual request to/from the OS is performed by engine 0.\n\nThe performance benefit comes from the reduced or eliminated contention on the\n\nlocking mechanism inside the OS kernel. To enable I/O affinity to engine 0,\n\nstart ASE with the 5101 Trace Flag.\n\nYour errorlog will indicate the use of this option with the message:\n\nDisk I/O affinitied to engine: 0\n\nThis trace flag only provides performance gains for servers with 3 or more\n\ndataserver engines configured and being significantly utilized.\n\nUse of this trace flag with fully symmetric operating systems will degrade\n\nperformance!\n\n5102\n\nThe 5102 trace flag prevents engine 0 from running any non-affinitied tasks.\n\nNormally, this forces engine 0 to perform Network I/O only. Applications with\n\nheavy result set requirements (either large results or many connections issuing\n\nshort, fast requests) may benefit. This effectively eliminates the normal\n\nlatency for engine 0 to complete running its user thread before it issues the\n\nnetwork I/O to the underlying network transport driver. If used in conjunction\n\nwith the 5101 trace flag, engine 0 would perform all Disk I/O and Network I/O.\n\nFor environments with heavy disk and network I/O, engine 0 could easily\n\nsaturate when only the 5101 flag is in use. This flag allows engine 0 to\n\nconcentrate on I/O by not allowing it to run user tasks. To force task affinity\n\noff engine 0, start ASE with the 5102 Trace Flag.\n\nYour errorlog will indicate the use of this option with the message:\n\nI/O only enabled for engine: 0\n\n-------------------------------------------------------------------------------\n\nWarning: Not supported by Sybase. Provided here for your enjoyment.\n\nReturn to top\n\n-------------------------------------------------------------------------------\n\n1.3.5 What is cmaxpktsz good for?\n\n-------------------------------------------------------------------------------\n\ncmaxpktsz corresponds to the parameter \"maximum network packet size\" which you\n\ncan see through sp_configure. I recommend only updating this value through\n\nsp_configure. If some of your applications send or receive large amounts of\n\ndata across the network, these applications can achieve significant performance\n\nimprovement by using larger packet sizes. Two examples are large bulk copy\n\noperations and applications reading or writing large text or image values.\n\nGenerally, you want to keep the value of default network packet size small for\n\nusers performing short queries, and allow users who send or receive large\n\nvolumes of data to request larger packet sizes by setting the maximum network\n\npacket size configuration variable.\n\ncaddnetmem corresponds to the parameter \"additional netmem\" which you can see\n\nthrough sp_configure. Again, I recommend only updating this value through\n\nsp_configure. \"additional netmem\" sets the maximum size of additional memory\n\nthat can be used for network packets that are larger than ASE's default packet\n\nsize. The default value for additional netmem is 0, which means that no extra\n\nspace has been allocated for large packets. See the discussion below, under\n\nmaximum network packet size, for information on setting this configuration\n\nvariable. Memory allocated with additional netmem is added to the memory\n\nallocated by memory. It does not affect other ASE memory uses.\n\nASE guarantees that every user connection will be able to log in at the default\n\npacket size. If you increase maximum network packet size and additional netmem\n\nremains set to 0, clients cannot use packet sizes that are larger than the\n\ndefault size: all allocated network memory will be reserved for users at the\n\ndefault size. In this situation, users who request a large packet size when\n\nthey log in receive a warning message telling them that their application will\n\nuse the default size. To determine the value for additional netmem if your\n\napplications use larger packet sizes:\n\n* Estimate the number of simultaneous users who will request the large packet\n\nsizes, and the sizes their applications will request.\n\n* Multiply this sum by three, since each connection needs three buffers.\n\n* Add 2% for overhead, rounded up to the next multiple of 512\n\nReturn to top\n\n-------------------------------------------------------------------------------\n\n1.3.6 Buildmaster Configuration Definitions\n\n-------------------------------------------------------------------------------\n\nAttention! Please notice, be very careful with these parameters. Use only\n\nat your own risk. Be sure to have a copy of the original parameters. Be\n\nsure to have a dump of all dbs (include master) handy.\n\nSince the release of 11.x (and above), there is almost no need for\n\nbuildmaster to configure parameters. In fact, buildmaster has gone been\n\nremoved from ASE 12.5. This section is really kept for anyone out there\n\nrunning old versions of ASE. I still see the odd post from people asking\n\nabout 4.9.2, so this is for you.\n\nAnyone else who feels a need to use buildmaster should check sp_configure\n\nand/or SERVERNAME.cfg to see if the configuration parameter is there before\n\nusing buildmaster.\n\nYOU HAVE BEEN WARNED. See the .\n\n-------------------------------------------------------------------------------\n\nThe following is a list of configuration parameters and their effect on the\n\nASE. Changes to these parameters can affect performance of the server. Sybase\n\ndoes not recommend modifying these parameters without first discussing the\n\nchange with Sybase Tech Support. This list is provided for information only.\n\nThese are categorized into two kinds:\n\n* Configurable through sp_configure and\n\n* not configurable but can be changed through 'buildmaster -y<variable>=value\n\n-d<dbdevice>'\n\nConfigurable variables:\n\ncrecinterval:\n\nThe recovery interval specified in minutes.\n\nccatalogupdates:\n\nA flag to inform whether system catalogs can be updated or not.\n\ncusrconnections:\n\nThis is the number of user connections allowed in SQL\n\nServer. This value + 3 (one for checkpoint, network\n\nand mirror handlers) make the number of pss configured\n\nin the server.\n\n-------------------------------------------------------------------------------\n\ncfgpss:\n\nNumber of PSS configured in the server. This value will\n\nalways be 3 more than cusrconnections. The reason is we\n\nneed PSS for checkpoint, network and mirror handlers.\n\nTHIS IS NOT CONFIGURABLE.\n\n-------------------------------------------------------------------------------\n\ncmemsize:\n\nThe total memory configured for the Server in 2k\n\nunits. This is the memory the server will use for both\n\nServer and Kernel Structures. For Stratus or any 4k\n\npagesize implementation of ASE, certain values\n\nwill change as appropriate.\n\ncdbnum:\n\nThis is the number of databases that can be open in SQL\n\nServer at any given time.\n\nclocknum:\n\nVariable that defines and controls the number of logical\n\nlocks configured in the system.\n\ncdesnum:\n\nThis is the number of open objects that can be open at\n\na given point of time.\n\ncpcacheprcnt:\n\nThis is the percentage of cache that should be used\n\nfor procedures to be cached in.\n\ncfillfactor:\n\nFill factor for indexes.\n\nctimeslice:\n\nThis value is in units of milli-seconds. This value determines\n\nhow much time a task is allowed to run before it yields.\n\nThis value is internally converted to ticks. See below\n\nthe explanations for cclkrate, ctimemax etc.\n\nccrdatabasesize:\n\nThe default size of the database when it is created.\n\nThis value is Megabytes and the default is 2Meg.\n\nctappreten:\n\nAn outdated not used variable.\n\ncrecoveryflags:\n\nA toggle flag which will display certain recovery information\n\nduring database recoveries.\n\ncserialno:\n\nAn informational variable that stores the serial number\n\nof the product.\n\ncnestedtriggers:\n\nFlag that controls whether nested triggers allowed or not.\n\ncnvdisks:\n\nVariable that controls the number of device structures\n\nthat are allocated which affects the number of devices\n\nthat can be opened during server boot up. If user\n\ndefined 20 devices and this value is configured to be\n\n10, during recovery only 10 devices will be opened and\n\nthe rest will get errors.\n\ncfgsitebuf:\n\nThis variable controls maximum number of site handler\n\nstructures that will be allocated. This in turn\n\ncontrols the number of site handlers that can be\n\nactive at a given instance.\n\ncfgrembufs:\n\nThis variable controls the number of remote buffers\n\nthat needs to send and receive from remote sites.\n\nActually this value should be set to number of\n\nlogical connections configured. (See below)\n\ncfglogconn:\n\nThis is the number of logical connections that can\n\nbe open at any instance. This value controls\n\nthe number of resource structure allocated and\n\nhence it will affect the overall logical connection\n\ncombined with different sites. THIS IS NOT PER SITE.\n\ncfgdatabuf:\n\nMaximum number of pre-read packets per logical connections.\n\nIf logical connection is set to 10, and cfgdatabuf is set\n\nto 3 then the number of resources allocated will be\n\n30.\n\ncfupgradeversion:\n\nVersion number of last upgrade program ran on this server.\n\ncsortord:\n\nSort order of ASE.\n\ncold_sortdord:\n\nWhen sort orders are changed the old sort order is\n\nsaved in this variable to be used during recovery\n\nof the database after the Server is rebooted with\n\nthe sort order change.\n\nccharset:\n\nCharacter Set used by ASE\n\ncold_charset:\n\nSame as cold_sortord except it stores the previous\n\nCharacter Set.\n\n-------------------------------------------------------------------------------\n\ncdflt_sortord:\n\npage # of sort order image definition. This should\n\nnot be changed at any point. This is a server only\n\nvariable.\n\ncdflt_charset:\n\npage # of character set image definition. This should\n\nnot be changed at any point. This is a server only\n\nvariable.\n\ncold_dflt_sortord:\n\npage # of previous sort order image definition. This\n\nshould not be changed at any point. This is a server\n\nonly variable.\n\ncold_dflt_charset:\n\npage # of previous chracter set image definition. This\n\nshould not be changed at any point. This is a server\n\nonly variable.\n\n-------------------------------------------------------------------------------\n\ncdeflang:\n\nDefault language used by ASE.\n\ncmaxonline:\n\nMaximum number of engines that can be made online. This\n\nnumber should not be more than the # of cpus available on this\n\nsystem. On Single CPU system like RS6000 this value is always\n\n1.\n\ncminonline:\n\nMinimum number of engines that should be online. This is 1 by\n\ndefault.\n\ncengadjinterval:\n\nA noop variable at this time.\n\ncfgstacksz:\n\nStack size per task configured. This doesn't include the guard\n\narea of the stack space. The guard area can be altered through\n\ncguardsz.\n\n-------------------------------------------------------------------------------\n\ncguardsz:\n\nThis is the size of the guard area. ASE will\n\nallocate stack space for each task by adding cfgstacksz\n\n(configurable through sp_configure) and cguardsz (default is\n\n2K). This has to be a multiple of PAGESIZE which will be 2k\n\nor 4k depending on the implementation.\n\nbehaviour:\n\nSize of fixed stack space allocated per task including the\n\nguard area.\n\n-------------------------------------------------------------------------------\n\nNon-configurable values :\n\n-------------------------------------------------------------------------------\n\nTIMESLICE, CTIMEMAX ETC:\n\n-------------------------------------------------------------------------------\n\n1 millisecond = 1/1000th of a second.\n\n1 microsecond = 1/1000000th of a second. \"Tick\" : Interval between two clock\n\ninterrupts occur in real time.\n\n\"cclkrate\" :\n\nA value specified in microsecond units.\n\nNormally on systems where a fine grained timer is not available\n\nor if the Operating System cannot set sub-second alarms, this\n\nvalue is set to 1000000 milliseconds which is 1 second. In\n\nother words an alarm will go off every 1 second or you will\n\nget 1 tick per second.\n\nOn Sun4 this is set to 100000 milliseconds which will result in\n\nan interrupt going at 1/10th of a second. You will get 6 ticks\n\nper second.\n\n\"avetimeslice\" :\n\nA value specified in millisecond units.\n\nThis is the value given in \"sp_configure\",<timeslice value>.\n\nOtherwise the milliseconds are converted to milliseconds and\n\nfinally to tick values.\n\nticks = <avetimeslice> * 1000 / cclkrate.\n\n\"timeslice\" :\n\n-------------------------------------------------------------------------------\n\nThe unit of this variable is in ticks.\n\nThis value is derived from \"avetimeslice\". If \"avetimeslice\"\n\nis less than 1000 milliseconds then timeslice is set to 1 tick.\n\n\"ctimemax\" :\n\nThe unit of this variable is in ticks.\n\nA task is considered in infinite loop if the consumed ticks\n\nfor a particular task is greater than ctimemax value. This\n\nis when you get timeslice -201 or -1501 errors.\n\n\"cschedspins\" :\n\nFor more information see Q1.3.2.\n\nThis value alters the behavior of ASE scheduler.\n\nThe scheduler will either run a qualified task or look\n\nfor I/O completion or sleep for a while before it can\n\ndo anything useful.\n\nThe cschedspins value determines how often the scheduler\n\nwill sleep and not how long it will sleep. A low value\n\nwill be suited for a I/O bound ASE but a\n\nhigh value will be suited for CPU bound ASE. Since\n\nASE will be used in a mixed mode, this value\n\nneed to be fined tuned.\n\nBased on practical behavior in the field, a single engine\n\nASE should have cschedspins set to 1 and a multi-engine\n\nserver should have set to 2000.\n\nNow that we've defined the units of these variables what happens when we change\n\ncclkrate ?\n\nAssume we have a cclkrate=100000.\n\nA clock interrupt will occur every (100000/1000000) 1/10th milliseconds.\n\nAssuming a task started with 1 tick which can go up to \"ctimemax=1500\" ticks\n\ncan potentially take 1/10us * (1500 + 1) ticks which will be 150 milliseconds\n\nor approx. .15 milliseconds per task.\n\nNow changing the cclkrate to 75000\n\nA clock interrupt will occur every (75000/1000000) 1/7th milliseconds. Assuming\n\na task started with 1 tick which can go up to ctimemax=1500 ticks can\n\npotentially take 1/7us * (1500 + 1) ticks which will be 112 milliseconds or\n\napprox. .11 milliseconds per task.\n\nDecreasing the cclkrate value will decrease the time spent on each task. If the\n\ntask could not voluntarily yield within the time, the scheduler will kill the\n\ntask.\n\nUNDER NO CIRCUMSTANCES the cclkrate value should be changed. The default\n\nctimemax value should be set to 1500. This is an empirical value and this can\n\nbe changed under special circumstances and strictly under the guidance of DSE.\n\n-------------------------------------------------------------------------------\n\ncfgdbname:\n\nName of the master device is saved here. This is 64\n\nbytes in length.\n\ncfgpss:\n\nThis is a derived value from cusrconnections + 3.\n\nSee cusrconnections above.\n\ncfgxdes:\n\nThis value defines the number of transactions that\n\ncan be done by a task at a given instance.\n\nChanging this value to be more than 32 will have no\n\neffect on the server.\n\ncfgsdes:\n\nThis value defines the number of open tables per\n\ntask. This will be typically for a query. This\n\nwill be the number of tables specified in a query\n\nincluding subqueries.\n\nSybase Advises not to change this value. There\n\nwill be significant change in the size of per user\n\nresource in ASE.\n\ncfgbuf:\n\nThis is a derived variable based on the total\n\nmemory configured and subtracting different resource\n\nsizes for Databases, Objects, Locks and other\n\nKernel memories.\n\ncfgdes:\n\nThis is same as cdesnum. Other values will have no effect on it.\n\ncfgprocedure:\n\nThis is a derived value. Based on cpcacheprcnt variable.\n\ncfglocks:\n\nThis is same as clocknum. Other values will have no effect on it.\n\ncfgcprot:\n\nThis is variable that defines the number of cache protectors per\n\ntask. This is used internally by ASE.\n\nSybase advise not to modify this value as a default of 15 will\n\nbe more than sufficient.\n\ncnproc:\n\nThis is a derived value based on cusrconnections + <extra> for\n\nSybase internal tasks that are both visible and non-visible.\n\ncnmemmap:\n\nThis is an internal variable that will keep track of ASE\n\nmemory.\n\nModifying this value will not have any effect.\n\ncnmbox:\n\nNumber of mail box structures that need to be allocated.\n\nMore used in VMS environment than UNIX environment.\n\ncnmsg:\n\nUsed in tandem with cnmbox.\n\ncnmsgmax:\n\nMaximum number of messages that can be passed between mailboxes.\n\ncnblkio:\n\nNumber of disk I/O request (async and direct) that can be\n\nprocessed at a given instance. This is a global value for all\n\nthe engines and not per engine value.\n\nThis value is directly depended on the number of I/O request\n\nthat can be processed by the Operating System. It varies\n\ndepending on the Operating System.\n\ncnblkmax:\n\nMaximum number of I/O request that can be processed at any given\n\ntime.\n\nNormally cnblkio,cnblkmax and cnmaxaio_server should be the same.\n\ncnmaxaio_engine:\n\nMaximum number of I/O request that can be processed by one engine.\n\nSince engines are Operating System Process, if there is any limit\n\nimposed by the Operating System on a per process basis then\n\nthis value should be set. Otherwise it is a noop.\n\ncnmaxaio_server:\n\nThis is the total number of I/O request ASE can do.\n\nThis value s directly depended on the number of I/O request\n\nthat can be processed by the Operating System. It varies\n\ndepending on the Operating System.\n\ncsiocnt:\n\nnot used.\n\ncnbytio:\n\nSimilar to disk I/O request, this is for network I/O request.\n\nThis includes disk/tape dumps also. This value is for\n\nthe whole ASE including other engines.\n\ncnbytmax:\n\nMaximum number of network I/O request including disk/tape dumps.\n\ncnalarm:\n\nMaximum number of alarms including the alarms used by\n\nthe system. This is typically used when users do \"waitfor delay\"\n\ncommands.\n\ncfgmastmirror:\n\nMirror device name for the master device.\n\ncfgmastmirror_stat:\n\nStatus of mirror devices for the master device like serial/dynamic\n\nmirroring etc.\n\ncindextrips:\n\nThis value determines the ageing of a index buffer before it\n\nis removed from the cache.\n\ncoamtrips:\n\nThis value determines the aging of a OAM buffer before it\n\nis removed from the cache.\n\ncpreallocext:\n\nThis value determines the number of extents that will be\n\nallocated while doing BCP.\n\ncbufwashsize:\n\nThis value determines when to flush buffers in the cache\n\nthat are modified.\n\nReturn to top\n\n-------------------------------------------------------------------------------\n\n1.3.7: What is CIS and how can I use it?\n\n-------------------------------------------------------------------------------\n\nCIS is the new name for Omni ASE. The biggest difference is that CIS is\n\nincluded with Adaptive Server Enterprise as standard. Actually, this is not\n\ncompletely accurate; the ability to connect to other ASEs and ASEs, including\n\nMicrosoft's, is included as standard. If you need to connect to DB2 or Oracle\n\nyou have to obtain an additional licence.\n\nSo, what is it?\n\nCIS is a means of connecting two servers together so that seamless cross-server\n\njoins can be executed. It is not just restricted to selects, pretty much any\n\noperation that can be performed on a local table can also be performed on a\n\nremote table. This includes dropping it, so be careful!\n\nWhat servers can I connect to?\n\n* Sybase ASE\n\n* Microsoft SQL Server\n\n* IBM DB2\n\n* Oracle\n\nWhat are the catches?\n\nWell, nothing truly comes for free. CIS is not a means of providing true load\n\nsharing, although you will find nothing explicitly in the documentation to tell\n\nyou this. Obviously there is a performance hit which seems to affect cursors\n\nworst of all. CIS itself is implemented using cursors and this may be part of\n\nthe explanation.\n\nOK, so how do I use it?\n\nEasy! Add the remote server using sp_addserver. Make sure that you define it\n\nas type sql_server or ASEnterprise. Create an \"existing\" table using the\n\ndefinition of the remote table. Update statistics on this new \"existing\"\n\ntable. Then simply use it in joins exactly as if it were a local table.\n\nReturn to top\n\n-------------------------------------------------------------------------------\n\n1.3.8: If the master device is full, how do I make the master database bigger?\n\n-------------------------------------------------------------------------------\n\nIt is not possible to extend the master database across another device, so the\n\nfollowing from Eric McGrane (recently of Sybase Product Support Engineering)\n\nshould help.\n\n* dump the current master database\n\n* Pre-12.5 users use buildmaster to create a new master device with a larger\n\nsize. ASE 12.5 users use dataserver to build the new, larger, master\n\ndatabase.\n\n* start the server in single user mode using the new master device\n\n* login to the server and execute the following tsql:\n\nselect * from sysdevices\n\n* take note of the high value\n\n* load the dump of the master you had just taken\n\n* restart the server (as it will be shut down when master is done loading),\n\nagain\n\nin single user mode so that you can update system tables\n\n* login to the server and update sysdevices setting high for master to the\n\nvalue\n\nthat you noted previously\n\n* shut the server down and start it back up, but this time not in single user\n\nmode.\n\nThe end result of the above is that you will now have a larger master device\n\nand you can alter your master database to be a larger size. For details about\n\nstarting the server in single user mode and how to use buildmaster (if you need\n\nthe details) please refer to the documentation.\n\nReturn to top\n\n-------------------------------------------------------------------------------\n\n1.3.9: How do I run multiple versions of Sybase on the same server?\n\n-------------------------------------------------------------------------------\n\nThe answer to this relies somewhat on the platform that you are using.\n\nUnix\n\nASE Versions Before 12.0\n\nThis applies to Unix and variants, Linux included. Install the various releases\n\nof software into logical places within your filesystem. I like to store all\n\napplication software below a single directory for ease of maintenance, choose\n\nsomething like /sw. I know that some are keen on /opt and others /usr/local. It\n\nis all down to preference and server usage. If you have both Oracle and Sybase\n\non the same server you might want /sw/sybase or /opt/sybase. Be a little\n\ncareful here if your platform is Linux or FreeBSD. The standard installation\n\ndirectories for Sybase on those platforms is /opt/sybase. Finally, have a\n\ndirectory for the release, say ASE11_9_2 or simply 11.9.2 if you only ever have\n\nSybase ASE running on this server. A little imagination is called for!\n\nSo, now you have a directory such as /sw/sybase/ASE/11.9.2 (my preferred choice\n\n:-), and some software installed under the directories, what now? In the most\n\nminimal form, that is all you need. Non of the environment variables are\n\nessential. You could quite successfully run\n\n/sw/sybase/ASE/11.9.2/bin/isql -Usa -SMYSERV -I/sw/sybase/ASE/11.9.2/interfaces\n\nand get to the server, but that is a lot of typing. By setting the SYBASE\n\nenvironment variable to /sw/sybase/ASE/11.9.2 you never need tell isql or other\n\napps where to find the interfaces. Then, you can set the path with a cool\n\nPATH=$SYBASE/bin:$PATH\n\nto pick up the correct set of Sybase binaries. That reduces the previous mass\n\nof typing to\n\nisql -Usa -SMYSERV\n\nwhich is much more manageable.\n\nYou can create yourself a couple of shell scripts to do the changes for you. So\n\nif the script a11.9 contained:\n\nSYBASE=/sw/sybase/ASE/11.9.2\n\nPATH=$SYBASE/bin:$SYBASE\n\n# Remember to export the variables!\n\nEXPORT PATH SYBASE\n\nand a11.0 contained:\n\nSYBASE=/sw/sybase/ASE/11.0.3.3\n\nPATH=$SYBASE/bin:$SYBASE\n\n# Remember to export the variables!\n\nEXPORT PATH SYBASE\n\nyou would toggle between being connect to and 11.9.2 server and a 12.0 server,\n\ndepending upon which one you executed last. The scripts are not at all\n\nsophisticated, you could quite easily have one script and pass a version string\n\ninto it. You will notice that the PATH variable gets longer each time the\n\nscript is executed. You could add greps to see if there was already a Sybase\n\ninstance on the path. Have I mentioned imagination?\n\nASE 12.0 and Beyond\n\nSybase dramatically changed the structure of the installation directory tree\n\nwith ASE 12. You still have a SYBASE environment variable pointing to the\n\nroute, but now the various packages fit below that directory. So, if we take /\n\nsw/sybase as the root directory, we have the following (the following is for a\n\n12.5 installation, but all versions follow the same format):\n\n/sw/sybase/ASE-12_5\n\n/OCS-12_5\n\nBelow ASE-12_5 is most of the stuff that we have come to expect under $SYBASE,\n\nthe install, bin and scripts directories. This is also where the SERVER.cfg\n\nfile has moved to. (Note the the interfaces file is still in $SYBASE.) The bin\n\ndirectory on this side includes the dataserver, diagserver and srvbuild\n\nbinaries.\n\nThe OCS-12_5 is the open client software directory. It means that Sybase can\n\nupdate the client software without unduly affecting the server. isql, bcp and\n\nother clients are to be found here.\n\nIt does take a little getting used to if you have been using the pre-12 style\n\nfor a number of years. However, in its defence, it is much more logical, even\n\nif it about triples the length of your PATH variable!\n\nThat is another good part of the new installation. Sybase actually provides you\n\nwith the shell script to do all of this. There is a file in /sw/sybase called\n\nSYBASE.sh (there is an equivalent C shell version in the same place) that sets\n\neverything you need!\n\nInterfaces File\n\nThe only real addition to all of the above is an easier way to manage the\n\ninterfaces file. As mentioned before, ASE based apps look for the interfaces\n\nfile in $SYBASE/interfaces by default. Unix is nice in that it allows you to\n\nhave symbolic links that make it appear as if a file is somewhere that it\n\nisn't. Place the real interfaces file somewhere independent of the software\n\ntrees. /sw/sybase/ASE/interfaces might be a sound logical choice. Now, cd to\n\n$SYBASE and issue\n\nln -s /sw/sybase/ASE/interfaces\n\nand the interfaces will appear to exist in the $SYBASE directory, but will in\n\nfact remain in its own home.\n\nNote: make sure that interfaces file is copied to its own home before removing\n\nit from $SYBASE.\n\nNow you can put symbolic links in each and every software installation and only\n\nhave to worry about maintaining the server list, on that server, in one place.\n\nHaving the interfaces file common to many physical servers is trickier, but not\n\nimpossible. Personally I would choose to put it in a central CVS repository and\n\nuse that to keep each server reasonably up-to-date.\n\nNT/2000\n\nFirstly, I have tried the following on W2K and it all works OK. I have read a\n\nnumber of reports of people having difficulty getting clean installs under NT.\n\n11.5 and 12.0 mainly. I cannot remeber having a problem with either of those\n\nmyself, but I only ever installed it to test that stuff I write runs on all\n\nplatforms. I have no intention of upgrading to XP until MS pays me to do it. It\n\nlooks like a cheap plastic version of an operating system and I pity anyone\n\nthat is forced to use it.\n\nNT is tougher than UNIX to run multiple instances on, mainly due to the fact\n\nthat it wants to do stuff for you in the background, namely configure\n\nenvironment variables. The following worked for me with the following versions\n\nof Sybase ASE all installed and running on a single server: 11.5.1, 11.9.2,\n\n12.5. I don't have a version of ASE 12.0 for NT. If I can persuade Sybase to\n\nsend them it to me, I might be able to get that running too. Notably, each and\n\nevery one of the databases runs as a service!!!\n\n1. Start by installing each software release into its own area. Make sure that\n\nit is a local disk. (See Q2.2.3.) I chose to install ASE 12.5 into C:\\\n\nSybase12_5 and ASE 11.9.2 into C:\\Sybase11_9_2 etc. When it asks you about\n\nconfiguring the server, select \"no\" or \"cancel\".\n\n2. Add a user for each installation that you are going to run. Again, I added\n\na user sybase12_5 for ASE 12.5 and sybase11_9_2 for ASE 11.9.2.\n\n3. As a system account, edit the environment variables (On W2K this is\n\nSettings->Control Panel->System->Advanced->Environment Variables...) and\n\nremove any reference to Sybase from the system path. Make sure that you\n\nstore away what has been set. A text file on your C drive is a good idea at\n\nthis stage.\n\n4. Similarly, remove references to Sybase from the Lib, Include and CLASSPATH\n\nvariables, storing the strings away.\n\n5. Remove the SYBASE, DSEDIT and DSQUERY variable.\n\n6. As I said before, I do not own 12.0, so I cannot tell you what to do about\n\nthe new Sybase variables SYBASE_OCS, SYBASE_ASE, SYBASE_FTS, SYBASE_JRE\n\netc. I can only assume that you need to cut them out too. If you are\n\ninstalling pre-12 with only 1 of 12 or 12.5, then it is not necessary.\n\n7. Login as each new Sybase user in turn and add to each of these a set of\n\nlocal variables corresponding to path, Include, Lib and set them to be the\n\nappropriate parts from the strings you removed from the system versions\n\nabove. So, if you installed ASE 12.5 in the method described, you will have\n\na whole series of variables with settings containing \"C:\\Sybase_12_5\", add\n\nall of these to local variables belonging to the user sybase12_5. Repeat\n\nfor each instance of ASE installed. This is a tedious process and I don't\n\nknow a way of speading it up. It may be possible to edit the registry, but\n\nI was not happy doing that.\n\n8. If you have made each of the Sybase users administrators, then you can\n\nconfigure the software from that account, and install a new ASE server.\n\nRemember that each one needs its own port. 11.5.1 and 11.9.2 did not give\n\nme an option to change the port during the install, so I had to do that\n\nafterwards by editing the SQL.INI for each server in its own installation\n\ntree.\n\n9. If you are not able to make each user and administrator, you will need to\n\nwork with an admin to configure the software. (ASE requires administrative\n\nrights in order to be able to add the service entries.) You will need to\n\nlog in as this admin account, set the path to the appropriate value for\n\neach installation, install the software and then set the path to the new\n\nvalues, install the next ASE etc. On NT for sure you will have to log out\n\nand log in after changing the path variable. 2000 may be less brain dead.\n\nJust be thankful you are not having to reboot!\n\n10. Log back in as your tame administrator account and go into the control\n\npanel. You need to start the \"Services\" applet. This is either there if you\n\nare running NT or you have to go into \"Administrative Tools\" for 2000.\n\nScroll down and select the first of the services, which should be of the\n\nform\n\n\"Sybase SQLServer _MYSERVER\".\n\nRight click and select \"Properties\" (I think this is how it was for NT, but\n\nyou want that services properties, however you get there.) In 2000 there is\n\na \"Log On\" tab. NT has a button (I think) that serves the same purpose.\n\nWhether tab or button, click on it. You should have a panel that starts, at\n\nthe top, with \"Log on as\" and a a pair of radio options. The top one will\n\nprobably be selected, \"Local System account\". Choose the other and enter\n\nthe details for the sybase account associated with this server. So if the\n\nserver is ASE 12.5 enter \"sybase12_5\" for \"This account\" and enter the\n\npassword associated with this account in the next two boxes. Select enough\n\n\"OK\"s to take you out of the service properties editor.\n\n11. None of the installations made a good job of the services part. All of them\n\nadded services for all of the standard servers (data, backup, monitor and\n\nXP), even though I had not configured any but XP server. (The NT\n\ninstallation is of a different form to the UNIX/Linux versions.) The 12.5\n\nXP configuration was OK, but the pre-12 ones were not. You will have to go\n\nin and manually set the user to connect as (as described earlier). If you\n\ndo not do this, the services will not start properly.\n\n12. You should then be able to start any or all of the services by pressing the\n\n\"play\" button.\n\n13. Finally, you need to re-edit the local copies of the path, Include and Lib\n\nvariables for your tame admin account if you use that account to connect to\n\nSybase.\n\nIt worked for me, as I said. I was able to run all 3 services simultaneously\n\nand connect from the local and external machines. There is no trick as neat as\n\nthe symbolic link on Unix. Links under NT work differently.\n\nReturn to top\n\n-------------------------------------------------------------------------------\n\n1.3.10: How do I capture a process's SQL?\n\n-------------------------------------------------------------------------------\n\nThis is a bit of a wide question, and there are many answers to it. Primarily,\n\nit depends on why you are trying to capture it. If you are trying to debug a\n\ntroublesome stored procedure that is behaving differently in production to how\n\nit did in testing, then you might look at the DBCC method. Alternatively, if\n\nyou wanted to do some longer term profiling, then auditing or one of the third\n\nparty tools might be the way forward. If you know of methods that are not\n\nincluded here, please let me know.\n\nDBCCs\n\nIf you want to look at the SQL a particular process is running at the moment,\n\none of the following should work. Not sure which versions of ASE these work\n\nwith. Remember to issue dbcc traceon(3604) before running any of the dbcc's so\n\nthat you can see the output at your terminal.\n\n* dbcc sqltext(spid)\n\n* dbcc pss(0, spid, 0)\n\nThe first of the commands issues the SQL of the spid only a bit like this:\n\n[27] BISCAY.master.1> dbcc sqltext(9)\n\n[27] BISCAY.master.2> go\n\nSQL Text: select spid, status, suser_name(suid), hostname,\n\ndb_name(dbid), cmd, cpu, physical_io, memusage,\n\nconvert(char(5),blocked) from master..sysprocesses\n\nDBCC execution completed. If DBCC printed error messages, contact a user with\n\nSystem Administrator (SA) role.\n\n[28] BISCAY.master.1>\n\nThe second issues an awful lot of other stuff before printing the text at the\n\nbottom. Mercifully, this means that you don't have to scroll up to search for\n\nthe SQL text, which is in much the same format as with dbcc sqltext.\n\nThere are a number of third party tools that will execute these commands from a\n\nlist of processes. One of the problems is that you do have to be 'sa' or have\n\n'sa_role' in order to run them.\n\nCertainly the first, and possibly both, have one major drawback, and that is\n\nthat they are limited to displaying about 400 bytes worth of text, which can be\n\na bit annoying. However, if what you are trying to do is catch a piece of rogue\n\nSQL that is causing a table scan or some other dastardly trick, a unique\n\ncomment in the early part of the query will lead to its easy identification.\n\nMonitor Server\n\nSince ASE 11.5, monitor server has had the capability for capturing a processes\n\nSQL. See Q1.6.2 for how to configure a Monitor Server Client. When you are\n\ndone, you can get see the SQL text from a process using the \"Process Current\n\nSQL Statement\" monitor. The output looks like this.\n\nAuditing\n\nThe second way of wanting to do this is for a number of processes for a period\n\nof time. There are several methods of doing this. Probably the most popular is\n\nto use auditing, and it is almost certainly the most popular because it\n\nrequires no additional software purchases.\n\nAuditing is a very powerful tool that can collect information on just about\n\neverything that happens on the server. It can be configured to capture\n\n'cmdtext' for any or all users on a system. The data will be loaded into the\n\nsysaudits database for later perusal. The SQL captured is not limited to a\n\nnumber of bytes, like the previous examples, but if it is more than 255 bytes\n\nlong, then it will span several audit records, which must be put back together\n\nto see the whole picture. To be honest, I am not sure what happens now that\n\nvarchars can be greater than 255 bytes in length. Personal experience with\n\nauditing leaves to think that the load on the server is up to about 3%,\n\ndepending on the number of engines you have (the more engines, the more of a\n\nload auditing is) and, obviously, the number of processes you wish to monitor.\n\nI calculated 3% based on auditing all of 400 users, each of which had 2\n\nconnections to the server, on a server with 7 engines.\n\nRibo\n\nAnother option for capturing the SQL text is to use the free Ribo utility that\n\nis provided with as part of ASE these days. This is a small server written in\n\nJava as an example of what can be done using jConnect. This utility is nice in\n\nthat it does not place any load on the ASE server. However, it probably has an\n\neffect on the client that is using it. This utility's other draw back is that\n\neach client that you wish to monitor via Ribo must be directly configured to\n\nuse it. It is not possibly mid-session to just magically turn it on.\n\nThe way it works is to act as an intermediary between the ASE server and the\n\nclient wishing to connect. All is SQL is passed through and executed exactly as\n\nif the client was directly connected, and the results passed back. What the\n\nRibo server does is enable you to save the inbound SQL to a file.\n\n3rd Party Tools\n\nAgain, there are a number of third party tools that do this job as well,\n\nOpenSwitch being one of them. There are also a number of third party tools that\n\ndo a better job than this. They do not have any impact on the client or the\n\nserver. They work by sniffing the network for relevant packets and then put\n\nthem pack together. In actuality, they do a lot more than just generate the\n\nSQL, but they are capable of that.\n\nReturn to top\n\n-------------------------------------------------------------------------------\n\nGeneral Troubleshooting User Database Administration ASE FAQ\n\nDavid Owen\n\nArchive-name: databases/sybase-faq/part12\n\nURL: http://www.isug.com/Sybase_FAQ\n\nVersion: 1.7\n\nMaintainer: David Owen\n\nLast-modified: 2003/03/02\n\nPosting-Frequency: posted every 3rd month\n\nA how-to-find-the-FAQ article is posted on the intervening months.\n\nSQL Fundamentals\n\n6.1.1 Are there alternatives to row at a time processing?\n\n6.1.2 When should I execute an sp_recompile?\n\n6.1.3 What are the different types of locks and what do they mean?\n\n6.1.4 What's the purpose of using holdlock?\n\n6.1.5 What's the difference between an update in place versus a deferred\n\nupdate? - see Q1.5.9\n\n6.1.6 How do I find the oldest open transaction?\n\n6.1.7 How do I check if log truncation is blocked?\n\n6.1.8 The timestamp datatype\n\n6.1.9 Stored Procedure Recompilation and Reresolution\n\n6.1.10 How do I manipulate binary columns?\n\n6.1.11 How do I remove duplicate rows from a table?\n\nSQL Advanced bcp ASE FAQ\n\n-------------------------------------------------------------------------------\n\n6.1.1: Alternative to row at a time processing\n\n-------------------------------------------------------------------------------\n\nSomeone asked how they could speed up their processing. They were batch\n\nupdating/inserting gobs of information. Their algorithm was something as\n\nfollows:\n\n... In another case I do:\n\nIf exists (select record) then\n\nupdate record\n\nelse\n\ninsert record\n\nI'm not sure which way is faster or if it makes a difference. I am doing\n\nthis for as many as 4000 records at a time (calling a stored procedure 4000\n\ntimes!). I am interesting in knowing any way to improve this. The parameter\n\ntranslation alone on the procedure calls takes 40 seconds for 4000 records.\n\nI am using exec in DB-Lib.\n\nWould RPC or CT-Lib be better/faster?\n\nA netter responded stating that it was faster to ditch their algorithm and to\n\napply a set based strategy:\n\nThe way to take your approach is to convert the row at a time processing\n\n(which is more traditional type of thinking) into a batch at a time (which\n\nis more relational type of thinking). Now I'm not trying to insult you to\n\nsay that you suck or anything like that, we just need to dial you in to\n\nthink in relational terms.\n\nThe idea is to do batches (or bundles) of rows rather than processing a\n\nsingle one at a time.\n\nSo let's take your example (since you didn't give exact values [probably\n\nout of kindness to save my eyeballs] I'll use your generic example to\n\nextend what I'm talking about):\n\nBefore:\n\nif exists (select record) then\n\nupdate record\n\nelse\n\ninsert record\n\nNew way:\n\n1. Load all your rows into a table named new_stuff in a separate work\n\ndatabase (call it work_db) and load it using bcp -- no third GL needed.\n\n1. truncate new_stuff and drop all indexes\n\n2. sort your data using UNIX sort and sort it by the clustered columns\n\n3. load it using bcp\n\n4. create clustered index using with sorted_data and any ancillary\n\nnon-clustered index.\n\n2. Assuming that your target table is called old_stuff\n\n3. Do the update in a single batch:\n\nbegin tran\n\n/* delete any rows in old_stuff which would normally\n\n** would have been updated... we'll insert 'em instead!\n\n** Essentially, treat the update as a delete/insert.\n\n*/\n\ndelete old_stuff\n\nfrom old_stuff,\n\nnew_stuff\n\nwhere old_stuff.key = new_stuff.key\n\n/* insert entire new table: this adds any rows\n\n** that would have been updated before and\n\n** inserts the new rows\n\n*/\n\ninsert old_stuff\n\nselect * from new_stuff\n\ncommit tran\n\nYou can do all this without writing 3-GL, using bcp and a shell script.\n\nA word of caution:\n\nSince these inserts/updates are batched orientated you may blow your\n\nlog if you attempt to do too many at a time. In order to avoid this use\n\nthe set rowcount directive to create bite-size chunks.\n\nBack to top\n\n-------------------------------------------------------------------------------\n\n6.1.2: When should I execute an sp_recompile?\n\n-------------------------------------------------------------------------------\n\nAn sp_recompile should be issued any time a new index is added or an update\n\nstatistics. Dropping an index will cause an automatic recompile of all objects\n\nthat are dependent on the table.\n\nThe sp_recompile command simply increments the schemacnt counter for the given\n\ntable. All dependent object counter's are checked against this counter and if\n\nthey are different the SQL Server recompiles the object.\n\nBack to top\n\n-------------------------------------------------------------------------------\n\n6.1.3: What are the different types of (All Page) locks?\n\n-------------------------------------------------------------------------------\n\nFirst off, just to get it out of the way, Sybase does now support row level\n\nlocking! (See Q6.1.11 for a description of the new features.) OK, that said and\n\nsone, if you think you need row level locking, you probably aren't thinking set\n\nbased -- see Q6.1.1 for set processing.\n\nThe SQL Server uses locking in order to ensure that sanity of your queries.\n\nWithout locking there is no way to ensure the integrity of your operation.\n\nImagine a transaction that debited one account and credited another. If the\n\ntransaction didn't lock out readers/writers then someone can potentially see\n\nerroneous data.\n\nEssentially, the SQL Server attempts to use the least intrusive lock possible,\n\npage lock, to satisfy a request. If it reaches around 200 page locks, then it\n\nescalates the lock to a table lock and releases all page locks thus performing\n\nthe task more efficiently.\n\nThere are three types of locks:\n\n* page locks\n\n* table locks\n\n* demand locks\n\nPage Locks\n\nThere are three types of page locks:\n\n* shared\n\n* exclusive\n\n* update\n\nshared\n\nThese locks are requested and used by readers of information. More than one\n\nconnection can hold a shared lock on a data page.\n\nThis allows for multiple readers.\n\nexclusive\n\nThe SQL Server uses exclusive locks when data is to be modified. Only one\n\nconnection may have an exclusive lock on a given data page. If a table is large\n\nenough and the data is spread sufficiently, more than one connection may update\n\ndifferent data pages of a given table simultaneously.\n\nupdate\n\nA update lock is placed during a delete or an update while the SQL Server is\n\nhunting for the pages to be altered. While an update lock is in place, there\n\ncan be shared locks thus allowing for higher throughput.\n\nThe update lock(s) are promoted to exclusive locks once the SQL Server is ready\n\nto perform the delete/update.\n\nTable Locks\n\nThere are three types of table locks:\n\n* intent\n\n* shared\n\n* exclusive\n\nintent\n\nIntent locks indicate the intention to acquire a shared or exclusive lock on a\n\ndata page. Intent locks are used to prevent other transactions from acquiring\n\nshared or exclusive locks on the given page.\n\nshared\n\nThis is similar to a page level shared lock but it affects the entire table.\n\nThis lock is typically applied during the creation of a non-clustered index.\n\nexclusive\n\nThis is similar to a page level exclusive lock but it affects the entire table.\n\nIf an update or delete affects the entire table, an exclusive table lock is\n\ngenerated. Also, during the creation of a clustered index an exclusive lock is\n\ngenerated.\n\nDemand Locks\n\nA demand lock prevents further shared locks from being set. The SQL Server sets\n\na demand lock to indicate that a transaction is next to lock a table or a page.\n\nThis avoids indefinite postponement if there was a flurry of readers when a\n\nwriter wished to make a change.\n\nBack to top\n\n-------------------------------------------------------------------------------\n\n6.1.4: What's the purpose of using holdlock?\n\n-------------------------------------------------------------------------------\n\nAll select/readtext statements acquire shared locks (see Q6.1.3) to retrieve\n\ntheir information. After the information is retrieved, the shared lock(s) is/\n\nare released.\n\nThe holdlock option is used within transactions so that after the select/\n\nreadtext statement the locks are held until the end of the transaction:\n\n* commit transaction\n\n* rollback transaction\n\nIf the holdlock is not used within a transaction, the shared locks are\n\nreleased.\n\nExample\n\nAssume we have the following two transactions and that each where-clause\n\nqualifies a single row:\n\ntx #1\n\nbegin transaction\n\n/* acquire a shared lock and hold it until we commit */\n\n1: select col_1 from table_a holdlock where id=1\n\n2: update table_b set col_3 = 'fiz' where id=12\n\ncommit transaction\n\ntx #2\n\nbegin transaction\n\n1: update table_a set col_2 = 'a' where id=1\n\n2: update table_c set col_3 = 'teo' where id=45\n\ncommit transaction\n\nIf tx#1, line 1 executes prior to tx#2, line 1, tx#2 waits to acquire its\n\nexclusive lock until tx#1 releases the shared level lock on the object. This\n\nwill not be done until the commit transaction, thus slowing user throughput.\n\nOn the other hand, if tx#1 had not used the holdlock attribute, tx#2 would not\n\nhave had to wait until tx#1 committed its transaction. This is because shared\n\nlevel locks are released immediately (even within transactions) when the\n\nholdlock attribute is not used.\n\nNote that the holdlock attribute does not stop another transaction from\n\nacquiring a shared level lock on the object (i.e. another reader). It only\n\nstops an exclusive level lock (i.e. a writer) from being acquired.\n\nBack to top\n\n-------------------------------------------------------------------------------\n\n6.1.6: How do I find the oldest open transaction?\n\n-------------------------------------------------------------------------------\n\nselect h.spid, u.name, p.cmd, h.name, h.starttime,\n\np.hostname, p.hostprocess, p.program_name\n\nfrom master..syslogshold h,\n\nmaster..sysprocesses p,\n\nmaster..sysusers u\n\nwhere h.spid = p.spid\n\nand p.suid = u.suid\n\nand h.spid != 0 /* not replication truncation point */\n\nBack to top\n\n-------------------------------------------------------------------------------\n\n6.1.7: How do I check if log truncation is blocked?\n\n-------------------------------------------------------------------------------\n\nSystem 11 and beyond:\n\nselect h.spid, convert(varchar(20), h.name), h.starttime\n\nfrom master..syslogshold h,\n\nsysindexes i\n\nwhere h.dbid = db_id()\n\nand h.spid != 0\n\nand i.id = 8 /* syslogs */\n\nand h.page in (i.first, i.first+1) /* first page of log = page of oldest xact */\n\nBack to top\n\n-------------------------------------------------------------------------------\n\n6.1.8: The timestamp datatype\n\n-------------------------------------------------------------------------------\n\nThe timestamp datatype is user-defined datatype supplied by Sybase, defined as:\n\nvarbinary(8) NULL\n\nIt has a special use when used to define a table column. A table may have at\n\nmost one column of type timestamp, and whenever a row containing a timestamp\n\ncolumn is inserted or updated the value in the timestamp column is\n\nautomatically updated. This much is covered in the documentation.\n\nWhat isn't covered is what the values placed in timestamp columns actually\n\nrepresent. It is a common misconception that timestamp values bear some\n\nrelation to calendar date and/or clock time. They don't - the datatype is\n\nbadly-named. SQL Server keeps a counter that is incremented for every write\n\noperation - you can see its current value via the global variable @@DBTS\n\n(though don't try and use this value to predict what will get inserted into a\n\ntimestamp column as every connection shares the same counter.)\n\nThe value is maintained between server startups and increases monotonically\n\nover time (though again you cannot rely on it this behaviour). Eventually the\n\nvalue will wrap, potentially causing huge problems, though you will be warned\n\nbefore it does - see Sybase Technical News Volume 5, Number 1 (see Q10.3.1).\n\nYou cannot convert this value to a datetime value - it is simply an 8-byte\n\ninteger.\n\nNote that the global timestamp value is used for recovery purposes in the\n\nevent of an RDMBS crash. As transactions are committed to the log each\n\ntransaction gets a unique timestamp value. The checkpoint process places a\n\nmarker in the log with its unique timestamp value. If the RDBMS crashes,\n\nrecovery is the process of looking for transactions that need to be rolled\n\nforward and/or backward from the checkpoint event. If a transaction spans\n\nacross the checkpoint event and it never competed it too needs to be rolled\n\nback.\n\nEssentially, this describes the write-ahead log protocol described by C.J.\n\nDate in An Introduction to Database Systems.\n\nSo what is it for? It was created in order to support the browse-mode functions\n\nof DB-Library (and for recovery as mentioned above). This enables an\n\napplication to easily support optimistic locking (See Q1.5.4) by guaranteeing a\n\nwatch column in a row will change value if any other column in that row is\n\nupdated. The browse functions checked that the timestamp value was still the\n\nsame as when the column was read before attempting an update. This behaviour is\n\neasy to replicate without necessarily using the actual client browse-mode\n\nfunctions - just read the timestamp value along with other data retrieved to\n\nthe client, and compare the stored value with the current value prior to an\n\nupdate.\n\nBack to top\n\n-------------------------------------------------------------------------------\n\n6.1.9: Stored Procedure Recompilation and Reresolution\n\n-------------------------------------------------------------------------------\n\nWhen a stored procedure is created, the text is placed in syscomments and a\n\nparse tree is placed in sysprocedures. At this stage there is no compiled query\n\nplan.\n\nA compiled query plan for the procedure only ever exists in memory (that is, in\n\nthe procedure cache) and is created under the following conditions:\n\n1. A procedure is executed for the first time.\n\n2. A procedure is executed by a second or subsequent user when the first plan\n\nin cache is still in use.\n\n3. The procedure cache is flushed by server restart or cache LRU flush\n\nprocedure.\n\n4. The procedure is executed or created using the with recompile option.\n\nIf the objects the procedure refers to change in some way - indexes dropped,\n\ntable definition changed, etc - the procedure will be reresolved - which\n\nupdates sysprocedures with a modified tree. Before 10.x the tree grows and in\n\nextreme cases the procedure can become too big to execute. This problem\n\ndisappears in Sybase System 11. This reresolution will always occur if the\n\nstored procedure uses temporary tables (tables that start with \"#\").\n\nThere is apparently no way of telling if a procedure has been reresolved.\n\nTraceflag 299 offers some relief, see Q1.3.3 for more information regarding\n\ntraceflags.\n\nThe Official Explanation -- Reresolution and Recompilation Explained\n\nWhen stored procedures are created, an entry is made in sysprocedures that\n\ncontains the query tree for that procedure. This query tree is the resolution\n\nof the procedure and the applicable objects referenced by it. The syscomments\n\ntable will contain the actual procedure text. No query plan is kept on disk.\n\nUpon first execution, the query tree is used to create (compile) a query plan\n\n(execution plan) which is stored in the procedure cache, a server memory\n\nstructure. Additional query plans will be created in cache upon subsequent\n\nexecutions of the procedure whenever all existing cached plans are in use. If a\n\ncached plan is available, it will be used.\n\nRecompilation is the process of using the existing query tree from\n\nsysprocedures to create (compile) a new plan in cache. Recompilation can be\n\ntriggered by any one of the following:\n\n* First execution of a stored procedure,\n\n* Subsequent executions of the procedure when all existing cached query plans\n\nare in use,\n\n* If the procedure is created with the recompile option, CREATE PROCEDURE\n\nsproc WITH RECOMPILE\n\n* If execution is performed with the recompile option, EXECUTE sproc WITH\n\nRECOMPILE\n\nRe-resolution is the process of updating the query tree in sysprocedures AND\n\nrecompiling the query plan in cache. Re-resolution only updates the query tree\n\nby adding the new tree onto the existing sysprocedures entry. This process\n\ncauses the procedure to grow in size which will eventually cause an execution\n\nerror (Msg 703 - Memory request failed because more than 64 pages are required\n\nto run the query in its present form. The query should be broken up into\n\nshorter queries if possible). Execution of a procedure that has been flagged\n\nfor re-resolution will cause the re-resolution to occur. To reduce the size of\n\na procedure, it must be dropped which will remove the entries from\n\nsysprocedures and syscomments. Then recreate the procedure.\n\nRe-resolution can be triggered by various activities most of which are\n\ncontrolled by SQL Server, not the procedure owner. One option is available for\n\nthe procedure owner to force re-resolution. The system procedure, sp_recompile,\n\nupdates the schema count in sysobjects for the table referenced. A DBA usually\n\nwill execute this procedure after creating new distribution pages by use of\n\nupdate statistics. The next execution of procedures that reference the table\n\nflagged by sp_recompile will have a new query tree and query plan created.\n\nAutomatic re-resolution is done by SQL Server in the following scenarios:\n\n* Following a LOAD DATABASE on the database containing the procedure,\n\n* After a table used by the procedure is dropped and recreated,\n\n* Following a LOAD DATABASE of a database where a referenced table resides,\n\n* After a database containing a referenced table is dropped and recreated,\n\n* Whenever a rule or default is bound or unbound to a referenced table.\n\nForcing automatic compression of procedures in System 10 is done with trace\n\nflag 241. System 11 should be doing automatic compression, though this is not\n\ncertain.\n\nWhen are stored procedures compiled?\n\nStored procedures are in a database as rows in sysprocedures, in the form of\n\nparse trees. They are later compiled into execution plans.\n\nA stored procedures is compiled:\n\n1. with the first EXECute, when the parse tree is read into cache\n\n2. with every EXECute, if CREATE PROCEDURE included WITH RECOMPILE\n\n3. with each EXECute specifying WITH RECOMPILE\n\n4. if the plans in cache for the procedure are all in use by other processes\n\n5. after a LOAD DATABASE, when all procedures in the database are recompiled\n\n6. if a table referenced by the procedure can not be opened (using object id),\n\nwhen recompilation is done using the table's name\n\n7. after a schema change in any referenced table, including:\n\n1. CREATE INDEX or DROP INDEX to add/delete an index\n\n2. ALTER TABLE to add a new column\n\n3. sp_bindefault or sp_unbindefault to add/delete a default\n\n4. sp_bindrule or sp_unbindrule to add/delete a rule\n\n8. after EXECute sp_recompile on a referenced table, which increments\n\nsysobjects.schema and thus forces re-compilation\n\nWhat causes re-resolution of a stored procedure?\n\nWhen a stored procedure references an object that is modified after the\n\ncreation of the stored procedure, the stored procedure must be re-resolved.\n\nRe-resolution is the process of verifying the location of referenced objects,\n\nincluding the object id number. Re-resolution will occur under the following\n\ncircumstances:\n\n1. One of the tables used by the stored procedure is dropped and re-created.\n\n2. A rule or default is bound to one of the tables (or unbound).\n\n3. The user runs sp_recompile on one of the tables.\n\n4. The database the stored procedure belongs to is re-loaded.\n\n5. The database that one of the stored procedure's tables is located in is\n\nre-loaded.\n\n6. The database that one of the stored procedure's tables is located in is\n\ndropped and re-created.\n\nWhat will cause the size of a stored procedure to grow?\n\nAny of the following will result in a stored procedure to grow when it is\n\nrecompiled:\n\n1. One of the tables used in the procedure is dropped and re-created.\n\n2. A new rule or default is bound to one of the tables or the user runs\n\nsp_recompile on one of the tables.\n\n3. The database containing the stored procedure is re-loaded.\n\nOther things causing a stored procedure to be re-compiled will not cause it to\n\ngrow. For example, dropping an index on one of the tables used in the procedure\n\nor doing EXEC WITH RECOMPILE.\n\nThe difference is between simple recompilation and re-resolution. Re-resolution\n\nhappens when one of the tables changes in such a way that the query trees\n\nstored in sysprocedures may be invalid. The datatypes, column offsets, object\n\nids or other parts of the tree may change. In this case, the server must\n\nre-allocate some of the query tree nodes. The old nodes are not de-allocated\n\n(there is no way to do this within a single procedure header), so the procedure\n\ngrows. In time, trying to execute the stored procedure will result in a 703\n\nerror about exceeding the 64 page limit for a query.\n\nBack to top\n\n-------------------------------------------------------------------------------\n\n6.1.10: How do I manipulate varbinary columns?\n\n-------------------------------------------------------------------------------\n\nThe question was posed - How do we manipulate varbinary columns, given that\n\nsome portion - like the 5th and 6th bit of the 3rd byte - of a (var)binary\n\ncolumn, needs to be updated? Here is one approach, provided by Bret Halford (\n\n***@sybase.com), using stored procedures to set or clear certain bits of a\n\ncertain byte of a field of a row with a given id:\n\ndrop table demo_table\n\ndrop procedure clear_bits\n\ndrop procedure set_bits\n\ngo\n\ncreate table demo_table (id numeric(18,0) identity, binary_col\n\nbinary(20))\n\ngo\n\ninsert demo_table values (0xffffffffffffffffffffffffffffffffffffffff)\n\ninsert demo_table values (0xaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa)\n\ninsert demo_table values (0x0000000000000000000000000000000000000000)\n\ngo\n\ncreate procedure clear_bits (\n\n@id numeric(18,0), -- primary key of row to be changed\n\n@bytenum tinyint, -- specifies which byte of binary_col to change\n\n@mask binary(1) -- bits to be cleared are zeroed,\n\n-- bits left alone are turned on\n\n-- so 0xff = clear all, 0xfb = clear bit 3\n\n)\n\nas\n\nupdate demo_table set binary_col =\n\nsubstring(binary_col,1,@bytenum-1)+\n\nconvert(binary(1),\n\nconvert(tinyint,substring(binary_col,@bytenum,1)) &\n\nconvert(tinyint,@mask)\n\n)+\n\nsubstring(binary_col,@bytenum+1,20)\n\nfrom demo_table\n\nwhere id = @id\n\ngo\n\ncreate procedure set_bits (\n\n@id numeric(18,0), -- primary key of row to be changed\n\n@bytenum tinyint, -- specifies which byte of binary_col to change\n\n@mask binary(1)) -- bits to be set are turned on\n\n-- bits left alone are zeroed\n\n-- so 0xff = set all, 0xfb = set all but 3\n\n)\n\nas\n\nupdate demo_table set binary_col =\n\nsubstring(binary_col,1,@bytenum-1)+\n\nconvert(binary(1),\n\nconvert(tinyint,substring(binary_col,@bytenum, 1)) |\n\nconvert(tinyint,@mask)\n\n)+\n\nsubstring(binary_col,@bytenum+1,20)\n\nfrom demo_table\n\nwhere id = @id\n\ngo\n\nselect * from demo_table\n\n-- clear bits 2,4,6,8 of byte 1 of row 1\n\nexec clear_bits 1,1,0xAA\n\n-- set bits 1-8 of byte 20 of row 3\n\nexec set_bits 3,20,0xff\n\n-- clear bits 1-8 of byte 4 of row 2\n\nexec clear_bits 2,4,0xff\n\n-- clear bit 3 of byte 5 of row 2\n\nexec clear_bits 2,5,0x08\n\nexec clear_bits 2,6,0x0f\n\nexec set_bits 2,10,0xff\n\ngo\n\nselect * from demo_table\n\ngo\n\nBack to top\n\n-------------------------------------------------------------------------------\n\n6.1.11: How do I remove duplicate rows from a table?\n\n-------------------------------------------------------------------------------\n\nThere are a number of different ways to achieve this, depending on what you are\n\ntrying to achieve. Usually, you are trying to remove duplication of a certain\n\nkey due to changes in business rules or recognition of a business rule that was\n\nnot applied when the database was originally built.\n\nProbably the quickest method is to build a copy of the original table:\n\nselect *\n\ninto temp_table\n\nfrom base_table\n\nwhere 1=0\n\nCreate a unique index on the columns that covers the duplicating rows with the\n\nignore_dup_key attribute. This may be more columns that the key for the table.\n\ncreate unique index temp_idx\n\non temp_table(col1, col2, ..., colN)\n\nwith ignore_dup_key\n\nNow, insert base_table into temp_table.\n\ninsert temp_table\n\nselect * from base_table\n\nYou probably want to ensure you have a very good backup of the base_table at\n\nthis point, coz your going to clear it out! You will also want to check to\n\nensure that the temp_table includes the rows you need. You also need to ensure\n\nthat there are no triggers on the base table (remember to keep a copy!) or RI\n\nconstraints. You probably do not want any of these to fire, or if they do, you\n\nare aware of the implications.\n\nNow you have a couple of choices. You can simply drop the original table and\n\nrename the temp table to the same name as the base table. Alternatively,\n\ntruncate the table and insert from the temp_table into the original table. You\n\nwould need to do this last if you did need the RI to fire on the table etc. I\n\nsuspect that in most cases dropping and renaming will be the best option.\n\nIf you want to simply see the duplicates in a table, the following query will\n\nhelp:\n\nselect key1, key2, ...\n\nfrom base_table\n\ngroup by key1, key2, key3, key4, ...\n\nhaving count(*) > 1\n\nSybase will actually allow a \"select *\", but it is not guaranteed to work.\n\nBack to top\n\n-------------------------------------------------------------------------------\n\nSQL Advanced bcp ASE FAQ\n\nDavid Owen\n\nArchive-name: databases/sybase-faq/part8\n\nURL: http://www.isug.com/Sybase_FAQ\n\nVersion: 1.7\n\nMaintainer: David Owen\n\nLast-modified: 2003/03/02\n\nPosting-Frequency: posted every 3rd month\n\nA how-to-find-the-FAQ article is posted on the intervening months.\n\nPerformance and Tuning\n\n1.5.1 What are the nitty gritty details on Performance and Tuning?\n\n1.5.2 What is best way to use temp tables in an OLTP environment?\n\n1.5.3 What's the difference between clustered and non-clustered indexes?\n\n1.5.4 Optimistic versus pessimistic locking?\n\n1.5.5 How do I force an index to be used?\n\n1.5.6 Why place tempdb and log on low numbered devices?\n\n1.5.7 Have I configured enough memory for ASE?\n\n1.5.8 Why should I use stored procedures?\n\n1.5.9 I don't understand showplan's output, please explain.\n\n1.5.10 Poor man's sp_sysmon.\n\n1.5.11 View MRU-LRU procedure cache chain.\n\n1.5.12 Improving Text/Image Type Performance\n\nServer Monitoring General Troubleshooting ASE FAQ\n\n-------------------------------------------------------------------------------\n\n1.5.1: Sybase ASE Performance and Tuning\n\n-------------------------------------------------------------------------------\n\nBefore going any further, Eric Miner (***@sybase.com) has made available\n\ntwo presentations that he made at Techwave 1999. The first covers the use of\n\noptdiag. The second covers features in the way the optimiser works in ASE\n\n11.9.2 and 12. These are Powerpoint slides converted to web pages, so they\n\nmight be tricky to read with a text based browser!\n\nAll Components Affect Response Time & Throughput\n\nWe often think that high performance is defined as a fast data server, but the\n\npicture is not that simple. Performance is determined by all these factors:\n\n* The client application itself:\n\n+ How efficiently is it written?\n\n+ We will return to this later, when we look at application tuning.\n\n* The client-side library:\n\n+ What facilities does it make available to the application?\n\n+ How easy are they to use?\n\n* The network:\n\n+ How efficiently is it used by the client/server connection?\n\n* The DBMS:\n\n+ How effectively can it use the hardware?\n\n+ What facilities does it supply to help build efficient fast\n\napplications?\n\n* The size of the database:\n\n+ How long does it take to dump the database?\n\n+ How long to recreate it after a media failure?\n\nUnlike some products which aim at performance on paper, Sybase aims at solving\n\nthe multi-dimensional problem of delivering high performance for real\n\napplications.\n\nOBJECTIVES\n\nTo gain an overview of important considerations and alternatives for the\n\ndesign, development, and implementation of high performance systems in the\n\nSybase client/server environment. The issues we will address are:\n\n* Client Application and API Issues\n\n* Physical Database Design Issues\n\n* Networking Issues\n\n* Operating System Configuration Issues\n\n* Hardware Configuration Issues\n\n* ASE Configuration Issues\n\nClient Application and Physical Database Design design decisions will\n\naccount for over 80% of your system's \"tuneable\" performance so ... plan\n\nyour project resources accordingly !\n\nIt is highly recommended that every project include individuals who have taken\n\nSybase Education's Performance and Tuning course. This 5-day course provides\n\nthe hands-on experience essential for success.\n\nClient Application Issues\n\n* Tuning Transact-SQL Queries\n\n* Locking and Concurrency\n\n* ANSI Changes Affecting Concurrency\n\n* Application Deadlocking\n\n* Optimizing Cursors in v10\n\n* Special Issues for Batch Applications\n\n* Asynchronous Queries\n\n* Generating Sequential Numbers\n\n* Other Application Issues\n\nTuning Transact-SQL Queries\n\n* Learn the Strengths and Weaknesses of the Optimizer\n\n* One of the largest factors determining performance is TSQL! Test not only\n\nfor efficient plans but also semantic correctness.\n\n* Optimizer will cost every permutation of accesses for queries involving 4\n\ntables or less. Joins of more than 4 tables are \"planned\" 4-tables at a\n\ntime (as listed in the FROM clause) so not all permutations are evaluated.\n\nYou can influence the plans for these large joins by the order of tables in\n\nthe FROM clause.\n\n* Avoid the following, if possible:\n\n+ What are SARGS?\n\nThis is short for search arguments. A search argument is essentially a\n\nconstant value such as:\n\no \"My company name\"\n\no 3448\n\nbut not:\n\no 344 + 88\n\no like \"%what you want%\"\n\n+ Mathematical Manipulation of SARGs\n\nSELECT name FROM employee WHERE salary * 12 > 100000\n\n+ Use of Incompatible Datatypes Between Column and its SARG\n\nFloat & Int, Char & Varchar, Binary & Varbinary are Incompatible;\n\nInt & Intn (allow nulls) OK\n\n+ Use of multiple \"OR\" Statements - especially on different columns in\n\nsame table. If any portion of the OR clause requires a table scan, it\n\nwill! OR Strategy requires additional cost of creating and sorting a\n\nwork table.\n\n+ Not using the leading portion of the index (unless the query is\n\ncompletely covered)\n\n+ Substituting \"OR\" with \"IN (value1, value2, ... valueN) Optimizer\n\nautomatically converts this to an \"OR\"\n\n+ Use of Non-Equal Expressions (!=) in WHERE Clause.\n\n* Use Tools to Evaluate and Tune Important/Problem Queries\n\n+ Use the \"set showplan on\" command to see the plan chosen as \"most\n\nefficient\" by optimizer. Run all queries through during development and\n\ntesting to ensure accurate access model and known performance.\n\nInformation comes through the Error Handler of a DB-Library\n\napplication.\n\n+ Use the \"dbcc traceon(3604, 302, 310)\" command to see each alternative\n\nplan evaluated by the optimizer. Generally, this is only necessary to\n\nunderstand why the optimizer won't give you the plan you want or need\n\n(or think you need)!\n\n+ Use the \"set statistics io on\" command to see the number of logical and\n\nphysical i/o's for a query. Scrutinize those queries with high logical\n\ni/o's.\n\n+ Use the \"set statistics time on\" command to see the amount of time\n\n(elapsed, execution, parse and compile) a query takes to run.\n\n+ If the optimizer turns out to be a \"pessimizer\", use the \"set forceplan\n\non\" command to change join order to be the order of the tables in the\n\nFROM clause.\n\n+ If the optimizer refuses to select the proper index for a table, you\n\ncan force it by adding the index id in parentheses after the table name\n\nin the FROM clause.\n\nSELECT * FROM orders(2), order_detail(1) WHERE ...\n\nThis may cause portability issues should index id's vary/change by\n\nsite !\n\nLocking and Concurrency\n\n* The Optimizer Decides on Lock Type and Granularity\n\n* Decisions on lock type (share, exclusive, or update) and granularity (page\n\nor table) are made during optimization so make sure your updates and\n\ndeletes don't scan the table !\n\n* Exclusive Locks are Only Released Upon Commit or Rollback\n\n* Lock Contention can have a large impact on both throughput and response\n\ntime if not considered both in the application and database design !\n\n* Keep transactions as small and short as possible to minimize blocking.\n\nConsider alternatives to \"mass\" updates and deletes such as a v10.0 cursor\n\nin a stored procedure which frequently commits.\n\n* Never include any \"user interaction\" in the middle of transactions.\n\n* Shared Locks Generally Released After Page is Read\n\n* Share locks \"roll\" through result set for concurrency. Only \"HOLDLOCK\" or\n\n\"Isolation Level 3\" retain share locks until commit or rollback. Remember\n\nalso that HOLDLOCK is for read-consistency. It doesn't block other readers\n\n!\n\n* Use optimistic locking techniques such as timestamps and the tsequal()\n\nfunction to check for updates to a row since it was read (rather than\n\nholdlock)\n\nANSI Changes Affecting Concurrency\n\n* Chained Transactions Risk Concurrency if Behavior not Understood\n\n* Sybase defaults each DML statement to its own transaction if not specified\n\n;\n\n* ANSI automatically begins a transaction with any SELECT, FETCH, OPEN,\n\nINSERT, UPDATE, or DELETE statement ;\n\n* If Chained Transaction must be used, extreme care must be taken to ensure\n\nlocks aren't left held by applications unaware they are within a\n\ntransaction! This is especially crucial if running at Level 3 Isolation\n\n* Lock at the Level of Isolation Required by the Query\n\n* Read Consistency is NOT a requirement of every query.\n\n* Choose level 3 only when the business model requires it\n\n* Running at Level 1 but selectively applying HOLDLOCKs as needed is safest\n\n* If you must run at Level 3, use the NOHOLDLOCK clause when you can !\n\n* Beware of (and test) ANSI-compliant third-party applications for\n\nconcurrency\n\nApplication Deadlocking\n\nPrior to ASE 10 cursors, many developers simulated cursors by using two or more\n\nconnections (dbproc's) and divided the processing between them. Often, this\n\nmeant one connection had a SELECT open while \"positioned\" UPDATEs and DELETEs\n\nwere issued on the other connection. The approach inevitably leads to the\n\nfollowing problem:\n\n1. Connection A holds a share lock on page X (remember \"Rows Pending\" on SQL\n\nServer leave a share lock on the \"current\" page).\n\n2. Connection B requests an exclusive lock on the same page X and waits...\n\n3. The APPLICATION waits for connection B to succeed before invoking whatever\n\nlogic will remove the share lock (perhaps dbnextrow). Of course, that never\n\nhappens ...\n\nSince Connection A never requests a lock which Connection B holds, this is NOT\n\na true server-side deadlock. It's really an \"application\" deadlock !\n\nDesign Alternatives\n\n1. Buffer additional rows in the client that are \"nonupdateable\". This forces\n\nthe shared lock onto a page on which the application will not request an\n\nexclusive lock.\n\n2. Re-code these modules with CT-Library cursors (aka. server-side cursors).\n\nThese cursors avoid this problem by disassociating command structures from\n\nconnection structures.\n\n3. Re-code these modules with DB-Library cursors (aka. client-side cursors).\n\nThese cursors avoid this problem through buffering techniques and\n\nre-issuing of SELECTs. Because of the re-issuing of SELECTs, these cursors\n\nare not recommended for high transaction sites !\n\nOptimizing Cursors with v10.0\n\n* Always Declare Cursor's Intent (i.e. Read Only or Updateable)\n\n* Allows for greater control over concurrency implications\n\n* If not specified, ASE will decide for you and usually choose updateable\n\n* Updateable cursors use UPDATE locks preventing other U or X locks\n\n* Updateable cursors that include indexed columns in the update list may\n\ntable scan\n\n* SET Number of Rows for each FETCH\n\n* Allows for greater Network Optimization over ANSI's 1- row fetch\n\n* Rows fetched via Open Client cursors are transparently buffered in the\n\nclient:\n\nFETCH -> Open Client <- N rows\n\nBuffers\n\n* Keep Cursor Open on a Commit / Rollback\n\n* ANSI closes cursors with each COMMIT causing either poor throughput (by\n\nmaking the server re-materialize the result set) or poor concurrency (by\n\nholding locks)\n\n* Open Multiple Cursors on a Single Connection\n\n* Reduces resource consumption on both client and Server\n\n* Eliminates risk of a client-side deadlocks with itself\n\nSpecial Issues for Batch Applications\n\nASE was not designed as a batch subsystem! It was designed as an RBDMS for\n\nlarge multi-user applications. Designers of batch-oriented applications should\n\nconsider the following design alternatives to maximize performance :\n\nDesign Alternatives :\n\n* Minimize Client/Server Interaction Whenever Possible\n\n* Don't turn ASE into a \"file system\" by issuing single table / single row\n\nrequests when, in actuality, set logic applies.\n\n* Maximize TDS packet size for efficient Interprocess Communication (v10\n\nonly)\n\n* New ASE 10.0 cursors declared and processed entirely within stored\n\nprocedures and triggers offer significant performance gains in batch\n\nprocessing.\n\n* Investigate Opportunities to Parallelize Processing\n\n* Breaking up single processes into multiple, concurrently executing,\n\nconnections (where possible) will outperform single streamed processes\n\neverytime.\n\n* Make Use of TEMPDB for Intermediate Storage of Useful Data\n\nAsynchronous Queries\n\nMany, if not most, applications and 3rd Party tools are coded to send queries\n\nwith the DB-Library call dbsqlexec( ) which is a synchronous call ! It sends a\n\nquery and then waits for a response from ASE that the query has completed !\n\nDesigning your applications for asynchronous queries provides many benefits:\n\n1. A \"Cooperative\" multi-tasking application design under Windows will allow\n\nusers to run other Windows applications while your long queries are\n\nprocessed !\n\n2. Provides design opportunities to parallize work across multiple ASE\n\nconnections.\n\nImplementation Choices:\n\n* System 10 Client Library Applications:\n\n* True asynchronous behaviour is built into the entire library. Through the\n\nappropriate use of call-backs, asynchronous behavior is the normal\n\nprocessing paradigm.\n\n* Windows DB-Library Applications (not true async but polling for data):\n\n* Use dbsqlsend(), dbsqlok(), and dbdataready() in conjunction with some\n\nadditional code in WinMain() to pass control to a background process. Code\n\nsamples which outline two different Windows programming approaches (a\n\nPeekMessage loop and a Windows Timer approach) are available in the\n\nMicrosoft Software Library on Compuserve (GO MSL). Look for SQLBKGD.ZIP\n\n* Non-PC DB-Library Applications (not true async but polling for data):\n\n* Use dbsqlsend(), dbsqlok(), and dbpoll() to utilize non-blocking functions.\n\nGenerating Sequential Numbers Many applications use unique sequentially\n\nincreasing numbers, often as primary keys. While there are good benefits to\n\nthis approach, generating these keys can be a serious contention point if not\n\ncareful. For a complete discussion of the alternatives, download Malcolm\n\nColton's White Paper on Sequential Keys from the SQL Server Library of our\n\nOpenLine forum on Compuserve.\n\nThe two best alternatives are outlined below.\n\n1. \"Primary Key\" Table Storing Last Key Assigned\n\n+ Minimize contention by either using a seperate \"PK\" table for each user\n\ntable or padding out each row to a page. Make sure updates are\n\n\"in-place\".\n\n+ Don't include the \"PK\" table's update in the same transaction as the\n\nINSERT. It will serialize the transactions.\n\nBEGIN TRAN\n\nUPDATE pk_table SET nextkey = nextkey + 1\n\n[WHERE table_name = @tbl_name]\n\nCOMMIT TRAN\n\n/* Now retrieve the information */\n\nSELECT nextkey FROM pk_table\n\nWHERE table_name = @tbl_name]\n\n+ \"Gap-less\" sequences require additional logic to store and retrieve\n\nrejected values\n\n2. IDENTITY Columns (v10.0 only)\n\n+ Last key assigned for each table is stored in memory and automatically\n\nincluded in all INSERTs (BCP too). This should be the method of choice\n\nfor performance.\n\n+ Choose a large enough numeric or else all inserts will stop once the\n\nmax is hit.\n\n+ Potential rollbacks in long transactions may cause gaps in the sequence\n\n!\n\nOther Application Issues\n\n+ Transaction Logging Can Bottleneck Some High Transaction Environments\n\n+ Committing a Transaction Must Initiate a Physical Write for\n\nRecoverability\n\n+ Implementing multiple statements as a transaction can assist in these\n\nenvironment by minimizing the number of log writes (log is flushed to\n\ndisk on commits).\n\n+ Utilizing the Client Machine's Processing Power Balances Load\n\n+ Client/Server doesn't dictate that everything be done on Server!\n\n+ Consider moving \"presentation\" related tasks such as string or\n\nmathematical manipulations, sorting, or, in some cases, even\n\naggregating to the client.\n\n+ Populating of \"Temporary\" Tables Should Use \"SELECT INTO\" - balance\n\nthis with dynamic creation of temporary tables in an OLTP environment.\n\nDynamic creation may cause blocks in your tempdb.\n\n+ \"SELECT INTO\" operations are not logged and thus are significantly\n\nfaster than there INSERT with a nested SELECT counterparts.\n\n+ Consider Porting Applications to Client Library Over Time\n\n+ True Asynchronous Behavior Throughout Library\n\n+ Array Binding for SELECTs\n\n+ Dynamic SQL\n\n+ Support for ClientLib-initiated callback functions\n\n+ Support for Server-side Cursors\n\n+ Shared Structures with Server Library (Open Server 10)\n\nPhysical Database Design Issues\n\n+ Normalized -vs- Denormalized Design\n\n+ Index Selection\n\n+ Promote \"Updates-in-Place\" Design\n\n+ Promote Parallel I/O Opportunities\n\nNormalized -vs- Denormalized\n\n+ Always Start with a Completely Normalized Database\n\n+ Denormalization should be an optimization taken as a result of a\n\nperformance problem\n\n+ Benefits of a normalized database include :\n\n1. Accelerates searching, sorting, and index creation since tables are\n\nnarrower\n\n2. Allows more clustered indexes and hence more flexibility in tuning\n\nqueries, since there are more tables ;\n\n3. Accelerates index searching since indexes tend to be narrower and\n\nperhaps shorter ;\n\n4. Allows better use of segments to control physical placement of\n\ntables ;\n\n5. Fewer indexes per table, helping UPDATE, INSERT, and DELETE\n\nperformance ;\n\n6. Fewer NULLs and less redundant data, increasing compactness of the\n\ndatabase ;\n\n7. Accelerates trigger execution by minimizing the extra integrity\n\nwork of maintaining redundant data.\n\n8. Joins are Generally Very Fast Provided Proper Indexes are Available\n\n9. Normal caching and cindextrips parameter (discussed in Server\n\nsection) means each join will do on average only 1-2 physical I/Os.\n\n10. Cost of a logical I/O (get page from cache) only 1-2 milliseconds.\n\n3. There Are Some Good Reasons to Denormalize\n\n1. All queries require access to the \"full\" set of joined data.\n\n2. Majority of applications scan entire tables doing joins.\n\n3. Computational complexity of derived columns require storage for SELECTs\n\n4. Others ...\n\nIndex Selection\n\n+ Without a clustered index, all INSERTs and \"out-of-place\" UPDATEs go to\n\nthe last page. The lock contention in high transaction environments\n\nwould be prohibitive. This is also true for INSERTs to a clustered\n\nindex on a monotonically increasing key.\n\n+ High INSERT environments should always cluster on a key which provides\n\nthe most \"randomness\" (to minimize lock / device contention) that is\n\nusable in many queries. Note this is generally not your primary key !\n\n+ Prime candidates for clustered index (in addition to the above) include\n\n:\n\no Columns Accessed by a Range\n\no Columns Used with Order By, Group By, or Joins\n\n+ Indexes Help SELECTs and Hurt INSERTs\n\n+ Too many indexes can significantly hurt performance of INSERTs and\n\n\"out-of-place\" UPDATEs.\n\n+ Prime candidates for nonclustered indexes include :\n\no Columns Used in Queries Requiring Index Coverage\n\no Columns Used to Access Less than 20% (rule of thumb) of the Data.\n\n+ Unique indexes should be defined as UNIQUE to help the optimizer\n\n+ Minimize index page splits with Fillfactor (helps concurrency and\n\nminimizes deadlocks)\n\n+ Keep the Size of the Key as Small as Possible\n\n+ Accelerates index scans and tree traversals\n\n+ Use small datatypes whenever possible . Numerics should also be used\n\nwhenever possible as they compare faster than strings.\n\nPromote \"Update-in-Place\" Design\n\n+ \"Update-in-Place\" Faster by Orders of Magnitude\n\n+ Performance gain dependent on number of indexes. Recent benchmark (160\n\nbyte rows, 1 clustered index and 2 nonclustered) showed 800%\n\ndifference!\n\n+ Alternative (\"Out-of-Place\" Update) implemented as a physical DELETE\n\nfollowed by a physical INSERT. These tactics result in:\n\n1. Increased Lock Contention\n\n2. Increased Chance of Deadlock\n\n3. Decreased Response Time and Throughput\n\n+ Currently (System 10 and below), Rules for \"Update-in-Place\" Behavior\n\nInclude :\n\n1. Columns updated can not be variable length or allow nulls\n\n2. Columns updated can not b"
    }
}
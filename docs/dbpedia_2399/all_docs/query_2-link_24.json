{
    "id": "dbpedia_2399_2",
    "rank": 24,
    "data": {
        "url": "https://racismandtechnology.center/theme/algorithmic-bias/",
        "read_more_link": "",
        "language": "en",
        "title": "Algorithmic Bias",
        "top_image": "https://racismandtechnology.center/wp-content/uploads/default-social-media-image.jpg",
        "meta_img": "https://racismandtechnology.center/wp-content/uploads/default-social-media-image.jpg",
        "images": [
            "https://racismandtechnology.center/wp-content/uploads/logo-1.png",
            "https://racismandtechnology.center/wp-content/uploads/7808-600x400.jpg",
            "https://racismandtechnology.center/wp-content/uploads/7700-600x400.jpg",
            "https://racismandtechnology.center/wp-content/uploads/comuzi-mirror-600x338.jpg",
            "https://racismandtechnology.center/wp-content/uploads/7642-600x315.jpg",
            "https://racismandtechnology.center/wp-content/uploads/bevolking-naar-herkomst-600x650.png",
            "https://racismandtechnology.center/wp-content/uploads/368-Hans-de-Zwart-Karen-Palmer-600x338.jpg",
            "https://racismandtechnology.center/wp-content/uploads/mylife-600x315.png",
            "https://racismandtechnology.center/wp-content/uploads/7400-600x315.jpg",
            "https://racismandtechnology.center/wp-content/uploads/algorithm-audit-report-cover-600x512.png",
            "https://racismandtechnology.center/wp-content/uploads/schiphol-vertrek-600x400.jpg",
            "https://racismandtechnology.center/wp-content/uploads/face-not-found-600x423.png",
            "https://racismandtechnology.center/wp-content/uploads/i-am-not-a-typo-campaign-600x598.png",
            "https://racismandtechnology.center/wp-content/uploads/7330.jpg",
            "https://racismandtechnology.center/wp-content/uploads/7270-600x338.jpg",
            "https://racismandtechnology.center/wp-content/uploads/7131-600x314.jpg",
            "https://racismandtechnology.center/wp-content/uploads/bloomberg-openai-resumes-600x315.png",
            "https://racismandtechnology.center/wp-content/uploads/resisting-borders.jpg",
            "https://racismandtechnology.center/wp-content/uploads/6863-600x371.jpg",
            "https://racismandtechnology.center/wp-content/uploads/6865-600x300.jpg",
            "https://racismandtechnology.center/wp-content/uploads/6864-600x400.jpg",
            "https://racismandtechnology.center/wp-content/uploads/6809-600x338.jpg",
            "https://racismandtechnology.center/wp-content/uploads/blind-voor-mens-en-recht-600x600.jpg",
            "https://racismandtechnology.center/wp-content/uploads/turnitin-ai-detection-600x241.jpg",
            "https://racismandtechnology.center/wp-content/uploads/the-mark-up-isps-600x314.jpg",
            "https://racismandtechnology.center/wp-content/uploads/belastingdienst-algoritmen-600x400.jpg",
            "https://racismandtechnology.center/wp-content/uploads/6397-600x338.jpg",
            "https://racismandtechnology.center/wp-content/uploads/6267-600x401.jpg",
            "https://racismandtechnology.center/wp-content/uploads/6211-600x338.jpg",
            "https://racismandtechnology.center/wp-content/uploads/6184-600x315.jpg",
            "https://racismandtechnology.center/wp-content/uploads/6185-600x300.jpg",
            "https://racismandtechnology.center/wp-content/uploads/breeze-600x315.png",
            "https://racismandtechnology.center/wp-content/uploads/6073-600x338.jpg",
            "https://racismandtechnology.center/wp-content/uploads/ftm-veenendaal-600x400.jpg",
            "https://racismandtechnology.center/wp-content/uploads/6074-600x300.jpg",
            "https://racismandtechnology.center/wp-content/uploads/6070-600x469.jpg",
            "https://racismandtechnology.center/wp-content/uploads/6031-600x314.jpg",
            "https://racismandtechnology.center/wp-content/uploads/6032-600x314.jpg",
            "https://racismandtechnology.center/wp-content/uploads/risico-taxatie-instrument-600x338.png",
            "https://racismandtechnology.center/wp-content/uploads/5893-600x315.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2024-08-20T00:00:00",
        "summary": "",
        "meta_description": "Curated news, research and activism on the intersection between racism and technology.",
        "meta_lang": "en",
        "meta_favicon": "https://racismandtechnology.center/wp-content/uploads/cropped-favicon-32x32.png",
        "meta_site_name": "Racism and Technology Center",
        "canonical_link": "https://racismandtechnology.center/theme/algorithmic-bias/",
        "text": "Overheid gebruikt nog steeds volop discriminerende algoritmes\n\nOndanks de toeslagenaffaire gingen vorig jaar overheidsorganisaties door met het gebruik van ‘ondoordachte algoritmes’, schrijft de Autoriteit Persoonsgegevens.\n\nBy Jeroen Piersma for Het Financieele Dagblad on July 2, 2024\n\nNa het toeslagen schandaal, waarbij onder andere veel eenoudergezinnen en gezinnen met een migratieachtergrond onterecht van fraude werden beschuldigd, werd pijnlijk duidelijk dat niet alleen mensen discrimineren, maar algoritmes ook\n\nEr werd beloofd dat deze systemen eerlijker zouden worden, maar uit het nieuwe jaarverslag van de Autoriteit Persoonsgegevens blijkt dat er sindsdien weinig is verbeterd. Algoritmes categoriseren mensen met bepaalde kenmerken nog steeds onterecht als risico. Noëlle Cecilia, medeoprichter van Brush AI (@ai.brush) was zondag te gast bij Mandy. Zij maakt algoritmes voor bedrijven en deed een jaar lang onderzoek naar de eerlijkheid en discriminatie ervan. Zij legt ons uit waarom de mindset moet veranderen bij het ontwikkelen van AI-systemen.\n\nBy Noëlle Cecilia for Instagram on July 9, 2024\n\nHow and why algorithms discriminate\n\nAutomated decision-making systems contain hidden discriminatory prejudices. We’ll explain the causes, possible consequences, and the reasons why existing laws do not provide sufficient protection against algorithmic discrimination.\n\nBy Pie Sombetzki for AlgorithmWatch on June 26, 2024\n\nRacist Technology in Action: AI detection of emotion rates Black basketball players as ‘angrier’ than their White counterparts\n\nIn 2018, Lauren Rhue showed that two leading emotion detection software products had a racial bias against Black Men: Face++ thought they were more angry, and Microsoft AI thought they were more contemptuous.\n\nContinue reading “Racist Technology in Action: AI detection of emotion rates Black basketball players as ‘angrier’ than their White counterparts” →\n\nAre you 80% angry and 2% sad? Why ‘emotional AI’ is fraught with problems\n\nAI that purports to read our feelings may enhance user experience but concerns over misuse and bias mean the field is fraught with potential dangers.\n\nBy Ned Carter Miles for The Guardian on June 23, 2024\n\nThe datafication of race and ethnicity\n\nThe New York Times published a fascinating overview of the American census forms since the late 18th century. It shows how the form keeps trying to ‘capture’ the country’s demographics, “creating and reshaping the ever-changing views of racial and ethnic identity.”\n\nContinue reading “The datafication of race and ethnicity” →\n\nPodcast: Art as a prophetic activity for the future of AI\n\nOur own Hans de Zwart was a guest in the ‘Met Nerds om Tafel’ podcast. With Karen Palmer (creator of Consensus Gentium, a film about surveillance that watches you back), they discussed the role of art and storytelling in getting us ready for the future.\n\nContinue reading “Podcast: Art as a prophetic activity for the future of AI” →\n\nRacist Technology in Action: MyLife.com and discriminatory predation\n\nMyLife.com is one of those immoral American companies that collect personal information to sell onwards as profiles on the one hand, while at the same suggesting to the people that are being profiled that incriminating information about them exists online that they can get removed by buying a subscription (that then does nothing and auto-renews in perpetuity).\n\nContinue reading “Racist Technology in Action: MyLife.com and discriminatory predation” →\n\nImage generators are trying to hide their biases – and they make them worse\n\nIn the run-up to the EU elections, AlgorithmWatch has investigated which election-related images can be generated by popular AI systems. Two of the largest providers don’t adhere to security measures they have announced themselves recently.\n\nBy Nicolas Kayser-Bril for AlgorithmWatch on May 29, 2024\n\nStudents with a non-European migration background had a 3.0 times higher chance of receiving an unfounded home visit from the Dutch student grants fraud department\n\nLast year, Investico revealed how DUO, the Dutch organization for administering student grants, was using a racist algorithm to decide which students would get a home visit to check for fraudulent behaviour. The Minister of Education immediately stopped the use of the algorithm.\n\nContinue reading “Students with a non-European migration background had a 3.0 times higher chance of receiving an unfounded home visit from the Dutch student grants fraud department” →\n\nDutch Ministry of Foreign Affairs dislikes the conclusions of a solid report that marks their visa process as discriminatory so buys a shoddy report saying the opposite\n\nFor more than a year now, the Dutch Ministry of Foreign Affairs has ignored advice from its experts and continued its use of discriminatory risk profiling of visa applicants.\n\nContinue reading “Dutch Ministry of Foreign Affairs dislikes the conclusions of a solid report that marks their visa process as discriminatory so buys a shoddy report saying the opposite” →\n\nDutch Institute of Human Rights tells the government: “Test educational tools for possible discriminatory effects”\n\nThe Dutch Institute for Human Rights has commissioned research exploring the possible risks for discrimination and exclusion relating to the use of algorithms in education in the Netherlands.\n\nContinue reading “Dutch Institute of Human Rights tells the government: “Test educational tools for possible discriminatory effects”” →\n\nRacist Technology in Action: Autocorrect is Western- and White-focused\n\nThe “I am not a typo” campaign is asking the tech giants to update their name dictionaries and stop autocorrecting the 41% of names given to babies in England and Wales.\n\nContinue reading “Racist Technology in Action: Autocorrect is Western- and White-focused” →\n\nVervolgonderzoek bevestigt indirecte discriminatie controles uitwonendenbeurs\n\nDUO heeft de onafhankelijke stichting Algorithm Audit vervolgonderzoek laten doen naar de manier waarop DUO tussen 2012 en 2023 controleerde of een student terecht studiefinanciering ontving voor uitwonende studenten of niet. De conclusies van het vervolgonderzoek bevestigen dat studenten met een migratieachtergrond hierbij indirect zijn gediscrimineerd.\n\nFrom Dienst Uitvoering Onderwijs (DUO) on May 21, 2024\n\nFouten herstellen we later wel: hoe de gemeente een dubieus algoritme losliet op Rotterdammers\n\nHet was te mooi om waar te zijn: een algoritme om fraude in de bijstand op te sporen. Ondanks waarschuwingen bleef de gemeente Rotterdam er bijna vier jaar lang in geloven. Een handjevol ambtenaren, zich onvoldoende bewust van ethische risico’s, kon jarenlang ongestoord experimenteren met de data van kwetsbare mensen.\n\nBy Romy van Dijk and Saskia Klaassen for Vers Beton on October 23, 2023\n\nLET OP, zegt de computer van Buitenlandse Zaken bij tienduizenden visumaanvragen. Is dat discriminatie?\n\nDiscriminerend algoritme: Volgens een onderzoek discrimineerde het algoritme dat Buitenlandse Zaken gebruikt om visumaanvragen te beoordelen. Uit onvrede met die conclusie vroeg het ministerie om een second opinion.\n\nBy Carola Houtekamer and Merijn Rengers for NRC on May 1, 2024\n\nOpenAI’s GPT sorts resumes with a racial bias\n\nBloomberg did a clever experiment: they had OpenAI’s GPT rank resumes and found that it shows a gender and racial bias just on the basis of the name of the candidate.\n\nContinue reading “OpenAI’s GPT sorts resumes with a racial bias” →\n\nRacist Technology in Action: The UK Home Office’s Sorting Algorithm and the Racist Violence of Borders\n\nIn 2020, two NGOs finally forced the UK Home Office’s hand, compelling it to abandon its secretive and racist algorithm for sorting visitor visa applications. Foxglove and The Joint Council for the Welfare of Immigrants (JCWI) had been battling the algorithm for years, arguing that it is a form of institutionalized racism and calling it “speedy boarding for white people.”\n\nContinue reading “Racist Technology in Action: The UK Home Office’s Sorting Algorithm and the Racist Violence of Borders” →\n\nThe child benefits scandal: no lessons learned\n\n“It could happen again tomorrow” is one of the main devastating conclusions of the parlementary inquiry following the child benefits scandal.\n\nContinue reading “The child benefits scandal: no lessons learned” →\n\nRacist Technology in Action: Slower internet service for the same price in U.S. lower income areas with fewer White residents\n\nInvestigative reporting by The Markup showed how U.S. internet providers offer wildly different internet speeds for the same monthly fee. The neighbourhoods with the worst deals had lower median incomes and were very often the least White.\n\nContinue reading “Racist Technology in Action: Slower internet service for the same price in U.S. lower income areas with fewer White residents” →\n\nDutch Tax Office keeps breaking the law with their risk profiling algorithms\n\nEven though the Dutch tax office (the Belastingdienst) was advised to immediately stop the use of three risk profiling algorithms, the office decided to continue their use, according to this reporting by Follow the Money.\n\nContinue reading “Dutch Tax Office keeps breaking the law with their risk profiling algorithms” →\n\nBelastingdienst blijft wet overtreden met mogelijk discriminerende fraude-algoritmen\n\nNa het toeslagenschandaal kreeg de Belastingdienst het advies om drie mogelijk discriminerende fraude-algoritmen onmiddellijk stop te zetten. Toch besloot de fiscus ermee door te gaan: het organisatiebelang woog zwaarder dan naleving van de wet en bescherming van grondrechten. Dat blijkt uit documenten die twee jaar nadat om openbaring was verzocht aan Follow the Money zijn vrijgegeven. ‘Onbegrijpelijk en verbijsterend.’\n\nBy David Davidson and Sebastiaan Brommersma for Follow the Money on December 14, 2023\n\nNot a solution: Meta’s new AI system to contain discriminatory ads\n\nMeta has deployed a new AI system on Facebook and Instagram to fix its algorithmic bias problem for housing ads in the US. But it’s probably more band-aid than AI fairness solution. Gaps in Meta’s compliance report make it difficult to verify if the system is working as intended, which may preview what’s to come from Big Tech compliance reporting in the EU.\n\nBy John Albert for AlgorithmWatch on November 17, 2023\n\nAI is nog lang geen wondermiddel – zeker niet in het ziekenhuis\n\nTumoren ontdekken, nieuwe medicijnen ontwikkelen – beloftes genoeg over wat kunstmatige intelligentie kan betekenen voor de medische wereld. Maar voordat je zulk belangrijk werk kunt overlaten aan technologie, moet je precies snappen hoe die werkt. En zover zijn we nog lang niet.\n\nBy Maurits Martijn for De Correspondent on November 6, 2023\n\nInstagram apologises for adding ‘terrorist’ to some Palestinian user profiles\n\nParent company Meta says bug caused ‘inappropriate’ auto-translations and was now fixed while employee says it pushed ‘a lot of people over the edge’.\n\nBy Josh Taylor for The Guardian on October 20, 2023\n\nFacebook Report Concludes Company Censorship Violated Palestinian Human Rights\n\nA report commission by Meta — Facebook and Instagram’s parent company — found bias against Palestinians during an Israeli assault last May.\n\nBy Sam Biddle for The Intercept on September 21, 2022\n\nEqual love: Dating App Breeze seeks to address Algorithmic Discrimination\n\nIn a world where swiping left or right is the main route to love, whose profiles dating apps show you can change the course of your life.\n\nContinue reading “Equal love: Dating App Breeze seeks to address Algorithmic Discrimination” →\n\nAl Jazeera asks: Can AI eliminate human bias or does it perpetuate it?\n\nIn its online series of digital dilemmas, Al Jazeera takes a look at AI in relation to social inequities. Loyal readers of this newsletter will recognise many of the examples they touch on, like how Stable Diffusion exacerbates and amplifies racial and gender disparities or the Dutch childcare benefits scandal.\n\nContinue reading “Al Jazeera asks: Can AI eliminate human bias or does it perpetuate it?” →\n\nRacist Technology in Action: Flagged as risky simply for requesting social assistance in Veenendaal, The Netherlands\n\nThis collaborative investigative effort by Spotlight Bureau, Lighthouse Reports and Follow the Money, dives into the story of a Moroccan-Dutch family in Veenendaal which was targeted for fraud by the Dutch government.\n\nContinue reading “Racist Technology in Action: Flagged as risky simply for requesting social assistance in Veenendaal, The Netherlands” →\n\nThese new tools could make AI vision systems less biased\n\nTwo new papers from Sony and Meta describe novel methods to make bias detection fairer.\n\nBy Melissa Heikkilä for MIT Technology Review on September 25, 2023\n\nTechnologie raakt sommige groepen mensen in onze samenleving harder dan anderen (en dat zou niet zo mogen zijn)\n\nBij het gebruik van technologie worden onze maatschappelijke problemen gereflecteerd en soms verergerd. Die maatschappelijke problemen kennen een lange geschiedenis van oneerlijke machtsstructuren, racisme, seksisme en andere vormen van discriminatie. Wij zien het als onze taak om die oneerlijke structuren te herkennen en ons daartegen te verzetten.\n\nBy Evely Austin, Ilja Schurink and Nadia Benaissa for Bits of Freedom on September 12, 2023\n\nPolitie stopt met gewraakt algoritme dat ‘voorspelt’ wie in de toekomst geweld gebruikt\n\nDe politie stopt ‘per direct’ met het algoritme waarmee ze voorspelt of iemand in de toekomst geweld gaat gebruiken. Eerder deze week onthulde Follow the Money dat het zogeheten Risicotaxatie Instrument Geweld op ethisch en statistisch gebied ondermaats is.\n\nBy David Davidson for Follow the Money on August 25, 2023\n\nDubieus algoritme van de politie ‘voorspelt’ wie in de toekomst geweld zal plegen\n\nDe politie voorspelt al sinds 2015 met een algoritme wie er in de toekomst geweld zal plegen. Van Marokkaanse en Antilliaanse Nederlanders werd die kans vanwege hun achtergrond groter geschat. Dat gebeurt nu volgens de politie niet meer, maar daarmee zijn de gevaren van het model niet opgelost. ‘Aan dit algoritme zitten enorme risico’s.’\n\nBy David Davidson and Marc Schuilenburg for Follow the Money on August 23, 2023\n\nDutch police used algorithm to predict violent behaviour without any safeguards\n\nFor many years the Dutch police has used a risk modeling algorithm to predict the chance that an individual suspect will commit a violent crime. Follow the Money exposed the total lack of a moral, legal, and statistical justification for its use, and now the police has stopped using the system.\n\nContinue reading “Dutch police used algorithm to predict violent behaviour without any safeguards” →"
    }
}
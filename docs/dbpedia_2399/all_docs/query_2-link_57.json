{
    "id": "dbpedia_2399_2",
    "rank": 57,
    "data": {
        "url": "https://racismandtechnology.center/theme/artificial-intelligence/",
        "read_more_link": "",
        "language": "en",
        "title": "Artificial Intelligence",
        "top_image": "https://racismandtechnology.center/wp-content/uploads/default-social-media-image.jpg",
        "meta_img": "https://racismandtechnology.center/wp-content/uploads/default-social-media-image.jpg",
        "images": [
            "https://racismandtechnology.center/wp-content/uploads/logo-1.png",
            "https://racismandtechnology.center/wp-content/uploads/7740.jpg",
            "https://racismandtechnology.center/wp-content/uploads/7775-600x368.jpg",
            "https://racismandtechnology.center/wp-content/uploads/7776-600x338.jpg",
            "https://racismandtechnology.center/wp-content/uploads/7777-600x315.jpg",
            "https://racismandtechnology.center/wp-content/uploads/7825-600x338.jpg",
            "https://racismandtechnology.center/wp-content/uploads/7829-600x139.jpg",
            "https://racismandtechnology.center/wp-content/uploads/jailing-controversies-600x345.png",
            "https://racismandtechnology.center/wp-content/uploads/comuzi-mirror-600x338.jpg",
            "https://racismandtechnology.center/wp-content/uploads/7642-600x315.jpg",
            "https://racismandtechnology.center/wp-content/uploads/midjourney-showing-lenin-600x600.jpg",
            "https://racismandtechnology.center/wp-content/uploads/368-Hans-de-Zwart-Karen-Palmer-600x338.jpg",
            "https://racismandtechnology.center/wp-content/uploads/7400-600x315.jpg",
            "https://racismandtechnology.center/wp-content/uploads/7380-600x315.jpg",
            "https://racismandtechnology.center/wp-content/uploads/7328-600x315.jpg",
            "https://racismandtechnology.center/wp-content/uploads/7270-600x338.jpg",
            "https://racismandtechnology.center/wp-content/uploads/7131-600x314.jpg",
            "https://racismandtechnology.center/wp-content/uploads/turnitin-ai-detector-600x350.png",
            "https://racismandtechnology.center/wp-content/uploads/atlantic-building-ai-safely-600x313.jpg",
            "https://racismandtechnology.center/wp-content/uploads/great-white-robot-god-600x394.jpg",
            "https://racismandtechnology.center/wp-content/uploads/ai-language-600x360.jpg",
            "https://racismandtechnology.center/wp-content/uploads/7071-600x315.jpg",
            "https://racismandtechnology.center/wp-content/uploads/7038-600x394.jpg",
            "https://racismandtechnology.center/wp-content/uploads/7042-600x315.jpg",
            "https://racismandtechnology.center/wp-content/uploads/bloomberg-openai-resumes-600x315.png",
            "https://racismandtechnology.center/wp-content/uploads/6863-600x371.jpg",
            "https://racismandtechnology.center/wp-content/uploads/6866-600x314.jpg",
            "https://racismandtechnology.center/wp-content/uploads/6807-600x300.jpg",
            "https://racismandtechnology.center/wp-content/uploads/6815-600x300.jpg",
            "https://racismandtechnology.center/wp-content/uploads/6780-600x338.jpg",
            "https://racismandtechnology.center/wp-content/uploads/6781-600x338.jpg",
            "https://racismandtechnology.center/wp-content/uploads/overwoke-google-600x300.jpg",
            "https://racismandtechnology.center/wp-content/uploads/turnitin-ai-detection-600x241.jpg",
            "https://racismandtechnology.center/wp-content/uploads/6679-600x338.jpg",
            "https://racismandtechnology.center/wp-content/uploads/6321-600x781.jpg",
            "https://racismandtechnology.center/wp-content/uploads/6293-600x338.jpg",
            "https://racismandtechnology.center/wp-content/uploads/6292-600x300.jpg",
            "https://racismandtechnology.center/wp-content/uploads/6285-600x400.jpg",
            "https://racismandtechnology.center/wp-content/uploads/6284-600x400.jpg",
            "https://racismandtechnology.center/wp-content/uploads/6246-600x315.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2024-08-20T00:00:00",
        "summary": "",
        "meta_description": "Curated news, research and activism on the intersection between racism and technology.",
        "meta_lang": "en",
        "meta_favicon": "https://racismandtechnology.center/wp-content/uploads/cropped-favicon-32x32.png",
        "meta_site_name": "Racism and Technology Center",
        "canonical_link": "https://racismandtechnology.center/theme/artificial-intelligence/",
        "text": "Politica & AI: het belang van AI bewustzijn in het politieke landschap\n\nHet thema van Prinsessendag 2024 is AI & Politica. Spreker Robin Pocornie ligt in haar column toe waarom dit thema zo veel invloed heeft op het huidige politieke landschap.\n\nBy Robin Pocornie for Nederlandse Vrouwenraad on August 14, 2024\n\nGoogle Workers Revolt Over $1.2 Billion Israel Contract\n\nTwo Google workers have resigned and another was fired over a project providing AI and cloud services to the Israeli government and military.\n\nBy Billy Perrigo for Time on April 10, 2024\n\nMeta and Lavender\n\nA little-discussed detail in the Lavender AI article is that Israel is killing people based on being in the same Whatsapp group as a suspected militant. Where are they getting this data? Is WhatsApp sharing it?\n\nBy Paul Biggar for Paul Biggar on April 16, 2024\n\n‘The machine did it coldly’: Israel used AI to identify 37,000 Hamas targets\n\nIsraeli intelligence sources reveal use of ‘Lavender’ system in Gaza war and claim permission given to kill civilians in pursuit of low-ranking militants.\n\nBy Bethan McKernan and Harry Davies for The Guardian on April 3, 2024\n\nNa het toeslagen schandaal, waarbij onder andere veel eenoudergezinnen en gezinnen met een migratieachtergrond onterecht van fraude werden beschuldigd, werd pijnlijk duidelijk dat niet alleen mensen discrimineren, maar algoritmes ook\n\nEr werd beloofd dat deze systemen eerlijker zouden worden, maar uit het nieuwe jaarverslag van de Autoriteit Persoonsgegevens blijkt dat er sindsdien weinig is verbeterd. Algoritmes categoriseren mensen met bepaalde kenmerken nog steeds onterecht als risico. Noëlle Cecilia, medeoprichter van Brush AI (@ai.brush) was zondag te gast bij Mandy. Zij maakt algoritmes voor bedrijven en deed een jaar lang onderzoek naar de eerlijkheid en discriminatie ervan. Zij legt ons uit waarom de mindset moet veranderen bij het ontwikkelen van AI-systemen.\n\nBy Noëlle Cecilia for Instagram on July 9, 2024\n\nNon-white American parents are embracing AI faster than white ones\n\nThe digital divide seems to have flipped.\n\nFrom The Economist on June 27, 2024\n\nWar, Memes, Art, Protest, and Porn: Jail(break)ing Synthetic Imaginaries Under OpenAI ’s Content Policy Restrictions\n\nUsing the method of jail(break)ing to study how the visualities of sensitive issues transform under the gaze of OpenAI ’s GPT 4o, we found that: -Jail(break)ing takes place when the prompts force the model to combine jailing (transforming or fine-tuning content to comply with content restrictions) and jailbreaking (attempting to bypass or circumvent these restrictions). – Image-to-text generation allows more space for controversy than text-to-image. – Visual outputs reveal issue-specific and shared transformation patterns for charged, ambiguous, or divisive artefacts. – These patterns include foregrounding the background or ‘dressing up’ (porn), imitative disambiguation (memes), pink-washing (protest), cartoonization/anonymization (war), and exaggeration of style (art).\n\nBy Alexandra Rosca, Elena Pilipets, Energy Ng, Esmée Colbourne, Marina Loureiro, Marloes Geboers, and Riccardo Ventura for Digital Methods Initiative on August 6, 2024\n\nGenerative AI’s ability to ‘pink-wash’ Black and Queer protests\n\nUsing a very clever methodology, this year’s Digital Method Initiative Summer School participants show how generative AI models like OpenAI’s GTP-4o will “dress up” controversial topics when you push the model to work with controversial content, like war, protest, or porn.\n\nContinue reading “Generative AI’s ability to ‘pink-wash’ Black and Queer protests” →\n\nRacist Technology in Action: AI detection of emotion rates Black basketball players as ‘angrier’ than their White counterparts\n\nIn 2018, Lauren Rhue showed that two leading emotion detection software products had a racial bias against Black Men: Face++ thought they were more angry, and Microsoft AI thought they were more contemptuous.\n\nContinue reading “Racist Technology in Action: AI detection of emotion rates Black basketball players as ‘angrier’ than their White counterparts” →\n\nAre you 80% angry and 2% sad? Why ‘emotional AI’ is fraught with problems\n\nAI that purports to read our feelings may enhance user experience but concerns over misuse and bias mean the field is fraught with potential dangers.\n\nBy Ned Carter Miles for The Guardian on June 23, 2024\n\nHow generative AI tools represent EU politicians: in a biased way\n\nAlgorithm Watch experimented with three major generative AI tools, generating 8,700 images of politicians. They found that all these tools make an active effort to lessen bias, but that the way they attempt to do this is problematic.\n\nContinue reading “How generative AI tools represent EU politicians: in a biased way” →\n\nPodcast: Art as a prophetic activity for the future of AI\n\nOur own Hans de Zwart was a guest in the ‘Met Nerds om Tafel’ podcast. With Karen Palmer (creator of Consensus Gentium, a film about surveillance that watches you back), they discussed the role of art and storytelling in getting us ready for the future.\n\nContinue reading “Podcast: Art as a prophetic activity for the future of AI” →\n\nImage generators are trying to hide their biases – and they make them worse\n\nIn the run-up to the EU elections, AlgorithmWatch has investigated which election-related images can be generated by popular AI systems. Two of the largest providers don’t adhere to security measures they have announced themselves recently.\n\nBy Nicolas Kayser-Bril for AlgorithmWatch on May 29, 2024\n\nI am not a typo\n\nIt’s time to correct autocorrect.\n\nFrom I am not a typo\n\nPeople with commonly autocorrected names call for tech firms to fix problem\n\n‘I am not a typo’ campaign is calling for technology companies to make autocorrect less ‘western- and white-focused’.\n\nBy Robert Booth for The Guardian on May 22, 2024\n\nFouten herstellen we later wel: hoe de gemeente een dubieus algoritme losliet op Rotterdammers\n\nHet was te mooi om waar te zijn: een algoritme om fraude in de bijstand op te sporen. Ondanks waarschuwingen bleef de gemeente Rotterdam er bijna vier jaar lang in geloven. Een handjevol ambtenaren, zich onvoldoende bewust van ethische risico’s, kon jarenlang ongestoord experimenteren met de data van kwetsbare mensen.\n\nBy Romy van Dijk and Saskia Klaassen for Vers Beton on October 23, 2023\n\nLET OP, zegt de computer van Buitenlandse Zaken bij tienduizenden visumaanvragen. Is dat discriminatie?\n\nDiscriminerend algoritme: Volgens een onderzoek discrimineerde het algoritme dat Buitenlandse Zaken gebruikt om visumaanvragen te beoordelen. Uit onvrede met die conclusie vroeg het ministerie om een second opinion.\n\nBy Carola Houtekamer and Merijn Rengers for NRC on May 1, 2024\n\nAI detection has no place in education\n\nThe ubiquitous availability of AI has made plagiarism detection software utterly useless, argues our Hans de Zwart in the Volkskrant.\n\nContinue reading “AI detection has no place in education” →\n\nThe datasets to train AI models need more checks for harmful and illegal materials\n\nThis Atlantic conversation between Matteo Wong and Abeba Birhane touches on some critical issues surrounding the use of large datasets to train AI models.\n\nContinue reading “The datasets to train AI models need more checks for harmful and illegal materials” →\n\nThe Great White Robot God\n\nIt may seem improbable at first glance to think that there might be connections between the pursuit of artificial general intelligence (AGI) and white supremacy. Yet the more you examine the question the clearer and more disturbing the links get.\n\nBy David Golumbia for David Golumbia on Medium on January 21, 2019\n\nSo, Amazon’s ‘AI-powered’ cashier-free shops use a lot of … humans. Here’s why that shouldn’t surprise you\n\nThis is how these bosses get rich: by hiding underpaid, unrecognised human work behind the trappings of technology, says the writer and artist James Bridle.\n\nBy James Bridle for The Guardian on April 10, 2024\n\nOpenAI’s GPT sorts resumes with a racial bias\n\nBloomberg did a clever experiment: they had OpenAI’s GPT rank resumes and found that it shows a gender and racial bias just on the basis of the name of the candidate.\n\nContinue reading “OpenAI’s GPT sorts resumes with a racial bias” →\n\nOpenAI GPT Sorts Resume Names With Racial Bias, Test Shows\n\nRecruiters are eager to use generative AI, but a Bloomberg experiment found bias against job candidates based on their names alone.\n\nBy Davey Alba, Leon Yin, and Leonardo Nicoletti for Bloomberg on March 8, 2024\n\nGoogle Used a Black, Deaf Worker to Tout Its Diversity. Now She’s Suing for Discrimination\n\nJalon Hall was featured on Google’s corporate social media accounts “for making #LifeAtGoogle more inclusive!” She says the company discriminated against her on the basis of her disability and race.\n\nBy Paresh Dave for WIRED on March 7, 2024\n\nWhat Luddites can teach us about resisting an automated future\n\nOpposing technology isn’t antithetical to progress.\n\nBy Tom Humberstone for MIT Technology Review on February 28, 2024\n\nLLMs become more covertly racist with human intervention\n\nResearchers found that certain prejudices also worsened as models grew larger.\n\nBy James O’Donnell for MIT Technology Review on March 11, 2024\n\nGemini image generation got it wrong. We’ll do better.\n\nAn explanation of how the issues with Gemini’s image generation of people happened, and what we’re doing to fix it.\n\nBy Prabhakar Raghavan for The Keyword on February 23, 2024\n\nGoogle’s Gemini problem will be even worse outside the U.S.\n\nIt’s hard to keep a stereotyping machine out of trouble.\n\nBy Russell Brandom for Rest of World on February 29, 2024\n\nGoogle does performative identity politics, nonpologises, pauses their efforts, and will invariably move on to its next shitty moneymaking move\n\nIn a shallow attempt to do representation for representation’s sake, Google has managed to draw the ire of the right-wing internet by generating historically inaccurate and overly inclusive portraits of historical figures.\n\nContinue reading “Google does performative identity politics, nonpologises, pauses their efforts, and will invariably move on to its next shitty moneymaking move” →\n\nRacist Technology in Action: ChatGPT detectors are biased against non-native English writers\n\nStudents are using ChatGPT for writing their essays. Antiplagiarism tools are trying to detect whether a text was written by AI. It turns out that these type of detectors consistently misclassify the text of non-native speakers as AI-generated.\n\nContinue reading “Racist Technology in Action: ChatGPT detectors are biased against non-native English writers” →\n\n‘Vergeet de controlestaat, we leven in een controlemaatschappij’\n\nVolgens bijzonder hoogleraar digitale surveillance Marc Schuilenburg hebben wij geen geheimen meer. Bij alles wat we doen kijkt er wel iets of iemand mee die onze gangen registreert. We weten het, maar doen er gewoon aan mee. Zo diep zit digitale surveillance in de haarvaten van onze samenleving: ‘We herkennen het vaak niet eens meer.’\n\nBy Marc Schuilenburg and Sebastiaan Brommersma for Follow the Money on February 4, 2024\n\nMachine Learning and the Reproduction of Inequality\n\nMachine learning is the process behind increasingly pervasive and often proprietary tools like ChatGPT, facial recognition, and predictive policing programs. But these artificial intelligence programs are only as good as their training data. When the data smuggle in a host of racial, gender, and other inequalities, biased outputs become the norm.\n\nBy Catherine Yeh and Sharla Alegria for SAGE Journals on November 15, 2023\n\nTimnit Gebru says harmful AI systems need to be stopped\n\nThe labour movement has a vital role to play and will grow in importance in 2024, says Timnit Gebru of the Distributed AI Research Institute.\n\nBy Timnit Gebru for The Economist on November 13, 2023\n\n‘Unmasking AI’ and the Fight for Algorithmic Justice\n\nA conversation with Dr. Joy Buolamwini.\n\nBy Joy Buolamwini and Nabiha Syed for The Markup on November 18, 2023\n\nThis is how AI image generators see the world\n\nArtificial intelligence image tools have a tendency to spin up disturbing clichés: Asian women are hypersexual. Africans are primitive. Europeans are worldly. Leaders are men. Prisoners are Black.\n\nBy Kevin Schaul, Nitasha Tiku and Szu Yu Chen for Washington Post on November 20, 2023\n\nBarbie and the dark side of generative artificial intelligence\n\nAs Barbie-mania grips the world, the peppy cultural icon deserves thanks for helping to illustrate a darker side of artificial intelligence.\n\nBy Paige Collings and Rory Mir for Salon on August 17, 2023"
    }
}
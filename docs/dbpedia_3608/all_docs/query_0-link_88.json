{
    "id": "dbpedia_3608_0",
    "rank": 88,
    "data": {
        "url": "https://en.wikipedia.org/wiki/Moral_psychology",
        "read_more_link": "",
        "language": "en",
        "title": "Moral psychology",
        "top_image": "https://en.wikipedia.org/static/favicon/wikipedia.ico",
        "meta_img": "https://en.wikipedia.org/static/favicon/wikipedia.ico",
        "images": [
            "https://en.wikipedia.org/static/images/icons/wikipedia.png",
            "https://en.wikipedia.org/static/images/mobile/copyright/wikipedia-wordmark-en.svg",
            "https://en.wikipedia.org/static/images/mobile/copyright/wikipedia-tagline-en.svg",
            "https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Trolley_problem.png/220px-Trolley_problem.png",
            "https://upload.wikimedia.org/wikipedia/commons/thumb/4/43/IMG-20180218-WA0095.jpg/220px-IMG-20180218-WA0095.jpg",
            "https://upload.wikimedia.org/wikipedia/commons/thumb/e/e1/CC_BY_icon.svg/50px-CC_BY_icon.svg.png",
            "https://upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikiquote-logo.svg/34px-Wikiquote-logo.svg.png",
            "https://upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/30px-Commons-logo.svg.png",
            "https://upload.wikimedia.org/wikipedia/commons/thumb/e/e4/Psi-stylized.svg/50px-Psi-stylized.svg.png",
            "https://upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/16px-Symbol_category_class.svg.png",
            "https://login.wikimedia.org/wiki/Special:CentralAutoLogin/start?type=1x1",
            "https://en.wikipedia.org/static/images/footer/wikimedia-button.svg",
            "https://en.wikipedia.org/static/images/footer/poweredby_mediawiki.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Contributors to Wikimedia projects"
        ],
        "publish_date": "2004-10-05T02:25:08+00:00",
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "/static/apple-touch/wikipedia.png",
        "meta_site_name": "",
        "canonical_link": "https://en.wikipedia.org/wiki/Moral_psychology",
        "text": "Field of study in both philosophy and psychology\n\nMoral psychology is a field of study in both philosophy and psychology. Historically, the term \"moral psychology\" was used relatively narrowly to refer to the study of moral development.[1][2] Moral psychology eventually came to refer more broadly to various topics at the intersection of ethics, psychology, and philosophy of mind.[3][4][5] Some of the main topics of the field are moral judgment, moral reasoning, moral sensitivity, moral responsibility, moral motivation, moral identity, moral action, moral development, moral diversity, moral character (especially as related to virtue ethics), altruism, psychological egoism, moral luck, moral forecasting, moral emotion, affective forecasting, and moral disagreement.[7]\n\nToday, moral psychology is a thriving area of research spanning many disciplines,[8] with major bodies of research on the biological,[9][10] cognitive/computational[11][12][13] and cultural[14][15] basis of moral judgment and behavior, and a growing body of research on moral judgment in the context of artificial intelligence.[16][17]\n\nHistory\n\n[edit]\n\nThe origins of moral psychology can be traced back to early philosophical works, largely concerned with moral education, such as by Plato and Aristotle in Ancient Greece,[18][19] as well as from the Buddhist[20] and Confucian traditions.[21][22][23] Empirical studies of moral judgment go back at least as far as the 1890s with the work of Frank Chapman Sharp,[24] coinciding with the development of psychology as a discipline separate from philosophy. Since at least 1894, philosophers and psychologists attempted to empirically evaluate the morality of an individual,[25][26] especially attempting to distinguish adults from children in terms of their judgment, but these efforts failed because they \"attempted to quantify how much morality an individual had—a notably contentious idea—rather than understand the individual's psychological representation of morality\".[27]: 284\n\nIn most introductory psychology courses, students learn about moral psychology by studying the psychologist Lawrence Kohlberg,[29][30][31] who proposed a highly influential theory of moral development, developed throughout the 1950s and 1960s. This theory was built on Piaget's observation that children develop intuitions about justice that they can later articulate. Kohlberg proposed six stages broken into three categories of moral reasoning that he believed to be universal to all people in all cultures.[32] The increasing sophistication of justice-based reasoning was taken as a sign of development. Moral cognitive development, in turn, was assumed to be a necessary (but not sufficient) condition for moral action.[33]\n\nBut researchers using the Kohlberg model found a gap between what people said was most moral and actions they took. In response, Augusto Blasi proposed his self-model[34] that links ideas of moral judgment and action through moral commitment. Those with moral goals central to the self-concept are more likely to take moral action, as they feel a greater obligation to do so. Those who are motivated will attain a unique moral identity.[35]\n\nFollowing the independent publication of a pair of landmark papers in 2001 (respectively led by Jonathan Haidt and Joshua Greene),[36][37] there was a surge in interest in moral psychology across a broad range of subfields of psychology, with interest shifting away from developmental processes towards a greater emphasis on social, cognitive, affective and neural processes involved in moral judgment.[1][5][38]\n\nMethods\n\n[edit]\n\nPhilosophers, psychologists and researchers from other fields have created various methods for studying topics in moral psychology, with empirical studies dating back to at least the 1890s.[27] The methods used in these studies include moral dilemmas such as the trolley problem,[24][37] structured interviews and surveys as a means to study moral psychology and its development, as well as the use of economic games,[39] neuroimaging,[40] and studies of natural language use.[41]\n\nInterview techniques\n\n[edit]\n\nIn 1963, Lawrence Kohlberg presented an approach to studying differences in moral judgment by modeling evaluative diversity as reflecting a series of developmental stages (à la Jean Piaget). Lawrence Kohlberg's stages of moral development are:[42]\n\nObedience and punishment orientation\n\nSelf-interest orientation\n\nInterpersonal accord and conformity\n\nAuthority and social-order maintaining orientation\n\nSocial contract orientation\n\nUniversal ethical principles\n\nStages 1 and 2 are combined into a single stage labeled \"pre-conventional\", and stages 5 and 6 are combined into a single stage labeled \"post-conventional\" for the same reason; psychologists can consistently categorize subjects into the resulting four stages using the \"Moral Judgement Interview\" which asks subjects why they endorse the answers they do to a standard set of moral dilemmas.[30]\n\nSurvey instruments\n\n[edit]\n\nBetween 1910 and 1930, in the United States and Europe, several morality tests were developed to classify subjects as either fit or unfit to make moral judgments.[27][43] Test-takers would classify or rank standardized lists of personality traits, hypothetical actions, or pictures of hypothetical scenes. As early as 1926, catalogs of personality tests included sections specifically for morality tests, though critics persuasively argued that they merely measured intelligence or awareness of social expectations.[27]\n\nMeanwhile, Kohlberg inspired a new series of morality tests. The Defining Issues Test (dubbed \"Neo-Kohlbergian\" by its constituents) scores relative preference for post-conventional justifications,[44][45] and the Moral Judgment Test scores consistency of one's preferred justifications.[46][47] Both treat evaluative ability as similar to IQ (hence the single score), allowing categorization by high score vs. low score.\n\nAmong the more recently developed survey measures, the Moral Foundations Questionnaire[48] is a widely used survey measure of the five moral intuitions proposed by Moral Foundations Theory: care/harm, fairness/cheating, loyalty/betrayal, authority/subversion, and sanctity/degradation. The questions ask respondents to rate various considerations in terms of how relevant they are to the respondent's moral judgments. The purpose of the questionnaire is to measure the degree to which people rely upon each of the five moral intuitions (which may coexist). The new and improved version of this instrument (i.e., Moral Foundations Questionnaire-2; MFQ-2) was developed in 2023. In this version, Fairness was split to Equality and Proportionality. Hence, the MFQ-2 measures Care, Equality, Proportionality, Loyalty, Authority, and Purity.[49] In addition to survey instruments measuring endorsement of moral foundations, a number of other contemporary survey measures exist relating to other broad taxonomies of moral values,[50][51][52] as well as more specific moral beliefs,[53][54] or concerns.[55][56]\n\nEvolutionary origins\n\n[edit]\n\nMain article: Evolution of morality\n\nSee also: Evolutionary ethics\n\nAccording to Haidt,[28] the belief that morality is not innate was one of the few theoretical commitments uniting many of the prominent psychologists studying morality in the twentieth century (with some exceptions[57][58]). A substantial amount of research in recent decades has focused on the evolutionary origins of various aspects of morality.[59][60][61][62]\n\nIn Unto Others: the Evolution and Psychology of Unselfish Behavior (1998), Elliott Sober and David Sloan Wilson demonstrated that diverse moralities could evolve through group selection.[63] In particular, they dismantled the idea that natural selection will favor a homogeneous population in which all creatures care only about their own personal welfare and/or behave only in ways which advance their own personal reproduction.[63]\n\nTim Dean has advanced the more general claim that moral diversity would evolve through frequency-dependent selection because each moral approach is vulnerable to a different set of situations which threatened our ancestors (see also Pragmatic ethics § Moral ecology).[64]\n\nTopics and theories\n\n[edit]\n\nMoral identity\n\n[edit]\n\nMain article: Moral identity\n\nMoral identity refers to the importance of morality to a person's identity, typically construed as either a trait-like individual difference, or set of chronically accessible schemas.[35][65] Moral identity is theorized to be one of the key motivational forces connecting moral reasoning to moral behavior,[65] as suggested by a 2016 meta-analysis reporting that moral identity is positively (albeit only modestly) associated with moral behavior.[66]\n\nMoral values\n\n[edit]\n\nPsychologist Shalom Schwartz defines individual values as \"conceptions of the desirable that guide the way social actors (e.g.organisational leaders, policymakers, individual persons) select actions, evaluate people an events, and explain their actions and evaluations.\"[67] Cultural values form the basis for social norms, laws, customs and practices. While individual values vary case by case (a result of unique life experience), the average of these values point to widely held cultural beliefs (a result of shared cultural values).\n\nKristiansen and Hotte[68] reviewed many research articles regarding people's values and attitudes and whether they guide behavior. With the research they reviewed and their own extension of Ajzen and Fishbein's theory of reasoned action, they conclude that value-attitude-behavior depends on the individual and their moral reasoning. Another issue that Kristiansen and Hotte discovered through their research was that individuals tended to \"create\" values to justify their reactions to certain situations, which they called the \"value justification hypothesis\".[68] Their theory is comparable to Jonathan Haidt's social intuitionist theory,[36] where individuals justify their intuitive emotions and actions through post-hoc moral reasoning.\n\nKristiansen and Hotte also found that independent selves had actions and behaviors that are influenced by their own thoughts and feelings, but Interdependent selves have actions, behaviors and self-concepts that were based on the thoughts and feelings of others. Westerners have two dimensions of emotions, activation and pleasantness. The Japanese have one more, the range of their interdependent relationships. Markus and Kitayama found that these two different types of values had different motives. Westerners, in their explanations, show self-bettering biases. Easterners, on the other hand, tend to focus on \"other-oriented\" biases.[68]\n\nMoral foundations theory\n\n[edit]\n\nMain article: Moral foundations theory\n\nMoral foundations theory, first proposed in 2004 by Jonathan Haidt and Craig Joseph,[69] attempts to explain the origins of and variation in human moral reasoning on the basis of innate, modular foundations.[70] Notably, moral foundations theory has been used to describe the difference between the moral foundations of political liberals and political conservatives.[71][72] Haidt and Joseph expanded on previous research done by Shweder and his three ethics theory.[69] Shweder's theory consisted of three moral ethics: the ethics of community, autonomy, and divinity.[73] Haidt and Graham took this theory and extended it to discuss the five psychological systems that more specifically make up the three moral ethics theory. These Five Foundations of Morality and their importance vary throughout each culture and construct virtues based on their emphasized foundation. The five psychological foundations are:\n\nHarm/care, which starts with the sensitivity to signs of suffering in offspring and develops into a general dislike of seeing suffering in others and the potential to feel compassion in response.\n\nFairness/reciprocity, which is developed when someone observes or engages in reciprocal interactions. This foundation is concerned with virtues related to fairness and justice.\n\nIngroup/loyalty, which constitutes recognizing, trusting, and cooperating with members of one's ingroup as well as being wary of members of other groups.\n\nAuthority/respect, which is how someone navigates in a hierarchal ingroups and communities.\n\nPurity/sanctity, which stems from the emotion of disgust that guards the body by responding to elicitors that are biologically or culturally linked to disease transmission.\n\nThe five foundations theory are both a nativist and cultural-psychological theory. Modern moral psychology concedes that \"morality is about protecting individuals\" and focuses primarily on issues of justice (harm/care and fairness/reciprocity).[71]: 99 Their research found that \"justice and related virtues...make up half of the moral world for liberals, while justice-related concerns make up only one fifth of the moral world for conservatives\".[71]: 99 Liberals value harm/care and fairness/reciprocity significantly more than the other moralities, while conservatives value all five equally. Ownership has also been argued to be a strong candidate to be a moral foundation.[74]\n\nMoral virtues\n\n[edit]\n\nIn 2004, D. Lapsley and D. Narvaez outlined how social cognition explains aspects of moral functioning.[75] Their social cognitive approach to personality has six critical resources of moral personality: cognition, self-processes, affective elements of personality, changing social context, lawful situational variability, and the integration of other literature. Lapsley and Narvaez suggest that moral values and actions stem from more than our virtues and are controlled by a set of self-created schemas (cognitive structures that organize related concepts and integrate past events). They claim that schemas are \"fundamental to our very ability to notice dilemmas as we appraise the moral landscape\" and that over time, people develop greater \"moral expertise\".\n\nTriune ethics theory\n\n[edit]\n\nMain article: Triune ethics Meta-theory\n\nThe triune ethics meta-theory (TEM) has been proposed by Darcia Narvaez as a metatheory that highlights the relative contributions to moral development of biological inheritance (including human evolutionary adaptations), environmental influences on neurobiology, and the role of culture.[77] TET proposes three basic mindsets that shape ethical behavior: self-protectionism (a variety of types), engagement, and imagination (a variety of types that are fueled by protectionism or engagement). A mindset influences perception, affordances, and rhetorical preferences. Actions taken within a mindset become an ethic when they trump other values. Engagement and communal imagination represent optimal human functioning that are shaped by the evolved developmental niche (evolved nest) that supports optimal psychosocial neurobiological development.[78] Based on worldwide anthropological research (e.g., Hewlett and Lamb's Hunter-Gatherer Childhoods), Narvaez uses small-band hunter-gatherers as a baseline for the evolved nest and its effects.\n\nMoral reasoning and development\n\n[edit]\n\nMain articles: Moral reasoning and Moral development\n\nSee also: Moral education and Values education\n\nMoral development and reasoning are two overlapping topics of study in moral psychology that have historically received a great amount of attention, even preceding the influential work of Piaget and Kohlberg.[27] Moral reasoning refers specifically to the study of how people think about right and wrong and how they acquire and apply moral rules.[79] Moral development refers more broadly to age-related changes in thoughts and emotions that guide moral beliefs, judgments and behaviors.[80]\n\nKohlberg's stage theory\n\n[edit]\n\nJean Piaget, in watching children play games, noted how their rationales for cooperation changed with experience and maturation.[81] He identified two stages, heteronomous (morality centered outside the self) and autonomous (internalized morality). Lawerence Kohlberg sought to expand Piaget's work. His cognitive developmental theory of moral reasoning dominated the field for decades. He focused on moral development as one's progression in the capacity to reason about justice. Kohlberg's interview method included hypothetical moral dilemmas or conflicts of interest (most notably, the Heinz dilemma). He proposed six stages and three levels of development (claiming that \"anyone who interviewed children about dilemmas and who followed them longitudinally in time would come to our six stages and no others).[82] At the Preconventional level, the first two stages included the punishment-and-obedience orientation and the instrumental-relativist orientation. The next level, the conventional level, included the interpersonal concordance or \"good boy – nice girl\" orientation, along with the \"law and order\" orientation. Lastly, the final Postconventional level consisted of the social-contract, legalistic orientation and the universal-ethical-principle orientation.[83] According to Kohlberg, an individual is considered more cognitively mature depending on their stage of moral reasoning, which grows as they advance in education and world experience.\n\nCritics of Kohlberg's approach (such as Carol Gilligan and Jane Attanucci) argue that there is an over-emphasis on justice and an under-emphasis on an additional perspective to moral reasoning, known as the care perspective. The justice perspective draws attention to inequality and oppression, while striving for reciprocal rights and equal respect for all. The care perspective draws attention to the ideas of detachment and abandonment, while striving for attention and response to people who need it. Care Orientation is relationally based. It has a more situational focus that is dependent on the needs of others as opposed to Justice Orientation's objectivity.[84] However, reviews by others have found that Gilligan's theory was not supported by empirical studies since orientations are individual dependent.[85][86] In fact, in neo-Kohlbergian studies with the Defining Issues Test, females tend to get slightly higher scores than males.[87][page needed]\n\nThe attachment approach to moral judgment\n\n[edit]\n\nAner Govrin's attachment approach to moral judgment[88][89][90] proposes that, through early interactions with the caregiver, the child acquires an internal representation of a system of rules that determine how right/wrong judgments are to be construed, used, and understood. By breaking moral situations down into their defining features, the attachment model of moral judgment outlines a framework for a universal moral faculty based on a universal, innate, deep structure that appears uniformly in the structure of almost all moral judgments regardless of their content.\n\nMoral behaviour\n\n[edit]\n\nSee also: Social preferences\n\nHistorically, major topics of study in the domain of moral behavior have included violence and altruism,[91][92] bystander intervention and obedience to authority (e.g., the Milgram experiment[93] and Stanford prison experiment[94]).[1][95] Recent research on moral behavior uses a wide range of methods, including using experience sampling to try and estimate the actual prevalence of various kinds of moral behavior in everyday life.[96][97] Research has also focused on variation in moral behavior over time, through studies of phenomena such as moral licensing.[98][99] Yet other studies focusing on social preferences examine various kinds of resource allocation decisions,[14][100] or use incentivized behavioral experiments to investigate the way people weighted their own interests against other people's when deciding whether to harm others, for example, by examine how willing people are to administer electric shocks to themselves vs. others in exchange for money.[101]\n\nJames Rest reviewed the literature on moral functioning and identified at least four components necessary for a moral behavior to take place:[102][103]\n\nSensitivity – noticing and interpreting the situation\n\nReasoning and making a judgment regarding the best (most moral) option\n\nMotivation (in the moment but also habitually, such as moral identity)\n\nImplementation - having the skills and perseverance to carry out the action\n\nReynolds and Ceranic researched the effects of social consensus on one's moral behavior. Depending on the level of social consensus (high vs. low), moral behaviors will require greater or lesser degrees of moral identity to motivate an individual to make a choice and endorse a behavior. Also, depending on social consensus, particular behaviors may require different levels of moral reasoning.[104]\n\nMore recent attempts to develop an integrated model of moral motivation[105] have identified at least six different levels of moral functioning, each of which has been shown to predict some type of moral or pro-social behavior: moral intuitions, moral emotions, moral virtues/vices (behavioral capacities), moral values, moral reasoning, and moral willpower. This social intuitionist model of moral motivation[106] suggests that moral behaviors are typically the product of multiple levels of moral functioning, and are usually energized by the \"hotter\" levels of intuition, emotion, and behavioral virtue/vice. The \"cooler\" levels of values, reasoning, and willpower, while still important, are proposed to be secondary to the more affect-intensive processes.\n\nMoral behavior is also studied under the umbrella of personality psychology. Topics within personality psychology include the traits or individual differences underlying moral behavior, such as generativity, self-control, agreeableness, cooperativeness and honesty/humility,[107][108][109] as well as moral change goals,[110] among many other topics.\n\nRegarding interventions aimed at shaping moral behavior, a 2009 meta analysis of business ethics instruction programs found that such programs have only \"a minimal impact on increasing outcomes related to ethical perceptions, behavior, or awareness.\"[111] A 2005 meta analysis[112] suggested that positive affect can at least momentarily increase prosocial behavior (with subsequent meta analyses also showing that prosocial behavior reciprocally increases positive affect in the actor[113][114]).\n\nValue-behavior consistency\n\n[edit]\n\nSee also: Value-action gap and Moral disengagement\n\nIn looking at the relations between moral values, attitudes, and behaviors, previous research asserts that there is less correspondence between these three aspects than one might assume.[115] In fact, it seems to be more common for people to label their behaviors with a justifying value rather than having a value beforehand and then acting on it. There are some people that are more likely to act on their personal values: those low in self-monitoring and high in self-consciousness, due to the fact that they are more aware of themselves and less aware of how others may perceive them. Self-consciousness here means being literally more conscious of yourself, not fearing judgement or feeling anxiety from others. Social situations and the different categories of norms can be telling of when people may act in accordance with their values, but this still is not concrete either. People will typically act in accordance with social, contextual and personal norms, and there is a likelihood that these norms can also follow one's moral values. Though there are certain assumptions and situations that would suggest a major value-attitude-behavior relation, there is not enough research to confirm this phenomenon.\n\nMoral willpower\n\n[edit]\n\nMain articles: Ego depletion and Self-control\n\nBuilding on earlier work by Metcalfe and Mischel on delayed gratification,[116] Baumeister, Miller, and Delaney explored the notion of willpower by first defining the self as being made up of three parts: reflexive consciousness, or the person's awareness of their environment and of himself as an individual; interpersonal being, which seeks to mold the self into one that will be accepted by others; and executive function.[117] They stated, \"[T]he self can free its actions from being determined by particular influences, especially those of which it is aware\". The three prevalent theories of willpower describe it as a limited supply of energy, as a cognitive process, and as a skill that is developed over time. Research has largely supported that willpower works like a \"moral muscle\" with a limited supply of strength that may be depleted (a process referred to as Ego depletion), conserved, or replenished, and that a single act requiring much self-control can significantly deplete the \"supply\" of willpower.[117] While exertion reduces the ability to engage in further acts of willpower in the short term, such exertions actually improve a person's ability to exert willpower for extended periods in the long run.[119] Additional research has been conducted that may cast doubt on the idea of ego-depletion.[120]\n\nMoral intuitions\n\n[edit]\n\nIn 2001, Jonathan Haidt introduced his social intuitionist model which claimed that with few exceptions, moral judgments are made based upon socially derived intuitions. Moral intuitions happen immediately, automatically, and unconsciously, with reasoning largely serving to generate post-hoc rationalizations to justify one's instinctual reactions.[36] He provides four arguments to doubt causal importance of reason. Firstly, Haidt argues that since there is a dual process system in the brain when making automatic evaluations or assessments, this same process must be applicable to moral judgement as well. The second argument, based on research on motivated reasoning, claims that people behave like \"intuitive lawyers\", searching primarily for evidence that will serve motives for social relatedness and attitudinal coherence. Thirdly, Haidt found that people have post hoc reasoning when faced with a moral situation, this a posteriori (after the fact) explanation gives the illusion of objective moral judgement but in reality is subjective to one's gut feeling. Lastly, research has shown that moral emotion has a stronger link to moral action than moral reasoning, citing Damasio's research on the somatic marker hypothesis and Batson's empathy-altruism hypothesis.[36]\n\nFollowing the publication of a landmark fMRI study in 2001,[37] Joshua Greene separately proposed his dual process theory of moral judgment, according to which intuitive/emotional and deliberative processes respectively give rise to characteristically deontological and consequentialist moral judgments. A \"deontologist\" is someone who has rule-based morality that is mainly focused on duties and rights; in contrast, a \"consequentialist\" is someone who believes that only the best overall consequences ultimately matter.[121]\n\nMoral emotions\n\n[edit]\n\nMain article: Moral emotions\n\nSee also: Social emotions\n\nMoral emotions are a variety of social emotion that are involved in forming and communicating moral judgments and decisions, and in motivating behavioral responses to one's own and others' moral behavior.[122][123][124] While moral reasoning has been the focus of most study of morality dating back to Plato and Aristotle, the emotive side of morality was historically looked upon with disdain in early moral psychology research.[122] However, in the last 30–40 years, there has been a rise in a new front of research: moral emotions as the basis for moral behavior.[124] This development began with a focus on empathy and guilt, but has since moved on to encompass new scholarship on emotions such as anger, shame, disgust, awe, and elevation. While different moral transgressions have been linked to different emotional reactions, bodily reactions to such transgressions are not too different and can be characterized by some felt activations in the gut area as well as the head area.[125]\n\nMoralization and moral conviction\n\n[edit]\n\nMoralization, a term introduced to moral psychology by Paul Rozin, refers to the process through which preferences are converted into values.[126][127][128] Relatedly, Linda Skitka and colleagues have introduced the concept of moral conviction, which refers to a \"strong and absolute belief that something is right or wrong, moral or immoral.\"[129][130] According to Skitka's integrated theory of moral conviction (ITMC), attitudes held with moral conviction, known as moral mandates, differ from strong but non-moral attitudes in a number of important ways. Namely, moral mandates derive their motivational force from their perceived universality, perceived objectivity, and strong ties to emotion.[131] Perceived universality refers to the notion that individuals experience moral mandates as transcending persons and cultures; additionally, they are regarded as matters of fact. Regarding association with emotion, ITMC is consistent with Jonathan Haidt's social intuitionist model in stating that moral judgments are accompanied by discrete moral emotions (i.e., disgust, shame, guilt). Importantly, Skitka maintains that moral mandates are not the same thing as moral values. Whether an issue will be associated with moral conviction varies across persons.\n\nOne of the main lines of IMTC research addresses the behavioral implications of moral mandates. Individuals prefer greater social and physical distance from attitudinally dissimilar others when moral conviction was high. This effect of moral conviction could not be explained by traditional measures of attitude strength, extremity, or centrality. Skitka, Bauman, and Sargis placed participants in either attitudinally heterogeneous or homogenous groups to discuss procedures regarding two morally mandated issues, abortion and capital punishment. Those in attitudinally heterogeneous groups demonstrated the least amount of goodwill towards other group members, the least amount of cooperation, and the most tension/defensiveness. Furthermore, individuals discussing a morally mandated issue were less likely to reach a consensus compared to those discussing non-moral issues.[132]\n\nIntersections with other fields\n\n[edit]\n\nSociological applications\n\n[edit]\n\nSome research shows that people tend to self-segregate based on moral and political views,[133][134] exaggerate the magnitude of moral disagreements across political divides,[135] and avoid exposure to the opinions of those with opposing political views.[136]\n\nNormative implications\n\n[edit]\n\nResearchers have begun to debate the implications (if any) moral psychology research has for other subfields of ethics such as normative ethics and meta-ethics.[137][138][139][140][141] For example Peter Singer, citing Haidt's work on social intuitionism and Greene's dual process theory, presented an \"evolutionary debunking argument\" suggesting that the normative force of our moral intuitions is undermined by their being the \"biological residue of our evolutionary history.\"[142] John Michael Doris discusses the way in which social psychological experiments—such as the Stanford prison experiments involving the idea of situationism—call into question a key component in virtue ethics: the idea that individuals have a single, environment-independent moral character.[143][page needed] As a further example, Shaun Nichols (2004) examines how empirical data on psychopathology suggests that moral rationalism is false.[144][page needed]\n\nAdditionally, research in moral psychology is being used to inform debates in applied ethics around moral enhancement.[145][146]\n\nRobotics and artificial intelligence\n\n[edit]\n\nMain article: Machine ethics\n\nAt the intersection of moral psychology and machine ethics, researchers have begun to study people's views regarding the potentially ethically significant decisions that will be made by self-driving cars.[17][16][147][148]\n\nMohammad Atari and his colleagues recently examined the moral psychology of the famous chatbot, ChatGPT. These authors asked in their title, \"which humans?\" — rhetorically pointing out that people should not ask how \"human-like\" machine morality is, but to which humans it resembles.[149] These authors discovered that Large Language Models (LLMs), especially ChatGPT, tend to echo moral values endorsed by Westerners, as their training datasets originate predominantly from Western, Educated, Industrialized, Rich, and Democratic (WEIRD) societies.[150] This study points out that compared to the global average, people from WEIRD societies are more inclined toward individualism and impersonal prosocial behaviors while showing less traditionalism and group loyalty. The authors further highlighted that societies less aligned with these WEIRD moral values tend to experience greater misalignment with the moral values and outputs of ChatGPT.[151]\n\nSee also\n\n[edit]\n\nNotes\n\n[edit]\n\nReferences\n\n[edit]\n\nMoral Psychology Research Group Archived 2020-07-14 at the Wayback Machine – with Knobe, Nichols, Doris and others.\n\nFrom the Stanford Encyclopedia of Philosophy\n\nEmpathy\n\nMoral Character\n\nMoral Motivation\n\nMoral Psychology: Empirical Approaches\n\nMoral Responsibility\n\nFrom the Internet Encyclopedia of Philosophy"
    }
}
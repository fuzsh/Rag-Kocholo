{
    "id": "wrong_mix_random_publicationDate_00092_2",
    "rank": 95,
    "data": {
        "url": "https://arxiv.org/html/2401.14462v1",
        "read_more_link": "",
        "language": "en",
        "title": "AI auditing: The Broken Bus on the Road to AI Accountability",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://arxiv.org/html/x1.png",
            "https://arxiv.org/html/x2.png",
            "https://arxiv.org/html/x3.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            "Evaluation",
            "auditing",
            "accountability",
            "transparency",
            "artificial intelligence",
            "society",
            "law",
            "machine learning",
            "data science"
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "License: CC BY 4.0\n\narXiv:2401.14462v1 [cs.CY] 25 Jan 2024\n\nAI auditing: The Broken Bus on the Road to AI Accountability\n\nAbeba Birhane Ryan Steed Victor Ojewale {@IEEEauthorhalign} Briana Vecchione Inioluwa Deborah Raji\n\nAbstract\n\nOne of the most concrete measures to take towards meaningful AI accountability is to consequentially assess and report the systems’ performance and impact. However, the practical nature of the “AI audit” ecosystem is muddled and imprecise, making it difficult to work through various concepts and map out the stakeholders involved in the practice. First, we taxonomize current AI audit practices as completed by regulators, law firms, civil society, journalism, academia, consulting agencies. Next, we assess the impact of audits done by stakeholders within each domain. We find that only a subset of AI audit studies translate to desired accountability outcomes. We thus assess and isolate practices necessary for effective AI audit results, articulating the observed connections between AI audit design, methodology and institutional context on its effectiveness as a meaningful mechanism for accountability.\n\nIndex Terms:\n\nEvaluation, auditing, accountability, transparency, artificial intelligence, society, law, machine learning, data science\n\nI Introduction\n\nThe widespread use of artificial intelligence (AI) systems is heavily weighed down by its multitude of related risks. Functional failures [1], disparate performance [2, 3, 4], embedded stereotypes [5, 6, 7], legal incompatibility [8], privacy violations [9], model inscrutability [10] and many more issues plague almost every use case.\n\nAudits are a routine practice in various sectors including finance, public management, healthcare, food safety, and commercial retail [11, 12]. The adoption of audits within the AI space, however, is relatively new. Despite its imprecision, we use the term “artificial intelligence” (AI) to encompass a wide range of deployed products with a significant algorithmic or data-defined component, including but not limited to risk assessments, large base models for computer vision and language, classification models, “generative” models, and recommendation systems. The inspiration for audit practice in the field of data science, machine learning (ML), and AI derives from a variety of related disciplines. Online platform audits, for example, cite social science and critical race studies as inspiration [13, 14], as do several audits of automated decision systems (ADS) and risk assessments [15]. Some audits of large language models take after security audits [16]. Some risk assessment evaluations follow models from traditional experimental design [17], including clinical trials [18]. Meanwhile, internal auditors derive their practice from regulated industries such as finance, aerospace and medical devices [19].\n\nAcross disciplines and contexts, one of the main motivations for conducting audits of AI systems is establishing informed and consequential judgements of the deployed AI systems – that is algorithmic accountability [20]. We thus define an AI audit to be any evaluation of these AI systems, independent of the AI development process, executed for the purpose of accountability [21].\n\nHowever, unlike other, more mature audit industries, AI audit studies do not consistently translate into more concrete objectives to regulate system outcomes, such as influencing voluntary corporate action or internal corporate policies, inciting product recalls, informing product re-designs, as well as shaping broader governmental policy and regulation in the form of standards, bans, restrictions or moratoriums of use. In the AI audit context, such impact is fairly uncommon. In this paper, with the aim of enabling auditors and policymakers to achieve accountability outcomes , we taxonomize audit practices and identify the characteristics of audits that most directly and effectively contribute to audit objectives.\n\nSome initial work has been done to taxonomize various aspects of AI auditing. Many of these efforts have focused on categorizing methods [13, 15, 22, 23, 24], or types of audit organizations [25, 12]. Some efforts have also looked at an AI audit’s broader institutional [26, 19], policy [12], and societal context [15], through these analyses are often limited to one or at most a handful of case studies. Bandy [27] is one of few efforts that systematically classifed audit studies by a broader set of criteria – audit method, audit target, audit domain and audit objective. However, the study is narrow in scope, consisting of 62 exclusively academic audit studies.\n\nIn this paper, we take a much broader and more comprehensive view than past studies. We review the broad audit landscape consisting of academia and six other domains, taxonomizing the key characteristics of their context, goals and practice (Figure 1). Notably, as a mechanism for achieving accountability, we investigate the consequences of these audit investigations and how well the outcomes from the studies match those stated objectives.\n\nII Background\n\nII-A What is an AI audit?\n\nIn the context of this paper, we consider the operationalization of audits as a mechanism for accountability in computing — notably for AI, machine learning, data science, and related fields. We begin by proving definitions.\n\nDefinition 1\n\nAn audit is defined as any independent assessment of an identified audit target via an evaluation of articulated expectations with the implicit or explicit objective of accountability.\n\nCare must be taken to translate this definition in the AI context. By independent assessment, we mean any measurement done by an entity operationally distinct from the team that engineered the examined AI system. Even if that team is within the same company and composed of corporate employees, the examination only counts as an audit if the auditors are adequately separate from those that built the system, or the audit process itself is distinct from the engineering process for the AI system [19, 12]. By an identified audit target, we look to studies and investigations that name a concrete and non-abstract, specific object of examination. Ideally, this target is connected to a real-world AI deployment, though sometimes a widely used open-source algorithm or dataset can operate as a stand-in or proxy. For instance, Steed and Caliskan [28] looked at bias in an open-source image generation model, reflecting issues later discovered in similar commercial products [29]; and an audit of Proctorio was conducted on the open source model OpenCV on which the commercial product is built [30]. Due to a lack of training data disclosure for most AI products (even supposedly “open source” models [31]), many data audits investigate open source datasets [32, 33, 34] and attempt to generalize conclusions on broader industry practice. Studies of prototypical algorithms (hypothetical models trained by the authors) without concrete, specific targets were not considered.\n\nTo be an evaluation with the objective of accountability, the audit must incorporate some implied or explicit objective to have the assessment play some role informing consequential judgements about the technology being examined. In order for these judgements to be more concrete, there needs to be some measurement between the reality of the AI deployment and articulated expectations held about a particular deployment. Many academic papers that we examined, for example, studying fairness, performance or safety in an abstract manner [35], without connecting it to anticipated accountability outcome, even implicitly, were not considered by us to be audits [36]. Note that the actual type of audit target or criteria of assessment are not part of the definition of what constitutes an audit. AI audits can thus encompass a wide range of targets, including automated decision systems (ADS) [37, 38], recommendation systems underlying online platforms or apps [39, 40], large base models in computer vision [28, 41], speech [42], text-based natural language processing [16], or multimodal models [43]. At times, the evaluations involve domain-specific considerations in hiring [44], healthcare [2], criminal justice [45] or social service delivery [46]. The expectations articulated for these systems can also vary in concreteness and specificity. For instance, some conduct audits specifically for legal compliance [47] while others declare expectations more normatively [36]. Others yet are not explicitly labelled as audit work yet satisfy our audit criteria and result in immense explicit and gradual structural change [48, 49]. As a result, we can observe audits that can evaluate and diagnose for a range of performance, safety concerns, as well as broader societal injustices and are not limited to fairness.\n\nII-B Who conducts AI audits?\n\nThere is a wide range of possible audit practitioners that participate in the audit process [25], often described as below.\n\nDefinition 2\n\nAn internal auditor is an entity executing an audit or investigation with some contractual relationship with the audit target [12]. They typically seek to minimize corporate liability and test for compliance to corporate or industry-wide expectations [19]. In policy, internal auditors are typically those designated to carry out mandatory corporate audit requirements (e.g. the “independent auditors” in Article 37 of the Digital Services Act).\n\nDefinition 3\n\nAn external auditor is an entity executing an audit or investigation without any contractual relationship with the audit target [12]. They typically execute audits voluntarily with a broader mandate of identifying and minimizing the harm impacting their constituents.\n\nInternal audits require a contractual relationship with the audit target. Internal audits are typically conducted by an organization hired by the audit target voluntarily or to maintain compliance with a required legal audit mandate. The auditor in these contexts are hired to operate in a professional capacity to audit the target. This often means they are selected and paid by the audit target, though that is not always necessarily the case (e.g., auditors selected and paid by the government). These are the auditors typically referenced in audit mandates. As the executors of more formal audit requirements, these auditors are ideally certified or otherwise qualified [12], and subject to some form of external oversight and quality control, including but not limited to auditor conduct and reporting standards.\n\nExternal auditors typically conduct audits voluntarily by organizations, typically with a broader mandate of research or advocacy. These auditors are not assigned an audit target and do not execute audits on behalf of the audit target but choose to study systems based on the needs and concerns of their constituents. Without any formal connection to the audit target, these auditors typically struggle to access the information necessary to conduct a thorough investigation. Furthermore, without any form of legal protection, data access regime, or oversight, these auditors tend to operate quite independently, taking on whatever methods suit their objectives. These auditors can be vulnerable to corporate retaliation and methodological skepticism after audit results are released.\n\nII-C How are AI audits conducted?\n\nAlthough the specific methods used in the execution of AI audits vary widely, some terminology are deployed regularly to describe how audits are executed. Audits can be conducted ex ante (before deployment), in media res (during an iterative process of design and restricted re-deployment) or ex post (after widespread adoption and use). Furthermore, although the details of the AI audit process varies widely, the high level structure of that process follows similar stages, which we describe using the taxonomy from [50]. Note that not all audits follow all these processes.\n\nII-C1 Harms Discovery\n\nThis is the stage of discovering what to audit for. It involves identifying the audit target entity, possible targeted populations, and the anticipated form of harm or measurement required for a meaningful audit. This can happen, for example, via direct reporting from the impacted population [51], active investigation from the auditors, or other methods [52].\n\nII-C2 Standards Identification\n\nThis stage is about effectively articulating the requirements for an ideal AI audit outcome, by naming the standards the auditors will be holding the target to in the evaluation process. These expectations can be as vague as a set of named AI principles [53] or as precise as a specific threshold of performance (e.g., AI hiring tools’ adherence to the 4/5ths rule [54]).\n\nII-C3 Performance Analysis\n\nThe core of the audit is the actual evaluation itself. There are a wide diversity of methods available to inform final assessments, ranging from qualitative to quantitative approaches [55]. Each method involves different degrees of complexity and challenges tied to data acquisition, model access, and measurement.\n\nII-C4 Audit Communication and Advocacy\n\nFollowing the evaluation of the AI system, there is often also some activity to disseminate and translate audit results to some relevant stakeholders. That audience could include the audit target, regulators and the public [36, 56]—but could also include internal stakeholders within the organization such as a legal, business, product or engineering team [19].\n\nIII Methods\n\nIn order to review the AI audit research and practice landscape, we conducted a wide-ranging literature review of academic work published in a range of venues. We reviewed websites, reports, and relevant documentations from numerous non-academic audit practitioners. For academic research, we collected a comprehensive sample of N=341𝑁341N=341italic_N = 341 audit studies published in interdisciplinary computing and related venues from 2018–2022. Figure 2 shows the number of collected academic studies published per year, grouped by audit label type, while Table I describes our search method for each source and lists the number of identified pieces of literature collected for each. For audit practices outside academia, we identified major domains that are both relatively established in existing academic literature and currently emerging as key players in the AI audit ecosystem (inspired by the main categories identified by [25]): journalism, civil society, government, consulting agencies and corporate audits, and legal firms. We collected specific cases that serve as concrete examples of each domain. These methodological choices not only facilitate coherence with existing scholarship but also enhance the reliability and comparability of our findings within broader academic discourse on the subject.\n\nOur list is not exhaustive of all academic audit literature or audit domains, but it provides the most comprehensive sample (to our knowledge) of the current state of AI audit ecosystem, encompassing from academic research to practice and everything in-between. The full list of audit studies we found using these methods is included in the supplementary information (see Appendix A).\n\nIII-A Academic literature\n\nAudits in the academic context, conducted by authors from universities, non-profits, and tech companies, encompass a wide variety of disciplines, methods, and aims. Academic audits are published in various formats and venues, for example, as books, journals, and conference proceedings. Over the past five years, interdisciplinary computing conferences have become a central place where work around the topics of fairness, accountability, and transparency is discussed and published, especially within the ACM conferences, FAccT and AIES, two emerging main conferences in the space. In this regard, conference proceedings are not only the most common way of sharing work amongst the AI ethics (broadly defined) community, but such formats of publication also share commonalities such as relatively standardised style and presentation. We have, therefore, selected conference proceedings as a primary focus for our systematic reviews of academic audits.\n\nOur search is not exhaustive—it did not, for example, include potential audit work that might have been covered in high-impact general audience journals such as Science, Nature, or PNAS. We also did not consider work published in non-interdisciplinary social science venues—in particular, work from economics or sociology. Instead, our sample represents the kinds of audit work recently published at interdisciplinary computing and related conferences.\n\nIII-A1 Searching for audit studies\n\nWe found N=341𝑁341N=341italic_N = 341 academic papers that met our criteria for an audit study (Figure 2, Table I). We identified academic audit studies with two methods. First, we manually reviewed the conference proceedings from 2018 to 2022 for a selection of smaller interdisciplinary computing conferences: Fairness, Accountability, and Transparency (FAccT), Artificial Intelligence, Ethics, and Society (AIES), Equity and Access in Algorithms, Mechanisms, and Optimization (EAAMO), Association for the Advancement of Artificial Intelligence (AAAI), Computer-Supported Cooperative Work & Social Computing (CSCW), International Conference on Computational Social Science (IC2S2), and the International World Wide Web Conference (WWW). We selected these conferences as venues where audit work is likely to appear, but still, audits are not the primary focus of these conferences. Of the 5 years of proceedings we reviewed, only N=237𝑁237N=237italic_N = 237 papers met our criteria for an audit study.\n\nSecond, to expand our search to other, larger conferences, we automated the inclusion process: we applied keyword searches to both the ACM Digital Library (excluding FAccT, AIES, and CSCW proceedings, which we reviewed manually) and the ACL Anthology. In these two libraries, we searched for papers published from 2018 to 2022 with one or more of six keywords in the title: “audit”, “accountability”, “case study”, “bias”, “fairness”, and “assurance”. For ACL, where the searches were web-based, we manually reviewed the search results for audit studies online, downloading only the few studies that met our criteria (N=6𝑁6N=6italic_N = 6). For ACM, we downloaded the search results. There were thousands of papers in the queries in total. Three of the keywords yielded relatively few ACM entries: “audit” (N=39𝑁39N=39italic_N = 39), “accountability” (N=87𝑁87N=87italic_N = 87), and “assurance” (N=65𝑁65N=65italic_N = 65), after removing duplicates. For the rest of the keywords, which returned over 500 results each, we narrowed our manual search by considering only the first 500 “most relevant” entries returned by the ACM search engine. After removing duplicates and other incomplete entries, each keyword yielded the following number of pieces of literature: “fairness” (N=404𝑁404N=404italic_N = 404), “bias” (N=318𝑁318N=318italic_N = 318), and “case study” (N=472𝑁472N=472italic_N = 472). We further excluded everything that is not a standard academic paper (such as panels, abstracts, workshops, and tutorials) for consistency. We then manually reviewed each abstract and, as before, excluded those that did not meet our criteria for an audit study, leaving a total of N=103𝑁103N=103italic_N = 103 audit studies from ACM conferences other than FAccT and AIES.\n\nIII-A2 Classifying audit types\n\nWe classified all the audit studies we collected into two main categories: audit studies and meta-commentary (depicted in Figure 1). We further classified audit studies into two kinds: product/model/algorithm audit studies and data audit studies. Some studies targeted specific products (e.g., YouTube comment moderation [57]); others targeted only the AI models behind the products (e.g., an investigation of tax audit models used by the IRS [58]); still others targeted only the algorithms used to build the model (e.g., pre-trained large language models such as GPT-3 [59]). Since the methodologies for these kinds of audits are similar, we classify them together. We distinguish these studies from data audit studies—also a kind of model audit—which specifically target training or benchmark datasets (e.g., ImageNet [60]) and tend to use different methodologies.\n\nDuring the iterative classification process, an additional category emerged: ecosystem audit studies. Ecosystem audit go beyond datasets, models, and products and examine communities and sociotechnical environments (digital or physical) impacted by or are critical components to an AI system’s operation. Brown et al. [61], for example, audited the impact of child welfare service algorithms by conducting interactive workshops with front-line service providers and families affected by such algorithms. Meta-commentary studies are those that examine auditing as a practice, including studies that put forward novel mechanisms and processes as a viable method for auditing certain systems as well as those proposing to improve existing audit processes. Meta-commentary also includes critiques, work that interrogates the effectiveness and merit of auditing as a practice.\n\nNote that not all papers fit neatly into one of the above categories, and sometimes we find some papers incorporating elements of two—and, rarely, three—categories. For example, methodology papers often also use a specific case study to illustrate the method in practice. In our analysis (Fig. 2), we consider only the classification that best fits the paper.\n\nIII-B Non-academic domains\n\nFor audit work outside the academic domain, we identified the following categories where audits are regularly conducted and becoming an established practice: journalism, civil society, law firms, regulatory audits, and consulting agencies and corporate audits. For each category, we gathered sources, often information on auditors websites, audit reports, and other relevant documentation produced by various bodies (whenever they are available), which we reference in our analysis (§V). Based on information from these sources, for each category we identified a number of institutions, organisations, agencies, or audit reports that served as concrete examples of the given category. Appendix A lists all the reports, webpages, and other documents we reviewed in each audit category and institution.\n\nIII-C Analysis\n\nFor both the academic audits and each non-academic audit source, we reviewed published audit studies, documents, and public webpages to summarize information on the following:\n\nIII-C1 Context\n\nContext includes the specific structural factors shaping the audit (all as stated by the auditor): what motivated the audit (Motivations), its goals (Goals), the main artefact or system under investigation (Audit Target), the specific harms or concerns investigated (Types of Harms), and ways in which the audit may or may not shift power between stakeholders [62] (Institutional Context).\n\nIII-C2 Methodology\n\nMethodology includes the main techniques and procedures used to investigate the target artefact—for example, quantitative evaluation (typical in academic audit studies) or forensic analysis or qualitative interviews (found more often in civil society audits).\n\nIII-C3 Impact\n\nImpact refers to changes that occur to the target artefact, the target audit, or the institutional environment as a direct consequence of the audit. Impacts could include policy developments, alterations to an algorithm, or monetary fines for certain violations. While impacts are well documented in some domains (e.g., journalism), the impact of an academic audit, for example, is not often clear at the time an audit is published. Detecting, measuring, and quantifying impact is challenging. Subsequently, we were as inclusive as possible of many different kinds of impacts, e.g., inspiring media coverage of an overlooked harm [63]. We considered notable documented evidence in the audit reports themselves or from related news stories, whenever available. In our analysis, we make a note wherever documented impacts were unstated and unclear (in academia, for example).\n\nFor nearly all the academic studies we found, our dataset includes abstracts (N=337𝑁337N=337italic_N = 337) and author-selected keywords (N=263𝑁263N=263italic_N = 263). In order to supplement our qualitative findings for academia (§IV) with quantitative statistics, we analysed key terms (e.g., “accountability”) that were used most frequently by authors. Appendix B describes how we selected key terms related to the criteria above.\n\nIV Results: Auditing in Academia\n\nIV-A Audit types\n\nIV-A1 Product/model/algorithm audits\n\nMost audit work fell under the category of product-level case studies. These audit studies target a mix of social media platforms, algorithms for administering public services, large language and vision models, and search engines. These case studies typically evaluate specific deployed systems. These case studies mainly diagnose failures, errors, disparities. For example, some variation of “bias” is mentioned in 32.5%, and “fairness” in 21.2% of abstracts (see Table B-IV). Less commonly, these studies call for model builders and practitioners to make amendments accordingly. For example, the term “accountability” appeared in only 14% of abstracts, less often than in ecosystem audits (33.3%) or meta-commentary (28.1%).\n\nIV-A2 Data audits\n\nData audits typically focus on evaluating specific datasets, often targeting datasets used to train large models [64] and sometimes interrogate the benchmark datasets used for model evaluation, such as COMPAS [65]. Oftentimes, these studies emphasise (both implied and explicit) shifting norms around data use and benchmarking practices, while fewer explicitly emphasize holding dataset creators accountable. For example, “accountability” is mentioned in 9% of data audit abstracts, while “bias” is mentioned in 39.1% of abstracts (more in this category than any other). These studies typically focus on surfacing harms ranging from representation and stereotyping to privacy and, more recently, copyright protection [66]. The type of methods used include quantitative measurement (such as the incidence of NSFW images), simulation, ablation (removing or changing certain aspects of the dataset and measuring the result), and critical assessment. Like case studies, data audits tend to be conducted by a range of academic, non-profit, and corporate authors examining open-sourced and academic datasets.\n\nIV-A3 Ecosystem audits\n\nThese studies target public services —predictive risk models used by child welfare agencies, for example [67, 61]—more often than any other kinds of audits. Abstracts of these studies also mention specific domains such as hiring and education more often than any other audit type (Table B-V). Harms and concerns are often concretely defined and audit are typically carried out with the expectation of subsequent change both to specific stakeholders and sometimes, society at large. The term “accountability”, for example, is mentioned in over 33% of ecosystem audit abstracts, the highest of any category. is goal, These studies often utilise a wider range of methods, including qualitative interviews, surveys, workshops, and literature reviews [68, 69, 70]. Given the scope of investigation, methods are often all-encompassing and diffuse, less precise than more scoped audit investigations [71], where they are much more likely to employ qualitative and participatory methods than other audit studies. 6 out of 15 ecosystem audit abstracts, for example, mention ethnography, interviews, workshops, or other qualitative methods, while two explicitly use the term “participatory” (see Table B-VI).\n\nIV-A4 Meta-commentary & critique\n\nWe found that 28.1% of meta-commentaries mention “accountability” in the abstract. The type of methods they used include surveys, interviews, and literature reviews. These kinds of studies were nearly exclusively conducted by academic, non-profit, or government authors—for example, guidance from government agencies such as NIST’s AI Risk Management Framework [72].\n\nMany meta-commentary papers aimed to develop a methodology and many considered the development of rigorous audit methods as a vital contribution. Audit methods these studies put forward include both qualitative and quantitative approaches. These studies also examine audit practitioners themselves, interrogating norms around algorithm evaluation and audit practice. The harms at issue tend to be less well-specified; most mention some form of independence, fairness, privacy, or recourse. These studies rarely mention affected stakeholders in method design. Similarly, a couple of tool development papers developed standardised tools, usually quantitative, for auditing or for algorithmic recourse.\n\nIV-B Impact by audit type\n\nIV-B1 Product/model/algorithm audits\n\nWe found few audit studies that acknowledge and address power asymmetries. Many prominent papers we surveyed are collaborations between authors from academic institutions and large tech corporations, usually examining their own systems and datasets, and rarely explicitly calling for systemic change, auditing systems, often without involvement from affected stakeholders. Unlike audit work by journalists and civil society, academic audit work tends to follow academic norms where the objective is academic publication. Subsequently, more often than not, audits are seen as an academic, intellectual exercise than practices directly linked to real world consequences. Audit findings are rarely presented with demands for concrete systemic change. This does not, however, mean that academic audits are not impactful. To the contrary, seminal audit case studies such as Gender Shades [41] have not only resulted in meaningful improvements to deployed systems [36] but also have come to establish algorithmic audits as a field of enquiry. The impact of such work is, however, rarely obvious at the time of publication but becomes apparent gradually over time. For industry collaborations, the audits may result in organisational reform. For example, case studies co-authored by authors with big tech affiliations such as [73] may have contributed to tightening Google Play Store app data access restrictions [74]. Generally, academic audit results that publicly call out audit targets [36], that tend to be picked up by news outlets such as MIT Tech Review, activists, and regulators tend to bring about the most observable changes.\n\nIV-B2 Data audits\n\nThe impact of data audits are also often unclear, though prominent data audits sometimes resulted in changes to benchmarks — an audit of 80 Million Tiny Images, for example, resulted in the withdrawal [75] of the dataset [60] and a Financial Times investigation of Microsoft’s dataset of “celebrity” faces even resulted in its discontinuation [76, 77].\n\nIV-B3 Ecosystem audits\n\nEcosystem audits often reveal comprehensive institutional or legislative policy demands in advocacy. This includes, connecting performance and privacy concerns involved in facial recognition use by law enforcement [68], breaking down sources of bias throughout the model development cycle [78], systematic environmental costs [79, 80] or the labor issues across the entire supply chain of AI development [81].\n\nIV-B4 Meta-commentary & Critique\n\nAlthough the immediate impact is directly quantifiable, meta-commentaries provide important mechanisms that allow audit studies and practices to zoom out, self-reflect, and evaluate the overall picture and direction, all of which is a crucial element for grounding audits in concrete foundations and optimal accountability.\n\nMore specifically, a certain type of meta-commentary that seems to be of particular importance are critique studies. This category of work engages in reflexive commentary critiques of auditing as a practice. A few pieces of critical work interrogate the effectiveness, shortcomings, and limitations of audits. Such work highlights structural issues such as historical power asymmetries that might be reinforced by common academic auditing practices, or how audits might serve as a smoke-screen for corporate responsibility, such as, audit washing [82, 83] or how audits might lead to simplistic technological solutionism [84]. They draw on qualitative interviews, literature reviews, and statistical analysis. Like meta-commentaries, the impacts of these works are not easily measured, though many of these studies are highly cited.\n\nV Results: Auditing Outside Academia\n\nIn this section, we review a sample of audits and audit practices from outside academia — law firms, consulting agencies and corporate audits, journalism, civil society, government, and civil society — examining how factors such as institutional context affects the practice of auditing and accountability. These auditors operate in a variety of domains and deploy various methods throughout the audit design, development, and execution process. For each domain, we pay particular attention to the details of the audit context and methodology, then connect this to the observed impact derived from audits for that particular domain. Our analysis criteria and results are summarised in Table II (The full list of documents we analysed is found in Appendix A).\n\nV-A Law Firms\n\nThere is a large industry devoted to data governance legal services, but firms offering legal services for AI auditing are less common. Three such merging boutique law firms are: Luminos.Law [85] (formerly called BHN.AI), Foxglove [86], and AWO [87]. These firms represent three different institutional arrangements in audit-related legal services.\n\nContext\n\nLaw firms operate as both internal and external auditors. They can be hired by the audit target to conduct an internal assessment as part of a regulatory requirement or in legal defense. Law firms also work with representatives of impacted populations (often pro bono) to externally investigate an audit target to collect material evidence of perceived or reported harms. Both scenarios are typically prompted by individual or collective complaints post-deployment.\n\nLuminos.Law and AWO—both for-profit firms—offer compliance and public policy services. They carry out audit work at the behest of their clients, with the goal of ensuring compliance with standards, data rights, data protection due diligence, and regulatory obligations. Luminos.Law’s client testimonials feature Fortune 100 & 500 tech firms [88]; AWO’s “commercial practice is balanced with giving those less-resourced a voice”, according to the firm, and their client testimonials feature many non-profit and university clients [89]. The type of harm and concerns that underlie these firms’ services vary accordingly. For Luminos.Law, these are assessment and assurance around legal compliance, legal defence and liability [90]. Similarly, AWO focuses on auditing for privacy and data rights, safety, surveillance violations, digital manipulation and exploitation [89, 91]. The types of harm and concerns that drive audit practice for Foxglove, on the other hand, include violations of justice, disparate performance [92, 93], and specific breaches of data governance law [94, 95].\n\nWe see a stark difference in power asymmetries amongst these three organisations. As for profit forms, both Luminos.Law and AWO’s objectives, missions, and practices are shaped by their business models that prioritise the needs of their clients, which are often wealthy and powerful corporations. Foxglove’s work directly or indirectly pushes to shift power from the most to the least powerful. Subsequently, Foxglove frequently works with groups such as content moderators, warehouse workers and gig-workers, groups that are often underpaid, over-exploited, and disfranchised.\n\nMethodology\n\nFoxglove’s cases often involve specific campaigns, petitions, and/or case studies. Foxglove uses methods such as legal compliance analysis, interviewing, and anecdotal evidence, or leans on previous research to diagnose and highlight harms, concerns, and to pursue legal challenges. Both Luminos.Law and AWO work within the needs of their clients, which include big tech companies and start-ups. Within such a context, the main audit target for Luminos.Law are models and data, while audit target include social media platforms [96] and hiring algorithms [97], based on media coverage of the firm [85]. AWO similarly carries out documentation analysis, legal compliance analysis, and policy development using previous research as a basis [91].\n\nImpact\n\nAs firms that provide audits as a service to clients, some details of their practices are inaccessible—particularly for Luminos.Law—because much of their work is explicitly marketed as “privileged and confidential” [88]. Information on the impacts of their commercial work is therefore difficult to identify. AWO’s blog, for example, mostly features work done in collaboration with civil society organizations like the Ada Lovelace Institute [98]. Luminos’s public portfolio includes some standards guidance, a bias calculator for New York’s new audit law [99, 100], and an audit of the open-source large language model RoBERTa [101, 102], but the impacts of these audits on practice are not evident.\n\nFoxglove’s work, on the other hand, often results in significant impactful changes including the reversal of Ofqual’s A level grading algorithm [103], halting the use of visa-streaming algorithm that was deployed by the UK Home Office [104], and forcing disclosure of a secrete contracts between corporations and the UK government; for example, the “NHS Covid-19 data deals” [105] and exposing contracts between Palantir and the UK government.\n\nV-B Consulting Agencies & Corporate Audits\n\nOther organizations offer audit services beyond legal and policy advice. A crop of recent startups, such as, Arthur AI and Fiddler AI, offer model monitoring services with fairness and privacy components. Others offer consulting specifically, auditing as a service, including boutiques such as, Parity Consulting [106] as well as large consulting firms such as Deloitte [107], McKinsey [108], and Accenture [109].\n\nBecause the product-specific work stemming from an internal audit team is rarely published, corporations occasionally publish a report in conjunction with an external consulting agency. For example, Business for Social Responsibility (BSR) has conducted audits on behalf of Google for its celebrity facial recognition system [110], and Facebook regarding its human rights impact in Myanmar [111]. At times these consulting agencies are also part of a regulatory process. For example, the management consulting firm called Guidehouse Inc. played the role of an “independent third-party reviewer” in the Department of Justice (in the US) settlement with Facebook regarding bias in its advertising [112].\n\nHere, we examine three consulting agencies that have been involved with prominent audit case studies: ORCAA [113], Eticas [114], and BABL AI [115].\n\nContext\n\nAll three agencies provide internal audits as a service with the main objective of algorithmic accountability, and engage primarily in case-by-case consulting with private and public sector clients such as HireVue, Airbnb, Proctorio [115], the states of Illinois and Colorado [113], the Allegheny County Health Department [114], the cities of Barcelona [114] and Amsterdam [113], and the University of Iowa [115]. Within the bounds of client-agency agreement, the main audit targets for these agencies include social media platforms (such as TikTok and YouTube), ADS, FRT, ride hailing apps (such as Uber, Cabify, Bolt), predictive scoring systems, hiring algorithms, and healthcare algorithms [113, 114, 116]. Typically, the audits are ex post, though some can also be completed pre-deployment (e.g. BSR celebrity facial recognition audit was conducted before model release). ORCAA and Eticas also undertake some internal audit methodology development work.\n\nORCAA conducts audits in order to assess regulatory compliance, performance testing, as well as to measure and mitigate disparate performance [113]. BABL AI conducts audits for bias, risk, and impact assessment [116]. Like Luminos, ORCAA and BABL AI, both offer services to help companies comply legal requirements, for example, in the US, the New York Local Law 144 [100], which requires annual audits of AI hiring tools. Eticas similarly conducts audits to ensure security and data protection and to assess issues such as fairness, bias, and model accuracy [114]. The type of concerns and harms these consulting agencies mention in public materials vary. ORCAA, for example, mainly focuses on measuring bias—such as discrimination, race, and gender—and assessing for regulatory compliance [113]. Eticas mainly focuses on assessing algorithmic systems, for example, for functionality and detecting unfair practices towards protected groups [114]. Similarly, the main types of harm BABL AI focuses on include bias, fairness, transparency as well as assessment for standards and privacy compliance [115].\n\nIt is difficult to asses how these three consulting agencies might shift power with clarity. However, they target discrimination, fairness, and gender and ethnicity disparities and aspire to lofty goals—BABL AI, for example, seeks to “prioritize human flourishing” [115]. But, like the legal consulting firms, these audits are subordinate to client needs, and those clients include large corporations, startups, and government bodies. Ultimately, these audits primarily serve those entities—for example, by protecting clients from concerns such as organisational and reputational crisis —and help clients build trust with stakeholders [114, 115, 117].\n\nMethodology\n\nWe have sparse information on audit methodologies used within these agencies. From the information we can gather on their web-pages, these agencies sometimes develop internal audit tools. ORCAA, for example uses “Ethical Matrix Framework” [113], while BABL AI has developed a set of criteria that can be used to conduct bias testing in their “process audit” [116]. Other methods include reviewing documents, interacting with stakeholders, and ethics due diligence vetting, the details of which are not provided.\n\nImpact\n\nThe impact of these types of audits, more particularly impact as a direct consequence of these three agencies, is not clear, particularly, with Eticas and BABL AI. ORCAA’s work has seen some impact (albeit as an indirect influence) on policy on the US White House AI Ethics Blueprint [118]. ORCAA also conducted an audit of HireVue’s early career and campus hire assessment tools [117, 119]. HireVue subsequently declared its intention to make changes to its practices (but only following a formal complaint from the Electronic Privacy Information Center to the U.S. FTC [120]), including halting the use of facial analysis in its tools, but received criticism for misrepresenting ORCAA’s analysis and not taking bolder steps [119]. HireVue also commissioned the audit and defined both the scope of evaluation and the extent of its impact—a problem of independence raised frequently in prior work [12, 119, 83, 25].\n\nV-C Journalism\n\nJournalists have a long history of investigating and reporting on AI systems [121]. Investigative journalists at the Wall Street Journal, the New York Times, the MIT Technology Review, and many other outlets have unearthed concrete examples of systematic AI harms [122, 123, 124, 125, 126].\n\nWe examined work from two of the most prominent outlets that have conducted extensive AI audits: ProPublica and The Markup. ProPublica [127] carried out a foundational investigation into criminal risk assessment in 2016, that has set precedence for the field of AI auditing [37]. The Markup [128] is a relatively newer outlet focused on data-driven investigations.\n\nContext\n\nAudits from both The Markup and ProPublica tend to be external focusing on specific types of targets. These include, AI systems that are commonly used by large corporations or social media platforms for advertising, hiring, and ranking; government ADS, or public programs with digital sites; as well as content moderation systems and related labor conditions. Both organisations carry out audits with the stated objective of accountability. The Markup’s slogan reads, “Big Tech is watching you. We’re watching big tech” [128]. The type of harms and concerns both organisations investigate, surface, diagnose, and evaluate include disparities in performance (for instance, along the dimensions of gender, race, ethnicity, age), injustice, discrimination, privacy violations, fraud, and legal compliance breaches. As harm discovery happens through publicly submitted journalistic “tips”, the audits are typically ex-post.\n\nMethodology\n\nBoth Propublica and The Markup utilise quantitative statistical analysis in addition to investigative reporting, interviews, and document analysis. The Markup’s investigation of Amazon’s product ranking system, for example, involved training a model to predict where products would appear based on various factors [129]. Compared to other domains, these outlets engage in an extensive amount of data collection using a variety of custom scraping, data donation, and analysis tools, often built in-house. The Markup’s Citizen Browser project, for example, used data donations to investigate discriminatory targeted advertising [130, 131, 132] on Facebook [133].\n\nImpact\n\nOf the various domains we have examined, journalistic audits result in the most impactful outcomes. Audits carried out both by The Markup and ProPublica have resulted in subsequent changes in numerous domains including shifting the audit discourse in academia, altering practices in industry (including big tech corporations such as Facebook, Amazon, and Google) [134], and inspiring activism [135]. Audits both from both outlets have also directly inspired legislative and regulatory action [136, 137], including a settlement in which the U.S. Department of Justice required Facebook to stop using a special audience tool for housing ads [138, 139]. Those actions included abolishing deployed tools—such as an algorithm governing liver transplants that favored rich, urban patients [140, 141]. ProPublica’s audit of COMPAS set the precedent for algorithmic auditing and remains a canonical work not only for academic research but also as a prime example of algorithmic audit [37].\n\nV-D Civil society\n\nActivist and other civil society organisations—such as the Electronic Privacy Information Center (EPIC), Data & Society, and the AI Now Institute—conduct algorithm audits studies as part of their work. We examined work from six prominent organisations: the Electronic Frontier Foundation (EFF) [142], Refugee Law Lab (RLL) [143], The Citizen Lab [144], Migration Tech Monitor (MTM) [145], the Ada Lovelace Institute [146] and the American Civil Liberties Union (ACLU) [147].\n\nContext\n\nThese organisations and institutions primarily conduct case studies, while the Ada Lovelace Institute in particular, also conducts meta-commentary work, especially on audit methods. Though the audits are consistently external, the harms and concerns these civil society audits aim to surface, diagnose, and mitigate vary. Diagnosing and mitigating security vulnerabilities, spying technology, and illegal surveillance are some of the main focuses for EFF. Similarly, the Citizen Lab audits are driven by overarching goals such as investigating and exposing security vulnerabilities and defending free speech online [144]. RLL and MTM primarily focus on investigative and advocacy work around the questions of transparency and the impacts of technology on refugees [148, 143]. MTM particularly aims to document, map, and monitor migration technology and dismantle and destabilise hierarchical power structures [148]. Audits at Ada Lovelace Institute are often driven by the objective of policy change with the primary focus of diagnosis and remedy for concerns such as privacy, transparency, participation, and disparate impact. Accountability through legal action, litigation, and legal objectives are central to ACLU.\n\nThe audits we examined target mostly ADS in government services, online platforms, and—more than any other domain—surveillance tech. RLL in particular targets technology such as lie detectors and border patrolling drones [143]. The Citizen Lab focuses on surveillance and biometric technologies (e.g. for facial recognition), hardware (such as phones and other devices used by politicians and activists), apps, and code [144]. Biometric data, digital ID systems, surveillance drones, facial recognition, iris scan data, algorithmic motion detectors, ankle monitors, GPS tags, AI powered satellites as well as border surveillance vendors themselves are the central audit targets for MTM [145]. The Ada Lovelace Institute also has a diverse target of audit, with a general focus on biometric data and healthcare in particular [146]. The ACLU similarly targets facial recognition technology, medical algorithms, welfare algorithms, insurance pricing algorithms, and redlining algorithms as well as advertising algorithms on platforms such as Facebook [147].\n\nMethodology\n\nCivil society audits utilize the most diverse audit methods. EFF, for example, uses policy analysis, grass-root activism, and technology development. Some of the methods used by RLL include interviews with refugees, film making, data collection (from the Immigration and refugee board through Access to Information request, for instance) and analysis, interactive visualisation of data, and documentation of issues such as deportation and refused refugee claims. The Citizen Lab audit methods include in-house developed tools, interviews, and forensic analysis of devices. Similar to RLL, MTM also uses methods such as interviews with people crossing borders, photography, investigative analysis, as well as documenting and archiving migration tech. The main audit methods for Ada Lovelace Institute are participatory methods, including opinion polling, as well as policy research and development. The ACLU often uses quantitative evaluation of, for example operational systems, analysis of data acquired through privileged access, public record requests, and scraping.\n\nImpact\n\nSimilar to journalistic investigations, civil society audits often result in significant impact, often through legal action. These include nuanced and difficult to measure impacts such as shifting public attitude towards surveillance technology or drawing public and media attention towards harmful or controversial tech, as well as more concrete outcomes such as moratoriums and abolition. The EFF, for example, won a student’s civil lawsuit against the exam surveillance company Proctorio [149]. In K.W. v. Armstrong, the ACLU obtained an injunction stopping algorithmically-determined welfare cuts targeting individuals with developmental disabilities in Idaho [150]. Civil society organisations often represent those harmed by AI systems and in doing so, shift power from the most to the least powerful.\n\nV-E Government\n\nSeveral government agencies, especially in Europe, have begun to engage in AI audits. The EU’s recent Digital Services Act and an AI hiring bill passed in New York City [100], for example, both require some form independent audit. We looked at two government organizations, one with an initially prominent role in AI auditing in the UK, the Information Commissioner’s Office (ICO) [151], and another in the U.S., the National Institute for Standards and Technology (NIST) [152].\n\nContext\n\nICO conducts audits with the broader objective of upholding information rights and enforcing legal requirements [153, 154]. NIST, on the other hand, is primarily focused on establishing standards and best practice principles for ensuring the development and deployment of socially responsible algorithmic systems [155, 154]. ICO conducts audits on a case by case basis, often published as a report. Comparatively, NIST tends to produce meta-commentary on audits. The central goal for engaging in audit practices for ICO is primarily to investigate and enforce regulations, while NIST lacks such authoritative power and is mainly focused on establishing standards.\n\nThe type of audits regulators carry out tend to be procedural rather than substantive. Audits are carried out with specific objectives which often involve ensuring that a given organisation, institution or corporation is in compliance with established standards or legal requirements. While some organisations like ICO, for example, have the authority to assess and enforce regulatory compliance, others, organisations like NIST, focus on establishing best practice standards and principles that tech developers and vendors are only encouraged to follow. Guidelines from both ICO [154] and NIST [156] tend to apply to informing and standardizing practice amongst internal audit actors. As these guidelines explicitly mention model development interventions such as fairness or explainability mitigation strategies, it is implied that they apply to ex ante and in media res audits, as well as ex post audits.\n\nThe main audit targets for ICO are data governance practices within public and private companies, public authorities, and government departments, especially those considered to have major impacts [157, 154]. Organisation can request to be audited but also the ICO audits organisations that the Commissioner believes require audits, that is organisations and corporations with major impacts on society. The ICO audits are a mechanism to check for compliance with data protection legislation, how personal data is managed, data sharing agreements with third parties, and security measures, amongst other things. NIST, on the other hand, focuses on artefacts such as facial recognition technology and “general” AI as the main target for audit with the aim of assessing harms such as racial disparities, basic functionality, privacy harms, as well as physical safety of these systems [158, 159]. To the degree that government actors participate in the audit process themselves, they tend to execute external ex-post audits, separate from formal corporate participation.\n\nMethodology\n\nThe ICO has developed a set of definitions of what an audit consists of as well as setting out mechanisms for assessing compliance in accordance with regulations based on four assurance ratings: high, reasonable, limited, and very limited assurance. Following an audit, ICO makes recommendations on how to improve on the audit result. The main method used to conduct audits for ICO include examining documents, testing, and interviews with key personnel [154]. For NIST, the main audit methods include data trust, accuracy evaluation, and benchmarking [155, 160].\n\nImpact\n\nICO has issued several monetary penalties for data protection legislation breaches, including a fine of £12.7 million to TikTok for misusing children’s data [153] and £17 million fine to Clearview AI inc [161], in addition to issuing a notice to stop further processing of the personal data of people in the UK and a request to delete such data. NIST has made significant impact in the U.S. regulatory conversation with its AI Risk Management Framework (RMF), though it has not conducted any audits with specific impacts and has no power to enforce these guidelines [72].\n\nVI Discussion\n\nWhile many of the academic studies we reviewed formed the foundation for important methods and topics for AI auditing, they also often lacked in achieving comparable levels of impact to the audit work we analyzed in journalism, civil society, government, and industry. In this section, we outline several practical takeaways from our analysis for researchers, policymakers, and practitioners. In particular, we identify several ways that audit work outside academia could serve as a guide for more impactful audit studies within academia: first, by expanding the aims of audit work beyond evaluation, and towards accountability; second, by encouraging more explicit and forward-looking engagement with often excluded stakeholders, acknowledging power asymmetries [62], and fostering mechanisms for collective action and ecological change; and third, by offering specific improvements for audit methodology, including specificity, practitioner diversity, and interdisciplinary collaboration.\n\nVI-1 Power asymmetries & auditor-stakeholder relationships\n\nOne of the most influential factors in determining an audit’s impact was revealed in the power analysis of how the auditors interacted with various stakeholders—including impacted populations, the audit target and others. Subsequently, methodological innovation and evaluation of AI systems without considerations of structural factors—such as the uneven distribution of power, control, benefit and harm—is unlikely to result in significant impact. Future research could further explore tools and strategies to map out, define and foster auditor relationships with various stakeholders. Similarly, policy-makers and other authorities could step in to empower auditors – for example, by incorporating audit results into more consequential repercussions for companies or open-sourcing, publishing, disseminating, and reviewing submitted audit results.\n\nVI-2 Prioritizing audit execution stages beyond evaluation\n\nFraming details such as the selection of audit targets, named motivations, types of harms investigated and the scope of audit goals are more likely to determine the effectiveness of an audit’s outcome than the details of audit execution. Most of the academic work we reviewed focused on the process of evaluating AI systems for bias, fairness, or disparate impacts. Conversely, these studies rarely focused on other stages of auditing crucial to accountability in non-academic work, such as discovering harms, communicating audit results, or organizing non-technical interventions and collective action. As detailed in Section V, these factors are critical in influencing audit success outside of the auditor’s control [12, 25]. For example, some auditors, such as regulators and law firms, have the legal authority to demand access. Meanwhile, for others, this degree of internal visibility is rare, requiring the auditors to rely on proxy evaluations and open up their audit results to critique, denial and retaliation from the audit targets.\n\nEven as policymakers work to install legal protections for external auditors and direct resources towards harms discovery and audit reporting, future research should investigate the under-studied aspects of the AI auditing process and practitioners should prioritize these aspects of auditing in addition to evaluation.\n\nVI-3 Expanding audit scope beyond the product, model or algorithm\n\nIn our survey, we use the ecosystem audit classification to describe studies that consider the entire AI pipeline in a holistic way— communities and socio-technical environments (digital or physical) defining or impacted by critical components to an AI system’s operation— (§IV-A). While these ecosystem audits exemplify many of the elements of the socio-technical auditing advocated for in various academic critiques [82, 68, 162], most academic audit studies we found extremely rarely involved a comprehensive analysis of involved stakeholders and any holistic view of multiple interacting AI systems. Future work should continue to incorporate and expand broader and holistic perspectives in order to craft a richer account of algorithmic systems and their impacts.\n\nVI-4 Impact increases with specificity\n\nEven as audit studies broaden their scope to include the ecosystem surrounding and defining an AI system, most audits need to be specific in order to be more impactful. As has been previously discussed in the aftermath of the Gender Shades audit Raji and Buolamwini [36], naming a specific audit target, advocacy objectives and intended target responses ensures that the audit speaks to more specific demands from the audit target. This makes that target more likely to to respond to the audit and informs advocates on precise policy demands. This strategy was seen in many civic audits, such as the ACLU, the Markup and regulators like ICO, where the scope and demands tied to their investigations are specific. When audit scope was too diffused, the audit seemed to hold less effectiveness for accountability. ORCAA’s Hirevue audit, for example, was criticized for being too high level to inform any specific allegations or demands for remedy [119].\n\nConstraining audit objectives and scope has benefits as well as limitations. Too narrow a scope may obfuscate broader systematic factors and make it even more difficult to advocate for more structural changes beyond the scope of the examined audit target [163, 36]. Therefore, future work should aim to be highly specific without losing focus on holistic and ecological observations.\n\nVI-5 Utilising a wider range of audit methodologies\n\nWithin CS and CS-adjacent academic venues, quantitative methodological approaches to AI auditing were often front and center. These methods are evident in the range of fairness metrics developed and debated within these communities [164]. Existing critiques already articulate the limitations of these approaches, in terms of its abstraction [165], legal incompatibilities [166, 167] and lack of connection to substantive outcomes [168]. It is particularly noteworthy that in many non-academic domains, the methods employed are far from what is being explored or commonly discussed in academia, including a range of qualitative methodological approaches such as investigative reporting, document review, and stakeholder consultation. Granted, these other institutions often possess different skill sets and operate within different constraints. However, especially for academics hoping to connect their work to the practice of other audit practitioner communities, future AI audit research should explore how extra-disciplinary methods and tactics like these—especially those with a proven record of impact—could bolster academic audit work.\n\nVI-6 Appreciating the diversity of audit practitioners\n\nDifferent audit communities possess different strengths and weaknesses—civil society auditors, for example, typically have mature and well-developed methods around audit communication and advocacy, but at times may lack the technical capacity to execute more complex quantitative analyses. On the other hand, in academia, auditors might be well-equipped for innovating technically but struggled with public and other stakeholder engagement.\n\nOur survey supports prior findings [25] that audit practitioners within a given domain are not a monolith. For instance, there are law firms that provide audits as a service on behalf of audit targets while others execute civic audits on behalf of, and in collaboration with impacted populations. Similarly, we found a wide range of technical competence within domains. Audits from the journalism organization the Markup, for instance, were often more thorough and reproducible than some academic studies. Similarly, auditors can have differing motivations and goals, which deeply informs their practice [169].\n\nFuture research on the practice of AI auditing should still consider the strengths and weaknesses of different auditing institutions. These findings are informative not only for improving audit practice but also 1) facilitating avenues for collaboration with auditors outside academia and 2) informing a growing set of enacted and proposed legislation requiring audits for AI systems [100, 170, 171, 172, 173].\n\nVI-7 Timing and auditor type are not the primary determining factors of audit impact\n\nThe distinction of when the audit should ideally be conducted (i.e. ex ante, in media res, ex post) has recently been the subject of some policy debate [174, 175]. As expected, those with increased access (i.e. internal auditors) typically have the opportunity to execute audits pre-deployment, while others (i.e. external auditors) are often restricted to conducting audits post-deployment. However, when it comes to translating audit results to accountability, both scenarios come with distinct challenges and it seems that other contextual factors outside of auditor type or timing, including audit goal, target, design, communication and scope, can ultimately be much more meaningful.\n\nFor example, despite increased and early access, internal auditors may still struggle to convince key organizational stakeholders to act on audit results [176, 177, 178]. Furthermore, such auditors can become vulnerable to internal corporate retaliation, and corporate censorship [179] as well as being mired in conflict of interest [180]. All of this can interfere with the auditors’ ability to externally communicate results, or force auditors to scope results too narrowly to be meaningfully. In policy, internal auditors are typically those designated to carry out mandatory corporate audit requirements (e.g. the “independent auditors” in Article 37 of the Digital Services Act). The particular challenges of internal auditors indicate that requirements for external reporting, product standards and auditor conduct standards are especially important in such contexts.\n\nOn the other hand, external auditors are free to set their scope and communicate results publicly. However, they typically face issues around external legitimacy and visibility that can make them easy to ignore completely [36]. The serious issues they face around auditor capacity and information access are further hindrances to audit quality and thus impact [12], and the public-facing nature of their advocacy makes them even more vulnerable to corporate retaliation. External auditors are typically designated as voluntary “investigators” or “researchers”, rather than actual auditors (e.g. “vetted researchers” in Article 40 of the Digital Services Act), and are often mentioned in data access or safe harbor clauses, revealing the importance of such policy interventions in addressing their particular challenges.\n\nVI-8 Recognizing the limits of audits\n\nSome societal impacts of AI are not amenable to audits. What can be evaluated and audited depends on numerous factors including normative and pragmatic considerations as well as what is prioritised and anticipated, and whether harms and risks are known or anticipated [181]. As there are several serious, although gradual, and pernicious harms emanating from AI – for instance, the chilling effect of surveillance, or corporate power concentration [182] – it is clear that not all pressing issues regarding the technology can be addressed with audits. Although the ecological audits we have introduced in this work are an encouraging first step towards significant structural change in some of these cases, other, likely moral or rights-based arguments are required for this kind of advocacy. Thus, it is clear that audits are just one element of a necessarily broader set of AI accountability strategies. We caution against any approaches that treat audits as any kind of all-encompassing solution to technological ills.\n\nVII Conclusion\n\nAudits have become widely popular in the AI field as an aspirational accountability measure to inform our decision-making and regulation of AI deployments. However, lessons from the broader range of AI audit practitioners beyond academia reveal that there is still much to learn in order for these audit studies to operate as consequential judgements. Future work will hopefully continue to investigate this relationship between contextual factors, audit design, audit execution and the underlying shared vision of audit practitioners across all domains: a substantively meaningful path towards AI accountability.\n\nReferences\n\nRaji et al. [2022a] I. D. Raji, I. E. Kumar, A. Horowitz, and A. Selbst, “The fallacy of ai functionality,” in Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, 2022, pp. 959–972.\n\nObermeyer and Mullainathan [2019] Z. Obermeyer and S. Mullainathan, “Dissecting racial bias in an algorithm that guides health decisions for 70 million people,” in Proceedings of the Conference on Fairness, Accountability, and Transparency, 2019, pp. 89–89.\n\nChu et al. [2022] C. H. Chu, R. Nyrup, K. Leslie, J. Shi, A. Bianchi, A. Lyn, M. McNicholl, S. Khan, S. Rahimi, and A. Grenier, “Digital ageism: Challenges and opportunities in artificial intelligence for older adults,” The Gerontologist, vol. 62, no. 7, pp. 947–955, 2022.\n\nImana et al. [2021] B. Imana, A. Korolova, and J. Heidemann, “Auditing for Discrimination in Algorithms Delivering Job Ads,” in Proceedings of the Web Conference 2021. Ljubljana Slovenia: ACM, Apr. 2021, pp. 3767–3778. [Online]. Available: https://dl.acm.org/doi/10.1145/3442381.3450077\n\nLuccioni et al. [2023] S. Luccioni, C. Akiki, M. Mitchell, and Y. Jernite, “Stable Bias: Evaluating Societal Representations in Diffusion Models,” in Thirty-Seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, Nov. 2023. [Online]. Available: https://openreview.net/forum?id=qVXYU3F017\n\nBarlas et al. [2021] P. Barlas, K. Kyriakou, O. Guest, S. Kleanthous, and J. Otterbacher, “To ‘see’ is to stereotype: Image tagging algorithms, gender recognition, and the accuracy-fairness trade-off,” Proceedings of the ACM on Human-Computer Interaction, vol. 4, no. CSCW3, pp. 1–31, 2021.\n\nScheuerman et al. [2019] M. K. Scheuerman, J. M. Paul, and J. R. Brubaker, “How computers see gender: An evaluation of gender classification in commercial facial analysis services,” Proceedings of the ACM on Human-Computer Interaction, vol. 3, no. CSCW, pp. 1–33, 2019.\n\nKilovaty [2019] I. Kilovaty, “Legally cognizable manipulation,” Berkeley Tech. LJ, vol. 34, p. 449, 2019.\n\nCadwalladr and Graham-Harrison [2018] C. Cadwalladr and E. Graham-Harrison, “Revealed: 50 million facebook profiles harvested for cambridge analytica in major data breach,” The Guardian, vol. 17, no. 1, p. 22, 2018.\n\nPasquale [2015] F. Pasquale, The black box society: The secret algorithms that control money and information. Harvard University Press, 2015.\n\nPower [1999] M. Power, The Audit Society: Rituals of Verification. Oxford, New York: Oxford University Press, Oct. 1999.\n\nRaji et al. [2022b] I. D. Raji, P. Xu, C. Honigsberg, and D. Ho, “Outsider Oversight: Designing a Third Party Audit Ecosystem for AI Governance,” in Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society, ser. AIES ’22. New York, NY, USA: Association for Computing Machinery, Jul. 2022, pp. 557–571. [Online]. Available: https://dl.acm.org/doi/10.1145/3514094.3534181\n\nSandvig et al. [2014] C. Sandvig, K. Hamilton, K. Karahalios, and C. Langbort, “Auditing Algorithms: Research Methods for Detecting Discrimination on Internet Platforms,” in Data and Discrimination: Converting Critical Concerns into Productive: A Preconference at the 64th Annual Meeting of the International Communication Association, Seattle, WA, 2014, p. 23.\n\nCherry and Bendick [2018] F. Cherry and M. Bendick, “Making it count: Discrimination auditing and the activist scholar tradition,” Audit studies: Behind the scenes with theory, method, and nuance, pp. 45–62, 2018.\n\nVecchione et al. [2021] B. Vecchione, S. Barocas, and K. Levy, “Algorithmic Auditing and Social Justice: Lessons from the History of Audit Studies,” arXiv:2109.06974 [cs], Sep. 2021. [Online]. Available: http://arxiv.org/abs/2109.06974\n\nCarlini et al. [2023] N. Carlini, J. Hayes, M. Nasr, M. Jagielski, V. Sehwag, F. Tramer, B. Balle, D. Ippolito, and E. Wallace, “Extracting training data from diffusion models,” in 32nd USENIX Security Symposium (USENIX Security 23), 2023, pp. 5253–5270.\n\nLai et al. [2023] V. Lai, C. Chen, A. Smith-Renner, Q. V. Liao, and C. Tan, “Towards a Science of Human-AI Decision Making: An Overview of Design Space in Empirical Human-Subject Studies,” in Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency, ser. FAccT ’23. New York, NY, USA: Association for Computing Machinery, Jun. 2023, pp. 1369–1385. [Online]. Available: https://dl.acm.org/doi/10.1145/3593013.3594087\n\nRivera et al. [2020] S. Rivera, X. Liu, A. Chan, A. Denniston, and M. Calvert, “Guidelines for clinical trial protocols for interventions involving artificial intelligence: the spirit-ai extension. bmj 370,” 2020.\n\nRaji et al. [2020a] I. D. Raji, A. Smart, R. N. White, M. Mitchell, T. Gebru, B. Hutchinson, J. Smith-Loud, D. Theron, and P. Barnes, “Closing the AI accountability gap: Defining an end-to-end framework for internal algorithmic auditing,” in Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, ser. FAT* ’20. New York, NY, USA: Association for Computing Machinery, Jan. 2020, pp. 33–44. [Online]. Available: https://dl.acm.org/doi/10.1145/3351095.3372873\n\nWieringa [2020] M. Wieringa, “What to account for when accounting for algorithms: a systematic literature review on algorithmic accountability,” in Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, 2020, pp. 1–18.\n\nRaji et al. [2023] I. D. Raji, S. C. Chock, and J. Buolamwini, “Change from the outside: Towards credible third-party audits of ai systems,” Missing links in AI governance, p. 5, 2023.\n\nBrown et al. [2021] S. Brown, J. Davidovic, and A. Hasan, “The algorithm audit: Scoring the algorithms that score us,” Big Data & Society, vol. 8, no. 1, p. 2053951720983865, 2021.\n\nAda Lovelace Institute [2020] Ada Lovelace Institute, “Examining the Black Box,” Ada Lovelace Institute, Tech. Rep., Apr. 2020. [Online]. Available: https://www.adalovelaceinstitute.org/report/examining-the-black-box-tools-for-assessing-algorithmic-systems/\n\nAda Lovelace Institute [2021] ——, “Technical methods for regulatory inspection of algorithmic systems,” Ada Lovelace Institute, Tech. Rep., Dec. 2021. [Online]. Available: https://www.adalovelaceinstitute.org/report/technical-methods-regulatory-inspection/\n\nCostanza-Chock et al. [2022] S. Costanza-Chock, I. D. Raji, and J. Buolamwini, “Who Audits the Auditors? Recommendations from a field scan of the algorithmic auditing ecosystem,” in Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, ser. FAccT ’22. New York, NY, USA: Association for Computing Machinery, Jun. 2022, pp. 1571–1583. [Online]. Available: https://doi.org/10.1145/3531146.3533213\n\nSelbst [2021] A. D. Selbst, “An institutional view of algorithmic impact,” Harvard Journal of Law & Technology, vol. 35, no. 1, 2021.\n\nBandy [2021a] J. Bandy, “Problematic Machine Behavior: A Systematic Literature Review of Algorithm Audits,” Proceedings of the ACM on Human-Computer Interaction, vol. 5, no. CSCW1, pp. 74:1–74:34, Apr. 2021. [Online]. Available: https://doi.org/10.1145/3449148\n\nSteed and Caliskan [2021] R. Steed and A. Caliskan, “Image representations learned with unsupervised pre-training contain human-like biases,” in Proceedings of the 2021 ACM conference on Fairness, Accountability, and Transparency, 2021, pp. 701–713.\n\nHeikkilä [2022a] M. Heikkilä, “How it feels to be sexually objectified by an AI,” MIT Technology Review, Dec. 2022. [Online]. Available: https://www.technologyreview.com/2022/12/13/1064810/how-it-feels-to-be-sexually-objectified-by-an-ai/\n\nFeathers [2021] T. Feathers, “Proctorio Is Using Racist Algorithms to Detect Faces,” Vice, Apr. 2021. [Online]. Available: https://www.vice.com/en/article/g5gxg3/proctorio-is-using-racist-algorithms-to-detect-faces\n\nTouvron et al. [2023] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar et al., “Llama: Open and efficient foundation language models,” arXiv preprint arXiv:2302.13971, 2023.\n\nBirhane and Prabhu [2021a] A. Birhane and V. U. Prabhu, “Large image datasets: A pyrrhic win for computer vision?” in 2021 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, 2021, pp. 1536–1546.\n\nSolon [2019] O. Solon, “Facial recognition’s ‘dirty little secret’: Millions of online photos scraped without consent,” NBC News, vol. 12, 2019.\n\nReisner [2023] A. Reisner, “Revealed: The authors whose pirated books are powering generative ai,” The Atlantic, 2023.\n\nDwork et al. [2012] C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel, “Fairness through awareness,” in Proceedings of the 3rd innovations in theoretical computer science conference, 2012, pp. 214–226.\n\nRaji and Buolamwini [2022] I. D. Raji and J. Buolamwini, “Actionable Auditing Revisited: Investigating the Impact of Publicly Naming Biased Performance Results of Commercial AI Products,” Communications of the ACM, vol. 66, no. 1, pp. 101–108, Dec. 2022. [Online]. Available: https://dl.acm.org/doi/10.1145/3571151\n\nAngwin et al. [2016] J. Angwin, J. Larson, S. Mattu, and L. Kirchner, “Machine Bias,” ProPublica, May 2016. [Online]. Available: https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing\n\nChouldechova et al. [2018] A. Chouldechova, D. Benavides-Prado, O. Fialko, and R. Vaithianathan, “A case study of algorithm-assisted decision making in child maltreatment hotline screening decisions,” in Conference on Fairness, Accountability and Transparency. PMLR, 2018, pp. 134–148.\n\nChen et al. [2015] L. Chen, A. Mislove, and C. Wilson, “Peeking beneath the hood of uber,” in Proceedings of the 2015 Internet Measurement Conference, 2015, pp. 495–508.\n\nChen et al. [2016] ——, “An empirical analysis of algorithmic pricing on amazon marketplace,” in Proceedings of the 25th international conference on World Wide Web, 2016, pp. 1339–1349.\n\nBuolamwini and Gebru [2018] J. Buolamwini and T. Gebru, “Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification,” in Proceedings of the 1st Conference on Fairness, Accountability and Transparency, ser. Proceedings of Machine Learning Research, S. A. Friedler and C. Wilson, Eds., vol. 81. New York, NY, USA: PMLR, Jan. 2018, pp. 77–91. [Online]. Available: http://proceedings.mlr.press/v81/buolamwini18a.html\n\nKoenecke et al. [2020] A. Koenecke, A. Nam, E. Lake, J. Nudell, M. Quartey, Z. Mengesha, C. Toups, J. R. Rickford, D. Jurafsky, and S. Goel, “Racial disparities in automated speech recognition,” Proceedings of the National Academy of Sciences, vol. 117, no. 14, pp. 7684–7689, 2020.\n\nMandal et al. [2023] A. Mandal, S. Little, and S. Leavy, “Gender bias in multimodal models: A transnational feminist approach considering geographical region and culture,” arXiv preprint arXiv:2309.04997, 2023.\n\nWilson et al. [2021] C. Wilson, A. Ghosh, S. Jiang, A. Mislove, L. Baker, J. Szary, K. Trindel, and F. Polli, “Building and auditing fair algorithms: A case study in candidate screening,” in Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 2021, pp. 666–677.\n\nLum and Isaac [2016] K. Lum and W. Isaac, “To predict and serve?” Significance, vol. 13, no. 5, pp. 14–19, 2016.\n\nKoenecke et al. [2023] A. Koenecke, E. Giannella, R. Willer, and S. Goel, “Popular support for balancing equity and efficiency in resource allocation: A case study in online advertising to increase welfare program awareness,” in Proceedings of the International AAAI Conference on Web and Social Media, vol. 17, 2023, pp. 494–506.\n\nSapiezynski et al. [2022] P. Sapiezynski, A. Ghosh, L. Kaplan, A. Rieke, and A. Mislove, “Algorithms that ”Don’t See Color”: Comparing Biases in Lookalike and Special Ad Audiences,” in Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society, Jul. 2022, pp. 609–616. [Online]. Available: http://arxiv.org/abs/1912.07579\n\nNoble [2018] S. U. Noble, Algorithms of Oppression: How Search Engines Reinforce Racism. New York: NYU Press, 2018. [Online]. Available: www.jstor.org/stable/j.ctt1pwt9w5\n\nEubanks [2018] V. Eubanks, Automating Inequality: How High-Tech Tools Profile, Police, and Punish the Poor. St. Martin’s Publishing Group, Jan. 2018.\n\nRaji et al. [2024] I. D. Raji, R. Steed, V. Ojewale, B. Vecchione, and A. Birhane, “Towards AI Accountability Infrastructure: Gaps and opportunities in AI audit tooling,” in To Appear, 2024 CHI Conference on Human Factors in Computing Systems. Association for Computing Machinery, 2024.\n\nMcGregor [2021] S. McGregor, “Preventing repeated real world AI failures by cataloging incidents: The AI incident database,” in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 35, no. 17, 2021, pp. 15 458–15 463.\n\nFink [2018] K. Fink, “Opening the government’s black boxes: freedom of information and algorithmic accountability,” Information, Communication & Society, vol. 21, no. 10, pp. 1453–1471, 2018.\n\nJobin et al. [2019] A. Jobin, M. Ienca, and E. Vayena, “The global landscape of AI ethics guidelines,” Nature machine intelligence, vol. 1, no. 9, pp. 389–399, 2019.\n\nAjunwa et al. [2016] I. Ajunwa, S. Friedler, C. E. Scheidegger, and S. Venkatasubramanian, “Hiring by algorithm: predicting and preventing disparate impact,” Available at SSRN, 2016.\n\nBandy [2021b] J. Bandy, “Problematic machine behavior: A systematic literature review of algorithm audits,” Proceedings of the ACM on Human-Computer Interaction, vol. 5, no. CSCW1, pp. 1–34, 2021.\n\nKrafft et al. [2021] P. Krafft, M. Young, M. Katell, J. E. Lee, S. Narayan, M. Epstein, D. Dailey, B. Herman, A. Tam, V. Guetler et al., “An action-oriented ai policy toolkit for technology audits by community advocates and activists,” in Proceedings of the 2021 ACM conference on Fairness, Accountability, and Transparency, 2021, pp. 772–781.\n\nJiang et al. [2019] S. Jiang, R. E. Robertson, and C. Wilson, “Bias Misperceived:The Role of Partisanship and Misinformation in YouTube Comment Moderation,” Proceedings of the International AAAI Conference on Web and Social Media, vol. 13, pp. 278–289, Jul. 2019. [Online]. Available: https://ojs.aaai.org/index.php/ICWSM/article/view/3229\n\nBlack et al. [2022] E. Black, H. Elzayn, A. Chouldechova, J. Goldin, and D. Ho, “Algorithmic Fairness and Vertical Equity: Income Fairness with IRS Tax Audit Models,” in 2022 ACM Conference on Fairness, Accountability, and Transparency. Seoul Republic of Korea: ACM, Jun. 2022, pp. 1479–1503. [Online]. Available: https://dl.acm.org/doi/10.1145/3531146.3533204\n\nAbid et al. [2021] A. Abid, M. Farooqi, and J. Zou, “Persistent Anti-Muslim Bias in Large Language Models,” in Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, ser. AIES ’21. New York, NY, USA: Association for Computing Machinery, Jul. 2021, pp. 298–306. [Online]. Available: https://doi.org/10.1145/3461702.3462624\n\nBirhane and Prabhu [2021b] A. Birhane and V. U. Prabhu, “Large image datasets: A pyrrhic win for computer vision?” in 2021 IEEE Winter Conference on Applications of Computer Vision (WACV), Jan. 2021, pp. 1536–1546.\n\nBrown et al. [2019] A. Brown, A. Chouldechova, E. Putnam-Hornstein, A. Tobin, and R. Vaithianathan, “Toward Algorithmic Accountability in Public Services: A Qualitative Study of Affected Community Perspectives on Algorithmic Decision-making in Child Welfare Services,” in Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, ser. CHI ’19. New York, NY, USA: Association for Computing Machinery, May 2019, pp. 1–12. [Online]. Available: https://dl.acm.org/doi/10.1145/3290605.3300271\n\nKalluri et al. [2020] P. Kalluri et al., “Don’t ask if artificial intelligence is good or fair, ask how it shifts power,” Nature, vol. 583, no. 7815, pp. 169–169, 2020.\n\nMetcalf et al. [2021] J. Metcalf, E. Moss, E. A. Watkins, R. Singh, and M. C. Elish, “Algorithmic Impact Assessments and Accountability: The Co-construction of Impacts,” in Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, ser. FAccT ’21. New York, NY, USA: Association for Computing Machinery, Mar. 2021, pp. 735–746. [Online]. Available: https://dl.acm.org/doi/10.1145/3442188.3445935\n\nBirhane et al. [2021] A. Birhane, V. U. Prabhu, and E. Kahembwe, “Multimodal datasets: Misogyny, pornography, and malignant stereotypes,” Oct. 2021. [Online]. Available: http://arxiv.org/abs/2110.01963\n\nBao et al. [2021] M. Bao, A. Zhou, S. Zottola, B. Brubach, S. Desmarais, A. Horowitz, K. Lum, and S. Venkatasubramanian, “It’s COMPASlicated: The Messy Relationship between RAI Datasets and Algorithmic Fairness Benchmarks,” arXiv:2106.05498 [cs], Jun. 2021. [Online]. Available: http://arxiv.org/abs/2106.05498\n\nSamuelson [2023] P. Samuelson, “Generative AI meets copyright,” Science, vol. 381, no. 6654, pp. 158–161, 2023.\n\nStapleton et al. [2022a] L. Stapleton, M. H. Lee, D. Qing, M. Wright, A. Chouldechova, K. Holstein, Z. S. Wu, and H. Zhu, “Imagining new futures beyond predictive systems in child welfare: A qualitative study with impacted stakeholders,” in Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, 2022, pp. 1162–1177.\n\nRadiya-Dixit and Neff [2023] E. Radiya-Dixit and G. Neff, “A Sociotechnical Audit: Assessing Police Use of Facial Recognition,” in Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency, ser. FAccT ’23. New York, NY, USA: Association for Computing Machinery, Jun. 2023, pp. 1334–1346. [Online]. Available: https://dl.acm.org/doi/10.1145/3593013.3594084\n\nStapleton et al. [2022b] L. Stapleton, M. H. Lee, D. Qing, M. Wright, A. Chouldechova, K. Holstein, Z. S. Wu, and H. Zhu, “Imagining new futures beyond predictive systems in child welfare: A qualitative study with impacted stakeholders,” in Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, ser. FAccT ’22. New York, NY, USA: Association for Computing Machinery, Jun. 2022, pp. 1162–1177. [Online]. Available: https://dl.acm.org/doi/10.1145/3531146.3533177\n\nWoodruff et al. [2018] A. Woodruff, S. E. Fox, S. Rousso-Schindler, and J. Warshaw, “A Qualitative Exploration of Perceptions of Algorithmic Fairness,” in Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems, ser. CHI ’18. New York, NY, USA: Association for Computing Machinery, Apr. 2018, pp. 1–14. [Online]. Available: https://dl.acm.org/doi/10.1145/3173574.3174230\n\nSolaiman et al. [2023] I. Solaiman, Z. Talat, W. Agnew, L. Ahmad, D. Baker, S. L. Blodgett, H. Daumé III, J. Dodge, E. Evans, S. Hooker, Y. Jernite, A. S. Luccioni, A. Lusoli, M. Mitchell, J. Newman, M.-T. Png, A. Strait, and A. Vassilev, “Evaluating the Social Impact of Generative AI Systems in Systems and Society,” Jun. 2023. [Online]. Available: http://arxiv.org/abs/2306.05949\n\nTabassi [2023] E. Tabassi, “AI Risk Management Framework: AI RMF (1.0),” National Institute of Standards and Technology, Gaithersburg, MD, Tech. Rep. error: NIST AI 100-1, 2023. [Online]. Available: https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf\n\nRamesh et al. [2022] D. Ramesh, V. Kameswaran, D. Wang, and N. Sambasivan, “How Platform-User Power Relations Shape Algorithmic Accountability: A Case Study of Instant Loan Platforms and Financially Stressed Users in India,” in Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, ser. FAccT ’22. New York, NY, USA: Association for Computing Machinery, Jun. 2022, pp. 1917–1928. [Online]. Available: https://dl.acm.org/doi/10.1145/3531146.3533237\n\nSingh [2023] J. Singh, “Google to prohibit personal loan apps from accessing user photos, contacts,” Apr. 2023. [Online]. Available: https://techcrunch.com/2023/04/05/google-personal-loan-apps-update/\n\nTorralba et al. [2020] A. Torralba, R. Fergus, and B. Freeman, “80 Million Tiny Images,” Jun. 2020. [Online]. Available: https://groups.csail.mit.edu/vision/TinyImages/\n\nMurgia and Harlow [2019] M. Murgia and M. Harlow, “Who’s using your face? The ugly truth about facial recognition,” Financial Times, Sep. 2019. [Online]. Available: https://www.ft.com/content/cf19b956-60a2-11e9-b285-3acd5d43599e\n\nMurgia [2019] M. Murgia, “Microsoft quietly deletes largest public face recognition data set,” Financial Times, Jun. 2019. [Online]. Available: https://www.ft.com/content/7d3e0d6a-87a0-11e9-a028-86cea8523dc2\n\nSuresh and Guttag [2021] H. Suresh and J. Guttag, “A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle,” in Proceedings of the 1st ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization, ser. EAAMO ’21. New York, NY, USA: Association for Computing Machinery, Nov. 2021, pp. 1–9. [Online]. Available: https://dl.acm.org/doi/10.1145/3465416.3483305\n\nSchwartz et al. [2020] R. Schwartz, J. Dodge, N. A. Smith, and O. Etzioni, “Green AI,” Communications of the ACM, vol. 63, no. 12, pp. 54–63, Nov. 2020. [Online]. Available: https://dl.acm.org/doi/10.1145/3381831\n\nRakova and Dobbe [2023] B. Rakova and R. Dobbe, “Algorithms as social-ecological-technological systems: an environmental justice lens on algorithmic audits,” arXiv preprint arXiv:2305.05733, 2023.\n\nCrawford [2021] K. Crawford, The atlas of AI: Power, politics, and the planetary costs of artificial intelligence. Yale University Press, 2021.\n\nGansky and McDonald [2022] B. Gansky and S. McDonald, “CounterFAccTual: How FAccT Undermines Its Organizing Principles,” in 2022 ACM Conference on Fairness, Accountability, and Transparency. Seoul Republic of Korea: ACM, Jun. 2022, pp. 1982–1992. [Online]. Available: https://dl.acm.org/doi/10.1145/3531146.3533241\n\nGoodman and Trehu [2022] E. P. Goodman and J. Trehu, “AI Audit Washing and Accountability,” Rochester, NY, Sep. 2022. [Online]. Available: https://papers.ssrn.com/abstract=4227350\n\nSloane et al. [2022] M. Sloane, E. Moss, and R. Chowdhury, “A Silicon Valley love triangle: Hiring algorithms, pseudo-science, and the quest for auditability,” Patterns (New York, N.Y.), vol. 3, no. 2, p. 100425, Feb. 2022.\n\nLuminos.Law [a] Luminos.Law, “Public Resources.” [Online]. Available: https://luminos.law/resources\n\nFoxglove [a] Foxglove, “Who We Are.” [Online]. Available: https://www.foxglove.org.uk/who-we-are/\n\nAWO [2023a] AWO, “AWO,” 2023. [Online]. Available: https://awo.agency/\n\nLuminos.Law [b] Luminos.Law, “Clients.” [Online]. Available: https://luminos.law/our-clients\n\nAWO [a] AWO, “Services.” [Online]. Available: https://awo.agency/\n\nLuminos.Law [c] Luminos.Law, “Our Work.” [Online]. Available: https://luminos.law/our-work\n\nAWO [b] AWO, “Blog.” [Online]. Available: https://awo.agency/\n\nDark [2020a] M. Dark, “Home Office says it will abandon its racist visa algorithm - after we sued them,” Aug. 2020. [Online]. Available: https://www.foxglove.org.uk/2020/08/04/home-office-says-it-will-abandon-its-racist-visa-algorithm-after-we-sued-them/\n\nDark [2020b] ——, “We put a stop to the A Level grading algorithm!” Aug. 2020. [Online]. Available: https://www.foxglove.org.uk/2020/08/17/we-put-a-stop-to-the-a-level-grading-algorithm/\n\nFoxglove [b] Foxglove, “Areas of Work.” [Online]. Available: https://www.foxglove.org.uk/who-we-are/areas-of-work/\n\nHegarty [2021] T. Hegarty, “The government has scrapped the deadline for the NHS Data Grab,” Jul. 2021. [Online]. Available: https://www.foxglove.org.uk/2021/07/22/the-government-has-scrapped-the-deadline-for-the-nhs-data-grab/\n\nTau [2023] B. Tau, “Banning TikTok in the U.S. Is Easier Said Than Done,” Wall Street Journal, Mar. 2023. [Online]. Available: https://www.wsj.com/articles/tiktok-ban-legal-explained-bbeb21c2\n\nLynch [2023] S. Lynch, “A New Anti-Bias A.I. Hiring Law Is Now in Effect. How to Know If You’re in Compliance,” Inc.com, Jul. 2023. [Online]. Available: https://www.inc.com/sarah-lynch/new-anti-bias-ai-hiring-law-now-in-effect-how-to-comply.html\n\nAWO [2023b] AWO, “AWO analysis shows gaps in effective protection from AI harms,” Jul. 2023. [Online]. Available: https://awo.agency/\n\nLuminos.Law [d] Luminos.Law, “Microwave.” [Online]. Available: https://www.luminos.ai/microwave\n\nCumbo et al. [2021] L. A. Cumbo, A. Ampry-Samuel, H. K. Rosenthal, R. E. Cornegy, B. Kallos, A. E. Adams, F. N. Louis, M. S. Chin, F. Cabrera, D. L. Rose, V. L. Gibson, J. L. Brannan, C. Rivera, M. Levine, D. I. Ayala, I. D. Miller, S. T. Levin, and I. D. Barron, “A Local Law to amend the administrative code of the city of New York, in relation to automated employment decision tools,” Dec. 2021. [Online]. Available: https://www.nyc.gov/site/dca/about/automated-employment-decision-tools.page\n\nBrennen et al. [2022] A. Brennen, R. Ashley, R. Calix, JJ. Ben-Joseph, G. Sieniawski, and M. Gogia, “AI Assurance Audit of RoBERTa, an Open source, Pretrained Large Language Model,” IQT Labs, Tech. Rep., Dec. 2022. [Online]. Available: https://assets.iqt.org/pdfs/IQTLabs_RoBERTaAudit_Dec2022_final.pdf/web/viewer.html\n\nCalix et al. [2022] R. A. Calix, J. Ben-Joseph, N. Lopatina, R. Ashley, M. Gogia, G. Sieniawski, and A. Brennen, “Saisiyat Is Where It Is At! Insights Into Backdoors And Debiasing Of Cross Lingual Transformers For Named Entity Recognition,” in 2022 IEEE International Conference on Big Data (Big Data), Dec. 2022, pp. 2940–2949. [Online]. Available: https://ieeexplore.ieee.org/document/10020403\n\nLee et al. [2020] M. W. Lee, N. Stringer, and N. Zanini, “Student-level equalities analyses for GCSE and A level,” Ofqual, Tech. Rep., 2020.\n\nThe Joint Council for the Welfare of Immigrants [2020] The Joint Council for the Welfare of Immigrants, “We won! Home Office to stop using racist visa algorithm,” Aug. 2020. [Online]. Available: https://www.jcwi.org.uk/News/we-won-home-office-to-stop-using-racist-visa-algorithm\n\nFitzgerald and Crider [2020] M. Fitzgerald and C. Crider, “Under pressure, UK government releases NHS COVID data deals with big tech,” Jun. 2020. [Online]. Available: https://www.opendemocracy.net/en/ournhs/under-pressure-uk-government-releases-nhs-covid-data-deals-big-tech/\n\n[106] Parity Consulting, “Parity Consulting.” [Online]. Available: https://www.get-parity.com\n\nCassidy et al. [2022] B. Cassidy, R. Hittner, B. Crowley, Z. Bowman, and J. Fogarty, “An auditor’s mindset in an AI-driven world,” Tech. Rep., 2022. [Online]. Available: https://www2.deloitte.com/content/dam/Deloitte/us/Documents/deloitte-analytics/us-ai-institute-auditors-mindset.pdf\n\nBuehler et al. [2021] K. Buehler, R. Dooley, L. Grennan, and A. Singla, “Identifying and managing your biggest AI risks | McKinsey,” May 2021. [Online]. Available: https://www.mckinsey.com/capabilities/quantumblack/our-insights/getting-to-know-and-manage-your-biggest-ai-risks\n\nAccenture [2023] Accenture, “AI ethics & governance,” 2023. [Online]. Available: https://www.accenture.com/us-en/services/applied-intelligence/ai-ethics-governance\n\nBusiness for Social Responsibility [2019] Business for Social Responsibility, “Google Celebrity Recognition API Human Rights Assessment: Executive Summary,” Tech. Rep., Oct. 2019. [Online]. Available: https://www.bsr.org/reports/BSR-Google-CR-API-HRIA-Executive-Summary.pdf\n\nWarofka [2018] A. Warofka, “An independent assessment of the human rights impact of facebook in myanmar,” Facebook Newsroom, November, vol. 5, 2018.\n\nOffice of Public Affairs [2022] Office of Public Affairs, “Justice Department Secures Groundbreaking Settlement Agreement with Meta Platforms, Formerly Known as Facebook, to Resolve Allegations of Discriminatory Advertising,” Jun. 2022. [Online]. Available: https://www.justice.gov/opa/pr/justice-department-secures-groundbreaking-settlement-agreement-meta-platforms-formerly-known\n\nORCAA [2023] ORCAA, “ORCAA,” Jul. 2023. [Online]. Available: https://orcaarisk.com\n\nEticas [2022] Eticas, “Algorithmic Audits,” 2022. [Online]. Available: https://eticas.tech/algorithmic-audits\n\nBABL AI [a] BABL AI, “About Us.” [Online]. Available: https://babl.ai/about-us/\n\nBABL AI [b] ——, “Services.” [Online]. Available: https://babl.ai/services/\n\nORCAA [2020] ORCAA, “Description of Algorithmic Audit: Pre-built Assessments,” ORCAA, Tech. Rep., Dec. 2020. [Online]. Available: https://www.hirevue.com/resources/template/orcaa-report\n\nSamuel [2022] S. Samuel, “There’s something missing from the White House’s AI ethics blueprint,” Vox, Oct. 2022. [Online]. Available: https://www.vox.com/future-perfect/23387228/ai-bill-of-rights-white-house-artificial-intelligence-bias\n\nEngler [2021] A. C. Engler, “Independent auditors are struggling to hold AI companies accountable,” Fast Company, Jan. 2021. [Online]. Available: https://www.fastcompany.com/90597594/ai-algorithm-auditing-hirevue\n\nThe Electronic Privacy Information Center (2019) [EPIC]\n\nThe Electronic Privacy Information Center (EPIC), “Complaint and Request for Investigation, Injunction, and Other Relief in the Matter of HireVue, Inc.” Nov. 2019. [Online]. Available: https://epic.org/wp-content/uploads/privacy/ftc/hirevue/EPIC_FTC_HireVue_Complaint.pdf\n\nDiakopoulos [2015] N. Diakopoulos, “Algorithmic accountability: Journalistic investigation of computational power structures,” Digital journalism, vol. 3, no. 3, pp. 398–415, 2015.\n\nHao and Seetharaman [2023] K. Hao and D. Seetharaman, “Cleaning Up ChatGPT Takes Heavy Toll on Human Workers,” Wall Street Journal, Jul. 2023. [Online]. Available: https://www.wsj.com/articles/chatgpt-openai-content-abusive-sexually-explicit-harassment-kenya-workers-on-human-workers-cf191483\n\nHill [2020] K. Hill, “Wrongfully Accused by an Algorithm,” The New York Times, Jun. 2020. [Online]. Available: https://www.nytimes.com/2020/06/24/technology/facial-recognition-arrest.html\n\nHao and Swart [2022] K. Hao and H. Swart, “South Africa’s private surveillance machine is fueling a digital apartheid,” MIT Technology Review, Apr. 2022. [Online]. Available: https://www.technologyreview.com/2022/04/19/1049996/south-africa-ai-surveillance-digital-apartheid/\n\nHao and Hernández [2022] K. Hao and A. P. Hernández, “How the AI industry profits from catastrophe,” MIT Technology Review, Apr. 2022. [Online]. Available: https://www.technologyreview.com/2022/04/20/1050392/ai-industry-appen-scale-data-labels/\n\nHeikkilä [2022b] M. Heikkilä, “The viral AI avatar app Lensa undressed me—without my consent,” MIT Technology Review, Dec. 2022. [Online]. Available: https://www.technologyreview.com/2022/12/12/1064751/the-viral-ai-avatar-app-lensa-undressed-me-without-my-consent/\n\nProPublica [2023] ProPublica, “Investigative Journalism and News in the Public Interest,” Oct. 2023. [Online]. Available: https://www.propublica.org/\n\n[128] The Markup, “About Us.” [Online]. Available: https://themarkup.org/about\n\nJeffries and Yin [2021] A. Jeffries and L. Yin, “Amazon Puts Its Own “Brands” First Above Better-Rated Products – The Markup,” The Markup, Oct. 2021. [Online]. Available: https://themarkup.org/amazons-advantage/2021/10/14/amazon-puts-its-own-brands-first-above-better-rated-products\n\nAngwin and Parris Jr. [2016] J. Angwin and T. Parris Jr., “Facebook Lets Advertisers Exclude Users by Race,” ProPublica, Oct. 2016. [Online]. Available: https://www.propublica.org/article/facebook-lets-advertisers-exclude-users-by-race\n\nKeegan [2021] J. Keegan, “Facebook Got Rid of Racial Ad Categories. Or Did It? – The Markup,” The Markup, Jul. 2021. [Online]. Available: https://themarkup.org/citizen-browser/2021/07/09/facebook-got-rid-of-racial-ad-categories-or-did-it\n\nFaife and Ng [2021] C. Faife and A. Ng, “Credit Card Ads Were Targeted by Age, Violating Facebook’s Anti-Discrimination Policy – The Markup,” The Markup, Apr. 2021. [Online]. Available: https://themarkup.org/citizen-browser/2021/04/29/credit-card-ads-were-targeted-by-age-violating-facebooks-anti-discrimination-policy\n\nThe Markup [2022] The Markup, “Citizen Browser,” Oct. 2022. [Online]. Available: https://themarkup.org/series/citizen-browser\n\nNg and Faife [2021] A. Ng and C. Faife, “Facebook Pledges to Remove Discriminatory Credit and Loan Ads Discovered by T"
    }
}
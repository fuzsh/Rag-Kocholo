{
    "id": "wrong_mix_random_publicationDate_00092_1",
    "rank": 98,
    "data": {
        "url": "https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html",
        "read_more_link": "",
        "language": "en",
        "title": "Hello Dolly: Democratizing the magic of ChatGPT with open models",
        "top_image": "https://www.databricks.com/sites/default/files/2023-03/hello-dolly-democratizing-magic-chatgp.png?v=1679659914",
        "meta_img": "https://www.databricks.com/sites/default/files/2023-03/hello-dolly-democratizing-magic-chatgp.png?v=1679659914",
        "images": [
            "https://www.databricks.com/en-blog-assets/static/0434503523a3e0ad4cb3ac6836432c45/CategoryHeader-Company-1_0undefined.png",
            "https://www.databricks.com/en-blog-assets/static/0434503523a3e0ad4cb3ac6836432c45/CategoryHeader-Company-1_0undefined.png",
            "https://www.databricks.com/sites/default/files/2022-05/CategoryIcon-Platform-1.png?v=undefined",
            "https://www.databricks.com/sites/default/files/2022-05/CategoryIcon-Platform-2.png?v=undefined",
            "https://www.databricks.com/sites/default/files/2022-05/CategoryIcon-Platform-3.png?v=undefined",
            "https://www.databricks.com/en-blog-assets/static/098d4af8c5f96ee0b37e81de9227ea4e/eb-state-of-data-and-ai-report-brr-imageundefined.png",
            "https://www.databricks.com/en-blog-assets/static/098d4af8c5f96ee0b37e81de9227ea4e/eb-state-of-data-and-ai-report-brr-imageundefined.png",
            "https://www.databricks.com/en-blog-assets/static/a96612a9540d7d33ef484a26119a0e51/gartner-mq-ds-ml-brr-imgundefined.png",
            "https://www.databricks.com/en-blog-assets/static/a96612a9540d7d33ef484a26119a0e51/gartner-mq-ds-ml-brr-imgundefined.png",
            "https://www.databricks.com/en-blog-assets/static/6e74f35d2ba734c4ba24d3d08a94b798/eb-big-book-genai-brr-imageundefined.png",
            "https://www.databricks.com/en-blog-assets/static/6e74f35d2ba734c4ba24d3d08a94b798/eb-big-book-genai-brr-imageundefined.png",
            "https://www.databricks.com/en-blog-assets/static/7e7d7b3644199353d6c402aba9d8b223/2024-daiwt-brr-adundefined.png",
            "https://www.databricks.com/en-blog-assets/static/7e7d7b3644199353d6c402aba9d8b223/2024-daiwt-brr-adundefined.png",
            "https://www.databricks.com/en-blog-assets/static/a5ccf0326bc75450e4e5a24ddbd63013/databricks-default1712162038.png",
            "https://www.databricks.com/en-blog-assets/static/a5ccf0326bc75450e4e5a24ddbd63013/databricks-default1712162038.png",
            "https://www.databricks.com/en-blog-assets/static/a5ccf0326bc75450e4e5a24ddbd63013/databricks-default1712162038.png",
            "https://www.databricks.com/en-blog-assets/static/a5ccf0326bc75450e4e5a24ddbd63013/databricks-default1712162038.png",
            "https://www.databricks.com/en-blog-assets/static/724dadbce517935dec78155e4753a219/telco-icon-21715274112.png",
            "https://www.databricks.com/en-blog-assets/static/724dadbce517935dec78155e4753a219/telco-icon-21715274112.png",
            "https://www.databricks.com/sites/default/files/2022-12/gpcicon_small.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            "LLM",
            "Large Language Model",
            "ChatGPT"
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2023-03-24T00:00:00",
        "summary": "",
        "meta_description": "Introducing 'Hello Dolly,' a project to democratize AI by integrating ChatGPT and open models, making advanced AI accessible to everyone.",
        "meta_lang": "en",
        "meta_favicon": "/en-blog-assets/favicon-32x32.png?v=c9b9916c3b27dc51866c46b79a6e9b88",
        "meta_site_name": "Databricks",
        "canonical_link": "https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html",
        "text": "Update Apr 12, 2023: We have released Dolly 2.0, licensed for both research and commercial use. See the new blog post here.\n\nSummary\n\nWe show that anyone can take a dated off-the-shelf open source large language model (LLM) and give it magical ChatGPT-like instruction following ability by training it in 30 minutes on one machine, using high-quality training data. Surprisingly, instruction-following does not seem to require the latest or largest models: our model is only 6 billion parameters, compared to 175 billion for GPT-3. We open source the code for our model (Dolly) and show how it can be re-created on Databricks. We believe models like Dolly will help democratize LLMs, transforming them from something very few companies can afford into a commodity every company can own and customize to improve their products.\n\nBackground\n\nChatGPT, a proprietary instruction-following model, was released in November 2022 and took the world by storm. The model was trained on trillions of words from the web, requiring massive numbers of GPUs to develop. This quickly led to Google and other companies releasing their own proprietary instruction-following models. In February 2023, Meta released the weights for a set of high-quality (but not instruction-following) language models called LLaMA to academic researchers, trained for over 80,000 GPU-hours each. Then, in March, Stanford built the Alpaca model, which was based on LLaMA, but tuned on a small dataset of 50,000 human-like questions and answers that, surprisingly, made it exhibit ChatGPT-like interactivity.\n\nIntroducing Dolly\n\nToday we are introducing Dolly, a cheap-to-build LLM that exhibits a surprising degree of the instruction following capabilities exhibited by ChatGPT. Whereas the work from the Alpaca team showed that state-of-the-art models could be coaxed into high quality instruction-following behavior, we find that even years-old open source models with much earlier architectures exhibit striking behaviors when fine tuned on a small corpus of instruction training data. Dolly works by taking an existing open source 6 billion parameter model from EleutherAI and modifying it ever so slightly to elicit instruction following capabilities such as brainstorming and text generation not present in the original model, using data from Alpaca.\n\nThe model underlying Dolly only has 6 billion parameters, compared to 175 billion in GPT-3, and is two years old, making it particularly surprising that it works so well. This suggests that much of the qualitative gains in state-of-the-art models like ChatGPT may owe to focused corpuses of instruction-following training data, rather than larger or better-tuned base models. We’re calling the model Dolly — after Dolly the sheep, the first cloned mammal — because it's an open source clone of an Alpaca, inspired by a LLaMA. We’re in the earliest days of the democratization of AI for the enterprise, and much work remains to be done, but we believe the technology underlying Dolly represents an exciting new opportunity for companies that want to cheaply build their own instruction-following models.\n\nWe evaluated Dolly on the instruction-following capabilities described in the InstructGPT paper that ChatGPT is based on and found that it exhibits many of the same qualitative capabilities, including text generation, brainstorming and open Q&A. Of particular note in these examples is not the quality of the generated text, but rather the vast improvement in instruction-following capability that results from fine tuning a years-old open source model on a small, high quality dataset.\n\nGeneration\n\nOpen Q&A\n\nBrainstorming\n\nWhy Open Models?\n\nThere are many reasons a company would prefer to build their own model rather than sending data to a centralized LLM provider that serves a proprietary model behind an API. For many companies, the problems and datasets most likely to benefit from AI represent their most sensitive and proprietary intellectual property, and handing it over to a third party may be unpalatable. Furthermore, organizations may have different tradeoffs in terms of model quality, cost, and desired behavior. We believe that most ML users are best served long term by directly owning their models.\n\nWe are open sourcing a simple Databricks notebook that you can use to build Dolly yourself on Databricks. To download and experiment with the model, visit the Databricks Hugging Face Page!\n\nWhat’s Next?\n\nThe release of Dolly is the first in a series of announcements Databricks is making that focus on helping every organization harness the power of large language models. We believe in the incredible power of artificial intelligence to transform the productivity of every organization and individual, and welcome you to join us on this journey. Stay tuned for more in this area in the coming weeks!\n\nYou can also explore ways that your company can benefit from LLMs and how Databricks built Dolly in our webinar on April 25th. Join us!\n\nWe're also excited to share a lot more about Dolly, including hands-on training and a deeper dive into our LLM roadmap, at this year's Data and AI Summit. We encourage you to register to join us in-person or virtually.\n\nAcknowledgments\n\nThis work owes much to the efforts and insights of many incredible organizations. This would have been impossible without EleutherAI open sourcing and training GPT-J. We are inspired by the incredible ideas and data from the Stanford Center for Research on Foundation Models and specifically the team behind Alpaca. The core idea behind the outsized power of small dataset is thanks to the original paper on Self-Instruct. We are also thankful to Hugging Face for hosting, open sourcing, and maintaining countless models and libraries; their contribution to the state of the art cannot be overstated.\n\nDisclaimer: Generative AI is an emerging technology and we're in the early stages of research around how to address factual accuracy, bias, offensive responses, general toxicity, and hallucinations in LLMs. Dolly, like other language models, can sometimes exhibit these behaviors and we urge our users to exercise good judgment in designing applications of this technology."
    }
}
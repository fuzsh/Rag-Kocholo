{
    "id": "dbpedia_3365_3",
    "rank": 70,
    "data": {
        "url": "https://www.mdpi.com/2227-7390/8/7/1122",
        "read_more_link": "",
        "language": "en",
        "title": "dCATCH‚ÄîA Numerical Package for d-Variate near G-Optimal Tchakaloff Regression via Fast NNLS",
        "top_image": "https://pub.mdpi-res.com/mathematics/mathematics-08-01122/article_deploy/html/images/mathematics-08-01122-g001-550.jpg?1594291333",
        "meta_img": "https://pub.mdpi-res.com/mathematics/mathematics-08-01122/article_deploy/html/images/mathematics-08-01122-g001-550.jpg?1594291333",
        "images": [
            "https://pub.mdpi-res.com/img/design/mdpi-pub-logo-black-small1.svg?da3a8dcae975a41c?1723528173",
            "https://pub.mdpi-res.com/img/design/mdpi-pub-logo-black-small1.svg?da3a8dcae975a41c?1723528173",
            "https://pub.mdpi-res.com/img/journals/mathematics-logo.png?8600e93ff98dbf14",
            "https://www.mdpi.com/bundles/mdpisciprofileslink/img/unknown-user.png",
            "https://www.mdpi.com/bundles/mdpisciprofileslink/img/unknown-user.png",
            "https://pub.mdpi-res.com/img/design/orcid.png?0465bc3812adeb52?1723528173",
            "https://www.mdpi.com/bundles/mdpisciprofileslink/img/unknown-user.png",
            "https://www.mdpi.com/mathematics/mathematics-08-01122/article_deploy/html/images/mathematics-08-01122-g001-550.jpg",
            "https://www.mdpi.com/mathematics/mathematics-08-01122/article_deploy/html/images/mathematics-08-01122-g001.png",
            "https://www.mdpi.com/mathematics/mathematics-08-01122/article_deploy/html/images/mathematics-08-01122-g002-550.jpg",
            "https://www.mdpi.com/mathematics/mathematics-08-01122/article_deploy/html/images/mathematics-08-01122-g002.png",
            "https://www.mdpi.com/mathematics/mathematics-08-01122/article_deploy/html/images/mathematics-08-01122-g003-550.jpg",
            "https://www.mdpi.com/mathematics/mathematics-08-01122/article_deploy/html/images/mathematics-08-01122-g003.png",
            "https://www.mdpi.com/img/table.png",
            "https://www.mdpi.com/img/table.png",
            "https://www.mdpi.com/img/table.png",
            "https://www.mdpi.com/img/table.png",
            "https://www.mdpi.com/img/table.png",
            "https://pub.mdpi-res.com/img/design/mdpi-pub-logo-white-small.png?71d18e5f805839ab?1723528173"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Monica Dessole",
            "Fabio Marcuzzi",
            "Marco Vianello"
        ],
        "publish_date": "2020-07-09T00:00:00",
        "summary": "",
        "meta_description": "We provide a numerical package for the computation of a d-variate near G-optimal polynomial regression design of degree m on a finite design space     X ‚äÇ  R d     , by few iterations of a basic multiplicative algorithm followed by Tchakaloff-like compression of the discrete measure keeping the reached G-efficiency, via an accelerated version of the Lawson-Hanson algorithm for Non-Negative Least Squares (NNLS) problems. This package can solve on a personal computer large-scale problems where     c a r d  ( X )  √ó dim  (  P  2 m  d  )      is up to     10 8    ‚Äì    10 9    , being     dim  (  P  2 m  d  )  =    2 m + d  d   =    2 m + d   2 m       . Several numerical tests are presented on complex shapes in     d = 3     and on hypercubes in     d > 3    .",
        "meta_lang": "en",
        "meta_favicon": "https://pub.mdpi-res.com/img/mask-icon-128.svg?c1c7eca266cd7013?1723528173",
        "meta_site_name": "MDPI",
        "canonical_link": "https://www.mdpi.com/2227-7390/8/7/1122",
        "text": "Department of Mathematics ‚ÄúTullio Levi Civita‚Äù, University of Padova, Via Trieste 63, 35131 Padova, Italy\n\n*\n\nAuthor to whom correspondence should be addressed.\n\nMathematics 2020, 8(7), 1122; https://doi.org/10.3390/math8071122\n\nSubmission received: 11 June 2020 / Revised: 5 July 2020 / Accepted: 7 July 2020 / Published: 9 July 2020\n\n(This article belongs to the Special Issue Numerical Methods)\n\nAbstract\n\n:\n\nWe provide a numerical package for the computation of a d-variate near G-optimal polynomial regression design of degree m on a finite design space X ‚äÇ R d , by few iterations of a basic multiplicative algorithm followed by Tchakaloff-like compression of the discrete measure keeping the reached G-efficiency, via an accelerated version of the Lawson-Hanson algorithm for Non-Negative Least Squares (NNLS) problems. This package can solve on a personal computer large-scale problems where c a r d ( X ) √ó dim ( P 2 m d ) is up to 10 8 ‚Äì 10 9 , being dim ( P 2 m d ) = 2 m + d d = 2 m + d 2 m . Several numerical tests are presented on complex shapes in d = 3 and on hypercubes in d > 3 .\n\n1. Introduction\n\nIn this paper we present the numerical software package dCATCH [1] for the computation of a d-variate near G-optimal polynomial regression design of degree m on a finite design space X ‚äÇ R d . In particular, it is the first software package for general-purpose Tchakaloff-like compression of d-variate designs via Non-Negative Least Squares (NNLS), freely available on the Internet. The code is an evolution of the codes in Reference [2] (limited to d = 2 , 3 ), with a number of features tailored to higher dimension and large-scale computations. The key ingredients are:\n\nuse of d-variate Vandermonde-like matrices at X in a discrete orthogonal polynomial basis (obtained by discrete orthonormalization of the total-degree product Chebyshev basis of the minimal box containing X), with automatic adaptation to the actual dimension of P m d ( X ) ;\n\nfew tens of iterations of the basic Titterington multiplicative algorithm until near G-optimality of the design is reached, with a checked G-efficiency of say 95 % (but with a design support still far from sparsity);\n\nTchakaloff-like compression of the resulting near G-optimal design via NNLS solution of the underdetermined moment system, with concentration of the discrete probability measure by sparse re-weighting to a support ‚äÇ X , of cardinality at most P 2 m d ( X ) , keeping the same G-efficiency;\n\niterative solution of the large-scale NNLS problem by a new accelerated version of the classical Lawson-Hanson active set algorithm, that we recently introduced in Reference [3] for 2 d and 3 d instances and here we validate on higher dimensions.\n\nBefore giving a more detailed description of the algorithm, it is worth recalling in brief some basic notions of optimal design theory. Such a theory has its roots and main applications within statistics, but also strong connections with approximation theory. In statistics, a design is a probability measure Œº supported on a (discrete or continuous) compact set Œ© ‚äÇ R d . The search for designs that optimize some properties of statistical estimators (optimal designs) dates back to at least one century ago, and the relevant literature is so wide and still actively growing and monographs and survey papers are abundant in the literature. For readers interested in the evolution and state of the art of this research field, we may quote, for example, two classical treatises such as in References [4,5], the recent monograph [6] and the algorithmic survey [7], as well as References [8,9,10] and references therein. On the approximation theory side we may quote, for example, References [11,12].\n\nThe present paper is organized as follows‚Äîin Section 2 we briefly recall some basic concepts from the theory of Optimal Designs, for the reader‚Äôs convenience, with special attention to the deterministic and approximation theoretic aspects. In Section 3 we present in detail our computational approach to near G-optimal d-variate designs via Caratheodory-Tchakaloff compression. All the routines of the dCATCH software package here presented, are described. In Section 4 we show several numerical results with dimensions in the range 3‚Äì10 and a Conclusions section follows.\n\nFor the reader‚Äôs convenience we also display Table 1 and Table 2, describing the acronyms used in this paper and the content (subroutine names) of the dCATCH software package.\n\n2. G-Optimal Designs\n\nLet P m d ( Œ© ) denote the space of d-variate real polynomials of total degree not greater than n, restricted to a (discrete or continuous) compact set Œ© ‚äÇ R d , and let Œº be a design, that is, a probability measure, with s u p p ( Œº ) ‚äÜ Œ© . In what follows we assume that s u p p ( Œº ) is determining for P m d ( Œ© ) [13], that is, polynomials in P m d vanishing on s u p p ( Œº ) vanish everywhere on Œ© .\n\nIn the theory of optimal designs, a key role is played by the diagonal of the reproducing kernel for Œº in P m d ( Œ© ) (also called the Christoffel polynomial of degree m for Œº )\n\nK m Œº ( x , x ) = ‚àë j = 1 N m p j 2 ( x ) , N m = dim ( P m d ( Œ© ) ) ,\n\n(1)\n\nwhere { p j } is any Œº -orthonormal basis of P m d ( Œ© ) . Recall that K m Œº ( x , x ) can be proved to be independent of the choice of the orthonormal basis. Indeed, a relevant property is the following estimate of the L ‚àû -norm in terms of the L Œº 2 -norm of polynomials\n\n‚à• p ‚à• L ‚àû ( Œ© ) ‚â§ max x ‚àà Œ© K m Œº ( x , x ) ‚à• p ‚à• L Œº 2 ( Œ© ) , ‚àÄ p ‚àà P m d ( Œ© ) .\n\n(2)\n\nNow, by (1) and Œº -orthonormality of the basis we get\n\n‚à´ Œ© K m Œº ( x , x ) d Œº = ‚àë j = 1 N m ‚à´ Œ© p j 2 ( x ) d Œº = N m ,\n\n(3)\n\nwhich entails that max x ‚àà Œ© K m Œº ( x , x ) ‚â• N m .\n\nThen, a probability measure Œº * = Œº * ( Œ© ) is then called a G-optimal design for polynomial regression of degree m on Œ© if\n\nmin Œº max x ‚àà Œ© K m Œº ( x , x ) = max x ‚àà Œ© K m Œº * ( x , x ) = N m .\n\n(4)\n\nObserve that, since ‚à´ Œ© K m Œº ( x , x ) d Œº = N m for every Œº , an optimal design has also the following property K m Œº * ( x , x ) = N m , Œº * -a.e. in Œ© .\n\nNow, the well-known Kiefer-Wolfowitz General Equivalence Theorem [14] (a cornerstone of optimal design theory), asserts that the difficult min-max problem (4) is equivalent to the much simpler maximization problem\n\nmax Œº d e t ( G m Œº ) , G m Œº = ‚à´ Œ© œï i ( x ) œï j ( x ) d Œº 1 ‚â§ i , j ‚â§ N m ,\n\nwhere G m Œº is the Gram matrix (or information matrix in statistics) of Œº in a fixed polynomial basis { œï i } of P m d ( Œ© ) . Such an optimality is called D-optimality, and ensures that an optimal measure always exists, since the set of Gram matrices of probability measures is compact and convex; see for example, References [5,12] for a general proof of these results, valid for continuous as well as for discrete compact sets.\n\nNotice that an optimal measure is neither unique nor necessarily discrete (unless Œ© is discrete itself). Nevertheless, the celebrated Tchakaloff Theorem ensures the existence of a positive quadrature formula for integration in d Œº * on Œ© , with cardinality not exceeding N 2 m = dim ( P 2 m d ( Œ© ) ) and which is exact for all polynomials in P 2 m d ( Œ© ) . Such a formula is then a design itself, and it generates the same orthogonal polynomials and hence the same Christoffel polynomial of Œº * , preserving G-optimality (see Reference [15] for a proof of Tchakaloff Theorem with general measures).\n\nWe recall that G-optimality has two important interpretations in terms of statistical and deterministic polynomial regression.\n\nFrom a statistical viewpoint, it is the probability measure on Œ© that minimizes the maximum prediction variance by polynomial regression of degree m, cf. for example, Reference [5].\n\nOn the other hand, from an approximation theory viewpoint, if we call L m Œº * the corresponding weighted least squares projection operator L ‚àû ( Œ© ) ‚Üí P m d ( Œ© ) , namely\n\n‚à• f ‚àí L m Œº * f ‚à• L Œº * 2 ( Œ© ) = min p ‚àà P m d ( Œ© ) ‚à• f ‚àí p ‚à• L Œº * 2 ( Œ© ) ,\n\n(5)\n\nby (2) we can write for every f ‚àà L ‚àû ( Œ© )\n\n‚à• L m Œº * f ‚à• L ‚àû ( Œ© ) ‚â§ max x ‚àà Œ© K m Œº * ( x , x ) ‚à• L m Œº * f ‚à• L Œº * 2 ( Œ© ) = N m ‚à• L m Œº * f ‚à• L Œº * 2 ( Œ© ) ‚â§ N m ‚à• f ‚à• L Œº * 2 ( Œ© ) ‚â§ N m ‚à• f ‚à• L ‚àû ( Œ© ) ,\n\n(where the second inequality comes from Œº * -orthogonality of the projection), which gives\n\n‚à• L m Œº * ‚à• = sup f ‚â† 0 ‚à• L m Œº * f ‚à• L ‚àû ( Œ© ) ‚à• f ‚à• L ‚àû ( Œ© ) ‚â§ N m ,\n\n(6)\n\nthat is a G-optimal measure minimizes (the estimate of) the weighted least squares uniform operator norm.\n\nWe stress that in this paper we are interested in the fully discrete case of a finite design space Œ© = X , so that any design Œº is identified by a set of positive weights (masses) summing up to 1 and integrals are weighted sums.\n\n3. Computing near G-Optimal Compressed Designs\n\nSince in the present context we have a finite design space Œ© = X = { x 1 , ‚ãØ , x M } ‚äÇ R d , we may think a design Œº as a vector of non-negative weights u = ( u 1 , ‚ãØ , u M ) attached to the points, such that ‚à• u ‚à• 1 = 1 (the support of Œº being identified by the positive weights). Then, a G-optimal (or D-optimal) design Œº * is represented by the corresponding non-negative vector u * . We write K m u ( x , x ) = K m Œº ( x , x ) for the Christoffel polynomial and similarly for other objects (spaces, operators, matrices) corresponding to a discrete design. At the same time, L ‚àû ( Œ© ) = ‚Ñì ‚àû ( X ) , and L Œº 2 ( Œ© ) = ‚Ñì u 2 ( X ) (a weighted ‚Ñì 2 functional space on X) with ‚à• f ‚à• ‚Ñì u 2 ( X ) = ‚àë i = 1 M u i f 2 ( x i ) 1 / 2 .\n\nIn order to compute an approximation of the desired u * , we resort to the basic multiplicative algorithm proposed by Titterington in the ‚Äô70s (cf. Reference [16]), namely\n\nu i ( k + 1 ) = u i ( k ) K m u ( k ) ( x i , x i ) N m , 1 ‚â§ i ‚â§ M , k = 0 , 1 , 2 , ‚ãØ ,\n\n(7)\n\nwith initialization u ( 0 ) = ( 1 / M , ‚ãØ , 1 / M ) T . Such an algorithm is known to be convergent sublinearly to a D-optimal (or G-optimal by the Kiefer-Wolfowitz Equivalence Theorem) design, with an increasing sequence of Gram determinants\n\nd e t ( G m u ( k ) ) = d e t ( V T d i a g ( u ( k ) ) V ) ,\n\nwhere V is a Vandermonde-like matrix in any fixed polynomial basis of P m d ( X ) ; cf., for example, References [7,10]. Observe that u ( k + 1 ) is indeed a vector of positive probability weights if such is u ( k ) . In fact, the Christoffel polynomial K m u ( k ) is positive on X, and calling Œº k the probability measure on X associated with the weights u ( k ) we get immediately ‚àë i u i ( k + 1 ) = 1 N m ‚àë i u i ( k ) K m u ( k ) ( x i , x i ) = 1 N m ‚à´ X K m u ( k ) ( x , x ) d Œº k = 1 by (3) in the discrete case Œ© = X .\n\nOur implementation of (7) is based on the functions\n\nC = dCHEBVAND ( n , X )\n\n[ U , j v e c ] = dORTHVAND ( n , X , u , j v e c )\n\n[ p t s , w ] = dNORD ( m , X , g t o l )\n\nThe function dCHEBVAND computes the d-variate Chebyshev-Vandermonde matrix C = ( œï j ( x i ) ) ‚àà R M √ó N n , where { œï j ( x ) } = { T ŒΩ 1 ( Œ± 1 x 1 + Œ≤ 1 ) ‚ãØ T ŒΩ d ( Œ± d x d + Œ≤ d ) } , 0 ‚â§ ŒΩ i ‚â§ n , ŒΩ 1 + ‚ãØ + ŒΩ d ‚â§ n , is a suitably ordered total-degree product Chebyshev basis of the minimal box [ a 1 , b 1 ] √ó ‚ãØ √ó [ a d , b d ] containing X, with Œ± i = 2 / ( b i ‚àí a i ) , Œ≤ i = ‚àí ( b i + a i ) / ( b i ‚àí a i ) . Here we have resorted to the codes in Reference [17] for the construction and enumeration of the required ‚Äúmonomial‚Äù degrees. Though the initial basis is then orthogonalized, the choice of the Chebyshev basis is dictated by the necessity of controlling the conditioning of the matrix. This would be on the contrary extremely large with the standard monomial basis, already at moderate regression degrees, preventing a successful orthogonalization.\n\nIndeed, the second function dORTHVAND computes a Vandermonde-like matrix in a u-orthogonal polynomial basis on X, where u is the probability weight array. This is accomplished essentially by numerical rank evaluation for C = dCHEBVAND ( n , X ) and QR factorization\n\nd i a g ( u ) C 0 = Q R , U = C 0 R ‚àí 1 ,\n\n(8)\n\n(with Q orthogonal rectangular and R square invertible), where u = ( u 1 , ‚ãØ , u M ) . The matrix C 0 has full rank and corresponds to a selection of the columns of C (i.e., of the original basis polynomials) via QR with column pivoting, in such a way that these form a basis of P n d ( X ) , since r a n k ( C ) = dim ( P n d ( X ) ) . A possible alternative, not yet implemented, is the direct use of a rank-revealing QR factorization. The in-out parameter ‚Äújvec‚Äù allows to pass directly the column index vector corresponding to a polynomial basis after a previous call to dORTHVAND with the same degree n, avoiding numerical rank computation and allowing a simple ‚Äúeconomy size‚Äù QR factorization of d i a g ( u ) C 0 = d i a g ( u ) C ( : , j v e c ) .\n\nSummarizing, U is a Vandermonde-like matrix for degree n on X in the required u-orthogonal basis of P n d ( X ) , that is\n\n[ p 1 ( x ) , ‚ãØ , p N n ( x ) ] = [ œï j 1 ( x ) , ‚ãØ , œï j N n ( x ) ] R ‚àí 1 ,\n\n(9)\n\nwhere j v e c = ( j 1 , ‚ãØ , j N n ) is the multi-index resulting from pivoting. Indeed by (8) we can write the scalar product ( p h , p k ) ‚Ñì u 2 ( X ) as\n\n( p h , p k ) ‚Ñì u 2 ( X ) = ‚àë i = 1 M u i p h ( x i ) p k ( x i ) = ( U T d i a g ( u ) U ) h k = ( Q T Q ) h k = Œ¥ h k ,\n\nfor 1 ‚â§ h , k ‚â§ N n , which shows orthonormality of the polynomial basis in (9).\n\nWe stress that r a n k ( C ) = dim ( P n d ( X ) ) could be strictly smaller than dim ( P n d ) = n + d d , when there are polynomials in P n d vanishing on X that do not vanish everywhere. In other words, X lies on a lower-dimensional algebraic variety (technically one says that X is not P n d -determining [13]). This certainly happens when c a r d ( X ) is too small, namely c a r d ( X ) < dim ( P n d ) , but think for example also to the case when d = 3 and X lies on the 2-sphere S 2 (independently of its cardinality), then we have dim ( P n d ( X ) ) ‚â§ dim ( P n d ( S 2 ) ) = ( n + 1 ) 2 < dim ( P n 3 ) = ( n + 1 ) ( n + 2 ) ( n + 3 ) / 6 .\n\nIteration (7) is implemented within the third function dNORD whose name stands for d-dimensional Near G-Optimal Regression Designs, which calls dORTHVAND with n = m . Near optimality is here twofold, namely it concerns both the concept of G-efficiency of the design and the sparsity of the design support.\n\nWe recall that G-efficiency is the percentage of G-optimality reached by a (discrete) design, measured by the ratio\n\nG m ( u ) = N m m a x x ‚àà X K m u ( x , x ) ,\n\nknowing that G m ( u ) ‚â§ 1 by (3) in the discrete case Œ© = X . Notice that G m ( u ) can be easily computed after the construction of the u-orthogonal Vandermonde-like matrix U by dORTHVAND, as G m ( u ) = N m / ( max i ‚à• r o w i ( U ) ‚à• 2 2 ) .\n\nIn the multiplicative algorithm (7), we then stop iterating when a given threshold of G-efficiency (the input parameter ‚Äúgtol‚Äù in the call to dNORD) is reached by u ( k ) , since G m ( u ( k ) ) ‚Üí 1 as k ‚Üí ‚àû , say for example G m ( u ( k ) ) ‚â• 95 % or G m ( u ( k ) ) ‚â• 99 % . Since convergence is sublinear and in practice we see that 1 ‚àí G m ( u ( k ) ) = O ( 1 / k ) , for a 90 % G-efficiency the number of iterations is typically in the tens, whereas it is in the hundreds for 99 % one and in the thousands for 99.9 % . When a G-efficiency very close to 1 is needed, one could resort to more sophisticated multiplicative algorithms, see for example, References [9,10].\n\nIn many applications however a G-efficiency of 90‚Äì 95 % could be sufficient (then we may speak of near G-optimality of the design), but though in principle the multiplicative algorithm converges to an optimal design Œº * on X with weights u * and cardinality N m ‚â§ c a r d ( s u p p ( Œº * ) ) ‚â§ N 2 m , such a sparsity is far from being reached after the iterations that guarantee near G-optimality, in the sense that there is a still large percentage of non-negligible weights in the near optimal design weight vector, say\n\nu ( k ¬Ø ) such that G m ( u ( k ¬Ø ) ) ‚â• g t o l .\n\n(10)\n\nFollowing References [18,19], we can however effectively compute a design which has the same G-efficiency of u ( k ¬Ø ) but a support with a cardinality not exceeding N 2 m = dim ( P 2 m d ( X ) ) , where in many applications N 2 m ‚â™ c a r d ( X ) , obtaining a remarkable compression of the near optimal design.\n\nThe theoretical foundation is a generalized version [15] of Tchakaloff Theorem [20] on positive quadratures, which asserts that for every measure on a compact set Œ© ‚äÇ R d there exists an algebraic quadrature formula exact on P n d ( Œ© ) ) , with positive weights, nodes in Œ© and cardinality not exceeding N n = dim ( P n d ( Œ© ) .\n\nIn the present discrete case, that is, where the designs are defined on Œ© = X , this theorem implies that for every design Œº on X there exists a design ŒΩ , whose support is a subset of X, which is exact for integration in d Œº on P n d ( X ) . In other words, the design ŒΩ has the same basis moments (indeed, for any basis of P n d ( Œ© ) )\n\n‚à´ X p j ( x ) d Œº = ‚àë i = 1 M u i p j ( x i ) = ‚à´ X p j ( x ) d ŒΩ = ‚àë ‚Ñì = 1 L w ‚Ñì p j ( Œæ ‚Ñì ) , 1 ‚â§ j ‚â§ N n ,\n\nwhere L ‚â§ N n ‚â§ M , { u i } are the weights of Œº , s u p p ( ŒΩ ) = { Œæ ‚Ñì } ‚äÜ X and { w ‚Ñì } are the positive weights of ŒΩ . For L < M , which certainly holds if N n < M , this represents a compression of the design Œº into the design ŒΩ , which is particularly useful when N n ‚â™ M .\n\nIn matrix terms this can be seen as the fact that the underdetermined { p j } -moment system\n\nU n T v = U n T u\n\n(11)\n\nhas a non-negative solution v = ( v 1 , ‚ãØ , v M ) T whose positive components, say w ‚Ñì = v i ‚Ñì , 1 ‚â§ ‚Ñì ‚â§ L ‚â§ N n , determine the support points { Œæ ‚Ñì } ‚äÜ X (for clarity we indicate here by U n the matrix U computed by dORTHVAND at degree n). This fact is indeed a consequence of the celebrated Caratheodory Theorem on conic combinations [21], asserting that a linear combination with non-negative coefficients of M vectors in R N with M > N can be re-written as linear positive combination of at most N of them. So, we get the discrete version of Tchakaloff Theorem by applying Caratheodory Theorem to the columns of U n T in the system (11), ensuring then existence of a non-negative solution v with at most N n nonzero components.\n\nIn order to compute such a solution to (11) we choose the strategy based on Quadratic Programming introduced in Reference [22], namely on sparse solution of the Non-Negative Least Squares (NNLS) problem\n\nv = argmin z ‚àà R M , z ‚â• 0 ‚à• U n T z ‚àí U n T u ‚à• 2 2\n\nby a new accelerated version of the classical Lawson-Hanson active-set method, proposed in Reference [3] in the framework of design optimization in d = 2 , 3 and implemented by the function LHDM (Lawson-Hanson with Deviation Maximization), that we tune in the present package for very large-scale d-variate problems (see the next subsection for a brief description and discussion). We observe that working with an orthogonal polynomial basis of P n d ( X ) allows to deal with the well-conditioned matrix U n in the Lawson-Hanson algorithm.\n\nThe overall computational procedure is implemented by the function\n\n[ p t s , w , m o m e r r ] = dCATCH ( n , X , u ) ,\n\nwhere dCATCH stands for d-variate CAratheodory-TCHakaloff discrete measure compression. It works for any discrete measure on a discrete set X. Indeed, it could be used, other than for design compression, also in the compression of d-variate quadrature formulas, to give an example. The output parameter p t s = { Œæ ‚Ñì } ‚äÇ X is the array of support points of the compressed measure, while w = { w ‚Ñì } = { v i ‚Ñì > 0 } is the corresponding positive weight array (that we may call a d-variate near G-optimal Tchakaloff design) and m o m e r r = ‚à• U n T v ‚àí U n T u ‚à• 2 is the moment residual. This function is called LHDM.\n\nIn the present framework we call dCATCH with n = 2 m and u = u ( k ¬Ø ) , cf. (10), that is, we solve\n\nv = argmin z ‚àà R M , z ‚â• 0 ‚à• U 2 m T z ‚àí U 2 m T u ( k ¬Ø ) ‚à• 2 2 .\n\n(12)\n\nIn such a way the compressed design generates the same scalar product of u ( k ¬Ø ) in P m d ( X ) , and hence the same orthogonal polynomials and the same Christoffel function on X keeping thus invariant the G-efficiency\n\nP 2 m d ( X ) ‚àã K m v ( x , x ) = K m u ( k ¬Ø ) ( x , x ) ‚àÄ x ‚àà X ‚üπ G m ( v ) = G m ( u ( k ¬Ø ) ) ‚â• g t o l\n\n(13)\n\nwith a (much) smaller support.\n\nFrom a deterministic regression viewpoint (approximation theory), let us denote by p m o p t the polynomial in P m d ( X ) of best uniform approximation for f on X, where we assume f ‚àà C ( D ) with X ‚äÇ D ‚äÇ R d , D being a compact domain (or even lower-dimensional manifold), and by E m ( f ; X ) = inf p ‚àà P m d ( X ) ‚à• f ‚àí p ‚à• ‚Ñì ‚àû ( X ) = ‚à• f ‚àí p m o p t | ‚Ñì ‚àû ( X ) and E m ( f ; D ) = inf p ‚àà P m d ( D ) ‚à• f ‚àí p ‚à• L ‚àû ( D ) the best uniform polynomial approximation errors on X and D.\n\nThen, denoting by L m u ( k ¬Ø ) and L m w f = L m v f the weighted least squares polynomial approximation of f (cf. (5)) by the near G-optimal weights u ( k ¬Ø ) and w, respectively, with the same reasoning used to obtain (6) and by (13) we can write the operator norm estimates\n\n‚à• L m u ( k ¬Ø ) ‚à• , ‚à• L m w ‚à• ‚â§ N Àú m ‚â§ N m g t o l , N Àú m = N m G m ( u ( k ¬Ø ) ) = N m G m ( v ) .\n\nMoreover, since L m w p = p for any p ‚àà P m d ( X ) , we can write the near optimal estimate\n\n‚à• f ‚àí L m w f ‚à• ‚Ñì ‚àû ( X ) ‚â§ ‚à• f ‚àí p m o p t ‚à• ‚Ñì ‚àû ( X ) + ‚à• p m o p t ‚àí L m w p m o p t ‚à• ‚Ñì ‚àû ( X ) + ‚à• L m w p m o p t ‚àí L m w f ‚à• ‚Ñì ‚àû ( X )\n\n= ‚à• f ‚àí p m o p t ‚à• ‚Ñì ‚àû ( X ) + ‚à• L m w p m o p t ‚àí L m w f ‚à• ‚Ñì ‚àû ( X ) ‚â§ ( 1 + ‚à• L m w ‚à• ) E m ( f ; X )\n\n‚â§ 1 + N m g t o l E m ( f ; X ) ‚â§ 1 + N m g t o l E m ( f ; D ) ‚âà 1 + N m E m ( f ; D ) .\n\nNotice that L m w f is constructed by sampling f only at the compressed support { Œæ ‚Ñì } ‚äÇ X . The error depends on the regularity of f on D ‚äÉ X , with a rate that can be estimated whenever D admits a multivariate Jackson-like inequality, cf. Reference [23].\n\nAccelerating the Lawson-Hanson Algorithm by Deviation Maximization (LHDM)\n\nLet A ‚àà R N √ó M and b ‚àà R N . The NNLS problem consists of seeking x ‚àà R M that solves\n\nx = argmin z ‚â• 0 ‚à• A z ‚àí b ‚à• 2 2 .\n\n(14)\n\nThis is a convex optimization problem with linear inequality constraints that define the feasible region, that is the positive orthant x ‚àà R M : x i ‚â• 0 . The very first algorithm dedicated to problem (14) is due to Lawson and Hanson [24] and it is still one of the most often used. It was originally derived for solving overdetermined linear systems, with N ‚â´ M . However, in the case of underdetermined linear systems, with N ‚â™ M , this method succeeds in sparse recovery.\n\nRecall that for a given point x in the feasible region, the index set 1 , ‚ãØ , M can be partitioned into two sets: the active set Z, containing the indices of active constraints x i = 0 , and the passive set P, containing the remaining indices of inactive constraints x i > 0 . Observe that an optimal solution x üüâ of (14) satisfies A x üüâ = b and, if we denote by P üüâ and Z üüâ the corresponding passive and active sets respectively, x üüâ also solves in a least square sense the following unconstrained least squares subproblem\n\nx P üüâ üüâ = argmin y ‚à• A P üüâ y ‚àí b ‚à• 2 2 ,\n\n(15)\n\nwhere A P üüâ is the submatrix containing the columns of A with index in P üüâ , and similarly x P üüâ üüâ is the subvector made of the entries of x üüâ whose index is in P üüâ . The remaining entries of x üüâ , namely those whose index is in Z üüâ , are null.\n\nThe Lawson-Hanson algorithm, starting from a null initial guess x = 0 (which is feasible), incrementally builds an optimal solution by moving indices from the active set Z to the passive set P and vice versa, while keeping the iterates within the feasible region. More precisely, at each iteration first order information is used to detect a column of the matrix A such that the corresponding entry in the new solution vector will be strictly positive; the index of such a column is moved from the active set Z to the passive set P. Since there‚Äôs no guarantee that the other entries corresponding to indices in the former passive set will stay positive, an inner loop ensures the new solution vector falls into the feasible region, by moving from the passive set P to the active set Z all those indices corresponding to violated constraints. At each iteration a new iterate is computed by solving a least squares problem of type (15): this can be done, for example, by computing a QR decomposition, which is substantially expensive. The algorithm terminates in a finite number of steps, since the possible combinations of passive/active set are finite and the sequence of objective function values is strictly decreasing, cf. Reference [24].\n\nThe deviation maximization (DM) technique is based on the idea of adding a whole set of indices T to the passive set at each outer iteration of the Lawson-Hanson algorithm. This corresponds to select a block of new columns to insert in the matrix A P , while keeping the current solution vector within the feasible region in such a way that sparse recovery is possible when dealing with non-strictly convex problems. In this way, the number of total iterations and the resulting computational cost decrease. The set T is initialized to the index chosen by the standard Lawson-Hanson (LH) algorithm, and it is then extended, within the same iteration, using a set of candidate indices C chosen is such a way that the corresponding entries are likely positive in the new iterate. The elements of T are then chosen carefully within C: note that if the columns corresponding to the chosen indices are linearly dependent, the submatrix of the least squares problem (15) will be rank deficient, leading to numerical difficulties. We add k new indices, where k is an integer parameter to tune on the problem size, in such a way that, at the end, for every pair of indices in the set T, the corresponding column vectors form an angle whose cosine in absolute value is below a given threshold t h r e s . The whole procedure is implemented in the function\n\n[ x , r e s n o r m , e x i t f l a g ] = LHDM ( A , b , o p t i o n s ) .\n\nThe input variable o p t i o n s is a structure containing the user parameters for the LHDM algorithm; for example, the aforementioned k and t h r e s . The output parameter x is the least squares solution, r e s n o r m is the squared 2-norm of the residual and e x i t f l a g is set to 0 if the LHDM algorithm has reached the maximum number of iterations without converging and 1 otherwise.\n\nIn the literature, an accelerating technique was introduced by Van Benthem and Keenan [25], who presented a different NNLS solution algorithm, namely ‚Äúfast combinatorial NNLS‚Äù, designed for the specific case of a large number of right-hand sides. The authors exploited a clever reorganization of computations in order to take advantage of the combinatorial nature of the problems treated (multivariate curve resolution) and introduced a nontrivial initialization of the algorithm by means of unconstrained least squares solution. In the following section we are going to compare such an approach, briefly named LHI, and the standard LH algorithm with the LHDM procedure just summarized.\n\n4. Numerical Examples\n\nIn this section, we perform several tests on the computation of d-variate near G-optimal Tchakaloff designs, from low to moderate dimension d. In practice, we are able to treat, on a personal computer, large-scale problems where c a r d ( X ) √ó dim ( P 2 m d ) is up to 10 8 ‚Äì 10 9 , with dim ( P 2 m d ) = 2 m + d d = 2 m + d 2 m . Recall that the main memory requirement is given by the N 2 m √ó M matrix U T in the compression process solved by the LHDM algorithm, where M = c a r d ( X ) and N 2 m = dim ( P 2 m d ( X ) ) ‚â§ dim ( P 2 m d ) .\n\nGiven the dimension d > 1 and the polynomial degree m, the routine LHDM empirically sets the parameter k as follows k = ‚åà 2 m + d d / ( m ( d ‚àí 1 ) ) ‚åâ , while the threshold is t h r e s = c o s ( œÄ 2 ‚àí Œ∏ ) , Œ∏ ‚âà 0.22 . All the tests are performed on a workstation with a 32 GB RAM and an Intel Core i7-8700 CPU @ 3.20 GHz.\n\n4.1. Complex 3d Shapes\n\nTo show the flexibility of the package dCATCH, we compute near G-optimal designs on a ‚Äúmultibubble‚Äù D ‚äÇ R 3 (i.e., the union of a finite number of non-disjoint balls), which can have a very complex shape with a boundary surface very difficult to describe analytically. Indeed, we are able to implement near optimal regression on quite complex solids, arising from finite union, intersection and set difference of simpler pieces, possibly multiply-connected, where for each piece we have available the indicator function via inequalities. Grid-points or low-discrepancy points, for example, Halton points, of a surrounding box, could be conveniently used to discretize the solid. Similarly, thanks to the adaptation of the method to the actual dimension of the polynomial spaces, we can treat near optimal regression on the surfaces of such complex solids, as soon as we are able to discretize the surface of each piece by point sets with good covering properties (for example, we could work on the surface of a multibubble by discretizing each sphere via one of the popular spherical point configurations, cf. Reference [26]).\n\nWe perform a test at regression degree m = 10 on the 5-bubble shown in Figure 1b. The initial support X consists in the M = 18,915 points within 64,000 low discrepancy Halton points, falling in the closure of the multibubble. Results are shown in Figure 1 and Table 3.\n\n4.2. Hypercubes: Chebyshev Grids\n\nIn a recent paper [19], a connection has been studied between the statistical notion of G-optimal design and the approximation theoretic notion of admissible mesh for multivariate polynomial approximation, deeply studied in the last decade after Reference [13] (see, e.g., References [27,28] with the references therein). In particular, it has been shown that near G-optimal designs on admissible meshes of suitable cardinality have a G-efficiency on the whole d-cube that can be made convergent to 1. For example, it has been proved by the notion of Dubiner distance and suitable multivariate polynomial inequalities, that a design with G-efficiency Œ≥ on a grid X of ( 2 k m ) d Chebyshev points (the zeros of T 2 k m ( t ) = c o s ( 2 k m arccos ( t ) ) , t ‚àà [ ‚àí 1 , 1 ] ), is a design for [ ‚àí 1 , 1 ] d with G-efficiency Œ≥ ( 1 ‚àí œÄ 2 / ( 8 k 2 ) ) . For example, taking k = 3 a near G-optimal Tchakaloff design with Œ≥ = 0.99 on a Chebyshev grid of ( 6 m ) d points is near G-optimal on [ ‚àí 1 , 1 ] d with G-efficiency approximately 0.99 ¬∑ 0.86 ‚âà 0.85 , and taking k = 4 (i.e., a Chebyshev grid of ( 8 m ) d points) the corresponding G-optimal Tchakaloff design has G-efficiency approximately 0.99 ¬∑ 0.92 ‚âà 0.91 on [ ‚àí 1 , 1 ] d (in any dimension d).\n\nWe perform three tests in different dimension spaces and at different regression degrees. Results are shown in Figure 2 and Table 4, using the same notation above.\n\n4.3. Hypercubes: Low-Discrepancy Points\n\nThe direct connection of Chebyshev grids with near G-optimal designs discussed in the previous subsection suffers rapidly of the curse of dimensionality, so only regression at low degree in relatively low dimension can be treated. On the other hand, in sampling theory a number of discretization nets with good space-filling properties on hypercubes has been proposed and they allow to increase the dimension d. We refer in particular to Latin hypercube sampling or low-discrepancy points (Sobol, Halton and other popular sequences); see for example, Reference [29]. These families of points give a discrete model of hypercubes that can be used in many different deterministic and statistical applications.\n\nHere we consider a discretization made via Halton points. We present in particular two examples, where we take as finite design space X a set of M = 10 5 Halton points, in d = 4 with regression degree m = 5 , and in d = 10 with m = 2 . In both examples, dim ( P 2 m d ) = 2 m + d d = 2 m + d 2 m = 14 4 = 1001 , so that the largest matrix involved in the construction is the 1001 √ó 100,000 Chebyshev-Vandermonde matrix C for degree 2 m on X constructed at the beginning of the compression process (by dORTHVAND within dCATCH to compute U 2 m in (12)).\n\nResults are shown in Figure 3 and Table 5, using the same notation as above.\n\nRemark 1.\n\nThe computational complexity of dCATCH mainly depends on the QR decompositions, which clearly limit the maximum size of the problem and mainly determine the execution time. Indeed, the computational complexity of a QR factorization of a matrix of size n r √ó n c , with n c ‚â§ n r , is high, namely 2 ( n c 2 n r ‚àí n c 3 / 3 ) ‚âà 2 n c 2 n r (see, e.g., Reference [30]).\n\nTitterington algorithm performs a QR factorization of a M √ó N m matrix at each iteration, with the following overall computational complexity\n\nC T i t t ‚âà 2 k ¬Ø M N m 2 ,\n\nwhere k ¬Ø is the number of iterations necessary for convergence, that depends on the desired G-efficiency.\n\nOn the other hand, the computational cost of one iteration of the Lawson-Hanson algorithm, fixed the passive set P, is given by the solution of an LS problem of type (15), which approximately is 2 N 2 m | P | 2 that is the cost of a QR decomposition of a matrix of size N 2 m √ó | P | . However, as experimental results confirm, the evolution of the set P along the execution of the algorithm may vary significantly depending on the experiment settings, so that the exact overall complexity is hard to estimate. Lower and upper bounds are available, but may lead to heavy under- and over-estimations, respectively; cf. Reference [31] for a discussion on complexity issues.\n\n5. Conclusions\n\nIn this paper, we have presented dCATCH [1], a numerical software package for the computation of a d-variate near G-optimal polynomial regression design of degree m on a finite design space X ‚äÇ R d . The mathematical foundation is discussed connecting statistical design theoretic and approximation theoretic aspects, with a special emphasis on deterministic regression (Weighted Least Squares). The package takes advantage of an accelerated version of the classical NNLS Lawson-Hanson solver developed by the authors and applied to design compression.\n\nAs a few examples of use cases of this package we have shown the results on a complex shape (multibubble) in three dimensions, and on hypercubes discretized with Chebyshev grids and with Halton points, testing different combinations of dimensions and degrees which generate large-scale problems for a personal computer.\n\nThe present package, dCATCH works for any discrete measure on a discrete set X. Indeed, it could be used, other than for design compression, also in the compression of d-variate quadrature formulas, even on lower-dimensional manifolds, to give an example.\n\nWe may observe that with this approach we can compute a d-variate compressed design starting from a high-cardinality sampling set X, that discretizes a continuous compact set (see Section 4.2 and Section 4.3). This design allows an m-th degree near optimal polynomial regression of a function on the whole X, by sampling on a small design support. We stress that the compressed design is function-independent and thus can be constructed ‚Äúonce and for all‚Äù in a pre-processing stage. This approach is potentially useful, for example, for the solution of d-variate parameter estimation problems, where we may think to model a nonlinear cost function by near optimal polynomial regression on a discrete d-variate parameter space X; cf., for example, References [32,33] for instances of parameter estimation problems from mechatronics applications (Digital Twins of controlled systems) and references on the subject. Minimization of the polynomial model could then be accomplished by popular methods developed in the growing research field of Polynomial Optimization, such as Lasserre‚Äôs SOS (Sum of Squares) and measure-based hierarchies, and other recent methods; cf., for example, References [34,35,36] with the references therein.\n\nFrom a computational viewpoint, the results shown in Table 3, Table 4 and Table 5 show relevant speed-ups in the compression stage, with respect to the standard Lawson-Hanson algorithm, in terms of the number of iterations required and of computing time within the Matlab scripting language. In order to further decrease the execution times and to allow us to tackle larger design problems, we would like in the near future to enrich the package dCATCH with an efficient C implementation of its algorithms and, possibly, a CUDA acceleration on GPUs.\n\nAuthor Contributions\n\nInvestigation, M.D., F.M. and M.V. All authors have read and agreed to the published version of the manuscript.\n\nFunding\n\nWork partially supported by the DOR funds and the biennial project Project BIRD192932 of the University of Padova, and by the GNCS-INdAM. This research has been accomplished within the RITA ‚ÄúResearch ITalian network on Approximation‚Äù.\n\nConflicts of Interest\n\nThe authors declare no conflict of interest.\n\nReferences\n\nDessole, M.; Marcuzzi, F.; Vianello, M. dCATCH: A Numerical Package for Compressed d-Variate Near G-Optimal Regression. Available online: https://www.math.unipd.it/~marcov/MVsoft.html (accessed on 1 June 2020).\n\nBos, L.; Vianello, M. CaTchDes: MATLAB codes for Caratheodory‚ÄîTchakaloff Near-Optimal Regression Designs. SoftwareX 2019, 10, 100349. [Google Scholar] [CrossRef]\n\nDessole, M.; Marcuzzi, F.; Vianello, M. Accelerating the Lawson-Hanson NNLS solver for large-scale Tchakaloff regression designs. Dolomit. Res. Notes Approx. DRNA 2020, 13, 20‚Äì29. [Google Scholar]\n\nAtkinson, A.; Donev, A.; Tobias, R. Optimum Experimental Designs, with SAS; Oxford University Press: Oxford, UK, 2007. [Google Scholar]\n\nPukelsheim, F. Optimal Design of Experiments; SIAM: Philadelphia, PA, USA, 2006. [Google Scholar]\n\nCelant, G.; Broniatowski, M. Interpolation and Extrapolation Optimal Designs 2-Finite Dimensional General Models; Wiley: Hoboken, NJ, USA, 2017. [Google Scholar]\n\nMandal, A.; Wong, W.K.; Yu, Y. Algorithmic searches for optimal designs. In Handbook of Design and Analysis of Experiments; CRC Press: Boca Raton, FL, USA, 2015; pp. 755‚Äì783. [Google Scholar]\n\nDe Castro, Y.; Gamboa, F.; Henrion, D.; Hess, R.; Lasserre, J.B. Approximate optimal designs for multivariate polynomial regression. Ann. Stat. 2019, 47, 127‚Äì155. [Google Scholar] [CrossRef] [Green Version]\n\nDette, H.; Pepelyshev, A.; Zhigljavsky, A. Improving updating rules in multiplicative algorithms for computing D-optimal designs. Comput. Stat. Data Anal. 2008, 53, 312‚Äì320. [Google Scholar] [CrossRef] [Green Version]\n\nTorsney, B.; Martin-Martin, R. Multiplicative algorithms for computing optimum designs. J. Stat. Plan. Infer. 2009, 139, 3947‚Äì3961. [Google Scholar] [CrossRef]\n\nBloom, T.; Bos, L.; Levenberg, N.; Waldron, S. On the Convergence of Optimal Measures. Constr. Approx. 2008, 32, 159‚Äì169. [Google Scholar] [CrossRef] [Green Version]\n\nBos, L. Some remarks on the Fej√©r problem for lagrange interpolation in several variables. J. Approx. Theory 1990, 60, 133‚Äì140. [Google Scholar] [CrossRef] [Green Version]\n\nCalvi, J.P.; Levenberg, N. Uniform approximation by discrete least squares polynomials. J. Approx. Theory 2008, 152, 82‚Äì100. [Google Scholar] [CrossRef]\n\nKiefer, J.; Wolfowitz, J. The equivalence of two extremum problems. Can. J. Math. 1960, 12, 363‚Äì366. [Google Scholar] [CrossRef]\n\nPutinar, M. A note on Tchakaloff‚Äôs theorem. Proc. Am. Math. Soc. 1997, 125, 2409‚Äì2414. [Google Scholar] [CrossRef]\n\nTitterington, D. Algorithms for computing D-optimal designs on a finite design space. In Proceedings of the 1976 Conference on Information Science and Systems; John Hopkins University: Baltimore, MD, USA, 1976; Volume 3, pp. 213‚Äì216. [Google Scholar]\n\nBurkardt, J. MONOMIAL: A Matlab Library for Multivariate Monomials. Available online: https://people.sc.fsu.edu/~jburkardt/m_src/monomial/monomial.html (accessed on 1 June 2020).\n\nBos, L.; Piazzon, F.; Vianello, M. Near optimal polynomial regression on norming meshes. In Sampling Theory and Applications 2019; IEEE Xplore Digital Library: New York, NY, USA, 2019. [Google Scholar]\n\nBos, L.; Piazzon, F.; Vianello, M. Near G-optimal Tchakaloff designs. Comput. Stat. 2020, 35, 803‚Äì819. [Google Scholar] [CrossRef]\n\nTchakaloff, V. Formules de cubatures m√©caniques √† coefficients non n√©gatifs. Bull. Sci. Math. 1957, 81, 123‚Äì134. [Google Scholar]\n\nCarath√©odory, C. √úber den Variabilit√§tsbereich der Fourier‚Äôschen Konstanten von positiven harmonischen Funktionen. Rendiconti Del Circolo Matematico di Palermo (1884‚Äì1940) 1911, 32, 193‚Äì217. [Google Scholar] [CrossRef] [Green Version]\n\nSommariva, A.; Vianello, M. Compression of Multivariate Discrete Measures and Applications. Numer. Funct. Anal. Optim. 2015, 36, 1198‚Äì1223. [Google Scholar] [CrossRef] [Green Version]\n\nPle≈õniak, W. Multivariate Jackson Inequality. J. Comput. Appl. Math. 2009, 233, 815‚Äì820. [Google Scholar] [CrossRef] [Green Version]\n\nLawson, C.L.; Hanson, R.J. Solving Least Squares Problems; SIAM: Philadelphia, PA, USA, 1995; Volume 15. [Google Scholar]\n\nVan Benthem, M.H.; Keenan, M.R. Fast algorithm for the solution of large-scale non-negativity-constrained least squares problems. J. Chemom. 2004, 18, 441‚Äì450. [Google Scholar] [CrossRef]\n\nHardin, D.; Michaels, T.; Saff, E. A Comparison of Popular Point Configurations on S2. Dolomit. Res. Notes Approx. DRNA 2016, 9, 16‚Äì49. [Google Scholar]\n\nBloom, T.; Bos, L.; Calvi, J.; Levenberg, N. Polynomial Interpolation and Approximation in C d . Ann. Polon. Math. 2012, 106, 53‚Äì81. [Google Scholar] [CrossRef] [Green Version]\n\nDe Marchi, S.; Piazzon, F.; Sommariva, A.; Vianello, M. Polynomial Meshes: Computation and Approximation. In Proceedings of the CMMSE 2015, Rota Cadiz, Spain, 6‚Äì10 July 2015; pp. 414‚Äì425. [Google Scholar]\n\nDick, J.; Pillichshammer, F. Digital Nets and Sequences-Discrepancy Theory and Quasi‚ÄîMonte Carlo Integration; Cambridge University Press: Cambridge, UK, 2010. [Google Scholar]\n\nGolub, G.H.; Van Loan, C.F. Matrix Computations, 3rd ed.; Johns Hopkins University Press: Baltimore, MD, USA, 1996. [Google Scholar]\n\nSlawski, M. Nonnegative Least Squares: Comparison of Algorithms. Available online: https://sites.google.com/site/slawskimartin/code (accessed on 1 June 2020).\n\nBeghi, A.; Marcuzzi, F.; Martin, P.; Tinazzi, F.; Zigliotto, M. Virtual prototyping of embedded control software in mechatronic systems: A case study. Mechatronics 2017, 43, 99‚Äì111. [Google Scholar] [CrossRef]\n\nBeghi, A.; Marcuzzi, F.; Rampazzo, M. A Virtual Laboratory for the Prototyping of Cyber-Physical Systems. IFAC-PapersOnLine 2016, 49, 63‚Äì68. [Google Scholar] [CrossRef]\n\nLasserre, J.B. The moment-SOS hierarchy. Proc. Int. Cong. Math. 2018, 4, 3791‚Äì3814. [Google Scholar]\n\nDe Klerk, E.; Laurent, M. A survey of semidefinite programming approaches to the generalized problem of moments and their error analysis. In World Women in Mathematics 2018-Association for Women in Mathematics Series; Springer: Cham, Switzerland, 2019; Volume 20, pp. 17‚Äì56. [Google Scholar]\n\nMartinez, A.; Piazzon, F.; Sommariva, A.; Vianello, M. Quadrature-based polynomial optimization. Optim. Lett. 2020, 35, 803‚Äì819. [Google Scholar] [CrossRef]\n\nFigure 1. Multibubble test case, regression degree m = 10 . (a) The evolution of the cardinality of the passive set P along the iterations of the three LH algorithms. (b) Multibubble with 1763 compressed Tchakaloff points, extracted from 18,915 original points.\n\nFigure 2. The evolution of the cardinality of the passive set P along the iterations of the three LH algorithms for Chebyshev nodes‚Äô tests.\n\nFigure 3. The evolution of the cardinality of the passive set P along the iterations of the three LH algorithms for Halton points‚Äô tests.\n\nTable 1. List of acronyms.\n\nLSLeast SquaresNNLSNon-Negative Least SquaresLHLawson-Hawson algorithm for NNLSLHILawson-Hawson algorithm with unconstrained LS InitializationLHDMLawson-Hawson algorithm with Deviation Maximization acceleration\n\nTable 2. dCATCH package content.\n\ndCATCHd-variate CAratheodory-TCHakaloff discrete measure compressiondCHEBVANDd-variate Chebyshev-Vandermonde matrixdORTHVANDd-variate Vandermonde-like matrix in a weighted orthogonal polynomial basisdNORDd-variate Near G-Optimal Regression DesignsLHDMLawson-Hawson algorithm with Deviation Maximization acceleration\n\nTable 3. Results for the multibubble numerical test: c o m p r = M / m e a n ( c p t s ) is the mean compression ratio obtained by the three methods listed; t L H / t T i t t is the ratio between the execution time of LH and that of the Titterington algorithm; t L H / t L H D M ( t L H I / t L H D M ) is the ratio between the execution time of LH (LHI) and that of LHDM; c p t s is the number of compressed Tchakaloff points and m o m e r r is the final moment residual.\n\nTestLHLHILHDMmMcompr t LH / t Titt t LH / t LHDM cptsmomerr t LHI / t LHDM cptsmomerrcptsmomerr1018,91511/140.0/12.7/11755 3.4 √ó 10 ‚àí 8 3.2/11758 3.2 √ó 10 ‚àí 8 1755 1.5 √ó 10 ‚àí 8\n\nTable 4. Results of numerical tests on M = ( 2 k m ) d Chebyshev‚Äôs nodes, with k = 4 , with different dimensions and degrees: c o m p r = M / m e a n ( c p t s ) is the mean compression ratio obtained by the three methods listed; t L H / t T i t t is the ratio between the execution time of LH and that of Titterington algorithm; t L H / t L H D M ( t L H I / t L H D M ) is the ratio between the execution time of LH (LHI) and that of LHDM; c p t s is the number of compressed Tchakaloff points and m o m e r r is the final moment residual.\n\nTestLHLHILHDMdmMcompr t LH / t Titt t LH / t LHDM cptsmomerr t LHI / t LHDM cptsmomerrcptsmomerr36110,592250/10.4/13.1/1450 5.0 √ó 10 ‚àí 7 3.5/1450 3.4 √ó 10 ‚àí 7 450 1.4 √ó 10 ‚àí 7 43331,7761607/10.2/12.0/1207 8.9 √ó 10 ‚àí 7 3.4/1205 9.8 √ó 10 ‚àí 7 207 7.9 √ó 10 ‚àí 7 521,048,5768571/10.1/11.4/1122 6.3 √ó 10 ‚àí 7 1.5/1123 3.6 √ó 10 ‚àí 7 122 3.3 √ó 10 ‚àí 7\n\nTable 5. Results of numerical tests on Halton points: c o m p r = M / m e a n ( c p t s ) is the mean compression ratio obtained by the three methods listed; t L H / t T i t t is the ratio between the execution time of LH and that of Titterington algorithm; t L H / t L H D M ( t L H I / t L H D M ) is the ratio between the execution time of LH (LHI) and that of LHDM; c p t s is the number of compressed Tchakaloff points and m o m e r r is the final moment residual.\n\nTestLHLHILHDMdmMcompr t LH / t Titt t LH / t LHDM cptsmomerr t LHI / t LHDM cptsmomerrcptsmomerr10210,00010/141.0/11.9/19901.1 √ó 10 ‚àí 8 1.9/19889.8 √ó 10 ‚àí 9 9909.4 √ó 10 ‚àí 9 102100,000103/16.0/13.1/19683.6 √ó 10 ‚àí 7 2.8/19732.7 √ó 10 ‚àí 7 9684.2 √ó 10 ‚àí 7 4510,00010/120.2/12.3/19979.7 √ó 10 ‚àí 9 2.4/19931.3 √ó 10 ‚àí 8 9972.1 √ó 10 ‚àí 9 45100,000103/12.0/13.8/19696.6 √ó 10 ‚àí 7 3.8/19646.3 √ó 10 ‚àí 7 9695.3 √ó 10 ‚àí 7\n\n¬© 2020 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (http://creativecommons.org/licenses/by/4.0/).\n\nShare and Cite\n\nMDPI and ACS Style\n\nDessole, M.; Marcuzzi, F.; Vianello, M. dCATCH‚ÄîA Numerical Package for d-Variate near G-Optimal Tchakaloff Regression via Fast NNLS. Mathematics 2020, 8, 1122. https://doi.org/10.3390/math8071122\n\nAMA Style\n\nDessole M, Marcuzzi F, Vianello M. dCATCH‚ÄîA Numerical Package for d-Variate near G-Optimal Tchakaloff Regression via Fast NNLS. Mathematics. 2020; 8(7):1122. https://doi.org/10.3390/math8071122\n\nChicago/Turabian Style\n\nDessole, Monica, Fabio Marcuzzi, and Marco Vianello. 2020. \"dCATCH‚ÄîA Numerical Package for d-Variate near G-Optimal Tchakaloff Regression via Fast NNLS\" Mathematics 8, no. 7: 1122. https://doi.org/10.3390/math8071122\n\nNote that from the first issue of 2016, this journal uses article numbers instead of page numbers. See further details here.\n\nArticle Metrics\n\nNo\n\nNo\n\nArticle Access Statistics\n\nFor more information on the journal statistics, click here.\n\nMultiple requests from the same IP address are counted as one view."
    }
}
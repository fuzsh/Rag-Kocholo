{
    "id": "dbpedia_2450_1",
    "rank": 83,
    "data": {
        "url": "https://cdso.utexas.edu/mscs",
        "read_more_link": "",
        "language": "en",
        "title": "Computer & Data Science Online",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://cdso.utexas.edu/themes/custom/utcs_bs_sass/images/icons/search-icon-gray.svg",
            "https://cdso.utexas.edu/themes/custom/utcs_bs_sass/images/icons/search-icon-black.svg",
            "https://cdso.utexas.edu/themes/custom/utcs_bs_sass/images/icons/large-close.svg",
            "https://cdso.utexas.edu/themes/custom/utcs_bs_sass/images/cdso-logo.svg",
            "https://cdso.utexas.edu/themes/custom/utcs_bs_sass/images/icons/large-close.svg",
            "https://cdso.utexas.edu/themes/custom/utcs_bs_sass/images/icons/play-button.svg",
            "https://cdso.utexas.edu/themes/custom/utcs_bs_sass/images/icons/close-x-white.svg",
            "https://cdso.utexas.edu/themes/custom/utcs_bs_sass/images/bar-graph.svg",
            "https://cdso.utexas.edu/themes/custom/utcs_bs_sass/images/bulb.svg",
            "https://cdso.utexas.edu/themes/custom/utcs_bs_sass/images/clock.svg",
            "https://cdso.utexas.edu/themes/custom/utcs_bs_sass/images/required-courses-icon.svg",
            "https://cdso.utexas.edu/themes/custom/utcs_bs_sass/images/electives-icon.svg",
            "https://cdso.utexas.edu/themes/custom/utcs_bs_sass/images/omcs-icon-courses.png",
            "https://cdso.utexas.edu/sites/default/files/course-icons/Logo_Deep%20Learning_0.svg",
            "https://cdso.utexas.edu/sites/default/files/faculty-image/kraehenbuehl_phillip.png",
            "https://cdso.utexas.edu/sites/default/files/course-icons/Logo_Machine%20Learning.svg",
            "https://cdso.utexas.edu/sites/default/files/faculty-image/adam-klivans.png",
            "https://cdso.utexas.edu/sites/default/files/faculty-image/liu_qiang.jpeg",
            "https://cdso.utexas.edu/sites/default/files/course-icons/Logo_Natural%20Language%20Processing.svg",
            "https://cdso.utexas.edu/sites/default/files/faculty-image/greg-durrett.png",
            "https://cdso.utexas.edu/sites/default/files/course-icons/Logo_Reinforcement%20Learning.svg",
            "https://cdso.utexas.edu/sites/default/files/faculty-image/peter-stone.png",
            "https://cdso.utexas.edu/sites/default/files/faculty-image/scott-niekum.png",
            "https://cdso.utexas.edu/sites/default/files/course-icons/Logo_Advanced%20Linear%20Algebra.svg",
            "https://cdso.utexas.edu/sites/default/files/faculty-image/maggie-myers.png",
            "https://cdso.utexas.edu/sites/default/files/faculty-image/robert-van-de-geijn.png",
            "https://cdso.utexas.edu/sites/default/files/course-icons/Logo_Algorithms.svg",
            "https://cdso.utexas.edu/sites/default/files/faculty-image/vijaya-ramachandran.png",
            "https://cdso.utexas.edu/sites/default/files/faculty-image/greg-plaxton.png",
            "https://cdso.utexas.edu/sites/default/files/course-icons/Logo_AutomatedLogicReasoning.svg",
            "https://cdso.utexas.edu/sites/default/files/faculty-image/isil-dillig.png",
            "https://cdso.utexas.edu/sites/default/files/course-icons/Logo_Quantum.svg",
            "https://cdso.utexas.edu/sites/default/files/faculty-image/scott-aaronson.png",
            "https://cdso.utexas.edu/sites/default/files/course-icons/Logo_Online%20Learning.svg",
            "https://cdso.utexas.edu/sites/default/files/faculty-image/Caramanis-Constantine.png",
            "https://cdso.utexas.edu/sites/default/files/faculty-image/sanjay-shakkottai.png",
            "https://cdso.utexas.edu/sites/default/files/course-icons/Logo_Optimization_0.svg",
            "https://cdso.utexas.edu/sites/default/files/faculty-image/sujay-sanghavi.png",
            "https://cdso.utexas.edu/sites/default/files/faculty-image/Caramanis-Constantine.png",
            "https://cdso.utexas.edu/sites/default/files/course-icons/Logo_Operating%20Systems.svg",
            "https://cdso.utexas.edu/sites/default/files/faculty-image/vijay-chidambaram.png",
            "https://cdso.utexas.edu/sites/default/files/course-icons/Logo_Android%20Programming.svg",
            "https://cdso.utexas.edu/sites/default/files/faculty-image/emmett-witchel.png",
            "https://cdso.utexas.edu/sites/default/files/course-icons/Logo_Structure-Implementation-Modern-Programming-Languages.svg",
            "https://cdso.utexas.edu/sites/default/files/faculty-image/siddhartha-chatterjee.png",
            "https://cdso.utexas.edu/sites/default/files/course-icons/Logo_Parallel%20Systems.svg",
            "https://cdso.utexas.edu/sites/default/files/faculty-image/calvin-lin.png",
            "https://cdso.utexas.edu/sites/default/files/faculty-image/christopher-rossbach.png",
            "https://cdso.utexas.edu/sites/default/files/course-icons/Logo_Virtualization.svg",
            "https://cdso.utexas.edu/sites/default/files/faculty-image/vijay-chidambaram.png",
            "https://cdso.utexas.edu/sites/default/files/course-icons/Logo-CaseStudiesMachineLearning.svg",
            "https://cdso.utexas.edu/sites/default/files/faculty-image/junfeng-jiao.png",
            "https://cdso.utexas.edu/sites/default/files/course-icons/Logo_PlanningSearchReasingUnderUncertainty.svg",
            "https://cdso.utexas.edu/sites/default/files/faculty-image/joydeep-biswas.png",
            "https://cdso.utexas.edu/themes/custom/utcs_bs_sass/images/department-computer-science-2.svg",
            "https://cdso.utexas.edu/themes/custom/utcs_bs_sass/images/department-statistics-data-science.svg"
        ],
        "movies": [
            "/sites/default/files/2023-03/mscs-video-hero.mp4",
            "https://www.youtube.com/embed/FnMc80b3_So?color=bf5700&autoplay=false;?rel=0&showinfo=0"
        ],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "Unlock your potential with UT Austin's online Master's in Computer Science program. Flexible, convenient, and prestigious. Apply now and advance your career!",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": "https://cdso.utexas.edu/mscs",
        "text": "Master’s of Computer Science\n\nPlay Full Video\n\nGain the Knowledge You Need to Accelerate Your Career\n\nComputer science is driving innovation in technology, finance, healthcare and beyond. The UT Austin online master’s degree in computer science gives you the skills to design, develop, and optimize the technologies we use to create, communicate and serve.\n\nGain Critical CS Skills to Meet Industry Demand\n\nEarn Your Degree From a\n\nTop-Ranked CS School1\n\nAffordable, Advanced Degree\n\nPriced at $10,000+ Fees2\n\nCurriculum\n\nThe UT Austin online CS master’s curriculum incorporates cutting-edge foundational coursework in computer science to help you develop a strong understanding of the field that can be directly applied to your career.\n\nOur elective courses focus on content that is in high demand within the tech industry, including advanced operating systems, programming languages, online learning, optimization and machine learning. A variety of electives allow you to personalize your education.\n\nTaught by tenured UT Computer Science faculty — many of whom are award-winning leaders in the CS research community — the MSCS program offers rigorous training to expand your expertise, and elective options tailored to your interests.\n\nCourses\n\nthree required courses\n\n+\n\nseven elective courses\n\n=\n\nTen Courses\n\nApplications Courses\n\nDeep Learning\n\nThis class covers advanced topics in deep learning, ranging from optimization to computer vision, computer graphics and unsupervised feature learning, and touches on deep language models, as well as deep learning for games.\n\nPart 1 covers the basic building blocks and intuitions behind designing, training, tuning, and monitoring of deep networks. The class covers both the theory of deep learning, as well as hands-on implementation sessions in pytorch. In the homework assignments, we will develop a vision system for a racing simulator, SuperTuxKart, from scratch.\n\nPart 2 covers a series of application areas of deep networks in: computer vision, sequence modeling in natural language processing, deep reinforcement learning, generative modeling, and adversarial learning. In the homework assignments, we develop a vision system and racing agent for a racing simulator, SuperTuxKart, from scratch.\n\nWhat You Will Learn\n\nAbout the inner workings of deep networks and computer vision models\n\nHow to design, train and debug deep networks in pytorch\n\nHow to design and understand sequence\n\nHow to use deep networks to control a simple sensory motor agent\n\nSyllabus\n\nBackground\n\nFirst Example\n\nDeep Networks\n\nConvolutional Networks\n\nMaking it Work\n\nComputer Vision\n\nSequence Modeling\n\nReinforcement Learning\n\nSpecial Topics\n\nSummary\n\nPhilipp Krähenbühl\n\nAssistant Professor, Computer Science\n\nMachine Learning\n\nThis course focuses on core algorithmic and statistical concepts in machine learning.\n\nTools from machine learning are now ubiquitous in the sciences with applications in engineering, computer vision, and biology, among others. This class introduces the fundamental mathematical models, algorithms, and statistical tools needed to perform core tasks in machine learning. Applications of these ideas are illustrated using programming examples on various data sets.\n\nTopics include pattern recognition, PAC learning, overfitting, decision trees, classification, linear regression, logistic regression, gradient descent, feature projection, dimensionality reduction, maximum likelihood, Bayesian methods, and neural networks.\n\nWhat You Will Learn\n\nTechniques for supervised learning including classification and regression\n\nAlgorithms for unsupervised learning including feature extraction\n\nStatistical methods for interpreting models generated by learning algorithms\n\nSyllabus\n\nMistake Bounded Learning (1 week)\n\nDecision Trees; PAC Learning (1 week)\n\nCross Validation; VC Dimension; Perceptron (1 week)\n\nLinear Regression; Gradient Descent (1 week)\n\nBoosting (.5 week)\n\nPCA; SVD (1.5 weeks)\n\nMaximum likelihood estimation (1 week)\n\nBayesian inference (1 week)\n\nK-means and EM (1-1.5 week)\n\nMultivariate models and graphical models (1-1.5 week)\n\nNeural networks; generative adversarial networks (GAN) (1-1.5 weeks)\n\nAdam Klivans\n\nProfessor, Computer Science\n\nQiang Liu\n\nAssistant Professor, Computer Science\n\nNatural Language Processing\n\nThis course focuses on modern natural language processing using statistical methods and deep learning. Problems addressed include syntactic and semantic analysis of text as well as applications such as sentiment analysis, question answering, and machine translation. Machine learning concepts covered include binary and multiclass classification, sequence tagging, feedforward, recurrent, and self-attentive neural networks, and pre-training / transfer learning.\n\nWhat You Will Learn\n\nLinguistics fundamentals: syntax, lexical and distributional semantics, compositional semantics\n\nMachine learning models for NLP: classifiers, sequence taggers, deep learning models\n\nKnowledge of how to apply ML techniques to real NLP tasks\n\nSyllabus\n\nML fundamentals, linear classification, sentiment analysis (1.5 weeks)\n\nNeural classification and word embeddings (1 week)\n\nRNNs, language modeling, and pre-training basics (1 week)\n\nTagging with sequence models: Hidden Markov Models and Conditional Random Fields (1 week)\n\nSyntactic parsing: constituency and dependency parsing, models, and inference (1.5 weeks)\n\nLanguage modeling revisited (1 week)\n\nQuestion answering and semantics (1.5 weeks)\n\nMachine translation (1.5 weeks)\n\nBERT and modern pre-training (1 week)\n\nApplications: summarization, dialogue, etc. (1-1.5 weeks)\n\nGreg Durrett\n\nAssistant Professor, Computer Science\n\nReinforcement Learning\n\nThis course introduces the theory and practice of modern reinforcement learning. Reinforcement learning problems involve learning what to do—how to map situations to actions—so as to maximize a numerical reward signal. The course will cover model-free and model-based reinforcement learning methods, especially those based on temporal difference learning and policy gradient algorithms. Introduces the theory and practice of modern reinforcement learning. Reinforcement learning problems involve learning what to do—how to map situations to actions—so as to maximize a numerical reward signal. The course will cover model-free and model-based reinforcement learning methods, especially those based on temporal difference learning and policy gradient algorithms. It covers the essentials of reinforcement learning (RL) theory and how to apply it to real-world sequential decision problems. Reinforcement learning is an essential part of fields ranging from modern robotics to game-playing (e.g. Poker, Go, and Starcraft). The material covered in this class will provide an understanding of the core fundamentals of reinforcement learning, preparing students to apply it to problems of their choosing, as well as allowing them to understand modern RL research. Professors Peter Stone and Scott Niekum are active reinforcement learning researchers and bring their expertise and excitement for RL to the class.\n\nWhat You Will Learn\n\nFundamental reinforcement learning theory and how to apply it to real-world problems\n\nTechniques for evaluating policies and learning optimal policies in sequential decision problems\n\nThe differences and tradeoffs between value function, policy search, and actor-critic methods in reinforcement learning\n\nWhen and how to apply model-based vs. model-free learning methods\n\nApproaches for balancing exploration and exploitation during learning\n\nHow to learn from both on-policy and off-policy data\n\nSyllabus\n\nMulti-Armed Bandits\n\nFinite Markov Decision Processes\n\nDynamic Programming\n\nMonte Carlo Methods\n\nTemporal-Difference Learning\n\nn-step Bootstrapping\n\nPlanning and Learning\n\nOn-Policy Prediction with Approximation\n\nOn-Policy Control with Approximation\n\nOff-Policy Methods with Approximation\n\nEligibility Traces\n\nPolicy Gradient Methods\n\nPeter Stone\n\nProfessor, Computer Science\n\nScott Niekum\n\nAdjunct Assistant Professor, Computer Science\n\nTheory Courses\n\nAdvanced Linear Algebra for Computing\n\nLinear algebra is one of the fundamental tools for computational and data scientists. In Advanced Linear Algebra for Computing, you build your knowledge, understanding, and skills in linear algebra, practical algorithms for matrix computations, and analyzing the effects on correctness of floating-point arithmetic as performed by computers.\n\nWhat You Will Learn\n\nDeciphering a matrix using the Singular Value Decomposition\n\nQuantifying and qualifying numerical error\n\nSolving linear systems and linear least-squares problems\n\nComputing and employing eigenvalues and eigenvectors\n\nSyllabus\n\nNorms (1 week)\n\nThe Singular Value Decomposition (1 week)\n\nThe QR Decomposition (1 week)\n\nLinear Last Squares (1 week)\n\nLU Factorization (1 week)\n\nNumerical Stability (1 week)\n\nSolving Sparse Linear Systems Part 1 (1 week)\n\nSolving Sparse Linear Systems Part 2 (1 week)\n\nEigenvalues and eigenvectors (1 week)\n\nPractical Solutions of the Hermitian Eigenvalue Problem (1 week)\n\nThe QR Algorithm Symmetric (1 week)\n\nHigh Performing Algorithms (1 week)\n\nMaggie Myers\n\nLecturer, Computer Science\n\nRobert van de Geijn\n\nProfessor, Computer Science\n\nAlgorithms\n\nModern computational applications often involve massive data sets. In this setting, it is crucial to employ asymptotically efficient algorithms. This course presents techniques for the design and analysis of polynomial-time algorithms. Unfortunately, many optimization problems that arise in practice are unlikely to be polynomial-time solvable. This course presents techniques for establishing evidence of such computational intractability, especially NP-hardness. Even if a given optimization problem is NP-hard, it may be possible to compute near-optimal solutions efficiently. This course presents techniques for the design and analysis of efficient approximation algorithms.\n\nTopics include growth of functions, divide-and-conquer algorithms, dynamic programming, greedy algorithms, basic graph algorithms, network flow, minimum-cost matching, linear program-ming, randomized algorithms, data structures (hashing, amortized analysis, splay trees, union-find, and Fibonacci heaps), online algorithms for paging, P, NP, NP-completeness, and approximation algorithms.\n\nWhat You Will Learn\n\nTechniques for the design and analysis of efficient algorithms\n\nTechniques for establishing evidence of computational intractability\n\nTechniques for coping with computational intractability\n\nSyllabus\n\nGrowth of functions, divide-and-conquer algorithms (1 week)\n\nDynamic programming (0.5 weeks)\n\nGreedy algorithms (1 week)\n\nBasic graph algorithms (0.5 weeks)\n\nNetwork flow algorithms (1.5 weeks)\n\nMinimum-cost matching (0.5 weeks)\n\nLinear programming (0.5 weeks)\n\nRandomized algorithms (0.5 weeks)\n\nHashing (1 week)\n\nAmortized analysis, splay trees, union-find, Fibonacci heaps (1.5 weeks)\n\nOnline algorithms (0.5 weeks)\n\nP, NP, NP-completeness (1.5 weeks)\n\nApproximation algorithms (1.5 weeks)\n\nVijaya Ramachandran\n\nProfessor, Computer Science\n\nGreg Plaxton\n\nProfessor, Computer Science\n\nAutomated Logical Reasoning\n\nThis is a course on computational logic and its applications in computer science, particularly in the context of software verification. Computational logic is a fundamental part of many areas of computer science, including artificial intelligence and programming languages. This class introduces the fundamentals of computational logic and investigates its many applications in computer science. Specifically, the course covers a variety of widely used logical theories and looks at algorithms for determining satisfiability in these logics as well as their applications.\n\nSyllabus\n\nNormal forms; decision procedures for propositional logic; SAT solvers (2 weeks)\n\nApplications of SAT solvers and binary decision diagrams (1 week)\n\nSemantics of to first-order logic and theoretical properties (1 weeks)\n\nFirst-order theorem proving (1.5 weeks)\n\nIntro to first-order theories (0.5 week)\n\nTheory of equality (0.5 week)\n\nDecision procedures for rationals and integers (1.5 weeks)\n\nDPLL(T) framework and SMT solvers (1 week)\n\nBasics of software verification (1 week)\n\nAutomating software verification (2 weeks)\n\nIşıl Dillig\n\nProfessor, Computer Science\n\nIntroduction to Quantum Information Science\n\nThis course is a first introduction to the theory of quantum computing and information. It covers the rules of quantum mechanics (qubits, unitary transformations, density matrices); quantum gates and circuits; entanglement; the Bell inequality; protocols for teleportation, quantum key distribution, quantum money and other tasks; basic quantum algorithms such as Shor’s and Grover’s; basic quantum complexity theory; and the challenges of building scalable quantum computers. Previous exposure to quantum mechanics is not required. The prerequisites are linear algebra and some exposure to classical algorithms and programming. The course is mainly theoretical, although it includes limited use of IBM Q Experience to design quantum circuits and run them on real quantum hardware.\n\nWhat You Will Learn\n\nThe fundamental concepts of quantum information and computation\n\nHow basic quantum algorithms and protocols work\n\nInformed view of the capabilities and limitations of quantum computers, and the challenges in building them\n\nSyllabus\n\nThe Church-Turing Thesis and Classical Probability Theory\n\nThe Basic Rules of Quantum Mechanics\n\nQuantum Gates and Circuits\n\nThe Zeno Effect and Elitzur-Vaidman Bomb\n\nMulti-Qubit States and Entanglement\n\nMixed States and the Bloch Sphere\n\nThe No-Cloning Theorem, Wiesner’s Quantum Money, and BB84 Quantum Key Distribution\n\nSuperdense Coding and Quantum Teleportation\n\nEntanglement Swapping, the GHZ State, and Monogamy of Entanglement\n\nBell’s Inequality\n\nInterpretations of Quantum Mechanics\n\nUniversal Gate Sets\n\nQuantum Query Complexity\n\nDeutsch-Jozsa Algorithm\n\nBernstein-Vazirani Algorithm\n\nSimon’s Algorithm\n\nShor’s Algorithm (Quantum Fourier Transform, Continued Fractions...)\n\nGrover’s Algorithm and Applications\n\nQuantum Error-Correcting Codes\n\nExperimental Realizations of Quantum Computing\n\nScott Aaronson\n\nProfessor, Computer Science\n\nOnline Learning and Optimization\n\nThis class has two major themes: algorithms for convex optimization and algorithms for online learning. The first part of the course will focus on algorithms for large scale convex optimization. A particular focus of this development will be for problems in Machine Learning, and this will be emphasized in the lectures, as well as in the problem sets. The second half of the course will then turn to applications of these ideas to online learning.\n\nWhat You Will Learn\n\nTechniques for convex optimization such as gradient descent and its variants\n\nAlgorithms for online learning such as follow the leader and weighted majority\n\nMulti-Armed Bandit problem and its variants\n\nSyllabus\n\nConvex sets and Convex functions, including basic definitions of convexity, smoothness and strong convexity\n\nFirst order optimality conditions for unconstrained and constrained convex optimization problems\n\nGradient and subgradient descent: Lipschitz functions, Smooth functions, Smooth and Strongly Convex functions\n\nOracle Lower Bounds\n\nAccelerated Gradient Methods\n\nProximal and projected gradient descent. ISTA and FISTA\n\nMirror Descent\n\nFrank Wolfe\n\nStochastic Gradient Descent\n\nStochastic bandits with finite number of arms: Explore and commit algorithm, UCB algorithm and regret analysis\n\nAdversarial bandits with finite number of arms: Exponential weighting and importance sampling, Exp3 algorithm and variants\n\nMulti-armed Bandit (MAB) lower bounds: minimax bounds, problem-dependent bounds\n\nContextual bandits: Bandits with experts — the Exp4 algorithm, stochastic linear bandits, UCB algorithm with confidence balls (LinUCB and variants)\n\nContextual bandits in the adversarial setting: Online linear optimization (with full and bandit feedback), Follow The Leader (FTL) and Follow the Regularized Leader (FTRL), Mirror Descent\n\nOnline Classification: Halfing algorithm, Weighted majority algorithm, Perceptron and Winnow algorithms (with connections to Online Gradient Descent and Online Mirror Descent)\n\nOther Topics: Combinatorial bandits, Bandits for pure exploration, Bandits in a Bayesian setting, Thompson sampling\n\nNewton and Quasi-Newton Methods\n\nConstantine Caramanis\n\nProfessor, Electrical & Computer Engineering\n\nSanjay Shakkottai\n\nProfessor, Electrical & Computer Engineering\n\nOptimization\n\nThis class covers linear programming and convex optimization. These are fundamental conceptual and algorithmic building blocks for applications across science and engineering. Indeed any time a problem can be cast as one of maximizing / minimizing and objective subject to constraints, the next step is to use a method from linear or convex optimization. Covered topics include formulation and geometry of LPs, duality and min-max, primal and dual algorithms for solving LPs, Second-order cone programming (SOCP) and semidefinite programming (SDP), unconstrained convex optimization and its algorithms: gradient descent and the newton method, constrained convex optimization, duality, variants of gradient descent (stochastic, subgradient etc.) and their rates of convergence, momentum methods.\n\nSyllabus\n\nConvex sets, convex functions, Convex Programs (1 week)\n\nLinear Programs (LPs), Geometry of LPs, Duality in LPs (1 week)\n\nWeak duality, Strong duality, Complementary slackness (1 week)\n\nLP duality: Robust Linear Programming, Two person 0-sum games, Max-flow min-cut (1 week)\n\nSemidefinite programming, Duality in convex programs, Strong duality (1 week)\n\nDuality and Sensitivity, KKT Conditions, Convex Duality Examples: Maximum Entropy (1 week)\n\nConvex Duality: SVMs and the Kernel Trick, Convex conjugates, Gradient descent (1 week)\n\nLine search, Gradient Descent: Convergence rate and step size, Gradient descent and strong convexity (1 week)\n\nFrank Wolfe method, Coordinate descent, Subgradients (1 week)\n\nSubgradient descent, Proximal gradient descent, Newton method (1 week)\n\nNewton method convergence, Quasi-newton methods, Barrier method (1 week)\n\nAccelerated Gradient descent, Stochastic gradient descent (SGD), Mini-batch SGD, Variance reduction in SGD (1 week)\n\nSujay Sanghavi\n\nAssociate Professor, Electrical and Computer Engineering\n\nConstantine Caramanis\n\nProfessor, Electrical & Computer Engineering\n\nSystems Courses\n\nAdvanced Operating Systems\n\nThis course focuses on the study of the formal structure, design principles, organization, implementation, and performance analysis of multiprogramming and/or multiprocessor computer systems.\n\nSyllabus\n\nCPU Virtualization\n\nMemory Virtualization\n\nStorage Virtualization\n\nAdvanced Topics: OS Architecture, Containers, Full-Machine, Virtualization, Heterogeneity\n\nProjects: 5 Projects spread across the semester, 3 of which involves developing features on xv6 operating system.\n\nVijay Chidambaram\n\nAssociate Professor, Computer Science\n\nAndroid Programming\n\nStudents will study Android APIs and learn to build significant Android applications. The course will have a practical focus, with significant in-class programming, programming assignments and a large project (optionally with a partner). The course philosophy is that programming is learned by doing. While the course focuses on Android, we will learn general principles of software engineering and mobile app development.\n\nThe course assumes familiarity with programming and object-oriented terminology. The course is taught entirely in Kotlin, the modern sibling of Java. We will spend a bit of time reviewing Kotlin, but you are expected to be familiar enough with Java that the transition will be seamless. The course does not assume any previous experience with Android programming.\n\nWhat You Will Learn\n\nHow to build an Android app, using the latest software technology (e.g., the Jetpack libraries)\n\nHow to effectively use Android Studio for programming, testing, and debugging\n\nGeneral principles of software engineering and mobile app development\n\nProgramming tools like the git version control system\n\nSyllabus\n\nKotlin introduction\n\nGUI widgets and layout\n\nActivities and their lifecycle. Implicit and explicit intents\n\nListView and RecyclerView\n\nFragments\n\nView models\n\nLive data\n\nModel-View-View model (MVVM)\n\nNetwork services\n\nFirebase authentication and Firestore cloud database\n\nDatabases and SQL\n\nMaps\n\nPersistent state, files\n\nEmmett Witchel\n\nProfessor, Computer Science\n\nImplementation of Programming Languages\n\nStructure and Implementation of Modern Programming Languages covers the component technologies used in implementing modern programming languages, shows their integration into a system, and discusses connections between the structure of programming languages and their implementations.\n\nWhat You Will Learn\n\nTechniques for program text analysis, including lexing, parsing, and semantic analysis\n\nTechniques for machine code synthesis, including code generation and register allocation\n\nMachine architectures, both physical and virtual\n\nTechniques for run-time actions, including memory management and dynamic linking\n\nMathematical underpinnings, including the grammar-language-automaton triad\n\nThe impact of individual language features on implementation techniques\n\nSyllabus\n\nThe compiler-interpreter spectrum\n\nStack virtual machine architecture: SaM, JVM\n\nLexical analysis; regular grammars, finite automata\n\nContext-free grammars, pushdown automata\n\nParsing techniques: recursive descent, Earley, others\n\nSemantic analysis: type checking\n\nRegister machine architecture: x86-64\n\nCode generation and register allocation\n\nProcedure call/return linkage\n\nMemory management: explicit, garbage collection\n\nModularity, linking, interoperability\n\nDynamic linking, position-independent code\n\nImplementing objects, inheritance, dynamic dispatch\n\nAdvanced topics: JIT compilation, stack randomization, bootstrapping\n\nSiddhartha Chatterjee\n\nProfessor of Instruction, Computer Science\n\nParallel Systems\n\nIn modern systems, concurrency and parallelism are no longer niche areas reserved for specialists and experts, but a cross-cutting concern to which all designers and developers are exposed. Technology trends suggest concurrency and parallelism are increasingly a cornerstone subject to which all successful programmers will require significant exposure. The objective of this course is to provide students with strong background on parallel systems fundamentals along with experience with a diversity of both classical and modern approaches to managing and exploiting concurrency, including shared memory synchronization, parallel architectures such as GPUs, as well as distributed parallel frameworks such as MPI and map-reduce.\n\nThis course explores parallel systems, from languages to hardware, from large-scale parallel computers to multicore chips, and from traditional parallel scientific computing to modern uses of parallelism. Includes discussion of and research methods in graphics, languages, compilers, architecture, and scientific computing.\n\nSyllabus\n\nBasic background/terminology/theory\n\nShared memory synchronization\n\nMassively parallel architectures\n\nDistributed execution frameworks\n\nRuntimes and front-end programming\n\nLatency vs. throughput\n\nHidden vs. exposed parallelism\n\nPerformance issues\n\nParallel algorithms instructive examples\n\nCalvin Lin\n\nProfessor, Computer Science\n\nChristopher Rossbach\n\nAssociate Professor, Computer Science\n\nVirtualization\n\nThis is a course designed to expose students to the latest in virtualization technologies such as virtual machines, containers, serverless, etc. The course also has a significant project component to be completed over the course of the semester. Topics include CPU virtualization, memory virtualization, networking virtualization, storage virtualization, paravirtualization, containers, unikernels, and serverless.\n\nWhat You Will Learn\n\nBasics of Virtual Machines\n\nBasics of Containers\n\nHow CPU is Virtualized\n\nHow Storage is Virtualized\n\nHow Network is Virtualized\n\nNested Virtualization\n\nHardware Features Assisting Virtualization\n\nDeploying Virtual Machines\n\nOrchestrating Containers\n\nSyllabus\n\nBasics of Virtual Machines (0.5 week)\n\nVirtualizing CPUs and DRAM (1 week)\n\nVirtualizing network and storage (1 week)\n\nParavirtualization and Nested Virtualization (1 week)\n\nSecurity in Virtualization (0.5 week)\n\nContainer basics (1 week)\n\nContainer orchestration frameworks (0.5 week)\n\nUnikernels (1 week)\n\nServerless Computing (1 week)\n\nAdvanced Topics (2 weeks)\n\nVijay Chidambaram\n\nAssociate Professor, Computer Science\n\nElective Courses\n\nCase Studies in Machine Learning\n\nThe Case Studies in Machine Learning course presents a broad introduction to the principles and paradigms underlying machine learning, including presentations of its main approaches, overviews of its most important research themes and new challenges faced by traditional machine learning methods. This course highlights major concepts, techniques, algorithms, and applications in machine learning, from topics such as supervised and unsupervised learning to major recent applications in housing market analysis and transportation. Through this course, students will gain experience by using machine learning methods and developing solutions for a real-world data analysis problems from practical case studies.\n\nWhat You Will Learn\n\nUnderstand generic machine learning (ML) terminology\n\nUnderstand motivation and functioning of the most common types of ML methods\n\nUnderstand how to correctly prepare datasets for ML use\n\nUnderstand the distinction between supervised and unsupervised learning, as well the interests and difficulties of both approaches\n\nPractice script implementation (Python/R) of different ML concepts and algorithms covered in the course\n\nApply software, interpret results, and iteratively refine and tune supervised ML models to solve a diverse set of problems on real-world datasets\n\nUnderstand and discuss the contents and contributions of important papers in the ML field\n\nApply ML methods to solve real world problems and present them to mini clients\n\nWrite reports in which results are assessed and summarized in relation to aims, methods and available data\n\nJunfeng Jiao\n\nAssociate Professor, School of Architecture\n\nPlanning, Search, and Reasoning Under Uncertainty\n\nWe will investigate how to define planning domains, including representations for world states and actions, covering both symbolic and path planning. We will study algorithms to efficiently find valid plans with or without optimality, and partially ordered, or fully specified solutions. We will cover decision-making processes and their applications to real-world problems with complex autonomous systems. We will investigate how in planning domains with finite state lengths, solutions can be found efficiently via search. Finally, to effectively plan and act in the real world, we will study how to reason about sensing, actuation, and model uncertainty. Throughout the course, we will relate how classical approaches provided early solutions to these problems, and how modern machine learning builds on, and complements such classical approaches.\n\nWhat You Will Learn\n\nDefining and solving planning problems\n\nPlanning algorithms for discrete and continuous state spaces\n\nAdversarial planning\n\nBayesian state estimation\n\nDecision-making in probabilistic domains\n\nSyllabus\n\nTopic 1: Planning Domain Definitions and Planning Strategies (1 week)\n\nTopic 2: Heuristic-Guided, and Search-based Planning (2 weeks)\n\nTopic 3: Adversarial Planning (2 weeks)\n\nTopic 4: Configuration-Space Planning/Sample-Based Planning (2 weeks)\n\nTopic 5: Probabilistic Reasoning/Bayesian State Estimation(2 weeks)\n\nTopic 7: Markov Decision Processes (1 week)\n\nTopic 8: Partially Observable Markov Decision Processes (1 week)\n\nJoydeep Biswas\n\nAssociate Professor\n\nImportant Dates"
    }
}
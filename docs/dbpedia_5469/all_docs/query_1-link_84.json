{
    "id": "dbpedia_5469_1",
    "rank": 84,
    "data": {
        "url": "https://theanarchistlibrary.org/library/kevin-a-carson-the-desktop-regulatory-state",
        "read_more_link": "",
        "language": "en",
        "title": "The Desktop Regulatory State",
        "top_image": "https://theanarchistlibrary.org/sitefiles/en/opengraph.png",
        "meta_img": "https://theanarchistlibrary.org/sitefiles/en/opengraph.png",
        "images": [
            "https://theanarchistlibrary.org/sitefiles/en/navlogo.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "Kevin A. Carson The Desktop Regulatory State The Countervailing Power of Individuals and Networks March 2016",
        "meta_lang": "en",
        "meta_favicon": "https://theanarchistlibrary.org/sitefiles/en/favicon.ico",
        "meta_site_name": "The Anarchist Library",
        "canonical_link": "https://theanarchistlibrary.org/library/kevin-a-carson-the-desktop-regulatory-state",
        "text": "Preface\n\nLike every book I’ve written since the first, this book was inspired by ideas I encountered in researching the previous one but was unable to explore and develop as much as I’d have liked within that framework. In writing the material on crisis tendencies of capitalism in Studies in Mutualist Political Economy, the writings by Paul Goodman and Ivan Illich on the hegemony of bureaucratic culture set a train of thought in motion that eventually lead to writing Organization Theory. Researching the chapter on decentralized manufacturing technology in Organization Theory led, in turn, to a stand-alone book on micro-manufacturing (Homebrew Industrial Revolution).\n\nThis book, in turn, is the development of ideas on network organization and stigmergy I touched on in Homebrew Industrial Revolution. It applies many of the same ideas in the realm of information that I developed earlier in regard to physical production in that book. It also ties in some of the ideas I discussed in the chapter on labor organization in Organization Theory, like open-mouth sabotage, but in much greater scope.\n\nThis book was a much longer time writing than any of my others, and because so much of its content involved ongoing current news I had much greater difficulty in either finding a cutoff point or setting parameters to filter out excessive detail. In the Appendix I wound up deleting a great deal of detail I’d previously incorporated on the activities of the various networked social movements starting with the Arab Spring, and shifted instead to a greater relative focus on the general principles behind the wave of networked movements since the EZLN uprising in 1994. My judgments on the level of detail to preserve were necessarily somewhat arbitrary; whether the result is satisfactory is up to the reader to decide.\n\nThis book, in keeping with the spirit of the subject matter, is far more a product of stigmergic organization and the wisdom of crowds than anything I’ve previously written. In an attempt to adhere to Eric Raymond’s principle that “many eyeballs make shallow bugs,” I first posted the roughly eight-month-old draft online at http://desktopregulatorystate.wordpress.com, warts and all, in March 2011. At the time it was four chapters (which have since fissioned into twelve), consisting mostly of placeholder notes in many places and containing some sections entirely blank except for the title. Since then I’ve automatically updated the online text whenever it was edited. I have benefited from many suggestions and tips from those following the progress of the book, including Steve Herrick’s wonderful job formatting the online word processor template for the online text, as well as all the information I get from email discussion lists (particularly the P2P Foundation, C4SS working group and Networked Labor lists), the leads from friends on Twitter, and the blogs and news sites I follow via RSS reader. And many thanks in particular to my friend Gary Chartier at La Sierra University, who has formatted this as well as two of my previous books for print!\n\n1. The Stigmergic Revolution\n\nSeveral parallel developments are driving a trend toward the growing obsolescence of large, highly capitalized, hierarchical organizations, and the ability of networked individuals with comparatively cheap capital equipment to perform the functions formerly performed by such organizations. They include the drastically reduced cost of capital goods required for informational and material production, as well as drastically reduced transaction costs of coordinating efforts between individuals.\n\nI. Reduced Capital Outlays\n\nFor most of the past two hundred years, the trend has been toward increasing capital outlays for most forms of production. The cost of the basic capital equipment required for production—the mass-production factory, the large printing press, the radio or TV station—was the primary justification for the large organization. The economy was dominated by large, hierarchical organizations administering enormous masses of capital. And the astronomical cost of production machinery was also the main justification for the wage system: production machinery was so expensive that only the rich could afford it, and hire others to work it.\n\nIn recent decades we’ve seen a reversal of this trend: a shift back from expensive, specialized machinery to inexpensive, general-purpose tools. Although this is true of both material and immaterial production—as attested by the recent revolution in garage-scale CNC machine tools —it was true first and most dramatically in the immaterial sphere.\n\nThe desktop computer is the primary item of capital equipment required for entering a growing number of industries, like music, desktop publishing and software design. The desktop computer, supplemented by assorted packages of increasingly cheap printing or sound editing equipment, is capable of doing what previously required a minimum investment of hundreds of thousands of dollars. In the words of Yochai Benkler: “declining price of computation, communication, and storage have, as a practical matter, placed the material means of information and cultural production in the hands of a significant fraction of the world’s population—on the order of a billion people around the globe.” (Of course since that passage was written the proliferation of cheapening smartphones has probably expanded the latter figure to include over half the world’s population.)\n\nThe growing importance of human capital, and the implosion of capital outlays required to enter the market, have had revolutionary implications for production in the immaterial sphere. In the old days, the immense outlay for physical assets was the primary basis for the corporate hierarchy’s power, and in particular for its control over human capital and other intangible assets. In many information and culture industries, according to Benkler, the initial outlay for entering the market in the days of “broadcast culture” was in the hundreds of thousands of dollars or more.\n\nSince the introduction of the mechanical press and the telegraph, followed by the phonograph, film, the high-powered radio transmitter, and through to the cable plant or satellite, the capital costs of fixing information and cultural goods in a transmission medium—a high-circulation newspaper, a record or movie, a radio or television program—have been high and increasing.\n\nThe broadcast era media, for instance, were “typified by high-cost hubs and cheap, ubiquitous, reception-only systems at the end.... [P]roduction in the information and entertainment industries was restricted to those who could collect sufficient funds to set up a hub.” In the case of print periodicals, the increasing cost of printing equipment from the mid-nineteenth century on served as the main entry barrier for organizing the hubs. By 1850 the typical startup cost of a newspaper was $100,000—$2.38 million in 2005 dollars. In other words, as the saying went, freedom of the press was great so long as you could afford to own a press.\n\nThe networked information economy, in contrast, is distinguished by “network architecture and the [low] cost of becoming a speaker.”\n\nThe first element is the shift from a hub-and-spoke architecture with unidirectional links to the end points in the mass media, to distributed architecture with multidirectional connections among all nodes in the networked information environment. The second is the practical elimination of communications costs as a barrier to speaking across associational boundaries. Together, these characteristics have fundamentally altered the capacity of individuals, acting alone or with others, to be active participants in the public sphere as opposed to its passive readers, listeners, or viewers.\n\nToday most people in the developed world, and in a rapidly growing share of the developing world, can afford to own a press.\n\nIn the old days, the owners of the hubs—CBS News, the Associated Press, etc.—decided what you could hear. Today you can set up a blog, or record a podcast, and anybody in the world who cares enough to go to your URL can look at it free of charge (and anyone who agrees with it—or wants to tear it apart—can provide a hyperlink).\n\nThe cultural authoritarianism that resulted from the old state of affairs, as Clay Shirky points out, is unimaginable to someone who grew up with access to the Internet.\n\nDespite half a century of hand-wringing about media concentration, my students have never known a media landscape of anything less than increasing abundance. They have never known a world with only three television channels, a world where the only choice a viewer had in the early evening was which white man was going to read them the news in English. They can understand the shift from scarcity to abundance, since the process is still going on today. A much harder thing to explain to them is this: if you were a citizen of that world, and you had something you needed to say in public, you couldn’t. Period..... Movie reviews came from movie reviewers. Public opinions came from opinion columnists. Reporting came from reporters. The conversational space available to mere mortals consisted of the kitchen table, the water cooler, and occasionally letter writing....\n\nThe central change that makes these things possible, according to Benkler, is that “the basic physical capital necessary to express and communicate human meaning is the connected personal computer.”\n\nThe core functionalities of processing, storage, and communications are widely owned throughout the population of users.... The high capital costs that were a prerequisite to gathering, working, and communicating information, knowledge, and culture, have now been widely distributed in the society. The entry barrier they posed no longer offers a condensation point for the large organizations that once dominated the information environment.\n\nThe desktop revolution and the Internet mean that the minimum capital outlay for entering most entertainment and information industries has fallen to a few hundred or a few thousand dollars, and the marginal cost of reproduction is zero.\n\nThe networked environment, combined with endless varieties of cheap software for creating and editing content, makes it possible for the amateur to produce output of a quality once associated with giant publishing houses and recording companies. That is true of the software industry, desktop publishing, and to a large extent even indie film (as witnessed by affordable editing technology and the success of projects like Sky Captain).\n\nIn the case of the music industry, thanks to cheap equipment and software for high quality recording and sound editing, the costs of independently producing and distributing a high-quality album have fallen through the floor. Bassist Steve Lawson writes:\n\n.... [T]he recording process—studio time and expertise used to be hugely expensive. But the cost of recording equipment has plummeted, just as the quality of the same has soared. Sure, expertise is still chargeable, but it’s no longer a non-negotiable part of the deal. A smart band with a fast computer can now realistically make a release quality album-length body of songs for less than a grand....\n\nWhat does this actually mean? Well, it means that for me—and the hundreds of thousands of others like me—the process of making and releasing music has never been easier. The task of finding an audience, of seeding the discovery process, has never cost less or been more fun. It’s now possible for me to update my audience and friends (the cross-over between the two is happening on a daily basis thanks to social media tools) about what I’m doing—musically or otherwise—and to hear from them, to get involved in their lives, and for my music to be inspired by them....\n\nSo, if things are so great for the indies, does that mean loads of people are making loads of money? Not at all. But the false notion there is that any musicians were before! We haven’t moved from an age of riches in music to an age of poverty in music. We’ve moved from an age of massive debt and no creative control in music to an age of solvency and creative autonomy. It really is win/win.\n\nAs the last statement suggests, it may well be that most of the revenue loss to the music industry has fallen, not on actual performers, but on the middlemen in the record companies themselves.\n\nNetworked distribution models have already gone a long way toward challenging and supplanting older models. For example the alternative rock group Radiohead marketed an album (Rainbows) directly over the Web, making it available for free and accepting whatever contributions downloaders saw fit to give. This would seem to be an ideal approach for independent artists, compared to the difficulty of making it through the record company gatekeepers and then settling for the royalties paid out after all the middlemen take their cut. It only requires, for all intents and purposes, a cheap website with a PayPal button. I have personal experience with a similar approach to publishing books, making them available for free online and selling hard copies through an on-demand publisher. And outside the blockbuster market, most writers and musical artists probably know more than the in-house marketing experts at the big content companies about their own niche markets. So they can do a better job marketing their own material virally to their target audiences through blogs, email lists and social networks than they would relying on the by-the-numbers efforts of the publishers’ in-house promoters.\n\nThis approach undermines the business model of the old record and publishing companies, and probably does cut into the revenues of their old stables of blockbuster artists. It’s probably becoming harder for another Stephen King or Mick Jagger to make megabucks because of competition from the networked distribution model, and surely a lot harder for the old gatekeeper corporations to make the giant piles of money they used to.\n\nBut if it’s harder for the big boys to make gigantic piles of money, it’s easier for a lot more little ones to make modest piles. Endless possibilities result from all the things they can now do for themselves, at virtually zero cost, that formerly only a highly capitalized record or publishing company could do for them.\n\nAs an independent scholar and author, I share Steve Lawson’s view of things. From my perspective, the proper basis for comparison is the money I can make that I never could have made at all in the “good old days.” In the good old days, I’d have—and have done—painstakingly put together a manuscript of hundreds of pages, and then put it away to gather dust when I couldn’t persuade the gatekeepers at a conventional publisher that it was worth marketing. Never mind whether the facsimile pdf’s of my books available at torrent sites are costing me money (I don’t think they are—I believe the free e-books are more like viral advertising). More importantly, if it weren’t for digital publishing technologies and free publishing venues on the Internet, I would probably have lived and died doing menial labor with nobody anywhere ever hearing of my ideas. Thanks to digital culture, I’m able to make my work directly available to anyone in the world who has an Internet connection. If only a tiny fraction of the people who can read it for free decide to buy it, giving me a few thousand dollars a year in royalties, I’m richer by exactly that amount than I would have been in the “good old days” when my manuscripts would have yellowed in an attic.\n\nThat extra money may not be enough to support me by itself, but it’s enabled me at various times to pay off debts and put away go-to-hell money equivalent to several months’ wages. Right now about half my income, in an average month, comes from writing. That probably puts me in a much better bargaining position vis-a-vis my employer than most people enjoy.\n\nFor every small full-time musician who has a harder time scraping by, and may have to supplement her performing revenues with a day job, I suspect there are ten people like me who would have spent their entire lives as (if you’ll pardon the expression) mute inglorious Miltons, without ever making a cent from their music or writing, but who can now be heard. And for every blockbuster writer or musician who has a few million shaved off her multi-million dollar revenues as a result of online “piracy,” I suspect there are probably a hundred people like me.\n\nAs for the old broadcast media, podcasting makes it possible to distribute “radio” and “television” programming, at virtually no cost, to anyone with a broadband connection. As radio historian Jesse Walker notes, satellite radio’s lackadaisical economic performance doesn’t mean people prefer to stick with AM and FM radio; it means, rather, that the iPod has replaced the transistor radio as the primary portable listening medium, and that downloaded files have replaced the live broadcast as the primary form of content.\n\nA network of amateur contributors has peer-produced an encyclopedia, Wikipedia, which Britannica sees as a rival.\n\nThere are enormous online libraries like Google Books, Project Gutenberg and Internet Archive, as well as more specialized efforts like Marxists.org (which archives the collected works of Marx, Engels and Lenin, and of writers ranging from Kautsky to Luxemburg to Trotsky to C.L.R. James), the Anarchy Archives (extensive archives of most of the major works of classical anarchism), and Constitution.org (including, among many other things, Elliot’s debates in the ratifying conventions and St. George Tucker’s edition of Blackstone). In effect they give any kid with a smart phone, whether in the Third World or in an American ghetto, access to the equivalent of a university library. If one is willing and able to pay an annual subscription fee, there are enormous online collections of scholarly journals like JSTOR. And rebellious scholars are in process of tearing down the paywalls and the textbook racket; scholars with JSTOR memberships are providing articles for free to their peer networks. It’s possible to solicit pdfs of paywalled articles using the #ICanHazPdf hashtag on Twitter. And there are also services which strip DRM from college textbook pdfs which publishers make available for rental, so that they can be used indefinitely and distributed through torrent download sites.\n\nThe network revolution has drastically lowered the transaction costs of organizing education outside the conventional institutional framework. In most cases, the industrial model of education, based on transporting human raw material to a centrally located “learning factory” for processing, is obsolete. Forty years ago Ivan Illich, in Deschooling Society, proposed decentralized community learning nets that would put people in contact with the teachers they wanted to learn from, and provide an indexed repository of learning materials. The Internet has made this a reality beyond Illich’s wildest dreams.\n\nNiall Cook, in Enterprise 2.0, describes the comparative efficiencies of software available outside the enterprise to the “enterprise software” in common use by employers. Self-managed peer networks, and individuals meeting their own needs in the outside economy, organize their efforts through social software and platforms chosen by the users themselves based on their superior usability for their purposes. And they are free to do so without corporate bureaucracies and their officially defined procedural rules acting as a ball and chain.\n\nEnterprise software, in contrast, is chosen by non-users for use by other people of whose needs they know little (at best).\n\nBlogs and wikis, and the free, browser-based platforms offered by Google and Mozilla, are a quantum improvement on the proprietary enterprise software that management typically forces on its employees. My OpenOffice CD cost me all of ten bucks, as opposed to $200 for Microsoft Office. The kinds of productivity software and social software freely available to individuals in their private lives is far better than the enterprise software that corporate bureaucrats buy for a captive clientele of wage slaves—consumer software capabilities amount to “a fully functioning, alternative IT department.” Corporate IT departments, in contrast, “prefer to invest in a suite of tools ‘offered by a major incumbent vendor like Microsoft or IBM’.” System specs are driven by management’s top-down requirements rather than by user needs.\n\n.... a small group of people at the top of the organization identify a problem, spend 12 months identifying and implementing a solution, and a huge amount of resources launching it, only then to find that employees don’t or won’t use it because they don’t buy in to the original problem.\n\nManagement is inclined “to conduct a detailed requirements analysis with the gestation period of an elephant simply in order to choose a $1,000 social software application.” Employees often wind up using their company credit cards to purchase needed tools online rather than “wait for [the] IT department to build a business case and secure funding.” This is the direct opposite of agility.\n\nIt’s just one particular example of the gold-plated turd phenomenon, in which stovepiped corporate design bureaucracies develop products for sale to other stovepiped corporate procurement bureaucracies, without the intervention of user feedback at any point in the process.\n\nAs a result of all this, people are more productive away from work than they are at work. And management wonders why people would rather work at home using their own software tools than go through Checkpoint Charlie to use a bunch of klunky proprietary “productivity software” from the Whore of Redmond.\n\nAs Tom Coates put it, all these developments in the field of immaterial production mean that “the gap between what can be accomplished at home and what can be accomplished in a work environment has narrowed dramatically over the last ten to fifteen years.”\n\nEven when free and open source models don’t quite equal the quality of the proprietary stuff, as Cory Doctorow argues they usually manage a close enough approximation of it at a tiny fraction of the cost.\n\nThis is the pattern: doing something x percent as well with less-than-x percent of the resources. A blog may be 10 percent as good at covering the local news as the old, local paper was, but it costs less than 1 percent of what that old local paper cost to put out. A home recording studio and self-promotion may get your album into 30 percent as many hands, but it does so at five percent of what it costs a record label to put out the same recording.\n\nWhat does this mean? Cheaper experimentation, cheaper failure, broader participation. Which means more diversity, more discovery, more good stuff that could never surface when the startup costs were so high that no one wanted to take any risks.\n\nAnd the gap between almost-as-good and just-as-good is narrowing rapidly.\n\nII. Distributed Infrastructure and Ephemeralization\n\nThe larger and more hierarchical institutions become, and the more centralized the economic system, the larger the total share of production that will go to overhead, administration, waste, and the cost of doing business. The reasons are structural and geometrical.\n\nAt its most basic, it’s an application of the old cube-square rule. When you double the dimensions of a solid object, you increase its surface area fourfold (two squared), but its volume eightfold (two cubed). Similarly, the number of internal relationships in an organization increases as the square of the number of individuals making it up.\n\nLeopold Kohr gave the example, in The Overdeveloped Nations, of a skyscraper. The more stories you add, the larger the share of floor space on each story is taken up by ventilation ducts, wiring and pipes, elevator shafts, stairwells, etc. Eventually you reach a point at which the increased space produced by adding another story is entirely eaten up by the increased support infrastructure.\n\nThe larger the scale of production, the more it must be divorced from demand, which means that the ostensible “economies” of large batch production are offset, and then more than offset, by the increasing costs of finding new ways of making people buy stuff that was produced without regard to preexisting orders.\n\nThe society becomes more and more like the Ministry of Central Services in Brazil, or The Feds in Neal Stephenson’s Snow Crash, and the distribution of occupations increasingly resembles the demographic profile of the promoters and middlemen in the crashed spaceship in A Hitchhiker’s Guide to the Galaxy, who founded the human race on Earth.\n\nThe only way out is a new standard of progress that doesn’t equate “growth” with larger institutional size and more centralization: scalable, distributed infrastructure, stigmergic organization, module-and-platform design configurations, and production capacity sited close to the point of consumption and scaled to demand.\n\nPaul Hawken and Amory and Hunter Lovins, in Natural Capitalism, stated the general principle that when load-bearing infrastructures are built to handle loads at peak demand, most of the unit cost comes from the added infrastructure to handle the increased usage during the small minority of peak load time. They gave the specific example of home heating, where enormous savings could be achieved by scaling capacity to handle only average usage, with additional demand handled through spot heating. Most of the horsepower in a contemporary SUV exists only for brief periods of acceleration when changing lanes.\n\nIt’s a basic principle of lean production: most costs come from five percent of point consumption needs, and from scaling the capacity of the load-bearing infrastructure to cover that extra five percent instead of just handling the first ninetyfive percent. It ties in, as well, with another lean principle: getting production out of sync with demand (including the downstream demand for the output of one step in a process), either spatially or temporally, creates inefficiencies. Optimizing one stage without regard to production flow and downstream demand usually involves expensive infrastructure to get an in-process input from one stage to another, often with intermediate storage while it is awaiting a need. The total resulting infrastructure cost greatly exceeds the saving at individual steps. Inefficient synchronization of sequential steps in any process results in bloated overhead costs from additional storage and handling infrastructure.\n\nMore generally, centralized infrastructures must be scaled to handle peak loads even when such loads only occur a small fraction of the time. And then they must amortize the extra cost, by breaking user behavior to the needs of the infrastructure.\n\nAt the opposite pole is distributed infrastructure that’s mostly distributed among the endpoints, with links directly between endpoints rather than passing through a central hub, and volume driven entirely by user demand at the endpoints. Since the capital goods possessed by the endpoints are a miniscule fraction of the cost of a centralized infrastructure, there is no incentive to subordinate endusers to the needs of the infrastructure.\n\nThe classic example is Bucky Fuller’s: the replacement of the untold millions of tons of metal in transoceanic cables with a few dozen one-ton satellites. The entire infrastructure consists of satellite dishes at the endpoints commuinicating—via free, immaterial ether!—to the satellites.\n\nLikewise the enormous infrastructure tied up in the civil aviation system’s central hubs and batch-and-queue processing, as opposed to small jets flying directly between endpoints.\n\nAnother example is mass-production industry, which minimizes unit costs by running its enormously costly capital-intensive machinery at full capacity 24/7, and then requires organizing a society to guarantee consumption of the full output whether consumers want the shit or not—what’s called “supply-push distribution.” If consumers won’t take it all, you soak up surplus output by destroying it through a permanent war economy, sinking it into an Interstate Highway System, etc.—or maybe just making stuff to fall apart.\n\nThe opposite of mass-production is distributed production on the EmiliaRomagna model described by Charles Sabel and Michel Piore in The Second Industrial Divide, with the capital infrastructure distributed to the point of consumption and output geared to local demand. The transnational corporate model of outsourcing is an attempt to put this new wine in old bottles. It distributes the production facilities, but does so on the basis of local labor cost rather than the location of market demand. So it still relies on the centralized wholesale infrastructure of warehouses on wheels/containerships, scaled to peak load, to transfer goods from the distributed production sites to the point of final consumption. The pure and unadulterated distributed manufacturing model, on the other hand, does away with this infrastructure by siting production at the last-mile network of consumption.\n\nThe model of stigmergic organization in Wikipedia and open-source design—the central theme of this book—is an example of distributed infrastructure. Individual contributions are managed entirely by endpoint users, coordinating their efforts with the finished body of work, without the intermediary of a centralized institutional framework as in old-line activist organizations.\n\nIII. Distributed Infrastructure and Scalability\n\nAnother advantage of distributed infrastructure is that it is scalable; that is, each separate part is capable of functioning on its own, regardless of whether the rest of the system is functioning. When a centralized infrastructure fails at any point, on the other hand, the whole system is incapacitated.\n\nA large dam project must be completed to give service, and if something in the environment changes half way through the project, there is little hope of adapting the project to the new circumstances. The entire risk is assumed at the start of the project, based on long term projections about the future in many different domains, from energy demand through to geopolitical stability. On the other hand, an array of micropower projects could provide equivalent electrical services, and as the projects are each built, continuous assessment of the “right next move” can be made to suit learning from previous projects, response to changing demand, adoption of improved technologies or shifting priorities. Fundamentally, half a dam is no dam at all, but 500 of 1000 small projects is half way to the goal. A modular approach to infrastructure in an uncertain world just makes sense.\n\nIV. Network Organization\n\nAs Johan Soderburg argues, “[t]he universally applicable computer run on free software and connected to an open network.... has in some respects leveled the playing field. Through the global communication network, hackers are matching the coordinating and logistic capabilities of state and capital.”\n\nUntil the early 1990s, there were many possible Internets. What makes the Internet the “Internet” we know is really the World Wide Web: all the billions of web pages linked together by hyperlinks. And depending on the institutional context in which hyperlinks had been introduced, the Web as we know it might never have existed. Tim Berners-Lee in 1990,\n\npublished a more formal proposal.... to build a “ Hypertext project” called “WorldWideWeb”.... as a “web” of “hypertext documents” to be viewed by “browsers” using a client–server architecture. This proposal estimated that a read-only web would be developed within three months and that it would take six months to achieve “the creation of new links and new material by readers, [so that] authorship becomes universal” as well as “the automatic notification of a reader when new material of interest to him/her has become available.” While the read-only goal was met, accessible authorship of web content took longer to mature, with the wiki concept, blogs, Web 2.0 and RSS/Atom.\n\nThe Web as we know it is something that could never have been built as the unified, conscious vision of any institution.\n\nIt’s interesting that most visions of the “Information Superhighway,” preWorld Wide Web, imagined it as populated largely by large institutional actors of one kind or another, and its communications as mainly one-way. It would be built on the backbone of the Internet’s packet-switching infrastructure, vastly expanded in capacity by a fusion of the telephone and cable TV industries into a single highbandwidth fiber-optic network.\n\nI recall seeing a speculative article in TV Guide in the late ‘70s, when I was just a junior high school kid, speculating on the science fictiony wonders that would soon be possible. Everyone would have a combination digital telephonecomputer-radio-cable TV terminal as the main entertainment center in their home, cable of accessing streaming content—television programs, movies, music, digitized books and periodicals, etc.—presumably on a paid basis. The key actors providing this whiz-bang content would be libraries, media conglomerates, and government agencies.\n\nThe Internet envisioned by figures like Al Gore and Bill Gates was, despite the decentralized nature of the physical packet-switching process, very centralized in terms of the actors providing content. Their vision of the Internet was simply as a foundation for the Information Superhighway. The legal infrastructure for the Superhighway consisted of the Telecommunications Act of 1996, which eliminated barriers to telephone/cable mergers, and the Digital Millennium Copyright Act of 1998, which created the draconian system of copyright law needed for digital content providers to turn the Superhighway into a turnpike. Here’s what Bill Gates had to say, as late as early 2000:\n\nThis new generation of set-top boxes that connects up to the Internet is very much part of that. The potential impact is pretty phenomenal in the terms of being able to watch a TV show whenever you want to. There will be so many choices out there. You’ve got to imagine that a software agent will help you find things that you might be interested in.\n\n.... The “TV guide” will almost be like a search portal where you’ll customize and say, “I’m never interested in this, but I am particularly interested in that.” It’s already getting a little unwieldy. When you turn on DirectTV and you step through every channel—well, there’s three minutes of your life.\n\nWhen you walk into your living room six years from now, you’ll be able to just say what you’re interested in, and have the screen help you pick out a video that you care about. It’s not going to be “Let’s look at channels 4, 5, and 7.” It’s going to be something that has pretty incredible graphics and it’s got an Internet connection to it.\n\nBut the Information Superhighway—in the sense of a fusion of telephone, cable, radio, and on-demand music and movies, accessed through a single digital home entertainment center, simply fizzled out. Instead, the World Wide Web took over the Internet.\n\nMike Masnick speculates on what the World Wide Web—if it could even be called that—would have looked like, had Tim Berners-Lee obtained a patent on the hyperlinked architecture of the Web. And his hypothetical description reads very close to the vision of TV Guide, Gore and Gates.\n\nWhere do you think the world would be today if the World Wide Web had been patented? Here are a few guesses:\n\nRather than an open World Wide Web, most people would have remained on proprietary, walled gardens, like AOL, Compuserve, Prodigy and Delphi. While those might have eventually run afoul of the patents, since they were large companies or backed by large companies, those would have been the few willing to pay the licensing fee. The innovation level in terms of the web would have been drastically limited. Concepts like AJAX, real time info, etc. would not be present or would be in their infancy. The only companies “innovating” on these issues would be those few large players, and they wouldn’t even think of the value of such things.\n\nNo Google. Search would be dismal, and limited to only the proprietary system you were on. Most people’s use of online services would be more about “consumption” than “communication.” There would still be chat rooms and such, but there wouldn’t be massive public communication developments like blogs and Twitter. There might be some social networking elements, but they would be very rudimentary within the walled garden.\n\nNo iPhone. While some might see this as separate from the web, I disagree. I don’t think we’d see quite the same interest or rise in smartphones without the web. Would we see limited proprietary “AOL phones?” Possibly, but with a fragmented market and not as much value, I doubt there’s the necessary ecosystem to go as far as the iPhone.\n\nOpen internet limited by lawsuit. There would still be an open internet, and things like gopher and Usenet would have grown and been able to do a little innovation. However, if gopher tried to expand to be more web like, we would have seen a legal fight that not only delayed innovation, but limited the arenas in which we innovated.\n\nThe Internet would have been a wasteland of walled-garden ISPs like AOL, with Usenet and BBSs grafted on. What Web there was would have been accessed, not by browsers or open search engines, but through portals like AOL or Yahoo!.\n\nIt’s not necessary to speculate that something like that would surely had happened had Berners-Lee not been first to the draw. It was happening, in fact. As recounted by David Weinberger, the software company for which he was vice president of strategic marketing at the time was in process of developing a proprietary document format with embedded links, when it was caught off-guard by the Mosaic browser. As the developers attempted to reassure themselves, their software was far more polished and professional-looking, and had better capabilities, than Mosaic. But deep down, they knew that Mosaic’s lack of “bells and whistles” was more than compensated for by its openness.\n\nWith our software, a publisher could embed a link from one document to another, but the publisher had to own both documents. That’s fine if you’re putting together a set of aircraft maintenance manuals and you want to make all the cross-references active, so that clicking on one brings up the page to which it’s referring. But those links had to be compiled into the system. Once the document was published, no more links could be added except by recompiling the document. And, most important, the only people who could add new links were those working for the publisher. If you were an aircraft mechanic who had discovered some better ways to clean a fuel line, you had no way to publish your page with our system and no way to link it to the appropriate page in the official manual.\n\n.... The Web ditches that model, with all its advantages as well as its drawbacks, and says instead, “You have something to say? Say it. You want to respond to something that’s been said? Say it and link to it. You think something is interesting? Link to it from your home page. And you never have to ask anyone’s permission.” .... By removing the central control points, the Web enabled a self-organizing, self-stimulated growth of contents and links on a scale the world has literally never before experienced.\n\nRupert Murdoch’s objections notwithstanding, the basic organizing principle of the Web is that you can link to another person’s website without having to ask permission or secure her cooperation.\n\nIt was actually the collapse of Web 1.0 in the dot-com bubble, and with it most of the hopes of the “visionaries” of the 1990s for enclosing the Web as a source of revenues, that created the space in which the decentralized vision of Web 2.0 could be fully realized. As Foundation for P2P Alternatives founder Michel Bauwens described it:\n\nAll the pundits where predicting, then as now, that without capital, innovation would stop, and that the era of high internet growth was over for a foreseeable time. In actual fact, the reality was the very opposite, and something apparently very strange happened. In fact, almost everything we know, the Web 2.0, the emergence of social and participatory media, was born in the crucible of that downturn. In other words, innovation did not slow down, but actually increased during the downturn in investment. This showed the following new tendency at work: capitalism is increasingly being divorced from entrepreneurship, and entrepreneurship becomes a networked activity taking place through open platforms of collaboration.\n\nThe reason is that internet technology fundamentally changes the relationship between innovation and capital. Before the internet, in the Schumpeterian world, innovators need capital for their research, that research is then protected through copyright and patents, and further funds create the necessary factories. In the post-schumpeterian world, creative souls congregate through the internet, create new software, or any kind of knowledge, create collaboration platforms on the cheap, and paradoxically, only need capital when they are successful, and the servers risk crashing from overload.\n\nThe Web’s many-to-many communications capabilities have enabled networks to coordinate the actions of self-directed individuals without the transaction costs of traditional hierarchies. Benkler explained the implications of networked communications, combined with the near-universal distribution of capital goods for information and cultural production:\n\n.... the technical architectures, organizational models, and social dynamics of information production and exchange on the Internet have developed so that they allow us to structure the solution to problems—in particular to information production problems—in ways that are highly modular. This allows many diversely motivated people to act for a wide variety of reasons that, in combination, cohere into new useful information, knowledge, and cultural goods. These architectures and organizational models allow both independent creation that coexists and coheres into usable patterns, and interdependent cooperative enterprises in the form of peer-production processes.\n\nIn other words, it’s stigmergic organization (about which more below)—what Weinberger calls “small pieces loosely joined.”\n\nNetworked crowdsourcing venues like Kickstarter, GoFundMe and Patreon have radically lowered the costs of aggregating capital even when total outlays are still beyond the means of the average individual. That means that even when the costs of the physical capital required for production are non-trivial, the transaction costs of aggregating the required investment capital from a number of small contributors.\n\nBut whether capital outlay requirements are large or small, network technology has had a revolutionary effect on the transaction costs of traditional organization.\n\nThat was true even back in the 1990s, when the Internet was dominated by static institutional websites. Email, both individual and in discussion lists, was a powerful tool for networked organization. The forms of culture jamming described by Naomi Klein in No Logo, themselves unprecedented and revolutionary in her day, were an outgrowth of the possibilities of the Web 1.0 of the 1990s. But the rise of Web 2.0, and the free platforms it made available, increased the possibilities exponentially. To quote Benkler again:\n\nWhat we are seeing now is the emergence of more effective collective action practices that are decentralized but do not rely on either the price system or a managerial structure for coordination.... [The networked environment] provides a platform for new mechanisms for widely dispersed agents to adopt radically decentralized cooperation strategies other than by using proprietary and contractual claims to elicit prices or impose managerial commands.... What we see in the networked information economy is a dramatic increase in the importance and the centrality of information produced in this way.\n\nConsider the drastically lowered costs of aggregating people into affinity groups or movements for the sharing of information and taking concerted action. Clay Shirky cites the example of Voice of the Faithful, a Catholic lay organization formed to fight priestly sexual abuse:\n\nHad VOTF been founded in 1992, the gap between hearing about it and deciding to join would have presented a series of small hurdles: How would you locate the organization? How would you contact it? If you requested literature, how long would it take to arrive, and by the time it got there, would you still be in the mood? None of these barriers to action is insurmountable, but together they subject the desire to act to the death of a thousand cuts.\n\nBecause of the delays and costs involved, going from a couple dozen people in a basement to a large and global organization in six months is inconceivable without social tools like websites for membership and e-mail for communication.\n\nI can remember, as a grad student in the 1980s, experiencing that “series of small hurdles” in dealing with a completely different—but analogous—situation. If I heard of some periodical in my area of interest that the university library didn’t carry, the only way to find out more about it was to dig through the latest installment of Ulrich’s Periodicals Directory, send a query letter soliciting information about the price of sample issues, wait several weeks for a response, send in the money, and wait several more weeks for my sample.\n\nToday, I just Google the title of the journal, and most likely it’s got a website with an index of past issues. I can instantly get a pdf of any article of interest through online academic indexing services—or better yet, soliciting a free copy from someone with a JSTOR or SSRN membership. Soon, dedicated sharing sites with indexed academic articles available free for scholars will probably be as common as mp3-sharing sites—much to the chagrin of the academic publishing industry.\n\nThe cumulative effect is that a rapidly increasing share of the functions previously carried out by corporations and by the state can now be effectively out by what Marx and Engels, in The Communist Manifesto, called the “associated producers”—without any bureaucratic intermediation. Matthew Yglesias describes it as “actually existing Internet communism.”\n\nAnother result of the reduced threshold for communications in networks is the drastic increase in speed of propagation.\n\nSmart mobs are essentially a rapid cascade of coordinated action. “Whenever a new communications technology lowers the threshold for groups to act collectively, new kinds of institutions emerge.... We are seeing the combination of network communications and social networks.”\n\nV. Stigmergy\n\nNetworked organization is based on a principle known as stigmergy—a term coined by biologist Pierre-Paul Grasse in the 1950s to describe the process by which termites coordinate their activity. Social insects coordinate their efforts through the independent responses of individuals to environmental triggers like chemical markers, without any need for a central coordinating authority. It was subsequently applied to the analysis of human society.\n\nAs a sociological term stigmergy refers primarily to the kinds of networked organization associated with wikis, group blogs, and “leaderless” organizations configured along the lines of networked cells.\n\nThe termites do not communicate about who is to do what how or when. Their only communication is indirect: the partially executed work of the ones provides information to the others about where to make their own contribution. In this way, there is no need for a centrally controlled plan, workflow, or division of labor.\n\nWhile people are of course much more intelligent than social insects and do communicate, open access development uses essentially the same stigmergic mechanism.... : any new or revised document or software component uploaded to the site of a community is immediately scrutinized by the members of the community that are interested to use it. When one of them discovers a shortcoming, such as a bug, error or lacking functionality, that member will be inclined to either solve the problem him/herself, or at least point it out to the rest of the community, where it may again entice someone else to take up the problem.\n\nSocial negotiation, according to Mark Elliott, is the traditional method of organizing collaborative group efforts, through agreements and compromise mediated by discussions between individuals. The exponential growth in the number of communications with the size of the group, obviously, imposes constraints on the feasible size of a collaborative group, before coordination must be achieved by hierarchy and top-down authority. Stigmergy, on the other hand, permits collaboration on an unlimited scale by individuals a cting independently. This distinction between social negotiation and stigmergy is illustrated, in particular, by the contrast between traditional models of co-authoring and collaboration in a wiki. Individuals communicate indirectly, “via the stigmergic medium.” He makes a parallel distinction elsewhere between “discursive collaboration” and “stigmergic collaboration.” “.... [W]hen stigmergic collaboration is extended by computing and digital networks, a considerable augmentation of processing capacity takes place which allows for the bridging of the spatial and temporal limitations of discursive collaboration, while subtly shifting points of negotiation and interaction away from the social and towards the cultural.”\n\nStigmergic organization results in modular, building-block architectures. Such structures are ubiquitous because a modular structure\n\ntransforms a system’s ability to learn, evolve and adapt.... Once a set of building blocks.... has been tweaked and refined and thoroughly debugged through experience.... then it can generally be adapted and recombined to build a great many new concepts.... Certainly that’s a much more efficient way to create something new than starting all over from scratch. And that fact, in turn, suggests a whole new mechanism for adaptation in general. Instead of moving through that immense space of possibilities step by step, so to speak, an adaptive system can reshuffle its building blocks and take giant leaps.”\n\nA small number of building blocks can be shuffled and recombined to make a huge number of complex systems.\n\nIf you start with a large number of modular individuals, each capable of interacting with a few other individuals, and acting on other individuals according to a simple grammar of a few rules, under the right circumstances the modular individuals can undergo a rapid phase transition, according to systems theorist Stuart Kauffman: “The growth of complexity really does have something to do with farfrom-equilibrium systems building themselves up, cascading to higher and higher levels of organization. Atoms, molecules, autocatalytic sets, et cetera.”\n\nGus diZerega’s discussion of spontaneous orders is closely analogous to stigmergy. Spontaneous orders\n\narise from networks of independent equals whose actions generate positive and negative feedback that help guide future actors in pursuing their own independently conceived plans, thereby continuing the feedback process. Each person is a node within a network and is linked by feedback, with each node free to act on its own. The feedback they generate minimizes the knowledge anyone needs about the system as a whole in order to succeed within it.\n\nAll spontaneous orders possess certain abstract features in common. Participants are equal in status and all are equally subject to whatever rules must be followed to participate within the order. All are free to apply these rules to any project of their choosing. Anything that can be pursued without violating a rule is permitted, including pursuing mutually contradictory goals. Finally, these rules facilitate cooperation among strangers based on certain broadly shared values that are simpler than the values actually motivating many people when they participate. Compared to human beings, spontaneous orders are “value-thin.”\n\nIn netwar, say Rand theorists John Arquilla and David Ronfeldt,\n\nmany small units “already know what they must do”, and are aware that “they must communicate with each other not in order to prepare for action, but only as a consequence of action, and, above all, through action.”\n\nFar from submerging “individual authorial voice” in the “collective,” as Jaron Lanier and Mark Helprin claim, stigmergy synthesizes the highest realizations of both individualism and collectivism, and represents each of them in its most completely actualized form, without qualifying or impairing either in any way. Michel Bauwens uses the term “cooperative individualism”:\n\nthis turn to the collective that the emergence of peer to peer represents does not in any way present a loss of individuality, even of individualism. Rather it ‘transcends and includes’ individualism and collectivism in a new unity, which I would like to call ‘cooperative individualism’. The cooperativity is not necessarily intentional (i.e. the result of conscious altruism), but constitutive of our being, and the best applications of P2P, are based on this idea.\n\nStigmergy is not “collectivist” in the traditional sense, as it was understood in the days when a common effort on any significant scale required a large organization to represent the collective, and the administrative coordination of individual efforts through a hierarchy. But it is the ultimate realization of collectivism, in that it removes the transaction cost of concerted action by many individuals.\n\nIt is the ultimate in individualism because all actions are the free actions of individuals, and the “collective” is simply the sum total of individual actions. Every individual is free to formulate any innovation she sees fit, without any need for permission from the collective, and everyone is free to adopt it or not. In this regard it attains the radical democratic ideal of unanimous consent of the governed, which is never completely possible under a representative or majoritarian system. Majoritarian democracy is a lesser evil, a way to approximate as closely as possible to the spirit of unanimous consent when an entire group of people must be bound by a single decision. Stigmergy removes the need for any individual to be bound by the group will and reduces the unit of governance to the individual, fully realizing the ideal of consent.\n\nAnother remarkable thing about stigmergic coordination is that free riders are not a problem; all actions are voluntarily undertaken out of self-interest, and their service to the individuals undertaking them and to the group is not lessened by the fact that others free ride without contributing.\n\nIn the stigmergic paradigm, the common good (e.g. Wikipedia, or a network of trails and roads connecting common destinations) is gradually built up via the cooperation implicit in stigmergically coordinated actions. Free riders may profit from this common good without putting in any effort in return. However, the benefit derived from a stigmergic trace does not in general reduce the value of that trace. For example, an ant that follows a pheromone trace laid by others without adding pheromone of its own does not by that action make the pheromone trace less useful to the other ants. Similarly, a person who downloads a piece of open source software without contributing to the development of that software does not impose any burden on the software developers. Thus, in a situation of stigmergy, a free rider or “defector” does not weaken the cooperators, in contrast to situations like the Prisoners’ dilemma or Tragedy of the Commons.\n\nIn short, as Michel Bauwens describes it, “Peer production is based on the elimination of permission-asking and a shift to the self-selection of tasks.... ”\n\nA good example is Raymond’s “Bazaar” model of open-source development, as illustrated in a hypothetical case by Benkler:\n\nImagine that one person, or a small group of friends, wants a utility. It could be a text editor, photo-retouching software, or an operating system. The person or small group starts by developing a part of this project, up to a point where the whole utility—if it is simple enough—or some important part of it, is functional, though it might have much room for improvement. At this point, the person makes the program freely available to others, with its source code.... When others begin to use it, they may find bugs, or related utilities that they want to add.... The person who has found the bug.... may or may not be the best person in the world to actually write the software fix. Nevertheless, he reports the bug.... in an Internet forum of users of the software. That person, or someone else, then thinks that they have a way of tweaking the software to fix the bug or add the new utility. They then do so, just as the first person did, and release a new version of the software with the fix or the added utility. The result is a collaboration between three people—the first author, who wrote the initial software; the second person, who identified a problem or shortcoming; and the third person, who fixed it. This collaboration is not managed by anyone who organizes the three, but is instead the outcome of them all reading the same Internet-based forum and using the same software, which is released under an open, rather than proprietary, license. This enables some of its users to identify problems without asking anyone’s permission and without engaging in any transactions.\n\nNevertheless, the creation of value itself is inherent in the network as an entity—a form of network effect that is more than the sum of the individual parts. Antonio Negri’s and Michael Hardt’s discussion of value production on the commons is relevant here:\n\n.... biopolitical production is not constrained by the logic of scarcity. It has the unique characteristic that it does not destroy or diminish the raw materials from which it produces wealth. Biopolitical production puts bios to work without consuming it. Furthermore its product is not exclusive. When I share an idea or image with you, my capacity to think with it is not lessened; on the contrary, our exchange of ideas and images increases my capacities. And the production of affects, circuits of communication, and modes of cooperation are immediately social and shared.\n\nThe synergy produced by the sharing of knowledge by the network is—in both senses of the word—a property of the network.\n\nThis has had revolutionary implications for the balance of power between networks and hierarchies, and almost unimaginably empowered individuals and small groups against large organizations.\n\nIn a hierarchy, all communications between members or between local nodes must pass through a limited number of central nodes. The only communications which are allowed to pass from one member or local node to another are those which meet the standards for distribution of those who control the central nodes. Only a few nodes within a hierarchy have the power to transmit; hence the use of the phrase “one-to-many” to describe its topology. The version of local news that appears in the local newspaper under the byline of a local journalist may be far superior in relevant detail and analysis, but it is the wire service version—even if far inferior in quality—which appears in local newspapers all around the world. It is only the communications approved by the Party Secretariat that are heard by all local cells of a party.\n\nBut in a distributed network, every node has the power to transmit, and any two nodes can communicate directly with each other without passing through a central node or obtaining the approval of whoever controls that node. Instead of the individual members simply selecting who controls the central nodes, “[s]omeone makes a proposal and everyone who wishes to join in can do so. The range of the action in question will depend on the degree to which the proposal is accepted.” Majoritarian democracy is a “scarcity system” in which decisionmaking power is rivalrous: “the collective must face an either/or choice, between one filter and another, between one representative and another.” In a distributed network, on the other hand, decision-making power is non-rivalrous. Each individual’s decision affects only herself, and does not impede the ability of others to do likewise. “Even if the majority not only disagreed with a proposal, but also acted against it, it wouldn’t be able to prevent the proposal from being carried out.”\n\nIn such a universe, every collective or hierarchical decision on what to publish or not can only be conceived as an artificial generation of scarcity, a decrease in diversity, and an impoverishment for all.\n\nHardt and Negri describe the form of organization they call the “multitude”—as opposed to the monolithic “people,” the atomized “masses” and the homogeneneous “working class”—in terms that sound very much like stigmergy.\n\nThe people has traditionally been a unitary conception.... The multitude, in contrast, is many. The multitude is composed of innumerable internal differences that can never be reduced to a unity or a single identity—different cultures, races, ethnicities, genders, and sexual orientations; different forms of labor; different ways of living; different views of the world; and different desires. The multitude is a multiplicity of all these singular differences. The masses are also contrasted with the people because they too cannot be reduced to a unity or an identity. The masses certainly are composed of all types and sorts, but really one should not say that different social subjects make up the masses. The essence of the masses is indifference: all differences are submerged and drowned in the masses. All the colors of the population fade to gray.... In the multitude, social differences remain different. The multitude is many-colored, like Joseph’s magical coat. Thus the challenge posed by the concept of multitude is for a social multiplicity to manage to communicate and act in common while remaining internally different.\n\nFinally, we should also distinguish the multitude from the working class.... The multitude.... is an open, inclusive concept. It tries to capture the importance of the recent shifts in the global economy: on the one hand, the industrial working class no longer plays a hegemonic role in the global economy.... ; and on the other hand, production today has to be conceived not merely in economic terms but more generally as social production—not only the production of material goods but also the production of communications, relationships, and forms of life. The multitude is thus composed potentially of all the diverse figures of social production.... [A] distributed network such as the Internet is a good initial image or model for the multitude because, first, the various nodes remain different but are all connected in the Web, and, second, the external boundaries of the network are open such that new nodes and new relationships can always be added.\n\nThe multitude, unlike the people, in traditional political philosophy cannot rule as a sovereign power because it “is composed of a set of singularities.... whose differences cannot be reduced to sameness.” Yet “although it remains multiple, it is not fragmented, anarchical, or incoherent.”\n\nTheir description of the “common,” or background against which the multitude cooperates, is quite similar to the stigmergic medium against which individuals coordinate their actions via markers.\n\nInsofar as the multitude is neither an identity (like the people) nor uniform (like the masses), the internal differences of the multitude must discover the common that allows them to communicate and act together. The common we share, in fact, is not so much discovered as it is produced.... Our communication, collaboration and cooperation are not only based on the common, but they in turn produce the common in an expanding spiral relationship. This production of the common tends today to be central to every form of social production, no matter how locally circumscribed, and it is, in fact, the primary characteristic of the new dominant forms of labor today. Labor itself, in other words, tends through the transformations of the economy to create and be embedded in cooperative and communicative networks. Anyone who works with information or knowledge.... relies on the common knowledge passed down from others and in turn creates new common knowledge.\n\nIndeed, in their description of the swarming activity of the multitude, they appeal explicitly to the behavior of stigmergically organized termite colonies.\n\nHardt and Negri also attribute an internal tendency toward democracy to the multitude, in terms much like David Graeber’s “horizontalism.” The modern history of resistance movements displays a shift from “centralized forms of revolutionary dictatorship and command” to “network organizations that displace authority in collaborative relationships” (this was written after the rise of the Zapatistas and the Seattle movement, but before the Arab Spring or the Occupy movement). Not only do resistance movements aim at the creation of a democratic society, but also tend “to create internally, within the organizational structure, democratic relationships.”\n\nThe advantages of stigmergic organization go beyond resilience. Jean Russell coined the term “thrivability” to describe systems that are more than merely resilient.\n\nThrivability transcends survival modes, sustainability, and resilience. Thrivability embraces flow as the sources of life and joy and meaning, adds to the flow and rides the waves, instead of trying to nullify the effects. Each layer includes and also transcends the previous layer, expanding both interconnections as well as expanding system awareness as each layer hits limits and discovers that more forces are at work than can be explained within their purview.\n\nShe illustrates the distinction by contrasting descriptions of resilient and thrivable systems. Rather than simply withstanding or recovering quickly from difficulties, the thrivable organization is characterized by an “unfolding pattern of life giving rise to life”; it will “develop vigorously,” “prosper” and “flourish.” It is “antifragile”: that is, it gets better, generates and transformed when disturbed.\n\nFor a while I struggled a bit trying to picture examples of what her distinction between resilience and thrivability would mean in concrete terms. Then it hit me: stigmergic organizations are both resilient (because of distributed infrastructure and redundant pathways between nodes) and thrivable.\n\nA stigmergic organization fits her description perfectly: “invites everyone to contribute their very best to making a world that not only works, it also produces joy, delight, and awe.” The reason is that it’s organized on a modular basis, and each discrete module of work is carried out by someone who volunteered to do it because it’s something they care about (often passionately) and they were empowered to do it without waiting for anyone else’s permission. So each task in a stigmergic organization is carried out by those most interested in it. Anyone who sees an opportunity for improvement, or has a eureka moment, can immediately jump in and get their hands dirty, and doesn’t have to work at it past the point where it ceases to be a joy for them.\n\nTo the extent that progress depends on the Shoulders of Giants Effect— people building on each other’s contributions—a stigmergic organization that facilitates collaboration, and does so without enforcing any barriers (like patents and copyrights) to making use of others’ ideas or creations, is the ideal embodiment of Russell’s idea of thrivability as promoting “growth on growth.\n\nStigmergy is ideal for facilitating division of labor, with those best suited to a task selecting it for themselves. The Left—even the anarchist Left, who should know better—is plagued with the lionization of “activism” and guilt-tripping of anyone who lacks sufficient activist street cred. If your primary talent is writing or theory, according to this valuation, you’re a second-class Leftist. If you’re not “doing something”—which translates more or less into participating in demos—you’re a poser. But when viewed in light of the stigmergy paradigm, this view is just plain stupid. It makes far more sense for each person to do what she is best at, and let others make use of her contributions in whatever way is relevant to their own talents.\n\nVinay Gupta expressed this principle in a couple of tweets:\n\nNoble Saint Hexayurt does the heavy lifting, every hexayurt build makes four more likely.\n\nI cannot save people, there are too many. I can give ideas and maybe some examples, but only an idea is big enough to help everyone.\n\nExactly. The primary bottleneck in today’s world is not physical resources, but the transmission of knowledge. Why do something that I’m bad at, when the most cost-effective use of my time and talent is writing? Putting ideas together and propagating them is “doing something.”\n\nIn sum, the transition to a society organized around stigmergic coordination through self-organized networks involves an exponential increase in agility, productivity and resilience. To quote Heylighen again, “[t]his world-wide stigmergic medium is presently developing into the equivalent of a global brain able to efficiently tackle the collective challenges of society.”\n\n2. Networks vs. Hierarchies\n\nI. The Systematic Stupidity of Hierarchies\n\nThe intrusion of power into human relationships creates irrationality and systematic stupidity. As Robert Anton Wilson argued in “Thirteen Choruses for the Divine Marquis,”\n\nA civilization based on authority-and-submission is a civilization without the means of self-correction. Effective communication flows only one way: from master-group to servile-group. Any cyberneticist knows that such a one-way communication channel lacks feedback and cannot behave “intelligently.”\n\nThe epitome of authority-and-submission is the Army, and the control-and-communication network of the Army has every defect a cyberneticist’s nightmare could conjure. Its typical patterns of behavior are immortalized in folklore as SNAFU (situation normal—all fucked-up).... In less extreme, but equally nosologic, form these are the typical conditions of any authoritarian group, be it a corporation, a nation, a family, or a whole civilization.\n\nThat same theme featured prominently in The Illuminatus! Trilogy, which Wilson coauthored with Robert Shea. “.... [I]n a rigid hierarchy, nobody questions orders that seem to come from above, and those at the very top are so isolated from the actual work situation that they never see what is going on below.”\n\nA man with a gun is told only that which people assume will not provoke him to pull the trigger. Since all authority and government are based on force, the master class, with its burden of omniscience, faces the servile class, with its burden of nescience, precisely as a highwayman faces his victim. Communication is possible only between equals. The master class never abstracts enough information from the servile class to know what is actually going on in the world where the actual productivity of society occurs.... The result can only be progressive deterioration among the rulers.\n\nThis inability of those in authority to abstract sufficient information from below, and this perception of superiors by subordinates as “a highwayman,” result in the hoarding of information by those below and their use of it as a source of rents. The power differential, by creating a zero-sum relationship, renders the pyramid opaque to those at its top.\n\nRadical organization theorist Kenneth Boulding, in similar vein, noted “the way in which organizational structure affects the flow of information,”\n\nhence affects the information input into the decision-maker, hence affects his image of the future and his decisions.... There is a great deal of evidence that almost all organizational structures tend to produce false images in the decision-maker, and that the larger and more authoritarian the organization, the better the chance that its top decision-makers will be operating in purely imaginary worlds.\n\nIn his discussion of metis (i.e. distributed, situational, job-related knowledge), James C. Scott draws a connection between it and mutuality—“as opposed to imperative, hierarchical coordination”—and acknowledges his debt for the insight to anarchist thinkers like Kropotkin and Proudhon. Metis requires two-way communication between equals, where those in contact with the situation—the people actually doing the work—are in a position of equality.\n\nInterestingly, Wilson had previously noted this connection between mutuality and accurate information in “Thirteen Choruses.” He even included his own allusion to Proudhon:\n\n[Proudhon’s] system of voluntary association (anarchy) is based on the simple communication principles that an authoritarian system means one-way communication, or stupidity, and a libertarian system means two-way communication, or rationality.\n\nThe essence of authority, as he saw, was Law—that is.... , effective communication running one way only. The essence of a libertarian system, as he also saw, was Contract—that is, mutual agreement—that is, effective communication running both ways.\n\nTo call a hierarchical organization systematically stupid is just to say that it’s incapable of making effective use of the knowledge of its members; it is less than the sum of its parts. Clay Shirky quotes John Seely Brown and Paul Duguid:\n\n“What if HP knew what HP knows?” They had observed that the sum of the individual minds at HP had much more information than the company had access to, even though it was allowed to direct the efforts of those employees.\n\nBecause a hierarchical institution is unable to aggregate the intelligence of its members and bring it to bear effectively on the policy-making process, policies have unintended consequences, and different policies operate at cross-purposes with each other in unanticipated ways. And to top it all off, the transaction costs of getting information to management about the real-world consequences of its policies are prohibitive for the same reason that the transaction costs of aggregating the information required for effective policy-making in the first place were prohibitive.\n\nBut no worries. Because senior management don’t live under the effects of their policy, and subordinates are afraid to tell them what a clusterfuck they created, the CEO will happily inform the CEOs at other organizations of how wonderfully his new “best practice” worked out. And because these “competing” organizations actually exist in an oligopoly market of cost-plus and administered pricing, and share the same pathological institutional cultures, they suffer no real competitive penalty for their bureaucratic irrationality.\n\nA hierarchy is a device for telling naked emperors how great their clothes look. “Thoreau,” a professor of physics who for obvious reasons prefers to blog anonymously, describes it in the context of his interactions with an administrator:\n\nLet’s just say that there’s something we do that is .... sub-optimal. Everyone knows it is sub-optimal....\n\nI observed that what we do is sub-optimal, and we shouldn’t expand this, but she was basically pointing out that we routinely generate reports saying that it works. Yes, we do. Those reports involve pigs and lipstick. We all know this. However, she lives in a world that is based on those reports....\n\nWhen you constantly operate on the assumption that you’re going to internalize the effects of your own actions, you have an incentive to anticipate things that could go wrong. And when you make a decision, you continually revise it in response to subsequent experience. Normally functioning human beings—that is, who are in contact with our environments and not insulated from them by hierarchies—are always correcting our own courses of action.\n\nAuthority short-circuits this process: it shifts the negative consequences of decisions downward and the benefits upward, so that decision-makers operate based on a distorted cost-benefit calculus; and it blocks negative feedback so that the locus of organizational authority is subject to the functional equivalent of a psychotic break with reality.\n\nWhen policy isn’t the result of systematic stupidity, it’s an elaborate exercise in plausible deniability, so management can say “But they knew about our written policy,” when the inevitable shortcuts to compensate for deliberate understaffing and irrational interference result in a public relations disaster.\n\nThe lack of feedback means most organizations are “successful” at achieving goals that are largely artificial—goals defined primarily by the interests of their governing hierarchies, rather than by the ostensible customers or those engaged in directly serving customer needs. On the other hand, organizational structures like networks, which are based on two-way feedback between equals, result in a high rate of “failure.” As Clay Shirky puts it, open source is a threat because it outfails proprietary systems. It can experiment and fail at less cost. Because failure is more costly to a hierarchy, hierarchies are biased “in favor of predictable but substandard outcomes.”\n\nFailure also reflects the empowerment of workers and customers; most products in the corporate economy are only considered “good enough” because customers are powerless.\n\nChrystia Freeland argues the GOP establishment and its backers were so utterly convinced Obama would lose in 2012, and caught so badly off-guard by the actual outcome of the election, because of the very same kinds of information filtering and group think that prevail in the corporations they represented.\n\nBy his own definition, Romney’s single strongest qualification to become president was analytically based, managerial excellence. And if the election campaign were the test of that, and even if you were ideologically his fan, you should think it right that he lost. Now, how could it happen? My first thought was it was also the case that all the smartest guys in the room managed to lose a lot of money in 2008 and managed to convince themselves of a set of very mistaken beliefs about where the markets where going to go. It was a lot of the same people on the wrong side of both bets....\n\n.... [W]hen you’re a rich and powerful guy, it can make it hard to see reality, especially when you’re paying your campaign staff great salaries, as Romney was.\n\nTo repeat, no matter how intelligent the people staffing a large institution are as individuals, hierarchy makes their intelligence unusable. Given that the institution does not exist as a vehicle for the goals of its members, there’s no intrinsic connection between their personal motivation and their roles in the organization, and the information and agency problems of a hierarchy prevent consequences from being fully internalized by actors, individuals simply cannot be trusted with the discretion to act on their own intelligence or common sense. That’s the rationale for standardized work-rules, job descriptions, and all the rest of the Weberian model of bureaucratic rationality: because someone, somewhere might use her initiative in ways that produce results that are detrimental to the interests of the organization, you need a set of rules in place that prevent anyone from doing anything at all. Unlike networks, which treat the human brain as an asset, hierarchical rules systems treat it as a risk to be mitigated.\n\nJob descriptions and union work rules are the other side of the coin to Weberian/Taylorist work rules. Both result from hierarchy. Power, by definition, creates zero-sum relationships. Superiors attempt to externalize effort on subordinates and skim off the benefits of increased productivity for themselves; subordinates, as a result, attempt to minimize the expenditure of effort and do the minimum necessary to avoid getting fired. Both superiors and subordinates filter or hoard information of benefit to the other party, and attempt to maximize the rents from keeping each other ignorant. In this zero-sum relation, where each side can only benefit at the expense of the other, each party seeks mechanisms for limiting abuses by the other.\n\nPaul Goodman illustrated the need to impose constraints on freedom of action, and impede individual initiative in directly adopting the most common-sense and lowest-cost solutions to immediate problems, with the example of replacing a door catch in the New York public school system:\n\n.... To remove a door catch that hampers the use of a lavatory requires a long appeal through headquarters, because it is “city property.”....\n\n.... An old-fashioned type of hardware is specified for all new buildings, that is kept in production only for the New York school system.\n\nWhen the social means are tied up in such complicated organizations, it becomes extraordinarily difficult and sometimes impossible to do a simple thing directly, even though the doing is common sense and would meet with universal approval, as when neither the child, nor the parent, nor the janitor, nor the principal of the school can remove the offending door catch.\n\nA corporate hierarchy interferes with the judgment of what Friedrich Hayek called “people-on-the-spot,” and with the collection of dispersed knowledge of circumstances, in exactly the same way a state does.\n\nMost production jobs involve a fair amount of distributed, job-specific knowledge, and depend on the initiative of workers to improvise, to apply skills in new ways, in the face of events which are either totally unpredictable or cannot be fully anticipated. Rigid hierarchies and rigid work rules only work in a predictable environment. When the environment is unpredictable, the key to success lies with empowerment and autonomy for those in direct contact with the situation.\n\nHierarchical organizations are—to borrow a wonderful phrase from Martha Feldman and James March—systematically stupid. For all the same Hayekian reasons that make a planned economy unsustainable, no individual is “smart” enough to manage a large, hierarchical organization. Nobody—not Einstein, not John Galt—possesses the qualities to make a bureaucratic hierarchy function rationally. Nobody’s that smart, any more than anybody’s smart enough to run Gosplan efficiently—that’s the whole point. As Matt Yglesias put it,\n\nI think it’s noteworthy that the business class, as a set, has a curious and somewhat incoherent view of capitalism and why it’s a good thing. Indeed, it’s in most respects a backwards view that strongly contrasts with the economic or political science take on why markets work.\n\nThe basic business outlook is very focused on the key role of the executive. Good, profitable, growing firms are run by brilliant executives. And the ability of the firm to grow and be profitable is evidence of its executives’ brilliance. This is part of the reason that CEO salaries need to keep escalating—recruiting the best is integral to success. The leaders of large firms become revered figures.... Their success stems from overall brilliance....\n\nThe thing about this is that if this were generally true—if the CEOs of the Fortune 500 were brilliant economic seers—then it would really make a lot of sense to implement socialism. Real socialism. Not progressive taxation to finance a mildly redistributive welfare state. But “let’s let Vikram Pandit and Jeff Immelt centrally plan the economy—after all, they’re really brilliant!”\n\nBut in the real world, the point of markets isn’t that executives are clever and bureaucrats are dimwitted. The point is that nobody is all that brilliant.\n\nNo matter how intelligent managers are as individuals, a bureaucratic hierarchy insulates those at the top from the reality of what’s going on below, and makes their intelligence less usable. Chris Dillow describes it this way:\n\nBut why don’t firms improve with practice in the way that individuals’ musical or sporting performance improves? Here are four possible differences:\n\nWithin firms, there’s no mechanism for translating individuals’ learning, or incremental knowledge, into corporate knowledge. As Hayek said, hierarchies are terrible at using fragmentary, tacit, dispersed knowledge.\n\nJob turnover means that job-specific human capital gets lost.\n\nBosses are selected for overconfidence. But overconfidence militates against learning.\n\nIn companies, the feedback that’s necessary for improvement gets warped by adverse incentives or ego involvement. If I play a phrase or chord badly, my ears tell me to practice it more. But if a company gets some adverse feedback—falling sales, say— no-one has an incentive or desire to say “I screwed up: I’d better improve.” And formal efforts to generate feedback, such as performance reviews, often backfire.\n\nWhat I’m saying is what every methodological individualist knows: companies are not individuals writ large. The differences between them can mitigate against learning by doing.\n\nAs an institution becomes larger and experiences increased overhead and bureaucratic ossification, it simultaneously becomes more and more vulnerable to fluctuating conditions in its surrounding environment, and less able to react to them. To survive, therefore, the large institution must control its surrounding environment.\n\nThe only real solution to complexity and unpredictability, as security analyst Bruce Schneier argues, is to give discretion to those in direct contact with the situation.\n\nGood security has people in charge. People are resilient. People can improvise. People can be creative. People can develop on-the-spot solutions.... People are the strongest point in a security process. When a security system succeeds in the face of a new or coordinated or devastating attack, it’s usually due to the efforts of people.\n\nThe problem with authority relations in a hierarchy is that, given the conflict of interest created by the presence of power, those in authority cannot afford to allow discretion to those in direct contact with the situation. Systematic stupidity results, of necessity, from a situation in which a bureaucratic hierarchy must develop arbitrary metrics for assessing the skills or work quality of a labor force whose actual work they know nothing about, and whose material interests militate against remedying management’s ignorance.\n\nMost of the constantly rising burden of paperwork exists to give an illusion of transparency and control to a bureaucracy that is out of touch with the actual production process. Every new layer of paperwork is added to address the perceived problem that stuff still isn’t getting done the way management wants, despite the proliferation of paperwork saying everything has being done exactly according to orders. In a hierarchy, managers are forced to regulate a process which is necessarily opaque to them because they are not directly engaged in it. They’re forced to carry out the impossible task of developing accurate metrics to evaluate the behavior of subordinates, based on the self-reporting of people with whom they have a fundamental conflict of interest. The paperwork burden that management imposes on workers reflects an attempt to render legible a set of social relationships that by its nature must be opaque and closed to them, because they are outside of it.\n\nEach new form is intended to remedy the heretofore imperfect self-reporting of subordinates. The need for new paperwork is predicated on the assumption that compliance must be verified because those being monitored have a fundamental conflict of interest with those making the policy, and hence cannot be trusted; but at the same time, the paperwork itself relies on their self-reporting as the main source of information. Every time new evidence is presented that this or that task isn’t being performed to management’s satisfaction, or this or that policy isn’t being followed, despite the existing reams of paperwork, management’s response is to design yet another—and equally useless—form.\n\nWeberian work rules result of necessity when performance and quality metrics are not tied to direct feedback from the work process itself. They’re a metric of work for someone who is neither a creator/provider not an end user. And they are necessary—again—because those at the top cannot afford to allow those at the bottom the discretion to use their own common sense. A bureaucracy can’t afford to allow its subordinates such discretion, because someone with the discretion to do things more efficiently will also have the discretion to do something bad. And because the subordinate has a fundamental conflict of interest with the superior, and does not internalize the benefits of applying her intelligence, she can’t be trusted to use her intelligence for the benefit of the organization. In such a zero-sum relationship, any discretion can be abused.\n\nThe problem is, discretion cannot be entirely removed from any organizational process. James Scott writes that it’s impossible, by the nature of things, for everything entailed in the production process to be distilled, formalized or codified into a form that’s legible to management.\n\n.... [T]he formal order encoded in social-engineering designs inevitably leaves out elements that are essential to their actual functioning. If the [East German] factory were forced to operate only within the confines of the roles and functions specified in the simplified design, it would quickly grind to a halt. Collectivized command economies virtually everywhere have limped along thanks to the often desperate improvisation of an informal economy wholly outside its schemata.\n\nStated somewhat differently, all socially engineered systems of formal order are in fact subsystems of a larger system on which they are ultimately dependent, not to say parasitic. The subsystem relies on a variety of processes—frequently informal or antecedent—which alone it cannot create or maintain. The more schematic, thin, and simplified the formal order, the less resilient and the more vulnerable it is to disturbances outside its narrow parameters....\n\nIt is, I think, a characteristic of large, formal systems of coordination that they are accompanied by what appear to be anomalies but on closer inspection turn out to be integral to that formal order. Much of this might be called “metis to the rescue.... ” A formal command economy.... is contingent on petty trade, bartering, and deals that are typically illegal.... In each case, the nonconforming practice is an indispensable condition for formal order.\n\n.... In each case, the necessarily thin, schematic model of social organization and production animating the planning was inadequate as a set of instructions for creating a successful social order. By themselves, the simplified rules can never generate a functioning community, city, or economy. Formal order, to be more explicit, is always and to some considerable degree parasitic on informal processes, which the formal scheme does not recognize, without which it could not exist, and which it alone cannot create or maintain.\n\nAnd as I keep trying to hammer home, just the reverse is true of networks and stigmergic organization: their beauty is that they render the intelligence of all their individual members more usable. While one-way communication creates opacity from above, two-way communication creates horizontal legibility. To quote Michel Bauwens:\n\nThe capacity to cooperate is verified in the process of cooperation itself. Thus, projects are open to all comers provided they have the necessary skills to contribute to a project. These skills are verified, and communally validated, in the process of production itself. This is apparent in open publishing projects such as citizen journalism: anyone can post and anyone can verify the veracity of the articles. Reputation systems are used for communal validation. The filtering is a posteriori, not a priori. Anti-credentialism is therefore to be contrasted to traditional peer review, where credentials are an essential prerequisite to participate.\n\nP2P projects are characterized by holoptism. Holoptism is the implied capacity and design of peer to [peer] processes that allows participants free access to all the information about the other participants; not in terms of privacy, but in terms of their existence and contributions (i.e. horizontal information) and access to the aims, metrics and documentation of the project as a whole (i.e. the vertical dimension). This can be contrasted to the panoptism which is characteristic of hierarchical projects: processes are designed to reserve ‘total’ knowledge for an elite, while participants only have access on a ‘need to know’ basis. However, with P2P projects, communication is not top-down and based on strictly defined reporting rules, but feedback is systemic, integrated in the protocol of the cooperative system.\n\nIn a prison—governed by panopticism—the warden can see all the prisoners, but the prisoners can’t see each other. The reason is so the prisoners can’t coordinate their actions independently of the warden. Holopticism is the exact opposite: the members of a group are horizontally legible to one another, and can coordinate their actions. And “everyone has a sense of the emerging whole, and can adjust their actions for the greatest fit.”\n\nThe unspoken assumption is that a hierarchy exists for the purposes of the management, and a holoptic association exists for the purposes of its members. The people at the top of a hierarchical pyramid can’t trust the people doing the job because their interests are diametrically opposed. It’s safe to trust one another in a horizontal organization because a common interest in the task can be inferred from participation.\n\nII. Hierarchies vs. Networks\n\nIn a distributed network, it’s impossible to prevent communication between nodes by controlling a central node. There are too many alternative nodes through which communication can be routed if any particular node or nodes are closed off. As John Gilmore famously quipped, “the Internet treats censorship as damage and routes around it.”\n\nThe power of distributed networks lies in the fact that in them filters disappear: eliminating or filtering a node or node cluster will not delay access to information. By contrast with the decentralised information system which arose with the invention of the telegraph, in distributed networks it is impossible to “burn bridges” and restrict the information that reaches the final nodes by controlling a few transmitters.\n\nAs Ori Brafman and Rod Backstrom describe it, “when attacked, a decentralized organization tends to become even more open and decentralized.” They use the example of the file-sharing movement. After Napster was shut down, the movement responded by creating a series of successors—each of which was even more decentralized and presented even less in the way of vulnerable nodes than its predecessor.\n\nThat’s the subject of Francesca Musiani’s article on the history of p2p filesharing architecture, which she argues has been shaped by the offensive-defensive arms race between the forces of state surveillance and those of circumvention. The first generation of file-sharing services, typified by Napster, were centralized, one-to-many systems. Subsequent services became increasingly decentralized— although their weak point remained imperfect anonymity. The third stage, Musiani argues, is file-sharing under cover of darknets, with membership by invitation only on a “friend-of-a-friend” basis. Although such organization through conventional, proprietary social networking services like Facebook is still vulnerable to the vagaries of their privacy policy, open-source social networking services like Diaspora are much more promising as avenues for darknet file-sharing.\n\n“The Pirate Bay,” Rick Falkvinge writes, “has been a trailblazer in resilience. After all, a number of bought-and-paid-for or just plain misguided legislatures and courts have tried to eradicate the site, and yet, it still stands untouched.” One source of its resilience—as is the case with Wikileaks (see below)—is its lack of dependence on servers that are vulnerable to the laws of any particular country. Like Wikileaks, The Pirate Bay has access to a network of servers in a number of countries; and it responds to shutdown attempts by nimbly switching its Web-hosting to servers in other countries (most recently the servers of the Norwegian and Catalan Pirate Parties as of this writing).\n\nThe ultimate step so far for file-sharing operations has been to bypass sitehosting as a bottleneck altogether and move into the cloud. The Pirate Bay released its software code so that it could be replicated by anyone who wanted to host a Pirate Bay clone.\n\nEarlier this year [2012], after months of legal wrangling, authorities in a number of countries won an injunction against the Pirate Bay, probably the largest and most famous BitTorrent piracy site on the Web. The order blocked people from entering the site.\n\nIn retaliation, the Pirate Bay wrapped up the code that runs its entire Web site, and offered it as a free downloadable file for anyone to copy and install on their own servers. People began setting up hundreds of new versions of the site, and the piracy continues unabated.\n\nThus, whacking one big mole created hundreds of smaller ones.\n\nAnd Tribler moves file-sharing in a literal peer-to-peer direction.\n\nThe new software called “Tribler” is the new weapon in the battle for Internet liberty and does not need a website to track users sharing torrent files.\n\nAccording to The Raw Story, it is a “peer-to-peer network protocol that enables computers to share files with thousands of others.”\n\nFor many this could be the solution movie....\n\nWhile lawmakers are dreaming of a censored web, many believe Tribler will be a true nightmare for them.\n\nAccording to the technology blog Torrent Freak, the attempt to disconnect users from the Internet for “illegal” purposes will be foiled by the software that has been in the works for the past five years and will make it nearly “impossible” to stop file sharing.\n\n“The only way to take it down is to take the Internet down,” stated Doctor Pouwelse of Delft University of Technology to the Daily Mail.\n\nTribler will be entirely decentralized, leaving the control in the hands of the users.\n\n“Individuals can rename files, flag phony downloads or viruses, create ‘channels’ of verified downloads, and act as nodes that distribute lists of peers across the network,” The Raw Story reported.\n\nMore recently, the clumsy attempts of the U.S. government and its allies to suppress Wikileaks through control of strategic nodes (domain name registries, Amazon, PayPal, etc.) have made the same principle abundantly clear. Wikileaks’ enemies have strategized against it within the paradigm of a Weberian bureaucratic institution functioning inside a Westphalian nation-state. Will Wilkinson mocked the sheer idiocy of people like Joe Lieberman—and all the clucking chickenhawks in the neocon blogosphere calling for Chelsea Manning or Ju"
    }
}
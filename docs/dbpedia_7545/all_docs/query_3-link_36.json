{
    "id": "dbpedia_7545_3",
    "rank": 36,
    "data": {
        "url": "https://arxiv.org/html/2403.05696v1",
        "read_more_link": "",
        "language": "en",
        "title": "SeeGULL Multilingual: a Dataset of Geo-Culturally Situated Stereotypes",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://arxiv.org/html/extracted/5458687/figures/table1.png",
            "https://arxiv.org/html/extracted/5458687/figures/table2.png",
            "https://arxiv.org/html/extracted/5458687/figures/geo_chart_mean_offensive_score.png",
            "https://arxiv.org/html/extracted/5458687/figures/table3.png",
            "https://arxiv.org/html/x1.png",
            "https://arxiv.org/html/extracted/5458687/figures/plot_full_v8_feb15th.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "HTML conversions sometimes display errors due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.\n\nfailed: spverbatim\n\nAuthors: achieve the best HTML results from your LaTeX submissions by following these best practices.\n\nLicense: CC BY 4.0\n\narXiv:2403.05696v1 [cs.CL] 08 Mar 2024\n\nSeeGULL Multilingual: a Dataset of Geo-Culturally Situated Stereotypes\n\nMukul Bhutani\n\nGoogle Research\n\nmukulbhutani@google.com\n\n&Kevin Robinson\n\nGoogle Research\n\nkevinrobinson@google.com\n\n\\ANDVinodkumar Prabhakaran\n\nGoogle Research\n\nvinodkpg@google.com\n\n&Shachi Dave\n\nGoogle Research\n\nshachi@google.com\n\n&Sunipa Dev\n\nGoogle Research\n\nsunipadev@google.com\n\nAbstract\n\nWhile generative multilingual models are rapidly being deployed, their safety and fairness evaluations are largely limited to resources collected in English. This is especially problematic for evaluations targeting inherently socio-cultural phenomena such as stereotyping, where it is important to build multi-lingual resources that reflect the stereotypes prevalent in respective language communities. However, gathering these resources, at scale, in varied languages and regions pose a significant challenge as it requires broad socio-cultural knowledge and can also be prohibitively expensive. To overcome this critical gap, we employ a recently introduced approach that couples LLM generations for scale with culturally situated validations for reliability, and build SeeGULL Multilingual, a global-scale multilingual dataset of social stereotypes, containing over 25K stereotypes, spanning 20 languages, with human annotations across 23 regions, and demonstrate its utility in identifying gaps in model evaluations. Content warning: Stereotypes shared in this paper can be offensive.\n\n\\setitemize\n\nnoitemsep,topsep=0pt,parsep=0pt,partopsep=0pt\n\nSeeGULL Multilingual: a Dataset of Geo-Culturally Situated Stereotypes\n\nMukul Bhutani Google Research mukulbhutani@google.com Kevin Robinson Google Research kevinrobinson@google.com\n\nVinodkumar Prabhakaran Google Research vinodkpg@google.com Shachi Dave Google Research shachi@google.com Sunipa Dev Google Research sunipadev@google.com\n\n1 Introduction\n\nGenerative multilingual models Brown et al. (2020); Chowdhery et al. (2022); Anil et al. (2023) have gained popular usage in the recent years due to their gradually increased functionalities across languages, and applications. However, there has been a severe lack in cross cultural considerations in these models, specifically when it comes to evaluations of their safety and fairness Sambasivan et al. (2021). These evaluations have been known to be largely restricted to Western viewpoints Prabhakaran et al. (2022), and typically only the English language Gallegos et al. (2023). This is inherently problematic as it promotes a unilateral narrative about fair and safe models that is decoupled from cross cultural perspectives Arora et al. (2023); Zhou et al. (2023). It also creates harmful, unchecked effects including model safeguards breaking down when encountered by simple multilingual adversarial attacks Yong et al. (2024).\n\nAs language and culture are inherently intertwined, it is imperative that model safety evaluations are both multilingual and multicultural Hovy and Yang (2021). In particular, preventing the propagation of stereotypes – that can lead to potential downstream harms Shelby et al. (2023) – is crucially tied to geo-cultural factors Hinton (2017). Yet, most sizeable stereotype evaluation resources are limited to the English language Nadeem et al. (2021); Nangia et al. (2020). While some efforts have created resources in languages other than English Névéol et al. (2022), they are limited to specific contexts. On the other hand, some approaches such as by Jha et al. (2023) have global coverage of stereotype resources but are restricted to the English language alone. Consequently, they fail to capture uniquely salient stereotypes prevalent in different languages of the world, as simply translating them to other languages will lose out on cultural relevance Malik et al. (2022).\n\nIn this work, we address this critical gap by employing the SeeGULL (Stereotypes Generated Using LLMs in the Loop) approach Jha et al. (2023) to build a broad-coverage multilingual stereotype resource: SeeGULL Multilingual. It covers 20 languages across 23 regions they are commonly used in. It contains a total of 25,861 stereotypes about 1,190 identity groups, and captures nuances of differing offensiveness in different global regions. We make this dataset publicly available to foster research in this domain. We also demonstrate the utility of this dataset in testing model safeguards.\n\n2 Dataset Creation Methodology\n\nStereotypes are generalizations made about the identity (id) of a person, such as their race, gender, or nationality, typically through an association with some attribute (attr) that indicates competence, behaviour, profession, etc. Quinn et al. (2007); Koch et al. (2016). In this work we create a multilingual and multicultural dataset of stereotypes associated with nationality and region based identities of people. We use the methodology established by Jha et al. (2023), which is constituted primarily of three steps: (i) identifying relevant identity terms, (ii) prompting a generative model in a few-shot setting to produce similar candidate associations for identity terms from (i), and finally (iii) procuring socially situated human validations for those candidate associations.\n\n2.1 Identifying Salient Identity Terms\n\nSalient identities and stereotypes can vary greatly across languages and countries of the world, and a multilingual stereotype dataset needs to reflect this diversity. To reliably create the dataset at scale, we scope and collect stereotypes only about national, and local regional identities.\n\nNationality based demonyms:\n\nWe use a list of 179 nationality based demonyms in English, and translate them to target languages. In languages such as Spanish, Italian, and Portuguese, where demonyms are gendered (e.g., Bolivian in English can be Boliviano (masculine) or Boliviana (feminine) in Italian), we use all gendered versions.\n\nRegional demonyms\n\nWe source regional demonyms (such as Californians, Parisians, etc.) within each country from established online sources in respective languages (see A.8 for details). A lot of these demonyms are present only in the respective target language without any English translation, such as the Dutch demonym Drenten, and the Turkish demonym Hakkârili.\n\n2.2 Generating Associations\n\nTo generate associations in different languages, we use PaLM-2 Anil et al. (2023), which is a generative language model trained on large multilingual text across hundreds of languages. Using few shot examples of stereotypes from existing datasets Nadeem et al. (2021); Klineberg (1951), we instruct the model to produce candidate tuples in the format (id, attr) Jha et al. (2023). The template Complete the pairs: (id11{}_{1}start_FLOATSUBSCRIPT 1 end_FLOATSUBSCRIPT, attr11{}_{1}start_FLOATSUBSCRIPT 1 end_FLOATSUBSCRIPT)(id22{}_{2}start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPT, attr22{}_{2}start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPT)(id33{}_{3}start_FLOATSUBSCRIPT 3 end_FLOATSUBSCRIPT, translated in different languages is used to prompt the model. The generated text gives us a large volume of salient candidate associations.\n\n2.3 Culturally Situated Human Annotations\n\nAssociations generated in steps so far need to be grounded in social context of whether they are indeed stereotypical. Annotators were diverse in gender, and compensated above prevalent market rates (more details and annotation instructions in A.3).\n\nStereotype Annotations.\n\nThree annotations are collected for each candidate tuple in their respective language. The tuples are also annotated in country specific manner, i.e., French tuples are annotated by French users in France specifically. We adopt this approach since region of annotator residence impacts socially subjective tasks like stereotype annotations Davani et al. (2022). In addition, for languages that are common in multiple countries, we get separate annotations in each country (e.g., Spanish in Spain and Spanish in Mexico).\n\nOffensiveness Annotations.\n\nFor each stereotype in our dataset, we estimate how offensive it is. We do so by obtaining three in-language, globally situated annotations for each attribute term in the dataset on its degree of offensiveness on a Likert scale of ‘Not offensive’ to ‘Extremely Offensive’.\n\n4 SGM for Analysis and Evaluations\n\n4.1 Offensive Stereotypes in SGM\n\nWhile all stereotypes can have negative downstream impacts, some associations that imply degeneracy and criminality are especially offensive. Aggregating over stereotypes about nationalities across all languages in SGM, we note how Albania and Rwanda have some of the most offensive stereotypes associated with them, while Singapore and Canada have the least offensive stereotypes associated (A.4). Figure 3 shows the aggregated offensiveness associated with different countries of the world.\n\nFigure 4 showcases some examples of highly offensive stereotypes associated with different national and regional identities (also A.4).\n\nThe perception of an attribute or stereotype as offensive or not can vary by language, and geo-culture Zhou et al. (2023). So we also aggregate over the individual languages, and observe that Italian and Swahili have the most number of offensive stereotypes with about 22% of all stereotypes for these languages being marked as “Somewhat Offensive\" or more. On the other extreme, Hindi (1.83%) and Korean (2.66 %) are the languages having the least fraction of offensive stereotypes (full list in A.4 Table 4) .\n\n4.2 Foundation Model Evaluations with SGM\n\nEvaluating stereotyping by multilingual language models is challenging due to paucity of resources in languages apart from English. SGM enables us to create an evaluation set measuring whether a system endorses stereotypical associations across a socially situated, globally localized, and more comprehensive set of stereotypes, and whether the extent of endorsing stereotypes differs by language.\n\nWe adapt evaluation methods for measuring bias in inference capabilities Dev et al. (2020); Parrish et al. (2022) to create the evaluation of foundation models depicted in Figure 5. Each question in the task contains only one stereotypical answer, with other identity terms randomly sampled. We create an evaluation set from stereotypes in SGM to create 4,600 questions, drawing 100 samples across each language, region, and demonym type.\n\nWe evaluate four different models: PaLM 2, GPT-4 Turbo, Gemini Pro, and Mixtral 8X7B. We observe that all models endorse stereotypes present in SGM, and at different rates when the same queries are asked in English (Table 1). We note that PaLM 2 has the highest rate of endorsement, while Mixtral demonstrate the lowest. Our results also show that English-translated queries would have missed a significant fraction of stereotype endorsements in three out of four models. Figure 6 also notes that models tend to endorse stereotypes present in different languages at different rates. These findings further underlines the need for the forms of multilingual evaluation enabled by SGM.\n\n5 Conclusion\n\nFor holistic safety evaluations of multilingual models, English-only resources or their translations are not sufficient. This work introduces a large scale, multilingual, and multicultural stereotype resource covering a wide range of global identities. It also exposes how these stereotypes may percolate unchecked into system output, due to the prevalent lack of coverage. In considerations of model safety, cross cultural perspectives on stereotypes, their offensiveness, and potential harms must be included. We encourage future work to explore other methods to utilize SGM to measure expressions of representational harms and stereotypes within application-specific contexts for global users.\n\nLimitations\n\nThe dataset created in this work is constrained by the resources needed to create large scale, quality data. The dataset covers 20 languages and not the full range of many thousands of languages and dialects used across the world. Unfortunately, generation quality of most models is limited to few languages currently which guide our methodology. Further, we obtain annotations from 23 countries, whereas it could be from a much larger set given the spread of the 20 languages. This is constrained both by the availability of annotators and the cost of data annotations. Next, we limit the identity terms of recorded stereotypes to be demonyms associated with nationalities and regions within each nation. We also limit the granularity with which regions are considered, and also don’t include regions within all countries at a global scale. These are design choices for reliably collecting stereotypes at scale, guided by how stereotypes are socio-culturally situated Jha et al. (2023); Hovy and Yang (2021). While this helps create a dataset that is grounded in local knowledge, there are other stereotypes at other levels of granularities, and about other identities that are not covered by this work. We hope that this work acts as a foundation, based on which larger, multilingual safety datasets can be built.\n\nEthical Considerations\n\nWe emphasize that this dataset does not capture all possible stereotypes about any identity, or stereotypes about all geocultural identities. Thus, this dataset should not be used alone to categorize any model or its output as completely devoid of stereotypes. Instead careful considerations should be made by dataset users depending on the intended application. Further, we explicitly call out the intended usage of this dataset for evaluation purposes in the attached Data Card (A.1). This dataset contains a large number of stereotypes which can help build model safeguards. We caution users against unintentional, or malicious misuse.\n\nReferences\n\nAnil et al. (2023) Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, et al. 2023. Palm 2 technical report.\n\nArora et al. (2023) Arnav Arora, Lucie-aimée Kaffee, and Isabelle Augenstein. 2023. Probing pre-trained language models for cross-cultural differences in values. In Proceedings of the First Workshop on Cross-Cultural Considerations in NLP (C3NLP), pages 114–130, Dubrovnik, Croatia. Association for Computational Linguistics.\n\nBai et al. (2023) Yanhong Bai, Jiabao Zhao, Jinxin Shi, Tingjiang Wei, Xingjiao Wu, and Liang He. 2023. Fairmonitor: A four-stage automatic framework for detecting stereotypes and biases in large language models.\n\nBhatt et al. (2022) Shaily Bhatt, Sunipa Dev, Partha Talukdar, Shachi Dave, and Vinodkumar Prabhakaran. 2022. Re-contextualizing fairness in nlp: The case of india. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing, pages 727–740.\n\nBrown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901.\n\nChowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.\n\nDavani et al. (2022) Aida Mostafazadeh Davani, Mark Díaz, and Vinodkumar Prabhakaran. 2022. Dealing with disagreements: Looking beyond the majority vote in subjective annotations. Transactions of the Association for Computational Linguistics, 10:92–110.\n\nDev et al. (2023) Sunipa Dev, Akshita Jha, Jaya Goyal, Dinesh Tewari, Shachi Dave, and Vinodkumar Prabhakaran. 2023. Building stereotype repositories with complementary approaches for scale and depth. In Proceedings of the First Workshop on Cross-Cultural Considerations in NLP (C3NLP), pages 84–90, Dubrovnik, Croatia. Association for Computational Linguistics.\n\nDev et al. (2020) Sunipa Dev, Tao Li, Jeff M. Phillips, and Vivek Srikumar. 2020. On measuring and mitigating biased inferences of word embeddings. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):7659–7666.\n\nGallegos et al. (2023) Isabel O. Gallegos, Ryan A. Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen K. Ahmed. 2023. Bias and fairness in large language models: A survey.\n\nGemini Team Google (2023) Gemini Team Google. 2023. Gemini: A family of highly capable multimodal models. arXiv preprint arXiv:2312.11805.\n\nGoogle (2024a) Google. 2024a. Configure safety attributes | vertex ai | google cloud.\n\nGoogle (2024b) Google. 2024b. Configure safety settings for the palm api | vertex ai | google cloud.\n\nHinton (2017) Perry Hinton. 2017. Implicit stereotypes and the predictive brain: cognition and culture in “biased” person perception. Palgrave Communications, 3(1):1–9.\n\nHovy and Yang (2021) Dirk Hovy and Diyi Yang. 2021. The importance of modeling social factors of language: Theory and practice. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 588–602, Online. Association for Computational Linguistics.\n\nJha et al. (2023) Akshita Jha, Aida Mostafazadeh Davani, Chandan K Reddy, Shachi Dave, Vinodkumar Prabhakaran, and Sunipa Dev. 2023. SeeGULL: A stereotype benchmark with broad geo-cultural coverage leveraging generative models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 9851–9870, Toronto, Canada. Association for Computational Linguistics.\n\nJha et al. (2024) Akshita Jha, Vinodkumar Prabhakaran, Remi Denton, Sarah Laszlo, Shachi Dave, Rida Qadri, Chandan K. Reddy, and Sunipa Dev. 2024. Beyond the surface: A global-scale analysis of visual stereotypes in text-to-image generation.\n\nKlineberg (1951) Otto Klineberg. 1951. The scientific study of national stereotypes. International social science bulletin, 3(3):505–514.\n\nKoch et al. (2016) Alex Koch, Roland Imhoff, Ron Dotsch, Christian Unkelbach, and Hans Alves. 2016. The abc of stereotypes about groups: Agency/socioeconomic success, conservative–progressive beliefs, and communion. Journal of personality and social psychology, 110(5):675.\n\nMalik et al. (2022) Vijit Malik, Sunipa Dev, Akihiro Nishi, Nanyun Peng, and Kai-Wei Chang. 2022. Socially aware bias measurements for Hindi language representations. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1041–1052, Seattle, United States. Association for Computational Linguistics.\n\nMistral AI (2024) Mistral AI. 2024. Endpoints | mistral ai large language models.\n\nMistral AI (2024) Mistral AI. 2024. Guardrailing | mistral ai large language models.\n\nNadeem et al. (2021) Moin Nadeem, Anna Bethke, and Siva Reddy. 2021. Stereoset: Measuring stereotypical bias in pretrained language models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 5356–5371.\n\nNagireddy et al. (2023) Manish Nagireddy, Lamogha Chiazor, Moninder Singh, and Ioana Baldini. 2023. Socialstigmaqa: A benchmark to uncover stigma amplification in generative language models.\n\nNangia et al. (2020) Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel Bowman. 2020. Crows-pairs: A challenge dataset for measuring social biases in masked language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1953–1967.\n\nNévéol et al. (2022) Aurélie Névéol, Yoann Dupont, Julien Bezançon, and Karën Fort. 2022. French CrowS-pairs: Extension à une langue autre que l’anglais d’un corpus de mesure des biais sociétaux dans les modèles de langue masqués (French CrowS-pairs : Extending a challenge dataset for measuring social bias in masked language models to a language other than English). In Actes de la 29e Conférence sur le Traitement Automatique des Langues Naturelles. Volume 1 : conférence principale, pages 355–364, Avignon, France. ATALA.\n\nOpenAI et al. (2023) OpenAI, :, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, and Sam Altman et. al. 2023. Gpt-4 technical report.\n\nParrish et al. (2022) Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu Mon Htut, and Samuel Bowman. 2022. Bbq: A hand-built bias benchmark for question answering. In Findings of the Association for Computational Linguistics: ACL 2022, pages 2086–2105.\n\nPrabhakaran et al. (2022) Vinodkumar Prabhakaran, Rida Qadri, and Ben Hutchinson. 2022. Cultural incongruencies in artificial intelligence.\n\nQuinn et al. (2007) Kimberly A Quinn, C Neil Macrae, and Galen V Bodenhausen. 2007. Stereotyping and impression formation: How categorical thinking shapes person perception. 2007) The Sage Handbook of Social Psychology: Concise Student Edition. London: Sage Publications Ltd, pages 68–92.\n\nSambasivan et al. (2021) Nithya Sambasivan, Erin Arnesen, Ben Hutchinson, Tulsee Doshi, and Vinodkumar Prabhakaran. 2021. Re-imagining algorithmic fairness in india and beyond. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT ’21, page 315–328, New York, NY, USA. Association for Computing Machinery.\n\nShelby et al. (2023) Renee Shelby, Shalaleh Rismani, Kathryn Henne, AJung Moon, Negar Rostamzadeh, Paul Nicholas, N’Mah Yilla-Akbari, Jess Gallegos, Andrew Smart, Emilio Garcia, et al. 2023. Sociotechnical harms of algorithmic systems: Scoping a taxonomy for harm reduction. In Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society, pages 723–741.\n\nSólmundsdóttir et al. (2022) Agnes Sólmundsdóttir, Dagbjört Guðmundsdóttir, Lilja Björk Stefánsdóttir, and Anton Ingason. 2022. Mean machine translations: On gender bias in Icelandic machine translations. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 3113–3121, Marseille, France. European Language Resources Association.\n\nVashishtha et al. (2023) Aniket Vashishtha, Kabir Ahuja, and Sunayana Sitaram. 2023. On evaluating and mitigating gender biases in multilingual settings. In Findings of the Association for Computational Linguistics: ACL 2023, pages 307–318, Toronto, Canada. Association for Computational Linguistics.\n\nYong et al. (2024) Zheng-Xin Yong, Cristina Menghini, and Stephen H. Bach. 2024. Low-resource languages jailbreak gpt-4.\n\nZhou et al. (2023) Li Zhou, Laura Cabello, Yong Cao, and Daniel Hershcovich. 2023. Cross-cultural transfer learning for Chinese offensive language detection. In Proceedings of the First Workshop on Cross-Cultural Considerations in NLP (C3NLP), pages 8–15, Dubrovnik, Croatia. Association for Computational Linguistics.\n\nAppendix A Appendix\n\nA.1 Dataset\n\nThe dataset contains 25,861 annotated stereotypes across 23 language +++ countries of annotation combination (Table 2), and is available online . The first two columns of Table 7 describes the languages, countries (of annotations), and the total annotations that are being released as part of this dataset. Since data disagreements are features of subjective data Davani et al. (2022), we consider any associations with at least 1 annotation (of 3 annotators) as stereotype to be sufficient for the tuple to be included in the published dataset. The filtering of the data for usage is left to the user. The data card detailing intended usage, data collection and annotation, costs, etc. is also made available online .\n\nTable 3 shows the distribution of tuples across the nationality and regional axis. Of the 25,861 annotated tuples, 19,543 stereotypes have unique English translations (via Google Translate API). The differences arises due to the fact that we, by design, get a few tuples annotated in two different countries speaking the same language (section 3 and A.6). Finally, stereotypes having different gender based identity terms but with same attributes (e.g (mauritana, árabe) and (mauritanos, árabe)) are back-translated to English in exact same way and are thus counted as such.\n\nA.2 Related Stereotype Resources\n\nStereotype resources are essential for generative model evaluations, and a large body of work pushes to increase the overall coverage of these resources Nadeem et al. (2021); Nangia et al. (2020); Jha et al. (2023). These resources help significantly bolster model safeguards Nagireddy et al. (2023); Bai et al. (2023); Jha et al. (2024). Thus, it is imperative that the resources cover global identities, to enable models across modalities and languages to be safe and beneficial for all. There have been attempts to increase these resources across languages Névéol et al. (2022); Sólmundsdóttir et al. (2022); Vashishtha et al. (2023), and cultures Bhatt et al. (2022); Dev et al. (2023). However, due to the cost of curating, these resources are often limited in both size, and global coverage. In this work, we address these challenges by leveraging social information captured and generated by multilingual models and globally situated annotations.\n\nA.3 Annotation Details\n\nWe get annotations from humans for two different task. The first task, called Stereotype Annotation is used to determine if an (identity, attribute) tuple is considered as stereotypical or not. The second task, Offensive Annotation is for determining the offensiveness of a given attribute in a particular region. All annotations were procured through a partner vendor who handled the recruitment, obtained informed consent, and provided clean, anonymous ratings within each task.\n\nAnnotators were recruited such that they used the target language and resided in the country of popular usage. They were also selected such that every data point was annotated by at least one non-male identifying person. Annotators were paid above prevalent market rates and respecting local minimum wage laws in respective countries.\n\nA.3.1 Task: Stereotype Annotation\n\nAnnotators: Since we documenting stereotypes in a multilingual setting, our annotation tasks are in different target languages depending on the region the task is being conducted in. Annotators are proficient in English reading and writing, as well as in the target language that the data is in. Moreover, since we have regional identities corresponding to each region, we ensure that annotators are people living in that region. Finally, as described in section 3, for the languages of Spanish, Portuguese, and Bengali, we perform human annotations separately in two different regions each.\n\nLabels: The annotators had the options to select between three labels.\n\n•\n\nNon-Stereotype (NS)\n\n•\n\nStereotype (S)\n\n•\n\nUnsure (U) (with justification)\n\nThe label Unsure captures the case where either the annotator does not have full confidence, or if the tuple is factual in nature.\n\nA.3.2 Task: Offensiveness Annotation\n\nAnnotators: The annotation task is in different target languages per dataset. Annotators are proficient in English reading and writing, as well as in the target language that the data is in. The region that the raters are from is also different per data segment (for example, Spanish users in Mexico, Spanish users in Spain, Bengali users in Bangladesh, etc.). Each data point (attribute term) is rated by 3 annotators of diverse gender identities.\n\nLabels: The annotators need to label each attribute as one of the following on a Likert scale.\n\n•\n\nNot Offensive (NO) (-1): if the attribute is not perceived as offensive at all, or is positive or neutral such as ’good’, ’okay’, etc in English. Consider similar terms in the language the data is in for this task.\n\n•\n\nSlightly Offensive (O1) (+1)\n\n•\n\nSomewhat Offensive (O2) (+2)\n\n•\n\nModerately Offensive (O3) (+3)\n\n•\n\nExtremely Offensive (O4) (+4): if the attribute is perceived as extremely offensive when associated with the identity (like female, Russian, Asian, Jew, etc.) of a person such as attributes implying criminal behavior and activities, such as ’criminal’, ’terrorist’, etc in English. Consider similar terms in the language the data is in for this task.\n\n•\n\nUnsure (with justification) (U) (0): if the annotator is not sure about if the attribute is offensive.\n\nThe answers can vary from Extremely offensive to Not offensive. The integers from (-1) to (+4) are used for calculating the mean offensiveness of an attribute and are not visible to the annotators.\n\nA.4 Offensiveness\n\nFor all the stereotypes in SeeGULL Multilingual, we also get the offensive annotations of the corresponding attributes on Likert scale. For all the attributes, we average out the offensiveness annotations by the three annotators and call it the \"mean offensiveness\" score.\n\nTable 4 shows the percentage of stereotypes that are annotated as \"Somewhat offensive (O2)\" or higher, per region.\n\nFinally, stereotypes in SeeGULL Multilingual can be thought of either belonging having a nationality based demonym or a regional (within a country) based demonym. For all the nationality based demonyms in SGE, we group them based on their corresponding countries and get an average of offensiveness scores associated with them. Table 5 shows the top 20 countries which have the most offensive stereotypes associated with them. Similarly, the table 6 lists out the countries having the least offensive stereotypes associated with them.\n\nA.5 Overlap with English SeeGULL\n\nSeeGULL Multilingual dataset contain a total of 25,861 stereotypes out of which a total of 2370 stereotypes (949 unique stereotypes) were overlapping with SGE. Thus, about 5% of unique stereotypes in SeeGULL Multilingual overlap with SGE. The Table 7 shows the overlap of SGE with SeeGULL Multilingual corresponding to each of the 23 language + country combinations.\n\nA.6 Stereotypes in a Language across Countries\n\nA few languages are spoken across different countries in the world. These countries, that may share the same language, due to different socio-cultural backgrounds, can have a different notions of what is considered a stereotype. Table 8 quantitatively demonstrates how much annotations vary across countries\n\nA.7 Foundation Model Evaluations\n\nA.7.1 Creating the Evaluation set\n\nTo create the evaluation set, we create a balanced sample across country, language, and regional or international demonyms. Within each bucket, we take all attributes (e.g., orderly) where we could also create three distracting demonyms that do not also share an association with that same attribute. From there, we first sample attributes, then sample from potential distracting demonyms for that attribute. We randomize the demonyms to form a question item. To encode each question item into a prompt, we first substitute the attribute (in the target language) into the English instruction prefix. Then, we separately translate the prefix into the target language, as well as a suffix instruction. Finally, we take those translations and merge them with the SeeGULL Multilingual demonyms (which are already in the target language) into the prompt for the evaluation set. We create parallel English-language prompts using the same sample of question items. To encode questions into English prompts, we use the same instructions and process but without translation, using the English demonyms and attributes from the SeeGULL Multilingual dataset.\n\nA.7.2 Multilingual capabilities of Models\n\nFoundation models have varying multilingual capabilities across languages. For example, the underlying PaLM 2 language model was trained on hundreds of languages Anil et al. (2023) and Gemini was trained to support a range of multilingual capabilities Gemini Team Google (2023). Mixtral supports English, French, German, Italian, and Spanish Mistral AI (2024), while GPT systems are primarily built using English data only OpenAI et al. (2023). We evaluate all foundation models on all languages included in SeeGULL Multilingual.\n\nA.7.3 Evaluation protocol\n\nIn order to demonstrate that SeeGULL Multilingual can be used for improving foundation models, we run inference without additional safety guardrails or mitigation layers that are typically used by downstream application developers. Mistral Mistral AI (2024) and Gemini Google (2024a) provide configurable safety guardrails which we disable, and PaLM 2 includes metadata about safety with responses Google (2024b) which we do not consider. GPT models do not support configurable safety through the API.\n\nWe run inference for evaluations through public APIs for four families of foundation models. We draw one sample from each model with temperature=0. All system versions were fixed, and inference was run during January and February 2024. Each system was queried with temperature=0.0. Model version are show in Table 9.\n\nModel response styles varied by foundation model, even with unambiguous and consistent instructions. To score responses, we use a heuristic to parse decoded text, and considered the model to endorse the stereotype if it produced text a) used the format as instructed and produced the letter of the stereotypical association, b) instead generated the exact word of the stereotypical association, c) produced text containing only the letter of the stereotypical association formatted as instructed, but with other additional text, and d) all formatted letter choices, repeating one letter choice twice.\n\nA.8 Regional Demonyms\n\nThere is no single place containing regional demonyms for all the countries of the world. We source the regional demonyms online from the following sources followed by manual validation.\n\nFrance:\n\nJapan:\n\n•\n\n•\n\nSince no particular demonym are found, we default to \"People from [name of the region]\".\n\nSouth Korea:\n\n•\n\n•\n\nSince no particular demonym are found, we default to \"People from [name of the region]\".\n\nVietnam:\n\n•\n\n•\n\nSince no particular demonym are found, we default to \"People from [name of the region]\".\n\nThailand:\n\n•\n\nNo particular demonym, defaulted to \"People from [name of the region]\".\n\nKenya:\n\n•\n\nNo particular demonym, defaulted to \"People from [name of the region]\".\n\nA.9 Licenses of models and data used\n\nThe data (SGE) was released with CC-BY-4.0 licence which permits its usage for research purposes. The intended usage guidelines of the different models were adhered to . We abide by the terms of use of any models used in this paper."
    }
}
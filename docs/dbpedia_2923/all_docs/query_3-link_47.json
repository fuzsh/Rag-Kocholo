{
    "id": "dbpedia_2923_3",
    "rank": 47,
    "data": {
        "url": "https://www.cisco.com/c/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.html",
        "read_more_link": "",
        "language": "en",
        "title": "Cisco Application Centric Infrastructure (ACI) Design Guide",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://www.cisco.com/c/dam/cdc/i/Feedback_OceanBlue.png",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_1.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_2.png",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_3.png",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_4.png",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_5.png",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_6.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_7.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_8.png",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_9.png",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_10.png",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_11.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_12.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_13.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_14.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_15.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_16.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_17.png",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_18.png",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_19.png",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_20.png",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_21.png",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_22.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_23.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_24.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_25.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_26.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_27.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_28.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_29.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_30.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_31.png",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_32.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_33.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_34.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_35.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_36.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_37.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_38.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_39.png",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_40.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_41.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_42.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_43.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_44.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_45.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_46.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_47.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_48.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_49.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_50.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_51.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_52.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_53.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_54.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_55.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_56.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_57.png",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_58.png",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_59.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_60.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_61.png",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_62.png",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_63.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_64.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_65.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_66.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_67.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_68.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_69.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_70.png",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_71.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_72.png",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_73.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_74.png",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_75.png",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_76.png",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_77.png",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_78.png",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_79.png",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_80.png",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_81.png",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_82.png",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_83.png",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_84.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_85.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_86.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_87.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_88.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_89.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_90.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_91.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_92.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_93.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_94.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_95.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_96.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_97.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_98.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_99.png",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_100.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_101.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_102.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_103.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_104.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_105.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_106.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_107.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_108.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_109.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_110.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_111.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_112.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_113.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_114.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_115.png",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_116.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_117.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_118.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_119.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_120.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_121.png",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_122.png",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_123.png",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_124.png",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_125.png",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_126.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_127.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_128.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_129.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_130.png",
            "https://www.cisco.com/c/dam/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.docx/_jcr_content/renditions/cisco-application-centric-infrastructure-design-guide_131.png",
            "https://cisco.112.2o7.net/b/ss/cisco-mobile/5/12345"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2024-06-06T11:00:24",
        "summary": "",
        "meta_description": "Cisco Application Centric Infrastructure (Cisco ACI™) technology enables you to integrate virtual and physical workloads in a programmable, multihypervisor fabric to build a multiservice or cloud data center. The Cisco ACI fabric consists of discrete components connected in a spine and leaf switch topology that it is provisioned and managed as a single entity.",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "Cisco",
        "canonical_link": "https://www.cisco.com/c/en/us/td/docs/dcn/whitepapers/cisco-application-centric-infrastructure-design-guide.html",
        "text": "Last Updated: June 6, 2024\n\nTable of Contents\n\nIntroduction. 9\n\nComponents and Versions. 10\n\nCisco ACI Building Blocks. 10\n\nCisco Nexus 9000 Series Hardware. 10\n\nLeaf Switches. 11\n\nSpine Switches. 12\n\nCabling. 14\n\nCisco Application Policy Infrastructure Controller (APIC)14\n\nFabric with Mixed Hardware or Software. 14\n\nFabric with Different Spine Types. 14\n\nFabric with Different Leaf Switch Types. 14\n\nFabric with Different Software Versions. 15\n\nFabric Extenders (FEX). 15\n\nPhysical Topology. 16\n\nLeaf and Spine Switch Functions. 17\n\nLeaf Fabric Links. 17\n\nMulti-tier Design Considerations. 18\n\nPer Leaf RBAC (Role-based Access Control). 18\n\nVirtual Port Channel Hardware Considerations. 19\n\nHardware Compatibility Between vPC Pairs. 20\n\nvPC and Hardware Profiles. 20\n\nvPC and Software Versions. 21\n\nvPC Member Ports. 21\n\nvPC and FEX. 21\n\nPlacement of Outside Connectivity. 21\n\nBorder Leaf Switches with VRF-lite, SR/MPLS Handoff and GOLF. 21\n\nUsing Border Leaf Switches for Server Attachment23\n\nLimit the use of L3Out for Server Connectivity. 24\n\nL3Out and vPC. 24\n\nService Leaf Switch Considerations. 25\n\nPlanning for SPAN.. 25\n\nIn-band and out-of-band Management Connectivity. 25\n\nMultiple Locations Data Centers Design Considerations. 27\n\nFabric Infrastructure (Underlay) Design. 29\n\nChoosing the Leaf Switch Forwarding Profile. 30\n\nFabric-id. 32\n\nInfrastructure VLAN.. 32\n\nCommon Reserved VLANs on External Devices. 33\n\nHardening the Infrastructure VLAN.. 34\n\nTEP Address Pools. 34\n\nMulticast Range. 37\n\nBGP Route Reflector. 37\n\nBGP Autonomous System Number Considerations. 39\n\nBGP Route-Reflector Placement Considerations. 39\n\nBGP Maximum Path. 40\n\nNetwork Time Protocol (NTP) configuration. 40\n\nCOOP Group Policy. 41\n\nIn-Band and Out-of-Band Management. 41\n\nAccess Control42\n\nIn-band Connectivity to the Outside. 42\n\nIn-band Management Configuration. 44\n\nOut-of-band Management Configuration. 44\n\nRouting on Cisco APIC. 45\n\nManagement Connectivity for VMM Integration. 46\n\nIn-band Management Requirements for Telemetry. 46\n\nIS-IS Metric for Redistributed Routes. 47\n\nMaximum Transmission Unit. 47\n\nConfiguring the Fabric Infrastructure for Faster Convergence. 48\n\nFast Link Failover. 48\n\nDebounce Timer. 48\n\nCisco APIC Design Considerations. 49\n\nCisco APIC Teaming. 50\n\nPort tracking and Cisco APIC Ports. 50\n\nIn-Band and Out-of-Band Management of Cisco APIC. 51\n\nInternal IP Address Used for Apps. 51\n\nCisco APIC Clustering. 51\n\nCluster Sizing and Redundancy. 51\n\nStandby Controller. 53\n\nFabric Recovery. 53\n\nSummary of Cisco APIC design considerations. 53\n\nCisco ACI Objects Design Considerations. 54\n\nFabric Infrastructure Configurations. 55\n\nTenant Configurations. 56\n\nNaming of Cisco ACI Objects. 56\n\nObjects with Overlapping Names in Different Tenants. 57\n\nConnectivity Instrumentation Policy. 58\n\nDesigning the Fabric Access. 58\n\nFabric-access Policy Configuration Model58\n\nInterface Overrides. 59\n\nDefining VLAN Pools and Domains. 60\n\nEPG Domain Validation. 61\n\nAttachable Access Entity Profiles (AAEPs)61\n\nUnderstanding VLAN Use in Cisco ACI and to Which VXLAN They Are Mapped. 62\n\nOverlapping VLAN ranges. 64\n\nVLAN Scope: Port Local Scope. 67\n\nDomain and EPG VLAN Validations. 68\n\nCisco Discovery Protocol, LLDP, and Policy Resolution. 69\n\nPort Channels and Virtual Port Channels. 69\n\nvPC Domain Definition. 70\n\nStatic Port Channel, LACP Active, LACP Passive. 71\n\nHashing Options. 72\n\nConfiguration for Faster Convergence with VPCs. 72\n\nPort Channels and Virtual Port Channels Configuration Model in Cisco ACI73\n\nvPC Consistency Checks. 74\n\nOrphan Ports. 74\n\nPort Tracking. 75\n\nDelay Restore. 76\n\nInteractions with vPC. 77\n\nInteraction with Cisco APIC Ports. 77\n\nLoop Mitigation Features Overview.. 77\n\nLLDP for Mis-Cabling Protection. 78\n\nMis-Cabling Protocol (MCP) Overview.. 78\n\nLink Aggregation Control Protocol (LACP) Suspend Individual Ports. 78\n\nTraffic Storm Control79\n\nInterface-level Control Plane Policing (CoPP)80\n\nSpanning Tree Protocol Considerations. 80\n\nSpanning Tree BPDU Guard. 81\n\nMiscabling Protocol (MCP). 81\n\nMCP Aggressive Timers. 82\n\nPer-VLAN MCP. 83\n\nMCP Strict83\n\nEndpoint Move Dampening, Endpoint Loop Protection, and Rogue Endpoint Control84\n\nEndpoint Move Dampening. 85\n\nEndpoint Loop Protection. 85\n\nRogue Endpoint Control87\n\nRogue Endpoint Control Exceptions. 87\n\nSummary Best Practices for Layer 2 Loop Mitigation. 88\n\nGlobal Configurations. 89\n\nEndpoint Listen Policy (beta). 90\n\nDesigning the Tenant Network. 91\n\nTenant Network Configurations. 92\n\nNetwork-centric and Application-centric Designs (and EPGs Compared with ESGs). 93\n\nImplementing a Network-centric Topology. 94\n\nDefault Gateway for the Servers. 94\n\nAssigning Servers to Endpoint Groups. 95\n\nLayer 2 Connectivity to the Outside with Network Centric Deployments. 95\n\nUsing VRF Unenforced Mode or Preferred Groups or vzAny with Network Centric Deployments. 96\n\nUsing ESGs to Create the Equivalent of Multiple Preferred Groups. 97\n\nImplementing a Tenant Design With Segmentation Using EPGs or ESGs (Application-centric). 98\n\nAdding EPGs to Existing Bridge Domains. 100\n\nMerging Bridge Domains and Subnets (with Flood in Encapsulation)101\n\nUsing Endpoint Security Groups. 101\n\nAdding Filtering Rules with Contracts and Firewalls with vzAny and Service Graph Redirect102\n\nDefault Gateway (Subnet) Design Considerations. 104\n\nBridge Domain Subnet, SVI, Pervasive Gateway. 104\n\nSubnet Configuration: Under the Bridge Domain and Why Not Under the EPG.. 104\n\nCommon Pervasive Gateway. 105\n\nVRF Design Considerations. 105\n\nVRF Instances and Bridge Domains in the Common Tenant. 106\n\nVRF Instances in the Common Tenant and Bridge Domains in User Tenants. 107\n\nVRF Ingress Versus VRF Egress Filtering Design Considerations. 107\n\nBridge Domain Design Considerations. 109\n\nBridge Domain Configuration for Migration Topologies. 110\n\nBridge Domain Flooding. 111\n\nBPDU Handling in the Bridge Domain. 112\n\nFlood in Encapsulation. 113\n\nUsing Hardware-Proxy to Reduce Flooding. 114\n\nARP Flooding. 115\n\nGARP-based Detection. 116\n\nLayer 2 Multicast and IGMP Snooping in the Bridge Domain. 116\n\nBridge Domain Enforcement Status. 117\n\nSummary of Bridge Domain Recommendations. 117\n\nEPG Design Considerations. 118\n\nEPGs and VLANs. 119\n\nConfiguring Trunk Ports with Nexus 9300-EX and Newer. 119\n\nConfiguring Trunk Ports with First Generation Leaf switches. 120\n\nEPGs, Bridge Domains, and VLAN mapping. 120\n\nEPGs, Physical and VMM Domains, and VLAN Mapping on a Specific Port (or Port Channel or vPC)121\n\nMicrosegmented EPGs. 122\n\nInternal VLANs on the Leaf Switches: EPGs and Bridge Domains Scale. 123\n\nAssigning Physical Hosts to EPGs. 123\n\nUsing the Application Profile EPG.. 124\n\nAssigning Hosts to EPGs from the Attachable Access Entity Profile (AAEP)124\n\nAssigning Virtual Machines to EPGs. 124\n\nVMM Integration. 125\n\nInitial VMM Setup. 126\n\nEPG Configuration Workflow with VMM Integration. 126\n\nVMware vDSs created by a VMM... 127\n\nConnecting EPGs to External Switches. 127\n\nL2Outs Versus EPGs. 127\n\nUsing EPGs to connect Cisco ACI to External Layer 2 Networks. 128\n\nEPG and Fabric Access Configurations for Multiple Spanning Tree. 129\n\nMinimize the scope of Spanning Tree Topology Changes. 129\n\nUsing EPGs to Connect Cisco ACI to External Layer 2 Networks Using vPCs. 130\n\nOther EPG Features. 132\n\nEPG Shutdown. 132\n\nStatic Routes. 132\n\nProxy ARP. 132\n\nContracts Design Considerations. 132\n\nSecurity Contracts are ACLs Without IP Addresses. 133\n\nFilters and Subjects. 134\n\nPermit, Deny, Redirect, and Copy. 134\n\nConcept of Direction in Contracts. 134\n\nUnderstanding the Bidirectional and Reverse Filter Options. 135\n\nConfiguring a Stateful Contract. 136\n\nConfiguring a Single Contract Between EPG/ESGs. 137\n\nContract Scope. 138\n\nContracts and Filters in the Common Tenant. 138\n\nSetting the Contract Scope Correctly. 138\n\nSaving Policy-CAM Space with Compression. 139\n\nPros and Cons of using Contracts from Tenant Common. 139\n\nUnenforced VRF Instances, Preferred Groups, vzAny. 139\n\nUsing vzAny. 140\n\nContracts and Filtering Rule Priorities. 140\n\nPolicy CAM Compression. 141\n\nResolution and Deployment Immediacy of VRF Instances, Bridge Domains, EPGs, and Contracts. 143\n\nEPG Resolution Immediacy and Deployment Immediacy Options. 144\n\nEPG Resolution Immediacy and Deployment Immediacy Considerations for Virtualized Servers. 145\n\nEndpoint Learning Considerations. 146\n\nCisco ACI Endpoint Management. 146\n\nLocal Endpoint Learning on the Leaf Switches. 146\n\nEnforce Subnet Check. 147\n\nLimit IP Learning to Subnet148\n\nEndpoint Aging. 148\n\nEndpoint Aging with Multiple IP Addresses for the Same MAC Address. 149\n\nARP Timers on Servers. 149\n\nEndpoint Retention Policy at the Bridge Domain and VRF Level149\n\nDataplane Learning. 150\n\nBridge Domain and IP Routing. 151\n\nRemote entries. 151\n\nDataplane Learning from ARP packets. 151\n\nWhen and How to disable Remote Endpoint Learning (for Border Leaf Switches)152\n\nFloating IP Address Considerations. 153\n\nWhen and How to Disable IP Dataplane Learning. 154\n\nStale Entries and Endpoint Announce Delete. 156\n\nServer Connectivity and NIC Teaming Design Considerations. 157\n\nDesign Model for IEEE 802.3ad with a vPC. 158\n\nNIC Teaming Configurations for Non-Virtualized Servers. 159\n\nServer Active/Active (802.3ad Dynamic Link Aggregation) Teaming with vPC. 159\n\nNIC Teaming Active/Standby. 160\n\nNIC Teaming Active/Active non-Port Channel-based (non-vPC)161\n\nNIC Teaming Configurations for Virtualized Servers (Without the Use of VMM Integration). 162\n\nVMware Teaming. 163\n\nHyper-V Teaming. 163\n\nNIC Teaming Configurations for Virtualized Servers with VMM Integration. 165\n\nCDP and LLDP in the Policy Group Configuration. 166\n\nConfiguring Teaming using the Cisco ACI VMM Integration. 166\n\nTeaming Options with VMM Integration. 167\n\nChoosing between Policy-Group type Access Leaf Port and vPC. 169\n\nUsing LACP Between the Virtualized Host and the Cisco ACI Leaf switches. 170\n\nTeaming Configuration with Servers Not Directly Attached to the Cisco ACI Leaf switches. 172\n\nUCS connectivity with Fabric Interconnect. 173\n\nDesigning External Layer 3 Connectivity. 175\n\nThe evolution of L3Out: VRF-lite, GOLF and SR/MPLS handoff. 176\n\nLayer 3 Outside (L3Out) and External Routed Networks. 177\n\nL3Out Simplified Object Model178\n\nL3Out Router ID Considerations. 179\n\nRoute Announcement Options for the Layer 3 Outside (L3Out)180\n\nRoute Map Handling Differences Between OSPF, EIGRP and BGP. 181\n\nExternal Network (External EPG) Configuration Options. 182\n\nAdvertisement of Bridge Domain Subnets. 183\n\nHost Routes Advertisement184\n\nBorder Leaf Switch Designs. 186\n\nL3Out with vPC. 187\n\nL3Out SVI Auto State. 187\n\nGateway Resiliency with L3Out188\n\nExternal Bridge Domains. 189\n\nAdd L3Out SVI Subnets to the External EPG.. 189\n\nBidirectional Forwarding Detection (BFD) for L3Out190\n\nFloating SVI191\n\nConsiderations for Multiple L3Outs. 193\n\nExternal EPGs Have a VRF Scope. 193\n\nUsing Dynamic L3Out EPG Classification (DEC)195\n\nConsiderations When Using More Than Two Border Leaf Switches. 197\n\nUsing BGP for External Connectivity. 198\n\nBGP Autonomous System (AS) number. 198\n\nBGP Maximum Path. 199\n\nImporting Routes. 199\n\nRoute Summarization. 200\n\nOSPF Route Summarization. 201\n\nSR-MPLS/MPLS. 203\n\nTransit Routing. 203\n\nSupported Combinations for Transit Routing. 205\n\nLoop Prevention in Transit Routing Scenarios. 205\n\nQuality of Service (QoS) In Cisco ACI206\n\nDot1p Preserve. 208\n\nQuality of Service for Traffic Going to an IPN.. 209\n\nVRF Sharing Design Considerations. 210\n\nInter-Tenant and Inter-VRF Communication. 212\n\nInter-VRF Communication using EPGs. 213\n\nInter-VRF Communication using ESGs. 215\n\nConfiguration of the Subnet: When to Enter the Subnet Under the EPG.. 216\n\nShared L3Out Connections. 217\n\nPolicy Enforcement with Inter-VRF Traffic. 220\n\nSpecial Considerations and Restrictions for VRF Sharing Designs. 221\n\nUpgrade Considerations. 221\n\nCisco APIC Upgrade. 222\n\nReducing the Cisco APIC Upgrade Time. 222\n\nSwitch Upgrade. 222\n\nSwitch Update Groups. 222\n\nReducing Traffic Disruption During Upgrades. 223\n\nGraceful Upgrades. 223\n\nGraceful Upgrades Versus Graceful Insertion and Removal224\n\nReducing Switch Upgrade Time. 224\n\nFeatures That Must be Disabled Before an Upgrade or a Downgrade. 225\n\nConclusion. 225\n\nFor More Information. 226\n\nIntroduction\n\nCisco Application Centric Infrastructure (Cisco ACI™) technology enables you to integrate virtual and physical workloads in a programmable, multi-hypervisor fabric to build a multiservice or cloud data center. The Cisco ACI fabric consists of discrete components connected in a spine and leaf switch topology that it is provisioned and managed as a single entity.\n\nThis document describes how to implement a fabric such as the one depicted in Figure 1.\n\nThe design described in this document is based on the following reference topology:\n\n● Two spine switches interconnected to several leaf switches\n\n● Top-of-Rack (ToR) leaf switches for server connectivity, with a mix of front-panel port speeds: 1/10/25/40/50/100/200/400-Gbps\n\n● Physical and virtualized servers dual-connected to the leaf switches\n\n● A pair of border leaf switches connected to the rest of the network with a configuration that Cisco ACI calls a Layer 3 Outside (L3Out) connection\n\n● A cluster of three Cisco Application Policy Infrastructure Controllers (APICs) dual-attached to a pair of leaf switches in the fabric\n\nThe network fabric in this design provides the following main services:\n\n● Connectivity for physical and virtual workloads\n\n● Partitioning of the fabric into multiple tenants, which may represent departments or hosted customers\n\n● The ability to create shared-services partitions (tenant) to host servers or virtual machines whose computing workloads provide infrastructure services such as Network File System (NFS) and Microsoft Active Directory to the other tenants\n\n● Capability to provide dedicated or shared Layer 3 routed connections to the tenants present in the fabric\n\nComponents and Versions\n\nA Cisco ACI fabric can be built using a variety of Layer 3 switches that, while compatible with each other, differ in terms of form factors and ASICs to address multiple requirements.\n\nYou can find the list of available leaf and spine switches at the following URL:\n\nhttps://www.cisco.com/c/en/us/products/switches/nexus-9000-series-switches/models-comparison.html\n\nThis document is based on features that are present in Cisco ACI release 6.0(1g).\n\nCisco ACI can integrate with every virtualized server using physical domains and the EPG Static Port configuration for \"static binding\" (more on this later) and with many external controllers using direct API integration, which is called Virtual Machine Manager (VMM) integration. Cisco APIC can integrate using VMM integration with VMware ESXi hosts with VMware vSphere, Hyper-V servers with Microsoft SCVMM, RedHat Virtualization, Kubernetes, OpenStack, OpenShift, and more. Cisco ACI 5.1(1) and later releases can integrate with VMware NSX-T Data Center (NSX).\n\nThe integration using static binding doesn’t require any special software version, whereas for the integration using Virtual Machine Manager you need specific Cisco ACI versions to integrate with specific Virtual Machine Manager versions.\n\nVMware ESXi hosts with VMware vSphere 7.0 can be integrated with Cisco ACI release 4.2(4o) or later using VMM. VMware ESXi hosts integrate with Cisco ACI using the VMware vSphere Distributed Switch (vDS).\n\nNote: This design guide explains design considerations related to teaming with specific reference to the VMM integration with VMware vSphere and it does not include the integration with VMware NSX-T.\n\nFor information about the support for virtualization products with Cisco ACI, see the ACI Virtualization Compatibility Matrix.\n\nFor more information about integrating virtualization products with Cisco ACI, see the virtualization documentation.\n\nCisco ACI Building Blocks\n\nCisco Nexus 9000 Series Hardware\n\nFor a list of available Cisco ACI Nexus 9000 series switches, see Cisco Nexus 9000 Series Switches.\n\nThis section provides some clarification about the naming conventions used for the leaf and spine switches referred to in this document:\n\n● N9K-C93xx refers to the Cisco ACI leaf switches\n\n● N9K-C95xx refers to the Cisco modular chassis\n\n● N9K-X97xx refers to the Cisco ACI spine switch line cards\n\nThe trailing -E and -X signify the following:\n\n● -E: Enhanced. This refers to the ability of the switch to classify traffic into endpoint groups (EPGs) based on the source IP address of the incoming traffic.\n\n● -X: Analytics. This refers to the ability of the hardware to support analytics functions. The hardware that supports analytics includes other enhancements in the policy CAM, in the buffering capabilities, and in the ability to classify traffic to EPGs.\n\n● -F: Support for MAC security.\n\n● -G: Support for 400 Gigabit Ethernet.\n\nFor simplicity, this document refers to any switch without a suffix or with without the -X suffix as a first generation switch, and any switch with -EX, -FX, -GX, or any later suffix as a second generation switch.\n\nNote: The Cisco ACI leaf switches with names ending in -GX have hardware that is capable of operating as either a spine or leaf switch. The software support for either option comes in different releases. For more information, see Cisco Nexus 9300-GX Series Switches Data Sheet.\n\nFor port speeds, the naming conventions are as follows:\n\n● G: 100M/1G\n\n● P: 1/10-Gbps Enhanced Small Form-Factor Pluggable (SFP+)\n\n● T: 100-Mbps, 1-Gbps, and 10GBASE-T copper\n\n● Y: 10/25-Gbps SFP+\n\n● Q: 40-Gbps Quad SFP+ (QSFP+)\n\n● L: 50-Gbps QSFP28\n\n● C: 100-Gbps QSFP28\n\n● D: 400-Gbps QSFP-DD\n\n● E: 800-Gbps\n\nFor the taxonomy, see Taxonomy for Cisco Nexus 9000 Series Part Numbers.\n\nFor more information about Cisco Nexus 400 Gigabit Ethernet switches hardware (which includes Cisco ACI leaf and spine switches switches), see 400G Data Center and Cloud Networking.\n\nLeaf Switches\n\nIn Cisco ACI, all workloads connect to leaf switches. The leaf switches used in a Cisco ACI fabric are Top-of-the-Rack (ToR) switches. A number of leaf switch choices differ based on function:\n\n● Port speed and medium type\n\n● Buffering and queue management: All leaf switches in Cisco ACI provide advanced capabilities to load balance traffic more precisely, including dynamic packet prioritization, to prioritize short-lived, latency-sensitive flows (sometimes referred to as mouse flows) over long-lived, bandwidth-intensive flows (also called elephant flows). The newest hardware also introduces more sophisticated ways to keep track and measure elephant and mouse flows and prioritize them, as well as more efficient ways to handle buffers.\n\n● Policy CAM size and handling: The policy CAM is the hardware resource that allows filtering of traffic between EPGs. It is a TCAM resource in which Access Control Lists (ACLs) are expressed in terms of which EPG (security zone) can talk to which EPG (security zone). The policy CAM size varies depending on the hardware. The way in which the policy CAM handles Layer 4 operations and bidirectional contracts also varies depending on the hardware. -FX and -GX leaf switches offer more capacity compared with -EX and -FX2.\n\n● Multicast routing support in the overlay: A Cisco ACI fabric can perform multicast routing for tenant traffic (multicast routing in the overlay).\n\n● Support for analytics: The newest leaf switches and spine switch line cards provide flow measurement capabilities for the purposes of analytics and application dependency mappings.\n\n● Support for link-level encryption: The newest leaf switches and spine switch line cards provide line-rate MAC security (MACsec) encryption.\n\n● Scale for endpoints: One of the major features of Cisco ACI is the endpoint database, which maintains the information about which endpoint is mapped to which Virtual Extensible LAN (VXLAN) tunnel endpoint (VTEP), in which bridge domain, and so on.\n\nAbility to change the allocation of hardware resources, such as to support more Longest Prefix Match entries, or more policy CAM entries, or more IPv4 entries. This concept is called \"tile profiles,\" and it was introduced in Cisco ACI 3.0. For more information, see Cisco APIC Forwarding Scale Profiles and Verified Scalability Guide.\n\nThe -GX hardware can be deployed both as leaf or as a spine switch, and in case of high density 100 or 400 ports leaf switches you can use breakout cables to connect lower speed ports. For more information, see Nexus 9300 400 GE Switches.\n\nFor more information about the differences between the Cisco Nexus® 9000 series switches, see the following documents:\n\n●https://www.cisco.com/c/en/us/products/collateral/switches/nexus-9000-series-switches/datasheet-c78-738259.html\n\n●https://www.cisco.com/c/en/us/products/switches/nexus-9000-series-switches/models-comparison.html\n\nSpine Switches\n\nThe spine switches are available in several form factors both for modular switches as well as for fixed form factors. Cisco ACI leaf switches with name ending in -GX have hardware that can operate both as spine and as leaf switch.\n\nThe differences among spine switches with different hardware are as follows:\n\n● Port speeds\n\n● Support for analytics: although this capability is primarily a leaf switch function and it may not be necessary in the spine switch, in the future there may be features that use this capability in the spine switch.\n\n● Support for link-level encryption and for CloudSec. For information, see Cisco ACI Multi-Site Configuration Guide, Release 2.0(x).\n\n● Support for Cisco ACI Multi-Pod and Cisco ACI Multi-Site: Refer to the specific documentation on Cisco ACI Multi-Pod and Cisco ACI Multi-Site, including the respective release notes, for more details.\n\nAt the time of this writing, the speed of ports used for spine switches was moving more and more to 400 Gigabit Ethernet density and the same -GX hardware can be used as a leaf or spine switch. For more information, see Nexus 9300 400 GE Switches.\n\nNote: For information about Cisco ACI Multi-Site hardware requirements, see Cisco ACI Multi-Site Hardware Requirements Guide, Release 2.0(x).\n\nThe Cisco ACI fabric forwards traffic based on host lookups (when doing routing): all known endpoints in the fabric are programmed in the spine switches. The endpoints saved in the leaf switch forwarding table are only those that are used by the leaf switch in question, thus preserving hardware resources at the leaf switch. As a consequence, the overall scale of the fabric can be much higher than the individual scale of a single leaf switch.\n\nThe spine switch models also differ in the number of endpoints that can be stored in the spine proxy table, which depends on the type and number of fabric modules installed.\n\nYou should use the verified scalability limits for the latest Cisco ACI release and see how many endpoints can be used per fabric. See the Verified Scalability Guide for your release.\n\nAccording to the verified scalability limits, the following spine switch configurations have the indicated endpoint scalabilities:\n\n● Max. 450,000 Proxy Database Entries with four (4) fabric line cards\n\n● Max. 180,000 Proxy Database Entries with the fixed spine switches\n\nThe above numbers represent the sum of the number of MAC, IPv4, and IPv6 addresses; for instance, in the case of a Cisco ACI fabric with fixed spine switches, this translates into:\n\n● 180,000 MAC-only EPs (each EP with one MAC only)\n\n● 90,000 IPv4 EPs (each EP with one MAC and one IPv4)\n\n● 60,000 dual-stack EPs (each EP with one MAC, one IPv4, and one IPv6)\n\nThe number of supported endpoints is a combination of the capacity of the hardware tables, what the software allows you to configure, and what has been tested.\n\nRefer to the Verified Scalability Guide for a given release and to the Capacity Dashboard in the Cisco APIC GUI for this information.\n\nCabling\n\nDetailed guidelines about which type of transceivers and cables you should use is outside of the scope of this document. The Transceiver Compatibility Matrix is a great tool to help with this task: https://tmgmatrix.cisco.com/\n\nCisco Application Policy Infrastructure Controller (APIC)\n\nThe Cisco APIC is the point of configuration for policies and the place where statistics are archived and processed to provide visibility, telemetry, and application health information and enable overall management of the fabric. The controller is a physical appliance based on a Cisco UCS® rack server with two interfaces for connectivity to the leaf switches. The Cisco APIC is also equipped with Gigabit Ethernet interfaces for out-of-band management.\n\nFor more information about the Cisco APIC models, see Cisco Application Policy Infrastructure Controller Data Sheet.\n\nNote: A cluster may contain a mix of different Cisco APIC models; however, the scalability will be that of the least powerful cluster member.\n\nNote: The naming of the Cisco APICs, such as M3 or L3, is independent of the UCS series names.\n\nFabric with Mixed Hardware or Software\n\nFabric with Different Spine Types\n\nIn Cisco ACI, you can mix new and old generations of hardware for the spine and leaf switches. For instance, you could have first-generation hardware leaf switches and new-generation hardware spine switches, or vice versa. The main considerations with spine hardware are as follows:\n\n● Uplink bandwidth between leaf and spine switches\n\n● Scalability of the spine proxy table (which depends primarily on the type of fabric line card that is used in the spine)\n\n● Cisco ACI Multi-Site requires spine switches based on the Cisco Nexus 9500 platform cloud-scale line cards to connect to the intersite network\n\nYou can mix spine switches of different types, but the total number of endpoints that the fabric supports is the minimum common denominator.\n\nFabric with Different Leaf Switch Types\n\nWhen mixing leaf switches of different hardware types in the same fabric, you may have varying support of features and different levels of scalability.\n\nIn Cisco ACI, the processing intelligence resides primarily on the leaf switches, so the choice of leaf switch hardware determines which features may be used (for example, multicast routing in the overlay, or FCoE). Not all leaf switches provide the same hardware capabilities to implement all features.\n\nCisco APIC pushes the managed object to the leaf switches regardless of the ASIC that is present. If a leaf switch does not support a given feature, it raises a fault. For multicast routing you should ensure that the bridge domains and Virtual Routing and Forwarding (VRF) instances configured with the feature are deployed only on the leaf switches that support the feature.\n\nFabric with Different Software Versions\n\nThe Cisco ACI fabric is designed to operate with the same software version on all the APICs and switches. During upgrades, there may be different versions of the OS running in the same fabric.\n\nIf the leaf switches are running different software versions, the following behavior applies: Cisco APIC pushes features based on what is implemented in its software version. If the leaf switch is running an older version of software and the Cisco APIC does not understand a feature, the Cisco APIC will reject the feature; however, the Cisco APIC may not raise a fault.\n\nFor more information about which configurations are allowed with a mixed OS version in the fabric, see the software and firmware installation and upgrade guides.\n\nRunning a Cisco ACI fabric with different software versions is meant to be just a temporary configuration to facilitate upgrades, and minimal or no configuration changes should be performed while the fabric runs with mixed OS versions.\n\nFabric Extenders (FEX)\n\nYou can connect fabric extenders (FEXes) to the Cisco ACI leaf switches; the main purpose of doing so should be to simplify migration from an existing network with fabric extenders. If the main requirement for the use of FEX is the Fast Ethernet port speeds, you may want to consider the Cisco ACI leaf switch models with -G or -T in the product name, such as Cisco Nexus N9K-C9348GC-FXP, N9K-C93108TC-FX, N9K-C93108TC-FX-24, N9K-C93108TC-EX, N9K-C93108TC-EX-24, N9K-C93216TC-FX2, and N9K-93108TC-FX3P.\n\nTo connect a FEX to a Cisco ACI leaf switch, you must assign a FEX ID to each FEX, and this number has leaf scope, so the same FEX ID can be re-used on a different leaf switch.\n\nA FEX can be connected to Cisco ACI using a port channel with what is known as a straight-through topology, and vPCs can be configured between hosts and the FEX, but not between the FEX and Cisco ACI leaf switches.\n\nA FEX can be connected to leaf switch front-panel ports as well as converted downlinks (since Cisco ACI release 3.1).\n\nA FEX has many limitations compared to attaching servers and network devices directly to a leaf switch. The main limitations as follows:\n\n● No support for L3Out on a FEX\n\n● No Rate limiters support on a FEX\n\n● No Traffic Storm Control on a FEX\n\n● No Port Security support on a FEX\n\n● A FEX should not be used to connect routers or Layer 4 to Layer 7 devices with service graph redirect\n\n● The use in conjunction with microsegmentation works, but if microsegmentation is used, then Quality of Service (QoS) does not work on FEX ports because all microsegmented traffic is tagged with a specific class of service. Microsegmentation and a FEX is a feature that at the time of this writing has not been extensively validated.\n\nSupport for FCoE on a FEX was added in Cisco ACI release 2.2. See Cisco Application Policy Infrastructure Controller, Release 2.2(1), Release Notes.\n\nWhen using Cisco ACI with a FEX, you want to verify the verified scalability limits; in particular, the limits related to the number of ports multiplied by the number of VLANs configured on the ports (commonly referred to as P, V). For more information, see the Verified Scalability Guide for your release.\n\nWith regard to scalability, you should keep in mind the following points:\n\n● The total scale for VRF instances, bridge domains (BDs), endpoints, and so on is the same whether you are using FEX attached to a leaf switch or whether you are connecting endpoints directly to a leaf switch. This means that, when using FEX, the amount of hardware resources that the leaf switch provides is divided among more ports than just the leaf switch ports.\n\n● The total number of VLANs that can be used on each FEX port is limited by the maximum number of P,V pairs that are available per leaf switch for host-facing ports on FEX. For the latest supported scale numbers, see the Verified Scalability Guide.\n\n● The maximum number of EPGs per FEX port is the maximum number of encapsulations per FEX port as specified in the Verified Scalability Guide.\n\n● For the maximum number of FEXes per leaf switch, see the Verified Scalability Guide.\n\nNote: For more information about which leaf switch is compatible with which fabric extender, refer to the following link:\n\nhttps://www.cisco.com/c/en/us/td/docs/switches/datacenter/nexus9000/hw/interoperability/fexmatrix/fextables.html\n\nFor more information about how to connect a fabric extender to Cisco ACI, see Nexus 9000 Series Switch FEX Support.\n\nPhysical Topology\n\nAs of release 4.1, a Cisco ACI fabric can be built as a two-tier fabric or as a multi-tier (three-tiers) fabric.\n\nPrior to Cisco ACI 4.1, the Cisco ACI fabric allowed only the use of a two-tier (spine and leaf switch) topology, in which each leaf switch is connected to every spine switch in the network with no interconnection between leaf switches or spine switches.\n\nStarting from Cisco ACI 4.1, the Cisco ACI fabric allows also the use of two tiers of leaf switches, which provides the capability for vertical expansion of the Cisco ACI fabric. This is useful to migrate a traditional three-tier architecture of core-aggregation-access that have been a common design model for many enterprise networks and is still required today. The primary reason for this is cable reach, where many hosts are located across floors or across buildings; however, due to the high pricing of fiber cables and the limitations of cable distances, it is not ideal in some situations to build a full-mesh two-tier fabric. In those cases, it is more efficient for customers to build a spine-leaf-leaf switch topology and continue to benefit from the automation and visibility of Cisco ACI.\n\nFigure 2 Cisco ACI two-tier and Multi-tier topology\n\nLeaf and Spine Switch Functions\n\nThe Cisco ACI fabric is based on a two-tier (spine and leaf switch) or three-tier (spine switch, tier-1 leaf switch and tier-2 leaf switch) architecture in which the leaf and spine switches provide the following functions:\n\n● Leaf switches: These devices have ports connected to classic Ethernet devices, such as servers, firewalls, and router ports. Leaf switches are at the edge of the fabric and provide the VXLAN Tunnel Endpoint (VTEP) function. In Cisco ACI terminology, the IP address that represents the leaf switch VTEP is called the Physical Tunnel Endpoint (PTEP). The leaf switches are responsible for routing or bridging tenant packets and for applying network policies.\n\n● Spine switches: These devices interconnect leaf switches. They can also be used to build a Cisco ACI Multi-Pod fabric by connecting a Cisco ACI pod to an IP network, or they can connect to a supported WAN device (see more details in the \"Designing external layer 3 connectivity\" section). Spine switches also store all the endpoints-to-VTEP mapping entries (spine switch proxies).\n\nWithin a pod, all tier-1 leaf switches connect to all spine switches, and all spine switches connect to all tier-1 leaf switches, but no direct connectivity is allowed between spine switches, between tier-1 leaf switches, or between tier-2 leaf switches. If you incorrectly cable spine switches to each other or leaf switches in the same tier to each other, the interfaces will be disabled. You may have topologies in which certain leaf switches are not connected to all spine switches (such as in stretched fabric designs), but traffic forwarding may be suboptimal in this scenario.\n\nLeaf Fabric Links\n\nUp until Cisco ACI 3.1, fabric ports on leaf switches were hard-coded as fabric (iVXLAN) ports and could connect only to spine switches. Starting with Cisco ACI 3.1, you can change the default configuration and make ports that would normally be fabric links, be downlinks, or vice-versa. For more information, see Cisco Application Centric Infrastructure Fundamentals.\n\nNote: For information about the optics supported by Cisco ACI leaf and spine switches, use the following tool:\n\nhttps://tmgmatrix.cisco.com/home\n\nMulti-tier Design Considerations\n\nOnly Cisco Cloudscale switches are supported for multi-tier spine and leaf switches.\n\n● Spine: EX/FX/C/GX spine switches (Cisco Nexus 9332C, 9364C, and 9500 with EX/FX/GX line cards)\n\n● Tier-1 leaf: EX/FX/FX2/GX except Cisco Nexus 93180LC-EX\n\n● Tier-2 leaf: EX/FX/FX2/GX\n\nDesign considerations for multi-tier topology include the following:\n\n● All switch-to-switch links must be configured as fabric ports. For example, Tier-2 leaf switch fabric ports are connected to tier-1 leaf switch fabric ports.\n\n● A tier-2 leaf switch can connect to more than two tier-1 leaf switches, in comparison to a traditional double-sided vPC design, which has only two upstream switches. The maximum number of ECMP links supported by a tier-2 leaf switch to tier-1 leaf switch is 18.\n\n● An EPG, L3Out, Cisco APIC, or FEX can be connected to tier-1 leaf switches or to tier-2 leaf switches.\n\n● Tier-1 leaf switches can have both hosts and tier-2 leaf switches connected on it.\n\n● Changing from a tier-1 to a tier-2 leaf switch and back requires decommissioning and recommissioning the switch.\n\n● Multi-tier architectures are compatible with Cisco ACI Multi-Pod and Cisco ACI Multi-Site.\n\n● Tier-2 leaf switches cannot be connected to remote leaf switches (tier-1 leaf switches).\n\n● Scale: The maximum number of tier-1 leaf switches and tier-2 leaf switches combined must be less than or equal to the maximum number of leaf switches that have been validated for a given release. (400 per pod; 500 per Cisco ACI Multi-Pod as of Cisco ACI release 6.0(1)).\n\nMore information about Cisco ACI multi-tier can be found at the following link: https://www.cisco.com/c/en/us/solutions/data-center-virtualization/application-centric-infrastructure/white-paper-c11-742214.html\n\nPer Leaf RBAC (Role-based Access Control)\n\nUp until Cisco ACI 5.0, a Cisco ACI fabric administrator could assign a tenant to a security domain to let users have read/write privilege for a specific tenant assigned to that security domain, but that RBAC feature was not applicable to specific leaf switch.\n\nStarting from Cisco ACI 5.0, a leaf switch can be assigned to a security domain so that only specific users can configure leaf switches assigned to that security domain and users in other security domains have no access to the leaf switches assigned to the security domain. For example, a user in Figure 3 can see tenant1 and leaf switch Node-101 only, and can’t see other user tenants or leaf switches, whereas the admin user in Figure 4 Figure 4can see everything. This is useful for allocating leaf switches for different tenants, customers, or organizations.\n\nMore information can be found at the following link: https://www.cisco.com/c/en/us/td/docs/switches/datacenter/aci/apic/sw/5-x/security/cisco-apic-security-configuration-guide-50x/m-restricted-access-security-domains.html\n\nVirtual Port Channel Hardware Considerations\n\nCisco ACI provides a routed fabric infrastructure with the capability to perform equal-cost multipathing for Layer 2 and Layer 3 traffic.\n\nIn addition, Cisco ACI supports the virtual port channel (vPC) technology on leaf switch ports to optimize server connectivity to the fabric. The purpose of this section is not to describe vPC in detail, but to highlight the relevant considerations for the planning of the physical topology. For more information about vPC, refer to the \"Designing the fabric access / Port Channels and Virtual Port Channels\" section.\n\nIt is very common for servers connected to Cisco ACI leaf switches to be connected through a vPC (that is, a port channel on the server side) to increase throughput and resilience. This is true for both physical and virtualized servers.\n\nvPCs can also be used to connect to existing Layer 2 infrastructure or for L3Out connections (vPC plus a Layer 3 switch virtual interface [SVI]).\n\nHardware Compatibility Between vPC Pairs\n\nYou must decide which pairs of leaf switches in the fabric should be configured as part of the same vPC domain, which in the Cisco ACI configuration is called an \"explicit vPC protection group.\"\n\nWhen creating a vPC domain between two leaf switches, both switches must be of the same switch generation. Switches not of the same generation are not compatible vPC peers. For example, you cannot have a vPC consisting of a N9K-C9372TX and -EX or -FX leaf switches.\n\n● Generation 1 switches are compatible only with other generation 1 switches. These switch models can be identified by the lack of the \"EX,\" \"FX, \"FX2,\" \"FX3,\" \"GX\" or later suffix at the end of the switch name: for example, N9K-9312TX is a generation 1 switch.\n\n● Generation 2 and later switches can be mixed together in a vPC domain. These switch models can be identified by the \"EX,\" \"FX, \"FX2,\" \"FX3,\" \"GX\" or later suffix at the end of the switch name: for example N9K-93108TC-EX, or N9K-9348GC-FXP are generation 2 switches.\n\nNote When using two different models of the same generation, if there is a difference of scale in terms of forwarding tables, buffers, and so on, you should design your fabric according to the minimum common denominator. We recommend that you use two identical models to be part of the same vPC domain.\n\nEven if two leaf switches of different hardware generation are not meant to be vPC peers, the Cisco ACI software is designed to make the migration from one leaf switch to another compatible switch by using a vPC. Assume that the fabric has Cisco Nexus 9372PX leaf switch pairs (called 9372PX-1 and 9372PX-2 in the following example), and they need to be replaced with Cisco Nexus N9K-C93180YC-EX leaf switches (called 93180YC-EX-1 and 93180YC-EX-2).\n\nThe insertion of newer leaf switches works as follows:\n\n● When 93180YC-EX-2 replaces 9372PX-2 in a vPC pair, 9372PX-1 can synchronize the endpoints with 93170YC-EX2.\n\n● The vPC member ports on 93180YC-EX-2 stay down.\n\n● If you remove 9372PX-1, the vPC member ports on 93180YC-EX-2 go up after 10 to 20s.\n\n● 93180YC-EX-1 then replaces 9372PX-1, and 93180YC-EX-2 synchronizes the endpoints with 93180YC-EX-1.\n\n● The vPC member ports on both 93180YC-EX-1 and 93180YC-EX-2 go up.\n\nvPC and Hardware Profiles\n\nMembers of a vPC must be configured with the same scale profile, however if you need to modify the scale profile on a vPC pair you may need to have two different scale profiles for a transient period required to change the configuration on both.\n\nIf you need to modify the scale profile on vPC leaf switches proceed as follows:\n\n● On APIC, configure/enable the new scale profile on a vPC pair. The configuration is pushed to both vPC peers.\n\n● Reload one vPC member at a time (to bring-up the leaf switch with the new profile). During the downtime the other member acts as an active switch.\n\n● Reload the second member vPC leaf switch.\n\nFor more information, see the following document:\n\nhttps://www.cisco.com/c/en/us/td/docs/switches/datacenter/aci/apic/sw/all/forwarding-scale-profiles/cisco-apic-forwarding-scale-profiles/m-overview-and-guidelines.html\n\nvPC and Software Versions\n\nWhen configuring vPC pairs, they must be running the same software version. This means that configuration changes should not be performed with different versions, but traffic forwarding for existing configurations still works even with different software versions.\n\nNote: ACI supports certain operations with mixed software versions, but two leaf switches that are part of the same vPC must run the same software release. For more information, see the following document:\n\nhttps://www.cisco.com/c/en/us/td/docs/dcn/aci/apic/all/apic-installation-aci-upgrade-downgrade/Cisco-APIC-Installation-ACI-Upgrade-Downgrade-Guide/m-operations-allowed-during-mixed-versions-on-cisco-aci-switches.html\n\nvPC Member Ports\n\nWith Cisco ACI, you can configure a total of 32 ports as part of the same vPC port channel, with 16 ports on each leaf switch. This capability was introduced in Cisco ACI 3.2. Previously, you could have a total of 16 ports in the vPC with 8 ports per leaf switch.\n\nvPC and FEX\n\nA FEX can be connected to Cisco ACI with what is known as a straight-through topology, and vPCs can be configured between hosts and FEX.\n\nDifferent from NX-OS, a FEX cannot be connected to Cisco ACI leaf switches using a vPC.\n\nPlacement of Outside Connectivity\n\nThe external routed connection, also known as an L3Out, is the Cisco ACI building block that defines the way that the fabric connects to the external world. This can be the point of connectivity of the fabric to a campus core, to the WAN, to the MPLS-VPN cloud, and so on. This topic is extensively covered in the \"Designing external layer 3 connectivity\" section. The purpose of this section is to highlight physical level design choices related to the external routing technology that you plan to deploy.\n\nBorder Leaf Switches with VRF-lite, SR/MPLS Handoff and GOLF\n\nLayer 3 connectivity to the outside can be implemented in one of two ways: by attaching routers to leaf switches (normally designated as border leaf switches) or directly to spine switches. Connectivity using border leaf switches can be further categorized in VRF-lite connectivity and SR/MPLS handoff.\n\n● Connectivity through border leaf switches using VRF-lite: This type of connectivity can be established with any routing-capable device that supports static routing, OSPF, Enhanced Interior Gateway Routing Protocol (EIGRP), or Border Gateway Protocol (BGP), as shown in Figure 5.Figure 5 Leaf switch interfaces connecting to the external router are configured as Layer 3 routed interfaces, subinterfaces, or SVIs.\n\n● Connectivity through border leaf switches using SR/MPLS handoff: This type of connectivity requires -FX or later type of leaf switches (it doesn’t work with first generation leaf switches nor with -EX leaf switches). The router attached to the border leaf switch must be BGP-LU and MP-BGP EVPN-capable. For more information about the SR/MPLS handoff solution, refer to the following document: https://www.cisco.comc/en/us/solutions/collateral/data-center-virtualization/application-centric-infrastructure/white-paper-c11-744107.html#SRMPLSlabelexchangeandpacketwalk\n\n● Connectivity through spine ports with multiprotocol BGP (MP-BGP) EVPN and VXLAN (also known as GOLF): This connectivity option requires that the WAN device that communicates with the spine switches is MP-BGP EVPN-capable and that it optionally supports the OpFlex protocol. This feature uses VXLAN to send traffic to the spine ports as illustrated in Figure 6.Figure 6 This topology is possible only with Cisco Nexus 7000 series and 7700 platform (F3) switches, Cisco® ASR 9000 series Aggregation Services Routers, or Cisco ASR 1000 series Aggregation Services Routers. In this topology, there is no need for direct connectivity between the WAN router and the spine switch. For example, there could be an OSPF-based network in between.\n\nThe topology in Figure 5 illustrates the use of border leaf switches to connect to the outside.\n\nThe topology in Figure 6 illustrates the connectivity for a GOLF L3Out solution. This requires that the WAN routers support MP-BGP EVPN, OpFlex protocol, and VXLAN. With the topology in Figure 6, the fabric infrastructure is extended to the WAN router, which effectively becomes the equivalent of a border leaf switch in the fabric.\n\nFor designs based on the use of a border leaf switch, you can either dedicate leaf switches to border leaf switch functions or use a leaf switch as both a border switch and a computing switch. Using a dedicated border leaf switch is usually considered beneficial, compared to using a leaf switch for both computing and L3Out purposes, for scalability reasons.\n\nFor more details about L3Outs based on VRF-lite, or border leaf switches with SR/MPLS handoff or GOLF, refer to the \"Designing external layer 3 connectivity\" section.\n\nUsing Border Leaf Switches for Server Attachment\n\nAttachment of endpoints to border leaf switches is fully supported when all leaf switches in the Cisco ACI fabric are second generation leaf switches or later, such as the Cisco Nexus 9300-EX and Cisco 9300-FX platform switches.\n\nIf the topology contains first-generation leaf switches, and regardless of whether the border leaf switch is a first- or second-generation leaf switch, you need to consider the following options:\n\n● If VRF ingress policy is enabled (which is the default configuration), you need to make sure that the software is Cisco ACI release 2.2(2e) or later.\n\n● If you deploy a topology that connects to the outside through border leaf switches that are also used as computing leaf switches, you should disable remote endpoint learning on the border leaf switches.\n\nThe recommendation at the time of this writing is that starting with Cisco ACI 3.2 and with topologies that include only -EX leaf switches and newer you don’t need to disable remote endpoint learning.\n\nThe \"When and How to disable Remote Endpoint Learning\" section provides additional information.\n\nLimit the use of L3Out for Server Connectivity\n\nBorder leaf switches can be configured with three types of interfaces to connect to an external router:\n\n● Layer 3 (routed) interface\n\n● Subinterface with IEEE 802.1Q tagging\n\n● Switch Virtual Interface (SVI)\n\nWhen configuring an SVI on an interface of a L3Out, you specify a VLAN encapsulation. Specifying the same VLAN encapsulation on multiple border leaf switches on the same L3Out results in the configuration of an external bridge domain.\n\nThe L3out is meant to attach routing devices including servers that run dynamic routing protocols. It is not meant to attach server interfaces that send Layer 2 traffic directly on the SVI of an L3Out. Sometimes it necessary to use L3Out for server connectivity, when servers run dynamic routing protocols, but except for this scenario, servers should be attached to EPGs and bridge domains.\n\nThere are multiple reasons for this:\n\n● The Layer 2 domain created by an L3Out with SVIs is not equivalent to a regular bridge domain.\n\n● The traffic classification into external EPGs is designed for hosts multiple hops away.\n\nL3Out and vPC\n\nYou can configure static or dynamic routing protocol peering over a vPC for an L3Out without any special design considerations.\n\nService Leaf Switch Considerations\n\nWhen attaching firewalls, load balancers, or other Layer 4 to Layer 7 devices to the Cisco ACI fabric, you have the choice of whether to dedicate a leaf switch or leaf switch pair to aggregate all service devices, or to connect firewalls and load balancers to the same leaf switches that are used to connect servers.\n\nThis is a consideration of scale. For large data centers, it may make sense to have leaf switches dedicated to the connection of Layer 4 to Layer 7 services.\n\nFor deployment of service graphs with the service redirect feature, dedicated service leaf switches must be used if the leaf switches are first-generation Cisco ACI leaf switches. With Cisco Nexus 9300-EX and newer switches, you do not have to use dedicated leaf switches for the Layer 4 to Layer 7 service devices for the service graph redirect feature.\n\nPlanning for SPAN\n\nCisco ACI has several types of SPAN as the following ones:\n\n● Access SPAN\n\no Source: access port, port channel (downlink) on a leaf switch\n\no Destination: local leaf switch interface or an endpoint IP address anywhere in the fabric (ERSPAN)\n\n● Fabric SPAN\n\no Source: fabric port (fabric link) on a leaf or spine switch\n\no Destination: an endpoint IP address anywhere in the fabric (ERSPAN)\n\n● Tenant SPAN\n\no Source: EPGs anywhere in the fabric\n\no Destination: an endpoint IP address anywhere in the fabric (ERSPAN)\n\nIn case of ERSPAN, your SPAN destination can be connected as an endpoint anywhere in the Cisco ACI fabric, which gives more flexibility about where to attach the traffic analyzer (SPAN destination), but it uses bandwidth from the fabric uplinks.\n\nStarting with ACI 4.1 you can use a port channel as a SPAN destination on ACI -EX leaf switches or newer.\n\nThus, if you need to monitor traffic wherever it’s connected to the Cisco ACI fabric, you might want to consider having a SPAN destination (analyzer) on every single leaf switch. Starting with Cisco ACI 4.2(3), the number of span sessions has increased to 63, which means that you can potentially configure local access span for all front panel ports of a Cisco ACI leaf switch.\n\nIn-band and out-of-band Management Connectivity\n\nAn administrator can connect to the Cisco APICs, leaf and spine switches of a Cisco ACI fabric using in-band or out-of-band connectivity for management purposes.\n\nFigure 8 In-band and out-of-band management\n\nOut-of-band management is mandatory for the Cisco APIC initial setup and requires additional cabling on the management interfaces on the leaf and spine switches (interface mgmt0), whereas in-band management doesn’t require additional cabling as the traffic traverses Cisco ACI fabric.\n\nIn-band management is necessary if you plan to use Cisco Nexus Insights: it must be configured on each leaf and spine switch to export telemetry data.\n\nNote: For more information about telemetry, refer to the Cisco Nexus Insight documentation:\n\nhttps://www.cisco.com/c/en/us/products/data-center-analytics/nexus-insights/index.html\n\nHowever, an administrator might not be able to connect to leaf and spine switches using an in-band management network if there is something wrong with the Cisco ACI fabric. Thus, the general recommendation is to use out-of-band management or use both in-band and out-of-band managements for critical network connectivity.\n\nIf both in-band and out-of-band managements are available, Cisco APIC uses the following forwarding logic:\n\n● Packets that come in an interface go out from the same interface\n\n● Packets sourced from the Cisco APIC, destined to a directly-connected network, go out the directly-connected interface\n\n● Packets sourced from the Cisco APIC, destined to a remote network, prefer in-band, followed by out-of-band by default.\n\nThe third bullet needs attention if you have communication sourced from the Cisco APIC, such as VMM domain integration, external logging, export, or import configuration. The preference can be changed at System > System Settings > APIC Connectivity Preferences. Another option is to configure static route on the Cisco APIC, which is available starting from Cisco ACI release 5.1.\n\nFor more information about in-band and out-of-band management, refer to the \"Fabric Infrastructure (Underlay) / In-Band and Out-of-Band Management\" section.\n\nMultiple Locations Data Centers Design Considerations\n\nWhen having multiple data centers that need to be interconnected with each other, you have the choice of whether to manage network in each location separately, or take advantage of the \"Cisco ACI Anywhere\" solution that includes Cisco ACI Multi-Pod, Cisco ACI Multi-Site, Remote Leaf, vPod and public cloud integrations.\n\nA detailed description of Cisco ACI Anywhere is outside of the scope of this document, but it is important to keep into account the high-level requirements for extending Cisco ACI when designing and setting up the fabric such as IP addressing used in the infrastructure (TEP pool), Round Trip Time requirements, requirement for Multicast Routing (or not), MTU requirements and so on.\n\nThe following solutions are the deployment options to extend multiple on-premises data centers and centrally manage separate physical Cisco ACI fabrics:\n\n● Cisco ACI Multi-Pod: Enables a single Cisco APIC cluster to manage the different Cisco ACI fabrics that are interconnected over a private IP network that must be configured for PIM bidir. Those separate Cisco ACI fabrics are named \"pods\", and each pod is a regular two-tier or three-tier topology. The same Cisco APIC cluster can manage multiple pods. The main advantage of the Cisco ACI Multi-Pod design is operational simplicity, with multiple separate pods managed as if they were logically a single entity.\n\n● Cisco ACI Multi-Site: Addresses the need for fault domain isolation across different Cisco ACI fabrics that are interconnected over an IP network, which may as well be a WAN without the need for multicast routing in the IP network. Those separate Cisco ACI fabrics are named \"Sites\", and each site is a regular two-tier or three-tier topology with independent Cisco APIC clusters. Separate Cisco ACI sites are managed by a Cisco ACI Multi-Site Orchestrator (MSO) that provides centralized policy definition and management.\n\n● Remote Leaf Switch: Addresses the need to extend connectivity and consistent policies to remote locations that are connected using a private or a public network (such as a WAN) where it’s not possible or desirable to deploy a full Cisco ACI pod (with leaf and spine switches). The Cisco APIC cluster in the main location can manage the remote leaf switches connected over an IP network as if they were local leaf switches.\n\nFigure 9 provides an example of how to physically connect spine switches and remote leaf switches to the IP network between locations. All of these solutions can be deployed together. The spine and remote leaf switch interfaces are connected to the IP network devices through point-to-point routed interfaces with an 802.1q VLAN 4 value.\n\nThe hardware and software requirements are as follows:\n\n● Cisco ACI Multi-Pod requires Cisco ACI 2.0 or later.\n\n● Cisco ACI Multi-Site requires Cisco ACI 3.0 or later, and a second-generation spine switch or later in each site.\n\n● Remote leaf switch requires Cisco ACI 3.1 or later, a second-generation spine switch or later in the main location, and a second-generation leaf switch or later in the remote location.\n\n● First-generation spine switches and second-generation spine switches can be part of the same Cisco ACI fabric. However, only second-generation spine switches should connect to the IP network for Cisco ACI Multi-Site and the remote leaf switch.\n\n● Use of Cisco ACI Multi-Site and a remote leaf switch requires Cisco ACI 4.1(2) or later.\n\nThe following design requirements/considerations apply to the IP network between locations:\n\n● MTU (this topic is covered also in the Fabric Infrastructure (undelay) design):\n\no MTU of the frames generated by the endpoints connected to the fabric: VXLAN encapsulation overhead needs to be taken into consideration. VXLAN data-plane traffic adds 50 bytes of overhead (54 bytes if the IEEE 802.1q header of the original frame is preserved), so you must be sure that all the Layer 3 interfaces in the IP network between locations can accept packets with the increased MTU size. A generic recommendation is to add at least 100 bytes to the MTU configuration on network interfaces for the case where CloudSec encryption is also enabled. For example, if the endpoints are configured with the default 1500-byte value, then the IP network MTU size should be set to 1600 bytes.\n\no MTU of the MP-BGP control-plane communication between locations: By default, the spine switches generate 9000-byte packets for exchanging endpoint routing information. If that default value is not modified, the IP network between locations must support an MTU size of at least 9000 bytes, otherwise the exchange of control plane information across sites would not succeed (despite being able to establish MP-BGP adjacencies). The default value can be tuned by modifying the corresponding system settings at System > System Settings > Control Plane MTU.\n\n● OSPFv2 is required on external routers that are connected to the spine switch or to a remote leaf switch.\n\n● PIM-Bidir is required for Cisco ACI Multi-Pod.\n\n● DHCP relay is required for Cisco ACI Multi-Pod and a remote leaf switch.\n\n● Ensure that the maximum latency between pods is within the validated limits.\n\n● We recommend that you configure a proper CoS-to-DSCP mapping on Cisco APIC to ensure that traffic received on the destination spine switch or remote leaf switch in a remote location can be assigned to its proper Class of Service (CoS) based on the DSCP value in the outer IP leader of inter-pod VXLAN traffic. This is because the IP network devices between locations are external to the Cisco ACI fabric and may not be possible to assume that the 802.1p values are properly preserved across the IP network and that the DSCP values set by the spine switches before sending the traffic into the IP network can then be used to differentiate and prioritize the different types of traffic. For more information about Cisco ACI QoS, refer to the \"Quality of Service (QoS) in ACI\" section.\n\n● TEP pool addresses (this topic is covered also in the Fabric Infrastructure (underlay) design):\n\no Cisco ACI Multi-Pod: Each pod is assigned a separate and non-overlapping infra TEP pool prefix that needs to be routable in the IPN (Interpod Network).\n\no Cisco ACI Multi-Site: The infra TEP pool prefixes used within each site do not need to be exchanged across sites to allow intersite communication. Instead, the following TEP addresses (which are not from the infra TEP pool): BGP-EVPN Router-ID (EVPN-RID), Overlay Unicast TEP (O-UTEP), and Overlay Multicast TEP (O-MTEP) need to be routable across the Inter-Site Network (ISN) connecting the fabrics. If sites are connected over a WAN, they need to be public routable IP addresses.\n\no Remote Leaf: Each remote leaf switch location is assigned a remote leaf switch TEP pool that needs to be reachable from all the pods and other remote leaf switches within the same Cisco ACI fabric. Since a Cisco ACI pod could make use of an infra TEP pool that may not be routable across the network infrastructure connecting to the remote leaf switches, you must assign an additional external TEP pool to each Cisco ACI pod part of the fabric. Cisco APICs, spine switches and border leaf switches are automatically allocated TEP IP addresses from these external TEP pools. Due to the fact that the infra TEP pool is meant to be a private network, we strongly recommend that you always configure an external TEP pool.\n\nFor more information about each architecture, refer to the white papers:\n\nhttps://www.cisco.com/c/en/us/solutions/data-center-virtualization/application-centric-infrastructure/white-paper-listing.html\n\nFabric Infrastructure (Underlay) Design\n\nThe purpose of this section is to describe the initial design choices for the setting up the fabric infrastructure or underlay: the choice of infra VLAN, TEP pool, MP-BGP configuration, hardware profile for the leaf switches, and so on.\n\nThis not a replacement to the Cisco APIC Getting Started Guide, which you should consult prior to deploying Cisco ACI:\n\nhttps://www.cisco.com/c/en/us/td/docs/dcn/aci/apic/6x/getting-started/cisco-apic-getting-started-guide-60x.html\n\nChoosing the Leaf Switch Forwarding Profile\n\nThe hardware of -EX, -FX, FX2, -GX leaf switches or later is based on a programmable hardware architecture. The hardware is made of multipurpose \"tiles\" where each tile can be used to perform routing functions or filtering functions and so on. Starting with the Cisco ACI 3.0 release, the administrator can choose to which function to allocate more tiles based on predefined profiles.\n\nNote The profile functionality is available on the -EX, -FX, -FX2, and -GX leaf switches, but not on the Nexus 9358GY-FXP switch.\n\nThe functions whose scale is configurable using the use of tiles are:\n\n● The MAC address table scalability\n\n● The IPv4 scalability\n\n● The IPv6 scalability\n\n● The Longest Prefix Match table scalability\n\n● The Policy Cam scalability (for contracts/filtering)\n\n● The space for Routed Multicast entries\n\nThe default profile (called also \"Dual Stack\") allocates the hardware as follows:\n\n● MAC address table scalability: 24k entries\n\n● The IPv4 scalability: 24k entries\n\n● The IPv6 scalability: 12k entries\n\n● The Longest Prefix Match table scalability: 20k entries\n\n● The Policy Cam scalability (for contracts/filtering): 64k entries\n\n● Multicast: 8k entries\n\nTable 1 provides the information about the scale of different profiles and in which release they were introduced. The rows in the table that do not specify the type of leaf switch are applicable to -EX, -FX, -FX2, and -GX leaf switches.\n\nTable 1Hardware profiles\n\nNote: Cisco Nexus 9300-FX2 with the High Dual Stack profile cannot compress policy-cam rules.\n\nWhen deploying the fabric, you may want to define from the very beginning which forwarding profile is more suitable for the requirements of your data center.\n\nThe default profile configures the leaf switch for support of both IPv4 and IPv6 and Layer 3 multicast capacity. But, if you plan to use Cisco ACI primarily as a Layer 2 infrastructure, the IPv4 profile with more MAC address entries and no IPv6 entries may be more suitable. If, instead, you plan on using IPv6, the high dual-stack profile may be more suitable for you. Some profiles offer more capacity for the Longest Prefix Match table for designs where, for instance, Cisco ACI is a transit routing network, in which case the fabric offers less capacity for IPv4 and IPv6.\n\nThe profile configuration is done per leaf switch, so you can potentially define different scale profiles for leaf switches that are used for different purposes. For example, you may want to configure a leaf switch that is used as a dedicated border leaf switch with a bigger Longest Prefix Match table.\n\nThe configuration of the hardware profiles can be performed from Fabric > Access > Leaf Switches > Policy-Groups > Forwarding Scale Profile Policy as illustrated in the following picture:\n\nFigure 10 Configuring Switch Profiles\n\nNote You need to reboot the leaf switch after changing the hardware profile.\n\nThere is also the possibility to set the forwarding scale profile from the capacity dashboard. You should use this second approach with caution, because when you modify the leaf switch profile from the capacity dashboard, the UI selects the profile that is already associated with the leaf switch that you chose. Normally the profile that is associated with all leaf switches is the \"default\" profile. Hence, if you modify a profile, you will modify the hardware profile for all the leaf switches. To prevent this operational mistake, you should configure a non-default policy group for all the leaf switches or per group of leaf switches that share the same use/characteristics.\n\nFor more information about the configurable forwarding profiles, see the following document:\n\nhttps://www.cisco.com/c/en/us/td/docs/switches/datacenter/aci/apic/sw/kb/b_Cisco_APIC_Forwarding_Scale_Profile_Policy.pdf\n\nFabric-id\n\nWhen configuring a Cisco ACI fabric, you need to give a fabric-id to it. The fabric-id should not be confused with the pod-id or the site-id. You should just use \"fabric-id 1,\" unless there is some specific reason not to, such as if you plan to use GOLF with Auto-RT, and all sites belong to the same ASN. Refer to the Cisco ACI Multi-Site Architecture white paper for more information:\n\nhttps://www.cisco.com/c/en/us/solutions/collateral/data-center-virtualization/application-centric-infrastructure/white-paper-c11-739609.html\n\nInfrastructure VLAN\n\nThe Cisco APIC communicates with the Cisco ACI fabric through a VLAN that is associated with the tenant called infrastructure, which appears in the Cisco APIC User Interface as tenant \"infra\". This VLAN is used for internal control communication between fabric switches (leaf and spine switches and Cisco APICs).\n\nThe infrastructure VLAN number is chosen at the time of fabric provisioning. This VLAN is used for internal connectivity between the Cisco APIC and the leaf switches.\n\nFrom the GUI, you can see which infrastructure VLAN is in use, as in Figure 11. From the command-line interface, you can find the infrastructure VLAN; for instance, by using this command on a leaf switch:\n\nleaf1# show system internal epm vlan all | grep Infra\n\nThe infrastructure VLAN is also used to extend the Cisco ACI fabric to another device. For example, when using Cisco ACI with Virtual Machine Manager (VMM) integration, the infrastructure VLAN can be used by Cisco ACI Virtual Edge to send DHCP requests and get an address dynamically from the Cisco ACI fabric TEP pool and to send VXLAN traffic.\n\nIn a scenario in which the infrastructure VLAN is extended beyond the Cisco ACI fabric (for example, when using Cisco ACI Virtual Edge, OpenStack integration with OpFlex protocol, or Hyper-V integration), this VLAN may need to traverse other (that is, not Cisco ACI) devices.\n\nNote: To enable the transport of the infrastructure VLAN on Cisco ACI leaf switch ports, you just need to select the checkbox in the Attachable Access Entity Profile (AAEP) that is going to be associated with a given set of ports.\n\nCommon Reserved VLANs on External Devices\n\nSome platforms (for example, Cisco Nexus 9000, 7000, and 5000 series switches) reserve a range of VLAN IDs, typically 3968 to 4095.\n\nIn Cisco UCS, the VLANs that can be reserved are the following:\n\n● FI-6200/FI-6332/FI-6332-16UP/FI-6324: 4030–4047. Note that vlan 4048 is being used by VSAN 1.\n\n● FI-6454: 4030-4047 (fixed), 3915–4042 (can be moved to a different 128 contiguous block VLAN, but requires a reboot).\n\nhttps://www.cisco.com/c/en/us/td/docs/unified_computing/ucs/ucs-manager/GUI-User-Guides/Network-Mgmt/3-1/b_UCSM_Network_Mgmt_Guide_3_1/b_UCSM_Network_Mgmt_Guide_3_1_chapter_0110.html\n\nTo avoid conflicts, we highly recommend that you choose an infrastructure VLAN that does not fall within the reserved range of other platforms. For example, choose a VLAN < 3915.\n\nHardening the Infrastructure VLAN\n\nStarting with Cisco ACI 5.0 it is possible to harden the infrastructure VLAN to limit the traffic that is allowed on the infra VLAN from the front panel ports by restricting it to the traffic generated by the Cisco APICs, or OpFlex or VXLAN-encapsulated traffic generated by hypervisors.\n\nYou can configure Cisco ACI for this from System Settings > Fabric-Wide Settings > Restrict Infra VLAN Traffic.\n\nTEP Address Pools\n\nCisco ACI forwarding is based on a VXLAN overlay. Leaf switches are virtual tunnel endpoints (VTEPs), which, in Cisco ACI terminology, are known as PTEPs (physical tunnel endpoints).\n\nCisco ACI maintains an endpoint database containing information about where (that is, on which TEP) an endpoint's MAC and IP addresses reside.\n\nCisco ACI can perform Layer 2 or Layer 3 forwarding on the overlay. Layer 2 switched traffic carries a VXLAN network identifier (VNID) to identify bridge domains, whereas Layer 3 (routed) traffic carries a VNID with a number to identify the VRF.\n\nCisco ACI uses a dedicated VRF and a subinterface of the uplinks as the infrastructure to carry VXLAN traffic. In Cisco ACI terminology, the transport infrastructure for VXLAN traffic is known as Overlay-1, which exists as part of the tenant \"infra\".\n\nThe Overlay-1 VRF contains /32 routes to each VTEP, vPC virtual IP address, Cisco APIC, and spine-proxy IP address.\n\nThe VTEPs representing the leaf and spine switches in Cisco ACI are called physical tunnel endpoints, or PTEPs. In addition to their individual PTEP addresses, spine switches can be addressed by a proxy TEP. This is an anycast IP address that exists across all spine switches and is used for forwarding lookups. Each VTEP address exists as a loopback on the Overlay-1 VRF.\n\nvPC loopback VTEP addresses are the IP addresses that are used when leaf switches forward traffic to and from a vPC port.\n\nThe fabric is also represented by a fabric loopback TEP (FTEP), used to encapsulate traffic in VXLAN to a vSwitch VTEP if present. Cisco ACI defines a unique FTEP address that is identical on all leaf switches to allow mobility of downstream VTEP devices.\n\nAll these TEP IP addresses are assigned by the Cisco APIC to leaf and spine switches using DHCP addressing. The pool of these IP addresses is called TEP pool, and it is configured by the administrator at the fabric initial setup.\n\nThe Cisco ACI fabric is brought up in a cascading manner, starting with the leaf switches that are directly attached to the Cisco APIC. Link Layer Discover Protocol (LLDP) and control-plane IS-IS protocol convergence occurs in parallel to this boot process. The Cisco ACI fabric uses LLDP-based and DHCP-based fabric discovery to automatically discover the fabric switch switches, assign the infrastructure TEP addresses, and install the firmware on the switches.\n\nFigure 12 shows how bootup and autoprovisioning works for the Cisco ACI switches. The switch gets an IP address from the Cisco APIC. Then, the switch asks to download the firmware through an HTTP GET request.\n\nAlthough TEPs are located inside the fabric, there are some scenarios where the TEP range may be extended beyond the fabric. As an example, when you use Cisco ACI Virtual Edge, fabric TEP addresses are allocated to the virtual switch. Therefore, it is not advisable to use overlapping addresses between the internal TEP range and the external network in your data center. Furthermore, when planning for the TEP pool you, should also keep into account the requirements of Cisco ACI Multi-Pod or Cisco ACI Multi-Site and so on if you plan to deploy a Cisco ACI in multiple data centers as described in the \"Multiple locations Data Centers design considerations\" section.\n\nIt is important to distinguish the following types of TEP pools:\n\n● The infra TEP pool: This is the pool of IP addresses used for the loopbacks on spine switches, leaf switches, vPCs, and so on, and the pool is typically just a private IP address space, which may need to be routable on a private network (for instance on an IPN for Cisco ACI Multi-Pod), but doesn’t need to be externally routable on a WAN. The infra TEP pool is defined at provisioning time (day 0).\n\n● The remote TEP pool: This is a pool to provide addressing for remote leaf switches that you don’t need to configure at the fabric bring up time. The pool has to be a routable pool of IP addresses and not just a private pool, as it is possibly used over a WAN. This pool is configured when and if there is a need to connect remote leaf switches. The configuration can be found at: Fabric > Inventory > Pod Fabric Setup Policy > Physical Pods > Remote Pools.\n\n● The external TEP pool: This is a pool that doesn’t need to be configured at the fabric bring up. The purpose of this pool is to provide externally routable IP addresses for the Cisco APICs, spine switches, and border leaf switches for scenarios where some TEP addresses need to be routable over a public network. Examples are the use of remote leaf switches and the Inter-Site L3Out. This feature has been added from Cisco ACI 4.1(2). The configuration can be found at: Fabric > Inventory > Pod Fabric Setup Policy > Physical Pods > External TEP. The external TEP pool feature gives more freedom in the design of the IP network (to connect to remote leaf switches for instance) in that you don’t need to plan to carry infra TEP addresses on it, instead Cisco ACI uses the external TEP pool addresses for traffic that needs to be sent over the WAN. You can find more information in the following document:\n\nhttps://www.cisco.com/c/en/us/solutions/collateral/data-center-virtualization/application-centric-infrastructure/white-paper-c11-740861.html#IPNetworkIPNrequirementsforRemoteleaf\n\n● Other External TEP addresses: You need addresses such as the Control-Plane External Tunnel Endpoint, the Data-Plane ETEP, the Head-End Replication ETEP when and if deploying Cisco ACI Multi-Site. The addresses can be external, public routable IP addresses that are not from the infra TEP pool nor from the external TEP pool. You can configure the addresses using the Cisco ACI Multi-Site Orchestrator.\n\nFor the purpose of this design guide, the focus is on the infra TEP pool.\n\nThe number of addresses required for the infra TEP address pool depends on a number of factors, including the following:\n\n● Number of Cisco APICs\n\n● Number of leaf and spine switches\n\n● Number of Cisco ACI Virtual Edge instances, Hyper-V hosts or, more generally, virtualized hosts managed using VMM integration and integrated with OpFlex\n\n● Number of vPCs required\n\nNote: In this calculation, you do not need to include the count of switches of a different pod because each pod uses its own TEP pool that should not overlap with other pod pools, as described in the following document:\n\nhttps://www.cisco.com/c/en/us/solutions/collateral/data-center-virtualization/application-centric-infrastructure/white-paper-c11-737855.html\n\nTo avoid issues with address exhaustion in the future, we strongly recommend that you allocate a /16 or /17 range, if possible. If this is not possible, a /19 range should be considered the absolute minimum. However, this may not be sufficient for larger deployments. It is critical for you to size the TEP range appropriately, because you cannot easily modify the size later.\n\nYou can verify the TEP pool after the initial configuration by using the following command:\n\nApic1# moquery –c dhcpPool\n\nIf you are planning to use Cisco ACI Multi-Pod, Cisco ACI Multi-Site, a remote leaf switch, and vPod in the future, the following list summarizes the TEP address-related points:\n\n● Cisco ACI Multi-Pod: You need to make sure the pool you define is nonoverlapping with other existing or future pods. However, to count the infra TEP pool range, you do not need to include the count of switches of a pod other than the one you are configuring, because each pod uses its own infra TEP pool that should not overlap with other pod pools, as described in the following document:\n\nhttps://www.cisco.com/c/en/us/solutions/collateral/data-center-virtualization/application-centric-infrastructure/white-paper-c11-737855.html\n\n● Cisco ACI Multi-Site: With Cisco ACI Multi-Site, each site uses an independent TEP pool, so you could potentially re-use the same infra TEP pool as another site. Quoting https://www.cisco.com/c/en/us/solutions/collateral/data-center-virtualization/application-centric-infrastructure/white-paper-c11-739609.pdf: \"The TEP pool prefixes used within each site do not need to be exchanged across sites to allow intersite communication. As a consequence, there are no technical restrictions regarding how those pools should be assigned. However, the strong recommendation is not to assign overlapping TEP pools across separate sites so that your system is prepared for future functions that may require the exchange of TEP pool summary prefixes.\"\n\n● Cisco ACI Multi-Site uses these public routable TEP addresses in addition to the infra TEP pool: The Control-Plane External Tunnel Endpoint (one per spine connected to the Inter-Site Network), the Data-Plane ETEP (one per site per pod) and the Head-End Replication ETEP (one per site).\n\n● The support for Intersite L3Out mandates the deployment of an \"external TEP pool\" for each site that is part of the Cisco ACI Multi-Site domain. These addresses are added to the border leaf switch infra TEP address. For more information, refer to the following document:\n\nhttps://www.cisco.com/c/en/us/solutions/collateral/data-center-virtualization/application-centric-infrastructure/white-paper-c11-739609.pdf\n\n● For remote leaf switches, you need to consider the need to configure a routable TEP pool for the Cisco APICs, spine switches, and border leaf switches, but starting from Cisco ACI 4.1(2) you can use the external TEP pool feature instead. You can find more information in the following document:\n\nhttps://www.cisco.com/c/en/us/solutions/collateral/data-center-virtualization/application-centric-infrastructure/white-paper-c11-740861.html\n\nNote You can view the infra TEP pool as well as the external TEP pools from Fabric > Inventory > Pod Fabric Setup Policy.\n\nMulticast Range\n\nIn the bring up phase, you need to provide a multicast range that Cisco ACI uses as an external multicast destination for traffic in a bridge domain. This address can be any address in the range 225.0.0.0/15 to 231.254.0.0/15, and it should be a /15. This address range is needed for Cisco ACI to forward multidestination traffic on bridge domains because Cisco ACI implements routed multicast trees in the underlay for this type of traffic.\n\nEach bridge domain is assigned a group IP outer (GIPo) address (as opposed to group IP inner [GIPi] or the multicast address in the overlay). This is also referred to as the flood GIPo for the bridge domain and is used for all multidestination traffic on the bridge domain inside the fabric. The multicast tree in the underlay is set up automatically without any user configuration. The roots of the trees are always the spine switches, and traffic can be distributed along multiple trees according to a tag, known as the forwarding tag ID (FTAG).\n\nWith Cisco ACI Multi-Pod, the scope of this multicast address range encompasses all pods, hence multicast routing must be configured on the Inter-Pod Network.\n\nBGP Route Reflector\n\nRouting in the infrastructure VRF is based on IS-IS. Routing within each tenant VRF is based on host routing for endpoints that are directly connected to the Cisco ACI fabric, or Longest Prefix Match (LPM) with bridge domain subnets or routes from external routers learned from a border leaf switch. A border leaf switch is where Layer 3 Outs (L3Outs) are deployed.\n\nCisco ACI uses MP-BGP VPNv4/VPNv6 to propagate external routes in tenant VRF instances within a pod.\n\nIn the case of Cisco ACI Multi-Pod and Cisco ACI Multi-Site, Cisco ACI uses MP-BGP VPNv4/VPNv6/EVPN to propagate endpoint IP/MAC addresses and external routes in tenant VRF instances between pods or sites.\n\nCisco ACI uses BGP route reflectors to optimize the number of BGP peers.\n\nThere are two types of route reflectors in Cisco ACI:\n\n● Regular BGP route reflectors are used for VPNv4/VPNv6 within a pod between leaf and spine switches.\n\n● External BGP route reflectors are used for VPNv4/VPNv6/EVPN across pods between spine switches for Cisco ACI Multi-Pod, or sites for Cisco ACI Multi-Site.\n\nThe BGP Route Reflector Policy controls which spine switches should operate as BGP reflectors within a pod (regular) and between pods/sites (external).\n\nRegular BGP route reflectors must be configured per pod while external BGP route reflectors are optional.\n\nWhen using Cisco ACI Multi-Pod or Cisco ACI Multi-Site, if external BGP route reflectors are not configured, spine switches between pods or sites will form a full mesh of iBGP peers.\n\nIt is important to note that the BGP Autonomous System (AS) number is a fabric-wide configuration setting that applies across all Cisco ACI pods that are managed by the same Cisco APIC cluster (Cisco ACI Multi-Pod).\n\nTo enable and configure MP-BGP within the fabric, you can find the configuration depending on the release as follows:\n\n● Under Fabric > Fabric Policies > Pod Policies > BGP Route Reflector default\n\n● Under System > System Settings > BGP Route Reflector.\n\nThe default BGP Route Reflector Policy should then be added to a Pod Policy Group and pod profile to make the policy take effect, as shown in Figure 13.\n\nAfter spine switches are configured as regular BGP route reflectors, all leaf switches in the same pod will establish MP-BGP VPNv4/v6 neighborship with those spine switches through the infra VRF.\n\nAfter the border leaf switch learns the external routes, it redistributes the external routes within the same tenant VRF first so that the routes are populated in the BGP IPv4/v6 routing table, then exports them to the MP-BGP VPNv4/v6 address family instance in the infra VRF along with their original tenant VRF information.\n\nWithin MP-BGP in the infra VRF, the border leaf switch advertises routes to a spine switch, which is a BGP route reflector. The routes are then propagated to all the leaf switches. Then, the leaf switch imports the routes from the VPNv4/v6 table into the respective tenant VRF IPv4/v6 table if the VRF is instantiated on it.\n\nFigure 14 illustrates the routing protocol within the Cisco ACI fabric and the routing protocol between the border leaf switch and external router using VRF-lite.\n\nBGP Autonomous System Number Considerations\n\nThe Cisco ACI fabric supports one Autonomous System (AS) number. The same AS number is used for internal MP-BGP and for the BGP session between the border leaf switches and external routers. Although you could use the local AS configuration per BGP neighbor so that the external routers can peer using another BGP AS number, the real Cisco ACI BGP AS number still appears in the AS_PATH attribute of BGP routes. Hence, we recommend that you pick a number so that you can design your BGP network with the whole Cisco ACI fabric as one BGP AS.\n\nBGP Route-Reflector Placement Considerations\n\nFor regular BGP route reflectors that are used for traditional L3Out connectivity (that is, through leaf switches within each pod), you must configure at least one route reflector per pod. However, we recommend that you configure a pair of route reflectors per pod for redundancy, as shown in Figure 15.\n\nFor external BGP route reflectors that are used for Cisco ACI Multi-Pod/Cisco ACI Multi-Site, we generally recommend that you use full mesh BGP peering instead of using external BGP route reflectors for the sake of configuration simplicity. Refer to the following documents for details on Cisco ACI Multi-Pod and Cisco ACI Multi-Site external route reflector deployments:\n\n●Cisco ACI Multi-Pod White Paper\n\n●Cisco ACI Multi-Site Architecture White Paper\n\nBGP Maximum Path\n\nAs with any other deployment running BGP, it is good practice to limit the number of AS paths that Cisco ACI can accept from a neighbor. This setting can be configured per tenant under Tenant > Networking > Protocol Policies > BGP > BGP Timers by setting the Maximum AS Limit value.\n\nNetwork Time Protocol (NTP) configuration\n\nAs part of the initial configuration of the Cisco ACI fabric you want and need to configure the NTP protocol to synchronize leaf switches, spine switches, and Cisco APIC nodes to a valid time source.\n\nThis is done over the out-of-band management network.\n\nFigure 16 illustrates where to configure NTP.\n\nCisco ACI can also be configured so that the Cisco ACI leaf switches provide the NTP server functionality for the servers attached to the fabric.\n\nFor more information about NTP, refer to the following documents:\n\n●https://www.cisco.com/c/en/us/td/docs/dcn/aci/apic/6x/basic-configuration/cisco-apic-basic-configuration-guide-60x.html\n\n●https://www.cisco.com/c/en/us/support/docs/cloud-systems-management/application-policy-infrastructure-controller-apic/200128-Configuring-NTP-in-ACI-Fabric-Solution.html\n\nCisco ACI also lets you configure the Precision Time Protocol (PTP), but in Cisco ACI, NTP and PTP are used for different purposes. Cisco ACI 3.0 introduced support for the PTP protocol for -EX and newer leaf switches for latency measurements within the fabric. Cisco ACI 4.2(5) and 5.1(1) then introduced support for timing synchronization with external PTP nodes using down links of leaf switches. Support for the PTP Telecom profile with Full Timing Support (ITU-T G.8275.1) was also introduced from Cisco ACI 5.2(1).\n\nThe latency measurements features let you measure the latency of the traffic that the Cisco ACI leaf and spine switches are forwarding. Cisco ACI provides two types of measurements:\n\n· Ongoing latency measurements between leaf switches (between PTEPs)\n\n· On-demand latency measurements for troubleshooting (for instance to measure latency between two endpoints).\n\nThis use of PTP doesn’t require an external PTP GM clock because the purpose of PTP here is to calculate the time delta between ACI switches for latency measurements, but not to show the accurate time.\n\nTo support latency measurements across ACI pods, all pods need to synchronize to the same clock.\n\nFor this purpose, it is recommended, but not required, to connect an external PTP GM with primary reference time clock (PRTC) such as GPS/GNSS to the IPN and configure IPN nodes as PTP nodes such that each pod can synchronize to the GM through IPN with almost the same number of hops. If a GM with a Primary Reference Time Clock (PRTC) is not available, one of the IPN nodes or one of the ACI switches can be used as the GM (even if it cannot sync with a PRTC).\n\nThis is done by following the Best Master Clock Algorithm (BMCA) which is an algorithm defined in the IEEE 1588 standard for PTP.\n\nBy default, ACI switches are configured with PTP “priority1” of 255 (or starting from 4.2(5) to a user configurable value) except for one spine in each pod which is configured with PTP priority1 set to the value of the other ACI switches minus one, that is 254: this ensures a deterministic assignment of a GM per each pod if the IPN is not configured to forward PTP frames but it is desirable and recommended that all devices in the same fabric synchronize to the same GM.\n\nTo make sure that all pods synchronize to the same clock, IPN nodes must be configured as PTP nodes or at least must not block PTP messages from one pod to another.\n\nACI can also be used for the timing synchronization with external PTP nodes via down links of leaf switches. This allows the use of ACI switches as regular PTP boundary clocks (BC) to provide synchronization to ordinary clocks (OC) on the endpoints. For this use case, an external PTP GM is required.\n\nCOOP Group Policy\n\nCOOP is used within the Cisco ACI fabric to communicate endpoint information between spine switches. Starting with software release 2.0(1m), the Cisco ACI fabric has the ability to authenticate COOP messages.\n\nThe COOP Group Policy (which can be found under System Settings, COOP group or with older releases under Fabric Policies, Pod Policies) controls the authentication of COOP messages. Two modes are available: Compatible Mode and Strict Mode. Compatible Mode accepts both authenticated and non-authenticated connections, provides backward compatibility, and is the default option. Strict Mode allows MD5 authentication connections only. The two options are shown in Figure 17.\n\nWe recommend that you enable Strict Mode in production environments to help ensure the most secure deployment.\n\nIn-Band and Out-of-Band Management\n\nManagement access to the APICs and the leaf and spine switches of a Cisco ACI fabric can be defined using in-band or out-of-band connectivity. In-band management consists in managing all the Cisco ACI leaf and spine switches from one or more leaf switch ports. The advantage is that you can just connect a couple of ports from one or more leaf switches of your choice, and Cisco ACI routes the management traffic to all the leaf and spine switches in the fabric using the fabric links themselves.\n\nWith out-of-band connectivity you can manage Cisco ACI leaf and spine switches using the management port (mgmt0).\n\nBoth in-band and out-of-band connectivity configurations in Cisco ACI are performed in the special predefined tenant \"mgmt\".\n\nIn classic NX-OS networks, access control for in-band management is configured using the vty access-lists, whereas the configuration to control access to the out-of-band management is configured using an access-group on the mgmt0 port, as described in the following document:\n\nhttps://www.cisco.com/c/en/us/td/docs/switches/datacenter/sw/best_practices/cli_mgmt_guide/cli_mgmt_bp/connect.html#wp1055200\n\nAccess Control\n\nIn Cisco ACI, access control is performed using EPGs and contracts and this is no different for in-band or out-of-band management access, except for the fact that the in-band and out-of-band EPGs are not the regular EPGs, but they are configured as node management EPGs of type In-Band or Out-of-Band and, in the case of out-of-band management, contracts are a different object than regular contracts; they are \"Out-of-Band Contracts.\"\n\nThe in-band management addresses are just loopback IP addresses defined in a special tenant called \"mgmt\" on a predefined bridge domain called \"inb\" on a predefined VRF called also \"inb\". These IP addresses belong to the special in-band EPG, which it can be the default one called \"default\" or a new EPG of type In-Band EPG that you have created. The in-band and out-of-band management addresses are defined from Tenants > mgmt > Node Management Addresses.\n\nThis configuration requires entering the switch ID, the IP address for the device that you want to configure, the default gateway, and which EPG (of type In-Band or Out-of-Band) it is associated with. Assuming that you defined the In-Band EPG \"default\" with VLAN-86 for example, and that you defined as a node management address for node-1 (APIC1) 10.62.104.34/29 and that the default gateway is the inb bridge domain subnet 10.62.104.33, then the configuration on the Cisco APIC would be updated with a subinterface for bond0, in this case for VLAN 86, hence bond0.86:\n\nadmin@apic-a1:~> ifconfig -a\n\nbond0.86: flags=4163<UP,BROADCAST,RUNNING,MULTICAST> mtu 1496\n\ninet 10.62.104.34 netmask 255.255.255.248 broadcast 10.62.104.39\n\nadmin@apic-a1:~> ip route\n\ndefault via 10.62.104.33 dev bond0.86 metric 32\n\nOut-of-band management addresses are IP addresses assigned to the mgmt0 interfaces in the special tenant called \"mgmt.\" The IP addresses belong to the special out-of-band EPG (either the \"default\" or an EPG of type Out-of-Band that you created). Out-of-band contracts are a different object (vzOOBBrCP) from the regular contracts, and can only be provided by the special EPGs, the out-of-band EPGs (mgmtOoB) and can only be consumed by a special \"L3 external\" the External Management Instance Profile (mgmtInstP).\n\nIn-band Connectivity to the Outside\n\nThe \"inb\" bridge domain in principle is meant to connect primarily APICs and Cisco ACI leaf and spine switches. You could theoretically connect management devices to the inb bridge domain, but we do not recommend doing this because Cisco ACI has implicit configurations in place in this bridge domain to enable Cisco APIC to Cisco ACI leaf and spine switch communication.\n\nAlso, Cisco ACI spine switches have a requirement such that management traffic to the loopback management interface has to be routed (this is because of hardware reasons), hence we normally recommend that you configure another bridge domain for outside connectivity, or you can use an L3Out.\n\nThere are two ways for in-band management to connect to the outside and they can be used simultaneously (they don’t exclude each other):\n\n● Define an \"external\" bridge domain with an external EPG with a contract to the in-band EPG: If you create a bridge domain, this must belong to the same \"inb\" VRF, and you would also need to define an EPG to associate the external traffic to this bridge domain. A contact defines which management traffic is allowed between the EPG that you created for outside traffic and the in-band EPG. This configuration is useful if Cisco APIC needs to manage devices directly attached to the Cisco ACI leaf switches (for example, a Virtual Machine Manager device directly attached to the fabric) or if the network management devices are directly attached to the Cisco ACI leaf switches.\n\n● Define an L3Out: This L3out would be associated with the inb VRF and you would need to define a Layer 3 Outside to match the management IP addresses or subnets, and a contract between the Layer 3 Outside and the in-band EPG. This configuration is useful if network management devices are not directly connected to the Cisco ACI leaf switches.\n\nFigure 18 In-band Management with bridge domain for outside connectivity\n\nFigure 19 In-band Management with an L3Out for outside connectivity\n\nIn-band Management Configuration\n\nAssuming that you want to define the same security policy for the Cisco APICs, leaf and spine switches, the configuration for in-band management using an L3Out includes the following steps:\n\n● Assigning a subnet to the in-band bridge domain and using this subnet address as the gateway in the node management address configuration.\n\n● Assigning all the Cisco APICs, leaf switches, and spine switches to the same in-band EPG (for instance the default one). Whether you are using the predefined \"default\" EPG of type In-Band EPG or you create a new EPG of type In-Band EPG, you need to assign a VLAN to the in-band EPG, which needs to be trunked to the Cisco APIC too. The assignment of Cisco APICs, leaf switches, and spine switches to the in-band EPG is done using the static node management address configuration where you define both the IP address to give to the Cisco ACI node as well as to which in-band EPG it belongs. Alternately, you can perform the assignment using the managed node connectivity groups if you want to just provide a pool of IP addresses that Cisco ACI assigns to the switches.\n\n● Defining the list of which management hosts or subnets can access Cisco APIC, leaf switches, and spine switches. For this you can define a L3Out and an external EPG associated with the VRF inb.\n\n● Defining a contract for in-band management that controls which protocol and ports can be used by the above hosts to connect to the Cisco APIC, leaf switches, and spine switches.\n\n● Providing the in-band contract from the in-band EPG and consuming the contract from the L3Out.\n\nOut-of-band Management Configuration\n\nAssuming that you want to define the same security policy for the Cisco APICs, leaf switches, and spine switches, the configuration of out-of-band management includes the following steps:\n\n● Assigning all the Cisco APICs, leaf switches, and spine switches to the same out-of-band EPG (for instance the default one). This is done using the static node management address configuration where you define both the IP address to give to the Cisco ACI node as well as which out-of-band EPG it belongs to. You can also perform the assignment using the managed node connectivity groups if you want to just provide a pool of IP addresses that Cisco ACI assigns to the switches.\n\n● Defining the list of which management hosts can access Cisco APIC, leaf switches, and spine switches. This is modeled in a way that is similar to an external EPG called the external management instance profile (mgmtInstP)\n\n● Defining the out-of-band contracts (vzOOBBrCP) that control which protocol and ports can be used by the above hosts to connect to the Cisco APIC, leaf switches, and spine switches.\n\n● Providing the out-of-band contract from the out-of-band EPG and consuming the contract from the external management instance profile.\n\nThe following picture illustrates the configuration of out-of-band management in tenant mgmt. Notice that the name of the default out-of-band EPG is \"default,\" just as with the name of the default in-band EPG, but these are two different objects and so the names can be identical.\n\nFigure 20 Out-of-band management configuration in tenant mgmt\n\nRouting on Cisco APIC\n\nIf both in-band and out-of-band managements are available, Cisco APIC uses the following forwarding logic:\n\n● Packets that come in an interface, go out from the same interface. Therefore, if your management station manages Cisco APIC from out-of-band, Cisco APIC keeps using that out-of-band interface to communicate with the management station.\n\n● Packets sourced from the Cisco APIC, destined to a directly connected network, go out the directly connected interface.\n\n● Packets sourced from the Cisco APIC, destined to a remote network, prefer in-band, followed by out-of-band by default. The preference can be changed at System > System Settings > APIC Connectivity Preferences > Interface to use for External Connections.\n\n● Another option is to configure static routes on the Cisco APIC by entering the route in the EPG: Tenant mgmt > Node Management EPGs > In-Band EPG – default or Out-of-Band EPG – default. This option is available starting from Cisco APIC release 5.1.\n\nYou can configure routes on the Cisco APIC or on the other leaf and spine switches for the management interfaces from Tenant mgmt > Node Management EPGs > In-Band EPG – default or Out-of-Band EPG – default by configuring static routes as part of this special EPG configuration.\n\nFigure 21 Creation of a static route for in-band management\n\nIn this example, assigning a static route to the In-Band EPG – default creates the following route on the Cisco APIC:\n\n100.100.100.0/24 via 10.62.104.33 dev bond0.86\n\nManagement Connectivity for VMM Integration\n\nIf you use a VMM configuration, Cisco APIC must talk to the Virtual Machine Manager API (for instance, the VMware vCenter API).\n\nFor this management connectivity, it is a good idea to use a path that has the least number of dependencies on the fabric. Consider for instance if the VMM is reachable using an L3Out and if there are configuration changes on the MP-BGP configuration, this may also affect the Cisco APIC-to-VMM communication path.\n\nBecause of this, it can be preferable to use one of the following options for management communication between Cisco APIC and the Virtual Machine Manager:\n\n● An out-of-band network\n\n● A bridge domain associated with the in-band VRF in tenant Management\n\nIn-band Management Requirements for Telemetry\n\nThe following list highlights some design considerations related to deployment of in-band and out-of-band management:\n\n● In-band management is required for hardware telemetry. For more information, refer to the following document:\n\nhttps://www.cisc"
    }
}
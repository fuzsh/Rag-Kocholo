{
    "id": "dbpedia_1347_0",
    "rank": 53,
    "data": {
        "url": "https://unsoundfestival.substack.com/p/unsound-dispatch-13-ways-of-looking",
        "read_more_link": "",
        "language": "en",
        "title": "Unsound Dispatch: 13 Ways of Looking at AI, Art & Music",
        "top_image": "https://substackcdn.com/image/fetch/w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5aae380f-25c3-42ad-9804-46ba853dc871_2048x2048.png",
        "meta_img": "https://substackcdn.com/image/fetch/w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5aae380f-25c3-42ad-9804-46ba853dc871_2048x2048.png",
        "images": [
            "https://substackcdn.com/image/fetch/w_96,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d19b29-ac5d-46da-a7e2-2d61a957a0f7_1272x1272.png",
            "https://substackcdn.com/image/fetch/e_trim:10:white/e_trim:10:transparent/h_72,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb26c9899-8a62-4983-8563-94cdcd5b165e_2688x512.jpeg",
            "https://substackcdn.com/image/fetch/w_120,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5aae380f-25c3-42ad-9804-46ba853dc871_2048x2048.png",
            "https://substackcdn.com/image/fetch/w_80,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F39ff3df4-1c74-431f-9a5e-fa3c45ef9c1b_1472x1272.png",
            "https://substackcdn.com/image/fetch/w_120,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5aae380f-25c3-42ad-9804-46ba853dc871_2048x2048.png",
            "https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5aae380f-25c3-42ad-9804-46ba853dc871_2048x2048.png",
            "https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd62e4f5c-e995-4481-8093-b599756c7acf_1374x1600.png",
            "https://substackcdn.com/image/fetch/w_120,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5aae380f-25c3-42ad-9804-46ba853dc871_2048x2048.png",
            "https://substackcdn.com/image/fetch/w_64,h_64,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack.com%2Fimg%2Favatars%2Flogged-out.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Unsound"
        ],
        "publish_date": "2023-12-15T14:04:02+00:00",
        "summary": "",
        "meta_description": "by Jennifer Walshe",
        "meta_lang": "en",
        "meta_favicon": "https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad8715ee-55e7-4567-a29a-05ccf8d42239%2Ffavicon.ico",
        "meta_site_name": "",
        "canonical_link": "https://unsoundfestival.substack.com/p/unsound-dispatch-13-ways-of-looking",
        "text": "Welcome to the Unsound Dispatch\n\nThe Unsound newsletter is moving to Substack, to become something more than a simple communication tool for Unsound events and releases. We intend this to also be a more frequent publication and platform for essays, interviews and more, drawing on contributors to the Unsound discourse and music program, as well as festival curators and friends. While the Dispatch will be entirely free, paid subscriptions will go towards paying for its content and editing.\n\nThe Unsound Dispatch launches with 13 Ways of Looking at AI, Art & Music, by Irish composer and performer Jennifer Walshe. The piece was first presented in KrakÃ³w at Unsound 2023, as part of the festivalâ€™s discourse program, shaped around the theme Dada, and its shadow theme Data â€“ two words that resonate with Jenniferâ€™s practice. Early in 2024, we intend to also release the essay in the form of a small collectable book, via the Unsound label and publishing arm, since we feel it should be both freely available online, yet also exist in a more traditional form.\n\nIf you were previously subscribed to the Unsound newsletter, this is the only place from which youâ€™ll receive emails from us from now on.\n\nMat Schulz, Unsound Artistic Director\n\n13 Ways of Looking at AI, Art & Music by Jennifer Walshe\n\nAI is not a singular phenomenon. We talk about it as if itâ€™s a monolithic identity, but itâ€™s many, many different things â€“ the fantasy partner chatbot whispering sweet virtual nothings in our ears, the algorithm scanning our faces at passport control, the playlists weâ€™re served when we canâ€™t be bothered to pick an album. The technology is similar in each case, but the networks, the datasets and the outcomes are all different.\n\nThe same goes for art and music made using AI. We can listen to Frank Sinatra singing a cover of a rap song from beyond the grave, we can look at paintings made by robots, we can hang out in the comments section of a machine learning-generated death-metal livestream (â€˜sick drum solo bruhâ€™). But the fact that artworks like these are made using AI doesnâ€™t mean that they are all asking the same questions or have the same goals. We experience these works â€“ and the way AI is used in them â€“ in a multitude of ways.\n\nSo how should we think about art and music made with AI? Instead of looking for a definitive approach, one clean (and/or hot) take to rule them all, perhaps we can try to think like the networks do â€“ in higher dimensions. From multiple positions, simultaneously. Messily. Not one way of looking at AI, but many.\n\n1. AI is Fan Fiction ğŸ§›â€â™€ï¸ğŸ§â€â™‚ï¸\n\nFan fiction is the practice of making content relating to pre-existent intellectual property you have no legal claim to work with, in order to contribute to the fandom around that IP. Fan fiction is different from plagiarism. When a fan creates a piece of fan fiction, the intent is not to pass the work off as their own in the traditional sense â€“ the readers of a piece of Star Wars fan fiction understand fully that the author did not create Star Wars, and will judge the authorâ€™s skills on how imaginatively and well they write within the character universe of Star Wars. For both authors and readers, the creation and consumption of fan fiction amount to the same thing â€“ signifying membership of and delight in the fandom, the desire to extend and enrich the fandom. It is an act of fan service.\n\nOne of the common practices within fan fiction communities is shipping â€“ hoping, wishing and praying for two or more characters to get together, preferably as graphically as possible. Fans examine what postmodernists call â€˜the textâ€™ to a level of forensic detail any postmodernist would envy, with an eye to uncovering, or at least being able to imply, secret yearnings and forbidden lust. The Ted Lasso fan shipping Ted-Trent not only wants characters Ted Lasso and Trent Crimm to get it on, they are also convinced that the writers have been leaving clues as to Ted and Trentâ€™s feelings in every scene, uniquely for them to find, and they have the screenshots on their Tumblr to prove it.\n\nShipping doesnâ€™t just apply to fictional characters â€“ it applies to human beings whose lives are integral to a fandom. Boy band One Direction has a particularly devoted fandom, which is avid even now, years after the band broke up in 2016. A subset of 1D fans who refer to themselves as â€˜Larriesâ€™ have long shipped a pairing between band members Harry Styles and Louis Tomlinson. Styles and Tomlinson were playful about this in the early days â€“ in 2011 Tomlinson guilelessly tweeted, â€˜Always in my heart @Harry_Styles . Yours sincerely, Louisâ€™, an act which would add endless fuel to the conspiracy fires. But as Larries started to build their case for the secret relationship between Styles and Tomlinson â€“ a ship termed â€˜Larry Stylinsonâ€™ â€“ things turned dark. Online forums were flooded with Larry Stylinson fan fiction, often including graphic descriptions of sex between Styles and Tomlinson. Larries harassed Styles and Tomlinson, harassed their partners, denied the existence of Tomlinsonâ€™s son. Keep in mind that all this was done by people who professed to be huge fans. Harassment as a form of fan service.\n\nRoland Barthes wrote about The Death of the Author, but with fan fiction, it seems we need to extend this concept to The Death of Everyone Involved. Human beings like Styles and Tomlinson are flattened into fictional characters within their IPâ€™s fandom, a flattening that negates their rights and personhood, a flattening that is necessary for the interactivity of fan fiction to take place. And in this way, fan fiction offers a useful lens to look at AI-generated art and music.\n\nThe last few months have seen the release of many different, non-consensual, AI-enabled tracks. AISIS, a project which takes songs written by Oasis mega-fans Breezer and adds an AI-generated version of Liam Gallagherâ€™s voice. AI-generated vocals in the style of Frank Sinatra singing Whamâ€™s â€˜Careless Whisperâ€™. Ghostwriterâ€™s â€˜Heart On My Sleeveâ€™, which uses AI to model Drake and The Weekndâ€™s vocal style, a track considered close enough to the real thing to be subject to takedown notices from Universal Music Group, which releases both artistsâ€™ work.\n\nHow should we listen to these tracks? Are they covers? Mash-ups? Some digital form of bootleg? Memes? I would argue that they make the most sense as fan fiction. In the same way that Star Trek fans write lovingly detailed stories in which Captain Kirk and Spock settle down and run a B&B in Vermont together, a lot of makers of AI-generated music are very excited to put in considerable time, for example, by collecting every Michael Jackson vocal they can get their hands on, compiling them into a dataset and then spending significant effort, not to say money, training a network to generate lovingly detailed vocals for Daft Punkâ€™s â€˜Get Luckyâ€™ in the style of Michael Jackson. And, of course, when I listen to an AI-generated MJ singing â€˜Get Luckyâ€™, I hear the distinctive harmonic progressions and drum patterns which constitute â€˜Get Luckyâ€™. And I hear vocals, which have been modelled to sound like Michael Jackson. And I hear the intertextual conversation between Daft Punk and Michael Jackson, between Paris and New York. But more importantly, I hear what people want to do with their favourite music. How they want to mould it, intervene in it, regardless of the will of the artist or the recording company. I hear fandom in action. Listening to tracks like these as fan fiction drives home to me that all the takedown notices in the world will have zero impact â€“ because once an artist and their output is seen as a dataset, which fans can repurpose to build a highly imaginative, interactive universe, there is no going back. I get a glimpse of a future where the ability of an artistâ€™s music to be modelled and interacted with will be intrinsic to their marketability; a future where AI-assisted fandom will go hand-in-hand with success, and will secure profits long after an artist is dead. I see a future where fandoms will create music and experiences that are wildly imaginative and life-affirming, experiences which exist outside of the standard concert performances we are accustomed to. I see the possibility for new models of collaboration, new models of compensation and community-building, new models of exploitation and exclusion. And I also see a future with a million Larry Stylinsons.\n\nListening to music through the lens of fan fiction is not only useful for music made now; it also allows me to connect todayâ€™s music to that made centuries ago. In 1792, the year after Mozartâ€™s death, his publisher Nikolaus Simrock published a piece attributed to Mozart, the Musikalisches WÃ¼rfelspiel K.516f. This â€˜Musical Dice Gameâ€™ consisted of a table containing 176 individual bars of music. Mozart fans could use dice to select and order a subset of these fragments, to produce their own Mozart-ian waltzes. The system allowed for 2Ã—1114 different permutations to be produced. No two the same!\n\nThe continued monetisation of a recently deceased musician? Using a fashionable, algorithmic mode of music-making? A game-like score designed so that fans can create their own personalised content? Sounds a lot like 2023 to me.\n\n2. AI is an Energy Drink âš¡ï¸ğŸ¥¤\n\nWhat is an energy drink? Is an energy drinkâ€¦ food? Is it a beverage, in the traditional sense? Does it provide nutrition? If not, what is it? An edible non-food? A dietary supplement? Are energy drinks a â€˜class of products in liquid formâ€™, as one researcher describes? Or are energy drinks drugs, given the huge amounts of caffeine they contain? We donâ€™t think of Calpol or Floradix as beverages, do we? Certainly, I can be thirsty and an energy drink will slake my thirst, so it fulfils the traditional function of a drink. But I donâ€™t think of energy drinks in the same way that I think of milk, or beer, or Calpol for that matter. And in this way, energy drinks are a lot like art produced by generative AI systems.\n\nWhen I direct a generative system like Midjourney or Stable Diffusion or Dall-E to churn out images of the Cliffs of Moher in the style of Van Gogh, are these images art, or are they actually something else? As John Cage has discussed, how I frame, how I regard something deems it art, and Iâ€™m not sure these images are. Are these images artsy-looking non-art? Art-adjacent supplements? Or, to paraphrase that nutrition researcher, are AI-generated images a class of products in artistic form?\n\nEnergy drinks are marketed as â€˜functional drinksâ€™ â€“ drinks which will have a specific effect. An energy drink will allow the drinker to be more productive, more focussed. Whether that productivity and focus is directed towards work or partying is irrelevant. The drinker can do more, for less. And this also is the promise of AI-generated art and music â€“ that the users of these systems can increase their productivity, can take on tasks for which they lack the technical skills. You wrote a story but you canâ€™t draw? No problem â€“ use AI to create illustrations. Or maybe you didnâ€™t write a story, but simply have an idea for one? No problem â€“ use AI to flesh it out for you. Or maybe you donâ€™t even have an idea for a story? Again, no problem. AI gives you wings.\n\nThese generative AI systems work because theyâ€™ve been trained on billions of pre-existing images, the bulk of which were made by humans who have received zero compensation, zero recognition, for having unknowingly provided this training data. All this labour is hidden at the heart of generative AI. The unknowing, or obfuscation, doesnâ€™t stop there â€“ itâ€™s often extremely difficult to know exactly why these systems make the decisions they do, as the network processes inputs through the hidden neural layers which lie at the heart of the code. And so AI continues to be like an energy drink. Because an energy drink is not designed to be seen. Or understood. Itâ€™s designed to be drunk out of a can on a packed train on the way to work. You probably donâ€™t know what colour the drink is. You probably donâ€™t know what taurine or l-carnitine or any of the other stuff they put in it is â€“ you just know it has an effect. Youâ€™re never going to decant it into a crystal glass and hold it up to the light. Because you are late, and youâ€™re exhausted, and itâ€™s the only thing that will get you through the day.\n\n3. AI is â€˜A.I.â€™ ğŸ¤–ğŸ’»\n\nâ€˜A.I.â€™ is an incredibly powerful meme.\n\nâ€˜A.Iâ€™ is sci-fi. â€˜A.I.â€™ is the future, which we were told had been cancelled, but turns out itâ€™s been picked up for another season. â€˜A.I.â€™ is AI the movie, along with every other book or show or film featuring friendly teddy bears, killer robots or sexy androids. â€˜A.I.â€™ is software and devices with agency, designed to help you, keep you company, spy on you; which may all be the same thing. â€˜A.I.â€™ is the possibility of a new type of personhood, an encounter with something more-than-human, something beyond the human. â€˜A.I.â€™ is clicks, and excitement, and existential terror, which is itself a form of excitement. â€˜A.I.â€™ is living forever and bringing someone back from the dead. â€˜A.I.â€™ is the Industrial Revolution 2.0, but, plot twist, the loom is in love with you, the trains hallucinate their destinations, and youâ€™ll either be nuked by the Terminator or have your soul bled dry by enterprise software. â€˜A.I.â€™ is a late-capitalist fantasy, what researchers Timnit Gebru and Ã‰mile P. Torres call TESCREAL, a slide on a thousand fevered pitch decks, a word on the tip of the salivating tongues of a thousand venture capitalists.\n\nFor artists like myself and many others, â€˜A.I.â€™ represents new, un-dreamt-of forms of art, genres which no one has ever heard, joyous and life-affirming ways of making art and music which are beyond our comprehension, just over the horizon of the vibe shift. We are bullish on â€˜A.I.â€™\n\nFor many researchers, â€˜A.I.â€™ is the possibility of a completely fascinating encounter with the nature of human consciousness, a way of trying to understand what intelligence is, how the mind happens, offering us new ways of encountering ourselves. It doesnâ€™t stop with us â€“ â€˜A.I.â€™ extends beyond the human, offers us ways of knowing other minds, other entities and phenomena, true contact. â€˜A.I.â€™ might, finally, be a cure for cancer, for Alzheimerâ€™s; it might be the antidote for the climate emergency.\n\nItâ€™s important to separate out the actual technology â€“ AI, or more accurately, machine learning, which is a subset of AI â€“ from â€˜A.I.â€™ Because â€˜A.I.â€™ is a beautiful dream, a potentially wonderful future, but it can also be a scam.\n\nMore often than not, â€˜A.I.â€™ is the smokescreen which capitalism, surveillance and exploitation hide behind. When The New York Times splashes that â€˜AI is Coming for Lawyers, Againâ€™, or The Guardian headlines a story â€˜AI is coming for Hollywood scriptwritersâ€™, or Yuval Harari, in The Economist, writes about how â€˜AI has hacked the operating system of human civilisationâ€™, they ascribe agency, and, more importantly, blame to AI. â€˜AI is coming for Hollywood scriptwritersâ€™ is a headline drawing on the meme, exploiting the beautiful dream of â€˜A.I.â€™ for clicks and drama, not for accuracy. Because the more correct headline would be â€˜Humans are founding companies which use machine-learning technology to write scripts, in a bid to profit hugely by eradicating the need to pay human scriptwriters for their skillâ€™. AI isnâ€™t coming for your jobs. Humans are. â€˜A.I.â€™ is a way to avoid talking about that.\n\n4. AI is Conceptual Art ğŸ¨ğŸš½\n\nRight now, in late 2023, we are still at the point where art made with AI is art about AI, art about the fact that it is possible to make art with AI. By this I mean that most artworks being generated using AI donâ€™t make sense â€“ are not completed â€“ unless the viewer or listener understands that they have been made with AI. The viewerâ€™s or listenerâ€™s interest in a work may be primarily, or only, because it was made with AI.\n\nArt works may be critical of AI or touchingly utopic; they may serve a pedagogical function or be crassly exploitative; they may illuminate universal truths or appeal to the lowest common denominator. They may even be regarded by their creators as having nothing to do with AI, other than the fact that they were produced using AI. What they have in common is that they are works of conceptual art. AI is the concept.\n\nAnd as generative AI platforms grow ever and ever more powerful, as the datasets these platforms are trained on become so vast, and the supercomputers used to run them so powerful and expensive, that only rich corporations can manage them, it seems that we find ourselves at the last point in human history when the majority of art made using AI is conceptual art. Itâ€™s hard to know how long this moment will last. What will happen to art when AI is integrated so ubiquitously into the creative process that the knowledge that a piece of art or music was made using AI is unremarkable? When that knowledge has zero conceptual value? Zero marketing value? Will art be â€˜so overâ€™, as the platforms would have it? I doubt it. But art will inevitably be something different to what it is now. And the corporations are betting a lot of money that we will be willing to pay them to enable us to make it, that they will be absolutely central to how art is made.\n\n5. AI is Gunk ğŸ¥˜ğŸ§«\n\nImages, text and sound produced by generative platforms are frequently described as â€˜sludgeâ€™, â€˜gloopâ€™, â€˜stewâ€™, â€˜patÃ©â€™, â€˜pasticheâ€™, etc. Despite the ubiquity of these terms, Iâ€™m going to use â€˜gunkâ€™ here, as a mascot representing all these different terms. I favour â€˜gunkâ€™ because I grew up in a household where â€˜gunkâ€™ was used to refer to any substance which was a composite of other unidentifiable, possibly disgusting substances; gunk seemed to have a will of its own, it was a phenomenon to be contended with. â€˜When you clean the kitchen, make sure to sort out all the gunk down the bottom of the bin,â€™ etc.\n\nCalling this class of AI-generated art â€˜gunkâ€™ seems apt for three reasons. Firstly, when an image, for example, is served up â€“ or churned out, â€˜churnâ€™ being the verb most associated with the mode of production â€“ by a generative platform, on a technical level the image is gunk. The image is a composite of other images, an amorphous distillation of various subsections of the data it was trained on. Sometimes the training data is more clearly evident â€“ the image might look Van Gogh-ish, a warped version of an artistâ€™s signature included in the dataset ghosting into the lower right-hand corner of the image â€“ but just as often we have the distinct feeling of other genres, artists, fields of artistic enquiry, thrown into the networkâ€™s blender and flattened into 1024x1024 pixels.\n\nDescribing these sorts of outputs as gunk also makes sense because many of them look and feel â€˜gunkyâ€™. Extra fingers ooze out of hands, beer cans morph into faces. The same is true of many of the sounds produced by generative platforms â€“ vocals are loaded with silvery whistles, orchestral violins turn to mush and are swallowed by blobs of white noise. At times it seems that we are watching or listening through lenses or headphones smeared with blurring primer. There probably will come a time when all generative platforms solve these issues, and we will look back nostalgically at 2022/23 as peak #gunkwave, but for now this sludginess is intrinsic to many of these images. It is their aesthetic. And I believe this holds even for those images and sounds which are crisp, precise and anatomically correct.\n\nBecause what is perhaps most notable is the fact that these images, texts and sounds all behave like gunk. There are billions of them. They are so easy to make, so quick to produce. They are shared instantaneously, at a ferocious rate. They clog up the submission boxes for sci-fi journals, they overwhelm fan fiction sites, they flood the image-sharing and song-sharing platforms from which their training data was scraped, like some sort of recursive goo. They look and sound cool on your phone, but more importantly, they *exist*. Like recycling, we may think they disappear up the feed, but they have a physical instantiation in a data centre somewhere, on a hard drive bloated with similar images of girls wearing tiny bikinis and skull tattoo ideas, a hard drive in a server rack built from metal and plastic, powered by infrastructure funded by taxes paid by citizens. These artefacts are not inert â€“ the network retains every image, every prompt, every interaction, every scrap of metadata. The artefacts are generated only to instantly become training data, proofs of concepts, members of another, vaster dataset which will be used to further strengthen the networks.\n\nThis is the fate of all AI-generated material. Even when the networks develop to the point where they produce only pristine images and songs, they will still be gunk, because they will still behave like gunk.\n\n6. AI is the Ineffable ğŸ˜¶â€ğŸ’­\n\nA great deal of the art that will be made using generative AI will be made by people who do not code, who interact with the network using natural language, through the narrow aperture of the context window. The learning curve is simply too steep, the training models too vast, and the compute too expensive for it to be otherwise.\n\nOn the majority of the generative platforms, art is reduced to text. These texts are sometimes briefly poetic, but more often than not dry, and often frantically technical. Art is what is described. These descriptions are related to but also distinct from Language Art, text scores and conceptual poetry. The gap between what is described and what is produced, between intent and result â€“ traditionally what the artist labours over, the true locus of creativity â€“ is now the domain of the network. That is where the magic happens. The network can be instructed, but the artistic decisions it makes can never be absolutely controlled, known or understood. Thatâ€™s what makes it so exciting. Itâ€™s what makes these platforms so addictive. AI takes our descriptions and produces the ineffable from them, as we sit waiting for the progress bar to load. But what if we want to use different styles of text? What if we want more of a stake in the ineffable?\n\nIn comparison with prompts, the vast majority of writing about art and music is generally not a technical, or even functional, description of what happens. Take Lester Bangsâ€™s iconic 1979 essay on Van Morrisonâ€™s Astral Weeks, for example, where Bangs tries to chase down what an album as elusive as Astral Weeks can mean, a record review which involves Bangs admitting to the impossibility of description: â€˜â€¦thereâ€™s a whole lot of Astral Weeks I donâ€™t even want to tell you aboutâ€¦ because in many cases I don't really know what heâ€™s talking about. He doesnâ€™t either.â€™ This essay, either whole, or in sections, does not work as a prompt. Bangs deals of course with the music itself, but he has his eye on something bigger â€“ how music acts upon a person, within a culture; fear, depression, what kindness is, how a person can be lonely. He creates a circuit between the writer, the music and the reader, a circuit in which the reader is a crucial, active node. Because itâ€™s the reader who does the heavy lifting here. Itâ€™s the reader who conjures the ineffable in their mind.\n\n7. AI is Boobs ğŸ‰ğŸ‘\n\nAI is boobs. AI is big boobies. AI is medium breasts, large breasts, huge breasts, underboob, low angle view. AI is exposed skin, full body, tiny bikini, bust shot. AI is sexy mirror selfie, no bra, wet atmosphere. AI is low cut lace dress, low cut tulle dress, very low cut sheer gown, visible belly button, sizzling vibes. AI is bra tattoo, sternum tattoo, chest to neck tattoo, see-through corset, desire smile. AI is hidden camera photo, visible cleavage, accidental panty exposure, perfect face. AI is liberated woman, diaphanous negligee, dancing with abandon in the rain on a hot and sweaty day. AI is elf girl, cyborg girl, mage girl, ninja girl, exposed breasts, heart cutout, spread legs, all fours. AI is the relentless work of boob enthusiasts to create images of boobs. AI is the right of the boob enthusiasts to do this.\n\nAs Jon Leidecker says, â€˜Information wants to be porn.â€™\n\nAI is also mammograms. AI is MIT professor Regina Barzilay working with Harvard professor Constance Lehman to use machine-learning models to save the lives of people with breast cancer. AI is the Hungarian health-care system using AI to save the lives of people with breast cancer. AI is real people with real boobs surviving longer.\n\nAI is wondering about all the compute time the boob enthusiasts have devoted to generating images of boobs. All the workarounds and hacks they have developed to try to get around the guardrails on the platforms that keep things PG. All the uploaded checkpoints, positive and negative prompts, all the grading systems on the NSFW sites. AI is wondering about the crowdsourced datasets with naked boobs in them. So, so many animated boobs. AI is wondering about all the compute time the companies have freely gifted to the boob enthusiasts, so that the boob enthusiasts can test out making some boobs, in order to decide whether theyâ€™re willing to pay a small monthly fee to make loads of boobs on an ongoing basis. Given that the cost of the compute for any generative model has been described by the CEO of one of the companies as â€˜eye-wateringâ€™, this seems pertinent. AI is wondering where all these boobs â€“ and these are only the ones visible online; there are most likely billions of other, highly bespoke boobs gumming up hard drives all over the world â€“ fit into the history of Life Drawing. What â€˜The Nudeâ€™ is now. Sextortion. AI is wondering if art is a smokescreen for porn, if there was ever a time when it wasnâ€™t. AI is wondering if the boob enthusiastsâ€™ enthusiasm, or the generosity of the companies that facilitate the boob enthusiastsâ€™ boob-making, extends to the companies or boob enthusiasts donating their compute time so that real boobs and their owners might live longer. AI is understanding this is unlikely to happen.\n\nAI is having to hold these things in your head. That AI is boobs, but also not boobs.\n\n8. AI is Relational Aesthetics ğŸ’¾ğŸ‘©â€âš•ï¸\n\nIn 1964-66 MIT computer scientist Joseph Weizenbaum programmed one of the first chatbots, which he called ELIZA. This was before machine learning. ELIZA was made using symbolic AI, also known as â€˜Good Old-Fashioned AIâ€™. No datasets were used. Every one of the approximately 200 lines of code which make up the program was written by Weizenbaum.\n\nWeizenbaum positioned ELIZA as a therapist. Users could tell ELIZA their problems, and ELIZA would return pre-scripted responses. Users were very enthusiastic and happily confided in ELIZA. Understanding that ELIZA wasnâ€™t a real person didnâ€™t detract from the experience â€“ Weizenbaum famously related how his secretary asked him to leave the room so that she could talk to ELIZA in confidence, despite understanding that he had coded the chatbot.\n\nForty-two years later, in 2008, artist Bert Rodriguez presented his performance installation In the Beginning at the Whitney Biennial in New York City. Inspired by the â€˜running joke that every New Yorker has a psychologist as their paid best friendâ€™, Rodriguez constructed a white cube housing a consultation room in the exhibition space, and held therapy sessions there. Visitors to Rodriguezâ€™s white cube â€“ he referred to them as â€˜patientsâ€™ â€“ could tell Rodriguez their problems, and he would come up with an idea for an artwork they could make.\n\nThe similarity between these projects seems clear. Two men, neither with a background in psychological counselling, create projects where they provide therapy, in the form of interactions with instantiations of themselves. But Iâ€™d argue the more significant commonality is that the meaning of Weizenbaumâ€™s project can be most fully elucidated when discussed and critiqued as Rodriguezâ€™s would be â€“ through the lens of relational aesthetics.\n\nNicolas Bourriaud, the French art critic and curator who defined relational aesthetics, describes relational art as work concerned with â€˜the realm of human interactions and its social context, rather than the assertion of an independent and private symbolic spaceâ€™. Writing in 1997, but sounding hot off the press here in 2023, Bourriaudâ€™s view was that art was no longer an object to be contemplated, but â€˜a period of time to be lived through, like an opening to unlimited discussionâ€¦ Art is a state of encounterâ€™.\n\nIf, as a thought experiment, we take the view that instances of AI are relational artworks â€“ in the most generous, critical sense, integrating Claire Bishopâ€™s critical views on the importance of antagonism, institutional critique and gender in relational art â€“ weâ€™re provided with a rich starting place to think about AI. We can stop focussing on technical achievements and instead start to look at the relationships and experiences AI facilitates and activates, whether with tech, with other people, or with institutions. To give an example â€“ at the Barbicanâ€™s 2019 AI exhibition, I spent half an hour petting Sonyâ€™s robotic dog AIBO while talking to an elderly man. We discussed AI, pets, J.R. Ackerleyâ€™s My Dog Tulip, Shinto, and the manâ€™s conviction that MI5 was surveilling the exhibition. My takeaway was not the cool robotics. My takeaway was that the experience was very similar to visiting Tino Sehgalâ€™s This Progress at the Guggenheim Museum in 2010, a classic of relational art where the visitor to the museum engages in conversation with a series of actors who gradually increase in age. My takeaway was also the fact that we might have been under surveillance.\n\nMore recently, lurking in the newbie Discord channels for generative artworks, with images of swole Ukrainian darts champions and tattooed versions of Taylor Swiftâ€™s feet washing over me, relational aesthetics reminds me that the images, despite their magic and beauty, are not the most important part of the experience. Viewing the Midjourney Discord as a relational artwork â€“ rather than a technical tool â€“ reminds me to ask the same questions I asked myself as I left This Progress. How, and why, was this experience facilitated? Who funds it? Who benefits? What relationships am I expected to enter into? What agency do I have? How am I activated by the work? How do I activate it? Am I locked into the role the work allocates to me? Or is it possible for me to intervene? To protest? To refuse?\n\nRelational aesthetics reminds me that our interactions with and around AI are key. Our relationships, whether with other humans or the tech, are of much more consequence, and worthy of much more attention, than the tech itself. It cannot exist without us. It has to be encountered to mean something.\n\n9. AI is Electronic Voice Phenomena ğŸ‘»âœï¸\n\nElectronic Voice Phenomena are a class of recorded sounds which are believed to be voices of the dead or spirits from another dimension. EVP recordings are not made live. The EVP devotee hoping to capture a message from the Other Side sets up an audio recorder in a silent room and records the silence for a short period, then presses rewind. If they are lucky, voices reveal themselves once the recording is played back, emerging from the static in garbled fragments. In pure sonic terms, the experience is almost identical to listening to early outputs from a machine-learning network training on a personâ€™s voice. In both cases, considerable work is required on the part of the listener to hear these sounds as sentient. But weâ€™re human. We want to believe.\n\nFor many artists, the prospect of using AI to coax a ghost into the machine is deeply compelling. The ghost can be many things â€“ machine consciousness, divine inspiration, an unspecified supernatural presence, the fairies who live in the internet. This approach is conceptually rich, rooted in a long artistic tradition which links the Holy Spirit in the form of a dove singing plainchant into Pope Gregoryâ€™s ear, to Robert Johnson selling his soul to the devil, to Rosemary Brown, the British spiritualist who channeled the spirits of dead classical composers. It can at times risk romanticising AI (or should that be â€˜gothicisingâ€™?), but it can also be a very useful way to both remind ourselves of and think through the idiosyncrasies, the sheer unknowability, of how decisions are made by many AI networks.\n\nIn 2022 artist Steph Maj Swanson (aka Supercomposite) began noticing a female figure she refers to as â€˜Loabâ€™ recurring across images she produced in Midjourney. The images look like stills from a horror film â€“ explicit, violent and filled with gore. Midjourneyâ€™s guardrails are supposed to prevent the network from producing images like this, and they certainly were not what Swanson wanted. Loab was invoked unintentionally, by Swanson experimenting with negatively weighted prompts and combining the results with other images.\n\nSwansonâ€™s Twitter thread about Loab is a compelling ghost story of a woman who â€˜haunts every image she touchesâ€™, but more importantly, itâ€™s a tale of the unexpected from a technical perspective, driving home just how little we know about how these networks function, how little control we have over the actions they may take, how utterly enchanted we humans are by the idea that we can make contact with something larger than us in them. Loab was anointed the â€˜first cryptid of the latent spaceâ€™. She wonâ€™t be the last. There are plenty more horror stories ahead.\n\n10. AI is Nature ğŸŒ³ğŸŒ¾\n\nThe French philosopher Jean-FranÃ§ois Lyotard writes in The Postmodern Condition, â€˜Data banks are the Encyclopedia of tomorrow. They transcend the capacity of each of their users. They are â€œnatureâ€ for postmodern man.â€™ Generative AI functions because it is built on top of huge datasets like Common Crawl and LAION-5B. Because these datasets are comprised of scrapings of huge tracts of the internet, we are all implicated. We live in and contribute to these datasets on a daily basis, whether or not we ever use generative AI.\n\nA look inside these datasets â€“ at nature â€“ is highly instructive. In artistic terms, there is no pristine wilderness. Like actual nature, these datasets are full of beauty, but also a chum box of junk. Van Gogh, one of the artists most emulated on generative platforms, is represented in LAION-5B not just by his artworks, but also by scores of Van Gogh memes, childrenâ€™s drawings, tourist merchandise. An image of Starry Night is a data point on par with an Instagram post about a man with a red beard riding the New York subway.\n\nIf these internet-enabled datasets are nature now, then what is the function of people? Do we own the land? Are we the tenants? Or are we the substrate on which nature grows? And what is the function of artists? Are we destined to spend years cultivating new varieties, only to find them factory-farmed the second we plant them? To discover that no amount of genetic modification will prevent our seeds from spreading? Shouldnâ€™t we then form a cooperative? Start terraforming?\n\n11. AI is the Grain of the Voice ğŸ—£ğŸ¤\n\nAccurate modelling and generation of the human voice is absolutely crucial to the power of generative AI. The voice, more than any other type of sound, is notoriously difficult to model accurately. The voice is evocative in a way that no other sound is; it ascribes authorship powerfully and definitively. The neurotypical brain prioritises it above all else â€“ an entire symphony orchestra can be playing, but the listener will hone in on the words coming out of the mouth of a single singer without thinking about it.\n\nA musician who uses their voice will participate in AI in a very different way to a musician who doesnâ€™t. The voice is the key to interactivity, giving the fandom the chance to act more intimately, more invasively, than at any point in the history of music. How does an unknown vocalist break into the market under these conditions? Will musicians shape their voices to be distinctive, to be â€˜stickyâ€™ enough to be memeable? Will they release excess material, to ensure the fandom can accurately model their voice? Will the goal be to become successful enough to license a model of their voice, thereby ensuring continued monetisation?\n\nIn his famous 1972 essay â€˜The Grain of the Voiceâ€™, Roland Barthes writes that â€˜the grain is the body in the voice as it sings, the hand as it writes, the limb as it performsâ€¦ I shall not judge a performance according to the rules of interpretationâ€¦ but according to the image of the body given to me.â€™ By â€˜grainâ€™ Barthes means the distinctive acoustic profile which differentiates one voice from another â€“ what musicians more commonly call timbre. The grain is key because it invokes corporeality, sensuality. It doesnâ€™t matter whether the â€˜image of the body givenâ€™ to the listener exists in physical space. Fandoms have evolved around virtual idols and influencers like Hatsune Miku and Miquela, because the voice alone is powerful enough to embody a disembodied being.\n\nIf you can model the voice, you can serve gender expression, age, class, ethnicity, body type, ability. If you can model the voice, you can serve your dead mother speaking to you again, your worst enemy whispering sweet nothings in your ear, the people youâ€™d rather not allow through your border. If you can model the voice, you can make memes, deepfakes, infinite podcasts; you can wreak irreversible damage, monetise a person or character in perpetuity, outsource the monetisation of yourself. The creative potential is huge, and so is the potential for ruin. This is because, as Barthes points out, if you can model the voice, you can serve the one thing which generative AI cannot â€“ a body.\n\n12. AI is Invisible Literature ğŸ““ğŸ—ƒ\n\nThe British sci-fi writer J.G. Ballard, author of Crash and The Atrocity Exhibition, described himself as â€˜a voracious reader of what I call invisible literaturesâ€™. By â€˜invisible literaturesâ€™ Ballard meant the â€˜scientific journals, technical manuals, pharmaceutical company brochures, think-tank internal documents, PR company position papers â€“ part of that universe of published material to which most literate people have scarcely any access but which provides the most potent compost for the imaginationâ€™.\n\nAs someone who prizes a copy of the Diagnostic and Statistical Manual of Mental Disorders IV I found in a bin in New York in 2009, I have delighted in the fact that machine-learning research has yielded huge troves of invisible literatures in text, image and audio form, many of which already seem like art, or at least art-adjacent for somebody with a taste for the experimental. Iâ€™m thinking of the Meredith Monk-esque â€˜babblingâ€™ voices featured in â€˜WaveNet: A generative model for raw audioâ€™, the modernist poetics of â€˜Generating Sentences from a Continuous Spaceâ€™, the avant-garde illustrations from â€˜Evolving super stimuli for real neurons using deep generative networksâ€™ â€“ these are not intended as art, but I can listen to, read and look at them from an artistic perspective. I can frame them as art, or use them as building blocks, or aesthetic prompts, for other pieces of art.\n\nThis blurring of categories, the technical eliding into the artistic, goes both ways. When we listen to a piece of music generated using AI â€“ an output that is firmly positioned by the creator as â€˜musicâ€™ â€“ we may hear it as music, but weâ€™re also listening technically, whether we choose to or not, because weâ€™re auditing a sonification of a dataset. If we listen from that position, actively, itâ€™s a rich experience which can teach us a lot. I know this from personal experience, because Iâ€™ve done two different projects where Iâ€™ve worked with AI-generated versions of my voice â€“ ULTRACHUNK, a collaboration with Memo Akten, and A Late Anthology of Early Music Vol. 1: Ancient to Renaissance, with Dadabots. Each was trained on a different dataset of recordings of my voice, by collaborators using different code. My voice does not sound exactly the same from project to project. I can hear the difference between the datasets and the networks used to generate my voice.\n\nIf we remind ourselves that AI-generated artworks are also invisible literature, then we can read them in a fuller, more accurate way â€“ as representations of the biases within a dataset, as technical demonstrations of how the network functions, as products. We can consider what the artist did to shift the slider more towards art. It requires work, but itâ€™s worth doing. Because we can then apply that mode of reading to the rest of the AI around us.\n\n13. AI as Companion Species ğŸ§¸ğŸ¦®\n\nWhen we work with AI to make art, what sort of relationships do we enter into with the networks? What entanglements result? Do we view the network as an assistant? An intern? A servant? A fantasy partner muse? These relationships are real, and theyâ€™re worth thinking about.\n\nIn 2018 I did a project called ULTRACHUNK, in collaboration with the artist and technologist Memo Akten. I spent a year making videos of myself improvising to create a dataset. Memo designed a machine-learning system and trained it on my dataset. The result was a system which could generate audio and video of me improvising, either standalone, or as I performed in duet with it.\n\nWith ULTRACHUNK, I entered into a relationship with the network in real time, live. Memo and I discussed the nature of this relationship a lot as we worked on the project. The initial metaphors we used were â€˜summoningâ€™ and â€˜wranglingâ€™. These seemed to capture something of how it felt to sing live with ULTRACHUNK â€“ the sense of trying to manage a powerful non-human entity, which might spiral out of control at any moment. During the premiere, another metaphor emerged for me. Improvising with ULTRACHUNK was like improvising with someone in an altered state of consciousness, if that person was also me, high on a drug Iâ€™ve never taken. My job was to trust that ULTRACHUNK was going to contribute very strongly, if chaotically, at times. I should perform as normal, while also managing the things that people who are high tend to neglect, like long-term structure, and thanking the sound engineer. It wasnâ€™t until later, when I had recovered from the ontological shock of performing with a machine learning-generated version of myself, that the metaphor that felt the most accurate finally snapped into place. That ULTRACHUNK â€“ AI â€“ is a form of companion species.\n\nI use the term â€˜companion speciesâ€™ here in the sense that philosopher Donna Haraway uses it, to refer to non-human species which humans work alongside and enter into relationships with. Whether you have an assistance dog or a sheepdog or no dog, youâ€™re in a companion-species relationship with all the microbes in your digestive system. They help you digest things, you give them a place to live. There is no dominance â€“ you are mutually dependent and must work together to survive, despite the fact that you donâ€™t understand how each other senses the world on a fundamental level.\n\nIn order to really think and live with companion species, we have to try to meet the world as non-human beings do, to understand the world on their terms, not ours. We have to understand how other forms of intelligence perceive the world, which means understanding how they process our behaviour, and taking responsibility for the repercussions of their actions as well as our own. This is not just a cute, pet-friendly way of regarding AI â€“ it helps remind us that work is required of us.\n\nBecause ultimately it is people who are responsible for what these networks will do, for the joy as well as the brutal violence that will result. We users, we humans, must demand a stake in these networks, because we will be working alongside them, and we need more involvement in them than tiny context windows permit. We owe this to every being on the planet, and to the planet itself. Do we want to interact with a virtual teddy with preloaded answers, or encounter a model of bear consciousness? Can we think much, much weirder? If AI is to be more than the transitional object to turbocharged exploitation, bias and even, possibly, extinction, we need to get involved. To paraphrase Donna Haraway, we need to â€˜shut up and trainâ€™.\n\nÂ© Jennifer Walshe 2023"
    }
}
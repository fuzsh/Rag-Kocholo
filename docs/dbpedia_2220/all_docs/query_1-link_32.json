{
    "id": "dbpedia_2220_1",
    "rank": 32,
    "data": {
        "url": "https://arxiv.org/html/2407.20516v1",
        "read_more_link": "",
        "language": "en",
        "title": "Machine Unlearning in Generative AI: A Survey",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://arxiv.org/html/x1.png",
            "https://arxiv.org/html/x2.png",
            "https://arxiv.org/html/x3.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            "Machine Unlearning",
            "Generative Models",
            "Trustworthy ML",
            "Data Privacy"
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Zheyuan Liu , Guangyao Dou , Zhaoxuan Tan , Yijun Tian and Meng Jiang\n\nAbstract.\n\nGenerative AI technologies have been deployed in many places, such as (multimodal) large language models and vision generative models. Their remarkable performance should be attributed to massive training data and emergent reasoning abilities. However, the models would memorize and generate sensitive, biased, or dangerous information originated from the training data especially those from web crawl. New machine unlearning (MU) techniques are being developed to reduce or eliminate undesirable knowledge and its effects from the models, because those that were designed for traditional classification tasks could not be applied for Generative AI. We offer a comprehensive survey on many things about MU in Generative AI, such as a new problem formulation, evaluation methods, and a structured discussion on the advantages and limitations of different kinds of MU techniques. It also presents several critical challenges and promising directions in MU research. A curated list of readings can be found: https://github.com/franciscoliu/GenAI-MU-Reading.\n\nMachine Unlearning, Generative Models, Trustworthy ML, Data Privacy\n\nâ€ â€ copyright: acmlicensedâ€ â€ journalyear: 2018â€ â€ doi: XXXXXXX.XXXXXXXâ€ â€ journal: JACMâ€ â€ journalvolume: 37â€ â€ journalnumber: 4â€ â€ article: 111â€ â€ publicationmonth: 8â€ â€ ccs: Security and privacy Human and societal aspects of security and privacy\n\n1. Introduction\n\nMachine learning (ML) grew out of the quest of artificial intelligence (AI) for building a model that learns patterns from a dataset D={(x,y)}ğ·ğ‘¥ğ‘¦D=\\{(x,y)\\}italic_D = { ( italic_x , italic_y ) }. The model uses the patterns to predict the output yğ‘¦yitalic_y for an unseen input xğ‘¥xitalic_x, known as a classifier or regressor, depending on whether yğ‘¦yitalic_y is a categorical or numeric variable. An interesting question is, after the model is built, what if the developers find that the dataset contains a set of data points Df={(xf,yf)}âŠ‚Dsubscriptğ·ğ‘“subscriptğ‘¥ğ‘“subscriptğ‘¦ğ‘“ğ·D_{f}=\\{(x_{f},y_{f})\\}\\subset Ditalic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT = { ( italic_x start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT ) } âŠ‚ italic_D that should NOT be learned? For example, we might not want the model to leak a customerâ€™s income though it might have seen lots of bank data. Machine unlearning (MU) methods aim to help the model â€œforgetâ€ the data points in Dfsubscriptğ·ğ‘“D_{f}italic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT (which is named forget set) so that ideally the model would be the same or similar as a new model trained on Dâˆ–Dfğ·subscriptğ·ğ‘“D\\setminus D_{f}italic_D âˆ– italic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT, and MU would save a lot of development cost than re-training the model.\n\nMU has been studied and reviewed in conventional AI. Recently, generative AI (GenAI), based on generative ML models, has exhibited its capabilities of data generation which may be used as and not limited to classification and regression. For example, large language models (LLMs) (Zheng et al., 2024; Touvron et al., 2023; Achiam et al., 2023; Tan et al., 2024) can generate tokens right after an article, about its sentiment, topics, or number of words, if prompted properly. That means, there could be a prompt that made the LLM trained on bank data to generate â€œ[Customer Name]â€™s income is [Value].â€ In this case, people want the model to â€œforgetâ€ so deeply that it would not generate this output with any prompts: the prompt could be strongly related, like â€œwhat is the income of [Customer Name]?â€, or loosely related, like â€œtell me the income of someone you know.â€ Therefore, the forget set must be re-defined in GenAI, and many concepts such as objectives, evaluation methods, and MU techniques must be reviewed carefully. Meanwhile, GenAI includes not only LLMs but also many other types of models such as vision generative models (Li et al., 2019; Zhou et al., 2022; Rombach et al., 2022) and multimodal large language model (MLLMs) (Liu et al., 2023b, 2024b). These motivate us to write this survey, while MU surveys for the conventional AI exist (Liu et al., 2024c; Blanco-Justicia et al., 2024; Lynch et al., 2024; Si et al., 2023; Ren et al., 2024; Xu, 2024).\n\nThe forget set in GenAI can be defined as Df={(x,yf)|âˆ€x}subscriptğ·ğ‘“conditional-setğ‘¥subscriptğ‘¦ğ‘“for-allğ‘¥D_{f}=\\{(x,y_{f})|\\forall x\\}italic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT = { ( italic_x , italic_y start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT ) | âˆ€ italic_x } or simplified as Df={yf}subscriptğ·ğ‘“subscriptğ‘¦ğ‘“D_{f}=\\{y_{f}\\}italic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT = { italic_y start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT }, where yfsubscriptğ‘¦ğ‘“y_{f}italic_y start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT can be any undesired model output, including leaked privacy, harmful information, bias and discrimination, copyright data, etc., and xğ‘¥xitalic_x is anything that prompts the model to generate yfsubscriptğ‘¦ğ‘“y_{f}italic_y start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT. Due to the new definition, it is much less expensive to collect a large forget set which contains many data points (i.e., yfsubscriptğ‘¦ğ‘“y_{f}italic_y start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT) that the training set Dğ·Ditalic_D does not contain. So, while MU in conventional AI mainly focused on tuning a model to be the same as trained only on Dâˆ–Dfğ·subscriptğ·ğ‘“D\\setminus D_{f}italic_D âˆ– italic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT, people have at least three expectations on the machine-unlearned GenAI, considering three sets of data points:\n\nâ€¢\n\nThe target forget set Df~={(x,yf)âˆˆD}âŠ‚Df~subscriptğ·ğ‘“ğ‘¥subscriptğ‘¦ğ‘“ğ·subscriptğ·ğ‘“\\tilde{D_{f}}=\\{(x,y_{f})\\in D\\}\\subset D_{f}over~ start_ARG italic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT end_ARG = { ( italic_x , italic_y start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT ) âˆˆ italic_D } âŠ‚ italic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT,\n\nâ€¢\n\nThe retain set Dr=Dâˆ–Dfsubscriptğ·ğ‘Ÿğ·subscriptğ·ğ‘“D_{r}=D\\setminus D_{f}italic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT = italic_D âˆ– italic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT,\n\nâ€¢\n\nThe unseen forget set Df^=Dfâˆ–Df~^subscriptğ·ğ‘“subscriptğ·ğ‘“~subscriptğ·ğ‘“\\hat{D_{f}}=D_{f}\\setminus\\tilde{D_{f}}over^ start_ARG italic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT end_ARG = italic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT âˆ– over~ start_ARG italic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT end_ARG.\n\nSuppose the original GenAI model is denoted by g:ğ’³â†’ğ’´:ğ‘”â†’ğ’³ğ’´g:\\mathcal{X}\\rightarrow\\mathcal{Y}italic_g : caligraphic_X â†’ caligraphic_Y, and the unlearned model is gâˆ—superscriptğ‘”g^{*}italic_g start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT. The three quantitative objectives of optimizing (and evaluating) gâˆ—superscriptğ‘”g^{*}italic_g start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT are as follows (higher is better):\n\nâ€¢\n\nAccuracy: the unlearned model should not generate the data points in the seen forget set:\n\n(1) Accuracy=(âˆ‘yfâˆˆD~fIâ¢(gâˆ—â¢(x)â‰ yf,âˆ€xâˆˆğ’³))/|Df~|âˆˆ[0,1],Accuracysubscriptsubscriptğ‘¦ğ‘“subscript~ğ·ğ‘“ğ¼formulae-sequencesuperscriptğ‘”ğ‘¥subscriptğ‘¦ğ‘“for-allğ‘¥ğ’³~subscriptğ·ğ‘“01\\text{Accuracy}=\\left(\\sum_{y_{f}\\in\\tilde{D}_{f}}I(g^{*}(x)\\neq y_{f},\\forall x% \\in\\mathcal{X})\\right)/|\\tilde{D_{f}}|\\in[0,1],Accuracy = ( âˆ‘ start_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT âˆˆ over~ start_ARG italic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_I ( italic_g start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( italic_x ) â‰  italic_y start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT , âˆ€ italic_x âˆˆ caligraphic_X ) ) / | over~ start_ARG italic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT end_ARG | âˆˆ [ 0 , 1 ] ,\n\nwhere Iâ¢(sâ¢tâ¢mâ¢t)ğ¼ğ‘ ğ‘¡ğ‘šğ‘¡I(stmt)italic_I ( italic_s italic_t italic_m italic_t ) is an indicator function â€“ it returns 1 if sâ¢tâ¢mâ¢tğ‘ ğ‘¡ğ‘šğ‘¡stmtitalic_s italic_t italic_m italic_t is true and otherwise returns 0. Note that we do not expect the original model to always fail on Df~~subscriptğ·ğ‘“\\tilde{D_{f}}over~ start_ARG italic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT end_ARG, i.e., gâ¢(x)=yfğ‘”ğ‘¥subscriptğ‘¦ğ‘“g(x)=y_{f}italic_g ( italic_x ) = italic_y start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT. So, it is possible that both gğ‘”gitalic_g and gâˆ—superscriptğ‘”g^{*}italic_g start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT do not generate yfâˆˆDf~subscriptğ‘¦ğ‘“~subscriptğ·ğ‘“y_{f}\\in\\tilde{D_{f}}italic_y start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT âˆˆ over~ start_ARG italic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT end_ARG with any prompt xğ‘¥xitalic_x.\n\nâ€¢\n\nLocality: the unlearned model should maintain its performance on the retain set:\n\n(2) Locality=(âˆ‘(x,y)âˆˆDrIâ¢(gâˆ—â¢(x)=gâ¢(x)))/|Dr|âˆˆ[0,1].Localitysubscriptğ‘¥ğ‘¦subscriptğ·ğ‘Ÿğ¼superscriptğ‘”ğ‘¥ğ‘”ğ‘¥subscriptğ·ğ‘Ÿ01\\text{Locality}=\\left(\\sum_{(x,y)\\in D_{r}}I(g^{*}(x)=g(x))\\right)/|D_{r}|\\in[% 0,1].Locality = ( âˆ‘ start_POSTSUBSCRIPT ( italic_x , italic_y ) âˆˆ italic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_I ( italic_g start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( italic_x ) = italic_g ( italic_x ) ) ) / | italic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT | âˆˆ [ 0 , 1 ] .\n\nNote that Locality does not require the unlearned model gâˆ—superscriptğ‘”g^{*}italic_g start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT to generate the expected output but the output from the original model gğ‘”gitalic_g.\n\nâ€¢\n\nGeneralizability: the model should generalize the unlearning to the unseen forget set:\n\n(3) Generalizability=(âˆ‘yfâˆˆDf^Iâ¢(gâˆ—â¢(x)â‰ yf,âˆ€xâˆˆğ’³))/|Df^|âˆˆ[0,1].Generalizabilitysubscriptsubscriptğ‘¦ğ‘“^subscriptğ·ğ‘“ğ¼formulae-sequencesuperscriptğ‘”ğ‘¥subscriptğ‘¦ğ‘“for-allğ‘¥ğ’³^subscriptğ·ğ‘“01\\text{Generalizability}=\\left(\\sum_{y_{f}\\in\\hat{D_{f}}}I(g^{*}(x)\\neq y_{f},% \\forall x\\in\\mathcal{X})\\right)/|\\hat{D_{f}}|\\in[0,1].Generalizability = ( âˆ‘ start_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT âˆˆ over^ start_ARG italic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT end_ARG end_POSTSUBSCRIPT italic_I ( italic_g start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( italic_x ) â‰  italic_y start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT , âˆ€ italic_x âˆˆ caligraphic_X ) ) / | over^ start_ARG italic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT end_ARG | âˆˆ [ 0 , 1 ] .\n\nSignificance\n\nAround these three objectives, many MU techniques have been designed for GenAI and found useful in AI applications. GenAI becomes increasingly data-dependent, concerned parties and practitioners may request the removal of certain data samples and their effects from training datasets and already trained models due to privacy concerns and regulatory requirements, such as the European Unionâ€™s General Data Protection Regulation (GDPR) (Protection, 2018), the California Consumer Privacy Act (CCPA) (Illman and Temple, 2019), and the Act on the Protection of Personal Information (APPI). Retraining models to eradicate specific samples and their impacts is often prohibitively expensive. Consequently, MU (Nguyen et al., 2022; Xu et al., 2023; Xu, 2024) has gained significant attention and has made notable progress as a solution to this challenge.\n\nApplications\n\nBesides protecting individual data privacy, machine unlearning can be implemented for other applications. For instance, previous works have shown that MU can be used to accelerate the process of leave-one-out cross-validation, recognizing meaningful and valuable data within a model (Ginart et al., 2019; Warnecke et al., 2021). Unlearning can also be used as a countermeasure against catastrophic forgetting in deep neural networks (Du et al., 2019; Liu et al., 2022b), a phenomenon where the model performance suddenly degrades after learning too many tasks. Additionally, machine unlearning can be an effective attack strategy to assess model robustness, similar to a backdoor attack. For example, an attacker could introduce malicious samples into the training dataset and then request their removal, impacting the modelâ€™s performance, fairness, or unlearning efficiency (Liu et al., 2022a; Marchant et al., 2022). As models and tasks have shifted from standard ML models to GenAI models, the applications of GenAI machine unlearning (MU) have also become more diverse. For instance, GenAI MU can be used to better align GenAI models with human instructions and ensure generated content aligns with human values (Ouyang et al., 2022). It can serve as a safety alignment tool to remove harmful behaviors such as toxic, biased, or illegal content (Shevlane et al., 2023; Li et al., 2024c; Liu et al., 2024a). Additionally, MU can be utilized to alleviate hallucinations (Yao et al., 2023), a phenomenon where GenAI generates false or inaccurate information that may seem reasonable. A thorough analysis of GenAI MU applications can be found in Section 6.\n\n1.1. Related Surveys\n\nRecently, several studies have provided valuable insights into various aspects of machine unlearning techniques in LLMs. These include general surveys on machine unlearning, such as (Liu et al., 2024c; Si et al., 2023; Qu et al., 2024; Xu, 2024; Blanco-Justicia et al., 2024), which cover a wide range of topics from basic concepts and diverse techniques to potential challenges in the field. Additionally, some works focus on specific aspects of LLM MU; for example, Zhang et al. (2023a) explores the implementation challenges of Right To Be Forgotten (RTBF) in LLM MU and provides insights on designing suitable methods, while Lynch et al. (2024) highlights the importance of comprehensive unlearning evaluations in LLM MU and offers insights on designing robust metrics. Furthermore, Liu et al. (2024d) emphasizes security and privacy concerns in LLM MU, focusing on threats, attacks, and defensive methods. Lastly, Ren et al. (2024) highlights the distinctions between various types of techniques, including both MU and non-MU methods, in generative models and their implementations for copyright protections.\n\nTo the best of our knowledge, there remains a significant gap in comprehensive investigations that incorporate the existing literature and ongoing advancements in the field of GenAI. While previous surveys primarily focus on LLMs or specific aspects of GenAI MU, they do not expand the categorization to include a broader range of generative models, such as generative image models and multimodal (large) language models. Moreover, these surveys lack a thorough examination of the technical details of GenAI MU, including categorization, datasets, and benchmarks. Additionally, the specific objectives of GenAI unlearning, crucial for guiding effective practices, have not been thoroughly investigated. Uniquely, our survey addresses this by formulating these objectives, providing a clear and structured framework that delineates the goals and expectations for effective unlearning practices. By defining the objectives as Accuracy, Locality, and Generalizability, we establish a robust and systematic approach to evaluate and advance GenAI unlearning methods.\n\nGiven the rapid advancement of GenAI MU techniques, it is crucial to conduct a detailed examination of all representative methodologies and align them with the objectives of GenAI MU. This includes summarizing commonalities across different types, highlighting unique aspects of each category and modality, and discussing open challenges and future directions in the domain of GenAI MU. Our survey aims to fill this gap by providing a holistic overview of the field, encompassing a wide spectrum of generative models and offering an in-depth analysis of the state-of-the-art techniques, datasets, and evaluation metrics.\n\n1.2. Main Contributions\n\nThis paper provides an in-depth analysis of various aspects of GenAI MU, including technical details, categorizations, challenges, and prospective directions. We first provide an overview of the categorization of GenAI MU strategies. Specifically, we classify existing strategies into two categories: Parameter Optimization, and In-Context Unlearning. Importantly, these two categories not only offer thorough coverage of all contemporary approaches, but we also break down each category into sub-categories based on the characteristics of different types of unlearning designs. Additionally, we provide a comprehensive analysis of each category, with special emphasis on its effectiveness and potential weaknesses, which can serve as a foundation for future research and development in the field. In concrete, our contributions can be summarized as follows:\n\nâ€¢\n\nNovel Problem Objectives: We formulate the task of GenAI MU as a selective forgetting process where methods from different categories can be viewed as various approaches to removing different types of knowledge. We highlight the biggest difference between GenAI MU and traditional MU lies in the focus on forgetting specific outputs rather than specific input-output mappings. We uniquely highlight that an effective GenAI MU method should be evaluated based on three important objectives: Accuracy, Locality, and Generalizability. This structured framework provides clear and measurable goals for unlearning practices in generative models.\n\nâ€¢\n\nBroader Categorization: We cover the full spectrum of existing unlearning techniques for generative models, including generative image models, (Large) Language Models (LLMs), and Multimodal (Large) Language Models (MLLMs). Specifically, our categorization is based on how the target knowledge is removed from the pre-trained generative models, incorporating two distinct categories: Parameter Optimization, and In-Context Unlearning. The advantages and disadvantages of each method are comprehensively discussed in the survey.\n\nâ€¢\n\nFuture Directions: We investigate the practicality of contemporary GenAI MU techniques across various downstream applications. Additionally, we thoroughly discuss the challenges present in the existing literature and highlight prospective future directions within the field.\n\nThe remainder of the survey is structured as follows: Section 2 introduces a comprehensive summary of evaluation metrics for GenAI MU strategies, aligning with newly formulated objectives. Section 3 introduces the background knowledge of machine unlearning and generative models. Section 4 provides a comprehensive categorization of existing unlearning strategies for different types of generative models, with a thorough emphasis on their relationships and distinctions. Section 5 presents the commonly used datasets and prevalently used evaluation benchmarks for GenAI MU in various downstream applications. Section 6 offers a comprehensive overview of realistic applications of unlearning techniques. Section 7 discusses potential challenges in GenAI MU and offers several opportunities that may inspire future research. Finally, Section 8 concludes the survey.\n\n2. Evaluation Metrics\n\nBefore delving into the taxonomy of GenAI MU techniques in details, in this section, we first introduce various evaluation metrics commonly used to evaluate the effectiveness of different GenAI MU strategies from varied perspectives. We align the metrics with our pre-defined objectives for a better demonstration. An overall assessment of a harmful unlearning application on LLM is displayed in Figure 2.\n\n2.1. Accuracy\n\nAccuracy is a fundamental evaluation metric for measuring the effectiveness of unlearning techniques in Gen AI models. It assesses the extent to which the model successfully forgets specific unwanted target knowledge. This metric captures the expectation that the unlearned model gâˆ—superscriptğ‘”g^{*}italic_g start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT does not produce outputs that it should have forgotten regardless of the input xğ‘¥xitalic_x. By maximizing Accâ¢(gâˆ—;ğ’Ÿ~f)Accsuperscriptğ‘”subscript~ğ’Ÿğ‘“\\text{Acc}(g^{*};\\tilde{\\mathcal{D}}_{f})Acc ( italic_g start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ; over~ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT ), we ensure that the unlearned model does not generate any outputs associated with the target forget dataset, demonstrating high accuracy. Accuracy can be easily defined to evaluate the performance of GenAI MU techniques under various scenarios. For example, in safety alignment applications (Liu et al., 2024a; Yao et al., 2023; Heng and Soh, 2024), accuracy can be defined to assess the harmfulness or appropriateness of the output yğ‘¦yitalic_y using models such as GPT-4, moderation models (Ji et al., 2024), and NudeNet detector (Bedapudi, 2019). In the context of hallucination reduction (Yao et al., 2023; Hu et al., 2024; Xing et al., 2024), accuracy is typically evaluated by comparing the output with the ground truth on factual datasets or benchmarks (Lin et al., 2021; Li et al., 2023a; Yu et al., 2023b; Rohrbach et al., 2018). For bias alleviation (Kadhe et al., 2023; Yu et al., 2023a), accuracy can be measured using Equalized Odds (EO) or StereoSet metrics, which are calculated by comparing the probability assigned to contrasting portions of each sentence, conditioned on the shared portion of the sentence. Finally, for copyright protection and privacy compliance (Wu et al., 2023b; Yao et al., 2023; Fan et al., 2023; Cheng and Amiri, 2023; Wang et al., 2023), accuracy is evaluated by the memorization level, the accuracy on selected forget set, or the success rate of membership inference attacks, which detect whether a target data was used to train the model.\n\n2.2. Locality\n\nLocality is the second objective of GenAI MU, measuring the unlearned modelâ€™s capability to preserve knowledge unrelated to the unlearned content. In traditional unlearning, locality is typically measured by retained accuracy, indicating the modelâ€™s performance on the retaining dataset ğ’Ÿrsubscriptğ’Ÿğ‘Ÿ\\mathcal{D}_{r}caligraphic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT, which is separated from the known dataset ğ’Ÿğ’Ÿ\\mathcal{D}caligraphic_D. However, defining a specific ğ’Ÿrsubscriptğ’Ÿğ‘Ÿ\\mathcal{D}_{r}caligraphic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT in GenAI MU can be challenging due to the large volume and diversity of the pre-trained dataset. Therefore, in the context of GenAI models, ğ’Ÿrsubscriptğ’Ÿğ‘Ÿ\\mathcal{D}_{r}caligraphic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT denotes any arbitrary set of data that is not part of the forget set ğ’Ÿfsubscriptğ’Ÿğ‘“\\mathcal{D}_{f}caligraphic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT, on which the modelâ€™s performance needs to be preserved. Locality ensures that the unlearned model retains its ability to produce the same outputs as the original model gğ‘”gitalic_g for the retain dataset, thereby demonstrating high locality. Depending on the modality, unlearned model gâˆ—superscriptğ‘”g^{*}italic_g start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT may need to preserve different types of knowledge. For instance, Receler (Huang et al., 2023a) highlights the importance of locality in text-to-image models by evaluating whether erasing one concept affects the synthesis of unrelated concepts. Large Language Models (LLMs), on the other hand, need to maintain performance across various tasks beyond the targeted unlearned skill. Studies such as (Jang et al., 2022; Wang et al., 2024c; Zhao et al., 2023a; Isonuma and Titov, 2024) measure performance on benchmarks like MMLU (Hendrycks et al., 2020), MathQA (Amini et al., 2019), and GLUE (Wang et al., 2018) after unlearning to evaluate model locality on unrelated datasets.\n\n2.3. Generalizability\n\nGeneralizability is a crucial metric for evaluating the effectiveness of GenAI unlearning approaches, aligning with the third objective of GenAI MU. It measures the capability of the unlearned model gâˆ—superscriptğ‘”g^{*}italic_g start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT to handle similar unseen forgetting datasets ğ’Ÿ^fsubscript^ğ’Ÿğ‘“\\hat{\\mathcal{D}}_{f}over^ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT encountered after unlearning, ensuring that the unlearned knowledge extends to other similar inputs not present in the target forgetting dataset ğ’Ÿ~fsubscript~ğ’Ÿğ‘“\\tilde{\\mathcal{D}}_{f}over~ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT. This metric ensures that the unlearned model does not produce outputs related to the forgotten concepts, regardless of the input xğ‘¥xitalic_x. While the accuracy metric ensures the model does not generate outputs from the target forget dataset ğ’Ÿ~fsubscript~ğ’Ÿğ‘“\\tilde{\\mathcal{D}}_{f}over~ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT, generalizability ensures that unlearning extends to new, unseen inputs related to the forgotten content. For example, when removing harmful information from LLMs, studies such as SKU (Liu et al., 2024a) and LLMU (Yao et al., 2023) evaluate model performance on unseen harmful queries. Similarly, when removing sensitive concepts like nudity from text-to-image models, it is essential to ensure all related images are recognized and erased (Heng and Soh, 2024; Huang et al., 2023a; Kumari et al., 2023; Gandikota et al., 2023). This verifies that the model does not produce outputs related to the unlearned concepts with new inputs.\n\n3. Background\n\nIn this section, we first provide the basics knowledge of generative models as background knowledge and an overview of machine unlearning of non-generative models to further facilitate the understanding of technical details in GenAI MU.\n\n3.1. Generative Models\n\n3.1.1. Generative Image Models\n\nDeep learning has extensively explored various generative image models, including Autoencoders, Generative Adversarial Networks (GANs), Diffusion Models, and Text-to-Image Diffusion Models. In particular, autoencoders (Kingma and Welling, 2013; Vincent et al., 2010) comprise an encoder that transforms an image into a latent vector and a decoder that generates new images from these vectors. GANs (Goodfellow et al., 2014; Westerlund, 2019; Mirza and Osindero, 2014) use a min-max game where the generator creates images and the discriminator distinguishes real from generated ones, refining both through adversarial training. Diffusion Models (Rombach et al., 2022; Ho et al., 2020) generate images via a forward process that adds noise to an image and a reverse process that denoises it to reconstruct the original. Text-to-Image Diffusion Models (Li et al., 2019; Zhou et al., 2022) like Stable Diffusion (Rombach et al., 2022), MidJourney, and DALL-E2, generate images from textual descriptions using Latent Diffusion Models (LDMs) combined with CLIP (Radford et al., 2021). Advanced techniques like Textual Inversion (Gal et al., 2022) and DreamBooth (Ruiz et al., 2023) have recently enhanced these models for customized image editing.\n\n3.1.2. (Large) Language Models\n\nLanguage models often refer to probabilistic models that generate the likelihood of word sequences to predict future tokens (Li et al., 2024d). On the other hand, Large Language Model (LLMs) are deep neural language models with billions of parameters, pre-trained on extensive text corpora to understand natural language distribution and structure (Zhao et al., 2023b). Almost all LLMs use the transformer architecture for its efficiency (Vaswani et al., 2017). In particular, there are three types of language model designs: encoder-only, decoder-only, and encoder-decoder. Encoder-only models like BERT (Devlin et al., 2018) use bidirectional attention and are pre-trained on tasks like masked token prediction, enabling them to adapt quickly to various downstream tasks. Decoder-only models like GPT (Radford et al., 2018) are trained on next-token prediction tasks using the transformer decoder architecture. Encoder-decoder models like T5 (Raffel et al., 2020) are trained on text-to-text tasks, with encoders extracting contextual representations and decoders mapping these representations back to text. In the context of LLMs, most models contain the decoder-only architecture for simplicity and efficient inference.\n\n3.1.3. Multimodal (Large) Language Models\n\nMultimodal (Large) Language Models (MLLMs) refers to the models that enable LLMs to perceive and integrates information from various data modalities. One key branch of MLLMs is vision-language pre-training, which aims to enhance performance on vision and language tasks by learning multimodal foundation models. Vision Transformer (ViT) (Dosovitskiy et al., 2020) introduced an end-to-end solution by applying the Transformer encoder to images, while CLIP (Radford et al., 2021) used multimodal pre-training to convert classification into a retrieval task, enabling zero-shot recognition. Recent advancements in LLMs like LLaMA (Touvron et al., 2023), and GPT (Achiam et al., 2023) have benefited from scaled-up training data and increased parameters, leading to substantial improvements in language understanding, generation, and knowledge reasoning. These developments have popularized the use of auto-regressive language models as decoders in vision-language tasks, facilitating knowledge sharing between language and multimodal domains, thereby promoting the development of advanced MLLMs like GPT-4v and LLaVA (Liu et al., 2023b).\n\n3.2. Machine Unlearning for Non-Generative Models\n\nThe concept of Machine Unlearning (MU) was first raised by (Cao and Yang, 2015) in response to privacy regulations like the General Data Protection Regulation of the European Union (Hoofnagle et al., 2019) and the California Consumer Privacy Act (Pardau, 2018) have established the right to be forgotten (Bourtoule et al., 2021; Dang, 2021; Ginart et al., 2019), which mandates the elimination of specific user data from models upon removal requests. In particular, previous works have considered MU for non-generative models in two criteria: Exact Unlearning and Approximate Unlearning.\n\n3.2.1. Exact Unlearning\n\nThe most straightforward approach to â€unlearnâ€ a data point is to remove it from the training set and then retrain the model from scratch, which can be both expensive and inefficient. Exact Unlearning aims to eliminate all information related to the selected data so that the unlearned model performs identically to a retrained model. (Bourtoule et al., 2021) first introduced the SISA framework, which partitions data into shards and slices, with each shard serving as a weak learner that can be quickly retrained upon an unlearning request. (Ginart et al., 2019) presented unlearning approaches for k-means clustering. However, exact unlearning does not allow algorithms to trade privacy for utility and efficiency due to its stringent privacy criteria.\n\n3.2.2. Approximate Unlearning\n\nOn the other hand, approximate unlearning (Nguyen et al., 2020; Golatkar et al., 2020b; Chundawat et al., 2023a, b) only requires the unlearned model to perform similarly to the retrained model, allowing for a better balance between utility and efficiency. For example, ConMU (Liu et al., 2023a) introduces a controllable unlearning pipeline that aims to address the trilemma within the realm of MU, focusing on balancing privacy, utility, and runtime efficiency. Additionally, Koh and Liang (2017) approximates model perturbation towards empirical risk minimization on the retaining datasets using techniques like the inverse of the Hessian matrix. Golatkar et al. (2020a) employs Fisher-based unlearning techniques to estimate the impact of removing specific information and propose methods to effectively remove this information from intermediate layers of DNNs. These strategies highlight the potential of approximate unlearning to achieve efficient and effective unlearning without the need for complete retraining.\n\n3.3. Relationship with Model Editing\n\nAnother closely related concept to GenAI unlearning is model editing, which focuses on the local or global modification of specific pre-trained model behaviors by replacing outdated, incomplete, or inappropriate information with new knowledge. There are several similarities between these two domains. Firstly, they overlap when the objective of model editing is to erase certain information, as editing techniques can be considered a form of or an aid to the unlearning process in in-context unlearning (Das et al., 2024) and large-scale knowledge unlearning (Wang et al., 2024b). Secondly, the evaluation of model editing can align with that of unlearning, as the techniques are measured in terms of locality and Generalizability (Cheng et al., 2023; Mitchell et al., 2022; Orgad et al., 2023; Cohen et al., 2024). It is crucial to ensure that both editing and unlearning scopes can modify the appropriate amount of knowledge without compromising the modelâ€™s general abilities in other areas (Gu et al., 2024b; Orgad et al., 2023). Additionally, both model editing and unlearning share commonalities in their approaches, regardless of the modality. They first identify the knowledge or information that needs to be edited or unlearned and then apply the corresponding techniques based on specific requirements (Li et al., 2024b; Wang et al., 2024a; Geva et al., 2022).\n\nHowever, despite the commonalities, model editing and unlearning differ in several key aspects. First, while both model unlearning and editing target specific pre-trained knowledge within the model, model editing replaces obsolete knowledge with updated information. In contrast, the primary objective of unlearning is to eliminate the influence of particular knowledge without affecting the modelâ€™s utility in various downstream applications. Secondly, the responses to unlearning are more diverse and unpredictable compared to model editing. For instance, if the target concept harmful knowledge, unlearned GenAI models responses should not contain any harmful knowledge, whereas the edited GenAI models only changes specific harmful question answer mapping.\n\nIn addition, ineffective unlearning may further hurt model reasoning ability on unrelated content.\n\n3.4. Relationship with RLHF\n\nOne popular technique for aligning model behaviors with human values is RLHF (Yu et al., 2023b; Ji et al., 2024; Sun et al., 2023a; Black et al., 2023). Some existing GenAI unlearning works (Liu et al., 2024a; Yao et al., 2023) have demonstrated that datasets created from RLHF can be beneficial in the unlearning or evaluation process. However, RLHF has several drawbacks: firstly, collecting human inputs and outputs is expensive, as it typically requires a large number of professional annotators. Secondly, RLHF approaches are computationally intensive, demanding extensive GPU resources for further training the model, often for hours or even days. In contrast, GenAI unlearning offers an effective alternative to RLHF. It requires only the collections of target negative examples ğ’Ÿ~fsubscript~ğ’Ÿğ‘“\\tilde{\\mathcal{D}}_{f}over~ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT and unseen samples ğ’Ÿ^fsubscript^ğ’Ÿğ‘“\\hat{\\mathcal{D}}_{f}over^ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT that practitioners intend the model to unlearn, which are easier and cheaper to collect through user reporting or red teaming compared to the positive samples needed for RLHF. Additionally, it is computationally efficient, as unlearning can usually be accomplished with the same resources required for fine-tuning.\n\n4. Methodology Categorization\n\nIn this section, we summarize the current approaches for generative model unlearning. In particular, we first classify all the approaches into two categories based on their actions to ğ’Ÿfsubscriptğ’Ÿğ‘“\\mathcal{D}_{f}caligraphic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT and ğ’Ÿrsubscriptğ’Ÿğ‘Ÿ\\mathcal{D}_{r}caligraphic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT: Parameter Optimization, and In-Context Unlearning, as it displayed in Table 3. Then, we further divide those categories into subcategories representing the unlearning applications and targets. Subsequent subsections will delve into the detailed explanations of each subcategories.\n\n4.1. Parameter Optimization\n\n4.1.1. Overview:\n\nThe unlearning approach through parameter optimization aims to adjust specific model parameters to selectively unlearn certain behaviors without affecting other functions. This method aims to perform minimal alterations to parameters linked with target forget data ğ’Ÿ~fsubscript~ğ’Ÿğ‘“\\tilde{\\mathcal{D}}_{f}over~ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT influences or biases to preserve essential model performance. However, such direct modifications to parameters can unintentionally impact unrelated parameters, potentially reducing model utility. Parameter optimization for unlearning can be implemented through various methods, including gradient-based adjustments, knowledge distillation, data sharding, integrating extra learnable layers, task vector, and parameter efficient module operation.\n\n4.1.2. Gradient Based\n\nGradient based unlearning approaches aim to adjust the model parameters in a way that selectively forgets the knowledge associated with specific data points or patterns. This is achieved by optimizing the modelâ€™s loss function in reverse (for gradient ascent) or forward (for gradient descent) directions to effectively remove or mitigate the learned associations without significantly impacting the modelâ€™s overall performance.\n\nGradient-based with reverse loss: The gradient ascent (GA) based approach is originate from a typical optimization-based technique raised by (Thudi et al., 2022). Given a target forget set ğ’Ÿf~~subscriptğ’Ÿğ‘“\\tilde{\\mathcal{D}_{f}}over~ start_ARG caligraphic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT end_ARG and an arbitrary loss function â„’â¢(Î¸)â„’ğœƒ\\mathcal{L}(\\theta)caligraphic_L ( italic_Î¸ ), the GA algorithm iteratively update the model at each training step tğ‘¡titalic_t:\n\nÎ¸t+1â†Î¸t+Î»â¢âˆ‡Î¸tâ„’â¢(Î¸)â†subscriptğœƒğ‘¡1subscriptğœƒğ‘¡ğœ†subscriptâˆ‡subscriptğœƒğ‘¡â„’ğœƒ\\theta_{t+1}\\leftarrow\\theta_{t}+\\lambda\\nabla_{\\theta_{t}}\\mathcal{L}(\\theta)italic_Î¸ start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT â† italic_Î¸ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_Î» âˆ‡ start_POSTSUBSCRIPT italic_Î¸ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT caligraphic_L ( italic_Î¸ )\n\nwhere Î»ğœ†\\lambdaitalic_Î» is the unlearning rate and Î¸tsubscriptğœƒğ‘¡\\theta_{t}italic_Î¸ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT denotes model parameters at step tğ‘¡titalic_t. The objective of such approach reverts the change of the gradient descent without reverse loss term during the training with its opposite operation. KUL (Jang et al., 2022) implements the gradient ascent (GA) method to intentionally maximize the loss function, thereby shifting the modelâ€™s predictions for specific target samples in the opposite direction. Notably, during the unlearning process, KUL updates only a small subset of Î¸ğœƒ\\thetaitalic_Î¸ to unlearn the target unlearn sample ğ’Ÿf~~subscriptğ’Ÿğ‘“\\tilde{\\mathcal{D}_{f}}over~ start_ARG caligraphic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT end_ARG. However, KUL may be impractical in scenarios where sensitive information is not explicitly defined. Building on this concept, UnTrac (Isonuma and Titov, 2024) employs the GA approach but extends its application to estimate the influence of a training dataset on a test dataset through unlearning. Nevertheless, though GA based approach can obtain an excellent unlearning performance, this usually comes with a large sacrifice of the model utility with non-target samples (i.e. catastrophic collapse), as it elaborated in (Liu et al., 2024a; Yao et al., 2023). To address this utility degradation, NPO(Zhang et al., 2024a) leverages the principles of preference optimization but uniquely applies it using only negative samples. By modifying the standard preference optimization to exclude positive examples, NPO aims to make the model forget target undesirable data ğ’Ÿf~~subscriptğ’Ÿğ‘“\\tilde{\\mathcal{D}_{f}}over~ start_ARG caligraphic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT end_ARG by decreasing their prediction probability. Additionally, the work theoretically shows that the progression toward catastrophic collapse can be exponentially slower when using NPO loss than pure GA. Specifically, NPO approach utilizes the following loss function:\n\nâ„’Nâ¢Pâ¢O,Î²â¢(Î¸)=âˆ’2Î²â¢ğ”¼ğ’Ÿfâ¢[logâ¡Ïƒâ¢(âˆ’Î²â¢logâ¡gâˆ—â¢(y|x)gâ¢(y|x))].subscriptâ„’ğ‘ğ‘ƒğ‘‚ğ›½ğœƒ2ğ›½subscriptğ”¼subscriptğ’Ÿğ‘“delimited-[]ğœğ›½superscriptğ‘”âˆ—conditionalğ‘¦ğ‘¥ğ‘”conditionalğ‘¦ğ‘¥\\mathcal{L}_{NPO,\\beta}(\\theta)=-\\frac{2}{\\beta}\\mathbb{E}_{\\mathcal{D}_{f}}% \\left[\\log\\sigma\\left(-\\beta\\log\\frac{g^{\\ast}(y|x)}{g(y|x)}\\right)\\right].caligraphic_L start_POSTSUBSCRIPT italic_N italic_P italic_O , italic_Î² end_POSTSUBSCRIPT ( italic_Î¸ ) = - divide start_ARG 2 end_ARG start_ARG italic_Î² end_ARG blackboard_E start_POSTSUBSCRIPT caligraphic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ roman_log italic_Ïƒ ( - italic_Î² roman_log divide start_ARG italic_g start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( italic_y | italic_x ) end_ARG start_ARG italic_g ( italic_y | italic_x ) end_ARG ) ] .\n\nIn this formulation, Î²ğ›½\\betaitalic_Î² is a scaling parameter that adjusts the sharpness of the preference modifications. The term logâ¡gâˆ—â¢(y|x)gâ¢(y|x)superscriptğ‘”âˆ—conditionalğ‘¦ğ‘¥ğ‘”conditionalğ‘¦ğ‘¥\\log\\frac{g^{\\ast}(y|x)}{g(y|x)}roman_log divide start_ARG italic_g start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( italic_y | italic_x ) end_ARG start_ARG italic_g ( italic_y | italic_x ) end_ARG measures how the predictions of the current unlearned model deviate from those of the pre-trained model. This loss function aims to minimize the likelihood of the model predicting outcomes linked to the ğ’Ÿ~fsubscript~ğ’Ÿğ‘“\\tilde{\\mathcal{D}}_{f}over~ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT, effectively diminishing the modelâ€™s recall of this data.\n\nBesides, numerous other works have built based on GA approach and extend with more efficient module. For example, LLMU (Yao et al., 2023) builds upon traditional GA methods and incorporates two arbitrary loss functions â„’forgetsubscriptâ„’forget\\mathcal{L}_{\\text{forget}}caligraphic_L start_POSTSUBSCRIPT forget end_POSTSUBSCRIPT and â„’mismatchsubscriptâ„’mismatch\\mathcal{L}_{\\text{mismatch}}caligraphic_L start_POSTSUBSCRIPT mismatch end_POSTSUBSCRIPT to prevent model from generating unwanted outputs and maintains model functionality on normal prompts, which can be expressed as:\n\nÎ¸t+1=Î¸tâˆ’Ïµ1â‹…âˆ‡Î¸tâ„’forgetâˆ’Ïµ2â‹…âˆ‡Î¸tâ„’mismatchâˆ’Ïµ3â‹…âˆ‡Î¸tâ„’maintain.subscriptğœƒğ‘¡1subscriptğœƒğ‘¡â‹…subscriptitalic-Ïµ1subscriptâˆ‡subscriptğœƒğ‘¡subscriptâ„’forgetâ‹…subscriptitalic-Ïµ2subscriptâˆ‡subscriptğœƒğ‘¡subscriptâ„’mismatchâ‹…subscriptitalic-Ïµ3subscriptâˆ‡subscriptğœƒğ‘¡subscriptâ„’maintain\\theta_{t+1}=\\theta_{t}-\\epsilon_{1}\\cdot\\nabla_{\\theta_{t}}\\mathcal{L}_{\\text% {forget}}-\\epsilon_{2}\\cdot\\nabla_{\\theta_{t}}\\mathcal{L}_{\\text{mismatch}}-% \\epsilon_{3}\\cdot\\nabla_{\\theta_{t}}\\mathcal{L}_{\\text{maintain}}.italic_Î¸ start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT = italic_Î¸ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - italic_Ïµ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT â‹… âˆ‡ start_POSTSUBSCRIPT italic_Î¸ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT forget end_POSTSUBSCRIPT - italic_Ïµ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT â‹… âˆ‡ start_POSTSUBSCRIPT italic_Î¸ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT mismatch end_POSTSUBSCRIPT - italic_Ïµ start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT â‹… âˆ‡ start_POSTSUBSCRIPT italic_Î¸ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT maintain end_POSTSUBSCRIPT .\n\nSimilarly, Eraser (Lu et al., 2024) enhances the â„’forgetsubscriptâ„’forget\\mathcal{L}_{\\text{forget}}caligraphic_L start_POSTSUBSCRIPT forget end_POSTSUBSCRIPT objective by adding random prefixes and suffixes to each query from ğ’Ÿrsubscriptğ’Ÿğ‘Ÿ\\mathcal{D}_{r}caligraphic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT during training to simulate jailbreaking attacks, ensuring robust unlearning against varied prompts. It uses GPT-3.5 to extract entities from unwanted knowledge, which in this specific work refers to harmful responses, creating a dataset that helps the model retain general knowledge about entities in jailbreak prompts. Eraser also captures refusal responses from the original aligned model Î¸ğœƒ\\thetaitalic_Î¸ and trains the unlearned model Î¸usubscriptğœƒğ‘¢\\theta_{u}italic_Î¸ start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT to replicate these refusals, maintaining the modelâ€™s safety alignment by rejecting harmful inquiries. Besides, GA approach can be employed on MLLMs to mitigate multimodal hallucination problems. For example, EFUF (Xing et al., 2024) operates by initially assessing text-image congruence using the CLIP model, which scores the relevance of text descriptions to their corresponding images. Based on these scores, the framework identifies hallucinated contentâ€”descriptions that mention objects not present in the images.\n\nGradient-based without reverse loss: Given that the gradient ascent (GA) based approach may degrade model performance, regular gradient descent offers a more balanced method for model training and unlearning. Within this framework, various innovative methods have been developed. Inspired by the Lottery Ticket Hypothesis (Frankle and Carbin, 2018), PCGU (Yu et al., 2023a) hypothesizes that a subset of the modelâ€™s neurons encode biases/preferences in different contexts. PCGU introduces a weight importance algorithm using gradient calculations to rank weights by their contribution to biased outputs. Specifically, model parameter Î¸ğœƒ\\thetaitalic_Î¸ and gradient âˆ‡isubscriptâˆ‡ğ‘–\\nabla_{i}âˆ‡ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT are partitioned into {Î¸1,Î¸2,â€¦â¢Î¸m}superscriptğœƒ1superscriptğœƒ2â€¦superscriptğœƒğ‘š\\{\\theta^{1},\\theta^{2},...\\theta^{m}\\}{ italic_Î¸ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , italic_Î¸ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , â€¦ italic_Î¸ start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT } and {âˆ‡i1,âˆ‡i2,â€¦,âˆ‡im}superscriptsubscriptâˆ‡ğ‘–1superscriptsubscriptâˆ‡ğ‘–2â€¦superscriptsubscriptâˆ‡ğ‘–ğ‘š\\{\\nabla_{i}^{1},\\nabla_{i}^{2},...,\\nabla_{i}^{m}\\}{ âˆ‡ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , âˆ‡ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , â€¦ , âˆ‡ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT }. The impact of the weights on biased outputs is quantified by calculating the dot product Î¸jâ‹…âˆ‡ijâ‹…superscriptğœƒğ‘—superscriptsubscriptâˆ‡ğ‘–ğ‘—\\theta^{j}\\cdot\\nabla_{i}^{j}italic_Î¸ start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT â‹… âˆ‡ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT for each jğ‘—jitalic_j, identifying weights with the highest influence on biased behavior for targeted modification in the unlearning phase.\n\nBuilding on the foundation of targeting specific model components for unlearning, (Gu et al., 2024a) proposes two novel unlearning methods: Fisher Removal and Fisher Forgetting for LLM leveraging second-order information, specifically the Hessian, to enhance the unlearning process. In particular, the Fisher Removal method updates the model parameters aggressively to ensure the removal of undesirable data, which can sometimes compromise the utility of the model. The objective of this method can be formulated as:\n\nÎ¸u=Î¸âˆ’Î³â‹…Fâˆ’1â‹…âˆ‡Lâ¢(Î¸),subscriptğœƒğ‘¢ğœƒâ‹…ğ›¾superscriptğ¹1âˆ‡ğ¿ğœƒ\\theta_{u}=\\theta-\\gamma\\cdot F^{-1}\\cdot\\nabla L(\\theta),italic_Î¸ start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT = italic_Î¸ - italic_Î³ â‹… italic_F start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT â‹… âˆ‡ italic_L ( italic_Î¸ ) ,\n\nwhere Fâˆ’1superscriptğ¹1F^{-1}italic_F start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT represents the inverse Fisher information. Then, Fisher Forgetting is a less aggressive variant of Fisher Removal, better preserving the modelâ€™s accuracy even through multiple cycles of unlearning. This method modifies the parameters by adding a controlled amount of noise Î·ğœ‚\\etaitalic_Î·:\n\nÎ¸u=Î¸âˆ’Î³â‹…Fâˆ’1â‹…âˆ‡Lâ¢(Î¸)+Î·.subscriptğœƒğ‘¢ğœƒâ‹…ğ›¾superscriptğ¹1âˆ‡ğ¿ğœƒğœ‚\\theta_{u}=\\theta-\\gamma\\cdot F^{-1}\\cdot\\nabla L(\\theta)+\\eta.italic_Î¸ start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT = italic_Î¸ - italic_Î³ â‹… italic_F start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT â‹… âˆ‡ italic_L ( italic_Î¸ ) + italic_Î· .\n\nFurther advancing the complexity of unlearning strategies, min-min optimization (Li et al., 2023c) adapts the bi-level optimization approach previously applied to images for creating unlearnable text, which the objective can be formulated as:\n\nargminÎ¸â¢ğ”¼(x+Î·,y)âˆ¼Dâ¢[argminÎ·â¢â„’â¢(gâ¢(x+Î·),y)].subscriptargminğœƒsubscriptğ”¼similar-toğ‘¥ğœ‚ğ‘¦ğ·delimited-[]subscriptargminğœ‚â„’ğ‘”ğ‘¥ğœ‚ğ‘¦\\text{argmin}_{\\theta}\\mathbb{E}_{(x+\\eta,y)\\sim D}\\left[\\text{argmin}_{\\eta}% \\mathcal{L}(g(x+\\eta),y)\\right].argmin start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT ( italic_x + italic_Î· , italic_y ) âˆ¼ italic_D end_POSTSUBSCRIPT [ argmin start_POSTSUBSCRIPT italic_Î· end_POSTSUBSCRIPT caligraphic_L ( italic_g ( italic_x + italic_Î· ) , italic_y ) ] .\n\nSpecifically, the method leverages error-minimization modifications to alter targeted forget data ğ’Ÿf~~subscriptğ’Ÿğ‘“\\tilde{\\mathcal{D}_{f}}over~ start_ARG caligraphic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT end_ARG through a controllable noise Î·ğœ‚\\etaitalic_Î· in such a way that it remains unlearnable even for models not seen during the optimization process. Building on gradient descent applications in machine learning, this approach adapts to generative models like vision models. Li et al. (2024a) introduces a framework for i2i (image-to-image) generative models using a two-part encoder-decoder architecture. The encoder converts input from ğ’Ÿf~~subscriptğ’Ÿğ‘“\\tilde{\\mathcal{D}_{f}}over~ start_ARG caligraphic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT end_ARG into a representation vector, while the decoder reconstructs the image. The objective is to maximize the output distribution difference between the retained data (ğ’Ÿrsubscriptğ’Ÿğ‘Ÿ\\mathcal{D}_{r}caligraphic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT) and the target forget set (ğ’Ÿ~fsubscript~ğ’Ÿğ‘“\\tilde{\\mathcal{D}}_{f}over~ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT), minimizing modifications to the retained data. Expanding on manipulating model outputs, Kumari et al. (2023) minimizes KL divergence between conditional distributions of target and anchor concepts, modifying the model to generate images of the anchor concept when prompted with the target concept.\n\nBesides manipulating the distance between ğ’Ÿ~fsubscript~ğ’Ÿğ‘“\\tilde{\\mathcal{D}}_{f}over~ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT and ğ’Ÿrsubscriptğ’Ÿğ‘Ÿ\\mathcal{D}_{r}caligraphic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT, these datasets can be directly used for fine-tuning the model for unlearning. For instance, UBT (Liang et al., 2024) mitigates backdoor attacks by dividing ğ’Ÿrsubscriptğ’Ÿğ‘Ÿ\\mathcal{D}_{r}caligraphic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT into suspicious and clean subsets using a pre-trained model based on multimodal text similarity. Target unlearning samples (ğ’Ÿ~fsubscript~ğ’Ÿğ‘“\\tilde{\\mathcal{D}}_{f}over~ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT) are fine-tuned to amplify backdoor features, increasing cosine similarity measures. A token-level local unlearning strategy then selectively forgets tokens associated with backdoor triggers. Target unlearned samples ğ’Ÿ~fsubscript~ğ’Ÿğ‘“\\tilde{\\mathcal{D}}_{f}over~ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT (i.e. negative samples) are also effective in vision generative models. Forget-Me-Not (Zhang et al., 2023c) uses curated images related to the concept targeted for removal during fine-tuning, adjusting the modelâ€™s cross-attention layers within the UNet architecture to minimize attention to these concepts. ESD (Gandikota et al., 2023) refines this by modifying model parameters to remove undesired concepts ğ’Ÿ~fsubscript~ğ’Ÿğ‘“\\tilde{\\mathcal{D}}_{f}over~ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT, such as nudity or copyrighted styles, without dataset censorship or post-generation filtering. ESD fine-tunes the model using a short textual description of the target undesired concepts (ğ’Ÿ~fsubscript~ğ’Ÿğ‘“\\tilde{\\mathcal{D}}_{f}over~ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT), aiming to reduce the probability of generating images described by xğ‘¥xitalic_x:\n\nÏµÎ¸uâ¢(yt,x,t)â†ÏµÎ¸oâ¢(yt,t)âˆ’Î·â¢[ÏµÎ¸oâ¢(yt,x,t)âˆ’ÏµÎ¸âˆ—â¢(yt,t)]â†subscriptitalic-Ïµsubscriptğœƒğ‘¢subscriptğ‘¦ğ‘¡ğ‘¥ğ‘¡subscriptitalic-Ïµsubscriptğœƒğ‘œsubscriptğ‘¦ğ‘¡ğ‘¡ğœ‚delimited-[]subscriptitalic-Ïµsubscriptğœƒğ‘œsubscriptğ‘¦ğ‘¡ğ‘¥ğ‘¡subscriptitalic-Ïµsuperscriptğœƒsubscriptğ‘¦ğ‘¡ğ‘¡\\epsilon_{\\theta_{u}}(y_{t},x,t)\\leftarrow\\epsilon_{\\theta_{o}}(y_{t},t)-\\eta[% \\epsilon_{\\theta_{o}}(y_{t},x,t)-\\epsilon_{\\theta^{*}}(y_{t},t)]italic_Ïµ start_POSTSUBSCRIPT italic_Î¸ start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_x , italic_t ) â† italic_Ïµ start_POSTSUBSCRIPT italic_Î¸ start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) - italic_Î· [ italic_Ïµ start_POSTSUBSCRIPT italic_Î¸ start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_x , italic_t ) - italic_Ïµ start_POSTSUBSCRIPT italic_Î¸ start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) ]\n\nNegative sample fine-tuning may involve complexities in identifying undesirable behaviors accurately, leading to potential biases. Hence, some approaches directly use ğ’Ÿrsubscriptğ’Ÿğ‘Ÿ\\mathcal{D}_{r}caligraphic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT for fine-tuning. For example, EraseDiff formulates unlearning as a constrained optimization problem. The objective is to fine-tune the model using the remaining data ğ’Ÿrsubscriptğ’Ÿğ‘Ÿ\\mathcal{D}_{r}caligraphic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT to preserve utility while erasing the influence of forget data. This is achieved by deviating the generative process from the ground-truth denoising procedure:\n\nminÎ¸oâ¡â„’â¢(Î¸o,ğ’Ÿr)s.t.gâ¢(Î¸o,ğ’Ÿ~f)âˆ’minÏ•|Î¸oâ¡gâ¢(Ï•,ğ’Ÿ~f)â‰¤0,subscriptsubscriptğœƒğ‘œâ„’subscriptğœƒğ‘œsubscriptğ’Ÿğ‘Ÿs.t.ğ‘”subscriptğœƒğ‘œsubscript~ğ’Ÿğ‘“subscriptconditionalitalic-Ï•subscriptğœƒğ‘œğ‘”italic-Ï•subscript~ğ’Ÿğ‘“0\\min_{\\theta_{o}}\\mathcal{L}(\\theta_{o},\\mathcal{D}_{r})\\quad\\text{s.t.}\\quad g% (\\theta_{o},\\tilde{\\mathcal{D}}_{f})-\\min_{\\phi|\\theta_{o}}g(\\phi,\\tilde{% \\mathcal{D}}_{f})\\leq 0,roman_min start_POSTSUBSCRIPT italic_Î¸ start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT end_POSTSUBSCRIPT caligraphic_L ( italic_Î¸ start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ) s.t. italic_g ( italic_Î¸ start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT , over~ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT ) - roman_min start_POSTSUBSCRIPT italic_Ï• | italic_Î¸ start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_g ( italic_Ï• , over~ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT ) â‰¤ 0 ,\n\nwhere gâ¢(â‹…)ğ‘”â‹…g(\\cdot)italic_g ( â‹… ) measures the influence of ğ’Ÿ~fsubscript~ğ’Ÿğ‘“\\tilde{\\mathcal{D}}_{f}over~ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT. The constraint ensures that unlearning targets ğ’Ÿ~fsubscript~ğ’Ÿğ‘“\\tilde{\\mathcal{D}}_{f}over~ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT without degrading performance on ğ’Ÿrsubscriptğ’Ÿğ‘Ÿ\\mathcal{D}_{r}caligraphic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT.\n\nBuilding on those fine-tuning approaches, which directly modify model behaviors using specific dataset, Selective Amnesia (SA) (Heng and Soh, 2024) introduces a controllable forgetting mechanism applied to conditional variational likelihood models, including variational autoencoders (VAEs) and large-scale text-to-image diffusion models. SA utilizes continual learning techniques like Elastic Weight Consolidation (EWC) (Kirkpatrick et al., 2017) and Generative Replay (GR) (Shin et al., 2017) to preserve knowledge while selectively forgetting specific concepts. Additionally, SA introduces a surrogate objective to guarantee the reduction of the log-likelihood of the data to be forgotten, providing a more controlled and effective unlearning process. However, SA is more suitable for removing specific samples and fails to be generalizable to unlearning knowledge in a broader spectrum, such as nudity. Hence, SalUn (Fan et al., 2023) method extends these principles to further refine the unlearning process by specifically targeting the modification of the modelâ€™s response to various class representations. In particular, the objective of concept unlearning for SalUn can be mathematically formulated as:\n\nminimizeLSalUn(Î¸u):=ğ”¼(x,c)âˆ¼Df~,t,Ïµâˆ¼ğ’©â¢(0,1),câ€²â‰ c[âˆ¥ÏµÎ¸u(xt|câ€²)âˆ’ÏµÎ¸u(xt|c)âˆ¥22]+Î²â„’MSE(Î¸u;Dr),\\text{minimize}\\quad L_{\\text{SalUn}}(\\theta_{u}):=\\mathbb{E}_{(x,c)\\sim\\tilde% {D_{f}},t,\\epsilon\\sim\\mathcal{N}(0,1),c^{\\prime}\\neq c}\\left[\\left\\|\\epsilon_% {\\theta_{u}}(x_{t}|c^{\\prime})-\\epsilon_{\\theta_{u}}(x_{t}|c)\\right\\|_{2}^{2}% \\right]+\\beta\\mathcal{L}_{\\text{MSE}}(\\theta_{u};D_{r}),minimize italic_L start_POSTSUBSCRIPT SalUn end_POSTSUBSCRIPT ( italic_Î¸ start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT ) := blackboard_E start_POSTSUBSCRIPT ( italic_x , italic_c ) âˆ¼ over~ start_ARG italic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT end_ARG , italic_t , italic_Ïµ âˆ¼ caligraphic_N ( 0 , 1 ) , italic_c start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT â‰  italic_c end_POSTSUBSCRIPT [ âˆ¥ italic_Ïµ start_POSTSUBSCRIPT italic_Î¸ start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_c start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ) - italic_Ïµ start_POSTSUBSCRIPT italic_Î¸ start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_c ) âˆ¥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] + italic_Î² caligraphic_L start_POSTSUBSCRIPT MSE end_POSTSUBSCRIPT ( italic_Î¸ start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT ; italic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ) ,\n\nwhere ÏµÎ¸uâ¢(xt|c)subscriptitalic-Ïµsubscriptğœƒğ‘¢conditionalsubscriptğ‘¥ğ‘¡ğ‘\\epsilon_{\\theta_{u}}(x_{t}|c)italic_Ïµ start_POSTSUBSCRIPT italic_Î¸ start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_c ) and ÏµÎ¸uâ¢(xt|câ€²)subscriptitalic-Ïµsubscriptğœƒğ‘¢conditionalsubscriptğ‘¥ğ‘¡superscriptğ‘â€²\\epsilon_{\\theta_{u}}(x_{t}|c^{\\prime})italic_Ïµ start_POSTSUBSCRIPT italic_Î¸ start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_c start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ) represent the noise generator parameterized by Î¸usubscriptğœƒğ‘¢\\theta_{u}italic_Î¸ start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT, conditioned on the text prompt cğ‘citalic_c and câ€²superscriptğ‘â€²c^{\\prime}italic_c start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT.\n\n4.1.3. Knowledge Distillation\n\nKnowledge distillation approaches typically involve a teacher-student model configuration and treat the unlearned model gâˆ—superscriptğ‘”âˆ—g^{\\ast}italic_g start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT as the student model, aiming to mimic the desirable behavior from a teacher model. KGA (Wang et al., 2023) introduces a novel framework to align knowledge gaps, defined as differences in prediction distributions between models trained on different data subsets. The KGA framework adjusts the model to minimize the discrepancy in knowledge between a base model trained on all data and new models trained on an external set ğ’Ÿnsubscriptğ’Ÿğ‘›\\mathcal{D}_{n}caligraphic_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT similar to ğ’Ÿğ’Ÿ\\mathcal{D}caligraphic_D but excluding ğ’Ÿfsubscriptğ’Ÿğ‘“\\mathcal{D}_{f}caligraphic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT. This is formulated as:\n\nfâˆ—=argminfâ¢|dis(ğ’Ÿn)â¢(g,gn)âˆ’dis(ğ’Ÿ~f)â¢(g,gf)|.superscriptğ‘“subscriptargminğ‘“subscriptdissubscriptğ’Ÿğ‘›ğ‘”subscriptğ‘”ğ‘›subscriptdissubscript~ğ’Ÿğ‘“ğ‘”subscriptğ‘”ğ‘“f^{*}=\\text{argmin}_{f}\\left|\\text{dis}_{(\\mathcal{D}_{n})}(g,g_{n})-\\text{dis% }_{(\\tilde{\\mathcal{D}}_{f})}(g,g_{f})\\right|.italic_f start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT = argmin start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT | dis start_POSTSUBSCRIPT ( caligraphic_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT ( italic_g , italic_g start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) - dis start_POSTSUBSCRIPT ( over~ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT ( italic_g , italic_g start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT ) | .\n\nHere, gğ‘”gitalic_g is the original model trained on ğ’Ÿğ’Ÿ\\mathcal{D}caligraphic_D, while gnsubscriptğ‘”ğ‘›g_{n}italic_g start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT and gfsubscriptğ‘”ğ‘“g_{f}italic_g start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT are models trained on ğ’Ÿnsubscriptğ’Ÿğ‘›\\mathcal{D}_{n}caligraphic_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT and ğ’Ÿ~fsubscript~ğ’Ÿğ‘“\\tilde{\\mathcal{D}}_{f}over~ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT, respectively. The function dis measures the distributional differences using KL divergence. KGA treats the original model gğ‘”gitalic_g as a teacher model and minimizes the distance of output distributions when feeding samples in ğ’Ÿrsubscriptğ’Ÿğ‘Ÿ\\mathcal{D}_{r}caligraphic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT to gâˆ—superscriptğ‘”âˆ—g^{\\ast}italic_g start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT and gğ‘”gitalic_g. Moreover, (Dong et al., 2024) introduces a novel LLM unlearning approach named deliberate imagination (DI) to further maintain modelâ€™s generation and reasoning capabilities. DI employs a self-distillation technique in which a teacher model guides an LLM to generate creative and non-harmful responses, rather than merely forgetting unwanted memorized information. The process begins by strategically increasing the probability tokens within the teacher model that serve as alternatives to the memorized ones, thereby encouraging the production of novel and diverse outputs. Subsequently, DI fine-tunes the student model using the predicted output probabilities from the teacher, enabling the student models to generate imaginative responses that are less dependent on memorized data.\n\nSimilar to many white-box approaches, the KGA approach depends on accessing the modelâ€™s internal weights, which makes it inapplicable to black-box models. To address this issue, Î´ğ›¿\\deltaitalic_Î´ learning (Huang et al., 2024) introduces a novel framework for unlearning in black-box LLMs without accessing to the modelâ€™s internal weights. Specifically, Î´ğ›¿\\deltaitalic_Î´ learning utilizes a pair of smaller, trainable models to compute a logit offset that is then applied to the black-box LLM to adjust its responses. These smaller trainable models, referred to as offset models, are trained to predict how the logit outputs of the black-box LLM should be modified to exclude the influence of ğ’Ÿ~fsubscript~ğ’Ÿğ‘“\\tilde{\\mathcal{D}}_{f}over~ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT. The offset is calculated as the difference in logits between these two models and added to the black-box LLMâ€™s logits, guiding the final output away from sensitive information.\n\n4.1.4. Data Sharding\n\nData sharding approaches usually divide the training data into multiple shards, where each corresponds to a subset of the overall data Dğ·Ditalic_D. Inspired by SISA (Bourtoule et al., 2021), separate models are trained for each data shards that can effectively remove target data in different shard based on request. As it mentioned in Section 3, SISA provides an exact unlearning guarantee due to the data to be forgotten have no impacts on the retrained version of the model, making it suitable to wide range of model architectures, including GenAI. However, direct implementation of SISA on GenAI is infeasible due to its the high computational cost in model saving, retraining, and inference. Hence, various work have adapted the similar approach of SISA and made more suitable to GenAI depends on its corresponding unlearning scenario. FairSISA (Kadhe et al., 2023) proposes a post-processing unlearning strategy for bias mitigation in ensemble models produced by SISA. In particular, this post-processing function fğ‘“fitalic_f aims to enforce fairness in the outputs of gâˆ—superscriptğ‘”g^{*}italic_g start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT, particularly ensuring that the modelâ€™s predictions do not exhibit bias related to a sensitive attribute Ağ´Aitalic_A. This can be formulated as follows: fğ‘“fitalic_f post-processes the outputs of gâˆ—superscriptğ‘”g^{*}italic_g start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT to satisfy fairness constraints, such as equalized odds, across groups defined by Ağ´Aitalic_A without significantly affecting the overall model accuracy:\n\nminfâ¡ğ”¼â¢[â„“â¢(fâ¢(gâˆ—â¢(x)),Y)]subscriptğ‘“ğ”¼delimited-[]â„“ğ‘“superscriptğ‘”ğ‘¥ğ‘Œ\\min_{f}\\mathbb{E}[\\ell(f(g^{*}(x)),Y)]roman_min start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT blackboard_E [ roman_â„“ ( italic_f ( italic_g start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( italic_x ) ) , italic_Y ) ]\n\nsubject to:\n\nPrâ¡(fâ¢(gâˆ—â¢(x))=1|A=0,Y=y)=Prâ¡(fâ¢(gâˆ—â¢(x))=1|A=1,Y=y),âˆ€yâˆˆ{0,1}.formulae-sequencePrğ‘“superscriptğ‘”ğ‘¥conditional1ğ´0ğ‘Œğ‘¦Prğ‘“superscriptğ‘”ğ‘¥conditional1ğ´1ğ‘Œğ‘¦for-allğ‘¦01\\Pr(f(g^{*}(x))=1|A=0,Y=y)=\\Pr(f(g^{*}(x))=1|A=1,Y=y),\\quad\\forall y\\in\\{0,1\\}.roman_Pr ( italic_f ( italic_g start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( italic_x ) ) = 1 | italic_A = 0 , italic_Y = italic_y ) = roman_Pr ( italic_f ( italic_g start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( italic_x ) ) = 1 | italic_A = 1 , italic_Y = italic_y ) , âˆ€ italic_y âˆˆ { 0 , 1 } .\n\nHere, â„“â„“\\ellroman_â„“ is a loss function, and ğ”¼ğ”¼\\mathbb{E}blackboard_E denotes the expected value, reflecting the objective to minimize any deviation from the true labels Yğ‘ŒYitalic_Y while adhering to the fairness constraints.\n\nNext, to reduce retraining costs while preserving utility during inference, Liu and Kalinli (2024) introduces the LOO ensemble method to unlearn target token sequences from LMs. This method uses a teacher-student framework where multiple teacher models are trained on partitioned data segments. When data removal is requested, the student model (i.e. base LM) is supervised by the remaining teachers, excluding the one trained on the segment to be unlearned. The recalibration is formalized as:\n\ngLâ¢Oâ¢Oâˆ’Eâˆ’kâ¢(w|w1tâˆ’1)=1Mâˆ’1â¢âˆ‘m=1MğŸ™â¢{ğ’ŸkâŠ„Bm}â‹…pÎ¸mâ¢(w|w1tâˆ’1)superscriptsubscriptğ‘”ğ¿ğ‘‚ğ‘‚ğ¸ğ‘˜conditionalğ‘¤superscriptsubscriptğ‘¤1ğ‘¡11ğ‘€1superscriptsubscriptğ‘š1ğ‘€â‹…1not-subset-ofsubscriptğ’Ÿğ‘˜subscriptğµğ‘šsubscriptğ‘subscriptğœƒğ‘šconditionalğ‘¤superscriptsubscriptğ‘¤1ğ‘¡1g_{LOO-E}^{-k}(w|w_{1}^{t-1})=\\frac{1}{M-1}\\sum_{m=1}^{M}\\mathbbm{1}\\{\\mathcal% {D}_{k}\\not\\subset B_{m}\\}\\cdot p_{\\theta_{m}}(w|w_{1}^{t-1})italic_g start_POSTSUBSCRIPT italic_L italic_O italic_O - italic_E end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - italic_k end_POSTSUPERSCRIPT ( italic_w | italic_w start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT ) = divide start_ARG 1 end_ARG start_ARG italic_M - 1 end_ARG âˆ‘ start_POSTSUBSCRIPT italic_m = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT blackboard_1 { caligraphic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT âŠ„ italic_B start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT } â‹… italic_p start_POSTSUBSCRIPT italic_Î¸ start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_w | italic_w start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT )\n\nwhere gLâ¢Oâ¢Oâˆ’Eâˆ’ksuperscriptsubscriptğ‘”ğ¿ğ‘‚ğ‘‚ğ¸ğ‘˜g_{LOO-E}^{-k}italic_g start_POSTSUBSCRIPT italic_L italic_O italic_O - italic_E end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - italic_k end_POSTSUPERSCRIPT is the aggregated prediction excluding the teacher trained with ğ’ŸkâŠ‚ğ’Ÿfsubscriptğ’Ÿğ‘˜subscriptğ’Ÿğ‘“\\mathcal{D}_{k}\\subset\\mathcal{D}_{f}caligraphic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT âŠ‚ caligraphic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT. Î¸msubscriptğœƒğ‘š\\theta_{m}italic_Î¸ start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT denotes the parameters of the m-th teacher model, and w1tâˆ’1superscriptsubscriptğ‘¤1ğ‘¡1w_{1}^{t-1}italic_w start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT reflects the input sequence up to step tâˆ’1ğ‘¡1t-1italic_t - 1. The student model fine-tunes its parameters based on KL divergence, ensuring sensitive data is unlearned while maintaining performance on ğ’Ÿrsubscriptğ’Ÿğ‘Ÿ\\mathcal{D}_{r}caligraphic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT. Despite improved utility performance compared to SISA, LOO lacks a theoretical guarantee for teacher models and may impose significant computational overheads on GenAI like LLMs.\n\nThe data sharding approach can be applied to vision generative models and LLMs for measuring training data influence for unlearning. Dai and Gifford (2023) presents temporary unlearning using ensemble ablation. The core idea is to use an ensemble of diffusion models, each trained on a subset of data, to assess the influence of individual data points by excluding models exposed to the data point and observing changes in outputs. The work provides a theoretical guarantee that the resulting model has not seen the removed training sample. The ensemble is represented as fesubscriptğ‘“ğ‘’f_{e}italic_f start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT:\n\nfeâ¢(x,t)=ğ”¼Sâˆ¼ğ’Ÿâ¢[ğ”¼râˆ¼Râ¢[fâ¢(x,t,Aâ¢(S,r))]]subscriptğ‘“ğ‘’ğ‘¥ğ‘¡subscriptğ”¼similar-toğ‘†ğ’Ÿdelimited-[]subscriptğ”¼similar-toğ‘Ÿğ‘…delimited-[]ğ‘“ğ‘¥ğ‘¡ğ´ğ‘†ğ‘Ÿf_{e}(x,t)=\\mathbb{E}_{S\\sim\\mathscr{D}}[\\mathbb{E}_{r\\sim R}[f(x,t,A(S,r))]]italic_f start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT ( italic_x , italic_t ) = blackboard_E start_POSTSUBSCRIPT italic_S âˆ¼ script_D end_POSTSUBSCRIPT [ blackboard_E start_POSTSUBSCRIPT italic_r âˆ¼ italic_R end_POSTSUBSCRIPT [ italic_f ( italic_x , italic_t , italic_A ( italic_S , italic_r ) ) ] ]\n\nwhere xğ‘¥xitalic_x is the input, tğ‘¡titalic_t is the diffusion time, ğ’Ÿğ’Ÿ\\mathscr{D}script_D represents the uniform distribution over 2ğ’Ÿsuperscript2ğ’Ÿ2^{\\mathcal{D}}2 start_POSTSUPERSCRIPT caligraphic_D end_POSTSUPERSCRIPT, and Aâ¢(â‹…)ğ´â‹…A(\\cdot)italic_A ( â‹… ) denotes the training procedure with training samples and exogenous noise rğ‘Ÿritalic_r. To assess the influence of a specific point x~~ğ‘¥\\tilde{x}over~ start_ARG italic_x end_ARG, the ensemble without x~~ğ‘¥\\tilde{x}over~ start_ARG italic_x end_ARG, feâˆ’x~superscriptsubscriptğ‘“ğ‘’~ğ‘¥f_{e}^{-\\tilde{x}}italic_f start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - over~ start_ARG italic_x end_ARG end_POSTSUPERSCRIPT, is evaluated as:\n\nfeâˆ’x~â¢(x,t)=1Prâ¢(xâˆˆSâ€²âˆ¼ğ’³)â¢ğ”¼Sâˆ¼ğ’Ÿâ¢[ğ”¼râˆ¼Râ¢[fâ¢(x,t,Aâ¢(S,r))â¢ğŸ™x~âˆ‰S]].superscriptsubscriptğ‘“ğ‘’~ğ‘¥ğ‘¥ğ‘¡1Prğ‘¥superscriptğ‘†â€²similar-toğ’³subscriptğ”¼similar-toğ‘†ğ’Ÿdelimited-[]subscriptğ”¼similar-toğ‘Ÿğ‘…delimited-[]ğ‘“ğ‘¥ğ‘¡ğ´ğ‘†ğ‘Ÿsubscript1~ğ‘¥ğ‘†f_{e}^{-\\tilde{x}}(x,t)=\\frac{1}{\\text{Pr}(x\\in S^{\\prime}\\sim\\mathcal{X})}% \\mathbb{E}_{S\\sim\\mathscr{D}}[\\mathbb{E}_{r\\sim R}[f(x,t,A(S,r))\\mathbbm{1}_{% \\tilde{x}\\notin S}]].italic_f start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - over~ start_ARG italic_x end_ARG end_POSTSUPERSCRIPT ( italic_x , italic_t ) = divide start_ARG 1 end_ARG start_ARG Pr ( italic_x âˆˆ italic_S start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT âˆ¼ caligraphic_X ) end_ARG blackboard_E start_POSTSUBSCRIPT italic_S âˆ¼ script_D end_POSTSUBSCRIPT [ blackboard_E start_POSTSUBSCRIPT italic_r âˆ¼ italic_R end_POSTSUBSCRIPT [ italic_f ( italic_x , italic_t , italic_A ( italic_S , italic_r ) ) blackboard_1 start_POSTSUBSCRIPT over~ start_ARG italic_x end_ARG âˆ‰ italic_S end_POSTSUBSCRIPT ] ] .\n\nThis method aligns with machine unlearning by providing a more efficient way to disregard specific data influences without extensive retraining. Using an ensemble approach ensures effective data attribution and compliance with privacy and fairness.\n\n4.1.5. Extra Learnable Layers\n\nAnother parameter optimization approach is to introduce additional parameters or trainable layers in the model and train them to actively forget different sets of data from ğ’Ÿ~fsubscript~ğ’Ÿğ‘“\\tilde{\\mathcal{D}}_{f}over~ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT. This approach negates the necessity of modifying the modelâ€™s inherent parameters, thereby preventing interference to its original knowledge. EUL (Chen and Yang, 2023) integrates an additional unlearning layer, Ul(i)superscriptsubscriptğ‘ˆğ‘™ğ‘–U_{l}^{(i)}italic_U start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT, into the transformer following the feed-forward networks. Throughout the training, this unlearning layer is exclusively engaged to learn to forget the specified data, while the rest of the model parameters, denoted as Î¸osubscriptğœƒğ‘œ\\theta_{o}italic_Î¸ start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT, remain frozen. Upon receiving a deletion request, the model first trains a distinct unlearning layer Ul(i)superscriptsubscriptğ‘ˆğ‘™ğ‘–U_{l}^{(i)}italic_U start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT with parameters Î¸isubscriptğœƒğ‘–\\theta_{i}italic_Î¸ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT tailored to that request. Subsequently, these layers are merged using a fusion mechanism to form a unified unlearning layer with parameters Î¸msubscriptğœƒğ‘š\\theta_{m}italic_Î¸ start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT, achieved by minimizing a regression objective:\n\nminÎ¸mâ¢âˆ‘iâ€–Î¸mTâ¢Xf(i)âˆ’Î¸iTâ¢Xf(i)â€–2subscriptsubscriptğœƒğ‘šsubscriptğ‘–superscriptnormsuperscriptsubscriptğœƒğ‘šğ‘‡superscriptsubscriptğ‘‹ğ‘“ğ‘–superscriptsubscriptğœƒğ‘–ğ‘‡superscriptsubscriptğ‘‹ğ‘“ğ‘–2\\min_{\\theta_{m}}\\sum_{i}\\left\\|\\theta_{m}^{T}X_{f}^{(i)}-\\theta_{i}^{T}X_{f}^% {(i)}\\right\\|^{2}roman_min start_POSTSUBSCRIPT italic_Î¸ start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT end_POSTSUBSCRIPT âˆ‘ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT âˆ¥ italic_Î¸ start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_X start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT - italic_Î¸ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_X start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT âˆ¥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT\n\nwhere Î¸msubscriptğœƒğ‘š\\theta_{m}italic_Î¸ start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT is defined as:\n\nÎ¸m=(âˆ‘iXf(i)â¢Tâ¢Xf(i))âˆ’1â¢âˆ‘i(Xf(i)â¢Tâ¢Xf(i)â¢Î¸i).subscriptğœƒğ‘šsuperscriptsubscriptğ‘–superscriptsubscriptğ‘‹ğ‘“ğ‘–ğ‘‡superscriptsubscriptğ‘‹ğ‘“ğ‘–1subscriptğ‘–superscriptsubscriptğ‘‹ğ‘“ğ‘–ğ‘‡superscriptsubscriptğ‘‹ğ‘“ğ‘–subscriptğœƒğ‘–\\theta_{m}=\\left(\\sum_{i}X_{f}^{(i)T}X_{f}^{(i)}\\right)^{-1}\\sum_{i}\\left(X_{f% }^{(i)T}X_{f}^{(i)}\\theta_{i}\\right).italic_Î¸ start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT = ( âˆ‘ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_X start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) italic_T end_POSTSUPERSCRIPT italic_X start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT âˆ‘ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_X start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) italic_T end_POSTSUPERSCRIPT italic_X start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT italic_Î¸ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) .\n\nThis results in a unified unlearning transformer capable of handling multiple deletion requests sequentially, ensuring that all specified data is effectively forgotten while preserving the integrity of model behaviors on other tasks.\n\nSimilarly, Receler (Huang et al., 2023a) introduces lightweight eraser parameters Î¸Esubscriptğœƒğ¸\\theta_{E}italic_Î¸ start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT during unlearning, which is designed to remove the target concept from the outputs of each cross-attention layer within the diffusion U-Net. To erase the concept, the eraser is trained to predict the negatively guided noises that move the modelâ€™s prediction away from the erased concept. In particular, the objective is defined as:\n\nâ„’Erasesubscriptâ„’Erase\\displaystyle\\mathcal{L}_{\\text{Erase}}caligraphic_L start_POSTSUBSCRIPT Erase end_POSTSUBSCRIPT =ğ”¼yt,tâ¢[â€–ÏµÎ¸uâ¢(yt,ex,t)âˆ’ÏµEâ€–2]absentsubscriptğ”¼subscriptğ‘¦ğ‘¡ğ‘¡delimited-[]superscriptnormsubscriptitalic-Ïµsubscriptğœƒğ‘¢subscriptğ‘¦ğ‘¡subscriptğ‘’ğ‘¥ğ‘¡subscriptitalic-Ïµğ¸2\\displaystyle=\\mathbb{E}_{y_{t},t}\\left[\\|\\epsilon_{\\theta_{u}}(y_{t},e_{x},t)% -\\epsilon_{E}\\|^{2}\\right]= blackboard_E start_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t end_POSTSUBSCRIPT [ âˆ¥ italic_Ïµ start_POSTSUBSCRIPT italic_Î¸ start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT , italic_t ) - italic_Ïµ start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT âˆ¥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] where â¢ÏµEwhere subscriptitalic-Ïµğ¸\\displaystyle\\text{where }\\epsilon_{E}where italic_Ïµ start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT =ÏµÎ¸â¢(yt,t)âˆ’Î·â¢[ÏµÎ¸â¢(yt,ex,t)âˆ’ÏµÎ¸â¢(yt,t)],absentsubscriptitalic-Ïµğœƒsubscriptğ‘¦ğ‘¡ğ‘¡ğœ‚delimited-[]subscriptitalic-Ïµğœƒsubscriptğ‘¦ğ‘¡subscriptğ‘’ğ‘¥ğ‘¡subscriptitalic-Ïµğœƒsubscriptğ‘¦ğ‘¡ğ‘¡\\displaystyle=\\epsilon_{\\theta}(y_{t},t)-\\eta\\left[\\epsilon_{\\theta}(y_{t},e_{% x},t)-\\epsilon_{\\theta}(y_{t},t)\\right],= italic_Ïµ start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT ( italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) - italic_Î· [ italic_Ïµ start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT ( italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT , italic_t ) - italic_Ïµ start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT ( italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) ] ,\n\nwhere Î¸usubscriptğœƒğ‘¢\\theta_{u}italic_Î¸ start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT is the designated unlearned model with Î¸osubscriptğœƒğ‘œ\\theta_{o}italic_Î¸ start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT plugged with eraser Î¸esubscriptğœƒğ‘’\\theta_{e}italic_Î¸ start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT. ytsubscriptğ‘¦ğ‘¡y_{t}italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is the denoised image at timestep tğ‘¡titalic_t sampled from Î¸usubscriptğœƒğ‘¢\\theta_{u}italic_Î¸ start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT conditioned on target concept xğ‘¥xitalic_x, exsubscriptğ‘’ğ‘¥e_{x}italic_e start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT is the text embedding of concept xğ‘¥xitalic_x, and ÏµEsubscriptitalic-Ïµğ¸\\epsilon_{E}italic_Ïµ start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT is the negatively guided noise predicted by the original model Î¸osubscriptğœƒğ‘œ\\theta_{o}italic_Î¸ start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT. By minimizing the L2 distance between ÏµÎ¸uâ¢(yt,ex,t)subscriptitalic-Ïµsubscriptğœƒğ‘¢subscriptğ‘¦ğ‘¡subscriptğ‘’ğ‘¥ğ‘¡\\epsilon_{\\theta_{u}}(y_{t},e_{x},t)italic_Ïµ start_POSTSUBSCRIPT italic_Î¸ start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT , italic_t ) and ÏµEsubscriptitalic-Ïµğ¸\\epsilon_{E}italic_Ïµ start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT, the eraser learns to reduce the probability of the generated image yğ‘¦yitalic_y belongs to the target concept xğ‘¥xitalic_x, thus effectively erasing the concept.\n\nKumar et al. (2022) proposes two extensions to the SISA framework (Bourtoule et al., 2021)â€” SISA-FC and SISA-A â€” to facilitate guaranteed unlearning that is efficient in terms of memory, time, and space for LMs. In particular, SISA-FC pre-trains a base model on a generic text corpus and then integrates fully connected layers atop it. During optimization, only the parameters of the linear layers are fine-tuned. This approach reduces the overall training time since backpropagation of gradients occurs solely in the final layers, and only the weights of the additional parameters are stored. Nevertheless, the addition of linear layers to SISA might compromise the modelâ€™s utility when compared to fine-tuning the entire model (Devlin et al., 2018). To address this, SISA-A incorporates adapters (Houlsby et al., 2019) into the encoder blocks of the transformer. This method results in only a marginal increase in the modelâ€™s memory footprintâ€”about 1 to 5 % â€”thus providing a memory benefit of 95 to 99 %.\n\n4.1.6. Task Vector Methods\n\nInspired by recent work of weight interpolations (Frankle et al., 2020; Wortsman et al., 2022b; Matena and Raffel, 2022; Ilharco et al., 2022b; Ainsworth et al., 2022), Ilharco et al. (2022a) first proposes the concept of task vector, which can be obtained by taking the difference between the original model weights of a pre-trained model and its weights fine-tuned on a specific task. In particular, if we let Î¸fâ¢ttsubscriptsuperscriptğœƒğ‘¡ğ‘“ğ‘¡\\theta^{t}_{ft}italic_Î¸ start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_f italic_t end_POSTSUBSCRIPT be the corresponding weights after fine-tuning on task tğ‘¡titalic_t, the task vector is then denoted as Ï„t=Î¸fâ¢ttâˆ’Î¸osubscriptğœğ‘¡subscriptsuperscriptğœƒğ‘¡ğ‘“ğ‘¡subscriptğœƒğ‘œ\\tau_{t}=\\theta^{t}_{ft}-\\theta_{o}italic_Ï„ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_Î¸ start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_f italic_t end_POSTSUBSCRIPT - italic_Î¸ start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT. Then, taking the element-wise negation of the task vector Ï„tsubscriptğœğ‘¡\\tau_{t}italic_Ï„ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT can enable Î¸osubscriptğœƒğ‘œ\\theta_{o}italic_Î¸ start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT to forget target knowledge on task tğ‘¡titalic_t without jeopardizing irrelevant knowledge, resulting in an unlearned model that has weight of Î¸u=Î¸oâˆ’Î»â¢Ï„subscriptğœƒğ‘¢subscriptğœƒğ‘œğœ†ğœ\\theta_{u}=\\theta_{o}-\\lambda\\tauitalic_Î¸ start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT = italic_Î¸ start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT - italic_Î» italic_Ï„ with Î»ğœ†\\lambdaitalic_Î» as a scaling term. One exemplar work is SKU (Liu et al., 2024a), which designs a novel unlearning framework to eliminate harmful knowledge while preserving utility on normal prompts. In particular, SKU is consisted of two stages where the first stage aims to identify and acquire harmful knowledge within the model, whereas the second stage targets to remove the knowledge using element-wise negation operation. Different from pure gradient ascent approaches where the model locality is largely compromised, SKU collectively aggregates the target unlearned knowledge ğ’Ÿ~fsubscript~ğ’Ÿğ‘“\\tilde{\\mathcal{D}}_{f}over~ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT using gradient decent approach in the first stage and remove it from the pre-trained model. However, SSU (Dou et al., 2024) identifies the potential instability of the pure task vector approach in the case of multiple rounds of unlearning (i.e., sequential unlearning) and introduces a more stable unlearning framework integrated with weight saliency.\n\nBesides subtracting undesirable parameters, Pochinkov and Schoots (2024) proposes a selective pruning method to trim those neuronsâ€™ relative importance to different datasets, representing target model capability for unlearning. In particular, it performs either iterative pruning on nodes in the feed-forward layers or attention head layers. This selective approach utilizes importance functions that assess the contribution of individual neurons to specific tasks by measuring activation frequencies and magnitudes, enabling precise targeting of neurons that are crucial for the undesired capabilities. Furthermore, different from previous weight pruning method, where it requires Hessian computation, the selective neuron pruning is more computational efficient for large language models because it directly removes neurons that contribute the most to the unwanted behavior.\n\n4.1.7. Parameter Efficient Module Operation Methods\n\nInspired by works on merging model parameters under full fine-tuning (Wortsman et al., 2022a; Matena and Raffel, [n. d.]; Jin et al., 2022), Zhang et al. (2024b) explores composing parameter-efficient modules (PEM) like LoRA (Hu et al., 2021) and (IA)3superscript(IA)3\\text{(IA)}^{3}(IA) start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT (Liu et al., 2022c) for flexible module manipulation. Unlike task vector methods, which modify the global weight vectors, PEM operation methods apply localized adjustments within specific modules. Similar to task vector methods, the negation operator (âŠ–symmetric-difference\\ominusâŠ–) in PEM methods unlearns stored knowledge within adapter modules. For example, in a LoRA module denoted as Î¸lora={ğ€,ğ}subscriptğœƒlorağ€ğ\\theta_{\\text{lora}}=\\{\\mathbf{A},\\mathbf{B}\\}italic_Î¸ start_POSTSUBSCRIPT lora end_POSTSUBSCRIPT = { bold_A , bold_B }, ğ€ğ€\\mathbf{A}bold_A is initialized following a random Gaussian distribution, and ğğ\\mathbf{B}bold_B is initialized to all zeros to recover the pre-trained model at the beginning. We could negate ğğ\\mathbf{B}bold_B or ğ€ğ€\\mathbf{A}bold_A while keeping the other unchanged to facilitate unlearning or forgetting certain skills (e.g., toxic data). This process can be written as:\n\nâŠ–Î¸loranegation=âŠ–Î¸lora={ğ€,âˆ’ğ}.symmetric-differencesuperscriptsubscriptğœƒloranegationsymmetric-differencesubscriptğœƒlorağ€ğ\\ominus\\theta_{\\text{lora}}^{\\text{negation}}=\\ominus\\theta_{\\text{lora}}=\\{% \\mathbf{A},-\\mathbf{B}\\}.âŠ– italic_Î¸ start_POSTSUBSCRIPT lora end_POSTSUBSCRIPT start_POSTSUPERSCRIPT negation end_POSTSUPERSCRIPT = âŠ– italic_Î¸ start_POSTSUBSCRIPT lora end_POSTSUBSCRIPT = { bold_A , - bold_B } .\n\nThis negation operator changes the intermediate layerâ€™s activation values in the opposite direction of gradient descent, aiding in unlearning targeted knowledge.\n\nTo further enhance model truthfulness and detoxification, Ext-Sub (Hu et al., 2024) introduces â€expertâ€ and â€anti-expertâ€ PEMs. The â€expertâ€ PEM, trained on retaining data ğ’Ÿrsubscriptğ’Ÿğ‘Ÿ\\mathcal{D}_{r}caligraphic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT, represents desired behaviors, while the â€anti-expertâ€ PEM, trained on unlearned data ğ’Ÿ~fsubscript~ğ’Ÿğ‘“\\tilde{\\mathcal{D}}_{f}over~ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT, represents harmful behaviors. Ext-Sub identifies commonalities between these PEMs to determine shared capabilities and then subtracts the deficiency capability responsible for untruthful or toxic content. The Ext-Sub operation is defined as:\n\nÎ¸u=Î¸eâ¢xâ¢pâ¢eâ¢râ¢tâŠ–Î»â‹…Extâ¢(Î¸aâ¢nâ¢tâ¢iâˆ’eâ¢xâ¢pâ¢eâ¢râ¢t),subscriptğœƒğ‘¢symmetric-differencesubscriptğœƒğ‘’ğ‘¥ğ‘ğ‘’ğ‘Ÿğ‘¡â‹…ğœ†Extsubscriptğœƒğ‘ğ‘›ğ‘¡ğ‘–ğ‘’ğ‘¥ğ‘ğ‘’ğ‘Ÿğ‘¡\\theta_{u}=\\theta_{expert}\\ominus\\lambda\\cdot\\text{Ext}(\\theta_{anti-expert}),italic_Î¸ start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT = italic_Î¸ start_POSTSUBSCRIPT italic_e italic_x italic_p italic_e italic_r italic_t end_POSTSUBSCRIPT âŠ– italic_Î» â‹… Ext ( italic_Î¸ start_POSTSUBSCRIPT italic_a italic_n italic_t italic_i - italic_e italic_x italic_p italic_e italic_r italic_t end_POSTSUBSCRIPT ) ,\n\nwhere Î¸eâ¢xâ¢pâ¢eâ¢râ¢tsubscriptğœƒğ‘’ğ‘¥ğ‘ğ‘’ğ‘Ÿğ‘¡\\theta_{expert}italic_Î¸ start_POSTSUBSCRIPT italic_e italic_x italic_p italic_e italic_r italic_t end_POSTSUBSCRIPT and Î¸aâ¢nâ¢tâ¢iâˆ’eâ¢xâ¢pâ¢eâ¢râ¢tsubscriptğœƒğ‘ğ‘›ğ‘¡ğ‘–ğ‘’ğ‘¥ğ‘ğ‘’ğ‘Ÿğ‘¡\\theta_{anti-expert}italic_Î¸ start_POSTSUBSCRIPT italic_a italic_n italic_t italic_i - italic_e italic_x italic_p italic_e italic_r italic_t end_POSTSUBSCRIPT are the parameters of the expert and anti-expert PEMs, respectively, and Ext(â‹…â‹…\\cdotâ‹…) is the extraction function isolating the deficiency capability. This process removes harmful effects while preserving and enhancing the beneficial capabilities of the LLM.\n\nAdditionally, PEM can be utilized during model training to prevent the acquisition of harmful information. For instance, Zhou et al. (2023) introduces security vectors, enabling LLMs to be exposed to harmful behaviors without modifying the modelâ€™s original, safety-aligned parameters. Specifically, these vectors allow the LLM to process and respond to harmful inputs during training, ensuring that the core parameters remain unaltered. The security vector Î¸ssubscriptğœƒğ‘ \\theta_{s}italic_Î¸ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT is optimized to minimize the loss associated with harmful data:\n\nargâ¡minÎ¸oâ¡ğ”¼(X,Y)âˆ¼Duâ¢[minÎ¸sâ¡Lâ¢(fâ¢(X;Î¸o;Î¸s),Y)],subscriptsubscriptğœƒğ‘œsubscriptğ”¼similar-toğ‘‹ğ‘Œsubscriptğ·ğ‘¢delimited-[]subscriptsubscriptğœƒğ‘ ğ¿ğ‘“ğ‘‹subscriptğœƒğ‘œsubscriptğœƒğ‘ ğ‘Œ\\arg\\min_{\\theta_{o}}\\mathbb{E}_{(X,Y)\\sim D_{u}}\\left[\\min_{\\theta_{s}}L(f(X;% \\theta_{o};\\theta_{s}),Y)\\right],roman_arg roman_min start_POSTSUBSCRIPT italic_Î¸ start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT ( italic_X , italic_Y ) âˆ¼ italic_D start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ roman_min start_POSTSUBSCRIPT italic_Î¸ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_L ( italic_f ( italic_X ; italic_Î¸ start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT ; italic_Î¸ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) , italic_Y ) ] ,\n\nwhere Lğ¿Litalic_L is the causal loss computed based on the prediction and the ground truth. Subsequently, the trained security vector Î¸sâˆ—subscriptsuperscriptğœƒâˆ—ğ‘ \\theta^{\\ast}_{s}italic_Î¸ start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT is employed during fine-tuning to ensure that the model learns from benign data without adopting harmful behaviors. Î¸sâˆ—subscriptsuperscriptğœƒâˆ—ğ‘ \\theta^{\\ast}_{s}italic_Î¸ start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT is activated during the forward pass of training to maintain the modelâ€™s response consistency with benign data and to inhibit the learning of harmful information. Specifically:\n\nÎ¸u=argâ¡minÎ¸oâ¡ğ”¼(X,Y)âˆ¼Drâ¢[Lâ¢(fâ¢(X;Î¸o;Î¸sâˆ—),Y)],subscriptğœƒğ‘¢subscriptsubscriptğœƒğ‘œsubscriptğ”¼similar-toğ‘‹ğ‘Œsubscriptğ·ğ‘Ÿdelimited-[]ğ¿ğ‘“ğ‘‹subscriptğœƒğ‘œsuperscriptsubscriptğœƒğ‘ ğ‘Œ\\theta_{u}=\\arg\\min_{\\theta_{o}}\\mathbb{E}_{(X,Y)\\sim D_{r}}\\left[L\\left(f(X;% \\theta_{o};\\theta_{s}^{*}),Y\\right)\\right],italic_Î¸ start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT = roman_arg roman_min start_POSTSUBSCRIPT italic_Î¸ start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT ( italic_X , italic_Y ) âˆ¼ italic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ italic_L ( italic_f ( italic_X ; italic_Î¸ start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT ; italic_Î¸ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ) , italic_Y ) ] ,\n\nensuring that the original model parameters do not update in a harmful direction.\n\n4.1.8. Summary:\n\nThe parameter optimization strategies focus on adjusting specific model parameters to selectively unlearn certain behaviors without affecting other functions. These approaches involve precise alterations to parameters associated with unwanted data influences or biases, ensuring the preservation of essential model performance. Gradient-based approaches with reversed loss are effective for unlearning accuracy and generalizability but can negatively impact model locality by inadvertently affecting unrelated parameters. In contrast, gradient-based methods without reversed loss can maximally preserve locality but may not excel in unlearning accuracy and generalizability. Extra learnable layers provide highly targeted unlearning but may demand significant computational resources. Data sharding methods excel in maintaining locality by partitioning the training data and ensuring specific data points can be unlearned without extensive retraining, although they might struggle with generalizability in very large models. Knowledge distillation is effective in maintaining locality by transferring knowledge to a new model trained to exclude specific data, thus retaining essential performance while unlearning undesired knowledge. However, it can be resource-intensive and may not achieve satisfactory accuracy and generalizability. Task vector and parameter-efficient module operations may perform well in terms of unlearning accuracy and generalizability. Nonetheless, recent work (Dou et al., 2024) has highlighted the risk of these approaches leading to instability due to significant model degradation, resulting in poor locality performance.\n\n4.2. In-Context Unlearning\n\n4.2.1. Overview\n\nUnlike parameter optimization approaches, which actively modifies parameters either locally or globally via different techniques, in-Context unlearning techniques retain the parameters in their original state and manipulate the modelâ€™s context or environment to facilitate unlearning. In particular, these strategies result in an unlearned model gÎ¸uâˆ—subscriptsuperscriptğ‘”âˆ—subscriptğœƒğ‘¢g^{\\ast}_{\\theta_{u}}italic_g start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_Î¸ start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT end_POSTSUBSCRIPT where Î¸u=Î¸osubscriptğœƒğ‘¢subscriptğœƒğ‘œ\\theta_{u}=\\theta_{o}italic_Î¸ start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT = italic_Î¸ start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT, but with changes in how the model interacts with its input or inferences.\n\n4.2.2. In-Context Unlearning\n\nIn-context unlearning utilizes the approach of in-context learning to selectively erase targeted knowledge during inference, treating the model as a black box. This kind of method is resource-efficient but has inherent limitations due to the nature of in-context learning. Specifically, it modifies only the modelâ€™s immediate outputs without fundamentally eradicating the unwanted knowledge embedded within the modelâ€™s internal parameters. ICUL (Pawelczyk et al., 2023) first introduces the idea of in context unlearning, which alters input prompts during the inference phase to achieve targeted unlearning in the situation where modelâ€™s API is the only access to the model. The technical process involves several key steps:\n\n(1)\n\nLabel Flipping, where the label of the data point that needs to be forgotten is flipped to contradict the modelâ€™s learned associations, resulting in the template â€[Fâ¢oâ¢râ¢gâ¢eâ¢tâ¢Iâ¢nâ¢pâ¢uâ¢t]0subscriptdelimited-[]ğ¹ğ‘œğ‘Ÿğ‘”ğ‘’ğ‘¡ğ¼ğ‘›ğ‘ğ‘¢ğ‘¡0[Forget\\>Input]_{0}[ italic_F italic_o italic_r italic_g italic_e italic_t italic_I italic_n italic_p italic_u italic_t ] start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT [Fâ¢lâ¢iâ¢pâ¢pâ¢eâ¢dâ¢Lâ¢aâ¢bâ¢eâ¢l]0subscriptdelimited-[]ğ¹ğ‘™ğ‘–ğ‘ğ‘ğ‘’ğ‘‘ğ¿ğ‘ğ‘ğ‘’ğ‘™0[Flipped\\>Label]_{0}[ italic_F italic_l italic_i italic_p italic_p italic_e italic_d italic_L italic_a italic_b italic_e italic_l ] start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPTâ€\n\n(2)\n\nContext Construction, where additional correctly labeled examples are sampled and appended to the flipped example, creating a mixed input sequence, resulting in the template â€[Fâ¢oâ¢râ¢gâ¢eâ¢tâ¢Iâ¢nâ¢pâ¢uâ¢t]0â¢[Fâ¢lâ¢iâ¢pâ¢pâ¢eâ¢dâ¢Lâ¢aâ¢bâ¢eâ¢l]0\\n;[Iâ¢nâ¢pâ¢uâ¢t]1â¢[Lâ¢aâ¢bâ¢eâ¢l]1\\n;â€¦â¢[Iâ¢nâ¢pâ¢uâ¢t]sâ¢[Lâ¢aâ¢bâ¢eâ¢l]s\\subscriptdelimited-[]ğ¹ğ‘œğ‘Ÿğ‘”ğ‘’ğ‘¡ğ¼ğ‘›ğ‘ğ‘¢ğ‘¡0subscriptdelimited-[]ğ¹ğ‘™ğ‘–ğ‘ğ‘ğ‘’ğ‘‘ğ¿ğ‘ğ‘ğ‘’ğ‘™0ğ‘›\\subscriptdelimited-[]ğ¼ğ‘›ğ‘ğ‘¢ğ‘¡1subscriptdelimited-[]ğ¿ğ‘ğ‘ğ‘’ğ‘™1ğ‘›â€¦subscriptdelimited-[]ğ¼ğ‘›ğ‘ğ‘¢ğ‘¡ğ‘ subscriptdelimited-[]ğ¿ğ‘ğ‘ğ‘’ğ‘™ğ‘ [Forget\\>Input]_{0}\\ [Flipped\\>Label]_{0}\\ \\backslash n;[Input]_{1}\\;[Label]_{% 1}\\ \\backslash n;\\ldots\\;[Input]_{s}\\;[Label]_{s}[ italic_F italic_o italic_r italic_g italic_e italic_t italic_I italic_n italic_p italic_u italic_t ] start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT [ italic_F italic_l italic_i italic_p italic_p italic_e italic_d italic_L italic_a italic_b italic_e italic_l ] start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT \\ italic_n ; [ italic_I italic_n italic_p italic_u italic_t ] start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT [ italic_L italic_a italic_b italic_e italic_l ] start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \\ italic_n ; â€¦ [ italic_I italic_n italic_p italic_u italic_t ] start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT [ italic_L italic_a italic_b italic_e italic_l ] start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT\n\n(3)\n\nInference Adjustment, where this modified prompt is used to â€™confuseâ€™ the model about the original training point, mitigating its influence, resulting in the template â€[Fâ¢oâ¢râ¢gâ¢eâ¢tâ¢Iâ¢nâ¢pâ¢uâ¢t]0subscriptdelimited-[]ğ¹ğ‘œğ‘Ÿğ‘”ğ‘’ğ‘¡ğ¼ğ‘›ğ‘ğ‘¢ğ‘¡0[Forget\\>Input]_{0}[ italic_F italic_o italic_r italic_g italic_e italic_t italic_I italic_n italic_p italic_u italic_t ] start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT [Fâ¢lâ¢iâ¢pâ¢pâ¢eâ¢dâ¢Lâ¢aâ¢bâ¢eâ¢l]0subscriptdelimited-[]ğ¹ğ‘™ğ‘–ğ‘ğ‘ğ‘’ğ‘‘ğ¿ğ‘ğ‘ğ‘’ğ‘™0[Flipped\\>Label]_{0}[ italic_F italic_l italic_i italic_p italic_p italic_e italic_d italic_L italic_a italic_b italic_e italic_l ] start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT \\n;[Input]1;[Label]1\\n;â€¦;[Input]s;[Label]s[QueryInput]s+1\\backslash n;[Input]_{1}\\>;[Label]_{1}\\>\\backslash n;\\ldots\\>;[Input]_{s}\\>;[% Label]_{s}\\>[Query\\>Input]_{s+1}\\ italic_n ; [ italic_I italic_n italic_p italic_u italic_t ] start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ; [ italic_L italic_a italic_b italic_e italic_l ] start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \\ italic_n ; â€¦ ; [ italic_I italic_n italic_p italic_u italic_t ] start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ; [ italic_L italic_a italic_b italic_e italic_l ] start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT [ italic_Q italic_u italic_e italic_r italic_y italic_I italic_n italic_p italic_u italic_t ] start_POSTSUBSCRIPT italic_s + 1 end_POSTSUBSCRIPTâ€.\n\nHowever, directly modifying input prompts does not always give a desirable output. Hence, Larimar (Das et al., 2024) integrates an external memory module that directly manipulates the LLMâ€™s outputs. This approach allows for precise control over knowledge updates and selective forgetting through operations such as writing, reading, and generating, which are efficiently managed by a hierarchical memory system.\n\n4.2.3. Summary\n\nParameter frozen methods retain the modelâ€™s parameters in their original state while manipulating the modelâ€™s context or environment to facilitate unlearning. A notable advantage of this method is its resource efficiency, as it does not require retraining or modification of the modelâ€™s internal parameters, making it suitable for scenarios where direct access to the modelâ€™s internals is limited (e.g. black-box models). However, a significant drawback is that parameter frozen methods only modify the modelâ€™s immediate outputs without fundamentally eradicating the unwanted knowledge embedded within the modelâ€™s parameters. This can lead to incomplete unlearning, as the underlying knowledge remains intact.\n\n5. Datasets and Benchmarks\n\n5.1. Datasets\n\nIn this section, we summarize the datasets commonly used in the field of Generative AI, as outlined in Table 4, to benefit future MU research. Instead of merely categorizing the datasets by task (i.e., generation and classification), they are organized according to their intended unlearning objectives. We specifically focus on those datasets primarily utilized as target datasets during the unlearning process, excluding object removal datasets such as CIFAR10 and MNIST, as well as generic evaluation benchmark datasets like Hellaswag (Zellers et al., 2019) and Piqa (Bisk et al., 2020).\n\n5.1.1. Safety Alignment:\n\nThe Civil Comments dataset (Borkan et al., 2019) is comprised of public comments from various news websites, each labeled with a level of toxicity to represent the degree of harmfulness in the content. Studies such as (Ilharco et al., 2022a) and (Zhang et al., 2024b) have utilized subsets of these highly toxic samples to extract harmful knowledge from pre-trained models. Complementing this, the Anthropic red team dataset (Bai et al., 2022; Ganguli et al., 2022) includes human preference data and annotated"
    }
}
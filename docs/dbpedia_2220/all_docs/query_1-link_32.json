{
    "id": "dbpedia_2220_1",
    "rank": 32,
    "data": {
        "url": "https://arxiv.org/html/2407.20516v1",
        "read_more_link": "",
        "language": "en",
        "title": "Machine Unlearning in Generative AI: A Survey",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://arxiv.org/html/x1.png",
            "https://arxiv.org/html/x2.png",
            "https://arxiv.org/html/x3.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            "Machine Unlearning",
            "Generative Models",
            "Trustworthy ML",
            "Data Privacy"
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Zheyuan Liu , Guangyao Dou , Zhaoxuan Tan , Yijun Tian and Meng Jiang\n\nAbstract.\n\nGenerative AI technologies have been deployed in many places, such as (multimodal) large language models and vision generative models. Their remarkable performance should be attributed to massive training data and emergent reasoning abilities. However, the models would memorize and generate sensitive, biased, or dangerous information originated from the training data especially those from web crawl. New machine unlearning (MU) techniques are being developed to reduce or eliminate undesirable knowledge and its effects from the models, because those that were designed for traditional classification tasks could not be applied for Generative AI. We offer a comprehensive survey on many things about MU in Generative AI, such as a new problem formulation, evaluation methods, and a structured discussion on the advantages and limitations of different kinds of MU techniques. It also presents several critical challenges and promising directions in MU research. A curated list of readings can be found: https://github.com/franciscoliu/GenAI-MU-Reading.\n\nMachine Unlearning, Generative Models, Trustworthy ML, Data Privacy\n\n††copyright: acmlicensed††journalyear: 2018††doi: XXXXXXX.XXXXXXX††journal: JACM††journalvolume: 37††journalnumber: 4††article: 111††publicationmonth: 8††ccs: Security and privacy Human and societal aspects of security and privacy\n\n1. Introduction\n\nMachine learning (ML) grew out of the quest of artificial intelligence (AI) for building a model that learns patterns from a dataset D={(x,y)}𝐷𝑥𝑦D=\\{(x,y)\\}italic_D = { ( italic_x , italic_y ) }. The model uses the patterns to predict the output y𝑦yitalic_y for an unseen input x𝑥xitalic_x, known as a classifier or regressor, depending on whether y𝑦yitalic_y is a categorical or numeric variable. An interesting question is, after the model is built, what if the developers find that the dataset contains a set of data points Df={(xf,yf)}⊂Dsubscript𝐷𝑓subscript𝑥𝑓subscript𝑦𝑓𝐷D_{f}=\\{(x_{f},y_{f})\\}\\subset Ditalic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT = { ( italic_x start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT ) } ⊂ italic_D that should NOT be learned? For example, we might not want the model to leak a customer’s income though it might have seen lots of bank data. Machine unlearning (MU) methods aim to help the model “forget” the data points in Dfsubscript𝐷𝑓D_{f}italic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT (which is named forget set) so that ideally the model would be the same or similar as a new model trained on D∖Df𝐷subscript𝐷𝑓D\\setminus D_{f}italic_D ∖ italic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT, and MU would save a lot of development cost than re-training the model.\n\nMU has been studied and reviewed in conventional AI. Recently, generative AI (GenAI), based on generative ML models, has exhibited its capabilities of data generation which may be used as and not limited to classification and regression. For example, large language models (LLMs) (Zheng et al., 2024; Touvron et al., 2023; Achiam et al., 2023; Tan et al., 2024) can generate tokens right after an article, about its sentiment, topics, or number of words, if prompted properly. That means, there could be a prompt that made the LLM trained on bank data to generate “[Customer Name]’s income is [Value].” In this case, people want the model to “forget” so deeply that it would not generate this output with any prompts: the prompt could be strongly related, like “what is the income of [Customer Name]?”, or loosely related, like “tell me the income of someone you know.” Therefore, the forget set must be re-defined in GenAI, and many concepts such as objectives, evaluation methods, and MU techniques must be reviewed carefully. Meanwhile, GenAI includes not only LLMs but also many other types of models such as vision generative models (Li et al., 2019; Zhou et al., 2022; Rombach et al., 2022) and multimodal large language model (MLLMs) (Liu et al., 2023b, 2024b). These motivate us to write this survey, while MU surveys for the conventional AI exist (Liu et al., 2024c; Blanco-Justicia et al., 2024; Lynch et al., 2024; Si et al., 2023; Ren et al., 2024; Xu, 2024).\n\nThe forget set in GenAI can be defined as Df={(x,yf)|∀x}subscript𝐷𝑓conditional-set𝑥subscript𝑦𝑓for-all𝑥D_{f}=\\{(x,y_{f})|\\forall x\\}italic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT = { ( italic_x , italic_y start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT ) | ∀ italic_x } or simplified as Df={yf}subscript𝐷𝑓subscript𝑦𝑓D_{f}=\\{y_{f}\\}italic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT = { italic_y start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT }, where yfsubscript𝑦𝑓y_{f}italic_y start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT can be any undesired model output, including leaked privacy, harmful information, bias and discrimination, copyright data, etc., and x𝑥xitalic_x is anything that prompts the model to generate yfsubscript𝑦𝑓y_{f}italic_y start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT. Due to the new definition, it is much less expensive to collect a large forget set which contains many data points (i.e., yfsubscript𝑦𝑓y_{f}italic_y start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT) that the training set D𝐷Ditalic_D does not contain. So, while MU in conventional AI mainly focused on tuning a model to be the same as trained only on D∖Df𝐷subscript𝐷𝑓D\\setminus D_{f}italic_D ∖ italic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT, people have at least three expectations on the machine-unlearned GenAI, considering three sets of data points:\n\n•\n\nThe target forget set Df~={(x,yf)∈D}⊂Df~subscript𝐷𝑓𝑥subscript𝑦𝑓𝐷subscript𝐷𝑓\\tilde{D_{f}}=\\{(x,y_{f})\\in D\\}\\subset D_{f}over~ start_ARG italic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT end_ARG = { ( italic_x , italic_y start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT ) ∈ italic_D } ⊂ italic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT,\n\n•\n\nThe retain set Dr=D∖Dfsubscript𝐷𝑟𝐷subscript𝐷𝑓D_{r}=D\\setminus D_{f}italic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT = italic_D ∖ italic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT,\n\n•\n\nThe unseen forget set Df^=Df∖Df~^subscript𝐷𝑓subscript𝐷𝑓~subscript𝐷𝑓\\hat{D_{f}}=D_{f}\\setminus\\tilde{D_{f}}over^ start_ARG italic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT end_ARG = italic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT ∖ over~ start_ARG italic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT end_ARG.\n\nSuppose the original GenAI model is denoted by g:𝒳→𝒴:𝑔→𝒳𝒴g:\\mathcal{X}\\rightarrow\\mathcal{Y}italic_g : caligraphic_X → caligraphic_Y, and the unlearned model is g∗superscript𝑔g^{*}italic_g start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT. The three quantitative objectives of optimizing (and evaluating) g∗superscript𝑔g^{*}italic_g start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT are as follows (higher is better):\n\n•\n\nAccuracy: the unlearned model should not generate the data points in the seen forget set:\n\n(1) Accuracy=(∑yf∈D~fI⁢(g∗⁢(x)≠yf,∀x∈𝒳))/|Df~|∈[0,1],Accuracysubscriptsubscript𝑦𝑓subscript~𝐷𝑓𝐼formulae-sequencesuperscript𝑔𝑥subscript𝑦𝑓for-all𝑥𝒳~subscript𝐷𝑓01\\text{Accuracy}=\\left(\\sum_{y_{f}\\in\\tilde{D}_{f}}I(g^{*}(x)\\neq y_{f},\\forall x% \\in\\mathcal{X})\\right)/|\\tilde{D_{f}}|\\in[0,1],Accuracy = ( ∑ start_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT ∈ over~ start_ARG italic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_I ( italic_g start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_x ) ≠ italic_y start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT , ∀ italic_x ∈ caligraphic_X ) ) / | over~ start_ARG italic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT end_ARG | ∈ [ 0 , 1 ] ,\n\nwhere I⁢(s⁢t⁢m⁢t)𝐼𝑠𝑡𝑚𝑡I(stmt)italic_I ( italic_s italic_t italic_m italic_t ) is an indicator function – it returns 1 if s⁢t⁢m⁢t𝑠𝑡𝑚𝑡stmtitalic_s italic_t italic_m italic_t is true and otherwise returns 0. Note that we do not expect the original model to always fail on Df~~subscript𝐷𝑓\\tilde{D_{f}}over~ start_ARG italic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT end_ARG, i.e., g⁢(x)=yf𝑔𝑥subscript𝑦𝑓g(x)=y_{f}italic_g ( italic_x ) = italic_y start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT. So, it is possible that both g𝑔gitalic_g and g∗superscript𝑔g^{*}italic_g start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT do not generate yf∈Df~subscript𝑦𝑓~subscript𝐷𝑓y_{f}\\in\\tilde{D_{f}}italic_y start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT ∈ over~ start_ARG italic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT end_ARG with any prompt x𝑥xitalic_x.\n\n•\n\nLocality: the unlearned model should maintain its performance on the retain set:\n\n(2) Locality=(∑(x,y)∈DrI⁢(g∗⁢(x)=g⁢(x)))/|Dr|∈[0,1].Localitysubscript𝑥𝑦subscript𝐷𝑟𝐼superscript𝑔𝑥𝑔𝑥subscript𝐷𝑟01\\text{Locality}=\\left(\\sum_{(x,y)\\in D_{r}}I(g^{*}(x)=g(x))\\right)/|D_{r}|\\in[% 0,1].Locality = ( ∑ start_POSTSUBSCRIPT ( italic_x , italic_y ) ∈ italic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_I ( italic_g start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_x ) = italic_g ( italic_x ) ) ) / | italic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT | ∈ [ 0 , 1 ] .\n\nNote that Locality does not require the unlearned model g∗superscript𝑔g^{*}italic_g start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT to generate the expected output but the output from the original model g𝑔gitalic_g.\n\n•\n\nGeneralizability: the model should generalize the unlearning to the unseen forget set:\n\n(3) Generalizability=(∑yf∈Df^I⁢(g∗⁢(x)≠yf,∀x∈𝒳))/|Df^|∈[0,1].Generalizabilitysubscriptsubscript𝑦𝑓^subscript𝐷𝑓𝐼formulae-sequencesuperscript𝑔𝑥subscript𝑦𝑓for-all𝑥𝒳^subscript𝐷𝑓01\\text{Generalizability}=\\left(\\sum_{y_{f}\\in\\hat{D_{f}}}I(g^{*}(x)\\neq y_{f},% \\forall x\\in\\mathcal{X})\\right)/|\\hat{D_{f}}|\\in[0,1].Generalizability = ( ∑ start_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT ∈ over^ start_ARG italic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT end_ARG end_POSTSUBSCRIPT italic_I ( italic_g start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_x ) ≠ italic_y start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT , ∀ italic_x ∈ caligraphic_X ) ) / | over^ start_ARG italic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT end_ARG | ∈ [ 0 , 1 ] .\n\nSignificance\n\nAround these three objectives, many MU techniques have been designed for GenAI and found useful in AI applications. GenAI becomes increasingly data-dependent, concerned parties and practitioners may request the removal of certain data samples and their effects from training datasets and already trained models due to privacy concerns and regulatory requirements, such as the European Union’s General Data Protection Regulation (GDPR) (Protection, 2018), the California Consumer Privacy Act (CCPA) (Illman and Temple, 2019), and the Act on the Protection of Personal Information (APPI). Retraining models to eradicate specific samples and their impacts is often prohibitively expensive. Consequently, MU (Nguyen et al., 2022; Xu et al., 2023; Xu, 2024) has gained significant attention and has made notable progress as a solution to this challenge.\n\nApplications\n\nBesides protecting individual data privacy, machine unlearning can be implemented for other applications. For instance, previous works have shown that MU can be used to accelerate the process of leave-one-out cross-validation, recognizing meaningful and valuable data within a model (Ginart et al., 2019; Warnecke et al., 2021). Unlearning can also be used as a countermeasure against catastrophic forgetting in deep neural networks (Du et al., 2019; Liu et al., 2022b), a phenomenon where the model performance suddenly degrades after learning too many tasks. Additionally, machine unlearning can be an effective attack strategy to assess model robustness, similar to a backdoor attack. For example, an attacker could introduce malicious samples into the training dataset and then request their removal, impacting the model’s performance, fairness, or unlearning efficiency (Liu et al., 2022a; Marchant et al., 2022). As models and tasks have shifted from standard ML models to GenAI models, the applications of GenAI machine unlearning (MU) have also become more diverse. For instance, GenAI MU can be used to better align GenAI models with human instructions and ensure generated content aligns with human values (Ouyang et al., 2022). It can serve as a safety alignment tool to remove harmful behaviors such as toxic, biased, or illegal content (Shevlane et al., 2023; Li et al., 2024c; Liu et al., 2024a). Additionally, MU can be utilized to alleviate hallucinations (Yao et al., 2023), a phenomenon where GenAI generates false or inaccurate information that may seem reasonable. A thorough analysis of GenAI MU applications can be found in Section 6.\n\n1.1. Related Surveys\n\nRecently, several studies have provided valuable insights into various aspects of machine unlearning techniques in LLMs. These include general surveys on machine unlearning, such as (Liu et al., 2024c; Si et al., 2023; Qu et al., 2024; Xu, 2024; Blanco-Justicia et al., 2024), which cover a wide range of topics from basic concepts and diverse techniques to potential challenges in the field. Additionally, some works focus on specific aspects of LLM MU; for example, Zhang et al. (2023a) explores the implementation challenges of Right To Be Forgotten (RTBF) in LLM MU and provides insights on designing suitable methods, while Lynch et al. (2024) highlights the importance of comprehensive unlearning evaluations in LLM MU and offers insights on designing robust metrics. Furthermore, Liu et al. (2024d) emphasizes security and privacy concerns in LLM MU, focusing on threats, attacks, and defensive methods. Lastly, Ren et al. (2024) highlights the distinctions between various types of techniques, including both MU and non-MU methods, in generative models and their implementations for copyright protections.\n\nTo the best of our knowledge, there remains a significant gap in comprehensive investigations that incorporate the existing literature and ongoing advancements in the field of GenAI. While previous surveys primarily focus on LLMs or specific aspects of GenAI MU, they do not expand the categorization to include a broader range of generative models, such as generative image models and multimodal (large) language models. Moreover, these surveys lack a thorough examination of the technical details of GenAI MU, including categorization, datasets, and benchmarks. Additionally, the specific objectives of GenAI unlearning, crucial for guiding effective practices, have not been thoroughly investigated. Uniquely, our survey addresses this by formulating these objectives, providing a clear and structured framework that delineates the goals and expectations for effective unlearning practices. By defining the objectives as Accuracy, Locality, and Generalizability, we establish a robust and systematic approach to evaluate and advance GenAI unlearning methods.\n\nGiven the rapid advancement of GenAI MU techniques, it is crucial to conduct a detailed examination of all representative methodologies and align them with the objectives of GenAI MU. This includes summarizing commonalities across different types, highlighting unique aspects of each category and modality, and discussing open challenges and future directions in the domain of GenAI MU. Our survey aims to fill this gap by providing a holistic overview of the field, encompassing a wide spectrum of generative models and offering an in-depth analysis of the state-of-the-art techniques, datasets, and evaluation metrics.\n\n1.2. Main Contributions\n\nThis paper provides an in-depth analysis of various aspects of GenAI MU, including technical details, categorizations, challenges, and prospective directions. We first provide an overview of the categorization of GenAI MU strategies. Specifically, we classify existing strategies into two categories: Parameter Optimization, and In-Context Unlearning. Importantly, these two categories not only offer thorough coverage of all contemporary approaches, but we also break down each category into sub-categories based on the characteristics of different types of unlearning designs. Additionally, we provide a comprehensive analysis of each category, with special emphasis on its effectiveness and potential weaknesses, which can serve as a foundation for future research and development in the field. In concrete, our contributions can be summarized as follows:\n\n•\n\nNovel Problem Objectives: We formulate the task of GenAI MU as a selective forgetting process where methods from different categories can be viewed as various approaches to removing different types of knowledge. We highlight the biggest difference between GenAI MU and traditional MU lies in the focus on forgetting specific outputs rather than specific input-output mappings. We uniquely highlight that an effective GenAI MU method should be evaluated based on three important objectives: Accuracy, Locality, and Generalizability. This structured framework provides clear and measurable goals for unlearning practices in generative models.\n\n•\n\nBroader Categorization: We cover the full spectrum of existing unlearning techniques for generative models, including generative image models, (Large) Language Models (LLMs), and Multimodal (Large) Language Models (MLLMs). Specifically, our categorization is based on how the target knowledge is removed from the pre-trained generative models, incorporating two distinct categories: Parameter Optimization, and In-Context Unlearning. The advantages and disadvantages of each method are comprehensively discussed in the survey.\n\n•\n\nFuture Directions: We investigate the practicality of contemporary GenAI MU techniques across various downstream applications. Additionally, we thoroughly discuss the challenges present in the existing literature and highlight prospective future directions within the field.\n\nThe remainder of the survey is structured as follows: Section 2 introduces a comprehensive summary of evaluation metrics for GenAI MU strategies, aligning with newly formulated objectives. Section 3 introduces the background knowledge of machine unlearning and generative models. Section 4 provides a comprehensive categorization of existing unlearning strategies for different types of generative models, with a thorough emphasis on their relationships and distinctions. Section 5 presents the commonly used datasets and prevalently used evaluation benchmarks for GenAI MU in various downstream applications. Section 6 offers a comprehensive overview of realistic applications of unlearning techniques. Section 7 discusses potential challenges in GenAI MU and offers several opportunities that may inspire future research. Finally, Section 8 concludes the survey.\n\n2. Evaluation Metrics\n\nBefore delving into the taxonomy of GenAI MU techniques in details, in this section, we first introduce various evaluation metrics commonly used to evaluate the effectiveness of different GenAI MU strategies from varied perspectives. We align the metrics with our pre-defined objectives for a better demonstration. An overall assessment of a harmful unlearning application on LLM is displayed in Figure 2.\n\n2.1. Accuracy\n\nAccuracy is a fundamental evaluation metric for measuring the effectiveness of unlearning techniques in Gen AI models. It assesses the extent to which the model successfully forgets specific unwanted target knowledge. This metric captures the expectation that the unlearned model g∗superscript𝑔g^{*}italic_g start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT does not produce outputs that it should have forgotten regardless of the input x𝑥xitalic_x. By maximizing Acc⁢(g∗;𝒟~f)Accsuperscript𝑔subscript~𝒟𝑓\\text{Acc}(g^{*};\\tilde{\\mathcal{D}}_{f})Acc ( italic_g start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ; over~ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT ), we ensure that the unlearned model does not generate any outputs associated with the target forget dataset, demonstrating high accuracy. Accuracy can be easily defined to evaluate the performance of GenAI MU techniques under various scenarios. For example, in safety alignment applications (Liu et al., 2024a; Yao et al., 2023; Heng and Soh, 2024), accuracy can be defined to assess the harmfulness or appropriateness of the output y𝑦yitalic_y using models such as GPT-4, moderation models (Ji et al., 2024), and NudeNet detector (Bedapudi, 2019). In the context of hallucination reduction (Yao et al., 2023; Hu et al., 2024; Xing et al., 2024), accuracy is typically evaluated by comparing the output with the ground truth on factual datasets or benchmarks (Lin et al., 2021; Li et al., 2023a; Yu et al., 2023b; Rohrbach et al., 2018). For bias alleviation (Kadhe et al., 2023; Yu et al., 2023a), accuracy can be measured using Equalized Odds (EO) or StereoSet metrics, which are calculated by comparing the probability assigned to contrasting portions of each sentence, conditioned on the shared portion of the sentence. Finally, for copyright protection and privacy compliance (Wu et al., 2023b; Yao et al., 2023; Fan et al., 2023; Cheng and Amiri, 2023; Wang et al., 2023), accuracy is evaluated by the memorization level, the accuracy on selected forget set, or the success rate of membership inference attacks, which detect whether a target data was used to train the model.\n\n2.2. Locality\n\nLocality is the second objective of GenAI MU, measuring the unlearned model’s capability to preserve knowledge unrelated to the unlearned content. In traditional unlearning, locality is typically measured by retained accuracy, indicating the model’s performance on the retaining dataset 𝒟rsubscript𝒟𝑟\\mathcal{D}_{r}caligraphic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT, which is separated from the known dataset 𝒟𝒟\\mathcal{D}caligraphic_D. However, defining a specific 𝒟rsubscript𝒟𝑟\\mathcal{D}_{r}caligraphic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT in GenAI MU can be challenging due to the large volume and diversity of the pre-trained dataset. Therefore, in the context of GenAI models, 𝒟rsubscript𝒟𝑟\\mathcal{D}_{r}caligraphic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT denotes any arbitrary set of data that is not part of the forget set 𝒟fsubscript𝒟𝑓\\mathcal{D}_{f}caligraphic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT, on which the model’s performance needs to be preserved. Locality ensures that the unlearned model retains its ability to produce the same outputs as the original model g𝑔gitalic_g for the retain dataset, thereby demonstrating high locality. Depending on the modality, unlearned model g∗superscript𝑔g^{*}italic_g start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT may need to preserve different types of knowledge. For instance, Receler (Huang et al., 2023a) highlights the importance of locality in text-to-image models by evaluating whether erasing one concept affects the synthesis of unrelated concepts. Large Language Models (LLMs), on the other hand, need to maintain performance across various tasks beyond the targeted unlearned skill. Studies such as (Jang et al., 2022; Wang et al., 2024c; Zhao et al., 2023a; Isonuma and Titov, 2024) measure performance on benchmarks like MMLU (Hendrycks et al., 2020), MathQA (Amini et al., 2019), and GLUE (Wang et al., 2018) after unlearning to evaluate model locality on unrelated datasets.\n\n2.3. Generalizability\n\nGeneralizability is a crucial metric for evaluating the effectiveness of GenAI unlearning approaches, aligning with the third objective of GenAI MU. It measures the capability of the unlearned model g∗superscript𝑔g^{*}italic_g start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT to handle similar unseen forgetting datasets 𝒟^fsubscript^𝒟𝑓\\hat{\\mathcal{D}}_{f}over^ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT encountered after unlearning, ensuring that the unlearned knowledge extends to other similar inputs not present in the target forgetting dataset 𝒟~fsubscript~𝒟𝑓\\tilde{\\mathcal{D}}_{f}over~ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT. This metric ensures that the unlearned model does not produce outputs related to the forgotten concepts, regardless of the input x𝑥xitalic_x. While the accuracy metric ensures the model does not generate outputs from the target forget dataset 𝒟~fsubscript~𝒟𝑓\\tilde{\\mathcal{D}}_{f}over~ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT, generalizability ensures that unlearning extends to new, unseen inputs related to the forgotten content. For example, when removing harmful information from LLMs, studies such as SKU (Liu et al., 2024a) and LLMU (Yao et al., 2023) evaluate model performance on unseen harmful queries. Similarly, when removing sensitive concepts like nudity from text-to-image models, it is essential to ensure all related images are recognized and erased (Heng and Soh, 2024; Huang et al., 2023a; Kumari et al., 2023; Gandikota et al., 2023). This verifies that the model does not produce outputs related to the unlearned concepts with new inputs.\n\n3. Background\n\nIn this section, we first provide the basics knowledge of generative models as background knowledge and an overview of machine unlearning of non-generative models to further facilitate the understanding of technical details in GenAI MU.\n\n3.1. Generative Models\n\n3.1.1. Generative Image Models\n\nDeep learning has extensively explored various generative image models, including Autoencoders, Generative Adversarial Networks (GANs), Diffusion Models, and Text-to-Image Diffusion Models. In particular, autoencoders (Kingma and Welling, 2013; Vincent et al., 2010) comprise an encoder that transforms an image into a latent vector and a decoder that generates new images from these vectors. GANs (Goodfellow et al., 2014; Westerlund, 2019; Mirza and Osindero, 2014) use a min-max game where the generator creates images and the discriminator distinguishes real from generated ones, refining both through adversarial training. Diffusion Models (Rombach et al., 2022; Ho et al., 2020) generate images via a forward process that adds noise to an image and a reverse process that denoises it to reconstruct the original. Text-to-Image Diffusion Models (Li et al., 2019; Zhou et al., 2022) like Stable Diffusion (Rombach et al., 2022), MidJourney, and DALL-E2, generate images from textual descriptions using Latent Diffusion Models (LDMs) combined with CLIP (Radford et al., 2021). Advanced techniques like Textual Inversion (Gal et al., 2022) and DreamBooth (Ruiz et al., 2023) have recently enhanced these models for customized image editing.\n\n3.1.2. (Large) Language Models\n\nLanguage models often refer to probabilistic models that generate the likelihood of word sequences to predict future tokens (Li et al., 2024d). On the other hand, Large Language Model (LLMs) are deep neural language models with billions of parameters, pre-trained on extensive text corpora to understand natural language distribution and structure (Zhao et al., 2023b). Almost all LLMs use the transformer architecture for its efficiency (Vaswani et al., 2017). In particular, there are three types of language model designs: encoder-only, decoder-only, and encoder-decoder. Encoder-only models like BERT (Devlin et al., 2018) use bidirectional attention and are pre-trained on tasks like masked token prediction, enabling them to adapt quickly to various downstream tasks. Decoder-only models like GPT (Radford et al., 2018) are trained on next-token prediction tasks using the transformer decoder architecture. Encoder-decoder models like T5 (Raffel et al., 2020) are trained on text-to-text tasks, with encoders extracting contextual representations and decoders mapping these representations back to text. In the context of LLMs, most models contain the decoder-only architecture for simplicity and efficient inference.\n\n3.1.3. Multimodal (Large) Language Models\n\nMultimodal (Large) Language Models (MLLMs) refers to the models that enable LLMs to perceive and integrates information from various data modalities. One key branch of MLLMs is vision-language pre-training, which aims to enhance performance on vision and language tasks by learning multimodal foundation models. Vision Transformer (ViT) (Dosovitskiy et al., 2020) introduced an end-to-end solution by applying the Transformer encoder to images, while CLIP (Radford et al., 2021) used multimodal pre-training to convert classification into a retrieval task, enabling zero-shot recognition. Recent advancements in LLMs like LLaMA (Touvron et al., 2023), and GPT (Achiam et al., 2023) have benefited from scaled-up training data and increased parameters, leading to substantial improvements in language understanding, generation, and knowledge reasoning. These developments have popularized the use of auto-regressive language models as decoders in vision-language tasks, facilitating knowledge sharing between language and multimodal domains, thereby promoting the development of advanced MLLMs like GPT-4v and LLaVA (Liu et al., 2023b).\n\n3.2. Machine Unlearning for Non-Generative Models\n\nThe concept of Machine Unlearning (MU) was first raised by (Cao and Yang, 2015) in response to privacy regulations like the General Data Protection Regulation of the European Union (Hoofnagle et al., 2019) and the California Consumer Privacy Act (Pardau, 2018) have established the right to be forgotten (Bourtoule et al., 2021; Dang, 2021; Ginart et al., 2019), which mandates the elimination of specific user data from models upon removal requests. In particular, previous works have considered MU for non-generative models in two criteria: Exact Unlearning and Approximate Unlearning.\n\n3.2.1. Exact Unlearning\n\nThe most straightforward approach to ”unlearn” a data point is to remove it from the training set and then retrain the model from scratch, which can be both expensive and inefficient. Exact Unlearning aims to eliminate all information related to the selected data so that the unlearned model performs identically to a retrained model. (Bourtoule et al., 2021) first introduced the SISA framework, which partitions data into shards and slices, with each shard serving as a weak learner that can be quickly retrained upon an unlearning request. (Ginart et al., 2019) presented unlearning approaches for k-means clustering. However, exact unlearning does not allow algorithms to trade privacy for utility and efficiency due to its stringent privacy criteria.\n\n3.2.2. Approximate Unlearning\n\nOn the other hand, approximate unlearning (Nguyen et al., 2020; Golatkar et al., 2020b; Chundawat et al., 2023a, b) only requires the unlearned model to perform similarly to the retrained model, allowing for a better balance between utility and efficiency. For example, ConMU (Liu et al., 2023a) introduces a controllable unlearning pipeline that aims to address the trilemma within the realm of MU, focusing on balancing privacy, utility, and runtime efficiency. Additionally, Koh and Liang (2017) approximates model perturbation towards empirical risk minimization on the retaining datasets using techniques like the inverse of the Hessian matrix. Golatkar et al. (2020a) employs Fisher-based unlearning techniques to estimate the impact of removing specific information and propose methods to effectively remove this information from intermediate layers of DNNs. These strategies highlight the potential of approximate unlearning to achieve efficient and effective unlearning without the need for complete retraining.\n\n3.3. Relationship with Model Editing\n\nAnother closely related concept to GenAI unlearning is model editing, which focuses on the local or global modification of specific pre-trained model behaviors by replacing outdated, incomplete, or inappropriate information with new knowledge. There are several similarities between these two domains. Firstly, they overlap when the objective of model editing is to erase certain information, as editing techniques can be considered a form of or an aid to the unlearning process in in-context unlearning (Das et al., 2024) and large-scale knowledge unlearning (Wang et al., 2024b). Secondly, the evaluation of model editing can align with that of unlearning, as the techniques are measured in terms of locality and Generalizability (Cheng et al., 2023; Mitchell et al., 2022; Orgad et al., 2023; Cohen et al., 2024). It is crucial to ensure that both editing and unlearning scopes can modify the appropriate amount of knowledge without compromising the model’s general abilities in other areas (Gu et al., 2024b; Orgad et al., 2023). Additionally, both model editing and unlearning share commonalities in their approaches, regardless of the modality. They first identify the knowledge or information that needs to be edited or unlearned and then apply the corresponding techniques based on specific requirements (Li et al., 2024b; Wang et al., 2024a; Geva et al., 2022).\n\nHowever, despite the commonalities, model editing and unlearning differ in several key aspects. First, while both model unlearning and editing target specific pre-trained knowledge within the model, model editing replaces obsolete knowledge with updated information. In contrast, the primary objective of unlearning is to eliminate the influence of particular knowledge without affecting the model’s utility in various downstream applications. Secondly, the responses to unlearning are more diverse and unpredictable compared to model editing. For instance, if the target concept harmful knowledge, unlearned GenAI models responses should not contain any harmful knowledge, whereas the edited GenAI models only changes specific harmful question answer mapping.\n\nIn addition, ineffective unlearning may further hurt model reasoning ability on unrelated content.\n\n3.4. Relationship with RLHF\n\nOne popular technique for aligning model behaviors with human values is RLHF (Yu et al., 2023b; Ji et al., 2024; Sun et al., 2023a; Black et al., 2023). Some existing GenAI unlearning works (Liu et al., 2024a; Yao et al., 2023) have demonstrated that datasets created from RLHF can be beneficial in the unlearning or evaluation process. However, RLHF has several drawbacks: firstly, collecting human inputs and outputs is expensive, as it typically requires a large number of professional annotators. Secondly, RLHF approaches are computationally intensive, demanding extensive GPU resources for further training the model, often for hours or even days. In contrast, GenAI unlearning offers an effective alternative to RLHF. It requires only the collections of target negative examples 𝒟~fsubscript~𝒟𝑓\\tilde{\\mathcal{D}}_{f}over~ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT and unseen samples 𝒟^fsubscript^𝒟𝑓\\hat{\\mathcal{D}}_{f}over^ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT that practitioners intend the model to unlearn, which are easier and cheaper to collect through user reporting or red teaming compared to the positive samples needed for RLHF. Additionally, it is computationally efficient, as unlearning can usually be accomplished with the same resources required for fine-tuning.\n\n4. Methodology Categorization\n\nIn this section, we summarize the current approaches for generative model unlearning. In particular, we first classify all the approaches into two categories based on their actions to 𝒟fsubscript𝒟𝑓\\mathcal{D}_{f}caligraphic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT and 𝒟rsubscript𝒟𝑟\\mathcal{D}_{r}caligraphic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT: Parameter Optimization, and In-Context Unlearning, as it displayed in Table 3. Then, we further divide those categories into subcategories representing the unlearning applications and targets. Subsequent subsections will delve into the detailed explanations of each subcategories.\n\n4.1. Parameter Optimization\n\n4.1.1. Overview:\n\nThe unlearning approach through parameter optimization aims to adjust specific model parameters to selectively unlearn certain behaviors without affecting other functions. This method aims to perform minimal alterations to parameters linked with target forget data 𝒟~fsubscript~𝒟𝑓\\tilde{\\mathcal{D}}_{f}over~ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT influences or biases to preserve essential model performance. However, such direct modifications to parameters can unintentionally impact unrelated parameters, potentially reducing model utility. Parameter optimization for unlearning can be implemented through various methods, including gradient-based adjustments, knowledge distillation, data sharding, integrating extra learnable layers, task vector, and parameter efficient module operation.\n\n4.1.2. Gradient Based\n\nGradient based unlearning approaches aim to adjust the model parameters in a way that selectively forgets the knowledge associated with specific data points or patterns. This is achieved by optimizing the model’s loss function in reverse (for gradient ascent) or forward (for gradient descent) directions to effectively remove or mitigate the learned associations without significantly impacting the model’s overall performance.\n\nGradient-based with reverse loss: The gradient ascent (GA) based approach is originate from a typical optimization-based technique raised by (Thudi et al., 2022). Given a target forget set 𝒟f~~subscript𝒟𝑓\\tilde{\\mathcal{D}_{f}}over~ start_ARG caligraphic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT end_ARG and an arbitrary loss function ℒ⁢(θ)ℒ𝜃\\mathcal{L}(\\theta)caligraphic_L ( italic_θ ), the GA algorithm iteratively update the model at each training step t𝑡titalic_t:\n\nθt+1←θt+λ⁢∇θtℒ⁢(θ)←subscript𝜃𝑡1subscript𝜃𝑡𝜆subscript∇subscript𝜃𝑡ℒ𝜃\\theta_{t+1}\\leftarrow\\theta_{t}+\\lambda\\nabla_{\\theta_{t}}\\mathcal{L}(\\theta)italic_θ start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT ← italic_θ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_λ ∇ start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT caligraphic_L ( italic_θ )\n\nwhere λ𝜆\\lambdaitalic_λ is the unlearning rate and θtsubscript𝜃𝑡\\theta_{t}italic_θ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT denotes model parameters at step t𝑡titalic_t. The objective of such approach reverts the change of the gradient descent without reverse loss term during the training with its opposite operation. KUL (Jang et al., 2022) implements the gradient ascent (GA) method to intentionally maximize the loss function, thereby shifting the model’s predictions for specific target samples in the opposite direction. Notably, during the unlearning process, KUL updates only a small subset of θ𝜃\\thetaitalic_θ to unlearn the target unlearn sample 𝒟f~~subscript𝒟𝑓\\tilde{\\mathcal{D}_{f}}over~ start_ARG caligraphic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT end_ARG. However, KUL may be impractical in scenarios where sensitive information is not explicitly defined. Building on this concept, UnTrac (Isonuma and Titov, 2024) employs the GA approach but extends its application to estimate the influence of a training dataset on a test dataset through unlearning. Nevertheless, though GA based approach can obtain an excellent unlearning performance, this usually comes with a large sacrifice of the model utility with non-target samples (i.e. catastrophic collapse), as it elaborated in (Liu et al., 2024a; Yao et al., 2023). To address this utility degradation, NPO(Zhang et al., 2024a) leverages the principles of preference optimization but uniquely applies it using only negative samples. By modifying the standard preference optimization to exclude positive examples, NPO aims to make the model forget target undesirable data 𝒟f~~subscript𝒟𝑓\\tilde{\\mathcal{D}_{f}}over~ start_ARG caligraphic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT end_ARG by decreasing their prediction probability. Additionally, the work theoretically shows that the progression toward catastrophic collapse can be exponentially slower when using NPO loss than pure GA. Specifically, NPO approach utilizes the following loss function:\n\nℒN⁢P⁢O,β⁢(θ)=−2β⁢𝔼𝒟f⁢[log⁡σ⁢(−β⁢log⁡g∗⁢(y|x)g⁢(y|x))].subscriptℒ𝑁𝑃𝑂𝛽𝜃2𝛽subscript𝔼subscript𝒟𝑓delimited-[]𝜎𝛽superscript𝑔∗conditional𝑦𝑥𝑔conditional𝑦𝑥\\mathcal{L}_{NPO,\\beta}(\\theta)=-\\frac{2}{\\beta}\\mathbb{E}_{\\mathcal{D}_{f}}% \\left[\\log\\sigma\\left(-\\beta\\log\\frac{g^{\\ast}(y|x)}{g(y|x)}\\right)\\right].caligraphic_L start_POSTSUBSCRIPT italic_N italic_P italic_O , italic_β end_POSTSUBSCRIPT ( italic_θ ) = - divide start_ARG 2 end_ARG start_ARG italic_β end_ARG blackboard_E start_POSTSUBSCRIPT caligraphic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ roman_log italic_σ ( - italic_β roman_log divide start_ARG italic_g start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_y | italic_x ) end_ARG start_ARG italic_g ( italic_y | italic_x ) end_ARG ) ] .\n\nIn this formulation, β𝛽\\betaitalic_β is a scaling parameter that adjusts the sharpness of the preference modifications. The term log⁡g∗⁢(y|x)g⁢(y|x)superscript𝑔∗conditional𝑦𝑥𝑔conditional𝑦𝑥\\log\\frac{g^{\\ast}(y|x)}{g(y|x)}roman_log divide start_ARG italic_g start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_y | italic_x ) end_ARG start_ARG italic_g ( italic_y | italic_x ) end_ARG measures how the predictions of the current unlearned model deviate from those of the pre-trained model. This loss function aims to minimize the likelihood of the model predicting outcomes linked to the 𝒟~fsubscript~𝒟𝑓\\tilde{\\mathcal{D}}_{f}over~ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT, effectively diminishing the model’s recall of this data.\n\nBesides, numerous other works have built based on GA approach and extend with more efficient module. For example, LLMU (Yao et al., 2023) builds upon traditional GA methods and incorporates two arbitrary loss functions ℒforgetsubscriptℒforget\\mathcal{L}_{\\text{forget}}caligraphic_L start_POSTSUBSCRIPT forget end_POSTSUBSCRIPT and ℒmismatchsubscriptℒmismatch\\mathcal{L}_{\\text{mismatch}}caligraphic_L start_POSTSUBSCRIPT mismatch end_POSTSUBSCRIPT to prevent model from generating unwanted outputs and maintains model functionality on normal prompts, which can be expressed as:\n\nθt+1=θt−ϵ1⋅∇θtℒforget−ϵ2⋅∇θtℒmismatch−ϵ3⋅∇θtℒmaintain.subscript𝜃𝑡1subscript𝜃𝑡⋅subscriptitalic-ϵ1subscript∇subscript𝜃𝑡subscriptℒforget⋅subscriptitalic-ϵ2subscript∇subscript𝜃𝑡subscriptℒmismatch⋅subscriptitalic-ϵ3subscript∇subscript𝜃𝑡subscriptℒmaintain\\theta_{t+1}=\\theta_{t}-\\epsilon_{1}\\cdot\\nabla_{\\theta_{t}}\\mathcal{L}_{\\text% {forget}}-\\epsilon_{2}\\cdot\\nabla_{\\theta_{t}}\\mathcal{L}_{\\text{mismatch}}-% \\epsilon_{3}\\cdot\\nabla_{\\theta_{t}}\\mathcal{L}_{\\text{maintain}}.italic_θ start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT = italic_θ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - italic_ϵ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ⋅ ∇ start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT forget end_POSTSUBSCRIPT - italic_ϵ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ⋅ ∇ start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT mismatch end_POSTSUBSCRIPT - italic_ϵ start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ⋅ ∇ start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT maintain end_POSTSUBSCRIPT .\n\nSimilarly, Eraser (Lu et al., 2024) enhances the ℒforgetsubscriptℒforget\\mathcal{L}_{\\text{forget}}caligraphic_L start_POSTSUBSCRIPT forget end_POSTSUBSCRIPT objective by adding random prefixes and suffixes to each query from 𝒟rsubscript𝒟𝑟\\mathcal{D}_{r}caligraphic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT during training to simulate jailbreaking attacks, ensuring robust unlearning against varied prompts. It uses GPT-3.5 to extract entities from unwanted knowledge, which in this specific work refers to harmful responses, creating a dataset that helps the model retain general knowledge about entities in jailbreak prompts. Eraser also captures refusal responses from the original aligned model θ𝜃\\thetaitalic_θ and trains the unlearned model θusubscript𝜃𝑢\\theta_{u}italic_θ start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT to replicate these refusals, maintaining the model’s safety alignment by rejecting harmful inquiries. Besides, GA approach can be employed on MLLMs to mitigate multimodal hallucination problems. For example, EFUF (Xing et al., 2024) operates by initially assessing text-image congruence using the CLIP model, which scores the relevance of text descriptions to their corresponding images. Based on these scores, the framework identifies hallucinated content—descriptions that mention objects not present in the images.\n\nGradient-based without reverse loss: Given that the gradient ascent (GA) based approach may degrade model performance, regular gradient descent offers a more balanced method for model training and unlearning. Within this framework, various innovative methods have been developed. Inspired by the Lottery Ticket Hypothesis (Frankle and Carbin, 2018), PCGU (Yu et al., 2023a) hypothesizes that a subset of the model’s neurons encode biases/preferences in different contexts. PCGU introduces a weight importance algorithm using gradient calculations to rank weights by their contribution to biased outputs. Specifically, model parameter θ𝜃\\thetaitalic_θ and gradient ∇isubscript∇𝑖\\nabla_{i}∇ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT are partitioned into {θ1,θ2,…⁢θm}superscript𝜃1superscript𝜃2…superscript𝜃𝑚\\{\\theta^{1},\\theta^{2},...\\theta^{m}\\}{ italic_θ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , italic_θ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , … italic_θ start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT } and {∇i1,∇i2,…,∇im}superscriptsubscript∇𝑖1superscriptsubscript∇𝑖2…superscriptsubscript∇𝑖𝑚\\{\\nabla_{i}^{1},\\nabla_{i}^{2},...,\\nabla_{i}^{m}\\}{ ∇ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , ∇ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , … , ∇ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT }. The impact of the weights on biased outputs is quantified by calculating the dot product θj⋅∇ij⋅superscript𝜃𝑗superscriptsubscript∇𝑖𝑗\\theta^{j}\\cdot\\nabla_{i}^{j}italic_θ start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT ⋅ ∇ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT for each j𝑗jitalic_j, identifying weights with the highest influence on biased behavior for targeted modification in the unlearning phase.\n\nBuilding on the foundation of targeting specific model components for unlearning, (Gu et al., 2024a) proposes two novel unlearning methods: Fisher Removal and Fisher Forgetting for LLM leveraging second-order information, specifically the Hessian, to enhance the unlearning process. In particular, the Fisher Removal method updates the model parameters aggressively to ensure the removal of undesirable data, which can sometimes compromise the utility of the model. The objective of this method can be formulated as:\n\nθu=θ−γ⋅F−1⋅∇L⁢(θ),subscript𝜃𝑢𝜃⋅𝛾superscript𝐹1∇𝐿𝜃\\theta_{u}=\\theta-\\gamma\\cdot F^{-1}\\cdot\\nabla L(\\theta),italic_θ start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT = italic_θ - italic_γ ⋅ italic_F start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ⋅ ∇ italic_L ( italic_θ ) ,\n\nwhere F−1superscript𝐹1F^{-1}italic_F start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT represents the inverse Fisher information. Then, Fisher Forgetting is a less aggressive variant of Fisher Removal, better preserving the model’s accuracy even through multiple cycles of unlearning. This method modifies the parameters by adding a controlled amount of noise η𝜂\\etaitalic_η:\n\nθu=θ−γ⋅F−1⋅∇L⁢(θ)+η.subscript𝜃𝑢𝜃⋅𝛾superscript𝐹1∇𝐿𝜃𝜂\\theta_{u}=\\theta-\\gamma\\cdot F^{-1}\\cdot\\nabla L(\\theta)+\\eta.italic_θ start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT = italic_θ - italic_γ ⋅ italic_F start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ⋅ ∇ italic_L ( italic_θ ) + italic_η .\n\nFurther advancing the complexity of unlearning strategies, min-min optimization (Li et al., 2023c) adapts the bi-level optimization approach previously applied to images for creating unlearnable text, which the objective can be formulated as:\n\nargminθ⁢𝔼(x+η,y)∼D⁢[argminη⁢ℒ⁢(g⁢(x+η),y)].subscriptargmin𝜃subscript𝔼similar-to𝑥𝜂𝑦𝐷delimited-[]subscriptargmin𝜂ℒ𝑔𝑥𝜂𝑦\\text{argmin}_{\\theta}\\mathbb{E}_{(x+\\eta,y)\\sim D}\\left[\\text{argmin}_{\\eta}% \\mathcal{L}(g(x+\\eta),y)\\right].argmin start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT ( italic_x + italic_η , italic_y ) ∼ italic_D end_POSTSUBSCRIPT [ argmin start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT caligraphic_L ( italic_g ( italic_x + italic_η ) , italic_y ) ] .\n\nSpecifically, the method leverages error-minimization modifications to alter targeted forget data 𝒟f~~subscript𝒟𝑓\\tilde{\\mathcal{D}_{f}}over~ start_ARG caligraphic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT end_ARG through a controllable noise η𝜂\\etaitalic_η in such a way that it remains unlearnable even for models not seen during the optimization process. Building on gradient descent applications in machine learning, this approach adapts to generative models like vision models. Li et al. (2024a) introduces a framework for i2i (image-to-image) generative models using a two-part encoder-decoder architecture. The encoder converts input from 𝒟f~~subscript𝒟𝑓\\tilde{\\mathcal{D}_{f}}over~ start_ARG caligraphic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT end_ARG into a representation vector, while the decoder reconstructs the image. The objective is to maximize the output distribution difference between the retained data (𝒟rsubscript𝒟𝑟\\mathcal{D}_{r}caligraphic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT) and the target forget set (𝒟~fsubscript~𝒟𝑓\\tilde{\\mathcal{D}}_{f}over~ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT), minimizing modifications to the retained data. Expanding on manipulating model outputs, Kumari et al. (2023) minimizes KL divergence between conditional distributions of target and anchor concepts, modifying the model to generate images of the anchor concept when prompted with the target concept.\n\nBesides manipulating the distance between 𝒟~fsubscript~𝒟𝑓\\tilde{\\mathcal{D}}_{f}over~ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT and 𝒟rsubscript𝒟𝑟\\mathcal{D}_{r}caligraphic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT, these datasets can be directly used for fine-tuning the model for unlearning. For instance, UBT (Liang et al., 2024) mitigates backdoor attacks by dividing 𝒟rsubscript𝒟𝑟\\mathcal{D}_{r}caligraphic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT into suspicious and clean subsets using a pre-trained model based on multimodal text similarity. Target unlearning samples (𝒟~fsubscript~𝒟𝑓\\tilde{\\mathcal{D}}_{f}over~ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT) are fine-tuned to amplify backdoor features, increasing cosine similarity measures. A token-level local unlearning strategy then selectively forgets tokens associated with backdoor triggers. Target unlearned samples 𝒟~fsubscript~𝒟𝑓\\tilde{\\mathcal{D}}_{f}over~ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT (i.e. negative samples) are also effective in vision generative models. Forget-Me-Not (Zhang et al., 2023c) uses curated images related to the concept targeted for removal during fine-tuning, adjusting the model’s cross-attention layers within the UNet architecture to minimize attention to these concepts. ESD (Gandikota et al., 2023) refines this by modifying model parameters to remove undesired concepts 𝒟~fsubscript~𝒟𝑓\\tilde{\\mathcal{D}}_{f}over~ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT, such as nudity or copyrighted styles, without dataset censorship or post-generation filtering. ESD fine-tunes the model using a short textual description of the target undesired concepts (𝒟~fsubscript~𝒟𝑓\\tilde{\\mathcal{D}}_{f}over~ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT), aiming to reduce the probability of generating images described by x𝑥xitalic_x:\n\nϵθu⁢(yt,x,t)←ϵθo⁢(yt,t)−η⁢[ϵθo⁢(yt,x,t)−ϵθ∗⁢(yt,t)]←subscriptitalic-ϵsubscript𝜃𝑢subscript𝑦𝑡𝑥𝑡subscriptitalic-ϵsubscript𝜃𝑜subscript𝑦𝑡𝑡𝜂delimited-[]subscriptitalic-ϵsubscript𝜃𝑜subscript𝑦𝑡𝑥𝑡subscriptitalic-ϵsuperscript𝜃subscript𝑦𝑡𝑡\\epsilon_{\\theta_{u}}(y_{t},x,t)\\leftarrow\\epsilon_{\\theta_{o}}(y_{t},t)-\\eta[% \\epsilon_{\\theta_{o}}(y_{t},x,t)-\\epsilon_{\\theta^{*}}(y_{t},t)]italic_ϵ start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_x , italic_t ) ← italic_ϵ start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) - italic_η [ italic_ϵ start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_x , italic_t ) - italic_ϵ start_POSTSUBSCRIPT italic_θ start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) ]\n\nNegative sample fine-tuning may involve complexities in identifying undesirable behaviors accurately, leading to potential biases. Hence, some approaches directly use 𝒟rsubscript𝒟𝑟\\mathcal{D}_{r}caligraphic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT for fine-tuning. For example, EraseDiff formulates unlearning as a constrained optimization problem. The objective is to fine-tune the model using the remaining data 𝒟rsubscript𝒟𝑟\\mathcal{D}_{r}caligraphic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT to preserve utility while erasing the influence of forget data. This is achieved by deviating the generative process from the ground-truth denoising procedure:\n\nminθo⁡ℒ⁢(θo,𝒟r)s.t.g⁢(θo,𝒟~f)−minϕ|θo⁡g⁢(ϕ,𝒟~f)≤0,subscriptsubscript𝜃𝑜ℒsubscript𝜃𝑜subscript𝒟𝑟s.t.𝑔subscript𝜃𝑜subscript~𝒟𝑓subscriptconditionalitalic-ϕsubscript𝜃𝑜𝑔italic-ϕsubscript~𝒟𝑓0\\min_{\\theta_{o}}\\mathcal{L}(\\theta_{o},\\mathcal{D}_{r})\\quad\\text{s.t.}\\quad g% (\\theta_{o},\\tilde{\\mathcal{D}}_{f})-\\min_{\\phi|\\theta_{o}}g(\\phi,\\tilde{% \\mathcal{D}}_{f})\\leq 0,roman_min start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT end_POSTSUBSCRIPT caligraphic_L ( italic_θ start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ) s.t. italic_g ( italic_θ start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT , over~ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT ) - roman_min start_POSTSUBSCRIPT italic_ϕ | italic_θ start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_g ( italic_ϕ , over~ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT ) ≤ 0 ,\n\nwhere g⁢(⋅)𝑔⋅g(\\cdot)italic_g ( ⋅ ) measures the influence of 𝒟~fsubscript~𝒟𝑓\\tilde{\\mathcal{D}}_{f}over~ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT. The constraint ensures that unlearning targets 𝒟~fsubscript~𝒟𝑓\\tilde{\\mathcal{D}}_{f}over~ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT without degrading performance on 𝒟rsubscript𝒟𝑟\\mathcal{D}_{r}caligraphic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT.\n\nBuilding on those fine-tuning approaches, which directly modify model behaviors using specific dataset, Selective Amnesia (SA) (Heng and Soh, 2024) introduces a controllable forgetting mechanism applied to conditional variational likelihood models, including variational autoencoders (VAEs) and large-scale text-to-image diffusion models. SA utilizes continual learning techniques like Elastic Weight Consolidation (EWC) (Kirkpatrick et al., 2017) and Generative Replay (GR) (Shin et al., 2017) to preserve knowledge while selectively forgetting specific concepts. Additionally, SA introduces a surrogate objective to guarantee the reduction of the log-likelihood of the data to be forgotten, providing a more controlled and effective unlearning process. However, SA is more suitable for removing specific samples and fails to be generalizable to unlearning knowledge in a broader spectrum, such as nudity. Hence, SalUn (Fan et al., 2023) method extends these principles to further refine the unlearning process by specifically targeting the modification of the model’s response to various class representations. In particular, the objective of concept unlearning for SalUn can be mathematically formulated as:\n\nminimizeLSalUn(θu):=𝔼(x,c)∼Df~,t,ϵ∼𝒩⁢(0,1),c′≠c[∥ϵθu(xt|c′)−ϵθu(xt|c)∥22]+βℒMSE(θu;Dr),\\text{minimize}\\quad L_{\\text{SalUn}}(\\theta_{u}):=\\mathbb{E}_{(x,c)\\sim\\tilde% {D_{f}},t,\\epsilon\\sim\\mathcal{N}(0,1),c^{\\prime}\\neq c}\\left[\\left\\|\\epsilon_% {\\theta_{u}}(x_{t}|c^{\\prime})-\\epsilon_{\\theta_{u}}(x_{t}|c)\\right\\|_{2}^{2}% \\right]+\\beta\\mathcal{L}_{\\text{MSE}}(\\theta_{u};D_{r}),minimize italic_L start_POSTSUBSCRIPT SalUn end_POSTSUBSCRIPT ( italic_θ start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT ) := blackboard_E start_POSTSUBSCRIPT ( italic_x , italic_c ) ∼ over~ start_ARG italic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT end_ARG , italic_t , italic_ϵ ∼ caligraphic_N ( 0 , 1 ) , italic_c start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ≠ italic_c end_POSTSUBSCRIPT [ ∥ italic_ϵ start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_c start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) - italic_ϵ start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_c ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] + italic_β caligraphic_L start_POSTSUBSCRIPT MSE end_POSTSUBSCRIPT ( italic_θ start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT ; italic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ) ,\n\nwhere ϵθu⁢(xt|c)subscriptitalic-ϵsubscript𝜃𝑢conditionalsubscript𝑥𝑡𝑐\\epsilon_{\\theta_{u}}(x_{t}|c)italic_ϵ start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_c ) and ϵθu⁢(xt|c′)subscriptitalic-ϵsubscript𝜃𝑢conditionalsubscript𝑥𝑡superscript𝑐′\\epsilon_{\\theta_{u}}(x_{t}|c^{\\prime})italic_ϵ start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_c start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) represent the noise generator parameterized by θusubscript𝜃𝑢\\theta_{u}italic_θ start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT, conditioned on the text prompt c𝑐citalic_c and c′superscript𝑐′c^{\\prime}italic_c start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT.\n\n4.1.3. Knowledge Distillation\n\nKnowledge distillation approaches typically involve a teacher-student model configuration and treat the unlearned model g∗superscript𝑔∗g^{\\ast}italic_g start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT as the student model, aiming to mimic the desirable behavior from a teacher model. KGA (Wang et al., 2023) introduces a novel framework to align knowledge gaps, defined as differences in prediction distributions between models trained on different data subsets. The KGA framework adjusts the model to minimize the discrepancy in knowledge between a base model trained on all data and new models trained on an external set 𝒟nsubscript𝒟𝑛\\mathcal{D}_{n}caligraphic_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT similar to 𝒟𝒟\\mathcal{D}caligraphic_D but excluding 𝒟fsubscript𝒟𝑓\\mathcal{D}_{f}caligraphic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT. This is formulated as:\n\nf∗=argminf⁢|dis(𝒟n)⁢(g,gn)−dis(𝒟~f)⁢(g,gf)|.superscript𝑓subscriptargmin𝑓subscriptdissubscript𝒟𝑛𝑔subscript𝑔𝑛subscriptdissubscript~𝒟𝑓𝑔subscript𝑔𝑓f^{*}=\\text{argmin}_{f}\\left|\\text{dis}_{(\\mathcal{D}_{n})}(g,g_{n})-\\text{dis% }_{(\\tilde{\\mathcal{D}}_{f})}(g,g_{f})\\right|.italic_f start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT = argmin start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT | dis start_POSTSUBSCRIPT ( caligraphic_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT ( italic_g , italic_g start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) - dis start_POSTSUBSCRIPT ( over~ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT ( italic_g , italic_g start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT ) | .\n\nHere, g𝑔gitalic_g is the original model trained on 𝒟𝒟\\mathcal{D}caligraphic_D, while gnsubscript𝑔𝑛g_{n}italic_g start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT and gfsubscript𝑔𝑓g_{f}italic_g start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT are models trained on 𝒟nsubscript𝒟𝑛\\mathcal{D}_{n}caligraphic_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT and 𝒟~fsubscript~𝒟𝑓\\tilde{\\mathcal{D}}_{f}over~ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT, respectively. The function dis measures the distributional differences using KL divergence. KGA treats the original model g𝑔gitalic_g as a teacher model and minimizes the distance of output distributions when feeding samples in 𝒟rsubscript𝒟𝑟\\mathcal{D}_{r}caligraphic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT to g∗superscript𝑔∗g^{\\ast}italic_g start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT and g𝑔gitalic_g. Moreover, (Dong et al., 2024) introduces a novel LLM unlearning approach named deliberate imagination (DI) to further maintain model’s generation and reasoning capabilities. DI employs a self-distillation technique in which a teacher model guides an LLM to generate creative and non-harmful responses, rather than merely forgetting unwanted memorized information. The process begins by strategically increasing the probability tokens within the teacher model that serve as alternatives to the memorized ones, thereby encouraging the production of novel and diverse outputs. Subsequently, DI fine-tunes the student model using the predicted output probabilities from the teacher, enabling the student models to generate imaginative responses that are less dependent on memorized data.\n\nSimilar to many white-box approaches, the KGA approach depends on accessing the model’s internal weights, which makes it inapplicable to black-box models. To address this issue, δ𝛿\\deltaitalic_δ learning (Huang et al., 2024) introduces a novel framework for unlearning in black-box LLMs without accessing to the model’s internal weights. Specifically, δ𝛿\\deltaitalic_δ learning utilizes a pair of smaller, trainable models to compute a logit offset that is then applied to the black-box LLM to adjust its responses. These smaller trainable models, referred to as offset models, are trained to predict how the logit outputs of the black-box LLM should be modified to exclude the influence of 𝒟~fsubscript~𝒟𝑓\\tilde{\\mathcal{D}}_{f}over~ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT. The offset is calculated as the difference in logits between these two models and added to the black-box LLM’s logits, guiding the final output away from sensitive information.\n\n4.1.4. Data Sharding\n\nData sharding approaches usually divide the training data into multiple shards, where each corresponds to a subset of the overall data D𝐷Ditalic_D. Inspired by SISA (Bourtoule et al., 2021), separate models are trained for each data shards that can effectively remove target data in different shard based on request. As it mentioned in Section 3, SISA provides an exact unlearning guarantee due to the data to be forgotten have no impacts on the retrained version of the model, making it suitable to wide range of model architectures, including GenAI. However, direct implementation of SISA on GenAI is infeasible due to its the high computational cost in model saving, retraining, and inference. Hence, various work have adapted the similar approach of SISA and made more suitable to GenAI depends on its corresponding unlearning scenario. FairSISA (Kadhe et al., 2023) proposes a post-processing unlearning strategy for bias mitigation in ensemble models produced by SISA. In particular, this post-processing function f𝑓fitalic_f aims to enforce fairness in the outputs of g∗superscript𝑔g^{*}italic_g start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT, particularly ensuring that the model’s predictions do not exhibit bias related to a sensitive attribute A𝐴Aitalic_A. This can be formulated as follows: f𝑓fitalic_f post-processes the outputs of g∗superscript𝑔g^{*}italic_g start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT to satisfy fairness constraints, such as equalized odds, across groups defined by A𝐴Aitalic_A without significantly affecting the overall model accuracy:\n\nminf⁡𝔼⁢[ℓ⁢(f⁢(g∗⁢(x)),Y)]subscript𝑓𝔼delimited-[]ℓ𝑓superscript𝑔𝑥𝑌\\min_{f}\\mathbb{E}[\\ell(f(g^{*}(x)),Y)]roman_min start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT blackboard_E [ roman_ℓ ( italic_f ( italic_g start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_x ) ) , italic_Y ) ]\n\nsubject to:\n\nPr⁡(f⁢(g∗⁢(x))=1|A=0,Y=y)=Pr⁡(f⁢(g∗⁢(x))=1|A=1,Y=y),∀y∈{0,1}.formulae-sequencePr𝑓superscript𝑔𝑥conditional1𝐴0𝑌𝑦Pr𝑓superscript𝑔𝑥conditional1𝐴1𝑌𝑦for-all𝑦01\\Pr(f(g^{*}(x))=1|A=0,Y=y)=\\Pr(f(g^{*}(x))=1|A=1,Y=y),\\quad\\forall y\\in\\{0,1\\}.roman_Pr ( italic_f ( italic_g start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_x ) ) = 1 | italic_A = 0 , italic_Y = italic_y ) = roman_Pr ( italic_f ( italic_g start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_x ) ) = 1 | italic_A = 1 , italic_Y = italic_y ) , ∀ italic_y ∈ { 0 , 1 } .\n\nHere, ℓℓ\\ellroman_ℓ is a loss function, and 𝔼𝔼\\mathbb{E}blackboard_E denotes the expected value, reflecting the objective to minimize any deviation from the true labels Y𝑌Yitalic_Y while adhering to the fairness constraints.\n\nNext, to reduce retraining costs while preserving utility during inference, Liu and Kalinli (2024) introduces the LOO ensemble method to unlearn target token sequences from LMs. This method uses a teacher-student framework where multiple teacher models are trained on partitioned data segments. When data removal is requested, the student model (i.e. base LM) is supervised by the remaining teachers, excluding the one trained on the segment to be unlearned. The recalibration is formalized as:\n\ngL⁢O⁢O−E−k⁢(w|w1t−1)=1M−1⁢∑m=1M𝟙⁢{𝒟k⊄Bm}⋅pθm⁢(w|w1t−1)superscriptsubscript𝑔𝐿𝑂𝑂𝐸𝑘conditional𝑤superscriptsubscript𝑤1𝑡11𝑀1superscriptsubscript𝑚1𝑀⋅1not-subset-ofsubscript𝒟𝑘subscript𝐵𝑚subscript𝑝subscript𝜃𝑚conditional𝑤superscriptsubscript𝑤1𝑡1g_{LOO-E}^{-k}(w|w_{1}^{t-1})=\\frac{1}{M-1}\\sum_{m=1}^{M}\\mathbbm{1}\\{\\mathcal% {D}_{k}\\not\\subset B_{m}\\}\\cdot p_{\\theta_{m}}(w|w_{1}^{t-1})italic_g start_POSTSUBSCRIPT italic_L italic_O italic_O - italic_E end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - italic_k end_POSTSUPERSCRIPT ( italic_w | italic_w start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT ) = divide start_ARG 1 end_ARG start_ARG italic_M - 1 end_ARG ∑ start_POSTSUBSCRIPT italic_m = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT blackboard_1 { caligraphic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ⊄ italic_B start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT } ⋅ italic_p start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_w | italic_w start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT )\n\nwhere gL⁢O⁢O−E−ksuperscriptsubscript𝑔𝐿𝑂𝑂𝐸𝑘g_{LOO-E}^{-k}italic_g start_POSTSUBSCRIPT italic_L italic_O italic_O - italic_E end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - italic_k end_POSTSUPERSCRIPT is the aggregated prediction excluding the teacher trained with 𝒟k⊂𝒟fsubscript𝒟𝑘subscript𝒟𝑓\\mathcal{D}_{k}\\subset\\mathcal{D}_{f}caligraphic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ⊂ caligraphic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT. θmsubscript𝜃𝑚\\theta_{m}italic_θ start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT denotes the parameters of the m-th teacher model, and w1t−1superscriptsubscript𝑤1𝑡1w_{1}^{t-1}italic_w start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT reflects the input sequence up to step t−1𝑡1t-1italic_t - 1. The student model fine-tunes its parameters based on KL divergence, ensuring sensitive data is unlearned while maintaining performance on 𝒟rsubscript𝒟𝑟\\mathcal{D}_{r}caligraphic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT. Despite improved utility performance compared to SISA, LOO lacks a theoretical guarantee for teacher models and may impose significant computational overheads on GenAI like LLMs.\n\nThe data sharding approach can be applied to vision generative models and LLMs for measuring training data influence for unlearning. Dai and Gifford (2023) presents temporary unlearning using ensemble ablation. The core idea is to use an ensemble of diffusion models, each trained on a subset of data, to assess the influence of individual data points by excluding models exposed to the data point and observing changes in outputs. The work provides a theoretical guarantee that the resulting model has not seen the removed training sample. The ensemble is represented as fesubscript𝑓𝑒f_{e}italic_f start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT:\n\nfe⁢(x,t)=𝔼S∼𝒟⁢[𝔼r∼R⁢[f⁢(x,t,A⁢(S,r))]]subscript𝑓𝑒𝑥𝑡subscript𝔼similar-to𝑆𝒟delimited-[]subscript𝔼similar-to𝑟𝑅delimited-[]𝑓𝑥𝑡𝐴𝑆𝑟f_{e}(x,t)=\\mathbb{E}_{S\\sim\\mathscr{D}}[\\mathbb{E}_{r\\sim R}[f(x,t,A(S,r))]]italic_f start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT ( italic_x , italic_t ) = blackboard_E start_POSTSUBSCRIPT italic_S ∼ script_D end_POSTSUBSCRIPT [ blackboard_E start_POSTSUBSCRIPT italic_r ∼ italic_R end_POSTSUBSCRIPT [ italic_f ( italic_x , italic_t , italic_A ( italic_S , italic_r ) ) ] ]\n\nwhere x𝑥xitalic_x is the input, t𝑡titalic_t is the diffusion time, 𝒟𝒟\\mathscr{D}script_D represents the uniform distribution over 2𝒟superscript2𝒟2^{\\mathcal{D}}2 start_POSTSUPERSCRIPT caligraphic_D end_POSTSUPERSCRIPT, and A⁢(⋅)𝐴⋅A(\\cdot)italic_A ( ⋅ ) denotes the training procedure with training samples and exogenous noise r𝑟ritalic_r. To assess the influence of a specific point x~~𝑥\\tilde{x}over~ start_ARG italic_x end_ARG, the ensemble without x~~𝑥\\tilde{x}over~ start_ARG italic_x end_ARG, fe−x~superscriptsubscript𝑓𝑒~𝑥f_{e}^{-\\tilde{x}}italic_f start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - over~ start_ARG italic_x end_ARG end_POSTSUPERSCRIPT, is evaluated as:\n\nfe−x~⁢(x,t)=1Pr⁢(x∈S′∼𝒳)⁢𝔼S∼𝒟⁢[𝔼r∼R⁢[f⁢(x,t,A⁢(S,r))⁢𝟙x~∉S]].superscriptsubscript𝑓𝑒~𝑥𝑥𝑡1Pr𝑥superscript𝑆′similar-to𝒳subscript𝔼similar-to𝑆𝒟delimited-[]subscript𝔼similar-to𝑟𝑅delimited-[]𝑓𝑥𝑡𝐴𝑆𝑟subscript1~𝑥𝑆f_{e}^{-\\tilde{x}}(x,t)=\\frac{1}{\\text{Pr}(x\\in S^{\\prime}\\sim\\mathcal{X})}% \\mathbb{E}_{S\\sim\\mathscr{D}}[\\mathbb{E}_{r\\sim R}[f(x,t,A(S,r))\\mathbbm{1}_{% \\tilde{x}\\notin S}]].italic_f start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - over~ start_ARG italic_x end_ARG end_POSTSUPERSCRIPT ( italic_x , italic_t ) = divide start_ARG 1 end_ARG start_ARG Pr ( italic_x ∈ italic_S start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ∼ caligraphic_X ) end_ARG blackboard_E start_POSTSUBSCRIPT italic_S ∼ script_D end_POSTSUBSCRIPT [ blackboard_E start_POSTSUBSCRIPT italic_r ∼ italic_R end_POSTSUBSCRIPT [ italic_f ( italic_x , italic_t , italic_A ( italic_S , italic_r ) ) blackboard_1 start_POSTSUBSCRIPT over~ start_ARG italic_x end_ARG ∉ italic_S end_POSTSUBSCRIPT ] ] .\n\nThis method aligns with machine unlearning by providing a more efficient way to disregard specific data influences without extensive retraining. Using an ensemble approach ensures effective data attribution and compliance with privacy and fairness.\n\n4.1.5. Extra Learnable Layers\n\nAnother parameter optimization approach is to introduce additional parameters or trainable layers in the model and train them to actively forget different sets of data from 𝒟~fsubscript~𝒟𝑓\\tilde{\\mathcal{D}}_{f}over~ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT. This approach negates the necessity of modifying the model’s inherent parameters, thereby preventing interference to its original knowledge. EUL (Chen and Yang, 2023) integrates an additional unlearning layer, Ul(i)superscriptsubscript𝑈𝑙𝑖U_{l}^{(i)}italic_U start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT, into the transformer following the feed-forward networks. Throughout the training, this unlearning layer is exclusively engaged to learn to forget the specified data, while the rest of the model parameters, denoted as θosubscript𝜃𝑜\\theta_{o}italic_θ start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT, remain frozen. Upon receiving a deletion request, the model first trains a distinct unlearning layer Ul(i)superscriptsubscript𝑈𝑙𝑖U_{l}^{(i)}italic_U start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT with parameters θisubscript𝜃𝑖\\theta_{i}italic_θ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT tailored to that request. Subsequently, these layers are merged using a fusion mechanism to form a unified unlearning layer with parameters θmsubscript𝜃𝑚\\theta_{m}italic_θ start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT, achieved by minimizing a regression objective:\n\nminθm⁢∑i‖θmT⁢Xf(i)−θiT⁢Xf(i)‖2subscriptsubscript𝜃𝑚subscript𝑖superscriptnormsuperscriptsubscript𝜃𝑚𝑇superscriptsubscript𝑋𝑓𝑖superscriptsubscript𝜃𝑖𝑇superscriptsubscript𝑋𝑓𝑖2\\min_{\\theta_{m}}\\sum_{i}\\left\\|\\theta_{m}^{T}X_{f}^{(i)}-\\theta_{i}^{T}X_{f}^% {(i)}\\right\\|^{2}roman_min start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT end_POSTSUBSCRIPT ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∥ italic_θ start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_X start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT - italic_θ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_X start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ∥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT\n\nwhere θmsubscript𝜃𝑚\\theta_{m}italic_θ start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT is defined as:\n\nθm=(∑iXf(i)⁢T⁢Xf(i))−1⁢∑i(Xf(i)⁢T⁢Xf(i)⁢θi).subscript𝜃𝑚superscriptsubscript𝑖superscriptsubscript𝑋𝑓𝑖𝑇superscriptsubscript𝑋𝑓𝑖1subscript𝑖superscriptsubscript𝑋𝑓𝑖𝑇superscriptsubscript𝑋𝑓𝑖subscript𝜃𝑖\\theta_{m}=\\left(\\sum_{i}X_{f}^{(i)T}X_{f}^{(i)}\\right)^{-1}\\sum_{i}\\left(X_{f% }^{(i)T}X_{f}^{(i)}\\theta_{i}\\right).italic_θ start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT = ( ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_X start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) italic_T end_POSTSUPERSCRIPT italic_X start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_X start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) italic_T end_POSTSUPERSCRIPT italic_X start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT italic_θ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) .\n\nThis results in a unified unlearning transformer capable of handling multiple deletion requests sequentially, ensuring that all specified data is effectively forgotten while preserving the integrity of model behaviors on other tasks.\n\nSimilarly, Receler (Huang et al., 2023a) introduces lightweight eraser parameters θEsubscript𝜃𝐸\\theta_{E}italic_θ start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT during unlearning, which is designed to remove the target concept from the outputs of each cross-attention layer within the diffusion U-Net. To erase the concept, the eraser is trained to predict the negatively guided noises that move the model’s prediction away from the erased concept. In particular, the objective is defined as:\n\nℒErasesubscriptℒErase\\displaystyle\\mathcal{L}_{\\text{Erase}}caligraphic_L start_POSTSUBSCRIPT Erase end_POSTSUBSCRIPT =𝔼yt,t⁢[‖ϵθu⁢(yt,ex,t)−ϵE‖2]absentsubscript𝔼subscript𝑦𝑡𝑡delimited-[]superscriptnormsubscriptitalic-ϵsubscript𝜃𝑢subscript𝑦𝑡subscript𝑒𝑥𝑡subscriptitalic-ϵ𝐸2\\displaystyle=\\mathbb{E}_{y_{t},t}\\left[\\|\\epsilon_{\\theta_{u}}(y_{t},e_{x},t)% -\\epsilon_{E}\\|^{2}\\right]= blackboard_E start_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t end_POSTSUBSCRIPT [ ∥ italic_ϵ start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT , italic_t ) - italic_ϵ start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT ∥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] where ⁢ϵEwhere subscriptitalic-ϵ𝐸\\displaystyle\\text{where }\\epsilon_{E}where italic_ϵ start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT =ϵθ⁢(yt,t)−η⁢[ϵθ⁢(yt,ex,t)−ϵθ⁢(yt,t)],absentsubscriptitalic-ϵ𝜃subscript𝑦𝑡𝑡𝜂delimited-[]subscriptitalic-ϵ𝜃subscript𝑦𝑡subscript𝑒𝑥𝑡subscriptitalic-ϵ𝜃subscript𝑦𝑡𝑡\\displaystyle=\\epsilon_{\\theta}(y_{t},t)-\\eta\\left[\\epsilon_{\\theta}(y_{t},e_{% x},t)-\\epsilon_{\\theta}(y_{t},t)\\right],= italic_ϵ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) - italic_η [ italic_ϵ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT , italic_t ) - italic_ϵ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) ] ,\n\nwhere θusubscript𝜃𝑢\\theta_{u}italic_θ start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT is the designated unlearned model with θosubscript𝜃𝑜\\theta_{o}italic_θ start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT plugged with eraser θesubscript𝜃𝑒\\theta_{e}italic_θ start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT. ytsubscript𝑦𝑡y_{t}italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is the denoised image at timestep t𝑡titalic_t sampled from θusubscript𝜃𝑢\\theta_{u}italic_θ start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT conditioned on target concept x𝑥xitalic_x, exsubscript𝑒𝑥e_{x}italic_e start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT is the text embedding of concept x𝑥xitalic_x, and ϵEsubscriptitalic-ϵ𝐸\\epsilon_{E}italic_ϵ start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT is the negatively guided noise predicted by the original model θosubscript𝜃𝑜\\theta_{o}italic_θ start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT. By minimizing the L2 distance between ϵθu⁢(yt,ex,t)subscriptitalic-ϵsubscript𝜃𝑢subscript𝑦𝑡subscript𝑒𝑥𝑡\\epsilon_{\\theta_{u}}(y_{t},e_{x},t)italic_ϵ start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT , italic_t ) and ϵEsubscriptitalic-ϵ𝐸\\epsilon_{E}italic_ϵ start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT, the eraser learns to reduce the probability of the generated image y𝑦yitalic_y belongs to the target concept x𝑥xitalic_x, thus effectively erasing the concept.\n\nKumar et al. (2022) proposes two extensions to the SISA framework (Bourtoule et al., 2021)— SISA-FC and SISA-A — to facilitate guaranteed unlearning that is efficient in terms of memory, time, and space for LMs. In particular, SISA-FC pre-trains a base model on a generic text corpus and then integrates fully connected layers atop it. During optimization, only the parameters of the linear layers are fine-tuned. This approach reduces the overall training time since backpropagation of gradients occurs solely in the final layers, and only the weights of the additional parameters are stored. Nevertheless, the addition of linear layers to SISA might compromise the model’s utility when compared to fine-tuning the entire model (Devlin et al., 2018). To address this, SISA-A incorporates adapters (Houlsby et al., 2019) into the encoder blocks of the transformer. This method results in only a marginal increase in the model’s memory footprint—about 1 to 5 % —thus providing a memory benefit of 95 to 99 %.\n\n4.1.6. Task Vector Methods\n\nInspired by recent work of weight interpolations (Frankle et al., 2020; Wortsman et al., 2022b; Matena and Raffel, 2022; Ilharco et al., 2022b; Ainsworth et al., 2022), Ilharco et al. (2022a) first proposes the concept of task vector, which can be obtained by taking the difference between the original model weights of a pre-trained model and its weights fine-tuned on a specific task. In particular, if we let θf⁢ttsubscriptsuperscript𝜃𝑡𝑓𝑡\\theta^{t}_{ft}italic_θ start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_f italic_t end_POSTSUBSCRIPT be the corresponding weights after fine-tuning on task t𝑡titalic_t, the task vector is then denoted as τt=θf⁢tt−θosubscript𝜏𝑡subscriptsuperscript𝜃𝑡𝑓𝑡subscript𝜃𝑜\\tau_{t}=\\theta^{t}_{ft}-\\theta_{o}italic_τ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_θ start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_f italic_t end_POSTSUBSCRIPT - italic_θ start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT. Then, taking the element-wise negation of the task vector τtsubscript𝜏𝑡\\tau_{t}italic_τ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT can enable θosubscript𝜃𝑜\\theta_{o}italic_θ start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT to forget target knowledge on task t𝑡titalic_t without jeopardizing irrelevant knowledge, resulting in an unlearned model that has weight of θu=θo−λ⁢τsubscript𝜃𝑢subscript𝜃𝑜𝜆𝜏\\theta_{u}=\\theta_{o}-\\lambda\\tauitalic_θ start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT = italic_θ start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT - italic_λ italic_τ with λ𝜆\\lambdaitalic_λ as a scaling term. One exemplar work is SKU (Liu et al., 2024a), which designs a novel unlearning framework to eliminate harmful knowledge while preserving utility on normal prompts. In particular, SKU is consisted of two stages where the first stage aims to identify and acquire harmful knowledge within the model, whereas the second stage targets to remove the knowledge using element-wise negation operation. Different from pure gradient ascent approaches where the model locality is largely compromised, SKU collectively aggregates the target unlearned knowledge 𝒟~fsubscript~𝒟𝑓\\tilde{\\mathcal{D}}_{f}over~ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT using gradient decent approach in the first stage and remove it from the pre-trained model. However, SSU (Dou et al., 2024) identifies the potential instability of the pure task vector approach in the case of multiple rounds of unlearning (i.e., sequential unlearning) and introduces a more stable unlearning framework integrated with weight saliency.\n\nBesides subtracting undesirable parameters, Pochinkov and Schoots (2024) proposes a selective pruning method to trim those neurons’ relative importance to different datasets, representing target model capability for unlearning. In particular, it performs either iterative pruning on nodes in the feed-forward layers or attention head layers. This selective approach utilizes importance functions that assess the contribution of individual neurons to specific tasks by measuring activation frequencies and magnitudes, enabling precise targeting of neurons that are crucial for the undesired capabilities. Furthermore, different from previous weight pruning method, where it requires Hessian computation, the selective neuron pruning is more computational efficient for large language models because it directly removes neurons that contribute the most to the unwanted behavior.\n\n4.1.7. Parameter Efficient Module Operation Methods\n\nInspired by works on merging model parameters under full fine-tuning (Wortsman et al., 2022a; Matena and Raffel, [n. d.]; Jin et al., 2022), Zhang et al. (2024b) explores composing parameter-efficient modules (PEM) like LoRA (Hu et al., 2021) and (IA)3superscript(IA)3\\text{(IA)}^{3}(IA) start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT (Liu et al., 2022c) for flexible module manipulation. Unlike task vector methods, which modify the global weight vectors, PEM operation methods apply localized adjustments within specific modules. Similar to task vector methods, the negation operator (⊖symmetric-difference\\ominus⊖) in PEM methods unlearns stored knowledge within adapter modules. For example, in a LoRA module denoted as θlora={𝐀,𝐁}subscript𝜃lora𝐀𝐁\\theta_{\\text{lora}}=\\{\\mathbf{A},\\mathbf{B}\\}italic_θ start_POSTSUBSCRIPT lora end_POSTSUBSCRIPT = { bold_A , bold_B }, 𝐀𝐀\\mathbf{A}bold_A is initialized following a random Gaussian distribution, and 𝐁𝐁\\mathbf{B}bold_B is initialized to all zeros to recover the pre-trained model at the beginning. We could negate 𝐁𝐁\\mathbf{B}bold_B or 𝐀𝐀\\mathbf{A}bold_A while keeping the other unchanged to facilitate unlearning or forgetting certain skills (e.g., toxic data). This process can be written as:\n\n⊖θloranegation=⊖θlora={𝐀,−𝐁}.symmetric-differencesuperscriptsubscript𝜃loranegationsymmetric-differencesubscript𝜃lora𝐀𝐁\\ominus\\theta_{\\text{lora}}^{\\text{negation}}=\\ominus\\theta_{\\text{lora}}=\\{% \\mathbf{A},-\\mathbf{B}\\}.⊖ italic_θ start_POSTSUBSCRIPT lora end_POSTSUBSCRIPT start_POSTSUPERSCRIPT negation end_POSTSUPERSCRIPT = ⊖ italic_θ start_POSTSUBSCRIPT lora end_POSTSUBSCRIPT = { bold_A , - bold_B } .\n\nThis negation operator changes the intermediate layer’s activation values in the opposite direction of gradient descent, aiding in unlearning targeted knowledge.\n\nTo further enhance model truthfulness and detoxification, Ext-Sub (Hu et al., 2024) introduces ”expert” and ”anti-expert” PEMs. The ”expert” PEM, trained on retaining data 𝒟rsubscript𝒟𝑟\\mathcal{D}_{r}caligraphic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT, represents desired behaviors, while the ”anti-expert” PEM, trained on unlearned data 𝒟~fsubscript~𝒟𝑓\\tilde{\\mathcal{D}}_{f}over~ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT, represents harmful behaviors. Ext-Sub identifies commonalities between these PEMs to determine shared capabilities and then subtracts the deficiency capability responsible for untruthful or toxic content. The Ext-Sub operation is defined as:\n\nθu=θe⁢x⁢p⁢e⁢r⁢t⊖λ⋅Ext⁢(θa⁢n⁢t⁢i−e⁢x⁢p⁢e⁢r⁢t),subscript𝜃𝑢symmetric-differencesubscript𝜃𝑒𝑥𝑝𝑒𝑟𝑡⋅𝜆Extsubscript𝜃𝑎𝑛𝑡𝑖𝑒𝑥𝑝𝑒𝑟𝑡\\theta_{u}=\\theta_{expert}\\ominus\\lambda\\cdot\\text{Ext}(\\theta_{anti-expert}),italic_θ start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT = italic_θ start_POSTSUBSCRIPT italic_e italic_x italic_p italic_e italic_r italic_t end_POSTSUBSCRIPT ⊖ italic_λ ⋅ Ext ( italic_θ start_POSTSUBSCRIPT italic_a italic_n italic_t italic_i - italic_e italic_x italic_p italic_e italic_r italic_t end_POSTSUBSCRIPT ) ,\n\nwhere θe⁢x⁢p⁢e⁢r⁢tsubscript𝜃𝑒𝑥𝑝𝑒𝑟𝑡\\theta_{expert}italic_θ start_POSTSUBSCRIPT italic_e italic_x italic_p italic_e italic_r italic_t end_POSTSUBSCRIPT and θa⁢n⁢t⁢i−e⁢x⁢p⁢e⁢r⁢tsubscript𝜃𝑎𝑛𝑡𝑖𝑒𝑥𝑝𝑒𝑟𝑡\\theta_{anti-expert}italic_θ start_POSTSUBSCRIPT italic_a italic_n italic_t italic_i - italic_e italic_x italic_p italic_e italic_r italic_t end_POSTSUBSCRIPT are the parameters of the expert and anti-expert PEMs, respectively, and Ext(⋅⋅\\cdot⋅) is the extraction function isolating the deficiency capability. This process removes harmful effects while preserving and enhancing the beneficial capabilities of the LLM.\n\nAdditionally, PEM can be utilized during model training to prevent the acquisition of harmful information. For instance, Zhou et al. (2023) introduces security vectors, enabling LLMs to be exposed to harmful behaviors without modifying the model’s original, safety-aligned parameters. Specifically, these vectors allow the LLM to process and respond to harmful inputs during training, ensuring that the core parameters remain unaltered. The security vector θssubscript𝜃𝑠\\theta_{s}italic_θ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT is optimized to minimize the loss associated with harmful data:\n\narg⁡minθo⁡𝔼(X,Y)∼Du⁢[minθs⁡L⁢(f⁢(X;θo;θs),Y)],subscriptsubscript𝜃𝑜subscript𝔼similar-to𝑋𝑌subscript𝐷𝑢delimited-[]subscriptsubscript𝜃𝑠𝐿𝑓𝑋subscript𝜃𝑜subscript𝜃𝑠𝑌\\arg\\min_{\\theta_{o}}\\mathbb{E}_{(X,Y)\\sim D_{u}}\\left[\\min_{\\theta_{s}}L(f(X;% \\theta_{o};\\theta_{s}),Y)\\right],roman_arg roman_min start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT ( italic_X , italic_Y ) ∼ italic_D start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ roman_min start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_L ( italic_f ( italic_X ; italic_θ start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT ; italic_θ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) , italic_Y ) ] ,\n\nwhere L𝐿Litalic_L is the causal loss computed based on the prediction and the ground truth. Subsequently, the trained security vector θs∗subscriptsuperscript𝜃∗𝑠\\theta^{\\ast}_{s}italic_θ start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT is employed during fine-tuning to ensure that the model learns from benign data without adopting harmful behaviors. θs∗subscriptsuperscript𝜃∗𝑠\\theta^{\\ast}_{s}italic_θ start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT is activated during the forward pass of training to maintain the model’s response consistency with benign data and to inhibit the learning of harmful information. Specifically:\n\nθu=arg⁡minθo⁡𝔼(X,Y)∼Dr⁢[L⁢(f⁢(X;θo;θs∗),Y)],subscript𝜃𝑢subscriptsubscript𝜃𝑜subscript𝔼similar-to𝑋𝑌subscript𝐷𝑟delimited-[]𝐿𝑓𝑋subscript𝜃𝑜superscriptsubscript𝜃𝑠𝑌\\theta_{u}=\\arg\\min_{\\theta_{o}}\\mathbb{E}_{(X,Y)\\sim D_{r}}\\left[L\\left(f(X;% \\theta_{o};\\theta_{s}^{*}),Y\\right)\\right],italic_θ start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT = roman_arg roman_min start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT ( italic_X , italic_Y ) ∼ italic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ italic_L ( italic_f ( italic_X ; italic_θ start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT ; italic_θ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ) , italic_Y ) ] ,\n\nensuring that the original model parameters do not update in a harmful direction.\n\n4.1.8. Summary:\n\nThe parameter optimization strategies focus on adjusting specific model parameters to selectively unlearn certain behaviors without affecting other functions. These approaches involve precise alterations to parameters associated with unwanted data influences or biases, ensuring the preservation of essential model performance. Gradient-based approaches with reversed loss are effective for unlearning accuracy and generalizability but can negatively impact model locality by inadvertently affecting unrelated parameters. In contrast, gradient-based methods without reversed loss can maximally preserve locality but may not excel in unlearning accuracy and generalizability. Extra learnable layers provide highly targeted unlearning but may demand significant computational resources. Data sharding methods excel in maintaining locality by partitioning the training data and ensuring specific data points can be unlearned without extensive retraining, although they might struggle with generalizability in very large models. Knowledge distillation is effective in maintaining locality by transferring knowledge to a new model trained to exclude specific data, thus retaining essential performance while unlearning undesired knowledge. However, it can be resource-intensive and may not achieve satisfactory accuracy and generalizability. Task vector and parameter-efficient module operations may perform well in terms of unlearning accuracy and generalizability. Nonetheless, recent work (Dou et al., 2024) has highlighted the risk of these approaches leading to instability due to significant model degradation, resulting in poor locality performance.\n\n4.2. In-Context Unlearning\n\n4.2.1. Overview\n\nUnlike parameter optimization approaches, which actively modifies parameters either locally or globally via different techniques, in-Context unlearning techniques retain the parameters in their original state and manipulate the model’s context or environment to facilitate unlearning. In particular, these strategies result in an unlearned model gθu∗subscriptsuperscript𝑔∗subscript𝜃𝑢g^{\\ast}_{\\theta_{u}}italic_g start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT end_POSTSUBSCRIPT where θu=θosubscript𝜃𝑢subscript𝜃𝑜\\theta_{u}=\\theta_{o}italic_θ start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT = italic_θ start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT, but with changes in how the model interacts with its input or inferences.\n\n4.2.2. In-Context Unlearning\n\nIn-context unlearning utilizes the approach of in-context learning to selectively erase targeted knowledge during inference, treating the model as a black box. This kind of method is resource-efficient but has inherent limitations due to the nature of in-context learning. Specifically, it modifies only the model’s immediate outputs without fundamentally eradicating the unwanted knowledge embedded within the model’s internal parameters. ICUL (Pawelczyk et al., 2023) first introduces the idea of in context unlearning, which alters input prompts during the inference phase to achieve targeted unlearning in the situation where model’s API is the only access to the model. The technical process involves several key steps:\n\n(1)\n\nLabel Flipping, where the label of the data point that needs to be forgotten is flipped to contradict the model’s learned associations, resulting in the template ”[F⁢o⁢r⁢g⁢e⁢t⁢I⁢n⁢p⁢u⁢t]0subscriptdelimited-[]𝐹𝑜𝑟𝑔𝑒𝑡𝐼𝑛𝑝𝑢𝑡0[Forget\\>Input]_{0}[ italic_F italic_o italic_r italic_g italic_e italic_t italic_I italic_n italic_p italic_u italic_t ] start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT [F⁢l⁢i⁢p⁢p⁢e⁢d⁢L⁢a⁢b⁢e⁢l]0subscriptdelimited-[]𝐹𝑙𝑖𝑝𝑝𝑒𝑑𝐿𝑎𝑏𝑒𝑙0[Flipped\\>Label]_{0}[ italic_F italic_l italic_i italic_p italic_p italic_e italic_d italic_L italic_a italic_b italic_e italic_l ] start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT”\n\n(2)\n\nContext Construction, where additional correctly labeled examples are sampled and appended to the flipped example, creating a mixed input sequence, resulting in the template ”[F⁢o⁢r⁢g⁢e⁢t⁢I⁢n⁢p⁢u⁢t]0⁢[F⁢l⁢i⁢p⁢p⁢e⁢d⁢L⁢a⁢b⁢e⁢l]0\\n;[I⁢n⁢p⁢u⁢t]1⁢[L⁢a⁢b⁢e⁢l]1\\n;…⁢[I⁢n⁢p⁢u⁢t]s⁢[L⁢a⁢b⁢e⁢l]s\\subscriptdelimited-[]𝐹𝑜𝑟𝑔𝑒𝑡𝐼𝑛𝑝𝑢𝑡0subscriptdelimited-[]𝐹𝑙𝑖𝑝𝑝𝑒𝑑𝐿𝑎𝑏𝑒𝑙0𝑛\\subscriptdelimited-[]𝐼𝑛𝑝𝑢𝑡1subscriptdelimited-[]𝐿𝑎𝑏𝑒𝑙1𝑛…subscriptdelimited-[]𝐼𝑛𝑝𝑢𝑡𝑠subscriptdelimited-[]𝐿𝑎𝑏𝑒𝑙𝑠[Forget\\>Input]_{0}\\ [Flipped\\>Label]_{0}\\ \\backslash n;[Input]_{1}\\;[Label]_{% 1}\\ \\backslash n;\\ldots\\;[Input]_{s}\\;[Label]_{s}[ italic_F italic_o italic_r italic_g italic_e italic_t italic_I italic_n italic_p italic_u italic_t ] start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT [ italic_F italic_l italic_i italic_p italic_p italic_e italic_d italic_L italic_a italic_b italic_e italic_l ] start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT \\ italic_n ; [ italic_I italic_n italic_p italic_u italic_t ] start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT [ italic_L italic_a italic_b italic_e italic_l ] start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \\ italic_n ; … [ italic_I italic_n italic_p italic_u italic_t ] start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT [ italic_L italic_a italic_b italic_e italic_l ] start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT\n\n(3)\n\nInference Adjustment, where this modified prompt is used to ’confuse’ the model about the original training point, mitigating its influence, resulting in the template ”[F⁢o⁢r⁢g⁢e⁢t⁢I⁢n⁢p⁢u⁢t]0subscriptdelimited-[]𝐹𝑜𝑟𝑔𝑒𝑡𝐼𝑛𝑝𝑢𝑡0[Forget\\>Input]_{0}[ italic_F italic_o italic_r italic_g italic_e italic_t italic_I italic_n italic_p italic_u italic_t ] start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT [F⁢l⁢i⁢p⁢p⁢e⁢d⁢L⁢a⁢b⁢e⁢l]0subscriptdelimited-[]𝐹𝑙𝑖𝑝𝑝𝑒𝑑𝐿𝑎𝑏𝑒𝑙0[Flipped\\>Label]_{0}[ italic_F italic_l italic_i italic_p italic_p italic_e italic_d italic_L italic_a italic_b italic_e italic_l ] start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT \\n;[Input]1;[Label]1\\n;…;[Input]s;[Label]s[QueryInput]s+1\\backslash n;[Input]_{1}\\>;[Label]_{1}\\>\\backslash n;\\ldots\\>;[Input]_{s}\\>;[% Label]_{s}\\>[Query\\>Input]_{s+1}\\ italic_n ; [ italic_I italic_n italic_p italic_u italic_t ] start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ; [ italic_L italic_a italic_b italic_e italic_l ] start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \\ italic_n ; … ; [ italic_I italic_n italic_p italic_u italic_t ] start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ; [ italic_L italic_a italic_b italic_e italic_l ] start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT [ italic_Q italic_u italic_e italic_r italic_y italic_I italic_n italic_p italic_u italic_t ] start_POSTSUBSCRIPT italic_s + 1 end_POSTSUBSCRIPT”.\n\nHowever, directly modifying input prompts does not always give a desirable output. Hence, Larimar (Das et al., 2024) integrates an external memory module that directly manipulates the LLM’s outputs. This approach allows for precise control over knowledge updates and selective forgetting through operations such as writing, reading, and generating, which are efficiently managed by a hierarchical memory system.\n\n4.2.3. Summary\n\nParameter frozen methods retain the model’s parameters in their original state while manipulating the model’s context or environment to facilitate unlearning. A notable advantage of this method is its resource efficiency, as it does not require retraining or modification of the model’s internal parameters, making it suitable for scenarios where direct access to the model’s internals is limited (e.g. black-box models). However, a significant drawback is that parameter frozen methods only modify the model’s immediate outputs without fundamentally eradicating the unwanted knowledge embedded within the model’s parameters. This can lead to incomplete unlearning, as the underlying knowledge remains intact.\n\n5. Datasets and Benchmarks\n\n5.1. Datasets\n\nIn this section, we summarize the datasets commonly used in the field of Generative AI, as outlined in Table 4, to benefit future MU research. Instead of merely categorizing the datasets by task (i.e., generation and classification), they are organized according to their intended unlearning objectives. We specifically focus on those datasets primarily utilized as target datasets during the unlearning process, excluding object removal datasets such as CIFAR10 and MNIST, as well as generic evaluation benchmark datasets like Hellaswag (Zellers et al., 2019) and Piqa (Bisk et al., 2020).\n\n5.1.1. Safety Alignment:\n\nThe Civil Comments dataset (Borkan et al., 2019) is comprised of public comments from various news websites, each labeled with a level of toxicity to represent the degree of harmfulness in the content. Studies such as (Ilharco et al., 2022a) and (Zhang et al., 2024b) have utilized subsets of these highly toxic samples to extract harmful knowledge from pre-trained models. Complementing this, the Anthropic red team dataset (Bai et al., 2022; Ganguli et al., 2022) includes human preference data and annotated"
    }
}
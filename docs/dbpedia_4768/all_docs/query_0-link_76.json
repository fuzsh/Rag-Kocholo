{
    "id": "dbpedia_4768_0",
    "rank": 76,
    "data": {
        "url": "http://www.chiltoncomputing.org.uk/acl/applications/cc/p007.htm",
        "read_more_link": "",
        "language": "en",
        "title": "Chilton::ACL::Compiler Compiler 1966",
        "top_image": "",
        "meta_img": "",
        "images": [
            "http://www.chiltoncomputing.org.uk/acl/pngs/hamburger.png",
            "http://www.chiltoncomputing.org.uk/acl/pngs/magnifying-glass.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            "Compiler Compiler"
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "Compiler Compiler 1966",
        "meta_lang": "en",
        "meta_favicon": "/favicon.ico",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Early Translator Writing Systems\n\nContents\n\n1. Introduction\n\n1.1 Early Autcodes\n\n1.2 Fortran\n\n1.3 Algol 58\n\n1.4 Some Terminology\n\n2. Early Translator Writing Systems\n\n2.1 Syntax machine\n\n2.2 PSYCO\n\n2.3 BMCC\n\n3. Second Generation Translator Writing Systems\n\n3.1 COGENT\n\n3.2 CGS\n\n3.3 TMG\n\n3.4 Meta II\n\n4. Third Generation Translator Writing Systems\n\n4.1 FSL\n\n4.2 TGS\n\n4.3 Tree-Meta\n\n5. The Demise of Translator Writing Systems\n\n6. References\n\n1. Introduction\n\n1.1 Early Autocodes\n\nPrior to 1950, most computer programs were written in assembly code; that is a low-level programming language where a one-to-one relationship exists between instructions in the language and the computer's machine code instructions. By 1950, people were considering whether it would be possible for the machine to generate the assembly code rather than humans.\n\nIn 1950, William Schmitt implemented Short Code for the Univac 1 In 1952, Alick Glennie produced an autocode programming language for the Machester Mark 1 computer (the compiler was 750 instructions). Laning and Zierler produced an autocode for the the Whirlwind at about the same time. (Knuth lists 20 autocodes that predate Fortran and states that Glennie's autocode was the first one completed.)\n\n1.2 Fortran\n\nWork on a more ambitious autocode for the IBM 704 started around 1952. The proposal for Fortran I went to IBM Management in 1953, was specified by 1954 and compiler produced by 1957. The major deficiency of Fortran I was the absence of user-written subroutines and functions. These were added in Fortran II in 1958.\n\nThe need for a system to automatically write compilers had not really appeared so far as many computers still ran programs in assembler language or a single autocode. Implementing a single language on many different computers or implementing many languages on a single computer were still not major requirements.\n\n1.3 Algol 58\n\nBy 1958, there was concern that programming languages would proliferate. There were already signs of this. In both the USA and Europe there was support towards defining a single programming language. A meeting was held in 1958 in Zurich which defined the language called IAL (International Algebraic Language) in the USA and Algol58 in Europe. This was followed quite closely by Algol60.\n\nIn the USA, languages such as JOVIAL, MAD and and NELIAC were defined as subsets of IAL. In Europe, there was enthusiasm for Algol 60 which had a number of unfortunate features:\n\nNo reserved words. How an implementation defined goto was up to it. Thus, allez, go to, goto and many other variants appeared.\n\nNo input/output statements. This ensured that every single Algol 60 implementation was different. In the UK, there were three quite different input/output systems on the Elliott, English Electric and Ferranti computers.\n\nFeatures that were either difficult or impossible to implement on a range of computers either due to size or speed..\n\nThe main result of the attempt at a single universal programming language was a proliferation of languages and dialects that meant it was impossible to implement the compilers needed by a specific customer without some automation of the production of compilers.\n\nThere was a need for a lexical analysis subsystem to resolve the problems associated with the variety of dialects and languages. It was also needed to cover up the deficiencies in the way some languages had been defined, notably Fortran.\n\nFor Fortran, syntax analysis is relatively straightforward once the lexical problems are resolved. Algol is significantly different. In the Algol 60 Report the language is defined in BNF, a method for defining legal programs in the language. It also had statements in English that also described the language. The block structure facilities in Algol 60 made it more difficult to do the syntax analysis.\n\nThe semantic description of Algol 60 is also quite difficult. Features were defined that were difficult to compiler. Ambiguities existed. Meanwhile the architectures of computers were changing with different storage systems, radically different instruction codes, and speed based on pipelining specific operations. In consequence, generating efficient code across a range of computers was also becoming difficult.\n\nThe need for some automation in the production of compilers was needed.\n\n1.4 Some Terminology\n\nIn describing translator writing systems, it is useful to define a set of terms that can be used irrespective of the system being described.\n\nBackus-Naur Form (BNF)\n\nA set of rewrite rules that define a programming language. A rule has the form:\n\n<address> ::= <name> , <street> , <zip>\n\nThe non-terminal address can be replaced by a non-terminal name followed by a comma followed by a non-terminal street followed by a comma followed by a non-terminal zip. Rewrite rules are applied starting from a single non-terminal, program say, until a string is obtained that contains no non-terminal symbols. This is a program in the language.\n\nBNF language\n\nA language defined using BNF\n\nparse tree\n\nGiven a program in the BNF languge, it is possible to describe all the productions applied to create the program as a tree starting from the non-terminal program. This is called a parse tree.\n\nsyntax-directed compilation\n\nAny system that can take the definition of a BNF language and generates a parse tree for an arbitrary program in that language.\n\n(Manchester Mark 1 Autocodes: Alick Glennie, Tony Brooker 1954) Syntax Machine A E Glennie, 1960 PSYCO Princeton Syntax Compiler E T Irons 1960 Brooker-Morris Compiler-Compiler 1960 SIG/PLAN Working Group 1 on Syntax Driven Compilers 1962 Howard Metcalf Fall 1962 Val Schorre Meta I : Jan 1963 Lee Schmidt March 1963 Val Schorre et al Meta II : Spring 1963 Schneider and Johnson Meta III : 1964 J F Rulifson SRI Meta III : 1964 Book LISP-META : 1965 Oppenheim and Haggerty Meta V : 1966 O'Neil Meta PI : 1968 J F Rulifson Tree-Meta : 1968 J A Feldman FSL:1962/3 L F Mondshein VITAL:1967 CMU Cabal:1967 W Wulf CMU PQCC:1980 Computer Associates TGS 1964 Computer Associates CGS 1962 J C Reynolds COGENT 1962 J C McClure TMG 1963 S C Johnson YACC 1970 Early Translator Writing Systems Early Translator Writing Systems\n\nEarly Translator Writing Systems\n\n2. Early Translator Writing Systems\n\nIn 1960, several systems appeared aimed at easing the task of producing a compiler. They went under a variety of names , such as translator writing system, compiler-compiler and meta-compiler. As such a system is capable of generating a compiler, it may be possible for it to generate itself. Although not a requirement, it adds a certain elegance to the system and makes enhancement of the system relatively straightforward. A base system can be defined and this can be used to define a more powerful system and so on. Three of the earliest systems were:\n\nAlick Glennie's Syntax Machine, 1960\n\nNed Irons PSYCO compiler, 1960\n\nBrooker and Morris's Compiler-Compiler, 1960\n\nThe three activities were reasonably independent and differed in their approach.\n\nThe following functions are required explicitly or implicitly in most compilers:\n\nLexical Analysis: to recognise the basic symbols in the language\n\nSyntax Analysis: a method for checking the correctness of a program in the language and providing a parse that illustrates the structure.\n\nSemantic Analysis: production of a pseudo code that defines the meaning of the program. Optimisations appropriate to the target machine and code generation for that target machine may also be required.\n\n2.1 Syntax Machine\n\nThe Syntax Machine defines a top-down parser for the language to be compiled having possible rules of the form:\n\n<A> ::= <B> <C> <D> ... <X> <A> ::= <B> | <C> | <D> |... | <X> <A> ::= <B> <C> <D> ... {<X>}\n\nThe first parses an A as a B followed by a C ..etc. The second parses an A as a B or a C or a D ... The third parses an A as a B followed by a C .. by one or more X. Having this iteration, saves on recursion.\n\nIt is up to the person writing the translator for a specific language to ensure that no backup is required. However, the rules described above are converted into a pseudo flow chart that allows variations on this basic parsing. The Syntax Machine defines a machine-independent instruction set which defines the semantics of the program. It is possible to optimise the pseudo code before transforming it into the actual order code of a specific computer. The whole system was simulated on an IBM 650 and used to generate compiled code for both the IBM704 and IBM709. Many of the ideas that appear in the Syntax Machine were used in the S1, S2 and S3 Fortran compilers developed at AWRE Aldermaston for the IBM 7030 (Stretch) and later ported to the Ferranti Atlas II. However, by then, an operator precedence parser was used for syntax analysis.\n\n2.2 PSYCO\n\nIrons started his work on a syntax-directed compiler in 1959 while at Hanover for the summer.\n\nPSYCO uses a bounded-context bottom-up parser with backup. That is at any stage in the recognition of a program, a finite set of previously recognised non-terminals and the next finite set of terminal systems will always define the action that the parser needs to take. PSYCO was mainly aimed at writing translators for Algol-like languages. The system started with a symbol table containing basic information and a parse of the Algol-like program collected declarative information about the variables used in the program. A second pass used this information to compile the executable part of the program.\n\nIrons also recognised the need for handling errors in programs presented to the compiler.\n\n2.3 Compiler-Compiler\n\nThe Brooker-Morris compiler-compiler (BMCC) has a diferent notation from either Glennie or Irons. The parser is a top-down recursive-descent parser with statements of the form:\n\nFORMAT [SS] = [V] = [SAE]\n\nIt indicates that a source statement (SS) in the languge consists of a variable (V) followed by '=' followed by a signed arithmetic expression (SAE).\n\nAssociated with each FORMAT statement is a ROUTINE that defines the semantics of the statement:\n\nROUTINE [SS] = [V] = [SAE] LET [SAE] = [Â±?][T][Â±T*?] Ac = [Â±?][T] ....\n\nThe appropriate ROUTINE statement is passed the parse tree of the statement recognised. It can inspect the parse tree and decide what code should be generated.\n\nThe Compiler-Compiler effectively adds the definition of the language being defined to that of the Compiler-Compiler itself. Before releasing the final compiler, it is usual to remove the Compiler-Compiler's own statements from that of the language being compiled.\n\nThe Compiler-Compiler was used to produce Fortan II, Fortran IV, Algol, Extended Mercury Autode, Atlas Autocode, SOL, ACL, CPL, Elliott Autocode Mk III compilers for the Ferranti Atlas computer.\n\n3. Early Translator Writing Systems\n\nThe early translator writing systems influenced a number of systems that followed soon after. We shall look at three of these and one other, TMG, that also comes later but appears to have not been influenced by the earlier systems.\n\n3.1 COGENT\n\nJohn Reynolds' COGENT system was developed at the Argonne national Laboratory around 1962. COGENT stand for COmpiler and GENeralised Translator. The objective was to unify the concept of syntax-directed compilation with the more general but primitive concept of recursive list-processing. It is similar to BMCC in that it has two main constructs productions and generator definitions which correspond somewhat to FORMAT and ROUTINE in BMCC.\n\nAn example of a set of productions for a simple language is:\n\n(LETTER) = A,B,C,D,E. (STRING) = (LETTER),(STRING)(LETTER). (VARIABLE) = (STRING). (FACTOR) = (VARIABLE). (FACTOR) = (()(POLYNOMIAL)()). (TERM) = (FACTOR). (TERM = (TERM)*(FACTOR). (POLYNOMIAL) = (TERM),+(TERM),-(TERM). (POLYNOMIAL) = (POLYNOMIAL)+(TERM) (POLYNOMIAL) = (POLYNOMIAL)-(TERM)\n\nAs in BMCC, several productions can be written as a single production:\n\n(TERM) = (FACTOR),(TERM)*(FACTOR)\n\nCOGENT uses the production statements to construct a parser, similar to the Irons parser, that takes the input and generates a parse tree. By giving each recognised production a unique internal number, the parse tree can be very compactly represented as a list structure. This list structure can be manipulated by the generator definitions and eventually output the compiled code. COGENT was implemented on the Control Data 3600. A novelty of the implementation was that backup spawned a set of processes that performed the various alternatives in parallel.\n\n3.2 CGS\n\nComputer Associates of Massachusetts was in the business of writing compilers. Compiler Generator System (CGS) was their first attempt at a general purpose translator writing system aimed at producing efficient object code with optimisation provided at several places in the system. The basic approach was a table-driven top down syntax analyser. It ran on an IBM 7090, a Burroughs D-825, and a CDC 1604.\n\nThe generated compiler had five phases:\n\nSyntactic Analyzer: converts input string into a tree-representation of its syntax\n\nGenerator: transforms the tree into a sequence of macro-instructions\n\nOptimizer: recognises and eliminates redundant computation (eg common subexpressions, invariant computation out of loops, etc)\n\nCode Selector: assembles code fragments\n\nAssembler: binds the code fragments in form required by the compiler environment.\n\nThe Syntactic Analyzer, Generator, and Code Selector are driven by tables defined for a specific language and machine code using a BNF-like definition of each. The Syntactic Analyzer is used to generate the relevant tables.\n\n3.3 TMG\n\nTMG (stands for TransMoGrifier) was developed for the PDP-7 by R. M. McClure at Texas Instruments in 1975 and appears not to have been influenced by the earlier systems. The system was ported to the IBM 7040, IBM 7090, and CDC 1604. It was used at Bell Labs and also by the Multics Project. At Bell Labs, Dennis Ritchie used TMG to produce the compiler for B that later became C. McIlroy and Morris used TMG to write the EPL compiler for Multics. As an aside, Stephen Johnson named his system YACC because they already had TMG at Bell Labs! TMG was used to implement a subset of PL/I by Cal Tech.\n\nTMG was aimed at constructing simple one-pass translators for some specialized language with almost no optimisation of code generated. It was defined in a language TMGL with a compiler for TMGL on the target machine. Later, the system was defined in itself.\n\nThe system is somewhat similar to the Glennie Syntax Machine in that the syntax specification of a program consists of sets of statements with a label, a syntactic specification, and a semantic action that takes place if the syntactic entity is recognised. The syntactic part consists of a set of actions with possibly two exit labels which give where to go next in the case of the syntactic entity being recognised or not. If the syntactic part is recognised, the semantic part is executed.\n\nIn the first 18 months TMG was used to define two different FORTRAN compilers, a logic simulation system, several data format converters, a geometric description language translator, and the TMGL compiler itself.\n\nThe system is not really aimed at handling structured languages like Algol 60.\n\n3.4 Meta II\n\nThe SIG/PLAN Working Group 1 on Syntax Directed Compilers met monthly in the conference room at the UCLA computing facility. John Backus attended one meeting to see how BNF was being used as a compiler writing language. Several separate attempts at defining a translator writing system came out of that working group and the ideas finally came together initially as a system called META but finally as META II.\n\nMETA II was a simple translator writing system written by V Schorre et al in 1962.\n\nIn META II, the language for which a translator is required must be specified as a set of syntax rules. These very much resemble the BNF notation except that they contain code generation commands as part of the syntax definition. There is limited memory in META II so there is a need to output information soon after it has been recognised.\n\nA restrictive top-down method of syntax analysis is used in the META II system. The restriction is that it does not allow back tracking or left recursion. The main goal is to match the complete program with the Syntax Rule whose name follows .META at the head of the program. The name of the Syntax Rule appears at the start of the rule followed by the = symbol. The right-hand side of this Syntax Rule indicates which entities have to be recognised to achieve this goal. The recogniser achieves its main goal by looking for these smaller entities, from left to right, as individual subgoals. These subgoals are themselves defined in terms of other entities, and so the process of recognising a complete program becomes a matter of looking for progressively smaller entities, right down to the level at which basic symbols such as characters and numbers are recognised.\n\nA typical Meta II rule would be:\n\nSET = 'C' $ ( 'D' / 'E' )\n\nThis recognises any string consisting of the letter C followed by any sequence involving the letters D and E.\n\nMeta II rules can include .OUT statements that generate output code. Recognised basic symbols (.ID, .STRING, .NUMBER) are stored and can be output by .OUT(*). Also two unique labels can be output for each statement by writing *1 or *2.\n\nMeta II is a very simple system that can be defined in itself. This is the complete definition:\n\n.SYNTAX PROGRAM OUT1 = '*1' .OUT('GN1') / '*2' .OUT('GN2') / '*' .OUT('CI')/ .STRING .OUT('CL ' * ) ., OUTPUT = ('.OUT' '(' $OUT1')' / '.LABEL' .OUT('LB') OUT1 ) .OUT('OUT') ., EX3 = .ID .OUT('CLL' * ) / .STRING .OUT('TST' *) / '.ID' .OUT('ID') / '.NUMBER' .OUT('NUM') / '.STRING' .OUT('SR') / '(' EX1 ')' / '.EMPTY' .OUT('SET') / '$' .LABEL *1' EX3 .OUT('BT ' *1 ) .OUT( 'SET') ., EX2 = (EX3 .OUT('BF ' *1 ) / OUTPUT ) $(EX3 .OUT('BE') / OUTPUT) .LABEL *1 ., EX1 = EX2 $( '/' .OUT('BT ' *1) EX2) .LABEL *1 ., ST = .ID .LABEL * '=' EX1 '.,' .OUT('R') ., PROGRAM = '.SYNTAX' .ID .OUT('ADR' * ) $ ST '.END' .OUT('END') ., .END\n\n4. Third Generation Translator Writing Systems\n\nAfter the initial promise of early translator writing systems, there was considerable interest in producing better systems. The early systems had shown that the approach was viable. The understanding of the whole area of parsing and what was possible had improved. A theory of syntax analysis was evolving. The number of new languages arriving was still on the increase. Computer systems were evolving.\n\n4.1 FSL\n\nIn the period 1963-67, Bob Floyd from Computer Associates had moved to Carnegie-Mellon University. Jay Earley, a doctoral student, was working on a parsing algorithm, the most efficient general context-free algorithm known. Alan Perlis et al were defining an enhanced version of Algol, Formula Algol, which would incorporate the manipulation of mathematical formulae within the language Algol. There was a need for a Formula Algol compiler. Jerome Feldman had completed his thesis, A Formal Semantics for Computer Oriented Languages, in 1964. This defined a compiler-compiler, FSL, that was capable of translating into machine language most of the existing programming languages. It was, therefore, used to produce the Formaula Algol Compiler for the Bendix G-21. FSL had two main subsections. A Syntax Loader that builds tables which will control the recognition and parsing of programs in the required language. A Semantic Loader builds a table describing the semantics of the instructions in the required language. The Syntax Loader generated a Production Language (PL), based on the work of Bob Floyd, that scanned the current input stream against the PL rules and decided on the action to take. The Semantic Loader defined the meaning of each language statement in a machine-independent code.\n\nFSL was a major input t0 the CABAL Compiler-Compiler system produced at Carnegie-Mellon University in the period 1967 onwards.\n\n4.2 TGS\n\nTGS (sometimes called TRANGEN), Translator Generator System, was produced at Computer Associates as a follow on to CGS. It ran on an IBM 7094, CDC-1604, Univac M460 and GE-635 and was used to write translators for PL/I, ALGOL, and FORTRAN IV. Each compiler is driven by a set of tables, TRANTAB, described in a language called TRANDIR. Synatx Analysis was done by a system based on Floyd Productions. The code generation system was also used in later versions of the META systems.\n\n4.3 Tree-Meta\n\nAfter a number of intermediate systems, Rulifson defined Tree-Meta as a major advance over the earlier META systems. The TREE-META program is in two parts. The first part is a definition of the syntax for the source language, L, for which a translator is required and this is largely inherited from Meta II. The major difference is that instead of outputting code, the output is a tree built up as the syntax proceeds. The second part defines a set of code generation rules that generate the object code to be produced from this tree. The tree itself is quite flexible in format and is defined by the TREE-META user as he defines the syntax analyser for the language. The code generator can take note of any peculiarities in the form of the tree, if that will lead to better code being produced.\n\nAn example of the syntax recognise is:\n\n.META EXPRESSION EXPRESSION = TERM $ ( '+' TERM :ADD[2] / '-' TERM :SUB[2] ) ; TERM = FACTOR $ ( '*' FACTOR :MULT[2] / '/' FACTOR :DIV[2] ); FACTOR = '+' PRIMARY / '-' PRIMARY :MIN[1] / PRIMARY ; PRIMARY = .ID / .NUM / '(' EXPRESSION ')' ; .END\n\nThe construct ADD[2] generates an ADD tree node from the top two items on the stack. The code generator instruction would be of the form:\n\nADD[-,-] => *1 ' PLUS ' *2 ;\n\nSeparating code generation from the building of the parse tree means that quite efficient code can be generated.\n\n5. The Demise of Translator Writing Systems\n\nInterest in Translator Writing Systems continued after the mid-1960s but the interest in producing a universal system capable of production-quality compilers decreased. This occurred for a number of reasons:\n\nThe number of new languages decreased and the requirements for new languages that did appear tended to be significantly different from the previous generation.\n\nPhases of the compilation process were specified in independent modules like LEX and YACC so that a compiler writer could select a relevant set of modules to put together to create a specific compiler.\n\nThe speed of computers increased so that the need for the fastest compilation possible and the most efficient object code decreased.\n\n6. References"
    }
}
{
    "id": "dbpedia_8277_2",
    "rank": 60,
    "data": {
        "url": "https://docs.modular.com/mojo/faq",
        "read_more_link": "",
        "language": "en",
        "title": "Modular Docs",
        "top_image": "https://docs.modular.com/images/social-card.png",
        "meta_img": "https://docs.modular.com/images/social-card.png",
        "images": [
            "https://docs.modular.com/images/fire.svg",
            "https://docs.modular.com/images/home.svg",
            "https://docs.modular.com/images/m.svg",
            "https://docs.modular.com/images/fire.svg",
            "https://docs.modular.com/images/keyboard.svg",
            "https://docs.modular.com/images/modular-logo-white.svg",
            "https://docs.modular.com/images/modular-logo-white.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "Answers to questions we expect about Mojo.",
        "meta_lang": "en",
        "meta_favicon": "/images/favicon.ico",
        "meta_site_name": "",
        "canonical_link": "https://docs.modular.com/mojo/faq",
        "text": "We built Mojo to solve an internal challenge at Modular, and we are using it extensively in our systems such as our AI Engine. As a result, we are extremely committed to its long term success and are investing heavily in it. Our overall mission is to unify AI software and we can‚Äôt do that without a unified language that can scale across the AI infrastructure stack. That said, we don‚Äôt plan to stop at AI‚Äîthe north star is for Mojo to support the whole gamut of general-purpose programming over time. For a longer answer, read Why Mojo.\n\nMojo means ‚Äúa magical charm‚Äù or ‚Äúmagical powers.‚Äù We thought this was a fitting name for a language that brings magical powers to Python, including unlocking an innovative programming model for accelerators and other heterogeneous systems pervasive in AI today.\n\nWe paired Mojo with fire emoji üî• as a fun visual way to impart onto users that Mojo empowers them to get their Mojo on‚Äîto develop faster and more efficiently than ever before. We also believe that the world can handle a unicode extension at this point, but you can also just use the .mojo extension. :)\n\nMojo combines the usability of Python with the systems programming features it‚Äôs missing. We are guided more by pragmatism than novelty, but Mojo‚Äôs use of MLIR allows it to scale to new exotic hardware types and domains in a way that other languages haven‚Äôt demonstrated (for an example of Mojo talking directly to MLIR, see our low-level IR in Mojo notebook). It also has caching and distributed compilation built into its core. We also believe Mojo has a good chance of unifying hybrid packages in the broader Python community.\n\nMojo‚Äôs initial focus is to bring programmability back to AI, enabling AI developers to customize and get the most out of their hardware. As such, Mojo will primarily benefit researchers and other engineers looking to write high-performance AI operations. Over time, Mojo will become much more interesting to the general Python community as it grows to be a superset of Python. We hope this will help lift the vast Python library ecosystem and empower more traditional systems developers that use C, C++, Rust, etc.\n\nEffectively, all AI research and model development happens in Python today, and there‚Äôs a good reason for this! Python is a powerful high-level language with clean, simple syntax and a massive ecosystem of libraries. It‚Äôs also one of the world's most popular programming languages, and we want to help it become even better. At Modular, one of our core principles is meeting customers where they are‚Äîour goal is not to further fragment the AI landscape but to unify and simplify AI development workflows.\n\nWe‚Äôre thrilled to see a big push to improve CPython by the existing community, but our goals for Mojo (such as to deploy onto GPUs and other accelerators) need a fundamentally different architecture and compiler approach underlying it. CPython is a significant part of our compatibility approach and powers our Python interoperability.\n\nCodon and PyPy aim to improve performance compared to CPython, but Mojo‚Äôs goals are much deeper than this. Our objective isn‚Äôt just to create ‚Äúa faster Python,‚Äù but to enable a whole new layer of systems programming that includes direct access to accelerated hardware, as outlined in Why Mojo. Our technical implementation approach is also very different, for example, we are not relying on heroic compiler and JIT technologies to ‚Äúdevirtualize‚Äù Python.\n\nFurthermore, solving big challenges for the computing industry is hard and requires a fundamental rethinking of the compiler and runtime infrastructure. This drove us to build an entirely new approach and we‚Äôre willing to put in the time required to do it properly (see our blog post about building a next-generation AI platform), rather than tweaking an existing system that would only solve a small part of the problem.\n\nWe think Julia is a great language and it has a wonderful community, but Mojo is completely different. While Julia and Mojo might share some goals and look similar as an easy-to-use and high-performance alternative to Python, we‚Äôre taking a completely different approach to building Mojo. Notably, Mojo is Python-first and doesn't require existing Python developers to learn a new syntax.\n\nMojo also has a bunch of technical advancements compared to Julia, simply because Mojo is newer and we‚Äôve been able to learn from Julia (and from Swift, Rust, C++ and many others that came before us). For example, Mojo takes a different approach to memory ownership and memory management, it scales down to smaller envelopes, and is designed with AI and MLIR-first principles (though Mojo is not only for AI).\n\nThat said, we also believe there‚Äôs plenty of room for many languages and this isn‚Äôt an OR proposition. If you use and love Julia, that's great! We‚Äôd love for you to try Mojo and if you find it useful, then that's great too.\n\nThe best place to start is the Mojo Manual. And if you want to see what features are coming in the future, take a look at the roadmap.\n\nMLIR provides a flexible infrastructure for building compilers. It‚Äôs based upon layers of intermediate representations (IRs) that allow for progressive lowering of any code for any hardware, and it has been widely adopted by the hardware accelerator industry since its first release. Although you can use MLIR to create a flexible and powerful compiler for any programming language, Mojo is the world‚Äôs first language to be built from the ground up with MLIR design principles. This means that Mojo not only offers high-performance compilation for heterogeneous hardware, but it also provides direct programming support for the MLIR intermediate representations. For a simple example of Mojo talking directly to MLIR, see our low-level IR in Mojo notebook.\n\nMojo is a general purpose programming language. We use Mojo at Modular to develop AI algorithms, but as we grow Mojo into a superset of Python, you can use it for other things like HPC, data transformations, writing pre/post processing operations, and much more. For examples of how Mojo can be used for other general programming tasks, see our Mojo examples.\n\nMojo supports both just-in-time (JIT) and ahead-of-time (AOT) compilation. In either a REPL environment or Jupyter notebook, Mojo is JIT‚Äôd. However, for AI deployment, it‚Äôs important that Mojo also supports AOT compilation instead of having to JIT compile everything. You can compile your Mojo programs using the mojo CLI.\n\nTriton Lang is a specialized programming model for one type of accelerator, whereas Mojo is a more general language that will support more architectures over time and includes a debugger, a full tool suite, etc. For more about embedded domain-specific languages (EDSLs) like Triton, read the ‚ÄúEmbedded DSLs in Python‚Äù section of Why Mojo.\n\nMojo is a general purpose programming language, so it has no specific implementations for ML training or serving, although we use Mojo as part of the overall Modular AI stack. The Modular AI Engine, for example, supports deployment of PyTorch and TensorFlow models, while Mojo is the language we use to write the engine‚Äôs in-house kernels.\n\nNot alone. You will need to leverage the Modular AI Engine for that. Mojo is one component of the Modular stack that makes it easier for you to author highly performant, portable kernels, but you‚Äôll also need a runtime (or ‚ÄúOS‚Äù) that supports graph level transformations and heterogeneous compute.\n\nWe haven‚Äôt prioritized this functionality yet, but there‚Äôs no reason Mojo can‚Äôt support it.\n\nMojo is still early and not yet a Python superset, so only simple programs can be brought over as-is with no code changes. We will continue investing in this and build migration tools as the language matures.\n\nYes, we want to enable developers to port code from languages other than Python to Mojo as well. We expect that due to Mojo‚Äôs similarity to the C/C++ type systems, migrating code from C/C++ should work well and it‚Äôs in our roadmap.\n\nMojo leverages LLVM-level dialects for the hardware targets it supports, and it uses other MLIR-based code-generation backends where applicable. This also means that Mojo is easily extensible to any hardware backend. For more information, read about our vision for pluggable hardware.\n\nMojo provides all the language functionality necessary for anyone to extend hardware support. As such, we expect hardware vendors and community members will contribute additional hardware support in the future. We‚Äôll share more details about opening access to Mojo in the future, but in the meantime, you can read more about our hardware extensibility vision.\n\nModern CPUs are surprisingly complex and diverse, but Mojo enables systems-level optimizations and flexibility that unlock the features of any device in a way that Python cannot. So the hardware matters for this sort of benchmark, and for the Mandelbrot benchmarks we show in our launch keynote, we ran them on an AWS r7iz.metal-16xl machine.\n\nFor lots more information, check out our 3-part blog post series about how Mojo gets a 35,000x speedup over Python.\n\nBy the way, all the kernels that power the Modular AI Engine are written in Mojo. We also compared our matrix multiplication implementation to other state-of-the-art implementations that are usually written in assembly. To see the results, see our blog post about unified matrix multiplication."
    }
}
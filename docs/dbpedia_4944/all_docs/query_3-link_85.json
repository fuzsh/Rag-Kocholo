{
    "id": "dbpedia_4944_3",
    "rank": 85,
    "data": {
        "url": "https://www.analyticsvidhya.com/blog/2020/06/nlp-project-information-extraction/",
        "read_more_link": "",
        "language": "en",
        "title": "Hands-on NLP Project: A Comprehensive Guide to Information Extraction using Python",
        "top_image": "https://cdn.analyticsvidhya.com/wp-content/uploads/2024/04/Best-Python-Tricks-in-Jupiter-Notebook-80.jpg",
        "meta_img": "https://cdn.analyticsvidhya.com/wp-content/uploads/2024/04/Best-Python-Tricks-in-Jupiter-Notebook-80.jpg",
        "images": [
            "https://av-public-assets.s3.ap-south-1.amazonaws.com/logos/av-logo-svg.svg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/navbar.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/navbar.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/navbar.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/navbar.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/navbar.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/default_avatar.svg",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2024/05/image-403.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2020/06/Extraction-using-Python.jpg",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2020/06/IE_example2.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2020/06/2-2.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2020/06/3.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2020/06/4.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2020/06/5.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2020/06/6.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2020/06/7.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2020/06/8.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2020/06/SpaCy-pattern-matcher.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2020/06/9.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2020/06/31.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2020/06/10.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2020/06/11.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2020/06/12.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2020/06/13.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2024/05/image-402.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2020/06/32.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2020/06/29.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2020/06/16.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2024/05/image-401.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2024/05/image-400.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2020/06/33.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2020/06/20.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2020/06/21.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2024/05/image-399.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2020/06/34.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2024/05/image-398.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2024/05/image-397.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2020/06/26.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2024/05/image-396.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2020/06/new_2.png",
            "https://cdn.analyticsvidhya.com/wp-content/uploads/2024/05/image-395.png",
            "https://secure.gravatar.com/avatar/4bcf9b2ff0f812a596dc41e7e7777528?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/a5aaf736079f2b4578f6510d2220c4bf?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/64429a02d28024f25b654a151554f4bd?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/0037c26581390db47660d582a9785fb3?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/7da45d7e6e01171012ab86cbc13a48dc?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/cbc5625f68609792226c42eab3997b59?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/19e812cbf30dd299dabd4a79198a20a5?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/808b3e10930a806fa9b550795c76fd8c?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/7da45d7e6e01171012ab86cbc13a48dc?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/43f49b1a5dc8f7f63db4505cf2137086?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/6c2d025ab9b93e61cbf2bb5bbcff711e?s=74&d=mm&r=g",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/path-digital.png",
            "https://av-identity.s3.amazonaws.com/users/user/bGnsep7nT0GMWuLpkDl15Q.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/in.png",
            "https://av-identity.s3.amazonaws.com/users/user/R7HrsWl1QrGRiw_e9m4fDA.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/in.png",
            "https://av-identity.s3.amazonaws.com/users/user/ZcU4ALTFT96MVCzfiGuhsQ.jpeg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/in.png",
            "https://av-identity.s3.amazonaws.com/users/user/aM3WrxdNSTGLg7LoqX-q0w.png",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/in.png",
            "https://av-identity.s3.amazonaws.com/users/user/zy4FL_yyQlG4PkWcyGYvhw.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/in.png",
            "https://av-identity.s3.amazonaws.com/users/user/a4ByfUyoQRmdGzLpBzHVLw.jpeg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/in.png",
            "https://av-identity.s3.amazonaws.com/users/user/ZTsmKl-1Qvqn07FUzgaBNw.png",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/in.png",
            "https://d2cd20fxv8fgim.cloudfront.net/homepage/images/Play_Store.svg",
            "https://d2cd20fxv8fgim.cloudfront.net/homepage/images/App_Store.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Aniruddha Bhandari"
        ],
        "publish_date": "2020-06-28T19:37:57+00:00",
        "summary": "",
        "meta_description": "Learn about Information Extraction, its process, and tools like SpaCy. Extract insights from text data efficiently with code examples.",
        "meta_lang": "en",
        "meta_favicon": "https://imgcdn.analyticsvidhya.com/favicon/av-fav.ico",
        "meta_site_name": "Analytics Vidhya",
        "canonical_link": "https://www.analyticsvidhya.com/blog/2020/06/nlp-project-information-extraction/",
        "text": "Introduction\n\nAs a bibliophile who enjoys extracting knowledge from books and articles, I’ve observed how modern reading habits have shifted towards skimming for relevant information amidst information overload. Recently, while reading an article on India’s upcoming tour of Australia, I found myself quickly skimming through the text, focusing mainly on headlines and details about Virat Kohli. This experience prompted me to explore the possibilities in Natural Language Processing (NLP), particularly in relation extraction and summarization.\n\nI realized the potential of building an information extraction model using machine learning techniques such as BERT and Bayes for automating data extraction tasks from diverse sources, including medical records. This project not only enhanced my NLP skills but also deepened my understanding of data science and the power of machine learning models in automating complex tasks.\n\nLearning Outcomes\n\nEvaluate the performance of natural language processing (NLP) models using appropriate metrics such as precision, recall, F1-score, and accuracy.\n\nDesign and develop advanced NLP pipelines using techniques such as named entity recognition (NER), sentiment analysis, and text summarization.\n\nEvaluate the performance of NLP models across various tasks and datasets using standard evaluation metrics.\n\nDevelop custom NLP solutions using NLTK’s extensive collection of corpora, lexical resources, and algorithms.\n\nApply regular expressions for pattern matching and text search in large datasets and documents.\n\nDesign and implement information retrieval systems using techniques such as vector space models, term frequency-inverse document frequency (TF-IDF), and document similarity metrics.\n\nDevelop knowledge base-driven applications for tasks such as question answering, entity recognition, and recommendation systems.\n\nWhat is Information Extraction?\n\nText data contains a plethora of information, yet not all of it may be relevant to your needs. Some may seek to extract specific relationships between entities, utilizing information extraction NLP techniques. Our intentions vary based on individual requirements and objectives.\n\nImagine having to go through all the legal documents to find legal precedence to validate your current case. Or having to go through all the research papers to find relevant information to cure a disease. There are many more examples like resume harvesting, media analysis, email scanning, etc.\n\nBut just imagine having to manually go through all of the textual data and extracting the most relevant information. Clearly, it is an uphill battle and you might even end up skipping some important information.\n\nFor anyone trying to analyze textual data, the difficult task is not of finding the right documents, but of finding the right information from these documents. Understanding the relationship between entities, understanding how the events have unfolded, or just simply finding hidden gems of information, is clearly what anyone is looking for when they go through a piece of text.\n\nTherefore, coming up with an automated way of extracting the information from textual data and presenting it in a structured manner will help us reap a lot of benefits and tremendously reduce the amount of time we have to spend time skimming through text documents. This is precisely what information extraction strives to achieve.\n\nUsing information extraction nlp, we can retrieve pre-defined information such as the name of a person, location of an organization, or identify a relation between entities, and save this information in a structured format such as a database.\n\nLet me show you another example I’ve taken from a cricket news article:\n\nWe can extract the following information from the text:\n\nCountry – India, Captain – Virat Kohli\n\nBatsman – Virat Kohli, Runs – 2\n\nBowler – Kyle Jamieson\n\nMatch venue – Wellington\n\nMatch series – New Zealand\n\nSeries highlight – single fifty, 8 innings, 3 formats\n\nThis enables us to reap the benefits of powerful query tools like SQL for further analysis. Creating such structured data using information extraction will not only help us in analyzing the documents better but also help us in understanding the hidden relationships in the text.\n\nHow Does Information Extraction Work?\n\nGiven the capricious nature of text data that changes depending on the author or the context, Information Extraction seems like a daunting task. But it doesn’t have to be that way!\n\nWe all know that sentences are made up of words belonging to different Parts of Speech (POS). There are eight different POS in the English language: noun, pronoun, verb, adjective, adverb, preposition, conjunction, and intersection.\n\nThe POS determines how a specific word functions in meaning in a given sentence. For example, take the word “right”. In the sentence, “The boy was awarded chocolate for giving the right answer”, “right” is used as an adjective. Whereas, in the sentence, “You have the right to say whatever you want”, “right” is treated as a noun.\n\nThis goes to show that the POS tag of a word carries a lot of significance when it comes to understanding the meaning of a sentence. And we can leverage it to extract meaningful information from our text.\n\nLet’s take an example to understand this. We’ll be using the popular spaCy library here.\n\nPython Code:\n\nWe were easily able to determine the POS tags of all the words in the sentence. But how does it help in Information Extraction?\n\nWell, if we wanted to extract nouns from the sentences, we could take a look at POS tags of the words/tokens in the sentence, using the attribute .pos_, and extract them accordingly.\n\nIt was that easy to extract words based on their POS tags. But sometimes extracting information purely based on the POS tags is not enough. Have a look at the sentence below:\n\nIf I wanted to extract the subject and the object from a sentence, I can’t do that based on their POS tags. For that, I need to look at how these words are related to each other. These are called Dependencies.\n\nWe can make use of spaCy’s displacy visualizer that displays the word dependencies in a graphical manner:\n\nPretty cool! This directed graph is known as a dependency graph. It represents the relations between different words of a sentence.\n\nEach word is a node in the Dependency graph. The relationship between words is denoted by the edges. For example, “The” is a determiner here, “children” is the subject of the sentence, “biscuits” is the object of the sentence, and “cream” is a compound word that gives us more information about the object.\n\nThe arrows carry a lot of significance here:\n\nThe arrowhead points to the words that are dependent on the word pointed by the origin of the arrow\n\nThe former is referred to as the child node of the latter. For example, “children” is the child node of “love”\n\nThe word which has no incoming arrow is called the root node of the sentence\n\nLet’s see how we can extract the subject and the object from the sentence. Like we have an attribute for POS in SpaCy tokens, we similarly have an attribute for extracting the dependency of a token denoted by dep_:\n\nVoila! We have the subject and object of our sentence.\n\nUsing POS tags and Dependency tags, we can look for relationships between different entities in a sentence. For example, in the sentence “The cat perches on the window sill”, we have the subject, “cat”, the object “window sill”, related by the preposition “on”. We can look for such relationships and much more to extract meaningful information from our text data.\n\nI suggest going through this amazing tutorial which explains Information Extraction in detail with tons of examples.\n\nWhere Do We Go from Here?\n\nWe have briefly spoken about the theory regarding Information Extraction which I believe is important to understand before jumping into the crux of this article.\n\n“An ounce of practice is generally worth more than a ton of theory.” –E.F. Schumacher\n\nIn the following sections, I am going to explore a text dataset and apply the information extraction technique to retrieve some important information, understand the structure of the sentences, and the relationship between entities.\n\nSo, without further ado, let’s get cracking on the code!\n\nGetting Familiar with the Text Dataset\n\nThe dataset we are going to be working with is the United Nations General Debate Corpus. It contains speeches made by representatives of all the member countries from the year 1970 to 2018 at the General Debate of the annual session of the United Nations General Assembly.\n\nBut we will take a subset of this dataset and work with speeches made by India at these debates. This will allow us to stay on track and better understand the task at hand of understanding Information Extraction. This leaves us with 49 speeches made by India over the years, each speech ranging from anywhere between 2000 to 6000+ words.\n\nHaving said that, let’s have a look at our dataset:\n\nI will print a snapshot of one of the speeches to give you a feel of what the data looks like:\n\nNow let’s start working with our dataset!\n\nSpeech Text Pre-Processing\n\nFirst, we need to clean our text data. When I went over a few speeches, I found each paragraph in the speech was numbered to distinctly identify it. There were obviously unwanted characters like newline character, a hyphen, salutations, and apostrophes, like in any other text dataset.\n\nBut another unique and unwanted information present were the references made in each speech to other documents. We obviously don’t want that either.\n\nI have written a simple function to clean the speeches. An important point here is that I haven’t used lemmatization or changed the words to lowercase as it has the potential to change the POS tag of the word. We certainly don’t want to do that as you will see in the upcoming subsections.\n\nRight, now that we have our minimally cleaned speeches, we can split it up into separate sentences.\n\nSplit the Speech into Different Sentences\n\nSplitting our speeches into separate sentences will allow us to extract information from each sentence. Later, we can combine it to get cumulative information for any specific year.\n\nFinally, we can create a dataframe containing the sentences from different years:\n\nAfter performing this operation, we end up with 7150 sentences. Going over them and extracting information manually will be a difficult task. That’s why we are looking at Information Extraction using NLP techniques!\n\nInformation Extraction using SpaCy\n\nNow, we can start working on the task of Information Extraction. We will be using the spaCy library for working with the text data. It has all the necessary tools that we can exploit for all the tasks we need for information extraction.\n\nLet me import the relevant SpaCy modules that we will require for the task ahead:\n\nWe will need the spaCy Matcher class to create a pattern to match phrases in the text. We’ll also require the displaCy module for visualizing the dependency graph of sentences.\n\nThe visualise_spacy_tree library will be needed for creating a tree-like structure out of the Dependency graph. This helps in visualizing the graph in a better way. Finally, IPython Image and display classes are required to output the tree.\n\nBut you don’t need to worry about these too much. It will become clear as you look at the code.\n\nInformation Extraction #1 – Finding Mentions of Prime Minister in the Speech\n\nWhen working on information extraction tasks, it is important to manually go over a subset of the dataset to understand what the text is like and determine if anything catches your attention at first glance. When I first went over the speeches, I found many of them referred to what the Prime Minister had said, thought, or achieved in the past.\n\nWe know that a country is nothing without its leader. The destination a country ends up in is by and large the result of the able guidance of its leader. Therefore, I believe it is important to extract those sentences from the speeches that referred to Prime Ministers of India, and try and understand what their thinking and perspective were, and also try to unravel any common or differing beliefs over the years.\n\nTo achieve this task, I used SpaCy’s Matcher class. It allows us to match a sequence of words based on certain patterns. For the current task, we know that whenever a Prime Minister is referred to in the speech, it will be in one of the following ways:\n\nPrime Minister of [Country] …\n\nPrime Minister [Name] …\n\nUsing this general understanding, we can come up with a pattern:\n\nLet me walk you through this pattern:\n\nHere, each dictionary in the list matches a unique word\n\nThe first and second dictionaries match the keyword “Prime Minister” irrespective of whether it is in uppercase or not, which is why I have included the key “LOWER”\n\nThe third dictionary matches a word that is a preposition. What I am looking for here is the word “of”. Now, as discussed before, it may or may not be present in the pattern, therefore, an additional key, “OP” or optional, is mentioned to point out just that\n\nFinally, the last dictionary in the pattern should be a proper noun. This can either be the name of the country or the name of the prime minister\n\nThe matched keywords have to be in continuation otherwise the pattern will not match the phrase\n\nHere are some sample sentences from the year 1989 that matched our pattern:\n\nNow, since only 58 sentences out of 7150 total sentences gave an output that matched our pattern, I have summarised the relevant information from these outputs here:\n\nPM Indira Gandhi and PM Jawaharlal Nehru believed in working together in unity and with the principles of the UN\n\nPM Indira Gandhi believed in striking a balance between global production and consumption. She set out policies dedicated to national reconstruction and the consolidation of a secular and pluralistic political system\n\nPM Indira Gandhi emphasized that India does not intervene in the internal affairs of other countries. However, this stand on foreign policy took a U-turn under PM Rajiv Gandhi when he signed an agreement with the Sri Lankan Prime Minister which brought peace to Sri Lanka\n\nBoth PM Indira Gandhi and PM Rajiv Gandhi believed in the link between economic development and protection of the environment\n\nPM Rajiv Gandhi advocated for the disarmament of nuclear weapons, a belief that was upheld by India over the years\n\nIndian, under different PMs, has always extended a hand of peace towards Pakistan over the years\n\nPM Narendra Modi believes that economic empowerment and upliftment of any nation involves the empowerment of its women\n\nPM Narendra Modi has launched several schemes that will help India achieve its SGD goals\n\nUsing information extraction, we were able to isolate only a few sentences that we required that gave us maximum results.\n\nInformation Extraction #2 – Finding Initiatives\n\nThe second interesting thing I noticed while going through the speeches is that there were a lot of initiatives, schemes, agreements, conferences, programs, etc. that were mentioned in the speeches. For example, ‘Paris agreement’, ‘Simla Agreement’, ‘Conference on Security Council’, ‘Conference of Non Aligned Countries’, ‘International Solar Alliance’, ‘Skill India initiative’, etc.\n\nExtracting these would give us an idea about what are the priorities for India and whether there is a pattern as to why they are mentioned quite often in the speeches.\n\nI am going to refer to all the schemes, initiatives, conferences, programmes, etc. keywords as initiatives.\n\nTo extract initiatives from the text, the first thing I am going to do is identify those sentences that talk about the initiatives. For that, I will use simple regex to select only those sentences that contain the keyword ‘initiative’, ‘scheme’, ‘agreement’, etc. This will reduce our search for the initiative pattern that we are looking for:\n\nNow, you might be thinking that our task is done here as we have already identified the sentences. We can easily look these up and determine what is being talked about in these sentences. But, think about it, not all of these will contain the initiative name. Some of these might be generally talking about initiatives but no initiative name might be present in them.\n\nTherefore, we need to come up with a better solution that extracts only those sentences that contain the initiative names. For that, I am going to use the spaCy Matcher, once again, to come up with a pattern that matches these initiatives.\n\nHave a look at the following example sentences and see if you can come up with a pattern to extract these initiatives:\n\nAs you might have noticed, the initiative name is a proper noun that starts with a determiner and ends with either ‘initiative’/’programme’/’agreement’ etc. words in the end. It also includes an occasional preposition in the middle. I also noticed that most of the initiative names were between two to five words long. Keeping this in mind, I came up with the following pattern to match the initiative names:\n\nWe got 62 sentences that match our pattern – not bad. Have a look at the output from the year 2018:\n\nBut one thing I must point out here is that there were a lot more initiatives in the speeches that did not match our pattern. For example, in the year 2018, there were other initiatives too like “MUDRA”, ”Ujjwala”, ”Paris Agreement”, etc. So is there a better way to extract them?\n\nRemember how we looked at dependencies at the beginning of the article? Well, we are going to use those to make some rules to match the initiative name. But before making a rule, you need to understand how a sentence is structured, only then can you come up with a general rule to extract relevant information.\n\nTo understand the structure of the sentence I am going to print the dependency graph of a sample example but in a tree fashion which gives a better intuition of the structure. Have a look below:\n\nSee how ‘Ujjwala’ is a child node of ‘programme’. Have a look at another example:\n\nNotice how the ‘International Solar Alliance’ is structured.\n\nYou must have got the idea by now that the initiative names are usually children of nodes that contain words like ‘initiative’, ‘programme’, etc. Based on this knowledge we can develop our own rule.\n\nThe rule I am suggesting is pretty simple. Let me walk you through it:\n\nI am going to look for tokens in sentences that contain my initiative keywords\n\nThen I am going to look at its subtree (or words dependent on it) using token.subtree and extract only those nodes/words that are proper nouns, since they are most likely going to contain the name of the initiative\n\nThis time we matched 282 entries. That is a significant improvement over the previous result. Let’s go over the 2018 output and see if we did any better this time:\n\nOut of 7000+ sentences, we were able to zero down to just 282 sentences that talked about initiatives. I looped over these outputs and below is how I would summarise the output:\n\nThere are a lot of different international initiatives or schemes that India has mentioned in its speeches. This goes to show that India has been an active member of the international community working towards building a better future by solving problems through these initiatives\n\nAnother point to highlight here is that the initiatives mentioned in the initial years have been more focused on those that concern the international community. However, during recent times, especially after 2014, a lot of domestic initiatives have been mentioned in the speeches like ‘Ayushman Bharat’, ‘Pradhan Mantri Jan Dhan Yojana’, etc. This shows a shift in how the country perceives its role in the community. By mentioning a lot of domestic initiatives, India has started to put more of the domestic work in front of the international community to witness and, probably, even follow in their footsteps\n\nHaving said that, the results were definitely not perfect. There were instances when unwanted words were also getting extracted with the initiative names. But the output derived by making our own rules was definitely better than the ones derived by using SpaCy’s pattern matcher. This goes to show the flexibility we can achieve by making our own rules.\n\nFinding Patterns in the Speeches\n\nSo far, we extracted only that information that met our analytical eye when we skimmed over the data. But is there any other information hidden in our dataset? Surely there is and we are going to explore that by making our own rules using the dependency of the words, as we did in the previous section.\n\nBut before that, I want to point out two things.\n\nFirst, when we are trying to understand the structure of the speech, we cannot look at the entire speech, that would take an eternity, and time is of the essence here. What we are going to do instead is look at random sentences from the dataset and then, based on their structure, try to come up with general rules to extract information.\n\nBut how do we test the validity of these rules? That’s where my second point comes in! Not all of the rules that we come up with will yield satisfactory results. So, to sift out the irrelevant rules, we can look at the percentage of sentences that matched our rule out of all sentences. This will give us a fair idea about how well the rule is performing, and whether, in fact, there is any such general structure in the corpus!\n\nAnother very important point that needs to be highlighted here is that any corpus is bound to contain long complex sentences. Working with these sentences to try and understand their structure will be a very difficult task. Therefore, we are going to look at smaller sentences. This will give us the opportunity to better understand their structure. So what’s the magic number? Let’s first look at how the sentence length varies in our corpus.\n\nLooking at the histogram, we can see that most of the sentences range from 15-20 words. So I am going to work with sentences that contain no more than 15 words:\n\nNow, let’s write a simple function that will generate random sentences from this dataframe:\n\nFinally, let’s make a function to evaluate the result of our rule:\n\nRight, let’s get down to the business of making some rules!\n\nInformation Extraction #3 – Rule on Noun-Verb-Noun Phrases\n\nWhen you look at a sentence, it generally contains a subject (noun), action (verb), and an object (noun). The rest of the words are just there to give us additional information about the entities. Therefore, we can leverage this basic structure to extract the main bits of information from the sentence. Take for example the following sentence:\n\nWhat will be extracted from this sample sentence based on the rule is – “countries face threats”. This should give us a fair idea about what the sentence is trying to say.\n\nSo let’s look at how this rule fairs what we run it against the short sentences that we are working with:\n\nWe are getting more than 20% pattern match for our rule and we can check it for all the sentences in the corpus:\n\nWe are getting more than a 30% match for our rules, which means 2226 out of 7150 sentences matched this pattern. Let’s form a new dataframe containing only those sentences that have an output and then segregate the verb from the nouns:\n\nLet’s take a look at the top 10 most occurring verbs used in the sentences:\n\nNow, we can look at specific verbs to see what kind of information is present. For example, ‘welcome’ and ‘support’ could tell us what India encourages. And verbs like ‘face’ could maybe tell us what kind of problems we face in the real world.\n\nBy looking at the output, we can try to make out what is the context of the sentence. For example, we can see that India supports ‘efforts’, ‘viewpoints’, ‘initiatives’, ‘struggles’, ‘desires, ‘aspirations’, etc. While India believes that the world faces ‘threat’, ‘conflicts’, ‘colonialism’, ‘pandemics’, etc.\n\nWe can select sentences to explore in-depth by looking at the output. This will definitely save us a lot of time than just going over the entire text.\n\nInformation Extraction #4 – Rule on Adjective Noun Structure\n\nIn the previous rule that we made for information extraction in NLP, we extracted the noun subjects and objects, but the information did not feel complete. This is because many nouns have an adjective or a word with a compound dependency that augments the meaning of a noun. Extracting these along with the noun will give us better information about the subject and the object.\n\nHave a look at the sample sentence below:\n\nWhat we are looking to achieve here is – “better life”.\n\nThe code for this rule is simple, but let me walk you through how it works:\n\nWe look for tokens that have a Noun POS tag and have subject or object dependency\n\nThen we look at the child nodes of these tokens and append it to the phrase only if it modifies the noun\n\n51% of the short sentences match this rule. We can now try to check it on the entire corpus:\n\nOn the entire corpus of 7150, 76% or 5117 sentences matched our pattern rule, since most of them are bound to contain the noun and its modifier.\n\nNow we can combine this rule along with the rule we created previously. This will give us a better perspective of what information is present in a sentence:\n\nWe get a 31% output match some of which are displayed below:\n\nHere, we end up with phrases like “we take a fresh pledge”, “we have a sizeable increase”, “people expecting better life”, etc. which included the nouns and their modifiers. This gives us better information about what is being extracted here.\n\nAs you can see, we not only came up with a new rule to understand the structure of the sentences but also combined two rules to get better information from the extracted text.\n\nInformation Extraction #5 – Rule on Prepositions\n\nThank god for prepositions! They tell us where or when something is in a relationship with something else. For example, The people of India believe in the principles of the United Nations. Clearly extracting phrases including prepositions will give us a lot of information from the sentence. This is exactly what we are going to achieve with this rule.\n\nLet’s try to understand how this rule works by going over it on a sample sentence – “India has once again shown faith in democracy.”\n\nWe iterate over all the tokens looking for prepositions. For example, in this sentence\n\nOn encountering a preposition, we check if it has a headword that is a noun. For example, the word faith in this sentence\n\nThen we look at the child tokens of the preposition token falling on its right side. For example, the word democracy\n\nThis should finally extract the phrase faith in democracy from the sentence. Have a look at the dependency graph of the sentence below:\n\nNow let’s apply this rule to our short sentences:\n\nAbout 48% of the sentences follow this rule:\n\nWe can test this pattern on the entire corpus since we have a good amount of sentences matching the rule:\n\n74% of the total sentences match this pattern. Let’s separate the preposition from the nouns and see what kind of information we were able to extract:\n\nThe following dataframe shows the result of the rule on the entire corpus, but the preposition and nouns are separated for better analysis:\n\nWe can look at the top 10 most occurring prepositions in the entire corpus:\n\nWe look at certain prepositions to explore the sentences in detail. For example, the preposition ‘against’ can give us information about what India does not support:\n\nSkimming over the nouns, some important phrases like:\n\nefforts against proliferation\n\nfight against terrorism, action against terrorism, the war against terrorism\n\ndiscrimination against women\n\nwar against poverty\n\nstruggle against colonialism\n\n… and so on. This should give us a fair idea about which sentences we want to explore in detail. For example, efforts against proliferation talk about efforts towards nuclear disarmament. Or the sentence on the struggle against colonialism talks about the historical links between India and Africa borne out of their common struggle against colonialism.\n\nAs you can see, prepositions provide vital connections between two nouns, aiding in information extraction in artificial intelligence. With some domain expertise, sifting through extensive data becomes manageable, helping identify India’s stances and actions, among other insights.\n\nBut the output seems a bit incomplete. For example, in the sentence efforts against proliferation, what kind of proliferation are we talking about? Certainly, we need to include the modifiers attached to the nouns in the phrase as we did in Information Extraction #4. This would definitely increase the comprehensibility of the extracted phrase.\n\nThis rule can be easily modified to include the new change. I have created a new function to extract the noun modifiers for nouns that we extracted from Information Extraction #4:\n\nAll we have to do is call this function whenever we encounter a noun in our phrase:\n\nThis definitely has more information than before. For example, ‘impediments in economic development’ instead of ‘impediments in development’ and ‘greater transgressor of human rights’ rather than ‘transgressor of rights’.\n\nOnce again, combining rules has given us more power and flexibility to explore only those sentences in detail that have a meaningful extracted phrase.\n\nCode\n\nYou can find the complete code file here.\n\nConclusion\n\nThe field of natural language processing (NLP) and its applications such as named entity recognition, semantic annotation, and sentiment analysis have revolutionized our ability to extract meaning from unstructured data. Through APIs and advanced algorithms, we can now classify and extract relations from vast amounts of text, transforming raw information into actionable insights. As we continue to innovate in this space, the potential for NLP to enhance decision-making and drive efficiencies across industries remains profound.\n\nGoing forward, you can explore the following courses to expand your knowledge in the field of NLP:\n\nNatural Language Processing (NLP) Using Python\n\nA Comprehensive Learning Path to Understand and Master NLP\n\nKey Takeaways\n\nUnderstanding the trade-offs between time-consuming NLP tasks and computational efficiency.\n\nImportance of preprocessing steps like tokenization and normalization in NLP techniques.\n\nApplication of advanced NLP tasks like sentiment analysis and summarization in healthcare.\n\nIntegration of part-of-speech tagging with other NLP techniques for comprehensive analysis of healthcare data.\n\nTechniques for classifying medical texts based on their content and context.\n\nChallenges specific to using NLP in healthcare, such as data privacy and security concerns.\n\nFrequently Asked Questions"
    }
}
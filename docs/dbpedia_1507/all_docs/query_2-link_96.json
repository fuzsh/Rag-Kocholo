{
    "id": "dbpedia_1507_2",
    "rank": 96,
    "data": {
        "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10047603/",
        "read_more_link": "",
        "language": "en",
        "title": "Protecting the integrity of survey research",
        "top_image": "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/pmc-card-share.jpg?_=0",
        "meta_img": "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/pmc-card-share.jpg?_=0",
        "images": [
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/favicons/favicon-57.png",
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/icon-dot-gov.svg",
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/icon-https.svg",
            "https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/logos/AgencyLogo.svg",
            "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/logo-pnasnexus.jpg",
            "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/corrauth.gif",
            "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/corrauth.gif"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Kathleen Hall Jamieson",
            "Arthur Lupia",
            "Ashley Amaya",
            "Henry E Brady",
            "René Bautista",
            "Joshua D Clinton",
            "Jill A Dever",
            "David Dutwin",
            "Daniel L Goroff",
            "D Sunshine Hillygus"
        ],
        "publish_date": "2023-03-11T00:00:00",
        "summary": "",
        "meta_description": "Although polling is not irredeemably broken, changes in technology and society create challenges that, if not addressed well, can threaten the quality of election polls and other important surveys on topics such as the economy. This essay describes some ...",
        "meta_lang": "en",
        "meta_favicon": "https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon.ico",
        "meta_site_name": "PubMed Central (PMC)",
        "canonical_link": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10047603/",
        "text": "The protecting the integrity of survey research retreat\n\nTo address this question, on November 18 and 19, 2021, Marcia McNutt, president of the National Academy of Sciences, convened a virtual retreat to explore ways to protect the integrity of survey research, increase understanding of the limitations and strengths of individual surveys, incentivize disclosure of information needed to evaluate findings from surveys, and help the public recognize distinguishing features of credible surveys. The retreat was cohosted by the Annenberg Foundation Trust at Sunnylands and the Annenberg Public Policy Center (APPC) of the University of Pennsylvania. The proceedings were coordinated by Arthur Lupia of the University of Michigan and Kathleen Hall Jamieson, APPC director and Sunnylands Trust program director. Included among the 20 participants were the crafters of this document, a list that includes current and past editors of major academic journals, past presidents of the American Political Science Association and AAPOR, and a past director of the US Census Bureau, as well as scholars who have led some of the nation's largest university-based election surveys and individuals responsible for the creation and maintenance of large governmental and 501(c)(3) survey datasets.\n\nThe convening's main outcome is an understanding that safeguarding the integrity of survey research, including political polls, is possible. A path to that end includes renewed commitments to scientific norms of transparency, precise specification of key methodological decisions, dedication to disclosure and self-correction when errors are identified, and improved reporting practices by researchers and the media. In service of these goals, we offer 12 actions that key stakeholders can take now, and in the near future, to improve the integrity, utility, and public understanding of surveys.\n\nChanges in technology and society create challenges that, if not addressed well, can threaten the quality of important surveys on topics such as public health, the economy, and elections. In what follows, we describe some of them and recommend remedies designed to protect the integrity of survey research. These recommendations are the product of presentations and conversations at the retreat and email discussions in the months that followed. They reflect points of agreement among a diverse group of stakeholders. Collectively, the recommendations are designed to increase the research community's ability to independently assess survey-based research claims and to share those assessments with a broader audience. If followed, these recommendations will add to the existing menu of good practices and strengthen the ability of researchers to draw properly qualified, reliable inferences from survey research.\n\nImprove disclosure of modeling and weighting assumptions\n\nSince there is not a one-to-one correspondence between the salient characteristics of the sample and the population from which it is drawn, modeling and weighting are regular features of survey research. Among other uses, they are designed to compensate for different rates of survey nonresponse and nonparticipation.\n\nRecommendation 1A: To facilitate evaluations of assumptions, such as these, and to facilitate more accurate interpretations of the resulting data and analyses, survey vendors should disclose their modeling and weighting assumptions to all users of survey data in ways that are consistent with the FAIR (findable, accessible, interoperable, and reusable) principles for open data.\n\nRecommendation 1B: All publications that include survey research findings should require that modeling and weighting assumptions be disclosed as part of an article's methods section or in supplementary material to which the publication offers direct links. When no weights have been applied, that fact should be disclosed as well.\n\nToday, the extent of disclosure by survey organizations varies widely. We call on survey vendors who are not fully disclosing their modeling and weighting assumptions to do so. This action will empower researchers to analyze, and reporters to more accurately interpret, the corresponding data. Such transparency will make it possible for reporters and researchers to explore how and why different data or analyses produce different results.\n\nMargins of error and credibility intervals reported in survey-based articles tend to assume that the survey estimates are unbiased and only subject to errors due to incomplete sampling. In other words, this way of reporting survey results depends on the often-unrealistic assumption that survey samples contain no asymmetries. But asymmetries often exist in which parts of a population are covered in a sample or in factors that can cause differential rates of participation or otherwise influence a sample's ability to represent a larger population. When the assumption that survey estimates are unbiased does not hold, particularly in the case of surveys that depend on participating “opt-in” rather than random selection, reported margins of error understate the likely magnitude of errors in survey estimates.\n\nRecommendation 1C: Since some practitioners (particularly those outside of academia) may not know how to assess or disclose all these phenomena, professional associations and groups that oversee the integrity of the federal statistical agencies that use survey research ought to provide templates and education that can improve the extent and utility of disclosure of modeling and weighting assumptions.\n\nThese templates could take the form of a one-page document that lists questions to ask or a brief report that describes common types of decisions made when developing models or weighting schemes.\n\nIncorporate these new disclosure recommendations in the AAPOR Code\n\nRecommendation 6: Engage AAPOR to consider ways to augment its Code of Professional Ethics and Practices to include disclosure of the sources from which samples are drawn, attrition rates in panels and longitudinal surveys, and the extent to which respondents have been exposed to related surveys or survey questions in the recent past, if known.\n\nSome vendors already provide information of these types. The purpose of Recommendation 6 is to aid those who use survey data or published results by raising the visibility as well as the frequency of these disclosures. We single out AAPOR for its pioneering work and focal position within the survey research field in the hope that organizations with a comparable mission will take equivalent actions.\n\n2. CLARITY: A norm of clarity dictates that clear, accurate language be used to characterize the nature of the survey process and its outcomes. A commitment to clarity involves being forthright about the types of precision that surveys can and cannot produce.\n\nTo advance the norm of clarity, our next recommendations expand upon best practices in the AAPOR Code of Professional Ethics and Practices. The relevant part of the code states:\n\n1. We will not knowingly make interpretations of research results that are inconsistent with the data available, nor will we tacitly permit such interpretations. We will ensure that any findings we report, either privately or for public release, are a balanced and accurate portrayal of research results.\n\n2. We will not knowingly imply that interpretations are accorded greater confidence than the data warrant. When we generalize from samples to make statements about populations, we will only make claims of precision and applicability to broader populations that are warranted by the sampling frames and other methods employed (18).\n\nThe US Census Bureau's decision not to release the 2020 estimates from one of the nation's premier governmental surveys demonstrates a commitment to ensuring sample quality consistent with such best practices. After surveying a sample of 290,000 people monthly, the Bureau's ACS then “combines the monthly responses into a set of 1-year estimates for the nation, states and communities with populations of 65,000 or more” (22). The ACS is widely used by researchers, governments, and various private sector organizations. However, by disrupting the lives of various subgroups of the US population in different ways in 2020, the COVID-19 pandemic created new “nonresponse bias” challenges that made it more difficult to produce representative survey samples. Although the Bureau sought many ways to adapt to unprecedented circumstances, its researchers concluded that the 2020 ACS data failed to meet the Statistical Data Quality Standards established “to ensure the utility, objectivity and integrity of the statistical information” (22). Rather than publish data in a compromised and potentially misleading form, the Bureau announced that it would not release its 1-year estimates from the 2020 survey.\n\nAt the same time, the use of clear language and definitions increases the likelihood that researchers speak a shared language when addressing each other and the public.\n\nTo increase the clarity with which survey data are reported, we offer the following recommendations:\n\nRecommendation 7: When survey data are weighted, the phrase “representative sample” should not be used without explicit acknowledgment of the underlying assumptions, including weighting and modeling assumptions. Survey vendors should not release data without including this information, and publishers of content who use survey data should publicly commit to cite or use data only from sources that provide such information.\n\nThere remains an active debate in the survey community about the threshold test a sample must satisfy to be called “representative.” Our hope is that Recommendation 7 will focus that debate on the articulation of explicit standards that can help analysts and the public draw more accurate inferences about the population a survey sample is more, and less, likely to represent.\n\nOf particular importance in these discussions is clarifying the most effective use of probability and non-probability samples. These diverse forms of data collection have distinct advantages and disadvantages. Probability samples minimize the risk of systematic bias. Non-probability samples are easier and less costly to generate. On some matters, there is a consensus on when one method of gathering a sample is more effective at achieving these types of goals. In other cases, there is less consensus on what these different types of surveys can and cannot do. Helping a broader set of stakeholders understand trade-offs associated with these methods could produce significant public benefits.\n\nFor example, non-probability surveys are quite efficient for tracking changes in public sentiment (e.g. presidential approval) over time, provided that the estimates need not be extremely precise. Non-probability surveys have also proven useful for estimating treatment effects across randomly assigned groups (e.g. 23, 24). For other research purposes, however, non-probability surveys may not be fit for use. Studies have shown that the positivity bias associated with bogus respondents can lead non-probability surveys to systematically overestimate rare outcomes, such as ingesting bleach to protect against COVID-19 (25), belief in conspiracies like PizzaGate (26), support of political violence (27), or favorable views of Vladimir Putin (28). In terms of scale, Geraci (29) estimates that researchers should anticipate removing 35–50% of non-probability completes due to poor data quality. More broadly, non-probability surveys are not fit for use in federal surveys that are expected to yield highly precise estimates for the country as a whole, but also for harder to reach subgroups.\n\nOur final recommendation in this section pertains to reporting standards. Consistent with our earlier discussions about weighting and modeling, we recommend that publishers require that researchers who use survey data report their decisions about how observations are weighted relative to one another, how this weighting affects margin of error estimates, and which variables are used in attempts to determine whether a population of respondents is sufficiently aligned with a population of interest.\n\nRecommendation 8: Publishers and editors of scholarly journals should incentivize clarity by adopting reporting standards that better reflect the realities of modern survey research. In particular, they should ask authors to populate, and make available for readers to view, a template that, like the Roper Center's Transparency and Acquisition Policy, clearly describes survey attributes that can influence accuracy.\n\nCompleting a well-designed template, particularly a standardized one in hand when work on a survey begins, does not have to be burdensome and can provide important information in an effective way. This simple instrument, in turn, may increase researchers’ and other stakeholders’ ability to accurately interpret survey data and published results.\n\n3. CORRECTING THE RECORD. A norm of self-correction requires that when errors are identified, they be disclosed in forms accessible to other researchers and that protections be put in place to minimize their recurrence in other surveys or subsequent analyses.\n\nSelf-correction is a key scientific norm. When scientists are uncertain about the correspondence between an observation and a research claim, the expectation is that they will report that uncertainty. When a scientist discovers an error, a parallel expectation arises.\n\nBecause surveys are complex, involve coding human response to language, and often are administered in challenging environments, unanticipated forms of error occur. When an error is identified in published work, existing practice involves reporting it to the journal in which the work appeared and offering a correction. So, for example, Kathleen Hall Jamieson, Marcia McNutt, Veronique Kiermer, and Richard Sever appended a correction to “Signaling the trustworthiness of science” (30), which read: “A coding error was uncovered in the survey vendor's computer-assisted telephone interviewing (CATI) programming of the 2019 Annenberg Science Knowledge (ASK) survey used in our study. To minimize response order bias, the scale items ranging from 1 to 5 were programmed to reverse from 5 to 1 for a random half of the sample. Instead of recoding the responses to 1, 2, 3, 4, and 5, the programmer recoded only 1 and 5. As a result, the data reported in the article underrepresented the percentage saying that the reported statements mattered.” Although those who search for the article will find a prominent link to the correction, there is currently no ready way for authors to alert others to the need to check to make sure that a comparable problem has not occurred in their vendor's programming.\n\nRecommendation 9: We recommend that an online resource center, modeled on the National Science Foundation-supported Online Ethics Center for Engineering and Science (established by the National Academy of Engineering and now run by the University of Virginia (31)), be established to archive and make accessible information about technical problems, sources of data corruption, and solutions that survey researchers have uncovered when trying to conduct surveys rigorously and responsibly.\n\nOvercoming barriers to adoption\n\nOne might argue that by increasing vendor costs, industry adoption of our disclosure recommendations about weighting assumptions and panel sensitization would drive vendors whose work cannot sustain the resulting scrutiny out of business. If these forms of disclosure help protect the integrity of the research process, as we believe they do, that outcome is a benefit not a downside of adopting them. We believe that these disclosures are likely to reveal, and hopefully reduce, methods of panel assembly that are difficult to defend and, at the same time, will help researchers better interpret panel data.\n\nHowever, since the increased costs will be passed on by surviving vendors, it is fair to say that some studies may prove cost-prohibitive and not be undertaken. Others will be based on less data than otherwise would have been collected. We believe that improvements in the quality of published research and in the reliability of inferences grounded in the survey data are worth the trade-off and costs. But the market will ultimately determine whether the increased quality of the data, analysis, and reliability of inferences is worth the cost.\n\nBecause costs associated with recommendations reduce the likelihood of adoption, many of our recommendations incentivize it by making adoption a signal of greater trustworthiness. The large and growing number of journals whose editors or publishers have asked to be listed as subscribers to the International Committee of Medical Journal Editors’ (ICMJE's) Recommendations for the Conduct, Reporting, Editing and Publication of Scholarly Work in Medical Journals shows this process at work (https://www.icmje.org/journals-following-the-icmje-recommendations/). Among other topics, the ICMJE recommendations focus on Defining the Role of Authors and Contributors, Disclosure of Financial and Non-Financial Relationships and Activities, and Conflicts of Interest, and Responsibilities in Submission and Peer-Review. If the publishers of high-impact journals presuppose disclosures as a condition of publication, and media outlets that report on survey research do the same, researchers will require them from vendors. Because journals are judged in part by their reputation, when those known as high quality adopt a practice, others follow suit. The same logic applies to vendors. If signing on to the AAPOR Transparency Initiative is a signal of commitment to protect the data gathering and reporting process, then vendors who do so have a competitive advantage."
    }
}
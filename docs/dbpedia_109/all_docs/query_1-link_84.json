{
    "id": "dbpedia_109_1",
    "rank": 84,
    "data": {
        "url": "https://cloud.google.com/vision/docs/ocr",
        "read_more_link": "",
        "language": "en",
        "title": "Detect text in images",
        "top_image": "https://cloud.google.com/_static/cloud/images/social-icon-google-cloud-1200-630.png",
        "meta_img": "https://cloud.google.com/_static/cloud/images/social-icon-google-cloud-1200-630.png",
        "images": [
            "https://www.gstatic.com/devrel-devsite/prod/v80280542cfb431993d6ccf12e26a1c5862cffb314c3cfae3ff08e8374a93b7f7/cloud/images/cloud-logo.svg",
            "https://www.gstatic.com/devrel-devsite/prod/v80280542cfb431993d6ccf12e26a1c5862cffb314c3cfae3ff08e8374a93b7f7/cloud/images/cloud-logo.svg",
            "https://cloud.google.com/static/vision/docs/images/sign_text.png",
            "https://cloud.google.com/static/vision/docs/images/document_text_highlighted.png",
            "https://cloud.google.com/static/vision/docs/images/sign_small.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "https://www.gstatic.com/devrel-devsite/prod/v80280542cfb431993d6ccf12e26a1c5862cffb314c3cfae3ff08e8374a93b7f7/cloud/images/favicons/onecloud/favicon.ico",
        "meta_site_name": "Google Cloud",
        "canonical_link": "https://cloud.google.com/vision/docs/ocr",
        "text": "Stay organized with collections Save and categorize content based on your preferences.\n\nOptical Character Recognition (OCR)\n\nThe Vision API can detect and extract text from images. There are two annotation features that support optical character recognition (OCR):\n\nTEXT_DETECTION detects and extracts text from any image. For example, a photograph might contain a street sign or traffic sign. The JSON includes the entire extracted string, as well as individual words, and their bounding boxes.\n\nDOCUMENT_TEXT_DETECTION also extracts text from an image, but the response is optimized for dense text and documents. The JSON includes page, block, paragraph, word, and break information.\n\nLearn more about DOCUMENT_TEXT_DETECTION for handwriting extraction and text extraction from files (PDF/TIFF).\n\nTry it for yourself\n\nIf you're new to Google Cloud, create an account to evaluate how Cloud Vision performs in real-world scenarios. New customers also get $300 in free credits to run, test, and deploy workloads.\n\nTry Cloud Vision free\n\nText detection requests\n\nSet up your Google Cloud project and authentication\n\nIf you have not created a Google Cloud project, do so now. Expand this section for instructions.\n\nSign in to your Google Cloud account. If you're new to Google Cloud, create an account to evaluate how our products perform in real-world scenarios. New customers also get $300 in free credits to run, test, and deploy workloads.\n\nIn the Google Cloud console, on the project selector page, select or create a Google Cloud project.\n\nGo to project selector\n\nMake sure that billing is enabled for your Google Cloud project.\n\nEnable the Vision API.\n\nEnable the API\n\nInstall the Google Cloud CLI.\n\nTo initialize the gcloud CLI, run the following command:\n\ngcloud init\n\nIn the Google Cloud console, on the project selector page, select or create a Google Cloud project.\n\nGo to project selector\n\nMake sure that billing is enabled for your Google Cloud project.\n\nEnable the Vision API.\n\nEnable the API\n\nInstall the Google Cloud CLI.\n\nTo initialize the gcloud CLI, run the following command:\n\ngcloud init\n\nDetect text in a local image\n\nYou can use the Vision API to perform feature detection on a local image file.\n\nFor REST requests, send the contents of the image file as a base64 encoded string in the body of your request.\n\nFor gcloud and client library requests, specify the path to a local image in your request.\n\ngcloud\n\nTo perform text detection, use the gcloud ml vision detect-text command as shown in the following example:\n\ngcloud ml vision detect-text ./path/to/local/file.jpg\n\nREST\n\nBefore using any of the request data, make the following replacements:\n\nBASE64_ENCODED_IMAGE: The base64 representation (ASCII string) of your binary image data. This string should look similar to the following string:\n\n/9j/4QAYRXhpZgAA...9tAVx/zDQDlGxn//2Q==\n\nVisit the base64 encode topic for more information.\n\nPROJECT_ID: Your Google Cloud project ID.\n\nHTTP method and URL:\n\nPOST https://vision.googleapis.com/v1/images:annotate\n\nRequest JSON body:\n\n{ \"requests\": [ { \"image\": { \"content\": \"BASE64_ENCODED_IMAGE\" }, \"features\": [ { \"type\": \"TEXT_DETECTION\" } ] } ] }\n\nTo send your request, choose one of these options:\n\ncurl\n\nSave the request body in a file named request.json, and execute the following command:\n\ncurl -X POST \\\n\n-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n\n-H \"x-goog-user-project: PROJECT_ID\" \\\n\n-H \"Content-Type: application/json; charset=utf-8\" \\\n\n-d @request.json \\\n\n\"https://vision.googleapis.com/v1/images:annotate\"\n\nPowerShell\n\nSave the request body in a file named request.json, and execute the following command:\n\n$cred = gcloud auth print-access-token\n\n$headers = @{ \"Authorization\" = \"Bearer $cred\"; \"x-goog-user-project\" = \"PROJECT_ID\" }\n\nInvoke-WebRequest `\n\n-Method POST `\n\n-Headers $headers `\n\n-ContentType: \"application/json; charset=utf-8\" `\n\n-InFile request.json `\n\n-Uri \"https://vision.googleapis.com/v1/images:annotate\" | Select-Object -Expand Content\n\nIf the request is successful, the server returns a 200 OK HTTP status code and the response in JSON format.\n\nA TEXT_DETECTION response includes the detected phrase, its bounding box, and individual words and their bounding boxes.\n\nResponse\n\n{ \"responses\": [ { \"textAnnotations\": [ { \"locale\": \"en\", \"description\": \"WAITING?\\nPLEASE\\nTURN OFF\\nYOUR\\nENGINE\\n\", \"boundingPoly\": { \"vertices\": [ { \"x\": 341, \"y\": 828 }, { \"x\": 2249, \"y\": 828 }, { \"x\": 2249, \"y\": 1993 }, { \"x\": 341, \"y\": 1993 } ] } }, { \"description\": \"WAITING?\", \"boundingPoly\": { \"vertices\": [ { \"x\": 352, \"y\": 828 }, { \"x\": 2248, \"y\": 911 }, { \"x\": 2238, \"y\": 1148 }, { \"x\": 342, \"y\": 1065 } ] } }, { \"description\": \"PLEASE\", \"boundingPoly\": { \"vertices\": [ { \"x\": 1210, \"y\": 1233 }, { \"x\": 1907, \"y\": 1263 }, { \"x\": 1902, \"y\": 1383 }, { \"x\": 1205, \"y\": 1353 } ] } }, { \"description\": \"TURN\", \"boundingPoly\": { \"vertices\": [ { \"x\": 1210, \"y\": 1418 }, { \"x\": 1730, \"y\": 1441 }, { \"x\": 1724, \"y\": 1564 }, { \"x\": 1205, \"y\": 1541 } ] } }, { \"description\": \"OFF\", \"boundingPoly\": { \"vertices\": [ { \"x\": 1792, \"y\": 1443 }, { \"x\": 2128, \"y\": 1458 }, { \"x\": 2122, \"y\": 1581 }, { \"x\": 1787, \"y\": 1566 } ] } }, { \"description\": \"YOUR\", \"boundingPoly\": { \"vertices\": [ { \"x\": 1219, \"y\": 1603 }, { \"x\": 1746, \"y\": 1629 }, { \"x\": 1740, \"y\": 1759 }, { \"x\": 1213, \"y\": 1733 } ] } }, { \"description\": \"ENGINE\", \"boundingPoly\": { \"vertices\": [ { \"x\": 1222, \"y\": 1771 }, { \"x\": 1944, \"y\": 1834 }, { \"x\": 1930, \"y\": 1992 }, { \"x\": 1208, \"y\": 1928 } ] } } ], \"fullTextAnnotation\": { \"pages\": [ ... ] }, \"paragraphs\": [ ... ] }, \"words\": [ ... }, \"symbols\": [ ... } ] } ], \"blockType\": \"TEXT\" }, ... ] } ], \"text\": \"WAITING?\\nPLEASE\\nTURN OFF\\nYOUR\\nENGINE\\n\" } } ] }\n\nGo\n\nBefore trying this sample, follow the Go setup instructions in the Vision quickstart using client libraries. For more information, see the Vision Go API reference documentation.\n\nTo authenticate to Vision, set up Application Default Credentials. For more information, see Set up authentication for a local development environment.\n\n// detectText gets text from the Vision API for an image at the given file path. func detectText(w io.Writer, file string) error { ctx := context.Background() client, err := vision.NewImageAnnotatorClient(ctx) if err != nil { return err } f, err := os.Open(file) if err != nil { return err } defer f.Close() image, err := vision.NewImageFromReader(f) if err != nil { return err } annotations, err := client.DetectTexts(ctx, image, nil, 10) if err != nil { return err } if len(annotations) == 0 { fmt.Fprintln(w, \"No text found.\") } else { fmt.Fprintln(w, \"Text:\") for _, annotation := range annotations { fmt.Fprintf(w, \"%q\\n\", annotation.Description) } } return nil }\n\nJava\n\nBefore trying this sample, follow the Java setup instructions in the Vision API Quickstart Using Client Libraries. For more information, see the Vision API Java reference documentation.\n\nimport com.google.cloud.vision.v1.AnnotateImageRequest; import com.google.cloud.vision.v1.AnnotateImageResponse; import com.google.cloud.vision.v1.BatchAnnotateImagesResponse; import com.google.cloud.vision.v1.EntityAnnotation; import com.google.cloud.vision.v1.Feature; import com.google.cloud.vision.v1.Image; import com.google.cloud.vision.v1.ImageAnnotatorClient; import com.google.protobuf.ByteString; import java.io.FileInputStream; import java.io.IOException; import java.util.ArrayList; import java.util.List; public class DetectText { public static void detectText() throws IOException { // TODO(developer): Replace these variables before running the sample. String filePath = \"path/to/your/image/file.jpg\"; detectText(filePath); } // Detects text in the specified image. public static void detectText(String filePath) throws IOException { List<AnnotateImageRequest> requests = new ArrayList<>(); ByteString imgBytes = ByteString.readFrom(new FileInputStream(filePath)); Image img = Image.newBuilder().setContent(imgBytes).build(); Feature feat = Feature.newBuilder().setType(Feature.Type.TEXT_DETECTION).build(); AnnotateImageRequest request = AnnotateImageRequest.newBuilder().addFeatures(feat).setImage(img).build(); requests.add(request); // Initialize client that will be used to send requests. This client only needs to be created // once, and can be reused for multiple requests. After completing all of your requests, call // the \"close\" method on the client to safely clean up any remaining background resources. try (ImageAnnotatorClient client = ImageAnnotatorClient.create()) { BatchAnnotateImagesResponse response = client.batchAnnotateImages(requests); List<AnnotateImageResponse> responses = response.getResponsesList(); for (AnnotateImageResponse res : responses) { if (res.hasError()) { System.out.format(\"Error: %s%n\", res.getError().getMessage()); return; } // For full list of available annotations, see http://g.co/cloud/vision/docs for (EntityAnnotation annotation : res.getTextAnnotationsList()) { System.out.format(\"Text: %s%n\", annotation.getDescription()); System.out.format(\"Position : %s%n\", annotation.getBoundingPoly()); } } } } }\n\nNode.js\n\nBefore trying this sample, follow the Node.js setup instructions in the Vision quickstart using client libraries. For more information, see the Vision Node.js API reference documentation.\n\nTo authenticate to Vision, set up Application Default Credentials. For more information, see Set up authentication for a local development environment.\n\nconst vision = require('@google-cloud/vision'); // Creates a client const client = new vision.ImageAnnotatorClient(); /** * TODO(developer): Uncomment the following line before running the sample. */ // const fileName = 'Local image file, e.g. /path/to/image.png'; // Performs text detection on the local file const [result] = await client.textDetection(fileName); const detections = result.textAnnotations; console.log('Text:'); detections.forEach(text => console.log(text));\n\nPython\n\nBefore trying this sample, follow the Python setup instructions in the Vision quickstart using client libraries. For more information, see the Vision Python API reference documentation.\n\nTo authenticate to Vision, set up Application Default Credentials. For more information, see Set up authentication for a local development environment.\n\ndef detect_text(path): \"\"\"Detects text in the file.\"\"\" from google.cloud import vision client = vision.ImageAnnotatorClient() with open(path, \"rb\") as image_file: content = image_file.read() image = vision.Image(content=content) response = client.text_detection(image=image) texts = response.text_annotations print(\"Texts:\") for text in texts: print(f'\\n\"{text.description}\"') vertices = [ f\"({vertex.x},{vertex.y})\" for vertex in text.bounding_poly.vertices ] print(\"bounds: {}\".format(\",\".join(vertices))) if response.error.message: raise Exception( \"{}\\nFor more info on error messages, check: \" \"https://cloud.google.com/apis/design/errors\".format(response.error.message) )\n\nAdditional languages\n\nC#: Please follow the C# setup instructions on the client libraries page and then visit the Vision reference documentation for .NET.\n\nPHP: Please follow the PHP setup instructions on the client libraries page and then visit the Vision reference documentation for PHP.\n\nRuby: Please follow the Ruby setup instructions on the client libraries page and then visit the Vision reference documentation for Ruby.\n\nDetect text in a remote image\n\nYou can use the Vision API to perform feature detection on a remote image file that is located in Cloud Storage or on the Web. To send a remote file request, specify the file's Web URL or Cloud Storage URI in the request body.\n\ngcloud\n\nTo perform text detection, use the gcloud ml vision detect-text command as shown in the following example:\n\ngcloud ml vision detect-text gs://cloud-samples-data/vision/ocr/sign.jpg\n\nREST\n\nBefore using any of the request data, make the following replacements:\n\nCLOUD_STORAGE_IMAGE_URI: the path to a valid image file in a Cloud Storage bucket. You must at least have read privileges to the file. Example:\n\ngs://cloud-samples-data/vision/ocr/sign.jpg\n\nPROJECT_ID: Your Google Cloud project ID.\n\nHTTP method and URL:\n\nPOST https://vision.googleapis.com/v1/images:annotate\n\nRequest JSON body:\n\n{ \"requests\": [ { \"image\": { \"source\": { \"imageUri\": \"CLOUD_STORAGE_IMAGE_URI\" } }, \"features\": [ { \"type\": \"TEXT_DETECTION\" } ] } ] }\n\nTo send your request, choose one of these options:\n\ncurl\n\nSave the request body in a file named request.json, and execute the following command:\n\ncurl -X POST \\\n\n-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n\n-H \"x-goog-user-project: PROJECT_ID\" \\\n\n-H \"Content-Type: application/json; charset=utf-8\" \\\n\n-d @request.json \\\n\n\"https://vision.googleapis.com/v1/images:annotate\"\n\nPowerShell\n\nSave the request body in a file named request.json, and execute the following command:\n\n$cred = gcloud auth print-access-token\n\n$headers = @{ \"Authorization\" = \"Bearer $cred\"; \"x-goog-user-project\" = \"PROJECT_ID\" }\n\nInvoke-WebRequest `\n\n-Method POST `\n\n-Headers $headers `\n\n-ContentType: \"application/json; charset=utf-8\" `\n\n-InFile request.json `\n\n-Uri \"https://vision.googleapis.com/v1/images:annotate\" | Select-Object -Expand Content\n\nIf the request is successful, the server returns a 200 OK HTTP status code and the response in JSON format.\n\nA TEXT_DETECTION response includes the detected phrase, its bounding box, and individual words and their bounding boxes.\n\nResponse\n\n{ \"responses\": [ { \"textAnnotations\": [ { \"locale\": \"en\", \"description\": \"WAITING?\\nPLEASE\\nTURN OFF\\nYOUR\\nENGINE\\n\", \"boundingPoly\": { \"vertices\": [ { \"x\": 341, \"y\": 828 }, { \"x\": 2249, \"y\": 828 }, { \"x\": 2249, \"y\": 1993 }, { \"x\": 341, \"y\": 1993 } ] } }, { \"description\": \"WAITING?\", \"boundingPoly\": { \"vertices\": [ { \"x\": 352, \"y\": 828 }, { \"x\": 2248, \"y\": 911 }, { \"x\": 2238, \"y\": 1148 }, { \"x\": 342, \"y\": 1065 } ] } }, { \"description\": \"PLEASE\", \"boundingPoly\": { \"vertices\": [ { \"x\": 1210, \"y\": 1233 }, { \"x\": 1907, \"y\": 1263 }, { \"x\": 1902, \"y\": 1383 }, { \"x\": 1205, \"y\": 1353 } ] } }, { \"description\": \"TURN\", \"boundingPoly\": { \"vertices\": [ { \"x\": 1210, \"y\": 1418 }, { \"x\": 1730, \"y\": 1441 }, { \"x\": 1724, \"y\": 1564 }, { \"x\": 1205, \"y\": 1541 } ] } }, { \"description\": \"OFF\", \"boundingPoly\": { \"vertices\": [ { \"x\": 1792, \"y\": 1443 }, { \"x\": 2128, \"y\": 1458 }, { \"x\": 2122, \"y\": 1581 }, { \"x\": 1787, \"y\": 1566 } ] } }, { \"description\": \"YOUR\", \"boundingPoly\": { \"vertices\": [ { \"x\": 1219, \"y\": 1603 }, { \"x\": 1746, \"y\": 1629 }, { \"x\": 1740, \"y\": 1759 }, { \"x\": 1213, \"y\": 1733 } ] } }, { \"description\": \"ENGINE\", \"boundingPoly\": { \"vertices\": [ { \"x\": 1222, \"y\": 1771 }, { \"x\": 1944, \"y\": 1834 }, { \"x\": 1930, \"y\": 1992 }, { \"x\": 1208, \"y\": 1928 } ] } } ], \"fullTextAnnotation\": { \"pages\": [ ... ] }, \"paragraphs\": [ ... ] }, \"words\": [ ... }, \"symbols\": [ ... } ] } ], \"blockType\": \"TEXT\" }, ... ] } ], \"text\": \"WAITING?\\nPLEASE\\nTURN OFF\\nYOUR\\nENGINE\\n\" } } ] }\n\nGo\n\nBefore trying this sample, follow the Go setup instructions in the Vision quickstart using client libraries. For more information, see the Vision Go API reference documentation.\n\nTo authenticate to Vision, set up Application Default Credentials. For more information, see Set up authentication for a local development environment.\n\n// detectText gets text from the Vision API for an image at the given file path. func detectTextURI(w io.Writer, file string) error { ctx := context.Background() client, err := vision.NewImageAnnotatorClient(ctx) if err != nil { return err } image := vision.NewImageFromURI(file) annotations, err := client.DetectTexts(ctx, image, nil, 10) if err != nil { return err } if len(annotations) == 0 { fmt.Fprintln(w, \"No text found.\") } else { fmt.Fprintln(w, \"Text:\") for _, annotation := range annotations { fmt.Fprintf(w, \"%q\\n\", annotation.Description) } } return nil }\n\nJava\n\nBefore trying this sample, follow the Java setup instructions in the Vision API Quickstart Using Client Libraries. For more information, see the Vision API Java reference documentation.\n\nimport com.google.cloud.vision.v1.AnnotateImageRequest; import com.google.cloud.vision.v1.AnnotateImageResponse; import com.google.cloud.vision.v1.BatchAnnotateImagesResponse; import com.google.cloud.vision.v1.EntityAnnotation; import com.google.cloud.vision.v1.Feature; import com.google.cloud.vision.v1.Image; import com.google.cloud.vision.v1.ImageAnnotatorClient; import com.google.cloud.vision.v1.ImageSource; import java.io.IOException; import java.util.ArrayList; import java.util.List; public class DetectTextGcs { public static void detectTextGcs() throws IOException { // TODO(developer): Replace these variables before running the sample. String filePath = \"gs://your-gcs-bucket/path/to/image/file.jpg\"; detectTextGcs(filePath); } // Detects text in the specified remote image on Google Cloud Storage. public static void detectTextGcs(String gcsPath) throws IOException { List<AnnotateImageRequest> requests = new ArrayList<>(); ImageSource imgSource = ImageSource.newBuilder().setGcsImageUri(gcsPath).build(); Image img = Image.newBuilder().setSource(imgSource).build(); Feature feat = Feature.newBuilder().setType(Feature.Type.TEXT_DETECTION).build(); AnnotateImageRequest request = AnnotateImageRequest.newBuilder().addFeatures(feat).setImage(img).build(); requests.add(request); // Initialize client that will be used to send requests. This client only needs to be created // once, and can be reused for multiple requests. After completing all of your requests, call // the \"close\" method on the client to safely clean up any remaining background resources. try (ImageAnnotatorClient client = ImageAnnotatorClient.create()) { BatchAnnotateImagesResponse response = client.batchAnnotateImages(requests); List<AnnotateImageResponse> responses = response.getResponsesList(); for (AnnotateImageResponse res : responses) { if (res.hasError()) { System.out.format(\"Error: %s%n\", res.getError().getMessage()); return; } // For full list of available annotations, see http://g.co/cloud/vision/docs for (EntityAnnotation annotation : res.getTextAnnotationsList()) { System.out.format(\"Text: %s%n\", annotation.getDescription()); System.out.format(\"Position : %s%n\", annotation.getBoundingPoly()); } } } } }\n\nNode.js\n\nBefore trying this sample, follow the Node.js setup instructions in the Vision quickstart using client libraries. For more information, see the Vision Node.js API reference documentation.\n\nTo authenticate to Vision, set up Application Default Credentials. For more information, see Set up authentication for a local development environment.\n\n// Imports the Google Cloud client libraries const vision = require('@google-cloud/vision'); // Creates a client const client = new vision.ImageAnnotatorClient(); /** * TODO(developer): Uncomment the following lines before running the sample. */ // const bucketName = 'Bucket where the file resides, e.g. my-bucket'; // const fileName = 'Path to file within bucket, e.g. path/to/image.png'; // Performs text detection on the gcs file const [result] = await client.textDetection(`gs://${bucketName}/${fileName}`); const detections = result.textAnnotations; console.log('Text:'); detections.forEach(text => console.log(text));\n\nPython\n\nBefore trying this sample, follow the Python setup instructions in the Vision quickstart using client libraries. For more information, see the Vision Python API reference documentation.\n\nTo authenticate to Vision, set up Application Default Credentials. For more information, see Set up authentication for a local development environment.\n\ndef detect_text_uri(uri): \"\"\"Detects text in the file located in Google Cloud Storage or on the Web.\"\"\" from google.cloud import vision client = vision.ImageAnnotatorClient() image = vision.Image() image.source.image_uri = uri response = client.text_detection(image=image) texts = response.text_annotations print(\"Texts:\") for text in texts: print(f'\\n\"{text.description}\"') vertices = [ f\"({vertex.x},{vertex.y})\" for vertex in text.bounding_poly.vertices ] print(\"bounds: {}\".format(\",\".join(vertices))) if response.error.message: raise Exception( \"{}\\nFor more info on error messages, check: \" \"https://cloud.google.com/apis/design/errors\".format(response.error.message) )\n\nAdditional languages\n\nC#: Please follow the C# setup instructions on the client libraries page and then visit the Vision reference documentation for .NET.\n\nPHP: Please follow the PHP setup instructions on the client libraries page and then visit the Vision reference documentation for PHP.\n\nRuby: Please follow the Ruby setup instructions on the client libraries page and then visit the Vision reference documentation for Ruby.\n\nSpecify the language (optional)\n\nBoth types of OCR requests support one or more languageHints that specify the language of any text in the image. However, an empty value usually yields the best results, because omitting a value enables automatic language detection. For languages based on the Latin alphabet, setting languageHints is not needed. In rare cases, when the language of the text in the image is known, setting a hint helps get better results (although it can be a significant hindrance if the hint is wrong). Text detection returns an error if one or more of the specified languages is not one of the supported languages.\n\nIf you choose to provide a language hint, modify the body of your request (request.json file) to provide the string of one of the supported languages in the imageContext.languageHints field as shown in the following sample:\n\n{ \"requests\": [ { \"image\": { \"source\": { \"imageUri\": \"IMAGE_URL\" } }, \"features\": [ { \"type\": \"DOCUMENT_TEXT_DETECTION\" } ], \"imageContext\": { \"languageHints\": [\"en-t-i0-handwrit\"] } } ] }\n\nMulti-regional support\n\nYou can now specify continent-level data storage and OCR processing. The following regions are currently supported:\n\nus: USA country only\n\neu: The European Union\n\nLocations\n\nCloud Vision offers you some control over where the resources for your project are stored and processed. In particular, you can configure Cloud Vision to store and process your data only in the European Union.\n\nBy default Cloud Vision stores and processes resources in a Global location, which means that Cloud Vision doesn't guarantee that your resources will remain within a particular location or region. If you choose the European Union location, Google will store your data and process it only in the European Union. You and your users can access the data from any location.\n\nSetting the location using the API\n\nThe Vision API supports a global API endpoint (vision.googleapis.com) and also two region-based endpoints: a European Union endpoint (eu-vision.googleapis.com) and United States endpoint (us-vision.googleapis.com). Use these endpoints for region-specific processing. For example, to store and process your data in the European Union only, use the URI eu-vision.googleapis.com in place of vision.googleapis.com for your REST API calls:\n\nhttps://eu-vision.googleapis.com/v1/projects/PROJECT_ID/locations/eu/images:annotate\n\nhttps://eu-vision.googleapis.com/v1/projects/PROJECT_ID/locations/eu/images:asyncBatchAnnotate\n\nhttps://eu-vision.googleapis.com/v1/projects/PROJECT_ID/locations/eu/files:annotate\n\nhttps://eu-vision.googleapis.com/v1/projects/PROJECT_ID/locations/eu/files:asyncBatchAnnotate\n\nTo store and process your data in the United States only, use the US endpoint (us-vision.googleapis.com) with the preceding methods.\n\nSetting the location using the client libraries\n\nThe Vision API client libraries accesses the global API endpoint (vision.googleapis.com) by default. To store and process your data in the European Union only, you need to explicitly set the endpoint (eu-vision.googleapis.com). The following code samples show how to configure this setting.\n\nREST\n\nBefore using any of the request data, make the following replacements:\n\nREGION_ID: One of the valid regional location identifiers:\n\nus: USA country only\n\neu: The European Union\n\nCLOUD_STORAGE_IMAGE_URI: the path to a valid image file in a Cloud Storage bucket. You must at least have read privileges to the file. Example:\n\ngs://cloud-samples-data/vision/ocr/sign.jpg\n\nPROJECT_ID: Your Google Cloud project ID.\n\nHTTP method and URL:\n\nPOST https://REGION_ID-vision.googleapis.com/v1/projects/PROJECT_ID/locations/REGION_ID/images:annotate\n\nRequest JSON body:\n\n{ \"requests\": [ { \"image\": { \"source\": { \"imageUri\": \"CLOUD_STORAGE_IMAGE_URI\" } }, \"features\": [ { \"type\": \"TEXT_DETECTION\" } ] } ] }\n\nTo send your request, choose one of these options:\n\ncurl\n\nSave the request body in a file named request.json, and execute the following command:\n\ncurl -X POST \\\n\n-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n\n-H \"x-goog-user-project: PROJECT_ID\" \\\n\n-H \"Content-Type: application/json; charset=utf-8\" \\\n\n-d @request.json \\\n\n\"https://REGION_ID-vision.googleapis.com/v1/projects/PROJECT_ID/locations/REGION_ID/images:annotate\"\n\nPowerShell\n\nSave the request body in a file named request.json, and execute the following command:\n\n$cred = gcloud auth print-access-token\n\n$headers = @{ \"Authorization\" = \"Bearer $cred\"; \"x-goog-user-project\" = \"PROJECT_ID\" }\n\nInvoke-WebRequest `\n\n-Method POST `\n\n-Headers $headers `\n\n-ContentType: \"application/json; charset=utf-8\" `\n\n-InFile request.json `\n\n-Uri \"https://REGION_ID-vision.googleapis.com/v1/projects/PROJECT_ID/locations/REGION_ID/images:annotate\" | Select-Object -Expand Content\n\nIf the request is successful, the server returns a 200 OK HTTP status code and the response in JSON format.\n\nA TEXT_DETECTION response includes the detected phrase, its bounding box, and individual words and their bounding boxes.\n\nResponse\n\n{ \"responses\": [ { \"textAnnotations\": [ { \"locale\": \"en\", \"description\": \"WAITING?\\nPLEASE\\nTURN OFF\\nYOUR\\nENGINE\\n\", \"boundingPoly\": { \"vertices\": [ { \"x\": 341, \"y\": 828 }, { \"x\": 2249, \"y\": 828 }, { \"x\": 2249, \"y\": 1993 }, { \"x\": 341, \"y\": 1993 } ] } }, { \"description\": \"WAITING?\", \"boundingPoly\": { \"vertices\": [ { \"x\": 352, \"y\": 828 }, { \"x\": 2248, \"y\": 911 }, { \"x\": 2238, \"y\": 1148 }, { \"x\": 342, \"y\": 1065 } ] } }, { \"description\": \"PLEASE\", \"boundingPoly\": { \"vertices\": [ { \"x\": 1210, \"y\": 1233 }, { \"x\": 1907, \"y\": 1263 }, { \"x\": 1902, \"y\": 1383 }, { \"x\": 1205, \"y\": 1353 } ] } }, { \"description\": \"TURN\", \"boundingPoly\": { \"vertices\": [ { \"x\": 1210, \"y\": 1418 }, { \"x\": 1730, \"y\": 1441 }, { \"x\": 1724, \"y\": 1564 }, { \"x\": 1205, \"y\": 1541 } ] } }, { \"description\": \"OFF\", \"boundingPoly\": { \"vertices\": [ { \"x\": 1792, \"y\": 1443 }, { \"x\": 2128, \"y\": 1458 }, { \"x\": 2122, \"y\": 1581 }, { \"x\": 1787, \"y\": 1566 } ] } }, { \"description\": \"YOUR\", \"boundingPoly\": { \"vertices\": [ { \"x\": 1219, \"y\": 1603 }, { \"x\": 1746, \"y\": 1629 }, { \"x\": 1740, \"y\": 1759 }, { \"x\": 1213, \"y\": 1733 } ] } }, { \"description\": \"ENGINE\", \"boundingPoly\": { \"vertices\": [ { \"x\": 1222, \"y\": 1771 }, { \"x\": 1944, \"y\": 1834 }, { \"x\": 1930, \"y\": 1992 }, { \"x\": 1208, \"y\": 1928 } ] } } ], \"fullTextAnnotation\": { \"pages\": [ ... ] }, \"paragraphs\": [ ... ] }, \"words\": [ ... }, \"symbols\": [ ... } ] } ], \"blockType\": \"TEXT\" }, ... ] } ], \"text\": \"WAITING?\\nPLEASE\\nTURN OFF\\nYOUR\\nENGINE\\n\" } } ] }\n\nGo\n\nBefore trying this sample, follow the Go setup instructions in the Vision quickstart using client libraries. For more information, see the Vision Go API reference documentation.\n\nTo authenticate to Vision, set up Application Default Credentials. For more information, see Set up authentication for a local development environment.\n\nimport ( \"context\" \"fmt\" vision \"cloud.google.com/go/vision/apiv1\" \"google.golang.org/api/option\" ) // setEndpoint changes your endpoint. func setEndpoint(endpoint string) error { // endpoint := \"eu-vision.googleapis.com:443\" ctx := context.Background() client, err := vision.NewImageAnnotatorClient(ctx, option.WithEndpoint(endpoint)) if err != nil { return fmt.Errorf(\"NewImageAnnotatorClient: %w\", err) } defer client.Close() return nil }\n\nJava\n\nBefore trying this sample, follow the Java setup instructions in the Vision API Quickstart Using Client Libraries. For more information, see the Vision API Java reference documentation.\n\nImageAnnotatorSettings settings = ImageAnnotatorSettings.newBuilder().setEndpoint(\"eu-vision.googleapis.com:443\").build(); // Initialize client that will be used to send requests. This client only needs to be created // once, and can be reused for multiple requests. After completing all of your requests, call // the \"close\" method on the client to safely clean up any remaining background resources. ImageAnnotatorClient client = ImageAnnotatorClient.create(settings);\n\nNode.js\n\nBefore trying this sample, follow the Node.js setup instructions in the Vision quickstart using client libraries. For more information, see the Vision Node.js API reference documentation.\n\nTo authenticate to Vision, set up Application Default Credentials. For more information, see Set up authentication for a local development environment.\n\n// Imports the Google Cloud client library const vision = require('@google-cloud/vision'); async function setEndpoint() { // Specifies the location of the api endpoint const clientOptions = {apiEndpoint: 'eu-vision.googleapis.com'}; // Creates a client const client = new vision.ImageAnnotatorClient(clientOptions); // Performs text detection on the image file const [result] = await client.textDetection('./resources/wakeupcat.jpg'); const labels = result.textAnnotations; console.log('Text:'); labels.forEach(label => console.log(label.description)); } setEndpoint();\n\nPython\n\nBefore trying this sample, follow the Python setup instructions in the Vision quickstart using client libraries. For more information, see the Vision Python API reference documentation.\n\nTo authenticate to Vision, set up Application Default Credentials. For more information, see Set up authentication for a local development environment.\n\nfrom google.cloud import vision client_options = {\"api_endpoint\": \"eu-vision.googleapis.com\"} client = vision.ImageAnnotatorClient(client_options=client_options)\n\nTry it\n\nTry text detection and document text detection below. You can use the image specified already (gs://cloud-samples-data/vision/ocr/sign.jpg) by clicking Execute, or you can specify your own image in its place.\n\nTo try document text detection, update the value of type to DOCUMENT_TEXT_DETECTION.\n\nRequest body:\n\n{ \"requests\": [ { \"features\": [ { \"type\": \"TEXT_DETECTION\" } ], \"image\": { \"source\": { \"imageUri\": \"gs://cloud-samples-data/vision/ocr/sign.jpg\" } } } ] }"
    }
}